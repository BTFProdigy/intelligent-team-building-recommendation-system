Proceedings of the 12th Conference of the European Chapter of the ACL, pages 229?237,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Cognitively Motivated Features for Readability Assessment 
 
 
 Lijun Feng  No?mie Elhadad  Matt Huenerfauth 
 The City University of New York,  Columbia University  The City University of New York, 
 Graduate Center  New York, NY, USA  Queens College & Graduate Center 
 New York, NY, USA  noemie@dbmi.columbia.edu New York, NY, USA 
 lijun7.feng@gmail.com  matt@cs.qc.cuny.edu 
 
  
 
Abstract 
We investigate linguistic features that correlate 
with the readability of texts for adults with in-
tellectual disabilities (ID).  Based on a corpus 
of texts (including some experimentally meas-
ured for comprehension by adults with ID), we 
analyze the significance of novel discourse-
level features related to the cognitive factors 
underlying our users? literacy challenges.  We 
develop and evaluate a tool for automatically 
rating the readability of texts for these users.  
Our experiments show that our discourse-
level, cognitively-motivated features improve 
automatic readability assessment. 
1 Introduction 
Assessing the degree of readability of a text has 
been a field of research as early as the 1920's. 
Dale and Chall define readability as ?the sum 
total (including all the interactions) of all those 
elements within a given piece of printed material 
that affect the success a group of readers have 
with it. The success is the extent to which they 
understand it, read it at optimal speed, and find it 
interesting? (Dale and Chall, 1949). It has long 
been acknowledged that readability is a function 
of text characteristics, but also of the readers 
themselves.  The literacy skills of the readers, 
their motivations, background knowledge, and 
other internal characteristics play an important 
role in determining whether a text is readable for 
a particular group of people. In our work, we 
investigate how to assess the readability of a text 
for people with intellectual disabilities (ID). 
Previous work in automatic readability as-
sessment has focused on generic features of a 
text at the lexical and syntactic level.  While such 
features are essential, we argue that audience-
specific features that model the cognitive charac-
teristics of a user group can improve the accura-
cy of a readability assessment tool.  The contri-
butions of this paper are: (1) we present a corpus 
of texts with readability judgments from adults 
with ID; (2) we propose a set of cognitively-
motivated features which operate at the discourse 
level; (3) we evaluate the utility of these features 
in predicting readability for adults with ID. 
Our framework is to create tools that benefit 
people with intellectual disabilities (ID), specifi-
cally those classified in the ?mild level? of men-
tal retardation, IQ scores 55-70.  About 3% of 
the U.S. population has intelligence test scores of 
70 or lower (U.S. Census Bureau, 2000).  People 
with ID face challenges in reading literacy.  They 
are better at decoding words (sounding them out) 
than at comprehending their meaning (Drew & 
Hardman, 2004), and most read below their men-
tal age-level (Katims, 2000).  Our research ad-
dresses two literacy impairments that distinguish 
people with ID from other low-literacy adults: 
limitations in (1) working memory and (2) dis-
course representation.  People with ID have 
problems remembering and inferring information 
from text (Fowler, 1998).  They have a slower 
speed of semantic encoding and thus units are 
lost from the working memory before they are 
processed (Perfetti & Lesgold, 1977; Hickson-
Bilsky, 1985).  People with ID also have trouble 
building cohesive representations of discourse 
(Hickson-Bilsky, 1985).  As less information is 
integrated into the mental representation of the 
current discourse, less is comprehended.   
Adults with ID are limited in their choice of 
reading material.  Most texts that they can readi-
ly understand are targeted at the level of reada-
bility of children.  However, the topics of these 
texts often fail to match their interests since they 
are meant for younger readers.  Because of the 
mismatch between their literacy and their inter-
ests, users may not read for pleasure and there-
fore miss valuable reading-skills practice time.  
In a feasibility study we conducted with adults 
229
with ID, we asked participants what they enjoyed 
learning or reading about.  The majority of our 
subjects mentioned enjoying watching the news, 
in particular local news.  Many mentioned they 
were interested in information that would be re-
levant to their daily lives.  While for some ge-
nres, human editors can prepare texts for these 
users, this is not practical for news sources that 
are frequently updated and specific to a limited 
geographic area (like local news). Our goal is to 
create an automatic metric to predict the reada-
bility of local news articles for adults with ID.  
Because of the low levels of written literacy 
among our target users, we intend to focus on 
comprehension of texts displayed on a computer 
screen and read aloud by text-to-speech software; 
although some users may depend on the text-to-
speech software, we use the term readability. 
This paper is organized as follows.  Section 2 
presents related work on readability assessment. 
Section 3 states our research hypotheses and de-
scribes our methodology.  Section 4 focuses on 
the data sets used in our experiments, while sec-
tion 5 describes the feature set we used for rea-
dability assessment along with a corpus-based 
analysis of each feature.  Section 6 describes a 
readability assessment tool and reports on evalu-
ation.  Section 7 discusses the implications of the 
work and proposes direction for future work. 
2 Related Work on Readability Metrics 
Many readability metrics have been established 
as a function of shallow features of texts, such as 
the number of syllables per word and number of 
words per sentence (Flesch, 1948; McLaughlin, 
1969; Kincaid et al, 1975). These so-called tra-
ditional readability metrics are still used today in 
many settings and domains, in part because they 
are very easy to compute. Their results, however, 
are not always representative of the complexity 
of a text (Davison and Kantor, 1982). They can 
easily misrepresent the complexity of technical 
texts, or reveal themselves un-adapted to a set of 
readers with particular reading difficulties. Other 
formulas rely on lexical information; e.g., the 
New Dale-Chall readability formula consults a 
static, manually-built list of ?easy? words to de-
termine whether a text contains unfamiliar words 
(Chall and Dale, 1995).  
Researchers in computational linguistics have 
investigated the use of statistical language mod-
els (unigram in particular) to capture the range of 
vocabulary from one grade level to another (Si 
and Callan, 2001; Collins-Thompson and Callan, 
2004). These metrics predicted readability better 
than traditional formulas when tested against a 
corpus of web pages. The use of syntactic fea-
tures was also investigated (Schwarm and Osten-
dorf, 2005; Heilman et al, 2007; Petersen and 
Ostendorf, 2009) in the assessment of text reada-
bility for English as a Second Language readers. 
While lexical features alone outperform syntactic 
features in classifying texts according to their 
reading levels, combining the lexical and syntac-
tic features yields the best results. 
Several elegant metrics that focus solely on 
the syntax of a text have also been developed.  
The Yngve (1960) measure, for instance, focuses 
on the depth of embedding of nodes in the parse 
tree; others use the ratio of terminal to non-
terminal nodes in the parse tree of a sentence 
(Miller and Chomsky, 1963; Frazier, 1985).  
These metrics have been used to analyze the 
writing of potential Alzheimer's patients to detect 
mild cognitive impairments (Roark, Mitchell, 
and Hollingshead, 2007), thereby indicating that 
cognitively motivated features of text are valua-
ble when creating tools for specific populations. 
Barzilay and Lapata (2008) presented early 
work in investigating the use of discourse to dis-
tinguish abridged from original encyclopedia 
articles.  Their focus, however, is on style detec-
tion rather than readability assessment per se.  
Coh-Metrix is a tool for automatically calculat-
ing text coherence based on features such as re-
petition of lexical items across sentences and 
latent semantic analysis (McNamara et al, 
2006).  The tool is based on comprehension data 
collected from children and college students. 
Our research differs from related work in that 
we seek to produce an automatic readability me-
tric that is tailored to the literacy skills of adults 
with ID.  Because of the specific cognitive cha-
racteristics of these users, it is an open question 
whether existing readability metrics and features 
are useful for assessing readability for adults 
with ID.  Many of these earlier metrics have fo-
cused on the task of assigning texts to particular 
elementary school grade levels.  Traditional 
grade levels may not be the ideal way to score 
texts to indicate how readable they are for adults 
with ID.  Other related work has used models of 
vocabulary (Collins-Thompson and Callan, 
2004).  Since we would like to use our tool to 
give adults with ID access to local news stories, 
we choose to keep our metric topic-independent. 
Another difference between our approach and 
previous approaches is that we have designed the 
features used by our readability metric based on 
230
the cognitive aspects of our target users.  For ex-
ample, these users are better at decoding words 
than at comprehending text meaning (Drew & 
Hardman, 2004); so, shallow features like ?sylla-
ble count per word? or unigram models of word 
frequency (based on texts designed for children) 
may be less important indicators of reading diffi-
culty.  A critical challenge for our users is to 
create a cohesive representation of discourse.  
Due to their impairments in semantic encoding 
speed, our users may have particular difficulty 
with texts that place a significant burden on 
working memory (items fall out of memory be-
fore they can be semantically encoded).   
While we focus on readability of texts, other 
projects have automatically generated texts for 
people with aphasia (Carroll et al, 1999) or low 
reading skills (Williams and Reiter, 2005). 
3 Research Hypothesis and Methods 
We hypothesize that the complexity of a text for 
adults with ID is related to the number of entities 
referred to in the text overall.  If a paragraph or a 
text refers to too many entities at once, the reader 
has to work harder at mapping each entity to a 
semantic representation and deciding how each 
entity is related to others.  On the other hand, 
when a text refers to few entities, less work is 
required both for semantic encoding and for in-
tegrating the entities into a cohesive mental re-
presentation.  Section 5.2 discusses some novel 
discourse-level features (based on the ?entity 
density? of a text) that we believe will correlate 
to comprehension by adults with ID.   
To test our hypothesis, we used the following 
methodology.  We collected four corpora (as de-
scribed in Section 4).  Three of them (Britannica, 
LiteracyNet and WeeklyReader) have been ex-
amined in previous work on readability.  The 
fourth (LocalNews) is novel and results from a 
user study we conducted with adults with ID.  
We then analyzed how significant each feature is 
on our Britannica and LiteracyNet corpora.  Fi-
nally, we combined the significant features into a 
linear regression model and experimented with 
several feature combinations. We evaluated our 
model on the WeeklyReader and LocalNews 
corpora. 
4 Corpora and Readability Judgments  
To study how certain linguistic features indicate 
the readability of a text, we collected a corpus of 
English text at different levels of readability.  An 
ideal corpus for our research would contain texts 
that have been written specifically for our au-
dience of adults with intellectual disabilities ? in 
particular if such texts were paired with alternate 
versions of each text written for a general au-
dience.  We are not aware of such texts available 
electronically, and so we have instead mostly 
collected texts written for an audience of child-
ren.  The texts come from online and commercial 
sources, and some have been analyzed previous-
ly by text simplification researchers (Petersen 
and Ostendorf, 2009).  Our corpus also contains 
some novel texts produced as part of an experi-
mental study involving adults with ID. 
4.1 Paired and Graded Generic Corpora: 
Britannica, LiteracyNet, and Weekly 
Reader 
The first section of our corpus (which we refer to 
as Britannica) has 228 articles from the Encyclo-
pedia Britannica, originally collected by (Barzi-
lay and Elhadad, 2003).  This consists of 114 
articles in two forms: original articles written for 
adults and corresponding articles rewritten for an 
audience of children.  While the texts are paired, 
the content of the texts is not identical: some de-
tails are omitted from the child version, and addi-
tional background is sometimes inserted.  The 
resulting corpus is comparable in content. 
Because we are particularly interested in mak-
ing local news articles accessible to adults with 
ID, we collected a second paired corpus, which 
we refer to as LiteracyNet, consisting of 115 
news articles made available through (West-
ern/Pacific Literacy Network / LiteracyNet, 
2008).  The collection of local CNN stories is 
available in an original and simplified/abridged 
form (230 total news articles) designed for use in 
literacy education. 
The third corpus we collected (Weekly Reader) 
was obtained from the Weekly Reader corpora-
tion (Weekly Reader, 2008).  It contains articles 
for students in elementary school.  Each text is 
labeled with its target grade level (grade 2: 174 
articles, grade 3: 289 articles, grade 4: 428 ar-
ticles, grade 5: 542 articles).  Overall, the corpus 
has 1433 articles. (U.S. elementary school grades 
2 to 5 generally are for children ages 7 to 10.) 
The corpora discussed above are similar to 
those used by Petersen and Ostendorf (2009).  
While the focus of our research is adults with ID, 
most of the texts discussed in this section have 
been simplified or written by human authors to 
be readable for children.  Despite the texts being 
intended for a different audience than the focus 
of our research, we still believe these texts to be 
231
of value.  It is rare to encounter electronically 
available corpora in which an original and a sim-
plified version of a text is paired (as in the Bri-
tannica and LiteracyNet corpora) or texts labeled 
as being at specific levels of readability (as in the 
Weekly Reader corpus). 
4.2 Readability-Specific Corpus: LocalNews 
The final section of our corpus contains local 
news articles that are labeled with comprehen-
sion scores.  These texts were produced for a fea-
sibility study involving adults with ID.  Each text 
was read by adults with ID, who then answered 
comprehension questions to measure their under-
standing of the texts.  Unlike the previous corpo-
ra, LocalNews is novel and was not investigated 
by previous research in readability. 
After obtaining university approval for our ex-
perimental protocol and informed consent 
process, we conducted a study with 14 adults 
with mild intellectual disabilities who participate 
in daytime educational programs in the New 
York area.  Participants were presented with ten 
articles collected from various local New York 
based news websites.  Some subjects saw the 
original form of an article and others saw a sim-
plified form (edited by a human author); no sub-
ject saw both versions.  The texts were presented 
in random order using software that displayed 
the text on the screen, read it aloud using text-to-
speech software, and highlighted each word as it 
was read.  Afterward, subjects were asked aloud 
multiple-choice comprehension questions. We 
defined the readability score of a story as the 
percentage of correct answers averaged across 
the subjects who read that particular story. 
A human editor performed the text simplifica-
tion with the goal of making the text more reada-
ble for adults with mild ID.  The editor made the 
following types of changes to the original news 
stories: breaking apart complex sentences, un-
embedding information in complex prepositional 
phrases and reintegrating it as separate sentences, 
replacing infrequent vocabulary items with more 
common/colloquial equivalents, omitting sen-
tences and phrases from the story that mention 
entities and phrases extraneous to the main 
theme of the article.  For instance, the original 
sentence ?They?re installing an induction loop 
system in cabs that would allow passengers with 
hearing aids to tune in specifically to the driver?s 
voice.? was transformed into ?They?re installing 
a system in cabs. It would allow passengers with 
hearing aids to listen to the driver?s voice.? 
This corpus of local news articles that have 
been human edited and scored for comprehen-
sion by adults with ID is small in size (20 news 
articles), but we consider it a valuable resource.  
Unlike the texts that have been simplified for 
children (the rest of our corpus), these texts have 
been rated for readability by actual adults with 
ID.  Furthermore, comprehension scores are de-
rived from actual reader comprehension tests, 
rather than self-perceived comprehension.  Be-
cause of the small size of this part of our corpus, 
however, we primarily use it for evaluation pur-
poses (not for training the readability models). 
5 Linguistic Features and Readability  
We now describe the set of features we investi-
gated for assessing readability automatically.  
Table 1 contains a list of the features ? including 
a short code name for each feature which may be 
used throughout this paper.  We have begun by 
implementing the simple features used by the 
Flesh-Kincaid and FOG metrics: average number 
of words per sentence, average number of syl-
lables per word, and percentage of words in the 
document with 3+ syllables. 
5.1 Basic Features Used in Earlier Work 
We have also implemented features inspired by 
earlier research on readability.  Petersen and Os-
tendorf (2009) included features calculated from 
parsing the sentences in their corpus using the 
Charniak parser (Charniak, 2000): average parse 
tree height, average number of noun phrases per 
sentence, average number of verb phrases per 
sentence, and average number of SBARs per sen-
tence. We have implemented versions of most of 
these parse-tree-related features for our project.  
We also parse the sentences in our corpus using 
Charniak?s parser and calculate the following 
features listed in Table 1: aNP, aN, aVP, aAdj, 
aSBr, aPP, nNP, nN, nVP, nAdj, nSBr, and nPP.   
5.2 Novel Cognitively-Motivated Features  
Because of the special reading characteristics of 
our target users, we have designed a set of cogni-
tively motivated features to predict readability of 
texts for adults with ID.  We have discussed how 
working memory limits the semantic encoding of 
new information by these users; so, our features 
indicate the number of entities in a text that the 
reader must keep in mind while reading each 
sentence and throughout the entire document.  It 
is our hypothesis that this ?entity density? of a 
232
text plays an important role in the difficulty of 
that text for readers with intellectual disabilities. 
The first set of features incorporates the Ling-
Pipe named entity detection software (Alias-i, 
2008), which detects three types of entities: per-
son, location, and organization.  We also use the 
part-of-speech tagger in LingPipe to identify the 
common nouns in the document, and we find the 
union of the common nouns and the named entity 
noun phrases in the text.  The union of these two 
sets is our definition of ?entity? for this set of 
features.  We count both the total number of 
?entity mentions? in a text (each token appear-
ance of an entity) and the total number of unique 
entities (exact-string-match duplicates only 
counted once).  Table 1 lists these features: nEM, 
nUE, aEM, and aUE.  We count the totals per 
document to capture how many entities the read-
er must keep track of while reading the docu-
ment.  We also expect sentences with more enti-
ties to be more difficult for our users to semanti-
cally encode due to working memory limitations; 
so, we also count the averages per sentence to 
capture how many entities the reader must keep 
in mind to understand each sentence.   
To measure the working memory burden of a 
text, we?d like to capture the number of dis-
course entities that a reader must keep in mind.  
However, the ?unique entities? identified by the 
named entity recognition tool may not be a per-
fect representation of this ? several unique enti-
ties may actually refer to the same real-world 
entity under discussion.  To better model how 
multiple noun phrases in a text refer to the same 
entity or concept, we have also built features us-
ing lexical chains (Galley and McKeown, 2003).  
Lexical chains link nouns in a document con-
nected by relations like synonymy or hyponomy; 
chains can indicate concepts that recur through-
out a text.  A lexical chain has both a length 
(number of noun phrases it includes) and a span 
(number of words in the document between the 
first noun phrase at the beginning of the chain 
and the last noun phrase that is part of the chain).  
We calculate the number of lexical chains in the 
document (nLC) and those with a span greater 
than half the document length (nLC2).  We be-
lieve these features may indicate the number of 
entities/concepts that a reader must keep in mind 
during a document and the subset of very impor-
tant entities/concepts that are the main topic of 
the document.  The average length and average 
span of the lexical chains in a document (aLCL 
and aLCS) may also indicate how many of the 
chains in the document are short-lived, which 
may mean that they are ancillary enti-
ties/concepts, not the main topics. 
The final two features in Table 1 (aLCw and 
aLCe) use the concept of an ?active? chain.  At a 
particular location in a text, we define a lexical 
chain to be ?active? if the span (between the first 
and last noun in the lexical chain) includes the 
current location.  We expect these features may 
indicate the total number of concepts that the 
reader needs to keep in mind during a specific 
moment in time when reading a text.  Measuring 
the average number of concepts that the reader of 
a text must keep in mind may suggest the work-
ing memory burden of the text over time.  We 
were unsure if individual words or individual 
noun-phrases in the document should be used as 
the basic unit of ?time? for the purpose of aver-
aging the number of active lexical chains; so, we 
included both features. 
5.3 Testing the Significance of Features 
To select which features to include in our auto-
matic readability assessment tool (in Section 6), 
Code Feature
aWPS average number of words per sentence
aSPW average number of syllables per word
%3+S % of words in document with 3+ syllables
aNP avg. num. NPs per sentence
aN avg. num. common+proper nouns per sentence
aVP avg. num. VPs per sentence
aAdj avg. num. Adjectives per sentence
aSBr avg. num. SBARs per sentence
aPP avg. num. prepositional phrases per sentence
nNP total number of NPs per sentence
nN total num. of common+proper nouns in document
nVP total number of VPs in the document
nAdj total number of Adjectives in the document
nSBr total number of SBARs in the document
nPP total num. of prepositional phrases in document
nEM number of entity mentions in document
nUE number of unique entit ies in document
aEM avg. num. entity mentions per sentence
aUE avg. num. unique entit ies per sentence
nLC number of lexical chains in document
nLC2 num. lex. chains, span > half document length
aLCL average lexical chain length
aLCS average lexical chain span
aLCw avg. num. lexical chains active at  each word
aLCn avg. num. lexical chains active at  each NP
Table 1: Implemented Features
233
we analyzed the documents in our paired corpora 
(Britannica and LiteracyNet).  Because they con-
tain a complex and a simplified version of each 
article, we can examine differences in readability 
while holding the topic and genre constant.  We 
calculated the value of each feature for each doc-
ument, and we used a paired t-test to determine if 
the difference between the complex and simple 
documents was significant for that corpus. 
Table 2 contains the results of this feature se-
lection process; the columns in the table indicate 
the values for the following corpora: Britannica 
complex, Britannica simple, LiteracyNet com-
plex, and LiteracyNet simple.  An asterisk ap-
pears in the ?Sig? column if the difference be-
tween the feature values for the complex vs. 
simple documents is statistically significant for 
that corpus (significance level: p<0.00001).   
The only two features which did not show a 
significant difference (p>0.01) between the com-
plex and simple versions of the articles were: 
average lexical chain length (aLCL) and number 
of lexical chains with span greater than half the 
document length (nLC2).  The lack of signific-
ance for aLCL may be explained by the vast ma-
jority of lexical chains containing few members; 
complex articles contained more of these chains 
? but their chains did not contain more members.  
In the case of nLC2, over 80% of the articles in 
each category contained no lexical chains whose 
span was greater than half the document length.  
The rarity of a lexical chain spanning the majori-
ty of a document may have led to there being no 
significant difference between complex/simple. 
6 A Readability Assessment Tool 
After testing the significance of features using 
paired corpora, we used linear regression and our 
graded corpus (Weekly Reader) to build a reada-
bility assessment tool.  To evaluate the tool?s 
usefulness for adults with ID, we test the correla-
tion of its scores with the LocalNews corpus. 
6.1 Versions of Our Model 
We began our evaluation by implementing three 
versions of our automatic readability assessment 
tool.  The first version uses only those features 
studied by previous researchers (aWPS, aSPW, 
%3+S, aNP, aN, aVP, aAdj, aSBr, aPP, nNP, nN, 
nVP, nAdj, nSBr, nPP).  The second version uses 
only our novel cognitively motivated features 
(section 5.2).  The third version uses the union of 
both sets of features.  By building three versions 
of the tool, we can compare the relative impact 
of our novel cognitively-motivated features.  For 
all versions, we have only included those fea-
tures that showed a significant difference be-
tween the complex and simple articles in our 
paired corpora (as discussed in section 5.3). 
6.2 Learning Technique and Training Data 
Early work on automatic readability analysis 
framed the problem as a classification task: 
creating multiple classifiers for labeling a text as 
being one of several elementary school grade 
levels (Collins-Thompson and Callan, 2004).  
Because we are focusing on a unique user group 
with special reading challenges, we do not know 
a priori what level of text difficulty is ideal for 
our users.  We would not know where to draw 
category boundaries for classification.  We also 
prefer that our assessment tool assign numerical 
difficulty scores to texts.  Thus, after creating 
this tool, we can conduct further reading com-
prehension experiments with adults with ID to 
determine what threshold (for readability scores 
assigned by our tool) is appropriate for our users. 
Feature
Brit. 
Com.
Brit. 
Simp. Sig
LitN. 
Com.
LitN. 
Simp. Sig
aWPS 20.13 14.37 * 17.97 12.95 *
aSPW 1.708 1.655 * 1.501 1.455 *
%3+S 0.196 0.177 * 0.12 0.101 *
aNP 8.363 6.018 * 6.519 4.691 *
aN 7.024 5.215 * 5.319 3.929 *
aVP 2.334 1.868 * 3.806 2.964 *
aAdj 1.95 1.281 * 1.214 0.876 *
aSBr 0.266 0.205 * 0.793 0.523 *
aPP 2.858 1.936 * 1.791 1.22 *
nNP 798 219.2 * 150.2 102.9 *
nN 668.4 190.4 * 121.4 85.75 *
nVP 242.8 69.19 * 88.2 65.52 *
nAdj 205 47.32 * 28.11 19.04 *
nSBr 31.33 7.623 * 18.16 11.43 *
nPP 284.7 70.75 * 41.06 26.79 *
nEM 624.2 172.7 * 115.2 82.83 *
nUE 355 117 * 81.56 54.94 *
aEM 6.441 4.745 * 5.035 3.789 *
aUE 4.579 3.305 * 3.581 2.55 *
nLC 59.21 17.57 * 12.43 8.617 *
nLC2 0.175 0.211 0.191 0.226
aLCL 3.009 3.022 2.817 2.847
aLCS 357 246.1 * 271.9 202.9 *
aLCw 1.803 1.358 * 1.407 1.091 *
aLCn 1.852 1.42 * 1.53 1.201 *
Table 2: Feature Values of Paired Corpora
234
To select features for our model, we used our 
paired corpora (Britannica and LiteracyNet) to 
measure the significance of each feature.  Now 
that we are training a model, we make use of our 
graded corpus (articles from Weekly Reader).  
This corpus contains articles that have each been 
labeled with an elementary school grade level for 
which it was written.  We divide this corpus ? 
using 80% of articles as training data and 20% as 
testing data.  We model the grade level of the 
articles using linear regression; our model is im-
plemented using R (R Development Core Team, 
2008).  
6.3 Evaluation of Our Readability Tool 
We conducted two rounds of training and evalua-
tion of our three regression models.  We also 
compare our models to a baseline readability as-
sessment tool: the popular Flesh-Kincaid Grade 
Level index (Kincaid et al, 1975).  
In the first round of evaluation, we trained and 
tested our regression models on the Weekly 
Reader corpus.  This round of evaluation helped 
to determine whether our feature-set and regres-
sion technique were successfully modeling those 
aspects of the texts that were relevant to their 
grade level.  Our results from this round of eval-
uation are presented in the form of average error 
scores.  (For each article in the Weekly Reader 
testing data, we calculate the difference between 
the output score of the model and the correct 
grade-level for that article.)  Table 3 presents the 
average error results for the baseline system and 
our three regression models.  We can see that the 
model trained on the shallow and parse-related 
features out-performs the model trained only on 
our novel features; however, the best model 
overall is the one is trained on all of the features.  
This model predicts the grade level of Weekly 
Reader articles to within roughly 0.565 grade 
levels on average.   
 
Readability Model (or baseline) Average Error 
Baseline: Flesh-Kincaid Index 2.569 
Basic Features Only 0.6032 
Cognitively Motivated Features Only 0.6110 
Basic + Cognitively-Motiv. Features 0.5650 
Table 3: Predicting Grade Level of Weekly Reader 
 
In our second round of evaluation, we trained 
the regression model on the Weekly Reader cor-
pus, but we tested it against the LocalNews cor-
pus.  We measured the correlation between our 
regression models? output and the comprehen-
sion scores of adults with ID on each text.  For 
this reason, we do not calculate the ?average er-
ror?; instead, we simply measure the correlation 
between the models? output and the comprehen-
sion scores. (We expect negative correlations 
because comprehension scores should increase as 
the predicted grade level of the text goes down.)  
Table 4 presents the correlations for our three 
models and the baseline system in the form of 
Pearson?s R-values.  We see a surprising result: 
the model trained only on the cognitively-
motivated features is more tightly correlated with 
the comprehension scores of the adults with ID.  
While the model trained on all features was bet-
ter at assigning grade levels to Weekly Reader 
articles, when we tested it on the local news ar-
ticles from our user-study, it was not the top-
performing model.  This result suggests that the 
shallow and parse-related features of texts de-
signed for children (the Weekly Reader articles, 
our training data) are not the best predictors of 
text readability for adults with ID.   
 
Readability Model (or baseline) Pearson?s R 
Baseline: Flesh-Kincaid Index -0.270 
Basic Features Only -0.283 
Cognitively Motivated Features Only -0.352 
Basic + Cognitively-Motiv. Features -0.342 
Table 4: Correlation to User-Study Comprehension 
7 Discussion 
Based on the cognitive and literacy skills of 
adults with ID, we designed novel features that 
were useful in assessing the readability of texts 
for these users.  The results of our study have 
supported our hypothesis that the complexity of a 
text for adults with ID is related to the number of 
entities referred to in the text.  These ?entity den-
sity? features enabled us to build models that 
were better at predicting text readability for 
adults with intellectual disabilities.  
This study has also demonstrated the value of 
collecting readability judgments from target us-
ers when designing a readability assessment tool.  
The results in Table 4 suggest that models 
trained on corpora containing texts designed for 
children may not always lead to accurate models 
of the readability of texts for other groups of 
low-literacy users.  Using features targeting spe-
cific aspects of literacy impairment have allowed 
us to make better use of children?s texts when 
designing a model for adults with ID. 
7.1 Future Work 
In order to study more features and models of 
readability, we will require more testing data for 
tracking progress of our readability regression 
235
models.  Our current study has illustrated the 
usefulness of texts that have been evaluated by 
adults with ID, and we therefore plan to increase 
the size of this corpus in future work.   In addi-
tion to using this corpus for evaluation, we may 
want to use it to train our regression models.  For 
this study, we trained on Weekly Reader text 
labeled with elementary school grade levels, but 
this is not ideal.  Texts designed for children may 
differ from those that are best for adults with ID, 
and ?grade levels? may not be the best way to 
rank/rate text readability for these users.  While 
our user-study comprehension-test corpus is cur-
rently too small for training, we intend to grow 
the size of this corpus in future work.   
We also plan on refining our cognitively moti-
vated features for measuring the difficulty of a 
text for our users.  Currently, we use lexical 
chain software to link noun phrases in a docu-
ment that may refer to similar entities/concepts.  
In future work, we plan to use co-reference reso-
lution software to model how multiple ?entity 
mentions? may refer to a single discourse entity.  
For comparison purposes, we plan to imple-
ment other features that have been used in earlier 
readability assessment systems.  For example, 
Petersen and Ostendorf (2009) created lists of the 
most common words from the Weekly Reader 
articles, and they used the percentage of words in 
a document not on this list as a feature.   
The overall goal of our research is to develop 
a software system that can automatically simplify 
the reading level of local news articles and 
present them in an accessible way to adults with 
ID.  Our automatic readability assessment tool 
will be a component in this future text simplifica-
tion system.  We have therefore preferred to in-
clude features in our tool that focus on aspects of 
the text that can be modified during a simplifica-
tion process.  In future work, we will study how 
to use our readability assessment tool to guide 
how a text revision system decides to modify a 
text to increase its readability for these users. 
7.2 Summary of Contributions 
We have contributed to research on automatic 
readability assessment by designing a new me-
thod for assessing the complexity of a text at the 
level of discourse.  Our novel ?entity density? 
features are based on named entity and lexical 
chain software, and they are inspired by the cog-
nitive underpinnings of the literacy challenges of 
adults with ID ? specifically, the role of slow 
semantic encoding and working memory limita-
tions.  We have demonstrated the usefulness of 
these novel features in modeling the grade level 
of elementary school texts and in correlating to 
readability judgments from adults with ID.   
Another contribution of our work is the collec-
tion of an initial corpus of texts of local news 
stories that have been manually simplified by a 
human editor.  Both the original and the simpli-
fied versions of these stories have been evaluated 
by adults with intellectual disabilities.  We have 
used these comprehension scores in the evalua-
tion phase of this study, and we have suggested 
how constructing a larger corpus of such articles 
could be useful for training readability tools. 
More broadly, this project has demonstrated 
how focusing on a specific user population, ana-
lyzing their cognitive skills, and involving them 
in a user-study has led to new insights in model-
ing text readability.  As Dale and Chall?s defini-
tion (1949) originally argued, characteristics of 
the reader are central to the issue of readability.  
We believe our user-focused research paradigm 
may be used to drive further advances in reada-
bility assessment for other groups of users. 
Acknowledgements 
We thank the Weekly Reader Corporation for 
making its corpus available for our research.  We 
are grateful to Martin Jansche for his assistance 
with the statistical data analysis and regression. 
References  
Alias-i. 2008. LingPipe 3.6.0. http://alias-
i.com/lingpipe (accessed October 1, 2008) 
Barzilay, R., Elhadad, N., 2003. Sentence alignment 
for monolingual comparable corpora. In Proc 
EMNLP, pp. 25-32. 
Barzilay R., Lapata, M., 2008. Modeling Local Cohe-
rence: An Entity-based Approach. Computational 
Linguistics. 34(1):1-34. 
Carroll, J., Minnen, G., Pearce, D., Canning, Y., Dev-
lin, S., Tait, J. 1999. Simplifying text for language-
impaired readers. In Proc. EACL Poster, p. 269. 
Chall, J.S., Dale, E., 1995. Readability Revisited: The 
New Dale-Chall Readability Formula. Brookline 
Books, Cambridge, MA. 
Charniak, E. 2000. A maximum-entropy-inspired 
parser.  In Proc. NAACL, pp. 132-139. 
Collins-Thompson, K., and Callan, J.  2004.  A lan-
guage modeling approach to predicting reading dif-
ficulty.  In Proc. NAACL, pp. 193-200. 
Dale, E. and J. S. Chall.  1949.  The concept of reada-
bility.  Elementary English 26(23). 
236
Davison, A., and Kantor, R.  1982.  On the failure of 
readability formulas to define readable texts: A case 
study from adaptations.  Reading Research Quar-
terly, 17(2):187-209. 
Drew, C.J., and Hardman, M.L. 2004.  Mental retar-
dation: A lifespan approach to people with intellec-
tual disabilities (8th ed.).  Columbus, OH: Merrill. 
Flesch, R.  1948.  A new readability yardstick.  Jour-
nal of Applied Psychology, 32:221-233. 
Fowler, A.E.  1998.  Language in mental retardation.  
In Burack, Hodapp, and Zigler (Eds.), Handbook of 
Mental Retardation and Development.  Cambridge, 
UK: Cambridge Univ. Press, pp. 290-333. 
Frazier, L.  1985.  Natural Language Parsing: Psy-
chological, Computational, and Theoretical Pers-
pectives, chapter Syntactic complexity, pp. 129-
189.  Cambridge University Press. 
Galley, M., McKeown, K. 2003. Improving Word 
Sense Disambiguation in Lexical Chaining. In 
Proc. IJCAI, pp. 1486-1488.  
Gunning, R. 1952.  The Technique of Clear Writing. 
McGraw-Hill. 
Heilman, M., Collins-Thompson, K., Callan, J., and 
Eskenazi, M.  2007.  Combining lexical and gram-
matical features to improve readability measures for 
first and second language texts.  In Proc. NAACL, 
pp. 460-467. 
Hickson-Bilsky, L.  1985.  Comprehension and men-
tal retardation.  International Review of Research in 
Mental Retardation, 13: 215-246. 
Katims, D.S.  2000.  Literacy instruction for people 
with mental retardation: Historical highlights and 
contemporary analysis.  Education and Training in 
Mental Retardation and Developmental Disabili-
ties, 35(1): 3-15. 
Kincaid, J. P., Fishburne, R. P., Rogers, R. L., and 
Chissom, B. S.  1975.  Derivation of new readabili-
ty formulas for Navy enlisted personnel, Research 
Branch Report 8-75, Millington, TN. 
Kincaid, J., Fishburne, R., Rodgers, R., and Chisson, 
B.  1975.  Derivation of new readability formulas 
for navy enlisted personnel.  Technical report, Re-
search Branch Report 8-75, U.S.  Naval Air Station. 
McLaughlin, G.H.  1969.  SMOG grading - a new 
readability formula.  Journal of Reading, 
12(8):639-646. 
McNamara, D.S., Ozuru, Y., Graesser, A.C., & Lou-
werse, M. (2006) Validating Coh-Metrix., In Proc. 
Conference of the Cognitive Science Society, pp. 
573.   
Miller, G., and Chomsky, N.  1963.  Handbook of 
Mathematical Psychology, chapter Finatary models 
of language users, pp. 419-491.  Wiley. 
Perfetti, C., and Lesgold, A.  1977.  Cognitive 
Processes in Comprehension, chapter Discourse 
Comprehension and sources of individual differ-
ences.  Erlbaum. 
Petersen, S.E., Ostendorf, M. 2009. A machine learn-
ing approach to reading level assessment. Computer 
Speech and Language, 23: 89-106. 
R Development Core Team. 2008. R: A Language 
and Environment for Statistical Computing. Vienna, 
Austria: R Foundation for Statistical Computing. 
http://www.R-project.org 
Roark, B., Mitchell, M., and Hollingshead, K.  2007.  
Syntactic complexity measures for detecting mild 
cognitive impairment.  In Proc. ACL Workshop on 
Biological, Translational, and Clinical Language 
Processing (BioNLP'07), pp. 1-8. 
Schwarm, S., and Ostendorf, M.  2005.  Reading level 
assessment using support vector machines and sta-
tistical language models.  In Proc. ACL, pp. 523-
530. 
Si, L., and Callan, J.  2001.  A statistical model for 
scientific readability.  In Proc. CIKM, pp. 574-576. 
Stenner, A.J. 1996. Measuring reading comprehension 
with the Lexile framework.  4th North American 
Conference on Adolescent/Adult Literacy. 
U.S. Census Bureau.  2000.  Projections of the total 
resident population by five-year age groups and 
sex, with special age categories: Middle series 
2025-2045.  Washington: U.S. Census Bureau, Po-
pulations Projections Program, Population Division. 
Weekly Reader, 2008. http://www.weeklyreader.com 
(Accessed Oct., 2008). 
Western/Pacific Literacy Network / Literacyworks, 
2008. CNN SF learning resources. 
http://literacynet.org/cnnsf/ (Accessed Oct., 2008). 
Williams, S., Reiter, E. 2005. Generating readable 
texts for readers with low basic skills. In Proc. Eu-
ropean Workshop on Natural Language Genera-
tion, pp. 140-147. 
Yngve, V.  1960.  A model and a hypothesis for lan-
guage structure.  American Philosophical Society, 
104: 446-466. 
237
Sentence Ordering in Multidocument Summarization
Regina Barzilay
Computer Science
Department
1214 Amsterdam Ave
New York, 10027, NY, USA
regina@cs.columbia.edu
Noemie Elhadad
Computer Science
Department
1214 Amsterdam Ave
New York, 10027, NY, USA
noemie@cs.columbia.edu
Kathleen R. McKeown
Computer Science
Department
1214 Amsterdam Ave
New York, 10027, NY, USA
kathy@cs.columbia.edu
ABSTRACT
The problem of organizing information for multidocument
summarization so that the generated summary is coherent
has received relatively little attention. In this paper, we
describe two naive ordering techniques and show that they
do not perform well. We present an integrated strategy for
ordering information, combining constraints from chronolog-
ical order of events and cohesion. This strategy was derived
from empirical observations based on experiments asking hu-
mans to order information. Evaluation of our augmented
algorithm shows a signicant improvement of the ordering
over the two naive techniques we used as baseline.
1. INTRODUCTION
Multidocument summarization poses a number of new
challenges over single document summarization. Researchers
have already investigated issues such as identifying repeti-
tions or contradictions across input documents and deter-
mining which information is salient enough to include in the
summary [1, 3, 6, 11, 15, 19]. One issue that has received
little attention is how to organize the selected information
so that the output summary is coherent. Once all the rel-
evant pieces of information have been selected across the
input documents, the summarizer has to decide in which
order to present them so that the whole text makes sense.
In single document summarization, one possible ordering of
the extracted information is provided by the input docu-
ment itself. However, [10] observed that, in single document
summaries written by professional summarizers, extracted
sentences do not retain their precedence orders in the sum-
mary. Moreover, in the case of multiple input documents,
this does not provide a useful solution: information may
be drawn from dierent documents and therefore, no one
document can provide an ordering. Furthermore, the order
between two pieces of information can change signicantly
from one document to another.
We investigate constraints on ordering in the context of
multidocument summarization. We rst describe two naive
ordering algorithms, used in several systems and show that
they do not yield satisfactory results. The rst, Majority
Ordering, is critically linked to the level of similarity of the
information ordering across the input texts. But many times
input texts have dierent structure, and therefore, this al-
gorithm is not acceptable. The second, Chronological Or-
dering, can produce good results when the information is
event-based and can, therefore, be ordered based on tempo-
ral occurence. However, texts do not always refer to events.
We have conducted experiments to identify additional con-
straints using a manually built collection of multiple order-
ings of texts. These experiments show that cohesion as an
important constraint. While it is recognized in the gener-
ation community that cohesion is a necessary feature for a
generated text, we provide an operational way to automati-
cally ensure cohesion when ordering sentences in an output
summary. We augment the Chronological Ordering algo-
rithm with a cohesion constraint, and compare it to the
naive algorithms.
Our framework is the MultiGen system [15], a domain in-
dependent multidocument summarizer which has been trained
and tested on news articles. In the following sections, we
rst give an overview of MultiGen. We then describe the
two naive ordering algorithms and evaluate them. We follow
this with a study of multiple orderings produced by humans.
This allows us to determine how to improve the Chronologi-
cal Ordering algorithm using cohesion as an additional con-
straint. The last section describes the augmented algorithm
along with its evaluation.
2. MULTIGEN OVERVIEW
MultiGen operates on a set of news articles describing
the same event. It creates a summary which synthesizes
common information across documents. In the case of mul-
tidocument summarization of articles about the same event,
source articles can contain both repetitions and contradic-
tions. Extracting all the similar sentences would produce a
verbose and repetitive summary, while extracting only some
of the similar sentences would produce a summary biased
towards some sources. MultiGen uses a comparison of ex-
tracted similar sentences to select the appropriate phrases
to include in the summary and reformulates them as a new
text.
MultiGen consists of an analysis and a generation compo-
nent. The analysis component [7] identies units of text
which convey similar information across the input docu-
ments using statistical techniques and shallow text analy-
sis. Once similar text units are identied, we cluster them
into themes. Themes are sets of sentences from dierent
documents that contain repeated information and do not
necessarily contain sentences from all the documents. For
each theme, the generation component [1] identies phrases
which are in the intersection of the theme sentences, and
selects them as part of the summary. The intersection sen-
tences are then ordered to produce a coherent text.
3. NAIVE ORDERING ALGORITHMS ARE
NOT SUFFICIENT
When producing a summary, any multidocument summa-
rization system has to choose in which order to present the
output sentences. In this section, we describe two algorithms
for ordering sentences suitable for domain independent mul-
tidocument summarization. The rst algorithm, Majority
Ordering (MO), relies only on the original orders of sen-
tences in the input documents. It is the rst solution one can
think of when addressing the ordering problem. The second
one, Chronological Ordering (CO) uses time related features
to order sentences. We analyze this strategy because it was
originally implemented in MultiGen and followed by other
summarization systems [18]. In the MultiGen framework,
ordering sentences is equivalent to ordering themes and we
describe the algorithms in terms of themes, but the con-
cepts can be adapted to other summarization systems such
as [3]. Our evaluation shows that these methods alone do
not provide an adequate strategy for ordering.
3.1 Majority Ordering
3.1.1 The Algorithm
Typically, in single document summarization, the order
of sentences in the output summary is determined by their
order in the input text. This strategy can be adapted to
multidocument summarization. Consider two themes, Th
1
and Th
2
; if sentences from Th
1
preceed sentences from Th
2
in all input texts, then presenting Th
1
before Th
2
is an ac-
ceptable order. But, when the order between sentences from
Th
1
and Th
2
varies from one text to another, this strategy
is not valid anymore. One way to dene the order between
Th
1
and Th
2
is to adopt the order occuring in the majority
of the texts where Th
1
and Th
2
occur. This strategy denes
a pairwise order between themes. However, this pairwise re-
lation is not transitive; for example, given the themes Th
1
and Th
2
occuring in a text, Th
2
and Th
3
occuring in another
text, and Th
3
and Th
1
occuring in a third text, there is a
conict between the orders (Th
1
; Th
2
; Th
3
) and (Th
3
; Th
1
).
Since transitivity is a necessary condition for a relation to be
called an order, this relation does not form a global order.
We, therefore, have to expand this pairwise relation to
a global order. In other words, we have to nd a linear
order between themes which maximizes the agreement be-
tween the orderings imposed by the input texts. For each
pair of themes, Th
i
and Th
j
, we keep two counts, C
i;j
and
C
j;i
| C
i;j
is the number of input texts in which sentences
from Th
i
occur before sentences from Th
j
and C
j;i
is the
same for the opposite order. The weight of a linear order
(Th
i
1
; : : : ; Th
i
k
) is dened as the sum of the counts for every
pair C
i
l
;i
m
, such that i
l
 i
m
and l; m 2 f1 : : : kg. Stating
this problem in terms of a directed graph where nodes are
themes, and a vertex from Th
i
to Th
j
has for weight C
i;j
,
we are looking for a path with maximal weight which tra-
verses each node exactly once. Unfortunately this problem
is NP-complete; this can be shown by reducing the travel-
ing salesman problem to this problem. Despite this fact, we
still can apply this ordering, because typically the length of
the output summary is limited to a small number of sen-
tences. For longer summaries, the approximation algorithm
described in [4] can be applied. Figures 1 and 2 show ex-
amples of produced summaries.
The main problem with this strategy is that it can pro-
duce several orderings with the same weight. This happens
when there is a tie between two opposite orderings. In this
situation, this strategy does not provide enough constraints
to determine one optimal ordering; one order is chosen ran-
domly among the orders with maximal weight.
The man accused of rebombing two Manhattan subways
in 1994 was convicted Thursday after the jury rejected the
notion that the drug Prozac led him to commit the crimes.
He was found guilty of two counts of attempted murder,
14 counts of rst-degree assault and two counts of criminal
possession of a weapon.
In December 1994, Leary ignited rebombs on two Manhat-
tan subway trains. The second blast injured 50 people { 16
seriously, including Leary.
Leary wanted to extort money from the Transit Authority.
The defense argued that Leary was not responsible for his
actions because of "toxic psychosis" caused by the Prozac.
Figure 1: A summary produced using the Majority Or-
dering algorithm, graded as Good.
A man armed with a handgun has surrendered to Spanish
authorities, peacefully ending a hijacking of a Moroccan jet.
O?cials in Spain say a person commandeered the plane.
After the plane was directed to Spain, the hijacker said he
wanted to be taken to Germany.
After several hours of negotiations, authorities convinced
the person to surrender early today.
Police said the man had a pistol, but a Moroccan security
source in Rabat said the gun was likely a \toy".
There were no reported injuries.
O?cials in Spain say the Boeing 737 left Casablanca, Mo-
rocco, Wednesday night with 83 passengers and a nine- per-
son crew headed for Tunis, Tunisia.
Spanish authorities directed the plane to an isolated section
of El Prat Airport and o?cials began negotiations.
Figure 2: A summary produced using the Majority Or-
dering algorithm, graded as Poor.
3.1.2 Evaluation
We asked three human judges to evaluate the order of
information in 20 summaries produced using the MO algo-
rithm into three categories| Poor, Fair and Good. We de-
ne a Poor summary, in an operational way, as a text whose
readability would be signicantly improved by reordering its
sentences. A Fair summary is a text which makes sense but
reordering of some sentences can yield a better readability.
Finally, a summary which cannot be further improved by
any sentence reordering is considered a Good summary.
The judges were asked to grade the summaries taking only
into account the order in which the information is presented.
To help them focus on this aspect of the texts, we resolved
dangling references beforehand. Figure 8 shows the grades
assigned to the summaries using majority to combine the
judges grades. In our experiments, judges had strong agree-
ment; they never gave three dierent grades to a summary.
TheMO algorithm produces a small number of Good sum-
maries, but most of the summaries were graded as Fair. For
instance, the summary graded Good shown in Figure 1 or-
ders the information in a natural way; the text starts with
a sentence summary of the event, then the outcome of the
trial is given, a reminder of the facts that caused the trial
and a possible explanation of the facts. Looking at the Good
summaries produced by MO, we found that it performs well
when the input articles follow the same order when present-
ing the information. In other words, the algorithm produces
a good ordering if the input articles orderings have high
agreement.
On the other hand, when analyzing Poor summaries, as in
Figure 2, we observe that the input texts have very dierent
orderings. By trying to maximize the agreement of the input
texts orderings, MO produces a new ordering that doesn't
occur in any input text. The ordering is, therefore, not guar-
anteed anymore to be acceptable. An example of a new pro-
duced ordering is given in Figure 2. The summary would be
more readable if several sentences were moved around (the
last sentence would be better placed before the fourth sen-
tence because they both talk about the Spanish authorities
handling the hijacking).
This algorithm can be used to order sentences accurately
if we are certain that the input texts follow similar orga-
nizations. This assumption may hold in limited domains.
However, in our case, the input texts we are processing do
not have such regularities. MO's performance critically de-
pends on the quality of the input texts, therefore, we should
design an ordering strategy which better ts our input data.
From here on, we will focus only on the Chronological Or-
dering algorithm and ways to improve it.
3.2 Chronological Ordering
3.2.1 The Algorithm
Multidocument summarization of news typically deals with
articles published on dierent dates, and articles themselves
cover events occurring over a wide range in time. Using
chronological order in the summary to describe the main
events helps the user understand what has happened. It
seems like a natural and appropriate strategy. As mentioned
earlier, in our framework, we are ordering themes; in this
strategy, we therefore need to assign a date to themes. To
identify the date an event occured requires a detailed in-
terpretation of temporal references in articles. While there
have been recent developments in disambiguating temporal
expressions and event ordering [12], correlating events with
the date on which they occurred is a hard task. In our case,
we approximate the theme time by its rst publication date;
that is, the rst time the theme has been reported in our
set of input articles. It is an acceptable approximation for
news events; the rst publication date of an event usually
corresponds to its occurrence in real life. For instance, in a
terrorist attack story, the theme conveying the attack itself
will have a date previous to the date of the theme describing
a trial following the attack.
Articles released by news agencies are marked with a pub-
lication date, consisting of a date and a time with three elds
(hour, minutes and seconds). Articles from the same news
agency are, then, guaranteed to have dierent publication
dates. This also holds for articles coming from dierent
news agencies. We never encountered two articles with the
same publication date during the development of MultiGen.
Thus, the publication date serves as a unique identier over
articles. As a result, when two themes have the same pub-
lication date, it means that they both are reported for the
rst time in the same article.
Our Chronological Ordering (CO) algorithm takes as in-
put a set of themes and orders them chronologically when-
ever possible. Each theme is assigned a date corresponding
to its rst publication. This establishes a partial order over
the themes. When two themes have the same date (that is,
they are reported for the rst time in the same article) we
sort them according to their order of presentation in this ar-
ticle. We have now a complete order over the input themes.
To implement this algorithm in MultiGen, we select for
each theme the sentence that has the earliest publication
date. We call it the time stamp sentence and assign its
publication date as the time stamp of the theme. Figures 3
and 4 show examples of produced summaries using CO.
One of four people accused along with former Pakistani
Prime Minister Nawaz Sharif has agreed to testify against
him in a case involving possible hijacking and kidnapping
charges, a prosecutor said Wednesday.
Raja Quereshi, the attorney general, said that the former
Civil Aviation Authority chairman has already given a state-
ment to police.
Sharif's lawyer dismissed the news when speaking to re-
porters after Sharif made an appearance before a judicial
magistrate to hear witnesses give statements against him.
Sharif has said he is innocent.
The allegations stem from an alleged attempt to divert
a plane bringing army chief General Pervez Musharraf to
Karachi from Sri Lanka on October 12.
Figure 3: A summary produced using the Chronological
Ordering algorithm graded as Good.
Thousands of people have attended a ceremony in Nairobi
commemorating the rst anniversary of the deadly bombings
attacks against U.S. Embassies in Kenya and Tanzania.
Saudi dissidentOsama bin Laden, accused of masterminding
the attacks, and nine others are still at large.
President Clinton said, "The intended victims of this vicious
crime stood for everything that is right about our country
and the world".
U.S. federal prosecutors have charged 17 people in the
bombings.
Albright said that the mourning continues.
Kenyans are observing a national day of mourning in honor
of the 215 people who died there.
Figure 4: A summary produced using the Chronological
Ordering algorithm graded as Poor.
3.2.2 Evaluation
Following the same methodology we used for the MO al-
gorithm evaluation, we asked three human judges to grade
20 summaries generated by the system using the CO algo-
rithm applied to the same collection of input texts. The
results are shown in Figure 8.
Our rst suspicion was that our approximation deviates
too much from the real chronological order of events, and,
therefore, lowers the quality of sentence ordering. To ver-
ify this hypothesis, we identied sentences that broke the
original chronological order and restored the ordering man-
ually. Interestingly, the displaced sentences were mainly
background information. The evaluation of the modied
summaries shows a slight but not visible improvement.
When comparing Good (Figure 3) and Poor (Figure 4)
summaries, we notice two phenomena: rst, many of the
badly placed sentences cannot be ordered based on their
temporal occurence. For instance, in Figure 4, the sentence
quoting Clinton is not one event in the sequence of events
being described, but rather a reaction to the main events.
This is also true for the sentence reporting Albright's reac-
tion. Assigning a date to a reaction, or more generally to
any sentence conveying background information, and plac-
ing it into the chronological stream of the main events does
not produce a logical ordering. The ordering of these themes
is therefore not covered by the CO algorithm.
The second phenomenon we observed is that Poor sum-
maries typically contain abrupt switches of topics and gen-
eral incoherences. For instance, in Figure 4, quotes from US
o?cials (third and fth sentences) are split and sentences
about the mourning (rst and sixth sentences) appear too
far apart in the summary. Grouping them together would
increase the readability of the summary. At this point, we
need to nd additional constraints to improve the ordering.
4. IMPROVING THE ORDERING:
EXPERIMENTS AND ANALYSIS
In the previous section, we showed that using naive or-
dering algorithms does not produce satisfactory orderings.
In this section, we investigate through experiments with hu-
mans, how to identify patterns of orderings that can improve
the algorithm.
Sentences in a text can be ordered in a number of ways,
and the text as a whole will still convey the same meaning.
But undoubtedly, some orders are denitely unacceptable
because they break conventions of information presentation.
One way to identify these conventions is to nd common-
alities between dierent acceptable orderings of the same
information. Extracting regularities in several acceptable
orderings can help us specify the main ordering constraints
for a given input type. Since a collection of multiple sum-
maries over the same set of articles doesn't exist, we created
our own collection of multiple orderings produced by dif-
ferent humans. Using this collection, we studied common
behaviors and mapped them to strategies for ordering.
Our collection of multiple orderings is available at
http://www.cs.columbia.edu/~noemie/ordering/. It was
built in the following way. We collected ten sets of articles.
Each set consisted of two to three news articles reporting the
same event. For each set, we manually selected the inter-
section sentences, simulating MultiGen
1
. On average, each
set contained 8.8 intersection sentences. The sentences were
cleaned of explicit references (for instance, occurrences of
\the President" were resolved to \President Clinton") and
connectives, so that participants wouldn't use them as clues
for ordering. Ten subjects participated in the experiment
and they each built one ordering per set of intersection sen-
tences. Each subject was asked to order the intersection
1
We performed a manual simulation to ensure that ideal
data was provided to the subjects of the experiments
sentences of a set so that they form a readable text. Over-
all, we obtained 100 orderings, ten alternative orderings per
set. Figure 5 shows the ten alternative orderings collected
for one set.
We rst observe that a surprising majority of orderings
are dierent. Out of the ten sets, only two sets had some
identical orderings (in one set, one pair of orderings were
identical while in the other set, two pairs of orderings were
identical). In other words, there are many acceptable order-
ings given one set of sentences. This conrms the intuition
that we do not need to look for a single ideal global ordering
but rather construct an acceptable one.
We also notice that, within the multiple orderings of a
set, some sentences always appear together. They do not
appear in the same order from one ordering to another, but
they share an adjacency relation. From now on, we refer to
them as blocks. For each set, we identify blocks by cluster-
ing sentences. We use as a distance metric between two sen-
tences the average number of sentences that separate them
over all orderings. In Figure 5, for instance, the distance
between the sentences D and G is 2. The blocks identied
by clustering are: sentences B, D, G and I; sentences A and
J; sentences C and F; and sentences E and H.
Participant 1 D B G I H F C J A E
Participant 2 D G B I C F A J E H
Participant 3 D B I G F J A E H C
Participant 4 D C F G I B J A H E
Participant 5 D G B I H F J A C E
Participant 6 D G I B F C E H J A
Participant 7 D B G I F C H E J A
Participant 8 D B C F G I E H A J
Participant 9 D G I B E H F A J C
Participant 10 D B G I C F A J E H
Figure 5: Multiple orderings for one set in our collec-
tion.
We observed that all the blocks in the experiment cor-
respond to clusters of topically related sentences. These
blocks form units of text dealing with the same subject, and
exhibit cohesive properties. For ordering, we can use this to
opportunistically group sentences together that all refer to
the same topic.
Collecting a set of multiple orderings is an expensive task;
it is di?cult and time consuming for a human to order sen-
tences from scratch. Furthermore, to discover signicant
commonalities across orderings, many multiple orderings of
the same set are necessary. We plan to extend our collection
and we are condent that it will provide more insights on
ordering. Still, the existing collection enables us to identify
cohesion as an important factor for ordering. We describe
next how we integrate the cohesion constraint in the CO
algorithm.
5. THE AUGMENTED ALGORITHM
In the output of the CO algorithm, disuencies arise when
topics are distributed over the whole text, violating cohesion
properties [13]. A typical scenario is illustrated in Figure 6.
The inputs are texts T
1
, T
2
, T
3
(in order of publication).
A
1
, A
2
and A
3
belong to the same theme whose intersection
sentence is A and similarly for B and C. The themes A and
B are topically related, but C is not related. Summary S
1
,
based only on chronological clues, contains two topical shifts;
from A to C and back from C to B. A better summary would
be S
2
which keeps A and B together.
AA C A
B
1 2 3
3
C1
...
B2
2A
C3 B
C
...
...
T T T S1 2 3 1
A
C
B
S 2
Figure 6: Input texts T
1
T
2
T
3
are summarized by the
Chronological Ordering (S
1
) or by the Augmented algo-
rithm (S
2
).
5.1 The Algorithm
Our goal is to remove disuencies from the summary by
grouping together topically related themes. This can be
achieved by integrating cohesion as an additional constraint
to the CO algorithm. The main technical di?culty in in-
corporating cohesion in our ordering algorithm is to iden-
tify and to group topically related themes across multiple
documents. In other words, given two themes, we need to
determine if they belong to the same cohesion block. For a
single document, segmentation [8] could be used to identify
blocks, but we cannot use such a technique to identify co-
hesion between sentences across multiple documents. The
main reason is that segmentation algorithms exploit the lin-
ear structure of an input text; in our case, we want to group
together sentences belonging to dierent texts.
Our solution consists of the following steps. In a prepro-
cessing stage, we segment each input text, so that given two
sentences within the same text, we can determine if they
are topically related. Assume the themes A and B, where
A contains sentences (A
1
: : :A
n
), and B contains sentences
(B
1
: : :B
m
). Recall that a theme is a set of sentences con-
veying similar information drawn from dierent input texts.
We denote #AB to be the number of pairs of sentences
(A
i
;B
j
) which appear in the same text, and #AB
+
to be
the number of sentence pairs which appear in the same text
and are in the same segment.
In a rst stage, for each pair of themes A and B, we com-
pute the ratio #AB
+
=#AB to measure the relatedness of
two themes. This measure takes into account both positive
and negative evidence. If most of the sentences in A and
B that appear together in the same texts are also in the
same segments, it means that A and B are highly topically
related. In this case, the ratio is close to 1. On the other
hand, if among the texts containing sentences from A and
B, only a few pairs are in the same segments, then A and B
are not topically related. Accordingly the ratio is close to 0.
A and B are considered related if this ratio is higher than
a predetermined threshold. In our experiments, we set it to
0.6.
This strategy denes pairwise relations between themes.
A transitive closure of this relation builds groups of related
themes and as a result ensures that themes that do not ap-
pear together in any article but are both related to a third
theme will still be linked. This creates an even higher degree
of relatedness among themes. Because we use a threshold
to establish pairwise relations, the transitive closure does
not produce elongated chains that could link together unre-
lated themes. We are now able to identify topically related
themes. At the end of the rst stage, they are grouped into
blocks.
In a second stage, we assign a time stamp to each block of
related themes, as the earliest time stamp of the themes it
contains. We adapt the CO algorithm described in 3.2.1 to
work at the level of the blocks. The blocks and the themes
correspond to, respectively, themes and sentences in the CO
algorithm. By analogy, we can easily show that the adapted
algorithm produces a complete order of the blocks. This
yields a macro-ordering of the summary. We still need to
order the themes inside each block.
In the last stage of the augmented algorithm, for each
block, we order the themes it contains by applying the CO
algorithm to them. Figure 7 shows an example of a summary
produced by the augmented algorithm.
This algorithm ensures that cohesively related themes will
not be spread over the text, and decreases the number of
abrupt switches of topics. Figure 7 shows how the Aug-
mented algorithm improves the sentence order compared
with the order in the summary produced by the CO al-
gorithm in Figure 4; sentences quoting US o?cials are now
grouped together and so are descriptions of the mourning.
Thousands of people have attended a ceremony in Nairobi
commemorating the rst anniversary of the deadly bomb-
ings attacks against U.S. Embassies in Kenya and Tanzania.
Kenyans are observing a national day of mourning in honor
of the 215 people who died there.
Saudi dissidentOsama bin Laden, accused of masterminding
the attacks, and nine others are still at large. U.S. federal
prosecutors have charged 17 people in the bombings.
President Clinton said, "The intended victims of this vicious
crime stood for everything that is right about our country
and the world". Albright said that the mourning continues.
Figure 7: A Summary produced using the Aug-
mented algorithm. Related sentences are grouped
into paragraphs.
5.2 Evaluation
Following the same methodology used to evaluate the MO
and the CO algorithms, we asked the judges to grade 20
summaries produced by the Augmented algorithm. Results
are shown in Figure 8.
The manual eort needed to compare and judge system
output is extensive; consider that each human judge had to
read three summaries for each input set as well as skim the
input texts to verify that no misleading order was introduced
in the summaries. Consequently, the evaluation that we
performed to date is limited. Still, this evaluation shows a
signicant improvement in the quality of the orderings from
the CO algorithm to the augmented algorithm. To assess the
signicance of the improvement, we used the Fisher exact
test, conating Poor and Fair summaries into one category.
This test is adapted to our case because of the reduced size
of our test set. We obtained a p value of 0.014 [20].
6. RELATED WORK
Finding an acceptable ordering has not been studied be-
fore in summarization. In single document summarization,
Poor Fair Good
Majority Ordering 2 12 6
Chronological Ordering 7 7 6
Augmented Ordering 2 7 11
Figure 8: Evaluation of the the Majority Ordering, the
Chronological Ordering and the Augmented Ordering.
summary sentences are typically arranged in the same order
that they were found in the full document (although [10]
reports that human summarizers do sometimes change the
original order). In multidocument summarization, the sum-
mary consists of fragments of text or sentences that were
selected from dierent texts. Thus, there is no complete
ordering of summary sentences that can be found in the
original documents.
The ordering task has been extensively investigated in the
generation community [14, 17, 9, 2, 16]. One approach is
top-down, using schemas [14] or plans [5] to determine the
organizational structure of the text. This appproach postu-
lates a rhetorical structure which can be used to select in-
formation from an underlying knowledge base. Because the
domain is limited, an encoding can be developed of the kinds
of propositional content that match rhetorical elements of
the schema or plan, thereby allowing content to be selected
and ordered. Rhetorical Structure Theory (RST) allows for
more exibility in ordering content. The relations occur be-
tween pairs of propositions. Constraints based on intention
(e.g., [17]), plan-like conventions [9], or stylistic constraints
[2] are used as preconditions on the plan operators contain-
ing RST relations to determine when a relation is used and
how it is ordered with respect to other relations.
MultiGen generates summaries of news on any topic. In
an unconstrained domain like this, it would be impossible
to enumerate the semantics for all possible types of sen-
tences which could match the elements of a schema, a plan
or rhetorical relations. Furthermore, it would be di?cult to
specify a generic rhetorical plan for a summary of news. In-
stead, content determination in MultiGen is opportunistic,
depending on the kinds of similarities that happen to exist
between a set of news documents. Similarly, we describe
here an ordering scheme that is opportunistic and bottom-
up, depending on the coherence and temporal connections
that happen to exist between selected text. Our approach
is similar to the use of basic blocks [16] where a bottom-up
technique is used to group together stretches of text in a
long, generated document by nding propositions that are
related by a common focus. Since this approach was devel-
oped for a generation system, it nds related propositions by
comparisons of proposition arguments at the semantic level.
In our case, we are dealing with a surface representation, so
we nd alternative methods for grouping text fragments.
7. CONCLUSION AND FUTURE WORK
In this paper we investigated information ordering con-
straints in multidocument summarization. We analyzed two
naive ordering algorithms, the Majority Ordering (MO) and
the Chronological Ordering (CO). We show that the MO al-
gorithm performs well only when all input texts follow sim-
ilar presentation of the information. The CO algorithm can
provide an acceptable solution for many cases, but is not
su?cient when summaries contain information that is not
event based. We report on the experiments we conducted
to identify other constraints contributing to ordering. We
show that cohesion is an important factor, and describe an
operational way to incorporate it in the CO algorithm. This
results in a denite improvement of the overall quality of au-
tomatically generated summaries.
In future work, we rst plan to extend our collection of
multiple orderings, so that we can extract more regulari-
ties and understand better how human order information to
produce a readable and uent text. Even though we did
not encounter any misleading inferences introduced by re-
ordering MultiGen output, we plan to do an extended study
of the side eects caused by reorderings. We also plan to
investigate whether the MO algorithm can be improved by
applying it on cohesive blocks of themes, rather than themes.
8. ACKNOWLEDGMENT
This work was partially supported by DARPA grant N66001-
00-1-8919, a Louis Morin scholarship and a Viros scholar-
ship. We thank Eli Barzilay for providing help with the
experiments interface, Michael Elhadad for the useful dis-
cussions and comments, and all the voluntary participants
in the experiments.
9. REFERENCES
[1] R. Barzilay, K. McKeown, and M. Elhadad.
Information fusion in the context of multi-document
summarization. In Proc. of the 37th Annual Meeting
of the Assoc. of Computational Linguistics, 1999.
[2] N. Bouayad-Agha, R. Power, and D. Scott. Can text
structure be incompatible with rhetorical structure?
In Proceedings of the First International Conference
on Natural Language Generation (INLG'2000),
Mitzpe Ramon, Israel, 2000.
[3] J. Carbonell and J. Goldstein. The use of mmr,
diversity-based reranking for reordering documents
and producing summaries. In Proceedings of the 21st
Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
1998.
[4] T. Cormen, C. Leiserson, and R. Rivest. Introduction
to Algorithms. The MIT Press, 1990.
[5] R. Dale. Generating Referring Expressions:
Constructing Descriptions in a Domain of Objects and
Processes. MIT Press, Cambridge, MA, 1992.
[6] N. Elhadad and K. McKeown. Generating patient
specic summaries of medical articles. Submitted,
2001.
[7] V. Hatzivassiloglou, J. Klavans, and E. Eskin.
Detecting text similarity over short passages:
Exploring linguistic feature combinations via machine
learning. In Proceedings of the Joint SIGDAT
Conference on Empirical Methods in Natural
Language Processing and Very Large Corpora, 1999.
[8] M. Hearst. Multi-paragraph segmentation of
expository text. In Proceedings of the 32th Annual
Meeting of the Association for Computational
Linguistics, 1994.
[9] E. Hovy. Automated discourse generation using
discourse structure relations. Articial Intelligence, 63,
1993. Special Issue on NLP.
[10] H. Jing. Summary generation through intelligent
cutting and pasting of the input document. Technical
report, Columbia University, 1998.
[11] I. Mani and E. Bloedorn. Multi-document
summarization by graph search and matching. In
Proceedings of the Fifteenth National Conference on
Articial Intelligence, 1997.
[12] I. Mani and G. Wilson. Robust temporal processing of
news. In Proceedings of the 38th Annual Meeting of
the Association for Computational Linguistics, 2000.
[13] K. McCoy and J. Cheng. Focus of attention:
Constraining what can be said next. In C. Paris,
W. Swartout, and W. Mann, editors, Natural
Language Generation in Articial Intelligence and
Computational Linguistics. Kluwer Academic
Publishers, 1991.
[14] K. McKeown. Text Generation: Using Discourse
Strategies and Focus Constraints to Generate Natural
Language Text. Cambridge University Press, England,
1985.
[15] K. McKeown, J. Klavans, V. Hatzivassiloglou,
R. Barzilay, and E. Eskin. Towards multidocument
summarization by reformulatin: Progress and
prospects. In Proceedings of the Seventeenth National
Conference on Articial Intelligence, 1999.
[16] D. Mooney, S. Carberry, and K. McCoy. The
generation of high-level structure for extended
explanations. In Proceedings of the International
Conference on Computational Linguistics
(COLING{90), pages 276{281, Helsinki, 1990.
[17] J. Moore and C. Paris. Planning text for advisory
dialogues: Capturing intentional and rhetorical
information. Journal of Computational Linguistics,
19(4), 1993.
[18] D. Radev, H. Jing, and M. Budzikowska.
Centroid-based summarization of multiple documents:
sentence extraction, utility-based evaluation, and user
studies. In Proceedings of the ANLP/NAACL 2000
Workshop on Automatic Summarization, 2000.
[19] D. Radev and K. McKeown. Generating natural
language summaries from multiple on-line sources.
Computational Linguistics, 24(3):469{500, September
1998.
[20] S. Siegal and N. J. Castellan. Non-Parametric
statistics for the behavioural sciences. McGraw Hill,
1988.
Sentence Alignment for Monolingual Comparable Corpora
Regina Barzilay
Department of Computer Science
Cornell University
Ithaca, NY 14853
regina@cs.cornell.edu
Noemie Elhadad
Department of Computer Science
Columbia University
New York, NY 10027
noemie@cs.columbia.edu
Abstract
We address the problem of sentence align-
ment for monolingual corpora, a phe-
nomenon distinct from alignment in par-
allel corpora. Aligning large compara-
ble corpora automatically would provide
a valuable resource for learning of text-to-
text rewriting rules. We incorporate con-
text into the search for an optimal align-
ment in two complementary ways: learn-
ing rules for matching paragraphs using
topic structure and further refining the
matching through local alignment to find
good sentence pairs. Evaluation shows
that our alignment method outperforms
state-of-the-art systems developed for the
same task.
1 Introduction
Text-to-text generation is an emerging area of re-
search in NLP (Chandrasekar and Bangalore, 1997;
Caroll et al, 1999; Knight and Marcu, 2000; Jing
and McKeown, 2000). Unlike in traditional concept-
to-text generation, text-to-text generation applica-
tions take a text as input and transform it into a new
text satisfying specific constraints, such as length in
summarization or style in text simplification. One
exciting new research direction is the automatic in-
duction of such transformation rules. This is a par-
ticularly promising direction given that there are nat-
urally occurring examples of comparable texts that
convey the same information yet are written in dif-
ferent styles. Presented with two such texts, one
can pair sentences that convey the same information,
thereby building a training set of rewriting examples
for the domain to which the texts belong. We believe
that automating this process will provide researchers
in text-to-text generation with valuable training and
testing resources, just as techniques to align multi-
lingual parallel corpora boosted research in Machine
Translation (MT).
In this paper, we address the task of aligning sen-
tences in text pairs. We focus on monolingual com-
parable corpora, that is, texts in the same language
(e.g., English) that overlap in the information they
convey. Stories about the same events from different
press agencies and texts presenting the same infor-
mation to experts and lay people are two examples.
In MT, the task of sentence alignment was exten-
sively studied for parallel corpora.1 A typical sen-
tence alignment algorithm can be roughly described
as a two-step process: (1) for each sentence pair
compute a local similarity value, independently of
the other sentences; (2) find an overall sequence of
mapped sentences, using both the local similarity
values and additional features.
In the case of monolingual corpora, step (2) might
seem unnecessary. Since the texts share the same
language, it would be enough to choose for local
similarity a function based on lexical cues only and
select sentence pairs with high lexical similarity.
Even a simple lexical function (e.g., one that counts
word overlap) could produce an accurate alignment.
1Sentence alignment for comparable multilingual corpora
was not addressed in previous research. Comparable cor-
pora have primarily been used to build bilingual lexical re-
sources (Fung and Yee, 1998).
After all, two sentences which share most of their
words are likely to paraphrase each other. The prob-
lem is that there are many sentences that convey
the same information but have little surface resem-
blance. As a result, simple word counts cannot dis-
tinguish the matching pair (A) in Figure 1 from the
unrelated pair (B). An accurate local similarity mea-
sure would have to account for many complex para-
phrasing phenomena. A simple, weak lexical simi-
larity function alone is not sufficient.
(A)
? Petersburg served as the capital of Russia for 200
years.
? For two centuries Petersburg was the capital of the
Russian Empire.
(B)
? The city is also the country?s leading port and center
of commerce.
? And yet, as with so much of the city, the port facili-
ties are old and inefficient.
Figure 1: Sentence pairs from our corpus sharing
two content words. (A) is a matching pair, (B) is not.
In MT, a weak similarity function is compensated
for by searching for a globally optimal alignment,
using dynamic programming or taking advantage
of the geometric/positional or contextual properties
of the text pair (Gale and Church, 1991; Shemtov,
1993; Melamed, 1999). But these techniques oper-
ate on the assumptions that there are limited inser-
tions and deletions between the texts and that the
order of the information is roughly preserved from
one text to another.
Texts from comparable corpora, as opposed to
parallel corpora, contain a great deal of ?noise.? In
Figure 2 which plots the manually identified align-
ment for a text pair in our corpus, only a small frac-
tion of the sentences got aligned (35 out of 31 ?
270 sentence pairs), which illustrates that there is no
complete information overlap. Consider two texts
written by different press agencies: while both re-
port on the same events, one may contain additional
interviews and the other, background information.
Another distinguishing characteristic of comparable
corpora is that the order in which the information
is presented can differ greatly from one text to an-
other. Analysis of comparable texts in different do-
mains (Paris, 1993; Barzilay et al, 2002) showed
that there is wide variability in the order in which
the same information can be presented. This is also
illustrated in Figure 2.
0
50
100
150
200
250
0 5 10 15 20 25 30
Se
nt
en
ce
s 
in
 T
ex
t 2
Sentences in Text 1
Manual Alignment
Figure 2: Manual alignment for a text pair in our
corpus. A point in (x,y) indicates that the sentences
x and y match.
We investigate a novel approach informed by text
structure for sentence alignment. Our method em-
phasizes the search for an overall alignment, while
relying on a simple local similarity function. We
incorporate context into the search process in two
complementary ways: (1) we map large text frag-
ments using hypotheses learned in a supervised fash-
ion and (2) we further refine the match through lo-
cal alignment within mapping fragments to find sen-
tence pairs. When the documents in the collection
belong to the same domain and genre, the fragment
mapping takes advantage of the topical structure of
the texts. This structure is derived in an unsuper-
vised fashion by analyzing commonalities among
texts on each side of the comparable corpora sep-
arately. Our experiments show that our overall ap-
proach identifies even pairs with low lexical sim-
ilarity. We also found that a fully unsupervised
method using a minimalist representation of contex-
tual information, viz., paragraph-level lexical simi-
larity, outperforms existing methods based on com-
plex local similarity functions.
In the next section, we provide an overview of
existing work on monolingual sentence alignment.
Section 3 describes our algorithm. In sections 4
and 5, we report on data collection and evaluation.
2 Related Work
Most of the work in monolingual corpus alignment
is in the context of summarization. In single docu-
ment summarization, alignment between full docu-
ments and summaries written by humans is used to
learn rules for text compression. Marcu (1999) com-
putes sentence similarity using a cosine-based met-
ric. Jing (2002) identifies phrases that were cut and
pasted together using a Hidden Markov Model with
features incorporating word identity and positioning
within sentences, thereby providing an alignment of
the document and its summary. However, both of
these methods construct an alignment by looking at
sentences one at a time, independently of the de-
cisions made about other sentences. Because sum-
maries often reuse original document text to a large
extent, these methods achieve good results.
In the context of multidocument summarization,
SimFinder (Hatzivassiloglou et al, 1999) identifies
sentences that convey similar information across in-
put documents to select the summary content. Even
though the input documents are about the same sub-
ject, they exhibit a great deal of lexical variability.
To address this issue, SimFinder employs a com-
plex similarity function, combining features that ex-
tend beyond a simple word count and include noun
phrase, proper noun, and WordNet sense overlap.
Since many documents are processed in parallel,
clustering is used to combine pairwise alignments.
In contrast to our approach, SimFinder does not take
the context around sentences into account.
3 Algorithm
Given a comparable corpus consisting of two collec-
tions and a training set of manually aligned text pairs
from the corpus, the algorithm follows four main
steps. Steps 1 and 2 take place at training time. Steps
3 and 4 are carried out when a new text pair (Text1,
Text2) is to be aligned.
1. Topical structure induction: by analyzing
multiple instances of paragraphs within the
texts of each collection, the topics characteris-
tic of the collections are identified through clus-
tering. Each paragraph in the training set gets
assigned the topic it verbalizes (Section 3.1.1.)
2. Learning of structural mapping rules: us-
ing the training set, rules for mapping para-
graphs are learned in a supervised fashion (Sec-
tion 3.1.2).
3. Macro alignment: given a new unseen pair
(Text1, Text2), each paragraph is automati-
cally assigned its topic. Paragraphs are mapped
following the learned rules (Section 3.2).
4. Micro alignment: for each mapped paragraph
pair, a local alignment is computed at the sen-
tence level. The final alignment for the text pair
is the union of all the aligned sentence pairs
(Section 3.3).
3.1 Off-Line Processing
Given two sentences with moderate lexical similar-
ity, we may not have enough evidence to decide ac-
curately whether they should be aligned. Looking at
the broader context they appear in can provide addi-
tional insight: if the types of information expressed
in the contexts are similar, then the specific infor-
mation expressed in the sentences is more likely to
be the same. On the other hand, if the types of in-
formation in the two contexts are unrelated, chances
are that the sentences should not be aligned. In our
implementation, context is represented by the para-
graphs to which the sentences belong.2 Our goal in
this phase is to learn rules for determining whether
two paragraphs are likely to contain sentences that
should be aligned, or whether, on the contrary, two
paragraphs are unrelated and, therefore, should not
be considered for further processing.
A potentially fruitful way to do so is to take ad-
vantage of the topical structure of texts. In a given
domain and genre, while the texts relate different
subjects, they all use a limited set of topics to con-
vey information; these topics are also known as the
Domain Communication Knowledge (Kittredge et
al., 1991). For instance, most texts describing dis-
eases will have topics such as ?symptoms? or ?treat-
ment.?3 If the task is to align a disease description
written for physicians and a text describing the same
disease for lay people, it is most likely that sentences
within the topic ?symptoms? in the expert version
will map to sentences describing the symptoms in
the lay version rather than those describing treat-
ment options. If we can automatically identify the
topic each paragraph conveys, we can decide more
accurately whether two paragraphs are related and
should be mapped for further processing.
2Texts without adequate paragraph marking could be seg-
mented using tools such as TextTiling (Hearst, 1994).
3We use the term topic differently than it is commonly used
in the topic detection task? there, a ?topic? would designate
which disease is described.
In the field of text generation, methods for
representing the semantic structure of texts have
been investigated through text schemata (McKeown,
1985) or rhetorical structures (Mann and Thompson,
1987). In our framework, we want to identify the
different topics of the text, but we are not concerned
with the relations holding between them or the order
in which they typically appear. We propose to iden-
tify the topics typical to each collection in the com-
parable corpus by using clustering, such that each
cluster represents a topic in the collection.
The process of learning paragraph mapping rules
is accomplished in two stages: first, we identify the
topics of each collection, Corpus1 and Corpus2,
and label each paragraph with its specific topic. Sec-
ond, using a training set of manually aligned text
pairs, we learn rules for mapping paragraphs from
Corpus1 to Corpus2. Two paragraphs are consid-
ered mapped if they are likely to contain sentences
that should be aligned.
3.1.1 Vertical Paragraph Clustering
We perform a clustering at the paragraph level for
each collection. We call this stage Vertical Clus-
tering because all the paragraphs of all the doc-
uments in Corpus1 get clustered, independently
of Corpus2; the same goes for the paragraphs in
Corpus2. At this stage, we are only interested in
identifying the topics of the texts in each collection,
each cluster representing a topic.
We apply a hierarchical complete link clustering.
Similarity is a simple cosine measure based on the
word overlap of the paragraphs, ignoring function
words. Since we want to group together paragraphs
that convey the same type of information across the
documents in the same collection, we replace all the
text-specific attributes, such as proper names, dates
and numbers, by generic tags.4 This way, we ensure
that two paragraphs are clustered not because they
relate the same specific information, but rather, be-
cause they convey the same type of information (an
example of two automatically clustered paragraphs
is shown in Figure 3). The number of clusters for
each collection is a parameter tuned on our training
set (see Section 4).
4We crudely consider any words with a capital letter a proper
name, except for each sentence?s first word.
Lisbon has a mild and equable climate, with a mean annual
temperature of 63 degree F (17 degree C). The proximity of
the Atlantic and the frequency of sea fogs keep the atmosphere
humid, and summers can be somewhat oppressive, although
the city has been esteemed as a winter health resort since the
18th century. Average annual rainfall is 26.6 inches (666 mil-
limetres).
Jakarta is a tropical, humid city, with annual temperatures
ranging between the extremes of 75 and 93 degree F (24 and
34 degree C) and a relative humidity between 75 and 85 per-
cent. The average mean temperatures are 79 degree F (26 de-
gree C) in January and 82 degree F (28 degree C) in October.
The annual rainfall is more than 67 inches (1,700 mm). Tem-
peratures are often modified by sea winds. Jakarta, like any
other large city, also has its share of air and noise pollution.
Figure 3: Two automatically clustered paragraphs in
the same collection (without date, number, and name
substitution).
Cluster1
Text Text
Par.2
Par.1
Par.3
Corpus1 Corpus2
Par.3
Par.1
Par.2
ClusterE
ClusterB
Text Textj1 j2
i1 i2
Figure 4: The training set for the paragraph mapping
step. An arrow between two paragraphs indicates
they contain at least one aligned sentence pair.
3.1.2 Horizontal Paragraph Mapping
Once the different topics, or clusters, are identi-
fied inside each collection, we can use this informa-
tion to learn rules for paragraph mapping (Horizon-
tal Mapping between texts from Corpus1 and texts
from Corpus2). Using a training set of text pairs,
manually aligned at the sentence level, we consider
two paragraphs to map each other if they contain at
least one aligned sentence pair (see Figure 4).
Our problem can be framed as a classification
task: given training instances of paragraph pairs (P ,
Q) from a text pair, classify them as mapping or not.
The features for the classification are the lexical sim-
ilarity of P and Q, the cluster number of P , and the
cluster number of Q. Here, similarity is again a sim-
ple cosine measure based on the word overlap of the
two paragraphs.5 These features are weak indicators
by themselves. Consequently, we use the publicly-
available classification tool BoosTexter (Singer and
Schapire, 1998) to combine them accurately.6
3.2 Macro Alignment: Find Candidate
Paragraph(s)
At this stage, the clustering and training are com-
pleted. Given a new unseen text pair (Text1,
Text2), the goal is to find a sentence alignment be-
tween them. Two sentences with very high lexical
similarity are likely to be aligned. We allow such
pairs in the alignment independently of their context.
This step allows us to catch the ?easy? paraphrases.
We focus next on how our algorithm identifies the
less obvious matching sentence pairs.
For each paragraph in each text, we identify the
cluster in its collection it is the closest to. Similarity
between the paragraph and each cluster is computed
the same way as in the Vertical Clustering step. We
then apply mapping classification to find the map-
ping paragraphs in the text pair (see Figure 5).
Text 2Text 1
Cluster G
Par. 13
Cluster E
Par. 7
Cluster B
Par. 2
Par. 2
Cluster 3
Figure 5: Macro Alignment: a paragraph in Text1
and its mapped candidate paragraphs in Text2.
3.3 Micro Alignment: Find Sentence Pair(s)
Once the paragraph pairs are identified in (Text1,
Text2), we want to find, for each paragraph pair,
the (possibly empty) subsets of sentence pairs which
constitute a good alignment. Context is used in the
following way: given two sentences with moder-
ate similarity, their proximity to sentence pairs with
high similarity can help us decide whether to align
them or not.
To combine the lexical similarity (again using co-
sine measure) and the proximity feature, we com-
5At this stage, we want to match on text-specific informa-
tion, unlike in the Vertical Clustering. We therefore use the
original text, without any substitution, to compute the similarity.
6Because BoosTexter cannot form conjunctive hypotheses,
we add a feature which encodes the combination of two cluster
numbers.
pute local alignments on each paragraph pair, us-
ing dynamic programming. The local alignment we
construct fits the characteristics of the data we are
considering. In particular, we adapt it to our frame-
work to allow many-to-many alignments and some
flips of order among aligned sentences. Given sen-
tences i and j, their local similarity sim(i, j) is:
sim(i, j) = cos(i, j) ? mismatch penalty
The weight s(i, j) of the optimal alignment between
the two sentences is computed as follows:
s(i, j) = max
?
?
?
?
?
?
?
?
?
?
s(i, j?1) ? skip penalty
s(i?1, j) ? skip penalty
s(i?1, j?1) + sim(i, j)
s(i?1, j?2) + sim(i, j) + sim(i, j?1)
s(i?2, j?1) + sim(i, j) + sim(i?1, j)
s(i?2, j?2) + sim(i, j?1) + sim(i?1, j)
The mismatch penalty penalizes sentence pairs with
very low similarity measure, while the skip penalty
prevents only sentence pairs with high similarity
from getting aligned.
4 Evaluation Setup
The Data. We compiled two collections from the
Encyclopedia Britannica and Britannica Elementary.
In contrast to the long (up to 15-page) detailed arti-
cles of the Encyclopedia Britannica, Britannica Ele-
mentary contains one- to two-page entries targeted
towards children. The elementary version gener-
ally contains a subset of the information presented in
the comprehensive version, but there are numerous
cases when the elementary entry contains additional
or more up-to-date pieces of information.7 The two
collections together exhibit many instances of com-
plex rewriting.
We collected 103 pairs of comprehensive/ele-
mentary city descriptions. We set aside a testing set
of 11 text pairs. The rest (92 pairs) was used for the
Vertical Clustering. Nine text pairs were used for
training (see Table 1 for statistics).
Human Annotation. Each text pair in the train-
ing and testing sets was annotated by two annota-
tors.8 In our guidelines to them, we defined two sen-
tences as aligned if they contain at least one clause
7Britannica Elementary is a new feature of the encyclopedia,
not all entries in the original Britannica have been fully updated.
8All the annotators were native speakers of English. The
authors did not take part in the annotation.
Sentences Paragraphs
Min Max Avg Min Max Avg
Comp. Train 87 313 180 19 59 37
Comp. Test 138 308 200 32 63 43
Elem. Train 34 64 47 8 12 10
Elem. Test 27 75 45 6 16 10
Table 1: Statistics for the training and testing sets for
the comprehensive and elementary versions.
Range Training Testing
0%?40% 149 (46.6%) 127 (45.2%)
40%?70% 103 (32.2%) 96 (34.2%)
70%?100% 68 (21.2%) 58 (20.6%)
Table 2: Distribution of manually aligned sentence
pairs among different similarity ranges.
that expresses the same information. We allowed
many-to-many alignments. On average, each anno-
tator spent 50 minutes per text pair. While the an-
notators agreed for most of the sentence pairs they
identified, there were some cases of disagreement.
Alignment is a tedious task, and sentence pairs can
easily be missed even by a careful human annotator.
For each text pair, a third annotator went through
contested sentence pairs, deciding on a case-by-case
basis whether to include it in the alignment. Over-
all, 320 sentence pairs were aligned in the training
set and 281 in the testing set. The other sentence
pairs which were not aligned served as negative ex-
amples, yielding a total of 4192 training instances
and 3884 testing instances.9
As a confirmation that there is no order preserva-
tion in comparable corpora, there were up to nine or-
der shifts in each of the annotated text pairs. Table 2
shows that a large fraction of manually aligned sen-
tence pairs have low lexical similarity. Similarity is
measured here by the number of words in common,
normalized by the number of types in the shorter
sentence.
Parameter Tuning. We tuned all the parameters
on our training set, obtaining the following values:
the skip penalty is 0.001, and the cosine threshold
for selecting pairs with high lexical similarity is 0.5.
BoosTexter was trained in 200 iterations. To find the
optimal number of clusters for each collection, Ver-
tical Clustering was performed with different num-
bers of clusters, ranging from 10 to 40; we selected
9Our corpus is available at http://www.cs.
columbia.edu/?noemie/alignment.
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0.1 0.2 0.3 0.4 0.5 0.6 0.7
Pr
ec
is
io
n
Recall
full
SimFinder
cosine
decomposition
Figure 6: Precision/Recall for SimFinder, Cosine,
Decomposition, and our full method.
the alternatives with the best performance on the
training set: 20 for both collections.
5 Results
We first present the comparison of our method with
other systems developed for the same task. Next we
focus on the impact of individual components on the
performance of our method.
5.1 Comparison with Other Systems
An obvious choice for a baseline in this task is the
following: any two sentences are considered aligned
if their cosine similarity exceeds a certain threshold.
We also compare our algorithm with two state-of-
the-art systems, SimFinder (Hatzivassiloglou et al,
1999) and Decomposition (Jing, 2002).10 Figure 6
shows Precision/Recall curves for the different sys-
tems. For our full system, we obtain different values
of Recall keeping constant the skip penalty and the
cosine threshold and varying the value of the mis-
match penalty from 0 to 0.45.11 This setup results
in recall values in a 25%?65% range. The curve for
SimFinder was obtained by running SimFinder with
different similarity thresholds, ranging from 0.1 to
0.95. In the case of Decomposition, there are sev-
eral hard-coded parameters which are not trainable.
As a result, we were able to obtain results for De-
composition only at a 55.8% Recall level. Table 3
reports Precision values at this level of Recall.
10Jing (2002) reports that Decomposition outperforms the al-
gorithm of Marcu (1999); we, therefore, did not compare our
method against his system.
11Varying the mismatch penalty is a natural choice: varying
the skip penalty produces a narrow range of Recall values, while
the cosine threshold controls only a small portion of the sen-
tence pairs that can be identified (the ones with high similarity).
System Precision
SimFinder 24%
Cosine 57.9%
Decomposition 64.3%
Without Topic Mapping 71.6%
Without Local Alignment 73.3%
Full Method 76.9%
Full Method (with ideal clusters) 80.9%
Table 3: Precision at 55.8% Recall.
(*) Gradually the German culture and language became more
widespread in the city.
Capping Prague?s rebirth, it was designated a European
City of Culture in 2000.
Prague is a centuries-old city with a wealth of historic
landmarks.
The physical attractions and landmarks of Prague are
many.
Figure 7: Aligned pairs, (*) denotes an incorrect
alignment.
Our full method outperforms both the base-
line (?Cosine?) and the more complex systems
(?SimFinder? and ?Decomposition?). Interestingly,
methods that use simple local similarity functions
significantly outperform SimFinder (SimFinder was
trained on newswire texts; we did not have access
to SimFinder?s training component for retraining on
our corpus). This confirms our hypothesis that while
it is an appealing idea, putting all one?s eggs in the
basket of a sophisticated local similarity measure to
achieve good performance may be too hard a task.
The simple cosine baseline is competitive with the
Hidden Markov Model of Decomposition (Decom-
position was specifically developed to identify sen-
tence pairs with cut-and-paste transformations, not
all possible paraphrase pairs). This suggests that
when looking for an alignment, Cosine is a good,
yet simple, starting local similarity measure. Adding
on top of it an explicit search mechanism relying
on the context surrounding the sentences, as in our
method, results in a performance improvement of
19% at 55.8% Recall. Figure 7 shows examples of
pairs identified by our method.
5.2 Analysis
Impact of Horizontal Paragraph Mapping. We
hypothesize that exploiting the regularity in map-
ping between semantic units, such as topics, im-
proves the alignment. We compared the perfor-
mance of our full method with a variation that does
not take any topical information into account. For
the paragraph mapping, we replaced the learned
rules by a single rule based on lexical similarity:
two paragraphs are mapped if their cosine measure
is above a pre-specified threshold.12 This new map-
ping is a good point of comparison because it does
not rely on any knowledge inferred from the other
texts in the corpus. The results confirm our hypoth-
esis: learning paragraph mapping based on topical
structures improves the performance (see ?Without
Topic Mapping? vs. ?Full Method?, Table 3).
This experiment also shows that representing con-
text, even simply using only the paragraphs and their
lexical similarity, achieves higher performance than
methods based on more complex local similarity
functions. It is an important finding, because this
simplified method can be used when topic struc-
ture cannot be derived (e.g., in heterogeneous col-
lections) or when no training data is available, since
it is unsupervised.
Impact of Cluster Quality. Our method uses
clustering to identify the different topics of each
collection. It is important to know how sensitive
our overall algorithm is to the quality of the iden-
tified clusters. Fortunately, in our corpus, some of
the texts contain section headings (e.g., ?Climate?
or ?Demography?). Even though our method ig-
nores this piece of information, we used it to derive
manually ?ideal? clusters.13 We obtained eight clus-
ters for the elementary version and 11 for the com-
prehensive one. When feeding these ideal clusters
instead of the automatically identified ones to the
learning module for paragraph mapping, we achieve
4% improvement in Precision (at 55.8% Recall). We
interpret this as a sign that the algorithm handles im-
perfect, automatically induced clusters fairly well.
Impact of Local Alignment. Our hypothesis
for computing local alignments between pairs of
mapped paragraphs is that our approach allows us to
identify additional matching sentence pairs: if two
sentences paraphrase each other but have a low co-
sine measure, looking at the sentence pairs around
12The threshold was tuned on our training data.
13The process was performed manually because the sections
are different from one text to another, both in names and levels
of detail, and because we needed to infer clusters for the para-
graphs that did not have section headings.
Full Dec. Cos.
Range Prec. Rec. Prec. Rec. Prec. Rec.
0%?40% 50% 25% 34% 28% 23% 15%
40%?70% 85% 73% 82% 74% 66% 86%
70%?100% 95% 95% 93% 88% 90% 95%
Table 4: Precision and Recall for different ranges
of lexical similarity for Decomposition, Cosine, and
our full method.
them may increase their chances of getting selected.
We compared our full method with a version of
our algorithm that does not perform local alignment
(?Without Local Alignment?). Instead, it simply se-
lects sentence pairs inside the mapped paragraphs
based on their cosine measure. This incomplete ver-
sion of the algorithm achieves 73.3% Precision (at
55.8% Recall), 3% lower than the full method, vali-
dating our hypothesis.
Impact of Lexical Similarity. We investigated
how the performance of our method depends on the
lexical similarity of the input sentences. Table 4
shows Precision and Recall for our method and oth-
ers at three sentence-similarity ranges based on word
overlap counts (at the overall Recall of 55.8%). Our
method outperforms the Cosine baseline and De-
composition on all similarity ranges.
6 Conclusions
The central finding of our work is that context plays
an important role in the task of sentence alignment
for monolingual comparable corpora. A weak sen-
tence similarity measure combined with contextual
information outperforms methods based on sophis-
ticated sentence similarity functions. Experiments
show that a simple representation of context is help-
ful. Relying on a more elaborate representation,
such as topical text structure, has an even stronger
impact on performance.
Acknowledgments
We would like to thank Michael Elhadad, Klara Kedem, Kathy
McKeown and Becky Passonneau for the useful discussions.
Thanks to Mirella Lapata, Lillian Lee, Vincent Ng and Bo Pang
for their helpful comments on previous drafts. We are grateful
to Hongyan Jing for giving us access to her code for Decompo-
sition. Finally we would like to thank all the annotators (they are
many). This paper is based upon work supported in part by the
National Science Foundation under ITR/IM grant IIS-0081334,
Digital Library Initiative Phase II Grant No. IIS-98-17434, and
a Sloan Research Fellowship to Lillian Lee. Any opinions, find-
ings, and conclusions or recommendations expressed above are
those of the authors and do not necessarily reflect the views of
the National Science Foundation or the Sloan Foundation.
References
Regina Barzilay, Noemie Elhadad, and Kathleen McKeown.
2002. Inferring strategies for sentence ordering in multi-
document news summarization. Journal of Artificial Intel-
ligence Research, 17:35?55.
John Carroll, Guido Minnen, Darren Pearce, Yvonne Canning,
Siobhan Devlin, and John Tait. 1999. Simplifying text for
language-impaired readers. In Proceedings of EACL.
Raman Chandrasekar and Srinivas Bangalore. 1997.
Automatic Induction of Rules for Text Simplification.
Knowledge-Based Systems, 10(3):183?190.
Pascale Fung and Lo Yuen Yee. 1998. An IR approach for
translating new words from nonparallel, comparable texts.
In Proceedings of ACL.
William Gale and Kenneth Church. 1991. A program for align-
ing sentences in bilingual corpora. In Proceedings of ACL.
Vasileios Hatzivassiloglou, Judith Klavans, and Eleazar Eskin.
1999. Detecting text similarity over short passages: Explor-
ing linguistic feature combinations via machine learning. In
Proceedings of EMNLP.
Marti Hearst. 1994. Multi-paragraph segmentation of exposi-
tory text. In Proceedings of ACL.
Hongyan Jing and Kathleen McKeown. 2000. Cut and paste
based summarization. In Proceedings of NAACL.
Hongyan Jing. 2002. Using hidden Markov modeling to de-
compose human-written summaries. Computational Lin-
guistics, 28(4):527?543.
Richard Kittredge, Tanya Korelsky, and Owen Rambow. 1991.
On the need for domain communication knowledge. Com-
putational Intelligence, 7(4):305?314.
Kevin Knight and Daniel Marcu. 2000. Statistics-based sum-
marization - step one: Sentence compression. In Proceed-
ings of AAAI.
William Mann and Sandra Thompson. 1987. Rhetorical struc-
ture theory: description and construction of text structures.
In G. Kempen, editor, Natural Language Generation: Recent
Advances in Artificial Intelligence, Psychology, and Linguis-
tics. Kluwer Academic.
Daniel Marcu. 1999. The automatic construction of large-scale
corpora for summarization research. In Proceedings of SI-
GIR.
Kathleen McKeown. 1985. Text Generation. Cambridge Uni-
versity Press.
Dan Melamed. 1999. Bitext maps and alignment via pattern
recognition. Computational Linguistics, 25(1):107?130.
Cecile Paris. 1993. User Modelling in Text Generation.
Frances Pinter Publishers.
Hadar Shemtov. 1993. Text alignment in a tool for translating
revised documents. In Proceedings of EACL.
Yoram Singer and Robert Schapire. 1998. Improved boosting
algorithms using confidence-rated predictions. In Proceed-
ings of COLT.
BioNLP 2007: Biological, translational, and clinical language processing, pages 49?56,
Prague, June 2007. c?2007 Association for Computational Linguistics
Mining a Lexicon of Technical Terms and Lay Equivalents
Noemie Elhadad and Komal Sutaria
Computer Science Department
The City College of New York
New York, NY 10031
noemie@cs.ccny.cuny.edu, kdsutaria@gmail.com
Abstract
We present a corpus-driven method for
building a lexicon of semantically equiva-
lent pairs of technical and lay medical terms.
Using a parallel corpus of abstracts of clin-
ical studies and corresponding news sto-
ries written for a lay audience, we identify
terms which are good semantic equivalents
of technical terms for a lay audience. Our
method relies on measures of association.
Results show that, despite the small size of
our corpus, a promising number of pairs are
identified.
1 Introduction
The field of health literacy has garnered much at-
tention recently. Studies show that most docu-
ments targeted at health consumers are ill-fitted to
the intended audience and its level of health liter-
acy (Rudd et al, 1999; McCray, 2005). While there
are many components involved in health literacy that
are specific to the reader (e.g., reading level and cul-
tural background), we investigate what can be done
from the standpoint of the text to adapt it to the liter-
acy level of a given reader. As such, we set ourselves
in the context of a text-to-text generation system,
where a technical text is edited to be more compre-
hensible to a lay reader. An essential resource for
such an editing tool is a lexicon of paraphrases, or
semantically equivalent terms. In this paper, we in-
vestigate a corpus-driven method for building such
a lexicon. We focus on terms that are recognized by
the UMLS (UMLS, 1995), both for technical and lay
candidate terms for equivalence.
Because we have lay audiences in mind, our defi-
nition of semantic equivalence must be broader than
a notion of strict medical equivalence utilized by
medical experts. Thus, while a medical dictionary
like UMLS assigns different concept unique identi-
fiers (CUIs) to two particular terms, such as percu-
taneous transluminal coronary angioplasty and an-
gioplasty, these terms should be considered seman-
tically equivalent for the purposes of lay readers.
Besides enabling a text tailoring system to adapt
technical texts for a lay audience, a lexicon of
semantically equivalent technical/lay terms would
benefit other tools as well. For instance, the Con-
sumer Health Vocabulary initiative1 is a comprehen-
sive list of UMLS terms familiar to lay readers. Our
lexicon could help augment the terms with equiva-
lence links to technical terms. While much research
of late has been devoted to identifying terms incom-
prehensible to lay readers, such research has not es-
tablished links between technical terms and equiva-
lent lay terms beyond their CUI information (Zeng
et al, 2005; Elhadad, 2006).
The key points of our approach are: (1) the use
of combined measures of association to identify
pairs of semantically equivalent terms, and (2) a
knowledge-based heuristic which acts as a powerful
filter for identifying semantically equivalent pairs.
Our method does not rely on human labeling of se-
mantically equivalent term pairs. As such, it is un-
supervised, and achieves results that are promising
considering the small size of the corpus from which
the results are derived.
This paper is organized as follows. The next sec-
tion describes our parallel corpus of paired techni-
cal/lay documents. The Methods section describes
the different measures of association we experi-
mented with, how we combine them to leverage their
complimentary strengths, and our semantic filter.
The Results section reports the evaluation against
our gold standard and a discussion of our results.
1http://www.consumerhealthvocab.org
49
2 Data Description
Because our ultimate goal is to learn, in a data-
driven fashion, semantic equivalents of terms that
are too technical for lay readers, we can benefit from
having instances of texts which relay similar infor-
mation but are conveyed in different styles. We
collect a corpus similar in structure to those used
in the field of statistical machine translation. But,
instead of having two collections in different lan-
guages, we collect texts written for two different au-
diences: medically trained readers (technical collec-
tion) and health consumers (lay collection).
The lay collection is composed of news stories
from the ReutersHealth E-line newsfeed2 summariz-
ing research in the medical field. Reuters journalists
take technical publications and report the main find-
ings and methods and, on occasion, include inter-
views with the authors of the scientific publication.
The stories are targeted at a lay audience with a 12th-
grade reading level. Furthermore, every story in our
collection contains a reference to the original scien-
tific publication. Thus, it is possible to gather the
original texts, which convey similar information but
were written for a technical audience. The stories
draw upon studies from reputable medical journals,
such as Annals of Internal Medicine, New England
Journal of Medicine and Lancet.
The technical collection in our corpus is com-
posed of the original scientific articles correspond-
ing to each news story in the lay collection. Accord-
ingly, the lay and technical collections contain the
same number of documents and are parallel at the
document level. That is, each technical document
has a lay equivalent and vice-versa. Because a lay
document is a summary of a technical article and is,
hence, much shorter than the original scientific ar-
ticle, we decided to include only the abstract of the
technical document in our collection. This way, the
technical and lay documents are comparable in con-
tent and length. It should be noted, however, that
the content in a technical/lay document pair is not
parallel, but comparable (McEnery and Xiao, 2007):
there is no natural sentence-to-sentence correspon-
dence between the two texts. This is to be expected:
technical abstracts contain many technical details,
while lay stories, to provide background, introduce
2http://www.reutershealth.com
Words Sentences
Min Max Avg Min Max Avg
Technical 137 565 317 5 18 10
Lay 187 1262 444 6 42 15
Table 1: Statistics for the Technical and Lay collec-
tions. Each contains 367 documents.
information entirely absent from abstracts. In addi-
tion, the lay stories drastically rearrange the order in
which information is typically conveyed in technical
abstracts. For these reasons, our corpus is not paral-
lel at the sentence level and, thus, differs from other
bilingual parallel corpora used in machine transla-
tion.
To ensure that some significant number of terms
appears with sufficient frequency in our corpus in
order to induce equivalent pairs automatically, we
focused on articles and stories in a single domain:
cardiology. We identified the original scientific ar-
ticle manually, as the lay document only contains a
reference, not an actual link. For this reason, only a
relatively small amount of data could be collected:
367 pairs of documents (see Table 1 for statistics).
3 Methods
3.1 Data Processing
We focus in this paper on finding term equiva-
lents when both terms are recognized by the UMLS.
Thus, our first step in processing our collections is to
identify terms as defined by the UMLS. Both collec-
tions are processed by our tool TermFinder (Teufel
and Elhadad, 2002). Sentences are identified and the
texts are tokenized and tagged with part-of-speech
information. Noun phrases are identified with a shal-
low parser. Next, terms are identified by looking up
the noun phrases in the meta-lexicon of UMLS for
an exact match. Terms are tagged with their con-
cept unique identifier (CUI) and a semantic type,
both provided by UMLS. For our purposes, we only
consider a subset of all the terms listed in UMLS,
based on their semantic type. This is due to the
fact that certain UMLS semantic types are unlikely
to yield technical terms in need of simplification.
As such, terms belonging to semantic types such
as ?Activity,? ?Family Group? or ?Behavior? were
left untagged. Terms with semantic types such as
?Disease or Syndrome? or ?Therapeutic or Preven-
50
Corresponding
lay doc. contains
lay term
Corresponding lay
doc. does not con-
tain lay term
Technical
doc. contains
tech term
a b
Technical doc.
does not con-
tain tech term
c d
Table 2: Contingency table for (tech term,
lay term).
tive Procedure,? on the other hand, were considered
terms. For instance, both the terms PTCA and percu-
taneous transluminal coronary angioplasty have the
same CUI C0002997, as they are considered syn-
onyms by UMLS. The term balloon angioplasty has
the CUI C0002996. Both C0002997 and C0002996
have the semantic type ?Therapeutic or Preventive
Procedure.?
3.2 Contingency Table
We call (tech term, lay term) a term pair, where
tech term is a term occurring in one or more tech-
nical documents and lay term is a term present
in at least one of the corresponding lay docu-
ments.3 For any such pair, we can compute a
contingency table based on co-occurrence. Our
definition of co-occurrence is slightly unusual:
tech term and lay term co-occur in one document
pair if tech term appears at least once in the techni-
cal document and lay term appears at least once in
the corresponding lay document. Our unit of content
is document frequency for a CUI, i.e., the number of
documents in which a given CUI appears. For in-
stance, in our data, the contingency table for the term
pair (MI, heart attack) shows the following counts:
the document frequency of the CUI corresponding
to MI in the technical collection is 98; the docu-
ment frequency of the CUI corresponding to heart
attack in the lay collection is 161. Among these doc-
uments, there are 84 technical/lay document pairs
(out of the total of 367 paired documents) in which
the CUI for MI occurs on the technical side and the
CUI for heart attack occurs on the lay side. Hence,
the contingency table for this term pair is, following
3This means that if tech term and lay term have no tech-
nical/lay document in common, lay term is not considered a
possible candidate for semantic equivalence for tech term.
the notations of Table 2: a = 84, b = 98-84 = 14, c =
161-84 = 77, and d = 367-98-161+84 = 192.
At this stage of processing, lexical terms are ab-
stracted by their CUIs. We do this to maximize the
possible evidence that two terms co-occur. For in-
stance, the document frequency for MI in our tech-
nical collection is 20, while the document frequency
for its corresponding CUI is 98. Section 3.7 de-
scribes how we proceed from identifying equivalent
terms at the CUI level to finding lexical equivalents.
3.3 Gold Standard
To evaluate the validity of our approach, we col-
lected all possible term pairs at the CUI level in our
corpus (that is, all the term pairs for which a con-
tingency table is computed). We then whittled this
set down to those pairs where each CUI occurs in
at least two documents. This resulted in 2,454 pairs
of CUIs. We asked our medical expert, an internist
in practice who interacts with patients on a daily ba-
sis, to indicate for each pair whether the terms were
equivalent from a medical standpoint in the con-
text of communicating with a patient.4 An opera-
tional test for testing the equivalence of two terms is
whether he would use one term for the other when
talking to a patient. We indicated to our expert that
the terms should be equivalent out of context. So,
for instance, while the pair (myocardial infarction,
complication) could be deemed equivalent in certain
specific contexts, these terms are not generally con-
sidered equivalent. Table 3 shows examples of pairs
annotated as semantic equivalents for lay readers.5
The list of terms contained only the actual lexical
terms and no information from the UMLS to avoid
biasing our expert.
Out of the 2,454 CUI pairs provided to our medi-
cal expert, 152 pairs were labeled as equivalent. Out
of the 152 pairs, only 8 (5.3%) had different seman-
tic types. Interestingly, 84 pairs (55.3%) had differ-
ent CUIs. This confirms our intuition that the notion
of semantic equivalence for lay readers is looser than
for medically knowledgeable readers.
4While it is in some ways counterintuitive to rely on a tech-
nical expert to identify lay semantic equivalents, this expertise
helps us validate equivalences from a medical standpoint.
5In the table, DIGN stands for ?Diagnostic Procedure,?
DISS for ?Disease or Symptom,? FIND for ?Finding,? and
PATH for ?Pathological Finding.?
51
Technical term Lay term
myocardial infarction | C0027051 | DISS heart attack | C0027051 | DISS
SBP | C0428880 | DIGN systolic blood pressure | C0428880 | DIGN
atrial fibrillation | C0004238 | PATH arrhythmia | C0003811 | PATH
hypercholesterolemia | C0020443 | DISS high cholesterol | C0848569 | FIND
mental stress | C0038443 | DISS stress | C0038435 | PATH
Table 3: Examples from the gold standard of term pairs considered equivalent.
3.4 Measures of Association
Given a term pair (tech term, lay term) and its cor-
responding contingency table, we want to determine
whether lay term is a valid semantic equivalent of
tech term from the standpoint of a lay reader. We
rely on three alternative measures of association in-
troduced in the Statistics literature: the ?2 statis-
tic, the ? measure, and odds ratio. All of these
measures are computed as a function of the contin-
gency table, and do not rely on any human labeling
for equivalence. Measures of association have been
used traditionally to identify collocations (Manning
and Schu?tze, 1999). Here we investigate their use
for building a lexicon.
3.4.1 The ?2 Statistic
The standard chi-square statistic (?2) is used
to determine whether the deviation of observed
data from an expected event occurs solely by
chance (Goodman and Kruskal, 1979). Our null
hypothesis for this task is that the presence of
lay term in a lay document is independent of the
presence of tech term in its correspondent techni-
cal document. Thus, any pair of terms for which the
?2 is above the critical value at a given level of sig-
nificance are considered semantic equivalents. One
important constraint for the measures to be valid is
that the observed data be large enough (more than
five observations per cell in the contingency table).
The ?2 statistic for our 2x2 contingency table,
and with N being the total number of document
pairs, is calculated as follows:
?2 = N(ad ? bc)
2
(a + b)(a + c)(c + d)(b + d)
Since ?2 is a true statistic, we can rely on critical
values to filter out pairs with low associative power.
In our case, we set the significance level at .001
(with a critical value for ?2 of 10.83).
C0011847 ? C0011847 Sum
C0011849 a = 13 b = 8 21
? C0011849 c = 40 d = 306 346
Sum 53 314 367
Table 4: Contingency table for (C0011849,
C0011847).
3.4.2 The ? and ?* Measures
The lambda measure (?) assesses the extent to
which we can predict the presence of lay term in a
lay document by knowing whether the original tech-
nical document contained tech term (Goodman and
Kruskal, 1979). ? is an asymmetrical measure of
association. Since a lay document is always writ-
ten based on an original technical document, it is
a plausible assumption that the presence of a spe-
cific term in the technical document influenced the
lexical choices of the author of the lay document.
Thus, we consider the presence of tech term in a
technical document the antecedent to the presence
of lay term in the corresponding lay document, and,
accordingly, operate in the setting of predicting the
presence of lay term.
We present the intuition behind ? in the context
of the following example. Consider the contingency
table for the technical CUI C0011849 (diabetes mel-
litus) and C0011847 (diabetes) in Table 4. The task
is, given a random lay document, to predict which of
two available categories it belongs to: either it con-
tains the lay CUI (in our example, CUI C0011847
for diabetes) or it does not. There are two possi-
ble cases: either (1) we do not have any knowl-
edge about the original technical document, or (2)
we know the original technical document and, there-
fore, we know whether it contains the antecedent (in
our example, CUI C0011849 for diabetes mellitus).
Without any prior knowledge (case (1)), the
safest prediction we can make about the lay doc-
ument is the category with the highest probabil-
52
ity. The probability of error in case (1) is Perr1 =
N?Max(a+c,b+d)
N .In our example, the safest bet is ? C0011847, with
a raw count of 314 documents, and a probability of
error of Perr1 = 0.1444.
If we have prior knowledge about the original
technical document (case (2)), then our safest pre-
diction differs. If we know that the technical doc-
ument contains the CUI C0011849 (diabetes melli-
tus), then our safest prediction is the category with
the highest probability: C0011847, with a raw count
of 13 documents. If, on the other hand, we know
that the technical document does not contain the
CUI C0011849, our safest prediction is the category
? C0011847, with a raw count of 306 documents.
Thus, overall the probability of error in case (2) is
Perr2 = N?(Max(a,b)+Max(c,d))N .In our example, knowledge about the original tech-
nical document lowers the probability of error to
Perr2 = 0.1308.
The ? measure is defined as the relative decrease
in probability of error in guessing the presence of
lay term in a lay document ? = Perr1?Perr2Perr1which, using our notation for contingency tables,
can be expressed as
? = Max(a, b) + Max(c, d) ? Max(a + c, b + d)N ?Max(a + c, b + d)
In our example, ? = 0.094. ? ranges between 0
and 1. A value of 1 means that knowledge about
the presence of tech term in the original techni-
cal document completely specifies the presence of
lay term in its corresponding lay document. A value
of 0 means that knowledge about the presence of
tech term in the original technical document does
not help in predicting whether lay term is present in
its corresponding lay document.
The ? measure is not a test of significance like
?2. For instance, while two independent variables
necessarily have a ? of 0, the opposite is not neces-
sarily true: it is possible for two dependent variables
to have a ? of 0. In our setting in particular, any
contingency table where a=b will provide a ? of 0.
Since ? is computed as a function of maxima of
rows and columns, ? can easily be biased toward the
original proportions in the antecedent. In our exam-
ple, for instance, a very large proportion of technical
documents has no occurrence of C0011849, diabetes
mellitus (94.3% of the technical documents). But
for our purposes, such contingencies should not af-
fect our measure of association, as the proportion of
technical documents happening not to contain a par-
ticular term is just an artificial consequence of cor-
pus collection. ?* is a variant of ? also proposed by
Goodman and Kruskal (1979) and is able to take this
fact into account. It is computed using the same for-
mula as ?, but the elements of the contingency table
are modified so that each category of the antecedent
is equally likely. In our case, this means: N*=1,
a*=0.5a/N(a+b), b*=0.5b/N(a+b), c*=0.5c/N(c+d),
and d*=0.5d/N(c+d). Going back to our example of
diabetes mellitus and diabetes, we now find ?? =
0.324, which is much higher than the original ? of
0.094, and which indicates a strong association.
We focus on ?* as a measure of association for
semantic equivalence of term pairs. Since ? and ?*
are not true statistics, there is no significance level
we can rely on to set a threshold for them. Instead,
we estimate an optimal threshold from the perfor-
mance of ?* on a development set. The develop-
ment set was obtained in the same manner as the
gold standard and contains 50 term pairs. This is
a small number of pairs, but the term pairs in the
development set were carefully chosen to contain
mostly semantically equivalent pairs. In our experi-
ments, the optimal value for ?* was 0.3. Thus, ?* is
used as a binary test for our purposes: tech term and
lay term are considered semantically equivalent if
their ?* is above 0.3.
3.4.3 Odds Ratio
Odds ratio is a measure of association that focuses
on the extent to which one category in the contin-
gency table affects another (Fleiss et al, 2003). For
our contingency table, the odds ratio is expressed as
follows:
OR = adbc
For instance, given the contingency table of Ta-
ble 4, the odds ratio for the pair (diabetes mellitus,
diabetes) is 12.43, which means that a lay docu-
ment is 12.43 times more likely to contain the CUI
C0011847, for diabetes, if its original technical doc-
ument contains the term C0011849, for diabetes
mellitus.
53
Like ?*, odds ratio is not a true statistic and,
therefore, does not have any critical value for sta-
tistical significance. We estimated the optimal value
of a threshold for OR based on the same develop-
ment set described above. The threshold for OR is
set to 6. Thus, OR is used as a binary test for our
purposes: tech term and lay term are considered
semantically equivalent if their OR is above 6.
3.5 Combining the Measures of Association
Each of the measures of association described above
leverages different characteristics of the contingency
tables, and similarly, each has its limitations. For
instance, ?2 cannot be computed when there are
not sufficient observations, and ?* can equal 0, even
when there is a strong association between the two
terms. We combine measures of association in the
following fashion: two terms are considered equiva-
lent if at least one of the measures determined so.
3.6 Semantic Filtering
The measures of association described above and
their combination provide information solely based
on corpus-derived data. Since all our counts are
based on co-occurrence, a measure of association by
itself can encompass many types of semantic rela-
tions. For instance, the pair for (stroke, brain) tests
positive with our three measures of association. In-
deed, there is a strong semantic association between
the two terms: strokes occur in the brain. These
terms, however, do not fit our definition of seman-
tic equivalence.
We rely on knowledge provided by the UMLS,
namely semantic types, to help us filter equiv-
alent types of associations among the candidate
term pairs. One can assume that sharing semantic
types is a necessary condition for semantic equiva-
lence. Our semantic filter consists of testing whether
tech term and lay term share the same semantic
types, as identified by our tool TermFinder.
3.7 Lexical Choice
So far, term pairs are at the CUI level. The measures
of association and the semantic filter provide a way
to identify candidates for semantic equivalence. We
still have to figure out which particular lexical items
among the different lexical terms of a given CUI are
appropriate for a lay reader. For instance, the pair
(C0027051, C0027051) is considered a good candi-
date for semantic equivalence. In the technical col-
lection, the lexical terms contributing to the CUI are
AMI, AMIs, MI, myocardial infarction, myocardial
infarct and myocardial necrosis. In the lay collec-
tion, however, the lexical terms contributing to the
same CUI are heart attack, heart attacks, and my-
ocardial infarction. Clearly, not all lexical items for
a given CUI are appropriate for a lay reader.
To select an appropriate lay lexical term, we rely
on the term frequency of each lexical item in the
lay collection (Elhadad, 2006). In our example, the
lexical term ?heart attack? has the highest term fre-
quency in the lay collection among all the variants
with the same CUI. Thus, we chose it as a semantic
equivalent of any lexical term of the CUI C0027051
in the technical collection.
If a technical term has several candidate semantic
equivalents at the CUI level, the lexical lay term is
chosen among all the lay terms. For instance, (ad-
verse effect, side effect) and (adverse effect, compli-
cations) are two valid equivalents, but side effects
has a term frequency of 16 in our lay collection, and
complications has a lay term frequency of 35. Thus,
complication is selected as the lay equivalent for ad-
verse effect.
4 Results
We report on the two steps of our system: (1) find-
ing semantic equivalents at the CUI level, and (2)
finding an appropriate lay lexical equivalent.
Finding Semantic Equivalents at the CUI Level
Table 5 shows the precision, recall and F-measure
(computed as the harmonic mean between precision
and recall) against our gold standard for the three
alternative measures of association, including dif-
ferent combinations of these, and also adding the
semantic filter. In addition, we report results for a
competitive baseline based solely on CUI informa-
tion, where tech term and lay term are considered
equivalent if they have the same CUI.
The baseline is fairly competitive only because
of its perfect precision (CUI in Table 5). Its recall,
however (44.7), indicates that building a lexicon of
technical and lay equivalents based solely on CUI
information would miss too many pairs within the
UMLS.
54
Method P R F Method P R F Method P R F
lam 40.8 20.4 27.2 chi,odds 20.6 78.3 32.6 CUI 100 44.7 61.8
chi 38.7 23.7 29.4 chi,lam,odds 20.6 80.3 32.8 sem,odds 57.8 71.1 63.7
sem,lam 76.3 19.1 30.5 sem,chi 81.8 23.7 36.7 sem,lam,odds 57.4 73.7 64.6
odds 20.4 74.3 32 chi,lam 38.2 39.5 38.8 sem,chi,odds 58.5 75 65.7
lam,odds 20.5 77 32.3 sem,chi,lam 79.5 38.2 51.6 sem,chi,lam,odds 57.9 77 66.1
Table 5: Precision, Recall and F measures for different variants of the system.
Relying on only one measure of association with-
out any semantic filtering to determine semantic
equivalents is not a good strategy: ?* (lam in Ta-
ble 5), ?2(chi) and OR (odds), by themselves, yield
the worst F measures. Interestingly, the measures
of association identify different equivalent pairs in
the pool of candidate pairs. Thus, combining them
increases the coverage (or recall) of the system.
For instance, ?* by itself has a low recall of 20.4
(lam). When combined with OR, it improves the re-
call from 74.3 (odds) to 77 (lam,odds); when com-
bined with ?2, it improves the recall from 23.7 (chi)
to 39.5 (chi,lam). Combining the three measures
of association (chi,lam,odds) yields the best recall
(80.3), confirming our hypothesis that the measures
are complementary and identify pairs with different
characteristics in our corpus.
While combining measures of association im-
proves recall, the semantic filter is very effective in
filtering inaccurate pairs and, therefore, improving
precision: ?*, for instance, improves from a pre-
cision of 40.8 (lam) to 76.3 (sem,lam) when the
filter is added, with very little change in recall.
The best variant of our system in terms of F mea-
sure is, not surprisingly, combining the three mea-
sures of association and adding the semantic filter
(sem,chi,lam,odds in Table 5).
The results of these experiments are surprisingly
good, considering that the contingency tables are
built from a corpus of only 367 document pairs and
rely on document frequency (not term frequency).
These quantities are much smaller than those used
in machine translation, for instance.
Finding Lay Lexical Equivalents We evaluate
our strategy for finding an appropriate lay lexical
item on the list of 152 term pairs identified by our
medical expert as semantic equivalents. Our strat-
egy achieves an accuracy of 86.7%.
5 Related Work
Our work belongs to the field of paraphrase identi-
fication. Much work has been done to build lexi-
cons of semantically equivalent phrases. In gener-
ation systems, a lexicon is built manually (Robin,
1994) or by relying on an electronic thesaurus like
WordNet (Langkilde and Knight, 1998) and setting
constraints on the type of accepted paraphrases (for
instance, accepting only synonyms as paraphrases,
and not hypernyms). Building paraphrase lexicons
from a corpus has also been investigated. Jacquemin
and colleagues (1997) identify morphological and
syntactic variants of technical terms. Barzilay and
McKeown (2001) identify multi-word paraphrases
from a sentence-aligned corpus of monolingual par-
allel texts. One interesting finding of this work is
that the mined paraphrases were distributed across
different semantic links in WordNet: some para-
phrases had a hypernym relation, while others were
synonyms, and others had no semantic links at all.
The composition of our gold standard confirms this
finding, since half of the semantically equivalent
terms had different CUIs (see Table 3 for examples
of such pairs).
If we consider technical and lay writing styles as
two sublanguages, it is easy to see an analogy be-
tween our task and that of machine translation. Iden-
tifying translations for words or phrases has been
deeply investigated in the field of statistical machine
translation. The IBM models of word alignments are
the basis for most algorithms to date. All of these are
instances of the EM algorithm (Expectation Maxi-
mization) and rely on large corpora aligned at the
sentence level. We cannot apply an EM-based model
to our task since we have a very small corpus of
paired technical/lay documents, and EM requires
large amounts of data to achieve accurate results.
Moreover, the technical and lay documents are not
parallel, and thus, we do not have access to a sen-
55
tence alignment. Of course, our task is easier than
the one of machine translation, since we focus on
?translating? only technical terms, rather than every
single word in a technical document.
Gale and Church (1991) do not follow the EM
model, but rather find French translations of English
words using a ?2-like measure of association. Their
corpus is the parallel, sentence-aligned Hansard cor-
pus. Our method differs from theirs, as we do build
the contingency table based on document frequen-
cies. Gale and Church employ sentence-level fre-
quencies. Our corpus is much smaller, and the sen-
tences are not aligned (for comparison, we have
367 document-pairs, while they have nearly 900,000
sentence pairs). Another difference between our ap-
proach and theirs is our use of the semantic filter
based on UMLS. We can afford to have such a filter
because we focus on finding semantic equivalents of
UMLS terms only.
6 Conclusions and Future Work
We presented an unsupervised method for identi-
fying pairs of semantically equivalent technical/lay
terms. Such a lexicon would benefit research in
health literacy. In particular, it would benefit a sys-
tem which automatically adapts a medical technical
text to different levels of medical expertise.
We collected a corpus of pairs of technical/lay
documents, where both documents convey similar
information, but each is written for a different au-
dience. Based on this corpus, we designed a method
based on three alternative measures of association
and a semantic filter derived from the UMLS. Our
experiments show that combining data-driven statis-
tics and a knowledge-based filter provides the best
results.
Our method is concerned specifically with pairs
of terms, as recognized from UMLS. While UMLS
provides high coverage for technical terms, that is
not the case for lay terms. In the future, we would
like to extend our investigation to pairs consisting
of a technical term and any noun phrase which is
sufficiently frequent in our lay collection. Finding
such pairs would have the side effect of augmenting
UMLS, a primarily technical resource, with mined
lay terms. One probable step towards this goal will
be to increase the size of our corpus of paired tech-
nical and lay documents.
References
R. Barzilay and K. McKeown. 2001. Extracting para-phrases from a parallel corpus. In Proc. ACL?01, pages50?57.
N. Elhadad. 2006. Comprehending technical texts:Predicting and defining unfamiliar terms. In Proc.
AMIA?06, pages 239?243.
J. Fleiss, B. Levin, and M.C. Paik. 2003. Statistical
Methods for Rates and Proportions. Wiley.
W. Gale and K. Church. 1991. Identifying word cor-
respondences in parallel texts. In Proc. Speech and
Natural Language Workshop, pages 152?157.
L. Goodman and W. Kruskal. 1979. Measures of Associ-
ation for Cross Classifications. Springler Verlag.
C. Jacquemin, J. Klavans, and E. Tzoukermann. 1997.Expansion of multi-word terms for indexing and re-
trieval using morphology and syntax. In Proc.
ACL?97, pages 24?31.
I. Langkilde and K. Knight. 1998. Generation that ex-ploits corpus-based statistical knowledge. In Proc.
COLING-ACL?98, pages 704?710.
C. Manning and H. Schu?tze. 1999. Foundations of Sta-
tistical Natural Language Processing. MIT Press.
A. McCray. 2005. Promoting health literacy. JAMA,
12(2):152?163.
A. McEnery and Z. Xiao. 2007. Parallel and comparable
corpora: What is happening? In Incorporating Cor-
pora. The Linguist and the Translator. Clevedon.
National Library of Medicine, Bethesda,Maryland, 1995. Unified Medical Lan-
guage System (UMLS) Knowledge Sources.
http://www.nlm.nih.gov/research/umls/.
J. Robin. 1994. Revision-Based Generation of Natu-
ral Language Summaries Providing Historical Back-
ground. Ph.D. thesis, Columbia University.
R. Rudd, B. Moeykens, and T. Colton. 1999. Annual Re-
view of Adult Learning and Literacy, chapter 5. Health
and literacy: a review of medical and public health lit-erature. Jossey Bass.
S. Teufel and N. Elhadad. 2002. Collection and Lin-guistic Processing of a Large-scale Corpus of MedicalArticles. In Proc. LREC?02, pages 1214?1218.
Q. Zeng, E. Kim, J. Crowell, and T. Tse. 2005. A textcorpora-based estimation of the familiarity of health
terminology. In Proc. ISBMDA?05, pages 184?192.
56
Coling 2010: Poster Volume, pages 276?284,
Beijing, August 2010
A Comparison of Features for Automatic Readability Assessment
Lijun Feng
City University of New York
lijun7.feng@gmail.com
Martin Jansche
Google, Inc.
jansche@acm.org
Matt Huenerfauth
City University of New York
matt@cs.qc.cuny.edu
Noe?mie Elhadad
Columbia University
noemie@dbmi.columbia.edu
Abstract
Several sets of explanatory variables ? in-
cluding shallow, language modeling, POS,
syntactic, and discourse features ? are com-
pared and evaluated in terms of their im-
pact on predicting the grade level of read-
ing material for primary school students.
We find that features based on in-domain
language models have the highest predic-
tive power. Entity-density (a discourse fea-
ture) and POS-features, in particular nouns,
are individually very useful but highly cor-
related. Average sentence length (a shal-
low feature) is more useful ? and less ex-
pensive to compute ? than individual syn-
tactic features. A judicious combination
of features examined here results in a sig-
nificant improvement over the state of the
art.
1 Introduction
1.1 Motivation and Method
Readability Assessment quantifies the difficulty
with which a reader understands a text. Automatic
readability assessment enables the selection of ap-
propriate reading material for readers of varying
proficiency. Besides modeling and understanding
the linguistic components involved in readability, a
readability-prediction algorithm can be leveraged
for the task of automatic text simplification: as sim-
plification operators are applied to a text, the read-
ability is assessed to determine whether more sim-
plification is needed or a particular reading level
was reached.
Identifying text properties that are strongly cor-
related with text complexity is itself complex. In
this paper, we explore a broad range of text proper-
ties at various linguistic levels, ranging from dis-
course features to language modeling features, part-
of-speech-based grammatical features, parsed syn-
tactic features and well studied shallow features,
many of which are inspired by previous work.
We use grade levels, which indicate the number
of years of education required to completely under-
stand a text, as a proxy for reading difficulty. The
corpus in our study consists of texts labeled with
grade levels ranging from grade 2 to 5. We treat
readability assessment as a classification task and
evaluate trained classifiers in terms of their predic-
tion accuracy. To investigate the contributions of
various sets of features, we build prediction models
and examine how the choice of features influences
the model performance.
1.2 Related Work
Many traditional readability metrics are linear mod-
els with a few (often two or three) predictor vari-
ables based on superficial properties of words, sen-
tences, and documents. These shallow features
include the average number of syllables per word,
the number of words per sentence, or binned word
frequency. For example, the Flesch-Kincaid Grade
Level formula uses the average number of words
per sentence and the average number of syllables
per word to predict the grade level (Flesch, 1979).
The Gunning FOG index (Gunning, 1952) uses av-
erage sentence length and the percentage of words
with at least three syllables. These traditional met-
rics are easy to compute and use, but they are not
reliable, as demonstrated by several recent stud-
ies in the field (Si and Callan, 2001; Petersen and
Ostendorf, 2006; Feng et al, 2009).
276
With the advancement of natural language pro-
cessing tools, a wide range of more complex text
properties have been explored at various linguis-
tic levels. Si and Callan (2001) used unigram
language models to capture content information
from scientific web pages. Collins-Thompson and
Callan (2004) adopted a similar approach and used
a smoothed unigram model to predict the grade lev-
els of short passages and web documents. Heilman
et al (2007) continued using language modeling
to predict readability for first and second language
texts. Furthermore, they experimented with vari-
ous statistical models to test their effectiveness at
predicting reading difficulty (Heilman et al, 2008).
Schwarm/Petersen and Ostendorf (Schwarm and
Ostendorf, 2005; Petersen and Ostendorf, 2006)
used support vector machines to combine features
from traditional reading level measures, statistical
language models and automatic parsers to assess
reading levels. In addition to lexical and syntactic
features, several researchers started to explore dis-
course level features and examine their usefulness
in predicting text readability. Pitler and Nenkova
(2008) used the Penn Discourse Treebank (Prasad
et al, 2008) to examine discourse relations. We
previously used a lexical-chaining tool to extract
entities that are connected by certain semantic re-
lations (Feng et al, 2009).
In this study, we systematically evaluate all
above-mentioned types of features, as well as a
few extensions and variations. A detailed descrip-
tion of the features appears in Section 3. Section
4 discusses results of experiments with classifiers
trained on these features. We begin with a descrip-
tion of our data in the following section.
2 Corpus
We contacted the Weekly Reader1 corporation, an
on-line publisher producing magazines for elemen-
tary and high school students, and were granted
access in October 2008 to an archive of their ar-
ticles. Among the articles retrieved, only those
for elementary school students are labeled with
grade levels, which range from 2 to 5. We selected
only this portion of articles (1629 in total) for the
1http://www.weeklyreader.com
Table 1: Statistics for the Weekly Reader Corpus
Grade docs. words/document words/sentence
mean std. dev. mean std. dev.
2 174 128.27 106.03 9.54 2.32
3 289 171.96 106.05 11.39 2.42
4 428 278.03 187.58 13.67 2.65
5 542 335.56 230.25 15.28 3.21
study.2 These articles are intended to build chil-
dren?s general knowledge and help them practice
reading skills. While pre-processing the texts, we
found that many articles, especially those for lower
grade levels, consist of only puzzles and quizzes,
often in the form of simple multiple-choice ques-
tions. We discarded such texts and kept only 1433
full articles. Some distributional statistics of the
final corpus are listed in Table 1.
3 Features
3.1 Discourse Features
We implement four subsets of discourse fea-
tures: entity-density features, lexical-chain fea-
tures, coreference inference features and entity grid
features. The coreference inference features are
novel and have not been studied before. We pre-
viously studied entity-density features and lexical-
chain features for readers with intellectual disabili-
ties (Feng et al, 2009). Entity-grid features have
been studied by Barzilay and Lapata (2008) in a
stylistic classification task. Pitler and Nenkova
(2008) used the same features to evaluate how well
a text is written. We replicate this set of features
for grade level prediction task.
3.1.1 Entity-Density Features
Conceptual information is often introduced in a
text by entities, which consist of general nouns
and named entities, e.g. people?s names, locations,
organizations, etc. These are important in text
comprehension, because established entities form
basic components of concepts and propositions, on
which higher level discourse processing is based.
Our prior work illustrated the importance of en-
tities in text comprehension (Feng et al, 2009).
2A corpus of Weekly Reader articles was previously used
in work by Schwarm and Ostendorf (2005). However, the two
corpora are not identical in size nor content.
277
Table 2: New Entity-Density Features
1 percentage of named entities per document
2 percentage of named entities per sentences
3 percentage of overlapping nouns removed
4 average number of remaining nouns per sentence
5 percentage of named entities in total entities
6 percentage of remaining nouns in total entities
We hypothesized that the number of entities in-
troduced in a text relates to the working memory
burden on their targeted readers ? individuals with
intellectual disabilities. We defined entities as a
union of named entities and general nouns (nouns
and proper nouns) contained in a text, with over-
lapping general nouns removed. Based on this, we
implemented four kinds of entity-density features:
total number of entity mentions per document, total
number of unique entity mentions per document,
average number of entity mentions per sentence,
and average number of unique entity mentions per
sentence.
We believe entity-density features may also re-
late to the readability of a text for a general au-
dience. In this paper, we conduct a more re-
fined analysis of general nouns and named entities.
To collect entities for each document, we used
OpenNLP?s3 name-finding tool to extract named
entities; general nouns are extracted from the out-
put of Charniak?s Parser (see Section 3.3). Based
on the set of entities collected for each document,
we implement 12 new features. We list several of
these features in in Table 2.
3.1.2 Lexical Chain Features
During reading, a more challenging task with enti-
ties is not just to keep track of them, but to resolve
the semantic relations among them, so that infor-
mation can be processed, organized and stored in
a structured way for comprehension and later re-
trieval. In earlier work (Feng et al, 2009), we
used a lexical-chaining tool developed by Galley
and McKeown (2003) to annotate six semantic re-
lations among entities, e.g. synonym, hypernym,
hyponym, etc. Entities that are connected by these
semantic relations were linked through the text to
form lexical chains. Based on these chains, we
implemented six features, listed in Table 3, which
3http://opennlp.sourceforge.net/
Table 3: Lexical Chain Features
1 total number of lexical chains per document
2 avg. lexical chain length
3 avg. lexical chain span
4 num. of lex. chains with span ? half doc. length
5 num. of active chains per word
6 num. of active chains per entity
Table 4: Coreference Chain Features
1 total number of coreference chains per document
2 avg. num. of coreferences per chain
3 avg. chain span
4 num. of coref. chains with span ? half doc. length
5 avg. inference distance per chain
6 num. of active coreference chains per word
7 num. of active coreference chains per entity
we use in our current study. The length of a chain
is the number of entities contained in the chain,
the span of chain is the distance between the index
of the first and last entity in a chain. A chain is
defined to be active for a word or an entity if this
chain passes through its current location.
3.1.3 Coreference Inference Features
Relations among concepts and propositions are of-
ten not stated explicitly in a text. Automatically re-
solving implicit discourse relations is a hard prob-
lem. Therefore, we focus on one particular type,
referential relations, which are often established
through anaphoric devices, e.g. pronominal refer-
ences. The ability to resolve referential relations is
important for text comprehension.
We use OpenNLP to resolve coreferences. En-
tities and pronominal references that occur across
the text and refer to the same person or object
are extracted and formed into a coreference chain.
Based on the chains extracted, we implement seven
features as listed in Table 4. The chain length,
chain span and active chains are defined in a sim-
ilar way to the lexical chain features. Inference
distance is the difference between the index of the
referent and that of its pronominal reference. If the
same referent occurs more than once in a chain,
the index of the closest occurrence is used when
computing the inference distance.
3.1.4 Entity Grid Features
Coherent texts are easier to read. Several computa-
tional models have been developed to represent and
278
measure discourse coherence (Lapata and Barzilay,
2005; Soricut and Marcu, 2006; Elsner et al, 2007;
Barzilay and Lapata, 2008) for NLP tasks such as
text ordering and text generation. Although these
models are not intended directly for readability re-
search, Barzilay and Lapata (2008) have reported
that distributional properties of local entities gen-
erated by their grid models are useful in detecting
original texts from their simplified versions when
combined with well studied lexical and syntactic
features. This approach was subsequently pursued
by Pitler and Nenkova (2008) in their readability
study. Barzilay and Lapata?s entity grid model is
based on the assumption that the distribution of
entities in locally coherent texts exhibits certain
regularities. Each text is abstracted into a grid
that captures the distribution of entity patterns at
the level of sentence-to-sentence transitions. The
entity grid is a two-dimensional array, with one di-
mension corresponding to the salient entities in the
text, and the other corresponding to each sentence
of the text. Each grid cell contains the grammatical
role of the specified entity in the specified sentence:
whether it is a subject (S), object (O), neither of
the two (X), or absent from the sentence (-).
We use the Brown Coherence Toolkit (v0.2) (El-
sner et al, 2007), based on (Lapata and Barzilay,
2005), to generate an entity grid for each text in
our corpus. The distribution patterns of entities
are traced between each pair of adjacent sentences,
resulting in 16 entity transition patterns4. We then
compute the distribution probability of each entity
transition pattern within a text to form 16 entity-
grid-based features.
3.2 Language Modeling Features
Our language-modeling-based features are inspired
by Schwarm and Ostendorf?s (2005) work, a study
that is closely related to ours. They used data
from the same data ? the Weekly Reader ? for
their study. They trained three language mod-
els (unigram, bigram and trigram) on two paired
complex/simplified corpora (Britannica and Litera-
cyNet) using an approach in which words with high
information gain are kept and the remaining words
4These 16 transition patterns are: ?SS?, ?SO?, ?SX?, ?S-?,
?OS?, ?OO?, ?OX?, ?O-?, ?XS?, ?XO?, ?XX?, ?X-?, ?-S?,
?-O?, ?-X?, ?- -?.
are replaced with their parts of speech. These lan-
guage models were then used to score each text
in the Weekly Reader corpus by perplexity. They
reported that this approach was more successful
than training LMs on text sequences of word la-
bels alone, though without providing supporting
statistics.
It?s worth pointing out that their LMs were not
trained on the Weekly Reader data, but rather on
two unrelated paired corpora (Britannica and Lit-
eracyNet). This seems counter-intuitive, because
training LMs directly on the Weekly Reader data
would provide more class-specific information for
the classifiers. They justified this choice by stating
that splitting limited Weekly Reader data for train-
ing and testing purposes resulted in unsuccessful
performance.
We overcome this problem by using a hold-
one-out approach to train LMs directly on our
Weekly Reader corpus, which contains texts rang-
ing from Grade 2 to 5. We use grade levels to
divide the whole corpus into four smaller subsets.
In addition to implementing Schwarm and Osten-
dorf?s information-gain approach, we also built
LMs based on three other types of text sequences
for comparison purposes. These included: word-
token-only sequence (i.e., the original text), POS-
only sequence, and paired word-POS sequence.
For each grade level, we use the SRI Language
Modeling Toolkit5 (with Good-Turing discounting
and Katz backoff for smoothing) to train 5 lan-
guage models (1- to 5-gram) using each of the four
text sequences, resulting in 4?5?4= 80 perplex-
ity features for each text tested.
3.3 Parsed Syntactic Features
Schwarm and Ostendorf (2005) studied four parse
tree features (average parse tree height, average
number of SBARs, noun phrases, and verb phrases
per sentences). We implemented these and addi-
tional features, using the Charniak parser (Char-
niak, 2000). Our parsed syntactic features focus on
clauses (SBAR), noun phrases (NP), verb phrases
(VP) and prepositional phrases (PP). For each
phrase, we implement four features: total num-
ber of the phrases per document, average number
of phrases per sentence, and average phrase length
5http://www.speech.sri.com/projects/srilm/
279
measured by number of words and characters re-
spectively. In addition to average tree height, we
implement two non-terminal-node-based features:
average number of non-terminal nodes per parse
tree, and average number of non-terminal nodes
per word (terminal node).
3.4 POS-based Features
Part-of-speech-based grammatical features were
shown to be useful in readability prediction (Heil-
man et al, 2007; Leroy et al, 2008). To extend
prior work, we systematically studied a number of
common categories of words and investigated to
what extent they are related to a text?s complex-
ity. We focus primarily on five classes of words
(nouns, verbs, adjectives, adverbs, and preposi-
tions) and two broad categories (content words,
function words). Content words include nouns,
verbs, numerals, adjectives, and adverbs; the re-
maining types are function words. The part of
speech of each word is obtained from examining
the leaf node based on the output of Charniak?s
parser, where each leaf node consists of a word and
its part of speech. We group words based on their
POS labels. For each class of words, we imple-
ment five features. For example, for the adjective
class, we implemented the following five features:
percent of adjectives (tokens) per document, per-
cent of unique adjectives (types) per document,
ratio of unique adjectives per total unique words
in a document, average number of adjectives per
sentence and average number of unique adjectives
per sentence.
3.5 Shallow Features
Shallow features refer to those used by traditional
readability metrics, such as Flesch-Kincaid Grade
Level (Flesch, 1979), SMOG (McLaughlin, 1969),
Gunning FOG (Gunning, 1952), etc. Although
recent readability studies have strived to take ad-
vantage of NLP techniques, little has been revealed
about the predictive power of shallow features.
Shallow features, which are limited to superficial
text properties, are computationally much less ex-
pensive than syntactic or discourse features. To en-
able a comparison against more advanced features,
we implement 8 frequently used shallow features
as listed in Table 5.
Table 5: Shallow Features
1 average number of syllables per word
2 percentage of poly-syll. words per doc.
3 average number of poly-syll. words per sent.
4 average number of characters per word
5 Chall-Dale difficult words rate per doc.
6 average number of words per sentence
7 Flesch-Kincaid score
8 total number of words per document
3.6 Other Features
For comparison, we replicated 6 out-of-vocabulary
features described in Schwarm and Ostendorf
(2005). For each text in the Weekly Reader corpus,
these 6 features are computed using the most com-
mon 100, 200 and 500 word tokens and types based
on texts from Grade 2. We also replicated the 12
perplexity features implemented by Schwarm and
Ostendorf (2005) (see Section 3.2).
4 Experiments and Discussion
Previous studies on reading difficulty explored vari-
ous statistical models, e.g. regression vs. classifica-
tion, with varying assumptions about the measure-
ment of reading difficulty, e.g. whether labels are
ordered or unrelated, to test the predictive power
of models (Heilman et al, 2008; Petersen and Os-
tendorf, 2009; Aluisio et al, 2010). In our re-
search, we have used various models, including
linear regression; standard classification (Logis-
tic Regression and SVM), which assumes no rela-
tion between grade levels; and ordinal regression/
classification (provided by Weka, with Logistic
Regression and SMO as base function), which as-
sumes that the grade levels are ordered. Our exper-
iments show that, measured by mean squared error
and classification accuracy, linear regression mod-
els perform considerably poorer than classification
models. Measured by accuracy and F-measure,
ordinal classifiers perform comparable or worse
than standard classifiers. In this paper, we present
the best results, which are obtained by standard
classifiers. We use two machine learning packages
known for efficient high-quality multi-class classi-
fication: LIBSVM (Chang and Lin, 2001) and the
Weka machine learning toolkit (Hall et al, 2009),
from which we choose Logistic Regression as clas-
sifiers. We train and evaluate various prediction
280
Table 6: Comparison of discourse features
Feature Set LIBSVM Logistic Regress.
Entity-Density 59.63?0.632 57.59?0.375
Lexical Chain 45.86?0.815 42.58?0.241
Coref. Infer. 40.93?0.839 42.19?0.238
Entity Grid 45.92?1.155 42.14?0.457
all combined 60.50?0.990 58.79?0.703
models using the features described in Section 3.
We evaluate classification accuracy using repeated
10-fold cross-validation on the Weekly Reader cor-
pus. Classification accuracy is defined as the per-
centage of texts predicted with correct grade levels.
We repeat each experiment 10 times and report the
mean accuracy and its standard deviation.
4.1 Discourse Features
We first discuss the improvement made by extend-
ing our earlier entity-density features (Feng et al,
2009). We used LIBSVM to train and test mod-
els on the Weekly Reader corpus with our earlier
features and our new features respectively. With
earlier features only, the model achieves 53.66%
accuracy. With our new features added, the model
performance is 59.63%.
Table 6 presents the classification accuracy of
models trained with discourse features. We see
that, among four subsets of discourse features,
entity-density features perform significantly better
than the other three feature sets and generate the
highest classification accuracy (LIBSVM: 59.63%,
Logistic Regression: 57.59%). While Logistic Re-
gression results show that there is not much perfor-
mance difference among lexical chain, coreference
inference, and entity grid features, classification
accuracy of LIBSVM models indicates that lexical
chain features and entity grid features are better
in predicting text readability than coreference in-
ference features. Combining all discourse features
together does not significantly improve accuracy
compared with models trained only with entity-
density features.
4.2 Language Modeling Features
Table 7 compares the performance of models gen-
erated using our approach and our replication of
Schwarm and Ostendorf?s (2005) approach. In our
approach, features were obtained from language
Table 7: Comparison of lang. modeling features
Feature Set LIBSVM Logistic Regress.
IG 62.52?1.202 62.14?0.510
Text-only 60.17?1.206 60.31?0.559
POS-only 56.21?2.354 57.64?0.391
Word/POS pair 60.38?0.820 59.00?0.367
all combined 68.38?0.929 66.82?0.448
IG by Schwarm 52.21?0.832 51.89?0.405
Table 8: Comparison of parsed syntactic features
Feature Set # Feat. LIBSVM
Original features 4 50.68?0.812
Expanded features 21 57.79?1.023
models trained on the Weekly Reader corpus. Not
surprisingly, these are more effective than LMs
trained on the Britannica and LiteracyNet corpora,
in Schwarm and Ostendorf?s approach. Our results
support their claim that LMs trained with infor-
mation gain outperform LMs trained with POS la-
bels. However, we also notice that training LMs on
word labels alone or paired word/POS sequences
achieved similar classification accuracy to the IG
approach, while avoiding the complicated feature
selection of the IG approach.
4.3 Parsed Syntactic Features
Table 8 compares a classifier trained on the four
parse features of Schwarm and Ostendorf (2005) to
a classifier trained on our expanded set of parse fea-
tures. The LIBSVM classifier with the expanded
feature set scored 7 points higher than the one
trained on only the original four features, improv-
ing from 50.68% to 57.79%. Table 9 shows a
detailed comparison of particular parsed syntactic
features. The two non-terminal-node-based fea-
tures (average number of non-terminal nodes per
tree and average number of non-terminal nodes
per word) have higher discriminative power than
average tree height. Among SBARs, NPs, VPs and
PPs, our experiments show that VPs and NPs are
the best predictors.
4.4 POS-based Features
The classification accuracy generated by models
trained with various POS features is presented
in Table 10. We find that, among the five word
classes investigated, noun-based features gener-
281
Table 9: Detailed comp. of syntactic features
Feature Set LIBSVM Logistic Regress.
Non-term.-node ratios 53.02?0.571 51.80?0.171
Average tree height 44.26?0.914 43.45?0.269
SBARs 44.42?1.074 43.50?0.386
NPs 51.56?1.054 48.14?0.408
VPs 53.07?0.597 48.67?0.484
PPs 49.36?1.277 46.47?0.374
all combined 57.79?1.023 54.11?0.473
Table 10: Comparison of POS features
Feature Set LIBSVM Logistic Regress.
Nouns 58.15?0.862 57.01?0.256
Verbs 54.40?1.029 55.10?0.291
Adjectives 53.87?1.128 52.75?0.427
Adverbs 52.66?0.970 50.54?0.327
Prepositions 56.77?1.278 54.13?0.312
Content words 56.84?1.072 56.18?0.213
Function words 52.19?1.494 50.95?0.298
all combined 59.82?1.235 57.86?0.547
ate the highest classification accuracy, which is
consistent with what we have observed earlier
about entity-density features. Another notable ob-
servation is that prepositions demonstrate higher
discriminative power than adjectives and adverbs.
Models trained with preposition-based features per-
form close to those trained with noun-based fea-
tures. Among the two broader categories, content
words (which include nouns) demonstrate higher
predictive power than function words (which in-
clude prepositions).
4.5 Shallow Features
We present some notable findings on shallow fea-
tures in Table 11. Experimental results generated
by models trained with Logistic Regression show
that average sentence length has dominating predic-
tive power over all other shallow features. Features
based on syllable counting perform much worse.
The Flesch-Kincaid Grade Level score uses a fixed
linear combination of average words per sentence
and average syllables per word. Combining those
two features (without fixed coefficients) results in
the best overall accuracy, while using the Flesch-
Kincaid score as a single feature is significantly
worse.
Table 11: Comparison of shallow features
Feature Set Logistic Regress.
Avg. words per sent. 52.17?0.193
Avg. syll. per word 42.51?0.264
above two combined 53.04?0.514
Flesch-Kincaid score 50.83?0.144
Avg. poly-syll. words per sent. 45.70?0.306
all 8 features combined 52.34?0.242
4.6 Comparison with Previous Studies
A trivial baseline of predicting the most frequent
grade level (grade 5) predicts 542 out of 1433 texts
(or 37.8%) correctly. With this in mind, we first
compare our study with the widely-used Flesch-
Kincaid Grade Level formula, which is a linear
function of average words per sentence and average
syllables per word that aims to predict the grade
level of a text directly. Since this is a fixed formula
with known coefficients, we evaluated it directly
on our entire Weekly Reader corpus without cross-
validation. We obtain the predicted grade level
of a text by rounding the Flesch-Kincaid score
to the nearest integer. For only 20 out of 1433
texts the predicted and labeled grade levels agree,
resulting in a poor accuracy of 1.4%. By contrast,
using the Flesch-Kincaid score as a feature of a
simple logistic regression model achieves above
50% accuracy, as discussed in Section 4.5.
The most closely related previous study is the
work of Schwarm and Ostendorf (2005). How-
ever, because their experiment design (85/15 train-
ing/test data split) and machine learning tool
(SV Mlight) differ from ours, their results are not
directly comparable to ours. To make a compar-
ison, we replicated all the features used in their
study and then use LIBSVM and Weka?s Logistic
Regression to train two models with the replicated
features and evaluate them on our Weekly Reader
corpus using 10-fold cross-validation.
Using the same experiment design, we train clas-
sifiers with three combinations of our features as
listed in Table 12. ?All features? refers to a naive
combination of all features. ?AddOneBest? refers
to a subset of features selected by a group-wise
add-one-best greedy feature selection. ?WekaFS?
refers to a subset of features chosen by Weka?s
feature selection filter.
?WekaFS? consists of 28 features selected au-
282
Table 12: Comparison with previous work
baseline accuracy (majority class) 37.8
Flesch-Kincaid Grade Level 1.4
Feature Set # Feat. LIBSVM Logistic Reg.
Schwarm 25 63.18?1.664 60.50?0.477
All features 273 72.21?0.821 63.71?0.576
AddOneBest 122 74.01?0.847 69.22?0.411
WekaFS 28 70.06?0.777 65.46?0.336
tomatically by Weka?s feature selection filter us-
ing a best-first search method. The 28 features
include language modeling features, syntactic fea-
tures, POS features, shallow features and out-of-
vocabulary features. Aside from 4 shallow features
and 5 out-of-vocabulary features, the other 19 fea-
tures are novel features we have implemented for
this paper.
As Table 12 shows, a naive combination of all
features results in classification accuracy of 72%,
which is much higher than the current state of the
art (63%). This is not very surprising, since we are
considering a greater variety of features than any
previous individual study. Our WekaFS classifier
uses roughly the same number of features as the
best published result, yet it has a higher accuracy
(70.06%). Our best results were obtained by group-
wise add-one-best feature selection, resulting in
74% classification accuracy, a big improvement
over the state of the art.
5 Conclusions
We examined the usefulness of features at various
linguistic levels for predicting text readability in
terms of assigning texts to elementary school grade
levels. We implemented a set of discourse features,
enriched previous work by creating several new
features, and systematically tested and analyzed
the impact of these features.
We observed that POS features, in particular
nouns, have significant predictive power. The high
discriminative power of nouns in turn explains the
good performance of entity-density features, based
primarily on nouns. In general, our selected POS
features appear to be more correlated to text com-
plexity than syntactic features, shallow features
and most discourse features.
For parsed syntactic features, we found that verb
phrases appear to be more closely correlated with
text complexity than other types of phrases. While
SBARs are commonly perceived as good predic-
tors for syntactic complexity, they did not prove
very useful for predicting grade levels of texts in
this study. In future work, we plan to examine this
result in more detail.
Among the 8 shallow features, which are used
in various traditional readability formulas, we iden-
tified that average sentence length has dominating
predictive power over all other lexical or syllable-
based features.
Not surprisingly, among language modeling
features, combined features obtained from LMs
trained directly on the Weekly Reader corpus show
high discriminative power, compared with features
from LMs trained on unrelated corpora.
Discourse features do not seem to be very use-
ful in building an accurate readability metric. The
reason could lie in the fact that the texts in the cor-
pus we studied exhibit relatively low complexity,
since they are aimed at primary-school students. In
future work, we plan to investigate whether these
discourse features exhibit different discriminative
power for texts at higher grade levels.
A judicious combination of features examined
here results in a significant improvement over the
state of the art.
References
Sandra Aluisio, Lucia Specia, Caroline Gasperin,
and Carolina Scarton. 2010. Readability assess-
ment for text simplification. In NAACL-HLT
2010: The 5th Workshop on Innovative Use of
NLP for Building Educational Applications.
Regina Barzilay and Mirella Lapata. 2008. Model-
ing local coherence: An entity-based approach.
Computational Linguistics, 34(1):1?34.
Chih-Chung Chang and Chih-Jen Lin. 2001. LIB-
SVM: A Library for Support Vector Machines.
Software available at http://www.csie.ntu.
edu.tw/~cjlin/libsvm.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st Con-
ference of the North American Chapter of the
ACL, pages 132?139.
283
Kevyn Collins-Thompson and Jamie Callan. 2004.
A language modeling approach to predicting
reading difficulty. In Proceedings of the Hu-
man Language Technology Conference of the
North American Chapter of the Association for
Computational Linguistics (HLT-NAACL 2004).
Micha Elsner, Joseph Austerweil, and Eugene
Charniak. 2007. A unified local and global
model for discourse coherence. In Proceed-
ings of the Conference on Human Language
Technology and North American chapter of the
Association for Computational Linguistics (HLT-
NAACL 2007).
Lijun Feng, Noe?mie Elhadad, and Matt Huener-
fauth. 2009. Cognitively motivated features for
readability assessment. In The 12th Conference
of the European Chapter of the Association for
Computational Linguistics (EACL 2009).
Rudolf Flesch. 1979. How to write plain English.
Harper and Brothers, New York.
Michel Galley and Kathleen McKeown. 2003. Im-
proving word sense disambiguation in lexical
chaining. In Proceedings of the 18th Inter-
national Joint Conference on Artificial Intelli-
gence.
Robert Gunning. 1952. The Technique of Clear
Writing. McGraw-Hill.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bern-
hard Pfahringer, Peter Reutemann, and Ian H.
Witten. 2009. The WEKA data mining software:
An update. SIGKDD Explorations, 11(1):10?18.
Michael J. Heilman, Kevyn Collins-Thompson,
Jamie Callan, and Maxine Eskenazi. 2007. Com-
bining lexical and grammatical features to im-
prove readability measures for first and second
language texts. In Human Language Technolo-
gies 2007: The Conference of the North Amer-
ican Chapter of the Association for Computa-
tional Linguistics.
Michael J. Heilman, Kevyn Collins-Thompson,
and Maxine Eskenazi. 2008. An analysis of sta-
tistical models and features for reading difficulty
prediction. In ACL 2008: The 3rd Workshop on
Innovative Use of NLP for Building Educational
Applications.
Mirella Lapata and Regina Barzilay. 2005. Auto-
matic evaluation of text coherence: Models and
representations. In Proceedings of the Interna-
tional Joint Conference on Artificial Intelligence
(IJCAI?05), pages 1085?1090.
Gondy Leroy, Stephen Helmreich, James R. Cowie,
Trudi Miller, and Wei Zheng. 2008. Evaluating
online health information: Beyond readability
formulas. In AMIA 2008 Symposium Proceed-
ings.
G. Harry McLaughlin. 1969. Smog grading a
new readability formula. Journal of Reading,
12(8):639?646.
Sarah E. Petersen and Mari Ostendorf. 2006. A
machine learning approach to reading level as-
sessment. Technical report, University of Wash-
ington CSE Technical Report.
Sarah E. Petersen and Mari Ostendorf. 2009. A ma-
chine learning approach to reading level assess-
ment. Computer Speech and Language, 23:89?
106.
Emily Pitler and Ani Nenkova. 2008. Revisiting
readability: A unified framework for predict-
ing text quality. In Proceedings of the 2008
Conference on Empirical Methods in Natural
Language Processing.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki,
L. Robaldo, A. Joshi, and B. Webber. 2008. The
Penn discourse treebank. In The Sixth Interna-
tional Conference on Language Resources and
Evaluation (LREC?08).
Sarah E. Schwarm and Mari Ostendorf. 2005.
Reading level assessment using support vector
machines and statistical language models. In
Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics.
Luo Si and Jamie Callan. 2001. A statistical model
for scientific readability. In Proceedings of the
Tenth International Conference on Information
and Knowledge Management.
Radu Soricut and Daniel Marcu. 2006. Discourse
generation using utility-trained coherence mod-
els. In Proceedings of the 21st International
Conference on Computational Linguistics and
44th Annual Meeting of the Association for Com-
putational Linguistics.
284
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 804?812,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
An Unsupervised Aspect-Sentiment Model for Online Reviews
Samuel Brody
Dept. of Biomedical Informatics
Columbia University
samuel.brody@dbmi.columbia.edu
Noemie Elhadad
Dept. of Biomedical Informatics
Columbia University
noemie@dbmi.columbia.edu
Abstract
With the increase in popularity of online re-
view sites comes a corresponding need for
tools capable of extracting the information
most important to the user from the plain text
data. Due to the diversity in products and ser-
vices being reviewed, supervised methods are
often not practical. We present an unsuper-
vised system for extracting aspects and deter-
mining sentiment in review text. The method
is simple and flexible with regard to domain
and language, and takes into account the in-
fluence of aspect on sentiment polarity, an is-
sue largely ignored in previous literature. We
demonstrate its effectiveness on both compo-
nent tasks, where it achieves similar results to
more complex semi-supervised methods that
are restricted by their reliance on manual an-
notation and extensive knowledge sources.
1 Introduction
Online review sites continue to grow in popularity as
more people seek the advice of fellow users regard-
ing services and products. Unfortunately, users are
often forced to wade through large quantities of writ-
ten data in order to find the information they want.
This has led to an increase in research in the areas
of opinion mining and sentiment analysis, with the
aim of providing systems that can automatically an-
alyze user reviews and extract the information most
relevant to the user.
One example of such an application is generat-
ing a summary of the important factors mentioned
in the reviews of a product (see Lerman et al 2009).
Another application is comparing two similar prod-
ucts. In this case, it is important to present to the
user the aspects in which the products differ, rather
than just provide a general star rating. A third exam-
ple is systems for generating automatic recommen-
dations, based on similarity between products, user
reviews, and history of previous purchases. These
types of application require an underlying frame-
work to identify the important aspects of the prod-
uct (also known as features or attributes), and the
sentiment expressed by the review writer.
Unsupervised Methods are desirable for this task,
for two reasons. First, due to the wide range and va-
riety of products and services being reviewed, the
framework must be robust and easily transferable
between domains. The second reason is the nature of
the data. Online reviews are often short and unstruc-
tured, and may contain many spelling and gram-
matical errors, as well as slang or specialized jar-
gon. These factors often present a problem to meth-
ods relying exclusively on dictionaries, manually-
constructed knowledge resources, and gazetteers, as
they may miss out on an important aspect of the
product or an indicator of sentiment. Unsupervised
methods, on the other hand, are not influenced by
the lexical form, and can handle unknown words or
word-forms, provided they occur frequently enough.
This insures that any emergent topic that is salient in
the data will be addressed by the system.
In this paper, we present an unsupervised system
which addresses the core tasks necessary to enable
advanced applications to handle review data. We in-
troduce a local topic model, which works at the sen-
tence level and employs a small number of topics, to
automatically infer the aspects. For sentiment detec-
tion, we present a method for automatically deriving
an unsupervised seed set of positive and negative ad-
jectives that replaces the manually constructed ones
commonly used in the literature. Our approach is
specifically designed to take into account the inter-
804
action between the two tasks.
The rest of the paper is structured as follows. In
Sec. 2 we provide relevant background, and place
our method in the context of previous work in the
field. We describe the data we used in Sec. 3, and our
experiments on the aspect and sentiment-polarity
components in Sec. 4 and 5, respectively. We con-
clude in Sec. 6 with a discussion of our results and
findings and directions for future research.
2 Previous Approaches
In this paper, we focus on the detection of two prin-
ciple elements in the review text: aspects and sen-
timent. In previous work these elements have been
treated, for the most part, as two separate tasks.
Aspect The earliest attempts at aspect detection
were based on the classic information extraction (IE)
approach of using frequently occurring noun phrases
(e.g., Hu and Liu 2004). Such approaches work well
in detecting aspects that are strongly associated with
a single noun, but are less useful when aspects en-
compass many low frequency terms (e.g., the food
aspect of restaurants, which involves many differ-
ent dishes), or are abstract (e.g. ambiance can be
described without using any concrete nouns at all).
Common solutions to this problem involve cluster-
ing with the help of knowledge-rich methods, in-
volving manually-constructed rules, semantic hier-
archies, or both (e.g., Popescu and Etzioni 2005,
Fahrni and Klenner 2008). Titov and McDonald
(2008b) underline the need for unsupervised meth-
ods for aspect detection. However, according to the
authors, existing topic models, such as standard La-
tent Dirichlet Allocation (LDA) (Blei et al, 2003),
are not suited to the task of aspect detection in re-
views, because they tend to capture global topics
in the data, rather than rateable aspects pertinent
to the review. To address this problem, they con-
struct a multi-grain topic model (MG-LDA), which
attempts to capture two layers of topics - global and
local, where the local topics correspond to rateable
aspects. MG-LDA distinguishes tens of local top-
ics, but the many-to-one mapping between these and
rateable aspects is not explicit in the system. To re-
solve this issue, the authors extend their model in
Titov and McDonald (2008a) and attempt to infer
such a mapping with the help of aspect-specific rat-
ings provided along with the review text.
Sentiment Sentiment analysis has been the fo-
cus of much previous research. In this discussion,
we will only mention work directly related to our
own. For a comprehensive survey of the subject, the
reader is directed to Pang and Lee (2008).
Most previous approaches rely on a manually
constructed lexicon of terms which are strongly pos-
itive or negative regardless of context. This informa-
tion on its own is usually insufficient, due to lack
of coverage and the fact that sentiment is often ex-
pressed through words whose polarity is highly do-
main and context specific. If a sentiment lexicon is
available for one domain, domain adaptation can be
used, provided the domains are sufficiently similar
(Blitzer et al, 2007). Another common solution is
through bootstrapping - using a seed group of terms
with known polarity to infer the polarity of domain
specific terms (e.g., Fahrni and Klenner 2008; Jijk-
oun and Hofmann 2009). The most minimalist ex-
ample of this approach is Turney (2002), who used
only a single pair of adjectives (good and poor) to
determine the polarity of other terms through mu-
tual information. For Chinese, Zagibalov and Carroll
(2008) use a single seed word meaning good, and six
common indicators of negation in their bootstrap-
ping approach. Often, when using a context indepen-
dent seed, large amounts of domain-specific data are
required, in order to obtain sufficient co-occurrence
statistics. Commonly, web queries are used to obtain
such data.
Independently of any specific task, Hatzivas-
siloglou and McKeown (1997) present a completely
unsupervised method for determining the polarity of
adjectives in a large corpus. A graph is created, in
which adjectives are nodes, and edges between them
are weighted according to a (dis)similarity function
based primarily on whether the two adjectives oc-
curred in a conjunction or disjunction in the corpus.
A heuristic approach is then used to split the graph
in two. The group containing the adjectives with the
higher average frequency is labeled as positive, and
the other as negative.
Combined Approaches Aspects can influence
sentiment polarity within a single domain. For ex-
ample, in the restaurant domain, cheap is usually
positive when discussing food, but negative when
discussing the decor or ambiance. Many otherwise
neutral terms (e.g., warm, heavy, soft) acquire a sen-
timent polarity in the context of a specific aspect.
805
Recent work has addressed this interaction in differ-
ent ways. Mei et al (2007) present a form of do-
main adaptation using an LDA model which treats
positive and negative sentiment as two additional
topics. Fahrni and Klenner (2008) directly address
the specificity of sentiment to the word it is modi-
fying. Aspects are defined by a manually specified
subset of the Wikipedia category hierarchy. For sen-
timent, the authors use a seed set of positive and
negative adjectives, and iteratively propagate sen-
timent polarity through conjunction relations (like
those used by Hatzivassiloglou and McKeown 1997,
above). Web queries are used to overcome the spar-
sity issue of these highly-specific patterns. In the IE
setting, Popescu and Etzioni (2005) extract frequent
terms, and cluster them into aspects. The sentiment
detection task is formulated as a Relaxation Label-
ing problem of finding the most likely sentiment la-
bels for opinion-bearing terms, while satisfying as
many local constraints as possible. The authors use
a variety of knowledge sources, web queries, and
hand crafted rules to detect relations between terms
(e.g., meronymy). These relations are used both for
the clustering, and as a basis for the constraints.
Our approach is designed to be as unsupervised
and knowledge-lean as possible, so as to make it
transferable across different types of products and
services, as well as across languages. Aspects are
determined via a local version of LDA, which oper-
ates on sentences, rather than documents, and em-
ploys a small number of topics that correspond di-
rectly to aspects. This approach overcomes the prob-
lems of frequent-term methods, as well as the issues
raised by Titov andMcDonald (2008b). We use mor-
phological negation indicators to automatically cre-
ate a seed set of highly relevant positive and nega-
tive adjectives, which are guaranteed to be pertinent
to the aspect at hand. These automatically-derived
seed sets achieve comparable results to the use of
manual ones, and the work of Zagibalov and Car-
roll (2008) suggests that the use of negation can be
easily transfered to other languages.
3 Data
Our primary dataset is the publicly available corpus
used in Ganu et al (2009). It contains over 50,000
restaurant reviews from Citysearch New York1. Ad-
1http://newyork.citysearch.com/
ditionally, to demonstrate the domain independence
of our system, we collected 1086 reviews for four
leading netbook computers from Amazon.com.
For evaluation purposes, we used the annotated
dataset from Ganu et al (2009), which is a sub-
set of 3,400 sentences from the Citysearch corpus.
These sentences were manually labeled for aspect
and sentiment. There were six manually defined as-
pect labels - Food & Drink, Service, Price, Atmo-
sphere, Anecdotes and Miscellaneous. A sentence
could contain multiple aspects, but, for our evalua-
tion, we used only sentences with a single label. For
sentiment, each sentence was given a single value -
Positive, Negative, Neutral or Conflict (indicating a
mixture of positive and negative sentiment).
We were also provided with a seed set of 128 pos-
itive and 88 negative adjectives used by Fahrni and
Klenner (2008), which were specifically selected to
be domain and target independent.
For the purpose of the experiments presented
here, we focused on sentences containing noun-
adjective pairs. Such pairs are one of the most com-
mon way of expressing sentiment about an aspect
and allow us to capture the interaction between the
two.
4 Aspect
4.1 Methodology
In order to infer the salient aspects in the data, we
employed the following steps:
Local LDA We used a standard implementation2
of LDA. In order to prevent the inference of global
topics and direct the model towards rateable aspects
(see Sec. 2), we treated each sentence as a separate
document. The output of the model is a distribution
over inferred aspects for each sentence in the data.
The parameters we employed were standard, out-of-
the-box settings (? = 0.1,? = 0.1, 3000 iterations),
with no specific tuning to our data. We ran the algo-
rithm with the number of aspects ranging from 10 to
20, and employed a cluster validation scheme (see
below) to determine the optimal number.
Model Order The issue of model order, i.e., deter-
mining the correct number of clusters, is an impor-
tant element in unsupervised learning. A common
2GibbsLDA++, by Xuan-Hieu Phan. Available at http://
gibbslda.sourceforge.net/.
806
approach (Levine and Domany, 2001; Lange et al,
2004; Niu et al, 2007) is to use a cluster validation
procedure. In such a procedure, different model or-
ders are compared, and the one with the most con-
sistent clustering is chosen. For the purpose of the
validation procedure, we have a cluster correspond-
ing to each aspect, and we label each sentence as
belonging to the cluster of the most probable aspect.
Given the collection of sentences in our data, D,
and two connectivity matricesC and C?, where a cell
i, j contains 1 if sentences di and d j belong to the
same cluster, we define a consistency function F
(following Niu et al 2007):
F(C,C?) =
?i, j 1{Ci, j = C?i, j = 1,di,d j ? D?}
?i, j 1{Ci, j = 1,di,d j ? D?}
(1)
We then employ the following procedure:
1. Run the LDA model with k topics on D to ob-
tain connectivity matrixCk.
2. Create a comparison connectivity matrix Rk
based on uniformly drawn random assignments
of the instances.
3. Sample random subset Di of size ?|D| from D.
4. Run the LDA model on Di to obtain connectiv-
ity matrixCik.
5. Create a comparison matrix Rik based on uni-
formly drawn random assignments of the in-
stances in Di.
6. Calculate scorei(k) = F(C?,C)?F(R?,R) where
F is given in Eq. 1.
7. Repeat steps 3 to 6 q times.
8. Return the average score over q iterations.
This procedure calculates the consistency of our
clustering solution, using a similar sized random as-
signment for comparison. It does this on q subsets to
reduce the effects of chance. The k with the high-
est score is chosen. In our experiments, we used
q= 5,?= 0.9. For both our datasets (restaurants and
netbooks), the highest-scoring k was 14.
Determining Representative Words For each as-
pect, we list all the nouns in the data according to a
score based on their mutual information with regard
to that aspect.
Scorea(w) = p(w,a) ? log
p(w,a)
p(w) ? p(a)
(2)
Where p(w), p(a), p(w,a) are the probabilities, ac-
cording to the LDA model, of the word w, the aspect
a, and the wordw labeled with aspect a, respectively.
We then select, for each aspect, the top ka rank-
ing words, such that they cover 75% of the word-
instances labeled by the LDA model with aspect la-
bel a. Due to the skewed frequency distribution of
words, this is a relatively small portion of the words
(typically 100-200). This set of representative words
for each aspect is used in the sentiment component
of our system (see Sec. 5.1).
4.2 Inferred Aspects
Table 1 presents the aspects inferred by our system
for the restaurant domain. The inferred aspects cover
all those defined in the manual annotation, but also
distinguish between a finer granularity of aspects,
based solely on the review text, e.g., between phys-
ical environment and ambiance, and between the at-
titude of the staff and the quality of the service.
In order to demonstrate that our method can be
transfered between very different domains and cat-
egories of products, we also ran our algorithm on
our set of netbook reviews. The inferred aspects
are presented in Table 2. The system identifies im-
portant aspects relevant to our data. Some of these
(e.g., software, hardware) might be suggested by hu-
man annotators, but some would probably be missed
unless the annotators carefully read through all the
reviews, e.g., theMemory aspect, which includes ad-
vice about upgrading specific models. This capabil-
ity of our system is important, as it demonstrates that
our method can be used to produce customized com-
parisons for the user and will take into account the
important common factors, as well as the unique as-
pects of each item.
4.3 Evaluation
To determine the quality of our automatically in-
ferred aspects, we compared the output of our sys-
tem to the sentence-level manual annotation of Ganu
et al (2009). To each sentence in the data, the LDA
model assigns a distribution {P(a)}a?A over the set
A of inferred aspects. By defining a threshold ta for
each aspect, we can label a sentence as belonging
to aspect a if P(a) > ta. By varying the threshold ta
we created precision-recall curves for the top three
rateable aspects in the restaurant domain, shown in
807
Inferred Aspect Representative Words Manual Aspect
Main Dishes chicken, sauce, rice, cheese, spicy, salad,
Food & Drink
Bakery hot, delicious, dessert, bagels, bread, chocolate
Food - General menu, fresh, sushi, fish, chef, cuisine
Wine & Drinks wine, list, glass, drinks, beer, bottle
Ambiance / Mood great, atmosphere, wonderful, music, experience, relaxed
Atmosphere
Physical Atmosphere bar, room, outside, seating, tables, cozy, loud
Staff service, staff, friendly, attentive, busy, slow
Staff
Service table, order, wait, minutes, reservation, forgot
Value portions, quality, worth, size, cheap Price
Anecdotes dinner, night, group, friends, date, family
Anecdotes
Anecdotes out, back, definitely, around, walk, block
General best, top, favorite, city, NYC
Misc.Misc. - Location never, restaurant, found, Paris, (New) York, location
Misc. place, eat, enjoy, big, often, stuff
Table 1: List of automatically inferred aspects for the restaurant domain, with some representative words for each
aspect (middle), and the corresponding aspect label from the manual annotation (right). Labels (left) were assigned by
the authors.
Aspect Representative Words
Performance power, performance, mode, fan, quiet
Hardware drive, wireless, bluetooth, usb, speakers, webcam
Memory ram, 2GB, upgrade, extra, 1GB, speed
Software using, office, software, installed, works, programs
Usability internet, video, web, movies, music, email, play
Portability around, light, work, portable, weight, travel
Comparison netbooks, best, reviews, read, decided, research
Aspect Representative Words
Mouse mouse, right, touchpad, pad, buttons, left
General great, little, machine, price, netbook, happy
Purchase amazon, purchased, bought, weeks, ordered
Looks looks, feel, white, finish, blue, solid, glossy
OS windows, xp, system, boot, linux, vista, os
Battery battery, life, hours, time, cell, last
Size screen, keyboard, size, small, enough, big
Table 2: List of automatically inferred aspects for the netbook dataset, with representative words for each aspect .
Figure 13. Although the data used in Titov and Mc-
Donald (2008a) was unavailable for direct compar-
ison, our method exhibits similar behavior and per-
formance (compare Fig. 4, there) on a domain with
similar characteristics (abstract aspects which en-
compass many low frequency words). This demon-
strates that our local version of LDA with few top-
ics overcomes the issues which confronted the au-
thors of that work (i.e., global topics and many-to-
one mapping of topics to aspects), without requiring
specially designed models or additional information
in the form of user-provided aspect-specific ratings
(see Sec. 2).
We believe the reason for this stems from the
composition of online reviews. Since many reviews
have similar mixtures of local topics (e.g., food, ser-
vice), standard LDA prefers global topics, which
3We combined the probabilities of all the inferred aspects
that match a single manually assigned aspect, according to the
mapping in Table 1.
distinguish more strongly between reviews (e.g., cui-
sine type, restaurant type). However, when em-
ployed at the sentence level, local topics (corre-
sponding to rateable aspects) provide a stronger way
to distinguish between individual sentences.
5 Sentiment
5.1 Methodology
For determining sentiment polarity, we developed
the following procedure. For each aspect, we ex-
tracted the relevant adjectives, built a conjunction
graph, automatically determined the seed set (or
used a manual one, for comparison), and propagated
the polarity scores to the rest of the adjectives. De-
tails of each step are described below.
Extracting Adjectives As a pre-processing step,
we parsed our data (using RASP, Briscoe and Car-
roll 2002). The parsed output was used to detect
negation and conjunction. If an adjective A partic-
808
(a) (b) (c)
Figure 1: Precision / Recall curves for the top three rateable aspects: (a) Food, (b) Service, and (c) Atmosphere.
ipated in a negation in the sentence, it was replaced
by a new adjective not-A. We then extract all cases
where an adjective modified a noun. For example,
from the sentence ?The food was tasty and hot, but
our waiter was not friendly.? we can extract the pairs
(tasty, food), (hot, food), (not-friendly, waiter).
Building the Graph Our method for determin-
ing sentiment polarity is based on an adaptation of
Hatzivassiloglou and McKeown (1997) (see Sec. 2).
Several issues confronted us when attempting to
adapt their method to our task. In the original arti-
cle, adjectives with no orientation were ignored. It
is unclear how this can be easily done in an unsu-
pervised fashion, and such sentiment-neutral adjec-
tives are ubiquitous in real-world data. Furthermore,
adjectives whose orientation depended on the con-
text were also ignored. These are of particular in-
terest in our task, and are likely to be missing or
incorrectly labeled in standard sentiment dictionar-
ies. For our purposes, since we need to handle ad-
jectives expressing various shades of sentiment, not
only strongly positive or negative ones, we are inter-
ested in a scoring method, rather than a binary label-
ing. Also, we do not want to use a general corpus,
but rather the text from the reviews themselves. This
usually means a much smaller corpus than the one
used in the original paper, but has the advantage of
being domain specific.
Our method of building the polarity graph differed
in several ways from the original. First, we did not
use disjunctions (e.g., ?but?) as indicators of opposite
polarity. The reason for this was that, in our domain
of online reviews, disjunctions often did not convey
contrast in polarity, but rather in perceived expecta-
tions, e.g., ?dainty but strong necklace?, and ?cheap
but delicious food?.
Instead of using regular expressions to capture ex-
plicit conjunctions, we retrieved all cases where our
parser indicated that two adjectives modified a sin-
gle noun in the same sentence.
To ensure that aspect-specific adjectives are han-
dled correctly, we built a separate graph for each as-
pect, by selecting the cases where the modified noun
was one of the representative words for that aspect
(see Sec. 4.1).
Constructing a Seed Set We used morphologi-
cal information and explicit negation to find pairs of
opposite polarity. Specifically, adjective pairs which
were distinguished only by one of the prefixes ?un?,
?in?, ?dis?, ?non?, or by the negation marker ?not-?
were selected for the seed set. Starting with the most
frequent pair, we assigned a positive polarity to the
more frequent member of the pair.
Then, in order of decreasing frequency, we as-
signed polarity to the other seed pairs, based on the
shortest path either of the members had to a previ-
ously labeled adjective. That member received its
neighbor?s polarity, and the other member of the pair
received the opposite polarity. When all pairs were
labeled, we corrected for misclassifications by iter-
ating through the pairs and reversing the polarity if
that improved consistency, i.e., if it caused the mem-
bers of the pair to match the polarities of more of
their neighbors. Finally, we reverse the polarity of
the seed groups if the negative group has a higher
total frequency.
Propagating Polarity Our propagation method is
based on the label propagation algorithm of Zhu
and Ghahramani (2002). The adjectives in the posi-
tive and negative seed groups are assigned a polarity
809
score of 1 and 0, respectively. All the rest start with
a score of 0.5. Then, an update step is repeated. In
update iteration t, for each adjective x that is not in
the seed, the following update rule is applied:
pt(x) =
?y?N(x)w(y,x) ? p
t?1(y)
?y?N(x)w(y,x)
(3)
Where pt(x) is the polarity of adjective x at step t,
N(x) is the set of the neighbors of x, andw(y,x) is the
weight of the edge connecting x and y. We set this
weight to be 1+ log(#mod(y,x)) where #mod(y,x)
is the number of times y and x both modified a single
noun. The update step is repeated to convergence.
5.2 Aspect-Specific Gold Standard
To evaluate the performance of the sentiment com-
ponent of our system, we created an aspect-specific
gold standard. For each of the top eight automati-
cally inferred aspects (corresponding to the Food,
Service and Atmosphere aspects in the annotation),
we constructed a polarity graph, as described in
Sec. 5.1. We retrieved a list of all adjectives that
participated in five or more modifications of nouns
from that specific aspect). Table 3 lists the number of
such adjectives in each aspect. We split the data into
ten portions and, for each portion, asked two volun-
teers to rate each adjective according to the polar-
ity of the sentiment it expresses in the context of the
specified aspect. The judges could select from the
following ratings: Strongly Negative, Weakly Nega-
tive, Neutral, Weakly Positive, Strongly Positive, and
N/A. As expected, exact inter-annotator agreement
was low - only 54%, but when considering two ad-
jacent ratings as equivalent (i.e, Strongly vs. Weakly
Negative or Positive, and Neutral vs. Weakly Neg-
ative or Positive), agreement was 93.3%. This indi-
cates there is some difficulty distinguishing between
the fine-grained categories we specified, but high
agreement at a coarser level, which advocates us-
ing a ranking approach for evaluation (see also Pang
and Lee 2005). We therefore translated the annota-
tor ratings to a numerical scale, from ?2 (Strongly
Negative) to +2 (Strongly Positive) at unit intervals.
After discarding adjectives where one or more anno-
tators gave a ?N/A? tag, we averaged the two annota-
tor numerical scores, and used this data as the gold
standard for our evaluation.
Aspect # Adj. # Rated % Neu.
Mood 293 206 17%
Staff 155 122 3%
Main Dishes 287 185 25%
Physical Atmo. 161 103 21%
Bakery 180 129 23%
Food - General 192 144 28%
Wine & Drinks 111 75 18%
Service 89 57 5%
Total 1468 1021 ?
Table 3: For each aspect, the number of frequently oc-
curring adjectives for each aspect (# Adj.), number of
adjectives remaining after removing those labeled ?N/A?
(# Rated), and percent of rated adjectives labeled ?Neu-
tral? by both annotators (% Neu.).
Auto. Manual
Aspect ?k Dk ?k Dk
Mood 0.53 0.23 0.56 0.22
Staff 0.57 0.22 0.60 0.20
Main Dishes 0.19 0.40 0.38 0.31
Physical Atmo. 0.34 0.33 0.25 0.37
Bakery 0.33 0.33 0.35 0.33
Food - General 0.19 0.41 0.41 0.30
Wine & Drinks 0.32 0.34 0.52 0.24
Service 0.41 0.30 0.54 0.23
Average 0.36 0.32 0.45 0.27
Table 4: Kendall coefficient and distance scores for eight
inferred aspects.
5.3 Evaluation Measures
Kendall?s tau coefficient (?k) and Kendall?s distance
(Dk) are commonly used (e.g., Jijkoun and Hofmann
2009) to compare rankings. These measures look at
the number of pairs of ranked items that agree or
disagree with the ordering in the gold standard. The
value of ?k ranges from -1 (perfect disagreement) to
1 (perfect agreement), with 0 indicating an almost
random ranking. The value ofDk ranges from 0 (per-
fect agreement) to 1 (perfect disagreement). It is im-
portant to note that only pairs that are ordered in the
gold standard are used in the comparison.
5.4 Evaluation Results
Table 4 reports Kendall?s coefficient (?k) and dis-
tance (Dk) values for our method when using our
automatically derived seed set (Auto.). For com-
parison, we ran our procedure using the manually
compiled seed set (Manual) of Fahrni and Klenner
810
Food - General: Mexican, French, Eastern, Turkish,
European, Tuscan, Mediterranean, American, Cuban,
Thai, Peruvian, Spanish, Korean, Vietnamese, Indian,
African, Japanese, Italian, Chinese, Asian
Mood: Vietnamese, Brazilian, Turkish, Eastern,
Caribbean, Cuban, Italian, Spanish, Japanese, Euro-
pean, Mediterranean, Colombian, Mexican, Asian,
Indian, Thai, British, American, French, Korean,
Chinese, Russian, Moroccan
Staff: British, European, Chinese, Indian, American,
Spanish, Asian, Italian, French
Table 5: Polarity ranking of cuisine adjectives (from most
positive) for three aspects.
(2008). Using the manual seed set obtains results
that correspond better to our gold standard. Our au-
tomatic method also achieves good results, and can
be used when a manual seed set is not available.
More importantly, correlation with the gold standard
may not indicate better suitability to the sentiment
detection task in reviews. For instance, it is interest-
ing to note that the worst correlation scores were on
the Main Dishes and Food - General aspects. If we
compare to Table 3, we can see these aspects have
the highest percentage of adjectives rated as neutral
by the annotators. However, in many cases, these ad-
jectives actually carry some sentiment in their con-
text. An example of this are adjectives describing
the type of cuisine, which are objective, and there-
fore usually considered neutral by annotators. Ta-
ble 5 shows the automatic ranking of cuisine type
from positive to negative in three aspects. It is inter-
esting to see that the rankings change according to
the aspect, and certain cuisines are strongly associ-
ated with specific aspects and not with others. This
is supported by Ganu et al (2009), who observed
during the annotation that, in the restaurant corpus,
French and Italian restaurants were strongly associ-
ated with the service aspect. This trend can be iden-
tified automatically by our method, and at a much
more detailed level than that noticed by a human an-
alyzing the data.
6 Discussion & Future Work
Our experiments confirm the value of a fully un-
supervised approach to the tasks of aspect detec-
tion and sentiment analysis. The aspects are inferred
from the data, and are more representative than
manually derived ones. For instance, in our restau-
rant domain, the manually constructed aspect list
omitted or over-generalized some important aspects,
while over-representing others. There was no sep-
arate Drinks category, even though it was strongly
present in the data. The Service aspect, dealing with
waiting time, reservations, and mistaken orders, was
an important emergent aspect on its own, but was
grouped under Staff in the manual annotation.
Adjectives can convey different sentiments de-
pending on the aspect being discussed. For exam-
ple, the adjective ?warm? was ranked very positive in
the Staff aspect, but slightly negative in the General
Food aspect. A knowledge-rich approach might ig-
nore such adjectives, thereby missing important ele-
ments of the review.
Finally, as online reviews belong to an informal
genre, with inventive spelling and specialized jar-
gon, it may be insufficient, for both aspect and
sentiment, to rely only on lexicons. For example,
our restaurant reviews included spelling errors such
as desert, decour/decore, anti-pasta, creme-brule,
sandwhich, omlette, exelent, tastey, as well as at
least six different common misspellings of restau-
rant. There were also specialized terms, such as Ko-
rma, Edamame, Dosa and Pho, all of which do not
appear in common dictionaries, and creative use of
adjectives, such as orgasmic and New-Yorky.
This work has opened many avenues for future re-
search and improvements. So far, we focused on ad-
jectives as sentiment indicators, however, there have
been studies showing that other parts of speech can
be very helpful for this task (e.g., Pang et al 2002;
Benamara et al 2007). Also, it would be interesting
to take a closer look at the interactions between as-
pect and sentiment, especially at a multiple-sentence
level (see Snyder and Barzilay 2007). Finally, we
feel that the true test of the usability of our system
should be through an application, and intend to pro-
ceed in that direction.
Acknowledgments
We?d like to thank Angela Fahrni and Manfred Klenner
for kindly allowing us access to their data and annotation.
We also wish to thank the volunteer annotators. This work
was partially supported by a Google Research Award.
References
Benamara, Farah, Carmine Cesarano, Antonio Picariello,
Diego Reforgiato, and V. S. Subrahmanian. 2007. Sen-
timent analysis: Adjectives and adverbs are better than
811
adjectives alone. In Proc. of the International Confer-
ence on Weblogs and Social Media (ICWSM).
Blei, David M., Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Machine
Learning Research 3:993?1022.
Blitzer, John, Mark Dredze, and Fernando Pereira. 2007.
Biographies, bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classification. In
Proc. of the 45th Annual Meeting of the Association of
Computational Linguistics. ACL, Prague, Czech Re-
public, pages 440?447.
Briscoe, Ted and John Carroll. 2002. Robust accurate
statistical annotation of general text. In Proc. of the 3rd
LREC. Las Palmas, Gran Canaria, pages 1499?1504.
Fahrni, Angela and Manfred Klenner. 2008. Old Wine
or Warm Beer: Target-Specific Sentiment Analysis of
Adjectives. In Proc.of the Symposium on Affective
Language in Human and Machine, AISB 2008 Con-
vention. pages 60 ? 63.
Ganu, Gayatree, Noemie Elhadad, and Amelie Marian.
2009. Beyond the stars: Improving rating predictions
using review text content. In WebDB.
Hatzivassiloglou, Vasileios and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proc. of the 35th Annual Meeting of the Asso-
ciation for Computational Linguistics. ACL, Madrid,
Spain, pages 174?181.
Hu, Minqing and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In KDD ?04: Proc. of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining. ACM, NewYork, NY,
USA, pages 168?177.
Jijkoun, Valentin and Katja Hofmann. 2009. Generating a
non-english subjectivity lexicon: Relations that matter.
In Proc. of the 12th Conference of the European Chap-
ter of the ACL (EACL 2009). ACL, Athens, Greece,
pages 398?405.
Lange, Tilman, Volker Roth, Mikio L. Braun, and
Joachim M. Buhmann. 2004. Stability-based val-
idation of clustering solutions. Neural Comput.
16(6):1299?1323.
Lerman, Kevin, Sasha Blair-Goldensohn, and Ryan Mc-
Donald. 2009. Sentiment summarization: evaluating
and learning user preferences. In EACL ?09: Proc. of
the 12th Conference of the European Chapter of the
Association for Computational Linguistics. ACL,Mor-
ristown, NJ, USA, pages 514?522.
Levine, Erel and Eytan Domany. 2001. Resampling
method for unsupervised estimation of cluster validity.
Neural Comput. 13(11):2573?2593.
Mei, Qiaozhu, Xu Ling, Matthew Wondra, Hang Su, and
ChengXiang Zhai. 2007. Topic sentiment mixture:
modeling facets and opinions in weblogs. In WWW
?07: Proc. of the 16th international conference on
World Wide Web. ACM, New York, NY, USA, pages
171?180.
Niu, Zheng-Yu, Dong-Hong Ji, and Chew-Lim Tan. 2007.
I2r: three systems for word sense discrimination, chi-
nese word sense disambiguation, and english word
sense disambiguation. In SemEval ?07: Proc. of the
4th International Workshop on Semantic Evaluations.
ACL, Morristown, NJ, USA, pages 177?182.
Pang, Bo and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. In Proc. of the ACL. pages
115?124.
Pang, Bo and Lillian Lee. 2008. Opinion mining and sen-
timent analysis. Foundations and Trends in Informa-
tion Retrieval 2(1-2):1?135.
Pang, Bo, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In EMNLP ?02: Proc. of the
conference on Empirical methods in natural language
processing. ACL, Morristown, NJ, USA, pages 79?86.
Popescu, Ana-Maria and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
HLT ?05: Proc. of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing. ACL, Morristown, NJ, USA, pages
339?346.
Snyder, Benjamin and Regina Barzilay. 2007. Multi-
ple aspect ranking using the good grief algorithm. In
Candace L. Sidner, Tanja Schultz, Matthew Stone, and
ChengXiang Zhai, editors, HLT-NAACL. The Associa-
tion for Computational Linguistics, pages 300?307.
Titov, Ivan and Ryan McDonald. 2008a. A joint model of
text and aspect ratings for sentiment summarization. In
Proc. of ACL-08: HLT . ACL, Columbus, Ohio, pages
308?316.
Titov, Ivan and RyanMcDonald. 2008b. Modeling online
reviews with multi-grain topic models. In WWW ?08:
Proc. of the 17th international conference on World
Wide Web. ACM, New York, NY, pages 111?120.
Turney, Peter. 2002. Thumbs up or thumbs down? se-
mantic orientation applied to unsupervised classifica-
tion of reviews. In Proc. of 40th Annual Meeting of
the Association for Computational Linguistics. ACL,
Philadelphia, Pennsylvania, USA, pages 417?424.
Zagibalov, Taras and John Carroll. 2008. Automatic seed
word selection for unsupervised sentiment classifica-
tion of chinese text. In COLING ?08: Proc. of the 22nd
International Conference on Computational Linguis-
tics. ACL, Morristown, NJ, USA, pages 1073?1080.
Zhu, X. and Z. Ghahramani. 2002. Learning from labeled
and unlabeled data with label propagation. Technical
report, CMU-CALD-02.
812
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 496?501,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Putting it Simply: a Context-Aware Approach to Lexical Simplification
Or Biran
Computer Science
Columbia University
New York, NY 10027
ob2008@columbia.edu
Samuel Brody
Communication & Information
Rutgers University
New Brunswick, NJ 08901
sdbrody@gmail.com
Noe?mie Elhadad
Biomedical Informatics
Columbia University
New York, NY 10032
noemie@dbmi.columbia.edu
Abstract
We present a method for lexical simplifica-
tion. Simplification rules are learned from a
comparable corpus, and the rules are applied
in a context-aware fashion to input sentences.
Our method is unsupervised. Furthermore, it
does not require any alignment or correspon-
dence among the complex and simple corpora.
We evaluate the simplification according to
three criteria: preservation of grammaticality,
preservation of meaning, and degree of sim-
plification. Results show that our method out-
performs an established simplification base-
line for both meaning preservation and sim-
plification, while maintaining a high level of
grammaticality.
1 Introduction
The task of simplification consists of editing an in-
put text into a version that is less complex linguisti-
cally or more readable. Automated sentence sim-
plification has been investigated mostly as a pre-
processing step with the goal of improving NLP
tasks, such as parsing (Chandrasekar et al, 1996;
Siddharthan, 2004; Jonnalagadda et al, 2009), se-
mantic role labeling (Vickrey and Koller, 2008) and
summarization (Blake et al, 2007). Automated sim-
plification can also be considered as a way to help
end users access relevant information, which would
be too complex to understand if left unedited. As
such, it was proposed as a tool for adults with
aphasia (Carroll et al, 1998; Devlin and Unthank,
2006), hearing-impaired people (Daelemans et al,
2004), readers with low-literacy skills (Williams and
Reiter, 2005), individuals with intellectual disabil-
ities (Huenerfauth et al, 2009), as well as health
INPUT: In 1900, Omaha was the center of a national
uproar over the kidnapping of Edward Cudahy, Jr., the
son of a local meatpacking magnate.
CANDIDATE RULES:
{magnate? king} {magnate? businessman}
OUTPUT: In 1900, Omaha was the center of a national
uproar over the kidnapping of Edward Cudahy, Jr., the
son of a local meatpacking businessman.
Figure 1: Input sentence, candidate simplification rules,
and output sentence.
consumers looking for medical information (El-
hadad and Sutaria, 2007; Dele?ger and Zweigen-
baum, 2009).
Simplification can take place at different levels of
a text ? its overall document structure, the syntax
of its sentences, and the individual phrases or words
in a sentence. In this paper, we present a sentence
simplification approach, which focuses on lexical
simplification.1 The key contributions of our work
are (i) an unsupervised method for learning pairs of
complex and simpler synonyms; and (ii) a context-
aware method for substituting one for the other.
Figure 1 shows an example input sentence. The
word magnate is determined as a candidate for sim-
plification. Two learned rules are available to the
simplification system (substitute magnate with king
or with businessman). In the context of this sen-
tence, the second rule is selected, resulting in the
simpler output sentence.
Our method contributes to research on lexical
simplification (both learning of rules and actual sen-
tence simplification), a topic little investigated thus
far. From a technical perspective, the task of lexi-
cal simplification bears similarity with that of para-
1Our resulting system is available for download at
http://www.cs.columbia.edu/ ob2008/
496
phrase identification (Androutsopoulos and Malaka-
siotis, 2010) and the SemEval-2007 English Lexi-
cal Substitution Task (McCarthy and Navigli, 2007).
However, these do not consider issues of readabil-
ity and linguistic complexity. Our methods lever-
age a large comparable collection of texts: En-
glish Wikipedia2 and Simple English Wikipedia3.
Napoles and Dredze (2010) examined Wikipedia
Simple articles looking for features that characterize
a simple text, with the hope of informing research
in automatic simplification methods. Yatskar et al
(2010) learn lexical simplification rules from the edit
histories of Wikipedia Simple articles. Our method
differs from theirs, as we rely on the two corpora as a
whole, and do not require any aligned or designated
simple/complex sentences when learning simplifica-
tion rules.4
2 Data
We rely on two collections ? English Wikipedia
(EW) and Simple English Wikipedia (SEW). SEW
is a Wikipedia project providing articles in Sim-
ple English, a version of English which uses fewer
words and easier grammar, and which aims to be
easier to read for children, people who are learning
English and people with learning difficulties. Due to
the labor involved in simplifying Wikipedia articles,
only about 2% of the EW articles have been simpli-
fied.
Our method does not assume any specific align-
ment or correspondance between individual EW and
SEW articles. Rather, we leverage SEW only as
an example of an in-domain simple corpus, in or-
der to extract word frequency estimates. Further-
more, we do not make use of any special properties
of Wikipedia (e.g., edit histories). In practice, this
means that our method is suitable for other cases
where there exists a simplified corpus in the same
domain.
The corpora are a snapshot as of April 23, 2010.
EW contains 3,266,245 articles, and SEW contains
60,100 articles. The articles were preprocessed as
follows: all comments, HTML tags, and Wiki links
were removed. Text contained in tables and figures
2http://en.wikipedia.org
3http://simple.wikipedia.org
4Aligning sentences in monolingual comparable corpora has
been investigated (Barzilay and Elhadad, 2003; Nelken and
Shieber, 2006), but is not a focus for this work.
was excluded, leaving only the main body text of
the article. Further preprocessing was carried out
with the Stanford NLP Package5 to tokenize the text,
transform all words to lower case, and identify sen-
tence boundaries.
3 Method
Our sentence simplification system consists of two
main stages: rule extraction and simplification. In
the first stage, simplification rules are extracted from
the corpora. Each rule consists of an ordered word
pair {original? simplified} along with a score indi-
cating the similarity between the words. In the sec-
ond stage, the system decides whether to apply a rule
(i.e., transform the original word into the simplified
one), based on the contextual information.
3.1 Stage 1: Learning Simplification Rules
3.1.1 Obtaining Word Pairs
All content words in the English Wikipedia Cor-
pus (excluding stop words, numbers, and punctua-
tion) were considered as candidates for simplifica-
tion. For each candidate word w, we constructed a
context vectorCVw, containing co-occurrence infor-
mation within a 10-token window. Each dimension
i in the vector corresponds to a single word wi in
the vocabulary, and a single dimension was added to
represent any number token. The value in each di-
mension CVw[i] of the vector was the number of oc-
currences of the corresponding wordwi within a ten-
token window surrounding an instance of the candi-
date word w. Values below a cutoff (2 in our exper-
iments) were discarded to reduce noise and increase
performance.
Next, we consider candidates for substitution.
From all possible word pairs (the Cartesian product
of all words in the corpus vocabulary), we first re-
move pairs of morphological variants. For this pur-
pose, we use MorphAdorner6 for lemmatization, re-
moving words which share a common lemma. We
also prune pairs where one word is a prefix of the
other and the suffix is in {s, es, ed, ly, er, ing}. This
handles some cases which are not covered by Mor-
phAdorner. We use WordNet (Fellbaum, 1998) as
a primary semantic filter. From all remaining word
pairs, we select those in which the second word, in
5http://nlp.stanford.edu/software/index.shtml
6http://morphadorner.northwestern.edu
497
its first sense (as listed in WordNet)7 is a synonym
or hypernym of the first.
Finally, we compute the cosine similarity scores
for the remaining pairs using their context vectors.
3.1.2 Ensuring Simplification
From among our remaining candidate word pairs,
we want to identify those that represent a complex
word which can be replaced by a simpler one. Our
definition of the complexity of a word is based on
two measures: the corpus complexity and the lexical
complexity. Specifically, we define the corpus com-
plexity of a word as
Cw =
fw,English
fw,Simple
where fw,c is the frequency of word w in corpus c,
and the lexical complexity as Lw = |w|, the length
of the word. The final complexity ?w for the word
is given by the product of the two.
?w = Cw ? Lw
After calculating the complexity of all words par-
ticipating in the word pairs, we discard the pairs for
which the first word?s complexity is lower than that
of the second. The remaining pairs constitute the
final list of substitution candidates.
3.1.3 Ensuring Grammaticality
To ensure that our simplification substitutions
maintain the grammaticality of the original sentence,
we generate grammatically consistent rules from
the substitution candidate list. For each candidate
pair (original, simplified), we generate all consis-
tent forms (fi(original), fi(substitute)) of the two
words using MorphAdorner. For verbs, we create
the forms for all possible combinations of tenses and
persons, and for nouns we create forms for both sin-
gular and plural.
For example, the word pair (stride, walk) will gen-
erate the form pairs (stride, walk), (striding, walk-
ing), (strode, walked) and (strides, walks). Signifi-
cantly, the word pair (stride, walked) will generate
7Senses in WordNet are listed in order of frequency. Rather
than attempting explicit disambiguation and adding complex-
ity to the model, we rely on the first sense heuristic, which is
know to be very strong, along with contextual information, as
described in Section 3.2.
exactly the same list of form pairs, eliminating the
original ungrammatical pair.
Finally, each pair (fi(original), fi(substitute)) be-
comes a rule {fi(original) ? fi(substitute)},
with weight Similarity(original, substitute).
3.2 Stage 2: Sentence Simplification
Given an input sentence and the set of rules learned
in the first stage, this stage determines which words
in the sentence should be simplified, and applies
the corresponding rules. The rules are not applied
blindly, however; the context of the input sentence
influences the simplification in two ways:
Word-Sentence Similarity First, we want to en-
sure that the more complex word, which we are at-
tempting to simplify, was not used precisely because
of its complexity - to emphasize a nuance or for its
specific shade of meaning. For example, suppose we
have a rule {Han? Chinese}. We would want to
apply it to a sentence such as ?In 1368 Han rebels
drove out the Mongols?, but to avoid applying it to
a sentence like ?The history of the Han ethnic group
is closely tied to that of China?. The existence of
related words like ethnic and China are clues that
the latter sentence is in a specific, rather than gen-
eral, context and therefore a more general and sim-
pler hypernym is unsuitable. To identify such cases,
we calculate the similarity between the target word
(the candidate for replacement) and the input sen-
tence as a whole. If this similarity is too high, it
might be better not to simplify the original word.
Context Similarity The second factor has to do
with ambiguity. We wish to detect and avoid cases
where a word appears in the sentence with a differ-
ent sense than the one originally considered when
creating the simplification rule. For this purpose, we
examine the similarity between the rule as a whole
(including both the original and the substitute words,
and their associated context vectors) and the context
of the input sentence. If the similarity is high, it is
likely the original word in the sentence and the rule
are about the same sense.
3.2.1 Simplification Procedure
Both factors described above require sufficient
context in the input sentence. Therefore, our sys-
tem does not attempt to simplify sentences with less
than seven content words.
498
Type Gram. Mean. Simp.
Baseline 70.23(+13.10)% 55.95% 46.43%
System 77.91(+8.14)% 62.79% 75.58%
Table 1: Average scores in three categories: grammatical-
ity (Gram.), meaning preservation (Mean.) and simplifi-
cation (Simp.). For grammaticality, we show percent of
examples judged as good, with ok percent in parentheses.
For all other sentences, each content word is ex-
amined in order, ignoring words inside quotation
marks or parentheses. For each word w, the set of
relevant simplification rules {w ? x} is retrieved.
For each rule {w ? x}, unless the replacement
word x already appears in the sentence, our system
does the following:
? Build the vector of sentence context SCVs,w in a
similar manner to that described in Section 3.1,
using the words in a 10-token window surround-
ing w in the input sentence.
? Calculate the cosine similarity of CVw and
SCVs,w. If this value is larger than a manually
specified threshold (0.1 in our experiments), do
not use this rule.
? Create a common context vector CCVw,x for the
rule {w ? x}. The vector contains all fea-
tures common to both words, with the feature
values that are the minimum between them. In
other words, CCVw,x[i] = min(CVw[i], CVx[i]).
We calculate the cosine similarity of the common
context vector and the sentence context vector:
ContextSim = cosine(CCVw,x, SCVs,w)
If the context similarity is larger than a threshold
(0.01), we use this rule to simplify.
If multiple rules apply for the same word, we use
the one with the highest context similarity.
4 Experimental Setup
Baseline We employ the method of Devlin and
Unthank (2006) which replaces a word with its most
frequent synonym (presumed to be the simplest) as
our baseline. To provide a fairer comparison to our
system, we add the restriction that the synonyms
should not share a prefix of four or more letters
(a baseline version of lemmatization) and use Mor-
phAdorner to produce a form that agrees with that
of the original word.
Type Freq. Gram. Mean. Simp.
Base High 63.33(+20)% 46.67% 50%
Sys. High 76.67(+6.66)% 63.33% 73.33%
Base Med 75(+7.14)% 67.86% 42.86%
Sys. Med 72.41(+17.25)% 75.86% 82.76%
Base Low 73.08(+11.54)% 53.85% 46.15%
Sys. Low 85.19(+0)% 48.15% 70.37%
Table 2: Average scores by frequency band
Evaluation Dataset We sampled simplification
examples for manual evaluation with the following
criteria. Among all sentences in English Wikipedia,
we first extracted those where our system chose to
simplify exactly one word, to provide a straightfor-
ward example for the human judges. Of these, we
chose the sentences where the baseline could also
be used to simplify the target word (i.e., the word
had a more frequent synonym), and the baseline re-
placement was different from the system choice. We
included only a single example (simplified sentence)
for each rule.
The evaluation dataset contained 65 sentences.
Each was simplified by our system and the baseline,
resulting in 130 simplification examples (consisting
of an original and a simplified sentence).
Frequency Bands Although we included only a
single example of each rule, some rules could be
applied much more frequently than others, as the
words and associated contexts were common in the
dataset. Since this factor strongly influences the
utility of the system, we examined the performance
along different frequency bands. We split the eval-
uation dataset into three frequency bands of roughly
equal size, resulting in 46 high, 44 med and 40 low.
Judgment Guidelines We divided the simplifica-
tion examples among three annotators 8 and ensured
that no annotator saw both the system and baseline
examples for the same sentence. Each simplification
example was rated on three scales: Grammaticality
- bad, ok, or good; Meaning - did the transforma-
tion preserve the original meaning of the sentence;
and Simplification - did the transformation result in
8The annotators were native English speakers and were not
the authors of this paper. A small portion of the sentence pairs
were duplicated among annotators to calculate pairwise inter-
annotator agreement. Agreement was moderate in all categories
(Cohen?s Kappa = .350? .455 for Simplicity, .475? .530 for
Meaning and .415? .425 for Grammaticality).
499
a simpler sentence.
5 Results and Discussion
Table 1 shows the overall results for the experiment.
Our method is quantitatively better than the base-
line at both grammaticality and meaning preserva-
tion, although the difference is not statistically sig-
nificant. For our main goal of simplification, our
method significantly (p < 0.001) outperforms the
baseline, which represents the established simplifi-
cation strategy of substituting a word with its most
frequent WordNet synonym. The results demon-
strate the value of correctly representing and ad-
dressing content when attempting automatic simpli-
fication.
Table 2 contains the results for each of the fre-
quency bands. Grammaticality is not strongly influ-
enced by frequency, and remains between 80-85%
for both the baseline and our system (considering
the ok judgment as positive). This is not surpris-
ing, since the method for ensuring grammaticality is
largely independent of context, and relies mostly on
a morphological engine. Simplification varies some-
what with frequency, with the best results for the
medium frequency band. In all bands, our system is
significantly better than the baseline. The most no-
ticeable effect is for preservation of meaning. Here,
the performance of the system (and the baseline) is
the best for the medium frequency group. However,
the performance drops significantly for the low fre-
quency band. This is most likely due to sparsity of
data. Since there are few examples from which to
learn, the system is unable to effectively distinguish
between different contexts and meanings of the word
being simplified, and applies the simplification rule
incorrectly.
These results indicate our system can be effec-
tively used for simplification of words that occur
frequently in the domain. In many scenarios, these
are precisely the cases where simplification is most
desirable. For rare words, it may be advisable to
maintain the more complex form, to ensure that the
meaning is preserved.
Future Work Because the method does not place
any restrictions on the complex and simple corpora,
we plan to validate it on different domains and ex-
pect it to be easily portable. We also plan to extend
our method to larger spans of texts, beyond individ-
ual words.
References
Androutsopoulos, Ion and Prodromos Malakasiotis.
2010. A survey of paraphrasing and textual entail-
ment methods. Journal of Artificial Intelligence
Research 38:135?187.
Barzilay, Regina and Noemie Elhadad. 2003. Sen-
tence alignment for monolingual comparable cor-
pora. In Proc. EMNLP. pages 25?32.
Blake, Catherine, Julia Kampov, Andreas Or-
phanides, David West, and Cory Lown. 2007.
Query expansion, lexical simplification, and sen-
tence selection strategies for multi-document
summarization. In Proc. DUC.
Carroll, John, Guido Minnen, Yvonne Canning,
Siobhan Devlin, and John Tait. 1998. Practical
simplication of english newspaper text to assist
aphasic readers. In Proc. AAAI Workshop on Inte-
grating Artificial Intelligence and Assistive Tech-
nology.
Chandrasekar, R., Christine Doran, and B. Srinivas.
1996. Motivations and methods for text simplifi-
cation. In Proc. COLING.
Daelemans, Walter, Anja Hthker, and Erik
Tjong Kim Sang. 2004. Automatic sentence
simplification for subtitling in Dutch and English.
In Proc. LREC. pages 1045?1048.
Dele?ger, Louise and Pierre Zweigenbaum. 2009.
Extracting lay paraphrases of specialized expres-
sions from monolingual comparable medical cor-
pora. In Proc. Workshop on Building and Using
Comparable Corpora. pages 2?10.
Devlin, Siobhan and Gary Unthank. 2006. Help-
ing aphasic people process online information. In
Proc. ASSETS. pages 225?226.
Elhadad, Noemie and Komal Sutaria. 2007. Mining
a lexicon of technical terms and lay equivalents.
In Proc. ACL BioNLP Workshop. pages 49?56.
Fellbaum, Christiane, editor. 1998. WordNet: An
Electronic Database. MIT Press, Cambridge,
MA.
Huenerfauth, Matt, Lijun Feng, and Noe?mie El-
hadad. 2009. Comparing evaluation techniques
500
for text readability software for adults with intel-
lectual disabilities. In Proc. ASSETS. pages 3?10.
Jonnalagadda, Siddhartha, Luis Tari, Jo?rg Haken-
berg, Chitta Baral, and Graciela Gonzalez. 2009.
Towards effective sentence simplification for au-
tomatic processing of biomedical text. In Proc.
NAACL-HLT . pages 177?180.
McCarthy, Diana and Roberto Navigli. 2007.
Semeval-2007 task 10: English lexical substitu-
tion task. In Proc. SemEval. pages 48?53.
Napoles, Courtney and Mark Dredze. 2010. Learn-
ing simple wikipedia: a cogitation in ascertaining
abecedarian language. In Proc. of the NAACL-
HLT Workshop on Computational Linguistics and
Writing. pages 42?50.
Nelken, Rani and Stuart Shieber. 2006. Towards
robust context-sensitive sentence alignment for
monolingual corpora. In Proc. EACL. pages 161?
166.
Siddharthan, Advaith. 2004. Syntactic simplifica-
tion and text cohesion. Technical Report UCAM-
CL-TR-597, University of Cambridge, Computer
Laboratory.
Vickrey, David and Daphne Koller. 2008. Apply-
ing sentence simplification to the CoNLL-2008
shared task. In Proc. CoNLL. pages 268?272.
Williams, Sandra and Ehud Reiter. 2005. Generating
readable texts for readers with low basic skills. In
Proc. ENLG. pages 127?132.
Yatskar, Mark, Bo Pang, Cristian Danescu-
Niculescu-Mizil, and Lillian Lee. 2010. For the
sake of simplicity: Unsupervised extraction of
lexical simplifications from wikipedia. In Proc.
NAACL-HLT . pages 365?368.
501
Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 64?71,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Cancer Stage Prediction Based on Patient Online Discourse
Mukund Jha
Computer Science
Columbia University
New York, NY 10027
mj2472@columbia.edu
Noe?mie Elhadad
Biomedical Informatics
Columbia University
New York, NY 10032
noemie@dbmi.columbia.edu
Abstract
Forums and mailing lists dedicated to par-
ticular diseases are increasingly popular
online. Automatically inferring the health
status of a patient can be useful for both
forum users and health researchers who
study patients? online behaviors. In this
paper, we focus on breast cancer forums
and present a method to predict the stage
of patients? cancers from their online dis-
course. We show that what the patients
talk about (content-based features) and
whom they interact with (social network-
based features) provide complementary
cues to predicting cancer stage and can be
leveraged for better prediction. Our meth-
ods are extendable and can be applied to
other tasks of acquiring contextual infor-
mation about online health forum partici-
pants.
1 Introduction
In this paper we investigate an automated method
of inferring the stage of a patient?s breast cancer
from discourse in an online forum. Such informa-
tion can prove invaluable both for forummembers,
by enriching their use of this rapidly developing
and increasingly popular medium, and for health
researchers, by providing them with tools to quan-
tify and better understand patient populations and
how they behave online.
Patients with chronic diseases like diabetes or
life-threatening conditions like breast cancer get
a wealth of information from medical profession-
als about their diagnoses, test results, and treat-
ment options, but such information is not always
satisfactory or sufficient for patients. Much of
that is essential to their everyday lives and the
management of their condition escapes the clin-
ical realm. Furthermore, patients feel informed
and empowered by exchanging experiences and
emotional support with others in the same circum-
stances. Thus, it is not surprising that patient com-
munities have flourished on the Web over the past
decade, through active disease-specific discussion
forums and mailing lists.
For health professionals, this new medium
presents exciting research avenues related to the-
ories of psycho-social support and how patients
manage their conditions. Qualitative analyses of
forums and mailing list posts show that breast
cancer patients and survivors provide and seek
support to and from their peers and that support,
while also emotional, is largely informational in
nature (Civan and Pratt, 2007; Meier et al, 2007).
Emotional support may include words of encour-
agement and prayers. Examples of informational
support are providing personal experiences with a
treatment, discussing new research, explaining a
pathology report to a peer, as well as exchanging
information pertinent to patients? daily lives, such
as whether to shave one?s head once chemotherapy
starts.
Given the kinds of benefits that patients and sur-
vivors seek and provide in online forums, it seems
likely that they would be inclined to gravitate to-
ward others whose circumstances most closely re-
semble their own, beyond sharing the general di-
agnosis of breast cancer. In fact, focus groups
and surveys conducted with breast cancer patients
identified and emphasized the need for online can-
cer forum participants to identify other patients of
a particular age, stage of illness, or having opted
for similar treatment (Rozmovits and Ziebland,
2004; van Uden-Kraan et al, 2008).
The stage of a patient?s cancer, in particular, can
be a crucial proxy for finding those whose experi-
ences are likely similar and relevant to one?s own.
For breast cancer, there are five high-level stan-
dard stages (0 to IV). While they do not give the
whole picture about a particular cancer (the stages
64
themselves can be described with finer granular-
ity and they do no not encompass additional in-
formation like hormonal sensitivity), physicians
have traditionally relied on them for prognosis and
determining treatment options. For patients and
survivors, they are a useful way to communicate
to their peers their health status, as evidenced by
the members? signatures on forums and mailing
lists (Meier et al, 2007).
Although many forums provide pre-set profile
fields for users to populate with important back-
ground information, such as the stage of their can-
cer (e.g., the popular forum on breastcancer.
org), in practice, only a fraction of members have
a complete profile. Thus, an automated way of in-
ferring member profile information via the social
network created by a forum?s users would help fill
in the blanks.
Beyond identifying other patients in a forum in
similar circumstances, such a tool can have nu-
merous practical benefits for both forum users and
health researchers who study patients? online be-
havior. When a patient searches for a particu-
lar piece of information in a forum, incorporat-
ing contextual information about the user into the
search mechanism can improve search results. For
example, a search tool can rank higher the posts
that were authored by patients with the same stage.
For health researchers, questions which bring a
better understanding of forum usage (i.e., ?are pa-
tients with stage IV cancer more or less active in a
forum than patients with early stage cancer?) can
be answered accurately only if all members of the
forums are taken into account, not just the ones
who filled out their member profiles. Furthermore,
in the context of health communication, the more
information is available about an individual, the
more effective the message can be, from generic
to personalized to targetted to tailored (Kreuter et
al., 2000). Our research contributes an automated
method to acquiring contextual information about
forum participants. We focus on cancer stage as
an exmple of context information.
Our research question is whether it is possible
to predict the stage of individuals? cancer based on
their online discourse. By discourse we mean both
the information she conveys and whom she talks
to in a forum. Following ethical guidelines in pro-
cessing of patient data online, we focus on a pop-
ular breast cancer forum with a large number of
participants (Eysenbach and Till, 2001). We show
that the content of members? posts and the stage
of their interlocutors can provide complementary
clues to identifying cancer stages.
2 Related Work
Researchers have begun to explore the possibility
of diagnosing patients based on their speech pro-
ductions. Content analysis methods, which rely on
patient speech transcripts or texts authored by pa-
tients, have been leveraged for understanding can-
cer coping mechanisms (Graves et al, 2005; Ban-
tum and Owen, 2009), psychiatric diagnoses (Ox-
man et al, 1988; Elvevaag et al, 2010), and the
analysis of suicide notes (Pestian et al, 2008).
In all cases, results, while not fully accurate, are
promising and show that patient-generated con-
tent is a valuable clue to diagnosis in an automated
framework.
Our work departs from these experiments in that
we do not attempt to predict the psychological
state of a patient, but rather the status of a clinical
condition. Staging breast cancer provides a way to
summarize the status of the cancer based on clin-
ical characteristics (the size of the tumor, whether
the cancer is invasive or not, whether cancer cells
are present in the lymph nodes, and whether the
cancer has spread beyond the breast). There are
five high-level stages for breast cancer. Stage 0
describes a non-invasive cancer. Stage I represents
early stage of an invasive cancer, where the tumor
size is less than 2 centimeters and no lymph nodes
are involved (that is, the cancer has not spread out-
side of the breast). Stages II and III describe a can-
cer with larger tumor size and/or the cancer has
spread outside of the breast. Stage IV describes
a cancer that have metastasized to distant parts of
the body, such as lungs and bones.
In our work, we analyze naturally occurring
content, generated by patients talking to each other
online. As such, our sample population is much
larger than in earlier works (typically less than 100
subjects). Like the researchers who focus on con-
tent analysis, we rely on the content generated by
patients, but we also hypothesize that whom the
patients interact with can help the prediction of
cancer stage.
In particular, we build a social network based
on patients? interactions to boost text-based pre-
dictions. Graph-based methods are becoming
increasingly popular in the NLP community,
and similar approaches have been employed and
65
shown to perform well in other areas like ques-
tion answering (Jurczyk, 2007) (Harabagiu et al,
2006), word-sense disambiguation (Niu et al,
2005), and textual entailment (Haghighi, 2005).
3 Methods
Our methods to predict cancer stage operate in a
supervised framework. We cast the task of stage
prediction as a 4-way classification (Stage I to IV).
We hypothesize that the discourse of patients on-
line, as defined by the content of their posts in a
forum, can be leveraged to predict cancer stage.
Furthermore, we hypothesize that the social net-
work derived by whom patients interact with can
provide an additional clue for stage detection.
We experimented with three methods of predict-
ing cancer stage:
Text-based stage prediction A classifier is
trained given the post history of a patient.
Network-based stage prediction A social net-
work representing the interactions among fo-
rummembers is built, and a label propagation
algorithm is applied to infer the stage of indi-
vidual patients.
Combined prediction A classifier which com-
bines text-based and network-based features.
Next we describe each method in detail, along
with our dataset and our experimental setup.
3.1 Data Collection and Preprocessing
We collected posts from the publicly available dis-
cussion board from breastcancer.org. It is
a popular forum, with more than 60,000 regis-
tered members, and more than 50,000 threads dis-
cussed in 60 subforums. To collect our dataset,
we crawled the content of the most popular subfo-
rums.1
Collected posts were translated from HTML
into an XML format, keeping track of author id,
1There were 17 such subforums: ?Just Diagnosed,? ?Help
Me Get Through Treatment,? ?Surgery - Before, During, and
After,? ?Chemotherapy - Before, During and After,? ?Ra-
diation Therapy - Before, During and After,? ?Hormonal
Therapy - Before, During and After,? ?Alternative, Com-
plementary and Holistic Treatment,? ?Stage I and II Breast
Cancer,? ?Just Diagnosed with a Recurrence or Metastasis,?
?Stage III Breast Cancer,? ?Stage IV Breast Cancer Sur-
vivors,? ?HER2/neu Positive Breast Cancer,? ?Deperession,
Anxiety and Post Traumatic Stress Disorder,? ?Fitness and
Getting Back in Shape,? ?Healthy Recipes for Everyday Liv-
ing,? ?Recommend Your Resources,? ?Clinical Trials, Re-
search, News, and Study Results.?
Nb. of threads 26,160
Nb. of posts 524,247
Nb. of threads with < 20 posts 22,334
Nb. of users with profile Stage I 2,226
Nb. of users with profile Stage II 2,406
Nb. of users with profile Stage III 1,031
Nb. of users with profile Stage IV 749
Total Nb. of users with profile 6,412
Nb. of active users profiled Stage I 1,317
Nb. of active users profiled Stage II 1,400
Nb. of active users profiled Stage III 580
Nb. of active users profiled Stage IV 448
Total Nb. of active users with profile 3,745
Table 1: General statistics of the dataset.
thread id, position of the post in the thread, body of
the post, and signature of the author (which is kept
separated from the body of the post). The con-
tent of the posts was tokenized, lower-cased and
stemmed. Images, URLs, and stop words were re-
moved.
To post in breastcancer.org, users must
register. They have the option to enter a profile
with pre-set fields related to their breast cancer di-
agnosis; in particular cancer stage between stage
I and IV. We collected the list of members who
entered their stage information, thereby providing
us with an annotated set of patients with their cor-
responding cancer stage. Table 1 shows various
statistics for our dataset. Active users are defined
as members who have posted more than 50 words
overall in the forums. Note the low number of
user with profile information (approximately 10%
of the overall number of registered participants in
the forum).
3.2 Text-Based Stage Prediction
We trained a text-based classifier relying on the
full post history of each patient. The full post
history was concatenated. Signature information,
which is derived automatically from the patient?s
profile (and thus contains stage information) was
removed from the posts. The classifier relied on
unigrams and bigrams only. Table 2 shows statis-
tics about post history length, measured as number
of words authored by a forum member.
3.3 Network-Based Stage Prediction
We hypothesize that patients tend to interact in a
forum with patients with similar stage. To test this
66
Stages Min Max Average Median
I 4 609,608 8,429 3,123
II 2 353,731 8,142 3,112
III 8 211,655 9,297 3,189
IV 10 893,326 17,083 326
Table 2: Statistics about number of words in post
history.
?
2
12
2
1
5
IV
III
I
IV
3
Figure 1: Nodes in the social network of forum
member interaction.
hypothesis, we represent the interactions of the pa-
tients as a social network. The nodes in the net-
work represent patients, and an edge is present be-
tween two nodes if the patients interact with each
other, that is they are part of the same threads of-
ten. Weights on edges represent the degree of in-
teraction. Higher weight on an edge between two
forum members indicates they interact more often.
More precisely, we build an undirected, weighted
network, where the nodes representing training in-
stances are labeled with their provided stage infor-
mation and their labels are fixed. Figure 1 shows
an example of node and its immediate neighbors
in the network. Of his five neighbors, four repre-
sent training instances and have a fixed stage, and
one represents a user with an unknown stage.
A label propagation algorithm is applied to the
network, so that every node in the network is as-
signed a stage between I and IV (Raghavan et al,
2007). Given a node and its immediate neighbors,
it looks for the most frequent labels, taking into ac-
count the edge weights. In our example, the prop-
agated label for the central node will be stage IV.
This label, in turn, will be used to assign a label to
the other nodes. When building the social network
of interactions, we experimented with the follow-
ing parameters.
Nodes in the network. We experimented with
including all the forum members who participated
in a conversation thread. Thus, it includes all the
members, even the ones without a known cancer
stage. This resulted in a network of 15,035 forum
participants. This way, the network covers more
interactions among more users, but is very sparse
in its initial labeling (only the training instances
in the dataset of active members with a known la-
bel are labeled). The label propagation algorithm
assigns labels to all the nodes, but we test its ac-
curacy only on the test instances. We also ex-
perimented with including only the patients in the
training and testing sets, thereby reducing the size
of the network but also decreasing the sparsity of
the labeling. This resulted in a network of 3,305
nodes.2
Drawing edges in the network. An edge be-
tween two users indicate they are frequently in-
teracting. One crude way is to draw an edge be-
tween every user participating in the same thread,
this however does not provide an accurate picture
and hence does not yield good results. In our ap-
proach we draw an edge in two steps. First, since
threads are often long and can span over multiple
topics, we only draw an edge if the two individ-
uals? posts are within five posts of each other in
the thread. Second, we then look for any direct
references made by a user to another user in their
post. In forum threads, users usually make a di-
rect reference by either by explicitly referring to
each other using their real name or internet alases
or by quoting each other, i.e., repeating or stating
what the other user has mentioned in her post. For
example in ?Hey Dana, I went through the same
thing the first time I went to my doctor...?, the au-
thor of the post is referring to another user with
name ?Dana?. We rely on such explicit references
to build accurate graph.3 To find direct explicit ref-
erences, we search in every post of a thread for any
mention of names (real or aliases) of users partic-
ipating in the thread and if one is found we draw
an edge between them.
We observed that users refer to each other very
2This number of nodes is less than the numbers of over-
all active members in our gold standard because some active
members have either posted in threads with only one post or
with more than 20 posts.
3An alternative approach is to identify quotes in posts. In
our particular dataset, quotes did not occur often, and thus
were ignored when assessing the degree of interaction be-
tween two forum members.
67
frequently using their real names instead of inter-
net names (which are long and often arbitrary).
These are often hard to detect because no data is
present which link users? forum aliases to their
real name. We use following approach to extract
real names of the users.
Extracting real names. For every user, we ex-
tract the last ten words (signature) from every post
posted by the user and concatenate them after re-
moving all stop words and other common signa-
ture terms (like thanks, all the best, love, good luck
etc.) using a pre-compiled list. We then mine for
the most frequent name occurring in the concate-
nated text using standard list of names and extract-
ing capitalized words. We also experimented with
using Named Entity Recognizers, but our simple
rule based name extractor gave us better results
with higher precision. Finally, we map the ex-
tracted real name with the user?s alias and utilize
them to find direct references between posts.
Weights Computation. The weight of an edge
between two nodes represents the degree of inter-
action between two corresponding users (the more
often they communicate, the higher the weight).
Since the label propagation algorithm takes into
account the weighted frequency of neighboring
nodes, these weights are crucial. We compute
the weights in following manner: for each pair of
users with an existing edge (as determined above),
we iterate through their posts in common threads,
and add the cosine similarity score between the
two posts to the weight of the edge. For edges
made through direct references we add the high-
est cosine similarity score between any two pair of
posts in that particular thread. This way we weigh
higher the edges made through direct reference as
we are more confident about them.
The full network of all users (15,035 nodes)
had 480,051 edges, and the restricted network of
dataset users (3,305 nodes) had 28,152 edges.
3.4 Combining Text-Based and
Network-Based Predictions
To test the hypothesis that text-based and network-
based predictions model different aspects of pa-
tients and thus provide complementary cues to
stage prediction, we trained a classifier which in-
corporates text-based and network-based features.
The combined classifier contained the following
features: text-based predicted label, confidence
score of the text-based prediction, network-based
predicted label, percentage of immediate neigh-
bors in the network with a stage I label, stage II,
III and IV labels (neighbors in the network with
no labels do not contribute to the counts). For in-
stance, the central node in Figure 1 is assigned the
feature values 1/4, 0, 1/4 and 1/2 for the ratio of
stage I, II, III and IV neighbors.
3.5 Experimental Setup
Our dataset for the three models consisted of the
3,745 active members. For all the models, we fol-
low a five-fold stratified cross validation scheme.
The text-based classification was carried out with
BoosTexter (Schapire and Singer, 2000), trained
with 800 rounds of boosting. The label propaga-
tion on the social network was carried out in R.4
The final decision-tree classification was carried
out in Weka, relying on an SVM classifier with
default parameters (Hall et al, 2009).
4 Results
Table 3 shows the results of the text-based predic-
tion, the network-based prediction and the com-
bined prediction for each stage measured by Pre-
cision, Recall and F-measure. For comparison, we
report on the results of a baseline text-based pre-
diction. The baseline prediction assigns a stage
based on the explicit mention of stage in the post
history of a patient. In practice, it is a rule-
based prediction with matching against the pattern
?stage [IV|four|4]? for stage IV prediction,
and similarly for other stages. The text-based pre-
diction yields better results than the baseline, with
a marked improvement for each stage.
The network-based prediction performs only
slightly worse than the text-based predictions. The
hypothesis that whom the patient interacts with in
the forums helps predict stage holds. To verify this
point further, we computed for each stage the av-
erage ratio of neighbors per stage based on the so-
cial network of interactions, as shown in Figure 2.
For instance, stage IV patients interact mostly with
their peers (49% of their posts are shared with
other stage IV users), and to some extent with
other patients (18% of their posts with stage I pa-
tients, 20% with stage II patients, and 13% with
stage III patients). Except for stage III patients, all
other patients are mostly interacting with similarly
staged patients.
4www.r-project.org
68
Baseline Text Based
Stage Precision Recall F Stage Precision Recall F
I 76.2 26.4 39.3 I 54.9 63.9 59.1
II 79.4 18.7 30.3 II 51.6 55.0 53.2
III 76.6 35.0 48.0 III 52.7 30.3 38.5
IV 76.4 50.7 60.9 IV 82.5 71.2 76.4
Network Based Combined
Stage Precision Recall F Stage Precision Recall F
I 50.4 56.7 53.4 I 57.1 65.4 61.0
II 49.6 49.1 49.3 II 56.6 53.5 55.0
III 65.7 27.7 39.0 III 56.1 48.3 51.9
IV 59.3 83.7 69.4 IV 84.7 81.3 83.0
Table 3: Stage prediction results (Precision, Recall, and F-measure).
When combining the text-based and the
network-based predictions in an overall classifier
the prediction yields the best results. These results
confirm the potential in combining the two facets
of patient discourse, content and social interaction.
The results presented in the table correspond to
a network built with the full set of users, including
those without any profile information. When re-
stricting the network on the patients with stage la-
bels only, we obtained similar results (F-measures
of 56% for stage I, 52% for stage II, 43% for stage
III, and 79% for stage IV). This shows that it is
worth modeling the full set of interactions and the
full network structure, even when a large number
of nodes have missing labels.
Finally, we also experimented with building
networks with no weights or with weights with-
out the 5-post-apart restriction. In both cases, the
results of the network-based and combined predic-
tions are lower than those presented in Table 3. We
interpret this fact as a confirmation that our edge
weighting strategy models to a promising extent
the degree of interaction among patients.
5 Discussion
Text-based prediction. Results confirm that
cancer stage can be predicted by a patient?s on-
line discourse. When examining the unigrams and
bigrams picked up by the classifier as predictive
of stage, we can get a sense of the frequent top-
ics of discussion of patients. For instance, the
phrases ?tumor mm? (referring to tumor size in
millimeters) and ?breast radiation? were highly
predictive of stage I patients. The words ?hat? and
?hair? were highly predictive of stages II and III,
Figure 2: Distribution of stage-wise interactions.
while stage IV patients were predicted by the pres-
ence of the phrases ?bone met.? (which stands for
bone metastasis), ?met lung? ?liver,? and ?lym-
phedema? (which is a side effect of cancer treat-
ment linked to the removal of lymph nodes and
tumor).
Figure 3 shows the overall accuracy of the text-
based classifier, when tested against the amount of
text available for the classification. As expected,
the longer the post history, the more accurate the
classification.
Representing degree of interaction among pa-
tients. In our experiments, we observed that the
weigthing scheme of edges had a strong impact
on the overall accuracy of stage prediction. The
more interaction was modeled (through distance
in thread and identification of explicit references),
the better the results. This confirms the hypothesis
that dialogue is helpful in predicting cancer stage,
and emphasizes the need for accurate techniques
69
Figure 3: Overall text-based prediction accuracy
against post history length.
to model interaction among forum participants in
a social network.
Discourse of Stage IV patients. Both the text-
based and the network-based predictions provide
higher precision and recall for the stage IV pa-
tients. This is emphasized by Figure 2, where
we see that, in our dataset, stage IV patients talk
mostly to each other. These results suggest that
stage IV patients have particular discourse, which
separates them from other patients. This presents
interesting avenues for future investigation.
6 Future Work and Conclusion
In this paper, we investigated breast cancer stage
prediction based on the online discourse of pa-
tients participating in a breast cancer-specific fo-
rum. We show that relying on lexical features de-
rived from the content of the posts of a patient
provides promising classification results. Further-
more, even a simple social network representing
patient interactions on a forum, yields predictions
with comparable results. Combining the two ap-
proaches boosts results, as content and interaction
seem to model complementary aspects of patient
discourse.
Our experiments show that stage IV patients ap-
pear to exhibit specific textual and social patterns
in forums. This point can prove useful to health re-
searchers who want to quantify patient behaviors
online.
The strategy of combining two facets of dis-
course (content and interactions) introduces sev-
eral interesting research questions. In the future,
we plan to investigate some of them. In a first step,
we plan to better model the interactions of patients
online. For instance, we would like to analyze the
content of the posts to determine further if two pa-
tients are in direct communication, and the domain
of their exchange (e.g., clinical vs. day-to-day vs.
emotional). As we have observed that the way
edges in the network are weighted has an impact
on overall performance, we could then investigate
whether the domain(s) of interaction among users
(clinical matters vs. emotional and instrumental
matters for instance) has an impact on predicting
cancer stage by taking the different domains of in-
teraction in account in the weight computation.
Finally, this work relies on a single, yet highly
active and popular, forum. We would like to
test our results on different breast cancer forums,
but also on other disease-specific forums, where
patients can be separated in clinically relevant
groups.
Acknowledgments
We thank Phani Nivarthi for his help on data col-
lection. This work is supported in part by a Google
Research Award. Any opinions, findings, or con-
clusions are those of the authors, and do not neces-
sarily reflect the views of the funding organization.
References
Erin Bantum and Jason Owen. 2009. Evaluating the
validity of computerized content analysis programs
for identification of emotional expression in cancer
narratives. Psychological Assessment, 21(1):79?88.
Andrea Civan and Wanda Pratt. 2007. Threading to-
gether patient expertise. In Proceedings of the AMIA
Annual Symposium, pages 140?144.
Brita Elvevaag, Peter Foltz, Mark Rosenstein, and
Lynn DeLisi. 2010. An automated method to ana-
lyze language use in patients with schizophrenia and
their first degree-relatives. Journal of Neurolinguis-
tics, 23:270?284.
Gunther Eysenbach and James Till. 2001. Ethical is-
sues in qualitative research on internet communities.
BMJ, 323:1103?1105.
Kristi Graves, John Schmidt, Julie Bollmer, Michele
Fejfar, Shelby Langer, Lee Blonder, and Michael
Andrykowski. 2005. Emotional expression and
emotional recognition in breast cancer survivors:
A controlled comparison. Psychology and Health,
20(5):579?595.
70
Aria Haghighi. 2005. Robust textual inference via
graph matching. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP?05, pages 387?394.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An up-
date. SIGKDD Explorations, 11(1).
Sanda Harabagiu, Finley Lacatusu, and Andrew Hickl.
2006. Answering complex questions with random
walk models. In Proceedings of SIGIR Conference
(SIGIR?06), pages 220?227.
Pawel Jurczyk. 2007. Discovering authorities in ques-
tion answer communities using link analysis. In
Proceedings of the ACM Conference on Information
and Knowledge Management (CIKM?07).
Matthew Kreuter, David Farrell, Laura Olevitch, and
Laura Brennan. 2000. Tailoring health messages:
customizing communication using computer tech-
nology. Lawrence Erlbaum Associates.
Andrea Meier, Elizabeth Lyons, Gilles Frydman,
Michael Forlenza, and Barbara Rimer. 2007. How
cancer survivors provide support on cancer-related
internet mailing lists. Journal of Medical Internet
Research, 9(2):e12.
Zheng-Yu Niu, Dong-Hong Ji, and Chew Lim Tan.
2005. Word sense disambiguation using label prop-
agation based semi-supervised learning. In Pro-
ceedings of the ACL Conference (ACL?05), pages
395?402.
Thomas Oxman, Stanley Rosenberg, Paula Schnurr,
and Gary Tucker. 1988. Diagnostic classification
through content analysis of patient speech. Ameri-
can Joural of Psychatry, 145:464?468.
John Pestian, Pawel Matykiewicz, Jacqueline Grupp-
Phelan, Sarah Arszman Lavanier, Jennifer Combs,
and Robert Kowatch. 2008. Using natural language
processing to classify suicide notes. In Proceedings
of BioNLP?08, pages 96?97.
Usha Raghavan, Reka Albert, and Soundar Kumara.
2007. Near linear time algorithm to detect commu-
nity structures in large-scale networks. Physics Re-
view, page E 76 036106.
Linda Rozmovits and Sue Ziebland. 2004. What do
patients with prostate or breast cancer want from
an Internet site? a qualitative study of information
needs. Patient Education and Counseling, 53:57?
64.
Robert Schapire and Yoram Singer. 2000. BoosTex-
ter: A boosting-based system for text categorization.
Machine Learning, 39(2/3):135?168.
Cornelia van Uden-Kraan, Constance Drossaert, Erik
Tall, Bret Shaw, Erwin Seydel, and Mart van de
Laar. 2008. Empowering processes and outcomes
of participation in online support groups for patients
with breast cancer, arthritis, or fibromyalgia. Quali-
tative Health Research, 18(3):405?417.
71
