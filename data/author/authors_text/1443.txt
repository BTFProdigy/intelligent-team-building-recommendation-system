Finite-State Registered Automata
for Non-Concatenative Morphology
Yael Cohen-Sygal?
University of Haifa
Shuly Wintner?
University of Haifa
We introduce finite-state registered automata (FSRAs), a new computational device within the
framework of finite-state technology, specifically tailored for implementing non-concatenative
morphological processes. This model extends and augments existing finite-state techniques,
which are presently not optimized for describing this kind of phenomena. We first define the
model and discuss its mathematical and computational properties. Then, we provide an extended
regular language whose expressions denote FSRAs. Finally, we exemplify the utility of the model
by providing several examples of complex morphological and phonological phenomena, which are
elegantly implemented with FSRAs.
1. Introduction
Finite-state (FS) technology has been considered adequate for describing the morpho-
logical processes of the world?s languages since the pioneering works of Koskenniemi
(1983) and Kaplan and Kay (1994). Several toolboxes provide extended regular expres-
sion description languages and compilers of the expressions to finite-state automata
(FSAs) and transducers (FSTs) (Karttunen et al 1996; Mohri 1996; van Noord and
Gerdemann 2001a). While FS approaches to most natural languages have generally been
very successful, it is widely recognized that they are less suitable for non-concatenative
phenomena; in particular, FS techniques are assumed not to be able to efficiently account
for the non-concatenative word formation processes that Semitic languages exhibit
(Lavie et al 1988).
While much of the inflectional morphology of Semitic languages can be rather
straightforwardly described using concatenation as the primary operation, the main
word formation process in such languages is inherently non-concatenative. The stan-
dard account describes words in Semitic languages as combinations of two morphemes:
a root and a pattern.1 The root consists of consonants only, by default three (although
longer roots are known). The pattern is a combination of vowels and, possibly, con-
sonants too, with ?slots? into which the root consonants can be inserted. Words are
created by interdigitating roots into patterns: The first consonant of the root is inserted
into the first consonantal slot of the pattern, the second root consonant fills the second
slot, and the third fills the last slot. After the root combines with the pattern, some
? Department of Computer Science, University of Haifa, 31905 Haifa, Israel. E-mail: yaelc@cs.haifa.ac.il.
? Department of Computer Science, University of Haifa, 31905 Haifa, Israel. E-mail: shuly@cs.haifa.ac.il.
1 An additional morpheme, vocalization, is used to abstract the pattern further; for the present purposes,
this distinction is irrelevant.
Submission received: 17 August 2004; revised submission received: 15 June 2005; accepted for publication: 26
September 2005.
? 2006 Association for Computational Linguistics
Computational Linguistics Volume 32, Number 1
Figure 1
Na??ve FSA with duplicated paths.
morpho-phonological alternations take place, which may be non-trivial but are mostly
concatenative.
The major problem that we tackle in this work is medium-distance dependencies,
whereby some elements that are related to each other in some deep-level representation
(e.g., the consonants of the root) are separated on the surface. While these phenomena
do not lie outside the descriptive power of FS systems, na??vely implementing them in
existing finite-state calculi is either impossible or, at best, results in large networks that
are inefficient to process, as the following examples demonstrate.
Example 1
We begin with a simplified problem, namely accounting for circumfixes. Consider three
Hebrew patterns: haaa, hitaaut, and mia, where the empty boxes indi-
cate the slots in the patterns into which the consonants of the roots are inserted. Hebrew
orthography2 dictates that these patterns be written ha, htut, and m, re-
spectively, i.e., the consonants are inserted into the ?? slots as one unit (i.e., the patterns
can be viewed as circumfixes). An automaton that accepts all the possible combinations
of three-consonant stems and these three circumfixes is illustrated in Figure 1.3 Given r
stems and p circumfixes, the number of its states is (2r + 2)p + 2, i.e., increases linearly
with the number of stems and circumfixes. The number of arcs in this automaton is
3rp + 2p, i.e, also O(rp). Evidently, the three basic different paths that result from the
three circumfixes have the same body, which encodes the stems. An attempt to avoid
the duplication of paths is represented by the automaton of Figure 2, which accepts the
language denoted by the regular expression (ht + h + m)(root)(ut + a + ). The number
of states here is 2r + 4, i.e., is independent of the number of circumfixes. The number
of arcs is (3r + 2p), that is, O(r + p), and thus, the complexity of the number of arcs is
also reduced. Obviously, however, such an automaton over-generates by accepting also
invalid words such as mut. In other words, it ignores the dependencies which hold
between prefixes and suffixes of the same circumfix. Since finite-state devices have no
2 Many of the vowels are not explicitly depicted in the Hebrew script.
3 This is an over-simplified example; in practice, the process of combining roots with patterns is highly
idiosyncratic, like other derivational morphological processes.
50
Cohen-Sygal and Wintner Non-Concatenative Morphology
Figure 2
Over-generating FSA.
memory, save for the states, there is no simple and space-efficient way to account for
such dependencies.
Example 2
Consider now a representation of Hebrew where all vowels are explicit, e.g., the pattern
hitae. Consider also the roots r.g.z, b.$.l, and g.b.r. The consonants of a given root
are inserted into the ?? slots to obtain bases such as hitragez, hitba$el, and hitgaber. The
finite state automaton of Figure 3 is the minimized automaton accepting the language;
it has fifteen states. If the number of three letter roots is r, then a general automaton
accepting the combinations of the roots with this pattern will have 4r + 3 states and 5r +
1 arcs. Notice the duplicated arcs which stem from copying the pattern in the different
paths.
Example 3
Another non-concatenative process is reduplication: The process in which a morpheme
or part of it is duplicated. Full reduplication is used as a pluralization process in Malay
and Indonesian; partial reduplication is found in Chamorro to indicate intensivity. It
can also be found in Hebrew as a diminutive formation of nouns and adjectives:
keleb klablab $apan $panpan zaqan zqanqan $axor $xarxar
dog puppy rabbit bunny beard goatee black dark
qatan qtantan
little tiny
Let ? be a finite alphabet. The language L = {ww | w ? ??} is known to be
trans-regular, therefore no finite-state automaton accepts it. However, the language
Ln = {ww | w ? ??, |w| = n} for some constant n is regular. Recognizing Ln is a finite
approximation of the general problem of recognizing L. The length of the words in
natural languages can in most cases be bounded by some n ? N, hence the amount
of reduplication in natural languages is practically limited. Therefore, the descriptive
power of Ln is sufficient for the amount of reduplication in natural languages (by
Figure 3
FSA for the pattern hitae.
51
Computational Linguistics Volume 32, Number 1
constructing Ln for a small number of different ns). An automaton that accepts Ln can
be constructed by listing a path for each accepted string (since ? and n are finite, the
number of words in Ln is finite). The main drawback of such an automaton is the
growth in its size as |?| and n increase: The number of strings in Ln is |?|n. Thus, finite-
state techniques can account for limited reduplication, but the resulting networks are
space-inefficient.
As a final, non-linguistic, motivating example, consider the problem of n-bit incre-
mentation, introduced by Kornai (1996).
Example 4
The goal of this example is to construct a transducer over ? = {0, 1} whose input is a
32 bit binary number and whose output is the result of adding 1 to the input. A
transducer that performs addition by 1 on binary numbers has only 5 states and 12 arcs,4
but this transducer is neither sequential nor sequentiable. The problem is that since the
input is scanned left to right but the carry moves right to left, the output of the first bit
has to be delayed, possibly even until the last input bit is scanned. Thus, for an n-bit
binary incrementor, 2n disjunctions have to be considered, and therefore a minimized
transducer has to assign a separate state to each combination of bits, resulting in 2n
states and a similar number of transitions.
In this work we propose a novel FS model which facilitates the expression of
medium-distance dependencies such as interdigitation and reduplication in an efficient
way. Our main motivation is theoretical, i.e., reducing the complexity of the number
of states and arcs in the networks; we show that these theoretical contributions result
in practical improvements. In Section 3 we define the model formally, show that it is
equivalent to FSAs and define many closure properties directly.5 We then (Section 4)
define a regular expression language for denoting FSRAs. In Section 5 we provide dedi-
cated regular expression operators for some non-concatenative phenomena and exem-
plify the usefulness of the model by efficiently accounting for the motivating examples.
In Section 6 we extend FSRAs to transducers. The model is evaluated through an actual
implementation in Section 7. We conclude with suggestions for future research.
2. Related Work
In spite of the common view that FS technology is in general inadequate for describing
non-concatenative processes, several works address the above-mentioned problems in
various ways. We summarize existing approaches in this section.
Several works examine the applicability of traditional two-level systems for imple-
menting non-concatenative morphology. Two-Level Morphology was used by Kataja
and Koskenniemi (1988) to create a rule system for phonological and morphophonolog-
ical alternations in Akkadian, accounting for word inflection and regular verbal deriva-
tion. As this solution effectively defines lexical representations of word-forms, its main
disadvantage is that the final network is the na??ve one, suffering from the space com-
plexity problems discussed above. Lavie et al (1988) examine the applicability of Two-
4 A complete explanation of the construction can be found in http://www.xrce.xerox.com/competencies/
content-analysis/fsCompiler/fsexamples.html#Add1.
5 Many of the formal proofs and constructions, especially the ones that are similar to the case of standard
FSAs, are suppressed; see Cohen-Sygal (2004) for the complete proofs and constructions.
52
Cohen-Sygal and Wintner Non-Concatenative Morphology
Level Morphology to the description of Hebrew Morphology, and in particular to verb
inflection. Their lexicon consists of three parts: verb primary bases (the past tense, third
person, singular, masculine), verb prefixes, and verb suffixes. They attempt to describe
Hebrew verb inflection as a concatenation of prefix+base+suffix, implementable by the
Two-Level model. However, they conclude that ?The Two-Level rules are not the natural
way to describe . . . verb inflection process. The only alternative choice . . . is to keep all
bases . . . it seems wasteful to save all the secondary bases of verbs of the same pattern.?
Other works deal with non-concatenative morphology by extending ordinary FSAs
without extending their expressivity. The traditional two-level model of Koskenniemi
(1983) is expanded into n-tape automata by Kiraz (2000), following the insight of Kay
(1987) and Kataja and Koskenniemi (1988). The idea is to use more than two levels of
expression: The surface level employs one representation, but the lexical form employs
multiple representations (e.g., root, pattern) and therefore can be divided into different
levels, one for each representation. Elements that are separated on the surface (such as
the root?s consonants) are adjacent on a particular lexical level. For example, to describe
circumfixation using this model, a 4-tape automaton of the form ?surface, PR pattern,
circumfix, stem? is constructed, so that each word is represented by 4 levels. The surface
level represents the final form of the word. The PR pattern is the pattern in which
the stem and the circumfix are combined (P represents the circumfix?s position and
R the root letter?s position), e.g., PRRRP. The circumfix and stem levels represent the
circumfix and the stem respectively.
For example, combining the Hebrew stem pqd with the circumfix ht-ut will have
the 4-level representation shown in Figure 4. Notice that the symbols representing the
circumfix in the PR pattern level (i.e., the occurrences of the symbol ?P?), the circumfix
symbols in the circumfix level, and the circumfix symbols in the surface level are located
in correlating places in the different levels. The same holds for the stem symbols. In
this way, it is clear which symbols of the surface word belong to the circumfix, which
belong to the stem, and how they combine together to create the final form of the word.
The 4-tape automaton of Figure 5 accepts all the combinations created by circumfixing
roots with the three circumfixes of Example 1. Each arc is attributed with a quadruplet,
consisting of four correlating symbols in the four levels. Notice that as in FSAs, the paths
encoding the roots are duplicated for each circumfix, so that this automaton is as space-
inefficient as ordinary FSAs. Kiraz (2000) does not discuss the space complexity of this
model, but the number of states still seems to increase with the number of roots and
patterns. Moreover, the n-tape model requires specification of dependencies between
symbols in different levels, which may be non-trivial.
Walther (2000a) suggests a solution for describing natural language reduplication
using finite-state methods. The idea is to enrich finite-state automata with three new
operations: repeat, skip, and self loops. Repeat arcs allow moving backwards within a
string and thus repeat a part of it (to model reduplication). Skip arcs allow moving
forwards in a string while suppressing the spell out of some of its letters; self loop arcs
model infixation. In Walther (2000b), the above technique is used to describe Temiar
Figure 4
4-tape representation for the Hebrew word htpqdut.
53
Computational Linguistics Volume 32, Number 1
Figure 5
4-tape automaton for circumfixation example.
reduplication, but no complexity analysis of the model is given. Moreover, this tech-
nique does not seem to be able to describe interdigitation.
Beesley and Karttunen (2000) describe a technique, called compile-replace, for
constructing FSTs, which involves reapplying the regular-expression compiler to its
own output. The compile-replace algorithm facilitates a compact definition of non-
concatenative morphological processes, but since such expressions compile to the na??ve
networks, no space is saved. Furthermore, this is a compile-time mechanism rather than
a theoretical and mathematically founded solution.
Other works extend the FS model by enabling some sort of context-sensitivity. Blank
(1985, 1989) presents a model, called Register Vector Grammar, introducing context-
sensitivity by representing the states and transitions of finite-state automata as ternary-
valued vectors, which need not be fully specified. No formal properties of this model are
discussed. In a similar vein, Kornai (1996) introduces vectorized finite-state automata,
where both the states and the transitions are represented by vectors of elements of
a partially ordered set. The vectors are manipulated by operations of unification and
overwriting. The vectors need not be fully determined, as some of the elements can
be unknown (free). In this way information can be moved through the transitions by
the overwriting operation and traversing these transitions can be sanctioned through
the unification operation. As one of the examples of the advantages of this model,
Kornai (1996) shows it can efficiently solve the problem of 32-bit binary incrementor
(Example 4). Using vectorized finite-state automata, a 32-bit incrementor is constructed
where first, using overwriting, the input is scanned and stored in the vectors, and
then, using unification, the result is calculated where the carry can be computed from
right to left. We return to this example in example 6, where we show how our own
model can solve it efficiently. The formalism presented by Kornai (1996) allows a
significant reduction in the network size, but its main disadvantage lies in the fact
that it significantly deviates from the standard methodology of developing finite-state
devices, and integration of vectorized automata with standard ones remains a challenge.
Moreover, it is unclear how, for a given problem, the corresponding network should be
constructed: Programming with vectorized automata seems to be unnatural, and no
regular expression language is provided for them.
A more general approach to the design of finite-state machines is introduced by
Mohri, Pereira, and Riley (2000). They introduce an object-oriented library for manipu-
54
Cohen-Sygal and Wintner Non-Concatenative Morphology
lating finite-state devices that is based on the algebraic concepts of rational power series
and semirings. This approach facilitates a high degree of generality as the algorithms
are defined for the general algebraic notions, which can then be specialized according
to the needs of the user. They exemplify the usefulness of this library by showing how to
specialize it for the manipulation of weighted automata and transducers. Our work can
be seen as another specialization of this general approach, tailored for ideally dealing
with our motivating examples.
Several works introduce the notion of registers, whether for solving similar prob-
lems or motivated by different considerations. Krauwer and des Tombe (1981) refer
to transducers with a finite number of registers when comparing transducers and
context free grammars with respect to their capabilities to describe languages. They
sketch a proof showing that such transducers are equivalent to ordinary finite-state
transducers. However, they never formally define the model and do not discuss its
ability to efficiently implement non-concatenative natural languages phenomena. More-
over, they do not show how the closure properties can be implemented directly on
these registered transducers, and do not provide any regular language denoting such
transducers.
Motivated by different considerations, Kaminski and Francez (1994) present a com-
putational model which extends finite state automata to the case of infinite alphabets.
This model is limited to recognizing only regular languages over infinite alphabets
while maintaining closure under Kleene star and boolean operations, with the excep-
tion of closure under complementation. The familiar automaton is augmented with
registers, used to store alphabet symbols, whose number is fixed for each automa-
ton and can vary from one automaton to another. The model is designed to deal
with infinite alphabets, and therefore it cannot distinguish between different symbols;
it can identify different patterns but cannot distinguish between different symbols
in the pattern as is often needed in natural languages. Our solution is reminiscent
of Kaminski and Francez (1994) in the sense that it augments finite-state automata
with finite memory (registers) in a restricted way, but we avoid the above-mentioned
problem. In addition, our model supports a register alphabet that differs from the
language alphabet, allowing the information stored in the registers to be more mean-
ingful. Moreover, our transition relation is a more simplified extension of the stan-
dard one in FSAs, rendering our model a conservative extension of standard FSAs
and allowing simple integration of existing networks with networks based on our
model.
Finally, Beesley (1998) directly addresses medium-distance dependencies between
separated morphemes in words. He proposes a method, called flag diacritics, which adds
features to symbols in regular expressions to enforce dependencies between separated
parts of a string. The dependencies are forced by different kinds of unification actions.
In this way, a small amount of finite memory is added, keeping the total size of the
network relatively small. Unfortunately, this method is not formally defined, nor are
its mathematical and computational properties proved. Furthermore, flag diacritics are
manipulated at the level of the extended regular expressions, although it is clear that
they are compiled into additional memory and operators in the networks themselves.
The presentations of Beesley (1998) and Beesley and Karttunen (2003) do not explicate
the implementation of such operators and do not provide an analysis of their com-
plexity. Our approach is similar in spirit, but we provide a complete mathematical and
computational analysis of such extended networks, including a proof that the model
is indeed regular and constructions of the main closure properties. We also provide
dedicated regular expression operations for non-concatenative processes and show
55
Computational Linguistics Volume 32, Number 1
how they are compiled into extended networks, thereby accounting for the motivating
examples.
3. Finite-state Registered Automata
We define a new model, finite-state registered automata (FSRA), aimed at facilitating
the expression of various non-concatenative morphological phenomena in an efficient
way. The model augments finite-state automata with finite memory (registers) in a
restricted way that saves space but does not add expressivity. The number of registers
is finite, usually small, and eliminates the need to duplicate paths as it enables the
automaton to ?remember? a finite number of symbols. In addition to being associated
with an alphabet symbol, each arc is also associated with an action on the registers.
There are two kinds of actions, read and write. The read action, denoted R, allows
traversing an arc only if a designated register contains a specific symbol. The write
action, denoted W, allows traversing an arc while writing a specific symbol into a
designated register. In this section we define FSRAs and show that they are equivalent
to standard FSAs (Section 3.1). We then directly define several closure operations over
FSRAs (Section 3.2) and provide some optimizations in Section 3.3. We conclude this
section with a discussion of minimization (Section 3.4).
3.1 Definitions and Examples
Definition
A finite-state registered automaton (FSRA) is a tuple A = ?Q, q0,?,?, n, ?, F?, where:
 Q is a finite set of states.
 q0 ? Q is the initial state.
 ? is a finite alphabet (the language alphabet).
 n ? N (indicating the number of registers).
 ? is a finite alphabet including the symbol ?#? (the registers alphabet).
We use meta-variables ui, vi to range over ? and u, v to range over ?n.
 The initial content of the registers is #n, meaning that the initial value
of all the registers is ?empty?.
 ? ? Q ? ? ? {} ? {R, W} ? {0, 1, 2, . . . , n ? 1} ? ?? Q is the transition
relation. The intuitive meaning of ? is as follows:
? (s,?, R, i,?, t) ? ? where i > 0 implies that if A is in state s, the input
symbol is ?, and the content of the i-th register is ?, then A may
enter state t.
? (s,?, W, i,?, t) ? ? where i > 0 implies that if A is in state s and the
input symbol is ?, then the content of the i-th register is changed
into ? (overwriting whatever was there before) and A may enter
state t.
? (s,?, R, 0, #, t) implies that if A is in state s and the input symbol is ?,
then A may enter state t. Notice that the content of register number 0
is always #. We use the shorthand notation (s,?, t) for such transitions.
 F ? Q is the set of final states.
56
Cohen-Sygal and Wintner Non-Concatenative Morphology
Definition
A configuration of A is a pair (q, u), where q ? Q and u ? ?n (q is the current state
and u represents the registers content). The set of all configurations of A is denoted by
Q c. The pair qc0 = (q0, #
n) is called the initial configuration, and configurations with the
first component in F are called final configurations. The set of final configurations is
denoted by Fc.
Definition
Let u = u0u1 . . . un?1 and v = v0v1 . . . vn?1. Given a symbol ? ? ? ? {} and an FSRA
A, we say that a configuration (s, u) produces a configuration (t, v), denoted (s, u) ?,A
(t, v), iff either one of the following holds:
 There exists i, 0 ? i ? n ? 1, and there exists ? ? ?, such that (s,?, R, i,?, t) ?
? and u = v and ui = vi = ?; or
 There exists i, 0 ? i ? n ? 1, and there exists ? ? ?, such that (s,?, W, i,?, t) ?
? and for all k, k ? {0, 1, ..., n ? 1}, such that k = i, uk = vk and vi = ?.
Informally, a configuration c1 produces a configuration c2 iff the automaton can
move from c1 to c2 when scanning the input ? (or without any input, when ? = ) in one
step. If the register operation is R, then the contents of the registers in the two configura-
tions must be equal, and in particular the contents of the designated register in the two
configurations should be the expected symbol (?). If the register operation is W, then the
contents of the registers in the two configurations is equal except for the designated reg-
ister, whose contents in the produced configuration should be the expected symbol (?).
Definition
A run of A on w is a sequence of configurations c0, ..., cr such that c0 = qc0, cr ? Fc, and
for every k, 1 ? k ? r, ck?1 ?k,A ck and w = ?1...?r. An FSRA A accepts a word w if
there exists a run of A on w. Notice that |w| may be less than r since some of the ?i may
be . The language recognized by an FSRA A, denoted by L(A), is the set of words over
?? accepted by A.
Example 5
Consider again example 1. We construct an efficient FSRA accepting all and only the
possible combinations of stems and circumfixes. If the number of stems is r, we define
an FSRA A = ?Q, q0,?,?, 2, ?, {qf}? where:
 Q = {q0, q1, . . . , q2r+2, qf}
 ? = {a, b, c, . . . , z, ht, ut}
 ? = {htut, ha, m, #}
 ? = {(q0, ht, W, 1, htut, q1), (q0, h, W, 1, ha, q1)}
?
{(q0, m, W, 1, m, q1), (q2r+2, ut, R, 1, htut, qf )}
?
{(q2r+2, a, R, 1, ha, qf ), (q2r+2,, R, 1, m, qf )}
?
{(q1,?1, qi), (qi,?2, qi+1), (qi+1,?3, q2r+2) | 2 ? i ? 2r and
?1?2?3 is the i-th stem}.
57
Computational Linguistics Volume 32, Number 1
This automaton is shown in Figure 6. The number of its states is 2r + 4 (like the FSA
of Figure 2), that is, O(r), and in particular independent of the number of circumfixes.
The number of arcs is also reduced from O(r ? p), where p indicates the number of
circumfixes, to O(r + p).
Example 6
Consider again example 2. The FSRA of Figure 7 also accepts the same language. This
automaton has seven states and will have seven states for any number of roots. The
number of arcs is also reduced to 3r + 3.
Next, we show that finite-state registered automata and standard finite state au-
tomata recognize the same class of languages. Trivially, every finite-state automaton
has an equivalent FSRA: Every FSA is also an FSRA since every transition (s,?, t) in an
FSRA is a shorthand notation for (s,?, R, 0, #, t). The other direction is also simple.
Theorem 1
Every FSRA has an equivalent finite-state automaton.
We prove this by constructing an equivalent FSA to a given FSRA. The construction is
based on the fact that in FSRAs the number of registers is finite, as are the sets ? and
Q, the register alphabet and states, respectively. Hence the number of configurations is
finite. The FSA?s states are the configurations of the FSRA, and the transition function
simulates the ?produces? relation. Notice that this relation holds between configurations
depending on ? only, similarly to the transition function in an FSA. The constructed
FSA is non-deterministic, with possible -moves. The formal proof is suppressed.
The number of configurations in A is |Q| ? |?|n, hence the growth in the number of
states when constructing A? from A might be in the worst case exponential in the num-
ber of registers. In other words, the move from FSAs to FSRAs can yield an exponential
reduction in the size of the network. As we show below, the reduction in the number of
states can be even more dramatic.
The FSRA model defined above allows only one register operation on each tran-
sition. We extend it to allow up to k register operations on each transition, where k
is determined for each automaton separately. The register operations are defined as a
sequence (rather than a set), in order to allow more than one operation on the same
Figure 6
FSRA for circumfixation.
58
Cohen-Sygal and Wintner Non-Concatenative Morphology
Figure 7
FSRA for the pattern hitae.
register over one transition. This extension allows further reduction of the network size
for some automata as well as other advantages that will be discussed presently.
Definition
An order-k finite-state registered automaton (FSRA-k) is a tuple A = ?Q, q0,?,?, n,
k, ?, F?, where:
 Q, q0,?,?, n, F and the initial content of the registers are as before.
 k ? N (indicating the maximum number of register operations allowed
on each arc).
 Let Actions?n = {R, W} ? {0, 1, 2, . . . , n ? 1} ? ?. Then
? ? Q ? ? ? {} ?
?
?
k
?
j=1
{
?a1, ..., aj? | for all i, 1 ? i ? j, ai ? Actions?n
}
?
?? Q
is the transition relation. ? is extended to allow each transition to be
associated with a series of up to k operations on the registers. Each
operation has the same meaning as before.
The register operations are executed in the order in which they are specified. Thus,
(s,?, ?a1, ..., ai?, t) ? ? where i ? k implies that if A is in state s, the input symbol is ? and
all the register operations a1, ..., ai are executed successfully, then A may enter state t.
Definition
Given a ? Actions?n we define a relation over ?n, denoted u a v for u, v ? ?n. We define
u a v where u = u0 . . . un?1 and v = v0 . . . vn?1 iff the following holds:
 if a = (R, i,?) for some i, 0 ? i ? n ? 1 and for some ? ? ?, then u = v
and ui = vi = ?.
 if a = (W, i,?) for some i, 0 ? i ? n ? 1 and for some ? ? ?, then for all
k ? {0, 1, . . . , n ? 1} such that k = i, uk = vk and vi = ?.
This relation is extended to series over Actions?n . Given a series ?a1, ..., ap? ? (Actions?n )p
where p ? N, we define a relation over ?n denoted u ?a1,...,ap? v for u, v ? ?n. We
define u ?a1,...,ap? v iff the following holds:
 if p = 1, then u a1 v.
 if p > 1, then there exists w ? ?n such that u a1 w and w ?a2,...,ap? v.
59
Computational Linguistics Volume 32, Number 1
Definition
Let u, v ? ?n. Given a symbol ? ? ? ? {} and an FSRA-k A, we say that a configuration
(s, u) produces a configuration (t, v), denoted (s, u) ?,A (t, v), iff there exist ?a1, . . . , ap? ?
(Actions?n )
p for some p ? N such that (s,?, ?a1, . . . , ap?, t) ? ? and u ?a1,...,ap? v.
Definition
A run of A on w is a sequence of configurations c0, ..., cr such that c0 = qc0, cr ? Fc, and for
every l, 1 ? l ? r, cl?1 ?l,A cl and w = ?1...?r. An FSRA-k A accepts a word w if there
exists a run of A on w. The language recognized by an FSRA-k A, denoted by L(A), is
the set of words over ?? accepted by A.
Example 7
Consider the Arabic nouns qamar (moon), kitaab (book), $ams (sun), and daftar (note-
book). The definite article in Arabic is the prefix al, which is realized as al when pre-
ceding most consonants; however, the ?l? of the prefix assimilates to the first consonant
of the noun when the latter is ?d?, ?$?, etc. Furthermore, Arabic distinguishes between
definite and indefinite case markers. For example, nominative case is realized as the
suffix u on definite nouns, un on indefinite nouns. Examples of the different forms of
Arabic nouns are:
word nominative definite nominative indefinite
qamar ?alqamaru qamarun
kitaab ?alkitaabu kitaabun
$ams ?a$$amsu $amsun
daftar ?addaftaru daftarun
The FSRA-2 of Figure 8 accepts all the nominative definite and indefinite forms of
the above nouns. In order to account for the assimilation, register 2 stores information
about the actual form of the definite article. Furthermore, to ensure that definite nouns
occur with the correct case ending, register 1 stores information of whether or not a
definite article was seen.
Figure 8
FSRA-2 for Arabic nominative definite and indefinite nouns.
60
Cohen-Sygal and Wintner Non-Concatenative Morphology
FSRA-k and FSRAs recognize the same class of languages. Trivially, every FSRA has
an equivalent FSRA-k: Every FSRA is an FSRA-k for k = 1. The other direction is also
simple.
Theorem 2
Every FSRA-k has an equivalent FSRA.
We show how to construct an equivalent FSRA (or FSRA-1) A? given an FSRA-k A. Each
transition in A is replaced by a series of transitions in A?, each of which performs one
operation on the registers. The first transition in the series deals with the new input
symbol and the rest are -transitions. This construction requires additional states to
enable the addition of transitions. Each transition in A that is replaced requires the
addition of as many states as the number of register operations performed on this
transition minus one. The formal construction is suppressed.
In what follows, the term FSRA will be used to denote FSRA-k. Simple FRSA will
be referred to as FSRA-1. For the sake of emphasis, however, the term FSRA-k will still
be used in some cases.
FSRA is a very space-efficient finite-state device. The next theorem shows how
ordinary finite-state automata can be encoded efficiently by the FSRA-2 model. Given
a finite-state automaton A, an equivalent FSRA-2 A? is constructed. A? has three states
and two registers (in fact, only one register is used since register number 0 is never
addressed). One state functions as a representative for the final states in A, another
one functions as a representative for the non-final states in A, and the third as an
initial state. The register alphabet consists of the states of A and the symbol ?#?. Each
arc in A has an equivalent arc in A? with two register operations. The first reads
the current state of A from the register and the second writes the new state into the
register. If the source state of a transition in A is a final state, then the source state of
the corresponding transition in A? will be the final states representative; if the source
state of a transition in A is a non-final state, then the source state of the corresponding
transition in A? will be the non-final states representative. The same holds also for the
target states. The purpose of the initial state is to write the start state of A into the
register. In this way A? simulates the behavior of A. Notice that the number of arcs in A?
equals the number of arcs in A plus one, i.e., while FSRAs can dramatically reduce the
number of states, compared to standard FSAs, a reduction in the number of arcs is not
guaranteed.
Theorem 3
Every finite-state automaton has an equivalent FSRA-2 with three states and two
registers.
Proof 1
Let A = ?Q, q0,?, ?, F? be an FSA and let f : Q ?
{
qf, qnf
}
be a total function defined
by
f (q) =
{
qf q ? F
qnf q /? F
61
Computational Linguistics Volume 32, Number 1
Construct an FSRA-2 A? = ?Q?, q?0,??,??, 2, 2, ??, F??, where:
 Q? = {q?0, qnf, qf}. q
?
0 is the initial state, qf is the final states representative,
and qnf is the non-final states representative
 ?? = ?
 ? = Q ? {#}
 F? = {qf}
 ?? = {(f (s),?, ?(R, 1, s), (W, 1, t)?, f (t)) | (s,?, t) ? ?}
?
{(q?0,, ?(W, 1, q0)?, f (q0))}
The formal proof that L(A) = L(A?) is suppressed. 
3.2 Closure Properties
The equivalence shown in the previous section between the classes of languages recog-
nized by finite-state automata and finite-state registered automata immediately implies
that finite-state registered automata maintain the closure properties of regular lan-
guages. Applying the regular operations to finite-state registered automata can be easily
done by converting them first into finite-state automata. However, as shown above,
such a conversion may result in an exponential increase in the size of the automaton,
invalidating the advantages of this model. Therefore, we show how some of these
operations can be defined directly for FSRAs. The constructions are mostly based on
the standard constructions for FSAs with some essential modifications. In what follows,
let A1 = ?Q1, q10,?1,?1, n1, k1, ?1, F1? and A2 = ?Q2, q20,?2,?2, n2, k2, ?2, F2? be finite-state
registered automata.
3.2.1 Union. Two FSRAs, A1, A2, are unioned into an FSRA A in the same way as in FSAs:
by adding a new initial state and connecting it with -arcs to each of the (former) initial
states of A1, A2. The number of registers and the maximal number of register operations
per arc in A is the maximum of the corresponding values in A1, A2. Notice that in
any specific run of A, the computation goes through just one of the original automata;
therefore the same set of registers can be used for strings of L(A1) or L(A2) as needed.
3.2.2 Concatenation. We show two different constructions of an FSRA A = ?Q, q0,?,
?, n, k, ?, F? to recognize L(A1) ? L(A2). Concatenation in finite-state automata is achieved
by leaving only the accepting states of the second automaton as accepting states and
adding an -arc from every accepting state of the first automaton to the initial state of
the second automaton. Doing just this in FSRA is insufficient because using the same
registers might cause undesired effects: The result might be affected by the content left
in the registers after dealing with a substring from L(A1). Thus, this basic construction
is used with care. The first alternative is to employ more registers in the FSRA. In this
way when dealing with a substring from L(A1) the first n1 registers are used, and when
moving to deal with a substring from L(A2) the next n2 registers are used. The second
alternative is to use additional register operations that clear the content of the registers
before handling the next substring from L(A2). This solution may be less intuitive but
will be instrumental for Kleene closure below.
62
Cohen-Sygal and Wintner Non-Concatenative Morphology
3.2.3 Kleene Closure. The construction is based on the concatenation construction.
Notice that it cannot be based on the first alternative (adding registers) due to the fact
that the number of iterations in Kleene star is not limited, and therefore the number of
registers needed cannot be bounded. Thus, the second alternative is used: Register op-
erations are added to delete the content of registers. The construction is done by turning
the initial state into a final one (if it is not already final) and connecting each of the final
states to the initial state with an -arc that is associated with a register operation that
deletes the contents of the registers, leaving them ready to handle the next substring.
3.2.4 Intersection. For the intersection construction, assume that A1 and A2 are -free
(we show an algorithm for removing -arcs in Section 3.3.1). The following construction
simulates the runs of A1 and A2 simultaneously. It is based on the basic construction for
intersection of finite-state automata, augmented by a simulation of the registers and
their behavior. Each transition is associated with two sequences of operations on the
registers, one for each automaton. The number of the registers is the sum of the number
of registers in the two automata. In the intersection automaton the first n1 registers
are designated to simulate the behavior of the registers of A1 and the next n2 registers
simulate the behavior of A2. In this way a word can be accepted by the intersection au-
tomaton iff it can be accepted by each one of the automata separately. Notice that register
operations from ?1 and ?2 cannot be associated with the same register. This guarantees
that no information is lost during the simulation of the two intersected automata.
3.2.5 Complementation. Ordinary FSAs are trivially closed under complementation.
However, given an FSA A whose language is L(A), the minimal FSA recognizing the
complement of L(A) can be exponentially large. More precisely, for any integer n > 2,
there exists a non-deterministic finite-state automaton (NFA) with n states A, such that
any NFA that accepts the complement of L(A) needs at least 2n?2 states (Holzer and
Kutrib 2002). We have no reason to believe that FSRAs will demonstrate a different
behavior; therefore, we maintain that in the worst case, the best approach for com-
plementing an FSRA would be to convert it into FSA and complement the latter. We
therefore do not provide a dedicated construction for this operator.
3.3 Optimizations
3.3.1 -removal. An -arc in an FSRA is an arc of the form (s,, ?a?, t) where a is used
as a meta-variable over
(
Actions?n
)+
(i.e., a represents a vector of register operations).
Notice that this kind of arc might occur in an FSRA by its definition. Given an FSRA
that might contain -arcs, an equivalent FSRA without -arcs can be constructed. The
construction is based on the algorithm for -removal in finite-state automata, but the
register operations that are associated with the -arc have to be dealt with, and this
requires some care. The resulting FSRA has one more state than the original, and some
additional arcs may be added, too.
The main problem is -loops; while these can be easily removed in standard FSAs,
here such loops can be associated with register operations which must be accounted for.
The number of possible sequences of register operations along an -loop is unbounded,
but it is easy to prove that there are only finitely many equivalence classes of such
sequences: Two sequences are in the same equivalence class if and only if they have
the same effect on the state of the machine; since each machine has a finite number of
configurations (see theorem 1), there are only finitely many such equivalence classes.
Therefore, the basic idea behind the construction is as follows: If there exists an
-path from q1 to q2 with the register operationsa over its arcs, and an arc (q2,?, ?b?, q3)
63
Computational Linguistics Volume 32, Number 1
Figure 9
 removal paradigm.
where ? = , and an -path from q3 to q4 with the register operations c over its arcs,
then the equivalent -free network will include the arcs (q2,?, ?b?, q3), (q1,?, ?a,b?, q3),
(q2,?, ?b,c?, q4), and (q1,?, ?a,b,c?, q3), with all the -arcs removed. This is illustrated
in Figure 9. Notice that if q1 and q2 are the same state, then states q2 and q3 will be
connected by two parallel arcs differing in their associated register operations; the same
holds for states q2 and q4. Similarly, when q3 and q4 are the same state.
In addition to the above changes, special care is needed for the case in which the
empty word is accepted by the original automaton. The formal construction is similar
in spirit to the -removal paradigm in weighted automata (Mohri 2000), where weights
along an -path need to be gathered. Therefore, we suppress the formal construction
and the proof of its correctness.
3.3.2 Optimizing Register Operations. In FSRAs, traversing an arc depends not
only on the input symbol but also on satisfying the series of register operations.
Sometimes, a given series of register operations can never be satisfied, and thus
the arc to which it is attached cannot be traversed. For example, the series of reg-
ister operations ?(W, 1, a), (R, 1, b)? can never be satisfied, hence an arc of the form
(q1,?, ?(W, 1, a), (R, 1, b)?, q2) is redundant. In addition, the constructions of Sections 3.2
and 3.3.1 might result in redundant states and arcs that can never be reached or can
never lead to a final state. Moreover, in many cases a series of register operations can
be minimized into a shorter series with the same effect. For example, the series of
register operations ?(W, 1, a), (R, 1, a), (W, 1, b)? is equal in its effect to the series ?(W, 1, b)?.
Therefore, we show an algorithm for optimizing a given FSRA by minimizing the series
of register operations over its arcs and removing redundant arcs and states.
For a given FSRA A = ?Q, q0,?,?, n, ?, F?, we construct an equivalent FSRA A? =
?Q, q0,?,?, n, ??, F? = Opt(A), such that ?? is created from ? by removing redundant
arcs and by optimizing all the series of register operations. We begin by defining
(
Actions?n
)+
|i as the subset of
(
Actions?n
)+
that consists only of operations over the
i-th register. Define a total function sati :
(
Actions?n
)+
|i ?? {true, false} by:
sati(a) =
{
true if there exist u, v ? ?n such that u a v
false otherwise
64
Cohen-Sygal and Wintner Non-Concatenative Morphology
sati(a) = true iff the series of register operations a is satisfiable, i.e., there exists a
configuration of register contents for which all the operations in the series can be
executed successfully. Determining whether sati(a) = true by exhaustively checking
all the vectors in ?n may be inefficient. Therefore, we show a necessary and sufficient
condition for determining whether sati(a) = true for some a ?
(
Actions?n
)+
|i, which can
be checked efficiently. In addition, this condition will be useful in optimizing the series
of register operations as will be shown later. A series of register operations over the i-th
register is not satisfiable if either one of the following holds:
 A write operation is followed by a read operation expecting a different
value.
 A read operation is immediately followed by a read operation expecting a
different value.
Theorem 4
For all a = ?(op1, i,?1), (op2, i,?2), . . . , (ops, i,?s)? ?
(
Actions?n
)+
|i, sati(a) = false if and
only if either one of the following holds:
1. There exists k, 1 ? k < s, such that opk = W and there exists m, k < m ? s,
such that opm = R, ?k = ?m and for all j, k < j < m, opj = R.
2. There exists k, 1 ? k < s, such that opk = opk+1 = R and ?k = ?k+1.
Notice that if i = 0, then by the definition of FSRAs, all the register operations in
the series are the same operation, which is (R, 0, #); and this operation can never fail.
In addition, if all the operations in the series are write operations, then again, by the
definition of FSRAs, these operations can never fail. If none of the two conditions of the
theorem holds, then the series of register operations is satisfiable.
We now show how to optimize a series of operations over a given register. An
optimized series is defined only over satisfiable series of register operations in the
following way:
 If all the operations are write operations, then leave only the last one (since
it will overwrite all its predecessors).
 If all the operations are read operations, then by theorem 4, they are all the
same operation, and in this case just leave one of them.
 If there are both read and write operations, then distinguish between two
cases:
? If the first operation is a write operation, leave only the last write
operation in the series.
? If the first operation is a read operation, leave the first operation
(which is read) and the last write operation in the series. If the last
write operation writes into the register the same symbol that the
read operation required, then the write is redundant; leave only the
read operation.
65
Computational Linguistics Volume 32, Number 1
Definition
Define a function mini :
(
Actions?n
)+
|i ??
(
Actions?n
)+
|i. Let a = ?(op1, i,?1), . . . , (ops, i,
?s)?. If sati(a) = true then:
 If for all k, 1 ? k ? s, opk = W, define mini(a) = ?(W, i,?s)?.
 If for all k, 1 ? k ? s, opk = R then define mini(a) = ?(R, i,?s)?.
 If there exists m, 1 ? m ? s such that opm = W and if there exists t,
1 ? t ? s, such that opt = R then:
mini(a) =
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?(W, i,?j)? if op1 = W and
for all k, j < k ? s, opk = R
?(R, i,?1), (W, i,?j)? if op1 = R and
for all k, j < k ? s, opk = R and ?1 = ?j
?(R, i,?1)? if op1 = R and if there exists j, 1 ? j ? s,
such that for all k, j < k ? s,
opk = R and ?1 = ?j
The formal proof that mini(a) is the minimal equivalent series of register operations
ofa is suppressed.
We now show how to optimize a series of register operations. Define a function
min :
(
Actions?n
)+ ??
(
Actions?n
)+ ? {null}. For all a ?
(
Actions?n
)+
define min(a) = b
whereb is obtained froma by:
 Each subseriesai ofa, consisting of all the register operations on the i-th
register, is checked for satisfaction. If sati(ai) = false then the arc cannot be
traversed and min(a) = b = null. If sati(ai) = true thenai is replaced ina by
min(ai). Notice that the order of the minimized subseries in the complete
series is unimportant as they operate on different registers.
 If there exists i = 0, such that ai is not empty, then the subseriesa0
consisting only of operations of the form (R, 0, #) is deleted froma.
Finally, given an FSRA A = ?Q, q0,?,?, n, ?, F?, construct an equivalent FSRA A? =
?Q, q0,?,?, n, ??, F? = Opt(A) where
?? =
{(
q1,?, ?min(a)?, q2
)
|
(
q1,?, ?a?, q2
)
? ? and min(a) = null
}
Opt(A) is optimized with respect to register operations.
Like FSAs, FSRAs may have states that can never be reached or can never lead to a
final state. These states (and their connected arcs) can be removed in the same way they
are removed in FSAs. In sum, FSRA optimization is done in two stages:
1. Minimizing the series of register operations over the FSRA transitions.
2. Removing redundant states and arcs.
Notice that stage 1 must be performed before stage 2 as it can result in further re-
duction in the size of the network when performing the second stage. For a given FSRA
A, define OPT(A) as the FSRA obtained from Opt(A) by removing all the redundant
66
Cohen-Sygal and Wintner Non-Concatenative Morphology
states and transitions. An FSRA A is optimized if OPT(A) = A (notice that OPT(A) is
unique, i.e., if B = OPT(A) and C = OPT(A), then B = C).
3.4 FSRA Minimization
FSRAs can be minimized along three different axes: states, arcs, and registers. Reduc-
tion in the number of registers can always be achieved by converting an FSRA to an
FSA (Section 3.1), eliminating registers altogether. Since FSRAs are inherently non-
deterministic (see the discussion of linearization below), their minimization is related
to the problem of non-deterministic finite-state automata (NFA) minimization, which
is known to be NP-hard.6 However, while FSRA arc minimization is NP-hard, FSRA
state minimization is different. Recall that in theorem 3 we have shown that any FSA
has an equivalent FSRA-2 with 3 states and 2 registers. It thus follows that any FSRA
has an equivalent FSRA-2 with 3 states (simply convert the FSRA to an FSA and then
convert it to an FSRA-2 with 3 states). Notice that minimizing an FSRA in terms of states
or registers can significantly increase the number of arcs. As many implementations of
finite-state devices use space that is a function of the number of arcs, the benefit that lies
in such minimization is limited. Therefore, a different minimization function, involving
all the three axes, is called for. We do not address this problem in this work. As for
arc minimization, we cite the following theorem. As its proof is most similar to the
corresponding proof on NFA, we suppress it.
Theorem 5
FSRA arc minimization is NP-hard.
The main advantage of finite-state devices is their linear recognition time. In finite-
state automata, this is achieved by determinizing the network, ensuring that the
transition relation is a function. In FSRAs, in contrast, a functional transition re-
lation does not guarantee linear recognition time, since multiple possible transi-
tions can exist for a given state and a given input symbol. For example, given an
FSRA A = ?Q, q0,?,?, n, k, ?, F?, and some q, q1, q2 ? Q and ? ? ?, two arcs such as
(q,?, ?(W, 1, a)?, q1), (q,?, ?(W, 1, b)?, q2) ? ? do not hamper the functionality of the FSRA
transition relation. However, they do imply that for the state q and for the same input
symbol (?), more than one possible arc can be traversed. We use deterministic to denote
FSRAs in which the transition relation is a function, and a new term, linearized, is used
to denote FSRAs for which linear recognition time is guaranteed.
Generally, a FSRA is linearized if it is optimized, -free, and given a current state
and a new input symbol, and at most one transition can be traversed. Thus, if the
transition relation includes two arcs of the form (q,?, ?a?, q1), (q,?, ?b?, q2), then a and
b must be a contradicting series of register operations. Two series of register operations
are contradicting if at most one of them is satisfiable. Since the FSRA is optimized, each
series of register operations is a concatenation of subseries, each operating on a differ-
ent register; and the subseries operating on the i-th register must be either empty or
?(W, i,?)? or ?(R, i,?)? or ?(R, i,?1), (W, i,?2)?. ?(W, i,?)? contradicts neither ?(R, i,?)? nor
?(R, i,?1), (W, i,?2)?. ?(R, i,?)? and ?(R, i,?1), (W, i,?2)? are contradicting only if ? = ?1.
6 While this theorem is a part of folklore, we were unable to find a formal proof. We explicitly prove this
theorem in Cohen-Sygal (2004).
67
Computational Linguistics Volume 32, Number 1
Definition
An FSRA A = ?Q, q0,?,?, n, k, ?, F?, is linearized if it is optimized, -free, and for
all (q,?, ?a?, q1), (q,?, ?b?, q2) ? ? such that ?a? = ?b?, where ?a? = ?(op11, i11,?11), . . . , (op1k ,
i1k ,?
1
k )? and ?b? = ?(op21, i21,?21), . . . , (op2m, i2m,?2m)? , there exists j1, 1 ? j1 ? k and there
exists j2, 1 ? j2 ? m, such that op1j1 = op
2
j2
= R, i1j1 = i
2
j2
and ?1j1 = ?
2
j2
.
A na??ve algorithm for converting a given FSRA into an equivalent linearized one
is to convert it to an FSA and then determinize it. In the worst case, this results in
an exponential increase in the network size. As the following theorem shows, FSRA
linearization is NP-complete.
Theorem 6
FSRA linearization is NP-complete.
Proof 2
Evidently, given an FSRA A, it can be verified in polynomial time that A is linearized.
Therefore, FSRA linearization is in NP.
Let ? be a CNF formula with m clauses and n variables. Construct an FSRA A such
that L(A) = {} if ? is satisfiable, otherwise L(A) = ?.
Let x1, . . . , xn be the variables of ?. Define A = ?Q, q0,?,?, n, 1, ?, F?, such that:
 Q = {q0, q1, . . . , qn+m}
 F = {qn+m}
 ? is irrelevant (choose any ?).
 ? = {T, F}.
? ? = {(qi?1,, (W, i, T), qi) | 1 ? i ? n} ? {(qi?1,, (W, i, F), qi) | 1 ? i ? n}
?
{(qn+i?1,, (R, j, T), qn+i) | 1 ? i ? m and xj occurs in the i-th clause}
?
{(qn+i?1,, (R, j, F), qn+i) | 1 ? i ? m and xj occurs in the i-th clause}
Notice that each path in A is of length m + n. The first n arcs in the path
write an assignment into the registers, then it is possible to traverse the
remaining m arcs in the path only if the assignment stored into the
registers satisfies ?.
For example, for the CNF formula (x1 ? x2 ? x5) ? (x1 ? x2) ? (x3 ? x4 ? x5), the FSRA of
Figure 10 is constructed. Observe that the number of states and arcs in this FSRA is
O(mn). Now, linearize A into an FSRA A? and assume this can be done in polynomial
time. By the definition of linearized FSRA, A? does not contain -arcs. Therefore,  ?
L(A?) iff the initial state of A? is also a final one. Hence, ? is satisfiable iff the initial state
of A? is also a final one. 
4. A Regular Expression Language for FSRAs
Regular expressions are a formal way for defining regular languages. Regular language
operations construct regular expressions in a convenient way. Several toolboxes (soft-
ware packages) provide extended regular expression description languages and compil-
68
Cohen-Sygal and Wintner Non-Concatenative Morphology
Figure 10
FSRA for a given CNF formula.
ers of the expressions to finite-state devices, automata, and transducers (see Section 1).
We provide a regular expression language for constructing FSRAs, the denotations
of whose expressions are FSRAs. In the following discussion we assume the regular
expression syntax of XFST (Beesley and Karttunen 2003) for basic expressions.7
Definition
Let Actions?n = {R, W} ? {0, 1, 2, . . . , n ? 1} ? ?, where n is the number of registers and
? is the register alphabet. If R is a regular expression and a ?
(
Actions?n
)+
is a series of
register operations, then the following are also regular expressions: a  R, a  R, a  R,
anda   R.
We now define the denotation of each of the above expressions. Let R be a regular
expression whose denotation is the FSRA A, and let a ?
(
Actions?n
)+
. The denotation
of a  R is an FSRA A? obtained from A by adding a new node, q, which becomes the
initial node of A?, and an arc from q to the initial node of A; this arc is labeled by 
and associated with a. Notice that in the regular expression a  R, R and a can contain
operations on joint registers. In some cases, one would like to distinguish between the
registers used in a and in R. Usually, it is up to the user to correctly manipulate the
usage of registers, but in some cases automatic distinction seems desirable. For example,
if R includes a circumfix operator (see below), its corresponding FSRA will contain
register operations created automatically by the operator. Instead of remembering that
circumfixation always uses register 1, one can simply distinguish between the registers
of a and R via the a   R operator. This operator has the same general effect as the
previous one, but the transition relation in its FSRA uses fresh registers that are added
to the machine.
In a similar way, the operators a  R and a  R are translated into networks. The
difference between these operators and the previous ones is that here, the register
operations in a are executed after traversing all the arcs in the FSRA denoted by R.
Using these additional operators, it is easy to show that every FSRA has a corresponding
regular expression denoting it, by a trivial modification of the construction presented by
Kleene (1956).
Example 8
Consider the case of vowel harmony in Warlpiri (Sproat 1992), where the vowel of
suffixes agrees in certain aspects with the vowel of the stem to which it is attached.
7 In particular, concatenation is denoted by juxtaposition and  is denoted by 0.
69
Computational Linguistics Volume 32, Number 1
A simplified account of the phenomenon is that suffixes come in two varieties, one with
?i? vowels and one with ?u? vowels. Stems whose last vowel is ?i? take suffixes of the
first variety, whereas stems whose last vowel is ?u? or ?a? take the other variety. The
following examples are from Sproat (1992) (citing Nash (1980)):
1. maliki+kil.i+l.i+lki+ji+li
(dog+PROP+ERG+then+me+they)
2. kud. u+kul.u+l.u+lku+ju+lu
(child+PROP+ERG+then+me+they)
3. minija+kul.u+l.u+lku+ju+lu
(cat+PROP+ERG+then+me+they)
An FSRA that accepts the above three words is denoted by the following complex
regular expression:
define LexI [m a l i k i]; % words ending in ?i?
define LexU [k u d u]; % words ending in ?u?
define LexA [m i n i j a]; % words ending in ?a?
! Join all the lexicons and write to register 1 ?u? or ?i?
! according to the stem?s last vowel.
define Stem [<(W,1,i)>  LexI] | [<(W,1,u)>  [LexU | LexA]];
! Traverse the arc only if the scanned symbol is the content of
! register 1.
define V [<(R,1,i)>  i] | [<(R,1,u)>  u];
define PROP [+ k V l V]; % PROP suffix
define ERG [+ l V]; % ERG suffix
define Then [+ l k V]; % suffix indicating ?then?
define Me [+ j V]; % suffix indicating ?me?
define They [+ l V]; % suffix indicating ?they?
! define the whole network
define WarlpiriExample Stem PROP ERG Then Me They;
Register 1 stores the last vowel of the stem, eliminating the need to duplicate paths
for each of the different cases. The lexicon is divided into three separate lexicons
(LexI, LexU, LexA), one for each word ending (?i?, ?u?, or ?a? respectively). The separate
lexicons are joined into one (the variable Stem) and when reading the last letter of
the base word, its type is written into register 1. Then, when suffixing the lexicon
base words, the variable V uses the the content of register 1 to determine which of
the symbols ?i?, ?u? should be scanned and allows traversing the arc only if the correct
symbol is scanned. Note that this solution is applicable independently of the size of the
lexicon, and can handle other suffixes in the same way.
Example 9
Consider again Example 7. The FSRA constructed for Arabic nominative definite and
indefinite nouns can be denoted by the following regular expression:
! Read the definite article (if present).
! Store in register 1 whether the noun is definite or indefinite.
! Store in register 2 the actual form of the definite article.
70
Cohen-Sygal and Wintner Non-Concatenative Morphology
define Prefix [<(W,1,indef)>  0] | [<(W,1,def),(W,2,l)>  ?al] |
[<(W,1,def),(W,2,$)>  ?a$] | [<(W,1,def),(W,2,d)>  ?ad];
! Normal base - definite and indefinite
define Base [ [<(R,2,l)>  0] | [<(R,1,indef)>  0] ]
[ [k i t a a b] | [q a m a r] ];
! Bases beginning with $ - definite and indefinite
define $Base [ [<(R,2,$)>  0] | [<(R,1,indef)>  0] ] [$ a m s];
! Bases beginning with d - definite and indefinite
define dBase [ [<(R,2,d)>  0] | [<(R,1,indef)>  0] ] [d a f t a r];
! Read definite and indefinite suffixes.
define Suffix [<(R,1,def)>  u] | [<(R,1,indef)>  un];
! The complete network.
define ArabicExample Prefix [Base | $Base | dBase] Suffix;
The variable Prefix denotes the arcs connecting the first two states of the FSRA,
in which the definite article (if present) is scanned and information indicating whether
the word is definite or not is saved into register 1. In addition, if the word is definite
then register 2 stores the actual form of the definite article. The lexicon is divided
into several parts: The Base variable denotes nouns that do not trigger assimilation.
Other variables ($Base, dBase) denote nouns that trigger assimilation, where for each
assimilation case, a different lexicon is constructed. Each part of the lexicon deals with
both its definite and indefinite nouns by allowing traversing the arcs only if the register
content is appropriate. The variable Suffix denotes the correct suffix, depending on
whether the noun is definite or indefinite. This is possible using the information that
was stored in register 1 by the variable Prefix.
5. Linguistic Applications
We demonstrated in examples 5 and 6 that FSRAs can model some non-concatenative
phenomena more efficiently than standard finite-state devices. We now introduce new
regular expression operators, accounting for our motivating linguistic phenomena, and
show how expressions using these operators are compiled into the appropriate FSRA.
5.1 Circumfixes
We introduce a dedicated regular expression operator for circumfixation and show how
expressions using this operator are compiled into the appropriate FSRA. The operator
accepts a regular expression, denoting a set of bases, and a set of circumfixes, each
of which is a pair of regular expressions (prefix, suffix). It yields as a result an FSRA
obtained by applying each circumfix to each of the bases. The main purpose of this
operator is to deal with cases in which the circumfixes are pairs of strings, but it is
defined such that the circumfixes can be arbitrary regular expressions.
Definition
Let ? be a finite set such that , {, }, ?, ?,? /? ?. We define the ? operation to be of
the form
R ? {??1?1???2?2? . . . ??m?m?}
71
Computational Linguistics Volume 32, Number 1
where: m ? N is the number of circumfixes; R is a regular expression over ? denoting
the set of bases; and ?i, ?i for 1 ? i ? m are regular expressions over ? denoting the
prefix and suffix of the i-th circumfix, respectively.
Notice that R,?i,?i may denote infinite sets. To define the denotation of this op-
erator, let A?i , A
?
i be the FSRAs denoted by ?i,?i, respectively. The operator yields an
FSRA constructed by concatenating three FSRAs. The first is the FSRA constructed from
the union of the FSRAs A??1 , . . . , A
??
m , where each A
??
i is an FSRA obtained from A
?
i
by adding a new node, q, which becomes the initial node of A??i , and an arc from q
to the initial node of A?i ; this arc is labeled by  and associated with ?(W, 1,?i?i)?
(register 1 is used to store the circumfix). In addition, the register operations of the
FSRA A?i are shifted by one register in order not to cause undesired effects by the
use of register 1. The second FSRA is the FSRA denoted by the regular expression R
(again, with one register shift) and the third is constructed in the same way as the
first one, the only difference being that the FSRAs are those denoted by ?1, . . . ,?m
and the associated register operation is ?(R, 1,?i?i)?. Notice that the concatenation
operation, defined in Section 3.2.2, adjusts the register operations in the FSRAs to be
concatenated, to avoid undesired effects caused by using joint registers. We use this
operation to concatenate the three FSRAs, leaving register 1 unaffected (to handle the
circumfix).
Example 10
Consider the participle-forming combinations in German, e.g., the circumfix ge-t. A
simplified account of the phenomenon is that German verbs in their present form take
an n suffix but in participle form they take the circumfix ge-t. The following examples
are from Sproat (1992):
sa?useln ?rustle? gesa?uselt ?rustled?
bru?sten ?brag? gebru?stet ?bragged?
The FSRA of Figure 11, which accepts the four forms, is denoted by the regular
expression
[s a? u s e l | b r u? s t e] ? {?n??g et?}
This regular expression can be easily extended to accept more German verbs in other
forms. More circumfixation phenomena in other languages such as Indonesian and
Arabic can be modeled in the same way using this operator.
Figure 11
Participle-forming combinations in German.
72
Cohen-Sygal and Wintner Non-Concatenative Morphology
Example 11
Consider again Example 5. The FSRA accepting all the possible combinations of stems
and the Hebrew circumfixes h-a, ht-ut, m- can be denoted by the regular expression
R ? {?ha??htut??m?} where R denotes an FSA accepting the roots.
5.2 Interdigitation
Next, we define a dedicated operator for interdigitation. It accepts a set of regular
expressions, representing a set of roots, and a list of patterns, each of which containing
exactly n slots. It yields as a result an FSRA denoting the set containing all the strings
created by splicing the roots into the slots in the patterns. For example, consider the He-
brew roots r.$.m, p.&.l, p.q.d and the Hebrew patterns hitae, mia, haaa.
The roots are all trilateral, and the patterns have three slots each. Given these two
inputs, the new operator yields an FSRA denoting the set {hitra$em, hitpa&el, hitpaqed,
mir$am, mip&al, mipqad, har$ama, hap&ala, hapqada}.
Definition
Let ? be a finite set such that , {, }, ?, ?,? /? ?. We define the splice operation to be of
the form
{??11,?12, ...,?1n?, ??21,?22, ...,?2n?, ..., ??m1,?m2, ...,?mn?}
?
{??11?12...?1n?1 n+1?, ??21?22...?2n?2 n+1?, ..., ??k1?k2...?kn?k n+1?}
where:
 n ? N is the number of slots (represented by ??) in the patterns into which
the roots letters should be inserted.
 m ? N is the number of roots to be inserted.
 k ? N is the number of patterns.
 ?ij,?ij are regular expressions (including regular expressions denoting
FSRAs).
The left set is a set of roots to be inserted into the slots in the right set of patterns.
For the sake of brevity, ?i and ?i are used as shorthand notations for ?i1?i2...?i(n+1)
and ?i1?i2...?in, respectively.
Consider first the case where ?ij ? ? ? {} for 1 ? i ? m and 1 ? j ? n and ?ij ?
? ? {} for 1 ? i ? k and 1 ? j ? n + 1. In this case the splice operation yields as a result
an FSRA-1 A = ?Q, q0,?,?, 3, ?, F?, such that L(A) = {?j1?i1?j2?i2...?jn?in?j(n+1) | 1 ?
i ? m , 1 ? j ? k}, where:
 Q = {q0, q1, ..., q2n+1}
 F = {q2n+1}
 ? =
(
{?ij| 1 ? i ? m , 1 ? j ? n} ? {?ij| 1 ? i ? k , 1 ? j ? n + 1}
)
\ {}
73
Computational Linguistics Volume 32, Number 1
Figure 12
Interdigitation FSRA ? general.
 ? = {?i| 1 ? i ? k} ? {?i| 1 ? i ? m} ? {#}.
 ? = {(q0,?i1, W, 1,?i, q1)| 1 ? i ? k}
?
{(q1,?i1, W, 2,?i, q2)| 1 ? i ? m}
?
{(q2j?2,?ij, R, 1,?i, q2j?1)| 1 ? i ? k , 2 ? j ? n + 1}
?
{(q2j?1,?ij, R, 2,?i, q2j)| 1 ? i ? m , 2 ? j ? n}
This FSRA is shown in Figure 12. It has 3 registers, where register 1 remembers the
pattern and register 2 remembers the root. Notice that the FSRA will have 3 registers
and 2n + 2 states for any number of roots and patterns. The number of arcs is k ? (n +
1) + m ? n. In the (default) case of trilateral roots, for m roots and k patterns the resulting
machine has a constant number of states and O(k + m) arcs.
In the general case, where ?ij and ?ij can be arbitrary regular expressions, the
construction of the FSRA denoted by this operation is done in the same way as in the
case of circumfixes with two main adjustments. The first is that in this case the final
FSRA is constructed by concatenating 2n + 1 intermediate FSRAs (n FSRAs for the n
parts of the roots and n + 1 FSRAs for the n + 1 parts of the patterns). The second is that
here, 2 registers are used to remember both the root and the pattern. We suppress the
detailed description of the construction.
Example 12
Consider again the Hebrew roots r.$.m, p.&.l, p.q.d and the Hebrew patterns hitae,
mia, and haaa. The splice operation
{?r, $, m??p, &, l??p, q, d?} ? {?hitae??mia??haaa?}
74
Cohen-Sygal and Wintner Non-Concatenative Morphology
yields the FSRA of Figure 13. The -arc was added only for the convenience of the
drawing.
It should be noted that like other processes of derivational morphology, Hebrew
word formation is highly idiosyncratic: Not all roots combine with all patterns, and
there is no systematic way to determine when such combinations will be realized
in the language. Yet, this does not render our proposed operators useless: One can
naturally characterize classes of roots and classes of patterns for which all the com-
binations exist. Furthermore, even when such a characterization is difficult to come by,
the splice operator can be used, in combination with other extended regular expres-
sion operators, to define complex expressions for generating the required language.
This is compatible with the general approach for using finite-state techniques, imple-
menting each phenomenon independently and combining them together using closure
properties.
5.3 Reduplication
We now return to the reduplication problem as was presented in example 3. We extend
the finite-state registered model to efficiently accept Ln = {ww | w ? ??, |w| = n}, a
finite instance of the general problem, which is arguably sufficient for describing
reduplication in natural languages. Using FSRAs as defined above does not improve
space efficiency, because a separate path for each reduplication is still needed. Notice
that the different symbols in Ln have no significance except the pattern they create.
Therefore, FSRAs are extended in order to be able to identify a pattern without actually
distinguishing between different symbols in it. The extended model, FSRA*, is obtained
from the FSRA-1 model by adding a new symbol, ?*?, assumed not to belong to ?, and
by forcing ? to be equal to ?. The ?*? indicates equality between the input symbol and
the designated register content, eliminating the need to duplicate paths for different
symbols.
Figure 13
Interdigitation example.
75
Computational Linguistics Volume 32, Number 1
Definition
Let ? /? ?. An FSRA* is an FSRA-1 where ? = ? (and thus includes ?#?) and the tran-
sition function is extended to be ? ? Q ? ? ? {, ?} ? {R, W} ? {0, 1, 2, . . . , n ? 1} ?
? ? {?} ? Q. The extended meaning of ? is as follows:
 (s,?, R, i,?, t) ? ?, (s,?, W, i,?, t) ? ? where ?,? = ? imply the same as
before.
 (s,?, R, i, ?, t) ? ? and (s, ?, R, i,?, t) ? ? for ? =  imply that if the
automaton is in state s, the input symbol is ? and the content of the i-th
register is the same ?, then the automaton may enter state t.
 (s,?, W, i, ?, t) ? ? and (s, ?, W, i,?, t) ? ? for ? =  imply that if the
automaton is in state s and the input symbol is ?, then the content of the
i-th register is changed to ?, and the automaton may enter state t.
 (s, ?, R, i, ?, t) ? ? implies that if the automaton is in state s, the input
symbol is some ? ? ? and the content of the i-th register is the same ?,
then the automaton may enter state t.
 (s, ?, W, i, ?, t) ? ? implies that if the automaton is in state s and the input
symbol is some ? ? ?, then the content of the i-th register is changed to the
same ?, and the automaton may enter state t.
With this extended model we can construct an efficient registered automaton for
Ln: The number of registers is n+1. Registers 1, ..., n remember the first n symbols to be
duplicated. Figure 14 depicts an extended registered automaton that accepts Ln for n =
4. Notice that the number of states depends only on n and not on the size of ?. Figure 15
schematically depicts an extended registered automaton that accepts Ln for some n ? N.
The language {ww | |w| ? n} for some n ? N can be generated by a union of FSRA*,
each one generating Ln for some i ? n. Since n is usually small in natural language
reduplication, the resulting automaton is manageable, and in any case, considerably
smaller than the na??ve automaton.
5.4 Assimilation
In example 7, FSRAs are used to model assimilation in Arabic nominative definite
nouns. Using the FSRA* model defined above, further reduction in the network size
can be achieved. The FSRA* of Figure 16 accepts all the nominative definite forms of the
Arabic nouns kitaab, qamar, and daftar (more nouns can be added in a similar way).
Register 1 stores information about the actual form of the definite article, to ensure that
assimilation occurs when needed and only then. Notice that in this FSRA, in contrast to
Figure 14
Reduplication for n = 4.
76
Cohen-Sygal and Wintner Non-Concatenative Morphology
Figure 15
Reduplication ? general case.
the FSRA of Figure 8, the definite Arabic article al is not scanned as one symbol but as
two separate symbols.
6. Finite-state Registered Transducers
We extend the FSRA model to finite-state registered transducers (FSRT), denoting
relations over two finite alphabets. The extension is done by adding to each transition an
output symbol. This facilitates an elegant solution to the problem of binary incrementors
which was introduced in Example 4.
Example 13
Consider again the 32-bit incrementor example introduced in Example 4. Recall that
a sequential transducer for an n-bit binary incrementor would require 2n states and a
similar number of transitions. Using the FSRT model, a more efficient n-bit transducer
can be constructed. A 4-bit FSRT incrementor is shown in Figure 17. The first four
transitions copy the input string into the registers, then the input is scanned (using
the registers) from right to left (as the carry moves), calculating the result, and the
last four transitions output the result (in case the input is 1n, an extra 1 is added in
the beginning). Notice that this transducer guarantees linear recognition time, since
from each state only one arc can be traversed in each step, even when there are
-arcs. In the same way, an n-bit transducer can be constructed for all n ? N. Such
a transducer will have n registers, 3n + 1 states and 6n arcs. The FSRT model solves
the incrementor problem in much the same way it is solved by vectorized finite-state
Figure 16
FSRA* for Arabic nominative definite nouns.
77
Computational Linguistics Volume 32, Number 1
Figure 17
4-bit incrementor using FSRT.
automata, but the FSRT solution is more intuitive and is based on existing finite-state
techniques.
It is easy to show that FSRTs, just like FSRAs, are equivalent to their non-registered
counterparts. It immediately implies that FSRTs maintain the closure properties of
regular relations. As in FSRAs, implementing the closure properties directly on FSRTs
is essential for benefiting from their space efficiency. The common operators such as
union, concatenation, etc., are implemented in the same ways as in FSRAs. A direct
implementation of FSRT composition is a na??ve extension of ordinary transducer com-
position, based on the intersection construction of FSRAs. We explicitly define these
operations in Cohen-Sygal (2004).
7. Implementation and Evaluation
In order to practically compare the space and time performance of FSRAs and FSAs, we
have implemented the special operators introduced in Sections 4 and 5 for circumfix-
ation and interdigitation, as well as direct construction of FSRAs. We have compared
FSRAs with ordinary FSAs by building corresponding networks for circumfixation,
interdigitation, and n-bit incrementation. For circumfixation, we constructed networks
for the circumfixation of 1,043 Hebrew roots and 4 circumfixes. For interdigitation we
constructed a network accepting the splicing of 1,043 roots into 20 patterns. For n-bit
incrementation we constructed networks for 10-bit, 50-bit, and 100-bit incrementors.
Table 1 displays the size of each of the networks in terms of states, arcs, and actual file
size.
78
Cohen-Sygal and Wintner Non-Concatenative Morphology
Table 1
Space comparison between FSAs and FSRAs.
Operation Network type States Arcs Registers File size
Circumfixation FSA 811 3,824 ? 47kB
(4 circumfixes, 1,043 roots) FSRA 356 360 1 16kB
Interdigitation FSA 12,527 31,077 ? 451kB
(20 patterns, 1,043 roots) FSRA 58 3,259 2 67kB
10-bit incrementor Sequential FST 268 322 ? 7kB
FSRT 31 60 10 2kB
50-bit incrementor Sequential FST 23,328 24,602 ? 600kB
FSRT 151 300 50 8kB
100-bit incrementor Sequential FST 176,653 181,702 ? 4.73Mb
FSRT 301 600 100 17kB
Table 2
Time comparison between FSAs and FSRAs.
200 words 1,000 words 5,000 words
Circumfixation FSA 0.01s 0.02s 0.08s
(4 circumfixes, 1,043 roots) FSRA 0.01s 0.02s 0.09s
Interdigitation FSA 0.01s 0.02s 1s
(20 patterns, 1,043 roots) FSRA 0.35s 1.42s 10.11s
10-bit incrementor Sequential FST 0.01s 0.05s 0.17s
FSRT 0.01s 0.06s 0.23s
50-bit incrementor Sequential FST 0.13s 0.2s 0.59s
FSRT 0.08s 0.4s 1.6s
Clearly, FSRAs provide a significant reduction in the network size. In particular, we
could not construct an n-bit incrementor FSA for any n greater than 100 as a result of
memory problems, whereas using FSRAs we had no problem constructing networks
even for n = 50, 000.
In addition, we compared the recognition times of the two models. For that purpose,
we used the circumfixation, interdigitation, 10-bit incrementation, and 50-bit incremen-
tation networks to analyze 200, 1,000, and 5,000 words. As can be seen in Table 2, time
performance is comparable for the two models, except for interdigitation, where FSAs
outperform FSRAs by a constant factor. The reason is that in this network the usage of
registers is massive and thereby, there is a higher cost to the reduction of the network
size, in terms of analysis time. This is an instance of the common tradeoff of time versus
space: FSRAs improve the network size at the cost of slower analysis time in some cases.
When using finite-state devices for natural language processing, often the generated
networks become too large to be practical. In such cases, using FSRAs can make network
size manageable. Using the closure constructions one can build desired networks of
reasonable size, and at the end decide whether to convert them to ordinary FSAs, if
time performance is an issue.
8. Conclusions
In this work we introduce finite-state registered networks (automata and transducers),
an extension of finite-state networks which adds a limited amount of memory, in the
79
Computational Linguistics Volume 32, Number 1
form of registers, to each transition. We show how FSRAs can be used to efficiently
model several non-concatenative morphological phenomena, including circumfixation,
root and pattern word formation in Semitic languages, vowel harmony, and limited
reduplication.
The main advantage of finite-state registered networks is their space efficiency. We
show that every FSA can be simulated by an equivalent FSRA with three states and
two registers. For the motivating linguistic examples, we show a significant decrease
in the number of states and the number of transitions. For example, to account for all
the possible combinations of r roots and p patterns, an ordinary FSA requires O(r ? p)
arcs whereas an FSRA requires only O(r + p). As a non-linguistic example, we show
a transducer that computes n-bit increments of binary numbers. While an ordinary
(sequential) FST requires O(2n) states and arcs, an FSRT which guarantees linear recog-
nition time requires only O(n) states and arcs.
In spite of their efficiency, finite-state registered networks are equivalent, in terms
of their expressive power, to ordinary finite state networks. We provide an algorithm for
converting FSRAs to FSAs and prove the equivalence of the models. Furthermore, we
provide direct constructions of the main closure properties of FSAs for FSRAs, including
concatenation, union, intersection, and composition.
In order for finite-state networks to be useful for linguistic processing, we provide
a regular expression language denoting FSRAs. In particular, we provide a set of
extended regular expression operators that denote FSRAs and FSRTs. We demonstrate
the utility of the operators by accounting for a variety of complex morphological and
phonological phenomena, including circumfixation (Hebrew and German), root-and-
pattern (Hebrew), vowel harmony (Warlpiri), assimilation (Arabic), and limited redu-
plication. These dedicated operators can be used in conjunction with standard finite
state calculi, thereby providing a complete set of tools for the computational treatment
of non-concatenative morphology.
This work opens a variety of directions for future research. An immediate question
is the conversion of FSAs to FSRAs. While it is always possible to convert a given FSA
to an FSRA (simply add one register which is never used), we believe that it is possible
to automatically convert space inefficient FSAs to more compact FSRAs. A pre-requisite
is a clear understanding of the parameters for minimization: These include the number
of states, arcs, and registers, and the size of the register alphabet. For a given FSRA, the
number of states can always be reduced to a constant (theorem 3) and registers can be
done away with entirely (by converting the FSRA to an FSA, Section 3.1). In contrast,
minimizing the number of arcs in an FSRA is NP-hard (Section 3.4). A useful conversion
of FSAs to FSRAs must minimize some combination of these parameters, and while it
may be intractable in general, it can be practical in many special cases. In particular,
the case of finite languages (acyclic FSAs) is both of practical importance and ? we
conjecture ? can result in good compaction.
More work is also needed in order to establish more properties of FSRTs. In particu-
lar, we did not address issues such as sequentiality or sequentiability for this model.
Similarly, FSRA? can benefit from further research. All the closure constructions for
FSRA?s can be done in a similar way to FSRAs, with the exception of intersection. For in-
tersection, we believe that the use of predicates (van Noord and Gerdemann 2001b) can
be beneficial. Furthermore, the use of predicates can be beneficial for describing natural
language reduplication where the reduplication is not as bounded as the example we
deal with in this work. In addition, the FSRA? model can be extended into transducers.
Finally, in Section 7 we discuss an implementation of FSRAs. Although we have
used this system to construct networks for several phenomena, we are interested in
80
Cohen-Sygal and Wintner Non-Concatenative Morphology
constructing a network for describing the complete morphology of a natural language
containing many non-concatenative phenomena, e.g., Hebrew. A morphological ana-
lyzer for Hebrew, based on finite-state calculi, already exists (Yona and Wintner 2005),
but is very space-inefficient and, therefore, hard to maintain. It would be beneficial to
compact such a network using FSRTs, and to inspect the time versus space tradeoff on
such a comprehensive network.
Acknowledgments
We are grateful to Dale Gerdemann for his
help and inspiration. We thank Victor Harnik
and Nissim Francez for their comments on
an earlier version of this paper. We are also
thankful to the anonymous reviewers, whose
comments helped substantially to improve
this article. This research was supported by
The Israel Science Foundation (grant
no. 136/01).
References
Beesley, Kenneth R. 1998. Constraining
separated morphotactic dependencies in
finite-state grammars. In Proceedings of
FSMNLP-98, pages 118?127, Bilkent,
Turkey.
Beesley, Kenneth R. and Lauri Karttunen.
2000. Finite-state non-concatenative
morphotactics. In Proceedings of the
Fifth Workshop of the ACL Special Interest
Group in Computational Phonology,
SIGPHON-2000, pages 1?12,
Luxembourg.
Beesley, Kenneth R. and Lauri Karttunen.
2003. Finite-State Morphology. CSLI
Publications.
Blank, Glenn D. 1985. A new kind of
finite-state automaton: Register vector
grammar. In Proceedings of the International
Joint Conference on Artificial Intelligence,
pages 749?755, UCLA.
Blank, Glenn D. 1989. A finite and real-time
processor for natural language.
Communications of the ACM,
32(10):1174?1189.
Cohen-Sygal, Yael. 2004. Computational
implementation of non-concatenative
morphology. Master?s thesis, Department
of Computer Science, University of Haifa,
Israel.
Holzer, Markus and Martin Kutrib. 2002.
State complexity of basic operations on
nondeterministic finite automata. In
Jean-Marc Champarnaud and Denis
Maurel, editors, Implementation and
Application of Automata, 7th International
Conference, CIAA 2002, volume 2608 of
Lecture Notes in Computer Science,
Springer, pages 148?157.
Kaminski, Michael and Nissim Francez.
1994. Finite memory automata. Theoretical
Computer Science, 134(2):329?364.
Kaplan, Ronald M. and Martin Kay. 1994.
Regular models of phonological rule
systems. Computational Linguistics,
20(3):331?378.
Karttunen, Lauri, Jean-Pierre Chanod,
Gregory Grefenstette, and Anne Schiller.
1996. Regular expressions for language
engineering. Natural Language Engineering,
2(4):305?328.
Kataja, Laura and Kimmo Koskenniemi.
1988. Finite-state description of Semitic
morphology: A case study of ancient
Akkadian. In Proceedings of COLING 88,
International Conference on Computational
Linguistics, pages 313?315, Budapest.
Kay, Martin. 1987. Nonconcatenative
finite-state morphology. In Proceedings of
the Third Conference of the European Chapter
of the Association for Computational
Linguistics, pages 2?10, Copenhagen,
Denmark.
Kiraz, George Anton. 2000. Multitiered
nonlinear morphology using multitape
finite automata: A case study on Syriac
and Arabic. Computational Linguistics,
26(1):77?105.
Kleene, S. C. 1956. Representation of events
in nerve nets and finite automata. In C. E.
Shannon and J. McCarthy, editors,
Automata Studies. Princeton University
Press, pages 3?42.
Kornai, Andra?s. 1996. Vectorized finite-state
automata. In Proceedings of the Workshop on
Extended Finite-State Models of Languages in
the 12th European Conference on Artificial
Intelligence, pages 36?41, Budapest.
Koskenniemi, Kimmo. 1983. Two-Level
Morphology: A General Computational Model
for Word-Form Recognition and Production.
The Department of General Linguistics,
University of Helsinki.
Krauwer, Steven and Louis des Tombe. 1981.
Transducers and grammars as theories of
language. Theoretical Linguistics, 8:173?202.
Lavie, Alon, Alon Itai, Uzzi Ornan, and Mori
Rimon. 1988. On the applicability of
two-level morphology to the inflection of
Hebrew verbs. Technical Report 513,
81
Computational Linguistics Volume 32, Number 1
Department of Computer Science,
Technion, 32000 Haifa, Israel.
Mohri, Mehryar. 1996. On some applications
of finite-state automata theory to natural
language processing. Natural Language
Engineering, 2(1):61?80.
Mohri, Mehryar. 2000. Generic
epsilon-removal algorithm for weighted
automata. In Sheng Yu and Andrei Paun,
editors, 5th International Conference, CIAA
2000, volume 2088, Springer-Verlag,
pages 230?242.
Mohri, Mehryar, Fernando Pereira, and
Michael Riley. 2000. The design principles
of a weighted finite-state transducer
library. Theoretical Computer Science,
231(1):17?32.
Nash, David. 1980. Topics in Warlpiri
Grammar. Ph.D. thesis, Massachusetts
Institute of Technology.
Sproat, Richard W. 1992. Morphology and
Computation. MIT Press, Cambridge, MA.
van Noord, Gertjan and Dale Gerdemann.
2001a. An extendible regular expression
compiler for finite-state approaches in
natural language processing. In O. Boldt
and H. Ju?rgensen, editors, Automata
Implementation, 4th International Workshop
on Implementing Automata, WIA?99,
Potsdam, Germany, Revised Papers,
number 2214 in Lecture Notes in Computer
Science. Springer.
van Noord, Gertjan and Dale Gerdemann.
2001b. Finite state transducers with
predicates and identity. Grammars,
4(3):263?286.
Walther, Markus. 2000a. Finite-state
reduplication in one-level prosodic
morphology. In Proceedings of
the First Conference of the North
American Chapter of the Association
for Computational Linguistics,
pages 296?302, Seattle.
Walther, Markus. 2000b. Temiar
reduplication in one-level prosodic
morphology. In Proceedings of
SIGPHON, Workshop on Finite-State
Phonology, pages 13?21, Luxembourg.
Yona, Shlomo and Shuly Wintner.
2005. A finite-state morphological
grammar of Hebrew. In Proceedings of
the ACL-2005 Workshop on Computational
Approaches to Semitic Languages,
Ann Arbor.
82
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 145?152,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Partially Specified Signatures: a Vehicle for Grammar Modularity
Yael Cohen-Sygal
Dept. of Computer Science
University of Haifa
yaelc@cs.haifa.ac.il
Shuly Wintner
Dept. of Computer Science
University of Haifa
shuly@cs.haifa.ac.il
Abstract
This work provides the essential founda-
tions for modular construction of (typed)
unification grammars for natural lan-
guages. Much of the information in such
grammars is encoded in the signature, and
hence the key is facilitating a modularized
development of type signatures. We intro-
duce a definition of signature modules and
show how two modules combine. Our def-
initions are motivated by the actual needs
of grammar developers obtained through a
careful examination of large scale gram-
mars. We show that our definitions meet
these needs by conforming to a detailed set
of desiderata.
1 Introduction
Development of large scale grammars for natural
languages is an active area of research in human
language technology. Such grammars are devel-
oped not only for purposes of theoretical linguis-
tic research, but also for natural language applica-
tions such as machine translation, speech genera-
tion, etc. Wide-coverage grammars are being de-
veloped for various languages (Oepen et al, 2002;
Hinrichs et al, 2004; Bender et al, 2005; King et
al., 2005) in several theoretical frameworks, e.g.,
LFG (Dalrymple, 2001) and HPSG (Pollard and
Sag, 1994).
Grammar development is a complex enterprise:
it is not unusual for a single grammar to be devel-
oped by a team including several linguists, com-
putational linguists and computer scientists. The
scale of grammars is overwhelming: for exam-
ple, the English resource grammar (Copestake
and Flickinger, 2000) includes thousands of types.
This raises problems reminiscent of those encoun-
tered in large-scale software development. Yet
while software engineering provides adequate so-
lutions for the programmer, no grammar develop-
ment environment supports even the most basic
needs, such as grammar modularization, combi-
nation of sub-grammars, separate compilation and
automatic linkage of grammars, information en-
capsulation, etc.
This work provides the essential foundations for
modular construction of signatures in typed unifi-
cation grammars. After a review of some basic
notions and a survey of related work we list a set
of desiderata in section 4, which leads to a defi-
nition of signature modules in section 5. In sec-
tion 6 we show how two modules are combined,
outlining the mathematical properties of the com-
bination (proofs are suppressed for lack of space).
Extending the resulting module to a stand-alone
type signature is the topic of section 7. We con-
clude with suggestions for future research.
2 Type signatures
We assume familiarity with theories of (typed)
unification grammars, as formulated by, e.g., Car-
penter (1992) and Penn (2000). The definitions
in this section set the notation and recall basic no-
tions. For a partial function F , ?F (x)?? means that
F is defined for the value x.
Definition 1 Given a partially ordered set ?P,??,
the set of upper bounds of a subset S ? P is the
set Su = {y ? P | ?x ? S x ? y}.
For a given partially ordered set ?P,??, if S ?
P has a least element then it is unique.
Definition 2 A partially ordered set ?P,?? is a
bounded complete partial order (BCPO) if for
every S ? P such that Su 6= ?, Su has a least
element, called a least upper bound (lub).
Definition 3 A type signature is a structure
?TYPE,?, FEAT, Approp?, where:
1. ?TYPE,?? is a finite bounded complete par-
tial order (the type hierarchy)
145
2. FEAT is a finite set, disjoint from TYPE.
3. Approp : TYPE?FEAT ? TYPE (the appro-
priateness specification) is a partial function
such that for every F ? FEAT:
(a) (Feature Introduction) there exists a
type Intro(F ) ? TYPE such that
Approp(Intro(F ), F )?, and for every
t ? TYPE, if Approp(t, F ) ?, then
Intro(F ) ? t;
(b) (Upward Closure) if Approp(s, F ) ?
and s ? t, then Approp(t, F ) ? and
Approp(s, F ) ? Approp(t, F ).
Notice that every signature has a least type,
since the subset S = ? of TYPE has the non-empty
set of upper bounds, Su = TYPE, which must
have a least element due to bounded completeness.
Definition 4 Let ?TYPE,?? be a type hierarchy
and let x, y ? TYPE. If x ? y, then x is a su-
pertype of y and y is a subtype of x. If x ? y,
x 6= y and there is no z such that x ? z ? y and
z 6= x, y then x is an immediate supertype of y
and y is an immediate subtype of x.
3 Related Work
Several authors address the issue of grammar mod-
ularization in unification formalisms. Moshier
(1997) views HPSG , and in particular its signa-
ture, as a collection of constraints over maps be-
tween sets. This allows the grammar writer to
specify any partial information about the signa-
ture, and provides the needed mathematical and
computational capabilities to integrate the infor-
mation with the rest of the signature. However,
this work does not define modules or module in-
teraction. It does not address several basic issues
such as bounded completeness of the partial or-
der and the feature introduction and upward clo-
sure conditions of the appropriateness specifica-
tion. Furthermore, Moshier (1997) shows how sig-
natures are distributed into components, but not
the conditions they are required to obey in order
to assure the well-definedness of the combination.
Keselj (2001) presents a modular HPSG, where
each module is an ordinary type signature, but
each of the sets FEAT and TYPE is divided into
two disjoint sets of private and public elements. In
this solution, modules do not support specification
of partial information; module combination is not
associative; and the only channel of interaction be-
tween modules is the names of types.
Kaplan et al (2002) introduce a system de-
signed for building a grammar by both extending
and restricting another grammar. An LFG gram-
mar is presented to the system in a priority-ordered
sequence of files where the grammar can include
only one definition of an item of a given type (e.g.,
rule) with a particular name. Items in a higher pri-
ority file override lower priority items of the same
type with the same name. The override convention
makes it possible to add, delete or modify rules.
However, a basis grammar is needed and when
modifying a rule, the entire rule has to be rewritten
even if the modifications are minor. The only in-
teraction among files in this approach is overriding
of information.
King et al (2005) augment LFG with a
makeshift signature to allow modular development
of untyped unification grammars. In addition, they
suggest that any development team should agree in
advance on the feature space. This work empha-
sizes the observation that the modularization of the
signature is the key for modular development of
grammars. However, the proposed solution is ad-
hoc and cannot be taken seriously as a concept of
modularization. In particular, the suggestion for
an agreement on the feature space undermines the
essence of modular design.
Several works address the problem of modular-
ity in other, related, formalisms. Candito (1996)
introduces a description language for the trees of
LTAG. Combining two descriptions is done by
conjunction. To constrain undesired combina-
tions, Candito (1996) uses a finite set of names
where each node of a tree description is associ-
ated with a name. The only channel of interac-
tion between two descriptions is the names of the
nodes, which can be used only to allow identifi-
cation but not to prevent it. To overcome these
shortcomings, Crabbe? and Duchier (2004) suggest
to replace node naming by colors. Then, when
unifying two trees, the colors can prevent or force
the identification of nodes. Adapting this solution
to type signatures would yield undesired order-
dependence (see below).
4 Desiderata
To better understand the needs of grammar devel-
opers we carefully explored two existing gram-
mars: the LINGO grammar matrix (Bender et al,
2002), which is a basis grammar for the rapid de-
velopment of cross-linguistically consistent gram-
146
mars; and a grammar of a fragment of Modern He-
brew, focusing on inverted constructions (Melnik,
2006). These grammars were chosen since they
are comprehensive enough to reflect the kind of
data large scale grammar encode, but are not too
large to encumber this process. Motivated by these
two grammars, we experimented with ways to di-
vide the signatures of grammars into modules and
with different methods of module interaction. This
process resulted in the following desiderata for a
beneficial solution for signature modularization:
1. The grammar designer should be provided
with as much flexibility as possible. Modules
should not be unnecessarily constrained.
2. Signature modules should provide means
for specifying partial information about the
components of a grammar.
3. A good solution should enable one module to
refer to types defined in another. Moreover,
it should enable the designer of module Mi
to use a type defined in Mj without specify-
ing the type explicitly. Rather, some of the
attributes of the type can be (partially) speci-
fied, e.g., its immediate subtypes or its appro-
priateness conditions.
4. While modules can specify partial informa-
tion, it must be possible to deterministically
extend a module (which can be the result of
the combination of several modules) into a
full type signature.
5. Signature combination must be associative
and commutative: the order in which mod-
ules are combined must not affect the result.
The solution we propose below satisfies these re-
quirements.1
5 Partially specified signatures
We define partially specified signatures (PSSs),
also referred to as modules below, which are struc-
tures containing partial information about a sig-
nature: part of the subsumption relation and part
of the appropriateness specification. We assume
enumerable, disjoint sets TYPE of types and FEAT
of features, over which signatures are defined.
We begin, however, by defining partially labeled
graphs, of which PSSs are a special case.
1The examples in the paper are inspired by actual gram-
mars but are obviously much simplified.
Definition 5 A partially labeled graph (PLG)
over TYPE and FEAT is a finite, directed labeled
graph S = ?Q, T,, Ap?, where:
1. Q is a finite, nonempty set of nodes, disjoint
from TYPE and FEAT.
2. T : Q ? TYPE is a partial function, marking
some of the nodes with types.
3. ? Q ? Q is a relation specifying (immedi-
ate) subsumption.
4. Ap ? Q? FEAT ?Q is a relation specifying
appropriateness.
Definition 6 A partially specified signa-
ture (PSS) over TYPE and FEAT is a PLG
S = ?Q, T,, Ap?, where:
1. T is one to one.
2. ?? is antireflexive; its reflexive-transitive
closure, denoted ?
?
?, is antisymmetric.
3. (a) (Relaxed Upward Closure) for all
q1, q?1, q2 ? Q and F ? FEAT, if
(q1, F, q2) ? Ap and q1
?
 q?1, then there
exists q?2 ? Q such that q2
?
 q?2 and
(q?1, F, q?2) ? Ap; and
(b) (Maximality) for all q1, q2 ? Q and F ?
FEAT, if (q1, F, q2) ? Ap then for all
q?2 ? Q such that q?2
?
 q2 and q2 6= q?2,
(q1, F, q?2) /? Ap.
A PSS is a finite directed graph whose nodes
denote types and whose edges denote the sub-
sumption and appropriateness relations. Nodes
can be marked by types through the function T ,
but can also be anonymous (unmarked). Anony-
mous nodes facilitate reference, in one module, to
types that are defined in another module. T is one-
to-one since we assume that two marked nodes de-
note different types.
The ?? relation specifies an immediate sub-
sumption order over the nodes, with the intention
that this order hold later for the types denoted by
nodes. This is why ?
?
? is required to be a partial
order. The type hierarchy of a type signature is a
BCPO, but current approaches (Copestake, 2002)
relax this requirement to allow more flexibility in
grammar design. PSS subsumption is also a par-
tial order but not necessarily a bounded complete
147
one. After all modules are combined, the resulting
subsumption relation will be extended to a BCPO
(see section 7), but any intermediate result can be a
general partial order. Relaxing the BCPO require-
ment also helps guaranteeing the associativity of
module combination.
Consider now the appropriateness relation. In
contrast to type signatures, Ap is not required
to be a function. Rather, it is a relation which
may specify several appropriate nodes for the val-
ues of a feature F at a node q. The intention is
that the eventual value of Approp(T (q), F ) be the
lub of the types of all those nodes q? such that
Ap(q, F, q?). This relaxation allows more ways for
modules to interact. We do restrict the Ap relation,
however. Condition 3a enforces a relaxed version
of upward closure. Condition 3b disallows redun-
dant appropriateness arcs: if two nodes are ap-
propriate for the same node and feature, then they
should not be related by subsumption. The feature
introduction condition of type signatures is not en-
forced by PSSs. This, again, results in more flex-
ibility for the grammar designer; the condition is
restored after all modules combine, see section 7.
Example 1 A simple PSS S1 is depicted in Fig-
ure 1, where solid arrows represent the ?? (sub-
sumption) relation and dashed arrows, labeled by
features, the Ap relation. S1 stipulates two sub-
types of cat, n and v, with a common subtype,
gerund. The feature AGR is appropriate for all
three categories, with distinct (but anonymous)
values for Approp(n, AGR) and Approp(v, AGR).
Approp(gerund, AGR) will eventually be the lub
of Approp(n, AGR) and Approp(v, AGR), hence
the multiple outgoing AGR arcs from gerund.
Observe that in S1, ?? is not a BCPO, Ap is
not a function and the feature introduction condi-
tion does not hold.
gerund
n v
cat agr
AGR
AGR
AGR
AGR
Figure 1: A partially specified signature, S1
We impose an additional restriction on PSSs:
a PSS is well-formed if any two different anony-
mous nodes are distinguishable, i.e., if each node
is unique with respect to the information it en-
codes. If two nodes are indistinguishable then one
of them can be removed without affecting the in-
formation encoded by the PSS. The existence of
indistinguishable nodes in a PSS unnecessarily in-
creases its size, resulting in inefficient processing.
Given a PSS S, it can be compacted into a PSS,
compact(S), by unifying all the indistinguishable
nodes in S. compact(S) encodes the same infor-
mation as S but does not include indistinguish-
able nodes. Two nodes, only one of which is
anonymous, can still be otherwise indistinguish-
able. Such nodes will, eventually, be coalesced,
but only after all modules are combined (to ensure
the associativity of module combination). The de-
tailed computation of the compacted PSS is sup-
pressed for lack of space.
Example 2 Let S2 be the PSS of Figure 2. S2 in-
cludes two pairs of indistinguishable nodes: q2, q4
and q6, q7. The compacted PSS of S2 is depicted in
Figure 3. All nodes in compact(S2) are pairwise
distinguishable.
q6 q7
b
q8
q2 q3 q4 q5
q1
a
F F F
F
Figure 2: A partially specified signature with in-
distinguishable nodes, S2
b
a
F FF
Figure 3: The compacted partially specified signa-
ture of S2
Proposition 1 If S is a PSS then compact(S) is a
well formed PSS.
148
6 Module combination
We now describe how to combine modules, an op-
eration we call merge bellow. When two mod-
ules are combined, nodes that are marked by the
same type are coalesced along with their attributes.
Nodes that are marked by different types cannot
be coalesced and must denote different types. The
main complication is caused when two anonymous
nodes are considered: such nodes are coalesced
only if they are indistinguishable.
The merge of two modules is performed in sev-
eral stages: First, the two graphs are unioned (this
is a simple pointwise union of the coordinates
of the graph, see definition 7). Then the result-
ing graph is compacted, coalescing nodes marked
by the same type as well as indistinguishable
anonymous nodes. However, the resulting graph
does not necessarily maintain the relaxed upward
closure and maximality conditions, and therefore
some modifications are needed. This is done by
Ap-Closure, see definition 8. Finally, the addi-
tion of appropriateness arcs may turn two anony-
mous distinguishable nodes into indistinguishable
ones and therefore another compactness operation
is needed (definition 9).
Definition 7 Let S1 = ?Q1, T1,1, Ap1?, S2 =
?Q2, T2,2, Ap2? be two PLGssuch that Q1 ?
Q2 = ?. The union of S1 and S2, denoted S1?S2,
is the PLG S = ?Q1 ? Q2, T1 ? T2,1 ? 2,
Ap1 ? Ap2?.
Definition 8 Let S = ?Q, T,, Ap? be a PLG.
The Ap-Closure of S, denoted ApCl(S), is the
PLG ?Q, T,, Ap??? where:
? Ap? = {(q1, F, q2) | q1, q2 ? Q and there
exists q?1 ? Q such that q?1
?
 q1 and
(q?1, F, q2) ? Ap}
? Ap?? = {(q1, F, q2) ? Ap? | for all q?2 ? Q,
such that q2
?
 q?2 and q2 6= q?2, (q1, F, q?2) /?
Ap?}
Ap-Closure adds to a PLG the arcs required for
it to maintain the relaxed upward closure and max-
imality conditions. First, arcs are added (Ap?) to
maintain upward closure (to create the relations
between elements separated between the two mod-
ules and related by mutual elements). Then, re-
dundant arcs are removed to maintain the maxi-
mality condition (the removed arcs may be added
by Ap? but may also exist in Ap). Notice that
Ap ? Ap? since for all (q1, F, q2) ? Ap, by
choosing q?1 = q1 it follows that q?1 = q1
?
 q1
and (q?1, F, q2) = (q1, F, q2) ? Ap and hence
(q?1, F, q2) = (q1, F, q2) ? Ap?.
Two PSSs can be merged only if the result-
ing subsumption relation is indeed a partial order,
where the only obstacle can be the antisymme-
try of the resulting relation. The combination of
the appropriateness relations, in contrast, cannot
cause the merge operation to fail because any vi-
olation of the appropriateness conditions in PSSs
can be deterministically resolved.
Definition 9 Let S1 = ?Q1, T1,1, Ap1?, S2 =
?Q2, T2,2, Ap2? be two PSSs such that Q1 ?
Q2 = ?. S1, S2 are mergeable if there are no
q1, q2 ? Q1 and q3, q4 ? Q2 such that the fol-
lowing hold:
1. T1(q1)?, T1(q2)?, T2(q3)? and T2(q4)?
2. T1(q1) = T2(q4) and T1(q2) = T2(q3)
3. q1
?
1 q2 and q3
?
2 q4
If S1 and S2 are mergeable, then their merge,
denoted S1?S2, is compact(ApCl(compact(S1?
S2))).
In the merged module, pairs of nodes marked
by the same type and pairs of indistinguishable
anonymous nodes are coalesced. An anonymous
node cannot be coalesced with a typed node, even
if they are otherwise indistinguishable, since that
will result in an unassociative combination oper-
ation. Anonymous nodes are assigned types only
after all modules combine, see section 7.1.
If a node has multiple outgoing Ap-arcs labeled
with the same feature, these arcs are not replaced
by a single arc, even if the lub of the target nodes
exists in the resulting PSS. Again, this is done to
guarantee the associativity of the merge operation.
Example 3 Figure 4 depicts a na??ve agreement
module, S5. Combined with S1 of Figure 1,
S1 ? S5 = S5 ? S1 = S6, where S6 is depicted
in Figure 5. All dashed arrows are labeled AGR,
but these labels are suppressed for readability.
Example 4 Let S7 and S8 be the PSSs depicted
in Figures 6 and 7, respectively. Then S7 ? S8 =
S8?S7 = S9, where S9 is depicted in Figure 8. By
standard convention, Ap arcs that can be inferred
by upward closure are not depicted.
149
n nagr gerund vagr v
agr
Figure 4: Na??ve agreement module, S5
gerund
n v vagr nagr
cat agr
Figure 5: S6 = S1 ? S5
Proposition 2 Given two mergeable PSSs S1, S2,
S1 ? S2 is a well formed PSS.
Proposition 3 PSS merge is commutative: for any
two PSSs, S1, S2, S1?S2 = S2?S1. In particular,
either both are defined or both are undefined.
Proposition 4 PSS merge is associative: for all
S1, S2, S3, (S1 ? S2) ? S3 = S1 ? (S2 ? S3).
7 Extending PSSs to type signatures
When developing large scale grammars, the sig-
nature can be distributed among several modules.
A PSS encodes only partial information and there-
fore is not required to conform with all the con-
straints imposed on ordinary signatures. After all
the modules are combined, however, the PSS must
be extended into a signature. This process is done
in 4 stages, each dealing with one property: 1.
Name resolution: assigning types to anonymous
nodes (section 7.1); 2. Determinizing Ap, convert-
ing it from a relation to a function (section 7.2); 3.
Extending ?? to a BCPO. This is done using the
algorithm of Penn (2000); 4. Extending Ap to a
full appropriateness specification by enforcing the
feature introduction condition: Again, we use the
person nvagr bool
vagr nagr
agr numNUM
PERSON DEF
Figure 6: An agreement module, S7
first second third + ?
sg
person
pl
bool
num
Figure 7: A partially specified signature, S8
first second third + ?
person bool
nvagr
vagr nagr sg pl
agr numNUM
DE
F
PE
RS
ON
Figure 8: S9 = S7 ? S8
algorithm of Penn (2000).
7.1 Name resolution
By the definition of a well-formed PSS, each
anonymous node is unique with respect to the in-
formation it encodes among the anonymous nodes,
but there may exist a marked node encoding the
same information. The goal of the name resolution
procedure is to assign a type to every anonymous
node, by coalescing it with a similar marked node,
if one exists. If no such node exists, or if there is
more than one such node, the anonymous node is
given an arbitrary type.
The name resolution algorithm iterates as long
as there are nodes to coalesce. In each iteration,
for each anonymous node the set of its similar
typed nodes is computed. Then, using this compu-
tation, anonymous nodes are coalesced with their
paired similar typed node, if such a node uniquely
exists. After coalescing all such pairs, the result-
ing PSS may be non well-formed and therefore the
PSS is compacted. Compactness can trigger more
pairs that need to be coalesced, and therefore the
above procedure is repeated. When no pairs that
need to be coalesced are left, the remaining anony-
mous nodes are assigned arbitrary names and the
algorithm halts. The detailed algorithm is sup-
pressed for lack of space.
150
Example 5 Let S6 be the PSS depicted in Fig-
ure 5. Executing the name resolution algorithm
on this module results in the PSS of Figure 9
(AGR-labels are suppressed for readability.) The
two anonymous nodes in S6 are coalesced with
the nodes marked nagr and vagr, as per their
attributes. Cf. Figure 1, in particular how two
anonymous nodes in S1 are assigned types from
S5 (Figure 4).
gerund
n v vagr nagr
cat agr
Figure 9: Name resolution result for S6
7.2 Appropriateness consolidation
For each node q, the set of outgoing appropriate-
ness arcs with the same label F , {(q, F, q?)}, is
replaced by the single arc (q, F, ql), where ql is
marked by the lub of the types of all q?. If no lub
exists, a new node is added and is marked by the
lub. The result is that the appropriateness relation
is a function, and upward closure is preserved; fea-
ture introduction is dealt with separately.
The input to the following procedure is a PSS
whose typing function, T , is total; its output is a
PSS whose typing function, T , is total and whose
appropriateness relation is a function. Let S =
?Q, T,, Ap? be a PSS. For each q ? Q and F ?
FEAT, let
target(q, F ) = {q? | (q, F, q?) ? Ap}
sup(q) = {q? ? Q | q?  q}
sub(q) = {q? ? Q | q  q?}
out(q) = {(F, q?) | (q, F, q?) ? Ap
Algorithm 1 Appropriateness consolidation
(S = ?Q, T,, Ap?)
1. Find a node q and a feature F for which
|target(q, F )| > 1 and for all q? ? Q such
that q?
?
 q, |target(q?, F )| ? 1. If no such
pair exists, halt.
2. If target(q, F ) has a lub, p, then:
(a) for all q? ? target(q, F ), remove the arc
(q, F, q?) from Ap.
(b) add the arc (q, F, p) to Ap.
(c) for all q? ? Q such that q ? q?, if
(q?, F, p) /? Ap then add (q?, F, p) to
Ap.
(d) go to (1).
3. (a) Add a new node, p, to Q with:
? sup(p) = target(q, F )
? sub(p) = (target(q, F ))u
? out(p) = ?q??target(q,F ) out(q?)
(b) Mark p with a fresh type from NAMES.
(c) For all q? ? Q such that q ? q?, add
(q?, F, p) to Ap.
(d) For all q? ? target(q, F ), remove the
arc (q, F, q?) from Ap.
(e) Add (q, F, p) to Ap.
(f) go to (1).
The order in which nodes are selected in step 1
of the algorithm is from supertypes to subtypes.
This is done to preserve upward closure. In ad-
dition, when replacing a set of outgoing appropri-
ateness arcs with the same label F , {(q, F, q?)},
by a single arc (q, F, ql), ql is added as an ap-
propriate value for F and all the subtypes of q.
Again, this is done to preserve upward closure. If
a new node is added (stage 3), then its appropriate
features and values are inherited from its immedi-
ate supertypes. During the iterations of the algo-
rithm, condition 3b (maximality) of the definition
of a PSS may be violated but the resulting graph is
guaranteed to be a PSS.
Example 6 Consider the PSS depicted in Fig-
ure 9. Executing the appropriateness consolida-
tion algorithm on this module results in the module
depicted in Figure 10. AGR-labels are suppressed.
gerund new
n v vagr nagr
cat agr
Figure 10: Appropriateness consolidation result
8 Conclusions
We advocate the use of PSSs as the correct con-
cept of signature modules, supporting interaction
151
among grammar modules. Unlike existing ap-
proaches, our solution is formally defined, mathe-
matically proven and can be easily and efficiently
implemented. Module combination is a commuta-
tive and associative operation which meets all the
desiderata listed in section 4.
There is an obvious trade-off between flexibility
and strong typedeness, and our definitions can be
finely tuned to fit various points along this spec-
trum. In this paper we prefer flexibility, follow-
ing Melnik (2005), but future work will investigate
other options.
There are various other directions for future re-
search. First, grammar rules can be distributed
among modules in addition to the signature. The
definition of modules can then be extended to in-
clude also parts of the grammar. Then, various
combination operators can be defined for grammar
modules (cf. Wintner (2002)). We are actively pur-
suing this line of research.
Finally, while this work is mainly theoretical,
it has important practical implications. We would
like to integrate our solutions in an existing envi-
ronment for grammar development. An environ-
ment that supports modular construction of large
scale grammars will greatly contribute to gram-
mar development and will have a significant im-
pact on practical implementations of grammatical
formalisms.
9 Acknowledgments
We are grateful to Gerald Penn and Nissim
Francez for their comments on an earlier version
of this paper. This research was supported by The
Israel Science Foundation (grant no. 136/01).
References
Emily M. Bender, Dan Flickinger, and Stephan Oepen.
2002. The grammar matrix: An open-source starter-
kit for the rapid development of cross-linguistically
consistent broad-coverage precision grammars. In
Proceedings of ACL Workshop on Grammar Engi-
neering. Taipei, Taiwan, pages 8?14.
Emily M. Bender, Dan Flickinger, Fredrik Fouvry, and
Melanie Siegel. 2005. Shared representation in
multilingual grammar engineering. Research on
Language and Computation, 3:131?138.
Marie-He?le`ne Candito. 1996. A principle-based hier-
archical representation of LTAGs. In COLING-96,
pages 194?199, Copenhagen, Denemark.
Bob Carpenter. 1992. The Logic of Typed Feature
Structures. Cambridge Tracts in Theoretical Com-
puter Science. Cambridge University Press.
Ann Copestake and Dan Flickinger. 2000. An
open-source grammar development environment
and broad-coverage English grammar using HPSG.
In Proceedings of LREC, Athens, Greece.
Ann Copestake. 2002. Implementing typed feature
structures grammars. CSLI publications, Stanford.
Benoit Crabbe? and Denys Duchier. 2004. Metagram-
mar redux. In CSLP, Copenhagen, Denemark.
Mary Dalrymple. 2001. Lexical Functional Gram-
mar, volume 34 of Syntax and Semantics. Academic
Press.
Erhard W. Hinrichs, W. Detmar Meurers, and Shuly
Wintner. 2004. Linguistic theory and grammar im-
plementation. Research on Language and Compu-
tation, 2:155?163.
Ronald M. Kaplan, Tracy Holloway King, and John T.
Maxwell. 2002. Adapting existing grammars:
the XLE experience. In COLING-02 workshop on
Grammar engineering and evaluation, pages 1?7,
Morristown, NJ, USA.
Vlado Keselj. 2001. Modular HPSG. Technical Re-
port CS-2001-05, Department of Computer Science,
University of Waterloo, Waterloo, Ontario, Canada.
Tracy Holloway King, Martin Forst, Jonas Kuhn, and
Miriam Butt. 2005. The feature space in parallel
grammar writing. Research on Language and Com-
putation, 3:139?163.
Nurit Melnik. 2005. From ?hand-written? to imple-
mented HPSG theories. In Proceedings of HPSG-
2005, Lisbon, Portugal.
Nurit Melnik. 2006. A constructional approach to
verb-initial constructions in Modern Hebrew. Cog-
nitive Linguistics, 17(2). To appear.
Andrew M. Moshier. 1997. Is HPSG featureless or un-
principled? Linguistics and Philosophy, 20(6):669?
695.
Stephan Oepen, Daniel Flickinger, J. Tsujii, and Hans
Uszkoreit, editors. 2002. Collaborative Language
Engineering: A Case Study in Efficient Grammar-
Based Processing. CSLI Publications, Stanford.
Gerald B. Penn. 2000. The algebraic structure of
attributed type signatures. Ph.D. thesis, School
of Computer Science, Carnegie Mellon University,
Pittsburgh, PA.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. University of Chicago
Press and CSLI Publications.
Shuly Wintner. 2002. Modular context-free grammars.
Grammars, 5(1):41?63.
152
XFST2FSA: Comparing Two Finite-State Toolboxes
Yael Cohen-Sygal
Department of Computer Science
University of Haifa
 
	
Shuly Wintner
Department of Computer Science
University of Haifa
Towards Modular Development
of Typed Unification Grammars
Yael Sygal?
University of Haifa
Shuly Wintner??
University of Haifa
Development of large-scale grammars for natural languages is a complicated endeavor: Gram-
mars are developed collaboratively by teams of linguists, computational linguists, and computer
scientists, in a process very similar to the development of large-scale software. Grammars are
written in grammatical formalisms that resemble very-high-level programming languages, and
are thus very similar to computer programs. Yet grammar engineering is still in its infancy: Few
grammar development environments support sophisticated modularized grammar development,
in the form of distribution of the grammar development effort, combination of sub-grammars,
separate compilation and automatic linkage, information encapsulation, and so forth.
This work provides preliminary foundations for modular construction of (typed) unification
grammars for natural languages. Much of the information in such formalisms is encoded by
the type signature, and we subsequently address the problem through the distribution of the
signature among the different modules. We define signature modules and provide operators
of module combination. Modules may specify only partial information about the components
of the signature and may communicate through parameters, similarly to function calls in pro-
gramming languages. Our definitions are inspired by methods and techniques of programming
language theory and software engineering and are motivated by the actual needs of grammar
developers, obtained through a careful examination of existing grammars. We show that our def-
initions meet these needs by conforming to a detailed set of desiderata. We demonstrate the utility
of our definitions by providing a modular design of the HPSG grammar of Pollard and Sag.
1. Introduction
Development of large-scale grammars for natural languages is an active area of research
in human language technology. Such grammars are developed not only for purposes
of theoretical linguistic research, but also for natural language applications such as
machine translation, speech generation, and so on. Wide-coverage grammars are being
developed for various languages (Abeille?, Candito, and Kinyon 2000; XTAG Research
? Department of Computer Science, University of Haifa, 31905 Haifa, Israel.
E-mail: yael.sygal@gmail.com.
?? Department of Computer Science, University of Haifa, 31905 Haifa, Israel.
E-mail: shuly@cs.haifa.ac.il.
Submission received: 3 June 2009; revised submission received: 21 June 2010; accepted for publication:
14 September 2010.
? 2011 Association for Computational Linguistics
Computational Linguistics Volume 37, Number 1
Group 2001; Oepen et al 2002; Hinrichs, Meurers, and Wintner 2004; Bender et al 2005;
King et al 2005; Mu?ller 2007) in several theoretical frameworks, including TAG (Joshi,
Levy, and Takahashi 1975), LFG (Dalrymple 2001), HPSG (Pollard and Sag 1994), and
XDG (Debusmann, Duchier, and Rossberg 2005).
Grammar development is a complex enterprise: It is not unusual for a single gram-
mar to be developed by a team including several linguists, computational linguists, and
computer scientists. The scale of grammars is overwhelming?large-scale grammars
can be made up by tens of thousands of line of code (Oepen et al 2000) and may
include thousands of types (Copestake and Flickinger 2000). Modern grammars are
written in grammatical formalisms that are often reminiscent of very-high-level, declar-
ative (mostly logical) programming languages, and are thus very similar to computer
programs. This raises problems similar to those encountered in large-scale software
development (Erbach and Uszkoreit 1990). Although whereas software engineering
provides adequate solutions for the programmer, grammar engineering is still in its
infancy.
In this work we focus on typed unification grammars (TUG), and their implementa-
tion in grammar-development platforms such as LKB (Copestake 2002), ALE (Carpenter
and Penn 2001), TRALE (Meurers, Penn, and Richter 2002), or Grammix (Mu?ller 2007).
Such platforms conceptually view the grammar as a single entity (even when it is
distributed over several files), and provide few provisions for modular grammar de-
velopment, such as mechanisms for defining modules that can interact with each other
through well-defined interfaces, combination of sub-grammars, separate compilation
and automatic linkage of grammars, information encapsulation, and so forth. This is
the main issue that we address in this work.1
We provide a preliminary yet thorough and well-founded solution to the problem
of grammar modularization. We first specify a set of desiderata for a beneficial solu-
tion in Section 1.1, and then survey related work, emphasizing the shortcomings of
existing approaches with respect to these desiderata. Much of the information in typed
unification grammars is encoded in the signature, and hence the key is facilitating a
modularized development of type signatures. In Section 2 we introduce a definition
of signature modules, and show how two signature modules combine and how the
resulting signature module can be extended to a stand-alone type signature. We lift our
definitions from signatures to full grammar modules in Section 3. In Section 4 we use
signature modules and their combination operators to work out a modular design of
the HPSG grammar of Pollard and Sag (1994), demonstrating the utility of signature
modules for the development of linguistically motivated grammars. We then outline
MODALE, an implementation of our solutions which supports modular development
of type signatures in the context of both ALE and TRALE (Section 5). We show in
Section 6 how our solution complies with the desiderata of Section 1.1, and conclude
with directions for future research.
1.1 Motivation
The motivation for modular grammar development is straightforward. Like software
development, large-scale grammar development is much simpler when the task can be
cleanly distributed among different developers, provided that well-defined interfaces
govern the interaction among modules. From a theoretical point of view, modularity
1 This article extends and revises Cohen-Sygal and Wintner (2006) and Sygal and Wintner (2008).
30
Sygal and Wintner Modular Typed Unification Grammars
facilitates the definition of cleaner semantics for the underlying formalism and the
construction of correctness proofs. The engineering benefits of modularity in program-
ming languages are summarized by Mitchell (2003, page 235), and are equally valid for
grammar construction:
In an effective design, each module can be designed and tested independently. Two
important goals in modularity are to allow one module to be written with little
knowledge of the code in another module and to allow a module to be redesigned
and re-implemented without modifying other parts of the system.
A suitable notion of modularity should support ?reuse of software, abstraction mech-
anisms for information hiding, and import/export relationships? (Brogi et al 1994,
page 1363). Similarly, Bugliesi, Lamma, and Mello (1994, page 444) state that
[a] modular language should allow rich forms of abstraction, parametrization, and
information hiding; it should ease the development and maintenance of large programs
as well as provide adequate support or reusability and separate and efficient
compilation; it should finally encompass a non-trivial notion of program equivalence
to make it possible to justify the replacement of equivalent components.
In the linguistic literature, however, modularity has a different flavor which has to
do with the way linguistic knowledge is organized, either cognitively (Fodor 1983) or
theoretically (Jackendoff 2002, pages 218?230). Although we do not directly subscribe to
this notion of modularity in this work, it may be the case that an engineering-inspired
definition of modules will facilitate a better understanding of the linguistic notion.
Furthermore, although there is no general agreement among linguists on the exact form
of grammar modularity, a good solution for grammar development must not reflect the
correctness of linguistic theories but rather provide the computational framework for
their implementation.
To consolidate the two notions of modularity, and to devise a solution that is on one
hand inspired by developments in programming languages and on the other useful for
linguists, a clear understanding of the actual needs of grammar developers is crucial. A
first step in this direction was done by Erbach and Uszkoreit (1990). In a similar vein,
we carefully explored two existing grammars: the LINGO grammar matrix (Bender,
Flickinger, and Oepen 2002),2 which is a framework for rapid development of cross-
linguistically consistent grammars; and a grammar of a fragment of modern Hebrew,
focusing on inverted constructions (Melnik 2006). These grammars were chosen since
they are comprehensive enough to reflect the kind of data large-scale grammars encode,
but are not too large to encumber this process.
Inspired by established criteria for modularity in programming languages, and
motivated by our observation of actual grammars, we define the following desiderata
for a beneficial solution for (typed unification) grammar modularization:
Signature focus: Much of the information in typed formalisms is encoded by the signa-
ture. This includes the type hierarchy, the appropriateness specification, and the
2 The LINGO grammar matrix is not a grammar per se, but rather a framework for grammar development
for several languages. We focused on its core grammar and several of the resulting, language-specific
grammars.
31
Computational Linguistics Volume 37, Number 1
type constraints. Hence, modularization must be carried out mainly through the
distribution of the signature between the different modules.3
Partiality: Modules should provide means for specifying partial information about the
components of a grammar: both the grammar itself and the signature over which
it is defined.
Extensibility: Although modules can specify partial information, it must be possible to
deterministically extend a module (which can be the result of the combination of
several modules) into a full grammar.
Consistency: Contradicting information in different modules must be detected when
modules are combined.
Flexibility: The grammar designer should be provided with as much flexibility as
possible. Modules should not be unnecessarily constrained.
(Remote) Reference: A good solution should enable one module to refer to entities
defined in another. Specifically, it should enable the designer of module Mi to use
an entity (e.g., a type or a feature structure) defined in Mj without specifying the
entity explicitly.
Parsimony: When two modules are combined, the resulting module must include all
the information encoded in each of the modules and the information resulting
from the combination operation. Additional information must only be added if it
is essential to render the module well-defined.
Associativity: Module combination must be associative and commutative: The order
in which modules are combined must not affect the result. However, this desider-
atum is not absolute?it is restricted to cases where the combination formulates
a simple union of data. In other cases, associativity and commutativity should be
considered with respect to the benefit the system may enjoy if they are abandoned.
Privacy: Modules should be able to hide (encapsulate) information and render it un-
available to other modules.
The solution we advocate here satisfies all these requirements. It facilitates col-
laborative development of grammars, where several applications of modularity are
conceivable:
 A single large-scale grammar developed by a team.
 Development of parallel grammars for multiple languages under a single
theory, as in Bender et al (2005), King et al (2005), or Mu?ller (2007). Here,
a core module is common to all grammars, and language-specific
fragments are developed as separate modules.
 A sequence of grammars modeling language development, for example
language acquisition or (historical) language change (Wintner, Lavie, and
MacWhinney 2009). Here, a ?new? grammar is obtained from a ?previous?
grammar; formal modeling of such operations through module
composition can shed new light on the linguistic processes that take place
as language develops.
3 We restrict ourselves to standard type signatures (as defined by Carpenter [1992] and Penn [2000]),
ignoring type constraints which are becoming common in practical systems. We defer an extension of our
results to type constraints to future work.
32
Sygal and Wintner Modular Typed Unification Grammars
1.2 Related Work
1.2.1 Modularity in Programming Languages. Vast literature addresses modularity in pro-
gramming languages, and a comprehensive survey is beyond the scope of this work. As
unification grammars are in many ways very similar to logic programming languages,
our desiderata and solutions are inspired by works in this paradigm.
Modular interfaces of logic programs were first suggested by O?keefe (1985) and
Gaifman and Shapiro (1989). Combination operators that were proved suitable for
Prolog include the algebraic operators ? and ? of Mancarella and Pedreschi (1988);
the union and intersection operators of Brogi et al (1990); the closure operator of Brogi,
Lamma, and Mello (1993); and the set of four operators (encapsulation, union, inter-
section, and import) defined by Brogi and Turini (1995). For a comprehensive survey,
see Bugliesi, Lamma, and Mello (1994).
The ?merge? operator that we present in Section 2.4.2 is closely related to union
operations proposed for logic programming languages. We define no counterpart of
intersection-type operations, although such operations are indeed conceivable. Our
?attachment? operation is more in line with Gaifman and Shapiro (1989).
1.2.2 Initial Approaches: Modularized Parsing. Early attempts to address modularity in
linguistic formalisms share a significant disadvantage: The modularization is of the
parsing process rather than the grammar.
Kasper and Krieger (1996) describe a technique for dividing a unification-based
grammar into two components, roughly along the syntax/semantics axis. Their mo-
tivation is efficiency; observing that syntax usually imposes constraints on permissible
structures, and semantics usually mostly adds structure, they propose to parse with the
syntactic constraints first, and apply the semantics later. This is achieved by recursively
deleting the syntactic and semantic information (under their corresponding attributes
in the rules and the lexicon) for the semantic and syntactic parsers, respectively. This
proposal requires that a single grammar be given, from which the two components can
be derived. A more significant disadvantage of this method is that coreferences between
syntax and semantics are lost during this division (because reentrancies that represent
the connection between the syntax and the semantics are removed). Kasper and Krieger
observe that the intersection of the languages generated by the two grammars does not
yield the language of the original grammar.
Zajac and Amtrup (2000) present an implementation of a pipeline-like composition
operator that enables the grammar designer to break a grammar into sub-grammars that
are applied in a sequential manner at run-time. Such an organization is especially useful
for dividing the development process into stages that correspond to morphological
processing, syntax, semantics, and so on. The notion of composition here is such that
sub-grammar Gi+1 operates on the output of sub-grammar Gi; such an organization
might not be suitable for all grammar development frameworks. A similar idea is pro-
posed by Basili, Pazienza, and Zanzotto (2000); it is an approach to parsing that divides
the task into sub-tasks, whereby a module component Pi takes an input sentence at a
given state of analysis Si and augments this information in Si+1 using a knowledge base
Ki. Here, too, it is the processing system, rather than the grammar, which is modularized
in a pipeline fashion.
1.2.3 Modularity in Typed Unification Grammars. Keselj (2001) presents a modular Head-
driven Phrase Structure Grammar (HPSG), where each module is an ordinary HPSG
grammar, including an ordinary type signature, but each of the sets FEAT, TYPE, and
RULES is divided into two disjoint sets of private and public elements. The public
33
Computational Linguistics Volume 37, Number 1
sets consist of those elements that can communicate with elements from corresponding
sets in other modules, and private elements are those that are internal to the module.
Merging two modules is then defined by set union; in particular, the type hierarchies
are merged by unioning the two sets of types and taking the transitive closure of the
union of the two BCPOs (see Definition 2). The success of the merge of two modules
requires that the union of the two BCPOs be a BCPO.
While this work is the first to concretely define signature modules, it provides
a highly insufficient mechanism for supporting modular grammar development: The
requirement that each module include a complete type hierarchy imposes strong lim-
itations on the kind of information that modules can specify. It is virtually impossible
to specify partial information that is consistent with the complete type hierarchy re-
quirement. Furthermore, module composition becomes order-dependent as we show in
Example 8 (Section 2.4.2). Finally, the only channel of interaction between modules is
the names of the types. Our work is similar in spirit to Keselj (2001), but it overcomes
these shortcomings and complies with the desiderata of Section 1.1.
Kaplan, King, and Maxwell (2002) introduce a system designed for building a gram-
mar by both extending and restricting another grammar. An LFG grammar is presented
to the system in a priority-ordered sequence of files containing phrase-structure rules,
lexical entries, abbreviatory macros and templates, feature declarations, and finite-state
transducers for tokenization and morphological analysis. The grammar can include
only one definition of an item of a given type with a particular name (e.g., there can be
only one NP rule, potentially with many alternative expansions), and items in a file with
higher priority override lower priority items of the same type with the same name. The
override convention makes it possible to add, delete, or modify rules. However, when a
rule is modified, the entire rule has to be rewritten, even if the modifications are minor.
Moreover, there is no real concept of modularization in this approach because the only
interaction among files is overriding of information.
King et al (2005) augment LFG with a makeshift signature to allow modular
development of untyped unification grammars. In addition, they suggest that any de-
velopment team should agree in advance on the feature space. This work emphasizes
the observation that the modularization of the signature is the key for modular devel-
opment of grammars. However, the proposed solution is ad hoc and cannot be taken
seriously as a concept of modularization. In particular, the suggestion for an agreement
on the feature space undermines the essence of modular design.
To support rapid prototyping of deep grammars, Bender and Flickinger (2005)
propose a framework in which the grammar developer can select pre-written grammar
fragments, accounting for common linguistic phenomena that vary across languages
(e.g., word order, yes?no questions, and sentential negation). The developer can spec-
ify how these phenomena are realized in a given language, and a grammar for that
language is automatically generated, implementing that particular realization of the
phenomenon, integrated with a language-independent grammar core. This framework
addresses modularity in the sense that the entire grammar is distributed between sev-
eral fragments that can be combined in different ways, according to the user?s choice.
However, the notion of modularity is rather different here, as modules are pre-written
pieces of code which the grammar designer does not develop and whose interaction he
or she has little control over.
1.2.4 Modularity in Related Formalisms. The previously mentioned works emphasize the
fact that existing approaches to modular grammar development in the area of unifica-
tion grammars are still insufficient. The same problem has also been addressed in some
34
Sygal and Wintner Modular Typed Unification Grammars
other, related, formalisms; we now survey such works and discuss the applicability of
the proposed solutions to the problem of modularity in typed unification grammars.
Wintner (2002) defines the concept of modules for CFGs: The set of nonterminals
is partitioned into three disjoint classes of internal, exported, and imported elements.
The imported elements are those that are supplied to the module by other modules, the
exported elements are those it provides to the outside world, and the internal ones are
local to it. Two modules can be combined only if the set of internal elements of each
module is disjoint from the exported and imported sets of the other module as well as if
the exported sets are disjoint. Then the combination of two modules is done by simple
measures of set union. This is the infrastructure underlying the definition of modular
HPSG discussed earlier (Keselj 2001).
Provisions for modularity have also been discussed in the context of tree-adjoining
grammars (TAG) (Joshi, Levy, and Takahashi 1975). A wide-coverage TAG may contain
hundreds or even thousands of elementary trees, and syntactic structure can be redun-
dantly repeated in many of them (XTAG Research Group 2001; Abeille?, Candito, and
Kinyon 2000). Consequently, maintenance and extension of such grammars is a complex
task. To address these issues, several high-level formalisms were developed (Vijay-
Shanker 1992; Candito 1996; Duchier and Gardent 1999; Kallmeyer 2001). These for-
malisms take the metagrammar approach, where the basic units are tree descriptions (i.e.,
formulas denoting sets of trees) rather than trees. Tree descriptions are constructed by a
tree logic and combined through conjunction or inheritance; a module in this approach
is merely a tree description, and modules are combined by means of the control logic.
When trees are semantic objects, (i.e., the denotation of tree descriptions), there can be
various ways to refer to nodes in the trees in order to control the possible combination
of grammar modules. Several mechanisms have been suggested to facilitate reference
across modules (Candito 1996; Perrier 2000; Crabbe? and Duchier 2004; Kahane 2006).
The solution that we propose here embraces the idea of moving from concrete
objects (e.g., a concrete type signature) to descriptions thereof; but we take special care
to do so in a way that maintains the associativity of the main grammar combination
operator, in contrast to some earlier approaches (Sygal and Wintner 2009).
Debusmann, Duchier, and Rossberg (2005) introduce Extensible Dependency Gram-
mar (XDG), which is a general framework for dependency grammars that supports
modular grammar design. An XDG grammar consists of dimensions, principles, and a
lexicon; it characterizes a set of well-formed analyses. Each dimension is an attributed
labeled graph, and when a grammar consists of multiple dimensions (e.g., multigraphs),
they share the same set of nodes. A lexicon for a dimension is a set of total assignments
of nodes and labels. The main mechanism XDG uses to control analyses are principles,
that can be either local (imposing a constraint on the possible analysis of a specific
dimension) or multi-dimensional (constraining the analysis of several dimensions
with respect to each other). In XDG, principles are formulated using a type-system
that includes several kinds of elementary types (e.g., nodes, edges, graphs, and even
multigraphs) and complex types that are constructed incrementally over the elementary
types. Then, parameters range over types to formulate parametric principles. A feasible
XDG analysis amounts to a labeled graph in which each dimension is a subgraph,
such that all (parametric) principles are maintained (this may require nodes in differ-
ent subgraphs to be identified). XDG supports modular grammar design where each
dimension graph is a grammar module, and module interaction is governed through
multi-dimensional parametric principles.
This work emphasizes the importance of types as a mechanism for modularity.
Our work shares with XDG the use of graphs as the basic components and the use
35
Computational Linguistics Volume 37, Number 1
of parameters to enforce interaction among modules. In both works, each module
introduces constraints on the type system and interaction among modules through
parameters is used to construct a multigraph in which some of the nodes are identified.
In our approach, however, the type system is part of the grammar specification, and
modules are combined via explicit combination operations. In contrast, in XDG the
type mechanism is used externally, to describe objects, and a general description logic is
used to impose constraints. Another major difference has to do with expressive power:
Whereas unification grammars are Turing-equivalent, XDG is probably mildly context-
sensitive (Debusmann 2006).
The grammar formalism (GF) (Ranta 2007) is a typed functional programming
language designed for multilingual grammars. Ranta introduces a module system for
GF where a module can be either one of three kinds: abstract, concrete, or a resource
module. Each of them reflects the kind of data this module may include. A module of
type abstract includes abstract syntax trees which represent grammatical information,
such as semantic or syntactic data. A module of type concrete includes relations between
trees in the abstract module and relations between strings in the target language.
Communication between modules of these two types is carried out through inheri-
tance hierarchies similarly to object-oriented programs. Resource modules are a means
for code-sharing, independently of the hierarchies. The system of modules supports
development of multilingual grammars through replacement of certain modules with
others. A given grammar can also be extended by adding new modules. Additionally, to
avoid repetition of code with minor variations, GF allows the grammar writer to define
operations which produce new elements.
GF is purposely designed for multilingual grammars which share a core represen-
tation, and individual extensions to different languages are developed independently.
As such, the theoretical framework it provides is tailored for such needs, but is lacking
where general purpose modular applications are considered (see section 1.1 for exam-
ples of such conceivable applications). Mainly, GF forces the developer to pre-decide on
the relations between all modules (through the concrete module and inheritance hierar-
chies), whereas in an ideal solution the interaction between all modules should be left to
the development process. Each module should be able to independently declare its own
interface with other modules; then, when modules combine they may do so in any way
that is consistent with the interfaces of other modules. Furthermore, reference to mutual
elements in GF is carried out only through naming, again resulting in a weak interface
for module interaction. Finally, the operations that the grammar writer can define in GF
are macros, rather than functions, as they are expanded by textual replacement.
2. Modularization of the Signature
2.1 Typed Signatures
We assume familiarity with theories of (typed) unification grammar, as formulated by,
for example, Carpenter (1992) and Penn (2000). The definitions in this section set the
notation and recall basic notions. For a partial function F, ?F(x)?? (?F(x)??) means that F
is defined (undefined) for the value x; ?F(x) = F(y)? means that either F is defined both
for x and for y and assigns them equal values or it is undefined for both.
Definition 1
Given a partially ordered set ?P,??, the set of upper bounds of a subset S ? P is the set
Su = {y ? P | ?x ? S x ? y}.
36
Sygal and Wintner Modular Typed Unification Grammars
For a given partially ordered set ?P,??, if S ? P has a least element then it is unique,
and is denoted min(S).
Definition 2
A partially ordered set ?P,?? is a bounded complete partial order (BCPO) iff for every
S ? P such that Su = ?, Su has a least element, called a least upper bound (lub) and
denoted
?
S.
Definition 3
A type hierarchy is a non-empty, finite, bounded complete partial order ?TYPE,?.
Every type hierarchy ?TYPE,? always has a least type (written ?), because the
subset S = ? of TYPE has the non-empty set of upper bounds, Su = TYPE, which must
have a least element due to bounded completeness.
Definition 4
Let ?TYPE,? be a type hierarchy and let x, y ? TYPE. If x  y, then x is a supertype of y
and y is a subtype of x. If x  y, x = y and there is no z such that x  z  y and z = x, y
then x is an immediate supertype of y and y is an immediate subtype of x.
We follow the definitions of Carpenter (1992) and Penn (2000) in viewing subtypes
as greater than their supertypes (hence the least element ? and the notion of lub), rather
than the other way round (inducing a glb interpretation), which is sometimes common
in the literature (Copestake 2002).
Definition 5
Given a type hierarchy ?TYPE,? and a finite set of features FEAT, an appropriateness
specification is a partial function, Approp : TYPE ? FEAT ? TYPE such that for every
F ? FEAT:
1. (Feature Introduction) there is a type Intro(F) ? TYPE such that:
 Approp(Intro(F),F)?, and
 for every t ? TYPE, if Approp(t,F)?, then Intro(F)  t, and
2. (Upward Closure / Right Monotonocy) if Approp(s,F)? and s  t, then
Approp(t,F)? and Approp(s,F)  Approp(t,F).
Definition 6
A type signature is a structure ?TYPE,, FEAT,Approp?, where ?TYPE,? is a type
hierarchy, FEAT is a finite set of features, FEAT and TYPE are disjoint, and Approp is an
appropriateness specification.
Again, note that type constraints are not addressed in this work.
37
Computational Linguistics Volume 37, Number 1
2.2 Overview
We define signature modules (also referred to as modules herein), which are structures
that provide a framework for modular development of type signatures. These structures
follow two guidelines.
1. Signature modules contain partial information about a signature: part of
the subtyping relation (sometimes referred to in the literature as type
subsumption) and part of the appropriateness specification. The key here is
a move from concrete type signatures to descriptions thereof; rather than
specify types, a description is a graph whose nodes denote types and
whose arcs denote elements of the subtyping and appropriateness
relations of signatures.
2. Modules may choose which information to expose to other modules and
how other modules may use the information they encode. The denotation
of nodes is extended by viewing them as parameters: Similarly to
parameters in programming languages, these are entities through which
information can be imported to or exported from other modules. This is
done similarly to the way parametric principles are used by Debusmann,
Duchier, and Rossberg (2005).
We begin by defining the basic structure of signature modules in Section 2.3. We
then introduce (Section 2.4) two combination operators for signature modules which
facilitate interaction and (remote) reference among modules. We end this section by
showing how to extend a signature module into a bona fide type signature (Section 2.5).
2.3 Signature Modules
The definition of a signature module is conceptually divided into two levels of in-
formation. The first includes all the genuine information that may be encoded by a
signature, such as subtyping and appropriateness relations, types, and so forth. The
second level includes the parametric casting of nodes. This casting is not part of the core
of a signature, but rather a device that enables advanced module communication. Con-
sequently, we define signature modules in two steps. First, we define partially specified
signatures (PSSs), which are finite directed graphs that encode partial information
about the signature. Then, we extend PSSs to signature modules which are structures,
based on PSSs, that provide also a complete mechanism for module interaction and
(remote) reference.
We assume enumerable, disjoint sets TYPE of types, FEAT of features, and NODES
of nodes, over which signatures are defined.
Definition 7
A partially labeled graph (PLG) over TYPE and FEAT is a finite, directed labeled graph
P = ?Q,T,,Ap?, where:
1. Q ? NODES is a finite, nonempty set of nodes.
2. T : Q ? TYPE is a partial function, marking some of the nodes with types.
38
Sygal and Wintner Modular Typed Unification Grammars
3. ? Q ? Q is a relation specifying (immediate) subtyping.
4. Ap ? Q ? FEAT ? Q is a relation specifying appropriateness.
A partially specified signature (PSS) over TYPE and FEAT is a partially labeled graph
P = ?Q,T,,Ap?, where:
5. T is one to one.
6. ?? is antireflexive; its reflexive-transitive closure, denoted ?? ?, is
antisymmetric.
7. (Relaxed Upward Closure) for all q1, q?1, q2 ? Q and F ? FEAT, if
(q1,F, q2) ? Ap and q1 
?
q?1, then there exists q
?
2 ? Q such that q2 
?
q?2 and
(q?1,F, q
?
2) ? Ap
A PSS is a finite, directed graph whose nodes denote types and whose edges denote
the subtyping and appropriateness relations. Nodes can be marked by types through
the function T, but can also be anonymous (unmarked). Anonymous nodes facilitate
reference, in one module, to types that are defined in another module. T is one-to-one
(item 5), because we require that two marked nodes denote different types.
The ?? relation (item 3) specifies an immediate subtyping order over the nodes,
with the intention that this order hold later for the types denoted by nodes. This is why
?
?
? is required to be a partial order (item 6). The type hierarchy of an ordinary type
signature is required to be a BCPO, but current approaches (Copestake 2002) relax this
requirement to allow more flexibility in grammar design. Similarly, the type hierarchy
of PSSs is partially ordered but this order is not necessarily a bounded complete one.
Only after all modules are combined is the resulting subtyping relation extended to
a BCPO (see Section 2.5); any intermediate result can be a general partial order. Relax-
ing the BCPO requirement also helps guarantee the associativity of module combination
(see Example 8).4
Consider now the appropriateness relation. In contrast to type signatures, Ap is not
required to be a function. Rather, it is a relation which may specify several appropriate
nodes for the values of a feature F at a node q (item 4). The intention is that the eventual
value of Approp(T(q),F) be the lub of the types of all those nodes q? such that Ap(q,F, q?).
This relaxation reflects our initial motivation of supporting partiality in modular gram-
mar development, since different modules may specify different appropriate values ac-
cording to their needs and available information. After all modules are combined, all the
specified values are replaced by a single appropriate value, their lub (see Section 2.5). In
this way, each module may specify its own appropriate values without needing to know
the value specification of other modules. We do restrict the Ap relation, however, by a
relaxed version of upward closure (item 7). Finally, the feature introduction condition of
type signatures (Definition 5, item 1) is not enforced by signature modules. This, again,
4 The fact that the subtyping relation is only extended to a BCPO after all modules are combined implies
a lack of incrementality in the system that may be problematic for grammar developers, as modules
cannot be tested and evaluated independently. This situation, however, is not unlike the scenario of
programming languages, where modules can typically be developed and compiled, but not tested,
independently of a complete system.
39
Computational Linguistics Volume 37, Number 1
results in more flexibility for the grammar designer; the condition can be restored, if it
is desirable, after all modules combine (see Section 2.5).
Example 1
A simple PSS P1 is depicted in Figure 1, where solid arrows represent the ?? (subtyp-
ing) relation and dashed arrows, labeled by features, the Ap relation. P1 stipulates two
subtypes of cat, n and v, with a common subtype, gerund. The feature AGR is appropriate
for all three categories, with distinct (but anonymous) values for Approp(n, AGR) and
Approp(v, AGR). Approp(gerund, AGR) will eventually be the lub of Approp(n, AGR) and
Approp(v, AGR), hence the multiple outgoing AGR arcs from gerund.
Observe that in P1, ?? is not a BCPO, Ap is not a function, and the feature introduc-
tion condition does not hold.
Definition 8
A pre-signature module over TYPE and FEAT is a structure S = ?P, Int, Imp,Exp? where
P = ?Q,T,,Ap? is a PLG and:
1. Int ? Q is a set of internal types
2. Imp ? Q is an ordered set of imported parameters
3. Exp ? Q is an ordered set of exported parameters
4. Int ? Imp = Int ? Exp = ?
5. for all q ? Q such that q ? Int, T(q)?
We refer to elements of (the sequences) Imp and Exp using indices, with the notation
Imp[i],Exp[j], respectively.
A signature module over TYPE and FEAT is a pre-signature module S = ?P, Int,
Imp,Exp? in which P is a PSS.
Signature modules extend the denotation of nodes by viewing them as parameters:
Similarly to parameters in programming languages, parameters are entities through
which information can be imported from or exported to other modules. The nodes
of a signature module are distributed among three sets of internal, imported, and
exported nodes. If a node is internal it cannot be imported or exported; but a node
Figure 1
A partially specified signature, P1.
40
Sygal and Wintner Modular Typed Unification Grammars
Figure 2
A signature module, S1.
can be simultaneously imported and exported. A node which does not belong to any
of the sets is called external. All nodes denote types, but they differ in the way they
communicate with nodes in other modules. As their name implies, internal nodes are
internal to one module and cannot interact with nodes in other modules. Such nodes
provide a mechanism similar to local variables in programming languages.
Non-internal nodes may interact with the nodes in other modules: Imported nodes
expect to receive information from other modules, while exported nodes provide informa-
tion to other modules. External nodes differ from imported and exported nodes in the
way they may interact with other modules, and provide a mechanism similar to global
variables in programming languages. Because anonymous nodes facilitate reference, in
one module, to information encoded in another module, such nodes cannot be internal.
The imported and exported nodes are ordered in order to control the assignment of
parameters when two modules are combined, as will be shown subsequently.5 In the
examples, the classification of nodes is encoded graphically as follows:
Internal Imported Exported External
Example 2
Figure 2 depicts a module S1, based on the PSS of Figure 1. S1 = ?P1, Int1, Imp1,Exp1?,
where P1 is the PSS of Figure 1, Int1 = ?, Imp1 = {q4, q5}, and Exp1 = ?.
Herein, the meta-variable q (with or without subscripts) ranges over nodes, S (with
or without subscripts) ? over (pre-)signature modules, P (with or without subscripts)
over PLGs and PSSs, and Q,T,,Ap (with the same subscripts) over their constituents.
2.4 Combination Operators for Signature Modules
We introduce two operators for combining signature modules. The first operator, merge,
is a symmetric operation which simply combines the information encoded in the two
5 In fact, Imp and Exp can be general sets, rather than lists, as long as the combination operations can
deterministically map nodes from Exp to nodes of Imp. For simplicity, we limit the discussion to the
familiar case of lists, where matching elements from Exp to Imp is done by the location of the element
on the list, see Definitions 13 and 14.
41
Computational Linguistics Volume 37, Number 1
modules. The second operator, attachment, is a non-symmetric operation which uses
the concept of parameters and is inspired by function composition. A signature module
is viewed as a function whose input is a graph with a list of designated imported
nodes and whose output is a graph with a list of designated exported nodes. When
two signature modules are attached, similarly to function composition, the exported
nodes of the second module instantiate the imported parameters of the first module.
Additionally, the information encoded by the second graph is added to the information
encoded by the first one.
The parametric view of modules facilitates interaction between modules in two
channels: by naming or by reference. Through interaction by naming, nodes marked
by the same type are coalesced. Interaction by reference is achieved when the imported
parameters of the calling module are coalesced with the exported nodes of the called
module, respectively. The merge operation allows modules to interact only through
naming, whereas attachment facilitates both ways of interaction.
For both of the operators, we assume that the two signature modules are consistent:
One module does not include types which are internal to the other module and the
two signature modules have no common nodes. If this is not the case, nodes, and in
particular internal nodes, can be renamed without affecting the operation.
Definition 9
Let S1 = ??Q1,T1,1,Ap1?, Int1, Imp1,Exp1?, S2 = ??Q2,T2,2,Ap2?, Int2, Imp2,Exp2? be
two pre-signature modules. S1 and S2 are consistent iff all the following conditions
hold:
1. {T1(q) | q ? Int1} ? {T2(q) | q ? Q2 and T2(q)?} = ?
2. {T2(q) | q ? Int2} ? {T1(q) | q ? Q1 and T1(q)?} = ?
3. Q1 ? Q2 = ?
We begin by introducing the compactness algorithm which is used when two
modules are combined as a mechanism to coalesce corresponding nodes in the two
modules.
2.4.1 Compactness. When two modules are combined, a crucial step in the combination is
the identification of corresponding nodes in the two modules that should be coalesced.
Such pairs of nodes can be either of two kinds:
1. Two typed nodes which are labeled by the same type should be coalesced
(along with their attributes).
2. Two anonymous nodes which are indistinguishable, that is, have
isomorphic environments, should be coalesced. The environment of a
node q is the subgraph that includes all the reachable nodes via any kind
of arc (from q or to q) up to and including a typed node. The intuition is
that if two anonymous nodes have isomorphic environments, then they
cannot be distinguished and therefore should coincide. Two nodes, only
one of which is anonymous, can still be otherwise indistinguishable. Such
nodes will, eventually, be coalesced, but only after all modules are
combined (to ensure the associativity of module combination).
42
Sygal and Wintner Modular Typed Unification Grammars
Additionally, during the combination of modules, some arcs may become redun-
dant (such arcs are not prohibited by the definition of a module). Redundant arcs can
be of two kinds:
1. A subtyping arc (q1, q2) is redundant if it is a member of the transitive
closure of , where  excludes (q1, q2).
2. An appropriateness arc (q1,F, q2) is redundant if there exists q3 ? Q such
that q2 
?
q3 and (q1,F, q3) ? Ap. (q1,F, q2) is redundant due to the lub
intention of appropriateness arcs: The eventual value of Approp(T(q1),F)
will be an upper bound of (at least) both q2 and q3. Because q2 
?
q3,
(q1,F, q2) is redundant.
Redundant arcs encode information that can be inferred from other arcs and therefore
may be removed without affecting the data encoded by the signature module.
While our main interest is in signature modules, the compactness algorithm is
defined over the more general case of pre-signature modules. This more general
notion will be helpful in the definition of module combination. Informally, when a
pre-signature module is compacted, redundant arcs are removed, nodes marked by
the same type are coalesced, and anonymous indistinguishable nodes are identified.
Additionally, the parameters and arities are induced from those of the input pre-
signature module. All parameters may be coalesced with each other, as long as they are
otherwise indistinguishable. If (at least) one of the coalesced nodes is an internal node,
then the result is an internal node. Otherwise, if one of the nodes is imported then
the resulting parameter is imported as well. Similarly, if one of the nodes is exported
then the resulting parameter is exported. Notice that in the case of signature modules,
because T is one to one, an internal node may be coalesced only with other internal
nodes.
The actual definitions of indistinguishability and the compactness algorithm are
mostly technical and are therefore deferred to the Appendix. We do provide two simple
examples to illustrate the general idea.
Example 3
Consider the signature module of Figure 3. (q1, q4) is a redundant subtyping arc be-
cause even without this arc, there is a subtyping path from q1 to q4. (q1,F, q3) is a
redundant appropriateness arc: Eventually the appropriate value of q1 and F should be
the lub of q3 and q5, but since q5 is a subtype of q3, it is sufficient to require that it be at
least q5.
Example 4
Consider S2, the pre-signature module depicted in Figure 4. Note that S2 is not a
signature module (because it includes two nodes labeled by a) and that compactness
is defined over pre-signature modules rather than signature modules as this is the case
for which it will be used during combination. In compact(S2), q1 and q2 are coalesced
because they are both marked by the type a. Additionally, q3 and q6 are coalesced with q4
and q7, respectively, because these are two pairs of anonymous nodes with isomorphic
environments. q5 is not coalesced with q3 and q4 because q5 is typed and q3 and q4 are
not, even though they are otherwise indistinguishable. q8 is not coalesced with q6 and
q7 because they are distinguishable: q8 has a supertype marked by a whereas q6 and q7
have anonymous supertypes.
43
Computational Linguistics Volume 37, Number 1
Figure 3
A signature module with redundant arcs.
Figure 4
Compactness.
2.4.2 Merge. The merge operation combines the information encoded by two signature
modules: Nodes that are marked by the same type are coalesced along with their
attributes. Nodes that are marked by different types cannot be coalesced and must
denote different types. The main complication arises when two anonymous nodes are
considered?such nodes are coalesced only if they are indistinguishable.
The merge of two modules is defined in several stages: First, the two graphs are
unioned (this is a simple pointwise union of the coordinates of the graph, see Defi-
nition 10). Then, the resulting graph is compacted, coalescing nodes marked by the
same type as well as indistinguishable anonymous nodes. However, the resulting graph
does not necessarily maintain the relaxed upward closure condition, and therefore some
modifications are needed. This is done by Ap-Closure (see Definition 11). Finally, the
addition of appropriateness arcs may turn two anonymous distinguishable nodes into
indistinguishable ones and may also add redundant arcs, therefore another compact-
ness step is needed (Definition 12).
Definition 10
Let S1 = ??Q1,T1,1,Ap1?, Int1, Imp1,Exp1?, S2 = ??Q2,T2,2,Ap2?, Int2, Imp2,Exp2? be
two consistent pre-signature modules. The union of S1 and S2, denoted S1 ? S2, is the
pre-signature module
S = ??Q1 ? Q2,T1 ? T2,1 ? 2,Ap1 ? Ap2?, Int1 ? Int2, Imp1 ? Imp2,Exp1 ? Exp2?
(where ??? is the concatenation operator).
44
Sygal and Wintner Modular Typed Unification Grammars
Definition 11
Let S = ??Q,T,,Ap?, Int, Imp,Exp? be a pre-signature module. The Ap-Closure of S,
denoted ApCl(S), is the pre-signature module ??Q,T,,Ap??, Int, Imp,Exp? where
Ap? = {(q1,F, q2) | q1, q2 ? Q
and there exists q?1 ? Q such that
q?1
?
 q1 and (q?1,F, q2) ? Ap}
Ap-Closure adds to a pre-signature module the required arcs for it to maintain
the relaxed upward closure condition: Arcs are added to create the relations between
elements separated between the two modules and related by mutual elements. Notice
that Ap ? Ap? by choosing q?1 = q1.
Two signature modules can be merged only if the resulting subtyping relation is
indeed a partial order, where the only obstacle can be the antisymmetry of the resulting
relation. The combination of the appropriateness relations, in contrast, cannot cause
the merge operation to fail because any violation of the appropriateness conditions
in signature modules can be deterministically resolved. Note that our specification
language does not support inequations; there is no way to specify that two nodes must
not be identified with each other. Such extensions are indeed possible, but are beyond
the scope of this work.
Definition 12
Let S1 = ??Q1,T1,1,Ap1?, Int1, Imp1,Exp1?, S2 = ??Q2,T2,2,Ap2?, Int2, Imp2,Exp2? be
two consistent signature modules. S1,S2 are mergeable if there are no q1, q2 ? Q1 and
q3, q4 ? Q2 such that the following hold:
1. q1 = q2 and q3 = q4
2. T1(q1)?, T1(q2)?, T2(q3)? and T2(q4)?
3. T1(q1) = T2(q4) and T1(q2) = T2(q3)
4. q1 
?
1 q2 and q3 
?
2 q4
If S1 and S2 are mergeable, then their merge, denoted S1 uniondbl S2, is:
compact(ApCl(compact(S1 ? S2)))
In the merged module, pairs of nodes marked by the same type and pairs of
indistinguishable anonymous nodes are coalesced. An anonymous node cannot be
coalesced with a typed node, even if they are otherwise indistinguishable, because
that would result in a non-associative combination operation. Anonymous nodes are
assigned types only after all modules combine (see Section 2.5.1).
If a node has multiple outgoing Ap-arcs labeled with the same feature, these arcs are
not replaced by a single arc, even if the lub of the target nodes exists in the resulting sig-
nature module. Again, this is done to guarantee the associativity of the merge operation
(see Example 9).
45
Computational Linguistics Volume 37, Number 1
Figure 5
Merge: intermediate steps.
Example 5
Let S3 and S4 be the signature modules depicted in Figure 5. S3 uniondbl S4 and the intermediate
pre-signature modules are also shown in this figure. First, S3 and S4 are unioned. Then,
in compact(S3 ? S4) the two nodes typed by a are coalesced, as are the nodes typed by
c. Notice that this pre-signature module is not a signature module because it does not
maintain the relaxed upward closure condition. To enforce this condition appropriate-
ness arcs are added to yield ApCl(compact(S3 ? S4)), but this signature module includes
indistinguishable anonymous nodes and therefore another compactness operation is
required to yield the final result.
Example 6
Figure 6 depicts a naive agreement module, S5. Combined with S1 of Figure 1, S1 uniondbl S5 =
S5 uniondbl S1 = S6. All dashed arrows are labeled AGR, but these labels are suppressed for
readability.
In what follows, by standard convention, Ap arcs that can be inferred by upward
closure are not depicted.
Figure 6
Merge.
46
Sygal and Wintner Modular Typed Unification Grammars
Example 7
Let S7 and S8 be the signature modules depicted in Figure 7. S7 includes general
agreement information and S8 specifies detailed values for several specific properties.
Then, S7 uniondbl S8 = S8 uniondbl S7 = S9. In this way, the high level organization of the agreement
module is encoded by S7, and S8 provides low level details pertaining to each agreement
feature individually.
The following example motivates our decision to relax the BCPO condition and
defer the conversion of signature modules to BCPOs to a separate resolution stage
(Section 2.5).
Example 8
Let S10,S11,S12 be the signature modules depicted in Figure 8. The merge of S10 with S11
results in a non-BCPO. However, the additional information supplied by S12 resolves
the problem, and S10 uniondbl S11 uniondbl S12 is bounded complete.
Example 9
Let S13,S14,S15 be the signature modules depicted in Figure 9. In S13 the appropriate
value for a and F is b and in S14 it is c. Hence S13 uniondbl S14 states that the appropriate value
for a and F should be lub(b, c). Although in this module there is no such element, in S15
lub(b, c) is determined to be d. In S13 uniondbl S14 uniondbl S15 the two outgoing arcs from the node
marked by a are not replaced by a single arc whose target is the node marked by d, since
Figure 7
Merge.
47
Computational Linguistics Volume 37, Number 1
Figure 8
BCPO relaxation.
Figure 9
Merge of signature modules.
other signature modules may specify that the lub of b and c is some type other than d.
These multiple outgoing arcs are preserved to maintain the associativity of the merge
operation.
Proposition 1
Merge is commutative: For any two signature modules, S1,S2, Let S = S1 uniondbl S2 and S? =
S2 uniondbl S1 where P,P? are their underlying PSSs, respectively. Then P = P?. In particular,
either both are defined or both are undefined.
The proof follows immediately from the fact that the merge operation is defined by
set union and equivalence relations which are commutative operations.
Proposition 2
Merge is associative up to isomorphism:6 for all S1,S2,S3, Let S = (S1 uniondbl S2) uniondbl S3 and
S? = S1 uniondbl (S2 uniondbl S3) where P,P? are their underlying PSSs, respectively. Then P ? P?.
6 The definition of signature module isomorphism is a simple extension of graph isomorphism; see the
Appendix for more details.
48
Sygal and Wintner Modular Typed Unification Grammars
The proof of associativity is similar in spirit to the proof of the associativity of (polar-
ized) forest combination (Sygal and Wintner 2009) and is therefore suppressed.
2.4.3 Attachment. Consider again S1 and S9, the signature modules of Figures 1 and
7, respectively. S1 stipulates two distinct (but anonymous) values for Approp(n, AGR)
and Approp(v, AGR). S9 stipulates two nodes, typed nagr and vagr, with the intention
that these nodes be coalesced with the two anonymous nodes of S1. However, the
?merge? operation defined in the previous section cannot achieve this goal, since the two
anonymous nodes in S1 have different attributes from their corresponding typed nodes
in S9. In order to support such a unification of nodes we need to allow a mechanism
that specifically identifies two designated nodes, regardless of their attributes. The
parametric view of nodes facilitates exactly such a mechanism.
The attachment operation is an asymmetric operation, like function composition,
where a signature module, S1, receives as input another signature module, S2. The
information encoded in S2 is added to S1 (as in the merge operation), but additionally,
the exported parameters of S2 are assigned to the imported parameters of S1: Each of
the exported parameters of S2 is forced to coalesce with its corresponding imported
parameter of S1, regardless of the attributes of these two parameters (i.e., whether they
are indistinguishable or not).
Definition 13
Let S1= ??Q1,T1,1,Ap1?, Int1, Imp1,Exp1? and S2= ??Q2,T2,2,Ap2, ?, Int2, Imp2, Exp2?
be two consistent signature modules. S2 can be attached to S1 if the following conditions
hold:
1. |Imp1| = |Exp2|
2. for all i, 1 ? i ? |Imp1|, if T1(Imp1[i])? and T2(Exp2[i])?, then
T1(Imp1[i]) = T2(Exp2[i])
3. S1 and S2 are mergeable
4. for all i, j, 1 ? i ? |Imp1| and 1 ? j ? |Imp1|, if Imp1[i] 
?
1 Imp1[j], then
Exp2[j] 
?
2 Exp2[i]
The first condition requires that the number of formal parameters of the calling module
be equal to the number of actual parameters in the called module. The second condition
states that if two typed parameters are attached to each other, they are marked by the
same type. If they are marked by two different types they cannot be coalesced.7 Finally,
the last two conditions guarantee the antisymmetry of the subtyping relation in the
resulting signature module: The third condition requires the two signature modules to
be mergeable. The last condition requires that no subtyping cycles be created by the
attachment of parameters: If q1 is a supertype of q?1 in S1 and q2 is a supertype of q
?
2 in
S2, then q?2 and q2 cannot be both attached to q1 and q
?
1, respectively. Notice that as in the
merge operation, two signature modules can be attached only if the resulting subtyping
7 A variant of attachment can be defined in which if two typed parameters, which are attached to each
other, are marked by two different types, then the type of the exported node overrides the type of the
imported node.
49
Computational Linguistics Volume 37, Number 1
relation is indeed a partial order, where the only obstacle can be the antisymmetry of the
resulting relation. The combination of the appropriateness relations, in contrast, cannot
cause the attachment operation to fail because any violation of the appropriateness
conditions in signature modules can be deterministically resolved.8
Definition 14
Let S1 = ??Q1,T1,1,Ap1?, Int1, Imp1,Exp1? and S2 = ??Q2,T2,2,Ap2, ?, Int2, Imp2, Exp2?
be two consistent signature modules. If S2 can be attached to S1, then the attachment of
S2 to S1, denoted S1(S2), is: S1(S2) = compact(ApCl(compact(S))), where S = ??Q,T,,
Ap?, Int, Imp,Exp? is defined as follows: Let ? be an equivalence relation over Q1 ? Q2
defined by the reflexive and symmetric closure of {(Imp1[i],Exp2[i]) | 1 ? i ? |Imp1|}.
Then:
 Q = {[q]? | q ? Q1 ? Q2}
 T([q]?) =
{
T1 ? T2(q?) there exists q? ? [q]? such that T1 ? T2(q?)?
? otherwise
 = {([q1]?, [q2]?) | ?q?1 ? [q1]? and ?q?2 ? [q2]? and (q?1, q?2) ?1 ? 2}
 Ap = {([q1]?,F, [q2]?) | ?q?1 ? [q1]? and ?q?2 ? [q2]? and (q?1,F, q?2) ?
Ap1 ? Ap2}
 Int = {[q]? | q ? Int1 ? Int2}
 Imp = {[q]? | q ? Imp1}
 Exp = {[q]? | q ? Exp1}
 the order of Imp and Exp is induced by the order of Imp1 and Exp1,
respectively
When a module S2 is attached to a module S1, all the exported nodes of S2 are first
attached to the imported nodes of S1, respectively, through the equivalence relation,
???. In this way, for each imported node of S1, all the information encoded by the
corresponding exported node of S2 is added. Notice that each equivalence class of
??? contains either one or two nodes. In the former case, these nodes are either non-
imported nodes of S1 or non-exported nodes of S2. In the latter, these are pairs of an
imported node of S1 and its corresponding exported node from S2. Hence ??? is trivially
transitive. Then, similarly to the merge operation, pairs of nodes marked by the same
type and pairs of indistinguishable anonymous nodes are coalesced. In contrast to the
merge operation, in the attachment operation two distinguishable anonymous nodes, as
well as an anonymous node and a typed node, can be coalesced. This is achieved by the
parametric view of nodes and the view of one module as an input to another module.
The imported and exported nodes of the resulting module are the equivalence
classes of the imported and exported nodes of the first module, S1, respectively. The
nodes of S2 which are neither internal nor exported are classified as external nodes in the
resulting module. This asymmetric view of nodes stems from the view of S1 receiving
S2 as input: In this way, S1 may import further information from other modules.
8 Relaxed variants of these conditions are conceivable; for example, one can require |Imp1| ? |Exp2| rather
than |Imp1| = |Exp2|; or that T1(Imp1[i]) and T2(Exp2[i]) be consistent rather than equal.
50
Sygal and Wintner Modular Typed Unification Grammars
Notice that in the attachment operation internal nodes facilitate no interaction
between modules, external nodes facilitate interaction only through naming, and im-
ported and exported nodes facilitate interaction both through naming and by reference.
Example 10
Consider again S1 and S9, the signature modules of Figures 1 and 7, respectively. Let
S1a and S9a be the signature modules of Figure 10 (these signature modules have the
same underlying graphs as those of S1 and S9, respectively, with different classification
of nodes). Notice that all nodes in both S1a and S9a are non-internal. Let Imp1a = ?q4, q5?
and let Exp9a = ?p9, p10?. S1a(S9a) is depicted in the same figure. Notice how q4, q5 are
coalesced with p9, p10, respectively, even though q4, q5 are anonymous and p9, p10 are
typed and each pair of nodes has different attributes. Such unification of nodes cannot
be achieved with the merge operation.
Figure 10
Attachment.
51
Computational Linguistics Volume 37, Number 1
2.4.4 Example: Parametric Lists. Lists and parametric lists are extensively used in typed
unification-based formalisms, for example in HPSG. The mathematical foundations
for parametric lists were established by Penn (2000). As an example of the utility of
signature modules and the attachment operation, we show how they can be used to
construct parametric lists in a straightforward way.
Consider Figure 11. The signature module List depicts a parametric list module. It
receives as input, through the imported node q3, a node which determines the type of
the list members. The entire list can then be used through the exported node q4. Notice
that q2 is an external anonymous node. Although its intended denotation is the type
ne list, it is anonymous in order to be unique for each copy of the list, as will be shown
subsequently. Now, if Phrase is a simple module consisting of one exported node, of type
phrase, then the signature module obtained by List(Phrase) is obtained by coalescing q3,
the imported node of List with the single exported node of Phrase.
Other modules can now use lists of phrases; for example, the module Struct uses
an imported node as the appropriate value for the feature COMP-DTRS. Via attachment,
this node can be instantiated by List(Phrase) as in Struct(List(Phrase)). The single node
of Phrase instantiates the imported node of List, thus determining a list of phrases. The
entire list is then attached to the signature module Struct, where the root of the list
instantiates the imported node typed by phrase list in Struct.
More copies of the list with other list members can be created by different calls to the
module List. Each such call creates a unique copy of the list, potentially with different
types of list elements. Uniqueness is guaranteed by the anonymity of the node q2 of
List: q2 can be coalesced only with anonymous nodes with the exact same attributes,
that is, only with nodes whose appropriate value for the feature FIRST is a node typed
Figure 11
Implementing parametric lists with signature modules.
52
Sygal and Wintner Modular Typed Unification Grammars
by phrase. If q2 would have been typed by ne list it could be coalesced with any other
node marked by the same type, such as other such nodes from different copies of the
list, resulting in a list whose members have various types. Observe that the uniqueness
of each copy of the list could be achieved also by declaring q2 an internal node, but this
solution prevents other modules from referring to this node, as is reasonably desired. q1
(of List) is typed by elist. Because only one copy of this node is required for all the list
copies, there is no problem with typing this node.
Compared with the parametric type signatures of Penn (2000), our implementation
of parametric lists is simple and general: It falls out directly as one application of
signature modules, whereas the construction of Penn requires dedicated machinery
(parametric subtyping, parametric appropriateness, coherence, etc.) We conjecture that
signature modules can be used to simulate parametric type signatures in the general
case, although we do not have a proof of such a result.
2.4.5 Example: The ?Addendum? Operator in LKB. The ?addendum? operator9 was added to
the type definition language of LKB (Copestake 2002) in 2005, to allow the grammar
developer to add attributes to an already defined type without the need to repeat
previously defined attributes of that type. The need for such an operation arose as
a consequence of the development of frameworks that generate grammars from pre-
written fragments (e.g., the LINGO grammar matrix, Bender, Flickinger, and Oepen
2002), since editing of framework-source files may lead to errors.
Signature modules trivially support this operator, either by the merge operation (in
which case different attributes of a typed node are gathered from different modules)
or by attachment, where attributes can be assigned to a specific node, even without
specifying its type.
2.5 Extending Signature Modules to Type Signatures
Signature modules encode only partial information, and are therefore not required to
conform with all the constraints imposed on ordinary signatures. After modules are
combined, however, the resulting signature module must be extended into a bona fide
signature. For that purpose we use four algorithms, each of which deals with one
property:
1. Name resolution: This algorithm assigns types to anonymous nodes
(Section 2.5.1).
2. Appropriateness consolidation: This algorithm determinizes Ap, converts
it from a relation to a function and enforces upward closure (Section 2.5.2).
3. Feature introduction completion: This algorithm (whose use is optional)
enforces the feature introduction condition. This is done using the
algorithm of Penn (2000).
4. BCPO completion: This algorithm extends ?? to a BCPO. Again, we use
the algorithm of Penn (2000).
9 See http://depts.washington.edu/uwcl/twiki/bin/view.cgi/Main/TypeAddendum.
53
Computational Linguistics Volume 37, Number 1
The input to the resolution algorithm is a signature module and its output is a bona
fide type signature.
Algorithm 1 (Resolve (S))
1. S := NameResolution(S)
2. S := BCPO?Completion(S)
3. S := ApCl(S)
4. S := ApConsolidate(S)
5. S := FeatureIntroductionCompletion(S)
6. S := BCPO?Completion(S)
7. S := ApCl(S)
8. S := ApConsolidate(S)
9. return S
The order in which the four algorithms are executed is crucial for guaranteeing
that the result is indeed a bona fide signature. First, the resolution algorithm assigns
types to anonymous nodes via the name resolution algorithm (stage 1). The BCPO
completion algorithm (stage 2) of Penn (2000) adds types as least upper bounds for sets
of types which have upper bounds but do not have a minimal upper bound. However,
the algorithm does not determine the appropriateness specification of these types. A
natural solution to this problem is to use Ap-Closure (stage 3) but this may lead to a
situation in which the newly added nodes have multiple outgoing Ap-arcs with the
same label. To solve the problem, we execute the BCPO completion algorithm before
the Ap-consolidation algorithm (stage 4), which also preserves bounded completeness.
Now, the feature introduction completion algorithm (stage 5) of Penn assumes that
the subtyping relation is a BCPO and that the appropriateness specification is indeed
a function and hence, it is executed after the BCPO completion and Ap-consolidation
algorithms. However, as Penn observes, this algorithm may disrupt bounded complete-
ness and therefore the result must undergo another BCPO completion and therefore
another Ap-consolidation (stages 6?8).
A signature module is extended to a type signature after all the information from
the different modules have been gathered. Therefore, there is no need to preserve the
classification of nodes and only the underlying PSS is of interest. However, because the
resolution procedure uses the compactness algorithm which is defined over signature
modules, we define the following algorithms over signature modules as well. In cases
where the node classification needs to be adjusted, we simply take the trivial classifica-
tion (i.e., Int = Imp = Exp = ?).
2.5.1 Name Resolution. During module combination only pairs of indistinguishable
anonymous nodes are coalesced. Two nodes, only one of which is anonymous, can still
be otherwise indistinguishable but they are not coalesced during combination to ensure
the associativity of module combination. The goal of the name resolution procedure is
to assign a type to every anonymous node, by coalescing it with a typed node with an
identical environment, if one exists. If no such node exists, or if there is more than one
such node, the anonymous node is given an arbitrary type.
54
Sygal and Wintner Modular Typed Unification Grammars
The name resolution algorithm iterates as long as there are nodes to coalesce. In each
iteration, for each anonymous node the set of its typed equivalent nodes is computed
(stage 1). Then, using the computation of stage 1, anonymous nodes are coalesced with
their corresponding typed node, if such a node uniquely exists (stage 2.1). Coalescing all
such pairs may result in a signature module that may include indistinguishable anony-
mous nodes and therefore the signature module is compacted (stage 2.2). Compactness
can trigger more pairs that need to be coalesced, and therefore this procedure is repeated
(stage 2.3). When no pairs that need to be coalesced are left, the remaining anonymous
nodes are assigned arbitrary names and the algorithm halts.
We first define NodeCoalesce(S, q, q?): this is a signature module S? that is obtained
from S by coalescing q with q?.
Definition 15
Let S = ??Q,T,,Ap?, Int, Imp,Exp? be a signature module and let q, q? ? Q. Define
NodeCoalesce(S, q, q?) = ??Q1,T1,1,Ap1?, Int1, Imp1,Exp1? where:
 Q1 = Q\{q}
 T1 = T |Q1
 1= {(q1, q2) | q1  q2 and q1, q2 = q} ? {(p, q?) | p  q} ? {(q?, p) | q  p}
 Ap1 = {(q1,F, q2) | (q1,F, q2) ? Ap and q1, q2 = q} ?
{(p,F, q?) | (p,F, q) ? Ap} ? {(q?,F, p) | (q,F, p) ? Ap}
 Int = Imp = Exp = ?
The input to the name resolution algorithm is a signature module and its output is a
signature module whose typing function, T, is total. Let S = ??Q,T,,Ap?, Int, Imp,Exp?
be a signature module, and let NAMES ? TYPE be an enumerable set of fresh types from
which arbitrary names can be taken to mark nodes in Q. The following algorithm marks
all the anonymous nodes in S:
Algorithm 2 (NameResolution (S = ??Q,T,,Ap?, Int, Imp,Exp?))
1. for all q ? Q such that T(q)?, compute Qq = {q? ? Q | T(q?)? and q? is
equivalent to q}.
2. let Q = {q ? Q | T(q)? and |Qq| = 1}. If Q = ? then:
2.1. for all q ? Q, S := NodeCoalesce(S, q, q?), where Qq = {q?}
2.2. S := compact(S)
2.3. go to (1)
3. Mark remaining anonymous nodes in Q with arbitrary unique types from
NAMES and halt.
For a given anonymous node, the calculation of its typed equivalent nodes is mostly
technical and is therefore suppressed.
55
Computational Linguistics Volume 37, Number 1
Figure 12
Name resolution result for S10.
Example 11
Consider the signature module S6 depicted in Figure 6. Executing the name resolution
algorithm on this module results in the signature module of Figure 12 (AGR-labels are
suppressed for readability.) The two anonymous nodes in S6 are coalesced with the
nodes marked nagr and vagr, as per their attributes. Compare to Figure 1, in particular
how two anonymous nodes in S1 are assigned types from S5 (Figure 6).
2.5.2 Appropriateness Consolidation. For each node q, the set of outgoing appropriateness
arcs with the same label F, {(q,F, q?)}, is replaced by the single arc (q,F, ql), where ql is
marked by the lub of the types of all q?. If no lub exists, a new node is added and is
marked by the lub. The result is an appropriateness relation which is a function, and in
which upward closure is preserved; feature introduction is dealt with separately.
The input to the following procedure is a signature module whose typing func-
tion, T, is total; its output is a signature module whose typing function is total and
whose appropriateness relation is a function that maintains upward closure. Let S =
??Q,T,,Ap?, Int, Imp,Exp? be a signature module. For each q ? Q and F ? FEAT, let
 target(q,F) = {q? | (q,F, q?) ? Ap}
 sup(q) = {q? ? Q | q?  q}
 sub(q) = {q? ? Q | q  q?}
Algorithm 3 (ApConsolidate (S = ??Q,T,,Ap?, Int, Imp, Exp?))
1. Set Int := Imp := Exp := ?
2. Find a node q and a feature F for which |target(q,F)| > 1 and for all
q? ? Q such that q? ? q, |target(q?,F)| ? 1 (i.e., q is a minimal node with
respect to a topological ordering of Q). If no such pair exists, halt
3. If target(q,F) has a lub, p, then:
(a) for all q? ? target(q,F), remove the arc (q,F, q?) from Ap
(b) add the arc (q,F, p) to Ap
(c) for all q? ? target(q,F) and for all q?? ? sub(q?), if p = q?? then add the
arc (p, q??) to 
56
Sygal and Wintner Modular Typed Unification Grammars
4. (a) Otherwise, If target(q,F) has no lub, add a new node, p, to Q with:
 sup(p) = target(q,F)
 sub(p) =
?
q??target(q,F) sub(q
?)
(b) Mark p with a fresh type from NAMES
(c) For all q? ? target(q,F), remove the arc (q,F, q?) from Ap
(d) Add (q,F, p) to Ap
5. S := ApCl(S)
6. S := compact(S)
7. go to (2)
The order in which nodes are selected in step 2 of the algorithm is from supertypes
to subtypes. This is done to preserve upward closure. When a set of outgoing appro-
priateness arcs with the same label F, {(q,F, q?)}, is replaced by a single arc (q,F, ql), all
the subtypes of all q? are added as subtypes of ql (stage 3c). This is done to maintain
the upwardly closed intention of appropriateness arcs (see Example 13). Additionally,
ql is added as an appropriate value for F and all the subtypes of q. This is achieved
by the Ap-Closure operation (stage 5). Again, this is done to preserve upward closure.
If a new node is added (stage 3), then its subtypes are inherited from its immediate
supertypes. Its appropriate features and values are also inherited from its immediate
supertypes through the Ap-Closure operation (stage 5). In both stages 3 and 4, a final
step is compaction of the signature module in order to remove redundant arcs.
Example 12
Consider the signature module depicted in Figure 12. Executing the appropriateness
consolidation algorithm on this module results in the module depicted in Figure 13.
Example 13
Consider the signature modules depicted in Figure 14. Executing the appropriateness
consolidation algorithm on S16, the two outgoing arcs from a labeled with F are first
replaced by a single outgoing arc to a newly added node, new1, which is the lub of
b and c. During this first iteration, new1 is also added as a supertype of e and f . The
result of these operations is S17. Notice that in S16, the arc (a,F, b) is interpreted as ?the
appropriate value of a and F is at least b.? In particular, this value may be e. S17 maintains
Figure 13
Appropriateness consolidation: result.
57
Computational Linguistics Volume 37, Number 1
Figure 14
Appropriateness consolidation.
this interpretation by means of the subtyping arc that is added from new1 to e. Then, the
two outgoing arcs from d labeled with F (to e and f ) are replaced by a single outgoing arc
to a newly added node, new2, which is the lub of e and f . The result of these operations
is S18, which is also the final result.
3. Grammar Modules
Before extending signature modules to grammar modules, we first recall basic notions
of typed unification grammars. For the following definitions we assume that a type
signature ?TYPE,, FEAT,Approp? has been specified.
Definition 16
A path is a finite sequence of features, and the set PATHS = FEAT? is the collection of
paths.  is the empty path.
Definition 17
A typed pre-feature structure (pre-TFS) is a triple ??,?, ? where:
 ? ? PATHS is a non-empty set of Paths
 ? : ? ? TYPE is a total function, assigning a type for all paths
 ? ? ? ? is a relation specifying reentrancy
A typed feature structure (TFS) is a pre-TFS A = ??,?, ? for which the following
requirements hold:
 ? is prefix-closed: if ?? ? ? then ? ? ? (where ?,? ? PATHS)
 A is fusion-closed: if ?? ? ? and ?  ?? then ??? ? ? and ??  ???
  is an equivalence relation with a finite index (with [] the set of its
equivalence classes) including at least the pair (,)
 ? respects the equivalence: if ?1  ?2 then ?(?1) = ?(?2)
58
Sygal and Wintner Modular Typed Unification Grammars
Definition 18
A TFS A = ??,?, ? is well-typed iff whenever ? ? ? and F ? FEAT are such that ?F ?
?, then Approp(?(?),F)?, and Approp(?(?),F)  ?(?F).
A grammar is defined over a concrete type signature and is a structure including a
set of rules (each constructed from a series of TFSs), a lexicon mapping words to sets of
TFSs and a start symbol which is a TFS.
We are now ready to define grammar modules and the way in which they interact.
A grammar module is a structure M = ?S,G?, where S is a signature module and G is a
grammar. The grammar is defined over the signature module analogously to the way
ordinary grammars are defined over type signatures, albeit with two differences:
1. TFSS are defined over type signatures, and therefore each path in the TFS
is associated with a type. When TFSS are defined over signature modules
this is not the case, because signature modules may include anonymous
nodes. Therefore, the standard definition of TFSS is modified such that
every path in a TFS is assigned a node in the signature module over which
it is defined, rather than a type.
2. Enforcing all TFSS in the grammar to be well-typed is problematic for
three reasons:
(a) Well-typedness requires that ?(?F) be an upper bound of all the
(target) nodes which are appropriate for ?(?) and F. However,
each module may specify only a subset of these nodes. The whole
set of target nodes is known only after all modules combine.
(b) A module may specify several appropriate values for ?(?) and F,
but it may not specify any upper bound for them.
(c) Well-typedness is not preserved under module combination. The
natural way to preserve well-typedness under module combination
requires addition of nodes and arcs, which would lead to a
non-associative combination.
To solve these problems, we enforce only a relaxed version of well
typedness. The relaxation is similar to the way upward closure is relaxed:
Whenever ?(?) = q, ?(?F) is required to be a subtype of one of the values
q? such that (q,F, q?) ? Ap. This relaxation supports the partiality and
associativity requirements of modular grammar development
(Section 1.1). After all modules are combined, the resulting grammar is
extended to maintain well-typedness.
The two combination operators, merge and attachment, are lifted from signature
modules to grammar modules. In both cases, the components of the grammars are
combined using simple set union. This reflects our initial observation (Section 1.1) that
most of the information in typed formalisms is encoded by the signature, and therefore
modularization is carried out mainly through the distribution of the signature between
the different modules; the lifting of the signature combination operation to operations
on full grammar modules is therefore natural and conservative.
59
Computational Linguistics Volume 37, Number 1
Figure 15
The main fragments of the signature.
Figure 16
A signature module, Sign.
Finally, grammar modules are extended to bona fide typed unification grammars
by extending the underlying signature module into an ordinary type signature and
adjusting the grammar accordingly.10
4. Modular Construction of the Basic HPSG Signature
To demonstrate the utility of signature modules for practical grammar engineering we
use signature modules and their combination operators in this section to work out a
modular design of the HPSG grammar of Pollard and Sag (1994). This is a grammar of
English whose signature, covering several aspects of syntax and semantics, is developed
throughout the book. The signature is given (Pollard and Sag 1994, Appendix A1) as one
unit, making it very hard to conceptualize and, therefore, to implement and maintain.
We reverse-engineered this signature, breaking it up into smaller-scale modules that
emphasize fragments of the theory that are more local, and the interactions among such
fragments through ?merge? and ?attachment?.11 Some of the fragments make use of the
signature module List of Figure 11.
10 In practice, an extra adjustment is required in order to restore well-typedness, but we suppress this
technicality.
11 Of course, other ways to break up the given signature to modules are conceivable. In particular, the
Synsem module of Figure 19 may better be broken into two modules.
60
Sygal and Wintner Modular Typed Unification Grammars
We begin with a module defining objects (Figure 15), where the type object is the
most general type. This module defines the main fragments of the signature.
Figure 16 defines the module Sign. It consists of the type sign, and its two subtypes
word and phrase. The latter is exported and will be used by other modules, as we
presently show. In addition, two of the appropriate features of sign are lists; note that
the values of PHON and RETRIEVED are imported.
Next, we consider constituent structure, and in particular headed structures, in
Figure 17. Note in particular that the feature COMP-DTRS, defined at head struc, takes
as values a list of phrases; this is an imported type, which is obtained as a result of
several attachment operations (Figure 11).
Figure 18 describes the fragment of the signature rooted by head. This is basically
a specification of the inventory of syntactic categories defined by the theory. Note how
simple it is to add, remove, or revise a category by accessing this fragment only.
Figure 19 provides straight-forward definitions of category and synsem, respectively.
As another example, Figure 20 depicts the type hierarchy of nominal objects, which
is completely local (in the sense that it does not interact with other modules, except
at the root). Finally, Figure 21 abstracts over the internal structure of Phonstring and
Figure 17
Phrase structure.
61
Computational Linguistics Volume 37, Number 1
Figure 18
A signature module, Head.
Figure 19
Signature modules.
Quantifier; these are only representatives of the actual signature modules which define
these fragments.
The full HPSG signature consists of several more fragments that we do not depict
here. With this in mind, the HPSG signature can now be constructed in a modular way
from the fragments defined earlier. The construction is given in Figure 22.
First, we produce two lists of phonestring and quantifier, which are merged into one
module through the operation
List(Phonestring) uniondbl List(Quantifier)
Then, this module instantiates the two imported nodes phonestring list and quantifier list
in the module Sign through the operation
Sign(List(Phonestring) uniondbl List(Quantifier))
Notice how the order of the parameters ensures the correct instantiation. Now, in the
second element, List(Sign) both creates a list of phrase (since phrase is an exported
62
Sygal and Wintner Modular Typed Unification Grammars
Figure 20
A classification of nominal objects.
Figure 21
Parametric signature modules.
Figure 22
HPSG signature construction.
node in the module Sign) and unifies the information in the two modules. Similarly,
ConStruc(List(Sign)) unifies the information in the three modules and instantiates the
node phrase list in the module ConStruc. In the same way, List(Synsem) both creates
a list of synsem (since synsem is an exported node in the module Synsem) and unifies
the information in the two modules. Then, Cat(List(Synsem)) unifies the information in
the three modules and instantiates the node synsem list in the module Cat. Finally, all the
information from the different modules is unified through the merge operation. Other
modules can be added, either by merge or by attachment. Additionally, the internal
structure of each module can be locally modified. Such changes become much easier
given the smaller size and theoretical focus of each of the modules.
This modular approach has significant advantages over the monolithic approach
of Pollard and Sag (1994): The signature of Pollard and Sag is hard to conceptualize
63
Computational Linguistics Volume 37, Number 1
because all the information is presented in a single hierarchy. In contrast, looking at
each small fragment (module) separately, it is easier to understand the information
encoded in the module. Contemporary type signatures are in fact much larger; working
with small fragments in such grammars is instrumental for avoiding or tracking errors.
Moreover, grammar maintenance is significantly simplified, because changes can be
done locally, at the level of specific modules. Of course, when a new grammar is devel-
oped from scratch, modularization can be utilized in such a way as to reflect indepen-
dent fragments of the linguistic theory in separate modules.
While the grammar of Pollard and Sag (1994) is not really large-scale, it is large
enough to reflect the kind of knowledge organization exhibited by linguistically moti-
vated grammars, but is at the same time modest enough so that its redesign in a modular
way can be easily comprehended. It is therefore useful as a practical example of how
type signatures can be constructed from smaller, simpler signature modules. Real-world
grammars are not only much larger, they also tend to be more complex, and in particular
express interactions in domains other than the type signature (specifically, as type
constraints and as phrase-structure rules). Extending our solution to such interactions
is feasible, but is beyond the scope of this preliminary work.
5. MODALE: A Platform for Modular Development of Type Signatures
Two leading implementation platforms are available for the development of typed
unification grammars: The Linguistic Knowledge Building system (LKB) (Copestake
2002) and TRALE (Meurers, Penn, and Richter 2002), an extension of the Attribute
Logic Engine (ALE) (Carpenter and Penn 2001). MODALE (MODular ALE) is a system
that supports modular development of type signatures in both ALE and TRALE. The
main features of the system are:
 The system provides a description language with which signature
modules can be specified. The description language is intuitive and is built
upon the description language of ALE. For example, the description of S1,
the signature module of Figure 2, is shown in Figure 23.
 Signature modules may be combined using either one of the two
combination operators, merge and attachment, or by a complex
combination involving several operators.
 Signature modules can be resolved to yield bona fide type signatures.
 The system compiles resolved modules into output files using either ALE
or TRALE syntax; these files can be directly manipulated by one of the two
systems.
 Signature modules can be printed using the syntax of the description
language. This feature allows inspection of a signature module that was
created as a result of several combination operators.
ALE and TRALE share the same underlying core, and are based on data structures
and algorithms that take advantage of type signature properties such as bounded
completeness, upward closure, and feature introduction, none of which can be assumed
when working with a signature module. As a result, our implementation is not a direct
adaption of the existing ALE/TRALE code, but a new system that was developed from
64
Sygal and Wintner Modular Typed Unification Grammars
Figure 23
MODALE description of S1.
scratch. Extending the algorithms of Penn (2000) from type signatures into signature
modules is left as a direction for future research.
The MODALE system provided us with an opportunity to experimentally evaluate
the time efficiency of module combination. Indeed, the combination and resolution
algorithms are computationally inefficient as they require repeated calculations of graph
isomorphism, a problem which is neither known to be solvable in polynomial time
nor NP-complete.12 However, in the signatures we have experimented with so far, we
encountered no time issues. Furthermore, it is important to note that these calculations
are executed only once, in compile time, and have no impact on the run time of
ALE/TRALE, which is the crucial stage in which efficiency is concerned.
6. Discussion and Conclusions
We presented a complete definition of typed unification grammar modules and their
interaction. Unlike existing approaches, our solution is formally defined, mathemati-
cally proven, can be easily and efficiently implemented, and conforms to each of the
desiderata listed in Section 1.1, as we now show.
Signature focus: Our solution focuses on the modularization of the signature (Sec-
tion 2), and the extension to grammar modules (Section 3) is natural and conser-
vative. We do restrict ourselves in this work to standard type signatures without
type constraints. We defer the extension of type signatures to include also type
constraints to future work.
Partiality: Our solution provides the grammar developer with means to specify any
piece of information about the signature. A signature module may specify only
partial information about the subtyping and appropriateness relations. Further-
more, the appropriateness relation is not a function as in ordinary signatures, and
12 Garey and Johnson (1979) provide a list of 12 major problems whose complexity status was open at
the time of writing. Recognition of graph isomorphism is one of those, and one of the only two whose
complexity remains unresolved today.
65
Computational Linguistics Volume 37, Number 1
the developer may specify several appropriate nodes for the values of a feature
F at a node q. The anonymity of nodes and relaxed upward closure also provide
means for partiality. Another relaxation that supports partiality is not enforcing
feature introduction and the BCPO conditions. Finally, the possibility of distribut-
ing the grammar between several modules and the relaxation of well-typedness
also support this desideratum.
Extensibility: In Section 2.5 we show how a signature module can be deterministically
extended into a bona fide signature.
Consistency: When modules are combined, either by merge or by attachment, the
signature modules are required to be mergeable or attachable, respectively. In
this way, contradicting information in different modules is detected prior to the
combination. Notice that two signature modules can be combined only if the
resulting subtyping relation is indeed a partial order.
Flexibility: The only restrictions we impose on modules are meant to prevent subtyp-
ing cycles.
(Remote) Reference: This requirement is achieved by the parametric view of nodes.
Anonymity of nodes also supports this desideratum.
Parsimony: When two modules are combined, they are first unioned; thus the resulting
module includes all the information encoded in each of the modules. Additional
information is added in a conservative way by compaction and Ap-closure in
order to guarantee that the resulting module is indeed well-defined.
Associativity: We provide two combination operations, merge and attachment. The at-
tachment operation is an asymmetric operation, such as the function application,
and therefore associativity is not germane. The merge operation, which is sym-
metric, is both commutative and associative and therefore conforms with this
desideratum.
Privacy: Privacy is achieved through internal nodes which encode information that
other modules cannot view or refer to.
Modular construction of grammars, and of type signatures in particular, is an
essential requirement for the maintainability and sustainability of large-scale grammars.
We believe that our definition of signature modules, along with the operations of
merge and attachment, provide grammar developers with powerful and flexible tools for
collaborative development of natural language grammars, as demonstrated in Section 4.
Modules provide abstraction; for example, the module List of Figure 11 defines the
structure of a list, abstracting over the type of its elements. In a real-life setting, the
grammar designer must determine how to abstract away certain aspects of the devel-
oped theory, thereby identifying the interaction points between the defined module and
the rest of the grammar. A first step in this direction was done by Bender and Flickinger
(2005); we believe that we provide a more general, flexible, and powerful framework to
achieve the full goal of grammar modularization.
This work can be extended in various ways. First, this work focuses on the modular-
ity of the signature. This is not accidental, and reflects the centrality of the type signature
in typed unification grammars. An extension of signature modules to include also type
constraints is called for and will provide a better, fuller solution to the problem of
grammar modularization. In a different track, we also believe that extra modularization
capabilities can still be provided by means of the grammar itself. This direction is left
for future research.
Although the present work is mainly theoretical, it has important practical implica-
tions. An environment that supports modular construction of large-scale grammars will
66
Sygal and Wintner Modular Typed Unification Grammars
greatly contribute to grammar development and will have a significant impact on prac-
tical implementations of grammatical formalisms. The theoretical basis we presented
in this work was implemented as a system, MODALE, that supports modular develop-
ment of type signatures (Section 5). Once the theoretical basis is extended to include
also type constraints, and they, as well as grammar modules, are fully integrated in a
grammar development system, immediate applications of modularity are conceivable
(see Section 1.1). Furthermore, although there is no general agreement among linguists
on the exact form of modularity in grammar, a good modular interface will provide the
necessary infrastructure for the implementation of different linguistic theories and will
support their comparison in a common platform.
Finally, our proposed mechanisms clearly only fill very few lacunae of existing
grammar development environments, and various other provisions will be needed in
order for grammar engineering to be as well-understood a task as software engineering
now is. We believe that we make a significant step in this crucial journey.
Appendix: Compactness
We provide a formal definition of the compactness algorithm in this section. For an
example of the following two definitions see Example 3.
Definition 19
Let S = ??Q,T,,Ap?, Int, Imp,Exp? be a pre-signature module. (q1, q2) ? is a redun-
dant subtyping arc if there exist p1, . . . , pn ? Q, n ? 1, such that q1  p1  p2  . . . 
pn  q2.
Definition 20
Let P = ??Q,T,,Ap?, Int, Imp,Exp? be a pre-signature module. (q1,F, q2) ? Ap is a re-
dundant appropriateness arc if there exists q?2 ? Q such that q2 
? q?2, q2 = q?2 and
(q1,F, q?2) ? Ap.
The following definitions set the basis for determining whether two nodes are indis-
tinguishable or not. Because signature modules are just a special case of directed, labeled
graphs, we can adapt the well-defined notion of graph isomorphism to pre-signature
modules. Informally, two pre-signature modules are isomorphic when their underlying
PSSs have the same structure; the identities of their nodes may differ without affecting
the structure. In our case, we require also that an anonymous node be mapped only to
an anonymous node and that two typed nodes, mapped to each other, be marked by the
same type. However, the classification of nodes as internal, imported, and/or exported
has no effect on the isomorphism since it is not part of the core of the information
encoded by the signature module.
Definition 21
Two pre-signature modules S1 = ??Q1,T1,1,Ap1?, Int1, Imp1,Exp1?, S2 = ??Q2,T2,2,
Ap2?, Int2, Imp2,Exp2? are isomorphic, denoted S1?S2, if there exists a total, one-to-one
and onto function i (isomorphism) mapping the nodes of S1 to the nodes of S2, such
that all the following hold:
1. for all q ? Q1, T1(q) = T2(i(q))
2. for all q, q? ? Q1, q 1 q? iff i(q) 2 i(q?)
3. for all q, q? ? Q1 and F ? FEAT, (q,F, q?) ? Ap1 iff (i(q),F, i(q?)) ? Ap2
67
Computational Linguistics Volume 37, Number 1
The environment of a node q is the set of nodes accessible from q via any sequence
of arcs (subtyping or appropriateness, in any direction), up to and including the first
typed node. The environment of a typed node includes itself only.
Definition 22
Let S = ??Q,T,,Ap?, Int, Imp,Exp? be a pre-signature module. For all q ? Q let the
environment of q, denoted env(q), be the smallest set such that:
 q ? env(q)
 If q?? ? env(q) and T(q??)? and for some q? ? Q and F ? FEAT, either q?  q??
or q??  q? or (q?,F, q??) ? Ap or (q??,F, q?) ? Ap, then q? ? env(q)
Definition 23
Let S = ??Q,T,,Ap?, Int, Imp,Exp? be a pre-signature module and let Q? ? Q. The
strict restriction of S to Q?, denoted S|strictQ? , is ??Q?,T2,2,Ap2?, Int2, Imp2,Exp2?, where:
 T2 = T|Q?
 q1 2 q2 iff q1  q2, q1, q2 ? Q? and either T(q1)? or T(q2)? (or both)
 (q1,F, q2) ? Ap2 iff (q1,F, q2) ? Ap, q1, q2 ? Q? and either T(q1)? or T(q2)?
(or both)
 Int2 = Int|Q?
 Imp2 = Imp|Q?
 Exp2 = Exp|Q?
The strict restriction of a pre-signature module, S, to a set of nodes Q?, is the
subgraph induced by the nodes of Q? without any labeled or unlabeled arcs connecting
two typed nodes in Q?.
Definition 24
Let S = ??Q,T,,Ap?, Int, Imp,Exp? be a pre-signature module. Two nodes q1, q2 ? Q
are indistinguishable, denoted q1 ? q2, if S |strictenv(q1 )? S |
strict
env(q2 )
via an isomorphism i such
that i(q1) = q2.
Example 14
Let S1 be the signature module of Figure A1
env(q4) = env(q7) = {q1, q4, q7}, env(q2) = env(q6) = {q1, q2, q6},
env(q5) = {q1, q5, q8} and env(q1) = {q1}
The strict restrictions of S1 to these environments are depicted in Figure A2. q2 ? q4 and
q6 ? q7, where in both cases the isomorphism is
i = {q1 ? q1, q2 ? q4, q6 ? q7}
68
Sygal and Wintner Modular Typed Unification Grammars
Figure A1
A signature module with indistinguishable nodes, S1.
Figure A2
Strict restriction subgraphs.
However, q5 is distinguishable from q2 and q4 because T(q8) = T(q6) and T(q8) = T(q7).
Notice also that q3 is distinguishable from q2, q4 and q5 because it has no outgoing
appropriateness arcs.
Proposition 3
Let S = ??Q,T,,Ap?, Int, Imp,Exp? be a pre-signature module. Then ??? is an equiva-
lence relation over Q.
Definition 25
A pre-signature module S = ??Q,T,,Ap?, Int, Imp,Exp? is non-redundant if it in-
cludes no redundant subtyping and appropriateness arcs and for all q1, q2 ? Q, q1 ? q2
implies q1 = q2.
Definition 26
Let S = ??Q,T,,Ap?, Int, Imp,Exp? be a pre-signature module. The coalesced pre-
signature module, denoted coalesce(S), is ??Q1,T1,1,Ap1?, Int1, Imp1,Exp1? where:
 Q1 = {[q]? | q ? Q} (Q1 is the set of equivalence classes with respect to ?)
 T1([q]?) = T(q?) for some q? ? [q]?
 1= {([q1]?, [q2]?) | (q1, q2) ?}
69
Computational Linguistics Volume 37, Number 1
 Ap1 = {([q1]?,F, [q2]?) | (q1,F, q2) ? Ap}
 Int1 = {[q]? | q ? Int}
 Imp1 = {[q]? | q ? Imp and [q]? /? Int}
 Exp1 = {[q]? | q ? Exp and [q]? /? Int}
 the order of Imp1 and Exp1 is induced by the order of Imp and Exp,
respectively, with recurring elements removed
When a pre-signature module is coalesced, indistinguishable nodes are identified.
Additionally, the parameters and arities are induced from those of the input pre-
signature module. All parameters may be coalesced with each other, as long as they
are otherwise indistinguishable. If (at least) one of the coalesced nodes is an internal
node, then the result is an internal node. Otherwise, if one of the nodes is imported then
the resulting parameter is imported as well. Similarly, if one of the nodes is exported
then the resulting parameter is exported.
The input to the compactness algorithm is a pre-signature module and its output is
a non-redundant signature module which encodes the same information.
Algorithm 4 (compact (S = ??Q,T,,Ap?, Int, Imp, Exp?))
1. Let S1 = ??Q1,T1,1,Ap1?, Int1, Imp1,Exp1? be such that:
 Q1 = Q
 T1 = T
 1= {(q1, q2) ?| (q1, q2) is a non-redundant subtyping arc in S}
 Ap1 = {(q1,F, q2) ? Ap | (q1,F, q2) is a non-redundant
appropriateness arc in S}
 Int1 = Int
 Imp1 = Imp
 Exp1 = Exp
2. S? = coalesce(S1)
3. If S? is non-redundant, return S?, otherwise return compact(S?)
The compactness algorithm iterates as long as the resulting pre-signature module
includes redundant arcs or nodes. In each iteration, all the redundant arcs are first
removed and then all indistinguishable nodes are coalesced. However, the identifica-
tion of nodes can result in redundant arcs or can trigger more nodes to be coalesced.
Therefore, the process is repeated until a non-redundant signature module is obtained.
Notice that the compactness algorithm coalesces pairs of nodes marked by the same
type regardless of their incoming and outgoing arcs. Such pairs of nodes may exist in a
pre-signature module (but not in a signature module).
Example 15
Consider again S1, the signature module of Figure A1. The compacted signature module
of S1 is depicted in Figure A3. Notice that S1 has no redundant arcs to be removed and
70
Sygal and Wintner Modular Typed Unification Grammars
Figure A3
The compacted signature module of S1.
Figure A4
A compactness example.
that q2 and q6 were coalesced with q4 and q7, respectively. All nodes in compact(S1) are
pairwise distinguishable and no arc is redundant.
Example 16
Consider S2,S3,S4,S5, the signature modules depicted in Figure A4. Executing the
compactness algorithm on S2, first the redundant subtyping arc from q1 to q6 is removed,
resulting in S3 which has no redundant arcs. Then, q2 and q3 are coalesced, resulting in
S4. In S4, {q2, q3} ? {q4} and {q5} ? {q6}, and after coalescing these two pairs, the result
is S5 which is non-redundant.
Proposition 4
The compactness algorithm terminates.
Proposition 5
The compactness algorithm is deterministic: it always produces the same result.
Proposition 6
If S is a signature module then compact(S) is a non-redundant signature module.
Proposition 7
If S is a non-redundant signature module then compact(S) ? S.
71
Computational Linguistics Volume 37, Number 1
Acknowledgments
This research was supported by the
Israel Science Foundation (grants 136/01,
137/06). We are grateful to Nurit Melnik
and Gerald Penn for extensive discussions
and constructive feedback, and to the
CL reviewers for detailed, useful
comments. All remaining errors are, of
course, our own.
References
Abeille?, Anne, Marie-He?le`ne Candito,
and Alexandra Kinyon. 2000. FTAG:
developing and maintaining a
wide-coverage grammar for French. In
Erhard Hinrichs, Detmar Meurers, and
Shuly Wintner, editors, Proceedings of the
ESSLLI-2000 Workshop on Linguistic Theory
and Grammar Implementation, pages 21?32.
Basili, R., M. T. Pazienza, and F. M. Zanzotto.
2000. Customizable modular lexicalized
parsing. In Proceedings of the Sixth
International Workshop on Parsing
Technologies (IWPT 2000), pages 41?52,
Trento.
Bender, Emily M., Dan Flickinger, and
Stephan Oepen. 2002. The grammar
matrix: An open-source starter-kit for the
rapid development of cross-linguistically
consistent broad-coverage precision
grammars. In Proceedings of the Workshop
on Grammar Engineering and Evaluation
at the 19th International Conference on
Computational Linguistics, pages 8?14,
Taipei.
Bender, Emily M. and Dan Flickinger. 2005.
Rapid prototyping of scalable grammars:
Towards modularity in extensions to a
language-independent core. In Proceedings
of IJCNLP-05, pages 203?208, Jeju Island.
Bender, Emily M., Dan Flickinger, Fredrik
Fouvry, and Melanie Siegel. 2005. Shared
representation in multilingual grammar
engineering. Research on Language and
Computation, 3:131?138.
Brogi, Antonio, Evelina Lamma, and
Paola Mello. 1993. Composing open logic
programs. Journal of Logic and Computation,
3(4):417?439.
Brogi, Antonio, Paolo Mancarella, Dino
Pedreschi, and Franco Turini. 1990.
Composition operators for logic theories.
In J. W. Lloyd, editor, Computational
Logic ? Symposium Proceedings,
pages 117?134.
Brogi, Antonio, Paolo Mancarella, Dino
Pedreschi, and Franco Turini. 1994.
Modular logic programming. ACM
Transactions on Programming Languages
and Systems, 16(4):1361?1398.
Brogi, Antonio and Franco Turini. 1995. Fully
abstract compositional semantics for an
algebra of logic programs. Theoretical
Computer Science, 149:201?229.
Bugliesi, Michele, Evelina Lamma, and
Paola Mello. 1994. Modularity in logic
programming. Journal of Logic
Programming, 19?20:443?502.
Candito, Marie-He?le`ne. 1996. A
principle-based hierarchical representation
of LTAGs. In COLING-96, pages 194?199,
Copenhagen.
Carpenter, Bob. 1992. The Logic of Typed
Feature Structures. Cambridge Tracts in
Theoretical Computer Science. Cambridge
University Press, Cambridge.
Carpenter, Bob and Gerald Penn. 2001.
ALE?the attribute logic engine: User?s
guide. Technical report, Department of
computer science, University of Toronto
and SpeechWorks Research.
Cohen-Sygal, Yael and Shuly Wintner. 2006.
Partially specified signatures: A vehicle
for grammar modularity. In Proceedings
of the 21st International Conference on
Computational Linguistics and 44th Annual
Meeting of the Association for Computational
Linguistics, pages 145?152, Sydney.
Copestake, Ann. 2002. Implementing typed
feature structures grammars. CSLI
Publications, Stanford, CA.
Copestake, Ann and Dan Flickinger. 2000.
An open-source grammar development
environment and broad-coverage English
grammar using HPSG. In Proceedings
of the Second Conference on Language
Resources and Evaluation (LREC-2000),
pages 591?600.
Crabbe?, Benoit and Denys Duchier. 2004.
Metagrammar redux. In Proceedings of
The International Workshop on Constraint
Solving and Language Processing (CSLP),
pages 32?47.
Dalrymple, Mary. 2001. Lexical Functional
Grammar, volume 34 of Syntax and
Semantics. Academic Press, Oxford.
Debusmann, Ralph. 2006. Extensible
Dependency Grammar: A Modular Grammar
Formalism Based On Multigraph Description.
Ph.D. thesis, University of Saarlandes.
Debusmann, Ralph, Denys Duchier, and
Andreas Rossberg. 2005. Modular
grammar design with typed parametric
principles. In Proceedings of FG-MOL 2005:
The 10th Conference on Formal Grammar and
The 9th Meeting on Mathematics of Language,
pages 113?122.
72
Sygal and Wintner Modular Typed Unification Grammars
Duchier, Denys and Claire Gardent. 1999. A
constraint-based treatment of descriptions.
In Third International Workshop on
Computational Semantics (IWCS-3),
pages 71?85.
Erbach, Gregor and Hans Uszkoreit. 1990.
Grammar engineering: Problems and
prospects. CLAUS report 1, University
of the Saarland and German Research
Center for Artificial Intelligence.
Fodor, Jerry. 1983. The Modularity of Mind.
MIT Press, Cambridge, MA.
Gaifman, Haim and Ehud Shapiro. 1989.
Fully abstract compositional semantics
for logic programming. In 16th Annual
ACM Symposium on Principles of Logic
Programming, pages 134?142, Austin, TX.
Garey, Michael R. and David S. Johnson.
1979. Computers and Intractability: A Guide
to the Theory of NP-Completeness. W. H.
Freeman, New York.
Hinrichs, Erhard W., W. Detmar Meurers,
and Shuly Wintner. 2004. Linguistic theory
and grammar implementation. Research on
Language and Computation, 2:155?163.
Jackendoff, Ray. 2002. Foundations of
Language. Oxford University Press, Oxford.
Joshi, Aravind K., Leon S. Levy, and Masako
Takahashi. 1975. Tree adjunct grammars.
Journal of Computer and System Sciences,
10(1):136?163.
Kahane, Sylvain. 2006. Polarized unification
grammars. In Proceedings of the 21st
International Conference on Computational
Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics
(COLING-ACL 2006), pages 137?144,
Sydney.
Kallmeyer, Laura. 2001. Local tree
description grammars. Grammars,
4(2):85?137.
Kaplan, Ronald M., Tracy Holloway King,
and John T. Maxwell. 2002. Adapting
existing grammars: The XLE experience.
In COLING-02 Workshop on Grammar
Engineering and Evaluation, pages 1?7,
Morristown, NJ.
Kasper, Walter and Hans-Ulrich Krieger.
1996. Modularizing codescriptive
grammars for efficient parsing. In
Proceedings of the 16th Conference on
Computational Linguistics, pages 628?633,
Copenhagen.
Keselj, Vlado. 2001. Modular HPSG. In
Proceedings of the 2001 IEEE Systems,
Man, and Cybernetics Conference,
pages 2867?2872, Tucson, AZ.
King, Tracy Holloway, Martin Forst, Jonas
Kuhn, and Miriam Butt. 2005. The feature
space in parallel grammar writing.
Research on Language and Computation,
3:139?163.
Mancarella, Paolo and Dino Pedreschi.
1988. An algebra of logic programs. In
Robert A. Kowalski and Kenneth A.
Bowen, editors, Logic Programming:
Proceedings of the Fifth International
Conference and Symposium,
pages 1006?1023, Cambridge, MA.
Melnik, Nurit. 2006. A constructional
approach to verb-initial constructions in
modern Hebrew. Cognitive Linguistics,
17(2):153?198.
Meurers, W. Detmar, Gerald Penn, and Frank
Richter. 2002. A Web-based instructional
platform for constraint-based grammar
formalisms and parsing. In Proceedings of
the ACL Workshop on Effective Tools and
Methodologies for Teaching NLP and CL,
pages 18?25.
Mitchell, John C. 2003. Concepts in
Programming Languages. Cambridge
University Press, Cambridge.
Mu?ller, Stefan. 2007. The Grammix CD
ROM. A software collection for developing
typed feature structure grammars. In
Tracy Holloway King and Emily M.
Bender, editors, Grammar Engineering
across Frameworks 2007, Studies in
Computational Linguistics ONLINE.
CSLI Publications, Stanford, CA,
pages 259?266.
Oepen, Stephan, Dan Flickinger, Hans
Uszkoreit, and Jun-Ichi Tsujii. 2000.
Introduction to this special issue. Natural
Language Engineering, 6(1):1?14.
Oepen, Stephan, Daniel Flickinger, J. Tsujii,
and Hans Uszkoreit, editors. 2002.
Collaborative Language Engineering: A Case
Study in Efficient Grammar-Based Processing.
CSLI Publications, Stanford, CA.
O?Keefe, R. 1985. Towards an algebra for
constructing logic programs. In J. Cohen
and J. Conery, editors, Proceedings of
IEEE Symposium on Logic Programming,
pages 152?160, New York.
Penn, Gerald B. 2000. The Algebraic Structure
of Attributed Type Signatures. Ph.D. thesis,
School of Computer Science, Carnegie
Mellon University, Pittsburgh, PA.
Perrier, Guy. 2000. Interaction grammars.
In Proceedings of the 18th Conference on
Computational Linguistics (COLING 2000),
pages 600?606.
Pollard, Carl and Ivan A. Sag. 1994.
Head-Driven Phrase Structure Grammar.
University of Chicago Press and CSLI
Publications, Stanford, CA.
73
Computational Linguistics Volume 37, Number 1
Ranta, Aarne. 2007. Modular grammar
engineering in GF. Research on Language
and Computation, 5(2):133?158.
Sygal, Yael and Shuly Wintner. 2008. Type
signature modules. In Philippe de Groote,
editor, Proceedings of FG 2008: The
13th Conference on Formal Grammar,
pages 113?128.
Sygal, Yael and Shuly Wintner. 2009.
Associative grammar combination
operators for tree-based grammars.
Journal of Logic, Language and Information,
18(3):293?316.
Vijay-Shanker, K. 1992. Using descriptions
of trees in a tree adjoining grammar.
Computational Linguistics, 18(4):481?517.
Wintner, Shuly. 2002. Modular context-free
grammars. Grammars, 5(1):41?63.
Wintner, Shuly, Alon Lavie, and Brian
MacWhinney. 2009. Formal grammars
of early language. In Orna Grumberg,
Michael Kaminski, Shmuel Katz, and
Shuly Wintner, editors, Languages:
From Formal to Natural, volume 5533
of Lecture Notes in Computer Science.
Springer Verlag, Berlin and Heidelberg,
pages 204?227.
XTAG Research Group. 2001. A lexicalized
tree adjoining grammar for English.
Technical Report IRCS-01-03, IRCS,
University of Pennsylvania.
Zajac, Re?mi and Jan W. Amtrup. 2000.
Modular unification-based parsers.
In Proceedings of the Sixth International
Workshop on Parsing Technologies
(IWPT 2000), pages 278?288, Trento.
74
