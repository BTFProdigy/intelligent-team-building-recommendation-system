Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 110?118,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Context-Dependent Alignment Models for Statistical Machine Translation
Jamie Brunning, Adria` de Gispert and William Byrne
Machine Intelligence Laboratory
Department of Engineering, Cambridge University
Trumpington Street, Cambridge, CB2 1PZ, U.K.
{jjjb2,ad465,wjb31}@eng.cam.ac.uk
Abstract
We introduce alignment models for Machine Trans-
lation that take into account the context of a source
word when determining its translation. Since the use
of these contexts alone causes data sparsity prob-
lems, we develop a decision tree algorithm for clus-
tering the contexts based on optimisation of the
EM auxiliary function. We show that our context-
dependent models lead to an improvement in align-
ment quality, and an increase in translation quality
when the alignments are used in Arabic-English and
Chinese-English translation.
1 Introduction
Alignment modelling for Statistical Machine Translation
(SMT) is the task of determining translational correspon-
dences between the words in pairs of sentences in parallel
text. Given a source language word sequence f J1 and a
target language word sequence eI1, we model the transla-
tion probability as P(eI1|fJ1 ) and introduce a hidden vari-
able aI1 representing a mapping from the target word po-
sitions to source word positions such that ei is aligned to
fai . Then P(eI1|f j1 ) =
?
aI1 P(eI1, aI1|f
j
1 ) (Brown et al,
1993).
Previous work on statistical alignment modelling has
not taken into account the source word context when de-
termining translations of that word. It is intuitive that a
word in one context, with a particular part-of-speech and
particular words surrounding it, may translate differently
when in a different context. We aim to take advantage
of this information to provide a better estimate of the
word?s translation. The challenge of incorporating con-
text information is maintaining computational tractability
of estimation and alignment, and we develop algorithms
to overcome this.
The development of efficient estimation procedures
for context-dependent acoustic models revolutionised the
field of Automatic Speech Recognition (ASR) (Young et
al., 1994). Clustering is used extensively for improv-
ing parameter estimation of triphone (and higher order)
acoustic models, enabling robust estimation of param-
eters and reducing the computation required for recog-
nition. Kannan et al (1994) introduce a binary tree-
growing procedure for clustering Gaussian models for
triphone contexts based on the value of a likelihood ra-
tio. We adopt a similar approach to estimate context-
dependent translation probabilities.
We focus on alignment with IBM Model 1 and HMMs.
HMMs are commonly used to generate alignments from
which state of the art SMT systems are built. Model 1 is
used as an intermediate step in the creation of more pow-
erful alignment models, such as HMMs and further IBM
models. In addition, it is used in SMT as a feature in Min-
imum Error Training (Och et al, 2004) and for rescor-
ing lattices of translation hypotheses (Blackwood et al,
2008). It is also used for lexically-weighted phrase ex-
traction (Costa-jussa` and Fonollosa, 2005) and sentence
segmentation of parallel text (Deng et al, 2007) prior to
machine translation.
1.1 Overview
We first develop an extension to Model 1 that allows the
use of arbitrary context information about a source word
to estimate context-dependent word-to-word translation
probabilities. Since there is insufficient training data to
accurately estimate translation probabilities for less fre-
quently occurring contexts, we develop a decision tree
clustering algorithm to form context classes. We go on to
develop a context-dependent HMM model for alignment.
In Section 3, we evaluate our context-dependent mod-
els on Arabic-English parallel text, comparing them to
our baseline context-independent models. We perform
morphological decomposition of the Arabic text using
MADA, and use part-of-speech taggers on both lan-
guages. Alignment quality is measured using Alignment
Error Rate (AER) measured against a manually-aligned
parallel text. Section 4 uses alignments produced by
110
our improved alignment models to initialise a statistical
machine translation system and evaluate the quality of
translation on several data sets. We also apply part-of-
speech tagging and decision tree clustering of contexts to
Chinese-English parallel text; translation results for these
languages are presented in Section 4.2.
1.2 Previous and related work
Brown et al (1993) introduce IBM Models 1-5 for align-
ment modelling; Vogel et al (1996) propose a Hidden
Markov Model (HMM) model for word-to-word align-
ment, where the words of the source sentence are viewed
as states of an HMM and emit target sentence words;
Deng and Byrne (2005a) extend this to an HMM word-to-
phrase model which allows many-to-one alignments and
can capture dependencies within target phrases.
Habash and Sadat (2006) perform morphological de-
composition of Arabic words, such as splitting of pre-
fixes and suffixes. This leads to gains in machine trans-
lation quality when systems are trained on parallel text
containing the modified Arabic and processing of Arabic
text is carried out prior to translation. Nie?en and Ney
(2001a) perform pre-processing of German and English
text before translation; Nie?en and Ney (2001b) use mor-
phological information of the current word to estimate
hierarchical translation probabilities.
Berger et al (1996) introduce maximum entropy mod-
els for machine translation, and use a window either side
of the target word as context information. Varea et al
(2002) test for the presence of specific words within a
window of the current source word to form features for
use inside a maximum entropy model of alignment.
Toutanova et al (2002) use part-of-speech informa-
tion in both the source and target languages to estimate
alignment probabilities, but this information is not in-
corporated into translation probabilities. Popovic? and
Ney (2004) use the base form of a word and its part-of-
speech tag during the estimation of word-to-word transla-
tion probabilities for IBM models and HMMs, but do not
defined context-dependent estimates of translation prob-
abilities.
Stroppa et al (2007) consider context-informed fea-
tures of phrases as components of the log-linear model
during phrase-based translation, but do not address align-
ment.
2 Use of source language context in
alignment modelling
Consider the alignment of the target sentence e = eI1 with
the source sentence f = fJ1 . Let a = aI1 be the align-
ments of the target words to the source words. Let cj be
the context information of fj for j = 1, . . . , J . This con-
text information can be any information about the word,
e.g. part-of-speech, previous and next words, part-of-
speech of previous and next words, or longer range con-
text information.
We follow Brown et al (1993), but extend their mod-
elling framework to include information about the source
word from which a target word is emitted. We model the
alignment process as:
P(eI1, aI1, I |fJ1 , cJ1 ) =
P(I |fJ1 , cJ1 )
I?
i=1
[P(ei|ai1, ei?11 , fJ1 , cJ1 , I)
? P(ai|ei?11 , ai?11 , fJ1 , cJ1 , I)
] (1)
We introduce word-to-word translation tables that depend
on the source language context for each word, i.e. the
probability that f translates to e given f has context c is
t(e|f, c). We assume that the context sequence is given
for a source word sequence. This assumption can be
relaxed to allow for multiple tag sequences as hidden
processes, but we assume here that a tagger generates
a single context sequence cJ1 for a word sequence fJ1 .
This corresponds to the assumption that, for a context se-
quence c?J1 , P(c?J1 |fJ1 ) = ?cJ1 (c?J1 ); hence
P(eI1, aI1|fJ1 ) =
?
c?J1
P(eI1, aI1, c?J1 |fJ1 ) = P(eI1, aI1|cJ1 , fJ1 )
For Model 1, ignoring the sentence length distribution,
PM1(eI1, aI1|fJ1 , cJ1 ) = 1(J + 1)I
I?
i=1
t(ei|fai , cai). (2)
Estimating translation probabilities separately for ev-
ery possible context of a source word individually leads
to problems with data sparsity and rapid growth of the
translation table. We therefore wish to cluster source con-
texts which lead to similar probability distributions. Let
Cf denote the set of all observed contexts of source word
f . A particular clustering is denoted
Kf = {Kf,1, . . . ,Kf,Nf},
where Kf is a partition of Cf . We define a class mem-
bership function ?f such that for any context c, ?f (c)
is the cluster containing c. We assume that all contexts
in a cluster give rise to the same translation probability
distribution for that source word, i.e. for a cluster K,
t(e|f, c) = t(e|f, c?) for all contexts c, c? ? K and all
target words e; we write this shared translation probabil-
ity as t(e|f,K).
The Model 1 sentence translation probability for a
given alignment (Equation 2) becomes
PM1(eI1, aI1|fJ1 , cJ1 ) = 1(J + 1)I
I?
i=1
t(ei|fai , ?f (cai)).
(3)
111
For HMM alignment, we assume that the transition prob-
abilities a(ai|ai?1) are independent of the word contexts
and the sentence translation probability is
PH(eI1, aI1|fJ1 , cJ1 ) =
I?
i=1
a(ai|ai?1, J)t(ei|fai , ?f (cai)).
(4)
Section 2.1.1 describes how the context classes are deter-
mined by optimisation of the EM auxiliary function. Al-
though the translation model is significantly more com-
plex than that of context-independent models, once class
membership is fixed, alignment and parameter estimation
use the standard algorithms.
2.1 EM parameter estimation
We train using Expectation Maximisation (EM), optimis-
ing the log probability of the training set {e(s), f (s)}Ss=1
(Brown et al, 1993). Given model parameters ??, we es-
timate new parameters ? by maximisation of the EM aux-
iliary function
?
s,a
P??(a|f (s), c(s), e(s)) log P?(e(s), a, I(s)|f (s), c(s)).
We assume the sentence length distribution and align-
ment probabilities do not depend on the contexts of the
source words; hence the relevant part of the auxiliary
function is
?
e
?
f
?
c?Cf
??(e|f, c) log t(e|f, c), (5)
where
??(e|f, c) = ?
s
I(s)?
i=1
J(s)?
j=1
[
?c(c(s)j )?e(e(s)i )?f (f (s)j )
? P??(ai = j|e(s), f (s), c(s))
]
Here ?? can be computed under Model 1 or the HMM,
and is calculated using the forward-backward algorithm
for the HMM.
2.1.1 Parameter estimation with clustered contexts
We can re-write the EM auxiliary function (Equation
5) in terms of the cluster-specific translation probabilities:
?
e
?
f
|Kf |?
l=1
?
c?Kf,l
??(e|f, c) log t(e|f, c)
= ?
e
?
f
?
K?Kf
??(e|f,K) log t(e|f,K) (6)
where ??(e|f,K) = ?
c?K
??(e|f, c)
Following the usual derivation, the EM update for the
class-specific translation probabilities becomes
t?(e|f,K) = ?
?(e|f,K)?
e? ??(e?|f,K)
. (7)
Standard EM training can be viewed a special case of this,
with every context of a source word grouped into a sin-
gle cluster. Another way to view these clustered context-
dependent models is that contexts belonging to the same
cluster are tied and share a common translation proba-
bility distribution, which is estimated from all training
examples in which any of the contexts occur.
2.2 Decision trees for context clustering
The objective for each source word is to split the contexts
into classes to maximise the likelihood of the training
data. Since it is not feasible to maximise the likelihood
of the observations directly, we maximise the expected
log likelihood by considering the EM auxiliary function,
in a similar manner to that used for modelling contextual
variations of phones for ASR (Young et al, 1994; Singer
and Ostendorf, 1996). We perform divisive clustering in-
dependently for each source word f , by building a binary
decision tree which forms classes of contexts which max-
imise the EM auxiliary function. Questions for the tree
are drawn from a set of questions Q = {q1, . . . , q|Q|}
concerning the context information of f .
Let K be any set of contexts of f , and define
L(K) = ?
e
?
c?K
??(e|f, c) log t(e|f, c)
= ?
e
?
c?K
??(e|f, c) log
?
c?K ??(e|f, c)?
e?
?
c?K ??(e?|f, c)
.
This is the contribution to the EM auxiliary function of
source word f occurring in the contexts of K. Let q be
a binary question about the context of f , and consider
the effect on the partial auxiliary function (Equation 6)
of splitting K into two clusters using question q. Define
Kq be the set of contexts in K which answer ?yes? to q
and Kq? be the contexts which answer ?no?. Define the
objective function
Qf,q(K) =
?
e
?
c?Kq
??(e|f, c) log t(e|f, c)
+?
e
?
c?Kq?
??(e|f, c) log t(e|f, c)
= L(Kq) + L(Kq?)
When the node is split using question q, the increase in
objective function is given by
Qf,q(K)? L(K) = L(Kq?) + L(Kq)? L(K).
112
We choose q to maximise this.
In order to build the decision tree for f , we take the set
of all contexts Cf as the initial cluster at the root node.
We then find the question q? such that Qf,q(Cf ) is maxi-
mal, i.e.
q? = arg max
q?Q
Qf,q(Cf )
This splits Cf , so our decision tree now has two nodes.
We iterate this process, at each iteration splitting (into
two further nodes) the leaf node that leads to the great-
est increase in objective function. This leads to a greedy
search to optimise the log likelihood over possible state
clusterings.
In order to control the growth of the tree, we put in
place two thresholds:
? Timp is the minimum improvement in objective func-
tion required for a node to be split; without it, we
would continue splitting nodes until each contained
only one context, even though doing so would cause
data sparsity problems.
? Tocc is the minimum occupancy of a node, based on
how often the contexts at that node occur in the train-
ing data; we want to ensure that there are enough ex-
amples of a context in the training data to estimate
accurately the translation probability distribution for
that cluster.
For each leaf node l and set of contextsKl at that node,
we find the question ql that, when used to split Kl, pro-
duces the largest gain in objective function:
ql = arg max
q?Q
[L(Kl,q) + L(Kl,q?)? L(Kl)]
= arg max
q?Q
[L(Kl,q) + L(Kl,q?)]
We then find the leaf node for which splitting gives the
largest improvement:
l? = arg max
l
[L(Kl,ql) + L(Kl,q?l)? L(Kl)]
If the following criteria are both satisfied at that node, we
split the node into two parts, creating two leaf nodes in
its place:
? The objective function increases sufficiently
L(Kl,ql) + L(Kl,q?l)? L(Kl) > Timp
? The occupancy threshold is exceeded for both child
nodes:
?
e
?
c?Kl,x
??(e|f, c) > Tocc for x = q, q?
We perform such clustering for every source word in the
parallel text.
shares NNS ? ? ? ? ? ? ?
bank NN ? ? ? ? ? ? ?
the DT ? ? ? ? ? ? ?
of IN ? ? ? ? ? ? ?
% PUNC ? ? ? ? ? ? ?
12 NN ? ? ? ? ? ? ?
selling VBG ? ? ? ? ? ? ?
of IN ? ? ? ? ? ? ?
deal NN ? ? ? ? ? ? ?
the DT ? ? ? ? ? ? ?
Sfq
pN
N
by
EN
N
12
NN
%
PU
NC
mn
IN
>
shm
NN
Al
bn
kN
N
city NN ? ? ? ? ? ? ? ?
the DT ? ? ? ? ? ? ? ?
in IN ? ? ? ? ? ? ? ?
liquor NN ? ? ? ? ? ? ? ?
selling VBG ? ? ? ? ? ? ? ?
were VBD ? ? ? ? ? ? ? ?
owners NNS ? ? ? ? ? ? ? ?
whose WP$ ? ? ? ? ? ? ? ?
houses NNS ? ? ? ? ? ? ? ?
several JJ ? ? ? ? ? ? ? ?
and CC ? ? ? ? ? ? ? ?
w+
CC
mn
Az
lN
N
Ed
pJ
J
>
SH
Ab
hA
NN
yb
yE
wn
VB
P
Al
xm
wr
NN
fy
IN
Al
md
yn
pN
N
Figure 1: Alignment of the English selling in different contexts.
In the first, it is preceded by of and links to the infinitive of the
Arabic verb byE; in the second, it is preceded by were and links
to an inflected form of the same Arabic verb, ybyEwn.
3 Evaluation of alignment quality
Our models were built using the MTTK toolkit (Deng
and Byrne, 2005b). Decision tree clustering was imple-
mented and the process parallelised to enable thousands
of decision trees to be built. Our context-dependent (CD)
Model 1 models trained on context-annotated data were
compared to the baseline context-independent (CI) mod-
els trained on untagged data.
The models were trained using data allowed for the
NIST 08 Arabic-English evaluation1, excluding the UN
collections, comprising 300k parallel sentence pairs, a to-
tal of 8.4M words of Arabic and 9.5M words of English.
The Arabic language incorporates into its words sev-
eral prefixes and suffixes which determine grammatical
features such as gender, number, person and voice. The
MADA toolkit (Habash and Sadat, 2006) was used to
perform Arabic morphological word decomposition and
part-of-speech tagging. It determines the best analysis
for each word in a sentence and splits word prefixes and
suffixes, based on the alternative analyses provided by
BAMA (Buckwalter, 2002). We use tokenisation scheme
1http://nist.gov/speech/tests/mt/2008
113
?D2?, which splits certain prefixes and has been reported
to improve machine translation performance (Habash and
Sadat, 2006). The alignment models are trained on this
processed data, and the prefixes and suffixes are treated
as words in their own right; in particular their contexts
are examined and clustered.
The TnT tagger (Brants, 2000), used as distributed
with its model trained on the Wall Street Journal portion
of the Penn treebank, was used to obtain part-of-speech
tags for the English side of the parallel text. Marcus et al
(1993) gives a complete list of part-of-speech tags pro-
duced. No morphological analysis is performed for En-
glish.
Automatic word alignments were compared to a
manually-aligned corpus made up of the IBM Arabic-
English Word Alignment Corpus (Ittycheriah et al,
2006) and the word alignment corpora LDC2006E86 and
LDC2006E93. This contains 28k parallel text sentences
pairs: 724k words of Arabic and 847k words of English.
The alignment links were modified to reflect the MADA
tokenisation; after modification, there are 946k word-to-
word alignment links.
Alignment quality was evaluated by computing Align-
ment Error Rate (AER) (Och and Ney, 2000) relative to
the manual alignments. Since the links supplied con-
tain only ?sure? links and no ?possible? links, we use the
following formula for computing AER given reference
alignment links S and hypothesised alignment links A:
AER = 1? 2|S?A||S|+|A| .
3.1 Questions about contexts
The algorithm presented in Section 2 allows for any infor-
mation about the context of the source word to be consid-
ered. We could consider general questions of the form ?Is
the previous word x?? and ?Does word y occur within n
words of this one??. To maintain computational tractabil-
ity, we restrict the questions to those concerning the part-
of-speech tag assigned to the current, previous and next
words. We do not ask questions about the identities of the
words themselves. For each part-of-speech tag T , we ask
the question ?Does w have tag T??. In addition, we group
part-of-speech tags to ask more general questions: e.g.
the set of contexts which satisfies ?Is w a noun?? contains
those that satisfy ?Is w a proper noun?? and ?Is w a sin-
gular or mass noun??. We also ask the same questions
of the previous and next words in the source sentence.
In English, this gives a total of 152 distinct questions,
each of which is considered when splitting a leaf node.
The MADA part-of-speech tagger uses a reduced tag set,
which produces a total of 68 distinct questions.
Figure 1 shows the links of the English source word
selling in two different contexts where it links to different
words in Arabic, which are both forms of the same verb.
The part-of-speech of the previous word is useful for dis-
-4.22e+07
-4.2e+07
-4.18e+07
-4.16e+07
-4.14e+07
-4.12e+07
-4.1e+07
-4.08e+07
-4.06e+07
-4.04e+07
 11  12  13  14  15  16  17  18  19  20
Lo
g 
pr
ob
ab
ilit
y 
of
 tr
ai
ni
ng
 d
at
a
Iteration
CI Model 1
Threshold 10
Threshold 20
Threshold 60
-3.1e+06
-3.05e+06
-3e+06
-2.95e+06
-2.9e+06
-2.85e+06
-2.8e+06
-2.75e+06
 11  12  13  14  15  16  17  18  19  20
Lo
g 
pr
ob
ab
ilit
y 
of
 tr
ai
ni
ng
 d
at
a
Iteration
CI Model 1
Threshold 10
Threshold 20
Threshold 60
Figure 2: Increase in log probability of training data during
training for varying Timp, with Model 1, for Arabic to English
(top) and English to Arabic (bottom)
criminating between the two cases, whereas a context-
independent model would assign the same probability to
both Arabic words.
3.2 Training Model 1
Training is carried out in both translation directions. For
Arabic to English, the Arabic side of the parallel text is
tagged and the English side remains untagged; we view
the English words as being generated from the Arabic
words and questions are asked about the context of the
Arabic words to determine clusters for the translation ta-
ble. For English to Arabic, the situation is reversed: we
used tagged English text as the source language and un-
tagged Arabic text, with morphological decomposition,
as the target language.
Standard CI Model 1 training, initialised with a uni-
form translation table so that t(e|f) is constant for all
source/target word pairs (f, e), was run on untagged data
for 10 iterations in each direction (Brown et al, 1993;
Deng and Byrne, 2005b). A decision tree was built to
cluster the contexts and a further 10 iterations of training
were carried out using the tagged words-with-context to
produce context-dependent models (CD Model 1). The
114
English question Frequency
Is Next Preposition 1523
Is Prev Determiner 1444
Is Prev Preposition 1209
Is Prev Adjective 864
Is Next Noun Singular Mass 772
Is Prev Noun Singular Mass 690
Is Next Noun Plural 597
Is Next Noun 549
Arabic question Frequency
Is Prev Preposition 1110
Is Next Preposition 993
Is Prev Noun 981
Is Next Noun 912
Is Prev Coordinating Conjunction 627
Is Prev Noun SingularMass 607
Is Next Punctuation 603
Is Next Adjective Adverb 559
Table 1: Most frequent root node context questions
models were then evaluated using AER at each train-
ing iteration. A number of improvement thresholds Timp
were tested, and performance compared to that of models
found after further iterations of CI Model 1 training on
the untagged data. In both alignment directions, the log
probability of the training data increases during training
(see Figure 2). As expected, the training set likelihood
increases as the threshold Timp is reduced, allowing more
clusters and closer fitting to the data.
3.2.1 Analysis of frequently used questions
Table 1 shows the questions used most frequently at
the root node of the decision tree when clustering con-
texts in English and Arabic. Because they are used first,
these are the questions that individually give the great-
est ability to discriminate between the different contexts
of a word. The list shows the importance of the left and
right contexts of the word in predicting its translation: of
the most common 50 questions, 25 concern the previous
word, 19 concern the next, and only 6 concern the part-
of-speech of the current word. For Arabic, of the most
frequent 50 questions, 21 concern the previous word, 20
concern the next and 9 the current word.
3.2.2 Alignment Error Rate
Since MT systems are usually built on the union of the
two sets of alignments (Koehn et al, 2003), we consider
the union of alignments in the two directions as well as
those in each direction. Figure 3 shows the change in
AER of the alignments in each direction, as well as the
alignment formed by taking their union at corresponding
thresholds and training iterations.
Timp Arabic-English (%) English-Arabic (%)
10 30601 (25.33) 26011 (39.87)
20 11193 (9.27) 18365 (28.15)
40 1874 (1.55) 9104 (13.96)
100 307 (0.25) 1128 (1.73)
Table 2: Words [number (percentage)] with context-dependent
translation for varying Timp
3.2.3 Variation of improvement threshold Timp
There is a trade-off between modelling the data accu-
rately, which requires more clusters, and eliminating data
sparsity problems, which requires each cluster to contain
contexts that occur frequently enough in the training data
to estimate the translation probabilities accurately. Use of
a smaller threshold Timp leads to more clusters per word
and an improved ability to fit to the data, but this can lead
to reduced alignment quality if there is insufficient data
to estimate the translation probability distribution accu-
rately for each cluster. For lower thresholds, we observe
over-fitting and the AER rises after the second iteration of
CD training, similar to the behaviour seen in Och (2002).
Setting Timp = 0 results in each context of a word having
its own cluster, which leads to data sparsity problems.
Table 2 shows the percentage of words for which the
contexts are split into multiple clusters for CD Model 1
with varying improvement thresholds. This occurs when
there are enough training data examples and sufficient
variability between the contexts of a word that splitting
the contexts into more than one cluster increases the EM
auxiliary function. For words where the contexts are not
split, all the contexts remain in the same cluster and pa-
rameter estimation is exactly the same as for the unclus-
tered context-independent models.
3.3 Training HMMs
Adding source word context to translation has so far led
to improvements in AER for Model 1, but the perfor-
mance does not match that of HMMs trained on untagged
data; we therefore train HMMs on tagged data.
We proceed with Model 1 and Model 2 trained in the
usual way, and context-independent (CI) HMMs were
trained for 5 iterations on the untagged data. Statistics
were then gathered for clustering at various thresholds,
after which 5 further EM iterations were performed with
tagged data to produce context-dependent (CD) HMMs.
The HMMs were trained in both the Arabic to English
and the English to Arabic directions. The log likelihood
of the training set varies with Timp in much the same
way as for Model 1, increasing at each iteration, with
greater likelihood at lower thresholds. Figure 4 shows
how the AER of the union alignment varies with Timp
during training. As with Model 1, the clustered HMM
115
 49.2
 49.4
 49.6
 49.8
 50
 50.2
 50.4
 50.6
 50.8
 10  11  12  13  14  15  16  17  18  19  20
AE
R
Iteration
CI Model 1
Threshold 10
Threshold 20
Threshold 60
49.6
49.8
50.0
50.2
50.4
50.6
50.8
51.0
51.2
 10  11  12  13  14  15  16  17  18  19  20
AE
R
Iteration
CI Model 1
Threshold 10
Threshold 20
Threshold 60
Threshold 100
49.0
49.2
49.4
49.6
49.8
50.0
50.2
50.4
50.6
50.8
51.0
 10  11  12  13  14  15  16  17  18  19  20
AE
R
Iteration
CI Model 1
Threshold 10
Threshold 20
Threshold 60
Figure 3: Variation of AER during Model 1 training for varying
Timp, for Arabic to English (top), English to Arabic (middle)
and their union (bottom)
34.4
34.5
34.6
34.7
34.8
34.9
35.0
35.1
35.2
35.3
 5  6  7  8  9  10
AE
R
Iteration
CI HMM
Threshold 10
Threshold 20
Threshold 60
Figure 4: AER of the union alignment for varying Timp with the
HMM model
 72
 74
 76
 78
 80
 40  45  50  55  60
Pr
ec
is
io
n
Recall
p0=0.95
p0=0.00
p0=0.95
p0=0.00
English-Arabic CD HMM
English-Arabic CI HMM
Arabic-English CD HMM
Arabic-English CI HMM
Figure 5: Precision/recall curves for the context-dependent
HMM and the baseline context-independent HMM, for Arabic
to English and English to Arabic. p0 varies from 0.00 to 0.95 in
steps of 0.05.
models produce alignments with a lower AER than the
baseline model, and there is evidence of over-fitting to
the training data.
3.3.1 Alignment precision and recall
The HMM models include a null transition probability,
p0, which can be modified to adjust the number of align-
ments to the null token (Deng and Byrne, 2005a). Where
a target word is emitted from null, it is not included in
the alignment links, so this target word is viewed as not
being aligned to any source word; this affects the preci-
sion and recall. The results reported above use p0 = 0.2
for English-Arabic and p0 = 0.4 for Arabic-English; we
can tune these values to produce alignments with the low-
est AER. Figure 5 shows precision-recall curves for the
CD HMMs compared to the CI HMMs for both transla-
tion directions. For a given value of precision, the CD
HMM has higher recall; for a given value of recall, the
CD HMM has higher precision.
We do not report F-score (Fraser and Marcu, 2006)
since in our experiments we have not found strong cor-
relation with translation performance, but we note that
these results for precision and recall should lead to im-
proved F-scores as well.
4 Evaluation of translation quality
We have shown that the context-dependent models pro-
duce a decrease in AER measured on manually-aligned
data; we wish to show this improved model performance
leads to an increase in translation quality, measured by
BLEU score (Papineni et al, 2001). In addition to the
Arabic systems already evaluated by AER, we also report
results for a Chinese-English translation system.
Alignment models were evaluated by aligning the
training data using the models in each translation direc-
116
tion. HiFST, a WFST-based hierarchical translation sys-
tem described in (Iglesias et al, 2009), was trained on
the union of these alignments. MET (Och, 2003) was
carried out using a development set, and the BLEU score
evaluated on two test sets. Decoding used a 4-gram lan-
guage model estimated from the English side of the entire
MT08 parallel text, and a 965M word subset of monolin-
gual data from the English Gigaword Third Edition.
For both Arabic and English, the CD HMM models
were evaluated as follows. Iteration 5 of the CI HMM
was used to produce alignments for the parallel text train-
ing data: these were used to train the baseline system.
The same data is aligned using CD HMMs after two
further iterations of training and a second WFST-based
translation system built from these alignments. The mod-
els are evaluated by comparing BLEU scores with those
of the baseline model.
4.1 Arabic to English translation
Alignment models were trained on the NIST MT08
Arabic-English parallel text, excluding the UN portion.
The null alignment probability was chosen based on the
AER, resulting in values of p0 = 0.05 for Arabic to
English and p0 = 0.10 for English to Arabic. We per-
form experiments on the NIST Arabic-English transla-
tion task. The mt02 05 tune and mt02 05 test data sets
are formed from the odd and even numbered sentences
of the NIST MT02 to MT05 evaluation sets respectively;
each contains 2k sentences and 60k words. We use
mt02 05 tune as a development set and evaluate the sys-
tem on mt02 05 test and the newswire portion of the
MT08 set, MT08-nw. Table 3 shows a comparison of the
system trained using CD HMMs with the baseline sys-
tem, which was trained using CI HMM models on un-
tagged data. The context-dependent models result in a
gain in BLEU score of 0.3 for mt02 05 test and 0.6 for
MT08-nw.
4.2 Chinese to English translation
The Chinese training set was 600k random parallel text
sentences of the newswire LDC collection allowed for
NIST MT08, a total of 15.2M words of Chinese and
16.6M words of English. The Chinese text was tagged us-
ing the MXPOST maximum-entropy part of speech tag-
ging tool (Ratnaparkhi, 1996) trained on the Penn Chi-
nese Treebank 5.1; the English text was tagged using the
TnT part of speech tagger (Brants, 2000) trained on the
Wall Street Journal portion of the English Penn treebank.
The development set tune-nw and validation set test-nw
contain a mix of the newswire portions of MT02 through
MT05 and additional developments sets created by trans-
lation within the GALE program. We also report results
on the newswire portion of the MT08 set. Again we see
an increase in BLEU score for both test sets: 0.5 for test-
Arabic-English
Alignments tune mt02 05 test MT08-nw
CI HMM 50.0 49.4 46.3
CD HMM 50.0 49.7 46.9
Chinese-English
Alignments tune test-nw MT08-nw
CI HMM 28.1 28.5 26.9
CD HMM 28.5 29.0 27.7
Table 3: Comparison, using BLEU score, of the CD HMM with
the baseline CI HMM
nw and 0.8 for MT08-nw.
5 Conclusions and future work
We have introduced context-dependent Model 1 and
HMM alignment models, which use context information
in the source language to improve estimates of word-
to-word translation probabilities. Estimation of parame-
ters using these contexts without smoothing leads to data
sparsity problems; therefore we have developed decision
tree clustering algorithms to cluster source word contexts
based on optimisation of the EM auxiliary function. Con-
text information is incorporated by the use of part-of-
speech tags in both languages of the parallel text, and the
EM algorithm is used for parameter estimation.
We have shown that these improvements to the model
lead to decreased AER compared to context-independent
models. Finally, we compare machine translation sys-
tems built using our context-dependent alignments. For
both Arabic- and Chinese-to-English translation, we
report an increase in translation quality measured by
BLEU score compared to a system built using context-
independent alignments.
This paper describes an initial investigation into
context-sensitive alignment models, and there are many
possible directions for future research. Clustering the
probability distributions of infrequently occurring may
produce improvements in alignment quality, different
model training schemes and extensions of the context-
dependence to more sophisticated alignment models will
be investigated. Further translation experiments will be
carried out.
Acknowledgements
This work was supported in part by the GALE program of
the Defense Advanced Research Projects Agency, Con-
tract No. HR0011-06-C-0022. J. Brunning is supported
by a Schiff Foundation graduate studentship. Thanks to
Yanjun Ma, Dublin City University, for training the Chi-
nese part of speech tagger.
117
References
A. L. Berger, S. Della Pietra, and V. J. Della Pietra. 1996. A
maximum entropy approach to natural language processing.
Computational Linguistics, 22(1):39?71.
Graeme Blackwood, Adria` de Gispert, Jamie Brunning, and
William Byrne. 2008. European language translation with
weighted finite state transducers: The CUED MT system for
the 2008 ACL workshop on SMT. In Proceedings of the
Third Workshop on Statistical Machine Translation, pages
131?134, Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Thorsten Brants. 2000. TnT ? a statistical part-of-speech tag-
ger. In Proceedings of the 6th Applied Natural Language
Processing Conference: ANLP-2000, Seattle, USA.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra,
and Robert L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Computational
Linguistics, 19(2):263?311.
T. Buckwalter. 2002. Buckwalter Arabic morphological ana-
lyzer.
Marta Ruiz Costa-jussa` and Jos?e A. R. Fonollosa. 2005.
Improving phrase-based statistical translation by modifying
phrase extraction and including several features. In Proceed-
ings of the ACL Workshop on Building and Using Parallel
Texts, pages 149?154, June.
Yonggang Deng and William Byrne. 2005a. HMM word and
phrase alignment for statistical machine translation. In Proc.
of HLT-EMNLP.
Yonggang Deng and William Byrne. 2005b. JHU-Cambridge
statistical machine translation toolkit (MTTK) user manual.
Yonggang Deng, Shankhar Kumar, and William Byrne. 2007.
Segmentation and alignment of parallel text for statistical
machine translation. Journal of Natural Language Engineer-
ing, 13:3:235?260.
Alexander Fraser and Daniel Marcu. 2006. Measuring word
alignment quality for statistical machine translation. Tech-
nical Report ISI-TR-616, ISI/University of Southern Califor-
nia, May.
Nizar Habash and Fatiha Sadat. 2006. Arabic preprocessing
schemes for statistical machine translation. In HLT-NAACL.
G. Iglesias, A. de Gispert, E. R. Banga, and W. Byrne. 2009.
Hierarchical phrase-based translation with weighted finite
state transducers. In Procedings of NAACL-HLT, 2009,
Boulder, Colorado.
Abraham Ittycheriah, Yaser Al-Onaizan, and Salim Roukos.
2006. The IBM Arabic-English word alignment corpus, Au-
gust.
A. Kannan, M. Ostendorf, and J. R. Rohlicek. 1994. Maxi-
mum likelihood clustering of Gaussians for speech recogni-
tion. Speech and Audio Processing, IEEE Transactions on,
2(3):453?455, July.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Sta-
tistical phrase-based translation. In NAACL ?03: Proceed-
ings of the 2003 Conference of the North American Chapter
of the Association for Computational Linguistics on Human
Language Technology, pages 48?54.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice
Santorini. 1993. Building a large annotated corpus of
English: the Penn Treebank. Computational Linguistics,
19(2):313?330.
Sonja Nie?en and Hermann Ney. 2001a. Morpho-syntactic
analysis for reordering in statistical machine translation. In
Proceedings of MT Summit VIII, pages 247?252, September.
Sonja Nie?en and Hermann Ney. 2001b. Toward hierarchical
models for statistical machine translation of inflected lan-
guages. In Proceedings of the workshop on Data-driven
methods in machine translation, pages 1?8, Morristown, NJ,
USA. Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2000. A comparison of
alignment models for statistical machine translation. In Pro-
ceedings of the 18th conference on Computational Linguis-
tics, pages 1086?1090.
F. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Yamada,
A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng, V. Jain,
Z. Jin, and D. Radev. 2004. A smorgasbord of features for
statistical machine translation. In Proceedings of NAACL.
Franz Josef Och. 2002. Statistical Machine Translation: From
Single Word Models to Alignment Templates. Ph.D. thesis,
Franz Josef Och.
Franz Josef Och. 2003. Minimum error rate training in statisti-
cal machine translation. In ACL ?03: Proceedings of the 41st
Annual Meeting on Association for Computational Linguis-
tics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2001. Bleu: a method for automatic evaluation of ma-
chine translation. In Proceedings of ACL, pages 311?318.
Maja Popovic? and Hermann Ney. 2004. Improving word align-
ment quality using morpho-syntactic information. In In Pro-
ceedings of COLING, page 310.
Adwait Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Processing,
pages 133?142.
H. Singer and M. Ostendorf. 1996. Maximum likelihood suc-
cessive state splitting. Proceedings of ICASSP, 2:601?604.
Nicolas Stroppa, Antal van den Bosch, and Andy Way. 2007.
Exploiting source similarity for SMT using context-informed
features. In Proceedings of the 11th Conference on Theoreti-
cal and Methodological Issues in Machine Translation (TMI
2007), pages 231 ? 240.
Kristina Toutanova, H. Tolga Ilhan, and Christopher D. Man-
ning. 2002. Extensions to HMM-based statistical word
alignment models. In Proceedings of EMNLP, pages 87?94.
Ismael Garc??a Varea, Franz J. Och, Hermann Ney, and Fran-
cisco Casacuberta. 2002. Improving alignment quality in
statistical machine translation using context-dependent max-
imum entropy models. In Proceedings of COLING, pages
1?7.
Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996.
HMM-based word alignment in statistical translation. In
Proceedings of COLING, pages 836?841.
S. J. Young, J. J. Odell, and P. C. Woodland. 1994. Tree-based
state tying for high accuracy acoustic modelling. In HLT ?94:
Proceedings of the workshop on Human Language Technol-
ogy, pages 307?312.
118
Proceedings of the Third Workshop on Statistical Machine Translation, pages 131?134,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
European Language Translation with Weighted Finite State Transducers:
The CUED MT System for the 2008 ACL Workshop on SMT
Graeme Blackwood, Adria` de Gispert, Jamie Brunning, William Byrne
Machine Intelligence Laboratory
Department of Engineering, Cambridge University
Trumpington Street, Cambridge, CB2 1PZ, U.K.
{gwb24|ad465|jjjb2|wjb31}@cam.ac.uk
Abstract
We describe the Cambridge University En-
gineering Department phrase-based statisti-
cal machine translation system for Spanish-
English and French-English translation in the
ACL 2008 Third Workshop on Statistical Ma-
chine Translation Shared Task. The CUED
system follows a generative model of trans-
lation and is implemented by composition of
component models realised as Weighted Fi-
nite State Transducers, without the use of a
special-purpose decoder. Details of system
tuning for both Europarl and News translation
tasks are provided.
1 Introduction
The Cambridge University Engineering Department
statistical machine translation system follows the
Transducer Translation Model (Kumar and Byrne,
2005; Kumar et al, 2006), a phrase-based generative
model of translation that applies a series of transfor-
mations specified by conditional probability distri-
butions and encoded as Weighted Finite State Trans-
ducers (Mohri et al, 2002).
The main advantages of this approach are its mod-
ularity, which facilitates the development and eval-
uation of each component individually, and its im-
plementation simplicity which allows us to focus on
modeling issues rather than complex decoding and
search algorithms. In addition, no special-purpose
decoder is required since standard WFST operations
can be used to obtain the 1-best translation or a lat-
tice of alternative hypotheses. Finally, the system
architecture readily extends to speech translation, in
which input ASR lattices can be translated in the
same way as for text (Mathias and Byrne, 2006).
This paper reviews the first participation of CUED
in the ACL Workshop on Statistical Machine Trans-
lation in 2008. It is organised as follows. Firstly,
section 2 describes the system architecture and its
main components. Section 3 gives details of the de-
velopment work conducted for this shared task and
results are reported and discussed in section 4. Fi-
nally, in section 5 we summarise our participation in
the task and outline directions for future work.
2 The Transducer Translation Model
Under the Transducer Translation Model, the gen-
eration of a target language sentence tJ1 starts with
the generation of a source language sentence sI1 by
the source language model PG(sI1). Next, the source
language sentence is segmented into phrases accord-
ing to the unweighted uniform phrasal segmenta-
tion model PW (uK1 ,K|sI1). This source phrase se-
quence generates a reordered target language phrase
sequence according to the phrase translation and re-
ordering model PR(xK1 |uK1 ). Next, target language
phrases are inserted into this sequence according to
the insertion model P?(vR1 |xK1 , uK1 ). Finally, the
sequence of reordered and inserted target language
phrases are transformed to word sequences tJ1 under
the target phrasal segmentation model P?(tJ1 |vR1 ).
These component distributions together form a joint
distribution over the source and target language sen-
tences and their possible intermediate phrase se-
quences as P (tJ1 , vR1 , xK1 , uK1 , sI1).
In translation under the generative model, we start
with the target sentence tJ1 in the foreign language
131
and search for the best source sentence s?I1. Encod-
ing each distribution as a WFST leads to a model of
translation as the series of compositions
L = G ? W ? R ? ? ?? ? T (1)
in which T is an acceptor for the target language
sentence and L is the word lattice of translations ob-
tained during decoding. The most likely translation
s?I1 is the path in L with least cost.
2.1 TTM Reordering Model
The TTM reordering model associates a jump se-
quence with each phrase pair. For the experi-
ments described in this paper, the jump sequence
is restricted such that only adjacent phrases can be
swapped; this is the MJ1 reordering model of (Ku-
mar and Byrne, 2005). Although the reordering
probability for each pair of phrases could be esti-
mated from word-aligned parallel data, we here as-
sume a uniform reordering probability p tuned as de-
scribed in section 3.1. Figure 1 shows how the MJ1
reordering model for a pair of phrases x1 and x2 is
implemented as a WFST.
0 1
x : x
x2 : x1
x1 : x2
p / b=+1
1 / b=?1
1?p / b=0
Figure 1: The uniform MJ1 reordering transducer.
3 System Development
CUED participated in two of the WMT shared task
tracks: French?English and Spanish?English. For
both tracks, primary and contrast systems were sub-
mitted. The primary submission was restricted
to only the parallel and language model data dis-
tributed for the shared task. The contrast submission
incorporates large additional quantities of English
monolingual training text for building the second-
pass language model described in section 3.2.
Table 1 summarises the parallel training data, in-
cluding the total number of sentences, total num-
ber of words, and lower-cased vocabulary size. The
Spanish and French parallel texts each contain ap-
proximately 5% News Commentary data; the rest
is Europarl data. Various single-reference develop-
ment and test sets were provided for each of the
tracks. However, the 2008 evaluation included a new
News task, for which no corresponding development
set was available.
sentences words vocab
FR 39.9M 124k
EN
1.33M 36.4M 106k
ES 38.2M 140k
EN 1.30M 35.7M 106k
Table 1: Parallel corpora statistics.
All of the training and system tuning was per-
formed using lower-cased data. Word alignments
were generated using GIZA++ (Och and Ney, 2003)
over a stemmed version of the parallel text. Stems
for each language were obtained using the Snowball
stemmer1. After unioning the Viterbi alignments,
the stems were replaced with their original words,
and phrase-pairs of up to five foreign words in length
were extracted in the usual fashion (Koehn et al,
2003).
3.1 System Tuning
Minimum error training (Och, 2003) under
BLEU (Papineni et al, 2001) was used to optimise
the feature weights of the decoder with respect
to the dev2006 development set. The following
features are optimized:
? Language model scale factor
? Word and phrase insertion penalties
? Reordering scale factor
? Insertion scale factor
? Translation model scale factor: u-to-v
? Translation model scale factor: v-to-u
? Three phrase pair count features
The phrase-pair count features track whether each
phrase-pair occurred once, twice, or more than twice
1Available at http://snowball.tartarus.org
132
in the parallel text (Bender et al, 2007). All de-
coding and minimum error training operations are
performed with WFSTs and implemented using the
OpenFST libraries (Allauzen et al, 2007).
3.2 English Language Models
Separate language models are used when translating
the Europarl and News sets. The models are esti-
mated using SRILM (Stolcke, 2002) and converted
to WFSTs for use in TTM translation. We use the of-
fline approximation in which failure transitions are
replaced with epsilons (Allauzen et al, 2003).
The Europarl language model is a Kneser-
Ney (Kneser and Ney, 1995) smoothed default-
cutoff 5-gram back-off language model estimated
over the concatenation of the Europarl and News
language model training data. The News language
model is created by optimising the interpolation
weights of two component models with respect to
the News Commentary development sets since we
believe these more closely match the newstest2008
domain. The optimised interpolation weights were
0.44 for the Europarl corpus and 0.56 for the much
smaller News Commentary corpus. For our contrast
submission, we rescore the first-pass translation lat-
tices with a large zero-cutoff stupid-backoff (Brants
et al, 2007) language model estimated over approx-
imately five billion words of newswire text.
4 Results and Discussion
Table 2 reports lower-cased BLEU scores for the
French?English and Spanish?English Europarl
and News translation tasks. The NIST scores are
also provided in parentheses. The row labelled
?TTM+MET? shows results obtained after TTM
translation and minimum error training, i.e. our pri-
mary submission constrained to use only the data
distributed for the task. The row labelled ?+5gram?
shows translation results obtained after rescoring
with the large zero-cutoff 5-gram language model
described in section 3.2. Since this includes addi-
tional language model data, it represents the CUED
contrast submission.
Translation quality for the ES?EN task is
slightly higher than that of FR?EN. For Europarl
translation, most of the additional English language
model training data incorporated into the 5-gram
rescoring step is out-of-domain and so does not sub-
stantially improve the scores. Rescoring yields an
average gain of just +0.5 BLEU points.
Translation quality is significantly lower in both
language pairs for the new news2008 set. Two fac-
tors may account for this. The first is the change
in domain and the fact that no training or devel-
opment set was available for the News translation
task. Secondly, the use of a much freer translation
in the single News reference, which makes it dif-
ficult to obtain a good BLEU score. However, the
second-pass 5-gram language model rescoring gains
are larger than those observed in the Europarl sets,
with approximately +1.7 BLEU points for each lan-
guage pair. The additional in-domain newswire data
clearly helps to improve translation quality.
Finally, we use a simple 3-gram casing model
trained on the true-case workshop distributed
language model data, and apply the SRILM
disambig tool to restore true-case for our final
submissions. With respect to the lower-cased scores,
true-casing drops around 1.0 BLEU in the Europarl
task, and around 1.7 BLEU in the News Commen-
tary and News tasks.
5 Summary
We have reviewed the Cambridge University Engi-
neering Department first participation in the work-
shop on machine translation using a phrase-based
SMT system implemented with a simple WFST ar-
chitecture. Results are largely competitive with the
state-of-the-art in this task.
Future work will examine whether further im-
provements can be obtained by incorporating addi-
tional features into MET, such as the word-to-word
Model 1 scores or phrasal segmentation models. The
MJ1 reordering model could also be extended to al-
low for longer-span phrase movement. Minimum
Bayes Risk decoding, which has been applied suc-
cessfully in other tasks, could also be included.
The difference in the gains from 5-gram lattice
rescoring suggests that, particularly for Europarl
translation, it is important to ensure the language
model data is in-domain. Some form of count mix-
ing or alternative language model adaptation tech-
niques may prove useful for unconstrained Europarl
translation.
133
Task dev2006 devtest2006 test2007 test2008 newstest2008
FR?EN TTM+MET 31.92 (7.650) 32.51 (7.719) 32.94 (7.805) 32.83 (7.799) 19.58 (6.108)
+5gram 32.51 (7.744) 32.96 (7.797) 33.33 (7.880) 33.03 (7.856) 21.22 (6.311)
ES?EN TTM+MET 33.11 (7.799) 32.25 (7.649) 32.90 (7.766) 33.11 (7.859) 20.99 (6.308)
+5gram 33.30 (7.835) 32.96 (7.740) 33.55 (7.857) 33.47 (7.893) 22.83 (6.513)
Table 2: Translation results for the Europarl and News tasks for various dev sets and the 2008 test sets.
Acknowledgements
This work was supported in part under the GALE
program of the Defense Advanced Research Projects
Agency, Contract No. HR0011-06-C-0022.
References
Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2003.
Generalized algorithms for constructing statistical lan-
guage models. In Proceedings of the 41st Meeting of
the Association for Computational Linguistics, pages
557?564.
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Woj-
ciech Skut, and Mehryar Mohri. 2007. OpenFST: a
general and efficient weighted finite-state transducer
library. In Proceedings of the 9th International Con-
ference on Implementation and Application of Au-
tomata, pages 11?23. Springer.
Oliver Bender, Evgeny Matusov, Stefan Hahn, Sasa
Hasan, Shahram Khadivi, and Hermann Ney. 2007.
The RWTH Arabic-to-English spoken language trans-
lation system. In Proceedings of the 2007 Automatic
Speech Understanding Workshop, pages 396?401.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 858?867.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Acoustics, Speech, and
Signal Processing, pages 181?184.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the 2003 Conference for Computational Lin-
guistics on Human Language Technology, pages 48?
54, Morristown, NJ, USA.
Shankar Kumar and William Byrne. 2005. Local phrase
reordering models for statistical machine translation.
In Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, pages 161?168.
Shankar Kumar, Yonggang Deng, and William Byrne.
2006. A weighted finite state transducer transla-
tion template model for statistical machine translation.
Natural Language Engineering, 12(1):35?75.
Lambert Mathias and William Byrne. 2006. Statistical
phrase-based speech translation. In 2006 IEEE Inter-
national Conference on Acoustics, Speech and Signal
Processing.
Mehryar Mohri, Fernando Pereira, and Michael Riley.
2002. Weighted finite-state transducers in speech
recognition. In Computer Speech and Language, vol-
ume 16, pages 69?88.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Meeting of the Association for Computational
Linguistics, pages 160?167, Morristown, NJ, USA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th Meeting of the Association for Computational
Linguistics, pages 311?318, Morristown, NJ, USA.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of Interna-
tional Conference on Spoken Language Processing.
134
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 155?160,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
The CUED HiFST System for the WMT10 Translation Shared Task
Juan Pino Gonzalo Iglesias?1 Adria` de Gispert
Graeme Blackwood Jamie Brunning William Byrne
Department of Engineering, University of Cambridge, Cambridge, CB2 1PZ, U.K.
{jmp84,gi212,ad465,gwb24,jjjb2,wjb31}@eng.cam.ac.uk
? Department of Signal Processing and Communications, University of Vigo, Vigo, Spain
Abstract
This paper describes the Cambridge Uni-
versity Engineering Department submis-
sion to the Fifth Workshop on Statistical
Machine Translation. We report results for
the French-English and Spanish-English
shared translation tasks in both directions.
The CUED system is based on HiFST, a
hierarchical phrase-based decoder imple-
mented using weighted finite-state trans-
ducers. In the French-English task, we
investigate the use of context-dependent
alignment models. We also show that
lattice minimum Bayes-risk decoding is
an effective framework for multi-source
translation, leading to large gains in BLEU
score.
1 Introduction
This paper describes the Cambridge University
Engineering Department (CUED) system submis-
sion to the ACL 2010 Fifth Workshop on Statis-
tical Machine Translation (WMT10). Our trans-
lation system is HiFST (Iglesias et al, 2009a), a
hierarchical phrase-based decoder that generates
translation lattices directly. Decoding is guided
by a CYK parser based on a synchronous context-
free grammar induced from automatic word align-
ments (Chiang, 2007). The decoder is imple-
mented with Weighted Finite State Transducers
(WFSTs) using standard operations available in
the OpenFst libraries (Allauzen et al, 2007). The
use of WFSTs allows fast and efficient exploration
of a vast translation search space, avoiding search
errors in decoding. It also allows better integration
with other steps in our translation pipeline such as
5-gram language model (LM) rescoring and lattice
minimum Bayes-risk (LMBR) decoding.
1Now a member of the Department of Engineering, Uni-
versity of Cambridge, Cambridge, CB2 1PZ, U.K.
# Sentences # Tokens # Types
(A)Europarl+News-Commentary
FR 1.7 M 52.4M 139.7kEN 47.6M 121.6k
(B)Europarl+News-Commentary+UN
FR 8.7 M 277.9M 421.0kEN 241.4M 482.1k
(C)Europarl+News-Commentary+UN+Giga
FR 30.2 M 962.4M 2.4MEN 815.3M 2.7M
Table 1: Parallel data sets used for French-to-
English experiments.
We participated in the French-English and
Spanish-English translation shared tasks in each
translation direction. This paper describes the de-
velopment of these systems. Additionally, we re-
port multi-source translation experiments that lead
to very large gains in BLEU score.
The paper is organised as follows. Section 2
describes each step in the development of our sys-
tem for submission, from pre-processing to post-
processing. Section 3 presents and discusses re-
sults and Section 4 describes an additional experi-
ment on multi-source translation.
2 System Development
We built three French-English and two Spanish-
English systems, trained on different portions of
the parallel data sets available for this shared task.
Statistics for the different parallel sets are sum-
marised in Tables 1 and 2. No additional parallel
data was used. As will be shown, the largest paral-
lel corpus gave the best results in French, but this
was not the case in Spanish.
2.1 Pre-processing
The data was minimally cleaned by replacing
HTML-related metatags by their corresponding
155
# Sentences # Tokens # Types
(A) Europarl + News-Commentary
SP 1.7M 49.4M 167.2kEN 47.0M 122.7k
(B) Europarl + News-Commentary + UN
SP 6.5M 205.6M 420.8kEN 192.0M 402.8k
Table 2: Parallel data sets used for Spanish-to-
English experiments.
UTF8 token (e.g., replacing ?&amp? by ?&?) as
this interacts with tokenization. Data was then to-
kenized and lowercased, so mixed case is added as
post-processing.
2.2 Alignments
Parallel data was aligned using the MTTK toolkit
(Deng and Byrne, 2005). In the English-to-French
and English-to-Spanish directions, we trained
a word-to-phrase HMM model with maximum
phrase length of 2. In the French to English and
Spanish to English directions, we trained a word-
to-phrase HMM Model with a bigram translation
table and maximum phrase length of 4.
We also trained context-dependent alignment
models (Brunning et al, 2009) for the French-
English medium-size (B) dataset. The context of
a word is based on its part-of-speech and the part-
of-speech tags of the surrounding words. These
tags were obtained by applying the TnT Tagger
(Brants, 2000) for English and the TreeTagger
(Schmid, 1994) for French. Decision tree clus-
tering based on optimisation of the EM auxiliary
function was used to group contexts that trans-
late similarly. Unfortunately, time constraints pre-
vented us from training context-dependent models
for the larger (C) dataset.
2.3 Language Model
For each target language, we used the SRILM
Toolkit (Stolcke, 2002) to estimate separate 4-
gram LMs with Kneser-Ney smoothing (Kneser
and Ney, 1995), for each of the corpora listed in
Tables 3, 4 and 5. The LM vocabulary was ad-
justed to the parallel data set used. The compo-
nent models of each language pair were then in-
terpolated to form a single LM for use in first-pass
translation decoding. For French-to-English trans-
lation, the interpolation weights were optimised
for perplexity on a development set.
Corpus # Sentences # Tokens
EU + NC + UN 9.0M 246.4M
CNA 1.3M 34.8M
LTW 12.9M 298.7M
XIN 16.0M 352.5M
AFP 30.4M 710.6M
APW 62.1M 1268.6M
NYT 73.6M 1622.5M
Giga 21.4M 573.8M
News 48.7M 1128.4M
Total 275.4M 6236.4M
Table 3: English monolingual training corpora.
Corpus # Sentences # Tokens
EU + NC + UN 9.0M 282.8
AFP 25.2M 696.0M
APW 12.7M 300.6M
News 15.2M 373.5M
Giga 21.4M 684.4M
Total 83.5 M 2337.3M
Table 4: French monolingual training corpora.
Corpus # Sentences # Tokens
NC + News 4.0M 110.8M
EU + Gigaword (5g) 249.4M 1351.5M
Total 253.4 M 1462.3M
Table 5: Spanish monolingual training corpora.
The Spanish-English first pass LM was trained
directly on the NC+News portion of monolingual
data, as we did not find improvements by using
Europarl. The second pass rescoring LM used all
available data.
2.4 Grammar Extraction and Decoding
After unioning the Viterbi alignments, phrase-
based rules of up to five source words in length
were extracted, hierarchical rules with up to two
non-contiguous non-terminals in the source side
were then extracted applying the restrictions de-
scribed in (Chiang, 2007). For Spanish-English
and French-English tasks, we used a shallow-1
grammar where hierarchical rules are allowed to
be applied only once on top of phrase-based rules.
This has been shown to perform as well as a
fully hierarchical grammar for a Europarl Spanish-
English task (Iglesias et al, 2009b).
For translation, we used the HiFST de-
156
coder (Iglesias et al, 2009a). HiFST is a hierarchi-
cal decoder that builds target word lattices guided
by a probabilistic synchronous context-free gram-
mar. Assuming N to be the set of non-terminals
and T the set of terminals or words, then we can
define the grammar as a set R = {Rr} of rules
Rr : N ? ??r,?r? / pr, where N ? N; and
?, ? ? {N ? T}+.
HiFST translates in three steps. The first step
is a variant of the CYK algorithm (Chappelier and
Rajman, 1998), in which we apply hypothesis re-
combination without pruning. Only the source
language sentence is parsed using the correspond-
ing source-side context-free grammar with rules
N ? ?. Each cell in the CYK grid is specified
by a non-terminal symbol and position: (N,x, y),
spanning sx+y?1x on the source sentence s1...sJ .
For the second step, we use a recursive algo-
rithm to construct word lattices with all possi-
ble translations produced by the hierarchical rules.
Construction proceeds by traversing the CYK grid
along the back-pointers established in parsing. In
each cell (N,x, y) of the CYK grid, we build a
target language word lattice L(N,x, y) containing
every translation of sx+y?1x from every derivation
headed by N . For efficiency, this lattice can use
pointers to lattices on other cells of the grid.
In the third step, we apply the word-based LM
via standard WFST composition with failure tran-
sitions, and perform likelihood-based pruning (Al-
lauzen et al, 2007) based on the combined trans-
lation and LM scores.
As explained before, we are using shallow-1 hi-
erarchical grammars (de Gispert et al, 2010) in
our experiments for WMT2010. One very inter-
esting aspect is that HiFST is able to build ex-
act search spaces with this model, i.e. there is no
pruning in search that may lead to spurious under-
generation errors.
2.5 Parameter Optimisation
Minimum error rate training (MERT) (Och, 2003)
under the BLEU score (Papineni et al, 2001) opti-
mises the weights of the following decoder fea-
tures with respect to the newstest2008 develop-
ment set: target LM, number of usages of the
glue rule, word and rule insertion penalties, word
deletion scale factor, source-to-target and target-
to-source translation models, source-to-target and
target-to-source lexical models, and three binary
rule count features inspired by Bender et al (2007)
indicating whether a rule occurs once, twice, or
more than twice in the parallel training data.
2.6 Lattice Rescoring
One of the advantages of HiFST is direct gener-
ation of large translation lattices encoding many
alternative translation hypotheses. These first-pass
lattices are rescored with second-pass higher-order
LMs prior to LMBR.
2.6.1 5-gram LM Lattice Rescoring
We build sentence-specific, zero-cutoff stupid-
backoff (Brants et al, 2007) 5-gram LMs esti-
mated over approximately 6.2 billion words for
English, 2.3 billion words for French, and 1.4 bil-
lion words for Spanish. For the English-French
task, the second-pass LM training data is the same
monolingual data used for the first-pass LMs (as
summarised in Tables 3, 4). The Spanish second-
pass 5-gram LM includes an additional 1.4 billion
words of monolingual data from the Spanish Giga-
Word Second Edition (Mendonca et al, 2009) and
Europarl, which were not included in the first-pass
LM (see Table 5).
2.6.2 LMBR Decoding
Minimum Bayes-risk (MBR) decoding (Kumar
and Byrne, 2004) over the full evidence space
of the 5-gram rescored lattices was applied to
select the translation hypothesis that maximises
the conditional expected gain under the linearised
sentence-level BLEU score (Tromble et al, 2008;
Blackwood and Byrne, 2010). The unigram preci-
sion p and average recall ratio r were set as de-
scribed in Tromble et al (2008) using the new-
stest2008 development set.
2.7 Hypothesis Combination
Linearised lattice minimum Bayes-risk decoding
(Tromble et al, 2008) can also be used as an ef-
fective framework for multiple lattice combination
(de Gispert et al, 2010). For the French-English
language pair, we used LMBR to combine transla-
tion lattices produced by systems trained on alter-
native data sets.
2.8 Post-processing
For both Spanish-English and French-English sys-
tems, the recasing procedure was performed with
the SRILM toolkit. For the Spanish-English sys-
tem, we created models from the GigaWord set
corresponding to each system output language.
157
Task Configuration newstest2008 newstest2009 newstest2010
FR ? EN
HiFST (A) 23.4 26.4 ?
HiFST (B) 24.0 27.3 ?
HiFST (B)CD 24.2 27.6 28.0
+5g+LMBR 24.6 28.4 28.9
HiFST (C) 24.7 28.4 28.5
+5g+LMBR 25.3 29.1 29.3
LMBR (B)CD+(C) 25.6 29.3 29.6
EN ? FR
HiFST (A) 22.5 24.2 ?
HiFST (B) 23.4 24.8 ?
HiFST (B)CD 23.3 24.8 26.7
+5g+LMBR 23.7 25.3 27.1
HiFST (C) 23.6 25.6 27.4
+5g+LMBR 23.9 25.8 27.8
LMBR (B)CD+(C) 24.2 26.1 28.2
Table 6: Translation Results for the French-English (FR-EN) language pair, shown in single-reference
lowercase IBM BLEU. Bold results correspond to submitted systems.
For the French-English system, the English model
was trained using the monolingual News corpus
and the target side of the News-Commentary cor-
pus, whereas the French model was trained using
all available constrained French data.
English, Spanish and French outputs were also
detokenized before submission. In French, words
separated by apostrophes were joined.
3 Results and Discussion
French?English Language Pair
Results are reported in Table 6. We can see
that using more parallel data consistently improves
performance. In the French-to-English direction,
the system HiFST (B) improves over HiFST (A)
by +0.9 BLEU and HiFST (C) improves over
HiFST (B) by +1.1 BLEU on the newstest2009
development set prior to any rescoring. The
same trend can be observed in the English-to-
French direction (+0.6 BLEU and +0.8 BLEU im-
provement). The use of context dependent align-
ment models gives a small improvement in the
French-to-English direction: system (B)CD im-
proves by +0.3 BLEU over system (B) on new-
stest2009. In the English-to-French direction,
there is no improvement nor degradation in per-
formance. 5-gram and LMBR rescoring also give
consistent improvement throughout the datasets.
Finally, combination between the medium-size
system (B)CD and the full-size system (C) gives
further small gains in BLEU over LMBR on each
individual system.
Spanish?English Language Pair
Results are reported in Table 7. We report experi-
mental results on two systems. The HiFST(A) sys-
tem is built on the Europarl + News-Commentary
training set. Systems HiFST (B),(B2) and (B3)
use UN data in different ways. System (B) simply
uses all the data for the standard rule extraction
procedure. System HiFST (B2) includes UN data
to build alignment models and therefore reinforce
alignments obtained from smaller dataset (A), but
extracts rules only from dataset (A). HiFST (B3)
combines hierarchical phrases extracted for sys-
tem (A) with phrases extracted from system (B).
Unfortunately, these three larger data strategies
lead to degradation over using only the smaller
dataset (A). For this reason, our best systems only
use the Euparl + News-Commentary parallel data.
This is surprising given that additional data was
helpful for the French-English task. Solving this
issue is left for future work.
4 Multi-Source Translation Experiments
Multi-source translation (Och and Ney, 2001;
Schroeder et al, 2009) is possible whenever mul-
tiple translations of the source language input sen-
tence are available. The motivation for multi-
source translation is that some of the ambiguity
that must be resolved in translating between one
pair of languages may not be present in a differ-
ent pair. In the following experiments, multiple
LMBR is applied for the first time to the task of
multi-source translation.
158
Task Configuration newstest2008 newstest2009 newstest2010
SP ? EN
HiFST (A) 24.6 26.0 29.1
+5g+LMBR 25.4 27.0 30.5
HiFST (B) 23.7 25.4 ?
HiFST (B2) 24.3 25.7 ?
HiFST (B3) 24.2 25.6 ?
EN ? SP HiFST (A) 23.9 24.5 28.0
+5g+LMBR 24.7 25.5 29.1
Table 7: Translation Results for the Spanish-English (SP-EN) language pair, shown in lowercase IBM
BLEU. Bold results correspond to submitted systems.
Configuration newstest2008 newstest2009 newstest2010
FR?EN HiFST+5g 24.8 28.5 28.8
+LMBR 25.3 29.0 29.2
ES?EN HiFST+5g 25.2 26.8 30.1
+LMBR 25.4 26.9 30.3
FR?EN + ES?EN LMBR 27.2 30.4 32.0
Table 8: Lowercase IBM BLEU for single-system LMBR and multiple LMBR multi-source translation
of French (FR) and Spanish (ES) into English (EN).
Separate second-pass 5-gram rescored lattices
EFR and EES are generated for each test set sen-
tence using the French-to-English and Spanish-to-
English HiFST translation systems. The MBR hy-
pothesis space is formed as the union of these lat-
tices. In a similar manner to MBR decoding over
multiple k-best lists in de Gispert et al (2009),
the path posterior probability of each n-gram u re-
quired for linearised LMBR is computed as a lin-
ear interpolation of the posterior probabilities ac-
cording to each individual lattice so that p(u|E) =
?FR p(u|EFR) + ?ES p(u|EES), where p(u|E) is the
sum of the posterior probabilities of all paths con-
taining the n-gram u. The interpolation weights
?FR + ?ES = 1 are optimised for BLEU score on
the development set newstest2008.
The results of single-system and multi-source
LMBR decoding are shown in Table 8. The opti-
mised interpolation weights were ?FR = 0.55 and
?ES = 0.45. Single-system LMBR gives relatively
small gains on these test sets. Much larger gains
are obtained through multi-source MBR combina-
tion. Compared to the best of the single-system 5-
gram rescored lattices, the BLEU score improves
by +2.0 for newstest2008, +1.9 for newstest2009,
and +1.9 for newstest2010. For scoring with re-
spect to a single reference, these are very large
gains indeed.
5 Summary
We have described the CUED submission to
WMT10 using HiFST, a hierarchical phrase-based
translation system. Results are very competitive in
terms of automatic metric for both English-French
and English-Spanish tasks in both directions. In
the French-English task, we have seen that the UN
and Giga additional parallel data are helpful. It
is surprising that UN data did not help for the
Spanish-English language pair.
Future work includes investigating this issue,
developing detokenization tailored to each output
language and applying context dependent align-
ment models to larger parallel datasets.
Acknowledgments
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7-ICT-2009-4) under grant
agreement number 247762, and was supported in
part by the GALE program of the Defense Ad-
vanced Research Projects Agency, Contract No.
HR0011-06-C-0022. Gonzalo Iglesias was sup-
ported by the Spanish Government research grant
BES-2007-15956 (projects TEC2006-13694-C03-
03 and TEC2009-14094-C04-04).
159
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer
library. In Proceedings of CIAA, pages 11?23.
Oliver Bender, Evgeny Matusov, Stefan Hahn, Sasa
Hasan, Shahram Khadivi, and Hermann Ney. 2007.
The RWTH Arabic-to-English spoken language
translation system. In Proceedings of ASRU, pages
396?401.
Graeme Blackwood and William Byrne. 2010. Ef-
ficient Path Counting Transducers for Minimum
Bayes-Risk Decoding of Statistical Machine Trans-
lation Lattices (to appear). In Proceedings of the
ACL.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings of
EMNLP-ACL, pages 858?867.
Thorsten Brants. 2000. Tnt ? a statistical part-of-
speech tagger. In Proceedings of ANLP, pages 224?
231, April.
Jamie Brunning, Adria` de Gispert, and William Byrne.
2009. Context-dependent alignment models for
statistical machine translation. In Proceedings of
HLT/NAACL, pages 110?118.
Jean-Ce?dric Chappelier and Martin Rajman. 1998. A
generalized CYK algorithm for parsing stochastic
CFG. In Proceedings of TAPD, pages 133?137.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Adria` de Gispert, Sami Virpioja, Mikko Kurimo, and
William Byrne. 2009. Minimum Bayes Risk Com-
bination of Translation Hypotheses from Alterna-
tive Morphological Decompositions. In Proceed-
ings of HLT/NAACL, Companion Volume: Short Pa-
pers, pages 73?76.
Adria` de Gispert, Gonzalo Iglesias, Graeme Black-
wood, Eduardo R. Banga, and William Byrne. 2010.
Hierarchical phrase-based translation with weighted
finite state transducers and shallow-n grammars (to
appear). In Computational Linguistics.
Yonggang Deng and William Byrne. 2005. HMM
Word and Phrase Alignment for Statistical Machine
Translation. In Proceedings of HLT/EMNLP, pages
169?176.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009a. Hierarchical phrase-
based translation with weighted finite state transduc-
ers. In Proceedings of NAACL, pages 433?441.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009b. The HiFST System for
the EuroParl Spanish-to-English Task. In Proceed-
ings of SEPLN, pages 207?214.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of ICASSP, volume 1, pages 181?184.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In Proceedings of HLT-NAACL, pages 169?
176.
Angelo Mendonca, David Graff, and Denise DiPersio.
2009. Spanish Gigaword Second Edition, Linguistic
Data Consortium.
Franz Josef Och and Hermann Ney. 2001. Statisti-
cal multi-source translation. In Machine Translation
Summit 2001, pages 253?258.
Franz J. Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proceedings of
ACL, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of ACL, pages 311?318.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
International Conference on New Methods in Lan-
guage Processing, volume 12. Manchester, UK.
Josh Schroeder, Trevor Cohn, and Philipp Koehn.
2009. Word Lattices for Multi-Source Translation.
In Proceedings of EACL, pages 719?727.
Andreas Stolcke. 2002. SRILM?An Extensible Lan-
guage Modeling Toolkit. In Proceedings of ICSLP,
volume 3, pages 901?904.
Roy W. Tromble, Shankar Kumar, Franz Och, and
Wolfgang Macherey. 2008. Lattice Minimum
Bayes-Risk decoding for statistical machine trans-
lation. In Proceedings of EMNLP, pages 620?629.
160
