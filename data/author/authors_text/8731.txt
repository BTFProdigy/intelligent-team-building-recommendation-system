A Stochast i c  Parser  Based  on a S t ructura l  Word  Pred ic t ion  Mode l  
Shinsuke MORI, Masafumi NISHIMURA, Nobuyasu ITOH, 
Shiho OGINO, Hideo WATANABE 
IBM Research ,  Tokyo  Resem:ch Laboratory ,  IBM Japan ,  Ltd.  
1623-14 Sh imotsurumz~ Ym~atosh i ,  24:2-8502, Japan.  
mor i~t r l . ibm.co . jp  
Abstract 
\]in this paper, we present a stochastic language 
model using dependency. This model considers a 
sentence as a word sequence and predicts each word 
from left to right. The history at each step of pre- 
diction is a sequence of partial parse krees covering 
the preceding words. First ore: model predicts the 
partial parse trees which have a dependency relation 
with the next word among them and then predicts 
the next word fi'om only the trees which have a de- 
pendency relation with the next word. Our: model is 
a generative stochastic model, thus this can be used 
not only as a parser but also as ~ language model 
of a speech recognizer. In our experiment, we pre- 
pared about 1,000 syntactically annotated Japanese 
sentences xtracted fl'om a financial newspaper and 
estimated the parameters of our model. We built a 
parser based on our: model and tested it on approx- 
imately 10O sentences of the same newspaper. The 
accuracy of the dependency relation was 89.9%, the 
highest, accuracy level obtained by Japanese stochas- 
tic parsers. 
1 In t roduct ion  
The stochastic language modeling, imported fl:om 
the speech recognition area, is one of the snccessflfl 
methodologies of natural language processing. In 
fact, all language models for speech recognition are, 
as far" a.s we know, based on an n-gram model and 
most practical part-of-speech (POS) taggers are also 
based on a word or POS n-gram model or its exten- 
sion (Church, 1.988; Cutting et el., 1992; Merialdo, 
1994; l)ennatas and Kokkinakis, 1.995). POS tag- 
ging is the first step of natural language process- 
ing, and stochastic taggers have solved this problem 
with satisfying accuracy for many applications. The 
next step is parsing, or that is to say discovering 
the structure of a given sentence. Recently, many 
parsers based on the stochastic approach ave been 
proposed. Although their reported accuracies are 
high, they are not accurate nough for many appli- 
cations at this stage, and more attempts have to be 
made to improve them fm:ther. 
One of the major applications of a parser is to 
parse the spoken text recognized by a speech rec- 
ognizer. This attempt is clearly aiming at spoken 
language understanding. If we consider how to con> 
bine a parser and a speech recognizer~ it is better if 
the parser is based on a generative stochastic model, 
as required for the language model of a speech rec- 
ognizer. Here, "generative" means that the sum of 
probabilities over all possible sentences is equal to 
or less than 1. If the language model is generative, 
it allows a seamless combination of the parser and 
the speech recognizer. This means that the speech 
recognizer has the stochastic parser as its language 
model and benefits richer information than a nor- 
mal n-gram model. Even though such a Colnbiim- 
tion is not possible in practices , the recognizer out- 
puts N-best sentences with their probabilities, and 
the parser, taking them as input, parses all of them 
and outputs the sentence with its parse tree that 
has the highest probability of all possible combina- 
tions. As a resnlt, a parser based on a generative 
stochastic language model may hell) a speech rec- 
ognizer to select the most syntactically reasonable 
sentence among candidates. Therefore, it is better 
if the language model of a parser is generative. 
In this paper, taking Japanese as the object lan- 
guage, we propose a generative stochastic language 
model and a parser based on it. This model treats a 
sentence as a word sequence and predicts each word 
from left to right. The history at each step of predic- 
tion is a sequence of partial parse trees covering the 
preceding words. To predict a word, our model first 
predicts which of the partial parse trees at this stage 
have dependency relation with the word, and then 
predicts the word fi'om the selected partial parse 
trees. In Japanese each word depends on a subse- 
quent word, that is to say, each dependency relation 
is left to right, it is not necessary to predict the di- 
rection of each dependency relation. So in order to 
extend our model to other languages, the model may 
have to predict the direction of each dependency. We 
built a parser based on this model, whose parame- 
ters are estimated fl:om 1,072 sentences in a finan- 
cial newspaper, and tested it on 1.19 sentences fl:om 
the same newspaper:. The accuracy of the depen- 
558 
dency relation was 89.9%, the highest obt.ained by 
any aapa.nese stochastic parsers. 
2 Stochast i c  Language Mode l  based  
on Dependency  
In this section, we propose a stochastic /angua.ge 
model based on dependency. Unlike most stochas- 
tic language models %r a. parser, our model is the- 
oreticMly based on a hidden Markov model. In our 
model a. sentence is predicted word by word fi'om left 
to right and the state at ea.ch step of prediction is 
basieMly a. sequence of words whose modifiea.nd has 
not appeared yet. According to a psyeholinguistic 
report on la.nguage structure (Yngve, 1960), there is 
an upper limit on the number of the words whose 
inodificaJ~ds ha.ve not appeared yet. This limit is de- 
termined by tim mmfloer of slots in sl~ort-term em- 
ory, 7 :k 2 (Miller, 1956). With this limitation, we 
Call design a pa.rser based on a linite state model. 
2.1 Sentence  Mode l  
'\]'he I)asic idett of our model is that each word would 
be better predicted from the words that have a. de- 
pendency rela.tion with the. word to be predicted 
than from the preceding two words (l.ri-gram model). 
Let us consider the complete structur('~ of the sen- 
tence in /"igure I and a \]tyl)otheti(:al struetm:e after 
the 1)rediction of tile lifth word at the top of Fig- 
ure 2. In this hypothetica.l st ; ructure ,  there are three 
trees: one root-only tree (/q, eomposc'd of wa) a.nd 
two two-node trees (l. conta.ining 'wz and 'w2, and l(, 
containing w4 an(1 'w5). If the last two trees (& and 
le) de4)end on the word we, this word may better 
be predicted from thes(~ two trees. I"rom this I)oint 
of view, our model Ill-st: predicts the t rees  del)cnding 
on the next word and then l)redicts the next word 
from thes(" trees. 
Now, let us make the tbllowing definitions in order 
to explain our model formally. 
? 11~ ~- t tq lv2 . . . ' t t )~  : a, seq l l cnce  o f  words .  \ ] \ ]ere a. 
word is define(l as a, pair consisting of a string of 
alplmbetic hara.cters and a, pa.rt of speech (e.g. 
the/DT).  
? ti = l i l 2 " " lk ,  : a, sequence of parrtiM parse 
trees covering the i-pretix words ('w~ w~. . .  wi). 
? t + trod t~- : subsequences of ti ha.ving a.nd 
not having a. dependeney relation with the next 
word respectively. In .h~p~mese, like many other 
langua.ges, no two dependency relations cross 
each other; thus tl = t~ t +, 
? (t w) : a tree with 'w as its root a.nd t as the 
sequence of all subtrees connected to the root. 
After wi+l has been predicted from the trees 
depending on it (t+), there a.re trees renmin- 
i,,~ (iT) a.d a. ,,ewly prod.eed t,.,'~e ((t?w~+,)); 
th,,s t~+, = t~ . (~,+,,,,~+,). 
1'(t~) 
--.~5 ................................................................................ , ( 
i ~ - -~ I i - -~1  - '~  " 
Wl W 2 W 3 W 4 W 5 W 6 
I }  ........................ + 
~hei  isubj\]il lending}!\[  : j ...................... ~la, ~  ~t,,el _ _ , _ _  
1 z ii - -3  w4 W5 i 1?6 
?;'(~,,01q) 
.... t.} .............................. <1 ~ w,~ > .................................................... 
w, w, }{ w:, w~ w.~ I% { 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  * ' .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  2 
/)(t~;), where t ,  t~ 4-. = = .(tat~;) 
Figure 2: Word prediction from a par t ia l  parse 
? Jhna:r : upper l imit on the munber number of 
words whose moditicands have not appeared 
yet. 
Under these definitions, our stochastic language 
model is defined as follows: 
~'("") = H t'(,,,I,,,,, w~...,,,~_~) 
i=1 
~ H l '(w, l t L , ) I ' ( t L , I t ; -~)  (:l) 
t,,, ET',~ i=1 
where 7;, is all possible bhm.ry trees with n nodes. 
lie,','., the first fi~.ctor, (P(wi l t+ 1)), is ca.lled the word 
prediction model and the second, (P (~'~1 } ti-1 ))' the 
state prediction model. Let us consider Figure 2 
aga.in. At. the top is the state just a.fter the predic- 
tion of the tilth word. The state prediction model 
then predicts the pa.rtial purse trees depending on 
the next word a.mong all partial parse trees, as shown 
in the second figure. Finally, the word prediction 
model predicts the next word Dora the partial parse 
trees depending on it. 
As described above, there may be an upper limit 
on the number of words whose modificands ha.ve not 
yet appeared. To put it in a.nother way, the length 
of the sequence of l)artial parse trees (ti) is limited. 
559 
W I W 2 W 3 W 4 W 5 W 6 W 7 W 8 W 9 Wlo 
Figure 1: Dependency struetm:e of a sentence. 
There%re, if the depth of the partial parse tree is also 
limited, the number of possible states is limited. Un- 
der this constraint, our model can be considered as 
a hidden Markov model. In a hidden Marker model, 
the first factor is called the output probability and 
the second, the transition probability. 
Since we assmne that no two dependency relations 
cross each other, the state prediction model only 
has to predict the mmaber of the trees depending 
on the ,text word. Tln,  s S'(t+_,lt, ._,) = ~':'(ylt~_~) 
where y is the number of trees in the sequence t?_ 1, 
According to the above assumption, the last y par- 
tial parse trees depend on the i-th word. Since the 
nmnber of possible parse trees for a word sequence 
grows exponentially with the number of the words, 
the space of the sequence of partial parse trees is 
huge even if the length of the sequence is limited. 
'?his inevitably causes a data-sparseness problem. 
To avoid this problern, we limited the number of 
levels of nodes used to distinguish trees. In our ex- 
periment, only the root and its children are checked 
to identify a partial parse tree. Hereafter, we repre- 
sent \]JLL to denote this model, in which the lexicon 
of the first level and thai; of the second level are con- 
sidered. Thus, in our experiment each word and the 
number of partial parse trees depending on it are 
predicted by a sequence of partial parse trees that 
take account of the nodes whose depth is two or less. 
It is worth noting that if the dependency structure 
of a sentence is linear - -  that is to say, if each word 
depends on the next word, - -  then our model will 
be equivalent to a word tri-gram model. 
We introduce an interpolation technique (Jelinek 
et al, 1991) into our model ike those used in n-gram 
models. By loosening tree identification regulations, 
we obtain a more general model. For example, if 
we check only the POS of the root and the I?OS 
of its children, we will obtain a model similar to a 
POS tri-gram model (denoted PPs' hereafter). If 
we check the lexicon of the root, but not that of its 
children, the model will be like a word bi-gram model 
(denoted PNL hereafter). As a smoothing method, 
we can interpolate the model PLL, similar to a word 
tri-gram model, with a more general model, PPP or 
PNL. In our experiment, as the following formula 
indicates, we interpolated seven models of different 
generalization levels: 
P(wiltt_l) ~--- ~6PLL(WiI',?_I) q- ,~5\]3pL(Wiit+i_l) 
q-,~4\]3pp (Wtit?_1) @ )t3PNL(W i\[tF_\] ) 
+ s'N,, It +, ) + x, PNN I*?-,) 
+,X0 (2) 
where X in PYx is the check level of the first level 
of the tree (N: none, P: POS, L: lexicon) and Y is 
that of the second level, and lG,c-gr<~m is the uniform 
distribution over the vocabulary W (-\])~U,O--gI'D,I~\](*/)) =
l / IWl). 
The state predictio,, model also in- 
terpola.ted in the salne way. in this case, the possi- 
ble events are y = 1 ,2 , . . . ,  Ym(~x, thus; /~a,0-gr<~m = 
l / y,,,ax .
2.2 Parmneter  Es t imat ion  
Since our model is a hidden Markov model, the pa- 
rameters of a model can l)e estimated from at. row 
corpus by EM algorithm (13amn, 1972). With this 
algorithm, the probability of the row corpus is ex- 
pected to be maxinfized regardless of the structure of 
ea.ch sentence. So the obtained model is not always 
appropriate for a. parser. 
In order to develop a model appropriate for a 
parser, it is better that the parameters are estimated 
from a syntactically annotated corlms by a maxi- 
mmn likelihood estimation (MI,E) (Meriaklo, 1994:) 
as follows: 
1,(wit+) MZ,,  j'(<t,+ 
-- f(< t+ w,:>) 
P(t+lt) M&E f(t+,t) 
f(t) 
where f (x)  represents the frequency of an event x in 
tile training corpus. 
The interpolation coeificients in the formula (2) 
are estimated by the deleted interpolation method 
(aelinek et al, 1991). 
2.3 Se lec t ing  Words  to  be Lex iea l i zed  
Generally speaking, a word-based n-gram model is 
better than a l>OS-based 'n-gram model in terms of 
560 
predictive power; however lexica.lization of some in- 
frequent words may be ha.rmfu\] beta.use it may c;mse 
a. data-sparseness problem. In a. practiea.1 tagger 
(I(upiec, \] 989), only the nlost, frequent \] 00 words a.re 
lexicalized. Also, in a, sta.te-ofthe-a.rt English pa.rser 
(Collins, 1997) only the words tha, t occur more tha,n 
d times in training data. are lexicalized. 
For this reason, our pa.rser selectn the words to be 
lexicalized at the time of lea.rning. In the lexical-  
ized models described above (P/A;, I},L and f~VL), 
only the selected words a.re \]exica.lized. The selec- 
tion criterion is parsing a.ccuracy (see section 4) of 
a. hekl-out corpus, a small part of the learning co l  
pus excluded from l)a, ramcter cstima.tion. Thus only 
the words tliat a.re 1)redicte(1 to improve the parsing 
a.Ccllra.oy of  the test corpilS> or i l l lkl loWll illpll{,> i/3"e 
lexicalized. The algorithm is as follows (see l,'igurc 
a): 
\]. In the initial sta.te a.ll words are in the class of 
their I)OS. 
2. All words are sorted ill descending order of their 
frequency, a.nd the following 1)rocens is executed 
for each word in this order: 
(a.) The word is lexicalizcd provisionally and 
the accura.cy el tile held-oul, corpus is (:;/l- 
cilia.ted. 
(b) Ir a.n illiproven\]ont in observed, the word is 
10xica.lized definitively. 
Tile result of this \]exica.liza.tion algoril.lun is used to 
identil~y a. \])a.rtia.l l)arse tree. That is to say, ()ill 3, Icx- 
iealized wordn are distinguished in lexicalized mod- 
els. It" IlO wordn Were nelcctxxl I:o be lexica/ized, {;hell 
/ 'NL  = \])N1 ' a,ii(I \ ])LL - -  \[)t"L = 191'1'. I t  is wort \ ] l  
nol ; ing that  i f  we t ry  to .ioi,, a word  w i th  al lo/,/ ler 
wet( l ,  then this  a,lgOlJithnl w i l l  be a, l lo r lna\ ]  top-down 
c, lustcring a.igorithnl. 
2.4 Unknown Word  Mode l  
To enable olir stocllastic la.nguage lnodel to handle 
unknowil words> we added a.li ui/knowii word model 
based Oil a cha.ra,cter \])i-giPa,nl nio(M, lr the next  
word is not in the vocabula.ry, the n\]o(lel predicts 
its POS a.nd the llllklloWll word model predicts the 
string of the word as folkm, s: 
re.q- \] 
s'(,wlS'OS) = 1-\[ ) 
i=1 
where 'w = x tx2 . . .xm,  xcl == aSm+l = \]~'\]'. 
1}'1", a special character corresponding to a word 
l)oundary, in introduced so tha.t the ntlilt of the l)rob- 
ability over all sl, r i l lgs is eqlla,\] to 71. 
In the l)ara.lneter cstima.tion described a.1)ove, a. 
learning corpus is divided into k parts. In our ex- 
i)erirnent, the vocabulary is composed of the wordn 
: - (~  ~ ,- '" l  .............................. -10& ....................................... 10S2 L_I @ 
. . .................................. . ,,,,,. 
i 
* / /  ..................... I 'OS ~ ........... ~ ............... I 'OS 2 
: - ~  ............... I 'OS I ...................... I 'OS 2 
lexicalize 
yes 
I10 
yes 
yes 
Figure 3: A conceptual figure of the le, xicaliza.tion 
algorit.hm. 
occurring in more than one l)artia\] corlmn and the 
other words are used for 1)arameter entima, tion of 
the unknown word model. The l)i-gram l)robal)ility 
of the unknown word model of a. I)OS in estimated 
\['ronl the words among them and belonging to the 
POS as follows: 
1 !l, o s (.~, i la: i - J ) M)~V .fP o s (*  i, * i -  ~ ) 
fPos (,,;~-,) 
The character I)i%ram model is also interl)ola.ted 
Wi/ , \ ] I  a l l i l i-~r~iill i l lode\]  and  a Zel'O-~l'aill l l lodc\] .  
The interl)olation coellicients are estinmi.d by the 
deleted interpolation method (,lelinek el. al., 1991). 
3 Syntact i c  Analysis 
(,el cJ dl.y, a. l)a.rscr may I)c considered an a module 
that recdvcs a. sequence of words annotated with a, 
I'()S and oul.putn its structm'e. Our parner, which 
includes a stochastic mflmown word model, however, 
is a.I)le to a.cc.el)t a cha.ra.ctc'r sequence as an input and 
execute segmenta.tion, POS tagging, and syntactic 
analysis nimultaneously I . In this section, wc exphfin 
our pa.rser, which is based on the language modal 
described in the preceding section. 
3.1 S to ( 'has t ie  Syntac | , i c  Ana lyzer  
A syntactic analyzer, bancd on a. stochastic language 
model, ca.lculatc's the pa.rse tree (see Figure 1) with 
the highest probabil ity for a given scquencc of char- 
acters x according to the following tbrmula.: 
:/' = , , , 'g i i i , ixP(Tl . ,  0 
"H~ (fl') :=;/~ 
1 There is no space \])etweell words ill ,Japo.llese 
561 
= argmax P(TIx)P(~ ) 
w(:c)=m 
= argn,~,xP(mlT)I~(r) ('." 13ayes' forn~ula) 
w(T)=.'e 
= argnmxP(T)  ('." P(mlT)= 1), 
W(T)=m 
where w(T) represents the concatenation of the 
word string in the syntactic trek T. P(T) in the last 
line is a stochastic language model, in our parser, 
it is the probability of a parse tree T defined by the 
stochastic dependency model including the unknown 
word model described in section 2. 
p(T) = I I  Its_,), (a) 
i=1 
where wlw2". "wn = w(T). 
3.2 Solut ion Search Algor i thm 
As shown in formula (3), our parser is based on a hid- 
den Markov model. It follows that Viterbi algorithm 
is applicable to search the best solution. Viterbi al- 
gorithm is capable of calculating the best solution in 
O(n) time, where n is the number of input charac- 
ters. 
The parser repeats a state tra.nsition, reading 
characters of the input sentence from left to right. In 
order that the structure of the input sentence may be 
a tree, the number of trees of the final state tn must 
be 1 and no more. Among the states that satisfy this 
condition, the parser selects the state with the high- 
est probability. Since our language model uses only 
the root and its children of a partial parse tree to dis- 
tinguish states, the last state does not have enough 
information to construct he parse tree. The parser 
can, however, calculate the parse tree fi'om the se- 
quence of states, or both the word sequence and the 
sequence of y, the number of trees that depend on 
the next word. Thus it memorizes these values at 
each step of prediction. After the most probable 
last state has been selected, the parser constructs 
the parse tree by reading these sequences fi:om top 
to bottom. 
4 Eva luat ion  
We developed a POS-based model and its lexical- 
ized version explained in section 2 to evaluate their 
predictive power, and implemented parsers based on 
them that calculate the most probable dependency 
tree fi'om a given character sequence, using the so- 
lution search algorithm explained in section 3 to ob- 
serve their accuracy. In this section, we present and 
discuss the experimental results. 
4.1 Condit ions on the Exper iments 
The corpus used in our experiments consists of ar- 
ticles extracted from a financial newspaper (Nihon 
Table 1: Corpus. 
learning 
test 
#sentences ~words 
1,072 30,292 
119 3,268 
#chars 
46,212 
4,909 
Keizai ,%inbun). Each sentence in tile articles is 
segmented into words and its dependency structure 
is annotated by linguists using an editor specially 
designed for this task at our site. The corpus was 
divided into ten parts; the parameters of the model 
were estimated fi:om nine of them and the model 
was tested on the rest (see Table 1). A small part 
of each leaning corpus is withheld from parameter 
estimation and used to select the words to be lex- 
icalized. After checking the learning corpus, the 
maximum number of partial parse trees is set to 10 
To evaluate the predictive power of our model, we 
calculated their cross entropy on the test corpns. In 
this process, the annotated tree in the test corpus is 
used as the structure of the sentences. Therefore the 
probability of each sentence in the test corpus is not 
the summation over all its possible derivations. To 
compare the POS-based model and the \]exicalized 
model, we constructed these models using the same 
learning corpus and calcnlated their cross entropy 
on the same test corpus. The POS-based model and 
the }exicalized model have the same mfl~nown word 
model, thus its contribution to the cross entropy is 
constant. 
We implemented a parser based on the depen- 
dency models. Since our models, inchsding a 
character-l)ased unknown word model, can return 
the best parse tree with its probability for any in- 
put, we can build a parser that receives a character 
sequence as input. It is not easy to evaluate, how- 
ever, because errors may occur in segmentation of
the sequence into words and in estimation of their 
POSs. For this reason, in the tbllowing description, 
we assume a word sequence as the input. 
The criterion for a parser is the accuracy of its out- 
put dependency relations. This criterion is widely 
used to evahmte Japanese dependency parsers. The 
accuracy is the ratio of the nnmber of the words a.n- 
notated with the same dependency to the numl)er of 
the words as in the corpus: 
accuracy 
=#=words ependiug on tilt correct word 
~words 
Tile last word and the second-to-last word of" a sen- 
tence are excluded, because there is no ambiguity. 
The last word has no word to depend on and the 
second-todast word depends always on the last word. 
562 
Table 2: Czoss entorpy alld acellraey of each model. 
language model cross enl\[,lTopy a, CCUFaicy 
selectively lexicalized 6.927 - ~ -  
completely lexicalized 6.651 87.1% 
POS-based 7.000 87.5% 
linear structure* -- 78.7% 
* F, adl word del)ends on l;he next word. 
4.2 Ewduat ion  
Table 2 shows the cross entropy and parsing accu- 
racy Of the baseline, where all words depend on the 
next word, the POS-based dependency model and 
two lexicalized dependency models. In the selec- 
tively lexicalized model, words to be lexicalized are 
selected by the aJgo,:ithm described in section 2. In 
the completely lexicalized model, all words arc lcxi- 
calized. This result attests experimentally that the 
pa.rser based on the selectively lexicalized model is 
the best parser. As for predictive power, however, 
the completely lexica.lized model has the lowest cross 
e~/tropy. Thus this model is estimated to be the best 
language model for speech recognition. Although 
there is no direct relation between cross entropy of 
l;he language model and error ra.te of a speech rec- 
ognizer, if we consider a spoken la.nguage parser, it 
ma.y be better to select the words to be lexicalized 
using other criterion. 
We calculated the cross entropy and the parsing 
accuracy (if' the model whose parameters arc esti- 
mated fi'om ;I/d, 1/16, and 1/64 of the learning 
corpus. The relation between the learning corpus 
size and the cross entrol)y or the l)arsing a.ccm:acy is 
shown in Figure d. The cross ent ropy  has a stronger 
tendency to decrease as the corpus size increases. 
As for accuracy, there is aJso a tendency for parsers 
to become more accurate as the size of the learning 
increases. The size of the cor/)us we h~we all this 
stage is not at all large, ltowever, its accuracy is at 
the top level of Japanese parsers, which nse 50,000- 
1.90,000 sentences. Therefore, we conclude that our 
approach is quite promising. 
5 Re la ted  Works  
lIistorica.lly, structures of natural languages have 
been described by a context-free grammar a.nd all\]- 
biguities have been resolved by parsers based on a 
context-free grammar (Fujisaki et al, 1989). In re- 
eenl, years, some attempts have been made in the 
area of parsing by a tinite state model (Otlazer, 1999) 
etc. Our parser is also based on a finite state model. 
Unlike these models, we focused on reports on a 
limit on language structure caused by the capacity 
our memory (Yngve, 1960) (Miller, 19561. Thus our 
20- -  
16 
q 
12 
0 ,-~ 8 
accuracy - -4  - _--4 
84.0% 86.2% 88.1% 89.9% 
% 
t'ross Clltropy \+\  "\  
I00 % 
8O 
60 N' 
40 
20 
0100 101 102 103 104 105 0 
#characters in learning corpus 
I,'igure d: Relation between cross entropy a.nd pars- 
i~lg accuracy .  
model is psycholinguistically more al)propriate. 
Recently, in the area of parsers based oll a. stochas- 
tic context-fi:ee grammar (SCFG), some researchers 
have pointed out the importance of t.he lexicon 
and proposed lexiealized models (Charniak, 1997; 
Collins, 1997). 111 these papers, they reported sig- 
nificant improvement of parsing accuracy. Taking 
these reports into account, we introduced a method 
of pa.rlJal lexicalization and reported significant im-- 
provement of parsing accuracy. Our lexicalization 
method is also a.pplicable to a. SCFG-based parser 
and improves its parsing accuracy. 
The model we present in this pal)er is a genera- 
tire stochastic language model. Chelba and aelinek 
119981 presented a similar model. In their model, 
each word is predicted t?om two right-most head 
words regardless of dependency rela.tion between 
these head words and the word. Eisner (\[996) also 
presented a. st;ochastie structura.1 language model, in 
which ea.ch word is predicted t?om its head word 
and the nearest one. This model is very similar to 
the parser presented by Collins 11.9961. The great- 
est difference between our model and these models 
is in that our model predicts the next word from the 
head words, or partial parse trees, depending on it. 
Clearly, it is not always two right-most head words 
that have dependency relation with the next word. 
It. follows that our model is linguistically more ap- 
propirate. 
There have been some attempts at stochastic 
Japal, ese parser (llaruno et al, 1998) (l"ujio and 
Matsmnoto, 19981 (Mori and Naga.o, 1.998). These 
Japanese parsers are based on a unit called bunsetsu, 
a sequence of one or more content words followed by 
zero or more traction words. The parsers take a 
sequence of units and outputs dependency relations 
between them. Unlike these parsers, our model de- 
563 
scribes dependencies between words; thus our model 
can easily be extended to other languages. As tbr 
the accuracy, although a direct comparison is not 
easy between our parser (89.9%; 1.,072 sentences) 
and these parsers (82% - 85%; 50,000 - 190,000 sen- 
tenees) because of the difference of the units and 
the corpus, our parser is one of the state-of-the-art 
parsers \[br Japanese language. It should be noted 
that ore: model describes relations among three or 
more units (case frame, consecutive dependency re- 
lations, etc.); thus our model benefits a greater deal 
from increase ot.' corpus size. 
6 Conc lus ion  
In this paper we have presented a stochastic lan- 
guage model based on dependency structure. This 
model treats a sentence as a word sequence and pre- 
dicts each word from left to right. "The history at 
each step of prediction is a sequence of partial parse 
trees covering the preceding words. To predict a 
word, ore: model first selects the partial parse trees 
that have a dependency relation with the word, and 
then predicts the next word from the selected partial 
parse trees. We also presented an algorithm %r lexi- 
calization. We lmilt parsers based on the POS-based 
model and its lexicalized version, whose parameters 
are estimated from 1,072 sentences of a financial 
newspaper. We tested the parsers on 119 sentences 
Dom the same newspaper, which we.re excluded fl:om 
the learning. The accuracy of the dependency rela- 
tion of the lexicalized parser was 89.9%, the highest 
obtained by any Japanese stochastic parser. 
References  
L. E. Baum. 1.972. An inequality and associated 
maximization technique in statistical estimation 
for probabilistie functions of Ma.rkov process. In- 
equalities, 3:1-8. 
Eugene Charniak. 1997. Statistical parsing with a 
context-fl:ee grammar and word statistics. In Pro- 
ceedings of the l/ith National ConfeTvnce on Arti- 
ficial Intclligence, pages 598-603. 
Ciprian Chelba and Frederic .l elinek. 1998. F, xploit- 
ing syntactic structure for language modeling. In 
Proceedings of the I Tth hdernational Conference 
on Computational Linguistics, pages 225-231. 
Kenneth Ward Church. 1988. A stochastic pa.rts 
program and noun phrase parser for unrestricted 
text. In Proceedings of the 3eeond Conference on 
Applied Natural Language Processing, pages 136- 
143. 
Michael John Collins, 1996. A new statistical parser 
based on bigram lexical dependencies. In Proceed- 
ings of the 34th Annual Meeting of the Association 
for Computational Linguistics, pages 184-191. 
Michael Collins. 1.997. Three genera.tire, lexiea.lised 
models for statistical parsing. In Proceedings of 
th(~ 35th Annual Meeting of the Association for 
Computational Linguistics, pages 16 -23. 
l)oug Cutting, Julian Kupiec, .Jan 1)edersen, and 
Penelope Sibun. 1992. A practical part-of-speech 
tagger. In Proceedings of the "lldrd Conference on 
Applied Natural Language Processing, pages 133 
l.d0. 
Evangelos Dermatas and George Kokkinakis. 1995. 
Automatic stochastic tagging of ha.tin:el language 
texts. Computational Linguistics, 21 (2):137-103. 
Jason M. Eisner. 1.996. Three new probabilistic 
models for dependency parsing: An exploration. 
In Proceedings of the 16th lnlernational Co~@r- 
ence on Computational Linguistics , pages 340 
345. 
Masakazu Fujio and Yuji Matsumoto. 1998. 
Japanese dependency structure analysis based on 
lexicalized statistics. In Proceedings (of the Third 
Conference on Empirical Methods in Natural Lan- 
guage Processing, pages 87-96. 
T. Fujisaki, F. ,\]elinek, .1. Cocke, E. Black, and 
T. Nishino. 1.989. A probabilistic parsing method 
for sentence disambiguation. In Proceedings of the 
International .Parsing Workshop. 
Masahiko Itarmm, Satoshi Shirai, and Yoshifnmi 
Ooyama. 1998, Using decision trees to construct a
practical parser. In Proceedings of the ITlh Inter- 
national Confer(race on Compul, alior~al Linguis- 
tics~ pages 505-511. 
Fredelick .\]elinek, 11,obert L. Mercer, and Salinr 
\[{oukos. 1991. Principles of lexica.1 language 
modeling for speech recognition. In Advances in 
,5'peeeh ,5'ignal Processing, chapter 21, pages 651- 
699. l)ekker. 
Julian Knpic'c, 1989. Augmenting a hidden Markov 
model 'or phrase-dependent word t.agging. In .Pro- 
ceedings of the DAI{I)A ,5'peeeh and Natural Lan- 
guage Workshop, pages 92- 08. 
Bernard Merialdo. 1994. '15~.gging English text with 
a probabilistie model. Computational Linguislics, 
~0(~):155 -171. 
George A. Miller. 1956. The magical number seven, 
plus or minus two: Some limits on our capacity 
for processing information. The Psychological l~e- 
view, 63:81-97. 
Shinsuke Mori and Makoto Nagao. 1998. A stochas- 
tic language model using dependency and its im- 
provement by word clustering. In Proceedings of 
the ITth International Co@'renee on Computa- 
tional Linguistics, pages 898-90~t. 
Kernel Ollazer. 1.999. l)ependency parsing with an 
extended finite state approach. In Proceedings of 
the 37t, h Annual Mecti~,g of the Association for 
Computational Linguistics, pages 254-260. 
Victor H. Yngve. 11960. A model and a hypothesis 
for language structure. The American Philosoph- 
ical Society, 104(5):444-466. 
564 
Finding Structural Correspondences from Bilingual Parsed Corpus 
for Corpus-b sed Translation 
Hideo Watanabe*, Sadao Kurohashi** and Eiji Aramaki** 
* IBM Researdt, Tokyo Research Laboratory 
1623-14 Shimotsuruma, Yamato, 
Kanagawa 242-8502, Japan 
watanabe@trl.ibm.co.jp 
** Graduate School of Inforlnatics, Kyoto University 
Yoshida-homnachi, Sakyo, 
Kyoto 606-8501, .JaI)an 
kuro@i.kyoto-u.ac.jp, 
aramaki@pine.kuee.kyoto-u.ac.jp 
Abstract 
In this paper, we describe a system and meth- 
ods for finding structural correspondences from the 
paired dependency structures of a source sentence 
and its translation in a target language. The sys- 
tem we have developed finds word correspondences 
first, then finds phrasal correspon(tences based on 
word correspondences. We have also developed a
GUI system with which a user can check and cor- 
rect tile correspondences retrieved by the system. 
These structural correspondences will be used as 
raw translation I)atterns in a corpus-based transla- 
tion system. 
1 Introduction 
So far, a number of methodologies and systelns 
for machine trauslation using large corpora exist. 
They include example-based at)proaches \[7, 8, 9, 
12\], pattern-based approaches \[10, 11, 14\], and sta- 
tistical approaches. For instance, example-based 
approaches use a large set of translation patterns 
each of which is a pair of parsed structures of a 
source-language fragment and its target-language 
translation fragment. Figure 1 shows an exanl- 
ple of translation by an example-based method, ill 
which translation patterns (pl) and (p2) are se- 
lected as similar to a (left hand) Japanese depen- 
dency structure, and an (right hand) English de- 
pendency structure is constructed by merging the 
target parts of these translation patterns 1.
In this kind of system, it is very important o 
collect a large set of translatiou patterns easily and 
efficiently. Previous systems, however, collect such 
translation patterns mostly manually. Therefore, 
they have problems in terms of the development 
cost. 
1Words in parenthesis at the nodes of the Japanese de- 
pendency structure are representative English translations, 
and are for explanation. 
This paper tries to provide solutions for this is- 
sue by proposing methods for finding structural 
correspondences of parsed trees of a translation 
pair. These structural correspondences are used as 
bases of translation patterns in corpus-based ap- 
proaches. 
Figure 2 shows an example of extracting struc- 
tural correspondences. In this figure, tile left tree 
is a Japanese dependency tree, the right tree is a 
dependency tree of its English translation, dotted 
arrows represent word correspondence, and a pair 
of boxes connected by a solid line represent phrasal 
correspondence. We would like to extract these 
 ,ook \ 
"4" - . .~  ...," ~.. a movie ~ 
Figure 2: An Example of Finding Structural Cor- 
respoudences 
word and phrasal correspondeuces automatically. 
In what follows, we will describe details of proce- 
dures for finding these structural correspondences. 
2 Finding Structural Correspondences 
This sectiou describes methods for finding struc- 
tural correspondences for a paired parsed trees. 
2.1 Data  St ructure  
Before going into the details of finding structural 
correspondences, we describe the data format of a 
906 
verb - -  
9a 
noun- -noun , 
n0mu--drink 
l l 0un  - -  n0un  
verb ! 
i 
,, 
t .  
"', dl lk 
 he  .__medicine 
l 
1 ~ # 
\[.-- 
I 
(p2) 
Figure 1: Translation Example by Examt)le-based ~li'anslation 
dependency structure. A det)endeney stru('ture as 
used in this pat)er is a tree consisting of nodes and 
links (or m:cs), wh('.re a node represents a content 
word, while a link rel)resents a fllnctional word or 
a relation between content words. For instance, as 
shown in Figure 2, a t)reposition "at;" is represented 
as a l ink  in l~,nglish. 
2.2 F ind ing  Word  Cor respondences  
The  tirst task for finding stru('tm:al corresI)On- 
den(:c's is to lind word (:orro, sl)ondenccs t)et;ween (;he 
nodes of a sour(:e parsed tree and the nodes of a 
t;wget parsed tree. 
Word correspondences are tkmn(1 by eonsull;ing a
source-to-target translation dictionary. Most words 
can find a unique 1;ranslation candidate in a target 
tree, but there are cases such that there are many 
translation candidates in a target parsed tree for 
a source word. Theretbre, the main task of tind- 
ing word correspondences is to determine the most 
plausible l;ranslation word mnong can(tidates. We 
call a pair of a source word and its translation 
candidate word in a target tree a word correspon- 
dence candidate denoted by WC(s,/,), where s is a 
source word and t is a target word. If 17\[TC(s,/,) ix a 
word correspondence andida.te such that there is 
rto other WC originating h'om s, then it is called 
WA word correspondence. 
The basic idea to select the most plausil)le word 
correspondence candidate ix to select a candidate 
which is near to another word correspondence whose 
source is also near to a sour(:e word in question. 
Suppose a source word s has multiple candidate 
translation target words t~ (i = 1,...,7~,), that  is, 
there are multiple 17FCs originating h'om .s'. We, 
denote these multiple word corresl)ondence candi- 
dates by WC(s, tl). For each I'VC of s, this proce- 
dure finds the neighbor WA correspondence whose 
distance to WC ix below a threshold. The distance 
between WC(sl,/,~) and WA(s.2,/,2) is defined as 
the distance between sl and .s2 plus the distmme 
between s2 and 1,2 where a distance between two 
nodes is defined as the number of nodes in the t)ath 
whoso, ends are the two nodes. Among I~VCs of 
.s for which neighbor H/A ix tound, the one with 
the smallest (listan(:(~ is chosen as the word corre- 
Sl)ondenee of s, and I/VCs whMl are not chosen 
are invalidated (or deleted). We call a word corre- 
spondence found t)y this procedure WX.  We use 
3 as t;he distance threshold of the above procedure 
currently. This procedure ix applied to all source 
nodes which have multii)le WCs. Figure 3 shows 
an example of WX word correspondence. In this 
examt)le, since the Japanese word "ki" has two En- 
glish l;ranslation word candidates "time" and "pe- 
riod," there are two WCs  (~7C 1 and WC2). The 
direct parent node "ymlryo" of "ki" has a WA cor- 
respondence (I/VA1) to "concern," and the direct 
child node "ikou" has also a WA correspondc'nee 
(WA2) to "transition." In this ease, since the dis- 
tance between I'VC2 and WA2 is smaller than the 
distan(:e between I.VC1 and WA1, I'VC~ in clmnged 
to a 1/l/X, and I~ITC1 is adandoned. 
In addition to WX correspondences, weconsider 
a special case such that given a word correspon- 
dence l'lZ(s,/,), if s has only one child node which ix 
907 
. . ,  ........ be ....... ",,, 
.. -~%omp t 
. . .WAI at / /  concern 
." time 
yuuryo ,.." 
(concern) -" 
ni ..,Wc1 same 
.... accompany 
ki..,*\[__ 
(time) .............. W_..G2 ............ 
'"-- period 
ikou ......... VVA2 of 
transition (transition) 
Figure 3: An Exmnt)le of WX Word Correst)on- 
(lence 
a leaf and t has also only one child node which ix a 
leaf, th(;n we COllStrllet a lleW word correspondence 
called 1US from these two leaf nodes. This WE 
procedure is al)plied to all word correspondences. 
Note tlmt this word correst)ondence is not to se.le, ct 
one of candidates, rather it is a new finding of word 
corre, spondence by utilizing a special structm:e. For 
instance, in Figure 3, if there is a word eorrespol> 
dence 1)etween "ki" and "period" and there is no 
word correst)ondence between "ikou" and "transi- 
tion," then I<V,g(iko'u~ transition) will be found 1)3' 
this 1)roeedure. 
These WX and WS t)rocedures are continuously 
al)plied until no new word correspondences arc t'(mnd. 
Aft;er al)l)lying the above WX and I'VS pro(:e- 
dures, there are some target words t such that t is a 
destination of a l,l/C(.s ", t) and there ix no other 1,176 , 
whose destination ix t:. In this case, the lUG(s,t)  
correspondence andidate is chosen as a valid word 
correspondence b tween s and/,,  and it; is called a 
HzZ word eorrest)ondence. 
We call a source node or a target node of a word 
correspondence an anchor node in what tbllows. 
The above t)rocedures for finding word corre- 
sI)ondences are summarized as follows: 
Find WCs by consulting translation dictionary; 
Find WAs; 
whi le  (true) { 
find WXs; 
find WSs; 
i f  no new word corresp, is found, then  break; 
} 
find WZs;  
2.3 F ind ing  Phrasa l  Co l ' res l )ondences  
The next step is to tind phrasal correspondences 
based on word eorl'eSl)ondences t'(mnd t) 3, 1)roce.- 
dures described in tim previous section. What  we 
would like to retrieve here, is a set of phrasal cor- 
respondences which (:overs all elements of a paired 
dependency trees. 
In what follows, we (:all a portion of a tree which 
consists of nodes in a 1)att~ from a node ?t I (;o all- 
oth(;r node nu which is a descen(lanl; of n:l a lin-. 
ear tree denoted by LT(v,1, n~), and we denote a 
minimal sul)tree including st)coiffed nodes hi,  ..., n.~, 
l)y T (n l , . . . ,n , ) .  For instan(:(,~ in the English tree 
structure (the right tree) in Figure 4, LT(tcch, nology ,
science) is a rectangular area covering %eclmol- 
? tg "e e ~ ogy," and SOl ,no ,, anti .T(J'acl;or, cou'ntrjl ) is a 
1)olygonal area covering "factor,""atDcl,, . . . .  t)ol- 
icy," and "country." 
The tirst step is to find a 1)air of word correst)on- 
dences W, (.~'~, t ) and ~4q(.,.~, t ~) such that .,, a.,t  
s2 constructs a linear tree LT(si ,  s2) and there is no 
anchor node in th(' 1)al;h from s~ to s2 other than .s'~ 
and .s2, where 1UI and H~ denote any tyi)e of word 
('orrest)on(lences 2 and we assmne there is a word 
corresI)ondence t)etwee, n roots of source and (;arget 
trees by defmflt. We construct a t)hrasal correspon- 
dence fi'om source nodes in LT(s , , s2)  and target 
l/o(les itl r \ ] ' ( t : l , / '2 ) ,  (l()llote(t by \];'(l~,~F'(.q'l, .";2), 5\].n(tl, t2)). 
For illstall('e~ ill F ig l l re 41~ \]"11~ \]~12~ 1)'2~ 1)3 and 
\])4 tu.'e source portions of phrasal et)rrespondences 
found in this step. 
The next stel) checks, for ea(:h 1', if all anchor 
l lo(les of wor(1 eorres1)Oll(leile(?s wllose SOUlT(;e o1 ~;al- 
get node is included in P are al,eo included in P. 
If a t)hrasal correst)ondenee satisiies this condition, 
then it is called closed, otherwise it ix called open. 
Further, nodes which are not included in the I ) in 
question are called open nodes. If a l ) ix ot)en, then 
it ix merged with other 1)hrasal correspondences 
having ol)en nodes of P so that the merged 1)hrasal 
correspondence b comes (-losed. 
Next, each P~,, is checked if there is another l)q 
which shares any nodes ottmr than anchor nodes 
with P.,,. If this is the case, these P:., and 1~ are 
lnerged into one phrasal correspondence. In Figure 
4, t)hrasal correspondences i 11 and P12 are merged 
into P1, since their source I)ortions LT (haikei, koku) 
and LT (haikci, seisaku) share "doukou" which is 
not an anchor node. 
Finally, any path whose nodes other than the 
root are not included in any 1)s but the root node 
ix included in a 1 ) is searched for. This procedure 
2Since WC is not a word correspondence (it is a candi- 
date, of word corresi)ondence), it is llOi; conside, red here. 
908 
is apl)lied I;o 1)oth source a.nd (;arget trees. A im.th 
found 1)y this 1)ro(:(xlur(~ is called an open pal, h,, m~(t 
its root no(le is called a pivot. If such an Ol)en path 
is found, it is t)rocessed as follows: l, br each 1)ivot 
node, (a) if the t)ivot is not an mmhor nod(;, then 
open lmths originating fl:om the pivot is merged 
into a 1 ) having I;he pivot, (b) if the pivot is an 
~LIlChOf l lo(lo~ {;hOll 3_ llOW t)hl'~lS~L1 c()rFos1)oII(|(~IlC( ~, iS 
created from Ol)(m 1)ai;hs originating from the m> 
thor nodes of the word (:orrcsl)on(l(:ncc. 
In Figure 4, w(: get tinally four phrasal (:orr(:- 
Sl)on(lences l~, f~, l~, an(l l~t. 
! haikei.!,,: I - ................... { -~ factor',,,l ', 
i /~0 :: a ect " 
, ,  ',,(tre, nd) i f \  ~ ~-"  l i ;  
k . '/ ;~oy, v __: 
~, koku ( seisak~'~{t - - - - .~-  - -~  
~' (C0UrlttV)_l , (p0%,~ :t' .. technology .lrltly 
~< :::>i--- -::-:~ 
= 7 :: TLI io. ; I 
(major) ~,_/_ 
- X-~ / 
' giutu"\]\] 
(technolo~ly)l' .? science 
kagaku " 
(scie, nce) . -  
P4 
/ 
t 
t /  ff 
i 
Figm:e d: An l~;xaml)le of Finding Phrasal  Corr(> 
S\])Olld(~,IIC(',S 
The above 1)ro(:edures fl)r finding l)hrasal (:orr(> 
Ht)oIIdoIICOS ~-LF(~ SlllIllIl?~riz(Kl gtS fo l lows :  
Find initial Ps; 
Mea'ge a.n Ol)Cn 1>~ with other i ' s  having 
open nodes of 1}; 
Create new Ps 1)y merging \])s 
which have more tlmn 2 (:ommon nodes; 
Find ot)en path, alld 
if the t)ivot is ml mmhor, | ;hen 
merge the path to P having the anchor, 
o therwise  create new l ) by merging 
all open t)ai,hs having l;lm pivot; 
3 Exper iments  
3.1 C, o r lms  and  D ic t ionary  
We used (l()(;lllil(~'ll|;s t'rolil White Papers on S(:i- 
en(-e and Technology (1.994 to \ ]996) pul)lished by 
the S(:ience mid Technology Agency (STA) of tim 
.\]al)mmse govcrlim(~nl;. STA lmblished th(;se White 
PaI)ers in both Jat)mmse and English. The Com- 
mmfications l{esea.rch Laboratory of" the Ministry 
of Posts and Telecommuni(:a.tion of the .\]al)mmse 
goverlmmnt supl)lied us with the l)ilingual corpus 
wtfich is already roughly aligned. We made a bilin- 
gual cortms consisting of pa.rs(;d dependency struc- 
tures by using the KNP\[2\] .\]al)mmso, 1)arser ((l(wel- 
Ol)ed by Kyoto (hfive)sity) for .Jal)anes(~ sentences 
and the ESG\[5\] English 1)arser (developed by IBM 
Watson i{e, sear(:h Center) for English s(~nl;(!nces. 
We mad(} al)oul; 500 senl;(m(:e l)airs, each of whi(:h 
11~1,'4 ;I, OIlC-I;O-OII(', 80,11|;(',11(;0 (-orresl)onden(:(~,, fl'OI\[l (,\]lO 
raw (t~tta of l;he, White l)al)crs, mid s(',l(;(;i;(xl rm> 
domly aboul; 130 s('aH;en(:c pairs for (',Xl)(Mm(;nts. 
ilow(wer, since a 1)nrser does not always \])ro(hwe 
(;orl'c(;\[; 1);~l"s(t t;re(}s~ wo (~x(:lude(1 some, ~(~ii|;(Hic(~ p;Lil's 
wlfich have severe 1)arse errors, and tinally got i\[15 
S(~,II\[;OIlC(; pairs as a, to, st s(%. 
As a trm~slation wor(1 dictionary/)etw(',(m .l at)ml(',s(; 
and English, we, tirsl; used ,l-to-l~; trmlslati()n (li(:- 
l, ionary which has mot(,' t lmn 100,000 (,ifl;l'i(;~, but 
we, fi)un(l l;}l~/{; l ller(? are som(~ word ('orr(~sl)Oll(l(~,llt;(~s 
not (:()v(ued in this di(:ti()nary. Tlmref()rG we merged 
(retries fi:om \]';-t;o-.I translatioll dictionary in order 
to get; much broad (:ov(wag(,'. The l;oDd nulnl)(}r ()f 
entries a.re now more I;ha.n \[50,000. 
3.2 Exper inmnta l  Resu l ts  
Td)le i shows l;he result of (~Xl)c, rimeni; fl)r tind- 
ing word correspond(nm(~s. A row with ALL in th(', 
l:yl)e cohmm shows Llle total  ~CClll'~lcy of WOI'(1 cor- 
r(Lqpolld('31c(~s and ol;\]l{~r rows sh()\v Llle .~iCClll'ktcy of 
each t, yt)e. It is clear that WA (:orr(~sl)Olld(~ll(;(',s 
have a very high a('cura(:y. Other word (:orresl)On-- 
do, nc(,,s also ha.ve a roJatively high ac(:ura(:y. 
Table 2 shows tim remflt of exl)erimenl,s for find~ 
ing 1)hrasal correspondences. The row with ALL in 
I;he l;yt)c cohlmn shows l;he l;ol;al accuracy of phrasal 
(:ol'r(~sl)ondo, n(:(~s found by the 1)rol)osed 1)rocedure. 
This ac(:macy level is not I)romising and it is not; 
useful for later 1)ro(:e, sses since it needs human (:he(:k- 
ing ml(l (:orrec?ion. Therefore, we sul)categoriz(~ 
each phrasal corl'eSpond(m('es, and check l;he a('- 
(:uracy for each subca.tegory. 
We consider the following sut)catcgories for 1)hrasal 
('x)rl'(}Sl)olidell(-(~s: 
? MIN ... The minimal  t)hrasal correst)ondence, 
that is, I'(1Zl'(.s'l, .s2), LT(t l ,  t2)) such that  (;herc 
909 
type 
ALL 
WA 
WX 
WS 
WZ 
1111111. nunl .  of SUCCESS 
of correct ratio found corresp. (%) corresp. 
771 745 96.63 
612 600 98.03 
131 118 90.07 
13 12 92.3 
15 15 100 
Table h Experimental Result of Word Correspon- 
dences 
are word correspondences W(s1,  t l )  and W 
(s2,t2), s2 is a direct child of St and t2 is a 
direct child of tl. 
? LTX ... P(LT(.s'I,S2),LT(tl,t2)) such that 
all nodes other titan s2 and t2 have only one 
child node. 
? LTY ... P(LT(sl,.S2), LT(tl, t2)) such that 
all nodes other than Sl, s2,1':1 and t.2 have only 
one child node. 
LTX is a special case of LTY, since Sl and tl of 
LTX must have only one child node, on the other 
hand, ones of LTY may have more than two child 
nodes. A subcategory test tbr a phrasal correspon- 
dence is done in the above order. Exmnples of these 
subcategories are shown in Fig 5. 
Tlm result of these subcategories are also shown 
in Table 2. Subcategories MIN and LTX have very 
high accuracy and this result is very promising, 
since we can avoid nmnual checking for ttmse phrasal 
correst)ondences , or we would check only these types 
of t)hrasal correspondences mmmally and discard 
other types. 
As stated earlier, since we removed only sen- 
tences with severe parsing errors from the test set, 
please note that the above mtmbers of experimental 
results are calculated for a bilingual parsed corpus 
including parsing errors. 
4 D iscuss ion  
There have been some studies on structural align- 
Inent of bilingual texts such as \[1, 4, 13, 3, 6\]. Our 
work is similar to these previous tudies at the con- 
ceptual level, but different in some aspects. \[1\] 
reported a method for extracting translation tem- 
plates by CKY parsing of bilingual sentences. This 
work is to get phrase-structure level phrasal cor- 
respondences, but our work is to get dependency- 
structure level phrasal correspondences. \[4\] pro- 
posed a method for extracting structural matclfing 
(pairs of dependency trees) by calculating matching 
similarities of two dependency structures. Their 
work focuses on tile parsing ambiguity resolution 
by calculating structural matching. Further, \[3, 6\] 
proposed structural alignnmnt of dependency struc- 
tures. Their work assuined tha.t least common an- 
cestors of each fragment of a structural correspon- 
dence are preserved, but our work does not have 
such structural restriction. \[13\] is different o oth- 
ers in that it tries to find phrasal correspondences 
by comt)aring a MT result and its manual correc- 
tion. 
In addition to these differences, the main differ- 
ence is to find classes (or categories) of phrasal cor- 
respondences which have high accuracy. In general, 
since bilingual structural alignment is very compli- 
cated and difficult task, it; is very hard to get more 
than 90% accuracy in total. If we get only such 
an accuracy rate, the result is not useful, since we 
need manual clmcks tbr the all correspondences re-
trieved. But, if we can get some classes of phrasal 
correspondence with, for instance, more than 90% 
accuracy rate, then we can reduce manual clmck- 
ing for phrasal correspondences in such classes, and 
this reduces the development cost of translation 
patterns used in later corpus-based translation pro- 
tess. As shown in the previous section, we could 
find ttmt all (:lasses of word correspondences and 
two subclasses of phrasal correspondences are more 
than 90% accurate. 
When actually using this automatically retrieved 
structural correspondence data, we must consider 
how to manually correct the incomplete parts and 
how to reuse mamlal correction data if the parser 
results are ctmnged. 
As for the tbrlner issue, we need an easy-to-use 
tool to modify correspondences to reduce the cost 
of mmmal operation. We have developed a GUI 
tool as shown in Figure 6. In this figure, the bot- 
tom half presents a pair of source and target depen- 
dency structures with word correspondences (solid 
lines) and phrasal correspondences ( equences of 
slmded circles). You can easily correct correspon- 
dences by looking at this graplfical presentation. 
As for tlm latter issue, we must develop meth- 
ods for reusing the manual correction data as much 
as possible even if tim parser outputs are changed. 
We have developed a tool for attaching phrasal 
correspondences by using existing phrasal corm- 
spondence data. This is implemented as follows: 
Each phrasal correspondence is assigned a signa- 
ture which is a pair of source and target, sentences, 
each of which tins bracketed segments which are in- 
cluded in the phrasal correspondence. For instance, 
910 
I 
tmihatu 
((~uebiomeR) 
,~10 
gijutu -,,- 
(tedr, do?.t?) 
I 
--,<Jeveloprned 
0f 
--,'.-tect'lrlOlogy 
ILl Zl.l~l.l ,~ 
\[<dime} 
go 
seityou 
(i, otldl} 
I 
ya t~.a d 0ute ki 
. . . . . .  ,.corlti nue 
*o 
- -  shob'o 
-. ~obj 
" "k 9 Kl t;dh 
/ ' / \ 
economic 
f 
kaga t,lJ 
"~. 
% 
unparallelled 
. ?tij d u - \ [ - -~e  c;h n 010 ?tl 
~<~o~l I "-)/. ' ,  ,,o  ol oy- .  x ,,0,,, 
ka nte n 
{a~laedl 
korera .,- 
science. 
# 
g 
z 
ee 
(a) MIN (b) LTX (c) LTY 
p Urp ose 
sPec I 
-,qhis 
Figure 5: Examples of Categories of Phrasal Correst)ondences 
A: 
5115511. of 
type found 
COl; l 'es i ) .  
ALL 678 
MIN 223 
LTX 17 
LTY 27 
B:  
I515151. of 
(:orrect 
co5-5"(~Sl). 
431 
215 
(~: 
SllC(;(~SS 
ratio 
~/A (%) 
63.56 
96A1 
D:  
nunL of nodes 
covered t)y A 
7248 
1234 
E: 
nunl. of nodes 
covered by B 
4278 
1194 
F: 
Sl lCCeSS 
ratio 
E/D (%) 
59.02 
96.76 
17 100 153 153 100 
20 I 74.07 253 191 75A9 
I 
Tal)le 2: lgxperinmntal Fh',sult ot' Phrasal Correst)on(len :es 
the following signature is made h)r a i)hrasal corre- 
Sl)on(lence (c) in Figure 5: 
(.~i:j) 
... \[korer~ no kanten karmlo\] kagaku \[gi- 
j u tu \ ]  ... 
... science and \[technology fl:om this 
lmrl)ose\] ... 
(/.~io) 
In the above e, xample, segments betwee, n '\[' and '\]' 
represent a phrasal correspondence. 
If new parsed dqmndency structures for a sen- 
tence pair is given, for each phrasal correspondence 
signature of the sentence pair, nodes in the struc- 
tures wtfich are inside 1)rackets of the signature are 
marked, mid if there is a minimal sul)tree consist- 
ing of only marked nodes, then a phrasal corre- 
Sl)ondence is reconstructed from the phrasal corre- 
spondence signature. By using this tool, we can 
efficiently reuse the manual efforts as much as pos- 
sible even if parsers are updated. 
5 Conc lus ion  
Ill this I)al)er, we have t)rol)osed methods for 
finding structural correspondences (word correst)on- 
dences and i)hrasal corr(;spondences) of bilingual 
parsed corpus. Further, we showed that the t)reci- 
sion of word correst)ond(mces and some catc'gories 
of t)hrasal corresl)ondences found 1)y our methods 
are highly accurate, and these correst)ondences can 
reduce the cost of trm~slation pattern accumula- 
tion. 
In addition to these results, we showed a GUI 
tool for mmmal correction and a tool for reusing 
previous correspondence data. 
As fld;ure directions, we will find more subclasses 
with high accuracy to reduce the cost for transla- 
tion pattern preparation. 
We believe that these methods and tools can ac- 
celerate the collection of a large set of translation 
patterns and the developlnent of a corlms-based 
translation system. 
911 
~rel id="28" type="P4" src="3.4,9,10,11,12.13" tgt="1,2.3,4,8,9,12" eval="T~R UE" score="O" geoeratlon='' subtype="orff' org=" con'lment='"'= 
~rel id="29" type="P5" src="1,2,3" tg~"l 0,11,12" BvaI="TRUE" score="0" generation="" subtype="org" org=" comment:="'> 
~rel ld="3O" type="P5" src="5.6.7" \[g1="5.6,7" eva~"TRUE" score="0" generation=" sublype="org" org="" cornmen~''~ 
<tel id="31" type="P5" src="7.8,9" tg~"7.F' evaI="TRUE" ecore="O" generation=" subtype="org" erg=" cerumen .t=""~. 
'~rel id="32" type="P5" src="3,4.9.10,11.12.13" tgt="1,2,3,4.e,g,12" evaI="TRUE" score="0" generation="' subtype="org" org='' comment=-"'-'. 
! 
\ 
L 
4\ 
h6 ac len ;~aad tactiilC:;Id6i' g,0tiCid;~ ; f  rn~\[,~r.c~un~les 
.. . . . . .  , . . . .  ? ~- .  
,. ? L:i : :  ? : i % 
Figure 6: An GUI tool for presenting/manipulating structural correspondences 
References  
\[1\] Kaji, H., Kids, Y., and Morimoto, Y., "Learning Trans- 
lation Templates from Bilingual Texts," Proc. of Coling 
92, pp. 672-678, I992. 
\[2\] Kurohashi, S., and Nagao, M., "A Syntactic Analy~ 
sis Method of Long Japanese Sentences based on the 
Detection of Conjunctive Structures," Computational 
Linguisties~ Voh 20, No. 4, 1994. 
\[3\] Grishman, R., "Iterative Alignment of Syntactic Struc- 
tures for a Bilingual Corpus," Proe. of 2nd Workshop 
for Very Large Corpora, pp. 57-68, 1994. 
\[4\] Matsumoto, Y., Ishimoto, H., and Utsuro, T., "Struc- 
tural Matching of Parallel Texts," Proc. of the 31st of 
ACL,  pp. 23-30, 1993. 
\[5\] MeCord, C. M., "Slot Grammars," Computational Lin- 
guistics, Voh 6, pp. 31-43, 1980. 
\[6\] Meyers, A., Yanharber, R., and Grishman, R., "Align- 
ment of Shared Forests for Bilingual Corpora," Proc. of 
the 16th of COLING, pp. 460-465, June 1996. 
\[7\] Nagao, M., "A Framework of a Mechanical Translation 
between Japanese and English by Analogy Principle," 
Elithorn, A. and Banerji, R. (eds.) : Artificial and Hu- 
man Intelligence , NATO 1984. 
\[8\] Sato, S., and Nagao, M. "Toward Memory-based Trans- 
lation," Proc. of 13th COLING, August 1990. 
\[19\] Sumita, E., Iida, II., and Kohyama, H. "'Translating 
with Examples: A New Approach to Machine 3Yanslao 
tion," Proc. of" Info Japan 90, 1990. 
\[10\] Takeda, K., "Pattern-Based Context-Free Grammars 
for Machine ~l~anslation," Proc. of 34th ACL, pp. 144-- 
15I, June 1996. 
\[11\] Takeda, K., "Pattern-Based Machine ~lYanslation," 
Proc. of 16th COLING, Vol. 2, pp. 1155-1158, August 
1996. 
\[12\] Watanabe, H. "A Similarity-Driven Transfer System," 
Proc. of the 14th COLING, Vol. 2, pp. 770.-776, 1992. 
\[13\] ~Vatanabe, H. "A Method for Extracting ~IYanslation 
Patterns from ~lS'anstation Examples," Proc. of the 5th 
Int. Conf. on Theoretical and Methodological Issues in 
Machine Translation, pp. 292-301, 1993. 
\[14\] Watanabe, H., and Takeda, K., "A Pattern-based Ma.- 
chine Translation System Extended by Example-based 
Processing," Proc. of the 36th ACL & 17th COLING, 
Vol. 2, pp. 1369o1373, 1998. 
912 
? 1" )" A Met;hod for Ac(:elerati lg CFG-l msing/)y \[Js\]ng \])ependency 
\]nformation 
Hideo \?atmJalm 
IBM lh'scarch, '.l.'okyo lh~'sc'arch Lal)oral:ory 
\]623-1d Shimotsuruma, Ymnato, \](anagawa 242-8502, Jalmn 
watanabt~((0trl.ilml.(:o.j i) 
Abst rac t  
'\].'hi.q lmlmr d(,scrib(;s an algorithnl for accc'lerat- 
ing l;h(; CF(~'qm.rsing t)ro(:t;ss by using (lel)(;nd(;ncy 
(or modifier-nlodifie(; relationship) infornmtion given 
by, for insi,an(:e, d('.llcnd('alcy cstimal,ion l)rograms 
Sll(:h as sl;o(:\]ulsl;i(: 1)arsers~ llSCl';,q Jltdic;d;ion in an 
inl;(,ra(;tiv(', al)t)li(:al;il)n ,mM \]inguisl;ic mmotal;i(nm 
;t(hh:(1 in a sour(:(' l;(.'xl;. This is a ml;l;hod for ('.n- 
\]mn(:ing exi,%ing grmnmard/as(',d CF(\]-l)arsing sys- 
Wan by using dc'tmnden(;y informal;ion. 
_1. Introduct ion 
Th(' parsing sysl;O.lil is ;i, key co111t)o11(111|; \]'or 11at- 
tual language, ai)i/lications uch as machine trans- 
lal;ion, informal;ion rel;rJ(wal, l;cxt ,'-;unllnariz;ll;ion, 
and its l)(;rfornlml(:(; (\])roct;ssintt; speed and act:u- 
racy) is very inq)orl;ant o l;h(! success of l;lms(' ap- 
pli,::ations. 
Tim umm\] CF(Ltmrsin/~ algorithlns \[3, 6\] k(,.(!p all 
interm('.dJat(; l)ossibiliti(~s which may or may not t)c 
tls(xl ill tim tinal pm:se r(;,qults. Tlmr(~for(!, we usu- 
ally reduce 1;hi;s(; illl;(.'l:lllcdial;o, l)ossibiliti('s which 
are unlikely to t)(; used as tinal results in the nlid- 
die ()f l;he process l)y using s(;vt;ral l)rmling t('x:h- 
ni(luCS. One good information ,sour(:('~ for pruning 
is d(;t)end(;n('y information |)c.tw('.cn words. It has 
nol; l)(;(m so easy l;o gel; such d('l)('n(h'.ncy informa- 
tion until a. few years a.go, but, th(; sil;uat;ion has 
ret:(ml;ly chang(,,d. 
Recent intensive studies on statistical alll)roach 
\[7, 1, 2\] a(lvanccxl statistical parsing systems, and 
wc can gel, relatively correct dct)en(h'ncy informa- 
tion using these systems, leurthc'r, if we SUl)t)osc 
an interactive NLP system, then there aa(, sore(, 
types of user intera(:tions which can b(; considered 
to determilm 1;11(; modifice c;mdidatc. I11 addition, 
recent studies on the linguistic infi)rnmtion mmo- 
(;a.l;ion \[10, 4, 12, 1.3\] provid(; tools l/y which a user 
can (;asily annotate  l inguist ic  intbrnmtion (si /ecial  
XML markup tags) into source texts ,  and we can 
OX\[)(X;I; |;0 ,qtX) a,ll increase of tho 11111111)(11 of l;exi;s 
wil;h linguistic information. This linguistic infor- 
nlai;ion usually includes dependtmc,y infornml;ion. 
For instmmc, the following example shows m~ &llllO- 
(;al;ion ('xaml/h' by Linguistic Annol;ation lmnguag(; 
described in \[12, 13\], and the id and rood atl;l'i})ll|;c,q 
inside tal:w (,hmmifl;s pc.ci\[\[y word dependencies. 
IIe (lal:w id=" 1" )saw(/lal:w) a man (lal:w 
,nod="1"  )wi th( / la l :w)  a tc'l(',scolm. 
in this (;xanll)h', the word "with" modifi(;s l;he word 
As shown in l;hc, above (~xample,% we can now 
get depc.ndcncy inlbrnmtion more easily than a ti;w 
years ago. This paper describes an algorithni for 
accelcrnting CFG-lmrsing systems by using su(:h 
d(;pcnd(;ncy (or modifier-moditi(;e r(~lationship) in- 
formation. Th(; prot)oscd algorithm does not as- 
sume all words are given dctmndency int'ormation~ 
ratht;r it works in case such that some of words are 
partia.lly given dep(;ndt',ncy infl)rnm.tion. 
2 Ol)t imiz ing A lgor i thm Using De- 
pendency  Infornmtion 
We use a. nornml CFG lmrsing sysi;('m with one' 
(;xl;(;nsion that for (m.t'h ru\]c (here, must he. o11c righl;- 
h;md sid(' (or \]{ITS) t('rm I mark(,d as a h(,ad, and 
th('. informati(m (if a head term is trmlsJhrr('.(l Lo l;hc. 
lc.ft-hmM side (or H IS)  tenn. In this lmtmr, a CIeG 
rule is (hmol;ed as follows: 
{x -~ ~q ... ~ . . . .  ~.;,} (,,, > 0) 
In tim above, notation, X is dm left-lmnd side 
(or LHS) term, mM I5- are right-hand side (or l lH$) 
terms, mM a RHS term followed by an asterisk '*' is 
a head term. The l;ypical usage of the head is that 
the LHS t(;nn shares many features of the head 
term in the RHS. For instmme, a matching word of 
the the LHS tcnn becomes the same as the one of 
the head term in the RHS. 
For each rule, an arc is constructed over a word 
segment in a.n input sentence. An aa'c is d('alot(,d 
using terms of its base rule as follows: 
I x -~ ~q ... E - .  ~1+,* ... 5, \ ]  
1A term expresses a non-terminal symbol in IAIS, an(1 ~'~ 
non-terminld or a terminal symbol in l/.IIS. 
913 
The LHS term of an arc nmans the LHS term 
of the base rule of the arc, and RHS terms of an 
arc means RHS terms of the base rule of the arc. 
In the above notation, a single dot indicates thai; 
RHS terms located to the left of a dot are inactive, 
that is, they already match the LHS term of some 
other arcs. Three dots are used to ret)resent zero or 
any number of terms. An arc whose RHS terms are 
all inactive is called an inactive arc, otherwise it is 
called an active arc. An arc covers a segment of 
input words; the start point of an arc is the index 
of the first word in the covering segment, and the 
end point of an arc is 1 plus the index of the last 
word in the covering segment. 
Basically, a standard CFG parsing algorithm such 
as \[3, 6\] consists of the following three operations. 
Initialization: For each word, arcs are generated 
froln rules such that the leftmost RHS term 
matches it. 
Operation A: For each inactive arc A, an arc is 
generated fl'om A and a rule R such that the 
leftmost RHS term of R ,natchcs the LHS 
term of A. 
Operation B: For each inactive arc A, an arc is 
generated from A and another active arc B 
such that the leftinost active RHS term of B 
matches the LHS term of A and the end t)oint 
of B is the stone as tile start point of A. 
We assume that some dependency information 
1)etween words are given, and such det)endency in- 
formation is denoted as follows: 
w.~w,  
The first of the above examples represents that 
a word I/V u modifies another word I~(~, attd W~, pre- 
cedes 14~j, while the second one represents that a 
word Rq, modifies another word H~j and W,, pre- 
cedes 1/1~/. 
Given this kind of dependency information, the 
following conditions are imposed on Operation A 
and Operation B. 
Condit ions for Operation A: 
Condition A1 (when the leftmost RHS term of 
a rule is a head term): 
Given an inactive arc Arc1 denoted by 
\[A ~ ...\] and a rule which has two or 
more RHS terms and the leftmost RHS 
term is a head denoted by {X -+ A * 
B ...}, Operation A is executed only if 
there is dependency information 144, 
lYb where 1,14~ is a word matching the 
LHS term A of Arci and lVb is a word 
located anywhere to the right of the end 
I)oint of Arc1. 
{X->A*  B ...} 
Wa WD 
- .  . . . . . . . . . . . .  ? - - "  
Figure 1: Condition A1 
Figure 1 shows the above condition. In this fig- 
ure, a thick arc ret)resents an inactive arc, a line 
represents a matching to be tried in this ot)eration, 
a dotted line represents a matching betweeu a term 
in an arc and a word, and a dotted arrow represents 
dependency infbrmation. In this case, this type of 
rule implies that a word matching the LHS term 
of the arc to be matched with the leftmost erm of 
the rule must be modified by any word which is lo- 
cated after the end t)oint of the arc, since the head 
term is the left;most erm of the rule. Therefore, if 
the A1 condition does not hold, Operation A is not 
required to be executed. 
Condition A2 (when the leftmost RHS term of 
a rule is not a head term): 
Given an inactive arc Arc1 denoted by 
\[A --+ ...\] and a rule which has two or 
more RHS terins and the leftmost I{HS 
term is not a head denoted by {X --+ A 
... D* ...}, Operation A is executed 
only if there is a dependency informa- 
tion 14~ ~ 1?~ where 1/1~ is a word 
Inatching the LHS term A of Arc1 and 
Wv is a word located anywhere after the 
end point of Arc1. 
Figure 2 shows the above condition. In this case, 
this type of rule ilnplies that a word matching tile 
LHS term of the arc to be matched with the left- 
most term of the rule Inust inodify any word which 
is located after the end point of the arc, since the 
head terin is not tile leftmost erm of the rule. 
Condit ions for Operation B: 
Condition B1 (when the leftmost active RHS 
term of an active arc is the head term): 
Given an active arc Area denoted by 
\[X --+ Ao ... A~ . B . . . . \ ]  and anin-  
active arc Arc1 denoted by \[B -+ ...\] 
914 
{X-> A ... D*  . . .}  
Wa Wb 
Figure 2: Condition A2 
such that the end point of Area is the 
santo as the start point of Arc r, Ol)era- 
tion B is executed only if, for each l,lz,~ 
(0 < i < n) which is a word match- 
ing the RItS term A~ of AreA, there 
is dependency information I'Vai => I'Vb, 
where Wi, is a word matching the LHS 
term B of Arc,. 
ix->. 
,: : -, V / \ 
Wao ... Wa,, Wb 
" " - -  . . . . . . . . . . . . . . . . . . . . . .  .-" 
l?igure 3: Condition B1 
Figure 3 shows the above condition. Ill this fig- 
ure, ~ dotted thick arc represellts an active m:c. hi 
this case, this type of active arc implies that words 
matching inactive terms before' the head term of 
the active art: must modify a. word matdfing tile 
LHS term of the inactive arc. 
Condition B2 (when the head term is on the left 
side of the leftinost active RHS term of an active 
arc): 
Given an active mc AreA denoted by 
\[X ~ ... A* ... B ...\] and all in- 
active arc Arc1 denoted by \[B --+ ...\] 
such that the end point of Area is the 
same as the start point of Arc1, Oper- 
ation B is executed only if there is de- 
t)endency information W,, ~ Wb where 
144~ is a word matching the RHS term 
A of AreA, and Wv is a word matching 
the LItS terln B of Arc1. 
. . . . .  
IX ->,  " 
l" 
I 
Wa Wb 
Figure 4: Condition B2 
Figure 4 shows tile above condition. I n  this case, 
this type of active arc implies that a word lnatching 
the LIIS term of the inactive arc nmst modi~ a 
word matching the head tcrin of the active arc. 
Condition B3 (when the head term is on the 
right side of the leftlnost active RItS term of an 
active arc): 
Given an actiw; arc Area denoted by 
\[X ~ A .  B ... C* ...\] and an inactive 
arc Arc1 denoted by \[/3 -+ ...\] such that 
the end point of Area is the same as the 
start point of Arc,, Operation B is exe- 
cuted only if there is dependency infof  
nmtion Wb ~ 14~ where Wf, is a word 
matching the. LItS term B of Arcl, and 
l'14: is a word on the right side of the, 
end point of Arci. 
Wb ,. I," Wc 
Figure 5: Condition B3 
Figure 5 shows the above condition. In this case, 
this type of active arc implies that a word matching 
the. LHS term of tile inactive arc must modify a 
word after the end point of tile inactive arc. 
The dependency information is not necessarily 
given to all words. If there is any source word ex- 
cept for the root word of a sentence such that there 
915 
is no del)endency information originating fl:om it: 
then a set of such del)endeney inibrmation is called 
partial, otherwise, it is called total. If the given de- 
1)endency informatioll is partial, the A\] condition 
can not be used, since, even if there is no det)en- 
dency information targeting I.V,,, we eanllot know if 
such del)endency information does not really (,xist, 
or if such delmndency inlbrmation is llot Sul)plied. 
For other conditions, we check them only when all 
source words for dependency checking have depen- 
dency information. On the other hand, if the given 
dependency information ix total, all conditions are 
checked. 
3 Experiment 
We have imt)lemented the 1)reposed algorithm 
into an existing English CFG-parser we have devel- 
oped for a machine translation t)roduct \[8, 9, 11\] e 
, and conducted an experinmnt to know the effec- 
tiveness of this algorithm. 
We selected 280 test sentences rmxdomly from 
a sentence set created by .\]EIDA :~ for ewfluating 
translation systen L and made the correct dei)en-- 
(lency relation data for these selected test sentences. 
We collected the number of inactive arcs, the num- 
b(;r of active arcs, and the t)rocessing time for cases 
such that C modifiee candidates (one of which is 
the correct modifiee) are given to a word. 4 If C:=I 
then it; corresponds to the best case for a parser 
such that only one correct modifiee is given fin' each 
word, while if C is 3 or 4 then it; corresponds to the 
approximation of using a statistical modifiee esti- 
ination program for getting candidate modifiees. 
The graphs in Figure 6 indicate the reduction 
ratios of active arcs, inactive arcs, and 1)recessing 
time for using conditions for total dependency in- 
formation and conditions tbr partial del)endeney in- 
formation. The de'nominators for calculating these 
r&tios are the numbers of ar(:s and the processing 
time (seconds) in case of the parser without this al- 
gorithm. In these graphs, C=X indicates that X is 
the maxilnunl nulnber of moditlee candidates given 
to a word. 
From these gratlhs, we can so(; that the more 
words in a sentence, the better the 1)erformance. 
In a real domain, most sentences consist of more 
than ten words.  Therefore, looking at values for 
around 10 in the X axis, we can see that inactive 
arcs are reduced by about 40% and 25%, active arcs 
2This parser is used in a Web page translation software 
called "lnternet King of 3t'anslation" released from IBM 
.laI)an. 
a.lal)all Electronic Industry \])evcloi)ment Association 
4Modifiee candidates are selected randomly except for 
the correct oi1o. 
are reduced by about 65% and 35%, and t)rocessing 
time is reduced by about ~15% and 15%, for the 
ideal case (C-1)  and more practical cases (C=3 
or 4), respectively, in the (:as('. of total del)endency 
information. Please note that, since the 1)arser in 
which this algorithm ix impleumnted has already 
several pruning mechanisms, we can expect more 
reduction (or pertbrmance gain) for generic CFG 
pars(',rs. 
4 Discussion 
As a study for accelerating the parsing tu'ocess 
using dependency information, Imaichi\[5\] reported 
an algoritlnn for Japanese language. The condi- 
tions introduced by hnaichi are described by using 
the notation in this paper as ~bllows: 
Condition MI: 
Given an active arc Area denoted by 
\[X -~ A . 13.\] and an inactive arc Arc1 
denoted by \[B -+  ...\] such that the end 
point of Area is the same as the start 
point of Arc j, Operation B is executed 
only if there is dependency infl)rmat.ion 
I<1~, -=> lYb where 1'15~ is a word matching 
the RIIS term A of AreA, and lVt, is 
a word mat(:hing the I~HS term ,r3 of 
mrcl. 
Con(lition M2: 
Given an inactive ar(: Arc1 denoted 1)y 
\[A -+ ...\] and a rule denoted by {X --~ 
A ...}, Operatioil A is execllted only 
if there is no det)endency iifl'ormation 
Wt. => l'l/-,~ where 1.'I~ ix a word match- 
ing the LHS term A of Arc1 and lYt. is 
a word loca.ted before the start point of 
A~rc l  . 
The condition M1 correspouds to B 1. Since hnaiehi's 
algorithln considers only .Japmmse in which all words 
other than the last; word modifies one of the suc- 
ceeding words, it does not deal with cases usually 
seen in Eurot)eall languages where a word modities 
one of the preceding words. Therefore, it is not 
applicable to any language other than Jat)anese in 
general. Fnrthcr, since a CFG rule is restricted to 
be in Chomsky normal form, hnaichi's algorithm is 
limited in terms of at)plicability. 
Since the algorithm proposed in this pal)er does 
not have any restrictions on the dependency direc- 
tion and the CFG rule format, it can be applicable 
to any CFG-parsers ill any languages. 
916 
Reductiof Ratio o\[ Inactive Arcs 
for Total Dependency Inf0. 
60 
85o 
'~\[ 40 
O 
qJ 
10 
0 
4 5 6 -! 8 9 10 11 12 
NUt'rl 01" WOIL~; 
O0 
Reduction I~atio of Active Arcs 
for loLal Dependency Info. 
80 
70 
~ 60 i i  
'~0J ,50 
ft- 
=40 o 
~: 20 
10 
4 5 fi 7 8 ? 10 11 1 
\[J\]J G=1 
I~ 0=2 
\[ \ ]  O~-3 
I~ 0:-4 
Nl.lm of 'lil'~l'Orl\[}~ 
O0 
\[\]\]J O=l 
.~ 0--2 
\ [ \ ]  0=3 
\[ \ ]  0=4 
60 
Reduction Ratio of lime 
for Total Dependency Info. 
~.5O 
'~ 40 ~0 
~30 
,\]3 
~clO 
4 5 6 7 8 g 10 11 12 
Nurl'l of "~l'l,l'Orlj4,. 1: 
\ [ \ ]  0=1 
\ [ \ ]  O--2 
\ [ \ ]  0--3 
\ [ \ ]  O=4 
Reduction Raito of Inactive .Arcs 
\[or Partial Dependency In\[o. 
60 
~5o 
0 
-P 40 
0 
~20 
E 10 
4 5 6 1 8 9 10 11 
NLIFr'I Elf 'l,ll,?Oi"l.~-: 
(t,) 
t21 O=2 
O=3 
F_1 0=4 
12 
Reduction Ratio ot Active Arcs 
for-Partial Dependency Info 
5O 
~40 
o 
o 
"~ 20 
"U j 
4 5 6 7 8 g 10 11 
NU~'\[\] ,:If ),ll,tClrl~:~: 
(d) 
~ O=l 
O=2 
O=4 
12 
50 
40 
C I 
o 
ailO 
Reduction Ratio of l ime 
for Partial Dependency Info. 
4 5 6 7 8 9 10 11 12 
Num ,:,f W0rd~. 
(,;) (f) 
~\] 0=1 
U O=2 ~ G=3 
6=4 
Figure. 6: l{educl;ion ratios o\[ inaci.ive arcs, acl~ive arcs, a.ml processing Lime 
917 
5 Conc lus ion  
We developed an algorithm for accelerating the 
performance of the CFG t)arsing process if we are 
given dependency information. From an experi- 
ment, we can show the effectiveness of this algo- 
rithm. 
By using this algorithm, we can enhance xist- 
ing grammar-based parsers using dependency in- 
formation given by stochastic parsers, interactive 
systems, and texts created by linguistic annotation 
systems. 
References 
\[1\] M. Collins. A new statistical parser based on 
bigram lexical dependencies. In Proc. of 3~fl~ 
AC?, pages 184-191, 1996. 
\[2\] M. Collins. Three generative, lexicalized models 
fbr statistical parsing. In P~vc. of 35th A CL, 
pages 16-23, 1997. 
\[3\] J. Earley. An efficient context-free parsing al- 
gorithm. In Readings in Natural Language Pro- 
cessin9. Morgan Kauflnan, 1969. 
\[4\] K. Hashida, K. Nagao, et al, Progress and 
Prospect of Global Document Annotation. (in 
Japanese) In P~vc. of ~th Annual Meeting of 
the Association of Natural Language Process- 
ing, pp. 618-621, 1998. 
\[5\] O. Imaichi, Y. Matsumoto, and M. Fujio. An 
integrated parsing method using stochastic in- 
formation and grammatical constraints. Jour- 
nal of Natural Language Prvcessing, 5(3):67-83, 
1998. 
\[6\] M. Kay. Algorithm schemata nd data struc- 
ture in syntactic processing. Technical Report; 
CSL-80-12, Xerox PARC, 1980. 
\[7\] D. M. Magerman. Statistical decision-tree mod- 
els fi)r parsing. In Prvc. of 33rd A CL, pages 
276-283, 1995. 
\[8\] K. Takeda. Pattern-based context-free gram- 
inars for machine translation. In Proc. of 3~th 
ACL, pages 144-151, 1996. 
\[9\] K. Takeda. Pattern-based machine translation. 
In Proc. of 16th Coling, volume 2, pages 1155 
1158, 1996. 
\[10\] Text Encoding Initiative 
(http://www.uic.edu:80/orgs/tei/) 
\[11\] H. Watanabe and K. Takeda. A pattern- 
based machine translation system extended by 
example-based processing. In Proc. of 17th Col- 
ing (Coling-ACL'98), volume 2, pages 1369- 
1373, 1998. 
\[12\] H. Watanabe, Linguistic Annotation Lan- 
guage - The Markup Language for Assist- 
ing NLP Programs -. IBM Research Report 
I/T0334, 1999. 
\[13\] H. Watanabe, K. Nagao, et al, Linguistic An- 
notation System for Improving the Performance 
of Natural Language Processing Programs. In 
Proc. of 6th Annual Meeting of The Association 
for NLP (in Japanese), pp. 171-174, 2000. 
918 
An Annotation System for Enhancing Quality of Natural Language
Processing
Hideo Watanabe*, Katashi Nagao**, Michael C. McCord*** and Arendse Bernth***
* IBM Research,
Tokyo Research Laboratory
1623-14 Shimotsuruma, Yamato,
Kanagawa 242-8502, Japan
hiwat@jp.ibm.com
** Dept. of Information Engineering
Nagoya University
Furo-cho, Chikusa-ku,
Nagoya 464-8603, Japan
nagao@nuie.nagoya-u.ac.jp
*** IBM T. J. Watson
Research Center
Route 134, Yorktown Heights,
NY 10598, USA
mcmccord@us.ibm.com,
arendse@us.ibm.com
Abstract
Natural language processing (NLP) programs are
confronted with various diculties in processing
HTML and XML documents, and have the po-
tential to produce better results if linguistic infor-
mation is annotated in the source texts. We have
therefore developed the Linguistic Annotation Lan-
guage (or LAL), which is an XML-compliant tag set
for assisting natural language processing programs,
and NLP tools such as parsers and machine trans-
lation programs which can accept LAL-annotated
input. In addition, we have developed a LAL-
annotation editor which allows users to annotate
documents graphically without seeing tags. Fur-
ther, we have conducted an experiment to check
the translation quality improvement by using LAL
annotation.
1 Introduction
Recently there has been increasing interest in
applying natural language processing (NLP) sys-
tems, such as keyword extraction, automatic text
summarization, and machine translation, to Inter-
net documents. However, there are various ob-
stacles that make it dicult for them to produce
good results. It is true that NLP technologies are
not perfect, but some of the diculties result from
problems in HTML. Further, in general, if linguis-
tic information is added to source texts, it greatly
helps NLP programs to produce better results. In
what follows, we would like to show some examples
related to machine translation.
In general, it is very helpful for machine transla-
tion programs to know boundaries on many levels
(such as sentence, phrases, and words) and to know
word-to-word dependency relations. For instance,
in the following example, since \St." has two possi-
ble meanings, \street" and \saint," it is dicult to
determine whether the following example consists
of one or two sentences.
I went to Newark St. Paul lived there
two years ago.
As another example, the following sentence has
two interpretations; one interpretation is that what
he likes is people and the other interpretation is
that what he likes is accommodating.
He likes accommodating people.
If there are tags indicating the direct-object mod-
ier of the word \like," then the correct interpreta-
tion is possible. NLP may be able to resolve these
ambiguities eventually by using advanced context
processing techniques, but current NLP technology
generally needs a hint from the author for these
sorts of ambiguities.
Further, there are issues in HTML/XML. When
MT systems are applied to Web pages, most of the
errors are generated by the linguistic incomplete-
ness of MT technology, but some are generated by
problems in HTML and XML tag usage. For in-
stance, writers often use <br> tag to sentence ter-
mination. Sometimes writers intend that a <br>
tag should terminate the sentence (even without
terminating punctuation such as a period), and in
other cases writers intend <br> only as a format-
ting device. In the HTML <table> shown in Figure
1, the writer intends each line of a cell to express
one linguistic unit. The MT program cannot tell
whether each line is a unit for translation, or, in-
stead, the two lines form one unit. In this example,
some MT programs would try to produce a transla-
tion of a unit \NetVista Models ThinkPad News."
As shown in the above examples, NLP appli-
cations do not achieve their full potential, on ac-
count of problems unrelated to the essential NLP
processes. If tags expressing linguistic information
<table><tr><td>
<a href="...">NetVista Models</a><br>
<a href="...">ThinkPad News</a><br>
</td></tr></table>
Figure 1: An example of using hbri tags in a table
are inserted into source documents, they help NLP
programs recognize document and linguistic struc-
tures properly, allowing the programs to produce
much better results. At the same time, it is true
that NLP technologies are incomplete, but their de-
ciencies can sometimes be circumvented through
the use of such tags. Therefore, this paper proposes
a set of tags for helping NLP programs, called Lin-
guistic Annotation Language (or LAL).
2 Linguistic Annotation Language
LAL is an XML-compliant tag set and its XML
namespace prex is lal.
The LAL tag set is designed to be as simple as
possible for the following reasons: (1) A simple tag
set is easier for developers to check manually. (2)
An easy-to-use annotation tool is mandatory for
this annotation scheme. Simplicity is important
for making an easy-to-use annotation tool, since if
we use a feature-rich tag set, the user must check
many annotation items.
2.1 Basic Tags
The sentence tag s is used to delimit a sentence.
<lal:s>This is the first sentence.</lal:s>
<lal:s>This is the second sentence.</lal:s>
The attribute type="hdr" means that the sen-
tence is a title or header.
The word tag w is used to delimit a word. It
can have attributes for additional information such
as base form (lex), part-of-speech (pos), features
(ftrs), and sense (sense) of a word. The values of
these attributes are language-dependent, and are
not described in this paper because of space limi-
tations. The following example illustrates some of
these tags and attributes.
<lal:s>
<lal:w lex="this" pos="det">This</lal:w>
<lal:w lex="be" pos="verb" ftr="sg,3rd">
is</lal:w>
<lal:w lex="a" pos="det">a</lal:w>
<lal:w lex="pen" pos="noun" ftr="sg,count">
pen</lal:w>
</lal:s>
The dependency (or word-to-word modication)
relationship can be expressed by using the id and
mod attributes of a word tag; that is, a word can
have the ID value of its modiee in a mod attribute.
The ID value of a mod attribute must be an ID value
of a word tag or a segment tag. For instance, the
following example contains attributes showing that
the word \with" modies the word \saw," meaning
that \she" has a telescope.
She <lal:w id="w1" lex="see" pos="v"
sense="see1">saw</lal:w> a man
<lal:w mod="w1">with</lal:w>
a telescope.
The phrase (or segment) tag seg is used to spec-
ify a phrase scope on any level. In addition, you
can specify the syntactic category for a phrase by
using an optional attribute cat. The following ex-
ample species the scope of a noun phrase \a man
... a telescope," and it is a noun phrase. This also
implies that the prepositional phrase \with a tele-
scope" modies the noun phrase \a man."
She saw <lal:seg cat="np">a man with a
telescope</lal:seg>.
The attribute para="yes" means that the seg-
ment is a coordinated segment. The following ex-
ample shows that the word \software" and the word
\hardware" are coordinated.
This company deals with <lal:seg cat="np"
para="yes">software and hardware</lal:seg>
for networking.
The ref attribute has the ID value of the refer-
ent of the current word. This can be used to specify
a pronoun referent, for instance:
<lal:s>He bought <lal:seg id="w1">a
new car</lal:seg> yesterday.</lal:s>
<lal:s>She was very surprised to
learn that <lal:w ref="w1">it</lal:w>
was very expensive.</lal:s>
2.2 Expressing Multiple Parses
As mentioned earlier, since natural language con-
tains ambiguities, it is useful for LAL annotation
to have a mechanism for expressing syntactic am-
biguities.
We have introduced a parse identier (or PID)
in attribute values for distinguishing parses. An
attribute value which may be changed according
to parses can be allowed to be expressed as space-
separated multiple values, each of which consists of
a PID prex followed by a colon and an attribute
value.
<lal:s>
<lal:w id="1" mod="2">He</lal:w>
<lal:w id="2" mod="0">likes</lal:w>
<lal:w id="3" mod="p1:2 p2:4">
accommodating</lal:w>
<lal:w id="4" mod="p1:3 p2:2">people
</lal:w>.</lal:s>
This example shows that there are two interpre-
tations whose PIDs are p1 and p2, and that the p1
interpretation is \He likes people" and p2 is \He
likes accommodating."
3 LAL-Aware NLP Programs
We have modied certain NLP systems to be
LAL-aware. ESG [5, 6] is an English parsing sys-
tem developed by the IBM Watson Research Cen-
ter, and updated to accept and generate LAL-annotated
English. We have also developed a Japanese pars-
ing system with LAL output functionality. These
LAL-aware versions of parsers are used as a back-
end process to show users the system's default in-
terpretation for a given sentence in the LAL-annotation
editor described below.
Further, the English to German, French, Span-
ish, Italian and Portuguese translation engines [6,
7] and English to Japanese translation engine [9]
are modied to accept LAL-annotated English HTML
input.
1
4 The LAL-Annotation Editor
Since inserting tags into documents manually is
not generally an easy task for end users, it is impor-
tant to provide an easy-to-use GUI-based editing
environment. In developing such an environment,
we took into consideration the following points: (1)
Users should not have to see any tags. (2) Users
should not have to see internal representations ex-
pressing linguistic information. (3) Users should be
able to view and modify linguistic information such
as feature values, but only if they want to.
Considering these points, we have found that
most of the errors made by NLP programs result
from their failure to recognize the phrasal struc-
tures of sentences. Therefore, we have decided to
1
In addition, Watanabe [11] reported on an algorithm
for accelerating CFG-parsing by using LAL tag informa-
tion, and it is implemented in the above English-to-Japanese
translation engine.
show only a structural view of a sentence in the ini-
tial screen; other information is shown only if the
user requests it.
The important issue here is how to represent the
syntactic structure of a sentence to the user. NLP
programs normally deal with a linguistic structure
by means of a syntactic tree, but such a structure
is not necessarily easy for end users to understand.
For instance, Figure 2 shows the dependency struc-
ture of the English sentence \IBM announced a new
computer system for children with voice function."
This dependency structure is dicult for end users,
partly because a dependency tree does not keep the
surface word order, so that it is dicult to map it
to the original sentence quickly.
2
Therefore, an im-
portant property for the linguistic structural view
is that users can easily reconstruct the original sur-
face sentence string.
The next important issue is how easily a user
can understand the overall linguistic structure. If
a user is, at rst, presented with detailed linguistic
structure at the word level, then it is dicult to
grasp the important linguistic skeleton of a sen-
tence. Therefore, another necessary property is
to give users a view in which the overall sentence
structure is easily recognized.
Figure 2: An example of tree structure of an En-
glish sentence
With these requirements in mind, we have devel-
oped a GUI tool called the LAL Editor. To satisfy
the last requirement, this editor has two presenta-
tion modes: the reduced presentation view and the
expanded presentation view. In the reduced pre-
sentation view, a main verb and its modiers are
basic units for presenting dependencies, and they
are located on dierent lines, keeping the surface
order. Figure 3 shows an example of this reduced
presentation view. In this view, since dependen-
cies that are obvious for native speakers (e.g. \a"
and \computer" ) are not displayed explicitly, the
user can concentrate on dependencies between key
2
You must perform an inorder tree walk to reconstruct a
surface sentence string.
Figure 3: Screen Images of LAL Editor - Reduced
View
units (or phrases). If the user nds any depen-
dency errors in the reduced view, he or she can
enter the expanded view mode in which all words
are basic units for presenting dependencies. Fig-
ure 4 (a, b) shows examples of this expanded view.
In these views, to satisfy the former requirement,
dependencies between basic units are expressed by
using indentation. Therefore you can easily recon-
struct the surface sentence string by just looking at
words from top to bottom and from left to right,
and easily know dependencies of words by looking
at words located in the same column. For details
of the algorithm, see [12].
In Figure 3, you can easily grasp the overall
structure. In this case, since the dependencies be-
tween \for" and \announced," and \with" and \an-
nounced" are wrong, the user can change the mode
to the expanded view (as shown in Figure 4 (a)).
In this view, the user can change dependencies by
dragging a modier to the correct modiee using
a mouse. The corrected dependency structure is
shown in Figure 4 (b).
In addition, the LAL Editor has the capability of
testing translation by using LAL annotation. Fig-
ure 5 shows a window in which the top pane shows
the input sentence, the second pane shows the LAL-
annotation of the input, the third pane shows the
translation result using the LAL annotation, and
the fourth pane shows the default translation with-
out using the LAL annotation. The user can easily
check whether the current annotation can improve
translations.
5 Experiment
We have conducted a small experiment for eval-
uating LAL annotation to our English-to-Japanese
machine translation system[9]. We gathered about
60 sentences from Web pages in the computer do-
main, and added LAL annotation to these sen-
(a) Expanded View (before correction)
(b) Expanded View (after correction)
Figure 4: Screen Images of LAL Editor - Expanded
View
tences with the LAL annotation editor. In this
experiment, only word-to-word modications were
corrected. Due to severe parsing errors and glitches
of the annotation editor, 53 of the 60 sentences
were used in this experiment. The average sentence
length for this test set was 21 words. Two evalu-
ators assigned a quality evaluation ranging from 1
(worst) to 5 (best) for each translation, with and
without use of annotation.
Translation results for 18 sentences (about 34%)
were better for the annotated case than the non-
annotated case. These better sentences were 1.16
Figure 5: Translation test window of LAL Editor
points better (27% better in quality score). On
the other hand, 26 sentences (about 49%) were not
changed, and 9 sentences (about 17%) were worse.
The main reason why these 9 sentences were worse
was the structural mismatch between the output
of the LAL Editor and the expected structure of
EtoJ translation system, since the LAL Editor and
the EtoJ MT system use dierent parsing systems.
We have developed a structure conversion routine
from LAL editor output to EtoJ input, but it does
not yet cover all situations. This is the reason why
these 9 sentences become worse.
Note that this experiment only uses word-to-
word modication corrections, so there is room for
producing better translations if we use other types
of annotation such as part-of-speech, and word sense.
6 Discussion
There have been several eorts to dene tags
for describing language resources, such as TEI [10],
OpenTag [8], CES [1], EAGLES [2], GDA [3]. The
main focus of these eorts other than GDA has
been to share linguistic resources by expressing them
in a standard tag set, and therefore they dene very
detailed levels of tags for expressing linguistic de-
tails. GDA has almost the same purposes but it
has also dened a very complex tag set. This com-
plexity discourages people from using these tag sets
when writing documents, and it also becomes dif-
cult to make an annotation tool for these tags.
LAL is not opposed to these previous eorts, but
attempts to strike a useful balance between expres-
siveness and simplicity, so that annotation can be
used widely.
As mentioned in the discussion of the experi-
ment, there is an issue when the parsing system
of LAL editor and the parsing system of a NLP
tool which accepts the output of LAL editor are
dierent. As mentioned before, we used the ESG
parser for producing LAL-annotated English, and
Japanese-to-EnglishMT system for accepting LAL-
annotated English. Since these systems have been
independently developed based on dierent approaches
by dierent developers, we found there are some
structural dierences. For instance, given a prepo-
sitional phrase Prep N, ESG's head word of the
prepositional phrase is Prep, but EtoJ MT engine's
head is N. In most cases, we can make systematic
conversion routines for dierent structures. In fact,
for most of sentences whose translation is worse
when annotation is used, we can provide struc-
tural conversion routines for linguistic structures
included in them. The basic idea of LAL-awareness
for NLP tools is that an NLP tool uses LAL infor-
mation as much as possible, but if LAL information
produces a severe conict with the internal process-
ing, then such information should not be used. Our
EtoJ MT program was basically implemented this
way based on the algorithm described in [11], but
we seem to need more research on this issue.
7 Conclusion
In this paper, we have proposed an XML-compliant
tag set called Linguistic Annotation Language (or
LAL), which helps NLP programs perform their
tasks more correctly. LAL is designed to be as
simple as possible so that humans can use it with
minimal help from assisting tools. We have also de-
veloped a GUI-based LAL annotation editor, and
have shown in an experiment that use of LAL anno-
tation enhances translation quality. We hope that
wide acceptance of LAL will make it possible to use
more intelligent Internet tools and services.
References
[1] CES, \Corpus Encoding Standard (CES),"
(http://www.cs.vassar.edu/CES/)
[2] EAGLES, \Expert Advisory Group on Language Engi-
neering Standards,"
(http://www.ilc.pi.cnr.it/EAGLES/home.html)
[3] GDA, \Global Document Annotation,"
(http://www.etl.go.jp/etl/nl/gda/)
[4] Koichi Hashida, Katashi Nagao, et. al, \Progress
and Prospect of Global Document Annotation," (in
Japanese) Proc. of 4th Annual Meeting of the Asso-
ciation of Natural Language Processing, pp. 618{621,
1998
[5] McCord, M. C., \Slot Grammars," Computational Lin-
guistics, Vol. 6, pp. 31{43, 1980.
[6] McCord, M. C., \Slot Grammar: A System for Sim-
pler Construction of Practical Natural Language Gram-
mars," in (ed) R. Studer, Natural Language and Logic:
International Scientic Symposium, Lecture Notes in
Computer Science, pp. 118{145, Springer Verlag, 1990.
[7] McCord, M. C., and Bernth, A., \The LMT Transfor-
mational System," Proc. of Proceedings of AMTA-98,
pp. 344{355, 1998.
[8] OpenTag, \A Standard Extraction/Abstraction Text
Format for Translation and NLP Tools,"
(http://www.opentag.org/)
[9] Takeda, K., \Pattern-Based Machine Translation,"
Proc. of 16th COLING, Vol. 2, pp. 1155{1158, August
1996.
[10] TEI, \Text Encoding Initiative (TEI),"
(http://www.uic.edu:80/orgs/tei/)
[11] Watanabe, H., \A Method for Accelerating CFG-
Parsing by Using Dependency Information," Proc. of
18th COLING, 2000.
[12] Watanabe, H., Nagao, K., McCord, M. C., and Bernth,
A., \Improving Natural Language Processing by Lin-
guistic Document Annotation," Proc. of COLING 2000
Workshop for Semantic Annotation and Intelligent Con-
tent, pp. 20{27, 2000.
Deeper Sentiment Analysis
Using Machine Translation Technology
KANAYAMA Hiroshi NASUKAWA Tetsuya
WATANABE Hideo
Tokyo Research Laboratory, IBM Japan, Ltd.
1623-14 Shimotsuruma, Yamato-shi, Kanagawa-ken, 242-8502 Japan
{hkana,nasukawa,hiwat}@jp.ibm.com
Abstract
This paper proposes a new paradigm for senti-
ment analysis: translation from text documents
to a set of sentiment units. The techniques of
deep language analysis for machine translation
are applicable also to this kind of text mining
task. We developed a high-precision sentiment
analysis system at a low development cost, by
making use of an existing transfer-based ma-
chine translation engine.
1 Introduction
Sentiment analysis (SA) (Nasukawa and Yi, 2003; Yi
et al, 2003) is a task to obtain writers? feelings as ex-
pressed in positive or negative comments, questions,
and requests, by analyzing large numbers of docu-
ments. SA is becoming a useful tool for the com-
mercial activities of both companies and individual
consumers, because they want to sort out opinions
about products, services, or brands that are scat-
tered in online texts such as product review articles,
replies given to questionnaires, and messages in bul-
letin boards on the WWW.
This paper describes a method to extract a set
of sentiment units from sentences, which is the key
component of SA. A sentiment unit is a tuple of
a sentiment1, a predicate, and its arguments. For
example, these sentences in a customer?s review of
a digital camera (1) contained three sentiment units
(1a), (1b), and (1c). Apparently these units indicate
that the camera has good features in its lens and
recharger, and a bad feature in its price.
It has excellent lens, but the price is too high.
I don?t think the quality of the recharger
has any problem.
?
?
? (1)
[favorable] excellent (lens) (1a)
[unfavorable] high (price) (1b)
[favorable] problematic+neg (recharger) (1c)
The extraction of these sentiment units is not a
trivial task because many syntactic and semantic op-
erations are required. First, the structure of a pred-
icate and its arguments may be changed from the
1Possible values of a sentiment are ?favorable?, ?unfavor-
able?, ?question?, and ?request?. In this paper the discussion
is mostly focused on the first two values.
syntactic form as in (1a) and (1c). Also modal, as-
pectual, and negation information must be handled,
as in (1c). Second, a sentiment unit should be con-
structed as the smallest possible informative unit so
that it is easy to handle for the organizing processes
after extraction. In (1b) the degree adverb ?too? is
omitted to normalize the expression. For (1c), the
predicate ?problematic? has the argument ?recharger?
instead of the head word of the noun phrase ?the
quality of the recharger?, because just using ?qual-
ity? is not informative to describe the sentiment of
the attribute of a real-world object. Moreover, dis-
ambiguation of sentiments is necessary: in (1b) the
adjective ?high? has the ?unfavorable? feature, but
?high? can be treated as ?favorable? in the expression
?resolution is high?.
We regard this task as translation from text to
sentiment units, because we noticed that the deep
language analysis techniques which are required for
the extraction of sentiment units are analogous to
those which have been studied for the purpose of
language translation. We implemented an accurate
sentiment analyzer by making use of an existing
transfer-based machine translation engine (Watan-
abe, 1992), replacing the translation patterns and
bilingual lexicons with sentiment patterns and a sen-
timent polarity lexicon. Although we used many
techniques for deep language analysis, the system
was implemented at a surprisingly low development
cost because the techniques for machine translation
could be reused in the architecture described in this
paper.
We aimed at the high precision extraction of senti-
ment units. In other words, our SA system attaches
importance to each individual sentiment expression,
rather than to the quantitative tendencies of repu-
tation. This is in order to meet the requirement of
the SA users who want to know not only the over-
all goodness of an object, but also the breakdown of
opinions. For example, when there are many posi-
tive opinions and only one negative opinion, the neg-
ative one should not be ignored because of its low
percentage, but should be investigated thoroughly
since valuable knowledge is often found in such a
minority opinion. Figure 1 illustrates an image of
the SA output. The outliner organizes positive and
negative opinions by topic words, and provides ref-
erences to the original text.
Favorable Unfavorable
battery long life - battery (3)
good - battery (2)
:
not good - battery (1)
s
lens
:
nice - lens (2)
:
(original document)
When I bought this camera,
I thought the battery
was not good, but the
problem was solved after
I replaced it with new one.
Figure 1: An image of an outliner which uses SA output.
Users can refer to the original text by clicking on the
document icons.
MT SA
Japanese
sentence
?parser
Japanese
tree structure
? transfer j
English
tree structure
Sentiment
fragments
? generator ?English
sentence
Sentiment
units
Transfer
patterns
Fragment
patterns
Bilingual
lexicon
Polarity
lexicon
Figure 2: The concept of the machine translation en-
gine and the sentiment analyzer. Some components are
shared between them. Also other components are similar
between MT and SA.
This means that the approach for SA should be
switched from the rather shallow analysis techniques
used for text mining (Hearst, 1999; Nasukawa and
Nagano, 2001), where some errors can be treated as
noise, into deep analysis techniques such as those
used for machine translation (MT) where all of the
syntactic and semantic phenomena must be handled.
We implemented a Japanese SA system using a
Japanese to English translation engine. Figure 2 il-
lustrates our SA system, which utilizes a MT engine,
where techniques for parsing and pattern matching
on the tree structures are shared between MT and
SA.
Section 2 reviews previous studies of sentiment
analysis. In Section 3 we define the sentiment unit
to be extracted for sentiment analysis. Section 4
presents the implementation of our system, compar-
ing the operations and resources with those used
for machine translation. Our system is evaluated
in Section 5. In the rest of paper we mainly use
Japanese examples because some of the operations
depend on the Japanese language, but we also use
English examples to express the sentiment units and
some language-independent issues, for understand-
ability.
2 Previous work on Sentiment
Analysis
Some prior studies on sentiment analysis focused on
the document-level classification of sentiment (Tur-
ney, 2002; Pang et al, 2002) where a document
is assumed to have only a single sentiment, thus
these studies are not applicable to our goal. Other
work (Subasic and Huettner, 2001; Morinaga et al,
2002) assigned sentiment to words, but they relied
on quantitative information such as the frequencies
of word associations or statistical predictions of fa-
vorability.
Automatic acquisition of sentiment expressions
have also been studied (Hatzivassiloglou and McKe-
own, 1997), but limited to adjectives, and only one
sentiment could be assigned to each word.
Yi et al (2003) pointed out that the multiple
sentiment aspects in a document should be ex-
tracted. This paper follows that approach, but ex-
ploits deeper analysis in order to avoid the analytic
failures reported by Nasukawa and Yi (2003), which
occurred when they used a shallow parser and only
addressed a limited number of syntactic phenomena.
In our in-depth approach described in the next sec-
tion, two types of errors out of the four reported by
Nasukawa and Yi (2003) were easily removed2.
3 Sentiment Unit
This section describes the sentiment units which are
extracted from text, and their roles in the sentiment
analysis and its applications.
A sentiment unit consists of a sentiment, a predi-
cate, its one or more arguments, and a surface form.
Formally it is expressed as in Figure 3.
The ?sentiment? feature categorizes a sentiment
unit into four types: ?favorable? [fav], ?unfavorable?
[unf], ?question? [qst], and ?request? [req]. A predi-
cate is a word, typically a verb or an adjective, which
conveys the main notion of the sentiment unit. An
argument is also a word, typically a noun, which
modifies the predicate with a case postpositional in
Japanese. They roughly correspond to a subject and
an object of the predicate in English.
For example, from the sentence (2)3, the extracted
sentiment unit is (2a).
ABC123-ha renzu-ga subarashii.
ABC123-TOPIC lens-NOM excellent
?ABC123 has an excellent lens.?
(2)
[fav] excellent ? ABC123, lens ? (2a)
The sentiment unit (2a) stands for the sentiment
is ?favorable?, the predicate is ?excellent? and its ar-
guments are ?ABC123? and ?lens?. In this case, both
?ABC123? and ?lens? are counted as words which are
associated with a favorable sentiment. Arguments
are used as the keywords in the outliner, as in the
leftmost column in Figure 1. Predicates with no ar-
gument are ignored, because they have no effects on
the view and often become noise.
2Though this paper handles Japanese SA, we also imple-
mented an English version of SA using English-French trans-
lation techniques, and that system solved the problems which
were mentioned in Nasukawa and Yi?s paper.
3?ABC123? is a fictitious product name.
<sentiment unit> ::= <sentiment> <predicate> <argument>+ <surface>
<sentiment> ::= favorable | unfavorable | question | request
<predicate> ::= <word> <feature>*
<argument> ::= <word> <feature>*
<surface> ::= <string>
Figure 3: The definition of a sentiment unit.
The predicate and its arguments can be different
from the surface form in the original text. Seman-
tically similar representations should be aggregated
to organize extracted sentiments, so the examples in
this paper use English canonical forms to represent
predicates and arguments, while the actual imple-
mentation uses Japanese expressions.
Predicates may have features, such as negation,
facility, difficulty, etc. For example, ?ABC123
doesn?t have an excellent lens.? brings a sentiment
unit ?[unf] excellent+neg ? ABC123, lens ??. Also
the facility/difficulty feature affects the sentiments
such as ?[unf] break+facil? for ?easy to break? and
?[unf] learn+diff? ?difficult to learn?.
The surface string is the corresponding part in the
original text. It is used for reference in the view of
the output of SA, because the surface string is the
most understandable notation of each sentiment unit
for humans.
We use the term sentiment polarity for the se-
lection of the two sentiments [fav] and [unf]. The
other two sentiments, [qst] and [req] are important
in applications, e.g. the automatic creation of FAQ.
Roughly speaking, [qst] is extracted from an inter-
rogative sentence, and [req] is used for imperative
sentences or expressions such as ?I want ...? and
?I?d like you to ...?. From a pragmatic point of view
it is difficult to distinguish between them4, but we
classify them using simple rules.
4 Implementation
This section describes operations and resources de-
signed for the extraction of sentiment units. There
are many techniques analogous to those for machine
translation, so first we show the architecture of the
transfer-based machine translation engine which is
used as the basis of the extraction of sentiment units.
4.1 Transfer-based Machine Translation
Engine
As illustrated on the left side of Figure 2, the
transfer-based machine translation system consists
of three parts: a source language syntactic parser,
a bilingual transfer which handles the syntactic tree
structures, and a target language generator. Here
the flow of the Japanese to English translation is
shown with the following example sentence (3).
4For example, the interrogative sentence ?Would you read
it?? implies a request.
kare hon ki
iru
watashi
ha wo ni
no
Figure 4: The Japanese syntactic tree for the sen-
tence (3).
Kare-ha watashi-no
He-TOPIC I-GEN
hon-wo ki-ni iru.
book-ACC mind-DAT enter
?He likes my book.?
(3)
First the syntactic parser parses the sentence (3)
to create the tree structure as shown in Figure 4.
Next, the transfer converts this Japanese parse
tree into an English one by applying the translation
patterns as in Figure 5. A translation pattern con-
sists of a tree of the source language, a tree of the
target language, and the word correspondences be-
tween both languages.
The patterns (a) and (b) in Figure 5 match with
the subtrees in Figure 4, as Figure 6 illustrates.
This matching operation is very complicated because
there can be an enormous number of possible combi-
nations of patterns. The fitness of the pattern com-
binations is calculated according to the similarity of
the source tree and the left side of the translation
pattern, the specificity of the translation pattern,
and so on. This example also shows the process
of matching the Japanese case markers (postposi-
tional particles). The source tree and the pattern
(a) match even though the postpositional particles
are different (?ha? and ?ga?). This process may be
much more complicated when a verb is transformed
into special forms e.g. passive or causative. Besides
this there are many operations to handle syntactic
and semantic phenomena, but here we take them for
granted because of space constraints.
Now the target fragments have been created as in
Figure 6, using the right side of the matched trans-
lation patterns as in Figure 5. The two fragments
are attached at the shared node ? noun2 ?, and lexi-
calized by using the bilingual lexicon. Finally the
target sentence ?He likes my book.? is generated by
the target language generator.
iru
noun noun ki
ga wo ni
like
noun noun
SUBJ OBJ
(a)
noun
no
watashi
noun
my
(b)
Figure 5: Two examples of Japanese-English trans-
lation patterns. The left side and the right side are
Japanese and English syntactic trees, respectively.
The ? noun ? works as a wildcard which matches with
any noun. Curves stand for correspondences be-
tween Japanese and English words.
kare hon ki
iru
watashi
ha wo ni
no
(a)
(b)
like
noun1 noun2
SUBJ OBJ
noun2
my
Figure 6: Transferring the Japanese tree in Figure 4
into the English tree. The patterns in Figure 5 create
two English fragments, and they are attached at the
nodes ? noun2 ? which share the same correspondent
node in the source language tree.
4.2 Techniques Required for Sentiment
Analysis
Our aim is to extract sentiment units with high pre-
cision. Moreover, the set of arguments of each pred-
icate should be selected necessarily and sufficiently.
Here we show that the techniques to meet these re-
quirements are analogous to the techniques for ma-
chine translation which have been reviewed in Sec-
tion 4.1.
4.2.1 Full parsing and top-down tree
matching
Full syntactic parsing plays an important role to ex-
tract sentiments correctly, because the local struc-
tures obtained by a shallow parser are not always
reliable. For example, expressions such as ?I don?t
think X is good?, ?I hope that X is good? are not fa-
vorable opinions about X, even though ?X is good?
appears on the surface. Therefore we use top-down
pattern matching on the tree structures from the full
parsing in order to find each sentiment fragment,
that is potentially a part of a sentiment unit.
In our method, initially the top node is examined
to see whether or not the node and its combination
of children nodes match with one of the patterns
in the pattern repository. In this top-down manner,
the nodes ?don?t think? and ?hope? in the above ex-
amples are examined before ?X is good?, and thus
the above expressions won?t be misunderstood to ex-
press favorable sentiments.
There are three types of patterns: principal pat-
terns, auxiliary patterns, and nominal patterns. Fig-
ure 7 illustrates examples of principal patterns: the
noun
warui
ga [unf]
bad ? noun ?
(c)
noun
iru
ki
wo ni [fav]
like ? noun ?
(d)
Figure 7: Examples of principal patterns.
declinable
to
omowa
nai
unit
+neg
(e)
declinable
monono
declinable
unit
unit
(f)
Figure 8: Examples of auxiliary patterns.
? declinable ? denotes a verb or an adjective in
Japanese. Note that the two unit s on the right side
of (f) are not connected. This means two separated
sentiment units can be obtained.
pattern (c) converts a Japanese expression ? noun -
ga warui? to a sentiment unit ?[unf] bad ? noun ??.
The pattern (d) converts an expression ? noun -wo
ki-ni iru? to a sentiment unit ?[fav] like ? noun ??,
where the subject (the noun preceding the postpo-
sitional ga) is excluded from the arguments because
the subject of ?like? is usually the author, who is not
the target of sentiment analysis.
Another type is the auxiliary pattern, which ex-
pands the scope of matching. Figure 8 has two
examples. The pattern (e) matches with phrases
such as ?X-wa yoi-to omowa-nai. ((I) don?t think
X is good.)? and produces a sentiment unit with the
negation feature. When this pattern is attached to
a principal pattern, its favorability is inverted. The
pattern (f) allows us to obtain two separate senti-
ment units from sentences such as ?Dezain-ga warui-
monono, sousasei-ha yoi. (The design is bad, but the
usability is good.)?.
4.2.2 Informative noun phrase
The third type of pattern is a nominal pattern. Fig-
ure 9 shows three examples. The pattern (g) is used
to avoid a formal noun (nominalizer) being an argu-
ment. Using this pattern, from the sentence ?Kawaii
no-ga suki-da. ((I) like pretty things)?, ?[fav] like
? pretty ?? can be extracted instead of ?[fav] like
? thing ??. The pattern (h) is used to convert a
noun phrase ?renzu-no shitsu (quality of the lens)?
into just ?lens?. Due to this operation, from Sen-
tence (4), an informative sentiment unit (4a) can be
obtained instead of a less informative one (4b).
Renzu-no shitsu-ga yoi.
lens-GEN quality-NOM good
?The quality of the lens is good.?
(4)
[fav] good ? lens ? (4a)
? [fav] good ? quality ? (4b)
adj
no
adj
(g)
noun
no
shitsu
noun
(h)
noun
noun
noun
noun
(i)
Figure 9: Examples of nominal patterns.
The pattern (i) is for compound nouns such as
?juuden jikan (recharging time)?. A sentiment
unit ?long ? time ?? is not informative, but ?long
? recharging time ?? can be regarded as a [unf] sen-
timent.
4.2.3 Disambiguation of sentiment polarity
Some adjectives and verbs may be used for both fa-
vorable and unfavorable predicates. This variation
of sentiment polarity can be disambiguated natu-
rally in the same manner as the word sense dis-
ambiguation in machine translation. The adjective
?takai (high)? is a typical example, as in (5a) and
(5b). In this case the sentiment polarity depends on
the noun preceding the postpositional particle ?ga?:
favorable if the noun is ?kaizoudo (resolution)?, unfa-
vorable if the noun is a product name. The semantic
category assigned to a noun holds the information
used for this type of disambiguation.
Kaizoudo-ga takai.
resolution-NOM high
?The resolution is high.?
? [fav] (5a)
ABC123-ga takai.
ABC123-NOM high (price)
?ABC123 is expensive.?
? [unf] (5b)
4.2.4 Aggregation of synonymous
expressions
In contrast to disambiguation, aggregation of syn-
onymous expressions is important to organize ex-
tracted sentiment units. If the different expressions
which convey the same (or similar) meanings are
aggregated into a canonical one, the frequency in-
creases and one can easily find frequently mentioned
opinions.
Using the translation architecture, any forms can
be chosen as the predicates and arguments by ad-
justing the patterns and lexicons. That is, monolin-
gual word translation is done in our method.
4.3 Resources for Sentiment Analysis
We prepared the following resources for sentiment
analysis:
Principal patterns: The verbal and adjectival
patterns for machine translation were converted
to principal patterns for sentiment analysis.
The left sides of the patterns are compatible
with the source language parts of the original
patterns, so we just assigned a sentiment po-
larity to each word. A total of 3752 principal
patterns were created.
Auxiliary/Nominal patterns: A total of 95 aux-
iliary patterns and 36 nominal patterns were
created manually.
Polarity lexicon: Some nouns were assigned sen-
timent polarity, e.g. [unf] for ?noise?. This po-
larity is used in expressions such as ?... ga ooi.
(There are many ...)?. This lexicon is also used
for the aggregation of words.
Some patterns and lexicons are domain-
dependent. The situation is the same as in
machine translation. Fortunately the translation
engine used here has a function to selectively use
domain-dependent dictionaries, and thus we can
prepare patterns which are especially suited for the
messages on bulletin boards, or for the domain of
digital cameras. For example, ?The size is small.?
is a desirable feature of a digital camera. We can
assign the appropriate sentiment (in this case, [fav])
by using a domain-specific principal pattern.
5 Evaluation
We conducted two experiments on the extraction of
sentiment units from bulletin boards on the WWW
that are discussing digital cameras. A total of 200
randomly selected sentences were analyzed by our
system. The resources were created by looking at
other parts of the same domain texts, and therefore
this experiment is an open test.
Experiment 1 measured the precision of the sen-
timent polarity, and Experiment 2 evaluated the in-
formativeness of the sentiment units. In this section
we handled only the sentiments [fav] and [unf] senti-
ments, thus the other two sentiments [qst] and [req]
were not evaluated.
5.1 Experiment 1: Precision and Recall
In order to see the reliability of the extracted sen-
timent polarities, we evaluated the following three
metrics:
Weak precision: The coincidence rate of the senti-
ment polarity between the system?s output and
manual output when both the system and the
human evaluators assigned either a favorable or
unfavorable sentiment.
Strong precision: The coincidence rate of the sen-
timent polarity between the system?s output
and manual output when the system assigned
either a favorable or unfavorable sentiment.
Recall: The detection rate of sentiment units
within the manual output.
These metrics are measured by using two meth-
ods: (A) our proposed method based on the machine
translation engine, and (B) the lexicon-only method,
which emulates the shallow parsing approach. The
latter method used the simple polarity lexicon of ad-
jectives and verbs, where an adjective or a verb had
only one sentiment polarity, then no disambigua-
tion was done. Except for the direct negation of
(A) MT (B) Lexicon only
Weak prec. 100% (31/31) 80% (41/51)
Strong prec. 89% (31/35) 44% (41/93)
Recall 43% (31/72) 57% (41/72)
Table 1: Precision and recall for the extraction of
sentiment units from 200 sentences.
(A) MT
Manual
f n u
f 20 3 0
Sy
ste
m
n 27 - 14
u 0 1 11
(B) Lexicon only
Manual
f n u
f 26 19 6
Sy
ste
m
n 14 - 7
u 4 23 15
Table 2: The breakdown of the results of Experi-
ment 1. The columns and rows show the manual
output and the system output, respectively (f: favor-
able, n: non-sentiment, u: unfavorable). The sum of
the bold numbers equals the numerators of the pre-
cision and recall.
an adjective or a verb5, no translation patterns were
used. Instead of the top-down pattern matching,
sentiment units were extracted from any part of the
tree structures (the results of full-parsing were used
also here).
Table 1 shows the results. With the MT frame-
work, the weak precision was perfect, and also the
strong precision was much higher, while the recall
was lower than for the lexicon-only method. Their
breakdowns in the two parts of Table 2 indicate that
most of errors where the system wrongly assigned
either of sentiments (i.e. human regarded an expres-
sion as non-sentiment) have been reduced with the
MT framework.
All of the above results are consistent with intu-
ition. The MT method outputs a sentiment unit
only when the expression is reachable from the root
node of the syntactic tree through the combina-
tion of sentiment fragments, while the lexicon-only
method picks up sentiment units from any node in
the syntactic tree. The sentence (6) is an exam-
ple where the lexicon-only method output the wrong
sentiment unit (6a). The MT method did not out-
put this sentiment unit, thus the precision values of
the MT method did not suffer from this example.
... gashitsu-ga kirei-da-to iu hyouka-ha
uke-masen-deshi-ta. (6)
?There was no opinion that the picture was sharp.?
? [fav] clear ? picture ? (6a)
In the lexicon-only method, some errors occurred
due to the ambiguity in sentiment polarity of an ad-
jective or a verb, e.g. ?Kanousei-ga takai. (Capa-
bilities are high.)? since ?takai (high/expensive)? is
always assigned the [unf] feature.
5?He doesn?t like it.? is regarded as negation, but ?I don?t
think it is good.? is not.
declinable
noun noun noun
ga wo ni
Figure 10: A na??ve predicate-argument structure
used by the system (C). Nouns preceding three ma-
jor postpositional particles ?ga?, ?wo?, and ?ni? are
supported as the slots of arguments. On the other
hand, in the system (A), there are over 3,000 prin-
cipal patterns that have information on appropriate
combinations for each verb and adjective.
(A) MT (C) Na??ve
Less redundant 2/35 0/35
More informative 13/35 1/35
Both 1/35 0/35
Table 3: Comparison of scope of sentiment units.
The numbers mean the counts of the better output
for each system among 35 sentiment units. The re-
mainder is the outputs that were the same in both
systems.
The recall was not so high, especially in the MT
method, but according to our error analysis the re-
call can be increased by adding auxiliary patterns.
On the other hand, it is almost impossible to increase
the precision without our deep analysis techniques.
Consequently, our proposed method outperforms the
shallow (lexicon-only) approach.
5.2 Experiment 2: Scope of Sentiment Unit
We also compared the appropriateness of the scope
of the extracted sentiment units between (A) the
proposed method with the MT framework and
(C) a method that supports only na??ve predicate-
argument structures as in Figure 10 and doesn?t use
any nominal patterns.
According to the results shown in Table 3, the MT
method produced less redundant or more informa-
tive sentiment units than did relying on the na??ve
predicate-argument structures in about half of the
cases among the 35 extracted sentiment units.
The following example (7) is a case where the sen-
timent unit output by the MT method (7a) was less
redundant than that output by the na??ve method
(7b). The translation engine understood that the
phrase ?kyonen-no 5-gatsu-ni (last May)? held tem-
poral information, therefore it was excluded from
the arguments of the predicate ?enhance?, while both
?function? and ?May? were the arguments of ?enhance?
in (7b). Apparently the argument ?May? is not nec-
essary here.
... kyonen-no 5-gatsu-ni kinou-ga
kairyou-sare-ta you-desu. (7)
?It seems the function was enhanced last May.?
[fav] enhance ? function ? (7a)
? [fav] enhance ? function, May ? (7b)
Example (8) is another case where the sentiment
unit output by the MT method (8a) was more infor-
mative than that output by the na??ve method (8b).
Than the Japanese functional noun ?hou?, its modi-
fier ?zoom? was more informative. The MT method
successfully selected the noun ?zoom? as the argu-
ment of ?desirable?.
... zuum-no hou-ga nozomashii. (8)
?A zoom is more desirable.?
[fav] desirable ? zoom ? (8a)
? [fav] desirable ? hou ? (8b)
The only one case we encountered where the
MT method extracted a less informative sentiment
unit was the sentence ?Botan-ga satsuei-ni pittari-
desu (The shutter is suitable for taking photos)?.
The na??ve method could produce the sentiment unit
?[fav] suitable ? shutter, photo ??, but the MT
method created ?[fav] suitable ? shutter ??. This is
due to the lack of a noun phrase preceding the post-
positional particle ?ni? in the principal pattern. Such
problems can be avoided by modifying the patterns,
and thus the effect of the combination of patterns
for SA has been shown here.
6 Conclusion
This paper has proposed a new approach to senti-
ment analysis: the translation from text to a set of
semantic fragments. We have shown that the deep
syntactic and semantic analysis makes possible the
reliable extraction of sentiment units, and the out-
lining of sentiments became useful because of the
aggregation of the variations in expressions, and the
informative outputs of the arguments. The experi-
mental results have shown that the precision of the
sentiment polarity was much higher than for the con-
ventional methods, and the sentiment units created
by our system were less redundant and more infor-
mative than when using na??ve predicate-argument
structures. Even though we exploited many advan-
tages of deep analysis, we could create a sentiment
analysis system at a very low development cost, be-
cause many of the techniques for machine translation
can be reused naturally when we regard the extrac-
tion of sentiment units as a kind of translation.
Many techniques which have been studied for the
purpose of machine translation, such as word sense
disambiguation (Dagan and Itai, 1994; Yarowsky,
1995), anaphora resolution (Mitamura et al, 2002),
and automatic pattern extraction from corpora
(Watanabe et al, 2003), can accelerate the further
enhancement of sentiment analysis, or other NLP
tasks. Therefore this work is the first step towards
the integration of shallow and wide NLP, with deep
NLP.
References
Ido Dagan and Alon Itai. 1994. Word sense dis-
ambiguation using a second language monolingual
corpus. Computational Linguistics, 20(4):563?
596.
Vasileios Hatzivassiloglou and Kathleen R. McKe-
own. 1997. Predicting the semantic orientation
of adjectives. In Proceedings of the 35th Annual
Meeting of the ACL and the 8th Conference of the
European Chapter of the ACL, pages 174?181.
Marti A. Hearst. 1999. Untangling text data min-
ing. In Proc. of the 37th Annual Meeting of the
Association for Computational Linguistics.
Teruko Mitamura, Eric Nyberg, Enrique Torrejon,
Dave Svoboda, Annelen Brunner, and Kathryn
Baker. 2002. Pronominal anaphora resolution in
the kantoo multilingual machine translation sys-
tem. In Proc. of TMI 2002, pages 115?124.
Satoshi Morinaga, Kenji Yamanishi, Kenji Tateishi,
and Toshikazu Fukushima. 2002. Mining product
reputations on the web. In Proc. of the 8th ACM
SIGKDD Conference.
Tetsuya Nasukawa and Tohru Nagano. 2001. Text
analysis and knowledge mining system. IBM Sys-
tems Journal, 40(4):967?984.
Tetsuya Nasukawa and Jeonghee Yi. 2003. Senti-
ment analysis: Capturing favorability using nat-
ural language processing. In Proc. of the Second
International Conferences on Knowledge Capture,
pages 70?77.
Bo Pang, Lillian Lee, and Shivakumar
Vaithyanathan. 2002. Thumbs up? Sentiment
classification using machine learning techniques.
In Proceedings of the 2002 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 79?86.
Pero Subasic and Alison Huettner. 2001. Affect
analysis of text using fuzzy semantic typing. IEEE
Trans. on Fussy Systems.
Peter D. Turney. 2002. Thumbs up or thumbs
down? Semantic orientation applied to unsuper-
vised classification of reviews. In Proc. of the 40th
ACL Conf., pages 417?424.
Hideo Watanabe, Sadao Kurohashi, and Eiji Ara-
maki. 2003. Finding translation patterns from de-
pendency structures. In Michael Carl and Andy
Way, editors, Recent Advances in Example-based
Machine Translation, pages 397?420. Kluwer Aca-
demic Publishers.
Hideo Watanabe. 1992. A similarity-driven trans-
fer system. In Proc. of the 14th COLING, Vol. 2,
pages 770?776.
David Yarowsky. 1995. Unsupervised word sense
disambiguation rivaling supervised methods. In
Meeting of the Association for Computational
Linguistics, pages 189?196.
Jeonghee Yi, Tetsuya Nasukawa, Razvan Bunescu,
and Wayne Niblack. 2003. Sentiment analyzer:
Extracting sentiments about a given topic using
natural language processing techniques. In Pro-
ceedings of the Third IEEE International Confer-
ence on Data Mining, pages 427?434.
