Proceedings of the SIGDIAL 2013 Conference, pages 21?30,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Toward a Better Understanding of Causality between Verbal Events:
Extraction and Analysis of the Causal Power of Verb-Verb Associations
Mehwish Riaz and Roxana Girju
Department of Computer Science and Beckman Institute
University of Illinois at Urbana-Champaign
Urbana, IL 61801, USA
{mriaz2, girju}@illinois.edu
Abstract
The identification of causal relations be-
tween verbal events is important for
achieving natural language understanding.
However, the problem has proven notori-
ously difficult since it is not clear which
types of knowledge are necessary to solve
this challenging problem close to human
level performance. Instead of employing a
large set of features proved useful in other
NLP tasks, we split the problem in smaller
sub problems. Since verbs play a very im-
portant role in causal relations, in this pa-
per we harness, explore, and evaluate the
predictive power of causal associations of
verb-verb pairs. More specifically, we pro-
pose a set of knowledge-rich metrics to
learn the likelihood of causal relations be-
tween verbs. Employing these metrics, we
automatically generate a knowledge base
(KBc) which identifies three categories
of verb pairs: Strongly Causal, Ambigu-
ous, and Strongly Non-causal. The knowl-
edge base is evaluated empirically. The re-
sults show that our metrics perform signif-
icantly better than the state-of-the-art on
the task of detecting causal verbal events.
1 Introduction
The identification of semantic relations between
events is a mandatory component of natural lan-
guage understanding. In this paper, we focus
on the identification of causal relations between
events represented by verbs. Following Riaz and
Girju (2010), we define a verbal event evi as
?[subjectvi] vi [objectvi]?, where the subject and
object of the verb may or may not be explicitly
present in an instance. Consider the following ex-
amples:
1. Yoga builds stamina because you maintain your poses
for a certain period of time. (CAUSE (emaintain, ebuild))
2. The monster storm Katrina raged ashore along the
Gulf Coast Monday morning. There were early re-
ports of buildings collapsing along the coast. (CAUSE
(erage, ecollapse))
In example 1, the two bold events are causally
connected by an explicit and unambiguous dis-
course marker (because). However, in English,
not all discourse markers unambiguously iden-
tify causality (Prasad et al, 2008) - for exam-
ple, Bethard and Martin (2008) proposed a cor-
pus of 1000 causal and non-causal event pairs con-
joined by the marker and. Even more, causal re-
lations can be encoded by implicit contexts - i.e.,
those where no discourse marker is present (ex-
ample 2). Despite the recent achievements ob-
tained in discourse processing, it is still unclear
what types of knowledge can contribute most to-
wards detecting causality in both explicit and im-
plicit contexts (Sporleder and Lascarides, 2008).
The complexity of the task of detecting causality
between events stems from the fact that there are
many factors involved, such as contextual features
of an instance (e.g., lexical items, tenses of verbs,
arguments of verbs, etc.), semantic and pragmatic
features of events, background knowledge, world
knowledge, common sense, etc. Prior approaches
have employed contextual features of an instance
to identify causality between events or discourse
segments (Bethard and Martin, 2008; Pitler and
Nenkova, 2009; Pitler et al, 2009). Although
contextual features provide important knowledge
about sentence(s) in which events appear, humans
also make use of other information such as back-
ground knowledge to comprehend causality. For
instance, in example 2 we use knowledge about
the causal association between verbal entities rage
and collapse to label it with causality.
This research is motivated by the need to extract
and analyze other type of knowledge necessary for
the identification of causal relations between ver-
bal events. We start from the fact that verbs are the
21
main components of language to express events
and semantic relations between events. Thus, in
order to identify and extract causal relations be-
tween events (denoted by (evi , evj )), it is critical
for a model to employ knowledge about the ten-
dency of a verb pair (vi, vj) to encode causation.
For example, the pair (kill, arrest) has a high ten-
dency to encode a cause relation irrespective of the
context in which it is used, thereby a good indica-
tor of causality. The state-of-the-art resources on
verb semantics, such as WordNet, VerbNet, Prop-
Bank, FrameNet, etc. (Miller, 1990; Kipper et al,
2000; Kingsbury et al, 2002; Baker et al, 1998),
provide information about the semantic classes,
thematic roles and selectional restrictions of verbs.
Among these, WordNet is the only resource which
provides information about the cause relation be-
tween verbs, but it has very limited coverage.
For VERBOCEAN, a semi-automatically generated
resource, Chklovski and Pantel (2004) have used
explicit lexical patterns (e.g., ?verb * by verb?) as
means of mining enablement (cause-effect) rela-
tions between verbs. Such approaches help detect-
ing causality with high precision but suffer from
limited coverage due to the highly implicit na-
ture of language. Moreover, such resources do
not provide any information about the likelihood
of a causal relation in verb pairs - e.g., (kill, ar-
rest) has a high tendency to encode cause rela-
tion as compared with the pair (build, maintain).
The pair (build, maintain) seems ambiguous be-
cause it can encode both cause and non-cause re-
lations depending on the context, as shown by ex-
amples 1 and 3. Thus, causality detection models
should employ knowledge about which verb pairs
are strongly causal (non-causal) in nature and for
which pairs the context plays an important role to
signal causality.
3. Republicans had not cut the funds for maintaining the
levee and building up the ecological protections. (NON-
CAUSE)
We propose a fully automated procedure to learn
the likelihood of causal relations in verb pairs. In
this process, we create three categories of verb
pairs: Strongly Causal (Sc), Ambiguous (Ac) and
Strongly Non-causal (S?c). The result is a knowl-
edge base (KBc) of causal associations of verbs.
In KBc, the category Sc (S?c) contains the verb
pairs which have the greatest (least) likelihood to
encode a causal relation, respectively. However,
the category Ac contains ambiguous verb pairs
which have the likelihood to encode both causal
and non-causal relations. The information about
such causal associations provides a rich knowl-
edge source to causality detection models.
The main contributions of our research are as
follows:
? We propose a set of novel metrics (i.e., Explicit
Causal Association (ECA), Implicit Causal As-
sociation (ICA) and Boosted Causal Associa-
tion (BCA)) to identify the likelihood of verb
pairs to encode causality. Our metrics exploit
the information available from a large number
of unlabeled explicit and implicit instances of
verb pairs for this purpose.
? We introduce an automated procedure to build
a training corpus of causal and non-causal
event pairs. This prevents us from the trou-
ble of annotating a large number of event pairs
for cause and non-cause relations. Our metrics
make use of supervision from the training cor-
pus to identify causality in verb pairs. We also
provide a mechanism to determine causal verb
pairs which remain undiscovered due to the is-
sue of training data sparseness.
? We revisit recent approaches employing distri-
butional similarity methods to predict causal-
ity between events (Riaz and Girju, 2010;
Do et al, 2011). The state-of-the-art met-
ric Cause-Effect Association (CEA) (Do et
al., 2011) identifies causality mainly based on
probabilities of verb-verb, verb-argument, and
argument-argument pairs. In comparison with
CEA, our metrics perform significantly better
by improving the prior knowledge about the
causal associations from CEA?s components.
After a brief review of related work in next sec-
tion, we describe our approach for acquisition of
training corpus in section 3. The model for the ex-
traction of causal associations is presented in sec-
tion 4, followed by the evaluation and discussion
in section 5 and conclusion in section 6.
2 Related Work
Causality has long been studied from various
perspectives by philosophers, data-mining re-
searchers and computer scientists (Menzies, 2008;
Woodward, 2008; Suppes, 1970; Silverstein et al,
2000; Pearl, 2000).
In NLP, the problem of detecting causality be-
tween events is a very challenging but less re-
searched topic. Previously, researchers have stud-
22
ied this task by focusing on supervised classifi-
cation models for both verbal and nominal events
(Girju, 2003; Bethard and Martin, 2008). Bethard
and Martin (2008), for example, have focused
mainly on the contextual features available in test
instances of verbal event pairs to predict causality.
They have relied on a small scale dataset of 1000
instances (697 training and 303 test) for this task.
Unlike above models, recently some researchers
have employed unsupervised causality detection
metrics and minimal supervision for this task. For
example, Riaz and Girju (2010) have proposed an
unsupervised metric Effect-Control Dependency
(ECD) to determine causality between events in
news scenarios. Following their model, Do et al
(2011) introduced an improved metric CEA which
uses PMI and some components of ECD to pre-
dict the causal relation in verbal and nominal event
pairs in a text document. They also proposed a
minimally supervised method using explicit dis-
course markers. For example, they used ILP
framework to assign a non-causal relation to all the
event pairs appearing in two discourse segments
connected by a non-causal marker. They evalu-
ated their model on a set of 20 documents, a highly
skewed evaluation set with around 2-3% causal
instances and 58% human inter-annotator agree-
ment on cause-effect relations. On verbal events,
they reported 38.3% F-score with CEA and 1-2%
improvement using minimally supervised method.
As compared with above mentioned metrics, we
introduce knowledge rich association measures
which employ supervision from the automatically
generated training corpus to learn causality.
Several other NLP researchers have studied
related topics e.g., identifying events, building
of temporal chain of events sharing a common
protagonist (participant), predicting future events
and identifying hidden links in news articles to
build a coherent chain (Chambers and Jurafsky,
2008; Chambers and Jurafsky, 2009; Radinsky
and Horvitz, 2013; Shahaf and Guestrin, 2010).
Unlike these tasks, our focus is on identifying
causality between events.
3 Acquisition of Training Corpus
In this section, we propose a fully automated pro-
cedure to build a training corpus of event pairs
which encode cause and non-cause relations. This
training corpus is used in our model to identify the
likelihood of cause relations in verb pairs. As dis-
cussed earlier, previous researchers have worked
with a small scale dataset of annotated event pairs.
The current task requires us to use a large train-
ing corpus to learn the pervasive relation of causal-
ity and the manual generation of such corpus is a
laborious task. Therefore, we decided to depend
on the unambiguous discourse markers because
and but to automatically collect training instances
of cause and non-cause event pairs, respectively.
For example, the marker because in the instance
1 of section 1 encodes a cause relation between
the events ebuild and emaintain. Some researchers
have utilized unambiguous discourse markers to
acquire training instances of semantic relations be-
tween discourse segments (Marcu and Echihabi,
2001; Sporleder and Lascarides, 2008). However,
the process is not simple for the current problem
since it is not always clear how to create a causal
instance of an event pair. Consider the following
meta instance I:
I : <s>/m1 . . . v1 . . . v2 . . . vk . . . because . . . vk+1
. . . vk+2, . . ., vr, . . .m2/</s>.
It is composed of main verbs (v1, v2, . . .,
vr), discourse markers (m1, m2), and sentence
boundaries (<s>, </s>). Here, we assume that
the discourse markers or the sentence boundaries
whichever appear first in I represent the bound-
aries of discourse segments for the marker because
(appendix A contains a table of notations used in
this paper). In I , there are k and r ? k main verbs
appearing before and after because, respectively.
The problem here is to determine the event pair en-
coding causality out of k? (r? k) choices. Here,
we consider that the most dependent pair among
all choices in I is the best candidate to encode
causality.
In this work, we propose the following function
f(I) to pick the most dependent pair:
f(I) = argmax
(vi?mc ,vjmc )
CD(vi, vj)? PSI(vi, vj) (1)
Here, i (j) refers to all verbs that appear be-
fore (after) the causal marker (i.e., mc) because in
I . CD (equation 2) is a component of predicate-
predicate association of CEA (Do et al, 2011)
to determine causal dependency of a pair (vi, vj).
Do et al (2011) used the score CD to determine
causality in an unsupervised fashion but here we
employ this to build a training corpus of causal
event pairs.
CD(vi, vj) = PMI(vi, vj)?max(vi, vj)? IDF (vi, vj) (2)
23
The functions PMI, max and IDF depend on co-
occurrence probabilities and idf scores to deter-
mine causal dependency. Due to space limitations,
for details we refer the reader to Do et al (2011).
Next, we define a novel penalization factor PSI
for the verbs of a pair appearing at greater distance
from the causal marker because. For example, this
assumes the verbs in the pair (v2, vk+2) are less
likely to be in a cause relation as compared with
(vk, vk+1) in I . We came up with this idea because
our initial experiments revealed that the causal in-
stances obtained by penalizing CD with PSI pro-
vide better training for our model as compared to
using only CD for this purpose. The similar be-
havior of reduction in the likelihood of causality
with respect to increase in distance between two
events was observed by Riaz and Girju (2010).
PSI(vi, vj) = ? log
pos(vi) + pos(vj)
2.0? (C(vp) + C(vq))
(3)
Here, C(vp) (C(vq)) is the count of the main
verbs appearing before (after) because, respec-
tively. The distance of the verb is measured in
terms of its position (i.e., pos(vi)) with respect to
because. The position is 1 for the verb closest to
because and 2 for the verb next to the closest verb.
PSI has maximum value for (vk, vk+1) and it re-
duces for other pairs with verbs at greater distance
from because in instance I .
In order to extract non-causal event pairs, we
utilized instances with two discourse segments
conjoined by the marker but which represents
comparison (non-causal) relation. Any event pair
collected from the two discourse segments in non-
causal relation encodes non-causality. Therefore,
we depend on selecting the closest verb pair from
the instances of form I with marker but instead of
because.
In this paper, we present the results produced
using a training corpus of 240K instances (50%
for each class) from the English Gigaword Cor-
pus. In order to prepare this corpus, we identified
discourse markers (i.e., m1, m2), if available, be-
fore and after because/but in each instance I and
assumed that only those markers which have dis-
course usage in I define boundaries of discourse
segments of because/but. We used the list of 100
explicit discourse markers provided by Prasad et
al. (2008) and the supervised approach of Pitler
and Nenkova (2009) to detect markers and the dis-
course versus non-discourse usage of these mark-
ers. We use this training corpus to identify cau-
sation for both explicit and implicit instances of
event pairs. Using this training corpus, a model
tends to give higher causal weights to those in-
stances in which events are connected by the ex-
plicit causal marker because as compared to im-
plicit instances of causation. Thus, to provide fair
supervision to both explicit and implicit instances
of event pairs, we remove the cue words because
and but which were used to automatically label the
training instances.
4 Causal Associations of Verb Pairs
In this section, we explain our approach to learn
the likelihood of causal relations in verb pairs by
exploiting information available from both explicit
and implicit instances of these pairs. We extracted
around 12, 000 documents from the English Gi-
gaword corpus to collect instances of verb pairs
from single sentences (intra-sentential) and adja-
cent sentences (inter-sentential) of text. In this set,
we added instances from 3, 000 articles on news
stories ?Hurricane Katrina? and the ?Iraq war?.
These articles were collected and used to iden-
tify causal relations in news scenarios by Riaz and
Girju (2010). We used these collections because
natural disaster and war-related news articles are
rich in causal events and chains of such events.
In order to identify the causal associations with
high confidence, we decided to apply our model on
those verb pairs which have at least 30 instances
in the above mentioned documents. We acquired
10, 455 such verb pairs. The set of intra- and inter-
sentential instances of these verb pairs is referred
to as the development set for our model.
4.1 Explicit Causal Association (ECA)
In order to find the likelihood of a verb pair to en-
code causal relations, we define our novel metric
Explicit Causal Association (ECA) as follows:
ECA(vi, vj) =
1
| V P |
?
I(vi,vj)?V P
(CD(vi, vj)? CI) (4)
where V P is the set of intra- and inter-sentential
instances (denoted by I(vi, vj)) of the verb pair
(vi, vj), CD determines the causal dependency of
the verb pair in unsupervised fashion (equation 2),
and CI finds the tendency of instance I of (vi, vj)
to belong to the cause class as compared to the
non-cause class using training corpus of event
pairs. The goal of ECA is to combine the unsu-
pervised causal dependency (i.e., CD) with the su-
pervised score of instance I of belonging to cause
24
class than the non-cause one (i.e., CI ). Here, CD
represents the prior knowledge about the causal
association based on co-occurrence probabilities
and idf scores (equation 2). It can discover lots
of false positives because the co-occurrence prob-
abilities can fail to differentiate causality from any
other type of correlation. Therefore, we improve
this prior knowledge with the help of supervision
from the training corpus containing instances of
both cause and non-cause relations. The global
decision of the causal association is made by tak-
ing the average of scores on all the instances con-
taining that verb pair. Notice that CD can also be
moved out from the summation function in equa-
tion 4.
We define the function CI as follows:
CI =
n?
k=1
log( P (fk | c)P (fk | ?c)
) (5)
Here, the notations c and ? c represent cause
and non-cause class, respectively. The notation
fk represents the feature of an instance I . In this
work, we use some language features of events
and context of an instance I which are defined
later in this section. P(fk | c) and P(fk | ?c) are
the smoothed probabilities of feature fk given the
cause and non-cause training instances. The value
of CI is positive only when the instance I has more
tendency to encode a cause relation than a non-
cause one. To avoid negative values, we map CI
scores to the range [0, 1] using CI?CminCmax?Cmin whereCmin (Cmax) is the minimum (maximum) value of
CI obtained on our development set, respectively.
Also, we add a small value  to CI to avoid 0 value.
Similarly, to avoid negative scores of PMI in equa-
tion 2 we can map it to the range [0,1].
We present below the features for the calcula-
tion of CI . We use lexical, syntatic and semantic
features on verbs and verb phrases of both events
of a pair. These features include words, lemmas,
part-of-speech tags, all senses from WordNet for
the verbs and the lexical items of verb phrases.
These features were introduced by Bethard and
Martin (2008) (for an in-depth description of these
features see Bethard and Martin (2008)). Next, we
describe the set of features which are the contribu-
tions of this research.
1. Verbs Arguments: Words, lemma, part-of-
speech tags and all senses from WordNet for
subject and object of verbs of both events.
2. Verbs and Arguments Pairs: For this fea-
ture, we take the cross product of both
events of a pair (evi ,evj ) where evi =
[subjectvi] vi [objectvi] and evj = [subjectvj ]
vj [objectvj ]. Some examples of this fea-
ture are (subjectvi ,subjectvj ), (subjectvi ,vj),
(subjectvi ,objectvj ), etc. In this work, we use
unordered pairs as features (i.e., (vi,vj)) is
same as (vj ,vi)) because the temporal order of
events is unknown for the unlabeled develop-
ment set instances. In future, this feature can
be improved by adding temporal information.
The next three features are taken from the min-
imum relevant context (mincontext) of a verb
pair which we define as follows. mincontext of
a pair (vi, vj) in an intra-sentential instance is
<s>/m1 . . . vi . . . vj . . .m2/</s> ? i.e., words be-
tween the discourse markers (i.e., m1, m2) or sen-
tence boundaries (i.e., <s>, </s>) whichever ap-
pear first in the sentence. The mincontext for the
pair (vi, vj) in an inter-sentential is given below:
<s> / m1 . . . vi . . .m2 / </s>
<s> / m1 . . . vj . . .m2 / </s>
3. Context Words: Lemmas of all words from
mincontext. This feature captures words other
than two events.
4. Context Main Verbs: All main verbs and their
lemmas from mincontext. It collects informa-
tion about all verbs that appear with the causal
and non-causal event pair.
5. Context Main Verb Pairs: The pairs of main
verbs from mincontext. The lemmas are taken
from the feature ?Context Main Verbs? and
then the pairs on these lemmas are used as this
feature. For example, for lemmas of verbs (i.e.,
v1, v2, . . . , vk), pairs (i.e., (v1, v2), (v1, vk),
etc.) are used for this feature. This feature
is used to get information about the interest-
ing causal chains of verbs that may appear in
causal instances.
We propose next a novel metric ICA to avoid
the problem of training data sparsity.
4.2 Implicit Causal Association (ICA)
In order to determine the causal associations us-
ing ECA, we depend on explicit cause and non-
cause training instances for supervision. However,
it is possible that some strongly causal verb pairs
may frequently appear in implicit causal contexts.
Therefore, the causality of such pairs can remain
uncaptured by ECA which merely relies on ex-
plicit training instances. For example, a pair (fall,
25
break) seems strongly causal, but it does not ap-
pear often in our explicit training corpus due to
training data sparsity. Thus, in order to handle
this problem, we propose a new metric called ICA.
This metric makes use of functions for the identi-
fication of roles of events in a cause relation. After
briefly describing the roles of events in causal re-
lations below, we continue with the description of
ICA.
4.2.1 Roles of Events in Cause Relation
Each of the two events in a cause relation can be
assigned either cause or effect role. For example
for the following training instance, the verb ap-
pearing after because represents cause event and
the verb before because represents effect event.
1. Yoga builds stamina because you maintain your poses
for a certain period of time. (Role: rC )
2. Yoga builds stamina because you maintain your poses for
a certain period of time. (Role: rE)
The notation rC and rE represents the classes of
cause and effect role of events, respectively. We
use core features of events to determine the like-
lihood of their roles in causation. These features
include lemma, part-of-speech tag, all senses from
WordNet of both verbs and their arguments (i.e.,
subject and object). Next, we use these features to
handle training data sparseness.
4.2.2 Handling of Training Data Sparsity
To deal with the problem of training data sparsity,
we define the metric ICA as follows:
ICA(vi, vj) =
1
| V P |
?
I(vi,vj)?V P
(CD(vi, vj)? CI
?ERM(evi ,evj )) (6)
where CD and CI are defined earlier and ERM
determines the likelihood of roles of the events in
the cause relation. We remind the reader that CD
is the unsupervised causal dependency of verb pair
and CI is the likelihood of instance I of the verb
pair to belong to the cause class than the non-cause
one using full set of features from section 4.1.
Events Roles Matching (ERM(evi ,evj )) (equa-tions 7 and 8) is the negative log-likelihood of
events evi and evj appearing as cause or effect role
determined using the explicit causal instances of
the training corpus and the core features of events
defined in section 4.2.1.
ERM(evi ,evj ) = ?1.0?max(S(evi , rC) + S(evj , rE),
S(evi , rE) + S(evj , rC)) (7)
S(evi , rC) =
n?
k=1
log(P (fk | rC)) (8)
S(evj , rE) =
n?
k=1
log(P (fk | rE))
Here, S(evi , rC) is the score of evi being the
cause event and S(evj , rE) is the score of evj be-
ing the effect event. These scores are computed
using smoothed probabilities ? i.e., P(fk | rC) and
P(fk | rE). Similarly, S(evi , rE) and S(evj , rC)
are calculated and max is taken. The high value
of ERM represents low matching of an event pair
(verbs and their arguments) in the explicit causal
instances of the training corpus. The high value
of ERM of an event pair can have one of the fol-
lowing two interpretations: (A) it is a non-causal
event pair, or (B) it is a causal event pair but this
pair and the pairs which are semantically closer to
it hardly appear in explicit causal contexts. In the
metric ICA, CI? CD(vi, vj) is used as a guiding
score to interpret ERM as follows:
1. If CI? CD(vi, vj) has high score then the value
of ERM is not penalized by this guiding score
because ERM?s value can be interpreted using
(B) above.
2. If CI? CD(vi, vj) has low score then the value
of ERM is penalized by this guiding score be-
cause (evi , evj ) can be a non-causal pair ac-
cording to the interpretation (A) above.
ICA is a boosting factor to determine causal
verb pairs which remain undiscovered because of
training data sparseness. We also define a Boosted
Causal Association (BCA) metric by adding ICA
to original ECA metric as follows:
BCA(vi, vj) =
1
| V P |
?
I(vi,vj)?V P
(CD(vi, vj)? CI +
CD(vi, vj)? CI ? ERM(evi ,evj )) (9)
To build the knowledge base of causal asso-
ciations (KBc), we generate a ranked list of all
verb pairs based on the likelihood of causality en-
coded by these pairs. Here, we assume that verb
pairs are uniformally distributed across three cat-
egories - i.e., top one-third and bottom one-third
ranked verb pairs belong to Strongly Causal (Sc)
and Strongly Non-Causal (S?c) categories and rest
of the pairs are considered Ambiguous (Ac). Fol-
lowing our assumption, we evaluate this catego-
rization in next section, but in future researchers
can perform empirical study of how to automat-
ically cluster verb pairs into three or more cate-
gories with respect to causation.
26
5 Evaluation and Discussion
In this section, we present our evaluation of
knowledge base to identify causality between ver-
bal events. Specifically we performed experiments
to evaluate (1) the ranking of verb pairs based on
their likelihood of encoding causality, and (2) the
quality of the three categories of verb pairs inKBc
(i.e., Sc, Ac and S?c). For this purpose, we col-
lected two test sets. For each test set, we randomly
selected 50 verb pairs from the list of 10, 455 verb
pairs in KBc. For each verb pair, we selected
randomly 3 intra- and 3 inter-sentential instances
from the English Gigaword corpus and the ?Hur-
ricane Katrina? and ?Iraq war? articles. In order
to keep the development set different from the test
sets, we automatically traversed the development
set to determine if any test instance is available in
it. In case of finding any such test instance, we
removed it from the development set to perform
evaluation on unseen test instances. Two annota-
tors were asked to provide Cause or Non-Cause
labels for each instance. They were provided with
annotation guidelines from the manipulation the-
ory of causality (Woodward, 2008). Given these
guidelines have been successfully used by Riaz
and Girju (2010), we use them here as well. For
ease of annotation, we randomly selected inter-
sentential instances such that the length of each
sentence is at most 40 words.
The human inter-annotator agreement achieved
on Test-set1 (Test-set2) is 90% (88.3%) and the
agreement on the cause class is 70% (62.7%), re-
spectively. The kappa score on Test-set1 (Test-
set2) is 0.75 (0.69), respectively. The Test-set1
(Test-set2) contains 25% (22%) causal instances,
respectively.
We employed Spearman?s rank correlation co-
efficient (equation 10) to compare the ranked list
of verb pairs based on the scores of our metrics
and the rank given by the human annotators. The
score P ranges from +1 to ?1 where +1 and ?1
show strong and negative correlation, respectively.
P = n(
?
xiyi)? (
?
xi)(
?
yi)?
n(
?
x2i )? (
?
xi)2
?
n(
?
y2i )? (
?
yi)2
(10)
Here, n is the total number of verb pairs in the
test set, xi is the human annotation rank and yi is
the metric?s rank of verb pair i of the test set. The
values of xi and yi are determined as follows. For
each verb pair, Ch is calculated which is the num-
ber of cause labels given by both human annota-
Metric CEA ECA ICA BCA
Test-set1 -0.077 0.144 0.427 0.435
Test-set2 0.167 0.217 0.353 0.338
Table 1: The Spearman?s rank correlation coeffi-
cient for the metrics CEA, ECA, ICA and BCA.
Figure 1: The percentage of causal (%c) and non-
causal (%?c) test instances in Sc,Ac and S?c gen-
erated by the metrics CEA, ECA, ICA and BCA.
tors out of 6 instances of a verb pair. The pairs are
ranked in descending order according to the score
Ch s.t. the top scored pair(s) gets rank 50 and the
next to the top pair(s) gets rank 49 and so on. Sim-
ilarly, ranks are given to the verb pairs according
to the metric?s scores. This way of evaluation was
also used by Beamer and Girju (2009) for tempo-
rally ordered adjacent verb pairs. But here, we are
working with verb pairs appearing in any temporal
order in both intra- and inter-sentential instances.
We used ECA, ICA and BCA scores to gener-
ate the ranked list of all verb pairs. In this work,
we also used the state-of-the-art causality iden-
tifier CEA (Do et al, 2011) as baseline metric.
For each verb pair, we computed the likelihood of
causality by taking the average of CEA scores on
all instances of that pair in the development set.
The results with Spearman?s rank correlation
coefficient in Table 1 show that CEA is not very
capable of matching the human ranked list of pairs
as compared with our metrics (i.e., ECA, ICA and
BCA). Specifically, the difference is significant
for Test-set1 where the correlation coefficient with
CEA goes below 0. This behavior of CEA makes
sense because it is unsupervised and requires more
knowledge to perform well. As compared with
ECA, both ICA and BCA perform significantly
better to match human ranking. The Spearman?s
score gain by BCA on Test-set1 is of about 30
(52) points over ECA (CEA) and the gain by ICA
on Test-set2 is of about 13 (18) points over ECA
(CEA), respectively.
In order to explain the behavior of our metrics
27
more clearly, we performed an evaluation of three
categories of verb pairs as follows. We generated
three categories of verb pairs using our metrics and
CEA. We combined two test sets to show the per-
centage of total causal and non-causal instances of
verb pairs that lie in Sc, Ac and S?c using follow-
ing procedure. If a verb pair belongs to Sc and has
3 causal and 2 non-causal instances after human
agreement, then these 5 instances are considered
members of Sc. This step is performed for all verb
pairs in the test set. After this the percentage of
total causal and non-causal test instances are cal-
culated for each category (see Figure 1).
Figure 1 reveals that ICA, BCA and CEA are
successful in pulling more causal instances in Sc
as compared to ECA. But, CEA has a hard time
distinguishing cause from non-cause instances be-
cause it also brings the highest percentage of non-
causal instances in Sc. The reason is the depen-
dence of CEA on PMI scores of pairs of verbs and
arguments to make decision for causality where
PMI is not good enough to distinguish a simple
correlation from an asymmetric relation of causal-
ity. However, ICA and BCA work better by plac-
ing less non-causal instances in Sc as compared
with CEA. ICA and BCA also work better be-
cause by pulling more causal instances in Sc and
Ac, these metrics are keeping least percentage of
causal instances in S?c. Also, ICA and BCA
bring more causal instances in Sc as compared
with ECA by handling training data sparseness.
Another important line of research is the con-
struction of a classifier on top of the component
of knowledge base for the classes of cause and
non-cause relations. This allows us to evaluate our
model in terms of standard evaluation measures -
i.e., precision, recall and F-score. These measures
can also be used to compare our model with su-
pervised classifier depending merely on shallow
contextual features with no information from the
knowledge base. Due to space limitations, we plan
to present such classifiers and evaluation in the fu-
ture.
5.1 Analysis
In this work, we have focused on determining the
predictive power of knowledge of causal associ-
ations of verb pairs to identify causality between
events. Our results reveal that our best metrics
(i.e., ICA and BCA) bring desired behavior of
keeping least percentage of total causal instances
in category S?c. However, there is need to build a
classifier on top of knowledge base which can help
detection of non-causal instances for verb pairs lie
in Sc and Ac. Here, we state some brief details
of our test set which can help building such clas-
sifier in future. An important aspect to consider
is the highly skewed nature of real distribution of
test set. There are only 23.69% causal instances
in the test set and majority of these instances (i.e.,
56.7%) are intra-sentential instances. Therefore, a
classifier should have mechanism to decide why
inter-sentential instances of event pair are non-
causal most of the time. For example, some inter-
sentential events may not even be directly relevant
at first place because they appear in different sen-
tences. Another critical point to consider is the en-
coding of non-causal instances by strongly causal
verb pairs. For example, we asked one of the an-
notators to identify strongly causal verb pairs out
of 100 verb pairs of the test set. There are 22
such verb pairs determined by our annotator and
each of these pairs contain 43% causal instances
on the average. There are many factors (e.g., tem-
poral information, arguments of verbs) which can
make an instance of strongly causal verb pair non-
causal. For example, (call, respond) may encode
causality only if ecall temporally precedes erespond
as demonstrated by the following instances.
1. Deputies spotted the truck parked at the home of the sus-
pect?s father and called for assistance. The Border Patrol
agents and others responded. (CAUSE)
2. Prime Minister of Israel promptly responded to the
widespread unrest in the West Bank and Gaza, saying that
he would call a timeout to rethink Israel?s commitment to
peace talks. (NON-CAUSE)
In future, the above issues need to be addressed
to improve performance for the current task.
6 Conclusion
In this research, we have developed a knowledge
base (KBc1) of causal associations of verb pairs
to detect causality. This resource provides the
causal associations in terms of three categories of
verb pairs (i.e., Strongly Causal, Ambiguous and
Strongly Non-Causal). We have proposed a set of
knowledge rich metrics to learn these associations.
Our analysis of results reveals the biases of differ-
ent metrics and brings important insights into the
future research directions to address the challenge
of detecting causality between verbal events.
1We will make the resource available.
28
References
Collin F. Baker, Charles J. Fillmore and John B. Lowe.
1998. The Berkeley FrameNet project. In proceed-
ings of COLING-ACL. Montreal, Canada.
Brandon Beamer and Roxana Girju. 2009. Using a Bi-
gram Event Model to Predict Causal Potential. In
proceedings of Computational Linguistics and intel-
ligent Text Processing (CICLING), 2009.
Steven Bethard and James H. Martin. 2008. Learning
Semantic Links from a Corpus of Parallel Temporal
and Causal Relations. In proceedings of ACL-08:
HLT.
Nathanael Chambers and Dan Jurafsky. 2008. Unsu-
pervised learning of narrative event chains. In pro-
ceedings of ACL-HLT 2008.
Nathanael Chambers and Dan Jurafsky. 2009. Unsu-
pervised learning of narrative schemas and their par-
ticipants. In proceedings of ACL 2009.
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the Web for Fine-Grained Semantic
Verb Relations. In proceedings of Conference on
Empirical Methods in Natural Language Processing
(EMNLP-04). Barcelona, Spain.
Quang X. Do, Yee S. Chen and Dan Roth. 2011. Min-
imally Supervised Event Causality Identication. In
proceedings of EMNLP-2011.
Roxana Girju. 2003. Automatic detection of causal
relations for Question Answering. Association for
Computational Linguistics ACL, Workshop on Mul-
tilingual Summarization and Question Answering
Machine Learning and Beyond 2003.
Paul Kingsbury, Martha Palmer and Mitch Marcus.
2002. Adding semantic annotation to the Penn Tree-
Bank. In proceedings of HLT-2002. San Diego, Cal-
ifornia.
Karin Kipper, Hoa T. Dang, and Martha Palmer. 2000.
Class-based construction of a verb lexicon. In pro-
ceedings of AAAI-2000. Austin, TX.
Daniel Marcu and Abdessamad Echihabi. 2001. An
unsupervised approach to recognizing discourse re-
lations. In proceedings of the 40th Annual Meet-
ing on Association for Computational Linguistics
(ACL).
Peter Menzies. 2008. Counterfactual theories of cau-
sation. Online Encyclopedia of Philosophy, 2008.
George A. Miller. 1990. WordNet: An online lexi-
cal database. International Journal of Lexicography,
3(4).
Judea Pearl. 2000. Causality. Cambridge University
Press.
Emily Pitler, Annie Louis and Ani Nenkova. 2009.
Automatic Sense Prediction for Implicit Discourse
Relations in Text. In proceedings of ACL-IJCNLP,
2009.
Emily Pitler and Ani Nenkova. 2009. Using Syntax
to Disambiguate Explicit Discourse Connectives in
Text. In proceedings of ACL-IJCNLP, 2009.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, Bonnie Web-
ber. 2010. The penn discourse treebank 2.0. In
proceedings of LREC 2008.
Kira Radinsky and Eric Horvitz. 2013. Mining the
Web to Predict Future Events. In proceedings of
sixth ACM international conference on Web search
and data mining, WSDM ?13.
Mehwish Riaz and Roxana Girju. 2010. Another Look
at Causality: Discovering Scenario-Specific Contin-
gency Relationships with No Supervision. In pro-
ceedings of the IEEE 4th International Conference
on Semantic Computing (ICSC).
Dafna Shahaf and Carlos Guestrin. 2010. Connecting
the Dots Between News Articles. In proceedings of
Knowledge Discovery and Data Mining KDD 2010.
Craig Silverstein, Sergey Brin, Rajeev Motwani and
Jeff Ullman. 2000. Scalable Techniques for Min-
ing Causal Structures. Data Mining and Knowledge
Discovery, 2000, 4(2?3):163?192.
Caroline Sporleder and Alex Lascarides. 2008. Using
automatically labelled examples to classify rhetor-
ical relations: An assessment. Journal of Natural
Language Engineering Volume 14 Issue 3, July 2008
Pages 369?416.
Patrick Suppes. 1970. A Probabilistic Theory of
Causality. Amsterdam: North-Holland Publishing
Company, 1970.
James Woodward. 2008. Causation and Manipulation.
Online Encyclopedia of Philosophy, 2008.
29
Appendix A. Notations
This appendix presents the details of important notations used in this paper.
Notation Equation(s) Explanation
evi 6, 7, 8, 9 Verbal event represented by the verb vi
KBc ? Knowledge base of causal associations of verb pairs
Sc ? Strongly Causal category of verb pairs
Ac ? Ambiguous category of verb pairs
S?c ? Strongly Non-Causal category of verb pairs
mi ? Discourse marker
mc 1 Causal marker (e.g., because)
f(I) 1 Function to select the most dependent pair from two dis-
course segments conjoined with causal marker
CD(vi, vj) 1, 2, 4, 6, 9 Causal dependency of the verb pair (vi, vj)
PSI(vi, vj) 1, 3 Penalization factor for the verbs of the pair (vi, vj) with
respect to their distance from the causal marker
pos(vi) 3 Distance of verb in terms of its position with respect to
causal marker
C(vp) 3 Count of main verbs appearing before causal marker
C(vq) 3 Count of main verbs appearing after causal marker
ECA(vi, vj) 4 Explicit Causal Association of the verb pair (vi, vj)
V P 4, 6, 9 Set of intra- and inter-sentential instances of a verb pair
I(vi, vj) 4, 6, 9 Instance of the verb pair (vi, vj)
CI 4, 5, 6, 9 Tendency of the instance I to belong to cause class than
the non-cause one
c 5 Cause class
?c 5 Non-cause class
Cmin ? Minimum value of CI obtained on the development set
Cmax ? Maximum value of CI obtained on the development set
rC 7, 8 Class of cause role
rE 7, 8 Class of effect role
ICA(vi, vj) 6 Implicit Causal Association of the verb pair (vi, vj)
ERM(evi , evj ) 6, 7 Events Roles Matching (ERM) determines the negativelog-likelihood of events to belong to class of cause or
effect role
S(evi , rC) 8 Score of evi to belong to the class of cause role
S(evj , rE) 8 Score of evj to belong to the class of effect role
P (fk|.) 5, 8 Probability of feature fk given some class
BCA(vi, vj) 9 Boosted Causal Association of the verb pair (vi, vj)
Table 2: Details of notations.
30
Proceedings of the EACL 2014 Workshop on Computational Approaches to Causality in Language, pages 48?57,
Gothenburg, Sweden, April 26, 2014.
c
?2014 Association for Computational Linguistics
Recognizing Causality in Verb-Noun Pairs
via Noun and Verb Semantics
Mehwish Riaz and Roxana Girju
Department of Computer Science and Beckman Institute
University of Illinois at Urbana-Champaign
Urbana, IL 61801, USA
{mriaz2,girju}@illinois.edu
Abstract
Several supervised approaches have been
proposed for causality identification by re-
lying on shallow linguistic features. How-
ever, such features do not lead to improved
performance. Therefore, novel sources
of knowledge are required to achieve
progress on this problem. In this paper,
we propose a model for the recognition of
causality in verb-noun pairs by employing
additional types of knowledge along with
linguistic features. In particular, we fo-
cus on identifying and employing seman-
tic classes of nouns and verbs with high
tendency to encode cause or non-cause re-
lations. Our model incorporates the in-
formation about these classes to minimize
errors in predictions made by a basic su-
pervised classifier relying merely on shal-
low linguistic features. As compared with
this basic classifier our model achieves
14.74% (29.57%) improvement in F-score
(accuracy), respectively.
1 Introduction
The automatic detection of causal relations is im-
portant for various natural language processing ap-
plications such as question answering, text sum-
marization, text understanding and event predic-
tion. Causality can be expressed using various nat-
ural language constructions (Girju and Moldovan,
2002; Chang and Choi, 2006). Consider the fol-
lowing examples where causal relations are en-
coded using (1) a verb-verb pair, (2) a noun-noun
pair and (3) a verb-noun pair.
1. Five shoppers were killed when a car blew up
at an outdoor market.
2. The attack on Kirkuk?s police intelligence
complex sees further deaths after violence
spilled over a nearby shopping mall.
3. At least 1,833 people died in hurricane.
Since, the task of automatic recognition of
causality is quite challenging, researchers have
addressed this problem by considering specific
constructions. For example, various models
have been proposed to identify causation between
verbs (Bethard and Martin, 2008; Beamer and
Girju, 2009; Riaz and Girju, 2010; Do et al., 2011;
Riaz and Girju, 2013) and between nouns (Girju
and Moldovan, 2002; Girju, 2003). Do et al.
(2011) have worked with verb-noun pairs for
causality detection but they focused only on a
small list of predefined nouns representing events.
In this paper, we focus on the task of identi-
fying causality encoded by verb-noun pairs (ex-
ample 3). We propose a novel model which first
predicts cause or non-cause relations using a su-
pervised classifier and then incorporates additional
types of knowledge to reduce errors in predictions.
Using a supervised classifier, our model identi-
fies causation by employing shallow linguistic fea-
tures (e.g., lemmas of verb and noun, words be-
tween verb and noun). Such features have been
used successfully for various NLP tasks (e.g., part-
of-speech tagging, named entity recognition, etc.)
but confinement to such features does not help
much to achieve performance for identifying cau-
sation (Riaz and Girju, 2013). Therefore, in our
model we plug in additional types of knowledge
to obtain better predictions for the current task.
For example, we identify the semantic classes of
nouns and verbs with high tendency to encode
cause or non-causal relations and use this knowl-
edge to achieve better performance. Specifically,
the contributions of this paper are as follows:
? In order to build a supervised classifier, we
use the annotations of FrameNet to generate a
training corpus of verb-noun instances encod-
ing cause and non-cause relations. We propose
a set of linguistic features to learn and identify
causal relations.
48
? In order to make intelligent predictions, it is
important for our model to have knowledge
about the semantic classes of nouns with high
tendency to encode causal or non-causal re-
lations. For example, a named entity such
as person, organization or location may have
high tendency to encode non-causality unless a
metonymic reading is associated with it. In our
approach, we identify such semantic classes of
nouns by exploiting a named entity recognizer,
the annotations of frame elements provided in
FrameNet and WordNet.
? Verbs are the important components of lan-
guage for expressing events of various types.
For example, Pustejovsky et al. (2003)
have classified events into eight semantic
classes: OCCURRENCE, PERCEPTION, RE-
PORTING, ASPECTUAL, STATE, I STATE,
I ACTION and MODAL. We argue that there
are some semantic classes in this list with high
tendency to encode cause or non-cause rela-
tions. For example, reporting events repre-
sented by verbs say, tell, etc., have high ten-
dency to just report other events instead of en-
coding causality with them. In our model, we
use such information to reduce errors in predic-
tions.
? Each causal relation is characterized by two
roles i.e., cause and its effect. In example 3
above, the noun ?hurricane? is cause and the
verb ?died? is its effect. However, a verb-noun
pair may not encode causality when a verb
and a noun represent same event. For exam-
ple, in instance ?Colin Powell presented fur-
ther evidence in his presentation.?, the verb
?presented? and the noun ?presentation? rep-
resent same event of ?presenting? and thus en-
coding non-cause relation with each other. In
our model, we determine the verb-noun pairs
representing same or distinct events to make
predictions accordingly.
? We adopt the framework of Integer Linear Pro-
gramming (ILP) (Roth and Yih, 2004; Do et al.,
2011) to combine all the above types of knowl-
edge for the current task.
This paper is organized as follows. In next sec-
tion, we briefly review the previous research done
for identifying causality. We introduce our model
and evaluation with discussion on results in sec-
tion 3 and 4, respectively. The section 5 of the
paper concludes our current research.
2 Related Work
In computational linguistics, researchers have al-
ways shown interest in the task of automatic
recognition of causal relations because success on
this task is critical for various natural language
applications (Girju, 2003; Chklovski and Pantel,
2004; Radinsky and Horvitz, 2013).
Following the successful employment of lin-
guistic features for various tasks (e.g., part-of-
speech tagging, named entity recognition, etc.),
initially NLP researchers proposed approaches re-
lying mainly on such features to identify causal-
ity (Girju, 2003; Bethard and Martin, 2008;
Sporleder and Lascarides, 2008; Pitler and
Nenkova, 2009; Pitler et al., 2009). However,
researchers have recently shifted their attention
from these features and tried to consider other
sources of knowledge for extracting causal rela-
tions (Beamer and Girju, 2009; Riaz and Girju,
2010; Do et al., 2011; Riaz and Girju, 2013).
For example, Riaz and Girju (2010) and Do et
al. (2011) have proposed unsupervised metrics for
learning causal dependencies between two events.
Do et al. (2011) have also incorporated minimal
supervision with unsupervised metrics. For a pair
of events (a, b), their model makes the decision of
cause or non-cause relation based on unsupervised
co-occurrence counts and then improves this deci-
sion by using minimal supervision from the causal
and non-causal discourse markers (e.g., because,
although, etc.).
In search of novel and effective types of knowl-
edge to identify causation between two verbal
events, Riaz and Girju (2013) have proposed a
model to learn a Knowledge Base (KB
c
) of verb-
verb pairs. In this knowledge base, the English
language verb-verb pairs are automatically clas-
sified into three categories: (1) Strongly Causal,
(2) Ambiguous and (2) Strongly Non-Causal. The
Strongly Causal and Strongly Non-Causal cate-
gories contain verb-verb pairs with highest and
least tendency to encode causality, respectively
and rest of the verb-verb pairs are considered am-
biguous with tendency to encode both types of
relations. They claim that this knowledge base
of verb-verb pairs is a rich source of causal as-
sociations. The incorporation of this resource
into a causality detection model can help identi-
fying causality with better performance. In this
research, we also try to go beyond the scope of
shallow linguistic features and identify additional
49
interesting types of knowledge for the current task.
3 Computational Model for Identifying
Causality
In this section, we introduce our model for iden-
tifying causality encoded by verb-noun pairs.
Specifically, we extract all main verbs and noun
phrases from a sentence and predict cause or non-
cause relation on verb-noun phrase (v-np) pairs.
In order to make task easier, we consider only
those v-np pairs where v (verb) is grammatically
connected to np (noun phrase). We assume that a v
and np are grammatically connected if there exists
a dependency relation between them in the depen-
dency tree. We apply a dependency parser (Marn-
effe et al., 2006) to identify such dependencies.
Our model first employs a supervised classifier re-
lying on linguistic features to make binary predic-
tions (i.e., does a verb-noun phrase pair encode a
cause or non-cause relation?). We then incorpo-
rate additional types of knowledge on top of these
binary predictions to improve performance.
3.1 Supervised Classifier
In this section, we propose a basic supervised
classifier to identify causation encoded by v-np
pairs. To set up this supervised classifier, we need
a training corpus of instances of v-np pairs en-
coding cause and non-cause relations. For this
purpose, we employ the annotations of FrameNet
project (Baker et al., 1998) provided for verbs.
For example, consider the following annotation
from FrameNet for the verb ?dying? with ar-
gument ?solvent abuse? where the pair ?dying-
solvent abuse? encodes causality.
A campaign has started to try to cut the
rising number of children dying [
cause
from solvent abuse].
To generate a training corpus, we collect anno-
tations of verbs from FrameNet s.t. the annotated
element (aka. frame element) is a noun phrase.
For example, we get a causal training instance of
?dying-solvent abuse? pair from the above anno-
tation. We assume that if a FrameNet?s annotated
element contains a verb in it then this may not rep-
resent a training instance of v-np pair. For exam-
ple, we do not consider the following annotation
in our training corpus where causality is encoded
between two verbs i.e., ?died-fell?.
A fitness fanatic died [
cause
when 26
stone of weights fell on him as he ex-
ercised].
After extracting training instances from
FrameNet, we assign them cause (c) and non-
cause (? c) labels. We manually examined the
inventory of labels of FrameNet and use the
following scheme to assign the c or ?c to each
training instance. All the annotations of FrameNet
with following labels are considered as causal
training instances and rest of the annotations are
considered as non-causal training instances.
Purpose, Internal cause, Result, Exter-
nal cause, Cause, Reason, Explanation,
Required situation, Purpose of Event,
Negative consequences, resulting ac-
tion, Effect, Cause of shine, Purpose of
Goods, Response action, Enabled situa-
tion, Grinding cause, Trigger
For this work, we have acquired 2, 158
(65, 777) cause (non-cause) training instances
from FrameNet. Since, the non-cause instances
are very large in number, our supervised model
tends to assign non-cause labels to almost all in-
stances. Therefore, we employ equal number of
cause and non-cause instances for training. In fu-
ture, we plan to extract more annotations from
the FrameNet and employ more than one human
annotators to assign the labels of cause and non-
cause relations to the full inventory of labels of
FrameNet.
? Lexical Features: verb, lemma of verb, noun
phrase, lemma of all words of noun phrase,
head noun of noun phrase, lemmas of all words
between verb and head noun of noun phrase.
? Semantic Features: We adopted this feature
from Girju (2003) to capture the semantics of
nouns. The 9 noun hierarchies of WordNet i.e.,
entity, psychological feature, abstraction, state,
event, act, group, possession, phenomenon are
used as this feature. Each of these hierarchies
is set to 1 if any sense of the head noun of noun
phrase lies in that hierarchy otherwise set to 0.
? Structural Features: This feature is applied
by considering both subject (i.e., sub in np)
and object (i.e., obj in np) of a verb. For ex-
ample, for a v-np pair the variable sub in np is
set to 1 if the subject of v is contained in np,
set to 0 if the subject of v is not contained in
np and set to -1 if the subject of v is not avail-
able in the instance. The subject and object of
a verb are its core arguments and may some-
time be part of an event represented by a verb.
Therefore, these argument may have high ten-
dency to encode non-cause relations.
50
We set up the following integer linear program
after acquiring predictions of c and ? c labels us-
ing our supervised classifier.
Z
1
= max
?
v-np?I
?
l?L
1
x
1
(v-np, l)P (v-np, l) (1)
?
l?L
1
x
1
(v-np, l) = 1 ? v-np ? I (2)
x
1
(v-np, l) ? {0, 1} ? v-np ? I ?l ? L
1
(3)
Here L
1
= {c,?c}, I is the set of all instances
of v-np pairs and x
1
(v-np, l) is the decision vari-
able set to 1 only if the label l ? L
1
is assigned
to v-np. The Equation 2 constraints that only one
label out of |L
1
| choices can be assigned to a v-np
pair. The equation 3 requires x
1
(v-np, l) to be a
binary variable. Specifically, we try to maximize
the objective function Z
1
(equation 1) which as-
signs the label cause or non-cause to all v-np pairs
(i.e., set the variables x
1
(v-np, l) to 1 or 0 for all
l ? L
1
and for all v-np pairs in I) depending
on the probabilities of assignment of labels (i.e.,
P (v-np, l))
1
. These probabilities can be obtained
by running a supervised classification algorithm
(e.g., Naive Bayes and Maximum Entropy). In our
experiments, we provide results using the follow-
ing probabilities acquired with Naive Bayes.
P (v-np, c) = 1.0?
?
n
k=1
logP (f
k
| c)
?
n
k=1
?
l?(c,?c)
logP (f
k
| l)
P (v-np,?c) = 1.0? P ((v, np), c) (4)
where f
k
is a feature, n is total number of fea-
tures and P (f
k
| l) is the smoothed probability of
a feature f
k
given the training instances of label l.
3.2 Knowledge of Semantic classes of nouns
Philospher Jaegwon Kim (Kim, 1993) (as cited by
Girju and Moldovan (2002)) pointed out that the
entities which represent either causes or effects are
often events, but also conditions, states, phenom-
ena, processes, and sometimes even facts. There-
fore, according to this our model should have
knowledge of the semantic classes of noun phrases
with high tendency to encode cause or non-cause
relations. Considering this type of knowledge, we
can automatically review and correct the wrong
predictions made by our basic supervised classi-
fier.
1
We use the integer linear program solver available at
http://sourceforge.net/projects/lpsolve/
We argue that if a noun phrase represents a
named entity then it can have least tendency to en-
code causal relations unless there is a metonymic
reading associated with it. For example, con-
sider the following cause and non-cause examples
where noun phrase is a named entity.
4. Sandy hit Cuba as a Category 3 hurricane.
5. Almost all the weapon sites in Iraq were de-
stroyed by the United States.
In example 4, Cuba is location and does not
encode causality. However, in example 5 the
pair ?destroyed-the United States? encode causal-
ity where a metonymic reading is associated with
the location. We apply Named Entity Recog-
nizer (Finkel et al., 2005) and assume if a noun
phrase is identified as a named entity then its cor-
responding verb-noun phrase pair encodes non-
cause relation. This constraint can lead to a false
negative prediction when the metonymic reading
is associated with a noun phrase. In order to
avoid as much false negatives as possible, we im-
ply the following simple rule i.e., if one of the
following cue words appear between a verb and a
noun phrase then do not apply the constraint stated
above.
by, from, because of, through, for
In our experiments, the above simple rule helps
avoiding some false negatives but in future any
subsequent improvement with a better metonomy
resolver (Markert and Nissim, 2009) should im-
prove the performance of our model.
In addition to named entities, there can be var-
ious noun phrases with least tendency to encode
causation. Consider the following example, where
?city? is a location and does not encode cause-
effect relation with the verb ?remained?.
Substantially fewer people remained in
the city during the Hurricane Ivan evac-
uation.
In this work, we identify the semantic classes
of noun phrases which do not normally represent
events, conditions, states, phenomena, processes
and thus have high tendency to encode non-cause
relations. For this purpose, we manually examine
the inventory of labels assigned to noun phrases
in FrameNet (see table 1) and classify these labels
into two classes (c
n
p and ?c
np
). Here, the class
c
np
(?c
np
) represents the labels of noun phrases
with high (less) tendency to encode cause-effect
relations. For example, the label ?Place? ? ?c
np
51
(see table 1) represents a location and it may have
least tendency to encode causality if metonymy is
not associated with it. Using the classification of
frame elements in table 1, we obtain the annota-
tions of noun phrases from FrameNet and catego-
rize these annotations into c
np
and ?c
np
classes.
On top of the annotations of these two semantic
classes, we build a supervised classifier for pre-
dicting c
np
or ?c
np
label for the noun phrases.
After obtaining predictions, we select all noun
phrases lying in class ?c
np
and apply the same
constraint stated above for the named entities. We
use the following set of features to set up a super-
vised classifier for c
np
and ?c
np
labels.
? Lexical Features: words of noun phrase, lem-
mas of all words of noun phrase, head word
of noun phrase, first two (three) (four) letters
of head noun of noun phrase, last two, (three)
(four) letters of head noun of noun phrase.
? Word Class Features: part-of-speech tags of
all words of noun phrase, part-of-speech tag of
head noun of noun phrase.
? Semantic Features: all (frequent) sense(s) of
head noun of noun phrase.
We have acquired 23,334 (81,279) training in-
stances of c
np
(?c
np
) class, respectively for this
work. We also use WordNet to obtain more train-
ing instances of these classes. We follow the ap-
proach similar to Girju and Moldovan (2002) and
adopt some senses of WordNet (shown in table 1)
to acquire training instances of noun phrases. For
example, considering the table 1, we assign ?c
np
label to any noun whose all senses in WordNet lie
in the semantic hierarchy originated by the sense
{time period, period of time, period}. Follow-
ing this scheme, we extract instances of nouns and
noun phrases from English GigaWord corpus and
assign the labels c
np
and ?c
np
to them by em-
ploying WordNet senses given in table 1. Girju
and Moldovan (2002) have used similar scheme
to rank noun phrases according to their tendency
to encode causation. In comparison to them, we
use the WordNet senses to increase the size of
our training set of noun phrases obtained using
FrameNet above. In addition to this, we build a
automatic classifier on the training data obtained
using labels of FrameNet and WordNet senses to
classify noun phrases of test instances into two se-
mantics classes (i.e., c
np
and ?c
np
). In our train-
ing corpus of there are 2, 214, 68 instances of noun
phrases (50% belongs to each of c
np
and ?c
np
classes).
We incorporate the knowledge of semantics of
nouns in our model by making the following ad-
ditions to the integer linear program introduced in
section 3.1.
Z
2
= Z
1
+
?
np:v-np?I
?
l?L
2
x
2
(np, l)P (np, l) (5)
?
l?L
2
x
2
(np, l) = 1 ? np : v-np ? I ?M (6)
x
2
(np, l) ? {0, 1} ? np : v-np ? I ?M (7)
?l ? L
2
x
1
(v-np,?c)? x
2
(np,?c
np
) ? 0 (8)
? np : v-np, ? v-np ? I ?M
Here L
2
= {c
np
,?c
np
} and M is the set of
instances of those v-np pairs for which we con-
sider the possibility of attachment of metonymic
reading with np, x
2
(np, l) is the decision variable
set to 1 only if the label l ? L
2
is assigned to
np. The Equation 6 constraints that only one la-
bel out of |L
2
| choices can be assigned to a np.
The equation 7 requires x
2
(np, l) to be a binary
variable. The constraint 8 assumes that if an np
belongs to the semantic class ?c
np
then its corre-
sponding pair v-np is assigned the label ?c. We
maximize the objective function Z
2
(equation 5)
of our integer linear program subject to the con-
straints introduced above. We predict the semantic
class of a noun phrase using the supervised classi-
fier for c
np
and ?c
np
classes and set the probabil-
ities i.e., P (np, l) = 1, P (np, {L
2
} ? {l}) = 0 if
the label l ? L
2
is assigned to np. Again we use
Naive Bayes to predict the labels for noun phrases.
Also before running this supervised classifier, we
run the named entity recognizer and assign ?c
np
labels to all noun phrases identified as named en-
tities. For our model, we apply named entity rec-
ognizer for seven classes i.e., LOCATION, PER-
SON, ORGANIZATION, DATE, TIME, MONEY,
PERCENT (Finkel et al., 2005).
3.3 Knowledge of Semantic classes of verbs
In this section, we introduce our method to in-
corporate the knowledge of semantic classes of
verbs to identify causation. Verbs are the com-
ponents of language for expressing events of var-
ious types. In TimeBank corpus, Pustejovsky et
al. (2003) have introduced eight semantic classes
of events i.e., OCCURRENCE, PERCEPTION,
REPORTING, ASPECTUAL, STATE, I STATE,
I ACTION and MODAL. According to the defi-
nitions of these classes provided by Pustejovsky
52
Semantic
Class
FrameNet Labels WordNet Senses
c
np
Event, Goal, Purpose, Cause, Internal
cause, External cause, Result, Means, Rea-
son, Phenomena
{act, deed, human action, human activity},
{phenomenon}, {state}, {psychological
feature}, {event}, {causal agent, cause,
causal agency}
?c
np
Artist, Performer, Duration, Time, Place,
Distributor, Area, Path, Direction, Sub-
region
{time period, period of time, period},
{measure, quantity, amount}, {group,
grouping}, {organization, organisation},
{time unit, unit of time}, {clock time, time}
Table 1: This table presents some examples of FrameNet labels in c
np
and ?c
np
classes. The full set of
labels in both semantic classes are given in appendix A. It also presents the WordNet senses of nouns
lying in c
np
and ?c
np
classes.
et al. (2003), the reporting events describe the
action of a person, declare something or narrate
an event e.g., the reporting events represented by
verbs say, tell, etc. Here, we argue that a reporting
event has the least tendency to encode causation
because such an event only describes or narrates
another event instead of encoding causality with
it. We assume that the verbs representing report-
ing events have least tendency to encode causa-
tion and thus their corresponding v-np pairs have
least tendency to encode causation. To add this
knowledge to our model, we consider two classes
of verbs i.e., c
v
and ?c
v
where the class c
v
(?c
v
)
contains the verbs with high (less) tendency to en-
code causation. Using above argument we claim
that all verbs representing reporting events belong
?c
v
class and verbs representing rest of the types
of events belong to c
v
class. We build a supervised
classifier which automatically classifies verbs into
c
v
and ?c
v
classes. We extract the instances of
verbal events (i.e., verbs or verbal phrases) from
TimeBank corpus and assign the labels c
v
and
?c
v
to these instances. Using these labeled in-
stances, we build a supervised classifier by adopt-
ing the same set features as introduced in Bethard
and Martin (2006) to identify semantic classes of
verbs. Due to space constraint, we refer the reader
to Bethard and Martin (2006) for the details of fea-
tures. Again we use Naive Bayes to take predic-
tions of c
v
and ?c
v
labels and their corresponding
probabilities using equation 4.
We incorporate the knowledge of semantics of
verbs in our model by making the following addi-
tions to the integer linear program.
Z
3
= Z
2
+
?
v:v-np?I
?
l?L
3
x
3
(v, l)P (v, l) (9)
?
l?L
3
x
3
(v, l) = 1 ? v : v-np ? I (10)
x
3
(v, l) ? {0, 1} ? v : v-np ? I ?l ? L
3
(11)
x
1
(v-np,?c)? x
3
(v,?c
v
) >= 0 (12)
? v : v-np, ? v-np ? I
x
3
(v, c
v
)? x
1
(v-np, c) >= 0 (13)
? v : v-np, ? v-np ? I
Here L
3
= {c
v
,?c
v
}, x
3
(v, l) is the decision
variable set to 1 only if the label l ? L
3
is as-
signed to v. The Equation 10 constraints that only
one label out of |L
3
| choices can be assigned to
a v. The equation 11 requires x
3
(v, l) to be a bi-
nary variable. The constraint 12 assumes if a verb
v belongs to the class c
v
(i.e., has least potential
to encode causation) then its corresponding pair
v-np encodes non-causality. The constraint 12 en-
forces that if a verb v belongs to the class ?c
v
then
its corresponding v-np pair is assigned the label
?c. Similarly, the constraint 16 enforces that if
a v-np pair encodes causality then its verb v has
potential to encode causal relation. We maximize
the objective function Z
3
subject to the constraints
introduced above.
3.4 Knowledge of Indistinguishable Verb and
Noun
As introduced earlier, each causal relation is char-
acterized by two roles i.e., cause and its effect. In
order to encode causal relation, two components
of an instance of verb-noun phrase pair need to
represent distinct events, processes or phenomena.
Employing simple lexical matching, we determine
if a verb and a noun phrase represent same event
or not as follows:
53
? We use NOMLEX (Macleod et al., 2009) to
transform a verb into its corresponding nomi-
nalization and use the following text segments
for lexical matching.
T
v
= [Subject] verb [Object]
2
T
n
= Head noun of noun phrase
? We remove stopwords and duplicate words
from T
v
and T
n
and take lemmas of all words.
If the subject or object or both arguments are
contained in noun phrase then we remove these
arguments from T
v
. We determine the proba-
bility of a verb (v) and a noun phrase (np) rep-
resenting same event as follows. If head noun
(i.e., T
n
) lexically matches with any word of T
v
then set P(v ? np) to 1 and 0 otherwise.
We assign non-cause relation if P(v ? np) = 1.
Next, we incorporate the knowledge of indistin-
guishable verb and noun in our model using the
following additions to our integer linear program.
Z
4
= Z
3
+
?
v-np?I
?
l?L
4
x
4
(v-np, l)P (v-np, l) (14)
?
l?L
4
x
4
(v-np, l) = 1 ? v-np ? I (15)
x
4
(v-np, l) ? {0, 1} ? v-np ? I, ?l ? L
4
x
1
(v-np,?c)? x
4
(v-np,?) ? 0 (16)
? v-np ? I
Here L
4
= {?, 6?} where the label ? (6?) rep-
resents same (distinct) events, x
4
(v-np, l) is the
decision variable set to 1 only if the label l ? L
4
is assigned to v-np. The Equation 15 constraints
that only one label out of |L
4
| choices can be as-
signed to a v-np pair. The equation 16 requires
x
4
(v-np, l) to be a binary variable. The con-
straint 16 enforces that if a v-np pair belongs to
the class ? then this pair is assigned the label ?c.
We maximize the objective function Z
4
subject to
the constraints introduced above.
4 Evaluation and Discussion
In this section we present the experiments, evalu-
ation procedures, and a discussion on the results
achieved through our model for the current task.
In order to evaluate our model, we generated a
test set with instances of form verb-noun phrase
where the verb is grammatically connected to the
noun phrase in an instance. For this purpose, we
2
Following Riaz ang Girju (2010), we assume that the
subject and object of a verb are parts of an event represented
by a verb. Therefore, we use these arguments along with a
verb for lexical matching with a noun phrase.
collected three wiki articles on the topics of Hurri-
cane Katrina, Iraq War and Egyptian Revolution
of 2011. We selected first 100 sentences from
these articles and applied part-of-speech tagger
(Toutanova et al., 2003) and dependency parser
(Marneffe et al., 2006) on these sentences. Using
each sentence, we extracted all verb-noun phrase
pairs where the verb has a dependency relation
with any word of noun phrase. We manually in-
spected all of the extracted instances and removed
those instances in which a word had been wrongly
classified as a verb by the part-of-speech tagger.
There are total 1106 instances in our test set. We
assigned the task of annotation of these instances
with cause and non-cause relations to a human
annotator. Using manipulation theory of causal-
ity (Woodward, 2008), we adopted the annotation
guidelines from Riaz and Girju (2010) which is as
follows: ?Assign cause label to a pair (a, b), if the
following two conditions are satisfied: (1), a tem-
porally precedes/overlap b in time, (2) while keep-
ing as many state of affairs constant as possible,
modifying a must entail predictably modifying b.
Otherwise assign non-cause label. ?
We have 149 (957) cause (non-cause) instances
in our test set
3
, respectively. We evaluate the per-
formance of our model using F-score and accuracy
evaluation measures (see table 2 for results).
The results in table 2 reveal that the ba-
sic supervised classifier is a naive model and
achieves only 27.27% F-score and 46.47% ac-
curacy. The addition of novel types of
knowledge introduced in section 3 (i.e., the
model Basic+SCN
M
+SCV+IVN) brings 14.74%
(29.57%) improvements in F-score (accuracy), re-
spectively. These results show that the knowledge
of semantics of nouns and verbs and the knowl-
edge of indistinguishable verb and noun are crit-
ical to achieve performance. The maximum im-
provement in results is achieved with the addition
of semantic classes of nouns (i.e., Basic+SCN
M
).
The consideration of association of metonymic
readings using model Basic+SCN
M
helps us to
maintain recall as compared with SCN
!M
and
therefore brings better F-score.
One can notice that almost all models suffer
from low precision which leads to lower F-scores.
Although, our model achieves 14.58% increase in
precision over basic supervised classifier, the lack
of high precision is still responsible for lower F-
3
We will make the test set available
54
Model Basic +SCN
!M
+SCN
M
+SCN
M
+SCV +SCN
M
+SCV+IVN
Accuracy 46.47 75.76 74.41 75.31 76.04
Precision 16.69 28.14 29.53 30.47 31.27
Recall 74.49 50.66 64.00 64.00 64.00
F-score 27.27 39.19 40.42 41.29 42.01
Table 2: This table presents results of the basic supervised classifier (i.e., Basic) and the models after
incrementally adding the knowledge of semantic classes of nouns without consideration of metonymic
readings (i.e., + SCN
!M
), the knowledge of semantic classes of nouns with consideration of metonymic
readings (i.e., + SCN
M
), the knowledge of semantic classes of verbs (i.e., +SCN
M
+SCV) and the knowl-
edge of indistinguishable verb and noun (i.e., +SCN
M
+SCV+IVN).
score. The highly skewed distribution of test set
with only 13.47% causal instances results in lots
of false positives. We manually examined false
positives to determine the language features which
may help us reducing more false positives without
affecting F-score. We noticed that the direct ob-
jects of the verbs are mostly part of the event rep-
resented by the verbs and therefore encodes non-
causation with the verbs. For example, consider
following instances:
6. The hurricane surge protection failures
prompted a lawsuit.
7. They provided weather forecasts.
In example 6, ?lawsuit? is the direct object of
the verb ?prompted? and is part of the event rep-
resented by the verb ?prompt?. However there
is a cause relation between ?protection failures?
and ?prompted?. Similarly in example 7, the di-
rect object ?forecasts? is part of the ?providing?
event and thus the noun phrase ?weather fore-
casts? encode non-cause relation with the verb
?provide?. Therefore, following this observation
we employed the training corpus of cause and non-
cause relations (see section 3.1) and learned the
structure of verb-noun phrase pairs encoding non-
cause relations most of the time. We considered
only those training instances where the subject
and/or object of the verb was available. For the
current purpose, we picked up following four fea-
tures (1) sub in np, (2) !sub in np, (3) obj in np
and (4) !obj in np. Just to remind the reader, the
feature sub in np (!sub in np) is set to 1 if the
subject of the verb is (not) contained in the noun
phrase np, respectively. For each of the above four
features, the percentage of cause and entropy of
relations with that feature are as follows:
? sub in np (%c = 34.72, Entropy = 0.931)
? !sub in np (%c = 59.71, Entropy = 0.972)
? obj in np (%c = 28.89, Entropy = 0.867)
? !obj in np (%c = 55.30, Entropy = 0.991).
There are two important observations from
above scores: (1) verbs mostly encode non-cause
relations with their objects and subjects (i.e., high
%?c with obj in np and sub in np), (2) among
obj in np and sub in np features, obj in np yields
least entropy i.e., there are least chances of encod-
ing causality of a verb with its object.
Considering the above statistics, we enforce the
constraint on each verb-noun phrase pair that if
the object of the verb is contained in the noun
phrase of the above pair then assigns non-cause
relation to that pair. Using this constraint, we ob-
tain 46.61% (80.74%) F-score (accuracy), respec-
tively. This confirms our observation that the ob-
ject of a verb is normally part of an event repre-
sented by the verb and thus it encodes non-cause
relation with the verb.
In this research, we have utilized novel types
of knowledge to improve the performance of our
model. In future, we need to consider more
additional information (e.g., predictions from
metonymy resolver) to achieve further progress.
5 Conclusion
In this paper, we have proposed a model for iden-
tifying causality in verb-noun pairs by employing
the knowledge of semantic classes of nouns and
verbs and the knowledge of indistinguishable noun
and verb of an instance along with shallow linguis-
tic features. Our empirical evaluation of model
has revealed that such novel types of knowledge
are critical to achieve a better performance on the
current task. Following the encouraging results
achieved by our model, we invite researchers to
investigate more interesting types of knowledge in
future to make further progress on the task of rec-
ognizing causality.
55
References
Collin F. Baker, Charles J. Fillmore and John B. Lowe.
1998. The Berkeley FrameNet project. In proceed-
ings of the Association for Computational Linguis-
tics and International Conference on Computational
Linguistics (COLING-ACL).
Brandon Beamer and Roxana Girju. 2009. Using
a Bigram Event Model to Predict Causal Poten-
tial. In proceedings of the Conference on Compu-
tational Linguistics and intelligent Text Processing
(CICLING).
Steven Bethard and James H. Martin. 2006. Identifica-
tion of Event Mentions and their Semantic Class. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP).
Steven Bethard and James H. Martin. 2008. Learning
Semantic Links from a Corpus of Parallel Temporal
and Causal Relations. In proceedings of the Associ-
ation for Computational Linguistics (ACL).
Du-Seong Chang and Key-Sun Choi. 2006. Incremen-
tal cue phrase learning and bootstrapping method for
causality extraction using cue phrase and word pair
probabilities. Information Processing and Manage-
ment, volume 42 issue 3, 662678.
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the Web for Fine-Grained Semantic
Verb Relations. In proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
Quang X. Do, Yee S. Chen and Dan Roth. 2011. Min-
imally Supervised Event Causality Identication. In
proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP).
Jenny R. Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating Non-local Informa-
tion into Information Extraction Systems by Gibbs
Sampling. In Proceedings of the Association for
Computational Linguistics (ACL).
Roxana Girju. 2003. Automatic detection of causal
relations for Question Answering. Association for
Computational Linguistics ACL, Workshop on Mul-
tilingual Summarization and Question Answering
Machine Learning and Beyond.
Roxana Girju and Dan Moldovan. 2002. Mining An-
swers for Causation Questions. In American Asso-
ciations of Artificial Intelligence (AAAI), 2002 Sym-
posium.
Jaegwon Kim. 1993. Causes and Events. Mackie on
Causation. In Cansation, Oxford Readings in Phi-
losophy, ed. Ernest Sosa, and Michael Tooley, Ox-
ford University Press.
Catherine Macleod, Ralph Grishman, Adam Meyers,
Leslie Barrett, Ruth Reeves. 1998. NOMLEX: A
Lexicon of Nominalizations. In proceedings of EU-
RALEX.
Katja Markert, Malvina Nissim 2009. Data and mod-
els for metonymy resolution. Language Resources
and Evaluation Volume 43 Issue 2, Pages 123?138.
Marie-Catherine de Marneffe, Bill MacCartney and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses.
In Proceedings of the International Conference on
Language Resources and Evaluation (LREC).
Emily Pitler, Annie Louis and Ani Nenkova. 2009.
Automatic Sense Prediction for Implicit Discourse
Relations in Text. In proceedings of ACL-IJCNLP.
Emily Pitler and Ani Nenkova. 2009. Using Syntax
to Disambiguate Explicit Discourse Connectives in
Text. In proceedings of ACL-IJCNLP.
James Pustejovsky, Patrick Hanks, Roser Saur, Andrew
See, Robert Gaizauskas, Andrea Setzer, Dragomir
Radev, Beth Sundheim, David Day, Lisa Ferro and
Marcia Lazo. 2003. The TIMEBANK Corpus. In
Proceedings of Corpus Linguistics.
Kira Radinsky and Eric Horvitz. 2013. Mining the
Web to Predict Future Events. In proceedings of
sixth ACM international conference on Web search
and data mining, (WSDM).
Mehwish Riaz and Roxana Girju. 2010. Another Look
at Causality: Discovering Scenario-Specific Contin-
gency Relationships with No Supervision. In pro-
ceedings of the IEEE 4th International Conference
on Semantic Computing (ICSC).
Mehwish Riaz and Roxana Girju 2013. Toward
a Better Understanding of Causality between Ver-
bal Events: Extraction and Analysis of the Causal
Power of Verb-Verb Associations. Proceedings of
the annual SIGdial Meeting on Discourse and Dia-
logue (SIGDIAL).
Dan Roth and Wen-tau Yih 2004. A Linear Program-
ming Formulation for Global Inference in Natural
Language Tasks. In Proceedings of the Annual Con-
ference on Computational Natural Language Learn-
ing (CoNLL).
Caroline Sporleder and Alex Lascarides. 2008. Using
automatically labelled examples to classify rhetor-
ical relations: An assessment. Journal of Natural
Language Engineering Volume 14 Issue 3.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Net-
work. In Proceedings of Human Language Technol-
ogy and North American Chapter of the Association
for Computational Linguistics (HLT-NAACL).
James Woodward. 2008. Causation and Manipulation.
Online Encyclopedia of Philosophy.
Appendix A. Semantic Classes of Nouns
This appendix presents the FrameNet labels we as-
sign to c
np
and ?c
np
classes (see section 3.2).
56
Semantic Class FrameNet Labels
c
np
Event, Goal, Purpose, Cause, Internal cause, External cause, Result, Means, Reason, Phenomena, Char-
acterization, Coordinated event, Final state, Information, Topic, Containing event, Mental content, Ac-
tion, Experience, Impactee, Impactor, Message, Question, Circumstances, Desired goal, Explanation,
Required situation, Complaint, Content, Activity, Intended goal, Phenomenon, State, Dependent state,
Forgery, Purpose of Event, Negative consequences, Inference, Appraisal, Noisy event, Function, Evi-
dence, Process, Paradigm, Standard, Old order, Focal occasion, Landmark occasion, resulting action,
Victim, Issue, Effect, State of affairs, Cause of shine, Qualification, Undesirable Event, Skill, Precept,
Outcome, Norm, Act, State of Affairs, Phenomenon 1, Phenomenon 2, Quality Eventuality, Expression,
Intended event, Cognate event, Epistemic stance, Goal conditions, Possession, Support Proposition,
Domain of Relevance, Charges, Idea, Initial subevent, Hypothetical event, Scene, Purpose of Goods,
Response action, Motivation, Executed, Affliction, Medication, Treatment, Stimulus, Last subevent,
Undesirable situation, Sleep state, Initial state, Enabled situation, Grinding cause, Finding, Case, Legal
Basis, Role of focal participant, Trigger, Authenticity, World state, Emotion, Emotional state, Evalua-
tion, New idea, Production, Performance, Undertaking, Destination event
?c
np
Artist, Performer, Duration, Time, Place, Distributor, Area, Path, Direction, Sub-region, Creator, Copy,
Original, Iteration, Manner, Frequency, Agent, Body part, Depictive, Theme, Subregion, Area, De-
gree, Angle, Fixed location, Path shape, Addressee, Entity, Individual 1, Individual 2, Road, Distance,
Speaker, Medium, Clothing, Wearer, Bodypart of agent, Locus, Cognizer, Salient entity, Name, Inspec-
tor, Ground, Unwanted entity, Location of inspector, Researcher, Population, Searcher, Sought entity,
Instrument, Created entity, Components, Forgoer, Desirable, Bad entity, Dodger, Experiencer, Vehicle,
Self mover, Speed, Cotheme, Consecutive, Re encoding, Supplier, Individuals, Driver, Complainer,
Communicator, Protagonist, Attribute, Final value, Item, Initial value, Difference, Group, Value range,
Co participant, Perceiver agentive, Target symbol, Location of perceiver, Location, Expected entity,
Focal participant, Time of Event, Variable, Limits, Limit1, Limit2, Point of contact, Goods, Lessee,
Lessor, Money, Rate, Unit, Reversive, Perceiver passive, Sound, Sound source, Location of source,
Fidelity, Official, Selector, Role, Concessive, New leader, Body, Old leader, Leader, Governed, Result
size, Size change, Dimension, Initial size, Elapsed time, Interval, Category, Criteria, Text, Final cor-
relate, Correlate, Initial correlate, Manipulator, Side 1, Sides, Side 2, Perpetrator, Value 1, Value 2,
Actor, Partner 2, Partner 1, Partners, Figure, Resident, Co resident, Student, Subject, Institution, Level,
Teacher, Undergoer, Subregion bodypart, Course, Owner, Defendant, Judge, Co abductee, Location
of appearance, Material, Accused, Arraign authority, Hair, Configuration, Emitter, Beam, Amount of
progress, Evaluee, Patient, Buyer, Seller, Recipient, Relay, Relative location, Connector, Items, Part
1, Part 2, Parts, Whole, Name source, Payer, Fine, Executioner, Interlocutor 1, Interlocutor 2, Inter-
locutors, Healer, Food, Cook, Container, Heating instrument, Temperature setting, Resource controller,
Resource, Donor, Constant location, Carrier, Sender, Co theme, Transport means, Holding location,
Rope, Knot, Handle, Containing object, Fastener, Enclosed region, Container portal, Aggregate, Sus-
pect, Authorities, Offense, Source of legal authority, Ingestor, Ingestibles, Sleeper, Pieces, Goal area,
Period of iterations, Mode of transportation, Produced food, Ingredients, Cognizer agent, Excreter,
Excreta, Air, Perceptual source, Escapee, Undesirable location, Evader, Capture, Pursuer, Amount of
discussion, Means of communication, Periodicity, Author, Honoree, Reader, Child, Mother, Father,
Egg, Flammables, Flame, Kindler, Mass theme, Address, Intermediary, Communication, Location of
communicator, Firearm, Indicated entity, Hearer, Sub region, Member, Object, Organization, Guardian,
New Status, Arguer, Criterion, Liquid, Impactors, Force, Coparticipant, Holding Location, Legal basis,
Precipitation, Quantity, Voice, Duration of endstate, Period of Iterations, Employer, Employee, Task,
Position, Compensation, Field, Place of employment, Amount of work, Contract basis, Recipients, Hot
Cold source, Temperature goal, Temperature change, Hot/Cold source, Dryee, Temperature, Traveler,
Iterations, Baggage, Deformer, Resistant surface, Fluid, Injured Party, Avenger, Injury, Punishment,
Offender, Grinder, Profiled item, Standard item, Profiled attribute, Standard attribute, Extent, Source
emitter, Emission, Sub source, Item 1, Item 2, Parameter, Form, Chosen, Change agent, Injuring entity,
Severity, Substance, Delivery device, Entry path, Wrong, Amends, Grounds, Expressor, Basis, Signs,
Manufacturer, Product, Factory, Consumer, Interested party, Performer1, Performer2, Whole patient,
Destroyer, Exporting area, Importing area, Accuracy, Time of Eventuality, Indicator, Indicated, Au-
dience, Valued entity, Journey, Duration of end state, Killer, Beneficiary, Destination time, Landmark
time, Seat of emotion, Arguers, Arguer1, Arguer2, Company, Asset, Origin, Sound maker, Static object,
Themes, Heat source, Following distance, Perceiver, Intended perceiver, Location of expressor, Path of
gaze, Relatives, Final temperature, Particular iteration, Participant 1, Language
Table 3: This table presents the FrameNet labels we assign to c
np
and ?c
np
classes.
57
Proceedings of the SIGDIAL 2014 Conference, pages 161?170,
Philadelphia, U.S.A., 18-20 June 2014.
c
?2014 Association for Computational Linguistics
In-depth Exploitation of Noun and Verb Semantics
to Identify Causation in Verb-Noun Pairs
Mehwish Riaz and Roxana Girju
Department of Computer Science and Beckman Institute
University of Illinois at Urbana-Champaign
Urbana, IL 61801, USA
{mriaz2,girju}@illinois.edu
Abstract
Recognition of causality is important to
achieve natural language discourse under-
standing. Previous approaches rely on
shallow linguistic features. In this work,
we propose to identify causality in verb-
noun pairs by exploiting deeper seman-
tics of nouns and verbs. Particularly, we
acquire and employ three novel types of
knowledge: (1) semantic classes of nouns
with a high and low tendency to encode
causality along with information regard-
ing metonymies, (2) data-driven seman-
tic classes of verbal events with the least
tendency to encode causality, and (3) ten-
dencies of verb frames to encode causal-
ity. Using these knowledge sources, we
achieve around 15% improvement in F-
score over a supervised classifier trained
using linguistic features.
1 Introduction
The identification of cause-effect relations is crit-
ical to achieve natural language discourse under-
standing. Causal relations are encoded in text us-
ing various linguistic constructions e.g., between
two verbs, a verb and a noun, two discourse seg-
ments, etc. In this research, we focus on identify-
ing causality encoded between a verb and a noun
(or noun phrase). For example, consider the fol-
lowing example:
1. At least 1,833 people died in the hurricane.
In example (1), the verb-noun phrase pair
?died?-?the hurricane? encodes causality where
event ?died? is the effect of ?hurricane? event.
Previously several approaches have been pro-
posed to identify causality between two verbs
(Bethard and Martin, 2008; Riaz and Girju, 2010;
Do et al., 2011; Riaz and Girju, 2013) and dis-
course segments (Sporleder and Lascarides, 2008;
Pitler and Nenkova, 2009; Pitler et al., 2009).
However, the problem of identifying causality in
verb-noun pairs has not received a considerable
attention. For example, Do et al. (2011) have
studied this task but they worked only with a list
of predefined nouns representing events. In this
work, we focus on the linguistic construction of
verb-noun (or noun phrase) pairs where noun can
be of any semantic type.
Traditional approaches for identifying causal-
ity mainly employ linguistic features (e.g., lexical
items, part-of-speech tags of words, etc.) in the
framework of supervised learning (Girju, 2003;
Sporleder and Lascarides, 2008; Bethard and Mar-
tin, 2008; Pitler and Nenkova, 2009; Pitler et al.,
2009) and do not involve deeper semantics of lan-
guage. Analysis of such approaches by Sporleder
and Lascarides (2008) have revealed that the lin-
guistic features are not always sufficient to achieve
a good performance on the task of identifying se-
mantic relations including causality. In this work,
we propose a model that deeply processes and
acquires the specific semantic information about
the participants of a verb-noun phrase (v-np) pair
(i.e., noun and verb semantics) to identify causal-
ity with a better performance over the baseline
model depending merely on shallow linguistic fea-
tures.
The work in this paper builds on our recent work
reported in Riaz and Girju (2014). In that previ-
ous model, we identified the semantic classes of
nouns and verbs with a high and low tendency to
encode causation. For example, a named entity
such as LOCATION may have the least tendency
to encode causation. We leveraged such informa-
tion about nouns to filter false positives. Sim-
ilarly, we utilized the TimeBank?s (Pustejovsky
et al., 2006) classification of verbal events (i.e.,
Occurrence, Perception, Aspectual, State, I State,
I Action and Reporting) and their definitions to
claim that the reporting events (e.g., say, tell, etc.)
161
just describe and narrate other events instead of
encoding causality with them. We proposed an In-
teger Linear Programming (ILP) model (Roth and
Yih, 2004; Do et al., 2011) to combine noun and
verb semantics with the decisions of a supervised
classifier which only relies on linguistic features.
In this paper, we extend our previous model by
acquiring and exploiting the following three novel
types of knowledge:
1. We learn the information about tendencies of
various verb frames to encode causation. For
example, our model identifies if the subject of
verb ?destroy? (?occur?) has a high (low) ten-
dency to encode causation. Such information
helps gain performance by exploiting causal
semantics of each verb frame separately. We
also learn and incorporate information about
the verb frames in general e.g., how likely it is
for the subject of any verb to encode causation
with its verb.
2. In Riaz and Girju (2014), we utilized the Time-
Bank?s definition of reporting events to argue
that such events have the least tendency to en-
code causation. Instead of relying on human
judgment we now introduce a data intensive ap-
proach to identify the TimeBank?s classes of
events with the least tendency to encode cau-
sation.
3. Although, information about the nouns with
the least tendency to encode causation helps to
filter false positives it can lead to false nega-
tives when metonymic readings are associated
with such nouns. Therefore, we introduce a
metonymy resolver on top of our current model
to avoid false negatives.
We provide details of our previously proposed
model in section 3. We introduce new model and
discuss its performance in sections 4 and 5. Sec-
tion 6 concludes the current research.
2 Relevant Work
In Natural Language Processing (NLP), re-
searchers are showing lots of interest in the task
of identifying causality due to its various applica-
tions e.g., question answering (Girju, 2003), sum-
marization (Chklovski and Pantel, 2004), future
prediction (Radinsky and Horvitz, 2013), etc.
Several approaches have been proposed to iden-
tify causality in pairs of verbal events (Bethard
and Martin, 2008; Riaz and Girju, 2010; Do et
al., 2011; Riaz and Girju, 2013) and discourse
segments (Sporleder and Lascarides, 2008; Pitler
and Nenkova, 2009; Pitler et al., 2009). However
causality a pervasive relation of language can be
encoded via various linguistic constructions. For
example, verbs and nouns are the key components
of language to represent events. Therefore in this
work we focus on identifying causality in verb-
noun pairs.
Previously researchers have followed the path
of utilizing linguistic features in the framework
of supervised learning (Girju, 2003; Bethard and
Martin, 2008; Sporleder and Lascarides, 2008;
Pitler and Nenkova, 2009; Pitler et al., 2009).
Though linguistic features are important but other
sources of knowledge are also critically required
to achieve progress on the current task.
In recent years, researchers have proposed un-
supervised metrics to identify causality between
events (Riaz and Girju, 2010; Do et al., 2011).
For example, Riaz and Girju (2010) and Do et
al. (2011) introduced unsupervised metrics to
learn causal dependencies between events. These
metrics mainly depend on probabilities of co-
occurrences of events and do not distinguish well
causality from any other types of correlation (Riaz
and Girju, 2013). In order to overcome this prob-
lem Riaz and Girju (2013) proposed some ad-
vanced metrics which combine probabilities of co-
occurrences of events with the supervised esti-
mates of cause and non-cause relations.
Considering the importance of employing rich
sources of knowledge other than linguistic features
for the current task, we have recently proposed a
model that incorporates semantic classes of nouns
and verbs with a high and low tendency to encode
causation (Riaz and Girju, 2014). In this work, we
exploit information about verb frames, data-driven
verb semantics and metonymies to achieve more
progress on our recent work.
3 Model for Recognizing Causality
In this section we provide an overview of our pre-
vious model (Riaz and Girju, 2014) for identifying
causality in v-np pairs where v (np) stands for verb
(noun phrase). This model works in the following
two stages: (1) A supervised classifier is used to
make binary predictions (i.e., the label cause (C)
or non-cause (?C)) employing linguistic features,
and (2) noun and verb semantics are then com-
bined with the predictions of supervised classifier
in the ILP framework to identify causality.
162
3.1 Supervised Classifier
To the best of our knowledge, there is no data
set of v-np pairs with the labels C and ?C avail-
able to us. For the current task we employ some
heuristics to extract a training corpus of v-np pairs
using FrameNet (Baker et al., 1998). FrameNet
provides frame elements for the verbs and hand
annotated examples (aka annotations) of these
frame elements. Consider the following annota-
tion from FrameNet ?They died [
Cause
from shot-
gun wounds]? where the frame element ?Cause?
is given for the verb ?died?. We remove the prepo-
sition ?from? from the above annotation of frame
element to acquire an instance of v-np (i.e., died-
shotgun wounds) pair. We extract all annotations
for verbs from FrameNet in which a frame element
must contain at least one noun and no verb in it.
We found such annotations for 729 distinct frame
elements. We manually assigned the labels C and
?C to these frame elements. Cause, Purpose, Rea-
son, Result, Explanation are some examples of the
frame elements to which we assigned the label C.
Using the above mentioned assignments of labels
C and ?C to frame elements, we have acquired
a training corpus of 4, 141 (77, 119) C (?C) in-
stances from FrameNet. In order to avoid class im-
balance while training we employ an equal num-
ber of instances of both labels.
Due to space constraints, we refer the reader to
Appendix A for the details of linguistic features
to build the supervised classifier. We employ both
Naive Bayes (NB) and Maximum Entropy (Max-
Ent) algorithms to acquire predictions and prob-
abilities of assignments of labels. We set up the
following ILP using these probabilities:
Z
1
= max
?
v-np?I
?
l?L
1
x
1
(v-np, l)P (v-np, l) (1)
?
l?L
1
x
1
(v-np, l) = 1 ? v-np ? I (2)
x
1
(v-np, l) ? {0, 1} ? v-np ? I ?l ? L
1
(3)
Here, L
1
= {C,?C}, I is the set of all v-np pairs.
x
1
(v-np, l) is a binary decision variable set to 1
only if the label l ? L
1
is assigned to a v-np pair
and only one label out of |L
1
| choices can be as-
signed to a v-np pair (see constraints 2 and 3). In
particular, we maximize the objective function Z
1
(1) assigning the labels l ? {L
1
} to v-np pairs de-
pending on the probabilities of assignments (i.e.,
P (v-np, l)) obtained through the supervised clas-
sifier.
3.2 Noun and Verb Semantics
We automatically acquire and employ semantic
classes of nouns and verbs with a high and low
tendency to encode causation. Such information
helps to reduce errors in predictions of the super-
vised classifier.
We derive two semantic classes of nouns for our
purpose i.e., C
np
and ?C
np
where the class C
np
(?C
np
) represents the noun phrases with a high
(low) tendency to encode causation. For exam-
ple, a noun phrase expression for a location has
the least tendency to encode causation unless a
metonymic reading is associated with it. In or-
der to acquire these classes, we extract annotations
of 936 distinct frame elements from FrameNet
in which a frame element must contain at least
one noun and no verb in it. These annotations
of frame elements roughly represent instances of
noun phrases (np). We manually assigned the la-
bels C
np
and ?C
np
to the frame elements. For
example, we assign the label ?C
np
to the frame
element ?Place? which represents a location (see
Appendix B for some examples of the frame ele-
ments with labels C
np
and ?C
np
). We also fol-
low the approach similar to Girju and Moldovan
(2002) to employ WordNet senses of nouns to ac-
quire more instances of the classes C
np
and ?C
np
(see Appendix B for the details). We have ac-
quired a total of 280, 212 instances of np (50%
for each of the two classes i.e., C
np
and ?C
np
)
using both FrameNet and WordNet. Using these
instances, we build a supervised classifier to iden-
tify the semantic class of np (see Appendix B for
the details of features to build the classifier). We
incorporate the knowledge of semantic classes of
nouns by making the following additions to ILP:
Z
2
= Z
1
+
?
v-np?I?M
?
l?L
2
x
2
(f
np
(v-np), l)P (f
np
(v-np), l)
(4)
?
l?L
2
x
2
(f
np
(v-np), l) = 1 ? v-np ? I ?M (5)
x
2
(f
np
(v-np), l) ? {0, 1} ? v-np ? I ?M (6)
?l ? L
2
x
1
(v-np,?C)? x
2
(f
np
(v-np),?C
np
) ? 0 (7)
? v-np ? I ?M
Here L
2
= {C
np
,?C
np
}. f
np
(v-np) is a func-
tion which returns np of a v-np pair. M is the set
of v-np pairs with metonymic readings associated
with np. Currently, this set is empty and in sec-
tion 4.3 we introduce a metonymy resolver to pop-
163
ulate this set. x
2
(f
np
(v-np), l) is a binary decision
variable set to 1 only if the label l ? L
2
is assigned
to np and only one label out of |L
2
| choices can
be assigned to np (see constraints 5 and 6). Con-
straint 7 enforces that if an np belongs to the class
?C
np
then its corresponding v-np pair is assigned
the label ?C. In particular, we maximize the ob-
jective function Z
2
(4) subject to the constraints
introduced till now. For each v-np pair, we predict
the semantic class of np using our supervised clas-
sifier for the labels l ? L
2
and set the probabilities
? i.e., P (f
np
(v-np), l) = 1, P (f
np
(v-np), {L
2
} ?
{l}) = 0 if the label l ? L
2
is assigned to np. Also
before running our supervised classifier, we run a
named entity recognizer (Finkel et al., 2005) and
assign the label ?C
np
to all noun phrases identi-
fied as named entities. We also determine associa-
tion of metonymies with the noun phrases identi-
fied as named entities.
For the current task we also acquire two seman-
tic classes of verbs i.e., C
e
v
and ?C
e
v
where the
class C
e
v
(?C
e
v
) contains the verbal events with a
high (low) tendency to encode causation. In order
to derive these two classes we exploit the Time-
Bank corpus (Pustejovsky et al., 2003) which pro-
vides seven semantic classes of verbal events ? i.e.,
Occurrence, Perception, Aspectual, State, I State,
I Action and Reporting. According to the defini-
tions of these classes, we claim that the report-
ing events (e.g., say, tell, etc.) just describe and
narrate other events instead of encoding causality
with them. Using this claim, we consider that all
instances of reporting verbal events of TimeBank
belong to the class ?C
e
v
and the rest of instances
of verbal events lie in the class C
e
v
. After ac-
quiring instances of the classes C
e
v
and ?C
e
v
, we
build a supervised classifier for these two classes.
We use the features introduced by Bethard and
Martin (2006) to build this classifier (see Bethard
and Martin (2006) for the details). Employing pre-
dictions and probabilities of assignments of the la-
bels C
e
v
and ?C
e
v
we add the following two con-
straints to ILP: (1) if the event represented by v
belongs to ?C
e
v
then the corresponding v-np pair
must be labeled with ?C and (2) if a v-np pair is
a causal pair then the event represented by v must
be labeled with C
e
v
.
4 Enriched Verb and Noun Semantics
This section describes the novel contributions of
this work i.e., identification of semantics of verb
frames, semantic classes of verbal events via a data
intensive approach and association of metonymic
readings with noun phrases to identify causality
with a better performance.
4.1 Verb Frames
We introduce a method to acquire tendencies of
various verb frames to encode causation. Consider
the following two examples to understand the ten-
dencies of verb frames of form {v, gr} to encode
causation where v is the verb and gr is the gram-
matical relation of np with the verb v.
1. The Great Storm of October 1987 almost totally de-
stroyed the eighty year old pinetum at Nymans Garden
in Sussex. (Cause (C))
2. The explosion occurred in the city?s main business area.
(Non-Cause (?C))
In above two examples the nps ?The Great
Storm of October 1987? and ?The explosion? have
the grammatical relations of subject with the verbs
?destroyed? and ?died?. In examples (1) and (2)
the verb frames {destroy, subject} and {occur,
subject} encode cause and non-cause relations.
These examples reveal that each verb frame has
its own tendency to encode causation. This type
of knowledge helps gain performance by exploit-
ing the semantics of each verb frame separately.
We leverage FrameNet annotations to acquire
such type of knowledge. We collect all annota-
tions of verbs from FrameNet and assign the la-
bels C and ?C to the frame elements as discussed
in section 3.1. In FrameNet, example (1) is given
as follows:
3. [
Cause
The Great Storm of October 1987] [
Degree
almost
totally] destroyed [
Undergoer
the eighty year old pinetum
at Nymans Garden in Sussex].
According to our assignments of labels C and
?C to the frame elements, example (1) is given
as ?[
C
The Great Storm of October 1987] [
?C
al-
most totally] destroyed [
?C
the eighty year old
pinetum at Nymans Garden in Sussex].?. After ac-
quiring instances of the labels C and ?C from ex-
ample (1), we populate the fields of a knowledge
base of verb frames (see Table 1). Fields of this
knowledge base are {v, gr}, count({v, gr},C) and
count({v, gr},?C). gr is the dependency relation
of the frame element with the verb v. We use Stan-
ford?s dependency parser (Marneffe et al., 2006)
to collect dependency relations. count({v, gr},C)
(count({v, gr},?C)) is the count of the label C
(?C) of the frame {v, gr}. As shown in Table 1,
164
for the frame element ?The Great Storm of Octo-
ber 1987?, the word ?Storm? has the dependency
relation of ?nsubj? with the verb ?destroy?. If
there exists more than one dependency relations
between the frame element and its verb then we
choose the very first relation in the text order. Ac-
cording to the counts given in Table 1, {destroy,
nsubj} has more tendency to encode a cause re-
lation than the non-cause one. We have acquired
7,156 and 114,898 instances of the labels C and
?C from FrameNet for populating the knowledge
base of verb frames. We compute tendencies of
verb frames to encode causality using the follow-
ing scores:
S({v, gr}, l) = S
1
({v, gr}, l)? S
2
({*, gr}, l) (8)
S
1
({v, gr}, l) =
count({v,gr},l)
count({v,gr},l)+count({v,gr},L
1
?{l})
S
2
({*, gr}, l) =
count({*,gr},l)
count({*,gr},l)+count({*,gr},L
1
?{l})
Counts of first component (S
1
) can be taken
from the knowledge base of verb frames of form
{v, gr}. The second component (S
2
) with counts
count({*, gr}, l) and count({*, gr}, L
1
? {l})
captures tendencies of verb frames in general.
For example, what is the tendency of any subject
to encode causality with its verb i.e., the score
S
2
({?, nsubj},C). We populate the knowledge
base of Table 1 with equal number of C and ?C
instances to calculate counts for S
2
. We make the
following additions to ILP to incorporate informa-
tion about verb frames:
Z
3
= Z
2
+
?
v-np?I?
g(v-np)?KB?
f
np
(v-np)?C
np
?
l?L
1
x
3
(g(v-np), l)S(g(v-np), l)
(9)
?
l?L
1
x
3
(g(v-np), l) = 1 ?
v-np?I?
g(v-np)?KB?
f
np
(v-np)?C
np
(10)
x
3
(g(v-np), l) ? {0, 1} ?l ? L
1
,?
v-np?I?
g(v-np)?KB?
f
np
(v-np)?C
np
(11)
x
3
(g(v-np), l) ? x
1
(v-np, l) ?l ? L
1
, (12)
?
v-np?I
?g(v-np)?KB
? f
np
(v-np)?C
np
x
1
(v-np, l) ? x
3
(g(v-np), l) ?l ? L
1
, (13)
?
v-np?I?
g(v-np)?KB?
f
np
(v-np)?C
np
Here, KB is the knowledge base of verb frames
and g(v-np) is the function which returns the verb
frame i.e., {v, gr}. This function returns NULL
value if there is no grammatical relation between
v and np in an instance. The above changes in
ILP are only applicable for the v-np pairs with
{v, gr} count({v, gr},C) count({v, gr},?C)
{destroy,nsubj} 1 0
{destroy,advmod} 0 1
{destroy,dobj} 0 1
[
C
The Great Storm of October 1987] [
?C
almost totally] de-
stroyed [
?C
the eighty year old pinetum at Nymans Garden in
Sussex].
Table 1: A knowledge base of verb frames. This
knowledge base is populated using the instances
of C and ?C labels given in this table.
g(v-np) ? KB and np identified as of class C
np
because we have already filtered the cases of np ?
?C
np
in section 3.2. x
3
(g(v-np), l) is a binary de-
cision variable set to 1 only if the label l ? L
1
is assigned to g(v-np) and only one label out of
|L
1
| choices can be assigned to g(v-np) (see con-
straints 10 and 11). We add information about verb
frames using constraints 12 and 13. These con-
straints enforce the predictions of the supervised
classifier of causality (section 3.1) to be consis-
tent with the predictions using tendencies of verb
frames (i.e., score S({v, gr}, l)). We maximize
objective function (9) subject to the above con-
straints. We remove those {v, gr} from KB which
have count({v, gr},C)+count({v, gr},?C) < 5
to avoid wrong predictions based on the small
counts of verb frames.
4.2 Data-driven Verb Semantics
In section 3.2 we considered that reporting events
belong to the class ?C
e
v
with the least tendency
to encode causation using the definition of these
events in the TimeBank corpus. Instead of re-
lying on definitions of events we now introduce
a data intensive approach to automatically iden-
tify the class ?C
e
v
of verbal events. In order
to identify this class we extract training instances
of verbal events encoding C and ?C relations.
Verbal events encode cause-effect relations using
verb-verb (e.g., Five shoppers were killed when a
car blew up.) and verb-noun linguistic construc-
tions. Therefore for the current purpose we use
the following two types of training instances: (A)
a training corpus of 240K instances of verb-verb
(v
i
-v
j
) pairs encoding C and ?C relations (named
as Training
v
i
-v
j
) (we refer the reader to Riaz and
Girju (2013) for the details of this training corpus)
and (B) the training corpus v-np instances intro-
duced in section 3.1 (named as Training
v-np
).
Following is the procedure to derive V
?C
?
V where V={Occurrence, Perception, Aspectual,
State, I State, I Action, Reporting} and the set
V
?C
contains the TimeBank?s semantic classes
165
with the least tendency to encode a cause relation.
1. Input: Training corpus, V
2. Output: Set V
?C
3. For each training instance k employ the supervised clas-
sifier of Bethard and Martin (2006) to do the following:
(a) if k ? Training
v
i
-v
j
then identify the semantic class
(sc) of both events represented by both verbs v
i
and
v
j
and add this information to a set i.e., T = T ?
(k
v
i
, sc
v
i
, l) ? (k
v
j
, sc
v
j
, l) where sc
v
i
is the se-
mantic class of event of the verb v
i
of instance k
and l ? {C,?C}.
(b) Else if k ? Training
v-np
then identify the semantic
class (sc) of event represented by the verb v and set
T = T ? (k
v
, sc
v
, l).
4. Using results of step 3, calculate tendency of each se-
mantic class sc ? V to encode non-causality (i.e.,
score(sc,?C)) as follows:
score(sc,?C) = score
1
(sc,?C)? score
2
(sc,?C)
score
1
(sc,?C) = (
count(sc,?C)
count(sc)
?
count(sc,C)
count(sc)
)
score
2
(sc,?C) = (
count(sc,?C)
count(?C)
?
count(sc,C)
count(C)
)
where count(m, n) is the number of instances of verbal
events with the labels m and n and count(m) is the number
of instances of verbal events with the label m.
5. Acquire a ranked list of semantic classes list
sc
= [sc
1
, sc
2
,
. . . sc
m
] s.t. score(sc
i
,?C) ? score(sc
i+1
,?C). From
this list we remove the class sc
i
if either score
1
(sc
i
, ?c)
< 0 or score
2
(sc
i
, ?c) < 0.
. The following steps are used to determine the cutoff
class sc
i
? list
sc
s.t. the semantic classes {sc
1
, sc
2
, . . .,
sc
i-1
} have the least tendency to encode causation.
6. result
sc
?1
= 0 and result
sc
0
= 0.
7. Remove sc
i
from the front of list
sc
and do the following:
(c) Predict the label (l) ?C for all tuples of form
(m, sc, l) ? T if sc ? {sc
1
, sc
2
, . . ., sc
i
} and pre-
dict C for the rest of the tuples.
(d) Using predictions from step (c), calculate the
result
sc
i
= F1-score ? accuracy for the label l ?
{C,?C}.
(e) If result
sc
i
?result
sc
i-1
< result
sc
i-1
?result
sc
i-2
then output {sc
1
, sc
2
, . . ., sc
i?1
}
(f) Else go to step 7.
Using the above procedure, we obtain the
sets {Aspectual} and {Reporting, I State} with
Training
v
i
-v
j
and Training
v-np
corpora. We con-
sider that the Aspectual, Reporting and I State
events of the TimeBank corpus belong to the class
?C
e
v
and rest of the events lie in C
e
v
. Using these
semantic classes we apply the constraints intro-
duced in section 3.2.
4.3 Metonymy Resolution:
Metonymy resolution is the task to determine if a
literal or non-literal reading is associated with a
{v, gr} count({v, gr},C
np
) count({v, gr},?C
np
)
{kill,nsubj} 1 0
{kill,dobj} 0 1
[
C
np
Pissed off Angelus] just kills [
?C
np
me]
Table 2: A knowledge base of verb frames. This
knowledge base is populated using the instances
of C
np
and ?C
np
labels given in this table.
natural language expression (Markert and Nissim,
2009). Consider the following example:
4. The United States has killed Osama bin Laden and has
custody of his body. (Cause (C))
In example (4) ?The United States? refers to a
non-literal reading i.e., the event of ?raid in Ab-
bottabad on May 2, 2011 by the United States?
rather than merely referring to a literal sense i.e.,
a country. The association of non-literal reading
with ?The United States? results in killing event.
Previously, researchers have worked with hand-
annotated selectional restrictions violation for this
task (Markert and Nissim, 2009). In the exam-
ple (4) a country cannot ?kill? someone and thus
a metonymic reading is associated with it. In this
work we identify association of metonymies with
noun phrases via verb frames and prepositions as
explained below in this section.
In the first part of our approach we employ
violations of tendencies of verb frames to iden-
tify if a non-literal reading is associated with a
noun phrases. Particularly, we build a knowledge
base of verb frames using C
np
and ?C
np
classes
as discussed in section 4.1. Consider the knowl-
edge base given in Table 2 populated using the
following FrameNet annotations ?[
Stimulus
Pissed
off Angelus] just kills [
Experiencer
me].? with as-
signments of labels C
np
and ?C
np
to the frame
elements. We populate the knowledge base using
only those FrameNet annotations in which a frame
element does not contain a verb.
Now we introduce our method to identify the
association of non-literal reading with the ?The
United States? in example (4). The supervised
classifier predicts the class ?C
np
for the np ?The
United States?. However, in the current state
of knowledge base (Table 2) P({destroy, nsubj},
C
np
) > P({destroy, nsubj}, ?C
np
) where P is
the probability. The prediction of ?C
np
for ?The
United States? violates the above probabilities.
Considering this violation, we predict the associa-
tion of metonymy with np.
In the second part of our approach we iden-
tify tendencies of prepositions to encode causation
166
and use violation of these tendencies to identify
metonymies. For this purpose, we use the training
corpus of v-np pairs with 4, 141 C and 77, 119 ?C
training instances (see section 3.1). We employ
only those training instances in which a preposi-
tion appears between v and np and there appears
no verb between them. From these instances, we
acquire a set of prepositions that appear between
v and np. Using this set of prepositions (PR) as
input to the following procedure, we acquire a set
of prepositions (PR
C
) with the highest tendency
to encode causation:
1. Input: Training Corpus of v-np pairs, PR
2. Output: PR
C
3. Calculate tendency of each preposition pr ? PR to en-
code causality (i.e., score(pr,C)) as follows:
score(pr,C) = score
1
(pr,C)? score
2
(pr,C)
score
1
(pr,C) = (
count(pr,C)
count(pr)
?
count(pr,?C)
count(pr)
)
score
2
(pr,C) = (
count(pr,C)
count(C)
?
count(pr,?C)
count(?C)
)
4. Acquire a ranked list of prepositions list
pr
= [pr
1
, pr
2
, . . .
pr
m
] s.t. score(pr
i
,C) ? score(pr
i+1
,C). From this
list we remove pr
i
if either score
1
(pr
i
, C) or score
2
(pr
i
,
C) < 0.
5. result
pr
?1
= 0, result
pr
0
= 0
6. Remove pr
i
from the front of the list
pr
and do the follow-
ing:
(a) Predict the label C for all v-np training instances
with pr ? {pr
1
, pr
2
, . . ., pr
i
} and assign the label
?C to the rest of the instances.
(b) Using predictions from step (a) calculate the
result
pr
i
= F1-score ? accuracy.
(c) If result
pr
i
-result
pr
i-1
< result
pr
i-1
-result
pr
i-2
then
output {pr
1
, pr
2
, . . ., pr
i?1
}.
(d) Else go to step 6.
The above procedure outputs the set PR
C
=
{for, by}. Now we introduce method to identify
association of non-literal reading for the example
?All weapon sites in Iraq were destroyed by the
United States? where ?the United States? ? ?C
np
as identified by the supervised classifier. However,
the preposition ?by? has a high tendency to en-
code causation and thus ?the United States? may
encode causation. Therefore, there is a possibil-
ity that this noun phrase has a non-literal sense
attached to it which results in encoding causality.
Using this method, we predict metonymies only
for the v-np instances where preposition appears
between v and np and there appears no verb be-
tween them. If any of two methods of metonymy
resolution predicts the association of metonymy
with np then we add v-np to the set M used in
ILP (see section 3.2).
5 Evaluation and Discussion
In this section we present experiments and discus-
sion on the performance achieved for the current
task. In order to evaluate our model, we generated
a test set of instances of v-np pairs. For this pur-
pose, we collected three wiki articles on the topics
of Hurricane Katrina, Iraq War and Egyptian Rev-
olution of 2011. We apply a part-of-speech tagger
and a dependency parser on all sentences of these
three articles (Toutanova et al., 2003; Marneffe et
al., 2006). We extracted all v-np pairs from each
sentence of these articles. For each of the these
three articles, we selected first 500 instances of v-
np pairs. Two annotators were asked to provide the
labels C and ?C to the instances of v-np pairs us-
ing the annotation guidelines from Riaz and Girju
(2010). We have achieved a 0.64 kappa score for
the human inter-annotator agreement on a total of
1,500 v-np instances. This results in a total of
1,365 instances of v-np pairs with 11.86% C pairs.
In this section, we present performance of the
following models (see Table 3):
1. Baseline: NB and MaxEnt (McCallum, 2002)
supervised classifiers using only the shallow
linguistic features (see section 3.1).
2. Basic noun and verb semantics: ILP with
the addition of semantic classes of nouns
without metonymy (denoted by +N
!M
) and
the addition of semantic classes of verbs
where ?C
e
v
={(R)eporting events} (denoted by
+N
!M
+V
{R}
). These models represent the
work proposed in Riaz and Girju (2014) (sec-
tion 3).
3. Noun semantics with metonymies: ILP
with the addition of noun semantics involv-
ing metonymies resolved via verb frames (de-
noted by +N
M
1
), metonymies resolved via verb
frames {v, gr} where gr ? GR = {csubj, csub-
jpass, nsubj, nsubjpass, xsubj, dobj, iobj, pobj,
agent} a set of core dependency relations of
subjects and objects (denoted by +N
M
1
GR
) and
metonymies resolved via both verb frames and
prepositions (denoted by +N
M
1
GR
+M
2
).
4. Verb frames and data-driven verb seman-
tics: ILP with the addition of information about
verb frames (denoted by +N
M
+VF where M =
M
1
GR
+M
2
), data-driven verb semantics i.e.,
?C
e
v
={(A)spectual, (R)eporting, (I) (S)tate
events} (denoted by +N
M
+V
{A,R,IS}
) and both
verb frames and data-driven verb semantics
(denoted by +N
M
+VF+V
{A,R,IS}
)
167
S B +N
!M
+N
!M
+V
{R}
+N
M
1
+N
M
1
GR
+N
M
1
GR
+M
2
+N
M
+VF +N
M
+V
{A,R,IS}
+N
M
+VF +V
{A,R,IS}
A 28.86 71.86 73.40 71.35 71.42 71.64 72.96 75.16 76.19
P 13.52 26.18 27.21 26.29 26.34 27.54 28.39 29.93 30.82
R 92.59 75.30 74.07 78.39 78.39 85.18 83.95 81.48 80.86
F 23.60 38.85 39.80 39.37 39.44 41.62 42.43 43.78 44.63
A 61.46 80.73 81.17 80.65 80.73 81.02 81.39 81.75 82.05
P 19.46 32.02 32.72 32.41 32.52 34.09 34.66 35.25 35.64
R 71.60 55.55 55.55 58.02 58.24 64.19 64.19 64.19 63.58
F 30.60 40.63 41.18 41.59 41.68 44.53 45.02 45.51 45.67
Table 3: Performance of (B)aseline, +N
!M
, +N
!M
+V
{R}
, +N
M
1
, +N
M
1
GR
, +N
M
1
GR
+M
2
, +N
M
+VF,
+N
M
+V
{A,R,IS}
and +N
M
+VF+V
{A,R,IS}
(see text for details) in terms of (S)cores of (A)ccuracy,
(P)recision, (R)ecall, (F)-score. The row 1 (2) of this table presents results over NB (MaxEnt) base-
line supervised classifier, respectively.
Table 3 shows that MaxEnt gives a very high ac-
curacy and F-score as compared with NB. Model
+N
!M
+V
{R}
with basic noun and verb seman-
tics introduced in section 3.2 results in more than
10% improvement in F-score over NB and Max-
Ent classifiers relying only on shallow linguis-
tic features. Model +N
M
+VF+V
{A,R,IS}
with en-
riched verb and noun semantics brings more than
4% improvement in F-score over +N
!M
+V
{R}
with MaxEnt as baseline. We perform statis-
tical significance test using bootstrap sampling
method given in Berg-Kirkpatrick et al. (2012)
(see Berg-Kirkpatrick et al. (2012) for the de-
tails). +N
M
+VF+V
{A,R,IS}
brings significant im-
provement in F-score over +N
!M
+V
{R}
with p-
value 0.0.
Though +N
!M
gives significantly better F-score
over baseline, it drops recall by more than 16%.
Metonymy resolution helps perform quite bet-
ter by recovering more than 8% recall with
+N
M
1
GR
+M
2
over +N
!M
. +N
M
1
GR
+M
2
also re-
sults in 3.9% improvement in F-score over +N
!M
with MaxEnt as baseline model (significant im-
provement with p-value 0.0). Metonymies re-
solved via verb frames with all and core grammat-
ical relations (i.e., set GR) recover more than 2%
recall and slightly improve F-score.
Model with the addition of information of verb
frames (i.e.,+N
M
+VF) brings 0.49% improve-
ment in F-score over +N
M
1
GR
+M
2
using Max-
Ent as baseline model (significant improvement
with p-value 0.027). Model with the addition of
data-driven verb semantics (i.e., +N
M
+V
{A,R,IS}
)
results in 0.98% improvement in F-score over
+N
M
1
GR
+M
2
using MaxEnt as baseline model
(significant improvement with p-value 0.0021).
Overall the model +N
M
+VF+V{A,R, IS} yields
more than 16% (20%) F-score (accuracy) over the
baseline models build via NB and MaxEnt.
5.1 Error Analysis
We performed error analysis for the model
+N
M
+VF+V
{A,R,IS}
by randomly selecting 50
False Positives (FP) and 50 False Negatives (FN).
For 32% FP instances information about verb
frames is not available in the knowledge base of
verb frames. To avoid this problem researchers
should exploit some abstractions e.g., {semantic
sense of v, gr} frames. Our model fails to iden-
tify the class ?C
np
for noun phrases of 29% FP
instances due to the lack of enough training data
for the semantic classes of nouns. In 21% FP
instances v and np are not even relevant to each
other. Our model first needs to determine rele-
vance between v and np before identifying causal-
ity. Remaining 18% instances have v and np in
temporal only sense, comparison relation or both
represent parts of same event. There is need to ex-
tract more knowledge sources to better distinguish
causality from any other type of relation.
77% FN instances are classified as non-causal
due to the lack of enough v-np training data and
require more sources of knowledge e.g., back-
ground knowledge. On remaining 23% FN in-
stances our model fails to identify C
np
class due
to the lack of enough training data for the seman-
tic classes of nouns.
6 Conclusion
This work has revealed that enriched semantics
of nouns and verbs help gain significant improve-
ment in performance over a baseline relying only
on shallow linguistic features. Through empiri-
cal evaluation and error analysis of our model we
have highlighted strengths and weaknesses of our
model for the current task. Our work has provided
a novel direction to exploit semantics of partici-
pants of causal relations to solve the challenge of
identifying causality.
168
References
Collin F. Baker, Charles J. Fillmore and John B. Lowe.
1998. The Berkeley FrameNet project. In proceed-
ings of COLING-ACL. Montreal, Canada.
Taylor Berg-Kirkpatrick, David Burkett, and Dan
Klein. 2012. An empirical investigation of statisti-
cal significance in NLP. In Proceedings of the Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL)..
Steven Bethard and James H. Martin. 2006. Identifica-
tion of Event Mentions and their Semantic Class. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP).
Steven Bethard and James H. Martin. 2008. Learning
Semantic Links from a Corpus of Parallel Temporal
and Causal Relations. In proceedings of ACL-08:
HLT.
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the Web for Fine-Grained Semantic
Verb Relations. In proceedings of Conference on
Empirical Methods in Natural Language Processing
(EMNLP-04). Barcelona, Spain.
Quang X. Do, Yee S. Chen and Dan Roth. 2011. Min-
imally Supervised Event Causality Identication. In
proceedings of EMNLP-2011.
Jenny R. Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating Non-local Informa-
tion into Information Extraction Systems by Gibbs
Sampling. In Proceedings of the Association for
Computational Linguistics (ACL).
Roxana Girju and Dan Moldovan. 2002. Mining An-
swers for Causation Questions. In American Asso-
ciations of Artificial Intelligence (AAAI), 2002 Sym-
posium.
Roxana Girju. 2003. Automatic detection of causal
relations for Question Answering. Association for
Computational Linguistics ACL, Workshop on Mul-
tilingual Summarization and Question Answering
Machine Learning and Beyond 2003.
Katja Markert and Malvina Nissim 2009. Data and
models for metonymy resolution. Language Re-
sources and Evaluation Volume 43 Issue 2, Pages
123?138.
Marie-Catherine de Marneffe, Bill MacCartney and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses.
In Proceedings of the International Conference on
Language Resources and Evaluation (LREC).
Andrew K. McCallum. 2002. MALLET:
A Machine Learning for Language Toolkit.
http://mallet.cs.umass.edu.
Emily Pitler, Annie Louis and Ani Nenkova. 2009.
Automatic Sense Prediction for Implicit Discourse
Relations in Text. In proceedings of ACL-IJCNLP,
2009.
Emily Pitler and Ani Nenkova. 2009. Using Syntax
to Disambiguate Explicit Discourse Connectives in
Text. In proceedings of ACL-IJCNLP, 2009.
James Pustejovsky, Patrick Hanks, Roser Saur, Andrew
See, Robert Gaizauskas, Andrea Setzer, Dragomir
Radev, Beth Sundheim, David Day, Lisa Ferro and
Marcia Lazo. 2003. The TIMEBANK Corpus. In
Proceedings of Corpus Linguistics.
Kira Radinsky and Eric Horvitz. 2013. Mining the
Web to Predict Future Events. In proceedings of
sixth ACM international conference on Web search
and data mining, WSDM ?13.
Mehwish Riaz and Roxana Girju. 2010. Another Look
at Causality: Discovering Scenario-Specific Contin-
gency Relationships with No Supervision. In pro-
ceedings of the IEEE 4th International Conference
on Semantic Computing (ICSC).
Mehwish Riaz and Roxana Girju 2013. Toward
a Better Understanding of Causality between Ver-
bal Events: Extraction and Analysis of the Causal
Power of Verb-Verb Associations. Proceedings of
the annual SIGdial Meeting on Discourse and Dia-
logue (SIGDIAL).
Mehwish Riaz and Roxana Girju 2014. Recognizing
Causality in Verb-Noun Pairs via Noun and Verb Se-
mantics. Proceedings of the Workshop on Computa-
tional Approaches to Causality in Language EACL,
2014.
Dan Roth and Wen-tau Yih 2004. A Linear Program-
ming Formulation for Global Inference in Natural
Language Tasks. In Proceedings of the Annual Con-
ference on Computational Natural Language Learn-
ing (CoNLL).
Caroline Sporleder and Alex Lascarides. 2008. Using
automatically labelled examples to classify rhetor-
ical relations: An assessment. Journal of Natural
Language Engineering Volume 14 Issue 3, July 2008
Pages 369?416.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Net-
work. In Proceedings of Human Language Technol-
ogy and North American Chapter of the Association
for Computational Linguistics (HLT-NAACL).
169
Appendix A. Supervised Classifier
In this appendix, we provide a set of linguistic fea-
tures taken from Riaz and Girju (2014) to iden-
tify causality in v-np pairs employing a supervised
classifier (see section 3.1 for the details).
? Lexical Features: verb, lemma of verb,
noun phrase, lemmas of all words of
noun phrase, head noun of noun phrase,
lemmas of all words between verb and
noun phrase.
? Syntatic Features: part-of-speech tags of verb
and head noun of noun phrase.
? Semantic Features: We adopted this fea-
ture from Girju (2003) to capture semantics
of nouns. The 9 noun hierarchies of Word-
Net i.e., entity, psychological feature, abstrac-
tion, state, event, act, group, possession, phe-
nomenon are used as this feature. Each of these
hierarchies is set to 1 if any sense of head noun
of noun phrase lies in that hierarchy, otherwise
set to 0.
? Structural Features: This feature is applied
by considering both subject (i.e., sub in np)
and object (i.e., obj in np) of verb (v). For ex-
ample, for the pair v-np the variable sub in np
is set to 1 if the subject ? np, set to 0 if the
subject 6? np and set to -1 if the subject is not
available in an instance. The subject and object
of a verb are its core arguments and may some-
time be part of an event represented by a verb.
Therefore, these arguments may have high ten-
dency to encode non-causation with their verb.
? Pairs: The following pairs (verb, head noun
of noun phrase), (subject
verb
, head noun of
noun phrase) and (object
verb
, head noun of
noun phrase) are used to capture relations.
Appendix B. Noun Semantics
In this appendix, some examples of the frame ele-
ments of FrameNet and the WordNet senses be-
longing to the classes C
np
and ?C
np
are given
in Tables 4 and 5 (see section 3.2 for the de-
tails). We employ training instances acquired us-
ing the FrameNet annotations and WordNet senses
for building a supervised classifier for the classes
C
np
and ?C
np
. Following is the list of features we
use for this supervised classifier:
? Lexical Features: All words of noun phrase,
lemmas of all words of noun phrase, head noun
of noun phrase, first two (three) (four) letters
SC FrameNet Labels
c
np
Event, Goal, Purpose, Cause, Internal cause, External
cause, Result, Means, Reason, Phenomena, Coordi-
nated event, Action, Activity, Circumstances, Desired
goal, Explanation
?c
np
Artist, Performer, Duration, Time, Place, Distributor,
Area, Path, Direction, Sub-region Frequency, Body
part, Area, Degree, Angle, Fixed location, Path shape,
Addressee, Interval
Table 4: Some examples of the frame elements of
FrameNet to which we assign the semantic classes
C
np
and ?C
np
.
SC WordNet Senses
c
np
{act, deed, human action, human activity},
{phenomenon}, {state}, {psychological feature},
{event}, {causal agent, cause, causal agency}
?c
np
{time period, period of time, period}, {measure,
quantity, amount}, {group, grouping}, {organization,
organisation}, {time unit, unit of time}, {clock time,
time}
Table 5: This table shows our selected Word-
Net senses of nouns belonging to classes C
np
and
?C
np
. For example, using the information pro-
vided in this table we assume that any noun con-
cept whose all senses of WordNet lie in the seman-
tic hierarchy of the sense {time period, period of
time, period} is of class ?C
np
. We use English
Gigaword corpus to collect instances of noun (or
noun phrases) and label them with C
np
and ?C
np
according to their senses in WordNet.
of head noun of noun phrase, last two, (three)
(four) letters of head noun of noun phrase.
? Word Class Features: part-of-speech tags of
all words of noun phrase and head noun of
noun phrase.
? Semantic Features: Frequent sense of head
noun of noun phrase.
170
