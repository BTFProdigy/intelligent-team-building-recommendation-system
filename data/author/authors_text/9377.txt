Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 481?488
Manchester, August 2008
Classifying What-type Questions by Head Noun Tagging 
Fangtao Li, Xian Zhang, Jinhui Yuan, Xiaoyan Zhu 
State Key Laboratory on Intelligent Technology and Systems 
Tsinghua National Laboratory for Information Science and Technology  
Department of Computer Sci. and Tech., Tsinghua University, Beijing 100084, China 
zxy-dcs@tsinghua.edu.cn 
 
Abstract 
Classifying what-type questions into 
proper semantic categories is found more 
challenging than classifying other types 
in question answering systems.  In this 
paper, we propose to classify what-type 
questions by head noun tagging. The ap-
proach highlights the role of head nouns 
as the category discriminator of what-
type questions. To reduce the semantic 
ambiguities of head noun, we integrate 
local syntactic feature, semantic feature 
and category dependency among adjacent 
nouns with Conditional Random Fields 
(CRFs). Experiments on standard ques-
tion classification data set show that the 
approach achieves state-of-the-art per-
formances. 
1 Introduction 
Question classification is a crucial component of 
modern question answering system. It classifies 
questions into several semantic categories which 
indicate the expected semantic type of answers to 
the questions. The semantic category helps to 
filter out irrelevant answer candidates, and de-
termine the answer selection strategies.   1 
The widely used question category criteria is a 
two-layered taxonomy developed by Li and Roth 
(2002) from UIUC. The hierarchy contains 6 
coarse classes and 50 fine classes as shown in 
Table 1. In this paper, we focus on fine-category 
classification. Each fine category will be denoted 
as ?Coarse:fine?, such as ?HUM:individual?. 
A what-type question is defined as the one 
whose question word is ?what?, ?which?, 
?name? or ?list?. It is a dominant type in ques-
tion answering system. Li and Roth (2006) find  
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
Coarse Fine 
ABBR abbreviation, expression 
DESC definition, description, manner, reason 
ENTY animal, body, color, creation, currency, dis-
ease/medicine, event, food, instrument, language, 
letter, other, plant, product, religion, sport, sub-
stance, symbol, technique, term, vehicle, word 
HUM description, group, individual, title 
LOC city, country, mountain, other, state 
NUM code, count, date, distance, money, order, other, 
percent, period, speed, temperature, size, weight 
Table 1.  Question Ontology 
that the distribution of what-type questions over 
the semantic classes is quite diverse, and they are 
more difficult to be classified than other types.  
Table 2 shows the classification accuracies of 
each question word in UIUC data set using Sup-
port Vector Machine (SVM) with unigram fea-
tures. What-type questions account for more than 
70 percent in the data set, but the classification 
accuracy of this type only achieves 75.50%. In 
this experiment, 90.53% (86 over 95) of the er-
rors are generated by what-type questions. Due to 
its challenge, this paper focuses on what-type 
question classification. 
 
 Total Wrong Accuracy 
What-type 351 86 75.50% 
Where 26 2 92.31% 
When 26 0 100.0% 
Who 47 3 93.62% 
How 46 4 91.30% 
Why 4 0 100.0% 
Total 500 95 81.00% 
Table 2.  Classification performance for each 
question words with unigram 
Head noun has been presented to  play an im-
portant role in classifying what-type questions 
(Metzler and Croft, 2005).  It refers to the noun 
reflecting the focus of a question, such as 
?flower? in the question ?What is Hawaii's state 
481
flower??. These nouns can effectively reduce the 
noise generated by other words. If the head noun 
?length? is identified from the question ?What is 
the length of the coastline of the state of 
Alaska??, this question can be easily classified 
into ?NUM:distance?. However, the above SVM 
misclassified this question into ?LOC:-state?, as 
the words ?state? and ?Alaska? confused the 
classifier. Considering another two questions 
expressed in (Zhang and Lee, 2002), ?Which 
university did the president graduate from?? and 
?Which president is a graduate of the Harvard 
University?, although they contain similar words, 
it is not difficult to distinguish them with the 
head nouns ?university? and ?president? respec-
tively. 
Nevertheless, a head noun may correspond to 
several semantic categories. In this situation, we 
need to incorporate the head noun context for 
disambiguation. The potentially useful context 
features include local syntactic features, semantic 
features and neighbor?s semantic category. Take 
the noun ?money? as an example, it possibly cor-
responds to two categories: ?NUM:money? and 
?ENTY:currency?. If there is an adjacent word 
falling into ?Loc:country? category, the ?money? 
tends to belong to ?ENTY:currency?. Otherwise, 
if the ?ENTY:product? or ?HUM:individual? 
surrounds it, the word ?money? may refer to 
?NUM:money?. 
Based on the above notions, we propose a new 
strategy to classify what-type questions by word 
tagging, and the selected head noun determines 
question category.  The question classification 
task is formulated into word sequence tagging 
problem. All the question words are divided into 
semantic words and non-semantic words. The 
semantic word expresses certain semantic cate-
gory, such as ?dog? corresponding to category 
?ENTITY:animal?, while ?have? corresponding 
to no category. The label for semantic words is 
one of the question categories, and ?O? is for 
non-semantic word. Here, we just consider the 
nouns as semantic words, others as non-semantic 
words. Each word in a question will be tagged as 
a label using Conditional Random Fields model, 
and the head noun?s label is chosen as the ques-
tion category.  
In conclusion, the CRFs based approach has 
two main steps: the first step is to tag all the 
words in questions using CRFs, and the second 
step is choosing the head noun?s label as the 
question category. It can use the head noun to 
eliminate the noisy words, and take advantages 
of CRFs model to integrate not only the syntactic 
and semantic features, but also the adjacent cate-
gories to tag head noun. 
The rest of this paper is organized as follows: 
Section 2 discusses related work. Section 3 in-
troduces the Condition Random Fields(CRFs) 
and the defined Long-Dependency CRFs 
(LDCRFs). Section 4 describes the features used 
in the LDCRFs. The head noun extraction me-
thod is presented in Section 5. We evaluate the 
proposed approach in Section 6. Section 7 con-
cludes this paper and discusses future work. 
2 Related works 
Question Answering Track was first introduced 
in the Text REtrieval Conference (TREC) in 
1999. Since then, question classification has been 
a popular topic in the research community of text 
mining. Simple question classification approach-
es usually employ hand-crafted rules (such as 
Prager et. al, 1999), which are effective for spe-
cific question taxonomy. However, laborious 
human effort is required to create these rules. 
Some other systems employed machine learn-
ing approaches to classify questions.  Li and 
Roth (2002) presented a hierarchical classifier 
based on the Sparse Network of Winnows (Snow) 
architecture. Tow classifiers were involved in 
this work: the first one classified questions into 
the coarse categories; and the other classified 
questions into fine categories. Several syntactic 
and semantic features, including semi-
automatically constructed class-specific relation-
al features, were extracted and compared in their 
experiments. The results showed that the hierar-
chical classifier was effective for question classi-
fication task.  
Metzler and Croft (2005) used prior know-
ledge about correlations between question words 
and types to train word-specific question classifi-
ers. They identified the question words firstly, 
and trained separate classifier for each question 
word. WordNet was used as semantic features to 
boost the classification performance. In this pa-
per, according to question word, all the questions 
are classifie into two categories: what-type ones 
and non-what-type one. 
Recent question classification methods have 
paid more attention on the syntactic structure of 
sentence. They used a parser to get the syntactic 
tree, and then took advantage of the structure 
information. Zhang and Lee (2002) proposed a 
tree kernel Support Vector Machine classifier 
and experiment results showed that syntactic in-
formation and tree kernel could solve this prob-
482
lem. Nguyen et al (2007) proposed a subtree 
mining method for question classification. They 
formulated question classification as tree catego-
ry determination, and maximum entropy and 
boosting model with subtree features were used. 
The experiment results showed that the subtree 
mining method can achieve a higher accuracy in 
question classification task.  
In this paper, we formulate the what-type 
question classification as word sequence tagging 
problem. The tagged label is either one of the 
question categories for nouns s or ?O? for other 
words. Since head noun can be the discriminator 
for a question, its tag is extracted as the question 
category in our work. A long-dependency Condi-
tional Random Fields Classifier is defined to tag 
question words with the features which not only 
include the syntactic and semantic features, but 
also the semantic categories? transition features. 
3 Conditional Random Fields 
Conditional Random Fields (CRFs) are a type of 
discriminative probabilistic model proposed for 
labeling sequential data (Lafferty et al 2001). Its 
definition is as follows:      
Definition: Let ( )G V E= ,  be a graph such that 
( )v v VY ?=Y , so that Y  is indexed by the vertices of G . 
Then ( ),X Y  is a conditional random field in case, when 
conditioned on X , the random variables vY  obey the Mar-
kov property with respect to the 
graph: ( )v wp Y Y w v| , , ? =X ( )v wp Y Y w v| , ,X ? , where w v?  
means that w  and v  are neighbors in G . 
   
The joint distribution over the label sequence Y  
given X  has the form  
1
( ) exp ( ) ( )
( ) i i i ie E i v V i
p t e e s v v
Z
? ?
? , ? ,
| = , | , + , | , ,? ?? ?? ?? ?Y X Y X Y XX
where ( )Z X  is a normalization factor, is  is a 
state feature function and it  is a transition fea-
ture function, i?  and i?  are the corresponding 
weights. 
 Here we assume the features are given, then 
the parameter estimation problem is to determine 
the parameters 1 2 1 2( , )? ? ? ? ?= , , , ,? ?  from 
training data. The inference problem is to find 
the most probable label sequence y?  for input 
sequence x . 
  In the training set, we label all the noun 
words with semantic question categories, and 
other words will be labeled by ?O?. We suppose 
that only adjacent noun words connect with each 
other, and there is no edge between noun and 
non-noun words, i.e., noun word and non-noun 
words may share neighbor?s state features, but 
they are not connected by an edge. A labeled ex-
ample is shown as ?What/O was/O 
Queen/HUM:individual Victria/HUM:individual 
?s/O title/HUM:title regarding/O India/LOC:-
country?. In this labeled sentence, only three 
edges connect four noun words: Queen, Victria, 
title and India.  
 
Figure 1. Long-Dependency CRFs, the dotted 
lines summarize many outgoing edges 
 
  With this assumption, we define a Long-
Dependency Conditional Random Fields 
(LDCRFs) model (see Figure 1). The long de-
pendency means that the target words may have 
no edge with its neighbors, but connect with oth-
er words at a long distance. It can be considered 
as a type of linear- chain CRFs.  Its parameter 
estimation problem and inference problem can be 
solved by the algorithm for chain-structure CRFs 
(Sutton and McCallum, 2007). 
4 Feature Sets 
One of the most attractive advantages  of CRFs is 
that they can integrate rich features, including 
not only state features, but also transition fea-
tures. In this section, we will introduce the syn-
tactic, semantic and transition features used in 
our sequence tagging approach. 
4.1 Syntactic Features 
The questions, which have similar syntactic style, 
intend to belong to the same category. Besides 
words, part-of-speech, chunker, parser informa-
tion and question length are used as syntactic 
features. 
All the words are lemmatized to root forms, 
and a window size (here is 4) is set to utilize the 
surrounding words. 
The part-of-speech (POS) tagging is com-
pleted by SS Tagger (Tsuruoka and Tsujii, 2005), 
with our own improvement. 
The noun phrase chunking (NP chunking) 
module uses the basic NP chunker software from 
483
(Ramshaw and Marcus, 1995) to recognize the 
noun phrases in the question.  
The importance of question syntactic structure 
is reported in (Zhang and Lee, 2002; Nguyen et 
al. 2007). They used complex machine learning 
method to capture the tree architecture. The 
LDCRFs based approach just selects parent node, 
relation with parent and governor for each target 
word generated from Minipar(Lin, 1999). 
The length of question is another important 
syntactic feature. In our experiment, a threshold 
is set to denote the length as ?high? or ?low?. 
4.2 Semantic Features 
Semantic features concern what words mean and 
how these meanings combine in sentence to form 
sentence meanings. Named Entity is a predefined 
semantic category for noun word. WordNet 
(Fellbaum, 1998) is a public semantic lexicon for 
English language, and it is used to get hypernym 
for noun word and synset for head verb which is 
the first notional verb in the sentence. 
   Named Entity: Named entity recognizer as-
signs a semantic category to the noun phrase. It 
is widely used to provide semantic information in 
text mining. In this paper, Stanford Named Entity 
Recognizer (Finkel et al 2005) is used to classify 
noun phrases into four semantic categories: 
PERSON, LOCATION, ORGANIZARION and 
MISC. 
Noun Hypernym: Hypernyms can be consi-
dered as semantic abstractions. It helps to narrow 
the gap between training set and testing set. For 
example, ?What is Maryland's state bird??, if we 
recursively find the bird?s hypernym ?animal?, 
which appeared in training set, this question can 
be easily classified. 
In training set, we try to select appropriate 
hypernyms for each category. An correct Word-
Net sense is first assigned for each polysemous 
noun, and then all its hypernyms are recursively 
extracted. The sense determination step is 
processed with the algorithm in (Pedersen et al 
2005). They disambiguate word sense by assign-
ing a target word the sense, which is most related 
to the senses of its neighboring words.  
Since the word sense disambiguation method 
has low performance, with F1-measure below 
50% reported in (Pedersen et al 2005), a feature 
selection method is used to extract the most dis-
criminative hypernyms. The hypernyms selection 
method is processed as follows: we first remove 
the low frequency hypernyms, and select the 
hypernyms using a chi-square method. The chi-
square value measures the lack of independence 
between a hypernym h and category jc . It is de-
fined as: 
2
2 ( ) ( )( , )
( ) ( ) ( ) ( )j
A B C D AD CB
h c
A C B D A B C D
? + + + ? ?= + ? + ? + ? +  
where A is the number of hypernym h, which 
belongs to category jc ; B is the number of  h out 
of jc ; C is the number of other hypernyms in jc ; 
D is the number of other hypernyms out of jc . 
We set a threshold to select the most discri-
minative hypernym set. Extracted examples are 
shown in Figure 2.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2. Examples of extracted hypernym  
 
It can be seen that these hypernyms are appro-
priate to describe the semantic meaning of the 
category.  They are expected to work as the 
class-specific relational features which are semi-
constructed by (Li and Roth, 2002). In our ap-
proach, we just use the noun?s minimum upper 
hypernym, existing in training set, as the feature. 
Head Verb Synset:  To avoid losing question 
verb information, we extract head verb, which is 
the first notional verb in a question, and expand 
it using WordNet synset as feature. The head 
verb extraction is based on the following simple 
rules: 
If the first word is ?name? or ?list?, the head 
verb will be denoted as this word. If the first verb 
following question word is ?do? or other aux-
iliary verb, the next verb is extracted as head 
verb. Otherwise the head verb is extracted as the 
first verb after question word. 
4.3 Transition Features 
State transition feature captures the contextual 
constraints among labels. We define it as  
( 1 ) ( )y yt e i i e e y y??, ?=< , + >, | , = | =< , > .Y X Y
ENTITY:animal: 
animal, carnivore, chordate, equine, 
horse, living_thing, vertebrate, mammal, 
odd-toed_ungulate, organism, placental  
ENTITY:food: 
alcohol, beer, beverage, brew, cereal, 
condiment, crop, drink, drug_of_abuse, 
flavorer, food, foodstuff, helping, indefi-
nite_quantity, ingredient, liquid, output, 
produce, small_indefinite_quantity, pro-
duction, solid, substance, vegetable 
484
Where e represents the edge between adjacent 
nouns. It captures adjacent categories as features 
to tag the target noun. Note that, for simplicity, 
the value of above feature is independent of the 
observations X.  
5 Head Noun Extraction 
After tagging all the words in a question, we will 
extract head noun and assign its tagged label to 
the question as the final question classification 
result. 
The head noun extraction is a simple heuristic 
method inspired by  (Metzler and Croft, 2005). 
We first run a POS tagger on each question, and 
post-process them to make sure that each sen-
tence has at least one noun word. Next, the first 
NP chunk after the question word is extracted by 
shallow parsing. The head noun is determined by 
the following heuristic rules: 
1. If the NP chunker is before the first verb, 
or the NP chunk is after the first verb but 
there is no possessive case after the NP 
chunker, we mark the rightmost word in 
the chunker as head noun. 
2. Otherwise, extract the next NP chunker 
and recursively process the above rules. 
Although this method may depend on the per-
formance of POS tagger and shallow parser, it 
achieves the accuracy of over 95% on the UIUC 
data set in our implementation. 
6 Experiments 
6.1 Experiment Settings 
Data Set: 
We evaluate the proposed approach on the 
UIUC data set (Li and Roth, 2002). 5500 ques-
tions are selected for training, and 500 questions 
are selected for testing. The classification catego-
ries have been introduced as question ontology   
in section 1. This paper only focuses on 50 fine 
classes. 
To train the LDCRFs, we manually labeled all 
the noun words with one of 50 fine categories.  
Other words are labeled with ?O?. One of the 
labeled examples is ?What/O was/O 
Queen/HUM:individual Victria/HUM:individual 
?s/O title/HUM:title regarding/O In-
dia/LOC:country?. Ten people labeled 3407 
what-type questions as training set. Each ques-
tion was independently annotated by two people 
and reviewed by the third. For words which have 
more than one category, the annotators selected 
the most salient one according to the context. For 
testing set, 351 what-type questions were se-
lected for experiments evaluation. 
Evaluation metric: 
Accuracy performance is widely used to evaluate 
question classification methods [Li and Roth, 
2002; Zhang and Lee, 2003, Melter and Croft, 
2004; Nguyen et al 2007].   
6.2 Approach Performance Evaluation 
 # Wrong Accuracy 
SVM 86 75.50% 
LDCRFs-
based 
80 77.20% 
Table 3. LDCRFS-based Approach V.S. SVM 
Table 3 shows the compared results between 
the proposed LDCRFs based approach and SVM 
with unigram feature. The LDCRFs based ap-
proach achieves accuracy of 77.20%, compared 
with 75.50% of SVM. Observing the detailed 
classification results, we conclude two advantag-
es of LDCRFs over SVMs. First LDCRFs based 
approach focuses on head noun to reduce the 
noise generated by other words. The question 
?What is the length of the coastline of the state of 
Alaska?? is misclassified as ?LOC:state? by 
SVM, whereas it is correctly classified by our 
approach. Second, LDCRFs based approach can 
utilize rich features, including not only state fea-
tures, but also transition features. With the new 
features involved, LDCRFs is expected to im-
prove classification performance. This unigram 
result is used as our baseline. The following ex-
periments are conducted to test the new feature 
contribution. 
Syntactic Features: 
In addition to words, four types of features, in-
cluding part-of-speech (POS), chunker, parser 
information (Parser), and question length 
(Length), are extracted as syntactic features. 
 
 Accuracy 
Unigram (U) 77.20% 
U+POS 78.35% 
U+Chunker 77.20% 
U+Parser 79.20% 
U+Length 77.49% 
Total Syn 80.06% 
Table 4. Syntactic Feature Performance 
From the syntactic feature results in Table 4, 
we can draw the following conclusions?  
(a). Among four types of syntactic features, pars-
485
er information contributes mostly. (Metzler and 
Croft, 2005) once claimed that it didn?t make 
improvement by just incorporating these infor-
mation as explicit feature, and they should be 
used implicitly via a tree structure. Without using 
the complex tree mining and representing tech-
nique, our LDCRFs-based approach just incorpo-
rates the word parent, relation with parent and 
word governor from Minipar as features. The 
experiments show that the parser information 
feature is able to capture the syntactic structure 
information, and it makes much improvement in 
this sequence tagging approach. 
(b) Question length makes small improvement. 
However, the chunker features make no im-
provement, consistent with the observation re-
ported by (Li and Roth, 2006). 
? The best accuracy (80.06%) is achieved by 
integrating all the syntactic features.  
Semantic Features: 
 
 Accuracy 
Unigram(U) 77.20% 
U+NE 77.20% 
U+HVSyn 78.63% 
U+NHype 78.35% 
Total Sem 80.06% 
Table 5. Semantic Feature Performance 
The semantic features include Named Entity 
(NE), Noun Hypernym (NHype) and Head Verb 
Synset (HVSyn).  
From Table 5 we can draw the following conclu-
sions: 
(a) NE makes no improvement in classification 
task. The reason is that the named entity recog-
nizer contains only four semantic categories. It is 
too coarse to distinguish 50 fined-categories. 
 (b) The LDCRFs-based approach just considers 
the noun words as semantic words. The head 
verb synsets (HVSyn) are imported as one of 
semantic features. The experiment results show 
that it is effective to incorporate the head verb as 
features, which achieves the best individual accu-
racy among semantic features. 
(c) Noun hypernyms (NHype) are the most im-
portant semantic features. They narrow the se-
mantic gap between training set and testing set. 
From Section 4.2, we can see that the selected 
noun hypernyms are appropriate for each catego-
ry. While, the experiment with NHype features 
doesn?t make considerable improvement as we 
previously thought. The reason may come from 
the fact that the word sense disambiguation me-
thod has low performance. A hypernym selection 
method is used in training set, but we didn?t 
tackle the error in testing set. Once the word 
sense disambiguation is wrong, it will not make 
improvement, but generate noise (see the discus-
sion examples in next section).  
(d) It is an interesting result that using all the se-
mantic features can achieve the same accuracy as 
the syntactic features (80.06%).  
Feature Combination: 
In this section, we carry out experiments to ex-
amine whether the performance can be boosted 
by integrating syntactic features and semantic 
features. Several results are shown in Table 6. 
The experiments show that: 
(a)  Parser Information and Head Verb Synset are 
both the most contributive features for syntactic 
set and semantic feature set. While the perfor-
mance with these two features can?t beat the per-
formance by combining Parser Information and 
Noun Hypernyms.  
 
 Accuracy 
U+POS+NE+HVSyn 80.91% 
U+Parser+NHype 81.77% 
U+Parser+HVSyn 80.91% 
U+POS+Length+NHype 80.63% 
Total 82.05% 
Table 6. Combined Feature Performance 
(b) The best result for classifying what-type 
questions with our approach is achieved by inte-
grating all the features. The accuracy is 82.05%, 
which is 18.7 percent error reduction (from 
22.08% to 17.95%) over unigram feature set. It 
shows that the features we extract are effectively 
used in our CRFs based approach. 
Transition Feature:  
Transition feature can capture the information 
between adjacent categories. It offers another 
semantic feature for LDCRFs-based approach.  
 
 No transition 
features  
With transi-
tion features 
Syn 79.20% 80.06% 
Sem 79.49% 80.06% 
Total 81.48% 82.05% 
Table 7. Transition Feature Performance 
The performances of all these three experi-
ment decline without the transition features. It 
shows that the dependency between adjacent se-
486
mantic categories contributes to the classifier 
performance.  
6.3 System Performance Comparison and 
Discussion  
In this section, the what-type questions and non-
what-type questions are combined to show the 
final result. Non-what-type questions are classi-
fied using SVM with unigrams as reported in 
Section 1, and what-type questions are classified 
by the LDCRFs based approach. The combined 
results are used to compare with the current 
question classification methods. 
 
Classifier Accuracy
Li?s Hierarchical method 84.20% 
Nguyen?s tree method 83.60% 
Metzler?s U+ WordNet method 82.20% 
LDCRFs-based with U+Parser 83.60% 
LDCRFs-based with U+NHype 83.00% 
LDCRFs-based with total features 85.60% 
Table 8. Comparison with related work 
Table 8 shows the accuracies of the LDCRFs 
based question classification approach with dif-
ferent feature sets, in comparison with the tree 
method (Nguyen et al 2007), the WordNet Me-
thod (Metzler and Croft, 2005) and the hierarchical 
method (Li and Roth, 2002). We can see the 
LDCRFs-based approach is effective: 
(a) Without formulating the syntactic structure as 
a tree, the LDCRFs-based approach still achieves 
accuracy 83.60% with unigram and parser infor-
mation, which is the same as Nguyen?s tree clas-
sifier.  
(b) Although the LDCRFs-based approach with 
unigrams and Noun Hypernyms generates 
noise as described in Section 6.2, it still out-
performs the Metzler?s method using WordNet 
and unigram features (83.00% v.s. 82.20%).  
(c) The experiment with total features achieves 
the accuracy of 85.60%. It outperforms Li?s Hie-
rarchical classifier, even they use semi-automatic 
constructed features.  
 
6.3.1 Analysis and Discussion  
Even the sequence tagging model achieves high 
accuracy performance, there still exists many 
problems. We use the matrix defined in Li and 
Roth (2002) to show the performance errors. The 
metric is defined as follows: 
*2 /( )ij i j i jD Err N N= +   
Where i jErr  is the number of questions in class 
i that are misclassified as belong to class j, Ni 
and Nj are the numbers of questions in class i and 
j respectively.  
From the matrix in Figure 3, we can see two 
major mistake pairs are ?ENTY:substance? and 
?ENTY:other?, ?ENTY:currency? and 
?NUM:money?. They really have similar mean-
ings, which confuses even human beings.  
 
 
Figure 3. The gray-scale map of Matrix D[n,n]. The 
gray scale of the small box in position [i,j] denotes 
D[i,j]. The larger Dij is, the darker the color is. 
 
Several factors influence the performance: 
(a) Head noun extraction error: This error is 
mainly caused by errors of POS tagger and shal-
low parser. For the wrong POS example 
?what/WP hemisphere/EX is/VBZ the/DT Phil-
ippines/NNPS in/IN ?/.?, ?Philippines? is ex-
tracted as head word. The result is misclassified 
into ?LOC:country?. For the shallow parser error 
example ?what/WP/B-NP is/VBZ/B-VP the/D 
T/BNP speed/NN/I-NP humminbirds/NNS /I-NP 
fly/V- BP/B-VP ?/./O?, ?hummingbirds? is ex-
tract as head word, rather than ?speed?. The 
question is misclassified into ?ENTY:animal?. 
(b) WordNet sense disambiguation errors: In 
question ?What is the highest dam in the U.S. ?? 
The real sense for dam is dam#1: a barrier con-
structed to contain the flow of water or to keep 
out the sea; while the disambiguation method 
determine the second sense as dam#2: a metric 
unit of length equal to ten meters.  
(c) Lack of head nouns: the CRFs based ap-
proach is sensitive to the Head Noun. If the ques-
tion doesn?t contain the head noun, it is difficult 
to produce the correct result, such as the question 
?What is done with worn or outdated flags?? In 
the future work, we will focus on the head noun 
absence problem. 
487
7 Conclusion 
In this paper, we propose a novel approach with 
Conditional Random Fields to classify what-type 
questions. We first use the CRFs model to label 
all the words in a question, and then choose the 
label of head noun as the question category.  As 
far as we know, this is the first trial to formulate 
question classification into word sequence tag-
ging problem. We believe that the model has two 
distinguished advantages: 
1. Extracting head noun can eliminate the noise 
generated by the non-head words 
2. The Conditional Random Fields model can 
integrate rich features, including not only the 
syntactic and semantic features, but also the 
transition features between labels.  
Experiments show that the LDCRFs-based ap-
proach can achieve comparable performance to 
those of the state-of-the-art question answering 
systems. With the addition of more features, the 
performance of the LDCRFs based approach can 
be expected to be further improved. 
Acknowledgement 
This work is supported by National Natural 
Science Foundation of China (60572084, 
60621062), Hi-tech Research and Development 
Program of China (2006AA02Z321), National 
Basic Research Program of China 
(2007CB311003).  Thank Shuang Lin and Jiao 
Li for revising this paper. Thanks for the review-
ers? comments. 
 References 
Christiane Fellbaum. 1998. WordNet: an Electronic 
Lexical Database. MIT Press. 
Prager, J., D. Radev, E. Brown, A. Coden, and V. 
Samn. 1999. `The use of predictive annotation for 
question answering in TREC'. In: Proceedings of 
the 8th Text Retrieval Conference (TREC-8). 
John Lafferty, Andrew McCallum, Fernando Pereira. 
2001. Conditional Random Fields: Probabilistic 
Models for Segmenting and Labeling Sequence Da-
ta. In Proceedings of ICML-2001. 
Li, X. and D. Roth. 2002. Learning question classifi-
ers. In Proceedings of the 19th International Confe-
rence on Compuatational Linguistics (COLING), 
pages 556?562. 
Zhang, D. and W. Lee. 2003. Question classification 
using support vector machines. In Proceedings of 
the 26th Annual International ACM SIGIR confe-
rence, pages 26?32 
Donald Metzler and W. Bruce Croft. 2004. Analysis 
of Statistical Question Classfication for Fact-based 
Questions. In Journal of Information Retrieval. 
Ted Pedersen, Satanjeev Banerjee, and Siddharth 
Patwardhan . 2005. Maximizing Semantic Related-
ness to Perform Word Sense Disambiguation. Uni-
versity of Minnesota Supercomputing Institute        
Research Report UMSI 2005/25, March. 
Xin Li, Dan Roth. 2006. Learning Question Classifi-
ers: The Role of Semantic Information. In Natural 
Language Engineering, 12(3):229-249 
Minh Le Nguyen, Thanh Tri Nguyen and Akira Shi-
mazu. 2007. Subtree Mining for Question Classifi-
cation Problem. In Proceedings of the 20th Interna-
tional Conference on Artificial Intelligence. Pages 
1695-1700. 
C. Sutton and A. McCallum. 2007. An introduction to 
conditional random fields for relational learning. 
In L. Getoor and B. Taskar (Eds.). Introduction to 
statistical relational learning. MIT Press. 
Y. Tsuruoka and J. Tsujii,. 2005. Bidirectional infe-
rence with the easiest-first strategy for tagging se-
quence data. In Proc. HLT/EMNLP?05, Vancouver, 
October, pp. 467-474. 
L. Ramshaw and M. Marcus. 1995. Text chunking 
using transformation-based learning, Proc. 3rd 
Workshop on Very Large Corpora, pp. 82?94. 
J.R. Finkel, T. Grenager and C. Manning. 2005. In-
corporating non-local information into information 
extraction systems by Gibbs sampling. Proc. 43rd 
Annual Meeting of ACL, pp. 363?370. 
D. Lin. 1999. MINIPAR: a minimalist parser. In Mar-
yland Linguistics Colloquium, University of Mary-
land, College Park. 
 
 
 
488
Proceedings of ACL-08: HLT, pages 710?718,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Using Conditional Random Fields to Extract Contexts and Answers of
Questions from Online Forums
Shilin Ding ? ? Gao Cong? ? Chin-Yew Lin? Xiaoyan Zhu?
?Department of Computer Science and Technology, Tsinghua University, Beijing, China
?Department of Computer Science, Aalborg University, Denmark
?Microsoft Research Asia, Beijing, China
dingsl@gmail.com gaocong@cs.aau.dk
cyl@microsoft.com zxy-dcs@tsinghua.edu.cn
Abstract
Online forum discussions often contain vast
amounts of questions that are the focuses of
discussions. Extracting contexts and answers
together with the questions will yield not only
a coherent forum summary but also a valu-
able QA knowledge base. In this paper, we
propose a general framework based on Con-
ditional Random Fields (CRFs) to detect the
contexts and answers of questions from forum
threads. We improve the basic framework by
Skip-chain CRFs and 2D CRFs to better ac-
commodate the features of forums for better
performance. Experimental results show that
our techniques are very promising.
1 Introduction
Forums are web virtual spaces where people can ask
questions, answer questions and participate in dis-
cussions. The availability of vast amounts of thread
discussions in forums has promoted increasing in-
terests in knowledge acquisition and summarization
for forum threads. Forum thread usually consists
of an initiating post and a number of reply posts.
The initiating post usually contains several ques-
tions and the reply posts usually contain answers to
the questions and perhaps new questions. Forum
participants are not physically co-present, and thus
reply may not happen immediately after questions
are posted. The asynchronous nature and multi-
participants make multiple questions and answers
?This work was done when Shilin Ding was a visiting stu-
dent at the Microsoft Research Asia
?This work was done when Gao Cong worked as a re-
searcher at the Microsoft Research Asia.
<context id=1>S1: Hi I am looking for a pet friendly
hotel in Hong Kong because all of my family is go-
ing there for vacation. S2: my family has 2 sons
and a dog.</context> <question id=1>S3: Is there
any recommended hotel near Sheung Wan or Tsing
Sha Tsui?</question><context id=2,3>S4: We also
plan to go shopping in Causeway Bay.</context>
<question id=2>S5: What?s the traffic situa-
tion around those commercial areas?</question>
<question id=3>S6: Is it necessary to take a
taxi?</question>. S7: Any information would be ap-
preciated.
<answer qid=1>S8: The Comfort Lodge near
Kowloon Park allows pet as I know, and usually fits
well within normal budget. S9: It is also conve-
niently located, nearby the Kowloon railway station
and subway.</answer>
<answer qid=2,3> S10: It?s very crowd in those ar-
eas, so I recommend MTR in Causeway Bay because
it is cheap to take you around </answer>
Figure 1: An example thread with question-context-
answer annotated
interweaved together, which makes it more difficult
to summarize.
In this paper, we address the problem of detecting
the contexts and answers from forum threads for the
questions identified in the same threads. Figure 1
gives an example of a forum thread with questions,
contexts and answers annotated. It contains three
question sentences, S3, S5 and S6. Sentences S1
and S2 are contexts of question 1 (S3). Sentence S4
is the context of questions 2 and 3, but not 1. Sen-
tence S8 is the answer to question 3. (S4-S5-S10) is
one example of question-context-answer triple that
we want to detect in the thread. As shown in the ex-
ample, a forum question usually requires contextual
information to provide background or constraints.
710
Moreover, it sometimes needs contextual informa-
tion to provide explicit link to its answers. For
example, S8 is an answer of question 1, but they
cannot be linked with any common word. Instead,
S8 shares word pet with S1, which is a context of
question 1, and thus S8 could be linked with ques-
tion 1 through S1. We call contextual information
the context of a question in this paper.
A summary of forum threads in the form of
question-context-answer can not only highlight the
main content, but also provide a user-friendly orga-
nization of threads, which will make the access to
forum information easier.
Another motivation of detecting contexts and an-
swers of the questions in forum threads is that it
could be used to enrich the knowledge base of
community-based question and answering (CQA)
services such as Live QnA and Yahoo! Answers,
where context is comparable with the question de-
scription while question corresponds to the question
title. For example, there were about 700,000 ques-
tions in the Yahoo! Answers travel category as of
January 2008. We extracted about 3,000,000 travel
related questions from six online travel forums. One
would expect that a CQA service with large QA data
will attract more users to the service. To enrich the
knowledge base, not only the answers, but also the
contexts are critical; otherwise the answer to a ques-
tion such as How much is the taxi would be useless
without context in the database.
However, it is challenging to detecting contexts
and answers for questions in forum threads. We as-
sume the questions have been identified in a forum
thread using the approach in (Cong et al, 2008).
Although identifying questions in a forum thread is
also nontrivial, it is beyond the focus of this paper.
First, detecting contexts of a question is important
and non-trivial. We found that 74% of questions in
our corpus, which contain 1,064 questions from 579
forum threads about travel, need contexts. However,
relative position information is far from adequate to
solve the problem. For example, in our corpus 63%
of sentences preceding questions are contexts and
they only represent 34% of all correct contexts. To
effectively detect contexts, the dependency between
sentences is important. For example in Figure 1,
both S1 and S2 are contexts of question 1. S1 could
be labeled as context based on word similarity, but it
is not easy to link S2 with the question directly. S1
and S2 are linked by the common word family, and
thus S2 can be linked with question 1 through S1.
The challenge here is how to model and utilize the
dependency for context detection.
Second, it is difficult to link answers with ques-
tions. In forums, multiple questions and answers
can be discussed in parallel and are interweaved to-
gether while the reply relationship between posts is
usually unavailable. To detect answers, we need to
handle two kinds of dependencies. One is the depen-
dency relationship between contexts and answers,
which should be leveraged especially when ques-
tions alone do not provide sufficient information to
find answers; the other is the dependency between
answer candidates (similar to sentence dependency
described above). The challenge is how to model
and utilize these two kinds of dependencies.
In this paper we propose a novel approach for de-
tecting contexts and answers of the questions in fo-
rum threads. To our knowledge this is the first work
on this.We make the following contributions:
First, we employ Linear Conditional Random
Fields (CRFs) to identify contexts and answers,
which can capture the relationships between con-
tiguous sentences.
Second, we also found that context is very im-
portant for answer detection. To capture the depen-
dency between contexts and answers, we introduce
Skip-chain CRF model for answer detection. We
also extend the basic model to 2D CRFs to model
dependency between contiguous questions in a fo-
rum thread for context and answer identification.
Finally, we conducted experiments on forum data.
Experimental results show that 1) Linear CRFs out-
perform SVM and decision tree in both context
and answer detection; 2) Skip-chain CRFs outper-
form Linear CRFs for answer finding, which demon-
strates that context improves answer finding; 3)
2D CRF model improves the performance of Linear
CRFs and the combination of 2D CRFs and Skip-
chain CRFs achieves better performance for context
detection.
The rest of this paper is organized as follows:
The next section discusses related work. Section 3
presents the proposed techniques. We evaluate our
techniques in Section 4. Section 5 concludes this
paper and discusses future work.
711
2 Related Work
There is some research on summarizing discussion
threads and emails. Zhou and Hovy (2005) seg-
mented internet relay chat, clustered segments into
subtopics, and identified responding segments of
the first segment in each sub-topic by assuming
the first segment to be focus. In (Nenkova and
Bagga, 2003; Wan and McKeown, 2004; Rambow
et al, 2004), email summaries were organized by
extracting overview sentences as discussion issues.
Carenini et al(2007) leveraged both quotation re-
lation and clue words for email summarization. In
contrast, given a forum thread, we extract questions,
their contexts, and their answers as summaries.
Shrestha and McKeown (2004)?s work on email
summarization is closer to our work. They used
RIPPER as a classifier to detect interrogative ques-
tions and their answers and used the resulting ques-
tion and answer pairs as summaries. However, it did
not consider contexts of questions and dependency
between answer sentences.
We also note the existing work on extracting
knowledge from discussion threads. Huang et
al.(2007) used SVM to extract input-reply pairs from
forums for chatbot knowledge. Feng et al (2006a)
used cosine similarity to match students? query with
reply posts for discussion-bot. Feng et al (2006b)
identified the most important message in online
classroom discussion board. Our problem is quite
different from the above work.
Detecting context for question in forums is related
to the context detection problem raised in the QA
roadmap paper commissioned by ARDA (Burger et
al., 2006). To our knowledge, none of the previous
work addresses the problem of context detection.
The method of finding follow-up questions (Yang
et al, 2006) from TREC context track could be
adapted for context detection. However, the follow-
up relationship is limited between questions while
context is not. In our other work (Cong et al, 2008),
we proposed a supervised approach for question de-
tection and an unsupervised approach for answer de-
tection without considering context detection.
Extensive research has been done in question-
answering, e.g. (Berger et al, 2000; Jeon et al,
2005; Cui et al, 2005; Harabagiu and Hickl, 2006;
Dang et al, 2007). They mainly focus on con-
structing answer for certain types of question from a
large document collection, and usually apply sophis-
ticated linguistic analysis to both questions and the
documents in the collection. Soricut and Brill (2006)
used statistical translation model to find the appro-
priate answers from their QA pair collections from
FAQ pages for the posted question. In our scenario,
we not only need to find answers for various types
of questions in forum threads but also their contexts.
3 Context and Answer Detection
A question is a linguistic expression used by a ques-
tioner to request information in the form of an an-
swer. The sentence containing request focus is
called question. Context are the sentences contain-
ing constraints or background information to the
question, while answer are that provide solutions. In
this paper, we use sentences as the detection segment
though it is applicable to other kinds of segments.
Given a thread and a set of m detected questions
{Qi}mi=1, our task is to find the contexts and an-
swers for each question. We first discuss using Lin-
ear CRFs for context and answer detection, and then
extend the basic framework to Skip-chain CRFs and
2D CRFs to better model our problem. Finally, we
will briefly introduce CRF models and the features
that we used for CRF model.
3.1 Using Linear CRFs
For ease of presentation, we focus on detecting con-
texts using Linear CRFs. The model could be easily
extended to answer detection.
Context detection. As discussed in Introduction
that context detection cannot be trivially solved by
position information (See Section 4.2 for details),
and dependency between sentences is important for
context detection. Recall that in Figure 1, S2 could
be labeled as context of Q1 if we consider the de-
pendency between S2 and S1, and that between S1
and Q1, while it is difficult to establish connection
between S2 and Q1 without S1. Table 1 shows that
the correlation between the labels of contiguous sen-
tences is significant. In other words, when a sen-
tence Yt?s previous Yt?1 is not a context (Yt?1 6= C)
then it is very likely that Yt (i.e. Yt 6= C) is also not a
context. It is clear that the candidate contexts are not
independent and there are strong dependency rela-
712
Contiguous sentences yt = C yt 6= C
yt?1 = C 901 1,081
yt?1 6= C 1,081 47,190
Table 1: Contingency table(?2 = 9,386,p-value<0.001)
tionships between contiguous sentences in a thread.
Therefore, a desirable model should be able to cap-
ture the dependency.
The context detection can be modeled as a clas-
sification problem. Traditional classification tools,
e.g. SVM, can be employed, where each pair of
question and candidate context will be treated as an
instance. However, they cannot capture the depen-
dency relationship between sentences.
To this end, we proposed a general framework to
detect contexts and answers based on Conditional
Random Fields (Lafferty et al, 2001) (CRFs) which
are able to model the sequential dependencies be-
tween contiguous nodes. A CRF is an undirected
graphical model G of the conditional distribution
P (Y|X). Y are the random variables over the la-
bels of the nodes that are globally conditioned on X,
which are the random variables of the observations.
(See Section 3.4 for more about CRFs)
Linear CRF model has been successfully applied
in NLP and text mining tasks (McCallum and Li,
2003; Sha and Pereira, 2003). However, our prob-
lem cannot be modeled with Linear CRFs in the
same way as other NLP tasks, where one node has a
unique label. In our problem, each node (sentence)
might have multiple labels since one sentence could
be the context of multiple questions in a thread.
Thus, it is difficult to find a solution to tag context
sentences for all questions in a thread in single pass.
Here we assume that questions in a given thread
are independent and are found, and then we can
label a thread with m questions one-by-one in m-
passes. In each pass, one question Qi is selected
as focus and each other sentence in the thread will
be labeled as context C of Qi or not using Linear
CRF model. The graphical representations of Lin-
ear CRFs is shown in Figure2(a). The linear-chain
edges can capture the dependency between two con-
tiguous nodes. The observation sequence x = <x1,
x2,...,xt>, where t is the number of sentences in a
thread, represents predictors (to be described in Sec-
tion 3.5), and the tag sequence y=<y1,...,yt>, where
yi ? {C,P}, determines whether a sentence is plain
text P or context C of question Qi.
Answer detection. Answers usually appear in the
posts after the post containing the question. There
are also strong dependencies between contiguous
answer segments. Thus, position and similarity in-
formation alone are not adequate here. To cope
with the dependency between contiguous answer
segments, Linear CRFs model are employed as in
context detection.
3.2 Leveraging Context for Answer Detection
Using Skip-chain CRFs
We observed in our corpus 74% questions lack con-
straints or background information which are very
useful to link question and answers as discussed in
Introduction. Therefore, contexts should be lever-
aged to detect answers. The Linear CRF model can
capture the dependency between contiguous sen-
tences. However, it cannot capture the long distance
dependency between contexts and answers.
One straightforward method of leveraging context
is to detect contexts and answers in two phases, i.e.
to first identify contexts, and then label answers us-
ing both the context and question information (e.g.
the similarity between context and answer can be
used as features in CRFs). The two-phase proce-
dure, however, still cannot capture the non-local de-
pendency between contexts and answers in a thread.
To model the long distance dependency between
contexts and answers, we will use Skip-chain CRF
model to detect context and answer together. Skip-
chain CRF model is applied for entity extraction
and meeting summarization (Sutton and McCallum,
2006; Galley, 2006). The graphical representation
of a Skip-chain CRF given in Figure2(b) consists
of two types of edges: linear-chain (yt?1 to yt) and
skip-chain edges (yi to yj).
Ideally, the skip-chain edges will establish the
connection between candidate pairs with high prob-
ability of being context and answer of a question.
To introduce skip-chain edges between any pairs of
non-contiguous sentences will be computationally
expensive, and also introduce noise. To make the
cardinality and number of cliques in the graph man-
ageable and also eliminate noisy edges, we would
like to generate edges only for sentence pairs with
high possibility of being context and answer. This is
713
(a) Linear CRFs (b) Skip-chain CRFs (c) 2D CRFs
Figure 2: CRF Models
Skip-Chain yv = A yv 6= A
yu = C 4,105 5,314
yu 6= C 3,744 9,740
Table 2: Contingence table(?2=615.8,p-value < 0.001)
achieved as follows. Given a question Qi in post Pj
of a thread with n posts, its contexts usually occur
within post Pj or before Pj while answers appear in
the posts after Pj . We will establish an edge between
each candidate answer v and one condidate context
in {Pk}jk=1 such that they have the highest possibil-
ity of being a context-answer pair of question Qi:
u = argmax
u?{Pk}jk=1
sim(xu, Qi).sim(xv, {xu, Qi})
here, we use the product of sim(xu, Qi) and
sim(xv, {xu, Qi} to estimate the possibility of be-
ing a context-answer pair for (u, v) , where sim(?, ?)
is the semantic similarity calculated on WordNet as
described in Section 3.5. Table 2 shows that yu and
yv in the skip chain generated by our heuristics in-
fluence each other significantly.
Skip-chain CRFs improve the performance of
answer detection due to the introduced skip-chain
edges that represent the joint probability conditioned
on the question, which is exploited by skip-chain
feature function: f(yu, yv, Qi,x).
3.3 Using 2D CRF Model
Both Linear CRFs and Skip-chain CRFs label the
contexts and answers for each question in separate
passes by assuming that questions in a thread are in-
dependent. Actually the assumption does not hold
in many cases. Let us look at an example. As in Fig-
ure 1, sentence S10 is an answer for both question
Q2 and Q3. S10 could be recognized as the answer
of Q2 due to the shared word areas and Causeway
bay (in Q2?s context, S4), but there is no direct re-
lation between Q3 and S10. To label S10, we need
consider the dependency relation between Q2 and
Q3. In other words, the question-answer relation be-
tween Q3 and S10 can be captured by a joint mod-
eling of the dependency among S10, Q2 and Q3.
The labels of the same sentence for two contigu-
ous questions in a thread would be conditioned on
the dependency relationship between the questions.
Such a dependency cannot be captured by both Lin-
ear CRFs and Skip-chain CRFs.
To capture the dependency between the contigu-
ous questions, we employ 2D CRFs to help context
and answer detection. 2D CRF model is used in
(Zhu et al, 2005) to model the neighborhood de-
pendency in blocks within a web page. As shown
in Figure2(c), 2D CRF models the labeling task for
all questions in a thread. For each thread, there are
m rows in the grid, where the ith row corresponds
to one pass of Linear CRF model (or Skip-chain
model) which labels contexts and answers for ques-
tion Qi. The vertical edges in the figure represent
the joint probability conditioned on the contiguous
questions, which will be exploited by 2D feature
function: f(yi,j , yi+1,j , Qi, Qi+1,x). Thus, the in-
formation generated in single CRF chain could be
propagated over the whole grid. In this way, context
and answer detection for all questions in the thread
could be modeled together.
3.4 Conditional Random Fields (CRFs)
The Linear, Skip-Chain and 2D CRFs can be gen-
eralized as pairwise CRFs, which have two kinds of
cliques in graph G: 1) node yt and 2) edge (yu, yv).
The joint probability is defined as:
p(y|x)= 1Z(x) exp
{?
k,t
?kfk(yt,x)+
?
k,t
?kgk(yu, yv,x)
}
714
where Z(x) is the normalization factor, fk is the
feature on nodes, gk is on edges between u and v,
and ?k and ?k are parameters.
Linear CRFs are based on the first order Markov
assumption that the contiguous nodes are dependent.
The pairwise edges in Skip-chain CRFs represent
the long distance dependency between the skipped
nodes, while the ones in 2D CRFs represent the de-
pendency between the neighboring nodes.
Inference and Parameter Estimation. For Linear
CRFs, dynamic programming is used to compute the
maximum a posteriori (MAP) of y given x. How-
ever, for more complicated graphs with cycles, ex-
act inference needs the junction tree representation
of the original graph and the algorithm is exponen-
tial to the treewidth. For fast inference, loopy Belief
Propagation (Pearl, 1988) is implemented.
Given the training Data D = {x(i),y(i)}ni=1, the
parameter estimation is to determine the parame-
ters based on maximizing the log-likelihood L? =?n
i=1 log p(y(i)|x(i)). In Linear CRF model, dy-
namic programming and L-BFGS (limited memory
Broyden-Fletcher-Goldfarb-Shanno) can be used to
optimize objective function L?, while for compli-
cated CRFs, Loopy BP are used instead to calculate
the marginal probability.
3.5 Features used in CRF models
The main features used in Linear CRF models for
context detection are listed in Table 3.
The similarity feature is to capture the word sim-
ilarity and semantic similarity between candidate
contexts and answers. The word similarity is based
on cosine similarity of TF/IDF weighted vectors.
The semantic similarity between words is computed
based on Wu and Palmer?s measure (Wu and Palmer,
1994) using WordNet (Fellbaum, 1998).1 The simi-
larity between contiguous sentences will be used to
capture the dependency for CRFs. In addition, to
bridge the lexical gaps between question and con-
text, we learned top-3 context terms for each ques-
tion term from 300,000 question-description pairs
obtained from Yahoo! Answers using mutual infor-
mation (Berger et al, 2000) ( question description
in Yahoo! Answers is comparable to contexts in fo-
1The semantic similarity between sentences is calculated as
in (Yang et al, 2006).
Similarity features:
? Cosine similarity with the question
? Similarity with the question using WordNet
? Cosine similarity between contiguous sentences
? Similarity between contiguous sentences using WordNet
? Cosine similarity with the expanded question using the lexical
matching words
Structural features:
? The relative position to current question
? Is its author the same with that of the question?
? Is it in the same paragraph with its previous sentence?
Discourse and lexical features:
? The number of Pronouns in the question
? The presence of fillers, fluency devices (e.g. ?uh?, ?ok?)
? The presence of acknowledgment tokens
? The number of non-stopwords
? Whether the question has a noun or not?
? Whether the question has a verb or not?
Table 3: Features for Linear CRFs. Unless otherwise
mentioned, we refer to features of the sentence whose la-
bel to be predicted
rums), and then use them to expand question and
compute cosine similarity.
The structural features of forums provide strong
clues for contexts. For example, contexts of a ques-
tion usually occur in the post containing the question
or preceding posts.
We extracted the discourse features from a ques-
tion, such as the number of pronouns in the question.
A more useful feature would be to find the entity in
surrounding sentences referred by a pronoun. We
tried GATE (Cunningham et al, 2002) for anaphora
resolution of the pronouns in questions, but the per-
formance became worse with the feature, which is
probably due to the difficulty of anaphora resolution
in forum discourse. We also observed that questions
often need context if the question do not contain a
noun or a verb.
In addition, we use similarity features between
skip-chain sentences for Skip-chain CRFs and simi-
larity features between questions for 2D CRFs.
4 Experiments
4.1 Experimental setup
Corpus. We obtained about 1 million threads
from TripAdvisor forum; we randomly selected 591
threads and removed 22 threads which has more than
40 sentences and 6 questions; the remaining 579 fo-
rum threads form our corpus 2. Each thread in our
2TripAdvisor (http://www.tripadvisor.com/ForumHome) is
one of the most popular travel forums; the list of 579 urls is
715
Model Prec(%) Rec(%) F1(%)
Context Detection
SVM 75.27 68.80 71.32
C4.5 70.16 64.30 67.21
L-CRF 75.75 72.84 74.45
Answer Detection
SVM 73.31 47.35 57.52
C4.5 65.36 46.55 54.37
L-CRF 63.92 58.74 61.22
Table 4: Context and Answer Detection
corpus contains at least two posts and on average
each thread consists of 3.87 posts. Two annotators
were asked to tag questions, their contexts, and an-
swers in each thread. The kappa statistic for identi-
fying question is 0.96, for linking context and ques-
tion given a question is 0.75, and for linking answer
and question given a question is 0.69. We conducted
experiments on both the union and intersection of
the two annotated data. The experimental results on
both data are qualitatively comparable. We only re-
port results on union data due to space limitation.
The union data contains 1,064 questions, 1,458 con-
texts and 3,534 answers.
Metrics. We calculated precision, recall,
and F1-score for all tasks. All the experimental
results are obtained through the average of 5 trials
of 5-fold cross validation.
4.2 Experimental results
Linear CRFs for Context and Answer Detection.
This experiment is to evaluate Linear CRF model
(Section 3.1) for context and answer detection by
comparing with SVM and C4.5(Quinlan, 1993). For
SVM, we use SVMlight(Joachims, 1999). We tried
linear, polynomial and RBF kernels and report the
results on polynomial kernel using default param-
eters since it performs the best in the experiment.
SVM and C4.5 use the same set of features as Lin-
ear CRFs. As shown in Table 4, Linear CRF model
outperforms SVM and C4.5 for both context and an-
swer detection. The main reason for the improve-
ment is that CRF models can capture the sequen-
tial dependency between segments in forums as dis-
cussed in Section 3.1.
given in http://homepages.inf.ed.ac.uk/gcong/acl08/; Removing
the 22 long threads can greatly reduce the training and test time.
position Prec(%) Rec(%) F1(%)
Context Detection
Previous One 63.69 34.29 44.58
Previous All 43.48 76.41 55.42
Anwer Detection
Following One 66.48 19.98 30.72
Following All 31.99 100 48.48
Table 5: Using position information for detection
Context Prec(%) Rec(%) F1(%)
No context 63.92 58.74 61.22
Prev. sentence 61.41 62.50 61.84
Real context 63.54 66.40 64.94
L-CRF+context 65.51 63.13 64.06
Table 6: Contextual Information for Answer Detection.
Prev. sentence uses one previous sentence of the current
question as context. RealContext uses the context anno-
tated by experts. L-CRF+context uses the context found
by Linear CRFs
We next report a baseline of context detection
using previous sentences in the same post with its
question since contexts often occur in the question
post or preceding posts. Similarly, we report a base-
line of answer detecting using following segments of
a question as answers. The results given in Table 5
show that location information is far from adequate
to detect contexts and answers.
The usefulness of contexts. This experiment is to
evaluate the usefulness of contexts in answer de-
tection, by adding the similarity between the con-
text (obtained with different methods) and candi-
date answer as an extra feature for CRFs. Table 6
shows the impact of context on answer detection
using Linear CRFs. Linear CRFs with contextual
information perform better than those without con-
text. L-CRF+context is close to that using real con-
text, while it is better than CRFs using the previous
sentence as context. The results clearly shows that
contextual information greatly improves the perfor-
mance of answer detection.
Improved Models. This experiment is to evaluate
the effectiveness of Skip-Chain CRFs (Section 3.2)
and 2D CRFs (Section 3.3) for our tasks. The results
are given in Table 7 and Table 8.
In context detection, Skip-Chain CRFs have simi-
716
Model Prec(%) Rec(%) F1(%)
L-CRF+Context 75.75 72.84 74.45
Skip-chain 74.18 74.90 74.42
2D 75.92 76.54 76.41
2D+Skip-chain 76.27 78.25 77.34
Table 7: Skip-chain and 2D CRFs for context detection
lar results as Linear CRFs, i.e. the inter-dependency
captured by the skip chains generated using the
heuristics in Section 3.2 does not improve the con-
text detection. The performance of Linear CRFs is
improved in 2D CRFs (by 2%) and 2D+Skip-chain
CRFs (by 3%) since they capture the dependency be-
tween contiguous questions.
In answer detection, as expected, Skip-chain
CRFs outperform L-CRF+context since Skip-chain
CRFs can model the inter-dependency between con-
texts and answers while in L-CRF+context the con-
text can only be reflected by the features on the ob-
servations. We also observed that 2D CRFs improve
the performance of L-CRF+context due to the de-
pendency between contiguous questions. In contrast
with our expectation, the 2D+Skip-chain CRFs does
not improve Skip-chain CRFs in terms of answer de-
tection. The possible reason could be that the struc-
ture of the graph is very complicated and too many
parameters need to be learned on our training data.
Evaluating Features. We also evaluated the con-
tributions of each category of features in Table 3
to context detection. We found that similarity fea-
tures are the most important and structural feature
the next. We also observed the same trend for an-
swer detection. We omit the details here due to space
limitation.
As a summary, 1) our CRF model outperforms
SVM and C4.5 for both context and answer detec-
tions; 2) context is very useful in answer detection;
3) the Skip-chain CRF method is effective in lever-
aging context for answer detection; and 4) 2D CRF
model improves the performance of Linear CRFs for
both context and answer detection.
5 Discussions and Conclusions
We presented a new approach to detecting contexts
and answers for questions in forums with good per-
formance. We next discuss our experience not cov-
ered by the experiments, and future work.
Model Prec(%) Rec(%) F1(%)
L-CRF+context 65.51 63.13 64.06
Skip-chain 67.59 71.06 69.40
2D 65.77 68.17 67.34
2D+Skip-chain 66.90 70.56 68.89
Table 8: Skip-chain and 2D CRFs for answer detection
Since contexts of questions are largely unexplored
in previous work, we analyze the contexts in our
corpus and classify them into three categories: 1)
context contains the main content of question while
question contains no constraint, e.g. ?i will visit NY at
Oct, looking for a cheap hotel but convenient. Any good
suggestion? ?; 2) contexts explain or clarify part of
the question, such as a definite noun phrase, e.g. ?We
are going on the Taste of Paris. Does anyone know if it is
advisable to take a suitcase with us on the tour., where
the first sentence is to describe the tour; and 3) con-
texts provide constraint or background for question
that is syntactically complete, e.g. ?We are inter-
ested in visiting the Great Wall(and flying from London).
Can anyone recommend a tour operator.? In our corpus,
about 26% questions do not need context, 12% ques-
tions need Type 1 context, 32% need Type 2 context
and 30% Type 3. We found that our techniques often
do not perform well on Type 3 questions.
We observed that factoid questions, one of fo-
cuses in the TREC QA community, take less than
10% question in our corpus. It would be interesting
to revisit QA techniques to process forum data.
Other future work includes: 1) to summarize mul-
tiple threads using the triples extracted from indi-
vidual threads. This could be done by clustering
question-context-answer triples; 2) to use the tradi-
tional text summarization techniques to summarize
the multiple answer segments; 3) to integrate the
Question Answering techniques as features of our
framework to further improve answer finding; 4) to
reformulate questions using its context to generate
more user-friendly questions for CQA services; and
5) to evaluate our techniques on more online forums
in various domains.
Acknowledgments
We thank the anonymous reviewers for their detailed
comments, and Ming Zhou and Young-In Song for
their valuable suggestions in preparing the paper.
717
References
A. Berger, R. Caruana, D. Cohn, D. Freitag, and V. Mit-
tal. 2000. Bridging the lexical chasm: statistical ap-
proaches to answer-finding. In Proceedings of SIGIR.
J. Burger, C. Cardie, V. Chaudhri, R. Gaizauskas,
S. Harabagiu, D. Israel, C. Jacquemin, C. Lin,
S. Maiorano, G. Miller, D. Moldovan, B. Ogden,
J. Prager, E. Riloff, A. Singhal, R. Shrihari, T. Strza-
lkowski16, E. Voorhees, and R. Weishedel. 2006. Is-
sues, tasks and program structures to roadmap research
in question and answering (qna). ARAD: Advanced
Research and Development Activity (US).
G. Carenini, R. Ng, and X. Zhou. 2007. Summarizing
email conversations with clue words. In Proceedings
of WWW.
G. Cong, L. Wang, C.Y. Lin, Y.I. Song, and Y. Sun. 2008.
Finding question-answer pairs from online forums. In
Proceedings of SIGIR.
H. Cui, R. Sun, K. Li, M. Kan, and T. Chua. 2005. Ques-
tion answering passage retrieval using dependency re-
lations. In Proceedings of SIGIR.
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. Gate: A framework and graphical
development environment for robust nlp tools and ap-
plications. In Proceedings of ACL.
H. Dang, J. Lin, and D. Kelly. 2007. Overview of the
trec 2007 question answering track. In Proceedings of
TREC.
C. Fellbaum, editor. 1998. WordNet: An Electronic Lex-
ical Database (Language, Speech, and Communica-
tion). The MIT Press, May.
D. Feng, E. Shaw, J. Kim, and E. Hovy. 2006a. An intel-
ligent discussion-bot for answering student queries in
threaded discussions. In Proceedings of IUI.
D. Feng, E. Shaw, J. Kim, and E. Hovy. 2006b. Learning
to detect conversation focus of threaded discussions.
In Proceedings of HLT-NAACL.
M. Galley. 2006. A skip-chain conditional random field
for ranking meeting utterances by importance. In Pro-
ceedings of EMNLP.
S. Harabagiu and A. Hickl. 2006. Methods for using tex-
tual entailment in open-domain question answering.
In Proceedings of ACL.
J. Huang, M. Zhou, and D. Yang. 2007. Extracting chat-
bot knowledge from online discussion forums. In Pro-
ceedings of IJCAI.
J. Jeon, W. Croft, and J. Lee. 2005. Finding similar
questions in large question and answer archives. In
Proceedings of CIKM.
T. Joachims. 1999. Making large-scale support vector
machine learning practical. MIT Press, Cambridge,
MA, USA.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of ICML.
A. McCallum and W. Li. 2003. Early results for named
entity recognition with conditional random fields, fea-
ture induction and web-enhanced lexicons. In Pro-
ceedings of CoNLL-2003.
A. Nenkova and A. Bagga. 2003. Facilitating email
thread access by extractive summary generation. In
Proceedings of RANLP.
J. Pearl. 1988. Probabilistic reasoning in intelligent sys-
tems: networks of plausible inference. Morgan Kauf-
mann Publishers Inc., San Francisco, CA, USA.
J. Quinlan. 1993. C4.5: programs for machine learn-
ing. Morgan Kaufmann Publishers Inc., San Fran-
cisco, CA, USA.
O. Rambow, L. Shrestha, J. Chen, and C. Lauridsen.
2004. Summarizing email threads. In Proceedings of
HLT-NAACL.
F. Sha and F. Pereira. 2003. Shallow parsing with condi-
tional random fields. In HLT-NAACL.
L. Shrestha and K. McKeown. 2004. Detection of
question-answer pairs in email conversations. In Pro-
ceedings of COLING.
R. Soricut and E. Brill. 2006. Automatic question an-
swering using the web: Beyond the Factoid. Informa-
tion Retrieval, 9(2):191?206.
C. Sutton and A. McCallum. 2006. An introduction to
conditional random fields for relational learning. In
Lise Getoor and Ben Taskar, editors, Introduction to
Statistical Relational Learning. MIT Press. To appear.
S. Wan and K. McKeown. 2004. Generating overview
summaries of ongoing email thread discussions. In
Proceedings of COLING.
Z. Wu and M. S. Palmer. 1994. Verb semantics and lexi-
cal selection. In Proceedings of ACL.
F. Yang, J. Feng, and G. Fabbrizio. 2006. A data
driven approach to relevancy recognition for contex-
tual question answering. In Proceedings of the Inter-
active Question Answering Workshop at HLT-NAACL
2006.
L. Zhou and E. Hovy. 2005. Digesting virtual ?geek?
culture: The summarization of technical internet relay
chats. In Proceedings of ACL.
J. Zhu, Z. Nie, J. Wen, B. Zhang, and W. Ma. 2005. 2d
conditional random fields for web information extrac-
tion. In Proceedings of ICML.
718
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 737?745,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Answering Opinion Questions with Random Walks on Graphs
Fangtao Li, Yang Tang, Minlie Huang, and Xiaoyan Zhu
State Key Laboratory on Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Sci. and Tech., Tsinghua University, Beijing 100084, China
{fangtao06,tangyang9}@gmail.com,{aihuang,zxy-dcs}@tsinghua.edu.cn
Abstract
Opinion Question Answering (Opinion
QA), which aims to find the authors? sen-
timental opinions on a specific target, is
more challenging than traditional fact-
based question answering problems. To
extract the opinion oriented answers, we
need to consider both topic relevance and
opinion sentiment issues. Current solu-
tions to this problem are mostly ad-hoc
combinations of question topic informa-
tion and opinion information. In this pa-
per, we propose an Opinion PageRank
model and an Opinion HITS model to fully
explore the information from different re-
lations among questions and answers, an-
swers and answers, and topics and opin-
ions. By fully exploiting these relations,
the experiment results show that our pro-
posed algorithms outperform several state
of the art baselines on benchmark data set.
A gain of over 10% in F scores is achieved
as compared to many other systems.
1 Introduction
Question Answering (QA), which aims to pro-
vide answers to human-generated questions auto-
matically, is an important research area in natu-
ral language processing (NLP) and much progress
has been made on this topic in previous years.
However, the objective of most state-of-the-art QA
systems is to find answers to factual questions,
such as ?What is the longest river in the United
States?? and ?Who is Andrew Carnegie?? In fact,
rather than factual information, people would also
like to know about others? opinions, thoughts and
feelings toward some specific objects, people and
events. Some examples of these questions are:
?How is Bush?s decision not to ratify the Kyoto
Protocol looked upon by Japan and other US al-
lies??(Stoyanov et al, 2005) and ?Why do peo-
ple like Subway Sandwiches?? from TAC 2008
(Dang, 2008). Systems designed to deal with such
questions are called opinion QA systems. Re-
searchers (Stoyanov et al, 2005) have found that
opinion questions have very different character-
istics when compared with fact-based questions:
opinion questions are often much longer, more
likely to represent partial answers rather than com-
plete answers and vary much more widely. These
features make opinion QA a harder problem to
tackle than fact-based QA. Also as shown in (Stoy-
anov et al, 2005), directly applying previous sys-
tems designed for fact-based QA onto opinion QA
tasks would not achieve good performances.
Similar to other complex QA tasks (Chen et al,
2006; Cui et al, 2007), the problem of opinion QA
can be viewed as a sentence ranking problem. The
Opinion QA task needs to consider not only the
topic relevance of a sentence (to identify whether
this sentence matches the topic of the question)
but also the sentiment of a sentence (to identify
the opinion polarity of a sentence). Current solu-
tions to opinion QA tasks are generally in ad hoc
styles: the topic score and the opinion score are
usually separately calculated and then combined
via a linear combination (Varma et al, 2008) or
just filter out the candidate without matching the
question sentiment (Stoyanov et al, 2005). How-
ever, topic and opinion are not independent in re-
ality. The opinion words are closely associated
with their contexts. Another problem is that exist-
ing algorithms compute the score for each answer
candidate individually, in other words, they do not
consider the relations between answer candidates.
The quality of a answer candidate is not only de-
termined by the relevance to the question, but also
by other candidates. For example, the good an-
swer may be mentioned by many candidates.
In this paper, we propose two models to ad-
dress the above limitations of previous sentence
737
ranking models. We incorporate both the topic
relevance information and the opinion sentiment
information into our sentence ranking procedure.
Meanwhile, our sentence ranking models could
naturally consider the relationships between dif-
ferent answer candidates. More specifically, our
first model, called Opinion PageRank, incorpo-
rates opinion sentiment information into the graph
model as a condition. The second model, called
Opinion HITS model, considers the sentences as
authorities and both question topic information
and opinion sentiment information as hubs. The
experiment results on the TAC QA data set demon-
strate the effectiveness of the proposed Random
Walk based methods. Our proposed method per-
forms better than the best method in the TAC 2008
competition.
The rest of this paper is organized as follows:
Section 2 introduces some related works. We will
discuss our proposed models in Section 3. In Sec-
tion 4, we present an overview of our opinion QA
system. The experiment results are shown in Sec-
tion 5. Finally, Section 6 concludes this paper and
provides possible directions for future work.
2 Related Work
Few previous studies have been done on opin-
ion QA. To our best knowledge, (Stoyanov et
al., 2005) first created an opinion QA corpus
OpQA. They find that opinion QA is a more chal-
lenging task than factual question answering, and
they point out that traditional fact-based QA ap-
proaches may have difficulty on opinion QA tasks
if unchanged. (Somasundaran et al, 2007) argues
that making finer grained distinction of subjective
types (sentiment and arguing) further improves the
QA system. For non-English opinion QA, (Ku et
al., 2007) creates a Chinese opinion QA corpus.
They classify opinion questions into six types and
construct three components to retrieve opinion an-
swers. Relevant answers are further processed by
focus detection, opinion scope identification and
polarity detection. Some works on opinion min-
ing are motivated by opinion question answering.
(Yu and Hatzivassiloglou, 2003) discusses a nec-
essary component for an opinion question answer-
ing system: separating opinions from fact at both
the document and sentence level. (Soo-Min and
Hovy, 2005) addresses another important compo-
nent of opinion question answering: finding opin-
ion holders.
More recently, TAC 2008 QA track (evolved
from TREC) focuses on finding answers to opin-
ion questions (Dang, 2008). Opinion questions
retrieve sentences or passages as answers which
are relevant for both question topic and question
sentiment. Most TAC participants employ a strat-
egy of calculating two types of scores for answer
candidates, which are the topic score measure and
the opinion score measure (the opinion informa-
tion expressed in the answer candidate). How-
ever, most approaches simply combined these two
scores by a weighted sum, or removed candidates
that didn?t match the polarity of questions, in order
to extract the opinion answers.
Algorithms based on Markov Random Walk
have been proposed to solve different kinds of
ranking problems, most of which are inspired by
the PageRank algorithm (Page et al, 1998) and the
HITS algorithm (Kleinberg, 1999). These two al-
gorithms were initially applied to the task of Web
search and some of their variants have been proved
successful in a number of applications, including
fact-based QA and text summarization (Erkan and
Radev, 2004; Mihalcea and Tarau, 2004; Otter-
bacher et al, 2005; Wan and Yang, 2008). Gener-
ally, such models would first construct a directed
or undirected graph to represent the relationship
between sentences and then certain graph-based
ranking methods are applied on the graph to com-
pute the ranking score for each sentence. Sen-
tences with high scores are then added into the
answer set or the summary. However, to the best
of our knowledge, all previous Markov Random
Walk-based sentence ranking models only make
use of topic relevance information, i.e. whether
this sentence is relevant to the fact we are looking
for, thus they are limited to fact-based QA tasks.
To solve the opinion QA problems, we need to
consider both topic and sentiment in a non-trivial
manner.
3 Our Models for Opinion Sentence
Ranking
In this section, we formulate the opinion question
answering problem as a topic and sentiment based
sentence ranking task. In order to naturally inte-
grate the topic and opinion information into the
graph based sentence ranking framework, we pro-
pose two random walk based models for solving
the problem, i.e. an Opinion PageRank model and
an Opinion HITS model.
738
3.1 Opinion PageRank Model
In order to rank sentence for opinion question an-
swering, two aspects should be taken into account.
First, the answer candidate is relevant to the ques-
tion topic; second, the answer candidate is suitable
for question sentiment.
Considering Question Topic: We first intro-
duce how to incorporate the question topic into
the Markov Random Walk model, which is simi-
lar as the Topic-sensitive LexRank (Otterbacher et
al., 2005). Given the set Vs = {vi} containing all
the sentences to be ranked, we construct a graph
where each node represents a sentence and each
edge weight between sentence vi and sentence vj
is induced from sentence similarity measure as fol-
lows: p(i ? j) = f(i?j)
P|Vs|
k=1 f(i?k)
, where f(i ? j)
represents the similarity between sentence vi and
sentence vj , here is cosine similarity (Baeza-Yates
and Ribeiro-Neto, 1999). We define f(i ? i) = 0
to avoid self transition. Note that p(i ? j) is usu-
ally not equal to p(j ? i). We also compute the
similarity rel(vi|q) of a sentence vi to the question
topic q using the cosine measure. This relevance
score is then normalized as follows to make the
sum of all relevance values of the sentences equal
to 1: rel?(vi|q) = rel(vi|q)P|Vs|
k=1 rel(vk|q)
.
The saliency score Score(vi) for sentence vi
can be calculated by mixing topic relevance score
and scores of all other sentences linked with it as
follows: Score(vi) = ?
?
j 6=i Score(vj) ? p(j ?
i)+(1??)rel?(vi|q), where ? is the damping fac-
tor as in the PageRank algorithm.
The matrix form is: p? = ?M?T p? + (1 ?
?)~?, where p? = [Score(vi)]|Vs|?1 is the vec-
tor of saliency scores for the sentences; M? =
[p(i ? j)]|Vs|?|Vs| is the graph with each entry
corresponding to the transition probability; ~? =
[rel?(vi|q)]|Vs|?1 is the vector containing the rel-
evance scores of all the sentences to the ques-
tion. The above process can be considered as a
Markov chain by taking the sentences as the states
and the corresponding transition matrix is given by
A? = ?M?T + (1 ? ?)~e~?T .
Considering Topics and Sentiments To-
gether: In order to incorporate the opinion infor-
mation and topic information for opinion sentence
ranking in an unified framework, we propose an
Opinion PageRank model (Figure 1) based on a
two-layer link graph (Liu and Ma, 2005; Wan and
Yang, 2008). In our opinion PageRank model, the
Figure 1: Opinion PageRank
first layer contains all the sentiment words from a
lexicon to represent the opinion information, and
the second layer denotes the sentence relationship
in the topic sensitive Markov Random Walk model
discussed above. The dashed lines between these
two layers indicate the conditional influence be-
tween the opinion information and the sentences
to be ranked.
Formally, the new representation for the two-
layer graph is denoted as G? = ?Vs, Vo, Ess, Eso?,
where Vs = {vi} is the set of sentences and Vo =
{oj} is the set of sentiment words representing the
opinion information; Ess = {eij |vi, vj ? Vs}
corresponds to all links between sentences and
Eso = {eij |vi ? Vs, oj ? Vo} corresponds to
the opinion correlation between a sentence and
the sentiment words. For further discussions, we
let ?(oj) ? [0, 1] denote the sentiment strength
of word oj , and let ?(vi, oj) ? [0, 1] denote the
strength of the correlation between sentence vi and
word oj . We incorporate the two factors into the
transition probability from vi to vj and the new
transition probability p(i ? j|Op(vi),Op(vj)) is
defined as f(i?j|Op(vi),Op(vj ))
P|Vs|
k=1 f(i?k|Op(vi),Op(vk))
when
? f 6=
0, and defined as 0 otherwise, where Op(vi) is de-
noted as the opinion information of sentence vi,
and f(i ? j|Op(vi),Op(vj)) is the new similar-
ity score between two sentences vi and vj , condi-
tioned on the opinion information expressed by the
sentiment words they contain. We propose to com-
pute the conditional similarity score by linearly
combining the scores conditioned on the source
opinion (i.e. f(i ? j|Op(vi))) and the destina-
tion opinion (i.e. f(i ? j|Op(vj))) as follows:
f(i ? j|Op(vi),Op(vj))
= ? ? f(i ? j|Op(vi)) + (1? ?) ? f(i ? j|Op(vj))
= ? ?
X
ok?Op(vi)
f(i ? j) ? pi(ok) ? ?(ok, vi)
+ (1? ?) ?
X
o
k?
?Op(vj))
(i ? j) ? pi(ok? ) ? ?(ok? , vj) (1)
where ? ? [0, 1] is the combination weight con-
trolling the relative contributions from the source
739
opinion and the destination opinion. In this study,
for simplicity, we define ?(oj) as 1, if oj ex-
ists in the sentiment lexicon, otherwise 0. And
?(vi, oj) is described as an indicative function. In
other words, if word oj appears in the sentence vi,
?(vi, oj) is equal to 1. Otherwise, its value is 0.
Then the new row-normalized matrix M?? is de-
fined as follows: M??ij = p(i ? j|Op(i),Opj).
The final sentence score for Opinion PageR-
ank model is then denoted by: Score(vi) = ? ?
?
j 6=i Score(vj) ? M??ji + (1 ? ?) ? rel?(si|q).
The matrix form is: p? = ?M??T p? + (1 ? ?) ? ~?.
The final transition matrix is then denoted as:
A? = ?M??T +(1??)~e~?T and the sentence scores
are obtained by the principle eigenvector of the
new transition matrix A?.
3.2 Opinion HITS Model
The word?s sentiment score is fixed in Opinion
PageRank. This may encounter problem when
the sentiment score definition is not suitable for
the specific question. We propose another opin-
ion sentence ranking model based on the popular
graph ranking algorithm HITS (Kleinberg, 1999).
This model can dynamically learn the word senti-
ment score towards a specific question. HITS al-
gorithm distinguishes the hubs and authorities in
the objects. A hub object has links to many au-
thorities, and an authority object has high-quality
content and there are many hubs linking to it. The
hub scores and authority scores are computed in a
recursive way. Our proposed opinion HITS algo-
rithm contains three layers. The upper level con-
tains all the sentiment words from a lexicon, which
represent their opinion information. The lower
level contains all the words, which represent their
topic information. The middle level contains all
the opinion sentences to be ranked. We consider
both the opinion layer and topic layer as hubs and
the sentences as authorities. Figure 2 gives the bi-
partite graph representation, where the upper opin-
ion layer is merged with lower topic layer together
as the hubs, and the middle sentence layer is con-
sidered as the authority.
Formally, the representation for the bipartite
graph is denoted as G# = ?Vs, Vo, Vt, Eso, Est?,
where Vs = {vi} is the set of sentences. Vo =
{oj} is the set of all the sentiment words repre-
senting opinion information, Vt = {tj} is the set
of all the words representing topic information.
Eso = {eij |vi ? Vs, oj ? Vo} corresponds to the
Figure 2: Opinion HITS model
correlations between sentence and opinion words.
Each edge eij is associated with a weight owij de-
noting the strength of the relationship between the
sentence vi and the opinion word oj . The weight
owij is 1 if the sentence vi contains word oj , other-
wise 0. Est denotes the relationship between sen-
tence and topic word. Its weight twij is calculated
by tf ? idf (Otterbacher et al, 2005).
We define two matrixes O = (Oij)|Vs|?|Vo| and
T = (Tij)|Vs|?|Vt| as follows, for Oij = owij ,
and if sentence i contains word j, therefore owij
is assigned 1, otherwise owij is 0. Tij = twij =
tfj ? idfj (Otterbacher et al, 2005).
Our new opinion HITS model is different from
the basic HITS algorithm in two aspects. First,
we consider the topic relevance when computing
the sentence authority score based on the topic hub
level as follows: Authsen(vi) ?
?
twij>0 twij ?
topic score(j)?hubtopic(j), where topic score(j)
is empirically defined as 1, if the word j is in the
topic set (we will discuss in next section), and 0.1
otherwise.
Second, in our opinion HITS model, there are
two aspects to boost the sentence authority score:
we simultaneously consider both topic informa-
tion and opinion information as hubs.
The final scores for authority sentence, hub
topic and hub opinion in our opinion HITS model
are defined as:
Auth(n+1)sen (vi) = (2)
? ?
X
twij>0
twij ? topic score(j) ?Hub(n)topic(tj)
+ (1? ?) ?
X
owij>0
owij ?Hub(n)opinion(oj)
Hub(n+1)topic (ti) =
X
twki>0
twki ?Auth(n)sen(vi) (3)
Hub(n+1)opinion(oi) =
X
owki>0
owki ?Auth(n)sen(vi) (4)
740
Figure 3: Opinion Question Answering System
The matrix form is:
a(n+1) = ? ? T ? e ? tTs ? I ? h(n)t + (1 ? ?) ? O ? h(n)o (5)
h(n+1)t = T
T ? a(n) (6)
h(n+1)o = O
T ? a(n) (7)
where e is a |Vt|?1 vector with all elements equal
to 1 and I is a |Vt| ? |Vt| identity matrix, ts =
[topic score(j)]|Vt|?1 is the score vector for topic
words, a(n) = [Auth(n)sen(vi)]|Vs|?1 is the vector
authority scores for the sentence in the nth itera-
tion, and the same as h(n)t = [Hub
(n)
topic(tj)]|Vt|?1,
h(n)o = [Hub(n)opinion(tj)]|Vo|?1. In order to guaran-
tee the convergence of the iterative form, authority
score and hub score are normalized after each iter-
ation.
For computation of the final scores, the ini-
tial scores of all nodes, including sentences, topic
words and opinion words, are set to 1 and the
above iterative steps are used to compute the new
scores until convergence. Usually the convergence
of the iteration algorithm is achieved when the dif-
ference between the scores computed at two suc-
cessive iterations for any nodes falls below a given
threshold (10e-6 in this study). We use the au-
thority scores as the saliency scores in the Opin-
ion HITS model. The sentences are then ranked
by their saliency scores.
4 System Description
In this section, we introduce the opinion question
answering system based on the proposed graph
methods. Figure 3 shows five main modules:
Question Analysis: It mainly includes two
components. 1).Sentiment Classification: We
classify all opinion questions into two categories:
positive type or negative type. We extract several
types of features, including a set of pattern fea-
tures, and then design a classifier to identify sen-
timent polarity for each question (similar as (Yu
and Hatzivassiloglou, 2003)). 2).Topic Set Expan-
sion: The opinion question asks opinions about
a particular target. Semantic role labeling based
(Carreras and Marquez, 2005) and rule based tech-
niques can be employed to extract this target as
topic word. We also expand the topic word with
several external knowledge bases: Since all the en-
tity synonyms are redirected into the same page in
Wikipedia (Rodrigo et al, 2007), we collect these
redirection synonym words to expand topic set.
We also collect some related lists as topic words.
For example, given question ?What reasons did
people give for liking Ed Norton?s movies??, we
collect all the Norton?s movies from IMDB as this
question?s topic words.
Document Retrieval: The PRISE search en-
gine, supported by NIST (Dang, 2008), is em-
ployed to retrieve the documents with topic word.
Answer Candidate Extraction: We split re-
trieved documents into sentences, and extract sen-
tences containing topic words. In order to im-
prove recall, we carry out the following process to
handle the problem of coreference resolution: We
classify the topic word into four categories: male,
female, group and other. Several pronouns are de-
fined for each category, such as ?he?, ?him?, ?his?
for male category. If a sentence is determined to
contain the topic word, and its next sentence con-
tains the corresponding pronouns, then the next
sentence is also extracted as an answer candidate,
similar as (Chen et al, 2006).
Answer Ranking: The answer candidates
are ranked by our proposed Opinion PageRank
method or Opinion HITS method.
Answer Selection by Removing Redundancy:
We incrementally add the top ranked sentence into
the answer set, if its cosine similarity with ev-
ery extracted answer doesn?t exceed a predefined
threshold, until the number of selected sentence
(here is 40) is reached.
5 Experiments
5.1 Experiment Step
5.1.1 Dataset
We employ the dataset from the TAC 2008 QA
track. The task contains a total of 87 squishy
741
opinion questions.1 These questions have simple
forms, and can be easily divided into positive type
or negative type, for example ?Why do people like
Mythbusters?? and ?What were the specific ac-
tions or reasons given for a negative attitude to-
wards Mahmoud Ahmadinejad??. The initial topic
word for each question (called target in TAC) is
also provided. Since our work in this paper fo-
cuses on sentence ranking for opinion QA, these
characteristics of TAC data make it easy to pro-
cess question analysis. Answers for all questions
must be retrieved from the TREC Blog06 collec-
tion (Craig Macdonald and Iadh Ounis, 2006).
The collection is a large sample of the blog sphere,
crawled over an eleven-week period from Decem-
ber 6, 2005 until February 21, 2006. We retrieve
the top 50 documents for each question.
5.1.2 Evaluation Metrics
We adopt the evaluation metrics used in the TAC
squishy opinion QA task (Dang, 2008). The TAC
assessors create a list of acceptable information
nuggets for each question. Each nugget will be
assigned a normalized weight based on the num-
ber of assessors who judged it to be vital. We use
these nuggets and corresponding weights to assess
our approach. Three human assessors complete
the evaluation process. Every question is scored
using nugget recall (NR) and an approximation to
nugget precision (NP) based on length. The final
score will be calculated using F measure with TAC
official value ? = 3 (Dang, 2008). This means re-
call is 3 times as important as precision:
F (? = 3) =
(32 + 1) ?NP ?NR
32 ?NP + NR
where NP is the sum of weights of nuggets re-
turned in response over the total sum of weights
of all nuggets in nugget list, and NP = 1 ?
(length ? allowance)/(length) if length is no
less than allowance and 0 otherwise. Here
allowance = 100 ? (?nuggets returned) and
length equals to the number of non-white char-
acters in strings. We will use average F Score to
evaluate the performance for each system.
5.1.3 Baseline
The baseline combines the topic score and opinion
score with a linear weight for each answer candi-
date, similar to the previous ad-hoc algorithms:
final score = (1 ? ?) ? opinion score + ?? topic score
(8)
13 questions were dropped from the evaluation due to no
correct answers found in the corpus
The topic score is computed by the cosine sim-
ilarity between question topic words and answer
candidate. The opinion score is calculated using
the number of opinion words normalized by the
total number of words in candidate sentence.
5.2 Performance Evaluation
5.2.1 Performance on Sentimental Lexicons
Lexicon Neg Pos Description
Name Size Size
1 HowNet 2700 2009 English translation
of positive/negative
Chinese words
2 Senti- 4800 2290 Words with a positive
WordNet or negative score
above 0.6
3 Intersec- 640 518 Words appeared in
tion both 1 and 2
4 Union 6860 3781 Words appeared in
1 or 2
5 All 10228 10228 All words appeared
in 1 or 2 without
distinguishing pos
or neg
Table 1: Sentiment lexicon description
For lexicon-based opinion analysis, the selec-
tion of opinion thesaurus plays an important role
in the final performance. HowNet2 is a knowledge
database of the Chinese language, and provides an
online word list with tags of positive and negative
polarity. We use the English translation of those
sentiment words as the sentimental lexicon. Sen-
tiWordNet (Esuli and Sebastiani, 2006) is another
popular lexical resource for opinion mining. Ta-
ble 1 shows the detail information of our used sen-
timent lexicons. In our models, the positive opin-
ion words are used only for positive questions, and
negative opinion words just for negative questions.
We initially set parameter ? in Opinion PageRank
as 0 as (Liu and Ma, 2005), and other parameters
simply as 0.5, including ? in Opinion PageRank,
? in Opinion HITS, and ? in baseline. The exper-
iment results are shown in Figure 4.
We can make three conclusions from Figure 4:
1. Opinion PageRank and Opinion HITS are both
effective. The best results of Opinion PageRank
and Opinion HITS respectively achieve around
35.4% (0.199 vs 0.145), and 34.7% (0.195 vs
0.145) improvements in terms of F score over the
best baseline result. We believe this is because our
proposed models not only incorporate the topic in-
formation and opinion information, but also con-
2http://www.keenage.com/zhiwang/e zhiwang.html
742
0 15
0.2
0.25
HowNet SentiWordNet Intersection Union All
0
0.05
0.1
.
Baseline Opinion PageRank Opinion HITS
Figure 4: Sentiment Lexicon Performance
sider the relationship between different answers.
The experiment results demonstrate the effective-
ness of these relations. 2. Opinion PageRank and
Opinion HITS are comparable. Among five sen-
timental lexicons, Opinion PageRank achieves the
best results when using HowNet and Union lexi-
cons, and Opinion HITS achieves the best results
using the other three lexicons. This may be be-
cause when the sentiment lexicon is defined appro-
priately for the specific question set, the opinion
PageRank model performs better. While when the
sentiment lexicon is not suitable for these ques-
tions, the opinion HITS model may dynamically
learn a temporal sentiment lexicon and can yield
a satisfied performance. 3. Hownet achieves the
best overall performance among five sentiment
lexicons. In HowNet, English translations of the
Chinese sentiment words are annotated by non-
native speakers; hence most of them are common
and popular terms, which maybe more suitable for
the Blog environment (Zhang and Ye, 2008). We
will use HowNet as the sentiment thesaurus in the
following experiments.
In baseline, the parameter ? shows the relative
contributions for topic score and opinion score.
We vary ? from 0 to 1 with an interval of 0.1, and
find that the best baseline result 0.170 is achieved
when ?=0.1. This is because the topic informa-
tion has been considered during candidate extrac-
tion, the system considering more opinion infor-
mation (lower ?) achieves better. We will use this
best result as baseline score in following experi-
ments. Since F(3) score is more related with re-
call, F score and recall will be demonstrated. In
the next two sections, we will present the perfor-
mances of the parameters in each model. For sim-
plicity, we denote Opinion PageRank as PR, Opin-
ion HITS as HITS, baseline as Base, Recall as r, F
score as F.
0.22
0.24
0.26
PR_r PR_F Base_r Base_F
F(3)
0.12
0.14
0.16
0.18
0.2
0 0.2 0.4 0.6 0.8 1 
Figure 5: Opinion PageRank Performance with
varying parameter ? (? = 0.5)
0.22
0.24
0.26
PR_r PR_F Base_r Base_F
F(3)
0.12
0.14
0.16
0.18
0.2
0 0.2 0.4 0.6 0.8 1
 
Figure 6: Opinion PageRank Performance with
varying parameter ? (? = 0.2)
5.2.2 Opinion PageRank Performance
In Opinion PageRank model, the value ? com-
bines the source opinion and the destination opin-
ion. Figure 5 shows the experiment results on pa-
rameter ?. When we consider lower ?, the system
performs better. This demonstrates that the desti-
nation opinion score contributes more than source
opinion score in this task.
The value of ? is a trade-off between answer
reinforcement relation and topic relation to calcu-
late the scores of each node. For lower value of ?,
we give more importance to the relevance to the
question than the similarity with other sentences.
The experiment results are shown in Figure 6. The
best result is achieved when ? = 0.8. This fig-
ure also shows the importance of reinforcement
between answer candidates. If we don?t consider
the sentence similarity(? = 0), the performance
drops significantly.
5.2.3 Opinion HITS Performance
The parameter ? combines the opinion hub score
and topic hub score in the Opinion HITS model.
The higher ? is, the more contribution is given
743
0.22
0.24
0.26
HITS_r HITS_F Base_r Base_FF(3)
0.12
0.14
0.16
0.18
0.2
0 0.2 0.4 0.6 0.8 1 
Figure 7: Opinion HITS Performance with vary-
ing parameter ?
to topic hub level, while the less contribution is
given to opinion hub level. The experiment results
are shown in Figure 7. Similar to baseline param-
eter ?, since the answer candidates are extracted
based on topic information, the systems consider-
ing opinion information heavily (?=0.1 in base-
line, ?=0.2) perform best.
Opinion HITS model ranks the sentences by au-
thority scores. It can also rank the popular opin-
ion words and popular topic words from the topic
hub layer and opinion hub layer, towards a specific
question. Take the question 1024.3 ?What reasons
do people give for liking Zillow?? as an example,
its topic word is ?Zillow?, and its sentiment polar-
ity is positive. Based on the final hub scores, the
top 10 topic words and opinion words are shown
as Table 2.
Opinion real, like, accurate, rich, right, interesting,
Words better, easily, free, good
Topic zillow, estate, home, house, data, value,
Words site, information, market, worth
Table 2: Question-specific popular topic words
and opinion words generated by Opinion HITS
Zillow is a real estate site for users to see the
value of houses or homes. People like it because it
is easily used, accurate and sometimes free. From
the Table 2, we can see that the top topic words
are the most related with question topic, and the
top opinion words are question-specific sentiment
words, such as ?accurate?, ?easily?, ?free?, not
just general opinion words, like ?great?, ?excel-
lent? and ?good?.
5.2.4 Comparisons with TAC Systems
We are also interested in the performance compar-
ison with the systems in TAC QA 2008. From Ta-
ble 3, we can see Opinion PageRank and Opinion
System Precision Recall F(3)
OpPageRank 0.109 0.242 0.200
OpHITS 0.102 0.256 0.205
System 1 0.079 0.235 0.186
System 2 0.053 0.262 0.173
System 3 0.109 0.216 0.172
Table 3: Comparison results with TAC 2008 Three
Top Ranked Systems (system 1-3 demonstrate top
3 systems in TAC)
HITS respectively achieve around 10% improve-
ment compared with the best result in TAC 2008,
which demonstrates that our algorithm is indeed
performing much better than the state-of-the-art
opinion QA methods.
6 Conclusion and Future Works
In this paper, we proposed two graph based sen-
tence ranking methods for opinion question an-
swering. Our models, called Opinion PageRank
and Opinion HITS, could naturally incorporate
topic relevance information and the opinion senti-
ment information. Furthermore, the relationships
between different answer candidates can be con-
sidered. We demonstrate the usefulness of these
relations through our experiments. The experi-
ment results also show that our proposed methods
outperform TAC 2008 QA Task top ranked sys-
tems by about 10% in terms of F score.
Our random walk based graph methods inte-
grate topic information and sentiment information
in a unified framework. They are not limited to
the sentence ranking for opinion question answer-
ing. They can be used in general opinion docu-
ment search. Moreover, these models can be more
generalized to the ranking task with two types of
influencing factors.
Acknowledgments: Special thanks to Derek
Hao Hu and Qiang Yang for their valuable
comments and great help on paper prepara-
tion. We also thank Hongning Wang, Min
Zhang, Xiaojun Wan and the anonymous re-
viewers for their useful comments, and thank
Hoa Trang Dang for providing the TAC eval-
uation results. The work was supported by
973 project in China(2007CB311003), NSFC
project(60803075), Microsoft joint project ?Opin-
ion Summarization toward Opinion Search?, and
a grant from the International Development Re-
search Center, Canada.
744
References
Ricardo Baeza-Yates and Berthier Ribeiro-Neto. 1999.
Modern Information Retrieval. Addison Wesley,
May.
Xavier Carreras and Lluis Marquez. 2005. Introduc-
tion to the conll-2005 shared task: Semantic role la-
beling.
Yi Chen, Ming Zhou, and Shilong Wang. 2006.
Reranking answers for definitional qa using lan-
guage modeling. In ACL-CoLing, pages 1081?1088.
Hang Cui, Min-Yen Kan, and Tat-Seng Chua. 2007.
Soft pattern matching models for definitional ques-
tion answering. ACM Trans. Inf. Syst., 25(2):8.
Hoa Trang Dang. 2008. Overview of the tac
2008 opinion question answering and summariza-
tion tasks (draft). In TAC.
Gu?nes Erkan and Dragomir R. Radev. 2004. Lex-
pagerank: Prestige in multi-document text summa-
rization. In EMNLP.
Andrea Esuli and Fabrizio Sebastiani. 2006. Senti-
wordnet: A publicly available lexical resource for
opinion mining. In LREC.
Jon M. Kleinberg. 1999. Authoritative sources in a
hyperlinked environment. J. ACM, 46(5):604?632.
Lun-Wei Ku, Yu-Ting Liang, and Hsin-Hsi Chen.
2007. Question analysis and answer passage re-
trieval for opinion question answering systems. In
ROCLING.
Tie-Yan Liu and Wei-Ying Ma. 2005. Webpage im-
portance analysis using conditional markov random
walk. In Web Intelligence, pages 515?521.
Rada Mihalcea and Paul Tarau. 2004. Textrank:
Bringing order into texts. In EMNLP.
Jahna Otterbacher, Gu?nes Erkan, and Dragomir R.
Radev. 2005. Using random walks for question-
focused sentence retrieval. In HLT/EMNLP.
Lawrence Page, Sergey Brin, Rajeev Motwani, and
Terry Winograd. 1998. The pagerank citation rank-
ing: Bringing order to the web. Technical report,
Stanford University.
Swapna Somasundaran, Theresa Wilson, Janyce
Wiebe, and Veselin Stoyanov. 2007. Qa with at-
titude: Exploiting opinion type analysis for improv-
ing question answering in online discussions and the
news. In ICWSM.
Kim Soo-Min and Eduard Hovy. 2005. Identifying
opinion holders for question answering in opinion
texts. In AAAI 2005 Workshop.
Veselin Stoyanov, Claire Cardie, and Janyce Wiebe.
2005. Multi-perspective question answering using
the opqa corpus. In HLT/EMNLP.
Vasudeva Varma, Prasad Pingali, Rahul Katragadda,
and et al 2008. Iiit hyderabad at tac 2008. In Text
Analysis Conference.
X. Wan and J Yang. 2008. Multi-document summa-
rization using cluster-based link analysis. In SIGIR,
pages 299?306.
Hong Yu and Vasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: Separating facts
from opinions and identifying the polarity of opinion
sentences. In EMNLP.
Min Zhang and Xingyao Ye. 2008. A generation
model to unify topic relevance and lexicon-based
sentiment for opinion retrieval. In SIGIR, pages
411?418.
745
Discovering patterns to extract protein-protein interactions from full 
biomedical texts 
*Minlie Huang1, +Xiaoyan Zhu1, Donald G. Payan2, Kunbin Qu2 and ++Ming Li3,1 
1State Key Laboratory of Intelligent Technology and Systems (LITS) 
Department of Computer Science and Technology, University of Tsinghua, Beijing, 100084, China 
2Rigel Pharmaceuticals Inc, 1180 Veterans. Blvd, South San Francisco, CA 94080, USA 
3Bioinformatics Laboratory, School of Computer Science, University of Waterloo, N2L 3G1, Canada 
*huangml00@mails.tsinghua.edu.cn 
+zxy-dcs@tsinghua.edu.cn 
++mli@uwaterloo.ca 
 
Abstract 
Although there have been many research pro-
jects to extract protein pathways, most such infor-
mation still exists only in the scientific literature, 
usually written in natural languages and defying 
data mining efforts. We present a novel and robust 
approach for extracting protein-protein interactions 
from the literature. Our method uses a dynamic 
programming algorithm to compute distinguishing 
patterns by aligning relevant sentences and key 
verbs that describe protein interactions. A match-
ing algorithm is designed to extract the interactions 
between proteins. Equipped only with a protein 
name dictionary, our system achieves a recall rate 
of about 80.0% and a precision rate of about 80.5%.  
1 Introduction 
Recently there are many accomplishments in 
literature data mining for biology, most of which 
focus on extracting protein-protein interactions. 
Most of such information is scattered in the vast 
scientific literature. Many research projects have 
been designed to collect protein-protein interaction 
data. Several databases are constructed to store 
such information, for example, Database of Inter-
acting Proteins (Xenarios et al, 2000; Salwinski et 
al., 2004). Most of the data in these databases were 
accumulated manually and inadequately, at high 
costs. Yet, scientists continue to publish their 
discoveries on protein-protein interactions in scien-
tific journals, without submitting their data to the 
databases. The fact is that most protein-protein 
interaction data still exist only in the scientific 
literature, written in natural languages and hard to 
be processed with computers.  
How to extract such information has been an 
active research subject. Among all methods, 
natural language processing (NLP) techniques are 
preferred and have been widely applied. These 
methods can be regarded as parsing-based methods. 
Both full and partial (or shallow) parsing strategies 
have been used. For example, a general full parser 
with grammars applied to the biomedical domain 
was used to extract interaction events by filling 
sentences into argument structures in (Yakushiji et 
al., 2001). No recall or precision rate was given. 
Another full parsing method, using bidirectional 
incremental parsing with combinatory categorial 
grammar (CCG), was proposed (Park et al, 2001). 
This method first localizes the target verbs, and 
then it scans the left and right neighborhood of the 
verb respectively. The lexical and grammatical 
rules of CCG are even more complicated than 
those of a general CFG. The recall and precision 
rates of the system were reported to be 48% and 
80%, respectively. Another full parser utilizing a 
lexical analyzer and context free grammar (CFG), 
extracts protein, gene and small molecule inter-
actions with a recall rate of 63.9% and a precision 
rate of 70.2% (Temkin et al, 2003). Similar 
methods such as preposition-based parsing to gene-
rate templates were proposed (Leroy and Chen, 
2002), processing only abstracts with a template 
precision of 70%. A partial parsing example is the 
relational parsing for the inhibition relation (Pus-
tejovsky et al, 2002), with a comparatively low 
recall rate of 57%. In conclusion, all the methods 
are inherently complicated, requiring many re-
sources, and the performances are not satisfactory.  
Some methods only focus on several special verbs. 
Another popular approach uses pattern matching. 
As an example, a set of simple word patterns and 
part-of-speech rules were manually coded, for each 
verb, to extract special kind of interactions from 
abstracts (Ono et al, 2001). The method obtains a 
recall rate of about 85% and a precision rate of 
about 94% for yeast and Escherichia coli, which is 
the best among all reported results. However, 
manually writing patterns for every verb is not 
practical for general purpose applications. In 
GENIES, more complicated patterns with syntactic 
and semantic constraints are used (Friedman et al, 
2001). GENIES even uses semantic information. 
However, GENIES' recall rate is low. In the above 
methods, patterns are hand-coded without 
exception. Because there are many verbs and their 
22
variants describing protein interactions, manually 
coding patterns for every verb and its variants is 
not feasible in practical applications.  
Most of the above methods process MEDLINE 
abstracts (Ng and Wong 1999; Thomas et al, 2000; 
Park et al, 2001; Yakushiji et al, 2001; Wong, 
2001; Marcotte et al, 2001; Leroy and Chen, 
2002). Because there is neither an accurate task 
definition on this problem nor a standard 
benchmark, it is hard to compare the results fairly 
among various methods (Hirschman et al, 2002). 
Furthermore as MEDLINE has become a standard 
resource for researchers, the results on the more 
difficult task of mining full text have been largely 
ignored. 
In this paper, we propose a novel and surprising-
ly robust method to discover patterns to extract 
interactions between proteins. It is based on 
dynamic programming (DP). In the realm of 
homology search between protein or DNA se-
quences, global and local alignment algorithm has 
been thoroughly researched (Needleman and 
Wunsch, 1970; Smith and Waterman, 1981). In our 
method, by aligning sentences using dynamic 
programming, the similar parts in sentences could 
be extracted as patterns. Compared with the pre-
vious methods, our proposal is different in the fol-
lowing ways: Firstly, it processes full biomedical 
texts, rather than only abstracts. Secondly, it auto-
matically mines verbs for describing protein inter-
actions. Thirdly, this method automatically dis-
covers patterns from a set of sentences whose 
protein names are identified, rather than manually 
creating patterns as most previous methods. Lastly, 
our method has low time complexity. It is able to 
process very long sentences. In contrast, for any 
full or partial parsing method, it is time- and 
memory-consuming to process long sentences. 
2 Method 
2.1 Alignment algorithm 
Suppose we have two sequences ),...,,( 21 nxxxX =  
and ),...,,( 21 myyyY =  which are defined over the 
alphabet }'',...,,{ 21 ?==? laaa . Each ai is called as a 
character, and '-'  denotes a white-space or a gap. 
We want to assign a score to measure how similar 
X and Y are. Define F(i,j) as the score of the 
optimal alignment between the initial segment 
from x1 to xi of X and the initial segment from y1 to 
yj of Y. F(i,j) is recursively calculated as follows: 
???
???
?
?+?
?+?
+??=
),'(')1,(
)'',(),1(
),()1,1(
,0
max),(
j
i
ji
ysjiF
xsjiF
yxsjiF
jiF
            (1a) 
??== ji yxjFiF ,,0),0(,0)0,(                    (1b) 
where s(a,b) is defined as follows: 
]))(*)((),(log[),( bpapbapbas =                (2) 
Here, p(a) denotes the appearance probability of 
character a, and p(a,b) denotes the probability that 
a and b appear at the same position in two aligned 
sequences. Probabilities p(a) and p(a,b) can be 
easily estimated by calculating appearance fre-
quencies for each pair with pre-aligned training 
data.  
Note that the calculation of scores for a gap will 
be different. In formula (2), when a or b is a gap, 
the scores can not be directly estimated by the 
formula because of two reasons: 1) the case that a 
gap aligns to another gap will never happen in the 
alignment algorithm since it is not optimal, 
therefore, what s('-', '-') exactly means is unclear;  2) 
Gap penalty should be negative, but it is unclear 
what p('-') should be. In DNA sequence alignment, 
these gap penalties are simply assigned with 
negative constants. Similarly, we tune each gap 
penalty for every character with some fixed 
negatives. Then a linear gap model is used. 
Given a sequence of gaps with length n which 
aligns to a sequence of ),...,,( 21 nxxxX =  with no gaps, 
the linear penalty is as follows: 
? = ?= ni ixsn 1 ),'(')(?                              (3) 
For sequence X of length n and sequence Y of 
length m, totally (n+1)*(m+1) scores will be 
calculated by applying equation (1a-b) recursively. 
Store the scores in a matrix F=F(xi, yi). Through 
back-tracing in F, the optimal local alignment can 
be found. 
In our method, the alphabet consists of three 
kinds of tags: 1) part-of-speech tags, as those used 
by Brill?s tagger (Brill et al, 1995); 2) tag PTN for 
protein names; 3) tag GAP for a gap or white-space. 
Gap penalties for main tags are shown in Table 1. 
 
Tag Penalty Tag Penalty Tag Penalty
PTN -10 IN -6 VBP -7 
NN -8 CC -6 VBD -7 
NNS -7 TO -1 VBG -7 
VBN -7 VB -7 VBZ -7 
RB -1 JJ -1 
Table 1. Gap penalties for main tags 
2.2 Pattern generating algorithm 
For our problem, a data structure called se-
quence structure, instead of a flat sequence, is used. 
Sequence structure consists of a sequence of tags 
(including PTN and GAP) and word indices in the 
original sentence for each tag (for tag PTN and 
GAP, word indices are set to -1). Through the 
structure, we are able to trace which words align 
together. 
23
Similarly, we also use another data structure 
called pattern structure which is made up of three 
parts: a sequence of tags; an array of word index 
lists for each tag, where each list defines a set of 
words for a tag that can appear at the correspond-
ing position of a pattern; a count of how many 
times the pattern has been extracted out in the 
training corpus. With the structure, the pattern 
generating algorithm is shown in Figure 1. The 
filtering rules are listed in Table 2.  
Note that a threshold d is used in the algorithm. 
If a pattern appears less than d times in the corpus, 
it will be discarded; otherwise those infrequent 
patterns will cause many matching errors. Through 
adjusting this parameter, generalization and usabi-
lity of patterns can be controlled. The larger the 
threshold is, the more general and accurate 
patterns are.  
Tags like JJ (adjective) and RB (adverb) are too 
common and can appear at every position in a 
sentence; hence if patterns include such kind of 
tags, they lose the generalization power. Some 
tags such as DT (determiner) only play a func-
tional role in a sentence and they are useless to 
pattern generation. Therefore, just as the first step 
in our algorithm shown in Figure 1, we remove 
directly the useless tags such as JJ, JJS (super-
lative adjective), JJR (comparative adjective), RB, 
RBS (superlative adverb), RBR (comparative 
adverb) and DT from the sequences. Furthermore, 
to control the form of a pattern, filtering rules 
shown in Table 2 are adapted. Verb or noun tags 
define interactions between proteins, thus they are 
indispensable for a pattern, as the first rule shows. 
The second rule guarantees the integrality of a 
pattern because tags like IN and TO must be 
followed by an object. The last one requires 
symmetry between the left and right neighborhood 
of CC tag. Actually more rigid or looser filtering 
rules than those shown in Table 2 can be applied 
to meet special demands, which will affect the 
forms of patterns. 
 
Table 2. Filtering rules. 
2.3 Pattern matching algorithm 
Because one pattern possibly matches a sen-
tence at different positions, we have to explore an 
algorithm that is able to find out multiple matches. 
 
Figure 1. Pattern generating algorithm. Time com-
plexity is O(n2) in the corpus size n. 
 
Here if we think a pattern as a motif, and sentence 
as a protein sequence, then our task is similar to 
finding out all motifs in the sequence. 
Suppose that ),...,,( 21 nxxxX =  is the sequence of 
tags for a sentence in which we look for multiple 
matches, and ),...,,( 21 myyyY =  is a pattern. We still 
use a score matrix F, while the recurrence, defined 
by formulas (4a-b), is different from that of pattern 
generating algorithm. Formula (4a) only allows 
matches to end when they have score at least T.  
??
?
=??
?=
=
mjTjiF
iF
iF
F
L,2,1,),1(
)0,1(
max)0,(
0)0,0(
   (4a) 
 
???
???
?
?+?
?+?
+??=
),'(')1,(
)'',(),1(
),()1,1(
),0,(
max),(
j
i
ji
ysjiF
xsjiF
yxsjiF
iF
jiF        (4b) 
The total score of all matches is obtained by 
adding an extra cell to the matrix, F(n+1,0), using 
(4a). By tracing back from cell (n+1,0) to (0,0), 
the individual match alignments will be obtained. 
Threshold T should not be identical for different 
patterns. Threshold T is calculated as follows: 
?== mi ii yysT 1 ),(?                                (5) 
where ?  is a factor, in our method we take ?=0.5. 
The right hand of formula (5) is the maximum 
score when a pattern matches a sentence perfectly. 
A match is accepted only when three conditions 
are satisfied: 1) a pattern has a local optimal match 
with the sentence; 2) words in matching part of the 
sentence can be found in the word set of the 
pattern; 3) decision rules are satisfied.  
1. If a pattern has neither verb tag nor noun 
tag, reject it. 
2. If the last tag of a pattern is IN or TO, 
reject it. 
3. If the left neighborhood of a CC tag is not 
equal the right neighborhood of the tag in 
a pattern, reject the pattern. 
Input:  an integer d,  
a sequence set ),...,,( 21 nsssS =  
Output: pattern set P 
1. Remove useless tags from each si in S 
2. For any )(),( jiSss ji ??  do 
a) Do local alignment for si and sj. Aligned 
output is Xa and Yb; 
b) Extract the identical characters at the same 
positions in Xa and Yb as pattern p. Add the 
corresponding word indices to pattern
structure; 
c) Judge whether p is legal, using the filtering 
rules. If it is illegal, go to step 2; 
d) If p exists in P, increase the count of p 
with 1. If not, add p to P with a count of 1;
3. For every p in P , do 
If the count of p is less than d, discard p;
4. Output P.
24
 
Figure 2. Pattern matching algorithm. Time com-
plexity of |))|*|*(||(| pXPO in pattern set size |P|, 
sequence length |X| and average length of pattern 
|| p  
 
To show details how well a pattern matches a 
sentence, a measurement data structure is defined, 
which is formalized as a vector. It will be referred 
to as mVector: 
 ),,,( cVbcPtncMatchcLenmVector =                  (6) 
where cLen is the length of a pattern; cMatch is 
the number of matched tags; cPtn is the number of 
protein name tag (PTN) skipped by the alignment 
in the sentence;  cVb is the number of skipped 
verbs. Based on the structure, decision rules 
shown in Table 3 are used in the pattern matching. 
There are two parameters P and V used in the 
decision rules, which can be adjusted according to 
the performance of the experiments. Here we take 
P=0 and V=2.  
 
 
Table 3. Decision rules.  
3 System overview 
Our system uses the framework of Pathway-
Finder (Yao et al, 2004). It consists of several 
modular components, as shown in Figure 3.  
The external resource required in our method is 
a dictionary of protein names, where about 60,000 
items are collected from both databases of 
PathwayFinder and several web databases, such as 
TrEMBL, SWISSPROT (O'Donovan et al, 2002), 
and SGD (Cherry et al, 1997), including many 
synonyms. The training corpus contains about 
1200 sentences which will be explained with 
details in the next section. Patterns generated at the 
training phase are stored in the pattern database.  
For an input sentence, firstly some filtering rules 
are adapted to remove useless expressions at the 
pre-processing phase. For example, remove 
citations, such as '[1]', and listing figures, such as 
'(1)'. Then protein names in the sentence are 
identified according to the protein name dictionary 
and the names are replaced with a unique label. 
Subsequently, the sentence is part-of-speech 
tagged by Brill?s tagger (Brill et al, 1995), where 
the tag of protein names is changed to tag PTN. 
Last, since a sequence of tags is obtained, it can be 
added into the corpus at the training phase or it can 
be used by the matching algorithm at the testing 
phase.  
Because the pattern acquisition algorithm is 
aligning sequences of tags, the accuracy of part-of-
speech tagging is crucial. However, Brill?s tagger 
only obtained overall 83% accuracy for biomedical 
texts. This is because biomedical texts contain 
many unknown words. Here we propose a simple 
and effective approach called pre-tagging strategy 
to improve the accuracy, just as the method used 
by (Huang et al, 2004). 
Figure 3. Architecture of our system. 
4 Results 
Our evaluation experiments are made up of three 
parts: mining verbs for patterns, extracting patterns 
and evaluating precision and recall rates. 
4.1 Mining verbs 
The algorithm shown in Figure 1 is performed 
on the whole corpus and one more filtering rule as 
follows, is used, besides those in Table 2: 
     If the pattern has no verb tag, reject it. 
With this rule, only patterns that have verbs are 
extracted. Here the threshold d is set to 10 to 
obtain high accurate verbs for the subsequent 
Input:  a pattern set ),...,,( 21 npppP = ,  
a sequence X 
Output: aligned result set R  
1. For every pattern pi in P, do 
a) Set threshold T for pattern pi, using 
formula (5); 
b) For X and the sequence of pattern pi; build 
score matrix F using formula (4a-b); 
c) Trace-back to find multiple matches. The 
results are },,,{ 21 araar XXXA L= ;  
d) For every result Xai  in Ar 
i. Check whether every word in Xai aligned 
to pi appears in the corresponding position 
of pi,   if not, go to step d); 
ii. Fill all data in mVector ; 
iii. Determine to accept or reject the match 
according to decision rules. If reject, go to 
step d); 
iv. Add Xai to the result set R; 
2. Output R. 
Input: two parameters P and V 
1. If cMatch ? cLen, reject the match; 
2. if cPtn > P, reject the match; 
3. if  cVb > V, reject the match;  
Protein Name 
Identification 
Generating 
Algorithm 
Sentence Protein interactions 
Corpus 
Matching algorithm Preprocessing
Protein Name
Database 
Pattern 
Database
POS Tagger 
Train
Test
25
experiments. Totally 94 verbs are extracted from 
367 verbs for describing interactions. Note that 
different tense verbs that have the same base form 
are counted as different ones. There are false 
positives which do not define interactions seman-
tically at all, such as 'affect', 'infect', 'localize', 
amounting to 16. Hence the accuracy is 83.0%.  
These verbs and their variants, particularly the 
gerund and noun form, (obtained from an English 
lexicon) are added into a list of filtering words, 
which is named as FWL (Filtering Word List). For 
example, for verb 'inhibit', its variants including 
'inhibition', 'inhibiting', 'inhibited' and 'inhibitor' 
are added into FWL. At the current phase, we add 
all verbs into FWL, including false positives 
because we think these verbs are also helpful to 
understand pathway networks between proteins.  
4.2 Extracting patterns 
Pattern generating algorithm is performed on the 
whole corpus with FWL. The threshold d is 5 here. 
The filtering rules in Table 2, plus the following 
rule, are applied. 
If a pattern has any verb or noun that is not in 
FWL, reject it. 
This ensures that the patterns have a good form 
and all their words are valid. In other word, this 
rule guarantees that the main verbs or nouns in 
every pattern exactly describe protein interactions. 
The experiment runs on about 1200 sentences, with 
threshold d=5, and 134 patterns are obtained.  
Some of them are listed in Figure 4.  
4.3 Evaluating precision and recall rates 
In this part, three tests are performed. The first 
test uses 383 sentences that only contain keyword 
interact and its variants. 293 of them are used to 
extract patterns and the rest are tested. The second 
one uses 329 sentences that only contain key word 
bind and its variants. 250 of them are used to 
generate patterns and the rest are tested. The third 
one uses 1205 sentences with all keywords, where 
1020 are used to generate patterns, the rest for test. 
As described before, we do not exclude those verbs 
such as 'affect', 'infect' and so on, therefore 
relations between proteins defined by these verbs 
or nouns are thought to be interactions. Note that 
the testing and training sentences are randomly 
partitioned, and they do not overlap in all these 
tests. The results are shown in Table 4. Some 
matching examples are shown in Figure 5. Simple 
sentences as sen1-2 are matched by only one 
pattern. But it is more common that several 
patterns may match one sentence at different 
positions, as in sen3-4. In examples sen5, the same 
pattern matches repeatedly at different positions 
since we used a 'multiple matches' algorithm.  
Keywords Recall Precision F-score 
Interact 80.5% 84.6% 82.5%
Bind 81.7% 82.8% 82.2%
All verbs 79.9% 80.3% 80.2%
Table 4. The recall and precision experiments.  
5 Discussion  
We have proposed a new method for automa-
tically generating patterns and extract protein 
interactions. In contrast, our method outperforms 
the previous methods in two main aspects: first, it 
automatically mines patterns from a set of sen-
tences whose protein names are identified; second, 
it is competent to process long and complicated 
sentences from full texts. 
In our method, a threshold d is used to control 
both the number of patterns and the generalization 
power of patterns. Although infrequent patterns are 
filtered by a small threshold, a glance to these 
patterns is meaningful. For example, on 293 
sentences containing keyword 'interact' and its 
variants, patterns whose count equals one are 
shown in Figure 6. Among the results, some are 
reasonable, such as 'PTN VBZ IN PTN IN PTN ' 
(protein1 interacts with protein2 through protein3). 
These kinds of patterns are rejected because of 
both insufficient training data and infrequently 
used expressions in natural language texts. Some 
patterns are not accurate, such as 'NNS IN PTN 
PTN PTN ', because there must be a coordinating 
conjunction between the three continuous protein 
names, otherwise it will cause many errors. Some 
patterns are even wrong, such as 'PTN NN PTN ' 
because there are never such segment 'protein1 
interaction protein2' defining a real interaction 
between protein1 and protein2. Some patterns, such 
as 'PTN VBZ IN CC IN PTN ' which should be 
'PTN VBZ IN PTN CC IN PTN ' (protein1 interacts 
with protein2 and with protein3), are not precise 
because the last filtering rule in Table 2 is used.  
Nevertheless, these patterns can be filtered out 
by the threshold. However, how to evaluate and 
maintain patterns becomes a real problem. For 
example, when the pattern generating algorithm is 
applied on about 1200 sentences, with a threshold 
d=0, approximate 800 patterns are generated, most 
of which appeared only once in the corpus. It is 
necessary to reduce such large amount of patterns. 
A MDL-based algorithm that measures the con-
fidence of each pattern and maintains them without 
human intervention is under development. 
Because our matching algorithm utilizes part-of-
speech tags, and our patterns do not contain any 
adjective (JJ), interactions defined by adjectives, 
such as 'inducible' and 'inhibitable', cannot be 
extracted correctly by our method currently.  
26
Pattern 
Count 
Pattern 
Form 
Word lists of 
pattern 
1914 PTN VBZ PTN * ;modifies promotes inhibits activates mediates blocks enhances forms ;* ; 
758 PTN VBZ IN PTN * ; interacts associates; with in within ;* ; 
402 NN IN PTN CC PTN interaction association activation modification degradation ;between with of 
from by ;* ;and or but ;* ; 
270 PTN NN IN PTN * ;interaction complex conjugation modification association ;with of on by in 
within between ;* ; 
199 PTN VBZ TO PTN * ;binds; to ;* ; 
99 PTN VBZ IN PTN CC PTN * ;assembles interacts associates; of in with from ;* ;and but ;* ; 
16 PTN CC PTN NN IN PTN * ;and or ;* ;interaction conjugation complex ubiquitination degradation 
modification activation recognition ;between of with by ;* ; 
5 PTN VBP IN PTN * ;interact ;with ;* ; 
Figure 4. Pattern examples. The star symbol denotes a protein name. Words for each component of a pattern 
are separated by a semicolon. For simplicity, words in a pattern are partially listed. 
 
 
Figure 5. Examples of protein interactions extracted from sentences. Words in bold are protein names. For 
every sentence, the patterns used in the matching algorithm are listed, followed by the corresponding results. 
 
Pattern 
Count 
Pattern 
Form 
Word lists of 
pattern 
1 PTN VBZ IN CC IN PTN  * ;interacts ;with ;and ;with ;* ; 
1 PTN VBZ IN PTN IN PTN * ;interacts ;with ;* ;through;* ; 
1 PTN NN PTN * ;interaction ;* ; 
1 NNS IN PTN PTN PTN  interactions interaction ;with between ;* ;* ;* ; 
Figure 6. Some patterns whose count equals one are generated by our algorithm. 293 sentences containing 
keyword 'interact' and its variants are used in the training. 
 
This can be demonstrated by the following sen-
tence, where words in bold are protein names.  
 ?The class II proteins are expressed 
constitutively on B-cells and EBV-transformed 
B-cells, and are inducible by IFN-gamma on a 
wide variety of cell types.?   
In this sentence, interaction between class II 
proteins and IFN-gamma is defined by an 
adjective inducible (tagged as JJ) does not match 
any pattern. To solve this problem, we are 
considering using word stemming and morpheme 
recognition to convert adjectives into their 
corresponding verbs with context. 
By analyzing our experimental results, We find 
that the current matching algorithm is not optimal 
and causes approximately one-third of total errors. 
This partially derives from the simple decision 
rules used in the matching algorithm. These rules 
may work well for some texts but partially fail for 
others because the natural language texts are 
multifarious. With these considerations, a more 
accurate and complicated matching algorithm is 
under development. 
6 Conclusion 
In this paper, a method for automatically 
generating patterns to extract protein-protein inter-
actions is proposed and implemented. The method 
is capable of discovering verbs and patterns in 
biomedical texts. The algorithm is fast and able to 
process long sentences. Experiments show that a 
recall rate of about 80% and a precision rate of 
about 80% are obtained. The approach is powerful, 
robust, and applicable to real and large-scale full 
texts. 
7 Acknowledgements 
The work was supported by Chinese Natural 
Sen1: Here, we show that HIPK2 is regulated by a ubiquitin-like protein, SUMO-1.  
Pattern: PTN VBN IN PTN           result: HIPK2 regulated by SUMO-1 
Sen2: SPB association of Plo1 is the earliest fission yeast mitotic event recorded to date. 
Pattern: PTN NN IN PTN    result: SPB association of Plo1 
Sen3: In the absence of Mad2, BubR1 inhibits the activity of APC by blocking the binding of Cdc20 to APC. 
Pattern: PTN VBZ PTN    result: BubR1 inhibits APC 
Pattern: NN IN PTN TO PTN   result: binding of Cdc20 to APC 
Sen4: All proteins of this family have Cdk-binding and anion-binding sites, but only mammalian Cks1 binds to Skp2 and promotes 
the association of Skp2 with p27 phosphorylated on Thr-187. 
Pattern: PTN VBZ TO PTN    result: Cks1 binds to Skp2 
Pattern: NN IN PTN IN PTN   result: association of Skp2 with p27 
Sen5: Evidence is also provided that, in vivo, E6 can interact with p53 in the absence of E6-AP and that E6-AP can interact with 
p53 in the absence of E6. 
Pattern: PTN VB IN PTN   result: E6 interact with p53 
Pattern: PTN VB IN PTN   result: E6-AP interact with p53 
27
Science Foundation under grant No.60272019 and 
60321002, the Canadian NSERC grant 
OGP0046506, CRC Chair fund, and the Killam 
Fellowship. We would like that thank Jinbo Wang 
and Daming Yao for their collaboration on the 
PathwayFinder system. 
References  
Brill,E. (1995) Transformation-based error-driven 
learn-ing and natural language processing: a case 
study in part-of-speech tagging. Computational 
Linguistics, 21(4), 543?565. 
Cherry,JM, Ball,C, Weng,S, Juvik,G, Schmidt,R, 
Adler,C, Dunn,B, Dwight,S, Riles,L, 
Mortimer,RK, Botstein,D (1997) Genetic and 
physical maps of Saccharomyces cerevisiae. 
Nature 387(6632  Suppl), 67-73. 
Friedman,C., Kra,P., Yu,H., Krauthammer,M., and 
Rzhetsky,A. (2001) Genies: a natural-language 
processing system for the extraction of molecular 
pathways from journal articles. Bioinformatics, 
17 suppl. 1:S74?82. 
Hirschman,L., Park,JC, Tsujii,J, Wong,L., Wu,C.H. 
(2002) Accomplishments and challenges in 
literature data mining for biology. Bioinformatics, 
18:1553--1561, December 2002. 
Huang,M., Zhu,X. and Li,M. (2004) A new 
method for automatic pattern acquisition to 
extract information from biomedical texts. In the 
Seventh International Conference on Signal 
Processing, August, Beijing, China. Accepted. 
Leroy,G.  and Chen,H. (2002) Filling preposition-
based templates to capture information from 
medical abstracts. In Pacific Symposium on 
Biocomputing 7, Hawaii, USA, pp. 350-361. 
Marcotte,EM, Xenarios,I., and Eisenberg,D. (2001) 
Mining literature for protein-protein interactions. 
Bioinformatics, 17(4), 359?363. 
Needleman,S.B. and Wunsch,C.D. (1970) A 
general method applicable to the search for 
similarities in the amino acid sequence of two 
proteins. J. Mol. Biol., 48, 443-453. 
Ng,S.K. and Wong,M. (1999) Toward routine 
automatic pathway discovery from on-line 
scientific text abstracts, Proceedings of 10th 
International Workshop on Genome 
Informatics,  Tokyo, December 1999, pp. 104-
112. 
O'Donovan,C., Martin,MJ, Gattiker,A., 
Gasteiger,E., Bairoch,A. and Apweiler,R. (2002) 
High-quality pro-tein knowledge resource: 
Swiss-Prot and TrEMBL. Briefings in 
Bioinformatics 2002 Sep; 3(3), 275-284. 
Ohta,T.,  Tateishi,Y., Collier,N., Nobata,C., and 
Tsujii,J. (2000)  Building an annotated corpus 
from biology research papers. Proc. COLING-
2000 Workshop on Semantic Annotation and 
Intelligent Content, Luxembourg, pp. 28-34. 
Ono,T., Hishigaki,H., Tanigami,A., and Takagi,T. 
(2001) Automated extraction of information on 
protein-protein interactions from the biological 
literature. Bioinformatics, 17(2), 155?161. 
Park,JC, Kim,HS, and Kim,JJ (2001) Bidirectional 
incremental parsing for automatic pathway 
identify-cation with combinatory categorical 
grammar. In Proceedings of the Pacific 
Symposium Biocom-putting, Hawaii, USA, pp 
396-407. 
Pustejovsky,J, Castano,J, Zhang,J, Kotecki,M, and 
Cochran,B (2002) Robust relational parsing over 
biomedical literature: extracting inhibit relations. 
In Proceedings of the seventh Pacific Symposium 
on Biocomputing (PSB 2002), pp. 362-373. 
Salwinski,L, Miller,CS, Smith,AJ, Pettit,FK, 
Bowie,JU, Eisenberg,D (2004) The database of 
interacting proteins: 2004 update. NAR 32 
Database issue: D449-51. 
Smith,T.F. and Waterman,M.S. (1981) 
Identification of common molecular 
subsequences. J. Mol. Biol., 147, 195-197. 
Thomas,J, Milward,D, Ouzounis,C, Pulman,S and 
Carroll,M (2000) Automatic extraction of protein 
interactions from scientific abstracts. In 
Proceedings of the Pacific Symposium on 
Biocomputing, Hawaii, USA, Jan 2000, pp. 541?
551. 
Wong,L. (2001) A protein interaction extraction 
system, Proceedings of Pacific Symposium on 
Biocomputing 2001, Hawaii, January 2001, pp. 
520-530. 
Xenarios,I, Rice,D.W., Salwinski,L., Baron,M.K., 
Marcotte,E.M., Eisenberg.D. (2000) DIP: The 
data-base of interacting proteins. NAR 28, 289-
91. 
Yakushiji,A., Tateisi,Y., Miyao,Y., Tsujii,J. (2001) 
Event extraction from biomedical papers using a 
full parser. In Proceedings of the sixth Pacific 
Symposium on Biocomputing (PSB 2001), 
Hawaii, USA, pp. 408-419. 
Yao,D., Wang,J., Lu,Y., Noble,N., Sun,H., Zhu,X., 
Lin,N., Payan,D.G., Li,M., Qu,K. (2004) 
Pathway-Finder: paving the way towards 
automatic pathway extraction. In Yi-Ping Phoebe 
Chen, ed., Bioinformatics 2004: Proceedings of 
the 2nd Asia-Pacific Bioinformatics Conference 
(APBC), 29 volume of CRPIT, pp. 53-62, 
Dunedin, New Zealand, January 2004. 
Australian Computer Society. 
28
Proceedings of the Workshop on BioNLP, pages 97?105,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
 
Towards Automatic Generation of Gene Summary 
 
 
Feng Jin Minlie Huang 
Dept. Computer Science and Technology Dept. Computer Science and Technology 
Tsinghua University Tsinghua University 
Beijing 100084, China Beijing 100084, China 
jinfengfeng@gmail.com aihuang@tsinghua.edu.cn 
Zhiyong Lu Xiaoyan Zhu 
National Center for Biotechnology Information Dept. Computer Science and Technology 
National Library of Medicine Tsinghua University 
Bethesda, 20894, USA Beijing 100084, China 
luzh@ncbi.nlm.nih.gov zxy-dcs@tsinghua.edu.cn 
 
  
  
 
 
Abstract 
In this paper we present an extractive system that au-
tomatically generates gene summaries from the biomed-
ical literature. The proposed text summarization system 
selects and ranks sentences from multiple MEDLINE 
abstracts by exploiting gene-specific information and 
similarity relationships between sentences. We evaluate 
our system on a large dataset of 7,294 human genes and 
187,628 MEDLINE abstracts using Recall-Oriented 
Understudy for Gisting Evaluation (ROUGE), a widely 
used automatic evaluation metric in the text summariza-
tion community. Two baseline methods are used for 
comparison. Experimental results show that our system 
significantly outperforms the other two methods with 
regard to all ROUGE metrics. A demo website of our 
system is freely accessible at 
http://60.195.250.72/onbires/summary.jsp.  
1 Introduction 
Entrez Gene is a database for gene-centric infor-
mation maintained at the National Center for Bio-
technology Information (NCBI). It includes genes 
from completely sequenced genomes (e.g. Homo 
sapiens). An important part of a gene record is the 
summary field (shown in Table 1), which is a small 
piece of text that provides a quick synopsis of what 
is known about the gene, the function of its en-
coded protein or RNA products, disease associa-
tions, genetic interactions, etc. The summary field, 
when available, can help biologists to understand 
the target gene quickly by compressing a huge 
amount of knowledge from many papers to a small 
piece of text. At present, gene summaries are gen-
erated manually by the National Library of Medi-
cine (NLM) curators, a time- and labor-intensive 
process. A previous study has concluded that ma-
nual curation is not sufficient for annotation of ge-
nomic databases (Baumgartner et al, 2007). 
Indeed, of the 5 million genes currently in Entrez 
Gene, only about 20,000 genes have a correspond-
ing summary. Even in humans, arguably the most 
important species, the coverage is modest: only 26% 
of human genes are curated in this regard. The goal 
of this work is to develop and evaluate computa-
tional techniques towards automatic generation of 
gene summaries. 
To this end, we developed a text summarization 
system that takes as input MEDLINE documents 
related to a given target gene and outputs a small 
set of genic information rich sentences. Specifical-
ly, it first preprocesses and filters sentences that do 
 
97
 Gene Number  of 
Abstracts 
GO terms Human-writtenSummary 
EFEMP1 26 calcium ion binding 
protein binding 
extracellular region 
proteinaceous extracellu-
lar matrix 
This gene spans approximately 18 kb of genomic DNA and consists of 12 ex-
ons. Alternative splice patterns in the 5\' UTR result in three transcript variants 
encoding the same extracellular matrix protein. Mutations in this gene are asso-
ciated with Doyne honeycomb retinal dystrophy. 
IL20RA 15 blood coagulation 
receptor activity 
integral to membrane 
membrane 
The protein encoded by this gene is a receptor for interleukin 20 (IL20), a cyto-
kine that may be involved in epidermal function. The receptor of IL20 is a hete-
rodimeric receptor complex consisting of this protein and interleukin 20 
receptor beta (IL20B). This gene and IL20B are highly expressed in skin. The 
expression of both genes is found to be upregulated in Psoriasis. 
Table1. Two examples of human-written gene summaries 
not include enough informative words for gene 
summaries. Next, the remaining sentences are 
ranked by the sum of two individual scores: a) an 
authority score from a lexical PageRank algorithm 
(Erkan and Radev, 2004) and b) a similarity score 
between the sentence and the Gene Ontology (GO) 
terms with which the gene is annotated (To date, 
over 190,000 genes have two or more associated 
GO terms). Finally, redundant sentences are re-
moved and top ranked sentences are nominated for 
the target gene.  
In order to evaluate our system, we assembled a 
gold standard dataset consisting of handwritten 
summaries for 7,294 human genes and conducted 
an intrinsic evaluation by measuring the amount of 
overlap between the machine-selected sentences 
and human-written summaries. Our metric for the 
evaluation was ROUGE1, a widely used intrinsic 
summarization evaluation metric. 
2 Related Work 
Summarization systems aim to extract salient text 
fragments, especially sentences, from the original 
documents to form a summary. A number of me-
thods for sentence scoring and ranking have been 
developed. Approaches based on sentence position 
(Edmundson, 1969), cue phrase (McKeown and 
Radev, 1995), word frequency (Teufel and Moens, 
1997), and discourse segmentation (Boguraev and 
Kennedy, 1997) have been reported. Radev et al 
(Radev et al, 2004) developed an extractive multi-
document summarizer, MEAD, which extracts a 
summary from multiple documents based on the 
document cluster centroid, position and first-
sentence overlap. Recently, graph-based ranking 
methods, such as LexPageRank (Erkan and Radev, 
2004) and TextRank (Mihalcea and Tarau, 2004), 
                                                          
1 http://haydn.isi.edu/ROUGE/ 
have been proposed for multi-document summari-
zation. Similar to the original PageRank algorithm, 
these methods make use of similarity relationships 
between sentences and then rank sentences accord-
ing to the ?votes? or ?recommendations? from 
their neighboring sentences. 
Lin and Hovy (2000) first introduced topic sig-
natures which are topic relevant terms for summa-
rization. Afterwards, this technique was 
successfully used in a number of summarization 
systems (Hickl et al, 2007, Gupta and Nenkova et 
al., 2007). In order to improve sentence selection, 
we adopted the idea in a similar way to identify 
terms that tend to appear frequently in gene sum-
maries and subsequently filter sentences that in-
clude none or few such terms. 
Compared with newswire document summariza-
tion, much less attention has been paid to summa-
rizing MEDLINE documents for genic information. 
Ling et al (Ling et al, 2006 and 2007) presented 
an automatic gene summary generation system that 
constructs a summary based on six aspects of a 
gene, such as gene products, mutant phenotype, etc. 
In their system, sentences were ranked according 
to a) the relevance to each category (namely the 
aspect), b) the relevance to the document where 
they are from; and c) the position where sentences 
are located. Although the system performed well 
on a small group of genes (10~20 genes) from Fly-
base, their method relied heavily on high-quality 
training data that is often hard to obtain in practice.  
Yang et al reported a system (Yang et al, 2007 
and 2009) that produces gene summaries by focus-
ing on gene sets from microarray experiments. 
Their system first clustered gene set into functional 
related groups based on free text, Medical Subject 
Headings (MeSH?) and Gene Ontology (GO) fea-
tures. Then, an extractive summary was generated 
for each gene following the Edmundson paradigm 
98
 (Edmundson, 1969). Yang et al also presented 
evaluation results based on human ratings of eight 
gene summaries.  
Another related work is the second task of Text 
REtrieval Conference 2  (TREC) 2003 Genomics 
Track. Participants in the track were required to 
extract GeneRIFs from MEDLINE abstracts 
(Hersh and Bhupatiraju, 2003). Many teams ap-
proached the task as a sentence classification prob-
lem using GeneRIFs in the Entrez database as 
training data (Bhalotia et al, 2003; Jelier et al, 
2003). This task has also been approached as a sin-
gle document summarization problem (Lu et al, 
2006).  
The gene summarization work presented here 
differs from the TREC task in that it deals with 
multiple documents. In contrast to the previously 
described systems for gene summarization, our 
approach has three novel features. First, we are 
able to summarize all aspects of gene-specific in-
formation as opposed to a limited number of prede-
termined aspects. Second, we exploit a lexical 
PageRank algorithm to establish similarity rela-
tionships between sentences. The importance of a 
sentence is based not only on the sentence itself, 
but also on its neighbors in a graph representation. 
Finally, we conducted an intrinsic evaluation on a 
large publicly available dataset. The gold standard 
assembled in this work makes it possible for com-
parisons between different gene summarization 
systems without human judgments.  
3 Method 
To determine if a sentence is extract worthy, we 
consider three different aspects: (1) the number of 
salient or informative words that are frequently 
used by human curators for writing gene summa-
ries; (2) the relative importance of a sentence to be 
included in a gene summary; (3) the gene-specific 
information that is unique between different genes.  
Specifically, we look for signature terms in 
handwritten summaries for the first aspect. Ideally, 
computer generated summaries should resemble 
handwritten summaries. Thus the terms used by 
human curators should also occur frequently in 
automatically generated summaries. In this regard, 
we use a method similar to Lin and Hovy (2000) to 
identify signature terms and subsequently use them 
                                                          
2 http://ir.ohsu.edu/genomics/ 
to discard sentences that contain none or few such 
terms. For the second aspect, we adopt a lexical 
PageRank method to compute the sentence impor-
tance with a graph representation. For the last as-
pect, we treat each gene as having its own 
properties that distinguish it from others. To reflect 
such individual differences in the machine-
generated summaries, we exploit a gene?s GO an-
notations as a surrogate for its unique properties 
and look for their occurrence in abstract sentences.  
Our gene summarization system consists of 
three components: a preprocessing module, a sen-
tence ranking module, and a redundancy removal 
and summary generation module. Given a target 
gene, the preprocessing module retrieves corres-
ponding MEDLINE abstracts and GO terms ac-
cording to the gene2pubmed and gene2go data 
provided by Entrez Gene. Then the abstracts are 
split into sentences by the MEDLINE sentence 
splitter in the LingPipe3 toolkit. The sentence rank-
ing module takes these as input and first filters out 
some non-informative sentences. The remaining 
sentences are then scored according to a linear 
combination of the PageRank score and GO relev-
ance score.  Finally, a gene summary is generated 
after redundant sentences are removed. The system 
is illustrated in Figure 1 and is described in more 
detail in the following sections.  
 
 
Figure 1. System overview  
3.1 Signature Terms Extraction 
There are signature terms for different topic texts 
(Lin and Hovy, 2000). For example, terms such as 
eat, menu and fork that occur frequently in a cor-
pus may signify that the corpus is likely to be 
                                                          
3 http://alias-i.com/lingpipe/ 
Abstracts 
Sentence Segmentation 
Tokenization 
Stemming 
Signature Filtering 
PageRank Scoring 
GO Scoring 
Redundancy Removal 
GO Terms Summary 
99
 about cooking or restaurants. Similarly, there are 
signature terms for gene summaries. 
We use the Pearson?s chi-square test (Manning 
and Sch?tze, 1999) to extract topic signature terms 
from a set of handwritten summaries by comparing 
the occurrence of terms in the handwritten summa-
ries with that of randomly selected MEDLINE ab-
stracts. Let R denote the set of handwritten 
summaries and R denote the set of randomly se-
lected abstracts from MEDLINE. The null hypo-
thesis and alternative hypothesis are as follows:  
0H : ( | ) ( | )i iP t R p P t R= =   
1 1 2H : ( | ) ( | )i iP t R p p P t R= ? =   
The null hypothesis says that the term it appears 
in R and in R with an equal probability and it is 
independent from R . In contrast, the alternative 
hypothesis says that the term it is correlated with
R . We construct the following 2-by-2 contingency 
table:  
 
 R  R  
it  11O  12O  
it  21O  22O  
Table 2. Contingency table for the chi-square test. 
 
where 
11O : the frequency of term it occurring in R ; 
12O : the frequency of it occurring in R ; 
21O  : the frequency of term i it t? occurring in R ; 
22O :  the frequency of it in R .  
Then the Pearson?s chi-square statistic is computed 
by  
22
2
, 1
( )ij ij
i j ij
O E
X
E
=
?
=?  
where ijO is the observed frequency and ijE is the 
expected frequency.  
In our experiments, the significance level is set 
to 0.001, thus the corresponding chi-square value 
is 10.83. Terms with 2X value above 10.83 would 
be selected as signature terms. In total, we obtained 
1,169 unigram terms. The top ranked (by 2X value) 
signature terms are listed in Table 3. Given the set 
of signature terms, sentences containing less than 3 
signature terms are discarded. This parameter was 
determined empirically during the system devel-
opment.  
 
protein 
gene 
encode 
family 
transcription 
member 
variant 
domain 
splice 
subunit 
receptor 
isoform 
alternative 
bind 
involve 
Table 3. A sample of unigram topic signature terms. 
3.2 Lexical PageRank Scoring 
The lexical PageRank algorithm makes use of the 
similarity between sentences and ranks them by 
how similar a sentence is to all other sentences. It 
originates from the original PageRank algorithm 
(Page et al, 1998) that is based on the following 
two hypotheses:  
(1) A web page is important if it is linked by many 
other pages.  
(2) A web page is important if it is linked by im-
portant pages.  
The algorithm views the entire internet as a large 
graph in which a web page is a vertex and a di-
rected edge is connected according to the linkage. 
The salience of a vertex can be computed by a ran-
dom walk on the graph. Such graph-based methods 
have been widely adapted to such Natural Lan-
guage Processing (NLP) problems as text summa-
rization and word sense disambiguation. The 
advantage of such graph-based methods is obvious: 
the importance of a vertex is not only decided by 
itself, but also by its neighbors in a graph represen-
tation.  The random walk on a graph can imply 
more global dependence than other methods. Our 
PageRank scoring method consists of two steps: 
constructing the sentence graph and computing the 
salience score for each vertex of the graph.  
Let { |1 }iS s i N= ? ? be the sentence collec-
tion containing all the sentences to be summarized. 
According to the vector space model (Salton et al, 
1975), each sentence is  can be represented by a 
vector is
G
with each component being the weight of 
a term in is . The weight associated with a term w  
is calculated by ( )* ( )tf w isf w , where ( )tf w is the 
frequency of the term w in sentence is and ( )isf w
100
 is the inverse sentence frequency 4  of term w :
( ) 1 log( / )wisf w N n= + , where N is the total 
number of sentences in S  and wn is the number of 
sentences containing w .The similarity score be-
tween two sentences is computed using the inner 
product of the corresponding sentence vectors, as 
follows:  
( , )
|| || || ||
i j
i j
i j
s s
sim s s
s s
?
=
?
G G
G G  
Taking each sentence as a vertex, and the simi-
larity score as the weight of the edge between two 
sentences, a sentence graph is constructed. The 
graph is fully connected and undirected because 
the similarity score is symmetric.  
The sentence graph can be modeled by an adja-
cency matrix M , in which each element corres-
ponds to the weight of an edge in the graph. Thus
[ ]ij N NM ?=M is defined as:  
,
|| || || ||
0,
i j
i jij
s s
if i j
s sM
otherwise
??
??
?= ???
G G
G G
 
We normalize the row sum of matrix M  in or-
der to assure it is a stochastic matrix such that the 
PageRank iteration algorithm is applicable. The 
normalized matrix is: 
1 1
, 0
0,
N N
ij ij ij
j jij
M M if M
M
otherwise
= =
?
??
= ???
? ? . 
Using the normalized adjacency matrix, the sa-
lience score of a sentence is is computed in an 
iterative manner:  
1
(1 )
( ) ( )
N
i j ji
j
d
score s d score s M
N
=
?
= ? ? +?   
where d is a damping factor that is typically be-
tween 0.8 and 0.9 (Page et al, 1998).  
If we use a column vector p to denote the sa-
lience scores of all the sentences in S , the above 
equation can be written in a matrix form as follows:  
[ (1 ) ]Tp d d p= ? + ? ? ?M U  
                                                          
4 Isf is equivalent to idf if we view each sentence as a docu-
ment. 
where U is a square matrix with all elements being 
equal to 1/ N . The component (1 )d? ?U can be 
considered as a smoothing term which adds a small 
probability for a random walker to jump from the 
current vertex to any vertex in the graph. This 
guarantees that the stochastic transition matrix for 
iteration is irreducible and aperiodic. Therefore the 
iteration can converge to a stable state.  
In our implementation, the damping factor d is 
set to 0.85 as in the PageRank algorithm (Page et 
al., 1998). The column vector p is initialized with 
random values between 0 and 1. After the algo-
rithm converges, each component in the column 
vector p corresponds to the salience score of the 
corresponding sentence. This score is combined 
with the GO relevance score to rank sentences. 
3.3 GO Relevance Scoring 
Up to this point, our system considers only gene-
independent features, in both sentence filtering and 
PageRank-based sentence scoring. These features 
are universal across different genes. However, each 
gene is unique because of its own functional and 
structural properties. Thus we seek to include 
gene-specific features in this next step.  
The GO annotations provide one kind of gene-
specific information and have been shown to be 
useful for selecting GeneRIF candidates (Lu et al, 
2006). A gene?s GO annotations include descrip-
tions in three aspects: molecular function; biologi-
cal process; and cellular component. For example, 
the human gene AANAT (gene ID 15 in Entrez 
Gene) is annotated with the GO terms in Table 4. 
 
GO ID GO term 
GO:0004059 aralkylamine N-acetyltransferase activi-
ty 
GO:0007623 circadian rhythm 
GO:0008152 metabolic process 
GO:0008415 acyltransferase activity 
GO:0016740 transferase activity 
Table 4. GO terms for gene AANAT 
 
The GO relevance score is computed as follows: 
first, the GO terms and the sentences are both 
stemmed and stopwords are removed. For example, 
the GO terms in Table 4 are processed into a set of 
stemmed words: aralkylamin, N, acetyltransferas, 
activ, circadian, rhythm, metabol, process, acyl-
transferas and transferas.  
101
 Second, the total number of occurrence of the 
GO terms appearing in a sentence is counted. Fi-
nally, the GO relevance score is computed as the 
ratio of the total occurrence to the sentence length. 
The entire process can be illustrated by the follow-
ing pseudo codes: 
 
1 tokenize and stem the GO terms; 
2 tokenize and stem all the sentences, remove stop 
words; 
3 for each sentence is , 1,...,i N=  
( ) 0
i
GOScore s =  
for each word w  in is  
if w in the GO term set 
( )
i
GOScore s ++ 
end if 
end for 
( ) ( ) / ( )
i i i
GOScore s GOScore s length s=  
end for  
 
where ( )ilength s is the number of distinct non-stop 
words in is . For each sentence is , the GO relev-
ance score is combined with the PageRank score to 
get the overall score (? is a weight parameter be-
tween 0 and 1; see Section 4.2 for discussion): 
( ) ( ) (1 ) ( )i i iscore s PRScore s GOScore s? ?= ? + ? ? . 
3.4 Redundancy Removal  
A good summary contains as much diverse infor-
mation as possible for a gene, while with as little 
redundancy as possible. For many well-studied 
genes, there are thousands of relevant papers and 
much information is redundant. Hence it is neces-
sary to remove redundant sentences before produc-
ing a final summary.  
We adopt the diversity penalty method (Zhang 
et al, 2005; Wan and Xiao, 2007) for redundancy 
removal. The idea is to penalize the candidate sen-
tences according to their similarity to the ones al-
ready selected. The process is as follows:  
(1) Initialize two sets, A ?= ,
{ | 1, 2,..., }iB s i K= =  containing all the extracted 
sentences;  
(2)  Sort the sentences in B by their scores in des-
cending order;  
(3) Suppose is is the top ranked sentence in B , 
move it from B to A . Then we penalize the re-
maining sentences in B as follows: 
For each sentence js  in B , j i?  
( ) ( ) ( , ) ( )j j j i iScore s Score s sim s s Score s?= ? ? ?  
where 0? > is the penalty degree factor, 
( , )j isim s s  is the similarity between is and js .  
(4) Repeat steps 2 and 3 until enough sentences 
have been selected. 
4 Results and Discussion 
4.1 Evaluation Metrics 
Unlike the newswire summarization, there are no 
gold-standard test collections available for evaluat-
ing gene summarization systems. The two previous 
studies mentioned in Section 2 both conducted ex-
trinsic evaluations by asking human experts to rate 
system outputs. Although it is important to collect 
direct feedback from the users, involving human 
experts makes it difficult to compare different 
summarization systems and to conduct large-scale 
evaluations (both studies evaluated nothing but a 
small number of genes). In contrast, we evaluated 
our system intrinsically on a much larger dataset 
consisting of 7,294 human genes, each with a pre-
existing handwritten summary downloaded from 
the NCBI?s FTP site5.  
The handwritten summaries were used as refer-
ence summaries (i.e. a gold standard) to compare 
with the automatically generated summaries. Al-
though the length of reference summaries varies, 
the majority of these summaries contain 80 to 120 
words. To produce a summary of similar length, 
we decided to select five sentences consisting of 
about 100 words. 
For the intrinsic evaluation of a large number of 
summaries, we made use of the ROUGE metrics 
that has been widely used in automatic evaluation 
of summarization systems (Lin and Hovy, 2003; 
Hickl et al, 2007). It provides a set of evaluation 
metrics to measure the quality of a summary by 
counting overlapping units such as n-grams or 
word sequences between the generated summary 
and its reference summary.  
                                                          
5 ftp://ftp.ncbi.nih.gov/gene/DATA/ASN_BINARY/ 
102
 We computed three ROUGE measures for each 
summary, namely ROUGE-1 (unigram based), 
ROUGE-2 (bigram based) and ROUGE-SU4 
(skip-bigram and unigram) (Lin and Hovy, 2003). 
Among them, ROUGE-1 has been shown to agree 
most with human judgments (Lin and Hovy, 2003). 
However, as biomedical concepts usually contain 
more than one word (e.g. transcription factor), 
ROUGE-2 and ROUGE-SU4 scores are also im-
portant for assessing gene summaries.  
4.2 Determining parameters for best perfor-
mance 
The two important parameters in our system ? the 
linear coefficient ? for the combination of Page-
Rank and GO scores and the diversity penalty de-
gree factor ? in redundancy removal ? are 
investigated in detail on a collection of 100 ran-
domly selected genes. First, by setting ? to values 
from 0 to 1 with an increment of 0.1 while holding 
?  steady at 0.7, we observed the highest ROUGE-
1score when ? was 0.8 (Figure 2). This suggests 
that the two scores (i.e. PageRank and GO score) 
complement to each other and that the PageRank 
score plays a more dominating role in the summed 
score. Next, we varied? gradually from 0 to 5 with 
an increment of 0.25 while holding ? steady at 
0.75.The highest ROUGE-1 score was achieved 
when? was 1.3 (Figure 3). For ROURE-2, the best 
performance was obtained when ? was 0.7 and ?
was 0.5. In order to balance ROUGE-1 and 
ROUGE-2 scores, we set ? to 0.75 and ? to 0.7 
for the remaining experiments.  
 
Figure 2. The blue line represents the changes in 
ROUGE-1 scores with different values of ? while ? is 
held at 0.7. 
 
Figure 3. The blue line represents the changes in 
ROUGE-1 scores with different values of ? while ? is 
held at 0.75. 
4.3 Comparison with other methods 
Because there are no publicly available gene sum-
marization systems, we compared our system with 
two baseline methods. The first is a well known 
publicly available summarizer - MEAD (Radev et 
al., 2004). We adopted the latest version of MEAD 
3.11 and used the default setting in MEAD that 
extracts sentences according to three features: cen-
troid, position and length. The second baseline ex-
tracts different sentences randomly from abstracts. 
Comparison results are shown in the following ta-
ble:  
 
System ROUGE-1 ROUGE-2 ROUGE-SU4
Our System 0.4725 0.1247 0.1828 
MEAD 0.3890 0.0961 0.1449 
Random 0.3434 0.0577 0.1091 
Table 5. Systems comparison on 7,294 genes. 
 
As shown in Table 5, our system significantly 
outperformed the two baseline systems in all three 
ROUGE measures. Furthermore, larger perfor-
mance gains are observed in ROUGE-2 and 
ROUGE-SU4 than in ROUGE-1. This is because 
many background words (e.g. gene, protein and 
enzyme) also appeared frequently as unigrams in 
randomly selected summaries. 
 
103
  
Figure 4. ROUGE-1 score distribution 
 
In Figure 4, we show that the majority of the 
summaries have a ROUGE-1 score greater than 0.4. 
Our further analysis revealed that almost half 
summaries with a low score (smaller than 0.3) ei-
ther lacked sufficient relevant abstracts, or the ref-
erence summary was too short or too long. In 
either case, only few overlapping words can be 
found when comparing the generated gene sum-
mary with the reference. The statistics for low 
ROUGE-1 score are listed in Table 6. We also note 
that almost half of the summaries that have low 
ROUGE-1 scores were due to other causes: mostly, 
machine generated summaries differ from human 
summaries in that they describe different function-
al aspects of the same gene product. Take the gene 
TOP2A (ID: 7153) for example. While both sum-
maries (handwritten and machine generated) focus 
on its encoded protein DNA topoisomerase, the 
handwritten summary describes the chromosome 
location of the gene whereas our algorithm selects 
statements about its gene expression when treated 
with a chemotherapy agent. We plan to investigate 
such differences further in our future work. 
 
Causes for Low Score Number of 
genes 
Few (?10) related abstracts 106 
Short reference summary (< 40 words) 27 
Long reference summary (> 150 words) 76 
Other 198 
Total 407 
Table 6. Statistics for low ROUGE-1 scores (<0.3) 
4.4 Results on various summary length 
Figure 5 shows the variations of ROUGE scores as 
the summary length increases. At all lengths and 
for both ROUGE-1 and ROUGE-2 measures, our 
proposed method performed better than the two 
baseline methods. By investigating the scores of 
different summary lengths, it can be seen that the 
advantage of our method is greater when the sum-
mary is short. This is of great importance for a 
summarization system as ordinary users typically 
prefer short content for summaries.  
 
 
Figure 5. Score variation for different summary length 
 
5 Conclusions and Future Work 
In this paper we have presented a system for gene-
rating gene summaries by automatically finding 
extract-worthy sentences from the biomedical lite-
rature. By using the state-of-the-art summarization 
techniques and incorporating gene specific annota-
tions, our system is able to generate gene summa-
ries more accurately than the baseline methods. 
Note that we only evaluated our system for human 
genes in this work. More summaries are available 
for human genes than other organisms, but our me-
thod is organism-independent and can be applied 
to any other species. 
This research has implications for real-world 
applications such as assisting manual database cu-
ration or updating existing gene records. The 
ROUGE scores in our evaluation show comparable 
performance to those in the newswire summariza-
tion (Hickl et al, 2007). Nonetheless, there are 
further steps necessary before making our system 
output readily usable by human curators. For in-
stance, human curators are generally in favor of 
sentences presented in a coherent order. Thus, in-
formation-ordering algorithms in multi-document 
summarization need to be investigated. We also 
plan to study the guidelines and scope of the cura-
tion process, which may provide additional impor-
tant heuristics to further refine our system output.  
Acknowledgments 
104
 The work is supported by NSFC project No. 
60803075, Chinese 973 Project No. 
2007CB311003. ZL is supported by the Intramural 
Program of the National Institutes of Health. The 
authors are grateful to W. John Wilbur and G. 
Craig Murray for their help on the early version of 
this manuscript.  
References 
W. A. Baumgartner, B. K. Cohen, L. M. Fox, G. Ac-
quaah-Mensah, L. Hunter. 2007. Manual Curation Is 
Not Sufficient for Annotation of Genomic Databases. 
Bioinformatics, Vol. 23, No. 13. (July 2007), pp. i41-
48. 
G. Bhalotia, P. I. Nakov, A. S. Schwartz and M. A. 
Hearst, BioText Team Report for the TREC 2003 
Genomics Track. In Proceedings of TREC 2003.  
B. Boguraev and C. Kennedy. 1997. Salience-based 
Content Characterization of Text Documents. In Pro-
ceedings of Workshop on Intelligent Scalable Text 
Summarization (ACL97/EACL97), pp. 2-9. 
J. Carbonell and J. Goldstein. 1998. The Use of MMR, 
Diversity-based Reranking for Reordering Docu-
ments and Producing Summaries. In ACM SIGIR, 
pages 335?336, August. 
H. P. Edmundson. 1969. New Methods in Automatic 
Extracting. Journal of the ACM (JACM) archive Vo-
lume 16,  Issue 2  (April 1969) Pages: 264 ? 285. 
G. Erkan and D. R. Radev. 2004. LexPageRank: Pres-
tige in Multi-Document Text Summarization. In Pro-
ceedings of 2004 Conference on Empirical Methods 
in Natural Language Processing (EMNLP 2004), 
Barcelona, Spain. 
S. Gupta, A.Nenkova and D.Jurafsky. 2007. Measuring 
Importance and Query Relevance in Topic-focused 
Multi-document Summarization. Proceedings of 
ACL 2007 short papers, Prague, Czech Republic. 
W. Hersh and R. T. Bhupatiraju. 2003. TREC Genomics 
track Overview. In Proceedings of TheTwelfth Text 
REtrieval Conference, 2003. 
A. Hickl, K. Roberts and F. Lacatusu. 2007. LCC's 
GISTexter at DUC 2007: Machine Reading for Up-
date Summarization. 
R. Jelier, M. Schuemie, C. Eijk, M. Weeber, E. Mulli-
gen, B. Schijvenaars, B. Mons, J. Kors. Searching for 
geneRIFs: Concept-based Query Expansion and 
Bayes Classification. In Proceedings of TREC 2003.  
C. Lin and E. Hovy. 2000. The Automated Acquisition 
of Topic Signatures for Text Summarization. In Pro-
ceedings of the COLING Conference. 
C. Lin and E. Hovy. 2003. Automatic Evaluation of 
Summaries Using N-gram Co-Occurrence Statistics. 
In HLT-NAACL, pages 71?78. 
X. Ling, J. Jiang, X. He, Q. Mei, C. Zhai and B. Schatz. 
2006. Automatically Generating Gene Summaries 
from Biomedical Literature. Proceedings of the Pa-
cific Symposium on Biocomputing 2006. 
X. Ling, J. Jiang, X. He, Q. Mei, C. Zhai and B. Schatz. 
2007. Generating Gene Summaries from Biomedical 
Literature: A Study of Semi-Structured Summariza-
tion. Information Processing and Management 43, 
2007, 1777-1791. 
Z. Lu, K. B. Cohen and L. Hunter. 2006. Finding Ge-
neRIFs via Gene Ontology Annotations. Pac Symp-
Biocomput. 2006:52-63. 
C. Manning and H. Sch?tze. 1999. Foundations of Sta-
tistical Natural Language Processing. Chapter 5, MIT 
Press. Cambridge, MA: May 1999. 
K. R. McKeown and D. R. Radev. 1995. Generating 
Summaries of Multiple News Articles. In Proceed-
ings, ACM Conference on Research and Develop-
ment in Information Retrieval SIGIR'95, pages 74?
82. 
R. Mihalcea and P. Tarau. TextRank: Bringing Order 
into Texts, in Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing 
(EMNLP 2004), Barcelona, Spain, July 2004. 
M. Newman. 2003. The Structure and Function of 
Complex Networks. SIAM Review 45.167?256 
(2003). 
L. Page, S. Brin, R. Motwani and T. Winograd. The 
PageRank Citation Ranking: Bringing Order to the 
Web. Technical report, Stanford University, Stanford, 
CA, 1998. 
D. R. Radev, H. Jing, M. Stys and D. Tam. 2004. Cen-
troid-based Summarization of Multiple Documents. 
Information Processing and Management, 40:919?
938. 
G. Salton, A. Wong, and C. S. Yang. 1975. A Vector 
Space Model for Automatic Indexing. Communica-
tions of the ACM, vol. 18, nr.11, pages 613?620. 
S. Teufel and M. Moens. 1997. Sentence Extraction as a 
Classification Task. Workshop ?Intelligent and scala-
ble Text summarization?, ACL/EACL 1997. 
X. Wan and J. Xiao. 2007. Towards a Unified Approach 
Based on Affinity Graph to Various Multi-document 
Summarizations. ECDL 2007: 297-308.  
J. Yang, A. M. Cohen, W. Hersh. Automatic Summari-
zation of Mouse Gene Information by Clustering and 
Sentence Extraction from MEDLINE Abstracts. 
AMIA 2007 Annual Meeting. Nov. 2007 Chicago, IL. 
J. Yang, A. M. Cohen, W. Hersh. 2008. Evaluation of a 
Gene Information Summarization System by Users 
During the Analysis Process of Microarray Datasets. 
In BMC Bioinformatics 2009 10(Suppl 2):S5. 
B. Zhang, H. Li, Y. Liu, L. Ji, W. Xi, W. Fan, Z. Chen, 
W. Ma. 2005. Improving Web Search Results Using 
Affinity Graph. The 28th Annual International ACM 
SIGIR Conference (SIGIR'2005), August 2005.  
105
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 116?124,
Beijing, August 2010
	
  		   	
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 653?661,
Beijing, August 2010
Structure-Aware Review Mining and Summarization
Fangtao Li1, Chao Han1, Minlie Huang1, Xiaoyan Zhu1,
Ying-Ju Xia2, Shu Zhang2 and Hao Yu2
1State Key Laboratory of Intelligent Technology and Systems?
1Tsinghua National Laboratory for Information Science and Technology?
1Department of Computer Science and Technology, Tsinghua University
2Fujitsu Research and Development Center
fangtao06@gmail.com; zxy_dcs@tsinghua.edu.cn
Abstract
In this paper, we focus on object feature 1
1 Introduction
based review summarization. Different from 
most of previous work with linguistic rules or 
statistical methods, we formulate the review
mining task as a joint structure tagging prob-
lem. We propose a new machine learning 
framework based on Conditional Random 
Fields (CRFs). It can employ rich features to 
jointly extract positive opinions, negative opi-
nions and object features for review sentences.
The linguistic structure can be naturally inte-
grated into model representation. Besides li-
near-chain structure, we also investigate con-
junction structure and syntactic tree structure
in this framework. Through extensive experi-
ments on movie review and product review 
data sets, we show that structure-aware mod-
els outperform many state-of-the-art ap-
proaches to review mining.
With the rapid expansion of e-commerce, people 
are more likely to express their opinions and 
hands-on experiences on products or services
they have purchased. These reviews are impor-
tant for both business organizations and personal 
costumers. Companies can decide on their strat-
egies for marketing and products improvement. 
Customers can make a better decision when pur-
1 Note that there are two meanings for word ?feature?. 
We use ?object feature? to represent the target entity,
which the opinion expressed on, and use ?feature? as
the input for machine learning methods.
chasing products or services. Unfortunately, 
reading through all customer reviews is difficult, 
especially for popular items, the number of re-
views can be up to hundreds or even thousands. 
Therefore, it is necessary to provide coherent 
and concise summaries for these reviews.
Figure 1. Feature based Review Summarization
Inspired by previous work (Hu and Liu, 2004; 
Jin and Ho, 2009), we aim to provide object fea-
ture based review summarization. Figure 1 
shows a summary example for movie ?Gone 
with the wind?. The object (movie) features, 
such as ?movie?, ?actor?, with their correspond-
ing positive opinions and negative opinions, are 
listed in a structured way. The opinions are 
ranked by their frequencies. This provides a con-
cise view for reviews. To accomplish this goal, 
we need to do three tasks:  1), extract all the ob-
ject features and opinions; 2), determine the sen-
timent polarities for opinions; 3), for each object 
feature, determine the relevant opinions, i.e. ob-
ject feature-opinion pairs.
For the first two tasks, most previous studies
employ linguistic rules or statistical methods (Hu 
and Liu, 2004; Popescu and Etzioni 2005). They 
mainly use unsupervised learning methods,
which lack an effective way to address infre-
quent object features and opinions. They are also
hard to incorporate rich overlapping features.
Gone With The Wind:
Movie:
     Positive: great, good, amazing, ? , breathtaking
     Negative: bad, boring, waste time, ? , mistake
Actor: 
     Positive: charming , brilliant , great, ? , smart 
     Negative: poor, fail, dirty, ? , lame
Music:
     Positive: great, beautiful, very good, ? , top
     Negative: annoying, noise, too long, ? , unnecessary 
    ? ?
653
Actually, there are many useful features, which 
have not been fully exploited for review mining.
Meanwhile, most of previous methods extract 
object features, opinions, and determine the po-
larities for opinions separately. In fact, the object 
features, positive opinions and negative opinions
correlate with each other. 
In this paper, we formulate the first two tasks,
i.e. object feature, opinion extraction and opi-
nion polarity detection, as a joint structure tag-
ging problem, and propose a new machine learn-
ing framework based on Conditional Random 
Fields (CRFs). For each sentence in reviews, we 
employ CRFs to jointly extract object features,
positive opinions and negative opinions, which 
appear in the review sentence. This framework
can naturally encode the linguistic structure. Be-
sides the neighbor context with linear-chain 
CRFs, we propose to use Skip-chain CRFs and 
Tree CRFs to utilize the conjunction structure
and syntactic tree structure. We also propose a
new unified model, Skip-Tree CRFs to integrate 
these structures. Here, ?structure-aware? refers 
to the output structure, which model the relation-
ship among output labels. This is significantly 
different from the previous input structure me-
thods, which consider the linguistic structure as 
heuristic rules (Ding and Liu, 2007) or input fea-
tures for classification (Wilson et al 2009). Our 
proposed framework has the following advan-
tages: First, it can employ rich features for re-
view mining. We will analyze the effect of fea-
tures for review mining in this framework.
Second, the framework can utilize the relation-
ship among object features, positive opinions 
and negative opinions. It jointly extracts these 
three types of expressions in a unified way.
Third, the linguistic structure information can be 
naturally integrated into model representation,
which provides more semantic dependency for 
output labels. Through extensive experiments on 
movie review and product review, we show our 
proposed framework is effective for review min-
ing.
The rest of this paper is organized as follows: 
In Section 2, we review related work. We de-
scribe our structure aware review mining me-
thods in Section 3. Section 4 demonstrates the 
process of summary generation. In Section 5, we 
present and discuss the experiment results. Sec-
tion 6 is the conclusion and future work.
2 Related Work
Object feature based review summary has been 
studied in several papers. Zhuang et al (2006) 
summarized movie reviews by extracting object 
feature keywords and opinion keywords. Object 
feature-opinion pairs were identified by using a 
dependency grammar graph. However, it used a
manually annotated list of keywords to recognize 
movie features and opinions, and thus the system 
capability is limited. Hu and Liu (2004) pro-
posed a statistical approach to capture object 
features using association rules. They only con-
sidered adjective as opinions, and the polarities 
of opinions are recognized with WordNet expan-
sion to manually selected opinion seeds. Popescu 
and Etzioni (2005) proposed a relaxation labe-
ling approach to utilize linguistic rules for opi-
nion polarity detection. However, most of these 
studies focus on unsupervised methods, which
are hard to integrate various features. Some stu-
dies (Breck et al 2007; Wilson et al 2009; Ko-
bayashi et al 2007) have used classification 
based methods to integrate various features. But 
these methods separately extract object features
and opinions, which ignore the correlation 
among output labels, i.e. object features and opi-
nions. Qiu et al (2009) exploit the relations of 
opinions and object features by adding some lin-
guistic rules. However, they didn?t care the opi-
nion polarity. Our framework can not only em-
ploy various features, but also exploit the corre-
lations among the three types of expressions, i.e.
object features, positive opinions, and negative 
opinions, in a unified framework. Recently, Jin 
and Ho (2009) propose to use Lexicalized HMM
for review mining. Lexicalized HMM is a va-
riant of HMM. It is a generative model, which is 
hard to integrate rich, overlapping features. It 
may encounter sparse data problem, especially 
when simultaneously integrating multiple fea-
tures. Our framework is based on Conditional 
Random Fields (CRFs). CRFs is a discriminative 
model, which can easily integrate various fea-
tures.
These are some studies on opinion mining with 
Conditional Random Fields. For example, with 
CRFs, Zhao et al(2008) and McDonald et al 
(2007) performed sentiment classification in sen-
tence and document level; Breck et al(2007) 
identified opinion expressions from newswire 
documents; Choi et al (2005) determined opi-
654
nion holders to opinions also from newswire da-
ta. None of previous work focuses on jointly ex-
tracting object features, positive opinions and 
negative opinions simultaneously from review 
data. More importantly, we also show how to 
encode the linguistic structure, such as conjunc-
tion structure and syntactic tree structure, into 
model representation in our framework. This is 
significantly different from most of previous 
studies, which consider the structure information 
as heuristic rules (Hu and Liu, 2004) or input 
features (Wilson et al 2009).
Recently, there are some studies on joint sen-
timent/topic extraction (Mei et al 2007; Titov 
and McDonald, 2008; Snyder and Barzilay, 
2007). These methods represent reviews as sev-
eral coarse-grained topics, which can be consi-
dered as clusters of object features. They are
hard to indentify the low-frequency object fea-
tures and opinions. While in this paper, we will 
extract all the present object features and corres-
ponding opinions with their polarities. Besides, 
the joint sentiment/topic methods are mainly
based on review document for topic extraction.
In our framework, we focus on sentence-level
review extraction.
3 Structure Aware Review Mining
3.1 Problem Definition
To produce review summaries, we need to first 
finish two tasks: identifying object features, opi-
nions, and determining the polarities for opi-
nions. In this paper, we formulate these two 
tasks as a joint structure tagging problem. We
first describe some related definitions:
Definition (Object Feature): is defined as whole 
target expression that the subjective expressions 
have been commented on. Object features can be 
products, services or their elements and proper-
ties, such as ?character?, ?movie?, ?director? for 
movie review, and ?battery?, ?battery life?,
?memory card? for product review.
Definition (Review Opinion): is defined as the 
whole subjective expression on object features.
For example, in sentence ?The camera is easy to 
use?, ?easy to use? is a review opinion. ?opinion? 
is used for short.
Definition (Opinion Polarity): is defined as the 
sentiment category for review opinion. In this 
paper, we consider two types of polarities: posi-
tive opinion and negative opinion. For example,
?easy to use? belongs to positive opinion.
For our review mining task, we need to 
represent three types of expressions: object fea-
tures, positive opinions, and negative opinions. 
These expressions may be words, or whole
phrases. We use BIO encoding for tag represen-
tation, where the non-opinion and neutral opi-
nion words are represented as ?O?. With Nega-
tion (N), which is only one word, such as ?not?,
?don?t?, as an independent tag, there are totally 8 
tags, as shown in Table 1. The following is an 
example to denote the tags:
The/O camera/FB comes/O with/O a/O piti-
ful/CB 32mb/FB compact/FI flash/FI card/FI ./O
FB Feature Beginning CB Negative Beginning
FI Feature Inside CI Negative Inside
PB Positive Beginning N Negation Word 
PI Positive Inside O Other 
Table 1. Basic Tag Set for Review Mining
3.2 Structure Aware Model
In this section, we describe how to encode dif-
ferent linguistic structure into model representa-
tion based on our CRFs framework.
3.2.1 Using Linear CRFs.
For each sentence in a review, our task is to ex-
tract all the object features, positive opinions and 
negative opinions. This task can be modeled as a 
classification problem. Traditional classification 
tools, e.g. Maximum Entropy model (Berger et 
al, 1996), can be employed, where each word or 
phrase will be treated as an instance. However, 
they independently consider each word or 
phrase, and ignore the dependency relationship 
among them.
Actually, the context information plays an im-
portant role for review mining. For example, 
given two continuous words with same part of 
speech, if the previous word is a positive opi-
nion, the next word is more likely a positive opi-
nion. Another example is that if the previous 
word is an adjective, and it is an opinion, the 
next noun word is more likely an object feature.
To this end, we formulate the review mining 
task as a joint structure tagging problem, and 
propose a general framework based on Condi-
tional Random Fields (CRFs) (Lafferty et al, 
2001) which are able to model the dependencies 
655
y1 yn-1y3y2 yn
x1 xn-1x3x2 xn
(a) Linear-chain  CRFs
y4
x1 xn-1x3x2 xnxn-2
?
x4
y1 yn-2y3 yn
y2 yn-1
(c) Tree-CRFs
y4
x1 xn-1x3x2 xnxn-2
?
x4
y1 yn-2y3 yn
y2 yn-1
(d) Skip-Tree CRFs
(b) Skip-chain  CRFs
Figure 2 CRFs models
between nodes. (See Section 3.2.5 for more 
about CRFs)
In this section, we propose to use linear-chain
CRFs to model the sequential dependencies be-
tween continuous words, as discussed above. It 
views each word in the sentence as a node, and 
adjacent nodes are connected by an edge. The 
graphical representation is shown in Figure 2(a).
Linear CRFs can make use of dependency rela-
tionship among adjacent words.
3.2.2 Leveraging Conjunction Structure
We observe that the conjunctions play important 
roles on review mining: If the words or phrases 
are connected by conjunction ?and?, they mostly 
belong to the same opinion polarity. If the words 
or phrases are connected by conjunction ?but?, 
they mostly belong to different opinion polarity,
as reported in (Hatzivassiloglou and McKeown,
1997; Ding and Liu, 2007). For example, ?This
phone has a very cool and useful feature ? the
speakerphone?, if we only detect ?cool?, it is 
hard to determine its opinion polarity. But if we 
see ?cool? is connected with ?useful? by con-
junction ?and?, we can easily acquire the polari-
ty of ?cool? as positive. This conjunction struc-
ture not only helps to determine the opinions, but 
also helps to recognize object features. For ex-
ample, ?I like the special effects and music in 
this movie?, with word ?music? and conjunction
?and?, we can easily detect that ?special effects? 
as an object feature.
To model the long distance dependency with 
conjunctions, we use Skip-chain CRFs model to 
detect object features and opinions. The graphi-
cal representation of a Skip-chain CRFs, given in 
Figure 2(b), consists of two types of edges: li-
near-edge (

to 

) and skip-edge (

to 

). 
The linear-edge is described as linear CRFs. The 
skip-edge is imported as follows:
We first identify the conjunctions in the re-
view sentence, with a collected conjunction set,
including ?and?, ?but?, ?or?, ?however?, ?al-
though? etc. For each conjunction, we extract its 
connected two text sequences. The nearest two 
words with same part of speech from the two 
text sequences are connected with the skip-edge. 
Here, we just consider the noun, adjective, and 
adverb. For example, in ?good pictures and 
beautiful music?, there are two skip-edges: one 
connects two adjective words ?good? and ?beau-
tiful?; the other connects two nouns ?pictures? 
and ?music?. We also employ the general senti-
ment lexicons, SentiWordNet (Esuli and Sebas-
tiani, 2006), to connect opinions. Two nearest 
opinion words, detected by sentiment lexicon,
from two sequences, will also be connected by 
skip-edge. If the nearest distance exceeds the 
threshold, this skip edge will be discarded. Here,
we consider the threshold as nine.
Skip-chain CRFs improve the performance of 
review mining, because it naturally encodes the 
conjunction structure into model representation 
with skip-edges.
3.2.3 Leveraging Syntactic Tree Structure
Besides the conjunction structure, the syntactic 
tree structure also helps for review mining. The
tree denotes the syntactic relationship among 
words. In a syntactic dependency representation, 
each node is a surface word. For example, the 
corresponding dependency tree (Klein and Man-
ning, 2003) for the sentence, ?I really like this 
long movie?, is shown in Figure 3.
y1 yn-1y3y2 yn
x1 xn-1x3x2 xn
656
like
longthis
really movieI
nsubj dobjadvmod
det amod
Figure 3. Syntactic Dependency Tree Representation
In linear-chain structure and skip-chain structure, 
?like? and ?movie? have no direct edge, but in 
syntactic tree, ?movie? is directly connected 
with ?like?, and their relationship ?dobj? is also 
included, which shows ?movie? is an objective 
of ?like?. It can provide deeper syntactic depen-
dencies for object features, positive opinions and 
negative opinions. Therefore, it is important to 
consider the syntactic structure in the review 
mining task. 
In this section, we propose to use Tree CRFs to
model the syntactic tree structure for review 
mining. The representation of a Tree CRFs is 
shown in Figure 2(c). The syntactic tree structure 
is encoded into our model representation. Each 
node is corresponding to a word in the depen-
dency tree. The edge is corresponding to depen-
dency tree edge. Tree CRFs can make use of de-
pendency relationship in syntactic tree structure
to boost the performance.
3.2.4 Integrating Conjunction Structure and 
Syntactic Tree Structure
Conjunction structure provides the semantic re-
lations correlated with conjunctions. Syntactic 
tree structure provides dependency relation in 
the syntactic tree. They represent different se-
mantic dependencies. It is interesting to consider 
these two dependencies in a unified model. We 
propose Skip-Tree CRFs, to combine these two 
structure information. The graphical representa-
tion of a Skip-Tree CRFs, given in Figure 2(d),
consists of two types of edges: tree edges and 
conjunction skip-edges. We hope to simulta-
neously model the dependency in conjunction 
structure and syntactic tree structure.
We also notice that there is a relationship 
?conj? in syntactic dependency tree. However, 
we find that it only connects two head words for 
a few coordinating conjunction, such as ?and", 
?or", ?but?. Our designed conjunction skip-edge
provides more information for joint structure 
tagging. We analyze more conjunctions to con-
nect not only two head words, but also the words 
with same part of speech. We also connect the 
words with sentiment lexicon. We will show that 
the skip-tree CRFs, which combine the two 
structures, is effective in the experiment section.
3.2.5 Conditional Random Fields
A CRFs is an undirected graphical model G of 
the conditional distribution (	|
). Y are the 
random variables over the labels of the nodes 
that are globally conditioned on X, which are the 
random variables of the observations. The condi-
tional probability is defined as: 
P
(
	 
|


)
=  
1
(
)
    



(, 	|, 
)
,
+   



(, 	|, 
)
Coling 2010: Poster Volume, pages 525?533,
Beijing, August 2010
A Comparative Study on Ranking and Selection Strategies for 
Multi-Document Summarization 
Feng Jin, Minlie Huang, Xiaoyan Zhu 
State Key Laboratory of Intelligent Technology and Systems 
Tsinghua National Laboratory for Information Science and Technology 
Dept. of Computer Science and Technology, Tsinghua University 
jinfengfeng@gmail.com,{aihuang,zxy-dcs}@tsinghua.edu.cn
Abstract 
This paper presents a comparative study 
on two key problems existing in extrac-
tive summarization: the ranking problem 
and the selection problem. To this end, 
we presented a systematic study of 
comparing different learning-to-rank al-
gorithms and comparing different selec-
tion strategies. This is the first work of 
providing systematic analysis on these 
problems. Experimental results on two 
benchmark datasets demonstrate three 
findings: (1) pairwise and listwise learn-
ing-to-rank algorithms outperform the 
baselines significantly; (2) there is no 
significant difference among the learn-
ing-to-rank algorithms; and (3) the in-
teger linear programming selection 
strategy generally outperformed Maxi-
mum Marginal Relevance and Diversity 
Penalty strategies. 
1 Introduction 
As the rapid development of the Internet, docu-
ment summarization has become an important 
task since document collections are growing 
larger and larger. Document summarization, 
which aims at producing a condensed version of 
the original document(s), helps users to acquire 
information that is both important and relevant 
to their information need.  So far, researchers 
have mainly focused on extractive methods 
which choose a set of salient textual units to 
form a summary.  Such textual units are typical-
ly sentences, sub-sentences (Gillick and Favre, 
2009), or excerpts (Sauper and Barzilay, 2009).  
Almost all extractive summarization methods 
face two key problems: the first problem is how 
to rank textual units, and the second one is how 
to select a subset of those ranked units. The 
ranking problem requires systems model the 
relevance of a textual unit to a topic or a query. 
In this paper, the ranking problem refers to ei-
ther sentence ranking or concept ranking. Con-
cepts can be unigrams, bigrams, semantic con-
tent units, etc., although in our experiment, only 
bigrams are used as concepts. The selection 
problem requires systems improve diversity or 
remove redundancy so that more relevant in-
formation can be covered by the summary as its 
length is limited. As our paper focuses on ex-
tractive summarization, the selection problem 
refers to selecting sentences. However, the se-
lection framework presented here is universal 
for selecting arbitrary textual units, as discussed 
in Section 4. 
There have been a variety of studies to ap-
proach the ranking problem. These include both 
unsupervised sentence ranking (Luhn, 1958; 
Radev and Jing, 2004, Erkan and Radev, 2004), 
and supervised methods (Ouyang et al, 2007; 
Shen et al, 2007; Li et al, 2009). Even given a 
list of ranked sentences, it is not trivial to select 
a subset of sentences to form a good summary 
which includes diverse information within a 
length limit. Three common selection strategies 
have been studied to address this problem: Max-
imum Marginal Relevance (MMR) (Carbonell 
and Goldstein, 1998), Diversity Penalty (DiP) 
(Wan, 2007), and integer linear programming 
(ILP) (McDonald, 2007; Gillick and Favre, 
2009). As different methods were often eva-
luated on different datasets, it is of great value 
to systematically compare ranking and selection 
strategies on the same dataset. However, to the 
best of our knowledge, there is still no work to 
compare different ranking strategies or compare 
different selection strategies.  
In this paper, we presented a comparative 
study on the ranking problem and the selection 
525
problem for extractive summarization. We 
compared three genres of learning-to-rank me-
thods for ranking sentences or concepts: SVR, a 
pointwise ranking algorithm; RankNet, a pair-
wise learning-to-rank algorithm; and ListNet, a 
listwise learning-to-rank algorithm. We adopted 
an ILP framework that is able to select sen-
tences based on sentence ranking or concept 
ranking. We compared it with other selection 
strategies such as MMR and Diversity Penalty. 
We conducted our comparative experiments on 
the TAC 2008 and TAC 2009 datasets, respec-
tively. Our contributions are two-fold: First, to 
the best of our knowledge, this is the first work 
of presenting systematic and in-depth analysis 
on comparing ranking strategies and comparing 
selection strategies. Second, this is the first 
work using pairwise and liswise learning-to-
rank algorithms to perform concept (word bi-
gram) ranking for extractive summarization.  
The rest of this paper is organized as follows. 
We introduce the related work in Section 2. In 
Section 3, we present three ranking algorithms, 
SVR, RankNet, and ListNet. We describe the 
sentence selection problem with an ILP frame-
work described in Section 4. We introduce fea-
tures in Section 5. Evaluation and experiments 
are presented in Section 6. Finally, we conclude 
this paper in Section 7. 
2 Related Work 
A number of extractive summarization studies 
used unsupervised methods with surface fea-
tures, linguistic features, and statistical features 
to guide sentence ranking (Edmundson, 1969; 
McKeown and Radev, 1995; Radev et al, 2004; 
Nekova et al, 2006). Recently, graph-based 
ranking methods have been proposed for sen-
tence ranking and scoring, such as LexRank 
(Erkan and Radev, 2004) and TextRank (Mihal-
cea and Tarau, 2004).  
There are also a variety of studies on su-
pervised learning methods for sentence ranking 
and selection. Kupiec et al (1995) developed a 
naive Bayes classifier to decide whether a sen-
tence is worthy to extract. Recently, Conditional 
Random Field (CRF) and Structural SVM have 
been employed for single document summariza-
tion (Shen et al, 2007; Li et al, 2009).  
Besides ranking sentences directly, there are 
some approaches that select sentences based on 
concept ranking. Radev et al (2004) used cen-
troid words whose tf*idf scores are above a 
threshold. Filatova and Hatzivassiloglou (2004) 
used atomic event as concept. Moreover, sum-
marization evaluation metrics such as Basic 
Element (Hovy et al, 2006), ROUGE (Lin and 
Hovy, 2003) and Pyramid (Passonneau et al, 
2005) are all counting the concept overlap be-
tween generated summaries and human-written 
summaries.  
Another important issue existing in extractive 
summarization is to find an optimal sentence 
subset which can cover diverse information. 
Maximal Marginal Relevance (MMR) (Carbo-
nell and Goldstein, 1998) and Diversity Penalty 
(Wan, 2007) are most widely used approaches 
to reduce redundancy. The two methods are es-
sentially based on greedy search. By contrast, 
ILP based approaches view summary generation 
as a global optimization problem. McDonald 
(2007) proposed a sentence-level ILP solution. 
Sauper and Barzilay (2009) presented an ex-
cerpt-level ILP method to generate Wikipedia 
articles. Gillick and Favre (2009) proposed a 
concept-level ILP, but they used document fre-
quency to score concepts (bigrams), without any 
learning process. Some recent studies (Gillick 
and Favre, 2009; Martins and Smith, 2009) also 
modeled sentence selection and compression 
jointly using ILP. Our ILP framework proposed 
here is based on these studies. Although various 
selection strategies have been proposed, there is 
no work to systematically compare these strate-
gies yet. 
Learning to rank attracts much attention in 
the information retrieval community recently. 
Pointwise, pairwise and listwise learning-to-
rank approaches have been extensively studied 
(Liu, 2009). Some of those have been applied to 
document summarization, such as SVR 
(Ouyang et al, 2007), classification SVM 
(Wang et al, 2007), and RankNet (Svore et al, 
2007). Again, there is no work to systematically 
compare these ranking algorithms. To the best 
of our knowledge, this is the first time that a 
listwise learning-to-rank algorithm, ListNet 
(Cao et al, 2007), is adapted to document sum-
marization in this paper. Moreover, pairwise 
and listwise learning-to-rank algorithms have 
never been used to perform concept ranking for 
extractive summarization.  
526
3 Ranking Sentences or Concepts 
Given a query and a collection of relevant doc-
uments, an extractive summarization system is 
required to generate a summary consisting of a 
set of text units (usually sentences). The first 
problem we need to consider is to determine the 
importance of these sentences according to the 
input query. We approach this ranking problem 
in two ways: the first way is to score sentences 
directly using learning-to-rank algorithms, and 
thus the goal of summarization is to select a 
subset of sentences, considering both relevance 
and redundancy. The second way is to score 
concepts within the document collection, and 
then the summarization task is to select a sen-
tence subset that can cover those important con-
cepts maximally. The problem of sentence se-
lection will be described in Section 4.  
Suppose the relevant document collection for 
a query q is Dq. From this collection, we obtain 
a set of sentences or concepts (e.g., word bi-
grams), S={s1,s2,?,sn} or C={c1,c2,?, cn}. Be-
fore training, each si or ci is associated with a 
gold standard score, yi. A feature vector, xj= 
?(sj/cj,q,Dq), is constructed for each sentence or 
concept. The learning algorithm will learn a 
ranking function f(xj) from a collection of 
query-document pairs {(qi,Dqi)|i= 1, 2,?,m}.  
We investigated three learning-to-rank me-
thods to learn f(xj). The first one is a pointwise 
ranking algorithm, support vector regression 
(SVR). This algorithm treats sentences (or con-
cepts) independently. The second method is a 
pairwise ranking algorithm, RankNet, which 
learns a ranking function from a list of sentence 
(or concept) pairs. Each pair is labeled as 1 if 
the first sentence si (or concept ci) ranks ahead 
of the second sj (or cj), and 0 otherwise. 
The listwise ranking algorithm, ListNet, 
learns the ranking function f(xj) in a different 
way. A list of sentences (or concepts) is treated 
as a whole. Both RankNet and ListNet take into 
account the dependency between sentences (or 
concepts). 
3.1  Support Vector Regression  
Support Vector Regression (SVR), a generaliza-
tion of the classical SVM formulation, attempts 
to learn a regression model. SVR has been ap-
plied to summarization in (Ouyang et al, 2007; 
Metzler and Kanungo, 2008). In our work, we 
train the SVR model to fit the gold standard 
score of each sentence or concept.  
Formally, the objective of SVR is to minim-
ize the following objective: 
2
, ,
1 1
|| || ( ( ))
2
i
i i
x
w b
w C v L y f x
N
?
?
?( ) =
? ?? ?? ?
+ ? + ?? ?? ?? ?? ?? ??
(1) 
where L(x)=|x|-? if x > ? and otherwise L(x)=0; 
yi is the gold standard score of xi; f(x) =wTx+b, 
the predicted score of x; C and v are two para-
meters; and N is the total number of training 
examples.  
3.2 RankNet  
RankNet is a pairwise learning-to-rank method 
(Burges et al, 2005). In this algorithm, training 
examples are handled pair by pair. Given a pair 
of feature vectors (xi, xj), the gold standard 
probability ijP is set to be 1 if the label of the 
pair is 1, which means xi ranks ahead of xj. The 
gold standard probability is 0 if the label of the 
pair is 0. Then the predicted probability Pij, 
which defines the probability of xi ranking 
ahead of xj by the model, is represented as a lo-
gistic function:  
exp( ( ) ( ))
1 exp( ( ) ( ))
i j
ij
i j
f x f x
P
f x f x
?
=
+ ?
            (2) 
where f(x) is the ranking function. The objective 
of the algorithm is to minimize the cross entro-
py between the gold standard probability and 
the predicted probability, which is defined as 
follows: 
( ) log (1 ) log(1 )ij ij ij ij ijC f P P P P= ? ? ? ?     (3) 
A three-layer neural network is used as the 
ranking function, as follows:  
3 32 2 21 2 3( ) ( ( ) )n ij jk nk j i
j k
f x g w g w x b b= + +? ?
 
 (4) 
where for weights w and bias b, the superscripts 
indicate the node layer while the subscripts in-
dicate the node indexes within each layer. And 
xnk is the k-th component of input feature vector 
xn. Then a gradient descent method is used to 
learn the parameters. For details, refer to 
(Burges et al, 2005). 
3.3 ListNet 
ListNet takes a list of items as input in the learn-
ing process. More specifically, suppose we have 
527
a list of feature vectors (x1, x2,?, xn) and each 
feature vector xi has an gold standard score yi, 
which has been assigned before training. Ac-
cordingly, we have a list of gold standard scores 
(y1, y2,?,yn). We also have a list of scores as-
signed by the algorithm during training, say, 
(f(x1), f(x2),?, f(xn)). Given a score list 
S={s1,s2,?,sn}, the probability that xj will rank 
the first place among the n items is defined as 
follows: 
1 1
( ) exp( )
( )
( ) exp( )
j j
s n n
k kk k
s s
P j
s s
= =
?
= =
?? ?
        (5) 
It is easy to prove that (Ps(1), Ps(2), ?, Ps(n)) is 
a probability distribution, as the sum of them 
equals to 1. Therefore, the cross entropy can be 
used to define the loss between the gold stan-
dard distribution Py(j) and the distribution Pf(j), 
as follows:  
1
( , ) ( ) log ( )
n
y f
j
L y f P j P j
=
= ??               (6) 
where y represents the gold standard score list  
(y1, y2,?,yn) and f=(f(x1), f(x2),?, f(xn)) is the 
score list output by the ranking algorithm.  
The function f is defined as a linear function, 
as follows: 
( ) Tw i if x w x=                          (7) 
Then the gradient of loss function L(y,f) with 
respect to the parameter vector w can be calcu-
lated as follows:  
1
1
1
( )( , )
( )
( )1
exp( ( ))
exp( ( ))
n
w jw
y j
j
n
w j
w jn
jw jj
f xL y f
w P x
w w
f x
f x
wf x
=
=
=
??? = = ?
? ?
?
+
?
?
??
 (8) 
 
During training, w is updated in a gradient des-
cent manner: w=w -??w and ? is the learning 
rate. For details, refer to (Cao et al, 2007). 
4 ILP-based Selection Framework 
After we have a way of ranking sentences or 
concepts, we face a sentence selection problem: 
selecting an optimal subset of sentences as the 
final summary. To integrate sentence/concept 
ranking, we adopted an integer linear program-
ming (ILP) framework to find the optimal sen-
tence subset (Filatova and Hatzivassiloglou, 
2004; McDonald, 2007; Gillick and Favre, 2009; 
Takamura and Okumura, 2009). ILP is a global 
optimization problem whose objective and con-
straints are linear in a set of integer variables.  
Formally, we define the problem of sentence 
selection as follows: 
maximize:  ( )*  xi i
i
f x z
? ?? ?? ??          (9) 
. .     * | |       uj j
j
z u Lims t ??  
       * ( , ) ,          u xj i
j
z I i j z i? ??  
      ( )* ( , )   ,    x xi j i jz z sim x x i j?+ < ?  
        , {0,1},      ,x ui jz z i j? ?  
where: 
xi ? the representation unit, such as a sentence 
or a concept. We term it representation unit be-
cause the summary quality is represented by the 
set of included xi; 
f(xi) - the ranking function given by the learn-
ing-to-rank algorithms; 
uj - the selection unit, for instance, a sentence in 
this paper. |uj| is the number of words in uj; 
x
iz - the indicator variable which denotes the 
presence or absence of xi in the summary; 
u
jz - the indicator variable which denotes inclu-
sion or exclusion of uj; 
I(i, j) - a  binary constant indicating that wheth-
er xi appears in uj. It is either 1 or 0; 
Lim - the length limit; 
sim(xi, xj) - a similarity measure for considering 
the redundancy; 
? - the redundancy threshold.  
The first constraint indicates the length limit. 
The second constraint asserts that if a represen-
tation unit xi is included in a summary, at least 
one selection unit that contains xi must be se-
lected. The third constraint considers redundan-
cy. If the representation unit is sentence, the 
similarity measure is defined as tf*idf similarity, 
and ?/2 is the similarity threshold, which was 
set to be 1 here. For concepts, the similarity 
measure can be defined as  
1,    
( , )
0,    otherwise
i j
i j
x x
sim x x
=?
= ?? .
 
However, other definition is also feasible, de-
pending on what has been selected as represen-
tation unit. 
528
Note that this framework is very general. If 
the representation unit xi is a sentence, the rank-
ing function is defined on sentence. Thus the 
ILP framework will find a set of sentences that 
can optimize the total scores of selected sen-
tences, subject to several constraints. If the re-
presentation unit is a concept, the ranking func-
tion measures the importance of a concept to be 
included in a summary. Thus the goal of ILP is 
to find a set of sentences by maximizing the 
scores of concepts covered by those selected 
sentences. 
 
Dq relevant document collection in response 
to query q 
d one single document 
wi unigram 
wiwi+1 bigram 
S sentence 
tfd(wi) the frequency of wi occurring in d 
dfD(wi) the number of documents containing wi 
in collection D 
Table 1. Notations for features. 
5 Features 
To facilitate the following description, some 
notations are defined in Table 1. In our dataset, 
each query has a title and narrative to precisely 
define an information need. The following is a 
query example from the TAC 2008 test dataset:  
<topic id = "D0801A">  
 <title> Airbus A380 </title> 
 <narrative> 
Describe developments in the production and 
launch of the Airbus A380. 
 </narrative> 
</topic> 
Features for sentence ranking and concept 
ranking are listed in the following. We use word 
bigrams as concept here. 
Sentence Features 
(1) Cluster frequency: ( )
qi
D iw S
tf w
??  
(2) Title frequency: ( )
i
d iw S
tf w
??  where d is a 
new document that consists of all the titles of 
documents in Dq.  
(3) Query frequency: ( )
i
d iw S
tf w
??  where d is 
a document consisting of the title and narrative 
fields of the current topic.  
(4) Theme frequency: ( )
qi i
D iw S w T
tf w
? ? ??  
where T is the top 10% frequent unigram words 
in Dq. 
(5) Document frequency of bigrams in the sen-
tence: 
1
1( )
i i
D i iw w S
df w w
+
+?? .  
(6) PageRank score: as described in (Mihalcea 
and Tarau, 2004), each sentence in Dq is a node 
in the graph and the cosine similarity between a 
pair of sentences is used as edge weight. 
Concept Features 
(1) Cluster frequency: 1( )qD i itf w w + , the fre-
quency of 1i iw w + occurring in Dq.  
(2) Title frequency: 1( )d i itf w w + , where d is a 
document consisting of all the titles of docu-
ments in Dq. 
(3) Query Frequency: the frequency of the bi-
gram occurring in the topic title and narrative. 
(4) Average term frequency: 
 1( )/ | |
q
d i i qd D
tf w w D+?? . |Dq| is the number of 
documents in the set. 
(5) Document frequency: the document fre-
quency of this bigram. 
(6) Minimal position: the minimal position of 
this bigram relative to the document length.  
(7) Average position: the average position of 
this bigram in collection Dq . 
6 Experimental Results 
6.1 Data Preprocessing 
We conducted experiments on the TAC 2008 
and TAC 2009 datasets. The task requires pro-
ducing a 100-word summary for each query (al-
so termed topic sometimes). There are 48 que-
ries in TAC 2008 and 44 queries in TAC 2009. 
A query example has been given in Section 5. 
Relevant documents for these queries have been 
specified. And four human-written summaries 
were supplied as reference summaries for each 
query. 
We segmented the relevant documents into 
sentences using the LingPipe toolkit 1  and 
stemmed words using the Porter Stemmer. 
Word bigrams are used as concepts in this paper. 
If the two words in a bigram are both stop-
words, the bigram will be discarded. The sen-
                                                 
1 http://alias-i.com/lingpipe/index.html 
529
tence features and bigram features are then cal-
culated. As our focus is on comparing different 
ranking strategies and selection strategies, we 
did not apply any sophisticated linguistic or se-
mantic processing techniques (as pre- or post-
processing). Thus we did not compare our re-
sults to those submitted to the TAC conferences.  
We train the learning algorithms on one data-
set and then evaluate the algorithms on the other. 
The generated summaries are evaluated using 
the ROUGE toolkit (Lin and Hovy, 2003).  
6.2 Preparing Training Samples 
As our work includes both sentence ranking and 
concept ranking, we need to establish two types 
of training data. Fortunately, we are able to do 
this based on the reference summaries and an-
notation results provided by the TAC confe-
rences.  
For the sentence ranking problem, we com-
pute the average ROUGE-1 score for each sen-
tence by comparing it to the four reference 
summaries for each query. This score is treated 
as the gold-standard score. In ListNet, these 
scores are directly used (see formula (5)). While 
in RankNet, the sentences for a query are 
grouped into 10 bins according to their 
ROUGE-1 scores, and then we extract sentences 
from different bins respectively to form a pair. 
We assume that a sentence in a higher scored 
bin should rank ahead of those sentences in 
lower scored bins.  
As for the concept ranking problem, gold-
standard scores are obtained from the human 
annotated Pyramid data. The weight of each 
semantic content unit (SCU) is the number of 
reference summaries in which the SCU appears. 
So straightforwardly, the gold-standard score of 
a bigram is the largest weight of all SCUs that 
contain the bigram. And if a bigram does not 
occur in any SCU, its score will be 0. Thus the 
bigram scores belong to the set {0,1,2,3,4} as 
there are four human-written summaries for 
each query. These scores are directly used in 
ListNet (see formula (5)). And in RankNet, bi-
gram pairs are constructed according to the 
gold-standard scores.  
6.3 Learning Parameters 
For SVR, the radial basis kernel function is em-
ployed and the optimal values for parameters C, 
v and g (for the kernel) are found using the gri-
dregression.py tool provided by LibSVM 
(Chang and Lin, 2001) with a 5-fold cross vali-
dation on the training set.  
RankNet applies a three-layer (one hidden 
layer) neural network with only one node in the 
output layer, as described in (Burges et al, 
2005). The number of hidden neurons was em-
pirically set to be 10. The learning rate was set 
to 0.001 for sentence ranking and 0.01 for bi-
gram ranking.  
As for ListNet, the learning rate for sentence 
ranking and concept ranking are both set to be 
0.1 empirically.  
6.4 Comparing Ranking Strategies 
In this section, we compared different ranking 
strategies for both sentence ranking and concept 
ranking. The sentence selection strategies were 
fixed to the ILP selection framework as shown 
in Section 4. We chose ILP as the selection 
strategy because we want to compare our sys-
tem with the following two methods (as base-
lines): 
(1) SENT_ILP: A sentence-level method pro-
posed by McDonald (2007) with ILP formula-
tion. We implemented the query-focused ver-
sion of the formulae as TAC 2008 and 2009 
required query-focused summarization. 
(2) DF_ILP: A concept-level ILP method using 
document frequency to score word bigrams 
(Gillick and Favre, 2009), without any learning 
process.  
The differences between our framework and 
SENT_ILP are: a) SENT_ILP used a redundan-
cy factor in the objective function whereas we 
modeled redundancy as constraints; b) 
SENT_ILP used tf*idf similarity to compute 
relevance scores whereas we used learning algo-
rithms.  
The ROUGE-1 and ROUGE-2 measures for 
each method are presented in Table 2 and Table 
3. Note that the performance on the TAC 2008 
dataset was obtained from the models that were 
trained on the TAC 2009 dataset. Then, the da-
tasets were interchanged for training and testing, 
respectively. Different learning-to-rank strate-
gies (SVR, RankNet, ListNet) do not show sig-
nificant differences between one and another, 
but they all outperform SENT_ILP substantially 
(p-value < 0.0001). And for concept ranking, 
RankNet and ListNet both achieve significantly 
better ROUGE-2 results (p-value < 0.005) than 
530
DF_ILP. This infers that considering more fea-
tures will have better results than using docu-
ment frequency to score concepts. The Wilcox-
on signed-rank test (Wilcoxon, 1945) is used for 
significance tests in our experiment. A good 
ranking strategy for modeling relevance is im-
portant for extractive summarization. RankNet 
which used a three-layer network (non-linear 
function) as the ranking function performs 
slightly better than ListNet which is based on a 
linear ranking function.  
 
Dataset Method ROUGE-1 ROUGE-2
TAC 
2008 
SVR 0.35086 0.08447 
RankNet 0.36025 0.09291 
ListNet 0.35365 0.09129 
SENT_ILP 0.31546 0.06500 
TAC 
2009 
SVR 0.36125 0.09659 
RankNet 0.36216 0.09778 
ListNet 0.35480 0.09126 
SENT_ILP 0.31962 0.07034 
Table 2. Results of sentence ranking strategies. 
 
Dataset Method ROUGE-1 ROUGE-2
TAC 2008 
SVR 0.36555 0.10291 
RankNet 0.37564 0.11213 
ListNet 0.36863 0.10660 
DF_ILP 0.36922 0.10373 
TAC 2009 
SVR 0.37126 0.10698 
RankNet 0.37513 0.11364 
ListNet 0.37499 0.11313 
DF_ILP 0.36347 0.10156 
Table 3. Results of concept ranking strategies. 
 
It is worth noting that Pyramid annotations 
may not cover all important bigrams, partly be-
cause SCUs in reference summaries have been 
rephrased by human annotators. Note that we 
simply extract original sentences to form a 
summary, thus it is possible that a bigram which 
is important in the original sentences does not 
appear in any rephrased SCUs at all. Such bi-
grams will have a gold-standard score of 0, 
which is erroneous supervision. For example, 
the bigrams hurricane katrina in topic D0804A 
about Katrina pet rescue and life support in 
D0806A about Terri Schiavo case are not anno-
tated in any SCUs, but these bigrams are both 
key terms for the topics.  
6.5 Comparing Selection Strategies 
In order to study the influence of different selec-
tion strategies, we compare the ILP selection 
strategy (as introduced in Section 4) with other 
popular selection strategies, based on the same 
sentence ranking algorithm (we chose sentence-
level RankNet). The baselines to be compared 
are as follows:  
(1) MMR: As shown in (Carbonell and 
Goldstein, 1998), the formula of MMR is: 
{ }1 2arg max ( , ) (1 ) max ( , )
i j
i i js R S s S
MMR D q s D s s? ?
? ? ?
= ? ?
 
where q is the given query; R is the set of all 
sentences; S is the set of already included sen-
tences; D1 is the normalized ranking score f(xi) 
of si, and D2 is the cosine similarity of the fea-
ture vectors for si  and sj. Our implementation 
was similar to the MMR strategy in the 
MEAD2summarizer. 
(2) DiP: Diversity penalty which penalizes the 
score of candidate sentences according to the 
already selected ones (Wan, 2007). 
 
Dataset Method ROUGE-1 ROUGE-2
TAC 2008
ILP 0.36025 0.09291 
MMR 0.35459 0.09086 
DiP 0.35263 0.08689 
TAC 2009
ILP 0.36216 0.09778 
MMR 0.35148 0.08881 
DiP 0.34714 0.08672 
Table 4. Comparing selection strategies. 
 
The corresponding ROUGE scores are pre-
sented in Table 4. ILP outperforms other selec-
tion strategies significantly on the TAC 2009 
dataset (both ILP vs. MMR and ILP vs. DiP). 
Although improvements are observed with ILP 
on the TAC 2008 dataset, the difference is not 
significant (using ILP vs. using MMR). MMR is 
comparable to DiP as they are both based on 
greedy search in nature.  
To investigate the difference between these 
strategies, we present in-depth analysis here. 
First, the average length of summaries generat-
ed by ILP is 97.1, while that by MMR and DiP 
are 95.5 and 92.7, respectively. Note that the 
required summary length is 100 and that more 
words can potentially cover more information. 
Thus, ILP can generate summaries with more 
information. This is because ILP is a global op-
timization algorithm, subject to the length con-
straint. Second, the average rank of sentences 
selected by ILP is 12.6, while that by MMR and 
                                                 
2 http://www.summarization.com/mead/ 
531
DiP is about 5, which is substantially different. 
ILP can search down the ranked list while the 
other two methods tend to only select the very 
top sentences. Third, there are 4.1 sentences on 
average in each ILP-generated summary, while 
the number for MMR and DiP generated sum-
maries are 2.7 and 2.5, respectively. Thus ILP 
tend to select shorter sentences than MMR and 
DiP. This may help reduce redundancy as long-
er sentences may contain more topic irrelevant 
clauses or phrases.  
6.6 Discussions 
Interestingly, although the learning-to-rank al-
gorithms combined with the ILP selection strat-
egy perform well in summarization, the perfor-
mance is still far from that of manual summari-
zation. In this study, we investigate the upper 
bound performance. We used the presented ILP 
framework to generate summaries based on the 
gold-standard scores, rather than the scores giv-
en by the learning algorithms. In other words, 
f(xi) in formula (9) is replaced by the gold-
standard scores. The ROUGE results are shown 
in Table 5. We also listed the best/worst/average 
ROUGE scores of human summaries in TAC by 
comparing one human summary (as generated 
summary) to the other three human summaries 
(as reference summaries). These results are sub-
stantially better than those by the learning algo-
rithms. Sentence- and concept- level ranking 
produces very close results to best human sum-
maries. Some ROUGE-2 scores are even higher 
than those of human summaries. This is reason-
able as human annotators may have difficulty in 
organizing content when there are many docu-
ments and sentences. The results reflect that 
there is a remarkable gap between the gold-
standard scores and the learned scores.  
 
Dataset Method ROUGE-1 ROUGE-2
TAC 
2008 
Sentence-level 0.44216 0.14842 
Concept-level 0.42222 0.16018 
Human Best 0.44220 0.13079 
Human Average 0.41417 0.11606 
Human Worst 0.38005 0.10736 
TAC 
2009 
Sentence-level 0.45500 0.15565 
Concept-level 0.43526 0.17118 
Human Best 0.45663 0.14864 
Human Average 0.44443 0.12680 
Human Worst 0.39652 0.11109 
Table 5. Upper bound performance. 
7 Conclusion and Future Work 
We presented systematic and extensive analysis 
on studying two key problems in extractive 
summarization: the ranking problem and the 
selection problem. We compared three genres of 
learning-to-rank algorithms for the ranking 
problem, and investigated ILP, MMR, and Di-
versity Penalty strategies for the selection prob-
lem. To the best of our knowledge, this is the 
first work of presenting systematic comparison 
and analysis on studying these problems. We 
also at the first time proposed to use learning-to-
rank algorithms to perform concept ranking for 
extractive summarization.  
Our future work will focus on: (1) exploiting 
more features that can reflect summary quality; 
(2) optimizing summarization evaluation me-
trics directly with new learning algorithms. 
Acknowledgments 
This work was partly supported by the Chinese 
Natural Science Foundation under grant No. 
60973104 and No. 60803075, and with the aid 
of a grant from the International Development 
Research Center, Ottawa, Canada IRCI project 
from the International Development.  
References 
Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, 
Matt Deeds, Nicole Hamilton and Greg Hullender. 
2005. Learning to Rank Using Gradient Descent. 
In Proceedings of the 22nd International Confe-
rence on Machine Learning.  
Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai and 
Hang Li. 2007. Learning to Rank: from Pairwise 
Approach to Listwise Approach. In Proceedings 
of ICML 2007.  
Jaime Carbonell and Jade Goldstein. 1998. The Use 
of MMR, Diversity-Based Reranking for Reorder-
ing Documents and Producing Summaries. In 
Proceedings of SIGIR, August 1998, pp. 335 - 336.  
Chih-Chung Chang and Chih-Jen Lin. 2001. 
LIBSVM: a Library for Support Vector Machines. 
Software available at 
http://www.csie.ntu.edu.tw/~cjlin/libsvm 
H. P. Edmundson. 1969. New Methods in Automatic 
Extracting. Journal of the ACM (JACM) Archive,  
Volume 16, Issue 2 (April 1969) Pages: 264 - 285.  
G. Erkan and Dragomir R. Radev. 2004. LexPage-
Rank: Prestige in Multi-Document Text Summa-
532
rization. In Proceedings of EMNLP 2004, Barce-
lona, Spain.  
Elena Filatova and Vasileios Hatzivassiloglou. 2004. 
Event-based Extractive Summarization. In Pro-
ceedings of ACL Workshop on Summarization, 
volume 111. 
Dan Gillick and Benoit Favre. 2009. A Scalable 
Global Model for Summarization. In Proceedings 
of the Workshop on Integer Linear Programming 
for Natural Language Processing.  
Eduard Hovy, Chin-yew Lin, Liang Zhou and Juni-
chi Fukumoto. 2006. Automated Summarization 
Evaluation with Basic Elements. In Proceedings 
of the Fifth Conference on Language Resources 
and Evaluation.  
Julian Kupiec, Jan Pedersen and Francine Chen. 
1995. A Trainable Document Summarizer. In 
Proceedings of SIGIR'95, pages 68 - 73, New 
York, USA.  
Liangda Li, Ke Zhou, Gui-Rong Xue, Hongyuan Zha 
and Yong Yu. 2009. Enhancing Diversity, Cover-
age and Balance for Summarization through 
Structure Learning. In Proceedings of the 18th In-
ternational Conference on World Wide Web.  
Chin-Yew Lin and Eduard Hovy. 2003. Automatic 
Evaluation of Summaries Using N-gram Co-
Occurrence Statistics. In Proceedings of HLT-
NAACL, pages 71-78. 
Tie-Yan Liu. 2009. Learning to Rank for Informa-
tion Retrieval, Foundation and Trends on Infor-
mation Retrieval.  Now Publishers.  
H.P. Luhn. 1958. The Automatic Creation of Litera-
ture Abstracts. In IBM Journal of Research and 
Development, Vol. 2, No. 2, pp. 159-165, April 
1958. 
Andr? F. T. Martins and Noah A. Smith. 2009. 
Summarization with a Joint Model for Sentence 
Extraction and Compression. In Proceedings of 
the Workshop on Integer Linear Programming for 
Natural Langauge Processing\. 
Ryan McDonald. 2007. A Study of Global Inference 
Algorithms in Multi-Document Summarization. In 
Proceedings of the 29th ECIR.  
Kathleen McKeown and Dragomir R. Radev. 1995. 
Generating Summaries of Multiple News Articles. 
In Proceedings of SIGIR'95, pages 74?82.  
Donald Metzler and Tapas Kanungo. 2008. Machine 
Learned Sentence Selection Strategies for Query-
Biased Summarization. SIGIR Learning to Rank 
Workshop.  
Rada Mihalcea and Paul Tarau. 2004. TextRank: 
Bringing Order into Texts. In Proceedings of 
EMNLP 2004, Barcelona, Spain, July 2004.  
Ani Nenkova, Lucy Vanderwende and Kathleen 
McKeown. 2006. A Compositional Context Sensi-
tive Multi-document Summarizer: Exploring the 
Factors that Influence Summarization. In Pro-
ceedings of SIGIR 2006.  
You Ouyang, Sujian Li, Wenjie Li. 2007. Develop-
ing Learning Strategies for Topic-based Summa-
rization. In Proceedings of the sixteenth ACM 
Conference on Information and Knowledge Man-
agement, 2007. 
Rebecca J. Passonneau, Ani Nenkova, Kathleen 
McKeown and Sergey Sigelman. 2005. Applying 
the Pyramid Method in DUC 2005. DUC 2005 
Workshop.  
Dragomir R. Radev, Hongyan Jing, Malgorzata Stys, 
and Daniel Tam. 2004. Centroid-based Summari-
zation of Multiple Documents. Information 
Processing and Management, 40:919?938.  
Christina Sauper and Regina Barzilay. 2009. Auto-
matically Generating Wikipedia Articles: A Struc-
ture-Aware Approach. In Proceedings of ACL 
2009.  
Dou Shen, Jian-Tao Sun, Hua Li, QiangYang and 
Zheng Chen. 2007. Document Summarization Us-
ing Conditional Random Fields. In IJCAI, pages 
2862 - 2867, 2007.  
Krysta Svore, Lucy Vanderwende, and Chris Burges. 
2007. Enhancing Single-Document Summariza-
tion by Combining RankNet and Third-Party 
Sources. In Proceedings of EMNLP-CoNLL 
(2007), pp. 448-457.. 
Hiroya Takamura and Manabu Okumura. Text 
Summarization Model Based on Maximum Cov-
erage Problem and its Variant. In Proceedings 
EACL, 2009.  
Xiaojun Wan and Jianguo Xiao. 2007. Towards a 
Unified Approach Based on Affinity Graph to 
Various Multi-document Summarizations. ECDL 
2007, 297-308.  
Changhu Wang, Feng Jing, Lei Zhang and Hong-
Jiang Zhang. 2007. Learning Query-Biased Web 
Page Summarization. In Proceedings of the six-
teenth ACM Conference on Information and 
Knowledge Management.  
Frank Wilcoxon. 1945. Individual comparisons by 
ranking methods. Biometrics, 1, 80-83. 
533
Coling 2010: Poster Volume, pages 766?774,
Beijing, August 2010
A Review Selection Approach for Accurate Feature Rating Estimation
Chong Long? Jie Zhang? Xiaoyan Zhu??
? State Key Laboratory on Intelligent Technology and Systems,
Tsinghua National Laboratory for Information Science and Technology,
Department of Computer Science, Tsinghua University
?School of Computer Engineering, Nanyang Technological University
?{Corresponding Author: zxy-dcs@tsinghua.edu.cn}
Abstract
In this paper, we propose a review se-
lection approach towards accurate esti-
mation of feature ratings for services on
participatory websites where users write
textual reviews for these services. Our
approach selects reviews that compre-
hensively talk about a feature of a service
by using information distance of the re-
views on the feature. The rating estima-
tion of the feature for these selected re-
views using machine learning techniques
provides more accurate results than that
for other reviews. The average of these
estimated feature ratings also better rep-
resents an accurate overall rating for the
feature of the service, which provides
useful feedback for other users to choose
their satisfactory services.
1 Introduction
Most of participatory websites such as Amazon
(amazon.com) do not collect from users feature1
ratings for services, simply because it may cost
users too much effort to provide detailed feature
ratings. Even for a very few websites that do col-
lect feature ratings such as a popular travel web-
site TripAdvisor (tripadvisor.com), a big portion
(approximately 43%) of users may still not pro-
vide them. However, feature ratings are useful
for users to make informed consumption deci-
sions especially in the case where users may be
interested more in some particular features of the
services. Machine learning techniques have been
proposed for sentiment classification (Pang et al,
2002; Mullen and Collier, 2004) based on anno-
tated samples from experts, but they have limited
1A feature broadly means an attribute or a function of a
service.
performance especially when estimating ratings
of a multi-point scale (Pang and Lee, 2005).
In this paper, we propose a novel review se-
lection approach for accurate feature rating es-
timation. More specifically, our approach se-
lects reviews written by the users who compre-
hensively talk about a certain feature of a ser-
vice - that are comprehensive on this feature, us-
ing information distance of reviews on the fea-
ture based on Kolmogorov complexity (Li and
Vita?nyi, 1997). This feature is obviously impor-
tant to the users. People tend to be more knowl-
edgable in the aspects they consider important.
These users therefore represent a subset of ex-
perts. Statistical analysis reveals that these ex-
pert users are more likely to agree on a common
rating for the feature of the service. The rating
estimation of the feature for these selected re-
views based on annotated samples from experts
using machine learning techniques is thus able to
provide more accurate results than that for other
reviews. This statistical evidence also allows us
to use the average of the estimated feature rat-
ings to better represent an overall opinion of ex-
perts for the feature of the service, which will
be particularly useful for assisting other users to
correctly make their consumption decisions.
We verify our approach and arguments based
on real data collected from the TripAdvisor web-
site. First, our approach is shown to be able to
effectively select reviews that comprehensively
talk about features of a service. We then adopt
the machine learning method proposed in (Pang
and Lee, 2005) and the Bayesian Network clas-
sifier (Russell and Norvig, 2002) for feature rat-
ing estimation. Our experimental results show
that the accuracy of estimating feature ratings
for these selected reviews is higher than that
for other reviews, for both the machine learning
766
methods. And, the average of these estimated
ratings is testified to closely represent the over-
all feature rating of the service. Our approach is
therefore verified to be a successful step towards
accurate feature rating estimation.
2 Related Work
Our work aims at estimating feature ratings of a
service based on its textual reviews. It is related
to sentiment classification. The task of sentiment
classification is to determine the semantic orien-
tations of words, sentences or documents. (Pang
et al, 2002) is the earliest work of automatic
sentiment classification at document level, using
several machine learning approaches with com-
mon textual features to classify movie reviews.
Mullen and Collier (Mullen and Collier, 2004)
integrated PMI values, Osgood semantic factors
and some syntactic relations into the features of
SVM. Pang and Lee (Pang and Lee, 2004) pro-
posed another machine learning method based
on subjectivity detection and minimum-cut in
graph. However, these approaches focus only on
binary classification of reviews.
In 2005, Pang and Lee extended their ear-
lier work in (Pang and Lee, 2004) to determine
a reviewer?s evaluation with respect to multi-
scales (Pang and Lee, 2005). The rating esti-
mation is viewed as multi-class sentiment cate-
gorization on documents. They used SVM re-
gression as the multi-class classifier, and also ap-
plied a meta-algorithm based on a metric label-
ing formulation of the problem, which alters a
given n-ary classifier?s output in an explicit at-
tempt to ensure that similar items receive sim-
ilar labels. They collected movie reviews from
a website named IMDB and tested the perfor-
mance of their classifier under both four-class
and five-class categorization. The five-class sen-
timent classification is adopted in the evaluation
of our method (see Section 5). The performance
of their approach is limited. One important rea-
son is that their method considers every review
when estimating a feature rating of a movie.
However, some reviews do not contain much of
the users? opinions about a certain feature sim-
ply because the users do not care much or are
not knowledgable about the feature. In our work,
we study the characteristics of reviews? feature
ratings. We investigate which reviews are more
useful for us to estimate feature ratings. From
some observations stated in the next section, we
will see that reviews written by different users
reflect their own preferred features of a service.
3 Accurate Feature Rating Estimation
Participatory websites allow users to write tex-
tual reviews to discuss features of services that
they have consumed. These reviews usually con-
tain words that strongly express the users? opin-
ions about the corresponding features. These
words contain important information for estimat-
ing a numerical rating for the feature. The es-
timated ratings can be used for assisting other
users when they need to choose which services
to consume. Machine learning techniques are
often used for training a learner based on an-
notated samples from experts and estimating a
rating for a feature discussed in a review. How-
ever, for a review that does not mention a fea-
ture or discusses it only in a limited sense, the
estimation accuracy is expected to be very low.
Besides, the opinion expressed by the user who
writes this kind of review is not representative
because this user obviously does not care much
about the feature. We believe that if we carefully
select reviews for estimating feature ratings, the
accuracy will be increased and the estimated rat-
ings will be more representative.
We then statistically analyze real data col-
lected from the TripAdvisor website. The results
reveal that users who comprehensively discuss
a feature of a service in their reviews are more
likely to agree on a common rating for this fea-
ture of the service. This phenomenon can also
be intuitively explained as follows. For the users
who comprehensively discuss about a feature,
the feature is obviously more important to them.
People tend to be more knowledgable in the as-
pects they consider important. These users there-
fore represent a subset of experts. Experts likely
provide more objective and representative feed-
back about the feature, and therefore the ratings
from them for the feature contain less noise and
767
are more similar.
Based on the above discussion that experts
tend to have similar opinions on a feature of a
service, a learner trained by a machine learning
technique based on annotated samples from ex-
perts should then be able to more accurately esti-
mate the feature ratings from reviews written by
other experts. Since the opinions of experts con-
verge, the average of the estimated feature rat-
ings also better represents an overall rating for
the feature of the service.
We propose a review selection approach us-
ing information distance of reviews on the fea-
ture based on Kolmogorov complexity, to select
reviews that comprehensively discuss a feature
of a service. We rank the reviews based on the
comprehensiveness on the feature. The top re-
views will be selected for the estimation of fea-
ture ratings. Also, the average of these estimated
feature ratings will be used for representing the
overall rating for the feature. Next, we will first
describe in detail how our approach selects com-
prehensive reviews on a given feature.
4 Our Review Selection Approach
Our review selection approach selects reviews
that comprehensively talk about a feature. Ac-
cording to this definition, a review?s comprehen-
siveness depends on the amount of information
discussed on a feature. We use Kolmogorov
complexity and information distance to measure
the amount of information. Kolmogorov com-
plexity was introduced almost half a century ago
by R. Solomonoff, A.N. Kolmogorov and G.
Chaitin, see (Li and Vita?nyi, 1997). It is now
widely accepted as an information theory for in-
dividual objects parallel to that of Shannon?s in-
formation theory which is defined on an ensem-
ble of objects.
4.1 Theory
Fix a universal Turing machine U . The Kol-
mogorov complexity (Li and Vita?nyi, 1997) of a
binary string x condition to another binary string
y, KU (x|y), is the length of the shortest (prefix-
free) program for U that outputs x with input y.
It can be shown that for different universal Tur-
ing machine U ?, for all x, y
KU (x|y) = KU ?(x|y) + C,
where the constant C depends only on U ?. Thus
KU (x|y) can be simply written as K(x|y). They
write K(x|?), where ? is the empty string, as
K(x). It has also been defined in (Bennett et
al., 1998) that the energy to convert between x
and y to be the smallest number of bits needed to
convert from x to y and vice versa. That is, with
respect to a universal Turing machine U , the cost
of conversion between x and y is:
E(x, y)=min{|p|: U(x, p)=y, U(y, p)=x}
(1)
It is clear that E(x, y) ? K(x|y) + K(y|x).
From this observation, the following theorem has
been proved in (Bennett et al, 1998):
Theorem 1 E(x, y) = max{K(x|y),K(y|x)}.
Thus, the max distance was defined in (Ben-
nett et al, 1998):
Dmax(x, y) = max{K(x|y),K(y|x)}. (2)
This distance is shown to satisfy the basic
distance requirements such as positivity, sym-
metricity, triangle inequality and is admissible.
Here for an object x, we can measure its in-
formation by Kolmogorov complexity K(x); for
two objects x and y, their shared information can
be measured by information distance D(x, y).
In (Long et al, 2008), the authors generalize
the theory of information distance to more than
two objects. Similar to Equation 1, given strings
x1, . . . , xn, they define the minimum amount of
thermodynamic energy needed to convert from
any xi to any xj as:
Em(x1, .., xn)=min{|p|:U(xi, p, j)=xj for all i,j}
Then it is proved in (Long et al, 2008) that:
Theorem 2 Modulo to an O(logn) additive fac-
tor,
min
i
K(x1 . . . xn|xi) ? Em(x1, . . . , xn)
Given n objects, the left-hand side of Equa-
tion 3 may be interpreted as the most compre-
hensive object that contains the most information
about all of the others.
768
4.2 Review Selection Method
Our review selection method is based on the in-
formation distance discussed in the previous sec-
tion. However, our problem is that neither the
Kolmogorov complexity K(?, ?) nor Dmax(?, ?)
is computable. Therefore, we find a way to ?ap-
proximate? these two measures. The most use-
ful information in a review article is the English
words that are related to the features. If we can
extract all of these related words from the review
articles, the size of the word set can be regarded
as a rough estimation of information content (or
Kolmogorov complexity) of the review articles.
In Section 5 we will see that this gives very good
practical results.
4.2.1 Outline
Our method is outlined in the following. First,
for each type of product or service (such as a ho-
tel), a small set of core feature words (such as
price and room) is generated through statistics.
Then, these feature words are used to generate
the expanded words. Third, a parser is used to
find the dependent words associated with the oc-
currence of the core feature words and expanded
words in a review. For each review-feature pair,
the union of the core feature words, expanded
words and dependent words in the review defines
the related word set of the review on the feature.
Lastly, information distance is used to select the
most comprehensive reviews on a feature.
4.2.2 Word Extraction
Feature words are the most direct and frequent
words describing a feature, for example, price,
room or service of a hotel. Given a feature, the
core feature words are the very few most com-
mon English words that are used to refer to that
feature. For example, both ?value? and ?price?
are used to refer to the same feature of a ho-
tel. In (Hu and Liu, 2004), the authors indicate
that when customers comment on product fea-
tures, the words they use converge. If we re-
move the feature words with frequency lower
than 1% of the total frequency of all feature
words, the remaining words, which are just core
feature words, can still cover more than 90%
occurrences. So firstly we extract those words
through statistics; then some of those with the
same meaning (such as ?value? and ?price?) are
grouped into one feature. They are just ?core fea-
ture words?.
Apart from core feature words, many other
less-frequently used words that are connected
to the feature also contribute to the information
content of the feature. For example, ?price? is
an important feature of a hotel, but the word
?price? is usually dropped from a sentence. In-
stead, words such as ?$?, ?dollars?, ?USD?, and
?CAD? are used. We use information distance
d(., .) based on Google to expand words (Cili-
brasi and Vita?nyi, 2007). Let ? be a feature and
A be the set of its core feature words. The dis-
tance between a word w and the feature ? is then
defined to be
d(w,?) = min
v?A
d(w, v)
A distance threshold is then used to determine
which words should be in the set of expanded
words for a given feature.
If a core feature word or an expanded word is
found in a sentence, the words which have gram-
matical dependent relationship with it are called
the dependent words (de Marneffe et al, 2006).
For example, in sentence ?It has a small, but
beautiful room?, the words ?small? and ?beauti-
ful? are both dependent words of the core feature
word ?room?. All these words also contribute to
the reviews and are important to determine the
reviewer?s attitude towards a feature.
The Stanford Parser (de Marneffe et al, 2006)
is used to parse each review. For review i and
feature j, the core feature words and expanded
words in the review are first computed. Then the
parsing result is examined to find all the depen-
dent words for the core feature words and ex-
panded words, all of which are called ?related
words?.
4.2.3 Computing Information Distance
If there are m reviews x1, x2, . . . , xm, n fea-
tures u1, u2, . . . , un, and the related word set Si
is defined to be the union of all the related words
that occur in the review xi. From the left-hand
side of Equation 3, the most comprehensive xi
769
on feature uk is such that
i = argmin
i
K(S1 . . . Sn|Si, uk). (3)
Let Si and Sj be two sets of words,
K(SiSj |uk) = K(Si ? Sj |uk),
K(Si|Sj , uk) = K(Si \ Sj |uk),
and
K(Si|uk)=
?
w
K(w|uk)?
?
w
(K(w, uk)?K(uk))
where w ? Si and w is in xi?s related word set on
feature uk. For each word w in a set S, the Kol-
mogorov complexity can be estimated through
coding theorem (Li and Vita?nyi, 1997):
K(w, uk)=? logP (w, uk), K(uk)=? logP (uk)
where P (w, uk) can be estimated by df(w, uk),
which is the document frequency of word w and
feature uk co-exist on the whole corpus. Sim-
ilarly, P (uk) can be estimated by feature uk?s
document frequency on the corpus. In the next
section, Equation 3 will be used to select reviews
that comprehensively talk about a feature.
5 Experimental Verification
In this section, we present a set of experimen-
tal results to support our work. Our experiments
are carried out using real data collected from the
travel website TripAdvisor. This website indexes
hotels from cities across the world. It collects
feedback from travelers. Feedback of each trav-
eler consists of a textual review written by the
traveler and numerical ratings (from 1, lowest,
to 5, highest) for different features of hotels (e.g.,
value, service, rooms).
Table 1: Summary of the Data Set
Location# Hotels# Feedback# Feedback with
feature rating
Boston 57 3949 2096
Sydney 47 1370 879
Vegas 40 5588 3144
We crawled this website to collect travelers?
feedback for hotels in three cities: Boston, Syd-
ney and Las Vegas. Note that during this crawl-
ing process, we carefully removed information
about travelers and hotels to protect their privacy.
For users? feedback, we recorded only the tex-
tual reviews and the numerical ratings on four
features: Value(V), Rooms(R), Service(S) and
Cleanliness(C). These features are rated by a sig-
nificant number of users. Table 1 summarizes
our data set. For each one of the cities, this table
contains information about the number of hotels,
the total amount of feedback and the amount of
feedback with feature ratings. In general, each
hotel has sufficient amount of feedback with fea-
ture ratings for us to evaluate our work.
Table 2: Comprehensive Reviews on Each Fea-
ture (Boston)
Top # V R S C
1 Y Y Y Y
2 Y Y Y Y
3 N Y Y N
4 Y Y Y N
5 Y Y Y Y
6 Y Y N Y
... ... ... ... ...
5.1 Evaluation of Review Selection
We first evaluate the performance of our re-
view selection approach using manually anno-
tated data. More specifically, in our data set,
for one city, 40 reviews (120 reviews in total)
are selected for manual annotation. The annota-
tor looks over each review and decides whether
the review is comprehensive on a given feature.
Comprehensive reviews on the feature are anno-
tated as ?Y?, and the reviews that are not com-
prehensive on this feature are annotated as ?N?.
For the review set of each city, the number of re-
views annotated as comprehensive is equal to or
less than 20% of the total number of the selected
reviews for this city (eight in this experiment).
Note that it is possible that one review can be
comprehensive on more than one features.
We then use our review selection approach
770
discussed in Section 4 to rank the reviews for ho-
tels in each city, according to their comprehen-
siveness on each feature. For example, the most
comprehensive review on the feature ?Value?,
which has the minimal information distance to
this feature (see Equation 3), is ranked No.1. Ta-
ble 2 shows the annotated reviews for Boston ho-
tels that are ranked on top six on each feature. It
can be obviously seen from the table that most
of these top reviews are labeled as comprehen-
sive reviews on respective features. Our com-
prehensive review selection approach generally
performs well.
Table 3: Performance of Comprehensive Review
Selection
City Feature Precision Recall F-Score
Boston V 0.833 0.714 0.769
R 1.000 0.875 0.933
S 0.857 1.000 0.923
C 0.833 1.000 0.909
Sydney V 0.667 1.000 0.800
R 0.600 0.857 0.706
S 0.667 0.857 0.750
C 0.750 1.000 0.857
Vegas V 0.778 1.000 0.875
R 0.727 1.000 0.842
S 0.714 0.714 0.714
C 0.667 0.800 0.727
To clearly present the performance of our
comprehensive review selection approach, we
use the measures of precision, recall and f-score.
The measure f-score is a single value that can
represent the result of our evaluation. It is the
harmonic mean of precision and recall. Suppose
there are n reviews in total. Let pjk (1 ? k ? n)
be the review ranked the kth comprehensive on
feature j. Define
zjk =
{
1 if pjk is labelled comprehensive on j;
0 otherwise.
The precision P , recall R, and f-score F of top k
comprehensive reviews on feature j are formal-
ized as follows
Pjk =
?k
l=1 zjl
k ,Rjk =
?k
l=1 zjl?N
l=1 zjl
,
Fjk =
2PjkRjk
Pjk +Rjk
For each ranked review set on feature j, the
maximum Fjk and its associated Pjk and Rjk are
listed in Table 3. From this table, it can be seen
that for the best f-scores, the precision and recall
values are mostly larger than 70%, that is, a great
part of reviews that are labeled as comprehensive
receive top rankings from our comprehensive re-
view selection approach. Our approach is thus
carefully verified to be able to accurately select
comprehensive reviews on any given feature.
5.2 Statistical Analysis
A group of users who comprehensively discuss
a certain feature are more likely to agree on a
common rating for that feature. In this experi-
ment, we use our review selection approach to
verify this argument.
Table 4: Deviation of Feature Ratings
City Feature 20% 50% All
V 0.884 (0.0003) 1.030 1.136
Boston R 0.940 (0.2248) 1.037 1.013
S 1.026 (0.0443) 1.130 1.144
C 0.798 (0.0093) 0.892 0.949
V 0.862 (0.0266) 1.009 1.054
Sydney R 0.788 (0.0497) 0.932 0.945
S 0.941 (0.0766) 1.162 1.116
C 0.651 (0.0037) 0.905 0.907
V 0.845 (0.0002) 1.236 1.291
Vegas R 1.105 (0.2111) 1.148 1.175
S 1.112 (0.0574) 1.286 1.269
C 0.936 (0.0264) 1.096 1.158
More specifically, for each city, hotels that re-
ceive no less than 10 reviews with feature ratings
are selected. We use our comprehensive review
selection approach to select top 20% and 50%
comprehensive reviews on each feature for ho-
tels in each city. We calculate the standard devi-
ation of their feature ratings, as well as that of all
feature ratings, for each hotel in a city. We then
average these standard deviations over the hotels
in the same city. The average values are listed
in Table 4. The feature ratings of comprehensive
reviews on the feature have smaller average stan-
771
dard deviations. Standard T-test is used to mea-
sure the significance of the results between top
20% comprehensive reviews and all reviews, city
by city and feature by feature. Their p-values are
shown in the braces, and they are significant at
the standard 0.05 significance threshold. It can
be seen from the table that although for some
items there does not seem to be a significant dif-
ference, the results are significant for the entire
data set.
Therefore, when these travelers write reviews
that are comprehensive on one feature, their rat-
ings for this feature tend to converge. This evi-
dence indicates that the estimation of ratings for
the feature from these comprehensive reviews
can provide better results, which will be con-
firmed in Section 5.3. These estimated feature
ratings can also be averaged to represent a spe-
cific opinion of these travelers on the feature,
which will be verified in Section 5.4.
5.3 Feature Rating Estimation
In this section, we carry out experiments to tes-
tify that the estimation of feature ratings for com-
prehensive reviews using our review selection
approach provides better performance than that
for all reviews. We adopt the approach of Pang
and Lee (Pang and Lee, 2005) described in Sec-
tion 2 for feature rating estimation. In short, they
applied a meta-algorithm, based on a metric la-
beling formulation of the problem to alter a given
n-ary SVM?s output in an explicit attempt. We
also adopt a Baysian Network classifier for fea-
ture rating estimation.
Similar to the method of Pang and Lee, we
build up a feature rating classification system to
estimate reviews? feature ratings. However, the
method of Pang and Lee focuses only on sin-
gle rating classification for a review and assumes
that every word of the review can contribute to
this single rating. While it comes to feature rat-
ing classification, the system has to decide which
terms or phrases in the review are talking about
this feature. We train a Naive Bayes classifier
to retrieve all the sentences related to a feature.
Then all the core feature words, expanded words
and dependent words are extracted to train a
SVM classifier and the Bayesian Network clas-
sifier for five-class classification (1 to 5). The
eight-fold cross-validation is used to train and
test the performance of feature rating estimation
on all the reviews and the top 20% comprehen-
sive reviews, respectively.
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 1.1
 1.2
Value Rooms Service Clean Average
Av
era
ge
 Di
ffe
ren
ce
Comprehensive ReviewsAll Reviews
Figure 1: Average Error of Feature Rating Esti-
mation for the Adopted Method of Pang and Lee
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 1.1
 1.2
Value Rooms Service Clean Average
Av
era
ge
 Di
ffe
ren
ce
Comprehensive ReviewsAll Reviews
Figure 2: Average Error of Feature Rating Esti-
mation for the Bayesian Network classifier
We formalize a performance measure as fol-
lows. Suppose there are n reviews in total. For a
test review i (1 ? i ? n), its real feature rating
(given by the review writer) is fi, and its predi-
cated feature rating (predicted by our classifica-
tion system) is gi. Both fi and gi are integers
between 1 and 5. The performance of the classi-
fication on all n reviews can be measured by the
average of the absolute difference (d) between
each fi and gi pair,
d =
?n
i=1 |fi ? gi|
n . (4)
The lower d is, the better performance the clas-
sifier can provide.
Figures 1 and 2 show the results for the perfor-
mance of feature rating estimation on all reviews
versus that on selected comprehensive reviews,
772
for the adopted approach of Pang and Lee and
the Baysian Network classifier respectively. It
can be seen that the average difference between
real feature ratings and estimated feature ratings
on each feature when using selected comprehen-
sive reviews is significantly lower than that when
using all reviews, for both the approaches. On
average, the performance of feature rating esti-
mation is improved by more than 12.5% using
our review selection approach. And, our review
selection approach is generally applicable to dif-
ferent classifiers.
5.4 Estimating Overall Feature Rating
Supported by the statistical evidence verified in
Section 5.2 that the users who write compre-
hensive reviews on one feature will more likely
agree on a common rating for this feature, we
can then use an average of the feature ratings for
top 20% comprehensive reviews to reflect a gen-
eral opinion of knowledgable/expert users. In
this section, we show directly the performance
of estimating an overall feature rating for a ho-
tel using ratings for the selected comprehensive
reviews, and compare it with that for all reviews.
Table 5: Performance of Estimating Overall Fea-
ture Rating for Comprehensive Reviews
City V R S C AVG
Boston 0.637 0.426 0.570 0.660 0.573
Sydney 0.273 0.729 0.567 0.680 0.562
Vegas 0.485 0.502 0.277 0.613 0.469
Average 0.465 0.552 0.471 0.651 0.535
Table 6: Performance of Estimating Overall Fea-
ture Rating for All reviews
City V R S C AVG
Boston 0.809 0.791 0.681 0.642 0.731
Sydney 0.433 0.886 0.588 0.593 0.625
Vegas 0.652 0.733 0.502 0.942 0.707
Average 0.631 0.803 0.590 0.726 0.688
Suppose there are m hotels. For each hotel
j, we first select the top 20% comprehensive re-
views on each feature using our review selection
approach. We average the real ratings of one fea-
ture provide by travelers for these reviews, de-
noted as f?j . We then estimate the feature rat-
ings for these comprehensive reviews using the
adopted machine learning method of Pang and
Lee. The average of these estimated ratings is
denoted as g?j . Similar to Equation 4, the av-
erage difference between all f?j and g?j pairs on
each feature for hotels in each city are calculated
and listed in Table 5. From this table, we can
see that the average difference between the es-
timated average feature rating and real average
feature rating is only about 0.53. Our review
selection approach produces fairly good perfor-
mance for estimating an overall feature rating for
a hotel. We then also calculate the average dif-
ference for all reviews. The results are listed in
Table 6. We can see that the average difference
is larger (about 0.69) in this case. The perfor-
mance of estimating an overall feature rating is
increased by nearly 23.2% through our review
selection approach.
6 Conclusion
In this paper, we presented a novel review selec-
tion approach to improve the accuracy of feature
rating estimation. We select reviews that com-
prehensively talk about a feature of one service,
using information distance of reviews on the fea-
ture based on Kolmogorov complexity. As eval-
uated using real data, the rating estimation for
the feature from these reviews provides more ac-
curate results than that for other reviews, inde-
pendent of which classifiers are used. The aver-
age of these estimated feature ratings also better
represents an accurate overall rating for the fea-
ture of the service.
In future work, we will further improve the ac-
curacy of estimating a general rating for a feature
of a service based on the selected comprehensive
reviews on this feature using our review selec-
tion approach. Comprehensive reviews may con-
tribute differently to the estimation of an overall
feature rating. In our next step, a more sophisti-
cated model will be developed to assign different
weights to these different reviews.
773
References
Bennett, C.H., P Gacs, M Li, P.M.B. Vita?nyi, and
W.H. Zurek. 1998. Information distance. IEEE
Transactions on Information Theory, 44(4):1407?
1423, July.
Cilibrasi, Rudi L. and Paul M.B. Vita?nyi. 2007.
The google similarity distance. IEEE Transactions
on Knowledge and Data Engineering, 19(3):370?
383, March.
de Marneffe, Marie Catherine, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses.
In The fifth international conference on Language
Resources and Evaluation (LREC), May.
Hu, Minqing and Bing Liu. 2004. Mining and sum-
marizing customer reviews. In 10th ACM Inter-
national Conference on Knowledge Discovery and
Data Mining, pages 168?177.
Li, M. and P. Vita?nyi. 1997. An Introduction
to Kolmogorov Complexity and its Applications.
Springer-Verlag.
Long, Chong, Xiaoyan Zhu, Ming Li, and Bin Ma.
2008. Information shared by many objects. In
ACM 17th Conference on Information and Knowl-
edge Management.
Mullen, Tony and Nigel Collier. 2004. Sentiment
analysis using support vector machines with di-
verse information sources. In Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 412?418, July.
Pang, Bo and Lillian Lee. 2004. A sentimental edu-
cation: Sentiment analysis using subjectivity sum-
marization based on minimum cuts. In Annual
Meeting of the Association of Computational Lin-
guistics (ACL), pages 271?278, July.
Pang, Bo and Lillian Lee. 2005. Seeing stars: Ex-
ploiting class relationships for sentiment catego-
rization with respect to rating scales. In Annual
Meeting of the Association of Computational Lin-
guistics (ACL), pages 115?124, June.
Pang, Bo, Lillian Lee, and Shivakumar
Vaithyanathan. 2002. Thumbs up? sentiment
classification using machine learning techniques.
In Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 79?86,
July.
Russell, S. and P. Norvig. 2002. Artificial Intel-
ligence: A Modern Approach. Second Edition,
Prentice Hall, Englewood Cliffs, New Jersey.
774
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1119?1128,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Function-based question classification for general QA
Fan Bu, Xingwei Zhu, Yu Hao and Xiaoyan Zhu
State Key Laboratory of Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Sci. and Tech., Tsinghua University
buf08@mails.tsinghua.edu.cn
etzhu192@hotmail.com
haoyu@mail.tsinghua.edu.cn
zxy-dcs@tsinghua.edu.cn
Abstract
In contrast with the booming increase of inter-
net data, state-of-art QA (question answering)
systems, otherwise, concerned data from spe-
cific domains or resources such as search en-
gine snippets, online forums and Wikipedia in
a somewhat isolated way. Users may welcome
a more general QA system for its capability
to answer questions of various sources, inte-
grated from existed specialized sub-QA en-
gines. In this framework, question classifica-
tion is the primary task.
However, the current paradigms of question
classification were focused on some speci-
fied type of questions, i.e. factoid questions,
which are inappropriate for the general QA.
In this paper, we propose a new question clas-
sification paradigm, which includes a ques-
tion taxonomy suitable to the general QA and
a question classifier based on MLN (Markov
logic network), where rule-based methods and
statistical methods are unified into a single
framework in a fuzzy discriminative learning
approach. Experiments show that our method
outperforms traditional question classification
approaches.
1 Introduction
During a long period of time, researches on question
answering are mainly focused on finding short and
concise answers from plain text for factoid questions
driven by annual trackes such as CLEF, TREC and
NTCIR. However, people usually ask more complex
questions in real world which cannot be handled by
these QA systems tailored to factoid questions.
During recent years, social collaborative applica-
tions begin to flourish, such asWikipedia, Facebook,
Yahoo! Answers and etc. A large amount of semi-
structured data, which has been accumulated from
these services, becomes new sources for question
answering. Previous researches show that different
sources are suitable for answering different ques-
tions. For example, the answers for factoid questions
can be extracted from webpages with high accuracy,
definition questions can be answered by correspond-
ing articles in wikipedia(Ye et al, 2009) while com-
munity question answering services provide com-
prehensive answers for complex questions(Jeon et
al., 2005). It will greatly enhance the overall per-
formance if we can classify questions into several
types, distribute each type of questions to suitable
sources and trigger corresponding strategy to sum-
marize returned answers.
Question classification (QC) in factoid QA is to
provide constraints on answer types that allows fur-
ther processing to pinpoint and verify the answer
(Li and Roth, 2004). Usually, questions are classi-
fied into a fine grained content-based taxonomy(e.g.
UIUC taxonomy (Li and Roth, 2002)). We can-
not use these taxonomies directly. To guide ques-
tion distribution and answer summarization, ques-
tions are classified according to their functions in-
stead of contents.
Motivated by related work on user goal classi-
fication(Broder, 2002; Rose and Levinson, 2004) ,
we propose a function-based question classification
category tailored to general QA. The category con-
tain six types, namely Fact, List, Reason, Solution,
Definition and Navigation. We will introduced this
1119
category in detail in Section 2.
To classify questions effectively, we unify rule-
based methods and statistical methods into a single
framework. Each question is splited into functional
words and content words. We generate strict pat-
terns from functional words and soft patterns from
content words. Each strict pattern is a regular ex-
pression while each soft pattern is a bi-gram clus-
ter. Given a question, we will evaluate its matching
degree to each patterns. The matching degree is ei-
ther 0 or 1 for strict pattern and between 0 and 1 for
soft pattern. Finally, Markov logic network (MLN)
(Richardson and Domingos, 2006) is used to com-
bine and evaluate all the patterns.
The classical MLN maximize the probability of
an assignment of truth values by evaluating the
weights of each formula. However, the real world
is full of uncertainty and is unnatural to be repre-
sented by a set of boolean values. In this paper,
we propose fuzzy discriminative weight learning of
Markov logic network. This method takes degrees
of confidence of each evidence predicates into ac-
count thus can model the matching degrees between
questions and soft patterns.
The remainder of this paper is organized as fol-
lows: In the next section we review related work
on question classification, query classification and
Markov logic network. Section 2 gives a detailed
introduction to our new taxonomy for general QA.
Section 4 introduces fuzzy discriminative weight
learning of MLN and our methodology to extract
strict and soft patterns. In Section 5 we compare our
method with previous methods on Chinese question
data from Baidu Zhidao and Sina iAsk. In the last
section we conclude this work.
Although we build patterns and do experiments
on Chinese questions, our method does not take ad-
vantage of the particularity of Chinese language and
thus can be easily implemented on other languages.
2 Related Work
Many question taxonomies have been proposed in
QA community. Lehnert (1977) developed the sys-
tem QUALM based on thirteen conceptual cate-
gories which are based on a theory of memory repre-
sentation. On the contrary, the taxonomy proposed
by Graesser et al (1992) has foundations both in the-
ory and in empirical research. Both of these tax-
onomies are for open-domain question answering.
With the booming of internet, researches on
question answering are becoming more practical.
Most taxonomies proposed are focused on factoid
questions, such as UIUC taxonomy (Li and Roth,
2002). UIUC taxonomy contains 6 coarse classes
(Abbreviation, Entity, Description, Human, Lo-
cation and Numeric Value) and 50 fine classes.
All coarse classes are factoid oriented except De-
scription. To classify questions effectively, Re-
searchers have proposed features of different levels,
such as lexical features, syntactic features (Nguyen
et al, 2007; Moschitti et al, 2007) and semantic fea-
tures (Moschitti et al, 2007; Li and Roth, 2004).
Zhang and Lee (2003) compared five machine learn-
ing methods and found SVM outperformed the oth-
ers.
In information retrieval community, researchers
have described frameworks for understanding goals
of user searches. Generally, web queries are classi-
fied into four types: Navigational, Informational,
Transactional (Broder, 2002) and Resource (Rose
and Levinson, 2004). Lee et al (2005) automatically
classify Navigational and Informational queries
based on past user-click behavior and anchor-link
distribution. Jansen and Booth (2010) investigate
the correspondence between three user intents and
eighteen topics. The result shows that user intents
distributed unevenly among different topics.
Inspired by Rose and Levinson (2004)?s work in
user goals classification, Liu et al (2008) describe
a three-layers cQA oriented question taxonomy and
use it to determine the expected best answer types
and summarize answers. Other than Navigational,
Informational and Transactional, the first layer
contains a new Social category which represents the
questions that do not intend to get an answer but to
elicit interaction with other people. Informational
contains two subcategories Constant and Dynamic.
Dynamic is further divided into Opinion, Context-
Dependent and Open.
Markov logic network (MLN) (Richardson and
Domingos, 2006) is a general model combining
first-order logic and probabilistic graphical models
in a single representation. Illustratively, MLN is a
first-order knowledge base with a weight attached
to each formula. The weights can be learnt ei-
1120
TYPE DESCRIPTION EXAMPLES
1. Fact People ask these questions for general facts.
The expected answer will be a short phrase.
Who is the president
of United States?
2. List People ask these questions for a list of an-
swers. Each answer will be a single phrase
or a phrase with explanations or comments.
List Nobel price
winners in 1990s.
Which movie star do
you like best?
3. Reason People ask these questions for opinions or ex-
planations. A good answer summary should
contain a variety of opinions or comprehen-
sive explanations. Sentence-level summariza-
tion can be employed.
Is it good to drink
milk while fasting?
What do you think of
Avatar?
4. Solution People ask these questions for problem shoot-
ing. The sentences in an answer usually have
logical order thus the summary task cannot be
performed on sentence level.
What should I do
during an earthquake?
How to make pizzas?
5. Definition People ask these questions for description of
concepts. Usually these information can be
found in Wikipedia. If the answer is a too
long, we should summarize it into a shorter
one.
Who is Lady Gaga?
What does the Matrix
tell about?
6. Navigation People ask these questions for finding web-
sites or resources. Sometimes the websites are
given by name and the resources are given di-
rectly.
Where can I download
the beta version of
StarCraft 2?
Table 1: Question Taxonomy for general QA
ther generatively (Richardson and Domingos, 2006)
or discriminatively (Singla and Domingos, 2005).
Huynh and Mooney (2008) applies ?
1
-norm regu-
larized MLE to select candidate formulas generated
by a first-order logic induction system and prevent
overfitting. MLN has been introduced to NLP and
IE tasks such as semantic parsing (Poon et al, 2009)
and entity relation extraction (Zhu et al, 2009).
3 A Question Taxonomy
We suggest a function-based taxonomy tailored to
general QA systems by two principles. First, ques-
tions can be distributed into suitable QA subsys-
tems according to their types. Second, we can
employ suitable answer summarization strategy for
each question type. The taxonomy is shown in Tab.
1.
At first glance, classifying questions onto this tax-
onomy seems a solved problem for English ques-
tions because of interrogative words. In most cases,
a question starting with ?Why? is for reason and
?How? is for solution. But it is not always the case
for other languages. From table 2 we can see two
questions in Chinese share same function word ??
??? but have different types.
In fact, even in English, only using interroga-
tive words is not enough for function-based ques-
tion classification. Sometimes the question content
is crucial. For example, for question ?Who is the
current president of U.S. ??, the answer is ?Barak
Obama? and the type is Fact. But for question ?Who
is Barak Obama??, it will be better if we return the
first paragraph from the corresponding Wiki article
instead of a short phrase ?current president of U.S.?.
Therefore the question type will be Definition.
Compared to Wendy Lehnert?s or Arthur
Graesser?s taxonomy, our taxonomy is more prac-
tical on providing useful information for question
1121
Question ?????????
How to cook Kung Pao Chicken?
Type Solution
Question ???????????
What do you think of Avatar?
Type Reason
Table 2: Two Chinese questions share same function
words but have different types
extraction and summarization. Compared to ours,
The UIUC taxonomy is too much focused on factoid
questions. Apart from Description, all coarse types
in UIUC can be mapped into Fact. The cQA
taxonomy proposed in Liu et al (2008) has similar
goal with ours. But it is hard to automatically
classify questions into that taxonomy, especially for
types Constant, Dynamic and Social. Actually the
author did not give implementation in the paper as
well. To examine reasonableness of our taxonomy,
we select and manually annotate 5800 frequent
asked questions from Baidu Zhidao (see Section
5.1). The distribution of six types is shown in Fig.
1. 98.5 percent of questions can be categorized
into our taxonomy. The proportion of each type is
between 7.5% and 23.8%.
The type Navigation was originally proposed in
IR community and did not cause too much concerns
in previous QA researches. But from Fig. 1 we
can see that navigational questions take a substan-
tial proportion in cQA data.
Moreover, we can further develop subtypes for
each type. For example, most categories in UIUC
Reason18.1%
Fact14.4%Solution19.7%Navigation14.8%
List23.8% Definition7.5% Other1.5%
Figure 1: Distribution of six types in Baidu Zhidao data
taxonomy can be regarded as refinement to Fact and
Navigation can be refined into Resource and Web-
site. We will not have further discussion on this is-
sue.
4 Methodology
Many efforts have been made to take advantage of
grammatical , semantic and lexical features in ques-
tion classification. Zhang and Lee (2003) proposed
a SVM based system which used tree kernel to in-
corporate syntactic features.
In this section, we propose a new question clas-
sification methodology which combines rule-based
methods and statistical methods by Markov logic
network. We do not use semantic and syntactic fea-
tures for two reasons. First, the questions posted on
online communities are casually written which can-
not be accurately parsed by NLP tools, especially for
Chinese. Second, the semantic and syntactic pars-
ing are time consuming thus unpractical to be used
in real systems.
We will briefly introduce MLN and fuzzy dis-
criminative learning in section 4.1. The construction
of strict patterns and soft patterns will be shown in
4.2 and 4.3. In section 4.4 we will give details on
MLN construction, inference and learning.
4.1 Markov Logic Network
A first-order knowledge base contains a set of for-
mulas constructed from logic operators and symbols
for predicates, constants, variables and functions.
An atomic formula or atom is a predicate symbol.
Formulas are recursively constructed from atomic
formulas using logical operators. The grounding
of a predicate (formula) is a replacement of all of
its arguments (variables) by constants. A possible
world is an assignment of truth values to all possible
groundings of all predicates.
In first-order KB, if a possible world violates
even one formula, it has zero probability. Markov
logic is a probabilistic extension and softens the hard
constraints by assigning a weight to each formula.
When a possible world violates one formula in the
KB, it is less probable. The higher the weight, the
greater the difference in log probability between a
world that satisfies the formula and a world does
not. Formally, Markov logic network is defined as
1122
follows:
Definition 1 (Richardson & Domingos 2004) A
Markov logic network L is a set of pairs (?
?
, ?
?
),
where ?
?
is a formula in first-order logic and ?
?
is a
real number. Together with a finite set of constants
C = {?
1
, ?
2
, ..., ?
???
}, it defines a Markov network
?
?,?
as follows:
1. ?
?,?
contains one binary node for each pos-
sible grounding of each predicate appearing in
L. The value of the node is 1 if the ground pred-
icate is true, and 0 otherwise.
2. ?
?,?
contains one feature for each possible
grounding of each formula ?
?
in L. The value
of this feature is 1 if the ground formula is true,
and 0 otherwise. The weight of the feature is
the ?
?
associated with ?
?
in L.
There is an edge between two nodes of ?
?,?
iff
the corresponding grounding predicates appear to-
gether in at least one grounding of one formula in
?. An MLN can be regarded as a template for con-
structing Markov networks. From Definition 1 and
the definition of Markov networks, the probability
distribution over possible worlds ? specified by the
ground Markov network ?
?,?
is given by
? (? = ?) =
1
?
exp
(
?
?
?=1
?
?
?
?
(?)
)
MLN weights can be learnt genera-
tively(Richardson and Domingos, 2006) or
discriminatively(Singla and Domingos, 2005). In
discriminative weight learning, ground atom set ?
is partitioned into a set of evidence atoms ? and
a set of query atoms ? . The goal is to correctly
predict the latter given the former. In this paper, we
propose fuzzy discriminative weight learning which
can take the prior confidence of each evidence atom
into account.
Formally, we denote the ground formula set by
? . Suppose each evidence atom ? is given with a
prior confidence ?
?
? [0, 1], we define a confidence
function ? : ? ? [0, 1] as follows. For each ground
atom ?, if ? ? ? then we have ?(?) = ?
?
, else
?(?) = 1. For each ground non-atomic formulas, ?
is defined on standard fuzzy operators, which are
?(??) = 1? ?(?)
?(?
1
? ?
2
) = min(?(?
1
), ?(?
2
))
?(?
1
? ?
2
) = max(?(?
1
), ?(?
2
))
We redefined the conditional likelihood of ?
given ? as
? (???) =
1
?
?
exp
?
?
?
???
?
?
?
?
?
(?, ?)
?
?
=
1
?
?
exp
?
?
?
???
?
?
?
?
?
?
(?, ?)
?
?
Where ?
?
is the set of ground formulas involving
query atoms, ?
?
is the set of formulas with at least
one grounding involving a query atom and ??
?
(?, ?)
is the sum of confidence of the groundings of the i th
formula involving query atoms. The gradient of the
conditional log-likelihood (CLL) is
?
??
?
log?
?
(???)
= ?
?
?
(?, ?)?
?
?
?
?
?
(?
?
??)?
?
?
(?, ?
?
)
= ?
?
?
(?, ?)? ?
?
[?
?
?
(?, ?)] (1)
By fuzzy discriminative learning we can incorpo-
rate evidences of different confidence levels into one
learning framework. Fuzzy discriminative learn-
ing will reduce to traditional discriminative learning
when all prior confidences equal to 1.
4.2 Strict Patterns
In our question classification task, we find function
words are much more discriminative and less sparse
than content words. Therefore, we extract strict pat-
terns from function words and soft patterns from
content words. The definition of content and func-
tion words may vary with languages. In this paper,
nouns, verbs, adjectives, adverbs, numerals and pro-
nouns are regarded as content words and the rest are
function words.
The outline of strict pattern extraction is shown
in Alg. 1. In line 3, we build template ??? by re-
moving punctuations and replacing each character
in each content word by a single dot. In line 4, we
generate patterns from the template as follows. First
we generate n-grams(n is between 2 and ? ) from
1123
Algorithm 1: Strict Pattern Extraction
Input: Question Set ? = {?
1
, ?
2
...?
?
},
Parameters ? and ?
Output: Pattern Set ?
Initialize Pattern Set ? ;1
for each Question ?
?
do2
String ???=ReplaceContentWords(?
?
,?.?);3
Pattern Set ?
?
?
=GeneratePatterns(???,? );4
for each Pattern ? in ?
?
?
do5
if ? in ? then6
UpdateTypeFreq(?,? );7
else8
Add ? to ? ;9
Merge similar patterns in ? ;10
Sort ? by Information Gain on type11
frequencies;
return top ? Patterns in ? ;12
??? during which each dot is treated as a character
of zero length. For coverage concern, if a gener-
ated n-gram ? is not start(end) with dot, we build
another n-gram ?? by adding a dot before(behind) ?
and add both ? and ?? into n-gram set. Then for each
n-gram, we replace each consecutive dot sequence
by ?.*? and the n-gram is transformed into a regular
expression. A example is shown in Tab. 3. Although
generated without exhaustively enumerating all pos-
sible word combinations, these regular expressions
can capture most long range dependencies between
function words.
Each pattern consists of a regular expression as
well as its frequency in each type of questions. Still
Question ???????????
Can I launch online banking services
on internet?
Template ?..??....?
Patterns .*?.*? .*?.*?.*
(?=4) .*??.* .*??.*?
.*??.*?.* .*?.*??.*
.*?.*??.*?.* ?.*?.*
?.*??.* ?.*??.*?
.*?.*?.*
Table 3: Strict patterns generated from a question
from Alg. 1, in line 5-9, if a pattern ? in question ?
?
with type ? is found in ? , we just update the fre-
quency of ? in ?, else ? is added to ? with only
freq. ? equals to 1. In line 10, we merge similar
patterns in ? . two patterns ?
1
and ?
2
are similar iff
?q?QmatchP(q,p1) ? matchP(q,p2), in which
matchP is defined in Section 4.4.
Since a large number of patterns are generated,
it is unpractical to evaluate all of them by Markov
logic network. We sort patterns by information gain
and only choose top? ?good? patterns in line 11-12
of Alg. 1. A ?good? pattern should be discriminative
and of wide coverage. The information gain IG of a
pattern ? is defined as
IG(?) = ? (?)
?
?
?=1
? (?
?
??) log? (?
?
??)+
? (?)
?
?
?=1
? (?
?
??) log? (?
?
??)?
?
?
?=1
? (?
?
) log? (?
?
)
in which ? is the number of question types, ? (?
?
) is
the probability of a question having type ?
?
, ? (?)(or
? (?)) is the probability of a question matching(or
not matching) pattern ?. ? (?
?
??)(or ? (?
?
??)) is
the probability of a question having type ?
?
given
the condition that the question matches(or does not
match) pattern ?. These probabilities can be approx-
imately calculated by type and pattern frequencies
on training data. From the definition we can see
that information gain is suitable for pattern selec-
tion. The more questions a pattern ? matches and
the more unevenly the matched questions distribute
among questions types, the higher IG(?) will be.
4.3 Soft Patterns
Apart from function words, content words are also
important in function-based question classification.
Content words usually contain topic information
which can be a good complement to function words.
Previous research on query classification(Jansen and
Booth, 2010) shows that user intents distribute un-
evenly among topics. Moreover, questions given by
users may be incomplete and contain not function
words. For these questions, we can only predict the
question types from topic information.
Compared with function words, content words
distribute much more sparsely among questions.
1124
When we represent topic information by content
words (or bi-grams), since the training set are small
and less frequent words (or bi-grams) are filtered
to prevent over-fitting, those features would be too
sparse to predict further unseen questions.
To solve this problem, we build soft patterns on
question set. Each question is represented by a
weighted vector of content bi-grams in which the
weight is bi-gram frequency. Cosine similarity is
used to compute the similarity between vectors.
Then we cluster question vectors using a simple
single-pass clustering algorithm(Frakes and Yates,
1992). That is, for each question, we compute its
similarity with each centroid of existing cluster. If
the similarity with nearest cluster is greater than
a minimum similarity threshold ?
1
, we assign this
question to that cluster, else a new cluster is created
for this question.
Each cluster is defined as a soft pattern. Unlike
strict patterns, a question can match a soft pattern
to some extent. In this paper, the degree of match-
ing is defined as the cosine similarity between ques-
tion and centroid of cluster. Soft patterns are flexible
and could alleviate the sparseness of content words.
Also, soft patterns can be pre-filtered by information
gain described in 4.2 if necessary.
4.4 Implementation
Currently, we model patterns into MLN as follows.
The main query predicate is Type(q,t), which
is true iff question q has type t. For strict pat-
terns, the evidence predicate MatchP(q,p) is true
iff question q is matched by strict pattern p. The
confidence of MatchP(q,p) is 1 for each pair of
(q,p). For soft patterns, the evidence predicate
MatchC(q,c) is true iff the similarity of question
q and the cluster c is greater than a minimum simi-
larity requirement ?
2
. If MatchC(q,c) is false, its
confidence is 1, else is the similarity between q and
c.
We represent the relationship between patterns
and types by a group of formulas below.
MatchP(q,+p)?Type(q,+t)
?
?
?
?=?
?Type(q,t?)
The ?+p, +t? notation signifies that the MLN con-
tains an instance of this formula for each (pattern,
type) pair. For the sake of efficacy, for each pattern-
type pair (p,t), if the proportion of type t in ques-
tions matching p is less than a minimum require-
ment ?, we remove corresponding formula from
MLN.
Similarly, we incorporate soft patterns by
MatchC(q,+c)?Type(q,+t)
?
?
?
?=?
?Type(q,t?)
Our weight learner use ?
1
-regularization (Huynh
and Mooney, 2008) to select formulas and prevent
overfitting. A good property of ?
1
-regularization is
its tendency to force parameters to exact zero by
strongly penalizing small terms (Lee et al, 2006).
After training, we can simply remove the formulas
with zero weights.
Formally, to learn weight for each formula, we
iteratively solve ?
1
-norm regularized optimization
problem:
? : ?
?
= argmax
?
log?
?
(???)? ????
1
where ?.?
1
is ?
1
-norm and parameter ? controls the
penalization of non-zero weights. We implement the
Orthant-Wise Limited-memory Quasi-Newton algo-
rithm(Andrew and Gao, 2007) to solve this opti-
mization.
Since we do not model relations among questions,
the derived markov network ?
?,?
can be broken up
into separated subgraphs by questions and the gradi-
ent of CLL(Eq. 1) can be computed locally on each
subgraph as
?
??
?
log?
?
(???)
=
?
?
(
?
?
?
(?
?
, ?
?
)??
?
[?
?
?
(?
?
, ?
?
)]
)
(2)
in which ?
?
and ?
?
are the evidence and query atoms
involving question ?. Eq. 2 can be computed fast
without approximation.
We initialize formula weights to the same posi-
tive value ?. Iteration started from uniform prior
can always converge to a better local maximum than
gaussian prior in our task.
5 Experiments
5.1 Data Preparation
To the best of our knowledge, there is not general
QA system(the system which can potentially answer
1125
all kinds of questions utilizing data from heteroge-
neous sources) released at present. Alteratively, we
test our methodology on cQA data based on obser-
vation that questions on cQA services are of var-
ious length, domain independent and wrote infor-
mally(even with grammar mistakes). General QA
systems will meet these challenges as well.
In our experiments, both training and test data
are from Chinese cQA services Baidu Zhidao and
Sina iAsk. To build training set, we randomly select
5800 frequent-asked questions from Baidu Zhidao.
A question is frequent-asked if it is lexically simi-
lar to at least five other questions. Then we ask 10
native-speakers to annotate these questions accord-
ing to question title and question description. If an
annotator cannot judge type from question title, he
can view the question description. If type can be
judged from the description, the question title will
be replaced by a sentence selected from it. If not,
this question will be labeled as Other.
Each question is annotated by two people. If a
question is labeled different types, another annotator
will judge it and make final decision. If this annota-
tor cannot judge the type, this question will also be
labeled as Other. As a result, disagreements show
up on eighteen percents of questions. After the third
annotator?s judgment, the distribution of each type
is shown in Fig. 1.
To examine the generalization capabilities, the
test data is composed of 700 questions randomly se-
lected from Baidu Zhidao and 700 questions from
Sina iAsk. The annotation process on test data is as
same as the one on training data.
5.2 Methods Compared and Results
We compare four methods listed as follows.
SVM with bi-grams. We extract bi-grams from
questions on training data as features. After filtering
the ones appearing only once, we collect 5700 bi-
grams. LIBSVM(Chang and Lin, 2001)is used as
the multi-class SVM classifier. All parameters are
adjusted to maximize the accuracy on test data. We
denote this method as ?SB?;
MLN with bi-grams. To compare MLN and
SVM, we treat bi-grams as strict patterns. If a ques-
tion contain a bi-gram, it matches the corresponding
pattern. We set ? = 0.01, ? = 0.3 and ? = 0.3.
As a result, 5700 bi-grams are represented by 10485
formulas. We denote this method as ?MB?;
MLNwith strict patterns and bi-grams. We ask
two native-speakers to write strict patterns for each
type. The pattern writers can view training data for
reference and write any Java-style regular expres-
sions. Then we carefully choose 50 most reliable
patterns. To overcome the low coverage, We also
use the method described in Sec. 4.2 to automati-
cally extract strict patterns from training set. We first
select top 3000 patterns by information gain, merge
these patterns with hand-crafted ones and combine
similar patterns. Then we represent these patterns
by formulas and learn the weight of each formula by
MLN. After removing the formula with low weights,
we finally retain 2462 patterns represented by 3879
formulas. To incorporate content information, we
extract bi-grams from questions with function words
removed and remove the ones with frequency lower
than two. With bi-grams added, we get 8173 formu-
las in total. All parameters here are the same as in
?MB?. We denote this method as ?MSB?;
MLN with strict patterns and soft patterns. To
incorporate content information, We cluster ques-
tions on training data with similarity threshold ?
1
=
0.4 and get 2588 clusters(soft patterns) which are
represented by 3491 formulas. We these soft pat-
terns with strict patterns extracted in ?MSB?, which
add up to 7370 formulas. We set ?
2
= 0.02 and the
other parameters as same as in ?MB?. We denote
this method as ?MSS?;
We separate test set into easy set and difficult set.
A question is classified into easy set iff it contains
function-words. As a result, the easy set contains
1253 questions. We measure the accuracy of these
four methods on easy data and the whole test data.
The results are shown in Tab 4. From the results we
can see that all methods perform better on easy ques-
tions and MLN outperforms SVM using same bi-
gram features. Although MSS is inferior to MSB on
F. num Easy data All data
SB NA 0.724 0.685
MB 10485 0.722 0.692
MSB 8173 0.754 0.714
MSS 7370 0.752 0.717
Table 4: Experimental results on Chinese cQA data
1126
F L S R D N
Prec. 0.63 0.65 0.83 0.76 0.69 0.55
Recall 0.55 0.74 0.86 0.76 0.44 0.58
F
1
0.59 0.69 0.84 0.76 0.54 0.56
Table 5: Precision, recall and F-score on each type
easy questions, it shows better overall performance
and uses less formulas.
We further investigate the performance on each
type. The precision, recall and F
1
-score of each type
by method MSS are shown in Tab. 5. From the re-
sults we can see that the performance on Solution
and Reason are significantly better than the others.
It is because the strict patterns for this two types are
simple and effective. A handful of patterns could
cover a wide range of questions with high precision.
It is difficult to distinguish Fact from List because
strict patterns for these two types are partly overlap
each other. Sometimes we need content information
to determine whether the answer is unique. Since
List appears more frequently than Fact on training
set, MLN tend to misclassify Fact toListwhich lead
to low recall of the former and low precision of the
latter. The recall of Definition is very low because
many definition questions on test set are short and
only consists of content words(e.g. a noun phrase).
This shortage could be remedied by building strict
patterns on POStagging sequence.
fraction lines, college entrance exam
???????????????...
Fact: 56.4% List: 33.3% Solu.: 5.5%
lose weight, summer, fast
???????????????...
Reas.: 53.8% Solu.: 42.3% List: 3.8%
TV series, interesting, recent
???????????????...
List: 84.0% Fact: 8.0% Navi.: 2.0%
converter, format, 3gp
??????3gp?mp4????...
Navi.: 75% List: 18.8% Solu.: 6.2%
Table 6: Selected soft patterns on training data
5.3 Case Study on Soft Patterns
To give an intuitive illustration of soft patterns, we
show some of them clustered on training data in Tab.
6. For each soft pattern, we list five most frequent
bi-grams and its distribution on each type(only top 3
frequent types are listed).
From the results we can see that soft patterns are
consistent with our ordinary intuitions. For exam-
ple, if user ask a questions about ?TV series?, he is
likely to ask for recommendation of recent TV series
and the question have a great chance to be List. If
user ask questions about ?lose weight?, he probably
ask something like ?How can I lose weight fast?? or
?Why my diet does not work?? . Thus the type is
likely to be Solution or Reason.
6 Conclusion and Future Work
We have proposed a new question taxonomy tai-
lored to general QA on heterogeneous sources.
This taxonomy provide indispensable information
for question distribution and answer summarization.
We build strict patterns and soft patterns to repre-
sent the information in function words and content
words. Also, fuzzy discriminative weight learning
is proposed for unifying strict and soft patterns into
Markov logic network.
Currently, we have not done anything fancy on the
structure of MLN. We just showed that under uni-
form prior and L1 regularization, the performance
of MLN is comparable to SVM. To give full play
to the advantages of MLN, future work will focus
on fast structure learning. Also, since questions on
online communities are classified into categories by
topic, we plan to perform joint question type infer-
ence on function-based taxonomy as well as topic-
based taxonomy by Markov logic. The model will
not only capture the relation between patterns and
types but also the relation between types in different
taxonomy.
Acknowledgment
This work was supported mainly by Canada?s IDRC
Research Chair in Information Technology program,
Project Number: 104519-006. It is also supported
by the Chinese Natural Science Foundation grant
No. 60973104.
1127
References
G. Andrew and J. Gao. 2008. Scalable training of L1-
regularized log-linear models. In Proc. of ICML 2007,
pp. 33-40.
A. Broder. 2002. A taxonomy of Web search. SIGIR
Forum, 36(2), 2002.
C.C. Chang and C.J. Lin. 2001. LIBSVM: a library
for support vector machines. Software available at
http://www.csie.ntu.edu.tw/ cjlin/libsvm.
W.B. Frakes and R. Baeza-Yates, editors. 1992. In-
formation Retrieval: Data Structures and Algorithms.
Prentice-Hall, 1992.
A.C. Graesser, N.K. Person and J.D. Huber. 1992. Mech-
anisms that generate questions. Questions and Infor-
mation Systems, pp. 167-187), Hillsdale, N.J.: Erl-
baum.
T.N. Huynh and R.J. Mooney. 2008. Discriminative
Structure and Parameter Learning for Markov Logic
Networks. In Proc. of ICML 2008, pp. 416-423.
B.J. Jansen and D. Booth. 2010. Classifying web queries
by topic and user intent. In Proc. of the 28th interna-
tional conference on human factors in computing sys-
tems, pp. 4285-4290.
J. Jeon, W.B. Croft and J.H. Lee. 2005. Finding similar
questions in large question and answer archives. In
Proc. of ACM CIKM 2005,pp. 76-83.
S. Lee, V. Ganapathi and D. Koller. 2005. Effi-
cient structure learning of Markov networks using ?
1
-
regularization.. Advances in Neural Information Pro-
cessing Systems 18.
U. Lee, Z. Liu and J. Cho. 2005. Automatic identification
of user goals in Web search. In Proc. of WWW 2005.
W. Lehnert. 1977. Human and computational question
answering. Cognitive Science, vol. 1, 1977, pp. 47-63.
X. Li and D. Roth. 2002. Learning question classifiers.
In Proc. of COLING 2002, pp. 556-562.
X. Li and D. Roth. 2004. Learning question classifiers:
the role of semantic information. Natural Language
Engineering.
Y. Liu, S. Li, Y. Cao, C.Y. Lin, D. Han and Y. Yu.
2008. Understanding and summarizing answers in
community-based question answering services. In
Proc. of COLING 2008.
A. Moschitti, S. Quarteroni, R. Basili and S. Manand-
har. 2007. Exploiting Syntactic and Shallow Semantic
Kernels for Question/Answer Classification. In Proc.
of ACL 2007.
M.L. Nguyen, T.T. Nguyen and A. Shimazu. 2007. Sub-
tree Mining for Question Classification Problem. In
Proc. of IJCAI 2007.
H. Poon and P. Domingos. 2009. Unsupervised semantic
parsing. In Proc. of EMNLP 2009, pp. 1-10
D.E. Rose and D. Levinson. 2004. Understanding user
goals in web search. In Proc. of WWW 2004.
M. Richardson and P. Domingos. 2006. Markov logic
networks. Machine Learning 62:107-136.
P. Singla and P. Domingos. 2005. Discriminative Train-
ing of Markov Logic Networks. In Proc. of AAAI
2005.
S. Ye, T.S. Chua and J. Lu. 2009. Summarizing Defini-
tion from Wikipedia. In Proc. of ACL 2009.
D. Zhang and W.S. Lee. 2003. Question classification
using support vector machines. In Proc. of ACM SI-
GIR 2003, pp. 26-32.
J. Zhu , Z. Nie, X. Liu, B. Zhang and J.R. Wen. 2009.
StatSnowball: a Statistical Approach to Extracting En-
tity Relationships. In Proc. of WWW 2009, pp. 101-
110
1128
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1614?1623,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Clustering Aspect-related Phrases by Leveraging Sentiment Distribution
Consistency
Li Zhao, Minlie Huang, Haiqiang Chen*, Junjun Cheng*, Xiaoyan Zhu
State Key Laboratory of Intelligent Technology and Systems
National Laboratory for Information Science and Technology
Dept. of Computer Science and Technology, Tsinghua University, Beijing, PR China
*China Information Technology Security Evaluation Center
zhaoli19881113@126.com aihuang@tsinghua.edu.cn
Abstract
Clustering aspect-related phrases in terms
of product?s property is a precursor pro-
cess to aspect-level sentiment analysis
which is a central task in sentiment analy-
sis. Most of existing methods for address-
ing this problem are context-based models
which assume that domain synonymous
phrases share similar co-occurrence con-
texts. In this paper, we explore a novel
idea, sentiment distribution consistency,
which states that different phrases (e.g.
?price?, ?money?, ?worth?, and ?cost?) of
the same aspect tend to have consistent
sentiment distribution. Through formal-
izing sentiment distribution consistency as
soft constraint, we propose a novel unsu-
pervised model in the framework of Poste-
rior Regularization (PR) to cluster aspect-
related phrases. Experiments demonstrate
that our approach outperforms baselines
remarkably.
1 Introduction
Aspect-level sentiment analysis has become a cen-
tral task in sentiment analysis because it can ag-
gregate various opinions according to a product?s
properties, and provide much detailed, complete,
and in-depth summaries of a large number of re-
views. Aspect finding and clustering, a precursor
process of aspect-level sentiment analysis, has at-
tracted more and more attentions (Mukherjee and
Liu, 2012; Chen et al., 2013; Zhai et al., 2011a;
Zhai et al., 2010).
Aspect finding and clustering has never been a
trivial task. People often use different words or
phrases to refer to the same product property (also
called product aspect or feature in the literature).
Some terms are lexically dissimilar while seman-
tically close, which makes the task more challeng-
ing. For example, ?price?, ?money? , ?worth? and
?cost? all refer to the aspect ?price? in reviews.
In order to present aspect-specific summaries of
opinions, we first of all, have to cluster different
aspect-related phrases. It is expensive and time-
consuming to manually group hundreds of aspect-
related phrases. In this paper, we assume that the
aspect phrases have been extracted in advance and
we keep focused on clustering domain synony-
mous aspect-related phrases.
Existing studies addressing this problem are
mainly based on the assumption that different
phrases of the same aspect should have similar co-
occurrence contexts. In addition to the traditional
assumption, we develop a new angle to address the
problem, which is based on sentiment distribution
consistency assumption that different phrases of
the same aspect should have consistent sentiment
distribution, which will be detailed soon later.
Figure 1: A semi-structured Review.
This new angle is inspired by this simple obser-
vation (as illustrated in Fig. 1): two phrases within
the same cluster are not likely to be simultaneously
placed in Pros and Cons of the same review. A
straightforward way to use this information is to
formulate cannot-link knowledge in clustering al-
gorithms (Chen et al., 2013; Zhai et al., 2011b).
However, we have a particularly different manner
to leverage the knowledge.
Due to the availability of large-scale semi-
structured customer reviews (as exemplified in
Fig. 1) that are supported by many web sites,
we can easily get the estimation of sentiment dis-
tribution for each aspect phrase by simply count-
ing how many times a phrase appears in Pros and
1614
Cons respectively. As illustrated in Fig. 2, we
can see that the estimated sentiment distribution
of a phrase is close to that of its aspect. The
above observation suggests the sentiment distri-
bution consistency assumption: different phrases
of the same aspect tend to have the same senti-
ment distribution, or to have statistically close
distributions. This assumption is also verified by
our data: for most (above 91.3%) phrase with rela-
tively reliable estimation (whose occurrence?50),
the KL-divergence between the sentiment distri-
bution of a phrase and that of its corresponding
aspect is less than 0.05.
Figure 2: The sentiment distribution of aspect
?battery? and its related-phrases on nokia 5130
with a large amount of reviews.
It is worth noting that, the sentiment distribution
of a phrase can be estimated accurately only when
we obtain a sufficient number of reviews. When
the number of reviews is limited, however, the es-
timated sentiment distribution for each phrase is
unreliable (as shown in Fig. 3). A key issue,
arisen here, is how to formulate this assumption in
a statistically robust manner. The proposed model
should be robust when only a limited number of
reviews are available.
Figure 3: The sentiment distribution of aspect
?battery? and its related-phrases on nokia 3110c
with a small mumber of reviews.
To deal with this issue, we model sentiment dis-
tribution consistency as soft constraint, integrated
into a probabilistic model that maximizes the data
likelihood. We design the constraint to work in
the following way: when we have sufficient ob-
servations, the constraint becomes tighter, which
plays a more important role in the learning pro-
cess; when we have limited observations, the con-
straint becomes very loose so that it will have less
effect on the model.
In this paper, we propose a novel unsupervised
model, Sentiment Distribution Consistency Reg-
ularized Multinomial Naive Bayes (SDC-MNB).
The context part is modeled by Multinomial Naive
Bayes in which aspect is treated as latent variable,
and Sentiment distribution consistency is encoded
as soft constraint within the framework of Poste-
rior Regularization (PR) (Graca et al., 2008). The
main contributions of this paper are summarized
as follows:
? We study the problem of clustering phrases
by integrating both context information
and sentiment distribution of aspect-related
phrases.
? We explore a novel concept, sentiment distri-
bution consistency(SDC), and model it as soft
constraint to guide the clustering process.
? Experiments show that our model outper-
forms the state-of-art approaches for aspect
clustering.
The rest of this paper is organized as follows.
We introduce the SDC-MNB model in Section 2.
We present experiment results in Section 3. In
Section 4, we survey related work. We summarize
the work in Section 5.
2 Sentiment Distribution Consistency
Regularized Multinomial Naive Bayes
In this section, we firstly introduce our assumption
sentiment distribution consistency formally and
show how to model the above assumption as soft
constraint , which we term SDC-constraint. Sec-
ondly, we show how to combine SDC-constraint
with the probabilistic context model. Finally, we
present the details for context and sentiment ex-
traction.
2.1 Sentiment Distribution Consistency
We define aspect as a set of phrases that refer to
the same property of a product and each phrase is
termed aspect-related phrase (or aspect phrase in
short). For example, the aspect ?battery? contains
aspect phrases such as ?battery?, ?battery life?,
?power?, and so on.
1615
F the aspect phrase set
f
j
the j
th
aspect phrase
y
j
the aspect for aspect phrase f
j
A the aspect set
a
i
the i
th
aspect
D the set of context documents
d
j
the context document of f
j
V the word vocabulary
w
t
the t
th
word in vocabulary V
w
d
j
,k
the k
th
word in d
j
N
tj
the number of times word w
t
occurs in d
j
P the product set
p
k
the k
th
product
u
ik
the sentiment distribution parameter
of aspect a
i
on p
k
s?
jk
the estimated sentiment distribution parameter
of phrase f
j
on p
k
n
jk
the occurrence times of aspect phrase f
j
on p
k
??
jk
the sample standard deviation
? the model parameters
p
?
(a
i
|d
j
) the posterior distribution of a
i
given d
j
q(y
j
= a
i
)
the projected posterior distribution
of a
i
given d
j
Table 1: Notations
Let us consider the sentiment distribution on a
certain aspect a
i
. In a large review dataset, as-
pect a
i
could receive many comments from differ-
ent reviewers. For each comment, we assume that
people either praise or complain about the aspect.
So each comment on the aspect can be seen as a
Bernoulli trial, where the aspect receives positive
comments with probability p
a
i
1
. We introduce a
random variable X
a
i
to denote the sentiment on
aspect a
i
, where X
a
i
= 1 means that aspect a
i
receives positive comments, X
a
i
= 0 means that
aspect a
i
receives negative comments. Obviously,
the sentiment on aspect a
i
follows the Bernoulli
distribution,
Pr(X
a
i
) = p
X
a
i
a
i
? (1 ? p
a
i
)
1?X
a
i
, X
a
i
? {0, 1}. (1)
Or in short,
X
a
i
? Bernoulli(p
a
i
)
Let us see the case for aspect phrase f
j
, where
f
j
? aspect a
i
. Similarly, each comment on an as-
pect phrase f
j
can also be seen as a Bernoulli trial.
We introduce a random variable X
f
j
to denote the
sentiment on aspect phrase f
j
, where X
f
j
= 1
means that aspect f
j
receives positive comments,
X
f
j
= 0 means that aspect f
j
receives negative
comments. As just discussed, we assume that each
aspect phrase follows the same distribution with
1
positive comment means that an aspect term is observed
in Pros of a review.
the corresponding aspect. This leads to the fol-
lowing formal description:
? Sentiment Distribution Consistency : The
sentiment distribution of aspect phrase is the
same as that of the corresponding aspect.
Formally, for all aspect phrase f
j
? aspect
a
i
, X
f
j
? Bernoulli(p
a
i
).
2.2 Sentiment Distribution Consistency
Constraint
Assuming the sentiment distribution of aspect a
i
is
given in advance, we need to judge whether an as-
pect phrase f
j
belongs to the aspect a
i
with limited
observations for f
j
. Let?s consider the example in
Fig. 4. For aspect phrase 3, we have no definite
answer due to the limited number of observations.
For aspect phrase 1, it seems that the sentiment
distribution is consistent with that of the left as-
pect. However, we can not say that the phrase be-
longs to the aspect because the distribution may
be the same for two different aspects. For aspect
phrase 2, we are confident that its sentiment dis-
tribution is different from that of the left aspect,
given sufficient observations.
Figure 4: Sentiment distribution of an aspect, and
observations on aspect phrases.
To be concise, we judge an aspect phrase
doesn?t belong to certain aspect only when we are
confident that they follow different sentiment dis-
tributions.
Inspired by the intuition, we conduct interval
parameter estimation for parameter p
f
j
(sentiment
distribution for phrase f
j
) with limited observa-
tions, and thus get a confidence interval for p
f
j
.
If p
a
i
(sentiment distribution for aspect a
i
) is not
in the confidence interval of p
f
j
, we then are con-
fident that they follow different distributions. In
other words, if aspect phrase f
j
? aspect a
i
, we
are confident that p
a
i
is in the confidence interval
of p
f
j
.
More formally, we use u
ik
to denote the senti-
ment distribution parameter of aspect a
i
on prod-
uct p
k
, and assume that u
ik
is given in advance.
1616
We want to know whether the sentiment distribu-
tion on aspect phrase f
j
is the same as that of as-
pect a
i
on product p
k
given a limited number of
observations (samples). It?s straightforward to cal-
culate the confidence interval for parameter s
jk
in
the Bernoulli distribution function. Let the sam-
ple mean of n
jk
samples be s?
jk
, and the sample
standard deviation be ??
jk
. Since the sample size
is small here, we use the Student-t distribution to
calculate the confidence interval. According to our
assumption, we are confident that u
ik
is in the con-
fidence interval if f
j
? a
i
.
s?
jk
?C
??
jk
?
n
jk
? u
ik
? s?
jk
+C
??
jk
?
n
jk
, ?f
j
? a
i
,?k. (2)
where we look for t-table to find C corresponding
to a certain confidence level(such as 95%) with the
freedom of n
jk
? 1. For simplicity, we represent
the above confidence interval by [s?
jk
? d
jk
, s?
jk
+
d
jk
], where d
jk
= C
??
jk
?
n
jk
.
We introduce an indicator variable z
ij
to repre-
sent whether the aspect phrase f
j
belongs to aspect
a
i
, as follows:
z
ji
=
{
1 ; if f
j
? a
i
0 ; otherwise
(3)
This leads to our SDC-constraint function.
? = z
ji
|u
ik
? s?
jk
| ? d
jk
,?i, j, k (4)
SDC-constraint are flexible for modeling Senti-
ment Distribution Consistency. The more obser-
vations we have, the smaller d
jk
is. For frequent
aspect phrase, the constraint can be very informa-
tive because it can filter unrelated aspects for as-
pect phrase f
j
. The less observations we have,
the larger d
jk
is. For rare aspect phrases, the con-
straint can be very loose, and will not have much
effect on the clustering process for aspect phrase
f
j
. In this way, the model can work very robustly.
SDC-constraints are data-driven constraints.
Usually we have many reviews about hundreds of
products in our dataset. For each aspect phrase,
there are |A| ? |P | constraints (the number of as-
pects times the number of product). With thou-
sands of constraints about which aspect it is not
likely to belong to, the model learns to which as-
pect a phrase f
j
should be assigned. Although
most constraints may be loose because of the lim-
ited observations, SDC-constraint can still play an
important role in the learning process.
2.3 Sentiment Distribution Consistency
Regularized Multinomial Naive Bayes
(SDC-MNB)
In this section, we present our probabilistic model
which employs both context information and sen-
timent distribution.
First of all, we extract a context document d
for each aspect phrase, which will be described in
Section 2.5. In other word, a phrase is represented
by its context document. Assuming that the doc-
uments in D are independent and identically dis-
tributed, the probability of generating D is then
given by:
p
?
(D) =
|D|
?
j=1
p
?
(d
j
) =
|D|
?
j=1
?
y
j
?A
p
?
(d
j
, y
j
) (5)
where y
j
is a latent variable indicating the aspect
label for aspect phrase f
j
, and ? is the model pa-
rameter.
In our problem, we are actually more inter-
ested in the posterior distribution over aspect,
i.e., p
?
(y
j
|d
j
). Once the learned parameter ? is
obtained, we can get our clustering result from
p
?
(y
j
|d
j
), by assigning aspect a
i
with the largest
posterior to phrase f
j
. We can also enforce SDC-
constraint in expectation(on posterior p
?
). We use
q(Y ) to denote the valid posterior distribution that
satisfy our SDC-constraint, and Q to denote the
valid posterior distribution space, as follows:
Q = {q(Y ) : E
q
[z
ji
|u
ik
? s?
jk
|] ? d
jk
, ?i, j, k}. (6)
Since posterior plays such an important role in
joining the context model and SDC-constraint, we
formulate our problem in the framework of Poste-
rior Regularization (PR). PR is an efficient frame-
work to inject constraints on the posteriors of la-
tent variables. Instead of restricting p
?
directly,
which might not be feasible, PR penalizes the dis-
tance of p
?
to the constraint set Q. The posterior-
regularized objective is termed as follows:
max
?
{log p
?
(D) ? min
q?Q
KL(q(Y )||p
?
(Y |D))} (7)
By trading off the data likelihood of the ob-
served context documents (as defined in the first
term), and the KL divergence of the posteriors
to the valid posterior subspace defined by SDC-
constraint (as defined in the second term), the ob-
jective encourages models with both desired pos-
terior distribution and data likelihood. In essence,
the model attempts to maximize data likelihood of
context subject (softly) to SDC-constraint.
1617
2.3.1 Multinomial Naive Bayes
In spirit to (Zhai et al., 2011a), we use Multino-
mial Naive Bayes (MNB) to model the context
document. Let w
d
j
,k
denotes the k
th
word in doc-
ument d
j
, where each word is from the vocabulary
V = {w
1
, w
2
, ..., w
|V |
}. For each aspect phrase
f
j
, the probability of its latent aspect being a
i
and
generating context document d
i
is
p
?
(d
j
, y
j
= a
i
) = p(a
i
)
|d
j
|
?
k=1
p(w
d
j
,k
|a
i
) (8)
where p(a
i
) and p(w
d
j
,k
|a
i
) are parameters of this
model. Each word w
d
j
,k
is conditionally indepen-
dent of all other words given the aspect a
i
.
Although MNB has been used in existing work
for aspect clustering, all of the studies used it in
a semi-supervised manner, with labeled data or
pseudo-labeled data. In contrast, MNB proposed
here is used in an unsupervised manner for aspect-
related phrases clustering.
2.3.2 SDC-constraint
As mentioned above, the constraint posterior setQ
is defined by
Q = {q(Y ) : q(y
j
= a
i
)|u
ik
? s?
jk
| ? d
jk
,?i, j, k}. (9)
We can see that Q denotes a set of linear con-
straints on the projected posterior distribution q.
Note that we do not directly observe u
ik
, the sen-
timent distribution of aspect a
i
on product p
k
. For
aspect phrase f
j
that belongs to aspect a
i
, we es-
timate u
ik
by counting all sentiment samples. We
use the posterior p
?
(a
i
|d
j
) to approximately rep-
resent how likely phrase f
j
belongs to aspect a
i
.
u
ik
=
1
?
|D|
j=1
n
jk
p
?
(a
i
|d
j
)
|D|
?
j=1
n
jk
p
?
(a
i
|d
j
)s?
jk
(10)
where p
?
(a
i
|d
j
) is short for p
?
(y
j
= a
i
|d
j
), the
probability that aspect phrase f
j
belongs to a
i
given the context document d
j
. We estimate u
ik
in
this way because observations for aspect are rela-
tively sufficient for a reliable estimation since ob-
servations for an aspect are aggregated from those
for all phrases belonging to that aspect.
2.4 The Optimization Algorithm
The optimization algorithm for the objective (see
Eq. 7) is an EM-like two-stage iterative algorithm.
In E-step, we first calculate the posterior distri-
bution p
?
(a
i
|d
j
), then project it onto the valid pos-
terior distribution space Q. Given the parameters
?, the posterior distribution can be calculated by
Eq. 11.
p
?
(a
i
|d
j
) =
p(a
i
)
?
|d
j
|
k=1
p(w
d
j
,k
|a
i
)
?
|A|
r=1
p(a
r
)
?
|d
j
|
k=1
p(w
d
j
,k
|a
r
)
(11)
We use the above posterior distribution to update
the sentiment parameter for each aspect by Eq. 10.
The projected posterior distribution q is calculated
by
q = argmin
q?Q
KL(q(Y )||p
?
(Y |D)) (12)
For each instance, there are |A| ? |P | constraints.
However, we can prune a large number of useless
constraints derived from limited observations. All
constraints with d
jk
> 1 can be pruned, due to
the fact that the parameter u
ik
, s?
jk
is within [0,1],
and the difference can not be larger than 1. This
optimization problem in Eq. 12 is easily solved via
the dual form by the projected gradient algorithm
(Boyd and Vandenberghe, 2004):
max
??0
(
?
|A|
?
i=1
|P |
?
k=1
?
ik
d
jk
?
log
|A|
?
i=1
p
?
(a
i
|d
j
)exp{?
|P |
?
k=1
?
ik
|u
ik
? s?
jk
|} ? ????
)
(13)
where ? controls the slack size for constraint. After
solving the above optimization problem and ob-
taining the optimal ?, we can calculate the pro-
jected posterior distribution q by
q(y
j
= a
i
) =
1
Z
p
?
(a
i
|d
j
)exp{?
|P |
?
k=1
?
ik
|u
ik
?s?
jk
|} (14)
where Z is the normalization factor. Note that sen-
timent distribution consistency is actually modeled
as instance-level constraint here, which makes it
very efficient to solve.
In M-step, the projected posteriors q(Y ) are
then used to compute sufficient statistics and up-
date the models parameters ?. Given the projected
posteriors q(Y ), the parameters can be updated by
Eq. 15,16.
p(a
i
) =
1 +
?
|D|
j=1
q(y
j
= a
i
)
|A| + |D|
(15)
p(w
t
|a
i
) =
1 +
?
|D|
j=1
N
ti
q(y
j
= a
i
)
|V | +
?
|V |
m=1
?
|D|
j=1
N
mj
q(y
j
= a
i
)
(16)
where N
tj
is the number of times that the word w
t
occurs in document d
j
.
The parameters are initialized randomly, and we
repeat E-step and M-step until convergence.
1618
2.5 Data Extraction
2.5.1 Context Extraction
In order to extract the context document d for each
aspect phrase, we follow the approach in Zhai et
al. (2011a). For each aspect phrase, we generate
its context document by aggregating the surround-
ing texts of the phrase in all reviews. The preced-
ing and following t words of a phrase are taken as
the context where we set t = 3 in this paper. Stop-
words and other aspect phrases are removed. For
example, the following review contains two aspect
phrases, ?screen? and ?picture?,
The LCD screen gives clear picture.
For ?screen?, the surrounding texts are {the,
LCD, gives, clear, picture}. We remove stop-
words ?the?, and the aspect term ?picture?, and
the resultant context of ?screen? in this review is
context(screen) ={LCD, screen, gives, clear}.
Similarly, the context of ?picture? in this review is
context(picture) ={gives, clear}.
By aggregating the contexts of all the reviews
that contain aspect phrase f
j
, we obtain the cor-
responding context document d
j
.
2.5.2 Sentiment Extraction
Since we use semi-structured reviews, we ob-
tain the estimated sentiment distribution by sim-
ply counting how many times each aspect phrase
appears in Pros and Cons reviews for each prod-
uct respectively. So for each aspect phrase f
j
, let
n
+
jk
denotes the times that f
j
appears in Pros of
all reviews for product p
k
, and let n
?
jk
denotes the
times that f
j
appears in Cons of all reviews for
product p
k
. So the total number of occurrence of a
phrase is n
jk
= n
+
jk
+ n
?
jk
. We have samples like
(1,1,1,0,0) where 1 means a phrase occurs in Pros
of a review, and 0 in Cons. Given a sequence of
such observations, the sample mean is easily com-
puted as s?
jk
=
n
+
jk
n
+
jk
+n
?
jk
. And the sample standard
deviation is ??
jk
=
?
(1?s?
jk
)
2
?n
+
jk
+(s?
jk
)
2
?n
?
jk
n
jk
?1
.
3 Experiments
3.1 Data Preparation
The details of our review corpus are given
in Table 2. This corpus contains semi-
structured customer reviews from four do-
mains: Camera, Cellphone, Laptop, and MP3.
These reviews were crawled from the following
web sites: www.amazon.cn, www.360buy.com,
www.newegg.com.cn, and www.zol.com. The as-
pect label of each aspect phrases is annotated by
human curators.
Camera Cellphone Laptop MP3
#Products 449 694 702 329
#Reviews 101,235 579,402 102,439 129,471
#Aspect Phrases 236 230 238 166
#Aspect 12 10 14 8
Table 2: Statistics of the review corpus. # denotes
the size.
3.2 Evaluation Measures
We adapt three measures Purity, Entropy, and
Rand Index for performance evaluation. These
measures have been commonly used to evaluate
clustering algorithms.
Given a data set DS, suppose its gold-standard
partition is G = {g
1
, ..., g
j
, ..., g
k
}, where k
is the number of clusters. A clustering algo-
rithm partitions DS into k disjoint subsets, say
DS
1
, DS
2
, ..., DS
k
.
Entropy: For each resulting cluster, we can mea-
sure its entropy using Eq. 17, where P
i
(g
j
) is the
proportion of data points of class g
j
in DS
i
. The
entropy of the entire clustering result is calculated
by Eq. 18.
entropy(DS
i
) = ?
k
?
j=1
P
i
(g
j
)log
2
P
i
(g
j
) (17)
entropy(DS) =
k
?
i=1
|DS
i
|
|DS|
entropy(DS
i
) (18)
Purity: Purity measures the extent that a cluster
contains only data from one gold-standard parti-
tion. The cluster purity is computed with Eq. 19.
The total purity of the whole clustering result (all
clusters) is computed with Eq. 20.
purity(DS
i
) = max
j
P
i
(g
j
) (19)
purity(DS) =
k
?
i=1
|DS
i
|
|DS|
purity(DS
i
) (20)
RI: The Rand Index(RI) penalizes both false posi-
tive and false negative decisions during clustering.
Let TP (True Positive) denotes the number of pairs
of elements that are in the same set in DS and in
the same set in G. TN (True Negative) denotes
number of pairs of elements that are in different
sets in DS and in different sets in G. FP (False
1619
Camera Cellphone Laptop MP3
P RI E P RI E P RI E P RI E
Kmeans 43.48% 83.52% 2.098 48.91% 84.80% 1.792 43.46% 87.11% 2.211 40.00% 70.98% 2.047
L-EM 54.89% 87.07% 1.690 51.96% 86.64% 1.456 48.94% 84.53% 2.039 44.24% 75.37% 1.990
LDA 36.84% 83.28% 2.426 48.65% 85.33% 1.833 35.02% 83.53% 2.660 36.12% 76.08% 2.296
Constraint-LDA 43.30% 86.01% 2.216 47.89% 86.04% 1.974 32.35% 84.86% 2.676 50.70% 81.42% 1.924
SDC-MNB 56.42% 88.16% 1.725 67.95% 90.62% 1.266 55.52% 90.72% 1.780 58.06% 83.57% 1.578
Table 3: Comparison to unsupervised baselines. (P is short for purity, E for entropy, and RI for random
index.)
Positive) denotes number of pairs of elements in
S that are in the same set in DS and in different
sets in G. FN (False Negative) denotes number of
pairs of elements that are in different sets in DS
and in the same set in G. The Rand Index(RI) is
computed with Eq. 21.
RI(DS) =
TP + TN
TP + TN + FP + FN
(21)
3.3 Evaluation Results
3.3.1 Comparison to unsupervised baselines
We compared our approach with several existing
unsupervised methods. Some of the methods aug-
mented unsupervised models by incorporating lex-
ical similarity and other domain knowledge. All
of them are context-based models.
2
We list these
models as follows.
? Kmeans: Kmeans is the most popular cluster-
ing algorithm. Here we use the context distri-
butional similarity (cosine similarity) as the
similarity measure.
? L-EM: This is a state-of-the-art unsupervised
method for clustering aspect phrases (Zhai et
al., 2011a). L-EM employed lexical knowl-
edge to provide a better initialization for EM.
? LDA: LDA is a popular topic model(Blei et
al., 2003). Given a set of documents, it out-
puts groups of terms of different topics. In
our case, each aspect phrase is processed as a
term.
3
Each sentence in a review is consid-
ered as a document. Each aspect is consid-
ered as a topic. In LDA, a term may belong
to more than one topic/group, but we take the
topic/group with the maximum probability.
2
In our method, we collect context document for each
aspect phrase. This process is conducted for L-EM and K-
means. But for LDA and Constraint-LDA, we take each sen-
tence of reviews as a document. This setting for the LDA
baselines is adapted from previous work.
3
Each aspect phrase is pre-processed as a single word
(e.g., ?battery life? is treated as battery-life). Other words
are normally used in LDA.
? Constraint-LDA: Constraint-LDA (Zhai et
al., 2011b) is a state-of-the-art LDA-based
method that incorporates must-link and
cannot-link constraints for this task. We set
the damping factor ? = 0.3 and relaxation
factor ? = 0.9, as suggested in the original
reference.
For all methods that depend on the random ini-
tiation, we use the average results of 10 runs as the
final result. For all LDA-based models, we choose
? = 50/T , ? = 0.1, and run 1000 iterations.
Experiment results are shown in Table 3. We
can see that our approach almost outperforms all
unsupervised baseline methods by a large margin
on all domains. In addition, we have the following
observations:
? LDA and Kmeans perform poorly due to the
fact that the two methods do not use any prior
knowledge. It is also shown that only using
the context distributional information is not
sufficient for clustering aspect phrases.
? Constraint-LDA and L-EM that utilize prior
knowledge perform better. We can see that
Constraint-LDA outperforms LDA in terms
of RI (Rand Index) on all domains. L-EM
achieves the best results against the baselines.
This demonstrates the effectiveness to incor-
porate prior knowledge.
? SDC-MNB produces the optimal results
among all models for clustering. Methods
that use must-links and cannot-links may suf-
fer from noisy links. For L-EM, we find
that it is sensitive to noisy must-links. As
L-EM assumes that must-link is transitive,
several noisy must-links may totally misla-
bel the softly annotated data. For Constraint-
LDA, it is more robust than L-EM, because
it doesn?t assume the transitivity of must-
link. However, it only promotes the RI (Rand
Index) consistently by leveraging pair-wise
prior knowledge, but sometimes it hurts the
1620
performance with respect to purity or en-
tropy. Our method is consistently better on
almost all domains, which shows the advan-
tages of the proposed model.
? SDC-MNB is remarkably better than base-
lines, particularly for the cellphone domain.
We argue that this is because we have the
largest number of reviews for each product
in the cellphone domain. The larger dataset
gives us more observations on each phrase,
so that we obtain more reliable estimation of
model parameters.
3.3.2 Comparison to supervised baselines
We further compare our methods with two super-
vised models. For each supervised model, we
provide a proportion of manually labeled data for
training, which is randomly selected from gold-
standard annotations. However, we didn?t use any
labeled data for our approach.
? MNB: The labeled seeds are used to train a
MNB classifier to classify all unlabeled as-
pect phrases into different classes.
? L-Kmeans: In L-Kmeans, the clusters of the
labeled seeds are fixed at the initiation and
remain unchanged during iteration.
Purity RI Entropy
MNB-5% 53.21% 85.77% 1.854
MNB-10% 59.55% 86.70% 1.656
MNB-15% 66.06% 88.39% 1.449
L-Kmeans-10% 53.54% 86.15% 1.745
L-Kmeans-15% 57.00% 86.89% 1.643
L-Kmeans-20% 60.97% 87.63% 1.528
SDC-MNB 59.49% 88.26% 1.580
Table 4: Comparison to supervised baselines.
MNB-5% means MNB with 5% labeled data.
We experiment with several settings: taking
5%, 10% and 15% of the manually labeled aspect
phrases for training, and the remainder as unla-
beled data. Experiment results is shown in Table
4 (the results are averaged over 4 domains). We
can see that our unsupervised approach is roughly
as good as the supervised MNB with 10% labeled
data. Our unsupervised approach is also slightly
better than L-Kmeans with 15% labeled data. This
result further demonstrates the effectiveness of our
model.
3.3.3 Influence of parameters
We vary the confidence level from 90% to 99.9%
to see how it impacts on the performance of SDC-
MNB. The results are presented in Fig. 5 (the re-
sults are averaged over 4 domains). We can see
that the performance of clustering is fairly stable
when changing the confidence level, which im-
plies the robustness of our model.
Figure 5: Influence of the confidence level on
SDC-MNB.
3.3.4 Analysis of SDC-constraint
As mentioned in Section 2.2, SDC-constraint is
dependent on the number of observations. More
observations we get, more informative the con-
straint is, which means the constraint is tighter and
d
jk
(see Eq.4) is smaller. For all k, we count how
many d
jk
is less than 0.2 (and 1) on average for
each aspect phrase f
j
. d
jk
is calculated with a
confidence level of 99%. The statistics of con-
straints is given in Table 5. We can see that the
cellphone domain has the most informative and
largest constraint set, that may explain why SDC-
MNB achieves the largest purity gain(over L-EM)
in cellphone domain.
#(d
jk
< 0.2) #(0.2 < d
jk
< 1) purity gain
Camera 3.02 8.78 1.53%
Cellphone 17.29 30.5 15.99%
Laptop 4.6 13.22 6.58%
MP3MP4 6.1 10.7 13.82%
Table 5: Constraint statistics on different domains.
4 Related Work
Our work is related to two important research
topics: aspect-level sentiment analysis, and
constraint-driven learning. For aspect-level senti-
ment analysis, aspect extraction and clustering are
key tasks. For constraint-driven learning, a variety
of frameworks and models for sentiment analysis
have been studied extensively.
There have been many studies on clustering
aspect-related phrases. Most existing studies are
1621
based on context information. Some works also
encoded lexical similarity and synonyms as prior
knowledge. Carenini et al. (2005) proposed a
method that was based on several similarity met-
rics involving string similarity, synonyms, and lex-
ical distances defined with WordNet. Guo et al.
(2009) proposed a multi-level latent semantic as-
sociation model to capture expression-level and
context-level topic structure. Zhai et al. (2010)
proposed an EM-based semi-supervised learning
method to group aspect expressions into user-
specified aspects. They employed lexical knowl-
edge to provide a better initialization for EM. In
Zhai et al. (2011a), an EM-based unsupervised
version was proposed. The so-called L-EM model
first generated softly labeled data by grouping fea-
ture expressions that share words in common, and
then merged the groups by lexical similarity. Zhai
et al. (2011b) proposed a LDA-based method
that incorporates must-link and cannot-link con-
straints.
Another line of work aimed to extract and clus-
ter aspect words simultaneously using topic mod-
eling. Titov and McDonald (2008) proposed the
multi-grain topic models to discover global and
local aspects. Branavan et al. (2008) proposed
a method which first clustered the key-phrases
in Pros and Cons into some aspect categories
based on distributional similarity, then built a topic
model modeling the topics or aspects. Zhao et al.
(2010) proposed the MaxEnt-LDA (a Maximum
Entropy and LDA combination) hybrid model to
jointly discover both aspect words and aspect-
specific opinion words, which can leverage syn-
tactic features to separate aspects and sentiment
words. Mukherjee and Liu (2012) proposed a
semi-supervised topic model which used user-
provided seeds to discover aspects. Chen et al.
(2013) proposed a knowledge-based topic model
to incorporate must-link and cannot-link informa-
tion. Their model can adjust topic numbers auto-
matically by leveraging cannot-link.
Our work is also related to general constraint-
driven(or knowledge-driven) learning models.
Several general frameworks have been proposed to
fully utilize various prior knowledge in learning.
Constraint-driven learning (Chang et al., 2008)
(CODL) is an EM-like algorithm that incorpo-
rates per-instance constraints into semi-supervised
learning. Posterior regularization (Graca et al.,
2007) (PR) is a modified EM algorithm in which
the E-step is replaced by the projection of the
model posterior distribution onto the set of dis-
tributions that satisfy auxiliary expectation con-
straints. Generalized expectation criteria (Druck
et al., 2008) (GE) is a framework for incorporating
preferences about model expectations into param-
eter estimation objective functions. Liang et al.
(2009) developed a Bayesian decision-theoretic
framework to learn an exponential family model
using general measurements on the unlabeled data.
In this paper, we model our problem in the frame-
work of posterior regularization.
Many works promoted the performance of sen-
timent analysis by incorporating prior knowledge
as weak supervision. Li and Zhang (2009) in-
jected lexical prior knowledge to non-negative ma-
trix tri-factorization. Shen and Li (2011) further
extended the matrix factorization framework to
model dual supervision from both document and
word labels. Vikas Sindhwani (2008) proposed a
general framework for incorporating lexical infor-
mation as well as unlabeled data within standard
regularized least squares for sentiment prediction
tasks. Fang (2013)proposed a structural learning
model with a handful set of aspect signature terms
that are encoded as weak supervision to extract la-
tent sentiment explanations.
5 Conclusions
Aspect finding and clustering is an important task
for aspect-level sentiment analysis. In order to
cluster aspect-related phrases, this paper has ex-
plored a novel concept, sentiment distribution con-
sistency. We formalize the concept as soft con-
straint, integrate the constraint with a context-
based probabilistic model, and solve the problem
in the posterior regularization framework. The
proposed model is also designed to be robust with
both sufficient and insufficient observations. Ex-
periments show that our approach outperforms
state-of-the-art baselines consistently.
Acknowledgments
This work was partly supported by the following
grants from: the National Basic Research Program
(973 Program) under grant No.2012CB316301
and 2013CB329403, the National Science Foun-
dation of China project under grant No.61332007
and No. 61272227, and the Beijing Higher Educa-
tion Young Elite Teacher Project.
1622
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. J. Mach. Learn.
Res., 3:993?1022, March.
Stephen Boyd and Lieven Vandenberghe. 2004. Con-
vex Optimization. Cambridge University Press, New
York, NY, USA.
S. R. K. Branavan, Harr Chen, Jacob Eisenstein, and
Regina Barzilay. 2008. Learning document-level
semantic properties from free-text annotations. In
Proceedings of the Association for Computational
Linguistics (ACL).
Giuseppe Carenini, Raymond T. Ng, and Ed Zwart.
2005. Extracting knowledge from evaluative text.
In Proceedings of the 3rd International Conference
on Knowledge Capture, K-CAP ?05, pages 11?18,
New York, NY, USA. ACM.
Ming-Wei Chang, Lev Ratinov, Nicholas Rizzolo, and
Dan Roth. 2008. Learning and inference with
constraints. In Proceedings of the 23rd National
Conference on Artificial Intelligence - Volume 3,
AAAI?08, pages 1513?1518. AAAI Press.
Zhiyuan Chen, Arjun Mukherjee, Bing Liu, Meichun
Hsu, Mal Castellanos, and Riddhiman Ghosh. 2013.
Exploiting domain knowledge in aspect extraction.
In EMNLP, pages 1655?1667. ACL.
Gregory Druck, Gideon Mann, and Andrew McCal-
lum. 2008. Learning from labeled features using
generalized expectation criteria. In Proceedings of
the 31st Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, SIGIR ?08, pages 595?602, New York,
NY, USA. ACM.
Lei Fang, Minlie Huang, and Xiaoyan Zhu. 2013. Ex-
ploring weakly supervised latent sentiment expla-
nations for aspect-level review analysis. In Qi He,
Arun Iyengar, Wolfgang Nejdl, Jian Pei, and Rajeev
Rastogi, editors, CIKM, pages 1057?1066. ACM.
Joao V. Graca, Lf Inesc-id, Kuzman Ganchev, Ben
Taskar, Joo V. Graa, L F Inesc-id, Kuzman Ganchev,
and Ben Taskar. 2007. Expectation maximization
and posterior constraints. In In Advances in NIPS,
pages 569?576.
Honglei Guo, Huijia Zhu, Zhili Guo, XiaoXun Zhang,
and Zhong Su. 2009. Product feature categorization
with multilevel latent semantic association. In Pro-
ceedings of the 18th ACM Conference on Informa-
tion and Knowledge Management, CIKM ?09, pages
1087?1096, New York, NY, USA. ACM.
Tao Li, Yi Zhang, and Vikas Sindhwani. 2009. A non-
negative matrix tri-factorization approach to senti-
ment classification with lexical prior knowledge. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP: Volume 1 - Volume 1, ACL ?09, pages
244?252, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Percy Liang, Michael I. Jordan, and Dan Klein. 2009.
Learning from measurements in exponential fami-
lies. In Proceedings of the 26th Annual Interna-
tional Conference on Machine Learning, ICML ?09,
pages 641?648, New York, NY, USA. ACM.
Arjun Mukherjee and Bing Liu. 2012. Aspect extrac-
tion through semi-supervised modeling. In Proceed-
ings of the 50th Annual Meeting of the Association
for Computational Linguistics: Long Papers - Vol-
ume 1, ACL ?12, pages 339?348, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Chao Shen and Tao Li. 2011. A non-negative matrix
factorization based approach for active dual super-
vision from document and word labels. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP ?11, pages 949?
958, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Vikas Sindhwani and Prem Melville. 2008.
Document-word co-regularization for semi-
supervised sentiment analysis. In ICDM, pages
1025?1030. IEEE Computer Society.
Ivan Titov and Ryan McDonald. 2008. Modeling on-
line reviews with multi-grain topic models. In Pro-
ceedings of the 17th International Conference on
World Wide Web, WWW ?08, pages 111?120, New
York, NY, USA. ACM.
Zhongwu Zhai, Bing Liu, Hua Xu, and Peifa Jia. 2010.
Grouping product features using semi-supervised
learning with soft-constraints. In Proceedings of
the 23rd International Conference on Computa-
tional Linguistics, COLING ?10, pages 1272?1280,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Zhongwu Zhai, Bing Liu, Hua Xu, and Peifa Jia.
2011a. Clustering product features for opinion min-
ing. In Proceedings of the Fourth ACM Interna-
tional Conference on Web Search and Data Mining,
WSDM ?11, pages 347?354, New York, NY, USA.
ACM.
Zhongwu Zhai, Bing Liu, Hua Xu, and Peifa Jia.
2011b. Constrained lda for grouping product fea-
tures in opinion mining. In Proceedings of the 15th
Pacific-Asia Conference on Advances in Knowl-
edge Discovery and Data Mining - Volume Part
I, PAKDD?11, pages 448?459, Berlin, Heidelberg.
Springer-Verlag.
Wayne X. Zhao, Jing Jiang, Hongfei Yan, and Xiaom-
ing Li. 2010. Jointly modeling aspects and opin-
ions with a MaxEnt-LDA hybrid. In Proceedings of
the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP ?10, pages 56?
65, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
1623
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 483?491,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Learning to Link Entities with Knowledge Base
Zhicheng Zheng, Fangtao Li, Minlie Huang, Xiaoyan Zhu
State Key Laboratory of Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China
{zhengzc04,fangtao06}@gmail.com, {aihuang,zxy-dcs}@tsinghua.edu.cn
Abstract
This paper address the problem of entity link-
ing. Specifically, given an entity mentioned in
unstructured texts, the task is to link this entity
with an entry stored in the existing knowledge
base. This is an important task for informa-
tion extraction. It can serve as a convenient
gateway to encyclopedic information, and can
greatly improve the web users? experience.
Previous learning based solutions mainly fo-
cus on classification framework. However, it?s
more suitable to consider it as a ranking prob-
lem. In this paper, we propose a learning to
rank algorithm for entity linking. It effectively
utilizes the relationship information among
the candidates when ranking. The experi-
ment results on the TAC 20091 dataset demon-
strate the effectiveness of our proposed frame-
work. The proposed method achieves 18.5%
improvement in terms of accuracy over the
classification models for those entities which
have corresponding entries in the Knowledge
Base. The overall performance of the system
is also better than that of the state-of-the-art
methods.
1 Introduction
The entity linking task is to map a named-entity
mentioned in a text to a corresponding entry stored
in the existing Knowledge Base. The Knowledge
Base can be considered as an encyclopedia for en-
tities. It contains definitional, descriptive or rele-
vant information for each entity. We can acquire the
knowledge of entities by looking up the Knowledge
1http://www.nist.gov/tac/
Base. Wikipedia is an online encyclopedia, and now
it becomes one of the largest repositories of encyclo-
pedic knowledge. In this paper, we use Wikipedia as
our Knowledge Base.
Entity linking can be used to automatically aug-
ment text with links, which serve as a conve-
nient gateway to encyclopedic information, and can
greatly improve user experience. For example, Fig-
ure 1 shows news from BBC.com. When a user is
interested in ?Thierry Henry?, he can acquire more
detailed information by linking ?Thierry Henry? to
the corresponding entry in the Knowledge Base.
Figure 1: Entity linking example
Entity linking is also useful for some information
extraction (IE) applications. We can make use of
information stored in the Knowledge Base to assist
the IE problems. For example, to answer the ques-
tion ?When was the famous basketball player Jor-
dan born??, if the Knowledge Base contains the en-
483
tity of basketball player Michael Jordan and his in-
formation (such as infobox2 in Wikipedia), the cor-
rect answer ?February 17, 1963? can be easily re-
trieved.
Entity linking encounters the problem of entity
ambiguity. One entity may refer to several entries
in the Knowledge Base. For example, the entity
?Michael Jordan? can be linked to the basketball
player or the professor in UC Berkeley.
Previous solutions find that classification based
methods are effective for this task (Milne and Wit-
ten, 2008). These methods consider each candidate
entity independently, and estimate a probability that
the candidate entry corresponds to the target entity.
The candidate with the highest probability was cho-
sen as the target entity. In this way, it?s more like
a ranking problem rather than a classification prob-
lem. Learning to rank methods take into account the
relations between candidates, which is better than
considering them independently. Learning to rank
methods are popular in document information re-
trieval, but there are few studies on information ex-
traction. In this paper, we investigate the application
of learning to rank methods to the entity linking task.
And we compare several machine learning methods
for this task. We investigate the pairwise learning to
rank method, Ranking Perceptron (Shen and Joshi,
2005), and the listwise method, ListNet (Cao et al,
2007). Two classification methods, SVM and Per-
ceptron, are developed as our baselines. In com-
parison, learning to rank methods show significant
improvements over classification methods, and List-
Net achieves the best result. The best overall per-
formance is also achieved with our proposed frame-
work.
This paper is organized as follows. In the next
section we will briefly review the related work. We
present our framework for entity linking in section
3. We then describe in section 4 learning to rank
methods and features for entity linking. A top1 can-
didate validation module will be explained in section
5. Experiment results will be discussed in section 6.
Finally, we conclude the paper and discusses the fu-
ture work in section 7.
2Infoboxes are tables with semi-structured information in
some pages of Wikipedia
2 Related Work
There are a number of studies on named entity dis-
ambiguation, which is quite relevant to entity link-
ing. Bagga and Baldwin (1998) used a Bag of Words
(BOW) model to resolve ambiguities among people.
Mann and Yarowsky (2003) improved the perfor-
mance of personal names disambiguation by adding
biographic features. Fleischman (2004) trained a
Maximum Entropy model with Web Features, Over-
lap Features, and some other features to judge
whether two names refer to the same individual.
Pedersen (2005) developed features to represent the
context of an ambiguous name with the statistically
significant bigrams.
These methods determined to which entity a spe-
cific name refer by measuring the similarity between
the context of the specific name and the context of
the entities. They measured similarity with a BOW
model. Since the BOW model describes the con-
text as a term vector, the similarity is based on co-
occurrences. Although a term can be one word or
one phrase, it can?t capture various semantic rela-
tions. For example, ?Michael Jordan now is the boss
of Charlotte Bobcats? and ?Michael Jordan retired
from NBA?. The BOW model can?t describe the re-
lationship between Charlotte Bobcats and NBA. Ma-
lin and Airoldi (2005) proposed an alternative sim-
ilarity metric based on the probability of walking
from one ambiguous name to another in a random
walk within the social network constructed from all
documents. Minkov (2006) considered extended
similarity metrics for documents and other objects
embedded in graphs, facilitated via a lazy graph
walk, and used it to disambiguate names in email
documents. Bekkerman and McCallum (2005) dis-
ambiguated web appearances of people based on the
link structure of Web pages. These methods tried to
add background knowledge via social networks. So-
cial networks can capture the relatedness between
terms, so the problem of a BOW model can be
solved to some extent. Xianpei and Jun (2009) pro-
posed to use Wikipedia as the background knowl-
edge for disambiguation. By leveraging Wikipedia?s
semantic knowledge like social relatedness between
named entities and associative relatedness between
concepts, they can measure the similarity between
entities more accurately. Cucerzan (2007) and
484
Bunescu (2006) used Wikipedia?s category informa-
tion in the disambiguation process. Using different
background knowledge, researcher may find differ-
ent efficient features for disambiguation.
Hence researchers have proposed so many effi-
cient features for disambiguation. It is important to
integrate these features to improve the system per-
formance. Some researchers combine features by
manual rules or weights. However, it is not conve-
nient to directly use these rules or weights in another
data set. Some researchers also try to use machine
learning methods to combine the features. Milne and
Witten (2008) used typical classifiers such as Naive
Bayes, C4.5 and SVM to combine features. They
trained a two-class classifier to judge whether a can-
didate is a correct target. And then when they try
to do disambiguation for one query, each candidate
will be classified into the two classes: correct tar-
get or incorrect target. Finally the candidate answer
with the highest probability will be selected as the
target if there are more than one candidates classi-
fied as answers. They achieve great performance in
this way with three efficient features. The classifier
based methods can be easily used even the feature
set changed. However, as we proposed in Introduc-
tion, it?s not the best way for such work. We?ll detail
the learning to rank methods in the next section.
3 Framework for Entity Linking
Input%a%query
Output%the%
final answer%
Figure 2: The framework for entity linking
Entity linking is to align a named-entity men-
tioned in a text to a corresponding entry stored in
the existing Knowledge Base. We proposed a frame-
work to solve the ?entity linking? task. As illustrated
in Figure 2, when inputting a query which is an en-
tity mentioned in a text, the system will return the
target entry in Knowledge Base with four modules:
1. Query Processing. First, we try to correct the
spelling errors in the queries by using query
spelling correction supplied by Google. Sec-
ond, we expand the query in three ways: ex-
panding acronym queries from the text where
the entity is located, expanding queries with the
corresponding redirect pages of Wikipedia and
expanding queries by using the anchor text in
the pages from Wikipedia.
2. Candidates Generation. With the queries gen-
erated in the first step, the candidate genera-
tion module retrieves the candidates from the
Knowledge Base. The candidate generation
module also makes use of the disambiguation
pages in Wikipedia. If there is a disambigua-
tion page corresponding to the query, the linked
entities listed in the disambiguation page are
added to the candidate set.
3. Candidates Ranking. In the module, we rank all
the candidates with learning to rank methods.
4. Top1 Candidate Validation. To deal with those
queries without appropriate matching, we fi-
nally add a validation module to judge whether
the top one candidate is the target entry.
The detail information of ranking module and val-
idation module will be introduced in next two sec-
tions.
4 Learning to Rank Candidates
In this section we first introduce the learning to rank
methods, and then describe the features for ranking
methods.
4.1 Learning to rank methods
Learning to rank methods are popular in the area of
document retrieval. There are mainly two types of
learning to rank methods: pairwise and listwise. The
pairwise approach takes as instances object pairs in
a ranking list for a query in learning. In this way,
it transforms the ranking problem to the classifica-
tion problem. Each pair from ranking list is labeled
based on the relative position or with the score of
485
ranking objects. Then a classification model can be
trained on the labeled data and then be used for rank-
ing. The pairwise approach has advantages in that
the existing methodologies on classification can be
applied directly. The listwise approach takes can-
didate lists for a query as instances to train ranking
models. Then it trains a ranking function by min-
imizing a listwise loss function defined on the pre-
dicted list and the ground truth list.
To describe the learning to rank methods, we first
introduce some notations:
? Query set Q = {q(i)?i = 1 : m}.
? Each query q(i) is associated with a list of ob-
jects(in document retrieval, the objects should
be documents), d(i) = {d(i)j ?j = 1 : n(i)}.
? Each object list has a labeled score list y(i) =
{y(i)j ?j = 1 : n(i)} represents the relevance de-
gree between the objects and the query.
? Features vectors x(i)j from each query-object
pair, j = 1 : n(i).
? Ranking function f, for each x(i)j it outputs a
score f(x(i)j ). After the training phase, to rank
the objects, just use the ranking function f to
output the score list of the objects, and rank
them with the score list.
In the paper we will compare two different learn-
ing to rank approaches: Ranking Perceptron for pair-
wise and ListNet for listwise. A detailed introduc-
tion on Ranking Perceptron (Shen and Joshi, 2005)
and ListNet (Cao et al, 2007) is given.
4.1.1 Ranking Perceptron
Ranking Perceptron is a pairwise learning to rank
method. The score function f!(x(i)j ) is defined as
< !, x(i)j >.
For each pair (x(i)j1 , x
(i)
j2 ), f!(x
(i)
j1 ? x
(i)
j2 ) is com-
puted. With a given margin function g(x(i)j1 , x
(i)
j2 ) and
a positive rate  , if f!(x(i)j1 ? x
(i)
j2 ) ? g(x
(i)
j1 , x
(i)
j2 ) ,
an update is performed:
!t+1 = !t + (x(i)j1 ? x
(i)
j2 )g(x
(i)
j1 , x
(i)
j2 )
After iterating enough times, we can use the func-
tion f! to rank candidates.
4.1.2 ListNet
ListNet takes lists of objects as instances in learn-
ing. It uses a probabilistic method to calculate the
listwise loss function.
ListNet transforms into probability distributions
both the scores of the objects assigned by the ora-
cle ranking function and the real score of the objects
given by human.
Let  denote a permutation on the objects. In List-
Net alorithm, the probability of  with given scores
is defined as:
Ps() =
n
?
j=1
exp(s(j))
?n
k=j exp(s(k))
Then the top k probability of Gk(j1, j2, ..., jk) can
be calculated as:
Ps(Gk(j1, j2, ..., jk)) =
k
?
t=1
exp(sjt)
?l
l=t exp(sjl)
The ListNet uses a listwise loss function with
Cross Entropy as metric:
L(y(i), z(i)) = ?
?
?g?Gk
Py(i)(g)log(Pz(i) (g))
Denote as f! the ranking function based on
Neural Network model !. The gradient of
L(y(i), z(i)(f!)) with respect to parameter ! can be
calculated as:
?! = ?L(y
(i), z(i)(f!))
?!
= ?
?
?g?Gk
?Pz(i)(f!)(g)
?!
Py(i)(g)
Pz(i)(f!)(g)
In each iteration, the ! is updated with ? ??!
in a gradient descent way. Here  is the learning
rate.
To train a learning to rank model, the manually
evaluated score list for each query?s candidate list is
required. We assign 1 to the real target entity and 0
to the others.
486
4.2 Features for Ranking
In the section, we will introduce the features used
in the ranking module. For convenience, we define
some symbols first:
? Q represents a query, which contains a named
entity mentioned in a text. CSet represents the
candidate entries in Knowledge Base for the
query Q. C represents a candidate in CSet.
? Q?s nameString represents the name string of
Q. Q?s sourceText represents the source text of
Q. Q?s querySet represents the queries which
are expansions of Q?s nameString.
? C?s title represents the title of corresponding
Wikipedia article of C. C?s titleExpand repre-
sents the union set of the redirect set of C and
the anchor text set of C. C?s article represents
the Wikipedia article of C.
? C?s nameEntitySet represents the set of all
named entities in C?s article labeled by Stan-
ford NER (Finkel et al, 2005). Q?s nameEnti-
tySet represents the set of all named entities in
Q?s sourceText.
? C?s countrySet represents the set of all coun-
tries in C?s article, and we detect the countries
from text via a manual edited country list. Q?s
countrySet represents the set of all countries
in Q?s sourceText. C?s countrySetInTitle rep-
resents the set of countries exist in one of the
string s from C?s titleExpand.
? C?s citySetInTitle represents the set of all cities
exist in one of the string s from C?s titleExpand,
and we detect the cities from text via a manual
edited list of famous cities. Q?s citySet repre-
sents the set of all cities in Q?s sourceText.
? Q?s type represents the type of query Q. It?s la-
beled by Stanford NER. C?s type is manually
labeled already in the Knowledge Base.
The features that used in the ranking module can
be divided into 3 groups: Surface, Context and Spe-
cial. Each of these feature groups will be detailed
next.
4.2.1 Surface Features
The features in Surface group are used to measure
the similarity between the query string and candidate
entity?s name string.
? StrSimSurface. The feature value is the max-
imum similarity between the Q?s nameString
and each string s in the set C?s titleExpand. The
string similarity is measured with edit distance.
? ExactEqualSurface. The feature value is 1 if
there is a string s in set C?s titleExpand same as
the Q?s nameString, or the Candidate C is ex-
tracted from the disambiguation page. In other
case, the feature value is set to 0.
? StartWithQuery. The feature value is 1 if there
is a string s in set C?s titleExpand starting with
the Q?s nameString, and C?s ExactEqualSur-
face is not 1. In other case, the feature value
is set to 0.
? EndWithQuery. The feature value is 1 if there
is a string s in set C?s titleExpand ending with
the Q?s nameString, and C?s ExactEqualSur-
face is not 1. In other case, the feature value
is set to 0.
? StartInQuery. The feature value is 1 if there is a
string s in set C?s titleExpand that s is the prefix
of the Q?s nameString, and C?s ExactEqualSur-
face is not 1. In other case, the feature value is
set to 0.
? EndInQuery. The feature value is 1 if there is a
string s in set C?s titleExpand that s is the post-
fix of the Q?s nameString, and C?s ExactEqual-
Surface is not 1. In other case, the feature value
is set to 0.
? EqualWordNumSurface. The feature value is
the maximum number of same words between
the Q?s nameString and each string s in the set
C?s titleExpand.
? MissWordNumSurface. The feature value is
the minimum number of different words be-
tween the Q?s nameString and each string s in
the set C?s titleExpand.
487
4.2.2 Context Features
The features in Context group are used to measure
the context relevance between query and the candi-
date entity. We mainly consider the TF-IDF similar-
ity and named entity co-occurrence.
? TFSimContext. The feature value is the TF-
IDF similarity between the C?s article and Q?s
sourceText.
? TFSimRankContext. The feature value is the
inverted rank of C?s TFSimContext in the CSet.
? AllWordsInSource. The feature value is 1 if all
words in C?s title exist in Q?s sourceText, and
in other case, the feature value is set to 0.
? NENumMatch. The feature value is the num-
ber of of same named entities between C?s
nameEntitySet and Q?s nameEntitySet. Two
named entities are judged to be the same if and
only if the two named entities? strings are iden-
tical.
4.2.3 Special Features
Besides the features above, we also find that the
following features are quite significant in the entity
linking task: country names, city names and types
of queries and candidates.
? CountryInTextMatch. The feature value is the
number of same countries between C?s coun-
trySet and Q?s countrySet.
? CountryInTextMiss. The feature value is the
number of countries that exist in Q?s country-
Set but do not exist in C?s countrySet.
? CountryInTitleMatch. The feature value is the
number of same countries between C?s coun-
trySetInTitle and Q?s countrySet.
? CountryInTitleMiss. The feature value is the
number of countries that exist in C?s country-
SetInTitle but do not exist in Q?s countrySet.
? CityInTitleMatch. The feature value is the
number of same cities between C?s citySetInTi-
tle and Q?s citySet.
? TypeMatch. The feature value is 0 if C?s type is
not consistent with Q?s type, in other case, the
feature value is set to 1.
When ranking the candidates in CSet, the fea-
tures? value was normalized into [0, 1] to avoid noise
caused by large Integer value or small double value.
5 Top 1 Candidate Validation
To deal with those queries without target entities in
the Knowledge Base, we supply a Top 1 candidate
validation module. In the module, a two-class classi-
fier is used to judge whether the top one candidate is
the true target entity. The top one candidate selected
from the ranking module can be divided into two
classes: target and non-target, depending on whether
it?s the correct target link of the query. According
to the performance of classification, SVM is chosen
as the classifier (In practice, the libsvm package is
used) and the SVM classifier is trained on the entire
training set.
Most of the features used in the validation mod-
ule are the same as those in ranking module, such as
StrSimSurface, EqualWordNumSurface, MissWord-
NumSurface, TFSimContext, AllWordsInSource,
NENumMatch and TypeMatch. We also design
some other features, as follows:
? AllQueryWordsInWikiText. The feature value
is one if Q?s textRetrievalSet contains C, and
in other case the feature value is set to zero.
The case that Q?s textRetrievalSet contains C
means the candidate C?s article contains the Q?s
nameString.
? CountryInTextPer. The feature is the percent-
age of countries from Q?s countrySet exist in
C?s countrySet too. The feature can be seen as
a normalization of CountryInTextMatch/Miss
features in ranking module.
? ScoreOfRank. The feature value is the score
of the candidate given by the ranking module.
The ScoreOfRank takes many features in rank-
ing module into consideration, so only fewer
features of ranking module are used in the clas-
sifier.
488
6 Experiment and Analysis
6.1 Experiment Setting
Algorithm Accuracy Improvement
over SVM
ListNet 0.9045 +18.5%
Ranking Perceptron 0.8842 +15.8%
SVM 0.7636 -
Perceptron 0.7546 -1.2%
Table 1: Evaluation of different ranking algorithm
Entity linking is initiated as a task in this year?s
TAC-KBP3 track, so we use the data from this track.
The entity linking task in the KBP track is to map
an entity mentioned in a news text to the Knowl-
edge Base, which consist of articles from Wikipedia.
The KBP track gives a sample query set which con-
sists of 416 queries for developing. The test set con-
sists of 3904 queries. 2229 of these queries can?t
be mapped to Knowledge Base, for which the sys-
tems should return NIL links. The remaining 1675
queries all can be aligned to Knowledge Base. We
will firstly analyze the ranking methods with those
non-NIL queries, and then with an additional vali-
dation module, we train and test with all queries in-
cluding NIL queries.
As in the entity linking task of KBP track, the ac-
curacy is taken as
accuracy = #(correct answered queries)#(total queries)
6.2 Evaluation of Machine Learning Methods
in ranking
As mentioned in the section of related work, learn-
ing to rank methods in entity linking performs bet-
ter than the classification methods. To justify this,
some experiments are designed to evaluate the per-
formance of our ranking module when adopting dif-
ferent algorithms.
To evaluate the performance of the ranking mod-
ule, we use all the queries which can be aligned to a
target entry in the Knowledge Base. The training set
contains 285 valid queries and the test set contains
1675.
3http://apl.jhu.edu/ paulmac/kbp.html
Set Features in Set
Set1 Surface Features
Set2 Set1+TF-IDF Features
Set3 Set2+AllWordsInSource
Set4 Set3+NENumMatch
Set5 Set4+CountryInTitle Features
Set6 Set5+CountryInText Features
Set7 Set6+CityInTitleMatch
Set8 Set7+MatchType
Table 2: Feature Sets
Three algorithms are taken into comparison: List-
Net, Ranking Perceptron, and classifier based meth-
ods. The classifier based methods are trained by di-
viding the candidates into two classes: target and
non-target. Then, the candidates are ranked accord-
ing to their probability of being classified as target.
two different classifiers are tested here, SVM and
Perceptron.
!
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
Set1 Set2 Set3 Set4 Set5 Set6 Set7 Set8
Ac
cu
ra
cy
Feature)Set
ListNet
Ranking!Perceptron
Figure 3: Comparison of ListNet and Ranking Perceptron
As shown in Table 1, the two learning to rank
methods perform much better than the classification
based methods. The experiment results prove our
point that the learning to rank algorithms are more
suitable in this work. And the ListNet shows slight
improvement over Ranking Perceptron, but since the
improvement is not so significant, maybe it depends
on the feature set. To confirm this, we compare the
two algorithms with different features, as showed
in Table 2. In Figure 3, The ListNet outperforms
Ranking Perceptron with all feature sets except Set1,
which indicates that the listwise approach is more
suitable than the pairwise approach. The pairwise
approach suffers from two problems: first, the ob-
jective of learning is to minimize classification er-
489
Systems Accuracy of all queries Accuracy of non-NIL queries Accuracy of NIL queries
System1 0.8217 0.7654 0.8641
System2 0.8033 0.7725 0.8241
System3 0.7984 0.7063 0.8677
ListNet+SVM 0.8494 0.79 0.8941
Table 3: Evaluation of the overall performance, compared with KBP results (System 1-3 demonstrate the top three
ranked systems)
rors but not to minimize the errors in ranking; sec-
ond, the number of pairs generated from list varies
largely from list to list, which will result in a model
biased toward lists with more objects. The issues are
also discussed in (Y.B. Cao et al, 2006; Cao et al,
2007). And the listwise approach can fix the prob-
lems well.
As the feature sets are added incrementally, it can
be used for analyzing the importance of the features
to the ranking task. Although Surface Group only
takes into consideration the candidate?s title and the
query?s name string, its accuracy is still higher than
60%. This is because many queries have quite small
number of candidates, the target entry can be picked
out with the surface features only. The result shows
that after adding the TF-IDF similarity related fea-
tures, the accuracy increases significantly to 84.5%.
Although TF-IDF similarity is a simple way to mea-
sure the contextual similarity, it performs well in
practice. Another improvement is achieved when
adding the CountryInTitleMatch and CountryInTi-
tleMiss features. Since a number of queries in test
set need to disambiguate candidates with different
countries in their titles, the features about coun-
try in the candidates? title are quite useful to deal
with these queries. But it doesn?t mean that the
features mentioned above are the most important.
Because many features correlated with each other
quite closely, adding these features doesn?t lead to
remarkable improvement. The conclusion from the
results is that the Context Features significantly im-
prove the ranking performance and the Special Fea-
tures are also useful in the entity linking task.
6.3 Overall Performance Evaluation
We are also interested in overall performance with
the additional validation module. We use all the
3904 queries as the test set, including both NIL
and non-NIL queries. The top three results from
the KBP track (McNamee and Dang, 2009) are se-
lected as comparison. The evaluation result in Table
3 shows that our proposed framework outperforms
the best result in the KBP track, which demonstrates
the effectiveness of our methods.
7 Conclusions and Future Work
This paper demonstrates a framework of learning to
rank for linking entities with the Knowledge Base.
Experimenting with different ranking algorithms, it
shows that the learning to rank methods perform
much better than the classification methods in this
problem. ListNet achieves 18.5% improvement over
SVM, and Ranking Perceptron gets 15.8% improve-
ment over SVM. We also observe that the listwise
learning to rank methods are more suitable for this
problem than pairwise methods. We also add a vali-
dation module to deal with those entities which have
no corresponding entry in the Knowledge Base. We
also evaluate the proposed method on the whole data
set given by the KBP track, for which we add a bi-
nary SVM classification module to validate the top
one candidate. The result of experiment shows the
proposed strategy performs better than all the sys-
tems participated in the entity linking task.
In the future, we will try to develop more sophis-
ticated features in entity linking and design a typical
learning to rank method for the entity linking task.
Acknowledgments
This work was partly supported by the Chinese Nat-
ural Science Foundation under grant No.60973104
and No. 60803075, partly carried out with the aid
of a grant from the International Development Re-
search Center, Ottawa, Canada IRCI project from
the International Development.
490
References
Bagga and Baldwin. 1998. Entity-Based Cross-
Document Coreferencing Using the Vector Spcae
Model. in Proceedings of HLT/ACL.
Gideon S. Mann and David Yarowsky. 2003. Unsuper-
vised Personal Name Disambiguation. in Proceedings
of CONIL.
Michael Ben Fleishman. 2004. Multi-Document Person
Name Resolution. in Proceedings of ACL.
Ted Pedersen, Amruta Purandare and Anagha Kulkarni.
2005. Name Discrimination by Clustering Similar
Contexts. in Proceedings of CICLing.
B.Malin and E. Airoldi. 2005. A Network Analysis
Model for Disambiguation of Names in Lists. in Pro-
ceedings of CMOT.
Einat Minkov, William W. Cohen and Andrew Y. Ng.
2006. Contextual Search and Name Disambiguation
in Email Using Graph. in Proceedings of SIGIR.
Ron Bekkerman and Andrew McCallum. 2005. Disam-
biguating Web Appearances of People in a Social Net-
work. in Proceedings of WWW.
Xianpei Han and Jun Zhao. 2009. Named Entity Disam-
biguation by Leveraging Wikipedia Semantic Knowl-
edge. in Proceedings of CIKM.
David Milne and Ian H. Witten. 2008. Learning to Link
with Wikipedia. in Proceedings of CIKM.
Herbrich, R., Graepel, T. and Obermayer K. 1999. Sup-
port vector learning for ordinal regression. in Pro-
ceedings of ICANN.
Freund, Y., Iyer, R., Schapire, R. E. and Singer, Y. 1998.
An efficient boosting algorithm for combining prefer-
ences. in Proceedings of ICML.
Burges, C., Shaked, T., Renshaw, E., Lazier, A., Deeds,
M., Hamilton, N. and Hullender, G. 2005. Learning to
rank using gradient descent. in Proceedings of ICML.
Cao, Y. B., Xu, J., Liu, T. Y., Li, H., Huang, Y. L. and
Hon, H. W. 2006. Adapting ranking SVM to document
retrieval. in Proceedings of SIGIR.
Cao, Z., Qin, T., Liu, T. Y., Tsai, M. F. and Li, H. 2007.
Learning to rank: From pairwise approach to listwise
approach. in Proceedings of ICML.
Qin, T., Zhang, X.-D., Tsai, M.-F., Wang, D.-S., Liu,
T.Y., and Li, H. 2007. Query-level loss functions for
information retrieval. in Proceedings of Information
processing and management.
L. Shen and A. Joshi. 2005. Ranking and Reranking with
Perceptron. Machine Learning,60(1-3),pp. 73-96.
Silviu Cucerzan. 2007. Large-Scale Named Entity Dis-
ambiguation Based on Wikipedia Data. in Proceed-
ings of EMNLP-CoNLL.
Razvan Bunescu and Marius Pasca. 2006. Using En-
cyclopedic Knowledge for Named Entity Disambigua-
tion. in Proceedings of EACL.
Paul McNamee and Hoa Dang. 2009. Overview
of the TAC 2009 Knowledge Base Population Track
(DRAFT). in Proceedings of TAC.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating Non-local Information
into Information Extraction Systems by Gibbs Sam-
pling. Proceedings of the 43nd Annual Meeting of
the Association for Computational Linguistics (ACL
2005), pp. 363-370.
491
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 410?419,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Cross-Domain Co-Extraction of Sentiment and Topic Lexicons
Fangtao Li?, Sinno Jialin Pan?, Ou Jin?, Qiang Yang? and Xiaoyan Zhu?
?Department of Computer Science and Technology, Tsinghua University, Beijing, China
?{fangtao06@gmail.com, zxy-dcs@tsinghua.edu.cn}
?Institute for Infocomm Research, Singapore
?jspan@i2r.a-star.edu.sg
?Hong Kong University of Science and Technology, Hong Kong, China
?{kingomiga@gmail.com, qyang@cse.ust.hk}
Abstract
Extracting sentiment and topic lexicons is im-
portant for opinion mining. Previous works
have showed that supervised learning methods
are superior for this task. However, the perfor-
mance of supervised methods highly relies on
manually labeled training data. In this paper,
we propose a domain adaptation framework
for sentiment- and topic- lexicon co-extraction
in a domain of interest where we do not re-
quire any labeled data, but have lots of labeled
data in another related domain. The frame-
work is twofold. In the first step, we gener-
ate a few high-confidence sentiment and topic
seeds in the target domain. In the second
step, we propose a novel Relational Adaptive
bootstraPping (RAP) algorithm to expand the
seeds in the target domain by exploiting the
labeled source domain data and the relation-
ships between topic and sentiment words. Ex-
perimental results show that our domain adap-
tation framework can extract precise lexicons
in the target domain without any annotation.
1 Introduction
In the past few years, opinion mining and senti-
ment analysis have attracted much attention in Natu-
ral Language Processing (NLP) and Information Re-
trieval (IR) (Pang and Lee, 2008; Liu, 2010). Senti-
ment lexicon construction and topic lexicon extrac-
tion are two fundamental subtasks for opinion min-
ing (Qiu et al, 2009). A sentiment lexicon is a list
of sentiment expressions, which are used to indicate
sentiment polarity (e.g., positive or negative). The
sentiment lexicon is domain dependent as users may
use different sentiment words to express their opin-
ion in different domains (e.g., different products). A
topic lexicon is a list of topic expressions, on which
the sentiment words are expressed. Extracting the
topic lexicon from a specific domain is important
because users not only care about the overall senti-
ment polarity of a review but also care about which
aspects are mentioned in review. Note that, similar
to sentiment lexicons, different domains may have
very different topic lexicons.
Recently, Jin and Ho (2009) and Li et al (2010a)
showed that supervised learning methods can
achieve state-of-the-art results for lexicon extrac-
tion. However, the performance of these meth-
ods highly relies on manually annotated training
data. In most cases, the labeling work may be time-
consuming and expensive. It is impossible to anno-
tate each domain of interest to build precise domain-
dependent lexicons. It is more desirable to automat-
ically construct precise lexicons in domains of inter-
est by transferring knowledge from other domains.
In this paper, we focus on the co-extraction task
of sentiment and topic lexicons in a target domain
where we do not have any labeled data, but have
plenty of labeled data in a source domain. Our
goal is to leverage the knowledge extracted from the
source domain to help lexicon co-extraction in the
target domain. To address this problem, we propose
a two-stage domain adaptation method. In the first
step, we build a bridge between the source and tar-
get domains by identifying some common sentiment
words as sentiment seeds in the target domain, such
as ?good?, ?bad?, ?nice?, etc. After that, we gener-
ate topic seeds in the target domain by mining some
general syntactic relation patterns between the sen-
timent and topic words from the source domain. In
the second step, we propose a Relational Adaptive
bootstraPping (RAP) algorithm to expand the seeds
in the target domain. Our proposed method can uti-
410
lize useful labeled data from the source domain as
well as exploit the relationships between the topic
and sentiment words to propagate information for
lexicon construction in the target domain. Experi-
mental results show that our proposed method is ef-
fective for cross-domain lexicon co-extraction.
In summary, we have three main contributions: 1)
We give a systematic study on cross-domain senti-
ment analysis in word level. While, most of previous
work focused on document level; 2) A new two-step
domain adaptation framework, with a novel RAP al-
gorithm for seed expansion, is proposed. 3) We con-
duct extensive evaluation, and the experimental re-
sults demonstrate the effectiveness of our methods.
2 Related Work
2.1 Sentiment or Topic Lexicon Extraction
Sentiment or topic lexicon extraction is to iden-
tify the sentiment or topic words from text. In the
past, many machine learning techniques have been
proposed for this task. Hu and Liu et al (2004)
proposed an association-rule-based method to ex-
tract topic words and a dictionary-based method to
identify sentiment words, independently. Wiebe et
al. (2004) and Rioff et al (2003) proposed to
identify subjective adjectives and nouns using word
clustering based on their distributional similarity.
Popescu and Etzioni (2005) proposed a relaxed la-
beling approach to utilize linguistic rules for opinion
polarity detection. Some researchers also proposed
to use topic modeling to identify implicit topics and
sentiment words (Mei et al, 2007; Titov and Mc-
Donald, 2008; Zhao et al, 2010; Li et al, 2010b),
where a topic is a cluster of words, which is differ-
ent from our fine-grained topic-word extraction.
Jin and Ho (2009) and Li et al (2010a) both pro-
posed to use supervised sequential labeling methods
for topic and opinion extraction. Experimental re-
sults showed that the supervised learning methods
can achieve state-of-the-art performance on lexicon
extraction. However, these methods need to manu-
ally annotate a lot of training data in each domain.
Recently, Qiu et al (2009) proposed a rule-based
semi-supervised learning methods for lexicon ex-
traction. However, their method requires to manu-
ally define some general syntactic rules among sen-
timent and topic words. In addition, it still requires
some annotated words in the target domain. In this
paper, we do not assume any predefined rules and
labeled data be available in the target domain.
2.2 Domain Adaptation
Domain adaptation aims at transferring knowledge
across domains where data distributions may be dif-
ferent (Pan and Yang, 2010). In the past few years,
domain adaptation techniques have been widely ap-
plied to various NLP tasks, such as part-of-speech
tagging (Ando and Zhang, 2005; Jiang and Zhai,
2007; Daume? III, 2007), named-entity recognition
and shallow parsing (Daume? III, 2007; Jiang and
Zhai, 2007; Wu et al, 2009). There are also
lots of studies for cross-domain sentiment analy-
sis (Blitzer et al, 2007; Tan et al, 2007; Li et al,
2009; Pan et al, 2010; Bollegala et al, 2011; He
et al, 2011; Glorot et al, 2011). However, most
of them focused on coarse-grained document-level
sentiment classification, which is different from our
fine-grained word-level extraction. Our work is sim-
ilar to Jakob and Gurevych (2010) which proposed a
Conditional Random Field (CRF) for cross-domain
topic word extraction. However, the performance
of their method highly depends on the manually de-
signed features. In our experiments, we compare our
method with theirs, and find that ours can achieve
much better results on cross-domain lexicon extrac-
tion. Note that our work is also different from a re-
cent work (Du et al, 2010), which focused on identi-
fying the polarity of adjective words by using cross-
domain knowledge. While we extract both topic and
sentiment words and allow non-adjective sentiment
words, which is more practical.
3 Cross-Domain Lexicon Co-Extraction
3.1 Problem Definition
Recall that, we focus on the setting where we have
no labeled data in the target domain, while we have
plenty of labeled data in the source domain. De-
note DS = {(wSi , ySi)}n1i=1 the source domain data,
where wSi represents a word in the source domain.
ySi ? Y is the corresponding label of wSi . Simi-
larly, we denote DT = {wTj}n2j=1 the target domain
data, where the input wTj is a word in the target do-
main. In lexicon extraction, Y ? {1, 2, 3}, where
yi = 1 denotes the corresponding word wi a sen-
timent word, yi = 2 denotes wi a topic word, and
yi = 3 denotes wi neither a sentiment nor topic
word. Our goal is to predict labels on DT to extract
topic and sentiment words for constructing topic and
411
sentiment lexicons, respectively.
3.2 Motivating Examples
In this section, we use some examples to introduce
the motivation behind our proposed method. Table 1
shows several reviews from two domains: movie and
camera. From the table, we can observe that there
are some common sentiment words across different
domains, such as ?great?, ?excellent? and ?amaz-
ing?. However, the topic words may be different.
For example, in the movie domain, topic words in-
clude ?movie? and ?script?. While in the camera do-
main, topic words include ?camera? and ?photos?.
Domain Review
camera
The camera is great.
it is a very amazing product.
i highly recommend this camera.
takes excellent photos.
photos had some artifacts and noise.
movie
This movie has good script, great
casting, excellent acting.
I love this movie.
Godfather was the most amazing movie.
The movie is excellent.
Table 1: Reviews in camera and movie domains. Bold-
faces are topic words and Italics are sentiment words.
Based on the observations, we can build a connec-
tion between the source and target domains by iden-
tifying the common sentiment words. Furthermore,
intuitively, there are some general syntactic relation-
ships or patterns between topic and sentiment words
across different domains. Therefore, if we can mine
the patterns from the source and target domain data,
then we are able to construct an indirect connection
between topic words across domains by using the
common sentiment words as a bridge, which makes
knowledge transfer across domains possible.
Figure 1 shows two dependency trees for the sen-
tence ?the camera is great? in the camera domain
and the sentence ?the movie is excellent? in the
movie domain, respectively. As can be observed, the
relationships between the topic and sentiment words
in the two sentences are the same. They both share
a ?TOPIC-nsubj-SENTIMENT? relation. Let the
camera domain be the source domain and the movie
domain be the target domain. If the word ?excel-
lent? is identified as a common sentiment word, and
the ?TOPIC-nsubj-SENTIMENT? relation extracted
from the camera domain is recognized as a common
syntactic pattern, then the word ?movie? can be pre-
dicted as a topic word in the movie domain with high
probability. After new topic words are extracted in
the movie domain, we can apply the same syntac-
tic pattern or other syntactic patterns to extract new
sentiment and topic words iteratively.
great
camera is
The
nsubj cop
det
(a) Camera domain.
excellent
movie is
The
nsubj cop
det
(b) Movie domain.
Figure 1: Examples of dependency tree structure.
More specifically, we use the shortest path be-
tween a topic word and a sentiment word in the cor-
responding dependency tree to denote the relation
between them. To get more general paths, we do
not take original words in the path into considera-
tion, but use their POS tags instead, such as ?NN?,
?VB?, ?JJ?, etc. As an example shown in Figure 2,
we can extract two paths or relationships between
topic and sentiment words from the dependency tree
of the sentence ?The movie has good script?: ?NN-
amod-JJ? from ?script? and ?good?, and ?NN-nsubj-
VB-dobj-NN-amod-JJ? from ?movie? and ?good?.
has(VB)
script(NN)
the(DT)
movie(NN)
good(JJ)
dobj nsubj
amod det
Figure 2: Example of pattern extraction.
In the following sections, we present the proposed
two-stage domain adaptation framework: 1) gener-
ating some sentiment and topic seeds in the target
domain; and 2) expanding the seeds in the target do-
main to construct sentiment and topic lexicons.
4 Seed Generation
Our basic idea is to first identify several common
sentiment words across domains as sentiment seeds.
Meanwhile, we mine some general patterns between
sentiment and topic words from the source domain.
Finally, we use the sentiment seeds and general pat-
terns to generate topic seeds in the target domain.
412
4.1 Sentiment Seed Generation
To identify common sentiment words across do-
mains, we extract all sentiment words from the
source domain as candidates. For each candidate,
we calculate its score based on the following metric:
S1(wi) = (pS(wi) + pT (wi)) e(?|pS(wi)?pT (wi)|), (1)
where pS(wi) and pT (wi) are the probabilities of the
word wi occurring in the source and target domains,
respectively. If a word wi has high S1 score, which
implies that the word wi occurs frequently and simi-
larly in both domains, then it can be considered as a
common sentiment word (Pan et al, 2010; Blitzer et
al., 2007). We select top r candidates with highest
S1 scores as sentiment seeds.
4.2 Topic Seed Generation
We extract all patterns between sentiment and topic
words in the source domain as candidates. For each
pattern candidate, we calculate its score based on a
metric defined in AutoSlog-TS (Riloff, 1996):
S2(Rj) = Acc(Rj)? log2(Freq(Rj)), (2)
where Acc(Rj) is the accuracy of the pattern Rj in
the source domain, and Freq(Rj) is the frequency
of the pattern Rj observed in target domain. This
metric aims to identify the patterns that are precise
in the source domain and observed frequently in the
target domain. We also select the top r patterns
with highest S2 scores. With the patterns and sen-
timent seeds, we extract topic-word candidates and
measure their scores based on a variant metric of
quadratic combination (Zhang and Ye, 2008):
S3(wk) =
?
Rj?A, wi?B
(S2(Rj)? S1(wi)) , (3)
where B is a set of sentiment seeds and A is a set of
patterns which the words wi and wk satisfy. We then
select the top r candidates as topic seeds.
5 Seed Expansion
After generating the topic and sentiment seeds, we
aim to expand them in the target domain to construct
topic and sentiment lexicons. In this section, we pro-
pose a new bootstrapping-based method to address
this problem.
Bootstrapping is the process of improving the per-
formance of a weak classifier by iteratively adding
training data and retraining the classifier. More
specifically, bootstrapping starts with a small set
of labeled ?seeds?, and iteratively adds unlabeled
data that are labeled by the classifier to the train-
ing set based on some selection criterion, and retrain
the classifier. Many bootstrapping-based algorithms
have been proposed to information extraction and
other NLP tasks (Blum and Mitchell, 1998; Riloff
and Jones, 1999; Jones et al, 1999; Wu et al, 2009).
One important issue in bootstrapping is how to
design a criterion to select unlabeled data to be
added to the training set iteratively. Our proposed
bootstrapping for cross-domain lexicon extraction
is based on the following two observations: 1) Al-
though the source and target domains are different,
part of source domain labeled data is still useful for
lexicon extraction in the target domain after some
adaptation; 2) The syntactic relationships among
sentiment and topic words can be used to expand the
seeds in the target domain for lexicon construction.
Based on the two observations, we propose a
new bootstrapping-based method named Relational
Adaptive bootstraPping (RAP), as summarized in
Algorithm 1, for expanding lexicons across do-
mains. In each iteration, we employ a cross-domain
classifier trained on the source domain lexicons and
the extracted target domain lexicons to predict the
labels of the target unlabeled data, and select top k2
predicted topic and sentiment words as candidates
based on confidence. With the extracted syntactic
patterns in the previous iterations, we construct a
bipartite graph between sentiment and topic words
on the extracted target domain lexicons and candi-
dates. After that, a graph-based score refinement al-
gorithm is performed on the graph, and the top k1
candidates are added to the extracted lexicons based
on the final scores. Accordingly, with the new ex-
tracted lexicons, we update the syntactic patterns in
each iteration. The details of RAP are presented in
the following sections.
5.1 Cross-Domain Classifier
In this paper, we employ Transfer AdaBoost (TrAd-
aBoost) (Dai et al, 2007) as the cross-domain learn-
ing algorithm in RAP. In TrAdaBoost, each word
wSi (or wTj ) is represented by a feature vector xSi
(or xTj ). A classifier trained on the source domain
data DS = {(xSi , ySi)} may perform poor on xTj
because of domain difference. The main idea of
TrAdaBoost is to re-weight the source domain data
based on a few of target domain labeled data, which
is referred to as seeds in our task. The re-weighting
413
aims to reduce the effect of the ?bad? source do-
main data while encourage the ?good? ones to get
a more precise classifier in target domain. In each
iteration of RAP, we train cross-domain classifiers
fTO and fTP for sentiment- and topic- word extrac-
tion using TrAdaBoost separately (taking sentiment
or topic words as positive instances). We use linear
Support Vector Machines (SVMs) as the base clas-
sifier in TrAdaBoost. For features to represent each
word, we use lexicon features, such as the previous,
current and next words, and POS tag features, such
as the previous, current and next words? POS tags.
Algorithm 1 Relational Adaptive bootstraPping
Require: Target domain data DT = DlT
?DuT , where DlT
consists of sentiment seeds B and topic seeds C and their
initial scores S1(wi), ?wi ? B and S3(wj), ?wj ? C, DuT
is the set of unlabeled target domain data; labeled source
domain data DS ; a cross-domain classifier; iteration num-
ber M and candidate selection number k1, k2.
Ensure: Expand C and B in the target domain.
1: Initialize a pattern set A = ?, S?1(wi) = S1(wi), wi ? B
and S?3(wj) = S3(wj), wj ? C. Consider all patterns
observed in the source domain as pattern candidates P .
2: for m = 1 . . .M do
3: Extract new pattern candidates to P with DlT in target
domain, update pattern score S?2(Rj), where Rj ? P ,
based on Eq. (4), and select the top k1 patterns to the
pattern set A.
4: Learn the cross-domain classifiers fTO and fTP for
sentiment- and topic- word extraction with DS
?DlT
separately. Predict the sentiment score hTfO (wTj ) and
topic score hTfP (wTj ) on D
u
T , and select k2 sentiment
words and topic words with highest scores as candidates.
5: Construct a bipartite graph between sentiment and topic
words on DlT and the k2 sentiment- and topic- word can-
didates, and calculate the normalized weights ?ij?s for
each edge of the graph.
6: Refine the scores S?1 and S?3 of the k2 sentiment and
topic word candidates using Eqs. (5) and (6) iteratively.
7: Select k1 new sentiment words and k1 new topic words
with the final scores, and add them to lexicons B and C.
Update S?1(wi) and S?3(wj) accordingly.
8: end for
9: return Expanded lexicons B and C.
5.2 Graph Construction
Based on the cross-domain classifiers fTO and fTP ,
we can predict the sentiment label score hTfO(wTi)
and topic label score hTfP (wTi) for the target domain
data wTi . According to all predicted values, we re-
spectively select top k2 new sentiment- and topic-
words as candidates. Together with the extracted
sentiment and topic lexicons in the target domain,
we build a bipartite graph among them as shown in
Figure 3. In the bipartite graph, one set of nodes
represents topic words, including new topic candi-
dates and words in the lexicon C, and the other set
of nodes represents sentiment words, including new
sentiment candidates and words in the lexicon B.
For a pair of sentiment and topic words wOTi and w
P
Tj ,
if there is a pattern Rj in the pattern set A that they
can satisfy, then there exists an edge eij between
them. Furthermore, each edge eij is associated with
a nonnegative weight ?ij , which is measured as fol-
lows, ?ij =
?
Rk?E S?2(Rk), where S?2 is the pattern
score. Similar to the metric defined in Eq. (3), the
pattern score is defined as:
S?2(Rj) =
?
{wi,wk}?E
(
S?1(wi)? S?3(wk)
)
, (4)
where E = {{wi, wj}|, wi ? B,wj ? C and
wi, wj satisfy Rj , Rj ? A}. Note that in the be-
ginning of each iteration, S?2 is updated based on the
new sentiment score S?1 and topic score S?3. We fur-
ther normalize ?ij by ??ij = ?ij/(
?
ij ?ij).
Topic words Sentiment words
music
movie
recommend
good
boring
script
NN-nsubj-VB-dobj-NN-amod-JJ
NN-amod-JJ
NN-nsubj-JJ
NN-amod-JJ
NN-d
obj-V
B
Figure 3: Topic and sentiment word graph.
5.3 Score Computation
We construct the bipartite graph to exploit the re-
lationships between sentiment and topic words to
propagate information for lexicon extraction. We
use the following reinforcement formulas to itera-
tively update the final sentiment score S?1(wTj ) and
topic score S?3(wTi), respectively:
S?1(wTj ) = ?
?
i
S?3(wTi)??ij + (1? ?)hTfO (wTj ), (5)
S?3(wTi) = ?
?
j
S?1(wTj )??ij + (1? ?)hTfP (wTi), (6)
where ? is a trade-off parameter between the pre-
dicted value by cross-domain classifier and the re-
inforcement scores from other nodes connected by
414
edge eij . Here ? is empirically set to be 0.5. With
Eqs. (5) and (6), the sentiment scores and topic
scores are iteratively refined until the state of the
graph trends to be stable. This can be considered
as an extension to the HITS algorithm(Kleinberg,
1999). Finally, we select k1 ? k2 sentiment and
topic words from the k2 candidates based on their
refined scores, and add them to the target domain
lexicons, respectively. We also update the sentiment
score S?1 and topic score S?3 for next iteration.
5.4 Special Cases
We now introduce two special cases of the RAP al-
gorithm. In Eqs. (5) and (6), if the parameter ? = 1,
then RAP only uses the relationships between sen-
timent and topic words with their patterns to propa-
gate label information in the target domain without
using the cross-domain classifier. We call this reduc-
tion relational bootstrapping. If ? = 0, then RAP
only utilizes useful source domain labeled data to as-
sist learning of the target domain classifier without
considering the relationships between sentiment and
topic words. We call this reduction adaptive boot-
strapping, which can be considered as a bootstrap-
ping version of TrAdaBoost. We also empirically
study these two special cases in experiments.
6 Experiments on Lexicon Evaluation
6.1 Data Set and Evaluation Criteria
We use the review dataset from (Li et al, 2010a),
which contains 500 movie and 601 product reviews,
for evaluation. The sentiment and topic words are
manually annotated. In this dataset, all types of
sentiment words are annotated instead of adjective
words only. For example, the verbs, such as ?like?,
?recommend?, and nouns, such as ?masterpiece?,
are also labeled as sentiment words. We construct
two cross-domain lexicon extraction tasks: ?prod-
uct vs. movie? and ?movie vs. product?, where the
word before ?vs.? corresponds with the source do-
main and the word after ?vs.? corresponds with the
target domain. We evaluate our methods in terms of
precision, recall and F-score (F1).
6.2 Baselines
The results of in-domain classifiers, which are
trained on plenty of target domain labeled data, can
be treated as upper-bounds. We denote iSVM and
iCRF the in-domain SVM and CRF classifiers in
experiments, and compare our proposed methods,
RAP, relational bootstrapping, and adaptive boot-
strapping, with the following baselines,
Unsupervised Method (Un) we implement a rule-
based method for lexicon extraction based on (Hu
and Liu, 2004), where adjective words that match
a rule is recognized as sentiment words, and nouns
that match a rule are recognized as topic words.
Semi-Supervised Method (Semi) we implement
the double propagation model proposed in (Qiu et
al., 2009). Since this method requires some target
domain labeled data, we manually label 30 senti-
ment words in the target domain.
Cross-Domain CRF (Cross-CRF) we implement
a cross-domain CRF algorithm proposed by (Jakob
and Gurevych, 2010).
TrAdaBoost We apply TrAdaBoost (Dai et al,
2007) on the source domain labeled data and the
generated seeds in the target domain to train a lexi-
con extractor.
6.3 Comparison Results
Comparison results on lexicon extraction are shown
in Table 2 and Table 3. From Table 2, we can ob-
serve that our proposed methods are effective for
sentiment lexicon extraction. The relational boot-
strapping method performs better than the unsuper-
vised method, TrAdaBoost and the cross-domain
CRF algorithm, and achieves comparable results
with the semi-supervised method. However, com-
pared to the semi-supervised method, our proposed
relational bootstrapping method does not require any
labeled data in the target domain. We can also ob-
serve that the adaptive bootstrapping method and the
RAP method perform much better than other meth-
ods in terms of F-score. The reason is that part of
the source domain labeled data may be useful for
learning the target classifier after reweighting. In
addition, we also observe that embedding the TrAd-
aBoost algorithm into a bootstrapping process can
further boost the performance of the classifier for
sentiment lexicon extraction.
Table 3 shows the comparison results on topic lex-
icon extraction. From the table, we can observe that
different from the sentiment lexicon extraction task,
the relational bootstrapping method performs better
than the adaptive bootstrapping method slightly. The
reason may be that for the sentiment lexicon extrac-
tion task, there exist some common sentiment words
415
product vs. movie movie vs. product
Prec. Rec. F1 Prec. Rec. F1
Un 0.82 0.31 0.45 0.74 0.23 0.35
Semi 0.71 0.44 0.54 0.62 0.45 0.52
Cross-CRF 0.69 0.40 0.51 0.65 0.34 0.45
Tradaboost 0.73 0.41 0.52 0.72 0.42 0.52
Adaptive 0.68 0.53 0.59 0.63 0.52 0.57
Relational 0.55 0.51 0.53 0.57 0.51 0.54
RAP 0.69 0.59 0.64 0.66 0.59 0.62
iSVM 0.82 0.60 0.70 0.80 0.61 0.68
iCRF 0.80 0.66 0.72 0.80 0.62 0.69
Table 2: Results on sentiment lexicon extraction. Num-
bers in boldface denote significant improvement.
product vs. movie movie vs. product
Prec. Rec. F1 Prec. Rec. F1
Un 0.41 0.32 0.36 0.53 0.35 0.41
Semi 0.54 0.59 0.56 0.75 0.50 0.60
Cross-CRF 0.70 0.23 0.34 0.80 0.24 0.37
Tradaboost 0.64 0.45 0.53 0.57 0.47 0.51
Adaptive 0.76 0.44 0.56 0.70 0.52 0.59
Relational 0.57 0.58 0.58 0.61 0.57 0.59
RAP 0.80 0.56 0.66 0.73 0.58 0.65
iSVM 0.83 0.73 0.78 0.85 0.70 0.77
iCRF 0.84 0.78 0.81 0.87 0.73 0.80
Table 3: Results on topic lexicon extraction. Numbers in
boldface denote significant improvement.
across domains, thus part of the labeled source do-
main data may be useful for the target learning task.
However, for the topic lexicon extraction task, the
topic words may be totally different, and as a result,
we may not be able to find useful source domain
labeled data to boost the performance for lexicon
extraction in the target domain. In this case, mu-
tual label propagation between sentiment and topic
words may be more reasonable for knowledge trans-
fer. RAP absorbs the advantages of the adaptive and
relational bootstrapping methods, thus can get the
best results in both lexicon extraction tasks.
We also observe that relational bootstrapping can
get better recall, but lower precision, compared to
adaptive bootstrapping. This is because relational
bootstrapping only utilizes the patterns to propagate
label information, which may cover more topic and
sentiment seeds, but include some noisy words. For
example, given two phases ?like the camera? and
?recommend the camera?, we can extract a pattern
?VB-dobj-NN?. However, by using this pattern and
the topic word ?camera?, we may extract ?take? as
a sentiment word from another phase ?take the cam-
era?, which is incorrect. The adaptive bootstrapping
method can utilize various features to make predic-
tions more precisely, which may have higher preci-
sion, but encounter the lower recall problem. For ex-
ample, ?flash? is not identified as a topic word in the
target product domain (camera domain). Our RAP
method can exploit both relationships between sen-
timent and topic words and part of labeled source
domain data for cross-domain lexicon extraction. It
can correctly identify the above two cases.
6.3.1 Parameter Sensitivity Study
In this section, we conduct experiments to study
the effect of different parameter settings. There are
several parameters in the framework: the number
of generated seeds r, the number of new candidates
k2 and the number of selections k in each iteration,
and the number of iterations M (? is empirically set
to 0.5 ). For the parameter k2, we just set it to a
large number (k2 = 100) such that have rich candi-
dates to build the bipartite graph. In the experiments
reported in the previous section, we set r = 20,
k1 = 10 and M = 50. Figures 4(a) and 4(b) show
the results under varying values of r in the ?product
vs. movie? task. Observe that for sentiment word
extraction, the results of the proposed methods are
not sensitive to the values of r. While for the topic
word extraction, the proposed methods perform well
when r falls in the range from 15 to 20.
5 10 15 20 25 30
0.45
0.5
0.55
0.6
0.65
0.7
Values of r
F?
sc
or
e
 
 
Relational
Adaptive
RAP
(a) Sentiment word extraction
5 10 15 20 25 30
0.45
0.5
0.55
0.6
0.65
0.7
Values of r
F?
sc
or
e
 
 
Relational
Adaptive
RAP
(b) Topic word extraction
Figure 4: Results on varying values of r.
0 10 20 30 40 50
0.4
0.45
0.5
0.55
0.6
0.65
Number of iterations
F?
sc
or
e
 
 
Relational
Adaptive
RAP
(a) Sentiment word extraction
0 10 20 30 40 50
0.4
0.45
0.5
0.55
0.6
0.65
0.7
Number of iterations
F?
sc
or
e
 
 
Relational
Adaptive
RAP
(b) Topic word extraction
Figure 5: Results on varying values of M .
We also test the sensitivity of the parameter k1
and find that the proposed methods work well and
robust when k1 falls in the range from 10 to 20.
416
Figures 5(a) and 5(b) show the results under vary-
ing numbers of iterations in the ?product vs. movie?
task. As we can see, our proposed methods converge
well when M ? 40.
7 Application: Sentiment Classification
To further verify the usefulness of the lexicons ex-
tracted by the RAP method, we apply the extracted
sentiment lexicon for sentiment classification.
7.1 Experiment Setting
Our work is motivated by the work of (Pang and
Lee, 2004), which only used subjective sentences
for document-level sentiment classification, instead
of using all sentences. In this experiment, we only
use sentiment related words as features to represent
opinion documents for classification, instead of us-
ing all words. Our goal is compare the sentiment
lexicon constructed by the RAP method with other
general lexicons on the impact of for sentiment clas-
sification. The general lexicons used for comparison
are described in Table 4.
We use the dataset from (Blitzer et al, 2007) for
sentiment classification. It contains a collection of
product reviews from Amazon.com. The reviews are
about four product domains: books, dvds, electron-
ics and kitchen appliance. In each domain, there are
1000 positive and 1000 negative reviews. To con-
struct domain specific sentiment lexicons, we apply
RAP on each product domain with the movie domain
described in Section 6.1 as the source domain. Fi-
nally, we use linear SVM as the classifier and the
classification accuracy as the evaluate criterion.
Lexicon Name Size Description
Senti-WordNet 6957 Words with a subjective score > 0.6
(Esuli and Sebastiani, 2006)
HowNet 4619 Eng. translation of subj. Chinese
words (Dong and Dong, 2006)
Subj. Clues 6878 Lexicons from (Wilson et al, 2005)
Table 4: Description of different lexicons.
7.2 Experimental Results
Experimental results on sentiment classification are
shown in Table 5, where we denote ?All? using all
unigram and bigram features instead of using sub-
jective words. As we can see that a classifier trained
with features constructed by our RAP method per-
formance best in all domains. Note that the num-
ber of features (sentiment words) constructed by our
method is much smaller than that of all unigram
and bigram features, which can reduce the classi-
fier training time dramatically. These promising re-
sults imply that our RAP can be applied for senti-
ment classification effectively and efficiently.
All Senti HowNet Subj. Clue Ours
dvd 82.55 79.80 80.57 80.93 84.05
book 80.71 76.22 78.22 79.48 81.65
electronic 84.43 82.42 83.05 83.22 86.71
kitchen 87.70 81.78 84.17 84.23 88.83
Table 5: Sentiment classification results (accuracy in %).
Numbers in boldface denotes significant improvement.
8 Conclusions
In this paper, we propose a two-stage framework for
co-extraction of sentiment and topic lexicons across
domains where we have no labeled data in the tar-
get domain but have plenty of labeled data in an-
other domain. In the first stage, we propose a sim-
ple strategy to generate a few high-quality sentiment
and topic seeds for the target domain. In the second
stage, we propose a novel Relational Adaptive boot-
straPping (RAP) method to expand the seeds, which
can exploit the relationships between topic and opin-
ion words, and make use of part of useful source do-
main labeled data for help. Extensive experimental
results show our proposed method can extract pre-
cise sentiment and topic lexicons from the target do-
main. Furthermore, the extracted sentiment lexicon
can be applied to sentiment classification effectively.
In the future work, besides the heterogeneous
relationships between topic and sentiment words,
we intend to investigate the homogeneous relation-
ships among topic words and those among sentiment
words (Qiu et al, 2009) to further boost the perfor-
mance of RAP method. Furthermore, in our frame-
work, we do not identify the polarity of the extracted
sentiment lexicon. We also plan to embed this com-
ponent into our unified framework. Finally, it is also
interesting to exploit multi-domain knowledge (Li
and Zong, 2008; Bollegala et al, 2011) for cross-
domain lexicon extraction.
9 Acknowledgement
This work was supported by the Chinese Natu-
ral Science Foundation No.60973104, National Key
Basic Research Program 2012CB316301, and Hong
Kong RGC GRF Projects 621010 and 621211.
417
References
Rie K. Ando and Tong Zhang. 2005. A framework for
learning predictive structures from multiple tasks and
unlabeled data. J. Mach. Learn. Res., 6:1817?1853.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classification. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 432?439,
Prague, Czech Republic. ACL.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proceed-
ings of the 11th Annual Conference on Computational
Learning Theory, pages 92?100.
Danushka Bollegala, David Weir, and John Carroll.
2011. Using multiple sources to construct a sentiment
sensitive thesaurus for cross-domain sentiment clas-
sification. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 132?141, Port-
land, Oregon. ACL.
Wenyuan Dai, Qiang Yang, Guirong Xue, and Yong Yu.
2007. Boosting for transfer learning. In Proceed-
ings of the 24th International Conference on Machine
Learning, pages 193?200, Corvalis, Oregon, USA,
June. ACM.
Hal Daume? III. 2007. Frustratingly easy domain adapta-
tion. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 256?
263, Prague, Czech Republic. ACL.
Zhendong Dong and Qiang Dong, editors. 2006.
HOWNET and the computation of meaning. World
Scientific Publishers, Norwell, MA, USA.
Weifu Du, Songbo Tan, Xueqi Cheng, and Xiaochun
Yun. 2010. Adapting information bottleneck method
for automatic construction of domain-oriented senti-
ment lexicon. In Proceedings of the 3rd ACM inter-
national conference on Web search and data mining,
pages 111?120, New York, NY, USA. ACM.
Andrea Esuli and Fabrizio Sebastiani. 2006. SENTI-
WORDNET: A publicly available lexical resource for
opinion mining. In In Proceedings of the 5th Confer-
ence on Language Resources and Evaluation, pages
417?422.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classification: A deep learning approach. In Pro-
ceedings of the 28th International Conference on Ma-
chine Learning, pages 513?520, Bellevue, Washing-
ton, USA.
Yulan He, Chenghua Lin, and Harith Alani. 2011. Auto-
matically extracting polarity-bearing topics for cross-
domain sentiment classification. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 123?131, Portland, Oregon. ACL.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 168?177, Seat-
tle, WA, USA. ACM.
Niklas Jakob and Iryna Gurevych. 2010. Extracting
opinion targets in a single- and cross-domain setting
with conditional random fields. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1035?1045, Cambridge,
Massachusetts, USA. ACL.
Jing Jiang and ChengXiang Zhai. 2007. Instance weight-
ing for domain adaptation in NLP. In Proceedings of
the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 264?271, Prague, Czech
Republic. ACL.
Wei Jin and Hung Hay Ho. 2009. A novel lexical-
ized HMM-based learning framework for web opinion
mining. In Proceedings of the 26th Annual Interna-
tional Conference on Machine Learning, pages 465?
472, Montreal, Quebec, Canada. ACM.
Rosie Jones, Andrew Mccallum, Kamal Nigam, and
Ellen Riloff. 1999. Bootstrapping for text learning
tasks. In In IJCAI-99 Workshop on Text Mining: Foun-
dations, Techniques and Applications, pages 52?63.
Jon M. Kleinberg. 1999. Authoritative sources in a hy-
perlinked environment. J. ACM, 46:604?632, Sept.
Shoushan Li and Chengqing Zong. 2008. Multi-domain
sentiment classification. In Proceedings of the 46th
Annual Meeting of the Association for Computational
Linguistics on Human Language Technologies: Short
Papers, pages 257?260, Columbus, Ohio, USA. ACL.
Tao Li, Vikas Sindhwani, Chris Ding, and Yi Zhang.
2009. Knowledge transformation for cross-domain
sentiment classification. In Proceedings of the 32nd
international ACM SIGIR conference on Research and
development in information retrieval, pages 716?717,
Boston, MA, USA. ACM.
Fangtao Li, Chao Han, Minlie Huang, Xiaoyan Zhu,
Ying-Ju Xia, Shu Zhang, and Hao Yu. 2010a.
Structure-aware review mining and summarization. In
Proceedings of the 23rd International Conference on
Computational Linguistics, pages 653?661, Beijing,
China.
Fangtao Li, Minlie Huang, and Xiaoyan Zhu. 2010b.
Sentiment analysis with global topics and local de-
pendency. In Proceedings of the Twenty-Fourth AAAI
Conference on Artificial Intelligence, Atlanta, Geor-
gia, USA. AAAI Press.
418
Bing Liu. 2010. Sentiment analysis and subjectivity.
Handbook of Natural Language Processing, Second
Edition.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and
ChengXiang Zhai. 2007. Topic sentiment mixture:
modeling facets and opinions in weblogs. In Pro-
ceedings of the 16th international conference on World
Wide Web, pages 171?180, Banff, Alberta, Canada.
ACM.
Sinno Jialin Pan and Qiang Yang. 2010. A survey
on transfer learning. IEEE Trans. Knowl. Data Eng.,
22(10):1345?1359, Oct.
Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, Qiang
Yang, and Chen Zheng. 2010. Cross-domain senti-
ment classification via spectral feature alignment. In
Proceedings of the 19th International Conference on
World Wide Web, pages 751?760, Raleigh, NC, USA,
Apr. ACM.
Bo Pang and Lillian Lee. 2004. A sentimental edu-
cation: sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd Annual Meeting on Association for Compu-
tational Linguistics, Barcelona, Spain. ACL.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1?135.
Ana-Maria Popescu and Oren Etzioni. 2005. Extracting
product features and opinions from reviews. In Pro-
ceedings of Human Language Technology Conference
and Conference on Empirical Methods in Natural Lan-
guage Processing, pages 339?346, Vancouver, British
Columbia, Canada. ACL.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2009.
Expanding domain sentiment lexicon through double
propagation. In Proceedings of the 21st international
jont conference on Artifical intelligence, pages 1199?
1204, Pasadena, California, USA. Morgan Kaufmann
Publishers Inc.
Ellen Riloff and Rosie Jones. 1999. Learning dictio-
naries for information extraction by multi-level boot-
strapping. In Proceedings of the 6th national con-
ference on Artificial intelligence, pages 474?479, Or-
lando, Florida, United States. AAAI.
Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003.
Learning subjective nouns using extraction pattern
bootstrapping. In Proceedings of the 7th conference
on natural language learning, pages 25?32, Edmon-
ton, Canada. ACL.
Ellen Riloff. 1996. Automatically generating extrac-
tion patterns from untagged text. In Proceedings of
the Thirteenth National Conference on Artificial In-
telligence, pages 1044?1049, Portland, Oregon, USA.
AAAI Press/MIT Press.
Songbo Tan, Gaowei Wu, Huifeng Tang, and Xueqi
Cheng. 2007. A novel scheme for domain-transfer
problem in the context of sentiment analysis. In Pro-
ceedings of the 16th ACM conference on Conference
on information and knowledge management, pages
979?982, Lisbon, Portugal. ACM.
Ivan Titov and Ryan McDonald. 2008. A joint model of
text and aspect ratings for sentiment summarization.
In Proceedings of the 46th Annual Meeting of the As-
sociation of Computational Linguistics: Human Lan-
guage Technologies, pages 308?316, Columbus, Ohio,
USA. ACL.
Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew
Bell, and Melanie Martin. 2004. Learning subjective
language. Comput. Linguist., 30:277?308, Sept.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the conference
on Human Language Technology and Empirical Meth-
ods in Natural Language Processing, pages 347?354,
Vancouver, British Columbia, Canada. ACL.
Dan Wu, Wee Sun Lee, Nan Ye, and Hai Leong Chieu.
2009. Domain adaptive bootstrapping for named en-
tity recognition. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 1523?1532, Singapore. ACL.
Min Zhang and Xingyao Ye. 2008. A generation
model to unify topic relevance and lexicon-based sen-
timent for opinion retrieval. In Proceedings of the
31st annual international ACM SIGIR conference on
Research and development in information retrieval,
pages 411?418, Singapore. ACM.
Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaom-
ing Li. 2010. Jointly modeling aspects and opin-
ions with a MaxEnt-LDA hybrid. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 56?65, Cambridge, Mas-
sachusetts, USA. ACL.
419
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 449?458,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
String Re-writing Kernel
Fan Bu1, Hang Li2 and Xiaoyan Zhu3
1,3State Key Laboratory of Intelligent Technology and Systems
1,3Tsinghua National Laboratory for Information Sci. and Tech.
1,3Department of Computer Sci. and Tech., Tsinghua University
2Microsoft Research Asia, No. 5 Danling Street, Beijing 100080,China
1bufan0000@gmail.com
2hangli@microsoft.com
3zxy-dcs@tsinghua.edu.cn
Abstract
Learning for sentence re-writing is a funda-
mental task in natural language processing and
information retrieval. In this paper, we pro-
pose a new class of kernel functions, referred
to as string re-writing kernel, to address the
problem. A string re-writing kernel measures
the similarity between two pairs of strings,
each pair representing re-writing of a string.
It can capture the lexical and structural sim-
ilarity between two pairs of sentences with-
out the need of constructing syntactic trees.
We further propose an instance of string re-
writing kernel which can be computed effi-
ciently. Experimental results on benchmark
datasets show that our method can achieve bet-
ter results than state-of-the-art methods on two
sentence re-writing learning tasks: paraphrase
identification and recognizing textual entail-
ment.
1 Introduction
Learning for sentence re-writing is a fundamental
task in natural language processing and information
retrieval, which includes paraphrasing, textual en-
tailment and transformation between query and doc-
ument title in search.
The key question here is how to represent the re-
writing of sentences. In previous research on sen-
tence re-writing learning such as paraphrase identifi-
cation and recognizing textual entailment, most rep-
resentations are based on the lexicons (Zhang and
Patrick, 2005; Lintean and Rus, 2011; de Marneffe
et al, 2006) or the syntactic trees (Das and Smith,
                  wrote     .                  Shakespeare  wrote  Hamlet.  
  *    was written by       .          Hamlet was written by Shakespeare.  
(B) ** 
* * 
(A) 
Figure 1: Example of re-writing. (A) is a re-writing rule
and (B) is a re-writing of sentence.
2009; Heilman and Smith, 2010) of the sentence
pairs.
In (Lin and Pantel, 2001; Barzilay and Lee, 2003),
re-writing rules serve as underlying representations
for paraphrase generation/discovery. Motivated by
the work, we represent re-writing of sentences by
all possible re-writing rules that can be applied into
it. For example, in Fig. 1, (A) is one re-writing rule
that can be applied into the sentence re-writing (B).
Specifically, we propose a new class of kernel func-
tions (Scho?lkopf and Smola, 2002), called string re-
writing kernel (SRK), which defines the similarity
between two re-writings (pairs) of strings as the in-
ner product between them in the feature space in-
duced by all the re-writing rules. SRK is different
from existing kernels in that it is for re-writing and
defined on two pairs of strings. SRK can capture the
lexical and structural similarity between re-writings
of sentences and does not need to parse the sentences
and create the syntactic trees of them.
One challenge for using SRK lies in the high com-
putational cost of straightforwardly computing the
kernel, because it involves two re-writings of strings
(i.e., four strings) and a large number of re-writing
rules. We are able to develop an instance of SRK,
referred to as kb-SRK, which directly computes the
number of common rewriting rules without explic-
449
itly calculating the inner product between feature
vectors, and thus drastically reduce the time com-
plexity.
Experimental results on benchmark datasets show
that SRK achieves better results than the state-of-
the-art methods in paraphrase identification and rec-
ognizing textual entailment. Note that SRK is very
flexible to the formulations of sentences. For ex-
ample, informally written sentences such as long
queries in search can also be effectively handled.
2 Related Work
The string kernel function, first proposed by Lodhi
et al (2002), measures the similarity between two
strings by their shared substrings. Leslie et al
(2002) proposed the k-spectrum kernel which repre-
sents strings by their contiguous substrings of length
k. Leslie et al (2004) further proposed a number of
string kernels including the wildcard kernel to fa-
cilitate inexact matching between the strings. The
string kernels defined on two pairs of objects (in-
cluding strings) were also developed, which decom-
pose the similarity into product of similarities be-
tween individual objects using tensor product (Basil-
ico and Hofmann, 2004; Ben-Hur and Noble, 2005)
or Cartesian product (Kashima et al, 2009).
The task of paraphrasing usually consists of para-
phrase pattern generation and paraphrase identifica-
tion. Paraphrase pattern generation is to automat-
ically extract semantically equivalent patterns (Lin
and Pantel, 2001; Bhagat and Ravichandran, 2008)
or sentences (Barzilay and Lee, 2003). Paraphrase
identification is to identify whether two given sen-
tences are a paraphrase of each other. The meth-
ods proposed so far formalized the problem as clas-
sification and used various types of features such
as bag-of-words feature, edit distance (Zhang and
Patrick, 2005), dissimilarity kernel (Lintean and
Rus, 2011) predicate-argument structure (Qiu et al,
2006), and tree edit model (which is based on a tree
kernel) (Heilman and Smith, 2010) in the classifica-
tion task. Among the most successful methods, Wan
et al (2006) enriched the feature set by the BLEU
metric and dependency relations. Das and Smith
(2009) used the quasi-synchronous grammar formal-
ism to incorporate features from WordNet, named
entity recognizer, POS tagger, and dependency la-
bels from aligned trees.
The task of recognizing textual entailment is to
decide whether the hypothesis sentence can be en-
tailed by the premise sentence (Giampiccolo et al,
2007). In recognizing textual entailment, de Marn-
effe et al (2006) classified sentences pairs on the
basis of word alignments. MacCartney and Man-
ning (2008) used an inference procedure based on
natural logic and combined it with the methods by
de Marneffe et al (2006). Harmeling (2007) and
Heilman and Smith (2010) classified sequence pairs
based on transformation on syntactic trees. Zanzotto
et al (2007) used a kernel method on syntactic tree
pairs (Moschitti and Zanzotto, 2007).
3 Kernel Approach to Sentence
Re-Writing Learning
We formalize sentence re-writing learning as a ker-
nel method. Following the literature of string kernel,
we use the terms ?string? and ?character? instead of
?sentence? and ?word?.
Suppose that we are given training data consisting
of re-writings of strings and their responses
((s1, t1),y1), ...,((sn, tn),yn) ? (?
????)?Y
where ? denotes the character set, ?? =
??
i=0?
i de-
notes the string set, which is the Kleene closure of
set ?, Y denotes the set of responses, and n is the
number of instances. (si, ti) is a re-writing consist-
ing of the source string si and the target string ti.
yi is the response which can be a category, ordinal
number, or real number. In this paper, for simplic-
ity we assume that Y = {?1} (e.g. paraphrase/non-
paraphrase). Given a new string re-writing (s, t) ?
?????, our goal is to predict its response y. That is,
the training data consists of binary classes of string
re-writings, and the prediction is made for the new
re-writing based on learning from the training data.
We take the kernel approach to address the learn-
ing task. The kernel on re-writings of strings is de-
fined as
K : (?????)? (?????)? R
satisfying for all (si, ti), (s j, t j) ? ?????,
K((si, ti),(s j, t j)) = ??(si, ti),?(s j, t j)?
where ? maps each re-writing (pair) of strings into
a high dimensional Hilbert space H , referred to as
450
feature space. By the representer theorem (Kimel-
dorf and Wahba, 1971; Scho?lkopf and Smola, 2002),
it can be shown that the response y of a new string
re-writing (s, t) can always be represented as
y = sign(
n
?
i=1
?iyiK((si, ti),(s, t)))
where ?i ? 0,(i = 1, ? ? ? ,n) are parameters. That is,
it is determined by a linear combination of the sim-
ilarities between the new instance and the instances
in training set. It is also known that by employing a
learning model such as SVM (Vapnik, 2000), such a
linear combination can be automatically learned by
solving a quadratic optimization problem. The ques-
tion then becomes how to design the kernel function
for the task.
4 String Re-writing Kernel
Let ? be the set of characters and ?? be the set of
strings. Let wildcard domain D ? ?? be the set of
strings which can be replaced by wildcards.
The string re-writing kernel measures the similar-
ity between two string re-writings through the re-
writing rules that can be applied into them. For-
mally, given re-writing rule set R and wildcard do-
main D, the string re-writing kernel (SRK) is defined
as
K((s1, t1),(s2, t2)) = ??(s1, t1),?(s2, t2)? (1)
where ?(s, t) = (?r(s, t))r?R and
?r(s, t) = n? i (2)
where n is the number of contiguous substring pairs
of (s, t) that re-writing rule r matches, i is the num-
ber of wildcards in r, and ? ? (0,1] is a factor pun-
ishing each occurrence of wildcard.
A re-writing rule is defined as a triple r =
(?s,?t ,?) where ?s,?t ? (? ? {?})? denote source
and target string patterns and ? ? ind?(?s)? ind?(?t)
denotes the alignments between the wildcards in the
two string patterns. Here ind?(? ) denotes the set of
indexes of wildcards in ? .
We say that a re-writing rule (?s,?t ,?) matches a
string pair (s, t), if and only if string patterns ?s and
?t can be changed into s and t respectively by sub-
stituting each wildcard in the string patterns with an
element in the strings, where the elements are de-
fined in the wildcard domain D and the wildcards
?s[i] and ?t [ j] are substituted by the same elements,
when there is an alignment (i, j) ? ? .
For example, the re-writing rule in Fig. 1 (A)
can be formally written as r = (? s,? t,?) where
? s = (?,wrote,?), ? t = (?,was,written,by,?) and
? = {(1,5),(3,1)}. It matches with the string pair in
Fig. 1 (B).
String re-writing kernel is a class of kernels which
depends on re-writing rule set R and wildcard do-
main D. Here we provide some examples. Obvi-
ously, the effectiveness and efficiency of SRK de-
pend on the choice of R and D.
Example 1. We define the pairwise k-spectrum ker-
nel (ps-SRK) K psk as the re-writing rule kernel un-
der R = {(?s,?t ,?)|?s,?t ? ?k,? = /0} and any
D. It can be shown that K psk ((s1, t1),(s2, t2)) =
Kspeck (s1,s2)K
spec
k (t1, t2) where K
spec
k (x,y) is equiv-
alent to the k-spectrum kernel proposed by Leslie et
al. (2002).
Example 2. The pairwise k-wildcard kernel (pw-
SRK) K pwk is defined as the re-writing rule kernel
under R= {(?s,?t ,?)|?s,?t ? (??{?})k,? = /0} and
D = ?. It can be shown that K pwk ((s1, t1),(s2, t2)) =
Kwc(k,k)(s1,s2)K
wc
(k,k)(t1, t2) where K
wc
(k,k)(x,y) is a spe-
cial case (m=k) of the (k,m)-wildcard kernel pro-
posed by Leslie et al (2004).
Both kernels shown above are represented as the
product of two kernels defined separately on strings
s1,s2 and t1, t2, and that is to say that they do not
consider the alignment relations between the strings.
5 K-gram Bijective String Re-writing
Kernel
Next we propose another instance of string re-
writing kernel, called the k-gram bijective string re-
writing kernel (kb-SRK). As will be seen, kb-SRK
can be computed efficiently, although it is defined
on two pairs of strings and is not decomposed (note
that ps-SRK and pw-SRK are decomposed).
5.1 Definition
The kb-SRK has the following properties: (1) A
wildcard can only substitute a single character, de-
noted as ???. (2) The two string patterns in a re-
writing rule are of length k. (3) The alignment
relation in a re-writing rule is bijective, i.e., there
is a one-to-one mapping between the wildcards in
451
the string patterns. Formally, the k-gram bijective
string re-writing kernel Kk is defined as a string
re-writing kernel under the re-writing rule set R =
{(?s,?t ,?)|?s,?t ? (??{?})k,? is bijective} and the
wildcard domain D = ?.
Since each re-writing rule contains two string pat-
terns of length k and each wildcard can only substi-
tute one character, a re-writing rule can only match
k-gram pairs in (s, t). We can rewrite Eq. (2) as
?r(s, t) = ?
?s?k-grams(s)
?
?t?k-grams(t)
??r(?s,?t) (3)
where ??r(?s,?t) = ? i if r (with i wildcards) matches
(?s,?t), otherwise ??r(?s,?t) = 0.
For ease of computation, we re-write kb-SRK as
Kk((s1, t1),(s2, t2))
= ?
?s1 ? k-grams(s1)
?t1 ? k-grams(t1)
?
?s2 ? k-grams(s2)
?t2 ? k-grams(t2)
K?k((?s1 ,?t1),(?s2 ,?t2))
(4)
where
K?k = ?
r?R
??r(?s1 ,?t1)??r(?s2 ,?t2) (5)
5.2 Algorithm for Computing Kernel
A straightforward computation of kb-SRK would
be intractable. The computation of Kk in Eq. (4)
needs computations of K?k conducted O((n? k +
1)4) times, where n denotes the maximum length
of strings. Furthermore, the computation of K?k in
Eq. (5) needs to perform matching of all the re-
writing rules with the two k-gram pairs (?s1 , ?t1),
(?s2 , ?t2), which has time complexity O(k!).
In this section, we will introduce an efficient algo-
rithm, which can compute K?k and Kk with the time
complexities of O(k) and O(kn2), respectively. The
latter is verified empirically.
5.2.1 Transformation of Problem
For ease of manipulation, our method transforms
the computation of kernel on k-grams into the com-
putation on a new data structure called lists of dou-
bles. We first explain how to make the transforma-
tion.
Suppose that ?1,?2 ? ?k are k-grams, we use
?1[i] and ?2[i] to represent the i-th characters of
them. We call a pair of characters a double. Thus
??? denotes the set of doubles and ?Ds ,?Dt ? (??
??1 = abbccbb ;               ??2 = abcccdd; 
??1 = cbcbbcb ;               ??2 = cbccdcd;  
Figure 2: Example of two k-gram pairs.
???= ?a?a???b?b?????????c?c???c?c?????????????
???= ?c?c???b?b???c?c???????????????c?c??????? 
Figure 3: Example of the pair of double lists combined
from the two k-gram pairs in Fig. 2. Non-identical dou-
bles are in bold.
?)k denote lists of doubles. The following operation
combines two k-grams into a list of doubles.
?1??2 = ((?1[1],?2[1]), ? ? ? ,(?1[k],?2[k])).
We denotes ?1 ? ?2[i] as the i-th element of the
list. Fig. 3 shows example lists of doubles combined
from k-grams.
We introduce the set of identical doubles I =
{(c,c)|c ? ?} and the set of non-identical doubles
N = {(c,c?)|c,c? ? ? and c 6= c?}. Obviously, I
?
N =
??? and I
?
N = /0.
We define the set of re-writing rules for double
lists RD = {rD = (?Ds ,?Dt ,?)|?Ds ,?Dt ? (I?{?})k,?
is a bijective alignment} where ?Ds and ?Dt are lists
of identical doubles including wildcards and with
length k. We say rule rD matches a pair of double
lists (?Ds ,?Dt ) iff. ?Ds ,?Dt can be changed into ?Ds
and ?Dt by substituting each wildcard pair to a dou-
ble in ??? , and the double substituting the wild-
card pair ?Ds [i] and ?Dt [ j] must be an identical dou-
ble when there is an alignment (i, j) ? ? . The rule
set defined here and the rule set in Sec. 4 only differ
on the elements where re-writing occurs. Fig. 4 (B)
shows an example of re-writing rule for double lists.
The pair of double lists in Fig. 3 can match with the
re-writing rule.
5.2.2 Computing K?k
We consider how to compute K?k by extending the
computation from k-grams to double lists.
The following lemma shows that computing the
weighted sum of re-writing rules matching k-gram
pairs (?s1 ,?t1) and (?s2 ,?t2) is equivalent to com-
puting the weighted sum of re-writing rules for dou-
ble lists matching (?s1??s2 ,?t1??t2).
452
                           a b * 1  c                    a b ?   c c ?   ?                         (a,a) (b,b)  ?   (c ,c)  (c ,c)  ?   ?                                          
                             
       c b c ?   ?   c ?                          (c, c ) (b,b)  (c ,c)  ?   ?   (c ,c)  ?                    
(A) (B) 
Figure 4: For re-writing rule (A) matching both k-gram
pairs shown in Fig. 2, there is a corresponding re-writing
rule for double lists (B) matching the pair of double lists
shown in Fig. 3.
?????????=??a?a?????b?b?????????????????????c?c???? 
?????????=??a?a?????b?b?????????????????????c?c???? 
Figure 5: Example of #???(?) for the two double lists
shown in Fig. 3. Doubles not appearing in both ?Ds and
?Dt are not shown.
Lemma 1. For any two k-gram pairs (?s1 ,?t1) and
(?s2 ,?t2), there exists a one-to-one mapping from
the set of re-writing rules matching them to the set of
re-writing rules matching the corresponding double
lists (?s1??s2 ,?t1??t2).
The re-writing rule in Fig. 4 (A) matches the k-
gram pairs in Fig. 2. Equivalently, the re-writing
rule for double lists in Fig. 4 (B) matches the pair
of double lists in Fig. 3. By lemma 1 and Eq. 5, we
have
K?k = ?
rD?RD
??rD(?s1??s2 ,?t1??t2) (6)
where ??rD(?Ds ,?Dt ) = ? 2i if the rewriting rule for
double lists rD with i wildcards matches (?Ds ,?Dt ),
otherwise ??rD(?Ds ,?Dt ) = 0. To get K?k, we just need
to compute the weighted sum of re-writing rules for
double lists matching (?s1 ??s2 ,?t1 ??t2). Thus,
we can work on the ?combined? pair of double lists
instead of two pairs of k-grams.
Instead of enumerating all possible re-writing
rules and checking whether they can match the given
pair of double lists, we only calculate the number of
possibilities of ?generating? from the pair of double
lists to the re-writing rules matching it, which can be
carried out efficiently. We say that a re-writing rule
of double lists can be generated from a pair of double
lists (?Ds , ?Dt ), if they match with each other. From
the definition of RD, in each generation, the identi-
cal doubles in ?Ds and ?Dt can be either or not sub-
stituted by an aligned wildcard pair in the re-writing
Algorithm 1: Computing K?k
Input: k-gram pair (?s1 ,?t1) and (?s2 ,?t2)
Output: K?k((?s1 ,?t1),(?s2 ,?t2))
1 Set (?Ds ,?Dt ) = (?s1??s2 ,?t1??t2) ;
2 Compute #???(?Ds ) and #???(?Dt );
3 result=1;
4 for each e ? ??? satisfies
#e(?Ds )+#e(?Dt ) 6= 0 do
5 ge = 0, ne = min{#e(?Ds ),#e(?Dt )} ;
6 for 0? i? ne do
7 ge = ge +a
(e)
i ? 2i;
8 result = result ?g;
9 return result;
rule, and all the non-identical doubles in ?Ds and ?Dt
must be substituted by aligned wildcard pairs. From
this observation and Eq. 6, K?k only depends on the
number of times each double occurs in the double
lists.
Let e be a double. We denote #e(?D) as the num-
ber of times e occurs in the list of doubles ?D. Also,
for a set of doubles S? ???, we denote #S(?D) as
a vector in which each element represents #e(?D) of
each double e ? S. We can find a function g such
that
K?k = g(#???(?s1??s2),#???(?t1??t2)) (7)
Alg. 1 shows how to compute K?k. #???(.) is com-
puted from the two pairs of k-grams in line 1-2. The
final score is made through the iterative calculation
on the two lists (lines 4-8).
The key of Alg. 1 is the calculation of ge based on
a(e)i (line 7). Here we use a
(e)
i to denote the number
of possibilities for which i pairs of aligned wildcards
can be generated from e in both ?Ds and ?Dt . a
(e)
i can
be computed as follows.
(1) If e ? N and #e(?Ds ) 6= #e(?Dt ), then a
(e)
i = 0
for any i.
(2) If e?N and #e(?Ds ) = #e(?Dt ) = j, then a
(e)
j =
j! and a(e)i = 0 for any i 6= j.
(3) If e ? I, then a(e)i =
(#e(?Ds )
i
)(#e(?Dt )
i
)
i!.
We next explain the rationale behind the above
computations. In (1), since #e(?Ds ) 6= #e(?Dt ), it is
impossible to generate a re-writing rule in which all
453
the occurrences of non-identical double e are substi-
tuted by pairs of aligned wildcards. In (2), j pairs of
aligned wildcards can be generated from all the oc-
currences of non-identical double e in both ?Ds and
?Dt . The number of combinations thus is j!. In (3),
a pair of aligned wildcards can either be generated
or not from a pair of identical doubles in ?Ds and
?Dt . We can select i occurrences of identical double
e from ?Ds , i occurrences from ?Dt , and generate all
possible aligned wildcards from them.
In the loop of lines 4-8, we only need to con-
sider a(e)i for 0? i?min{#e(?Ds ),#e(?Dt )}, because
a(e)i = 0 for the rest of i.
To sum up, Eq. 7 can be computed as below,
which is exactly the computation at lines 3-8.
g(#???(?Ds ),#???(?Dt )) = ?
e????
(
ne
?
i=0
a(e)i ?
2i) (8)
For the k-gram pairs in Fig. 2, we first create
lists of doubles in Fig. 3 and compute #???(?) for
them (lines 1-2 of Alg. 1), as shown in Fig. 5. We
next compute Kk from #???(?Ds ) and #???(?Dt ) in
Fig. 5 (lines 3-8 of Alg. 1) and obtain Kk = (1)(1+
? 2)(? 2)(2? 4)(1 + 6? 2 + 6? 4) = 12? 12 + 24? 10 +
14? 8 +2? 6.
5.2.3 Computing Kk
Algorithm 2 shows how to compute Kk. It pre-
pares two maps ms and mt and two vectors of coun-
ters cs and ct . In ms and mt , each key #N(.) maps a
set of values #???(.). Counters cs and ct count the
frequency of each #???(.). Recall that #N(?s1??s2)
denotes a vector whose element is #e(?s1 ??s2) for
e ? N. #???(?s1 ??s2) denotes a vector whose ele-
ment is #e(?s1??s2) where e is any possible double.
One can easily verify the output of the al-
gorithm is exactly the value of Kk. First,
K?k((?s1 ,?t1),(?s2 ,?t2)) = 0 if #N(?s1 ? ?s2) 6=
#N(?t1 ??t2). Therefore, we only need to consider
those ?s1 ??s2 and ?t1 ??t2 which have the same
key (lines 10-13). We group the k-gram pairs by
their key in lines 2-5 and lines 6-9.
Moreover, the following relation holds
K?k((?s1 ,?t1),(?s2 ,?t2)) = K?k((?
?
s1 ,?
?
t1),(?
?
s2 ,?
?
t2))
if #???(?s1??s2) = #???(?
?
s1??
?
s2) and #???(?t1?
?t2) = #???(?
?
t1 ??
?
t2), where ?
?
s1 , ?
?
s2 , ?
?
t1 , ?
?
t2 are
Algorithm 2: Computing Kk
Input: string pair (s1, t1) and (s2, t2), window
size k
Output: Kk((s1, t1),(s2, t2))
1 Initialize two maps ms and mt and two counters
cs and ct ;
2 for each k-gram ?s1 in s1 do
3 for each k-gram ?s2 in s2 do
4 Update ms with key-value pair
(#N(?s1??s2),#???(?s1??s2));
5 cs[#???(?s1??s2)]++ ;
6 for each k-gram ?t1 in t1 do
7 for each k-gram ?t2 in t2 do
8 Update mt with key-value pair
(#N(?t1??t2),#???(?t1??t2));
9 ct [#???(?t1??t2)]++ ;
10 for each key ?ms.keys?mt .keys do
11 for each vs ?ms[key] do
12 for each vt ?mt [key] do
13 result+= cs[vs]ct [vt ]g(vs,vt) ;
14 return result;
other k-grams. Therefore, we only need to take
#???(?s1??s2) and #???(?t1??t2) as the value un-
der each key and count its frequency. That is to say,
#??? provides sufficient statistics for computing K?k.
The quantity g(vs,vt) in line 13 is computed by
Alg. 1 (lines 3-8).
5.3 Time Complexity
The time complexities of Alg. 1 and Alg. 2 are
shown below.
For Alg. 1, lines 1-2 can be executed in
O(k). The time for executing line 7 is less
than #e(?Ds ) + #e(?Dt ) + 1 for each e satisfying
#e(?Ds ) 6= 0 or #e(?Dt ) 6= 0 . Since ?e???? #e(?Ds ) =
?e???? #e(?Dt ) = k, the time for executing lines 3-8
is less than 4k, which results in the O(k) time com-
plexity of Alg. 1.
For Alg. 2, we denote n = max{|s1|, |s2|, |t1|, |t2|}.
It is easy to see that if the maps and counters in the
algorithm are implemented by hash maps, the time
complexities of lines 2-5 and lines 6-9 are O(kn2).
However, analyzing the time complexity of lines 10-
454
                           a b * 1  c            
0
0.5
1
1.5
2
2.5
1 2 3 4 5 6 7 8
C/
n a
vg
2 
window size  K 
Worst
Avg.
Figure 6: Relation between ratio C/n2avg and window size
k when running Alg. 2 on MSR Paraphrases Corpus.
13 is quite difficult.
Lemma 2 and Theorem 1 provide an upper bound
of the number of times computing g(vs,vt) in line 13,
denoted as C.
Lemma 2. For ?s1 ?k-grams(s1) and ?s2 ,?
?
s2 ?k-
grams(s2), we have #???(?s1??s2) =
#???(?s1??
?
s2) if #N(?s1??s2) = #N(?s1??
?
s2).
Theorem 1. C is O(n3).
By Lemma 2, each ms[key] contains at most
n? k + 1 elements. Together with the fact that
?key ms[key] = (n? k + 1)
2, Theorem 1 is proved.
It can be also proved that C is O(n2) when k = 1.
Empirical study shows that O(n3) is a loose upper
bound for C. Let navg denote the average length of
s1, t1, s2 and t2. Our experiment on all pairs of sen-
tences on MSR Paraphrase (Fig. 6) shows that C is in
the same order of n2avg in the worst case and C/n
2
avg
decreases with increasing k in both average case and
worst case, which indicates that C is O(n2) and the
overall time complexity of Alg. 2 is O(kn2).
6 Experiments
We evaluated the performances of the three types
of string re-writing kernels on paraphrase identifica-
tion and recognizing textual entailment: pairwise k-
spectrum kernel (ps-SRK), pairwise k-wildcard ker-
nel (pw-SRK), and k-gram bijective string re-writing
kernel (kb-SRK). We set ? = 1 for all kernels. The
performances were measured by accuracy (e.g. per-
centage of correct classifications).
In both experiments, we used LIBSVM with de-
fault parameters (Chang et al, 2011) as the clas-
sifier. All the sentences in the training and test
sets were segmented into words by the tokenizer at
OpenNLP (Baldrige et al, ). We further conducted
stemming on the words with Iveonik English Stem-
mer (http://www.iveonik.com/ ).
We normalized each kernel by K?(x,y) =
K(x,y)?
K(x,x)K(y,y)
and then tried them under different
window sizes k. We also tried to combine the
kernels with two lexical features ?unigram precision
and recall? proposed in (Wan et al, 2006), referred
to as PR. For each kernel K, we tested the window
size settings of K1 + ...+Kkmax (kmax ? {1,2,3,4})
together with the combination with PR and we
report the best accuracies of them in Tab 1 and
Tab 2.
6.1 Paraphrase Identification
The task of paraphrase identification is to examine
whether two sentences have the same meaning. We
trained and tested all the methods on the MSR Para-
phrase Corpus (Dolan and Brockett, 2005; Quirk
et al, 2004) consisting of 4,076 sentence pairs for
training and 1,725 sentence pairs for testing.
The experimental results on different SRKs are
shown in Table 1. It can be seen that kb-SRK out-
performs ps-SRK and pw-SRK. The results by the
state-of-the-art methods reported in previous work
are also included in Table 1. kb-SRK outperforms
the existing lexical approach (Zhang and Patrick,
2005) and kernel approach (Lintean and Rus, 2011).
It also works better than the other approaches listed
in the table, which use syntactic trees or dependency
relations.
Fig. 7 gives detailed results of the kernels under
different maximum k-gram lengths kmax with and
without PR. The results of ps-SRK and pw-SRK
without combining PR under different k are all be-
low 71%, therefore they are not shown for clar-
Method Acc.
Zhang and Patrick (2005) 71.9
Lintean and Rus (2011) 73.6
Heilman and Smith (2010) 73.2
Qiu et al (2006) 72.0
Wan et al (2006) 75.6
Das and Smith (2009) 73.9
Das and Smith (2009)(PoE) 76.1
Our baseline (PR) 73.6
Our method (ps-SRK) 75.6
Our method (pw-SRK) 75.0
Our method (kb-SRK) 76.3
Table 1: Comparison with state-of-the-arts on MSRP.
455
                           a b * 1  c            
73.5
74
74.5
75
75.5
76
76.5
1 2 3 4
Accura
cy (%)
 
w i ndow size kmax 
kb_SR K+ PR
kb_SR K
ps_SRK +PR
pw_SRK +PR
P R
Figure 7: Performances of different kernels under differ-
ent maximum window size kmax on MSRP.
ity. By comparing the results of kb-SRK and pw-
SRK we can see that the bijective property in kb-
SRK is really helpful for improving the performance
(note that both methods use wildcards). Further-
more, the performances of kb-SRK with and without
combining PR increase dramatically with increasing
kmax and reach the peaks (better than state-of-the-art)
when kmax is four, which shows the power of the lex-
ical and structural similarity captured by kb-SRK.
6.2 Recognizing Textual Entailment
Recognizing textual entailment is to determine
whether a sentence (sometimes a short paragraph)
can entail the other sentence (Giampiccolo et al,
2007). RTE-3 is a widely used benchmark dataset.
Following the common practice, we combined the
development set of RTE-3 and the whole datasets of
RTE-1 and RTE-2 as training data and took the test
set of RTE-3 as test data. The train and test sets con-
tain 3,767 and 800 sentence pairs.
The results are shown in Table 2. Again, kb-SRK
outperforms ps-SRK and pw-SRK. As indicated
in (Heilman and Smith, 2010), the top-performing
RTE systems are often built with significant engi-
Method Acc.
Harmeling (2007) 59.5
de Marneffe et al (2006) 60.5
M&M, (2007) (NL) 59.4
M&M, (2007) (Hybrid) 64.3
Zanzotto et al (2007) 65.75
Heilman and Smith (2010) 62.8
Our baseline (PR) 62.0
Our method (ps-SRK) 64.6
Our method (pw-SRK) 63.8
Our method (kb-SRK) 65.1
Table 2: Comparison with state-of-the-arts on RTE-3.
                           a b * 1  c            
60.5
61.5
62.5
63.5
64.5
65.5
1 2 3 4
Accura
cy (%)
 
w i ndow size kmax 
kb_SR K+ PR
kb_SR K
ps_SRK +PR
pw_SRK +PR
PR
Figure 8: Performances of different kernels under differ-
ent maximum window size kmax on RTE-3.
neering efforts. Therefore, we only compare with
the six systems which involves less engineering. kb-
SRK still outperforms most of those state-of-the-art
methods even if it does not exploit any other lexical
semantic sources and syntactic analysis tools.
Fig. 8 shows the results of the kernels under dif-
ferent parameter settings. Again, the results of ps-
SRK and pw-SRK without combining PR are too
low to be shown (all below 55%). We can see that
PR is an effective method for this dataset and the
overall performances are substantially improved af-
ter combining it with the kernels. The performance
of kb-SRK reaches the peak when window size be-
comes two.
7 Conclusion
In this paper, we have proposed a novel class of ker-
nel functions for sentence re-writing, called string
re-writing kernel (SRK). SRK measures the lexical
and structural similarity between two pairs of sen-
tences without using syntactic trees. The approach
is theoretically sound and is flexible to formulations
of sentences. A specific instance of SRK, referred
to as kb-SRK, has been developed which can bal-
ance the effectiveness and efficiency for sentence
re-writing. Experimental results show that kb-SRK
achieve better results than state-of-the-art methods
on paraphrase identification and recognizing textual
entailment.
Acknowledgments
This work is supported by the National Basic Re-
search Program (973 Program) No. 2012CB316301.
References
Baldrige, J. , Morton, T. and Bierner G. OpenNLP.
http://opennlp.sourceforge.net/.
456
Barzilay, R. and Lee, L. 2003. Learning to paraphrase:
An unsupervised approach using multiple-sequence
alignment. Proceedings of the 2003 Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy, pp. 16?23.
Basilico, J. and Hofmann, T. 2004. Unifying collab-
orative and content-based filtering. Proceedings of
the twenty-first international conference on Machine
learning, pp. 9, 2004.
Ben-Hur, A. and Noble, W.S. 2005. Kernel methods for
predicting protein?protein interactions. Bioinformat-
ics, vol. 21, pp. i38?i46, Oxford Univ Press.
Bhagat, R. and Ravichandran, D. 2008. Large scale ac-
quisition of paraphrases for learning surface patterns.
Proceedings of ACL-08: HLT, pp. 674?682.
Chang, C. and Lin, C. 2011. LIBSVM: A library for sup-
port vector machines. ACM Transactions on Intelli-
gent Systems and Technology vol. 2, issue 3, pp. 27:1?
27:27. Software available at http://www.csie.
ntu.edu.tw/?cjlin/libsvm
Das, D. and Smith, N.A. 2009. Paraphrase identifi-
cation as probabilistic quasi-synchronous recognition.
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pp. 468?476.
de Marneffe, M., MacCartney, B., Grenager, T., Cer, D.,
Rafferty A. and Manning C.D. 2006. Learning to dis-
tinguish valid textual entailments. Proc. of the Second
PASCAL Challenges Workshop.
Dolan, W.B. and Brockett, C. 2005. Automatically con-
structing a corpus of sentential paraphrases. Proc. of
IWP.
Giampiccolo, D., Magnini B., Dagan I., and Dolan B.,
editors 2007. The third pascal recognizing textual en-
tailment challenge. Proceedings of the ACL-PASCAL
Workshop on Textual Entailment and Paraphrasing,
pp. 1?9.
Harmeling, S. 2007. An extensible probabilistic
transformation-based approach to the third recogniz-
ing textual entailment challenge. Proceedings of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing, pp. 137?142, 2007.
Heilman, M. and Smith, N.A. 2010. Tree edit models for
recognizing textual entailments, paraphrases, and an-
swers to questions. Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, pp. 1011-1019.
Kashima, H. , Oyama, S. , Yamanishi, Y. and Tsuda, K.
2009. On pairwise kernels: An efficient alternative
and generalization analysis. Advances in Knowledge
Discovery and Data Mining, pp. 1030-1037, 2009,
Springer.
Kimeldorf, G. and Wahba, G. 1971. Some results on
Tchebycheffian spline functions. Journal of Mathemat-
ical Analysis and Applications, Vol.33, No.1, pp.82-
95, Elsevier.
Lin, D. and Pantel, P. 2001. DIRT-discovery of inference
rules from text. Proc. of ACM SIGKDD Conference
on Knowledge Discovery and Data Mining.
Lintean, M. and Rus, V. 2011. Dissimilarity Kernels
for Paraphrase Identification. Twenty-Fourth Interna-
tional FLAIRS Conference.
Leslie, C. , Eskin, E. and Noble, W.S. 2002. The spec-
trum kernel: a string kernel for SVM protein classifi-
cation. Pacific symposium on biocomputing vol. 575,
pp. 564-575, Hawaii, USA.
Leslie, C. and Kuang, R. 2004. Fast string kernels using
inexact matching for protein sequences. The Journal
of Machine Learning Research vol. 5, pp. 1435-1455.
Lodhi, H. , Saunders, C. , Shawe-Taylor, J. , Cristianini,
N. and Watkins, C. 2002. Text classification using
string kernels. The Journal of Machine Learning Re-
search vol. 2, pp. 419-444.
MacCartney, B. and Manning, C.D. 2008. Modeling se-
mantic containment and exclusion in natural language
inference. Proceedings of the 22nd International Con-
ference on Computational Linguistics, vol. 1, pp. 521-
528, 2008.
Moschitti, A. and Zanzotto, F.M. 2007. Fast and Effec-
tive Kernels for Relational Learning from Texts. Pro-
ceedings of the 24th Annual International Conference
on Machine Learning, Corvallis, OR, USA, 2007.
Qiu, L. and Kan, M.Y. and Chua, T.S. 2006. Para-
phrase recognition via dissimilarity significance clas-
sification. Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing,
pp. 18?26.
Quirk, C. , Brockett, C. and Dolan, W. 2004. Monolin-
gual machine translation for paraphrase generation.
Proceedings of EMNLP 2004, pp. 142-149, Barcelona,
Spain.
Scho?lkopf, B. and Smola, A.J. 2002. Learning with
kernels: Support vector machines, regularization, op-
timization, and beyond. The MIT Press, Cambridge,
MA.
Vapnik, V.N. 2000. The nature of statistical learning
theory. Springer Verlag.
Wan, S. , Dras, M. , Dale, R. and Paris, C. 2006. Using
dependency-based features to take the ?Para-farce?
out of paraphrase. Proc. of the Australasian Language
Technology Workshop, pp. 131?138.
Zanzotto, F.M. , Pennacchiotti, M. and Moschitti, A.
2007. Shallow semantics in fast textual entailment
457
rule learners. Proceedings of the ACL-PASCAL
workshop on textual entailment and paraphrasing, pp.
72?77.
Zhang, Y. and Patrick, J. 2005. Paraphrase identifica-
tion by text canonicalization. Proceedings of the Aus-
tralasian Language Technology Workshop, pp. 160?
166.
458
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 531?541,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
New Word Detection for Sentiment Analysis
Minlie Huang, Borui Ye*, Yichen Wang, Haiqiang Chen**, Junjun Cheng**, Xiaoyan Zhu
State Key Lab. of Intelligent Technology and Systems, National Lab. for Information Science
and Technology, Dept. of Computer Science and Technology, Tsinghua University, Beijing 100084, PR China
*Dept. of Communication Engineering, Beijing University of Posts and Telecommunications
**China Information Technology Security Evaluation Center
aihuang@tsinghua.edu.cn
Abstract
Automatic extraction of new words is
an indispensable precursor to many NLP
tasks such as Chinese word segmentation,
named entity extraction, and sentimen-
t analysis. This paper aims at extract-
ing new sentiment words from large-scale
user-generated content. We propose a ful-
ly unsupervised, purely data-driven frame-
work for this purpose. We design statisti-
cal measures respectively to quantify the
utility of a lexical pattern and to measure
the possibility of a word being a newword.
The method is almost free of linguistic re-
sources (except POS tags), and requires
no elaborated linguistic rules. We also
demonstrate how new sentiment word will
benefit sentiment analysis. Experiment re-
sults demonstrate the effectiveness of the
proposed method.
1 Introduction
New words on the Internet have been emerg-
ing all the time, particularly in user-generated con-
tent. Users like to update and share their infor-
mation on social websites with their own language
styles, among which new political/social/cultural
words are constantly used.
However, such new words have made many
natural language processing tasks more challeng-
ing. Automatic extraction of new words is indis-
pensable to many tasks such as Chinese word seg-
mentation, machine translation, named entity ex-
traction, question answering, and sentiment analy-
sis. New word detection is one of the most critical
issues in Chinese word segmentation. Recent stud-
ies (Sproat and Emerson, 2003) (Chen, 2003) have
shown that more than 60% of word segmentation
errors result from new words. Statistics show that
more than 1000 new Chinese words appear every
year (Thesaurus Research Center, 2003). These
words are mostly domain-specific technical terms
and time-sensitive political/social /cultural terms.
Most of them are not yet correctly recognized by
the segmentation algorithm, and remain as out of
vocabulary (OOV) words.
New word detection is also important for sen-
timent analysis such as opinionated phrase ex-
traction and polarity classification. A sentiment
phrase with complete meaning should have a cor-
rect boundary, however, characters in a new word
may be broken up. For example, in a sentence
" ??/ n ??/ adv ?/ v ?/ n?artists' perfor-
mance is very impressive?" the two Chinese char-
acters??/v?/n(cool; powerful)?should always
be extracted together. In polarity classification,
new words can be informative features for clas-
sification models. In the previous example, "?
?(cool; powerful)" is a strong feature for clas-
sification models while each single character is
not. Adding new words as feature in classification
models will improve the performance of polarity
classification, as demonstrated later in this paper.
This paper aims to detect new word for senti-
ment analysis. We are particulary interested in ex-
tracting new sentiment word that can express opin-
ions or sentiment, which is of high value toward-
s sentiment analysis. New sentiment word, as ex-
emplified in Table 1, is a sub-class of multi-word
expressions which is a sequence of neighboring
words "whose exact and unambiguous meaning
or connotation cannot be derived from the mean-
ing or connotation of its components" (Choueka,
1988). Such new words cannot be directly iden-
tified using grammatical rules, which poses a ma-
jor challenge to automatic analysis. Moreover, ex-
isting lexical resources never have adequate and
timely coverage since new words appear constant-
ly. People thus resort to statistical methods such as
Pointwise Mutual Information (Church and Han-
ks, 1990), Symmetrical Conditional Probability
531
(da Silva and Lopes, 1999), Mutual Expectation
(Dias et al, 2000), Enhanced Mutual Information
(Zhang et al, 2009), and Multi-word Expression
Distance (Bu et al, 2010).
New word English Translation Polarity
?? lovely positive
?? tragic/tragedy negative
?? very cool; powerful positive
?? reverse one's expectation negative
Table 1: Examples of new sentiment word.
Our central idea for new sentiment word de-
tection is as follows: Starting from very few seed
words (for example, just one seed word), we can
extract lexical patterns that have strong statistical
association with the seed words; the extracted lex-
ical patterns can be further used in finding more
new words, and the most probable new words can
be added into the seed word set for the next iter-
ation; and the process can be run iteratively un-
til a stop condition is met. The key issues are to
measure the utility of a pattern and to quantify the
possibility of a word being a new word. The main
contributions of this paper are summarized as fol-
lows:
? We propose a novel framework for new word
detection from large-scale user-generated da-
ta. This framework is fully unsupervised
and purely data-driven, and requires very
lightweight linguistic resources (i.e., only
POS tags).
? We design statistical measures to quantify the
utility of a pattern and to quantify the possi-
bility of a word being a newword, respective-
ly. No elaborated linguistic rules are needed
to filter undesirable results. This feature may
enable our approach to be portable to other
languages.
? We investigate the problem of polarity predic-
tion of new sentiment word and demonstrate
that inclusion of new sentiment word benefits
sentiment classification tasks.
The rest of the paper is structured as follows:
we will introduce related work in the next section.
Wewill describe the proposedmethod in Section 3,
including definitions, the overview of the algorith-
m, and the statistical measures for addressing the
two key issues. We then present the experiments
in Section 4. Finally, the work is summarized in
Section 5.
2 Related Work
New word detection has been usually inter-
weaved with word segmentation, particularly in
Chinese NLP. In these works, new word detection
is considered as an integral part of segmentation,
where new words are identified as the most proba-
ble segments inferred by the probabilistic models;
and the detected new word can be further used to
improve word segmentation. Typical models in-
clude conditional random fields proposed by (Peng
et al, 2004), and a joint model trained with adap-
tive online gradient descent based on feature fre-
quency information (Sun et al, 2012).
Another line is to treat new word detection as
a separate task, usually preceded by part-of-speech
tagging. The first genre of such studies is to lever-
age complex linguistic rules or knowledge. For
example, Justeson and Katz (1995) extracted tech-
nical terminologies from documents using a regu-
lar expression. Argamon et al (1998) segmented
the POS sequence of a multi-word into small POS
tiles, counted tile frequency in the new word and
non-new-word on the training set respectively, and
detected new words using these counts. Chen and
Ma (2002) employed morphological and statisti-
cal rules to extract Chinese new word. The sec-
ond genre of the studies is to treat new word de-
tection as a classification problem. Zhou (2005)
proposed a discriminative Markov Model to de-
tect new words by chunking one or more separat-
ed words. In (Li et al, 2005), new word detec-
tion was viewed as a binary classification problem.
However, these supervisedmodels requires not on-
ly heavy engineering of linguistic features, but also
expensive annotation of training data.
User behavior data has recently been explored
for finding new words. Zheng et al (2009) ex-
plored user typing behaviors in Sogou Chinese
Pinyin input method to detect new words. Zhang
et al (2010) proposed to use dynamic time warp-
ing to detect new words from query logs. Howev-
er, both of the work are limited due to the public
unavailability of expensive commercial resources.
Statistical methods for new word detection
have been extensively studied, and in some sense
exhibit advantages over linguistics-based method-
s. In this setting, new word detection is mostly
532
known as multi-word expression extraction. To
measure multi-word association, the first model
is Pointwise Mutual Information (PMI) (Church
and Hanks, 1990). Since then, a variety of sta-
tistical methods have been proposed to measure
bi-gram association, such as Log-likelihood (Dun-
ning, 1993) and Symmetrical Conditional Proba-
bility (SCP) (da Silva and Lopes, 1999). Among
all the 84 bi-gram association measures, PMI has
been reported to be the best one in Czech data
(Pecina, 2005). In order to measure arbitrary n-
grams, most common strategies are to separate n-
gram into two parts X and Y so that existing bi-
gram methods can be used (da Silva and Lopes,
1999; Dias et al, 2000; Schone and Jurafsky,
2001). Zhang et al (2009) proposed Enhanced
Mutual Information (EMI) which measures the co-
hesion of n-gram by the frequency of itself and the
frequency of each single word. Based on the in-
formation distance theory, Bu et al (2010) pro-
posed multi-word expression distance (MED) and
the normalized version, and reported superior per-
formance to EMI, SCP, and other measures.
3 Methodology
3.1 Definitions
Definition 3.1 (Adverbial word). Words that are
used mainly to modify a verb or an adjective, such
as "?(too)", "??(very)", "??(very)", and "?
?(specially)".
Definition 3.2 (Auxiliary word). Words that are
auxiliaries, model particles, or punctuation marks.
In Chinese, such words are like "?,?,?,?,?",
and punctuation marks include "??????" and
so on.
Definition 3.3 (Lexical Pattern). A lexical pat-
tern is a triplet < AD, ?, AU >, where AD is an
adverbial word, the wildcard ? means an arbitrary
number of words 1, and AU denotes an auxiliary
word.
Table 2 gives some examples of lexical pat-
terns. In order to obtain lexical patterns, we can
define regular expressions with POS tags 2 and ap-
ply the regular expressions on POS tagged texts.
Since the tags of adverbial and auxiliary words are
1We set the number to 3 words in this work considering
computation costs.
2Such expressions are very simple and easy to write be-
cause we only need to consider POS tags of adverbial and
auxiliary word.
relatively static and can be easily identified, such
a method can safely obtain lexical patterns.
Pattern Frequency
<"?",*,"?"> 562,057
<"?",*,"?"> 387,649
<"?",*,"?"> 380,470
<"?",*,"?"> 369,702
Table 2: Examples of lexical pattern. The frequen-
cy is counted on 237,108,977 Weibo posts.
3.2 The Algorithm Overview
The algorithm works as follows: starting
from very few seed words (for example, a word
in Table 1), the algorithm can find lexical pattern-
s that have strong statistical association with the
seed words in which the likelihood ratio test (L-
RT) is used to quantify the degree of association.
Subsequently, the extracted lexical patterns can be
further used in finding more new words. We de-
sign several measures to quantify the possibility of
a candidate word being a new word, and the top-
ranked words will be added into the seed word set
for the next iteration. The process can be run iter-
atively until a stop condition is met. Note that we
do not augment the pattern set (P) at each iteration,
instead, we keep a fixed small number of patterns
during iteration because this strategy produces op-
timal results.
From linguistic perspectives, new sentiment
words are commonly modified by adverbial words
and thus can be extracted by lexical patterns. This
is the reason why the algorithm will work. Our al-
gorithm is in spirit to double propagation (Qiu et
al., 2011), however, the differences are apparen-
t in that: firstly, we use very lightweight linguis-
tic information (except POS tags); secondly, our
major contributions are to propose statistical mea-
sures to address the following key issues: first, to
measure the utility of lexical patterns; second, to
measure the possibility of a candidate word being
a new word.
3.3 Measuring the Utility of a Pattern
The first key issue is to quantify the utility of
a pattern at each iteration. This can be measured
by the association of a pattern to the current word
set used in the algorithm. The likelihood ratio test-
s (Dunning, 1993) is used for this purpose. This
association model has also been used to model as-
sociation between opinion target words by (Hai et
533
Algorithm 1: New word detection algorithm
Input:
D: a large set of POS tagged posts
W
s
: a set of seed words
k
p
: the number of patterns chosen at each
iteration
k
c
: the number of patterns in the candidate
pattern set
k
w
: the number of words added at each
iteration
K: the number of words returned
Output: A list of ranked new wordsW
1 Obtain all lexical patterns using regular
expressions on D;
2 Count the frequency of each lexical pattern
and extract words matched by each pattern ;
3 Obtain top k
c
frequent patterns as candidate
pattern set P
c
and top 5,000 frequent words as
candidate word setW
c
;
4 P = ?;W=W
s
; t = 0 ;
5 for |W| < K do
6 UseW to score each pattern in P
c
with
U(p) ;
7 P = {top k
p
patterns} ;
8 Use P to extract new words and if the
words are inW
c
, score them with F (w) ;
9 W = W
?
{top k
w
words} ;
10 W
c
=W
c
-W ;
11 Sort words inW with F (w) ;
12 Output the ranked list of words inW ;
al., 2012).
The LRT is well known for not relying crit-
ically on the assumption of normality, instead, it
uses the asymptotic assumption of the generalized
likelihood ratio. In practice, the use of likelihood
ratios tends to result in significant improvements
in text-analysis performance.
In our problem, LRT computes a contingency
table of a pattern p and a word w, derived from
the corpus statistics, as given in Table 3, where
k
1
(w, p) is the number of documents thatwmatch-
es pattern p, k
2
(w, p?) is the number of documents
that w occurs while p does not, k
3
(w?, p) is the
number of documents that p occurs while w does
not, and k
4
(w?, p?) is the number of documents con-
taining neither p nor w.
Statistics p p?
w k
1
(w, p) k
2
(w, p?)
w? k
3
(w?, p) k
4
(w?, p?)
Table 3: Contingency table for likelihood ratio test
(LRT).
Based on the statistics shown in Table 3, the
likelihood ratio tests (LRT) model captures the sta-
tistical association between a pattern p and a word
w by employing the following formula:
LRT (p, w) = log
L(?
1
, k
1
, n
1
) ? L(?
2
, k
2
, n
2
)
L(?, k
1
, n
1
) ? L(?, k
2
, n
2
)
(1)
where:
L(?, k, n) = ?
k
? (1 ? ?)
n?k; n
1
= k
1
+ k
3
;
n
2
= k
2
+ k
4
; ?
1
= k
1
/n
1
; ?
2
= k
2
/n
2
; ? =
(k
1
+ k
2
)/(n
1
+ n
2
).
Thus, the utility of a pattern can be measured
as follows:
U(p) =
?
w
i
?W
LRT (p, w
i
) (2)
where W is the current word set used in the algo-
rithm (see Algorithm 1).
3.4 Measuring the Possibility of Being New
Words
Another key issue in the proposed algorithm
is to quantify the possibility of a candidate word
being a new word. We consider several factors for
this purpose.
3.4.1 Likelihood Ratio Test
Very similar to the pattern utility measure, L-
RT can also be used to measure the association of
a candidate word to a given pattern set, as follows:
LRT (w) =
?
p
i
?P
LRT (w, p
i
) (3)
where P is the current pattern set used in the algo-
rithm (see Algorithm 1), and p
i
is a lexical pattern.
This measure only quantifies the association
of a candidate word to the given pattern set. It
tells nothing about the possibility of a word be-
ing a new word, however, a new sentiment word,
should have close association with the lexical pat-
terns. This has linguistic interpretations because
new sentiment words are commonly modified by
adverbial words and thus should have close associ-
ation with lexical patterns. This measure is proved
to be an influential factor by our experiments in
Section 4.3.
534
3.4.2 Left Pattern Entropy
If a candidate word is a new word, it will be
more commonly used with diversified lexical pat-
terns since the non-compositionality of new word
means that the word can be used in many differ-
ent linguistic scenarios. This can be measured by
information entropy, as follows:
LPE(w) = ?
?
l
i
?L(P
c
,w)
c(l
i
, w)
N(w)
? log
c(l
i
, w)
N(w)
(4)
where L(P
c
, w) is the set of left word of all pat-
terns by which word w can be matched in P
c
,
c(l
i
, w) is the count that word w can be matched
by patterns whose left word is l
i
, and N(w) is the
count that word w can be matched by the patterns
in P
c
. Note that we use P
c
, instead of P , because
the latter set is very small while computing entropy
needs a large number of patterns. Tuning the size
of P
c
will be further discussed in Section 4.4.
3.4.3 New Word Probability
Some words occur very frequently and can be
widely matched by lexical patterns, but they are
not new words. For example, "??(love to eat)"
and "??(love to talk)" can be matched by many
lexical patterns, however, they are not new words
due to the lack of non-compositionality. In such
words, each single character has high probability
to be a word. Thus, we design the following mea-
sure to favor this observation.
NWP (w) =
n
?
i=1
p(w
i
)
1? p(w
i
)
(5)
where w = w
1
w
2
. . . w
n
, each w
i
is a single char-
acter, and p(w
i
) is the probability of the character
w
i
being a word, as computed as follows:
p(w
i
) =
all(w
i
)? s(w
i
)
all(w
i
)
where all(w
i
) is the total frequency of w
i
, and
s(w
i
) is the frequency of w
i
being a single char-
acter word. Obviously, in order to obtain the value
of s(w
i
), some particular Chinese word segmen-
tation tool is required. In this work, we resort to
ICTCLAS (Zhang et al, 2003), a widely used tool
in the literature.
3.4.4 Non-compositionality Measures
New words are usually multi-word expres-
sions, where a variety of statistical measures have
been proposed to detect multi-word expressions.
Thus, such measures can be naturally incorporated
into our algorithm.
The first measure is enhanced mutual infor-
mation (EMI) (Zhang et al, 2009):
EMI(w) = log
2
F/N
?
n
i=1
F
i
?F
N
(6)
where F is the number of posts in which a multi-
word expression w = w
1
w
2
. . . w
n
occurs, F
i
is
the number of posts where w
i
occurs, andN is the
total number of posts. The key idea of EMI is to
measure word pair?s dependency as the ratio of its
probability of being a multi-word to its probability
of not being amulti-word. The larger the value, the
more possible the expression will be a multi-word
expression.
The second measure we take into account is
normalized multi-word expression distance (Bu et
al., 2010), which has been proposed to measure the
non-compositionality of multi-word expressions.
NMED(w) =
log|?(w)| ? log|?(w)|
logN ? log|?(w)|
(7)
where ?(w) is the set of documents in which all
single words in w = w
1
w
2
. . . w
n
co-occur, ?(w)
is the set of documents in which word w occurs
as a whole, and N is the total number of docu-
ments. Different from EMI, this measure is a strict
distance metric, meaning that a smaller value in-
dicates a larger possibility of being a multi-word
expression. As can be seen from the formula, the
key idea of this metric is to compute the ratio of the
co-occurrence of all words in a multi-word expres-
sions to the occurrence of the whole expression.
3.4.5 Configurations to Combine Various
Factors
Taking into account the aforementioned fac-
tors, we have different settings to score a new
word, as follows:
F
LRT
(w) = LRT (w) (8)
F
LPE
(w) = LRT (w) ? LPE(w) (9)
F
NWP
(w) = LRT (w) ? LPE(w) ?NWP (w) (10)
F
EMI
(w) = LRT (w) ? LPE(w) ? EMI(w) (11)
F
NMED
(w) =
LRT (w) ? LPE(w)
NMED(w)
(12)
535
4 Experiment
In this section, we will conduct the following
experiments: first, we will compare our method
to several baselines, and perform parameter tun-
ing with extensive experiments; second, we will
classify polarity of new sentiment words using t-
wo methods; third, we will demonstrate how new
sentiment words will benefit sentiment classifica-
tion.
4.1 Data Preparation
We crawled 237,108,977 Weibo posts from
http://www.weibo.com, the largest social website
in China. These posts range from January of 2011
to December of 2012. The posts were then part-of-
speech tagged using a Chinese word segmentation
tool named ICTCLAS (Zhang et al, 2003).
Then, we asked two annotators to label the top
5,000 frequent words that were extracted by lexi-
cal patterns as described in Algorithm 1. The an-
notators were requested to judge whether a candi-
date word is a new word, and also to judge the po-
larity of a new word (positive, negative, and neu-
tral). If there is a disagreement on either of the
two tasks, discussions are required to make the fi-
nal decision. The annotation led to 323 new word-
s, among which there are 116 positive words, 112
negative words, and 95 neutral words3.
4.2 Evaluation Metric
As our algorithm outputs a ranked list of
words, we adapt average precision to evaluate
the performance of new sentiment word detection.
The metric is computed as follows:
AP (K) =
?
K
k=1
P (k) ? rel(k)
?
K
k=1
rel(k)
where P (k) is the precision at cut-off k, rel(k) is
1 if the word at position k is a new word and 0 oth-
erwise, andK is the number of words in the ranked
list. A perfect list (all topK items are correct) has
an AP value of 1.0.
4.3 Evaluation of Different Measures and
Comparison to Baselines
First, we assess the influence of likelihood ra-
tio test, which measures the association of a word
to the pattern set. As can be seen from Table 4,
the associationmodel (LRT) remarkably boosts the
3All the resources are available upon request.
performance of new word detection, indicating L-
RT is a key factor for new sentiment word extrac-
tion. From linguistic perspectives, new sentiment
words are commonly modified by adverbial words
and thus should have close association with lexical
patterns.
Second, we compare different settings of our
method to two baselines. The first one is en-
hanced mutual information (EMI) where we set
F (w) = EMI(w) (Zhang et al, 2009) and the
second baseline is normalized multi-word expres-
sion distance (NMED) (Bu et al, 2010) where we
set F (w) = NMED(w). The results are shown
in Figure 1. As can be seen, all the proposed
measures outperform the two baselines (EMI and
NMED) remarkably and consistently. The set-
ting of F
NMED
produces the best performance.
AddingNMED orEMI leads to remarkable im-
provements because of their capability of measur-
ing non-compositionality of new words. Only us-
ingLRT can obtain a fairly good results whenK is
small, however, the performance drops sharply be-
cause it's unable to measure non-compositionality.
Comparison between LRT + LPE (or LRT +
LPE + NWP ) and LRT shows that inclusion
of left pattern entropy also boosts the performance
apparently. However, the new word probabili-
ty (NWP ) has only marginal contribution to im-
provement.
In the above experiments, we set k
p
= 5 (the
number of patterns chosen at each iteration) and
k
w
= 10 (the number of words added at each iter-
ation), which is the optimal setting and will be dis-
cussed in the next subsection. And only one seed
word "??(reverse one's expectation)" is used.
Figure 1: Comparative results of different measure
settings. X-axis is the number of words returned
(K), and Y-axis is average precision (AP (K)).
536
top K words ? 100 200 300 400 500
LPE 0.366 0.324 0.286 0.270 0.259
LRT+LPE 0.743 0.652 0.613 0.582 0.548
LPE+NWP 0.467 0.400 0.350 0.330 0.320
LRT+LPE+NWP 0.755 0.680 0.612 0.571 0.543
LPE+EMI 0.608 0.551 0.519 0.486 0.467
LRT+LPE+EMI 0.859 0.759 0.717 0.662 0.632
LPE+NMED 0.749 0.690 0.641 0.612 0.576
LRT+LPE+NMED 0.907 0.808 0.741 0.723 0.699
Table 4: Results with vs. without likelihood ratio test (LRT).
4.4 Parameter Tuning
Firstly, we will show how to obtain the op-
timal settings of k
p
and k
w
. The measure setting
we take here is F
NMED
(w), as shown in Formula
(12). Again, we choose only one seed word "?
?(reverse one's expectation)", and the number of
words returned is set to K = 300. Results in Ta-
ble 5 show that the performance drops consistent-
ly across different k
w
settings when the number of
patterns increases. Note that at the early stage of
Algorithm 1, larger k
p
(perhaps with noisy pattern-
s) may lead to lower quality of new words; while
larger k
w
(perhaps with noisy seed words) may
lead to lower quality of lexical patterns. Therefore,
we choose the optimal setting to small numbers, as
k
p
= 5, k
w
= 10.
Secondly, we justify whether the proposed al-
gorithm is sensitive to the number of seed words.
We set k
p
= 5 and k
w
= 10, and take F
NMED
as the weighting measure of new word. We exper-
imented with only one seed word, two, three, and
four seed words, respectively. The results in Ta-
ble 6 show very stable performance when different
numbers of seed words are chosen. It's interesting
that the performance is totally the same with dif-
ferent numbers of seed words. By looking into the
pattern set and the selected words at each iteration,
we found that the pattern set (P) converges soon
to the same set after a few iterations; and at the be-
ginning several iterations, the selected words are
almost the same although the order of adding the
words is different. Since the algorithm will finally
sort the words at step (11) and P is the same, the
ranking of the words becomes all the same.
Lastly, we need to decide the optimal number
of patterns in P
c
(that is, k
c
in Algorithm 1) be-
cause the set has been used in computing left pat-
tern entropy, see Formula (4). Too small size of
P
c
may lead to insufficient estimation of left pat-
tern entropy. Results in Table 7 shows that larg-
er P
c
decrease the performance, particularly when
the number of words returned (K) becomes larger.
Therefore, we set |P
c
| = 100.
4.5 Polarity Prediction of New Sentiment
Words
In this section, we attempt to classifying the
polarity of the annotated 323 new words. Two
methods are adapted with different settings for this
purpose. The first one is majority vote (MV), and
the second one is pointwise mutual information,
similar to (Turney and Littman, 2003). The ma-
jority vote method is formulated as below:
MV (w) =
?
w
p
?PW
#(w,w
p
)
|PW |
?
?
w
n
?NW
#(w,w
n
)
|NW |
where PW and NW are a positive and negative
set of emoticons (or seed words) respectively, and
#(w,w
p
) is the co-occurrence count of the input
wordw and the itemw
p
. The polarity is judged ac-
cording to this rule: ifMV (w) > th
1
, the word w
is positive; ifMV (w) < ?th
1
the word negative;
otherwise neutral. The threshold th
1
is manually
tuned.
And PMI is computed as follows:
PMI(w) =
?
w
p
?PW
PMI(w,w
p
)
|PW |
?
?
w
n
?NW
PMI(w,w
n
)
|NW |
where PMI(x, y) = log
2
(
Pr(x,y)
Pr(x)?Pr(y)
), and
Pr(?) denotes probability. The polarity is judged
according to the rule: if PMI(w) > th
2
, w is
positive; if PMI(w) < ?th
2
negative; otherwise
neutral. The threshold th
2
is manually tuned.
As for the resources PW and NW , we
have three settings. The first setting (denoted by
537
HH
H
H
H
H
k
w
k
p 2 3 4 5 10 20 50
5 0.753 0.738 0.746 0.741 0.741 0.734 0.715
10 0.753 0.738 0.746 0.741 0.741 0.728 0.712
15 0.753 0.738 0.746 0.741 0.754 0.734 0.718
20 0.763 0.738 0.744 0.749 0.749 0.735 0.717
Table 5: Parameter tuning results for k
p
and k
w
. The measure setting is F
NMED
(w), the seed word set
is {"??(reverse one's expectation)"}, and the number of words returned isK = 300.
# seeds ? 1 2 3 4
K=100 0.907 0.907 0.907 0.907
K=200 0.808 0.808 0.808 0.808
K=300 0.741 0.741 0.741 0.741
K=400 0.709 0.709 0.709 0.709
K=500 0.685 0.685 0.685 0.685
Table 6: Performance with different numbers of
seed words. The measure setting is F
NMED
(w),
and k
p
= 5, k
w
= 10. The seed words are chosen
from Table 1.
Large_Emo) is a set of most frequent 36 emoticons
in which there are 21 positive and 15 negative e-
moticons respectively. The second one (denoted
by Small_Emo) is a set of 10 emoticons, which
are chosen from the 36 emoticons, as shown in
Table 8. The third one (denoted by Opin_Words)
is two sets of seed opinion words, where PW={
??(happy),??(generous),??(beautiful), ?
?(kind),??(smart)} and NW ={??(sad),?
?(mean),??(ugly),??(wicked),?(stupid)}.
The performance of polarity prediction is
shown in Table 9. In two-class polarity classifi-
cation, we remove neutral words and only make
prediction with positive/negative classes. The first
observation is that the performance of using emoti-
cons is much better than that of using seed opin-
ion words. We conjecture that this may be be-
cause new sentiment words are more frequently
co-occurring with emoticons than with these opin-
ion words. The second observation is that three-
class polarity classification is much more diffi-
cult than two-class polarity classification because
many extracted new words are nouns such as "?
?(gay)","??(girl)", and "??(friend)". Such
nouns are more difficult to classify sentiment ori-
entation.
4.6 Application of New Sentiment Words to
Sentiment Classification
In this section, we justifywhether inclusion of
new sentiment word would benefit sentiment clas-
sification. For this purpose, we randomly sampled
and annotated 4,500 Weibo posts that contain at
least one opinion word in the union of the Hownet
4 opinion lexicons and our annotated new word-
s. We apply two models for polarity classification.
The first model is a lexicon-based model (denot-
ed by Lexicon) that counts the number of positive
and negative opinion words in a post respective-
ly, and classifies a post to be positive if there are
more positive words than negative ones, and to be
negative otherwise. The second model is a SVM
model in which opinion words are used as feature,
and 5-fold cross validation is conducted.
We experiment with different settings of
Hownet lexicon resources:
? Hownet opinion words (denoted by Hownet):
After removing some obviously inappropri-
ate words, the left lexicons have 627 posi-
tive opinion words and 1,038 negative opin-
ion words, respectively.
? Compact Hownet opinion words (denoted by
cptHownet): we count the frequency of the
above opinion words on the training data and
remove words whose document frequency is
less than 2. This results in 138 positive words
and 125 negative words.
Then, we add into the above resources the la-
beled new polar words(denoted byNW , including
116 positive and 112 negative words) and the top
100 words produced by the algorithm (denoted by
T100), respectively. Note that the lexicon-based
model requires the sentiment orientation of each
dictionary entry 5, we thus manually label the po-
4http://www.keenage.com/html/c_index.html.
5This is not necessary for the SVM model. All words in
the top 100 words can be used as feature.
538
|P
c
| ? 50 100 200 300 400 500
K=100 0.907 0.905 0.916 0.916 0.888 0.887
K=200 0.808 0.810 0.778 0.776 0.766 0.764
K=300 0.741 0.731 0.722 0.726 0.712 0.713
K=400 0.709 0.708 0.677 0.675 0.656 0.655
K=500 0.685 0.683 0.653 0.646 0.626 0.627
Table 7: Tuning the number of patterns in P
c
. The measure setting is F
NMED
(w), k
p
= 5, k
w
= 10,
and the seed word set is {"??(reverse one's expectation)"}.
Emoticon Polarity Emoticon Polarity
positive negative
positive negative
positive negative
positive negative
positive negative
Table 8: The ten emoticons used for polarity pre-
diction.
Methods? Majority vote PMI
Two-class polarity classification
Large_Emo 0.861 0.865
Small_Emo 0.846 0.851
Opin_Words 0.697 0.654
Three-class polarity classification
Large_Emo 0.598 0.632
Small_Emo 0.551 0.635
Opin_Words 0.449 0.486
Table 9: The accuracy of two/three-class polarity
classification.
larity of all top 100 words (we did NOT remove
incorrect new word). This results in 52 positive
and 34 negative words.
Results in Table 10 show that inclusion of
new words in both models improves the perfor-
mance remarkably. In the setting of the original
lexicon (Hownet), both models obtain 2-3% gains
from the inclusion of newwords. Similar improve-
ment is observed in the setting of the compact lex-
icon. Note, that T100 is automatically obtained
from Algorithm 1 so that it may contain words that
are not new sentiment words, but the resource also
improves performance remarkably.
5 Conclusion
In order to extract new sentiment words from
large-scale user-generated content, this paper pro-
poses a fully unsupervised, purely data-driven, and
# Pos/Neg Lexicon SVM
Hownet 627/1,038 0.737 0.756
Hownet+NW 743/1,150 0.770 0.779
Hownet+T100 679/1,172 0.761 0.774
cptHownet 138/125 0.738 0.758
cptHownet+NW 254/237 0.774 0.782
cptHownet+T100 190/159 0.764 0.775
Table 10: The accuracy of polarity classfication of
Weibo post with/without new sentiment words. N-
W includes 116/112 positive/negative words, and
T100 contains 52/34 positive/negative words.
almost knowledge-free (except POS tags) frame-
work. We design statistical measures to quantify
the utility of a lexical pattern and to measure the
possibility of a word being a new word, respec-
tively. The method is almost free of linguistic re-
sources (except POS tags), and does not rely on
elaborated linguistic rules. We conduct extensive
experiments to reveal the influence of different sta-
tistical measures in new word finding. Compara-
tive experiments show that our proposed method
outperforms baselines remarkably. Experiments
also demonstrate that inclusion of new sentiment
words benefits sentiment classification definitely.
From linguistic perspectives, our framework
is capable to extract adjective new words because
the lexical patterns usually modify adjective word-
s. As future work, we are considering how to ex-
tract other types of new sentiment words, such as
nounal new words that can express sentiment.
Acknowledgments
This work was partly supported by the fol-
lowing grants from: the National Basic Re-
search Program (973 Program) under grant No.
2012CB316301 and 2013CB329403, the National
Science Foundation of China project under grant
No. 61332007 and No. 60803075, and the Beijing
Higher Education Young Elite Teacher Project.
539
References
Shlomo Argamon, Ido Dagan, and Yuval Krymolows-
ki. 1998. A memory-based approach to
learning shallow natural language patterns. In
Proceedings of the 17th International Conference
on Computational Linguistics - Volume 1, COL-
ING '98, pages 67--73, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Fan Bu, Xiaoyan Zhu, and Ming Li. 2010. Measuring
the non-compositionality of multiword expres-
sions. In Proceedings of the 23rd International
Conference on Computational Linguistics, COL-
ING '10, pages 116--124, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Keh-Jiann Chen and Wei-Yun Ma. 2002. Un-
known word extraction for chinese documents. In
Proceedings of the 19th International Conference
on Computational Linguistics - Volume 1, COL-
ING '02, pages 1--7, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Aitao Chen. 2003. Chinese word segmentation us-
ingminimal linguistic knowledge. In Proceedings
of the Second SIGHAN Workshop on Chinese
Language Processing - Volume 17, SIGHAN '03,
pages 148--151, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Yaacov Choueka. 1988. Looking for nee-
dles in a haystack or locating interesting col-
location expressions in large textual databas-
es. In Proceeding of the RIAO'88 Conference
on User-Oriented Content-Based Text and Image
Handling, pages 21--24.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lex-
icography. Comput. Linguist., 16(1): 22--29,
March.
J Ferreira da Silva and G Pereira Lopes. 1999. A local
maxima method and a fair dispersion normaliza-
tion for extracting multi-word units from corpora.
In Sixth Meeting on Mathematics of Language,
pages 369--381.
Ga?l Dias, Sylvie Guillor?, and Jos? Gabriel Pereira
Lopes. 2000. Mining textual associations in text
corpora. 6th ACM SIGKDD Work. Text Mining.
TedDunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Comput. Linguist.,
19(1):61--74, March.
Zhen Hai, Kuiyu Chang, and Gao Cong. 2012.
One seed to find them all: Mining opinion fea-
tures via association. In Proceedings of the 21st
ACM International Conference on Information
and Knowledge Management, CIKM '12, pages
255--264, New York, NY, USA. ACM.
John S Justeson and SlavaMKatz. 1995. Technical ter-
minology: some linguistic properties and an algo-
rithm for identification in text. Natural language
engineering, 1(1):9--27.
Hongqiao Li, Chang-Ning Huang, Jianfeng Gao, and
Xiaozhong Fan. 2005. The use of svm for
chinese new word identification. In Natural
Language Processing--IJCNLP 2004, pages 723-
-732. Springer.
Pavel Pecina. 2005. An extensive empirical study of
collocation extraction methods. In Proceedings
of the ACL Student ResearchWorkshop, ACLstu-
dent '05, pages 13--18, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Fuchun Peng, Fangfang Feng, and Andrew McCal-
lum. 2004. Chinese segmentation and new
word detection using conditional random field-
s. In Proceedings of the 20th International
Conference on Computational Linguistics, COL-
ING '04, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2011. Opinion word expansion and target extrac-
tion through double propagation. Computational
linguistics, 37(1):9--27.
Patrick Schone and Daniel Jurafsky. 2001. Is
knowledge-free induction of multiword unit dic-
tionary headwords a solved problem. In Proc.
of the 6th Conference on Empirical Methods in
Natural Language Processing (EMNLP 2001),
pages 100--108.
Richard Sproat and Thomas Emerson. 2003. The first
international chinese word segmentation bakeoff.
In Proceedings of the Second SIGHANWorkshop
on Chinese Language Processing - Volume 17,
SIGHAN '03, pages 133--143, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Xu Sun, Houfeng Wang, and Wenjie Li. 2012.
Fast online training with frequency-adaptive
learning rates for chinese word segmentation
and new word detection. In Proceedings of
the 50th Annual Meeting of the Association
for Computational Linguistics: Long Papers -
Volume 1, ACL '12, pages 253--262, Strouds-
burg, PA, USA. Association for Computational
Linguistics.
Beijing Thesaurus Research Center. 2003. Xinhua Xin
Ciyu Cidian. Commercial Press, Beijing.
Peter D. Turney and Michael L. Littman. 2003. Mea-
suring praise and criticism: Inference of seman-
tic orientation from association. ACM Trans. Inf.
Syst., 21(4):315--346, October.
Hua-Ping Zhang, Hong-Kui Yu, De-Yi Xiong, and Qun
Liu. 2003. Hhmm-based chinese lexical analyzer
ictclas. In Proceedings of the Second SIGHAN
Workshop on Chinese Language Processing -
540
Volume 17, SIGHAN '03, pages 184--187,
Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Wen Zhang, Taketoshi Yoshida, Xijin Tang, and Tu-
Bao Ho. 2009. Improving effectiveness of
mutual information for substantival multiword
expression extraction. Expert Systems with
Applications, 36(8):10919--10930.
Yan Zhang, Maosong Sun, and Yang Zhang. 2010.
Chinese new word detection from query logs. In
Advanced Data Mining and Applications, pages
233--243. Springer.
Yabin Zheng, Zhiyuan Liu, Maosong Sun, Liyun Ru,
and Yang Zhang. 2009. Incorporating user be-
haviors in new word detection. In Proceedings of
the 21st International Jont Conference onArtifical
Intelligence, IJCAI'09, pages 2101--2106, San
Francisco, CA, USA.Morgan Kaufmann Publish-
ers Inc.
GuoDong Zhou. 2005. A chunking strategy towards
unknownword detection in chinese word segmen-
tation. In Natural Language Processing--IJCNLP
2005, pages 530--541. Springer.
541
Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering (IR4QA), pages 42?49
Manchester, UK. August 2008
Answer Validation by Information Distance Calculation
Fangtao Li, Xian Zhang, Xiaoyan Zhu
State Key Laboratory on Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China
zxy-dcs@mail.tsinghua.edu.cn
Abstract
In this paper,an information distance based
approach is proposed to perform answer
validation for question answering system.
To validate an answer candidate, the ap-
proach calculates the conditional informa-
tion distance between the question focus
and the candidate under certain condition
pattern set. Heuristic methods are de-
signed to extract question focus and gen-
erate proper condition patterns from ques-
tion. General search engines are employed
to estimate the Kolmogorov complexity,
hence the information distance. Experi-
mental results show that our approach is
stable and flexible, and outperforms tradi-
tional tfidf methods.
1 Introduction
Question answering(QA) system aims at finding
exact answers to a natural language question. In
order to correctly answer a question, several com-
ponents are implemented including question clas-
sification, passage retrieval, answer candidates
generation, answer validation etc. Answer Vali-
dation is to decide whether the candidate answers
are correct or not, or even to determine the accu-
rate confidence score to them. Most of QA systems
employ answer validation as the last step to iden-
tify the correct answer. If this component fails, it
is impossible to enable the question to be correctly
answered.
Automatic techniques for answer validation are
of great interest among question answering re-
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
search. With automatic answer validation, the
system will carry out different refinements of its
searching criteria to check the relevance of new
candidate answers. In addition, since most of
QA systems rely on complex architectures and the
evaluation of their performances requires a huge
amount of work, the automatic assessment of can-
didates with respect to a given question will speed
up both algorithm refinement and testing.
Currently, answer validation is mainly viewed
as a classification problem or ranking problem.
Different models, such as Support Vector Ma-
chine (Shen and Klakow, 2006) and Maximum En-
tropy Model (Ittycheriah et al, 2001), are used to
integrate sophisticated linguistic features to deter-
mine the correctness of candidates. The answer
validation exercise (Penas et al , 2007) aims at
developing systems able to decide whether the an-
swer is correct or not. They formulate answer val-
idation as a text entailment problem. These ap-
proaches are dependent on sophisticated linguis-
tic analysis of syntactic and semantic relations be-
tween question and candidates. It is quite expen-
sive to use deep analysis for automatic answer val-
idation, especially in large scale data set. Thus it
is appropriate to find an alternative solution to this
problem. Here, we just consider the English an-
swer validation task.
This paper proposes a novel approach based on
information retrieval on the Web. The answer val-
idation problem is reformulated as distance calcu-
lation from an answer candidate to a question. The
hypothesis is that, among all candidates, the cor-
rect answer has the smallest distance from ques-
tion. We employ conditional normalized min dis-
tance, which is based on Kolmogorov Complexity
theory (Li and Vitanyi, 1997), for this task. The
distance measures the relevance between question
42
focus and candidates conditioned on a surface pat-
tern set. For distance calculation, we first ex-
tract the question focus, and then a hierarchical
pattern set is automatically constructed as condi-
tion. Since Kolmogrov Complexity can be approx-
imated through frequency counts. Two types of
search engine ?Google? and ?Altavista? are used
to approximate the distance.
The paper is organized as follows: Section 2
describes related work. The fundamental Kol-
mogorov Complexity theory is introduced in Sec-
tion 3. Section 4 presents our proposed answer val-
idation method based on information retrieval. In
Section 5, we describe the experiments and discus-
sions. The paper is concluded in Section 6.
2 Related Work
Answer Validation is an emerging topic in Ques-
tion Answering, where open domain systems are
often required to rank huge amounts of answer
candidates. This task can be viewed as a classi-
fication problem or re-ranking problem.
Early question answering systems focused on
employing surface text patterns (Subbotin and
Subbotin, 2001) for answer validation. Xu et
al. (2003) identified that pattern-based approaches
got bad performances due to poor system recall.
Some researchers exploited machine learning tech-
niques with rich syntactic or semantic features to
measure the similarity between question and an-
swer. Ittycheriah et al (2001) used Maximum En-
tropy model to combine rich features and automat-
ically learn feature weights. These features in-
cluded query expansion features, focus features,
named entity features, dependency relation fea-
tures, pattern features et al Shen and Klakow
(2006) presented three methods, including feature
vector, string kernel and tree kernel, to represent
surface text features and parse tree features in Sup-
port Vector Machines. Ko et al (2007) pro-
posed a probabilistic graphical model to estimate
the probability of correctness for all candidate an-
swers. Four types of features were employed,
including knowledge-based features, data-driven
features, string distance feature and synonym fea-
tures.
Started in 2006, the annual Answer Validation
Exercise (Penas et al , 2007) aims to develop sys-
tems to decide if the answer to a question is correct
or not. The English answer validation task is refor-
mulated as a Text Entailment problem. The triplet,
including question, answer and supporting text, is
given. The system determines if the supporting
text can entail the hypothesis, which is a reformu-
lation from the question and answer. All partici-
pants used lexical processing, including lemmati-
zation and part-of speech tagging. Some systems
used first order logic representations, performed
semantic analysis and took the validation decision
with a theorem proof.
The above approaches should process deep syn-
tactic and semantic analysis for either questions or
candidate answers. The annotated linguistic re-
source is hard to acquire for the supervised clas-
sification problem. Another alternative solution
for answer validation is to exploit the redundancy
of large scale data. Eric et al (2007) devel-
oped AskMSR question answering system. They
focus on the Web as a gigantic data repository
with tremendous redundancy that can be exploited
to extract the correct answer. Lin (2007) im-
plemented another Web-based question answering
system, named ARANEA, which is used approxi-
mate tfidf method for answer validation.
3 Preliminaries
3.1 Kolmogorov complexity
Kolmogorov complexity , or algorithm entropy ,
K(x) of a string x is the length of the shortest bi-
nary program to compute x. It defines randomness
of an individual string. Kolmogorov complexity
has been widely accepted as an information theory
for individual objects parallel to that of Shannon?s
information theory which is defined on an ensem-
ble of objects. It has also found many applications
in computer science such as average case analysis
of algorithms (Li and Vitanyi, 1997). For a uni-
versal Turing machine U , the Kolmogorov com-
plexity of a binary string x condition to another
binary string y, K
U
(x|y), is the length of the short-
est (prefix-free) program for U that outputs x with
input y. It has been proved that for different uni-
versal Turing machine U
?
, for all x, y
K
U
(x|y) = K
U
?
(x|y) + C,
where the constant C depends only on U
?
. Thus we
simply write K
U
(x|y) as K(x|y). Define K(x) =
K(x|?), where ? is the empty string. For for-
mal definitions and a comprehensive study of Kol-
mogorov complexity, see (Li and Vitanyi, 1997).
43
3.2 Information Distance
Based on the Kolmogovov complexity theory, in-
formation distance (Bennett et al, 1998) is a uni-
versal distance metric, which has been success-
fully applied to many applications. The informa-
tion distance D(x, y) is defined as the length of
a shortest binary program which can compute x
given y as well as compute y from x. It has been
proved that , up to an additive logarithmic term,
D(x, y) = max{K(x|y),K(y|x)}. The normal-
ized version of D(x, y), called the normalized in-
formation distance(NID), is defined as
d
max
(x, y) =
max{K(x|y),K(y|x)}
max{K(x),K(y)}
(1)
Parallel to this, the min distance is proposed in
(Zhang et al , 2007), defined as
D
min
(x, y) = min{K(x|y),K(y|x)}. (2)
And the normalized version is
d
min
(x, y) =
min{K(x|y),K(y|x)}
min{K(x),K(y)}
. (3)
3.3 Conditional Information Distance
Conditional information distance is defined as
d
max
(x, y|c) =
max{K(x|y, c),K(y|x, c)}
max{K(x|c),K(y|c)}
, (4)
d
min
(x, y|c) =
min{K(x|y, c),K(y|x, c)}
min{K(x|c),K(y|c)}
. (5)
where c is given in both x to y and y to x compu-
tation.
The information distance is proved to be uni-
versal (Zhang et al , 2007), that is, if x and y
are ?close? under any distance measure, they are
?close? under the measure of information distance.
However, it is not clear yet how to find out such
?closeness? in traditional information distance the-
ory. Now the conditional information distance pro-
vides a possible solution.Figure 1 gives a more in-
terpretable explanation: the condition c could map
the original concepts x and y into different x
c
and
y
c
, thus the variant ?closeness? could be reflected
by the distance between x
c
and y
c
, as shown in
Figure1.
Figure 1: Conditional information distances under different
conditions c?s
The Kolmogorov complexity is non-
computable, that is, to use the information
distance measures, we must estimate the K(x)
first. There are traditionally two ways to do
this: (1) by compression (Li et al , 2001),
and (2) by frequency counting based on coding
theorem (Cilibrasi and Vitanyi, 2007). The second
approach is implemented in this paper.
4 Answer Validation with Information
Distance
Given a question q and a candidate answer c, the
answer validation task can be considered as deter-
mining the degree of relevance of c with respect
to q. The intuition of our approach is that the dis-
tance between question and the correct answer is
smaller than other candidates. Take the question
?What is the capital of the USA?? as an example,
among all candidates, the correct answer ?Wash-
ington? is closest to the question under some dis-
tance measure. Thus the answer validation prob-
lem is to determine a proper distance measure.
Fortunately, it has been proved that the informa-
tion distance (Bennett et al, 1998) is universal so
that the similarity between the question and the an-
swer can surely be discovered using this measure.
Direct calculation of the unconditional distance
is difficult and non-flexible. We find it possible
and convenient to estimate the conditional infor-
mation distance between question focus and the
answers, under certain context as the condition. As
explained previously, different conditions lead to
different distance. With the most proper condition
and the nearest distance, the best answer can be
identified out of previously determined candidates.
The conditional normalized min distance is em-
ployed for distance calculation, which is defined
44
Figure 2: Sample of conditional information distance calculation.
as:
d
min
(x, y|c)
=
K
(
c(x,y)
)
?max{K
(
c(x,?)
)
,K
(
c(?,y)
)
}
min{K
(
c(x,?)
)
,K
(
c(?,y)
)
}?K
(
c(?,?)
)
where x represents the answer candidates, y is
the question focus, and c is condition pattern. The
function c(x, y) will be described in the Distance
Calculation section.
Figure 2 shows the procedure of distance cal-
culation. Given a question and a set of candidates,
we calculate the min information distance between
question focus and candidates conditioned on sur-
face patterns. Obviously, in order to calculate in-
formation distance, there are three issues to be ad-
dressed:
1. Question Focus Extraction: since the question
answer distance is reformulated as the mea-
sure between question focus and answer con-
ditioned on the surface pattern, it is important
to extract some words or phrases as question
focus.
2. Condition Pattern Generation: Obviously, the
generation of the condition is the key part.
We have built a well revised algorithm, in
which proper conditions can be generated
from question sentence according to some
heuristic rules.
3. Distance Calculation: after question focus
and condition patterns are obtained, the last
step is calculating the conditional distance to
estimate the relevance between question and
answer candidates.
4.1 Question Focus Extraction
Most factoid questions refer to specific objects. A
question is asked to learn some knowledge for this
object from certain perspective. In our approach,
we take the key named entity or noun phrase, usu-
ally as the subject or the main object of the ques-
tion sentence as the reference object. Take the
question ?What city is Lake Washington by? as ex-
ample, the specific object is ?Lake Washington?.
The question focus is identified using some heuris-
tic rules as follows:
1. The question is processed by shallow parsing.
All the noun phrases(NP) are extracted as NP set.
2. All the named entities(NE) in the question are
extracted as NE set.
3. If only one same element is identified in both
NE and NP set, this element is considered as ques-
tion focus.
4. If step 3 fails, but two elements from NE and
NP set have overlap words, then choose the ele-
ment with more words as question focus.
5. If step 3 and 4 fail, choose the candidate,
which is nearest with verb phrase in dependency
tree, as question focus.
4.2 Condition Pattern Generation
A set of hierarchical patterns is automatically con-
structed for conditional min distance calculation.
4.2.1 Condition Pattern Construction
Several operations are defined for patterns con-
struction from the original question sentence. We
describe pattern set construction with a sam-
ple question ?What year was President Kennedy
killed??:
1. With linguistic analysis, the question is
split into pieces of tokens. These tokens in-
45
clude wh-word phrases, preposition phrases, noun
phrases, verb phrases, key verb, etc. The exam-
ple question is split into ?What year?(wh-word
phrase), ?was?(key verb) ?President Kennedy?
(noun phrases), ?killed?(verb phrase).
2. Replace the wh-word phrases with the candi-
date placeholder ?c?. Then the words ?What year?
is replaced with placeholder ?c?.
3. Replace the question focus with the focus
placeholder ?f?, and add this pattern to the pat-
tern set. The example question focus is identified
as ?President Kennedy?. It is replaced with place-
holder ?f?. The first pattern ??c? was ?f? killed??
is generated.
4. Voice Transformation: with morphology
techniques, verbs are expanded with all their tense
forms ( i.e. present, past tense and past participle).
The tokens? order is adjusted to transform between
active voice and passive voice. Both patterns are
added to the patterns set. For sample question,
the passive pattern is translated into active pattern,
??c? kill ?f??.
5. Preposition addition: for time and location
questions, the preposition (i.e. in, on and at) is
added before the candidate ?c?; Then the pattern
??c? was ?f? killed? is reformulated as ?(in |on)
?c? was ?f? killed?.
6. Tokens shift: preposition phrase token could
be shifted to the begin or the end of pattern, and
?key verb? must be shift before the ?verb phrase?.
Then the pattern ?(in |on) ?c? was ?f? killed? can
be reformulated as ??f? was killed (in |on) ?c??.
7. Definitional patterns: several heuristic pat-
terns, as introduced at (Hildebrandt et al , 2004),
are added into our final pattern sets, such as ??c?,
?f??.
By such heuristic rules, the original pattern set is
obtained from question sentence. The patterns are
initially enclosed in quotation marks, which means
exact matching. However, by eliminating these
quotations, or reducing the scope that they cover,
the matching is relaxed as words co-occurrence.
The patterns are expanded into different strict-level
patterns by adding or removing quotation marks
for each tokens or adjacent tokens combination.
Several condition pattern samples are shown in Ta-
ble 1
Table 1: Sample condition patterns, ? ?? ? denotes exact
match in web query.
? ? <f>(was | were) killed (in | on) <c>?
? ? (in | on) <c>, <f>(was | were) killed?
? ? (in | on) <c>? & ?<f>(was | were) killed?
? ? (in | on) <c>? & ?<f>? & ?(was | were) killed?
? in | on <c><f>(was | were) killed
Each operation introduced above is given a pre-
defined confidence coefficient(cc). Then the con-
fidence coefficient of a pattern is defined as the
multiplication of cc for all performed operations
to generate this pattern.
4.2.2 Condition Pattern Ranking
From the previous step, a set of condition pat-
terns and corresponding confidence coefficient are
obtained. Let p
i
denotes the ith pattern in the pat-
tern set, and cc
i
is the confidence coefficient for the
ith pattern. The confidence coefficient estimation
in previous section contains much noise. And the
patterns with similar confidence coefficient make
little difference. Therefore, the exact confidence
coefficient value is not directly used. We cluster
the patterns into different priority groups. C
j
de-
notes the pattern cluster with jth priority. Here,
the smaller j means higher priority. The condi-
tion patterns are ranked mainly based on confi-
dence coefficient and the number of double quo-
tation marks. The following algorithm shows each
step in detail:
Table 2: patterns ranking algorithm
Input patterns set C = {(p
i
, cc
i
)}
Algorithm
(1) Initialize C
j
= ?, j = 0
(2) if C is empty, end this algorithm
(3) Select (p
max
, cc
max
), where cc
max
?
cc
i
, (p
i
, cc
i
) ? C
(4) if C
j
is empty, add cc
max
into C
j
, jump to
(2)
(5) select the minimum confidence coefficient
(p
min
, cc
min
) from C
j
, compare it with
(p
max
, cc
max
). if the number of double
quotes(??) in p
min
is equal to the number in
p
max
, add p
max
into C
j
. otherwise, j =
j + 1, C
j
= {p
max
}.
(6) jump to (2) and repeat
4.3 Distance Calculation
Conditional min distance d
min
is used to mea-
sure the relevance between question and candidate.
From section 3, d
min
is not computable, but ap-
proximated by frequency counts based on the cod-
ing theory:
46
dmin
(x, y|c)
=
K
(
c(x,y)
)
?max{K
(
c(x,?)
)
,K
(
c(?,y)
)
}
min{K
(
c(x,?)
)
,K
(
c(?,y)
)
}?K
(
c(?,?)
)
=
log f
(
c(x,y)
)
?min{log f
(
c(x,?)
)
,log f
(
c(?,y)
)
}
max{log f
(
c(x,?)
)
,log f
(
c(?,y)
)
}?log f
(
c(?,?)
)
The function c(x, ?) means substituting ?c? in c
by answer candidate x and removing placeholder
?f? if any. Similar definition applies to c(y, ?),
c(x, y). For example, given pattern ??f? was in-
vented in ?c??, question focus ?the telegraph? and
a candidate ?1867?. c(x, ?) is ?was invented in
1867?. c(y, ?) is ?the telegraph was invented?, and
c(x, y) is ?the telegraph was invented in 1867?.
The frequency counts f(x) are estimated as the
number of returned pages by certain search en-
gine with respect to x . f(c(?, ?)) denote the to-
tal pages indexed in search engine. Two types of
search engines ?Google? and ?Altavista? are em-
ployed.
The patterns are selected in priority order to cal-
culate the information distance for each candidate.
5 Experiment and Discussion
5.1 Experiment Setup
Data set: The standard QA test collection (Lin
and Katz, 2006) is employed in our experiments. It
consists of 109 factoid questions, covering several
domains including history, geography, physics, bi-
ology, economics, fashion knowledge, and etc.. 20
candidates are prepared for each questions. All an-
swer candidates are first extracted by the imple-
mented question answering system. Then we re-
view the candidate set for each question. If the cor-
rect answer is not in this set, it is manually added
into the set.
Performance Metric: The top 1 answer precision
and mean reciprocal rank (MRR) are used for per-
formance evaluation.The top 1 answer means the
correct answer ranks first with our distance calcu-
lation method, and MRR =
1
n
?
?
i
(
1
rank
i
), in
which the
1
rank
i
is 1 if the correct answer occurs in
the first position; 0.5 if it firstly occurs in the sec-
ond position; 0.33 for the third, 0.25 for the fourth,
0.2 for the fifth and 0 if none of the first five an-
swers is correct.
The open source factoid QA system ARANEA
(downloaded from Jimmy Lin?s website in 2005)
is used for comparison, which implements an ap-
proximate tfidf algorithm for candidate scoring.
Both ARANEA and our proposed approaches use
the internet directly. Google is used as the search
engine for ARENEA, and our conditional normal-
ized min distance is calculated with Google and
Altavista respectively.
5.2 Experiment Results
The performances of our proposed approach and
ARANEA are shown in Table 3. For top 1 an-
swer precision, our conditional min distance cal-
culation method through Google achieves 69.7%,
and Altavista is 66.1%, which make 56.6%
(69.7% v.s.42.2% ) and 50.0% (66.1% v.s 42.2%)
improvement compared with ARENEA?s tfidf
method. Our proposed methods achieve 0.756 and
0.772 compared with ARENEA?s 0.581 for MRR
measure.
Table 3: Performance comparison, where d
min
(G) denotes
the distance calculation through ?Google?, d
min
(A) through
?Altavista?
tfidf d
min
(G) d
min
(A)
# of Top 1 46 72 69
% of Top 1 42.2 69.7 66.1
MRR 0.581 0.772 0.756
Table 4 shows some correct answer validation
examples. the Google Condition(GC) and the Al-
tavista Condition(AC) columns are the employed
condition patterns for distance calculation. For
question 1400, the conditional normalized google
min distance calculates the distance between ques-
tion focus ?the telegragh? and all 20 answer can-
didates. The minimum distance score is achieved
between ?the telegraph? and ?1837? with the con-
dition pattern ??f? was invented in ?c??. There-
fore, the candidate ?1837? is validated as the cor-
rect answer. Meanwhile, the minimum value for
conditional normalized altavista min distance is
achieved on the same condition.
These results demonstrate that the distance cal-
culation method provides a feasible solution for
answer validation.
In discussion section, we will study three ques-
tions:
1. What is the role of search engine?
2. What is the role of condition pattern?
3. What is the role of question focus?
47
Table 4: Question Examples in conditional information calculation through Google and Altavista. GC:Google Condition;
AC:Altavista Condition
ID Question GC AC Answer Question focus
1400 When was the telegraph
invented?
??y was in-
vented in ?s?
??y was
invented in
?x?
1837 the telegraph
1401 What is the democratic
party symbol?
??y is ?x? ??y is ?x? the don-
key
the democratic
party symbol
1411 What Spanish explorer
discovered the Missis-
sippi River?
??x discov-
ered ?y?
??x? ?dis-
covered?
??y?
Hernando
de Soto
the Mississippi
River
1412 Who is the governor of
Colorado?
??y is ?x? ??y, ?x? Gov. Bill
Ritter
the governor of
Colorado
1484 What college did Allen
Iverson attend?
??y attended
?x?
??x? ?did
?y?
Georgetown
Univer-
sity
Allen Iverson at-
tend
5.3 Discussions
5.3.1 Role of Search Engine
The rise of world-wide-web has enticed millions
of users to create billions of web pages. The re-
dundancy of web information is an important re-
source for question answering. Our Kolmogorov
Complexity based information distance is approx-
imated with query frequency obtained by search
engine. Two types of search engines ?Google? and
?Altavista? are employed in this paper. The num-
ber of top 1 correct answer is 72 through ?Google?
and 69 through ?Altavista?. There is little differ-
ence between two numbers, which shows that the
information distance based on Kolmogorov Com-
plexity is independent of special search engine.
The performance didn?t vary much with the change
of search engine. Actually, if the local data is ac-
cumulated large enough, the information distance
can be approximated without the internet. The
quality and size of data set affect the experiment
performance.
5.3.2 Role of Condition Pattern
Pattern set offers convenient and flexible condi-
tion for information distance calculation. In the
experiment, there are 61 questions correctly an-
swered by both Google and Altavista. 46 ques-
tions of them employ different patterns. Consider-
ing Question 1412, the condition pattern in Google
is ??c? is ?f??, while in Altavista, it is ??f?, ?c??.
However, the correct answer ?Gov. Bill Ritter? is
identified by both methods. The information dis-
tance is stable over specific condition patterns.
5.3.3 Role of Question Focus
Question focus is considered as the discrimina-
tor for the question. The distance between a ques-
tion and a candidate is reformulated as the distance
between question focus and candidate conditioned
on a set of surface patterns. The proposed ap-
proach may not properly extract the question fo-
cus, but the answers can be correctly identified
when the condition pattern becomes loose enough.
Take the question 1484 ?What college did Allen
Iverson attend?? as example, the verb ?attend? is
tagged as ?noun?, then question focus is mistak-
enly extracted as ?Allen Iverson attend?, instead of
the correct ?Allen Iverson?. The two conditional
information distance method still identify the cor-
rect answer ?Georgetown University?. Because
they both employed the looser condition patterns
???c?? ??f??? and ???c?? did ??f???.Therefore, our
proposed distance answer validation methods are
robust to the question focus selection component.
From the discussion above, it can be seen that
our algorithm is stable and robust, not depending
on the specific search engine, condition pattern,
and question focus.
6 Conclusions
We have presented a novel approach for answer
validation based on information distance. The an-
swer validation task is reformulated as distance
calculation between question focus and candidate
conditioned on a set of surface patterns. The ex-
periments show that our proposed answer valida-
tion method makes a great improvement compared
48
with ARANEA?s tfidf method. Furthermore, The
experiments show that our approach is stable and
robust, not depending on the specific search en-
gine, condition pattern, and question focus. In fu-
ture work, we will try to calculate information dis-
tance in the local constructed data set, and expand
this distance measure into other application fields.
Acknowledgement
This work is supported by National Natural Sci-
ence Foundation of China (60572084, 60621062),
Hi-tech Research and Development Program of
China (2006AA02Z321), National Basic Research
Program of China (2007CB311003).
References
Abraham Ittycheriah, Martin Franz, Wei-Jing Zhu, Ad-
wait Ratnaparkhi, and Richard J.Mammone. 2001.
Question answering using maximum entropy conm-
ponents. In Proceedings of the Second meeting of
the North American Chapter of the Association for
Computational Linguistics on Language tecnologies.
Anselmo Penas, A. Rodrigo, F. Verdejo. 2007.
Overview of the Answer Validation Exercise 2007.
Working Notes for the CLEF 2007 Workshop.
C.H. Bennett, P. Gacs, M. Li, P. Vit?anyi, W. Zurek..
1998. Information Distance. IEEE Trans. Inform.
Theory, 44:4, 1407?1423.
Eric Brill and Susan Dumais and Michele Banko. 2002.
An analysis of the AskMSR question-answering sys-
tem. EMNLP ?02: the ACL-02 conference on Em-
pirical methods in natural language processing.
Hildebrandt W., Katz B., and Lin J. 2004. Answer-
ing Definition Questions Using Multiple Knowledge
Sources. Proceedings of Human Language Technol-
ogy Conference. Boston, USA.
Jeongwoo Ko, Luo Si, Eric Nyberg. 2007. A Proba-
bilistic Graphical Model for Joint Answer Ranking
in Question Answering. In Proceedings of the 30th
annual international ACM SIGIR conference on Re-
search and development in information retrieval.
Jimmy Lin and Boris Katz. 2006. Building a reusable
test collection for question answering. J. Am. Soc.
Inf. Sci. Technol..
Jimmy Lin. 2007. An Exploration of the Principles Un-
derlying Redundancy-Based Factoid Question An-
swering. ACM Transactions on Information Sys-
tems, 27(2):1-55.
Jinxi Xu, Ana Licuanan and Ralph Weischedel. 2003.
Trec 2003 qa at bbn: Answering definitional ques-
tions. In Proceedings of the 12th Text REtrieval
Conference, Gaithersburgh, MD, USA.
Ming Li and Paul MB Vitanyi. 1997. An Introduc-
tion to Kolmogorov Complexity and Its Applications.
Working Notes for the CLEF 2007 Workshop.
M. Li, J. Badger, X. Chen, S. Kwong, P. Kearney,
H. Zhang.. 2001. An information-based sequence
distance and its application to whole mitochondrial
genome phylogeny. Bioinformatics, 17:2.
M. Subbotin and S. Subbotin. 2001. Patterns of Po-
tential Answer Expressions as Clues to the Right An-
swers. In TREC-10 Notebook papers. Gaithesburg,
MD.
R. Cilibrasi, P.M.B. Vit?anyi. 2007. An Exploration of
the Principles Underlying Redundancy-Based Fac-
toid Question Answering. EEE Trans. Knowledge
and Data Engineering, 19:3, 370?383.
Shen, Dan and Dietrich Klakow . 2006. Exploring cor-
relation of dependency relation paths for answer ex-
traction. In Proceedings of COLING-ACL, Sydney,
Australia.
Xian Zhang, Yu Hao, Xiaoyan Zhu, and Ming Li. 2007.
Information Distance from a Question to an Answer.
In Proceedings of the 13th ACM SIGKDD interna-
tional conference on Knowledge discovery and data
mining.
49
Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 10?18,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Recognizing Biomedical Named Entities using Skip-chain Conditional
Random Fields
Jingchen Liu Minlie Huang? Xiaoyan Zhu
Department of Computer Science and Technology
Tsinghua University, Beijing 100084, China
liu-jc04@mails.tsinghua.edu.cn
{aihuang, zxy-dcs}@tsinghua.edu.cn
Abstract
Linear-chain Conditional Random Fields
(CRF) has been applied to perform the
Named Entity Recognition (NER) task in
many biomedical text mining and infor-
mation extraction systems. However, the
linear-chain CRF cannot capture long dis-
tance dependency, which is very common
in the biomedical literature. In this pa-
per, we propose a novel study of capturing
such long distance dependency by defin-
ing two principles of constructing skip-
edges for a skip-chain CRF: linking sim-
ilar words and linking words having typed
dependencies. The approach is applied to
recognize gene/protein mentions in the lit-
erature. When tested on the BioCreAtIvE
II Gene Mention dataset and GENIA cor-
pus, the approach contributes significant
improvements over the linear-chain CRF.
We also present in-depth error analysis on
inconsistent labeling and study the influ-
ence of the quality of skip edges on the la-
beling performance.
1 Introduction
Named Entity Recognition (NER) is a key task in
most text mining and information extraction sys-
tems. The improvement in NER can benefit the
final system performance. NER is a challenging
task, particularly in the biomedical literature due
to the variety of biomedical terminologies and the
complicated syntactic structures.
Many studies have been devoted to biomedical
NER. To evaluate biomedical NER systems, sev-
eral challenge competitions had been held, such
as BioNLP/NLPBA in 20041, BioCreAtIvE I in
? Corresponding author
1http://research.nii.ac.jp/?collier/
workshops/JNLPBA04st.htm
2004 and BioCreAtIvE II in 20062. The overview
reports from these competitions, presenting state-
of-the-art of biomedical NER studies, show that
linear-chain Conditional Random Fields (CRF) is
one of the most commonly used models and has
the most competitive results (Yeh et al, 2005;
Smith et al, 2008). Linear-chain CRF has also
been successfully applied to other NLP tasks such
as POS-tagging (Lafferty et al, 2001) and sen-
tence chunking (Sha and Pereira, 2003). However,
in most of these applications, only linear-chain
CRF was fully exploited, assuming that only adja-
cent words are inter-dependent. The dependency
between distant words, which occurs frequently in
the biomedical literature, is yet to be captured.
In the biomedical literature, the repeated ap-
pearance of same or similar words in one sentence
is a common type of long distance dependencies.
This phenomenon is due to the complicated syn-
tactic structures and the various biomedical termi-
nologies in nature. See the following example:
?Both GH deficiency and impaired
spinal growth may result in short
stature, whereas the occurrence of early
puberty in association with GH defi-
ciency reduces the time available for
GH therapy.?
the mentions of GH are repeated three times. If
the entity are referred by a pronoun, the meaning
of the sentence will be confusing and unclear be-
cause of the complex sentence structure. In this
sentence:
?These 8-oxoguanine DNA glycosy-
lases, hOgg1 (human) and mOgg1
(murine) , are homologous to each other
and to yeast Ogg1.?
the words hOgg1, mOgg1 and Ogg1 are homolo-
gous genes belonging to different species, having
2http://www.biocreative.org/
10
very similar entity names. Some other types of
long distance dependencies also occur frequently
in the biomedical literature. For example, in this
sentence
?Western immunoblot analysis detected
p55gag and its cleavage products p39
and p27 in purified particles derived by
expression of gag and gag-pol, respec-
tively.?
the words p55gag, p39 and p27 conjuncted by
and, have similar semantic meanings but they are
separated by several tokens. A human curator
can easily recognize such long distance dependen-
cies and annotate these words consistently. How-
ever, when applying the linear-chain CRF, incon-
sistency errors in annotating these entities could
happen due to the inability of representing long
distance dependency.
In this paper, we present an approach of cap-
turing long distance dependencies between words.
We adopte the skip-chain CRF to improve the per-
formance of gene mention recognition. We de-
fine two principles of connecting skip-edges for
skip-chain CRF to capture long distance depen-
dencies. The efficacy of the principles is inves-
tigated with extensive experiments. We test our
method on two data sets and significant improve-
ments are observed over the linear-chain CRF. We
present in-depth error analysis on inconsistent la-
beling. We also investigat whether the quality of
connected edges affect the labeling performance.
The remainder of this paper is organized as fol-
lows: We survey related studies in Section 2. We
introduce linear-chain CRF and skip-chain CRF in
Section 3. The method of connecting skip-chain
edges is described in Section 4 . In Section 5 we
present our experiments and in-depth analysis. We
summarize our work in Section 6.
2 Related work
NER is a widely studied topic in text mining
research, and many new challenges are seen in
domain-specific applications, such as biomedical
NER (Zhou et al, 2004). The dictionary based
method is a common technique as biomedical the-
sauruses play a key role in understanding such
text. Most dictionary based NER systems fo-
cused on: (1) integrating and normalizing differ-
ent biomedical databases to improve the quality of
the dictionary to be used; (2) improving matching
strategies that are more suitable for biomedical ter-
minologies; and (3) making filtering rules for post-
processing to refine the matching results or to ad-
just the boundary of entities, see (Fukuda et al,
1998; Narayanaswamy et al, 2003; Yang et al,
2008). Many information extraction systems had
a dictionary matching module to perform prelim-
inary detection of named entities (Schuhmann et
al., 2007; Kolarik et al, 2007; Wang et al, 2010).
Applying machine learning techniques gener-
ally obtains superior performance for the biomedi-
cal NER task. The automated learning process can
induce patterns for recognizing biomedical names
and rules for pre- and post-processing. Gener-
ally speaking, there are two categories of ma-
chine learning based methods: one treats NER as
a classification task, while the other treats NER
as a sequence labeling task. For the first cate-
gory, Support Vector Machine (SVM) was a com-
monly adopted model (Kazama et al, 2002; Zhou
et al, 2004). Lee et al (2004) proposed a two-
step framework to perform biomedical NER using
SVM: firstly detecting the boundaries of named
entities using classifiers; secondly classifying each
named entity into predefined target types. For the
second category, a sentence was treated as a se-
quence of tokens and the objective was to find the
optimal label sequence for these tokens. The label
space was often defined as {B,I,O}, where B in-
dicates the beginning token of an entity, I denotes
the continuing token and O represents the token
outside an entity. The sequence labeling task can
be approached by Hidden Markov Model (HMM),
Conditional Random Field (CRF) , or a combina-
tion of different models (Zhou et al, 2005; Tatar
and Cicekli, 2009).
Since proposed in (Lafferty et al, 2001), CRF
has been applied to many sequence labeling
tasks, including recognizing gene mentions from
biomedical text (McDonald and Pereira, 2005).
The Gene Mention Recognition task was included
in both BioCreAtIvE I and BioCreAtIvE II chal-
lenges. CRF had been used in most of top per-
forming systems in the Gene Mention Recognition
task of BioCreAtIvE II (Smith et al, 2008). Some
novel use of linear-chain CRF was proposed. For
example, in (Kuo et al, 2007) labeling was per-
formed in forward and backward directions on the
same sentence and results were combined from
the two directions. Huang et al (2007) com-
bines a linear-chain CRF and two SVM models
11
to enhance the recall. Finkel et al (2005) used
Gibbs Sampling to add non-local dependencies
into linear-chain CRF model for information ex-
traction. However, the CRF models used in these
systems were all linear-chain CRFs. To the best of
our knowledge, no previous work has been done
on using non-linear-chain CRF in the biomedical
NER task.
Beyond the biomedical domain, skip-chain
CRF has been used in several studies to model
long distance dependency. In (Galley, 2006), skip
edges were linked between sentences with non-
local pragmatic dependencies to rank meetings.
In (Ding et al, 2008), skip-chain CRF was used
to detect the context and answers from online fo-
rums. The most close work to ours was in (Sut-
ton and McCallum, 2004), which used skip-chain
CRF to extract information from email messages
announcing seminars. By linking the same words
whose initial letter is capital, the method obtained
improvements on extracting speakers? name. Our
work is in the spirit of this idea, but we approach
it in a different way. We found that the problem is
much more difficult in the biomedical NER task:
that is why we systematically studied the princi-
ples of linking skip edges and the quality of con-
nected edges.
3 linear-chain and skip-chain CRF
Conditional Random Field is a probabilistic
graphic model. The model predicts the output
variables y for each input variables in x by calcu-
lating the conditional probability p(y|x) accord-
ing to the graph structure that represents the de-
pendencies between the y variables. Formally,
given a graph structure over y, the CRF model can
be written as:
p(y|x) =
1
Z(x)
?
Cp??
?
?c?Cp
?c(xc,yc; ?p) (1)
Z(x) is a normalization factor.
In this definition, the graph is partitioned into a
set of cliques ? = {C1, C2, . . . Cp}, where each
Cp is a clique template. Each ?c, called a factor,
is corresponding to one edge in the clique c, and
can be parameterized as:
?c(xc,yc; ?p) = exp
?
k=1
?pkfpk(xc,yc) (2)
Each feature function fpk(xc,yc) represents one
feature of x and the ?pk is the feature weight.
In the training phrase, the parameters is esti-
mated using an optimization algorithm such as
limited memory BFGS etc. In the testing phrase,
CRF finds the most likely label sequence for an
unseen instance by maximizing the probability de-
fined in (1).
In the NER task, one sentence is firstly tok-
enized into a sequences of tokens and each token
can be seen as one word. Each node in the graph is
usually corresponding to one word in a sentence.
Each x variable represents a set of features for one
word, and each y is the variable for the label of
one word. Note that when one edge is linked be-
tween two words, the edge is actually linked be-
tween their corresponding y variables. The y label
is one of {B,I,O}, in which B means the beginning
word of an entity, I means the inside word of an
entity, and O means outside an entity.
If we link each word with its immediate preced-
ing words to form a linear structure for one sen-
tence, we get a linear-chain CRF, defined as:
p?(y|x) =
1
Z(x)
T?
t=1
?t(yt, yt?1,x) (3)
This structure contains only one clique template.
If we add an extra clique template that contains
some skip edges between nonadjacent words, the
CRF become a skip-chain CRF, formulated as fol-
lows:
p?(y|x) =
1
Z(x)
T?
t=1
?t(yt, yt?1,x)?
?
(u,v)??
?uv(yu, yv,x) (4)
? is the edge set of the extra clique template con-
taining skip edges. An illustration of linear-chain
and skip-chain CRF is given in Figure 1. It is
straightforward to change a linear-chain CRF to
a skip-chain CRF by simply linking some addi-
tional skip edges. However, it must be careful to
add such edges because different graph structures
require different inference algorithms. Those in-
ference algorithms may have quite different time
complexity. For example, for the linear-chain
CRF, inference can be performed efficiently and
exactly by a dynamic-programming algorithm.
However, for the non-linear structure, approxi-
mate inference algorithms must be used. Solv-
ing arbitrary CRF graph structures is NP-hard. In
other word, we must be careful to link too many
12
Figure 1: The illustration of linear-chain CRF and skip-chain CRF. The blue edges represent the linear-
chain edges belonging to one clique template, while the red edges represent the skip edges belonging to
another clique template.
skip edges to avoid making the model impracti-
cal. Therefore, it is absolutely necessary to study
which kinds of edges will contribute to the perfor-
mance while avoiding over-connected edges.
3.1 Features
As our interest is in modifying the CRF graph
structure rather than evaluating the effectiveness
of features, we simply adopted features from the
state-of-the-art such as (McDonald and Pereira,
2005) and (Kuo et al, 2007).
? Common Features: the original word, the
stemmed word, the POS-tag of a word, the
word length, is or not the beginning or end-
ing word of the sentence etc.
? Regular Expression Features: a set of reg-
ular expressions to extract orthographic fea-
tures for the word.
? Dictionary Features: We use several lexi-
cons. For example, a protein name dictionary
compiled from SWISS-PROT, a species dic-
tionary from NCBI Taxonomy, a drug name
dictionary from DrugBank database, and a
disease name dictionary from several Internet
web site.
? N-gram Features: For each token, we ex-
tract the corresponding 2-4 grams into the
feature set.
Each word will include the adjacent words? fea-
tures within {?2,?1, 0, 1, 2} offsets. The features
used in the linear-chain CRF and skip-chain CRF
are all the same in our experiment.
4 Method
As the limitations discussed above, detecting
the necessary nodes to link should be the first
step in constructing a skip-chain CRF. In the
speaker name extraction task (Sutton and Mc-
Callum, 2004), only identical capitalized words
are linked, because there is few variations in the
speaker?s name. However, gene mentions often
involve words without obvious orthographic fea-
tures and such phenomena are common in the
biomedical literature such as RGC DNA sequence
and multisubunit TFIID protein. If we link all
the words like DNA, sequence and protein, the ef-
ficiency and performance will drop due to over-
connected edges. Therefore, the most important
step of detecting gene mentions is to determine
which edges should be connected.
4.1 Detect keywords in gene mention
We found that many gene mentions have at least
one important word for the identification of gene
mentions. For example, the word, Gal4, is such a
13
keyword in Gal4 protein and NS1A in NS1A pro-
tein. These words can distinguish gene mentions
from other common English words and phrases,
and can distinguish different gene mentions as
well. We define such words as the keyword of
a gene mention. The skip edges are limited to
only connect these keywords. We use a rule-based
method to detect keywords. By examining the an-
notated data, we defined keywords as those con-
taining at least one capital letter or digit. And at
the same time, keywords must conform to the fol-
lowing rules:
? Keywords are not stop words, single letters,
numbers, Greek letters, Roman numbers or
nucleotide sequence such as ATTCCCTGG.
? Keywords are not in the form of an upper-
case initial letter followed by lowercase let-
ters, such as Comparison and Watson. These
words have capital letters only because they
are the first word in the sentences, or they are
the names of people or other objects. This
rule will miss some correct candidates, but
reduces noise.
? Keywords do not include some common
words with capital letters such as DNA,
cDNA, RNA, mRNA, tRNA etc. and some fre-
quently appearing non-gene names such as
HIV and mmHg. We defined a lexicon for
such words on the training data.
4.2 Link similar keywords
After keyword candidates are detected, we judge
each pair of keywords in the same sentence to find
similar word pairs. Each word pair is examined by
these rules:
? They are exactly the same words.
? Words only differ in digit letters, such as
CYP1 and CYP2.
? Words with the same prefix, such as IgA and
IgG, or with the same suffix, such as ANF and
pANF.
The token pair will be linked by a skip edge if they
match at least one rule.
4.3 Link typed dependencies
Some long distance dependency cannot be de-
tected simply by string similarity. To capture such
dependency, we used stanford parser3 to parse sen-
tences and extract typed dependencies from parsed
results. The typed dependencies are a set of bi-
nary relations belonging to 55 pre-defined types to
provide a description of the grammatical relation-
ships in a sentence (Marneffe and Manning, 2008).
Some examples of typed dependencies are listed in
Table 1.
Type Description
conj conjuncted by the conjunc-
tion such as and
prep prepositional modifier
nn noun compound modifier
amod adjectival modifier
dep uncertain types
Table 1: Examples for typed dependencies.
The output of the parser is pairs of dependent
words, along with typed dependencies between
two words in a pair. For example, in the sentence:
?. . . and activate transcription of a set
of genes that includes G1 cyclins CLN1,
CLN2, and many DN, synthesis genes.?
a typed dependency nn(G1,CLN1) is extracted by
the parser, meaning the words G1 and CLN1 has a
typed dependency of nn because they form a noun
phrase under a dependency grammar: modifica-
tion. Similarly, in the sentence
?Using the same approach we have
shown that hFIRE binds the stimula-
tory proteins Sp1 and Sp3 in addition to
CBF.?
the words Sp1 and Sp3 can be detected to have a
typed dependency of conj and, and the two words
have a typed denpendency of prep in addition to
with CBF, respectively. The most common type
dependencies are conj and, nn and dep. The key-
words having typed dependencies will be linked
by a skip edge.
5 Experiment
We tested our method on two datasets: the Gene
Mention (GM) data in BioCreAtIvE II (BCIIGM)
3http://nlp.stanford.edu/software/
lex-parser.shtml
14
4and GENIA corpus5. The BCIIGM dataset was
used in the BioCreAtIvE II Gene Mention Recog-
nition task in 2006. It was built from the GENE-
TAG corpus (Tanabe et al, 2005) with some mod-
ification of the annotation. The dataset contains
15000 sentences for training and 5000 sentences
for testing. Two gold-standard sets, GENE and
ALTGENE, were provided for evaluation and an
official evaluation procedure in Perl script was
provided. The ALTGENE set provides alternate
forms for genes in the GENE set. In the official
evaluation, each identified string will be looked up
in both GENE and ALTGENE. If the correspond-
ing gene was found in either GENE or ALTGENE,
the identified string will be counted as a correct
answer.
The GENIA corpus is a widely used dataset in
many NER and information extraction tasks due
to its high quality annotation. The GENIA corpus
contains 2000 abstracts from MEDLINE, with ap-
proximately 18500 sentences. The corpus was an-
notated by biomedical experts according to a pre-
defined GENIA ontology. In this work, we only
used the annotated entities that have a category of
protein, DNA, or RNA. These categories are re-
lated to the definition of gene mention in BioCre-
AtIvE II. We only used strict matching evaluation
(no alternate forms check) for the GENIA corpus
as no ALTGENE-like annotation is available.
The performance is measured by precision, re-
call and F score. Each identified string is counted
as a true positive (TP) if it is matched by a gold-
standard gene mention, otherwise the identified
string is a false positive (FP). Each gold standard
gene mention is counted as a false negative (FN) if
it is not identified by the approach. Then the pre-
cision, recall and their harmonic average F score
is calculated as follows:
precision =
TP
TP + FP
recall =
TP
TP + FN
F =
2 ? precision ? recall
precision+ recall
To implement both linear-chain CRF and skip-
4http://sourceforge.net/projects/
biocreative/files/
5http://www-tsujii.is.s.u-tokyo.ac.jp/
GENIA/home/wiki.cgi?page=Technical+Term+
Annotation
chain CRF, we used the GRMM Java package6
which is an extended version of MALLET. The
package provides an implement of arbitrary struc-
ture CRF.
5.1 Result Comparison
We evaluated our approach on the BCIIGM
dataset and GENIA corpus. For the BCIIGM
dataset, two evaluation criteria were used: official
- exactly the same as that used in the BioCreAtIvE
II competition, with the official evaluation proce-
dure; and strict - strict matching for each identi-
fied string without checking its alternate forms in
ALTGENE. The GENIA dataset were randomly
divided into 10 parts to perform a 10-fold cross
validation. However, we didn?t do cross validation
on the BCIIGM dataset because the BioCreAtIvE
II competition annotations and evaluation proce-
dure were tailored to evaluating participating sys-
tems.
The comparative results are listed in Table 2.
We compared the two edge linking principles,
linking similar words and linking words having
typed dependencies. The F score from the skip-
chain CRF is better than that from the linear-chain
CRF. Significance tests were performed to check
whether these results have significant differences.
Paired two-tail t-tests were conducted with respect
to the F scores of linear-chain CRF vs. those of the
two skip-chain CRFs, respectively. The p-value
was 1.989?10?7 for the skip-chain CRF linked by
similar words vs. linear-chain CRF. The p-value
was 3.971 ? 10?5 for the skip-chain CRF linked
by typed dependencies vs. linear-chain CRF. This
shows that the improvement is significant.
Note that we did not compare our results on the
BCIIGM dataset to those submitted to the compe-
tition. There are two reasons for this: First, our
focus is on comparing the skip-chain CRF with
the linear-chain CRF. Second, in the competition,
most participating systems that used CRF also
applied other algorithms, or sophisticated rules
for adjusting detected boundaries or refining the
recognized results, to achieve competitive perfor-
mance. By contrast, we did not employ any post-
processing rule or algorithm to further improve the
performance. In this sense, comparing our results
to those has become unfair.
6http://mallet.cs.umass.edu/grmm/
index.php
15
Data Model Precision(%) Recall(%) F score(%)
BCIIGM official
linear-chain CRF 85.16 81.50 83.29
skip-chain CRF linked by sim-words 86.68 82.75 84.67
skip-chain CRF linked by typed-dep 86.73 82.36 84.49
BCIIGM strict
linear-chain CRF 74.09 69.49 71.73
skip-chain CRF linked by sim-words 76.26 71.53 73.82
skip-chain CRF linked by typed-dep 75.99 70.49 73.14
GENIA
linear-chain CRF 76.77 74.92 75.83
skip-chain CRF linked by sim-words 78.57 77.12 77.82
skip-chain CRF linked by typed-dep 78.18 76.87 77.52
Table 2: The result comparison between the linear-chain CRF and skip-chain CRF. BCIIGM is the
BioCreAtIvE II Gene Mention Recognition dataset. official means using the official provided evalua-
tion procedure and strict means using strict matching to evaluate the results. sim-words means similar
words and typed-dep means typed dependencies. The results for GENIA are averaged over 10-fold cross
validation.
5.2 Discussion
We provided in-depth analysis of our results on the
BCIIGM dataset. As one of our motivations for
connecting words with skip edges is to enhance
the consistency of labeling, we firstly examined
whether the proposed approach can provide con-
sistent labeling. Let us start from two typical ex-
amples. In the first sentence
?The response sequences were localized
between -67 and +30 in the simian cy-
tomegalovirus IE94 promoter and up-
stream of position +9 in the HCMV IE68
promoter.?
the word IE94 is missed (not labeled) while its
similar word IE68 is labeled correctly by the
linear-chain CRF. In the second sentence
?It is suggested that biliary secretion of
both TBZ and FBZ and their metabolites
may contribute to this recycling.?
the word TBZ is labeled as a gene mention in-
correctly (false positive) while its similar word
FBZ is not labeled at all (true negative) by the
linear-chain CRF. Both sentences are correctly la-
beled by the skip-chain CRF. Similar improve-
ments are also made by the skip-chain CRF model
linked by typed dependencies. To study label-
ing consistency, we counted the statistics of in-
consistency errors, as shown in Table 3. Two
kinds of inconsistency errors were counted: false
negatives correctable by consistency (FNCC) and
false positives correctable by consistency (FPCC).
An FNCC means that a gold-standard mention is
missed by the system while its skip edge linked
gene mention is correctly labeled, which is simi-
lar to the inconsistent miss in (Sutton and McCal-
lum, 2004), as the IE94 in the first example. An
FPCC means a non-gene mention is labeled as a
gene while its skip edge linked mention (also non-
gene mention) is not recognized, as TBZ in the sec-
ond example. These two kinds of inconsistency er-
rors lead to inconsistent false negatives (FN) and
false positives (FP). A good model should reduce
as much inconsistency errors as possible. The in-
consistency errors are reduced substantially as we
expected, showing that the reduction of inconsis-
tency errors is one reason for the performance im-
provements.
The skip-chain CRF linked by similar words
had better performance than the skip-chain CRF
linked by typed dependencies. This may infer that
the quality of skip edges has impact on the per-
formance. In order to study this issue, the qual-
ity of skip edges was examined. The statistics of
skip edges in the BCIIGM dataset for the two skip-
chain CRF models (linked by similar words and by
typed dependencies respectively) is shown in the
first two rows of Table 4. A skip edge is counted as
a correct edge if the edge links two words that are
both gene mentions in the gold-standard annota-
tion. The statistics shows that the skip-chain CRF
linked by similar words has a higher precision than
the model by typed dependencies. To make the
comparison more evident, we built another skip-
chain CRF whose skip edges were randomly con-
nected. The number of skip edges in this model
16
Skip edge
Model FPCC FNCC
type
sim-words
linear-chain 112 70
skip-chain 48 20
Percentage of reduction 57.14% 71.43%
typed-dep
linear-chain 32 29
skip-chain 9 5
Percentage of reduction 71.88% 82.76%
Table 3: Statistics of inconsistency errors for
the linear-chain CRF and skip-chain CRF. FPCC
is false positives correctable by consistency and
FNCC is false negatives correctable by consis-
tency in the table. The percentage is calculated
by dividing the reduction of errors by the error
number of linear-chain CRF, for example (112 ?
48)/48 = 57.14%.
approximately equals to that in the skip-chain CRF
linked by similar words. The percentage of cor-
rect skip-edges in this model is small, as shown
in the last row of Table 4. We tested this skip-
chain CRF model on the BCIIGM dataset under
the strict matching criterion. The performance of
the randomly linked skip-chain CRF is shown in
Table 5. As can be seen from the table, the perfor-
mance of the randomly connected skip-chain CRF
droped remarkably, even worse than that of the
linear-chain CRF. This confirms that the quality
of skip edges is a key factor for the performance
improvement.
Model Edges
Correct
Percentage
edges
sim-words 1912 1344 70.29%
typed-dep 728 425 53.38%
random 1906 41 2.15%
Table 4: Statistics of skip edges and correct
skip edges for the skip-chain CRF models. sim-
words means the skip-chain CRF linked by sim-
ilar words, typed-dep means the CRF linked by
typed dependencies and random means the skip-
chain CRF has randomly connected skip edges.
The edges are counted in the BCIIGM testing data.
From the above discussion, we summarize this
section as follows: (1) the skip-chain CRF with
high quality skip edges can reduce inconsistent la-
beling errors, and (2) the quality of skip edges is
crucial to the performance improvement.
Model P (%) R(%) F(%)
linear 74.09 69.49 71.73
sim-words 76.26 71.53 73.82
typed-dep 75.99 70.49 73.14
random 73.66 69.13 71.32
Table 5: Performance comparison between the
randomly linked skip-chain CRF and other mod-
els. The result was tested on the BCIIGM dataset
under the strict matching criterion. P, R and F
denote the precision, recall and F score respec-
tively. linear denotes the linear-chain CRF. sim-
words denotes the skip-chain CRF linked by sim-
ilar words. typed-dep denotes the skip-chain CRF
linked by typed dependencies. random denotes
the skip-chain CRF having randomly linked skip
edges.
6 Conclusion
This paper proposed a method to construct a skip-
chain CRF to perform named entity recognition in
the biomedical literature. We presented two prin-
ciples to connect skip edges to address the issue
of capturing long distance dependency: linking
similar keywords and linking words having typed
dependencies. We evaluated our method on the
BioCreAtIvE II GM dataset and GENIA corpus.
Significant improvements were observed. More-
over, we presented in-depth analysis on inconsis-
tent labeling errors and the quality of skip edges.
The study shows that the quality of linked edges is
a key factor of the system performance.
The quality of linked edges plays an important
role in not only performance but also time effi-
ciency. Thus, we are planning to apply machine
learning techniques to automatically induce pat-
terns for linking high-quality skip-edges. Further-
more, to refine the recognition results, we are plan-
ning to employ post-processing algorithms or con-
struct refinement rules.
Acknowledgments
This work was partly supported by the Chi-
nese Natural Science Foundation under grant No.
60803075 and No.60973104, and partly carried
out with the aid of a grant from the International
Development Research Center, Ottawa, Canada
IRCI project from the International Development.
17
References
Shilin Ding, Gao Cong, Chin-Yew Lin and Xiaoyan
Zhu. 2008. Using Conditional Random Fields to
Extract Contexts and Answers of Questions from On-
line Forums. In Proceedings of 46th Annual Meet-
ing of the Association for Computational Linguistics
(ACL?08), pp 710-718.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating Non-local Informa-
tion into Information Extraction Systems by Gibbs
Sampling. Proceedings of the 43rd Annual Meeting
of the ACL, pages 363C370.
K. Fukuda, A. Tamura, T. Tsunoda and T. Takagi.
1998. Toward information extraction: identifying
protein names from biological papers. Pacific Sym-
posium on Biocomputing. 1998.
Michel Galley. 2006. A Skip-Chain Conditional Ran-
dom Field for Ranking Meeting Utterances by Im-
portance. Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2006), pages 364-372.
Han-Shen Huang, Yu-Shi Lin, Kuan-Ting Lin, Cheng-
Ju Kuo, Yu-Ming Chang, Bo-Hou Yang, I-Fang
Chung and Chun-Nan Hsu. 2007. High-recall gene
mention recognition by unification of multiple back-
ward parsing models. Proceedings of the Second
BioCreative Challenge Evaluation Workshop, pages
109-111.
Jun?ichi Kazama, Takaki Makino, Yoshihiro Ohta and
Jun?ichi Tsujii. 2002. Tuning support vector
machines for biomedical named entity recognition.
Proceedings of the ACL-02 workshop on Natural
language processing in the biomedical domain - Vol-
ume 3.
Corinna Kolarik, Martin Hofmann-Apitius, Marc Zim-
mermann and Juliane Fluck. 2007. Identification of
new drug classification terms in textual resources.
Bioinformatics 2007 23(13):i264-i272
Cheng-Ju Kuo, Yu-Ming Chang, Han-Shen Huang,
Kuan-Ting Lin, Bo-Hou Yang, Yu-Shi Lin, Chun-
Nan Hsu and I-Fang Chung. 2007. Rich Feature
Set, Unification of Bidirectional Parsing and Dictio-
nary Filtering for High F-Score Gene Mention Tag-
ging. Proceedings of the Second BioCreative Chal-
lenge Evaluation Workshop, pages 105-107.
Ki-Joong Lee, Young-Sook Hwang, Seonho Kim and
Hae-Chang Rim. 2004. Biomedical named entity
recognition using two-phase model based on SVMs.
Journal of Biomedical Informatics, Volume 37, Issue
6, December 2004, Pages 436-447.
John Lafferty, Andrew McCallum and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proc. ICML-01, pages 282-289,
2001.
Ryan McDonald and Fernando Pereira. 2005. Identi-
fying gene and protein mentions in text using con-
ditional random fields. BMC Bioinformatics 2005,
6(Suppl 1):S6.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. Stanford typed dependencies manual.
M. Narayanaswamy, K.E. Ravikumar and K. Vijay-
Shanker. 2003. A biological named entity recog-
nizer. Pacific Symposium on Biocomputing. 2003.
Dietrich Rebholz-Schuhmann, Harald Kirsch, Miguel
Arregui, Sylvain Gaudan, Mark Riethoven and Pe-
ter Stoehr. 2007. EBIMed?text crunching to gather
facts for proteins from Medline. Bioinformatics
2007 23(2):e237-e244
Fei Sha and Fernando Pereira. 2003. Shallow Pars-
ing with Conditional Random Fields. Proceedings
of HLT-NAACL 2003, Main Papers, pp.134-141
Larry Smith, Lorraine K Tanabe, et al 2008.
Overview of BioCreative II gene mention recogni-
tion. Genome Biology 2008, 9(Suppl 2):S2.
Charles Sutton and Andrew McCallum. 2004. Collec-
tive Segmentation and Labeling of Distant Entities
in Information Extraction. ICML workshop on Sta-
tistical Relational Learning, 2004.
Lorraine Tanabe, Natalie Xie, Lynne H Thom, Wayne
Matten and W John Wilbur. 2005. GENETAG: a
tagged corpus for gene/protein named entity recog-
nition . BMC Bioinformatics 2005, 6(Suppl 1):S3
Serhan Tatar and Ilyas Cicekli. 2009. Two learning
approaches for protein name extraction. Journal of
Biomedical Informatics 42(2009) 1046-1055
Xinglong Wang, Jun?ichi Tsujii and Sophia Anani-
adou. 2010. Disambiguating the species of biomed-
ical named entities using natural language parsers.
Bioinformatics 2010 26(5):661-667
Zhihao Yang, Hongfei Lin and Yanpeng Li. 2008. Ex-
ploiting the performance of dictionary-based bio-
entity name recognition in biomedical literature.
Computational Biology and Chemistry 32(2008)
287-291.
Alexander Yeh, Alexander Morgan, Marc Colosimo
and Lynette Hirschman. 2005. BioCreAtIvE Task
1A: gene mention finding evaluation. BMC Bioin-
formatics 2005, 6(Suppl 1):S2.
GuoDong Zhou, Jie Zhang, Jian Su, Dan Shen,
ChewLim Tan. 2004. Recognizing names in
biomedical texts: a machine learning approach.
Bioinformatics 2004, Vol.20(7),pp.1178C1190.
GuoDong Zhou, Dan Shen, Jie Zhang, Jian Su1 and
SoonHeng Tan. 2005. Recognition of protein/gene
names from text using an ensemble of classifiers.
BMC Bioinformatics 2005, 6(Suppl 1):S7.
18
