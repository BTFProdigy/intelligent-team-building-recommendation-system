Proceedings of ACL-08: HLT, pages 245?253,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Exploiting Feature Hierarchy for Transfer Learning in Named Entity
Recognition
Andrew Arnold, Ramesh Nallapati and William W. Cohen
Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA, USA
{aarnold, nmramesh, wcohen}@cs.cmu.edu
Abstract
We present a novel hierarchical prior struc-
ture for supervised transfer learning in named
entity recognition, motivated by the common
structure of feature spaces for this task across
natural language data sets. The problem of
transfer learning, where information gained in
one learning task is used to improve perfor-
mance in another related task, is an important
new area of research. In the subproblem of do-
main adaptation, a model trained over a source
domain is generalized to perform well on a re-
lated target domain, where the two domains?
data are distributed similarly, but not identi-
cally. We introduce the concept of groups
of closely-related domains, called genres, and
show how inter-genre adaptation is related to
domain adaptation. We also examine multi-
task learning, where two domains may be re-
lated, but where the concept to be learned in
each case is distinct. We show that our prior
conveys useful information across domains,
genres and tasks, while remaining robust to
spurious signals not related to the target do-
main and concept. We further show that our
model generalizes a class of similar hierarchi-
cal priors, smoothed to varying degrees, and
lay the groundwork for future exploration in
this area.
1 Introduction
1.1 Problem definition
Consider the task of named entity recognition
(NER). Specifically, you are given a corpus of news
articles in which all tokens have been labeled as ei-
ther belonging to personal name mentions or not.
The standard supervised machine learning problem
is to learn a classifier over this training data that will
successfully label unseen test data drawn from the
same distribution as the training data, where ?same
distribution? could mean anything from having the
train and test articles written by the same author to
having them written in the same language. Having
successfully trained a named entity classifier on this
news data, now consider the problem of learning to
classify tokens as names in e-mail data. An intuitive
solution might be to simply retrain the classifier, de
novo, on the e-mail data. Practically, however, large,
labeled datasets are often expensive to build and this
solution would not scale across a large number of
different datasets.
Clearly the problems of identifying names in
news articles and e-mails are closely related, and
learning to do well on one should help your per-
formance on the other. At the same time, however,
there are serious differences between the two prob-
lems that need to be addressed. For instance, cap-
italization, which will certainly be a useful feature
in the news problem, may prove less informative in
the e-mail data since the rules of capitalization are
followed less strictly in that domain.
These are the problems we address in this paper.
In particular, we develop a novel prior for named
entity recognition that exploits the hierarchical fea-
ture space often found in natural language domains
(?1.2) and allows for the transfer of information
from labeled datasets in other domains (?1.3). ?2
introduces the maximum entropy (maxent) and con-
ditional random field (CRF) learning techniques em-
ployed, along with specifications for the design and
training of our hierarchical prior. Finally, in ?3 we
present an empirical investigation of our prior?s per-
formance against a number of baselines, demonstrat-
ing both its effectiveness and robustness.
1.2 Hierarchical feature trees
In many NER problems, features are often con-
structed as a series of transformations of the input
training data, performed in sequence. Thus, if our
task is to identify tokens as either being (O)utside or
(I)nside person names, and we are given the labeled
245
sample training sentence:
O O O O O I
Give the book to Professor Caldwell
(1)
one such useful feature might be: Is the token one
slot to the left of the current token Professor?
We can represent this symbolically as L.1.Professor
where we describe the whole space of useful features
of this form as: {direction = (L)eft, (C)urrent,
(R)ight}.{distance = 1, 2, 3, ...}.{value = Pro-
fessor, book, ...}. We can conceptualize this struc-
ture as a tree, where each slot in the symbolic name
of a feature is a branch and each period between slots
represents another level, going from root to leaf as
read left to right. Thus a subsection of the entire fea-
ture tree for the token Caldwell could be drawn
as in Figure 1 (zoomed in on the section of the tree
where the L.1.Professor feature resides).
direction
L
C
R
distance
1 2 ...
... ...
value
Professor
book
...
... ...
true false ...
Figure 1: Graphical representation of a hierarchical fea-
ture tree for token Caldwell in example Sentence 1.
Representing feature spaces with this kind of tree,
besides often coinciding with the explicit language
used by common natural language toolkits (Cohen,
2004), has the added benefit of allowing a model to
easily back-off, or smooth, to decreasing levels of
specificity. For example, the leaf level of the fea-
ture tree for our sample Sentence 1 tells us that the
word Professor is important, with respect to la-
beling person names, when located one slot to the
left of the current word being classified. This may
be useful in the context of an academic corpus, but
might be less useful in a medical domain where the
word Professor occurs less often. Instead, we
might want to learn the related feature L.1.Dr. In
fact, it might be useful to generalize across multiple
domains the fact that the word immediately preced-
ing the current word is often important with respect
LeftToken.*
LeftToken.IsWord.*
LeftToken.IsWord.IsTitle.*
LeftToken.IsWord.IsTitle.equals.*
LeftToken.IsWord.IsTitle.equals.mr
Table 1: A few examples of the feature hierarchy
to the named entity status of the current word. This
is easily accomplished by backing up one level from
a leaf in the tree structure to its parent, to represent
a class of features such as L.1.*. It has been shown
empirically that, while the significance of particular
features might vary between domains and tasks, cer-
tain generalized classes of features retain their im-
portance across domains (Minkov et al, 2005). By
backing-off in this way, we can use the feature hier-
archy as a prior for transferring beliefs about the sig-
nificance of entire classes of features across domains
and tasks. Some examples illustrating this idea are
shown in table 1.
1.3 Transfer learning
When only the type of data being examined is al-
lowed to vary (from news articles to e-mails, for
example), the problem is called domain adapta-
tion (Daume? III and Marcu, 2006). When the task
being learned varies (say, from identifying person
names to identifying protein names), the problem
is called multi-task learning (Caruana, 1997). Both
of these are considered specific types of the over-
arching transfer learning problem, and both seem
to require a way of altering the classifier learned
on the first problem (called the source domain, or
source task) to fit the specifics of the second prob-
lem (called the target domain, or target task).
More formally, given an example x and a class
label y, the standard statistical classification task
is to assign a probability, p(y|x), to x of belong-
ing to class y. In the binary classification case the
labels are Y ? {0, 1}. In the case we examine,
each example xi is represented as a vector of bi-
nary features (f1(xi), ? ? ? , fF (xi)) where F is the
number of features. The data consists of two dis-
joint subsets: the training set (Xtrain, Ytrain) =
{(x1, y1) ? ? ? , (xN , yN )}, available to the model for
its training and the test set Xtest = (x1, ? ? ? , xM ),
upon which we want to use our trained classifier to
make predictions.
246
In the paradigm of inductive learning,
(Xtrain, Ytrain) are known, while both Xtest and
Ytest are completely hidden during training time. In
this cases Xtest and Xtrain are both assumed to have
been drawn from the same distribution, D. In the
setting of transfer learning, however, we would like
to apply our trained classifier to examples drawn
from a distribution different from the one upon
which it was trained. We therefore assume there
are two different distributions, Dsource and Dtarget,
from which data may be drawn. Given this notation
we can then precisely state the transfer learning
problem as trying to assign labels Y targettest to test
data Xtargettest drawn from Dtarget, given training
data (Xsourcetrain , Y sourcetrain ) drawn from Dsource.
In this paper we focus on two subproblems of
transfer learning:
? domain adaptation, where we assume Y (the set
of possible labels) is the same for both Dsource
and Dtarget, while Dsource and Dtarget them-
selves are allowed to vary between domains.
? multi-task learning (Ando and Zhang, 2005;
Caruana, 1997; Sutton and McCallum, 2005;
Zhang et al, 2005) in which the task (and label
set) is allowed to vary from source to target.
Domain adaptation can be further distinguished by
the degree of relatedness between the source and tar-
get domains. For example, in this work we group
data collected in the same medium (e.g., all anno-
tated e-mails or all annotated news articles) as be-
longing to the same genre. Although the specific
boundary between domain and genre for a particu-
lar set of data is often subjective, it is nevertheless a
useful distinction to draw.
One common way of addressing the transfer
learning problem is to use a prior which, in conjunc-
tion with a probabilistic model, allows one to spec-
ify a priori beliefs about a distribution, thus bias-
ing the results a learning algorithm would have pro-
duced had it only been allowed to see the training
data (Raina et al, 2006). In the example from ?1.1,
our belief that capitalization is less strict in e-mails
than in news articles could be encoded in a prior that
biased the importance of the capitalization
feature to be lower for e-mails than news articles.
In the next section we address the problem of how
to come up with a suitable prior for transfer learning
across named entity recognition problems.
2 Models considered
2.1 Basic Conditional Random Fields
In this work, we will base our work on Condi-
tional Random Fields (CRF?s) (Lafferty et al, 2001),
which are now one of the most preferred sequential
models for many natural language processing tasks.
The parametric form of the CRF for a sentence of
length n is given as follows:
p?(Y = y|x) =
1
Z(x) exp(
n
?
i=1
F
?
j=1
fj(x, yi)?j)
(2)
where Z(x) is the normalization term. CRF learns a
model consisting of a set of weights ? = {?1...?F }
over the features so as to maximize the conditional
likelihood of the training data, p(Ytrain|Xtrain),
given the model p?.
2.2 CRF with Gaussian priors
To avoid overfitting the training data, these ??s are
often further constrained by the use of a Gaussian
prior (Chen and Rosenfeld, 1999) with diagonal co-
variance, N (?, ?2), which tries to maximize:
argmax
?
N
?
k=1
(
log p?(yk|xk)
)
? ?
F
?
j
(?j ? ?j)2
2?2j
where ? > 0 is a parameter controlling the amount
of regularization, and N is the number of sentences
in the training set.
2.3 Source trained priors
One recently proposed method (Chelba and Acero,
2004) for transfer learning in Maximum Entropy
models 1 involves modifying the ??s of this Gaussian
prior. First a model of the source domain, ?source,
is learned by training on {Xsourcetrain , Y sourcetrain }. Then a
model of the target domain is trained over a limited
set of labeled target data
{
Xtargettrain , Y
target
train
}
, but in-
stead of regularizing this ?target to be near zero (i.e.
setting ? = 0), ?target is instead regularized to-
wards the previously learned source values ?source
(by setting ? = ?source, while ?2 remains 1) and
thus minimizing (?target ? ?source)2.
1Maximum Entropy models are special cases of CRFs that
use the I.I.D. assumption. The method under discussion can
also be extended to CRF directly.
247
Note that, since this model requires Y targettrain in or-
der to learn ?target, it, in effect, requires two distinct
labeled training datasets: one on which to train the
prior, and another on which to learn the model?s fi-
nal weights (which we call tuning), using the previ-
ously trained prior for regularization. If we are un-
able to find a match between features in the training
and tuning datasets (for instance, if a word appears
in the tuning corpus but not the training), we back-
off to a standard N (0, 1) prior for that feature.
3
y
x i
i
(1)
(1)
(1)M
w (1)1
y
x i
i
(
M
y
x i
i
(
M
(2)
2)
(2)
(3)
3)
(3)
w w (1) w (1) w1 w w w1 w(1)2 3 4 (2) (2) (2)2 3 (3) (3)2
z z
z
1 2
Figure 2: Graphical representation of the hierarchical
transfer model.
2.4 New model: Hierarchical prior model
In this section, we will present a new model that
learns simultaneously from multiple domains, by
taking advantage of our feature hierarchy.
We will assume that there are D domains on
which we are learning simultaneously. Let there be
Md training data in each domain d. For our experi-
ments with non-identically distributed, independent
data, we use conditional random fields (cf. ?2.1).
However, this model can be extended to any dis-
criminative probabilistic model such as the MaxEnt
model. Let ?(d) = (?(d)1 , ? ? ? , ?
(d)
Fd ) be the param-
eters of the discriminative model in the domain d
where Fd represents the number of features in the
domain d.
Further, we will also assume that the features of
different domains share a common hierarchy repre-
sented by a tree T , whose leaf nodes are the features
themselves (cf. Figure 1). The model parameters
?(d), then, form the parameters of the leaves of this
hierarchy. Each non-leaf node n ? non-leaf(T ) of
the tree is also associated with a hyper-parameter zn.
Note that since the hierarchy is a tree, each node n
has only one parent, represented by pa(n). Simi-
larly, we represent the set of children nodes of a node
n as ch(n).
The entire graphical model for an example con-
sisting of three domains is shown in Figure 2.
The conditional likelihood of the entire training
data (y,x) = {(y(d)1 ,x
(d)
1 ), ? ? ? , (y
(d)
Md ,x
(d)
Md)}
D
d=1 is
given by:
P (y|x,w, z) =
{ D
?
d=1
Md
?
k=1
P (y(d)k |x
(d)
k ,?(d))
}
?
?
?
?
D
?
d=1
Fd
?
f=1
N (?(d)f |zpa(f (d)), 1)
?
?
?
?
?
?
?
?
n?Tnonleaf
N (zn|zpa(n), 1)
?
?
?
(3)
where the terms in the first line of eq. (3) represent
the likelihood of data in each domain given their cor-
responding model parameters, the second line repre-
sents the likelihood of each model parameter in each
domain given the hyper-parameter of its parent in the
tree hierarchy of features and the last term goes over
the entire tree T except the leaf nodes. Note that in
the last term, the hyper-parameters are shared across
the domains, so there is no product over d.
We perform a MAP estimation for each model pa-
rameter as well as the hyper-parameters. Accord-
ingly, the estimates are given as follows:
?(d)f =
Md
?
i=1
?
??(d)f
(
logP (ydi |x(d)i ,?(d))
)
+ zpa(f (d))
zn =
zpa(n) +
?
i?ch(n)(?|z)i
1 + |ch(n)| (4)
where we used the notation (?|z)i because node i,
the child node of n, could be a parameter node or
a hyper-parameter node depending on the position
of the node n in the hierarchy. Essentially, in this
model, the weights of the leaf nodes (model param-
eters) depend on the log-likelihood as well as the
prior weight of its parent. Additionally, the weight
248
of each hyper-parameter node in the tree is com-
puted as the average of all its children nodes and its
parent, resulting in a smoothing effect, both up and
down the tree.
2.5 An approximate Hierarchical prior model
The Hierarchical prior model is a theoretically well
founded model for transfer learning through feature
heirarchy. However, our preliminary experiments
indicated that its performance on real-life data sets is
not as good as expected. Although a more thorough
investigation needs to be carried out, our analysis in-
dicates that the main reason for this phenomenon is
over-smoothing. In other words, by letting the infor-
mation propagate from the leaf nodes in the hierar-
chy all the way to the root node, the model loses its
ability to discriminate between its features.
As a solution to this problem, we propose an
approximate version of this model that weds ideas
from the exact heirarchical prior model and the
Chelba model.
As with the Chelba prior method in ?2.3, this ap-
proximate hierarchical method also requires two dis-
tinct data sets, one for training the prior and another
for tuning the final weights. Unlike Chelba, we
smooth the weights of the priors using the feature-
tree hierarchy presented in ?1.1, like the hierarchical
prior model.
For smoothing of each feature weight, we chose to
back-off in the tree as little as possible until we had a
large enough sample of prior data (measured as M ,
the number of subtrees below the current node) on
which to form a reliable estimate of the mean and
variance of each feature or class of features. For
example, if the tuning data set is as in Sentence
1, but the prior contains no instances of the word
Professor, then we would back-off and compute
the prior mean and variance on the next higher level
in the tree. Thus the prior for L.1.Professor would
be N (mean(L.1.*), variance(L.1.*)), where mean()
and variance() of L.1.* are the sample mean and
variance of all the features in the prior dataset that
match the pattern L.1.* ? or, put another way, all the
siblings of L.1.Professor in the feature tree. If fewer
than M such siblings exist, we continue backing-off,
up the tree, until an ancestor with sufficient descen-
dants is found. A detailed description of the approx-
imate hierarchical algorithm is shown in table 2.
Input: Dsource = (Xsourcetrain , Y sourcetrain )
Dtarget = (Xtargettrain , Y
target
train );
Feature sets Fsource, F target;
Feature HierarchiesHsource,Htarget
Minimum membership size M
Train CRF using Dsource to obtain
feature weights ?source
For each feature f ? F target
Initialize: node n = f
While (n /? Hsource
or |Leaves(Hsource(n))| ?M)
and n 6= root(Htarget)
n? Pa(Htarget(n))
Compute ?f and ?f using the sample
{?sourcei | i ? Leaves(Hsource(n))}
Train Gaussian prior CRF using Dtarget as data
and {?f} and {?f} as Gaussian prior parameters.
Output:Parameters of the new CRF ?target.
Table 2: Algorithm for approximate hierarchical prior:
Pa(Hsource(n)) is the parent of node n in feature hierar-
chy Hsource; |Leaves(Hsource(n))| indicates the num-
ber of leaf nodes (basic features) under a node n in the
hierarchyHsource.
It is important to note that this smoothed tree is
an approximation of the exact model presented in
?2.4 and thus an important parameter of this method
in practice is the degree to which one chooses to
smooth up or down the tree. One of the benefits
of this model is that the semantics of the hierarchy
(how to define a feature, a parent, how and when
to back-off and up the tree, etc.) can be specified
by the user, in reference to the specific datasets and
tasks under consideration. For our experiments, the
semantics of the tree are as presented in ?1.1.
The Chelba method can be thought of as a hier-
archical prior in which no smoothing is performed
on the tree at all. Only the leaf nodes of the
prior?s feature tree are considered, and, if no match
can be found between the tuning and prior?s train-
ing datasets? features, a N (0, 1) prior is used in-
stead. However, in the new approximate hierarchical
model, even if a certain feature in the tuning dataset
does not have an analog in the training dataset, we
can always back-off until an appropriate match is
found, even to the level of the root.
Henceforth, we will use only the approximate hi-
erarchical model in our experiments and discussion.
249
Table 3: Summary of data used in experiments
Corpus Genre Task
UTexas Bio Protein
Yapex Bio Protein
MUC6 News Person
MUC7 News Person
CSPACE E-mail Person
3 Investigation
3.1 Data, domains and tasks
For our experiments, we have chosen five differ-
ent corpora (summarized in Table 3). Although
each corpus can be considered its own domain (due
to variations in annotation standards, specific task,
date of collection, etc), they can also be roughly
grouped into three different genres. These are: ab-
stracts from biological journals [UT (Bunescu et al,
2004), Yapex (Franze?n et al, 2002)]; news articles
[MUC6 (Fisher et al, 1995), MUC7 (Borthwick et
al., 1998)]; and personal e-mails [CSPACE (Kraut
et al, 2004)]. Each corpus, depending on its genre,
is labeled with one of two name-finding tasks:
? protein names in biological abstracts
? person names in news articles and e-mails
We chose this array of corpora so that we could
evaluate our hierarchical prior?s ability to generalize
across and incorporate information from a variety of
domains, genres and tasks.
In each case, each item (abstract, article or e-mail)
was tokenized and each token was hand-labeled as
either being part of a name (protein or person) or
not, respectively. We used a standard natural lan-
guage toolkit (Cohen, 2004) to compute tens of
thousands of binary features on each of these to-
kens, encoding such information as capitalization
patterns and contextual information from surround-
ing words. This toolkit produces features of the type
described in ?1.2 and thus was amenable to our hi-
erarchical prior model. In particular, we chose to
use the simplest default, out-of-the-box feature gen-
erator and purposefully did not use specifically en-
gineered features, dictionaries, or other techniques
commonly employed to boost performance on such
tasks. The goal of our experiments was to see to
what degree named entity recognition problems nat-
urally conformed to hierarchical methods, and not
just to achieve the highest performance possible.
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0 20 40 60 80 100
F1
Percent of target-domain data used for tuning
Intra-genre transfer performance evaluated on MUC6
(a) GAUSS: tuned on MUC6
(b) CAT: tuned on MUC6+7
(c) HIER: MUC6+7 prior, tuned on MUC6
(d) CHELBA: MUC6+7 prior, tuned on MUC6
Figure 3: Adding a relevant HIER prior helps compared
to the GAUSS baseline ((c) > (a)), while simply CAT?ing
or using CHELBA can hurt ((d) ? (b) < (a), except with
very little data), and never beats HIER ((c) > (b) ? (d)).
3.2 Experiments & results
We evaluated the performance of various transfer
learning methods on the data and tasks described
in ?3.1. Specifically, we compared our approximate
hierarchical prior model (HIER), implemented as a
CRF, against three baselines:
? GAUSS: CRF model tuned on a single domain?s
data, using a standard N (0, 1) prior
? CAT: CRF model tuned on a concatenation of
multiple domains? data, using a N (0, 1) prior
? CHELBA: CRF model tuned on one domain?s
data, using a prior trained on a different, related
domain?s data (cf. ?2.3)
We use token-level F1 as our main evaluation mea-
sure, combining precision and recall into one metric.
3.2.1 Intra-genre, same-task transfer learning
Figure 3 shows the results of an experiment in
learning to recognize person names in MUC6 news
articles. In this experiment we examined the effect
of adding extra data from a different, but related do-
main from the same genre, namely, MUC7. Line
a shows the F1 performance of a CRF model tuned
only on the target MUC6 domain (GAUSS) across a
range of tuning data sizes. Line b shows the same
experiment, but this time the CRF model has been
tuned on a dataset comprised of a simple concate-
nation of the training MUC6 data from (a), along
with a different training set from MUC7 (CAT). We
can see that adding extra data in this way, though
250
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 20 40 60 80 100
F1
Percent of target-domain data used for tuning
Inter-genre transfer performance evaluated on MUC6
(e) HIER: MUC6+7 prior, tuned on MUC6
(f) CAT: tuned on all domains
(g) HIER: all domains prior, tuned on MUC6
(h) CHELBA: all domains prior, tuned on MUC6
Figure 4: Transfer aware priors CHELBA and HIER ef-
fectively filter irrelevant data. Adding more irrelevant
data to the priors doesn?t hurt ((e) ? (g) ? (h)), while
simply CAT?ing it, in this case, is disastrous ((f) << (e).
the data is closely related both in domain and task,
has actually hurt the performance of our recognizer
for training sizes of moderate to large size. This is
most likely because, although the MUC6 and MUC7
datasets are closely related, they are still drawn from
different distributions and thus cannot be intermin-
gled indiscriminately. Line c shows the same com-
bination of MUC6 and MUC7, only this time the
datasets have been combined using the HIER prior.
In this case, the performance actually does improve,
both with respect to the single-dataset trained base-
line (a) and the naively trained double-dataset (b).
Finally, line d shows the results of the CHELBA
prior. Curiously, though the domains are closely re-
lated, it does more poorly than even the non-transfer
GAUSS. One possible explanation is that, although
much of the vocabulary is shared across domains,
the interpretation of the features of these words may
differ. Since CHELBA doesn?t model the hierarchy
among features like HIER, it is unable to smooth
away these discrepancies. In contrast, we see that
our HIER prior is able to successfully combine the
relevant parts of data across domains while filtering
the irrelevant, and possibly detrimental, ones. This
experiment was repeated for other sets of intra-genre
tasks, and the results are summarized in ?3.2.3.
3.2.2 Inter-genre, multi-task transfer learning
In Figure 4 we see that the properties of the hi-
erarchical prior hold even when transferring across
tasks. Here again we are trying to learn to recognize
person names in MUC6 e-mails, but this time, in-
stead of adding only other datasets similarly labeled
with person names, we are additionally adding bi-
ological corpora (UT & YAPEX), labeled not with
person names but with protein names instead, along
with the CSPACE e-mail and MUC7 news article
corpora. The robustness of our prior prevents a
model trained on all five domains (g) from degrading
away from the intra-genre, same-task baseline (e),
unlike the model trained on concatenated data (f ).
CHELBA (h) performs similarly well in this case,
perhaps because the domains are so different that al-
most none of the features match between prior and
tuning data, and thus CHELBA backs-off to a stan-
dard N (0, 1) prior.
This robustness in the face of less similarly related
data is very important since these types of transfer
methods are most useful when one possesses only
very little target domain data. In this situation, it
is often difficult to accurately estimate performance
and so one would like assurance than any transfer
method being applied will not have negative effects.
3.2.3 Comparison of HIER prior to baselines
Each scatter plot in Figure 5 shows the relative
performance of a baseline method against HIER.
Each point represents the results of two experi-
ments: the y-coordinate is the F1 score of the base-
line method (shown on the y-axis), while the x-
coordinate represents the score of the HIER method
in the same experiment. Thus, points lying be-
low the y = x line represent experiments for which
HIER received a higher F1 value than did the base-
line. While all three plots show HIER outperform-
ing each of the three baselines, not surprisingly,
the non-transfer GAUSS method suffers the worst,
followed by the naive concatenation (CAT) base-
line. Both methods fail to make any explicit dis-
tinction between the source and target domains and
thus suffer when the domains differ even slightly
from each other. Although the differences are
more subtle, the right-most plot of Figure 5 sug-
gests HIER is likewise able to outperform the non-
hierarchical CHELBA prior in certain transfer sce-
narios. CHELBA is able to avoid suffering as much
as the other baselines when faced with large differ-
ence between domains, but is still unable to capture
251
0.2
.4
.6
.8
1
0 .2 .4 .6 .8 1
G
AU
SS
(F
1)
HIER (F1)
0
.2
.4
.6
.8
1
0 .2 .4 .6 .8 1
CA
T
(F
1)
HIER (F1)
.4
.6
.8
.4 .6 .8
CH
EL
BA
(F
1)
HIER (F1)
?
y = x
MUC6@3%
MUC6@6%
MUC6@13%
MUC6@25%
MUC6@50%
MUC6@100%
CSPACE@3%
CSPACE@6%
CSPACE@13%
CSPACE@25%
CSPACE@50%
CSPACE@100%
Figure 5: Comparative performance of baseline methods (GAUSS, CAT, CHELBA) vs. HIER prior, as trained on nine
prior datasets (both pure and concatenated) of various sample sizes, evaluated on MUC6 and CSPACE datasets. Points
below the y = x line indicate HIER outperforming baselines.
as many dependencies between domains as HIER.
4 Conclusions, related & future work
In this work we have introduced hierarchical feature
tree priors for use in transfer learning on named en-
tity extraction tasks. We have provided evidence that
motivates these models on intuitive, theoretical and
empirical grounds, and have gone on to demonstrate
their effectiveness in relation to other, competitive
transfer methods. Specifically, we have shown that
hierarchical priors allow the user enough flexibil-
ity to customize their semantics to a specific prob-
lem, while providing enough structure to resist un-
intended negative effects when used inappropriately.
Thus hierarchical priors seem a natural, effective
and robust choice for transferring learning across
NER datasets and tasks.
Some of the first formulations of the transfer
learning problem were presented over 10 years
ago (Thrun, 1996; Baxter, 1997). Other techniques
have tried to quantify the generalizability of cer-
tain features across domains (Daume? III and Marcu,
2006; Jiang and Zhai, 2006), or tried to exploit the
common structure of related problems (Ben-David
et al, 2007; Scho?lkopf et al, 2005). Most of
this prior work deals with supervised transfer learn-
ing, and thus requires labeled source domain data,
though there are examples of unsupervised (Arnold
et al, 2007), semi-supervised (Grandvalet and Ben-
gio, 2005; Blitzer et al, 2006), and transductive ap-
proaches (Taskar et al, 2003).
Recent work using so-called meta-level priors to
transfer information across tasks (Lee et al, 2007),
while related, does not take into explicit account the
hierarchical structure of these meta-level features of-
ten found in NLP tasks. Daume? allows an extra de-
gree of freedom among the features of his domains,
implicitly creating a two-level feature hierarchy with
one branch for general features, and another for do-
main specific ones, but does not extend his hierar-
chy further (Daume? III, 2007)). Similarly, work on
hierarchical penalization (Szafranski et al, 2007) in
two-level trees tries to produce models that rely only
on a relatively small number of groups of variable,
as structured by the tree, as opposed to transferring
knowledge between branches themselves.
Our future work is focused on designing an al-
gorithm to optimally choose a smoothing regime
for the learned feature trees so as to better exploit
the similarities between domains while neutralizing
their differences. Along these lines, we are working
on methods to reduce the amount of labeled target
domain data needed to tune the prior-based mod-
els, looking forward to semi-supervised and unsu-
pervised transfer methods.
252
References
Rie K. Ando and Tong Zhang. 2005. A framework for
learning predictive structures from multiple tasks and
unlabeled data. In JMLR 6, pages 1817 ? 1853.
Andrew Arnold, Ramesh Nallapati, and William W. Co-
hen. 2007. A comparative study of methods for trans-
ductive transfer learning. In Proceedings of the IEEE
International Conference on Data Mining (ICDM)
2007 Workshop on Mining and Management of Bio-
logical Data.
Jonathan Baxter. 1997. A Bayesian/information theo-
retic model of learning to learn via multiple task sam-
pling. Machine Learning, 28(1):7?39.
Shai Ben-David, John Blitzer, Koby Crammer, and Fer-
nando Pereira. 2007. Analysis of representations for
domain adaptation. In NIPS 20, Cambridge, MA. MIT
Press.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In EMNLP, Sydney, Australia.
A. Borthwick, J. Sterling, E. Agichtein, and R. Grishman.
1998. NYU: Description of the MENE named entity
system as used in MUC-7.
R. Bunescu, R. Ge, R. Kate, E. Marcotte, R. Mooney,
A. Ramani, and Y. Wong. 2004. Comparative experi-
ments on learning information extractors for proteins
and their interactions. In Journal of AI in Medicine.
Data from ftp://ftp.cs.utexas.edu/pub/mooney/bio-
data/proteins.tar.gz.
Rich Caruana. 1997. Multitask learning. Machine
Learning, 28(1):41?75.
Ciprian Chelba and Alex Acero. 2004. Adaptation of
maximum entropy capitalizer: Little data can help a
lot. In Dekang Lin and Dekai Wu, editors, EMNLP
2004, pages 285?292. ACL.
S. Chen and R. Rosenfeld. 1999. A gaussian prior for
smoothing maximum entropy models.
William W. Cohen. 2004. Minorthird: Methods for
identifying names and ontological relations in text
using heuristics for inducing regularities from data.
http://minorthird.sourceforge.net.
Hal Daume? III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. In Journal of Artificial
Intelligence Research 26, pages 101?126.
Hal Daume? III. 2007. Frustratingly easy domain adapta-
tion. In ACL.
David Fisher, Stephen Soderland, Joseph McCarthy,
Fangfang Feng, and Wendy Lehnert. 1995. Descrip-
tion of the UMass system as used for MUC-6.
Kristofer Franze?n, Gunnar Eriksson, Fredrik Olsson, Lars
Asker, Per Lidn, and Joakim Co?ster. 2002. Protein
names and how to find them. In International Journal
of Medical Informatics.
Yves Grandvalet and Yoshua Bengio. 2005. Semi-
supervised learning by entropy minimization. In CAP,
Nice, France.
Jing Jiang and ChengXiang Zhai. 2006. Exploiting do-
main structure for named entity recognition. In Hu-
man Language Technology Conference, pages 74 ? 81.
R. Kraut, S. Fussell, F. Lerch, and J. Espinosa. 2004. Co-
ordination in teams: evidence from a simulated man-
agement game.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc.
18th International Conf. on Machine Learning, pages
282?289. Morgan Kaufmann, San Francisco, CA.
S.-I. Lee, V. Chatalbashev, D. Vickrey, and D. Koller.
2007. Learning a meta-level prior for feature relevance
from multiple related tasks. In Proceedings of Interna-
tional Conference on Machine Learning (ICML).
Einat Minkov, Richard C. Wang, and William W. Cohen.
2005. Extracting personal names from email: Ap-
plying named entity recognition to informal text. In
HLT/EMNLP.
Rajat Raina, Andrew Y. Ng, and Daphne Koller. 2006.
Transfer learning by constructing informative priors.
In ICML 22.
Bernhard Scho?lkopf, Florian Steinke, and Volker Blanz.
2005. Object correspondence as a machine learning
problem. In ICML ?05: Proceedings of the 22nd inter-
national conference on Machine learning, pages 776?
783, New York, NY, USA. ACM.
Charles Sutton and Andrew McCallum. 2005. Composi-
tion of conditional random fields for transfer learning.
In HLT/EMLNLP.
M. Szafranski, Y. Grandvalet, and P. Morizet-
Mahoudeaux. 2007. Hierarchical penalization.
In Advances in Neural Information Processing
Systems 20. MIT press.
B. Taskar, M.-F. Wong, and D. Koller. 2003. Learn-
ing on the test data: Leveraging ?unseen? features. In
Proc. Twentieth International Conference on Machine
Learning (ICML).
Sebastian Thrun. 1996. Is learning the n-th thing any
easier than learning the first? In NIPS, volume 8,
pages 640?646. MIT.
J. Zhang, Z. Ghahramani, and Y. Yang. 2005. Learning
multiple related tasks using latent independent compo-
nent analysis.
253
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 438?446,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Legal Docket-Entry Classification: Where Machine Learning stumbles
Ramesh Nallapati and Christopher D. Manning
Natural Language Processing Group
Department of Computer Science
Stanford University
Stanford, CA 94305
{nmramesh,manning}@cs.stanford.edu
Abstract
We investigate the problem of binary text clas-
sification in the domain of legal docket entries.
This work presents an illustrative instance of
a domain-specific problem where the state-
of-the-art Machine Learning (ML) classifiers
such as SVMs are inadequate. Our investiga-
tion into the reasons for the failure of these
classifiers revealed two types of prominent er-
rors which we call conjunctive and disjunctive
errors. We developed simple heuristics to ad-
dress one of these error types and improve the
performance of the SVMs. Based on the in-
tuition gained from our experiments, we also
developed a simple propositional logic based
classifier using hand-labeled features, that ad-
dresses both types of errors simultaneously.
We show that this new, but simple, approach
outperforms all existing state-of-the-art ML
models, with statistically significant gains. We
hope this work serves as a motivating example
of the need to build more expressive classifiers
beyond the standard model classes, and to ad-
dress text classification problems in such non-
traditional domains.
1 Introduction
Text Classification is a widely researched area, with
publications spanning more than a decade (Yang
and Liu, 1999). Although earlier models used logic
based rules (Apte? et al, 1994) and decision trees
(Lewis and Ringuette, 1994), recently the emphasis
has been on statistical classifiers such as the naive
Bayes model (McCallum and Nigam, 1998), logis-
tic regression (Zhang and Oles, 2001) and support
vector machines (Joachims, 1998). Although several
complex features were considered for classification,
eventually researchers have settled down to simple
bag-of-words features such as unigrams and some
times bigrams (Dumais et al, 1998), thereby com-
pletely ignoring the grammar and other semantic in-
formation in the text. Despite this fact, the state-
of-the-art performance is close to or above 90% on
F1 scores on most standard test collections such as
Reuters, 20 newsgroups, etc. (Bekkerman et al,
2003). As such, most researchers and practitioners
believe text classification technology has reached a
mature state, where it is suitable for deployment in
real life applications.
In this work, we present a text classification prob-
lem from the legal domain which challenges some
of our understanding of text classification problems.
In the new domain, we found that the standard ML
approaches using bag-of-words features perform rel-
atively poorly. Not only that, we noticed that the
linear form (or even polynomial form) used by these
classifiers is inadequate to capture the semantics of
the text. Our investigation into the shortcomings of
the traditional models such as SVMs, lead us to build
a simple propositional logic based classifier using
hand-labeled features that outperforms these strong
baselines.
Although the new model by itself is interesting,
the main objective of our work is to present the text
classification community with an interesting prob-
lem where the current models are found inadequate.
Our hope is that the new problem will encourage
researchers to continue to build more sophisticated
models to solve classification problems in diverse,
438
non-traditional domains.
The rest of the paper is organized as follows. In
section 2, we introduce the problem of legal docket
entry classification and describe the data with some
representative examples. In section 3, we describe
the experiments performed with SVMs and several
of its variants. We also identify the shortcomings
of the current classifiers in this section. In section
3.2, we present results from using human selected
features for the classification problem and motivate
their application for the docket entry classification
using propositional logic in subsection 3.3. We also
show that simple propositional logic using human
selected features and their labels outperforms the
state-of-the-art classifiers. We conclude the discus-
sion in section 4, where we argue the case for more
sophisticated classifiers for specialized domains.
2 Docket Entry Classification
In this section, we introduce the problem of legal
docket entry classification.
In any US district court of law, information on the
chronological events in a case is usually entered in
a document called the case docket. Each entry in a
docket lists an event that occured on a specific date
such as pleading, appeal, order, jury trial, judgment,
etc. The entries are brief descriptions of the events in
natural language. Sometimes, a single docket entry
can list multiple events that take place on the same
day. Table 1 displays a sample docket for a case.
Identifying various events in a court case is a cru-
cial first step to automatically understanding the pro-
gression of a case and also in gathering aggregate
statistics of court cases for further analysis. While
some events such as ?Complaint? may be easy to
identify using regular expressions, others are much
more complex and may require sophisticated mod-
eling.
In this work, we are primarily interested in iden-
tifying one such complex event called ?Order re:
Summary Judgment?. Summary Judgment is a le-
gal term which means that a court has made a deter-
mination (a judgment) without a full trial.1 Such a
judgment may be issued as to the merits of an entire
case, or of specific issues in that case. Typically, one
1See e.g., Wikipedia for more information:
http://en.wikipedia.org/wiki/Summary judgment
of the parties (plaintiff or defendant) involved in the
case moves a motion for summary judgment, (usu-
ally) in an attempt to eliminate the risk of losing a
trial. In an ?Order re: Summary Judgment? event,
the court may grant or deny a motion for summary
judgment upon inspecting all the evidence and facts
in the case. The task then, is to identify all docket
entries in a set of cases that list occurrences of ?Or-
der re: Summary Judgment? events. We will call
them OSJ events in short.
A few typical positive and negative docket entries
for the OSJ event from various cases are shown in
table 2. The examples require some explanation.
Firstly, all orders granting, denying or amending
motions for full or partial summary judgment are
considered OSJs. However, if the motion is denied
as moot or denied without prejudice, it is not an OSJ
event, as shown in the negative examples 1 and 2
in table 2. This is because in such cases, no de-
cision was made on substantive issues of the case.
Also, there are other kinds of orders that are issued
with reference to a summary judgment motion that
do not fall into the category of OSJ, such as negative
examples 3 through 9. To elaborate further, negative
example 3 is about amending the deadline for fil-
ing a summary judgment motion, but not a summary
judgment motion itself. Likewise, in negative exam-
ple 4, the judge denies a motion to shorten time on
a motion to vacate the order on summary judgment,
but not the motion on summary judgment itself. The
other negative examples are very similar in spirit and
we leave it as an exercise to the reader to interpret
why they are negatively labeled.
On first glance, it appears that a standard classifier
may do a good job on this data, since the classifica-
tion seems to depend mostly on certain key words
such as ?granting?, ?denying?, ?moot?, etc. Also no-
tice that some of the docket entries contain multiple
events, but as long as it contains the ?order re: sum-
mary judgment? event, it falls into the positive class.
This seems very similar to the standard case, where
a document may belong to multiple topics, but it is
still identified as on-topic by a binary classifier on
the corresponding topic.
Hence, as a first step, we attempted using a stan-
dard SVM classifier.
439
# Date Filed Text
1 10/21/2002 Original Complaint with JURY DEMAND filed. Cause: 35:271
Patent Infringement Modified on 10/24/2002 (Entered: 10/22/2002)
2 10/21/2002 Form mailed to Commissioner of Patents and Trademarks. (poa)
3 10/28/2002 Return of service executed as to Mathworks Inc 10/23/02
Answer due on 11/12/02 for Mathworks Inc (poa) (Entered: 10/28/2002)
4 11/4/2002 Unopposed Motion by Mathworks Inc The to extend time to answer or
otherwise respond to pla?s complaint (ktd) (Entered: 11/05/2002)
5 11/5/2002 ORDER granting [4-1] motion to extend time to answer or otherwise
respond to pla?s complaint, ans reset answer due on 11/27/02 for Mathworks Inc
? ? ? ? ? ? ? ? ? ? ? ?
Table 1: An example (incomplete) docket: each row in the table corresponds to a docket-entry
2.1 Data
We have collected 5,595 docket entries from several
court cases on intellectual property litigation, that
are related to orders pertaining to summary judg-
ment, and hand labeled them into OSJ or not OSJ
categories.2 The hand-labeling was done by a sin-
gle legal expert, who practised law for a number of
years. In all, 1,848 of these docket entries fall into
the OSJ category.
In all our experiments, we split the entire data ran-
domly into 20 disjoint subsets, where each set has
the same proportion of positive-to-negative exam-
ples as the original complete set. For all the clas-
sifiers we used in this work, we performed 20-fold
cross validation. We compute F1 scores on the held-
out data of each run and report overall F1 score as
the single point performance measure. We also per-
form statistical significance tests using the results
from the 20 cross-validation runs.
2.2 Preprocessing
Before we ran our classifiers, we removed all punc-
tuation, did casefolding, removed stopwords and
stemmed the words using the Porter stemmer. We
used unigrams and bigrams as our basic features.3
We considered all the words and bigrams as bi-
nary features and did not use any TF-IDF weight-
ing. Our justification for this decision is as fol-
lows: the docket text is typically very short and it is
2The data can be made available free of cost upon request.
Please email the first author for more information.
3In our preliminary experiments, we found that a combina-
tion of unigrams and bigrams works better than unigrams alone.
usually rare to see the same feature occurring mul-
tiple times in a docket entry. In addition, unlike
in standard text classification, some of the features
that are highly frequent across docket entries such
as ?denying?,?granting?, etc., are also the ones that
are highly discriminative. In such a case, down-
weighting these features using IDF weights might
actually hurt performance. Besides (Dumais et al,
1998) found that using binary features works as well
as using TF-IDF weights.
In addition, we also built a domain specific sen-
tence boundary detector using regular expressions.4
For constructing the features of a docket entry, we
only consider those sentences in the entry that con-
tain the phrase ?summary judgment? and its vari-
ants.5 Our preliminary experiments found that this
helps the classifier focus on the relevant features,
helping it to improve precision while not altering its
recall noticeably.
3 Experiments and results
3.1 Basic SVM
First we implemented the standard linear SVM6 on
this problem with only word-based features (uni-
grams and bigrams) as the input. Quite surprisingly,
the model achieves an F1 score of only 79.44% as
shown in entry 1 of table 5. On inspection, we no-
4It works well in most cases but is far from perfect, due to
the noisy nature of the data.
5The variants include ?sum jgm?, ?S/J?, ?summary adjudi-
cation?, ?summary jgm?, etc.
6All our SVM experiments were performed us-
ing the libsvm implementation downloadable from
http://www.csie.ntu.edu.tw/?cjlin/libsvm/
440
REPRESENTATIVE POSITIVE EXAMPLES
1. ORDER denying [36-1] motion for summary judgment on dfts Ranbaxy invalidity defenses by pltfs. (signed by
Judge Garrett E. Brown, Jr.)
2. ORDER GRANTING IN PART AND DENYING IN PART DEFENDANTS? MOTION FOR SUMMARY
JUDGMENT
3. ORDER re 78 MOTION to Amend/Correct Motion for Summary Judgment and supporting documents, filed by
Defendant Synergetics USA, Inc. ; ORDERED GRANTED.
4. MEMORANDUM AND ORDER re: 495 Third MOTION for Partial Summary Judgment Dismissing Mon-
santo?s Defenses Related to Dr. Barnes filed by Bayer BioScience N.V., motion is GRANTED IN PART AND
DENIED IN PART.
5. ORDER GRANTING IN PART PLTF S/J MOT; GRANTING IN PART PLTF MOT/CLARIFY; GRANTING
DEFT MOT/CLARIFY; PRTL S/J STAYED.
6. ORDER by Chief Judge Joe B. McDade. Court is granting in part and denying in part Deere?s motion for
reconsideration and clarification [42-2]; granting Toro?s motion for summary judgment of non-infringement [45-
1]; denying Deere?s motion for summary judgment [58-1];
7. ORDER GRANTING DEFT. MOTION FOR S/J AND DENYING PLTF. MOTIONS FOR S/J AND TO SUP-
PLEMENT.
REPRESENTATIVE NEGATIVE EXAMPLES
1. ORDER - denying w/out prejudice 17 Motion for Summary Judgment, denying w/out prejudice 49 Motion to
Amend/Correct . Signed by Judge Kent A. Jordan on 1/23/06.
2. Order denying as moot motion for summary judgment.
3. Order granting 53 Motion to Amend/Correct the deadline for filing summary jgm motions will be moved 12/1/03
to 12/8/03
4. ORDER by Judge Claudia Wilken denying plaintiff?s motion to shorten time on motion to vacate portions of
Court?s order on cross-motion for summary judgment on patent issues [695-1] [697-1]
5. MEMORANDUM AND ORDER: by Honorable E. Richard Webber, IT IS HEREBY ORDERED that Defendant
Aventis shall have 10 days from the date of this order to demonstrate why the Court should not grant summary
judgment to Monsanto of non-infringement of claims 1-8 and 12 of the ?565 patent and claim 4 of the ?372
patent.
6. ORDER by Judge Claudia Wilken DENYING motion for an order certifying for immediate appeal portions of
the courts? 2/6/03 order granting in part plaintiff?s motion for partial summary judgment [370-1]
7. ORDER by Judge William Alsup denying in part 12 Motion to Consolidate Cases except as to one issue, granting
in part for collateral estoppel 20 Motion for Summary Judgment
8. ORDER ( Chief Mag. Judge Jonathan G. Lebedoff / 9/11/02) that the court grants Andersen?s motion and orders
that Andersen be allowed to bring its motions for summary judgment
9. ORDER by Judge Susan J. Dlott denying motion to strike declaration of H Bradley Hammond attached to mem-
orandum in opposition to motion for partial summary judgment as to liability on the patent infringement and
validity claims [40-1] [47-1] [48-1]
Table 2: Order: re Summary Judgment: positive and negative docket entries. The entries are reproduced as they are.
441
ticed that the SVM assigns high weights to many
spurious features owing to their strong correlation
with the class.
As a natural solution to this problem, we selected
the top 100 features7 using the standard information
gain metric (Yang and Pedersen, 1997) and ran the
SVM on the pruned feature set. As one would ex-
pect, the performance of the SVM improved signif-
icantly to reach an F1 score of 83.08% as shown in
entry 2 of the same table. However, it is still a far cry
from the typical results on standard test beds where
the performance is above 90% F1. We suspected
that training data was probably insufficient, but a
learning curve plotting performance of the SVM as
a function of the amount of training data reached a
plateau with the amount of training data we had, so
this problem was ruled out.
To understand the reasons for its inferior perfor-
mance, we studied the features that are assigned
the highest weights by the classifier. Although the
SVM is able to assign high weights to several dis-
criminative features such as ?denied?, and ?granted?,
it also assigns high weights to features such as
?opinion?, ?memorandum?, ?order?, ?judgment?, etc.,
which have high co-occurrence rates with the posi-
tive class, but are not very discriminative in terms of
the actual classification.
This is indicative of the problems associated with
standard feature selection algorithms such as infor-
mation gain in these domains, where high correla-
tion with the label does not necessarily imply high
discriminative power of the feature. Traditional clas-
sification tasks usually fall into what we call the
?topical classification? domain, where the distribu-
tion of words in the documents is a highly discrimi-
native feature. On such tasks, feature selection algo-
rithms based on feature-class correlation have been
very successful. In contrast, in the current problem,
which we call ?semantic classification?, there seem
to be a fixed number of domain specific operative
words such as ?grant?, ?deny?, ?moot?, ?strike?, etc.,
which, almost entirely decide the class of the docket
entry, irrespective of the existence of other highly
correlated features. The information gain metric as
well as the SVM are not able to fully capture such
7We tried other numbers as well, but top 100 features
achieves the best performance.
features in this problem.
We leave the problem of accurate feature selec-
tion to future work, but in this work, we address the
issue by asking for human intervention, as we de-
scribe in the next section. One reason for seeking
human assistance is that it will give us an estimate
of upperbound performance of an automatic feature
selection system. In addition, it will also offer us a
hint as to whether the poor performance of the SVM
is because of poor feature selection. We will aim to
answer this question in the next section.
3.2 Human feature selection
Using human assistance for feature selection is a rel-
atively new idea in the text classification domain.
(Raghavan et al, 2006) propose a framework in
which the system asks the user to label documents
and features alternatively. They report that this re-
sults in substantial improvement in performance es-
pecially when the amount of labeled data is mea-
gre. (Druck et al, 2008) propose a new General-
ized Expectation criterion that learns a classification
function from labeled features alone (and no labeled
documents). They showed that feature labeling can
reduce annotation effort from humans compared to
document labeling, while achieving almost the same
performance.
Following this literature, we asked our annotators
to identify a minimal but definitive list of discrim-
inative features from labeled data. The annotators
were specifically instructed to identify the features
that are most critical in tagging a docket entry one
way or the other. In addition, they were also asked
to assign a polarity to each feature. In other words,
the polarity tells us whether or not the features be-
long to the positive class. Table 3 lists the complete
set of features identified by the annotators.
As an obvious next step, we trained the SVM in
the standard way, but using only the features from ta-
ble 3 as the pruned set of features. Remarkably, the
performance improves to 86.77% in F1, as shown in
entry 3 of table 5. Again, this illustrates the unique-
ness of this dataset, where a small number of hand
selected features (< 40) makes a huge difference
in performance compared to a state-of-the-art SVM
combined with automatic feature selection. We be-
lieve this calls for more future work in improving
feature selection algorithms.
442
Label Features
Positive grant, deny, amend, reverse,
adopt, correct, reconsider, dismiss
Negative strike, proposed, defer, adjourn,
moot, exclude, change, extend,
leave, exceed, premature, unseal,
hearing, extend, permission,
oral argument, schedule, ex parte,
protective order, oppose,
without prejudice, withdraw,
response, suspend, request,
case management order,
to file, enlarge, reset, supplement
placing under seal, show cause
reallocate, taken under submission
Table 3: Complete set of hand-selected features: morpho-
logical variants not listed
Notice that despite using human assistance, the
performance of the SVM is still not at a desirable
level. This clearly points to deficiencies in the model
other than poor feature selection. To understand the
problem, we examined the errors made by the SVM
and found that there are essentially two types of er-
rors: conjunctive and disjunctive. Representative ex-
amples for both kinds of errors are displayed in ta-
ble 4. The first example in the table corresponds
to a conjunctive error, where the SVM is unable to
model the binary switch like behavior of features.
In this example, although ?deny? is rightly assigned
a positive weight and ?moot? is rightly assigned a
negative weight, when both features co-occur in a
docket entry (as in ?deny as moot?), it makes the la-
bel negative.8 However, the combined weight of the
linear SVM is positive since the absolute value of
the weight assigned to ?deny? is higher than that of
?moot?, resulting in a net positive score. The second
example falls into the category of disjunctive errors,
where the SVM fails to model disjunctive behav-
ior of sentences. In this example, the first sentence
contains an OSJ event, but the second and third sen-
tences are negatives for OSJ. As we have discussed
earlier, this docket entry belongs to the OSJ category
since it contains at least one OSJ event. However, we
8This is very similar to the conjunction of two logical vari-
ables where the conjunction of the variables is negative when at
least one of them is negative. Hence the name conjunctive error.
see that the negative weights assigned by the SVM
to the second and third sentences result in an overall
negative classification.
As a first attempt, we tried to reduce the conjunc-
tive errors in our system. Towards this objective,
we built a decision tree9 using the same features
listed in table 3. Our intuition was that a decision
tree makes a categorical decision at each node in the
tree, hence it could capture the binary-switch like
behavior of features. However, the performance of
the decision tree is found to be statistically indistin-
guishable from the linear SVM as shown in entry
4 of table 5. As an alternative, we used an SVM
with a quadratic kernel, since it can also capture such
pairwise interactions of features. This resulted in a
fractional improvement in performance, but is again
statistically indistinguishable from the decision tree.
We also tried higher order polynomial kernels and
the RBF kernel, but the performance got no better.10
It is not easy to analyze the behavior of non-linear
kernels since they operate in a higher kernel space.
Our hypothesis is that polynomial functions capture
higher order interactions between features, but they
do not capture conjunctive behavior precisely.
As an alternative, we considered the following
heuristic: whenever two or more of the hand selected
features occur in the same sentence, we merged
them to form an n-gram. The intuition behind this
heuristic is the following: using the same example
as before, if words such as ?deny? and ?moot? oc-
cur in the same sentence, we form the bigram ?deny-
moot?, forcing the SVM to consider the bigram as a
separate feature. We hope to capture the conjunctive
behavior of some features using this heuristic. The
result of this approach, as displayed in entry 6 of
table 5, shows small but statistically significant im-
provement over the quadratic SVM, confirming our
theory. We also attempted a quadratic kernel using
sentence level n-grams, but it did not show any im-
provement.
Note that all the models and heuristics we used
above only address conjunctive errors, but not dis-
junctive errors. From the discussion above, we sus-
pect the reader already has a good picture of what
9We used the publicly available implementation from
www.run.montefiore.ulg.ac.be/?francois/software/jaDTi/
10We also tried various parameter settings for these kernels
with no success.
443
1. DOCKET ENTRY: order denying as moot [22-1] motion for summary judgment ( signed by judge
federico a. moreno on 02/28/06).
FEATURES (WEIGHTS): denying (1.907), moot (-1.475)
SCORE: 0.432; TRUE LABEL: Not OSJ; SVM LABEL: OSJ
2. DOCKET ENTRY: order granting dfts? 37 motion for summary judgment. further ordered denying
as moot pla?s cross-motion 42 for summary judgment. denying as moot dfts? motion to strike pla?s
cross-motion for summary judgment 55 . directing the clerk to enter judgment accordingly. signed by
judge mary h murguia on 9/18/07
FEATURES (WEIGHTS): granting (1.64), denying (3.57), strike(-2.05) moot(-4.22)
SCORE: -1.06; TRUE LABEL: OSJ; SVM LABEL: Not OSJ
Table 4: Representative examples for conjunctive and disjunctive errors of the linear SVM using hand selected features
an appropriate model for this data might look like.
The next section introduces this new model devel-
oped using the intuition gained above.
3.3 Propositional Logic using Human Features
and Labels
So far, the classifiers we considered received a per-
formance boost by piggybacking on the human se-
lected features. However, they did not take into ac-
count the polarity of these features. A logical next
step would be to exploit this information as well. An
appropriate model would be the generalized expec-
tation criterion model by (Druck et al, 2008) which
learns by matching model specific label expectations
conditioned on each feature, with the corresponding
empirical expectations. However, the base model
they use is a logistic regression model, which is a
log-linear model, and hence would suffer from the
same limitations as the linear SVM. There is also
other work on combining SVMs with labeled fea-
tures using transduction on unlabeled examples, that
are soft-labeled using labeled features (Wu and Sri-
hari, 2004), but we believe it will again suffer from
the same limitations as the SVM on this domain.
In order to address the conjunctive and disjunc-
tive errors simultaneously, we propose a new, but
simple approach using propositional logic. We con-
sider each labeled feature as a propositional variable,
where true or false corresponds to whether the la-
bel of the feature is positive or negative respectively.
Given a docket entry, we first extract its sentences,
and for each sentence, we extract its labeled features,
if present. Then, we construct a sentence-level for-
mula formed by the conjunction of the variables rep-
resenting the labeled features. The final classifier is
a disjunction of the formulas of all sentences in the
docket entry. Formally, the propositional logic based
classifier can be expressed as follows:
C(D) = ?N(D)i=1 (?Mij=1L(fij)) (1)
where D is the docket entry, N(D) is its number
of sentences, Mi is the number of labeled features
in the ith sentence, fij is the jth labeled feature in
the ith sentence and L() is a mapping from a fea-
ture to its label, and C(D) is the classification func-
tion where ?true? implies the docket entry contains
an OSJ event.
The propositional logic model is designed to ad-
dress the within-sentence conjunctive errors and
without-sentence disjunctive errors simultaneously.
Clearly, the within-sentence conjunctive behavior of
the labeled features is captured by applying logical
conjunctions to the labeled features within a sen-
tence. Similarly, the disjunctive behavior of sen-
tences is captured by applying disjunctions to the
sentence-level clauses. This model requires no train-
ing, but for reasons of fairness in comparison, at test-
ing time, we used only those human features (and
their labels) that exist in the training set in each
cross-validation run. The performance of this new
approach, listed in table 5 as entry 7, is slightly bet-
ter than the best performing SVM in entry 6. The
difference in performance in this case is statistically
significant, as measured by a paired, 2-tailed t-test at
95% confidence level (p-value = 0.007).
Although the improvement for this model is sta-
tistically significant, it does not entirely match our
444
# Model Recall (%) Precision (%) F1 (%)
1 Linear SVM with uni/bigrams only 75.19 84.21 79.44
2 Linear SVM with uni/bigrams only FS100 82.47 83.69 83.08*
3 Linear SVM with HF only 84.68 88.97 86.77*
4 Decision Tree with HF only 85.22 89.38 87.25
5 Quadratic SVM with HF only 84.14 90.98 87.43
6 Linear SVM with HF sentNgrams 84.63 93.37 88.78*
7 Propositional Logic with HF and their labels 85.71 93.45 89.67*
Table 5: Results for ?Order re: Summary Judgment?: FS100 indicates that only top 100 features were selected using
Information Gain metric; HF stands for human built features, sentNgrams refers to the case where all the human-built
features in a given sentence were merged to form an n-gram feature. A ?*? next to F1 value indicates statistically
significant result compared to its closest lower value, measured using a paired 2-tailed T-test, at 95% confidence level.
The highest numbers in each column are highlighted using boldface.
expectations. Our data analysis showed a variety of
errors caused mostly due to the following issues:
? Imperfect sentence boundary detection: since
the propositional logic model considers sen-
tences as strong conjunctions, it is more sen-
sitive to errors in sentence boundary detection
than SVMs. Any errors would cause the model
to form conjunctions with features in neighbor-
ing sentences and deliver an incorrect labeling.
? Incomplete feature set: Some errors are caused
because the feature set is not complete. For ex-
ample, negative example 4 in table 2 is tagged
as positive by the new model. This error could
have been avoided if the word ?shorten? had
been identified as a negative feature.
? Relevant but bipolar features: Although our
model assumes that the selected features ex-
hibit binary nature, this may not always be true.
For example the word allow is sometimes used
as a synonym for ?grant? which is a positive fea-
ture, but other times, as in negative example 8
in table 2, it exhibits negative polarity. Hence
it is not always possible to encode all relevant
features into the logic based model.
? Limitations in expressiveness: Some natural
language sentences such as negative example
5 in table 2 are simply beyond the scope of the
conjunctive and disjunctive formulations.
4 Discussion and Conclusions
Clearly, there is a significant amount of work to
be done to further improve the performance of the
propositional logic based classifier. One obvious
line of work is towards better feature selection in
this domain. One plausible technique would be to
use shallow natural language processing techniques
to extract the operative verbs acting on the phrase
?summary judgment?, and use them as the pruned
feature set.
Another potential direction would be to extend the
SVM-based system to model disjunctive behavior
of sentences.11 One way to accomplish this would
be to classify each sentence individually and then to
combine the outcomes using a disjunction. But for
this to be implemented, we would also need labels
at the sentence level during training time. One could
procure these labels from annotators, but as an alter-
native, one could learn the sentence-level labels in
an unsupervised fashion using a latent variable at the
sentence level, but a supervised model at the docket-
entry level. Such models may also be appropriate for
traditional document classification where each doc-
ument could be multi-labeled, and it is something
we would like attempt in the future.
In addition, instead of manually constructing the
logic based system, one could also automatically
learn the rules by using ideas from earlier work
on ILP (Muggleton, 1997), FOIL (Quinlan and
Cameron-Jones, 1993), etc.
11Recall that the heuristics we presented for SVMs only ad-
dress the conjunctive errors.
445
To summarize, we believe it is remarkable that
a simple logic-based classifier could outperform an
SVM that is already boosted by hand picked fea-
tures and heuristics such as sentence level n-grams.
This work clearly exposes some of the limitations of
the state-of-the-art models in capturing the intrica-
cies of natural language, and suggests that there is
more work to be done in improving the performance
of text based classifiers in specialized domains. As
such, we hope our work motivates other researchers
towards building better classifiers for this and other
related problems.
Acknowledgments
The authors would like to thank Stanford university
for financial support.
References
Chidanand Apte?, Fred Damerau, and Sholom M. Weiss.
1994. Automated learning of decision rules for text
categorization. ACM Trans. Inf. Syst., 12(3):233?251.
R. Bekkerman, R. El-Yaniv, N. Tishby, and Y. Win-
ter. 2003. Distributional word clusters vs. words for
text categorization. Journal of Machine Learning Re-
search.
Gregory Druck, Gideon Mann, and Andrew McCallum.
2008. Learning from labeled features using general-
ized expectation criteria. In Proceedings of ACM Spe-
cial Interest Group on Information Retreival, (SIGIR).
Susan Dumais, John Platt, David Heckerman, and
Mehran Sahami. 1998. Inductive learning algorithms
and representations for text categorization. In CIKM
?98: Proceedings of the seventh international con-
ference on Information and knowledge management,
pages 148?155, New York, NY, USA. ACM.
T. Joachims. 1998. Text categorization with support vec-
tor machines: Learning with many relevant features.
In ECML-98: 10th European Conference on Machine
Learning.
D.D. Lewis and M. Ringuette. 1994. Comparison of two
learning algorithms for text categorization. In Third
Annual Symposium on Document Analysis and Infor-
mation Retrieval.
A. McCallum and K. Nigam. 1998. A comparison of
event models for Na??ve Bayes text classification . In
AAAI-98 Workshop on Learning for Text Categoriza-
tion.
Stephen Muggleton. 1997. Inductive Logic Program-
ming: 6th International Workshop: Seleted Papers.
Springer.
J.R. Quinlan and R.M. Cameron-Jones. 1993. Foil: a
mid-term report. In Proceedings of European Confer-
ence on Machine Learning.
Hema Raghavan, Omid Madani, and Rosie Jones. 2006.
Active learning with feedback on features and in-
stances. J. Mach. Learn. Res., 7:1655?1686.
Xiaoyun Wu and Rohini Srihari. 2004. Incorporating
prior knowledge with weighted margin support vector
machines. In KDD ?04: Proceedings of the tenth ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, pages 326?333, New York,
NY, USA. ACM.
Yiming Yang and Xin Liu. 1999. A re-examination of
text categorization methods. In SIGIR ?99: Proceed-
ings of the 22nd annual international ACM SIGIR con-
ference on Research and development in information
retrieval, pages 42?49, New York, NY, USA. ACM.
Yiming Yang and Jan O. Pedersen. 1997. A compara-
tive study on feature selection in text categorization.
In ICML ?97: Proceedings of the Fourteenth Interna-
tional Conference on Machine Learning, pages 412?
420, San Francisco, CA, USA. Morgan Kaufmann
Publishers Inc.
Tong Zhang and Frank J. Oles. 2001. Text categorization
based on regularized linear classification methods. In-
formation Retrieval, 4(1):5?31.
446
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 248?256,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Labeled LDA: A supervised topic model for credit attribution in
multi-labeled corpora
Daniel Ramage, David Hall, Ramesh Nallapati and Christopher D. Manning
Computer Science Department
Stanford University
{dramage,dlwh,nmramesh,manning}@cs.stanford.edu
Abstract
A significant portion of the world?s text
is tagged by readers on social bookmark-
ing websites. Credit attribution is an in-
herent problem in these corpora because
most pages have multiple tags, but the tags
do not always apply with equal specificity
across the whole document. Solving the
credit attribution problem requires associ-
ating each word in a document with the
most appropriate tags and vice versa. This
paper introduces Labeled LDA, a topic
model that constrains Latent Dirichlet Al-
location by defining a one-to-one corre-
spondence between LDA?s latent topics
and user tags. This allows Labeled LDA to
directly learn word-tag correspondences.
We demonstrate Labeled LDA?s improved
expressiveness over traditional LDA with
visualizations of a corpus of tagged web
pages from del.icio.us. Labeled LDA out-
performs SVMs by more than 3 to 1 when
extracting tag-specific document snippets.
As a multi-label text classifier, our model
is competitive with a discriminative base-
line on a variety of datasets.
1 Introduction
From news sources such as Reuters to modern
community web portals like del.icio.us, a signif-
icant proportion of the world?s textual data is la-
beled with multiple human-provided tags. These
collections reflect the fact that documents are often
about more than one thing?for example, a news
story about a highway transportation bill might
naturally be filed under both transportation and
politics, with neither category acting as a clear
subset of the other. Similarly, a single web page
in del.icio.us might well be annotated with tags as
diverse as arts, physics, alaska, and beauty.
However, not all tags apply with equal speci-
ficity across the whole document, opening up new
opportunities for information retrieval and cor-
pus analysis on tagged corpora. For instance,
users who browse for documents with a particu-
lar tag might prefer to see summaries that focus
on the portion of the document most relevant to
the tag, a task we call tag-specific snippet extrac-
tion. And when a user browses to a particular
document, a tag-augmented user interface might
provide overview visualization cues highlighting
which portions of the document are more or less
relevant to the tag, helping the user quickly access
the information they seek.
One simple approach to these challenges can
be found in models that explicitly address the
credit attribution problem by associating individ-
ual words in a document with their most appropri-
ate labels. For instance, in our news story about
the transportation bill, if the model knew that the
word ?highway? went with transportation and that
the word ?politicians? went with politics, more
relevant passages could be extracted for either la-
bel. We seek an approach that can automatically
learn the posterior distribution of each word in a
document conditioned on the document?s label set.
One promising approach to the credit attribution
problem lies in the machinery of Latent Dirich-
let Allocation (LDA) (Blei et al, 2003), a recent
model that has gained popularity among theoreti-
cians and practitioners alike as a tool for automatic
corpus summarization and visualization. LDA is
a completely unsupervised algorithm that models
each document as a mixture of topics. The model
generates automatic summaries of topics in terms
of a discrete probability distribution over words
for each topic, and further infers per-document
discrete distributions over topics. Most impor-
tantly, LDA makes the explicit assumption that
each word is generated from one underlying topic.
Although LDA is expressive enough to model
248
multiple topics per document, it is not appropriate
for multi-labeled corpora because, as an unsuper-
vised model, it offers no obvious way of incorpo-
rating a supervised label set into its learning proce-
dure. In particular, LDA often learns some topics
that are hard to interpret, and the model provides
no tools for tuning the generated topics to suit an
end-use application, even when time and resources
exist to provide some document labels.
Several modifications of LDA to incorporate
supervision have been proposed in the literature.
Two such models, Supervised LDA (Blei and
McAuliffe, 2007) and DiscLDA (Lacoste-Julien
et al, 2008) are inappropriate for multiply labeled
corpora because they limit a document to being as-
sociated with only a single label. Supervised LDA
posits that a label is generated from each docu-
ment?s empirical topic mixture distribution. Dis-
cLDA associates a single categorical label variable
with each document and associates a topic mixture
with each label. A third model, MM-LDA (Ram-
age et al, 2009), is not constrained to one label
per document because it models each document as
a bag of words with a bag of labels, with topics for
each observation drawn from a shared topic dis-
tribution. But, like the other models, MM-LDA?s
learned topics do not correspond directly with the
label set. Consequently, these models fall short as
a solution to the credit attribution problem. Be-
cause labels have meaning to the people that as-
signed them, a simple solution to the credit attri-
bution problem is to assign a document?s words to
its labels rather than to a latent and possibly less
interpretable semantic space.
This paper presents Labeled LDA (L-LDA), a
generative model for multiply labeled corpora that
marries the multi-label supervision common to
modern text datasets with the word-assignment
ambiguity resolution of the LDA family of mod-
els. In contrast to standard LDA and its existing
supervised variants, our model associates each la-
bel with one topic in direct correspondence. In the
following section, L-LDA is shown to be a natu-
ral extension of both LDA (by incorporating su-
pervision) and Multinomial Naive Bayes (by in-
corporating a mixture model). We demonstrate
that L-LDA can go a long way toward solving the
credit attribution problem in multiply labeled doc-
uments with improved interpretability over LDA
(Section 4). We show that L-LDA?s credit attribu-
tion ability enables it to greatly outperform sup-
D
?
?
?
?
N
z
w
w
K
?
?
Figure 1: Graphical model of Labeled LDA: un-
like standard LDA, both the label set ? as well as
the topic prior ? influence the topic mixture ?.
port vector machines on a tag-driven snippet ex-
traction task on web pages from del.icio.us (Sec-
tion 6). And despite its generative semantics,
we show that Labeled LDA is competitive with
a strong baseline discriminative classifier on two
multi-label text classification tasks (Section 7).
2 Labeled LDA
Labeled LDA is a probabilistic graphical model
that describes a process for generating a labeled
document collection. Like Latent Dirichlet Allo-
cation, Labeled LDA models each document as a
mixture of underlying topics and generates each
word from one topic. Unlike LDA, L-LDA in-
corporates supervision by simply constraining the
topic model to use only those topics that corre-
spond to a document?s (observed) label set. The
model description that follows assumes the reader
is familiar with the basic LDA model (Blei et al,
2003).
Let each document d be represented by a tu-
ple consisting of a list of word indices w
(d)
=
(w
1
, . . . , w
N
d
) and a list of binary topic pres-
ence/absence indicators ?
(d)
= (l
1
, . . . , l
K
)
where eachw
i
? {1, . . . , V } and each l
k
? {0, 1}.
Here N
d
is the document length, V is the vocabu-
lary size and K the total number of unique labels
in the corpus.
We set the number of topics in Labeled LDA to
be the number of unique labels K in the corpus.
The generative process for the algorithm is found
in Table 1. Steps 1 and 2?drawing the multi-
nomial topic distributions over vocabulary ?
k
for
each topic k, from a Dirichlet prior ??remain
the same as for traditional LDA (see (Blei et al,
2003), page 4). The traditional LDA model then
draws a multinomial mixture distribution ?
(d)
over
allK topics, for each document d, from a Dirichlet
prior ?. However, we would like to restrict ?
(d)
to
be defined only over the topics that correspond to
249
1 For each topic k ? {1, . . . ,K}:
2 Generate ?
k
= (?
k,1
, . . . , ?
k,V
)
T
? Dir(?|?)
3 For each document d:
4 For each topic k ? {1, . . . ,K}
5 Generate ?
(d)
k
? {0, 1} ? Bernoulli(?|?
k
)
6 Generate ?
(d)
= L
(d)
??
7 Generate ?
(d)
= (?
l
1
, . . . , ?
l
M
d
)
T
? Dir(?|?
(d)
)
8 For each i in {1, . . . , N
d
}:
9 Generate z
i
? {?
(d)
1
, . . . , ?
(d)
M
d
} ? Mult(?|?
(d)
)
10 Generate w
i
? {1, . . . , V } ? Mult(?|?
z
i
)
Table 1: Generative process for Labeled LDA:
?
k
is a vector consisting of the parameters of the
multinomial distribution corresponding to the k
th
topic, ? are the parameters of the Dirichlet topic
prior and ? are the parameters of the word prior,
while ?
k
is the label prior for topic k. For the
meaning of the projection matrix L
(d)
, please re-
fer to Eq 1.
its labels ?
(d)
. Since the word-topic assignments
z
i
(see step 9 in Table 1) are drawn from this dis-
tribution, this restriction ensures that all the topic
assignments are limited to the document?s labels.
Towards this objective, we first generate the
document?s labels ?
(d)
using a Bernoulli coin toss
for each topic k, with a labeling prior probability
?
k
, as shown in step 5. Next, we define the vector
of document?s labels to be ?
(d)
= {k|?
(d)
k
= 1}.
This allows us to define a document-specific la-
bel projection matrix L
(d)
of size M
d
? K for
each document d, where M
d
= |?
(d)
|, as fol-
lows: For each row i ? {1, . . . ,M
d
} and column
j ? {1, . . . ,K} :
L
(d)
ij
=
{
1 if ?
(d)
i
= j
0 otherwise.
(1)
In other words, the i
th
row of L
(d)
has an entry of
1 in column j if and only if the i
th
document label
?
(d)
i
is equal to the topic j, and zero otherwise.
As the name indicates, we use the L
(d)
matrix to
project the parameter vector of the Dirichlet topic
prior ? = (?
1
, . . . , ?
K
)
T
to a lower dimensional
vector ?
(d)
as follows:
?
(d)
= L
(d)
?? = (?
?
(d)
1
, . . . , ?
?
(d)
M
d
)
T
(2)
Clearly, the dimensions of the projected vector
correspond to the topics represented by the labels
of the document. For example, suppose K = 4
and that a document d has labels given by ?
(d)
=
{0, 1, 1, 0}which implies ?
(d)
= {2, 3}, then L
(d)
would be:
(
0 1 0 0
0 0 1 0
)
.
Then, ?
(d)
is drawn from a Dirichlet distribution
with parameters ?
(d)
= L
(d)
? ? = (?
2
, ?
3
)
T
(i.e., with the Dirichlet restricted to the topics 2
and 3).
This fulfills our requirement that the docu-
ment?s topics are restricted to its own labels. The
projection step constitutes the deterministic step
6 in Table 1. The remaining part of the model
from steps 7 through 10 are the same as for reg-
ular LDA.
The dependency of ? on both ? and ? is in-
dicated by directed edges from ? and ? to ? in
the plate notation in Figure 1. This is the only ad-
ditional dependency we introduce in LDA?s repre-
sentation (please compare with Figure 1 in (Blei et
al., 2003)).
2.1 Learning and inference
In most applications discussed in this paper, we
will assume that the documents are multiply
tagged with human labels, both at learning and in-
ference time.
When the labels ?
(d)
of the document are ob-
served, the labeling prior ? is d-separated from
the rest of the model given ?
(d)
. Hence the model
is same as traditional LDA, except the constraint
that the topic prior ?
(d)
is now restricted to the
set of labeled topics ?
(d)
. Therefore, we can use
collapsed Gibbs sampling (Griffiths and Steyvers,
2004) for training where the sampling probability
for a topic for position i in a document d in La-
beled LDA is given by:
P (z
i
= j|z
?i
) ?
n
w
i
?i,j
+ ?
w
i
n
(?)
?i,j
+ ?
T
1
?
n
(d)
?i,j
+ ?
j
n
(d)
?i,?
+ ?
T
1
(3)
where n
w
i
?i,j
is the count of word w
i
in topic j, that
does not include the current assignment z
i
, a miss-
ing subscript or superscript (e.g. n
(?)
?i,j
)) indicates
a summation over that dimension, and 1 is a vector
of 1?s of appropriate dimension.
Although the equation above looks exactly the
same as that of LDA, we have an important dis-
tinction in that, the target topic j is restricted to
belong to the set of labels, i.e., j ? ?
(d)
.
Once the topic multinomials ? are learned from
the training set, one can perform inference on any
new labeled test document using Gibbs sampling
250
restricted to its tags, to determine its per-word la-
bel assignments z. In addition, one can also com-
pute its posterior distribution ? over topics by ap-
propriately normalizing the topic assignments z.
It should now be apparent to the reader how
the new model addresses some of the problems in
multi-labeled corpora that we highlighted in Sec-
tion 1. For example, since there is a one-to-one
correspondence between the labels and topics, the
model can display automatic topical summaries
for each label k in terms of the topic-specific dis-
tribution ?
k
. Similarly, since the model assigns a
label z
i
to each word w
i
in the document d au-
tomatically, we can now extract portions of the
document relevant to each label k (it would be all
words w
i
? w
(d)
such that z
i
= k). In addition,
we can use the topic distribution ?
(d)
to rank the
user specified labels in the order of their relevance
to the document, thereby also eliminating spurious
ones if necessary.
Finally, we note that other less restrictive vari-
ants of the proposed L-LDA model are possible.
For example, one could consider a version that
allows topics that do not correspond to the label
set of a given document with a small probability,
or one that allows a common background topic in
all documents. We did implement these variants
in our preliminary experiments, but they did not
yield better performance than L-LDA in the tasks
we considered. Hence we do not report them in
this paper.
2.2 Relationship to Naive Bayes
The derivation of the algorithm so far has fo-
cused on its relationship to LDA. However, La-
beled LDA can also be seen as an extension of
the event model of a traditional Multinomial Naive
Bayes classifier (McCallum and Nigam, 1998) by
the introduction of a mixture model. In this sec-
tion, we develop the analogy as another way to
understand L-LDA from a supervised perspective.
Consider the case where no document in the
collection is assigned two or more labels. Now
for a particular document d with label l
d
, Labeled
LDA draws each word?s topic variable z
i
from a
multinomial constrained to the document?s label
set, i.e. z
i
= l
d
for each word position i in the doc-
ument. During learning, the Gibbs sampler will
assign each z
i
to l
d
while incrementing ?
l
d
(w
i
),
effectively counting the occurences of each word
type in documents labeled with l
d
. Thus in the
singly labeled document case, the probability of
each document under Labeled LDA is equal to
the probability of the document under the Multi-
nomial Naive Bayes event model trained on those
same document instances. Unlike the Multino-
mial Naive Bayes classifier, Labeled LDA does
not encode a decision boundary for unlabeled doc-
uments by comparing P (w
(d)
|l
d
) to P (w
(d)
|?l
d
),
although we discuss using Labeled LDA for multi-
label classification in Section 7.
Labeled LDA?s similarity to Naive Bayes ends
with the introduction of a second label to any doc-
ument. In a traditional one-versus-rest Multino-
mial Naive Bayes model, a separate classifier for
each label would be trained on all documents with
that label, so each word can contribute a count
of 1 to every observed label?s word distribution.
By contrast, Labeled LDA assumes that each doc-
ument is a mixture of underlying topics, so the
count mass of single word instance must instead be
distributed over the document?s observed labels.
3 Credit attribution within tagged
documents
Social bookmarking websites contain millions of
tags describing many of the web?s most popu-
lar and useful pages. However, not all tags are
uniformly appropriate at all places within a doc-
ument. In the sections that follow, we examine
mechanisms by which Labeled LDA?s credit as-
signment mechanism can be utilized to help sup-
port browsing and summarizing tagged document
collections.
To create a consistent dataset for experimenting
with our model, we selected 20 tags of medium
to high frequency from a collection of documents
dataset crawled from del.icio.us, a popular so-
cial bookmarking website (Heymann et al, 2008).
From that larger dataset, we selected uniformly at
random four thousand documents that contained
at least one of the 20 tags, and then filtered each
document?s tag set by removing tags not present
in our tag set. After filtering, the resulting cor-
pus averaged 781 non-stop words per document,
with each document having 4 distinct tags on aver-
age. In contrast to many existing text datasets, our
tagged corpus is highly multiply labeled: almost
90% of of the documents have more than one tag.
(For comparison, less than one third of the news
documents in the popular RCV1-v2 collection of
newswire are multiply labeled). We will refer to
251
this collection of data as the del.icio.us tag dataset.
4 Topic Visualization
A first question we ask of Labeled LDA is how its
topics compare with those learned by traditional
LDA on the same collection of documents. We ran
our implementations of Labeled LDA and LDA
on the del.icio.us corpus described above. Both
are based on the standard collapsed Gibbs sam-
pler, with the constraints for Labeled LDA imple-
mented as in Section 2.
web search site blog css content google list page posted great work comments read nice post great april blog march june wordpress
book image pdf review library posted read copyright books title
web
boo
ks
scie
nce
com
p
ute
r
reli
gion
java
cult
ure
works water map human life work science time world years sleep
windows file version linux comp-uter free system software mac
comment god jesus people gospel bible reply lord religion written
applications spring open web java pattern eclipse development ajax
people day link posted time com-ments back music jane permalink
news information service web on-line project site free search home
web images design content java css website articles page learning
jun quote pro views added check anonymous card core power ghz
life written jesus words made man called mark john person fact name
8
house light radio media photo-graphy news music travel cover
game review street public art health food city history science
13
19
4
3
2
12
Tag (Labeled LDA) (LDA) Topic ID
Figure 2: Comparison of some of the 20 topics
learned on del.icio.us by Labeled LDA (left) and
traditional LDA (right), with representative words
for each topic shown in the boxes. Labeled LDA?s
topics are named by their associated tag. Arrows
from right-to-left show the mapping of LDA topics
to the closest Labeled LDA topic by cosine simi-
larity. Tags not shown are: design, education, en-
glish, grammar, history, internet, language, phi-
losophy, politics, programming, reference, style,
writing.
Figure 2 shows the top words associated with
20 topics learned by Labeled LDA and 20 topics
learned by unsupervised LDA on the del.icio.us
document collection. Labeled LDA?s topics are
directly named with the tag that corresponds to
each topic, an improvement over standard prac-
tice of inferring the topic name by inspection (Mei
et al, 2007). The topics learned by the unsu-
pervised variant were matched to a Labeled LDA
topic highest cosine similarity.
The topics selected are representative: com-
pared to Labeled LDA, unmodified LDA allocates
many topics for describing the largest parts of the
The Elements of Style , William Strunk , Jr.
Asserting that one must first know the rules to break them, this 
classic reference book is a must-have for any student and 
conscientious writer.  Intended for use in which the practice of
composition is combined with the study of literature, it gives in
brief space the principal requirements of plain English style and
concentratesattention on the rules of usage and principles of
composition most commonly violated.
Figure 3: Example document with important
words annotated with four of the page?s tags as
learned by Labeled LDA. Red (single underline)
is style, green (dashed underline) grammar, blue
(double underline) reference, and black (jagged
underline) education.
corpus and under-represents tags that are less un-
common: of the 20 topics learned, LDA learned
multiple topics mapping to each of five tags (web,
culture, and computer, reference, and politics, all
of which were common in the dataset) and learned
no topics that aligned with six tags (books, english,
science, history, grammar, java, and philosophy,
which were rarer).
5 Tagged document visualization
In addition to providing automatic summaries of
the words best associated with each tag in the cor-
pus, Labeled LDA?s credit attribution mechanism
can be used to augment the view of a single doc-
ument with rich contextual information about the
document?s tags.
Figure 3 shows one web document from the col-
lection, a page describing a guide to writing En-
glish prose. The 10 most common tags for that
document are writing, reference, english, gram-
mar, style, language, books, book, strunk, and ed-
ucation, the first eight of which were included in
our set of 20 tags. In the figure, each word that has
high posterior probability from one tag has been
annotated with that tag. The red words come from
the style tag, green from the grammar tag, blue
from the reference tag, and black from the educa-
tion tag. In this case, the model does very well at
assigning individual words to the tags that, subjec-
tively, seem to strongly imply the presence of that
tag on this page. A more polished rendering could
add subtle visual cues about which parts of a page
are most appropriate for a particular set of tags.
252
books
L-LDA this classic reference book is a must-have for any
student and conscientious writer. Intended for
SVM the rules of usage and principles of composition
most commonly violated. Search: CONTENTS Bibli-
ographic
language
L-LDA the beginning of a sentence must refer to the gram-
matical subject 8. Divide words at
SVM combined with the study of literature, it gives in brief
space the principal requirements of
grammar
L-LDA requirements of plain English style and concen-
trates attention on the rules of usage and principles of
SVM them, this classic reference book is a must-have for
any student and conscientious writer.
Figure 4: Representative snippets extracted by
L-LDA and tag-specific SVMs for the web page
shown in Figure 3.
6 Snippet Extraction
Another natural application of Labeled LDA?s
credit assignment mechanism is as a means of se-
lecting snippets of a document that best describe
its contents from the perspective of a particular
tag. Consider again the document in Figure 3. In-
tuitively, if this document were shown to a user
interested in the tag grammar, the most appropri-
ate snippet of words might prefer to contain the
phrase ?rules of usage,? whereas a user interested
in the term style might prefer the title ?Elements
of Style.?
To quantitatively evaluate Labeled LDA?s per-
formance at this task, we constructed a set of 29
recently tagged documents from del.icio.us that
were labeled with two or more tags from the 20 tag
subset, resulting in a total of 149 (document,tag)
pairs. For each pair, we extracted a 15-word win-
dow with the highest tag-specific score from the
document. Two systems were used to score each
window: Labeled LDA and a collection of one-
vs-rest SVMs trained for each tag in the system.
L-LDA scored each window as the expected prob-
ability that the tag had generated each word. For
SVMs, each window was taken as its own doc-
ument and scored using the tag-specific SVM?s
un-thresholded scoring function, taking the win-
dow with the most positive score. While a com-
plete solution to the tag-specific snippet extraction
Model Best Snippet Unanimous
L-LDA 72 / 149 24 / 51
SVM 21 / 149 2 / 51
Table 2: Human judgments of tag-specific snippet
quality as extracted by L-LDA and SVM. The cen-
ter column is the number of document-tag pairs for
which a system?s snippet was judged superior. The
right column is the number of snippets for which
all three annotators were in complete agreement
(numerator) in the subset of document scored by
all three annotators (denominator).
problem might be more informed by better lin-
guistic features (such as phrase boundaries), this
experimental setup suffices to evaluate both kinds
of models for their ability to appropriately assign
words to underlying labels.
Figure 3 shows some example snippets output
by our system for this document. Note that while
SVMs did manage to select snippets that were
vaguely on topic, Labeled LDA?s outputs are gen-
erally of superior subjective quality. To quantify
this intuition, three human annotators rated each
pair of snippets. The outputs were randomly la-
beled as ?System A? or ?System B,? and the anno-
tators were asked to judge which system generated
a better tag-specific document subset. The judges
were also allowed to select neither system if there
was no clear winner. The results are summarized
in Table 2.
L-LDA was judged superior by a wide margin:
of the 149 judgments, L-LDA?s output was se-
lected as preferable in 72 cases, whereas SVM?s
was selected in only 21. The difference between
these scores was highly significant (p < .001) by
the sign test. To quantify the reliability of the judg-
ments, 51 of the 149 document-tag pairs were la-
beled by all three annotators. In this group, the
judgments were in substantial agreement,
1
with
Fleiss? Kappa at .63.
Further analysis of the triply-annotated sub-
set yields further evidence of L-LDA?s advantage
over SVM?s: 33 of the 51 were tag-page pairs
where L-LDA?s output was picked by at least one
annotator as a better snippet (although L-LDA
might not have been picked by the other annota-
tors). And of those, 24 were unanimous in that
1
Of the 15 judgments that were in contention, only two
conflicted on which system was superior (L-LDA versus
SVM); the remaining disagreements were about whether or
not one of the systems was a clear winner.
253
all three judges selected L-LDA?s output. By con-
trast, only 10 of the 51 were tag-page pairs where
SVMs? output was picked by at least one anno-
tator, and of those, only 2 were selected unani-
mously.
7 Multilabeled Text Classification
In the preceding section we demonstrated how La-
beled LDA?s credit attribution mechanism enabled
effective modeling within documents. In this sec-
tion, we consider whether L-LDA can be adapted
as an effective multi-label classifier for documents
as a whole. To answer that question, we applied
a modified variant of L-LDA to a multi-label doc-
ument classification problem: given a training set
consisting of documents with multiple labels, pre-
dict the set of labels appropriate for each docu-
ment in a test set.
Multi-label classification is a well researched
problem. Many modern approaches incorporate
label correlations (e.g., Kazawa et al (2004), Ji
et al (2008)). Others, like our algorithm are
based on mixture models (such as Ueda and Saito
(2003)). However, we are aware of no methods
that trade off label-specific word distributions with
document-specific label distributions in quite the
same way.
In Section 2, we discussed learning and infer-
ence when labels are observed. In the task of mul-
tilabel classification, labels are available at train-
ing time, so the learning part remains the same as
discussed before. However, inferring the best set
of labels for an unlabeled document at test time is
more complex: it involves assessing all label as-
signments and returning the assignment that has
the highest posterior probability. However, this
is not straight-forward, since there are 2
K
possi-
ble label assignments. To make matters worse, the
support of ?(?
(d)
) is different for different label
assignments. Although we are in the process of
developing an efficient sampling algorithm for this
inference, for the purposes of this paper we make
the simplifying assumption that the model reduces
to standard LDA at inference, where the document
is free to sample from any of the K topics. This
is a reasonable assumption because allowing the
model to explore the whole topic space for each
document is similar to exploring all possible label
assignments. The document?s most likely labels
can then be inferred by suitably thresholding its
posterior probability over topics.
As a baseline, we use a set of multiple one-vs-
rest SVM classifiers which is a popular and ex-
tremely competitive baseline used by most previ-
ous papers (see (Kazawa et al, 2004; Ueda and
Saito, 2003) for instance). We scored each model
based on Micro-F1 and Macro-F1 as our evalua-
tion measures (Lewis et al, 2004). While the for-
mer allows larger classes to dominate its results,
the latter assigns an equal weight to all classes,
providing us complementary information.
7.1 Yahoo
We ran experiments on a corpus from the Yahoo
directory, modeling our experimental conditions
on the ones described in (Ji et al, 2008).
2
We
considered documents drawn from 8 top level cat-
egories in the Yahoo directory, where each doc-
ument can be placed in any number of subcate-
gories. The results were mixed, with SVMs ahead
on one measure: Labeled LDA beat SVMs on five
out of eight datasets on MacroF1, but didn?t win
on any datasets on MicroF1. Results are presented
in Table 3.
Because only a processed form of the docu-
ments was released, the Yahoo dataset does not
lend itself well to error analysis. However, only
33% of the documents in each top-level category
were applied to more than one sub-category, so the
credit assignment machinery of L-LDA was un-
used for the majority of documents. We there-
fore ran an artificial second set of experiments
considering only those documents that had been
given more than one label in the training data. On
these documents, the results were again mixed, but
Labeled LDA comes out ahead. For MacroF1,
L-LDA beat SVMs on four datasets, SVMs beat
L-LDA on one dataset, and three were a statistical
tie.
3
On MicroF1, L-LDA did much better than on
the larger subset, outperforming on four datasets
with the other four a statistical tie.
It is worth noting that the Yahoo datasets are
skewed by construction to contain many docu-
ments with highly overlapping content: because
each collection is within the same super-class such
as ?Arts?, ?Business?, etc., each sub-categories?
2
We did not carefully tune per-class thresholds of each of
the one vs. rest classifiers in each model, but instead tuned
only one threshold for all classifiers in each model via cross-
validation on the Arts subsets. As such, our numbers were on
an average 3-4% less than those reported in (Ji et al, 2008),
but the methods were comparably tuned.
3
The difference between means of multiple runs were not
significantly different by two-tailed paired t-test.
254
Dataset %MacroF1 %MicroF1
L-LDA SVM L-LDA SVM
Arts 30.70(1.62) 23.23 (0.67) 39.81(1.85) 48.42 (0.45)
Business 30.81(0.75) 22.82 (1.60) 67.00(1.29) 72.15 (0.62)
Computers 27.55(1.98) 18.29 (1.53) 48.95(0.76) 61.97 (0.54)
Education 33.78(1.70) 36.03 (1.30) 41.19(1.48) 59.45 (0.56)
Entertainment 39.42(1.38) 43.22 (0.49) 47.71(0.61) 62.89 (0.50)
Health 45.36(2.00) 47.86 (1.72) 58.13(0.43) 72.21 (0.26)
Recreation 37.63(1.00) 33.77 (1.17) 43.71(0.31) 59.15 (0.71)
Society 27.32(1.24) 23.89 (0.74) 42.98(0.28) 52.29 (0.67)
Table 3: Averaged performance across ten runs of multi-label text classification for predicting subsets
of the named Yahoo directory categories. Numbers in parentheses are standard deviations across runs.
L-LDA outperforms SVMs on 5 subsets with MacroF1, but on no subsets with MicroF1.
vocabularies will naturally overlap a great deal.
L-LDA?s credit attribution mechanism is most ef-
fective at partitioning semantically distinct words
into their respective label vocabularies, so we ex-
pect that Labeled-LDA?s performance as a text
classifier would improve on collections with more
semantically diverse labels.
7.2 Tagged Web Pages
We also applied our method to text classification
on the del.icio.us dataset, where the documents are
naturally multiply labeled (more than 89%) and
where the tags are less inherently similar than in
the Yahoo subcategories. Therefore we expect La-
beled LDA to do better credit assignment on this
subset and consequently to show improved perfor-
mance as a classifier, and indeed this is the case.
We evaluated L-LDA and multiple one-vs-rest
SVMs on 4000 documents with the 20 tag sub-
set described in Section 3. L-LDA and multiple
one-vs-rest SVMs were trained on the first 80% of
documents and evaluated on the remaining 20%,
with results averaged across 10 random permuta-
tions of the dataset. The results are shown in Ta-
ble 4. We tuned the SVMs? shared cost parameter
C(= 10.0) and selected raw term frequency over
tf-idf weighting based on 4-fold cross-validation
on 3,000 documents drawn from an independent
permutation of the data. For L-LDA, we tuned the
shared parameters of threshold and proportional-
ity constants in word and topic priors. L-LDA and
SVM have very similar performance on MacroF1,
while L-LDA substantially outperforms on Mi-
croF1. In both cases, L-LDA?s improvement is
statistically significantly by a 2-tailed paired t-test
at 95% confidence.
Model %MacroF1 %MicroF1
L-LDA 39.85 (.989) 52.12 (.434)
SVM 39.00 (.423) 39.33 (.574)
Table 4: Mean performance across ten runs of
multi-label text classification for predicting 20
tags on del.icio.us data. L-LDA outperforms
SVMs significantly on both metrics by a 2-tailed,
paired t-test at 95% confidence.
8 Discussion
One of the main advantages of L-LDA on mul-
tiply labeled documents comes from the model?s
document-specific topic mixture ?. By explicitly
modeling the importance of each label in the doc-
ument, Labeled LDA can effective perform some
contextual word sense disambiguation, which sug-
gests why L-LDA can outperform SVMs on the
del.icio.us dataset.
As a concrete example, consider the excerpt
of text from the del.icio.us dataset in Figure 5.
The document itself has several tags, including
design and programming. Initially, many of the
likelihood probabilities p(w|label) for the (con-
tent) words in this excerpt are higher for the label
programming than design, including ?content?,
?client?, ?CMS? and even ?designed?, while de-
sign has higher likelihoods for just ?website? and
?happy?. However, after performing inference on
this document using L-LDA, the inferred docu-
ment probability for design (p(design)) is much
higher than it is for programming. In fact, the
higher probability for the tag more than makes up
the difference in the likelihood for all the words
except ?CMS? (Content Management System), so
255
The website is designed, CMS works, content has been added and the client is happy.
The website is designed, CMS works, content has been added and the client is happy.
Before Inference
After Inference
Figure 5: The effect of tag mixture proportions for credit assignment in a web document. Blue (single
underline) words are generated from the design tag; red (dashed underline) from the programming tag.
By themselves, most words used here have a higher probability in programming than in design. But
because the document as a whole is more about design than programming(incorporating words not shown
here), inferring the document?s topic-mixture ? enables L-LDA to correctly re-assign most words.
that L-LDA correctly infers that most of the words
in this passage have more to do with design than
programming.
9 Conclusion
This paper has introduced Labeled LDA, a novel
model of multi-labeled corpora that directly ad-
dresses the credit assignment problem. The new
model improves upon LDA for labeled corpora
by gracefully incorporating user supervision in the
form of a one-to-one mapping between topics and
labels. We demonstrate the model?s effectiveness
on tasks related to credit attribution within docu-
ments, including document visualizations and tag-
specific snippet extraction. An approximation to
Labeled LDA is also shown to be competitive with
a strong baseline (multiple one vs-rest SVMs) for
multi-label classification.
Because Labeled LDA is a graphical model
in the LDA family, it enables a range of natu-
ral extensions for future investigation. For exam-
ple, the current model does not capture correla-
tions between labels, but such correlations might
be introduced by composing Labeled LDA with
newer state of the art topic models like the Cor-
related Topic Model (Blei and Lafferty, 2006) or
the Pachinko Allocation Model (Li and McCal-
lum, 2006). And with improved inference for un-
supervised ?, Labeled LDA lends itself naturally
to modeling semi-supervised corpora where labels
are observed for only some documents.
Acknowledgments
This project was supported in part by the Presi-
dent of Stanford University through the IRiSS Ini-
tiatives Assessment project.
References
D. M. Blei and J. Lafferty. 2006. Correlated Topic
Models. NIPS, 18:147.
D. Blei and J McAuliffe. 2007. Supervised Topic
Models. In NIPS, volume 21.
D. M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
Dirichlet alocation. JMLR.
T. L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. PNAS, 1:5228?35.
P. Heymann, G. Koutrika, and H. Garcia-Molina. 2008.
Can social bookmarking improve web search. In
WSDM.
S. Ji, L. Tang, S. Yu, and J. Ye. 2008. Extracting
shared subspace for multi-label classification. In
KDD, pages 381?389, New York, NY, USA. ACM.
H. Kazawa, H. Taira T. Izumitani, and E. Maeda. 2004.
Maximal margin labeling for multi-topic text catego-
rization. In NIPS.
S. Lacoste-Julien, F. Sha, and M. I. Jordan. 2008. Dis-
cLDA: Discriminative learning for dimensionality
reduction and classification. In NIPS, volume 22.
D. D. Lewis, Y. Yang, T. G. Rose, G. Dietterich, F. Li,
and F. Li. 2004. RCV1: A new benchmark collec-
tion for text categorization research. JMLR, 5:361?
397.
Wei Li and Andrew McCallum. 2006. Pachinko allo-
cation: Dag-structured mixture models of topic cor-
relations. In International conference on Machine
learning, pages 577?584.
A. McCallum and K. Nigam. 1998. A comparison of
event models for naive bayes text classification. In
AAAI-98 workshop on learning for text categoriza-
tion, volume 7.
Q. Mei, X. Shen, and C Zhai. 2007. Automatic label-
ing of multinomial topic models. In KDD.
D. Ramage, P. Heymann, C. D. Manning, and
H. Garcia-Molina. 2009. Clustering the tagged web.
In WSDM.
N. Ueda and K. Saito. 2003. Parametric mixture mod-
els for multi-labeled text includes models that can be
seen to fit within a dimensionality reduction frame-
work. In NIPS.
256
Semantic Language Models for
Topic Detection and Tracking
Ramesh Nallapati
Center for Intelligent Information Retrieval,
Department of Computer Science,
University of Massachusetts,
Amherst, MA 01003.
nmramesh@cs.umass.edu
Abstract
In this work, we present a new semantic lan-
guage modeling approach to model news sto-
ries in the Topic Detection and Tracking (TDT)
task. In the new approach, we build a unigram
language model for each semantic class in a
news story. We also cast the link detection sub-
task of TDT as a two-class classification prob-
lem in which the features of each sample con-
sist of the generative log-likelihood ratios from
each semantic class. We then compute a lin-
ear discriminant classifier using the perceptron
learning algorithm on the training set. Results
on the test set show a marginal improvement
over the unigram performance, but are not very
encouraging on the whole.
1 Introduction
TDT is a research program investigating methods for au-
tomatically organizing news stories by the events that
they discuss (Allan, 2002a). The goal of TDT consists of
breaking the stream of news into individual news stories,
to monitor the stories for events that have not been seen
before and to gather stories into groups that each discuss
a single topic.
Several approaches have been explored for compar-
ing news stories in TDT. The traditional vector space ap-
proach (Yang et al, 1999) using cosine similarity has by
far been the most consistently successful approach across
different tasks and several data sets.
In the recent past, a new probabilistic approach called
Language Modeling (Ponte and Croft, 1998) has proven
to be very effective in several information retrieval tasks.
One of the attractive features of language models is that
they are firmly rooted in the theory of probability thereby
allowing a researcher to explore more sophisticated mod-
els guided by the theoretical framework.
Allan et al(Allan et al, 1999) applied language mod-
els to the first story detection task of TDT and found that
its performance is on par with the traditional vector space
models, if not better. In the language modeling approach
to TDT, we measure the similarity of a news story D to
a topic by the probability of its generation from the topic
model M . Using the unigram assumption of indepen-
dence of terms, one can compute the probability of gen-
eration of a news story as the product of probabilities of
generation of the terms in the story, as shown in the fol-
lowing equation:
P (D|M) =
|D|
?
i=1
P (wi|M) (1)
where wi is the i-th term in the story. The topic model M
is typically evaluated from the statistics of a set of stories
that are known to be on the topic in consideration.
One potential drawback of the unigram language
model is that it treats all terms on an equal footing and
seems to ignore semantic information of the terms. We
believe that such information could be useful in determin-
ing the relative importance of a term to the topic of the
story. For example, terms that belong to the named-entity
type such as person, location, organization may convey
more information about the topic of the story than other
entity types. Likewise, one might expect that nouns and
verbs play a more important role than adjectives, adverbs
or propositions in determining the topic of the story.
The present work is an attempt to extend the language
modeling framework to incorporate a model of the rela-
tive importance of terms according to the semantic class
they belong to.
The remainder of the report is organized as follows.
Section 2 summarizes attempts made in the past in cap-
turing semantic-class information in information retrieval
related tasks. We present the methodology of the new se-
mantic language modeling approach in section 3. In sec-
tion 4, we present details of the link detection task and
                                                               Edmonton, May-June 2003
                                                   Student Research Workshop , pp. 1-6
                                                         Proceedings of HLT-NAACL 2003
its evaluation. Section 5 describes the experiments per-
formed and presents the results obtained. In section 6
we analyze the performance of the new model. Section
7 ends the discussion with a few observations and lays
down the path to future work.
2 Past work
Traditionally NLP techniques have not met with much
success in the IR domain. However, after several ad-
vances in tasks such as automatic tagging of text with
high level semantics such as parts-of-speech (Ratna-
parkhi, 1996), named-entities (Bikel et al, 1999),
sentence-parsing (Charniak, 1997), etc., there is increas-
ing hope that one could leverage this information into IR
techniques. Traditional vector space models (Salton et
al., 1975) and the more recent language models (Ponte
and Croft, 1998) tend to ignore any semantic information
and consider only word-tokens or word-stems as basic
features.
We know of no prior work in the language model-
ing framework that tries to incorporate semantic informa-
tion into IR models. However, in vector space modeling
framework, there have been a few attempts. For example,
Allan, et al(Allan et al, 1999) use an ad-hoc weight-
ing scheme to weight named-entities higher than other
tokens in their vector space models for the new event de-
tection task of TDT. They do not report any significant
improvements in their results. Additionally, the weight-
ing scheme is empirical and they present no principled
approach to compute the weights.
In the field of ad-hoc retrieval, emerging research on
integrating NLP tools into retrieval models seems encour-
aging. Mihalcea and Mihalcea (Mihalcea and Mihalcea,
2001) show that retrieval effectiveness can be improved
by indexing words with their semantic classes such as
parts-of-speech, named-entity-type, WordNet synonyms,
hypernyms, hyponyms, etc.
In this work, we present a principled approach to inte-
grating semantic information into the language modeling
framework and show how to compute the relative impor-
tance of various semantic classes automatically.
3 Semantic language models
Recall that our task involves analyzing and comparing the
content of news stories by the topics that they discuss.
The topic of a news story is typically characterized by an
event, one or more key players which may include per-
sons or organizations (the who? of the event), a location
to which the event is associated (the where? of the event),
a time of occurrence of the event (the when? of the event)
and a description of the event (the what? of the event).
Hence, when comparing news stories, it makes sense to
compare those features between the stories that answer
the above mentioned four ?wh? questions (Allan et al,
2002b). However, extracting these features may not be
a trivial task. It may need a deep understanding of the
semantics of the story.
As a first step towards that end, we can leverage
the ability of statistical taggers that can recognize au-
tomatically all instances of named-entities such as per-
sons, locations, organizations, and parts-of-speech such
as nouns, verbs, adjectives, etc., in a news story. As an
approximation to our exact answers to the four ?wh? ques-
tions, we will assume that the set of tokens labeled as per-
sons and organizations by the taggers correspond to an
answer to the who? question, the set of dates correspond
to the when? question, the set of locations to the where?
question and lastly, the set of nouns, verbs and adjectives
to the what? question. Our hope is that these categories
of named-entities and parts-of-speech help us capture the
semantics of the news story. Hence we will address these
categories as semantic classes in this work and our model
as semantic language model. Our model is a two-stage
process in which the first stage involves computing class-
specific likelihood ratios while the second stage consists
of combining the ratios using a weighted perceptron. The
ensuing discussion presents the mathematical description
of the two stage process.
3.1 Class-specific likelihood ratio
Let C = {C1, .., C|C|} be the set of semantic classes.
Let C(w) be a relation that maps a given occurrence of
a word w to its semantic class C ? C. Then, for any
story D, we define the list of features Fi(D) that belong
to class Ci as follows:
Fi(D) = {w1, .., wn | ?nj=1 (wj ? D)
?
(C(wj) = Ci)}
(2)
where n = |Fi(D)|. In other words, Fi(D) represents
the list of all tokens in the story D that fall into the cate-
gory Ci. Thus, each story is now represented as a set of
feature-lists of all the semantic-classes as shown below:
D ? {F1(D), ..., F|C|(D)} (3)
For each semantic class Ci and story D, we define the
class-specific semantic language model Mi(D) as fol-
lows:
P (w|Mi(D)) = ?
f(w,Fi(D))
|Fi(D)|
+(1??)f(w,Fi(GE))|Fi(GE)|(4)
where f(w,Fi(D)) is the number of occurrences of a
word w in a story D in the class Ci and GE is a gen-
eral English collection, while ? is a smoothing parameter
that lies between 0 and 1. Thus, the class-specific se-
mantic language model Mi(D) is a smoothed probability
distribution of words in class Ci of story D. This is anal-
ogous to the standard document language models used by
IR researchers.
Given two stories D1 and D2, the semantic class spe-
cific likelihood of D2 with respect to D1 is given by:
Li(D2|D1) = ln(
P (Fi(D2))|Mi(D1)
P (Fi(D2))|Mi(GE)
)
= ln(
n
?
j=1
( P (wj |Mi(D1))P (wj |Mi(GE))
)f(wj ,Fi(D2)))
(5)
where n = |Fi(D2)|. We compute the log-
likelihood ratio instead of just the generative probability
P (D2|M(D1)) to overcome the tendency of the genera-
tive probability to favor shorter stories.
The generative semantic-class-specific general English
model is given by:
P (w|Mi(GE)) =
f(w,Fi(GE))
|Fi(GE)|
(6)
3.2 Weighted Perceptron approach
Now, all that remains to be done is to com-
bine the semantic class-specific log-likelihood scores
[L1(D2|D1), .., L|C|(D2|D1)]T in a principled way to
obtain the overall similarity score of D1 with respect to
D2. Towards that end, we cast the link detection task as
a two-class classification problem, the two classes being
?on-topic? and ?off-topic?. In other words, each story-
pair (D1, D2) is a sample and the classification task in-
volves assigning the label ?on-topic? or ?off-topic? to the
story pair. We compute the semantic-class-specific log-
likelihood scores for all classes and treat them as com-
ponents of the feature vector x of the sample as shown
below:
xi(D1, D2) = Li(D2|D1) (7)
We use a linear discriminant function that is a linear
combination of the components of x for classification as
shown in the following equation:
g(y) = wT y (8)
where y is the augmented feature vector given by y =
[1,x]T , w = [w0, w1, .., w|C|]T is the weight vector. In
particular w0 is called the bias or threshold weight. For
a discriminant function of the form of equation 8, a two-
class classifier implements the following decision rule:
Decide ?on-topic? if g(y) > 0 and ?off-topic? otherwise.
The linear discriminant function clearly constitutes a per-
ceptron. Figure 1 shows a graphical representation of
the perceptron that takes the semantic-class-specific log-
likelihood scores as input.
|D2(D1L1
w
)2(Dn
F
)2(Di
F
)2(D
1
0w
?
)1|D2(DnLn
w
)1|D2(DiiL
w
)
1
:
:
:
:
D
F
)1(Dn
M
)
1
(D
i
M
)
1
(D
1
M
::
2
D
1
Figure 1: A graphical representation of the semantic lan-
guage model
As the figure indicates, for each story pair
(D1, D2), we build semantic-class-specific models
[M1(D1), ..,M|C|(D1)] from story D1 as given by
equation 4. We also construct the semantic class-specific
feature lists {F1(D2), .., F|C|(D2)} from story D2 as
defined in equation 2 and then compute the feature vector
x = [L1(D2|D1), .., L|C|(D2|D1)]T where each com-
ponent likelihood ratio is computed as given in equation
5. We then perform an inner product of the augmented
feature vector y and the weight vector w of the percep-
tron and the resulting score is output as shown in the
figure.
The standard perceptron learns the optimal weight vec-
tor w by minimizing its misclassification rate on a train-
ing set (Duda et al, 2000). However, in TDT, misses
(classification of an on-topic story pair as off-topic) are
penalized 10 times more strongly than false alarms (clas-
sification of an off-topic story pair as on-topic) (Allan,
2002a). We have therefore incorporated these penalties
into the criterion function to force the perceptron learn
the optimal classification based on TDT?s cost function.
4 Link Detection task and Evaluation
In this section, we describe one of the tasks called link de-
tection, on which we performed the experiments reported
in this work.
Link detection requires determining whether or not
two randomly selected stories (D1, D2) discuss the same
topic. The evaluation methodology of a link detection
system requires the system to output a score for each
story pair that represents the system?s confidence that
both stories in the pair discuss the same topic. The
system?s performance is then evaluated using a topic-
weighted Detection Error Trade-off (DET) curve (Mar-
tin et al, 1997) that plots miss rate against false alarm
over a large number of story pairs, at different values of
decision-threshold. A Link Detection cost function Clink
is then used to combine the miss and false alarm proba-
bilities at each value of threshold into a single normalized
evaluation score (Allan, 2002a). We use the minimum
value of Clink as the primary measure of effectiveness
and show DET curves to illustrate the error trade-offs. It
may be useful for the reader to remember that, since the
DET curve is an error-tradeoff plot, the closer the curve
is to the origin, the better is the performance, unlike the
standard precision-recall curve familiar to the IR commu-
nity.
5 Experiments and results
We have used Identifinder (Bikel et al, 1999) and Jtag
(Xu et al, 1994) respectively, to tag each term by its
named-entity type and its part of speech category. Addi-
tionally, we have used a list of 423 most frequent words
to remove stop words from stories. Stemming is done us-
ing the Porter stemmer (Porter, 1980) while the model is
implemented using Java.
As a training set, we have used a subset of TDT3
corpus that consists of news stories from eight English
sources collected roughly from October through Decem-
ber 1998. We have used manual transcriptions of stories
when the source is audio/video. The training set consists
of 7200 story pairs. For the general English model for this
set, we have used the same TDT3 natural English manu-
ally transcribed set consisting of 37,526 news stories.
For the test set, we have used a randomly chosen sub-
set of natural English, manually transcribed stories from
TDT2 corpus. It consists of 6,363 story pairs and the gen-
eral English statistics are derived from 40,851 stories.
In the unigram language modeling approach to link
detection, which we have used as baseline in our ex-
periments, we build a topic model M(D1) from one of
the stories D1 in the pair. We then compute the log-
likelihood ratio L(D2|D1) of the second story D2 with
respect to M(D1) similar to equation 5 but considering
the entire document as a single feature list. The semantic
language model score, on the other hand, is computed as
described in section 3.
Sometimes we may use a symmetrized version of the
formula, as shown below:
score(D1, D2) =
1
2(L(D2|D1) + L(D1|D2)) (9)
However, in this work, we have considered only the
asymmetric version of the formula to maintain simplic-
ity of the scoring function. For fair comparison, we have
used an asymmetric version of the baseline unigram lan-
guage model too.
We have considered the categories in figure 2 as our
semantic classes. Note that only terms that are not classi-
fied as persons, organizations or locations are considered
as candidates for nouns. The numbers in the table indi-
cate the weight assigned by the perceptron to each class.
We have trained the perceptron using the 7200 labeled
story-pairs of the training set.
The class All corresponds to the unigram model and
consists of all the terms of the story. Note that some of
the classes are defined as the union of two or more sub-
classes. We have done this to nullify the labeling error of
the named-entity and parts-of-speech taggers. For exam-
ple, we have noticed that Identinder mislabels Persons
as Organizations and vice versa quite frequently. Our
hope is that creating a new class that is a union of both
Persons and Organizations will offset such tagging er-
rors.
Class Perceptron weight
Persons(P) 0.034998
Organizations(O) 0.0258486
Locations(L) 0.0374133
Nouns(N) 0.134969
Verbs(V) -7.99771e-05
Adjectives(A) 0.017435
Adverbs(Ad) -0.0010557
P ? O 0.0647334
P ? O ? L 0.106417
N ? V 0.138696
N ? V ? A 0.16157
All 0.279056
Figure 2: Semantic classes and their weights
The optimum class-weights as learnt by the Percep-
tron offer some interesting insights. First we note that
the class All receives the highest weight and this seems
quite intuitive since this class contains all the informa-
tion in the story. However, somewhat surprisingly, the
class N ? V ? A receives higher weight than the class
P
?
O
?
L indicating that the former class contains more
topical information than the latter. Also, note that Per-
sons are more important than Locations which are in turn
more important than Organizations which seems to agree
with common sense.
Next we trained the unigram model on the training set
and found the optimum value of the smoothing parameter
? to be 0.2. We have used the same value for the smooth-
ing parameter in all the classes of the class-specific lan-
guage models and combined the class-specific likelihood
scores using the perceptron weights. A comparison of the
performance of semantic language model and unigram
model on the training set is shown in the DET curve of
figure 3. Quite disappointingly, the results indicate that
the overall performance as indicated by the minimum cost
in the DET curve has only worsened.
Figure 4 presents a comparison between unigram and
semantic language models on the test set. The smoothing
parameters and the perceptron weights are set to the val-
12
5
10
20
40
60
80
90
.01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90
M
is
s 
pr
ob
ab
ilit
y 
(in
 %
)
False Alarms probability (in %)
Lambda = 0.2
Random Performance
Perceptron weighted class-specific LM
Perceptron weighted class-specific LM TW Min DET Norm(Cost) = 0.1200
Unigram
Unigram TW Min DET Norm(Cost) = 0.1136
Figure 3: Comparison of semantic LM and unigram per-
formance on training set
ues learnt on the training set. This time, however, we note
that the minimum cost of the semantic language model is
slightly lower than that of the unigram model, but the im-
provement is very insignificant.
1
2
5
10
20
40
60
80
90
.01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90
M
is
s 
pr
ob
ab
ilit
y 
(in
 %
)
False Alarms probability (in %)
lambda = 0.2
Random Performance
Perceptron weighted class-specific LM
Perceptron weighted class-specific LM TW Min DET Norm(Cost) = 0.1248
Unigram
Unigram TW Min DET Norm(Cost) = 0.1260
Figure 4: Comparison of semantic LM and unigram per-
formance on test set
6 Discussion
In this section, we first briefly touch upon the variations
in the model we considered and the various experiments
we performed, but could not report in detail owing to
space constraints. Secondly we discuss why we think the
model?s performance is unsatisfactory.
We have considered a simple mixture model to start
with, wherein each class-specific model Mi(D1) gener-
ates a list of features in its class Fi(D2) but the model
itself is sampled with a prior probability of P (Mi(D1))
which we made dependent on |Fi(D1)|. This model?s
performance is found to be far below that of the uni-
gram approach and hence we abandoned it to favor the
perceptron-weighted likelihood ratios.
In terms of experiments done, we started out with the
basic semantic classes of P,O,L,N, V,A and Ad with-
out considering unions of the classes. We found that tak-
ing unions improved performance and we report the list
of classes whose combination performed the best.
Coming to the second part of our discussion, we are
yet to perform exploratory data analysis to understand the
reasons behind the unsatisfactory performance of the new
approach, but we believe the reasons could be three-fold:
Firstly, it is possible that we are operating the semantic
language model at a sub-optimal level. For example, we
have used the same value of the smoothing parameter that
we have learnt for the unigram model in all the classes of
the semantic language model. It is possible that different
classes may require different levels of smoothing for op-
timum performance. We believe one could use a gradient
descent algorithm on TDT?s cost function to learn from
the training set the optimum values of the smoothing pa-
rameters for different classes.
Secondly, a linear discriminant function that a percep-
tron implements is an overly simplistic classifier and may
not be doing a good job on separating the on-topic pairs
from the off-topic ones. A non-linear classifier such as an
SVM (Burges, 1998) could help improve our accuracy.
Lastly, it is possible that the unigram model is already
capturing the relative importance of terms that we are try-
ing to model using our semantic language models. The
likelihood ratio score we use in the unigram approach be-
haves similar to the tf-idf weights, which we know are
a powerful statistic to capture the relative importance of
terms. If this were true, then the semantic language model
may be rendered redundant.
The real reasons will only be revealed by an analysis
of the data and we hope to do this as part of our future
work.
7 Conclusions and Future work
In this work, we have presented a novel approach for link
detection task of TDT. The new approach has three key
ideas, namely modeling the relative importance of terms
by their semantic classes through a new semantic lan-
guage modeling approach, casting the link detection task
as a two-class classification problem and learning the op-
timum linear discriminant function using the perceptron
learning algorithm. We believe this is one of the earli-
est works that attempts incorporating semantic informa-
tion into the language modeling framework. Although
we have built the model specifically for the link detec-
tion task, it is general enough to be extended to the other
tasks of TDT such as Tracking, New Event Detection and
Clustering.
The results on train and test sets indicate that there is a
little or no improvement in the performance from the new
model as compared to the unigram approach.
As part of our future work, we would like to under-
stand the reasons behind the unsatisfactory performance
of the new model and try out a few improvements sug-
gested in section 6. The possible improvements could
consist of finding the optimal smoothing parameters for
each semantic class and using better non-linear classifiers
like SVM. Another possible area of improvement is to
consider more semantic classes such as dates, numbers,
etc.
We would also like to build systems for other tasks in
TDT based on semantic language models and test their
performance. We believe that semantic information is
more critical in tasks such as New Event Detection which
involves identifying the first story that discusses a par-
ticular event. New events are typically characterized by
mentions of new persons, locations or actions and our se-
mantic models are capable of capturing exactly such in-
formation.
Additionally, it has been suggested that statistical mod-
els such as the aspect model (Hoffman, 1999) and the
latent Dirichlet alocation (Blei et al, 2001) which gen-
erate words from a mixture of aspect-models can be ex-
ploited by modeling semantic classes as the aspects. We
will be studying the applicability of these ideas to the cur-
rent task as part of our future work.
We believe the main contribution of our work lies in
our attempt at incorporating semantic information in the
language modeling framework and combining scores in
a principled way. We believe we have only taken a first
step in this direction and much remains to be done as part
of future work.
Acknowledgments
I would like to thank Prof. James Allan for motivating the
idea of exploiting semantic information for TDT and Vic-
tor Lavrenko for his valuable comments. I am also grate-
ful to the anonymous reviewers for their very insightful
comments and suggestions. This work was supported
in part by the Center for Intelligent Information Re-
trieval and in part by SPAWARSYSCEN-SD grant num-
bers N66001-99-1-8912 and N66001-02-1-8903. Any
opinions, findings and conclusions or recommendations
expressed in this material are the author?s and do not nec-
essarily reflect those of the sponsor.
References
Allan, J., Jin, H., Rajman, M., Wayne, C., Gildea, D.,
Lavrenko, V., Hoberman, R., Caputo, D., Topic-Based
Novelty Detection, Summer Workshop Final Report,
Center for Language and Speech Processing, Johns
Hopkins University, 1999.
Allan, J., Introduction to Topic Detection and Tracking,
Topic Detection and Tracking: Event-based Informa-
tion Organization, James Allan, Editor, Kluwer Aca-
demic Publishers, 1-16, 2002a.
Allan, J., Lavrenko, V. and Nallapati, R. UMass at
TDT 2002, Topic Detection and Tracking: Workshop,
2002b.
Bikel, D. M., Schwartz, R. L. and Weischedel, R. M., An
Algorithm that Learns What?s in a Name, Machine
Learning, Vol. 34(1-3), p 211-231, 1999.
Blei, D. M., and Ng, A. Y., and Jordan, M. I., Latent
Dirichlet Allocation, Neural Information Processing
Systems 14, 2001.
Burges, C. J. C., A Tutorial on Support Vector Machines
for Pattern Recognition, Data Mining and Knowledge
Discovery, vol. 2(2), p 121-167, 1998.
Charniak, E., Statistical Parsing with a Context-Free
Grammar and Word Statistics, AAAI, p 598-603, 1997.
Duda, R. O., Hart, P. E. and Stork, D. G., Pattern Classi-
fication, Wiley-Interscience, 2nd edition, 2000.
Hoffman, T., Probabilistic Latent Semantic Analysis,
Proc. of Uncertainty in Articial Intelligence, 1999.
Martin, A., Doddington, G., Kamm, T. and Ordowski,
M., The DET curve in assessment of detection task
performance, EuroSpeech, 1895?1898, 1997.
Mihalcea R. and Mihalcea, S., Word Semantics for In-
formation Retrieval: Moving One Step Closer to the
Semantic Web, International Conference on Tools
with Articial Intelligence, 280-287, 2001.
Ponte, J. M. and Croft, W. B., A Language Modeling
Approach to Information Retrieval, ACM SIGIR, 275-
281, 1998.
Porter, M. F., An algorithm for suffix stripping, Pro-
gram, 14(3):130-137, 1980.
Ratnaparkhi, A., A maximum entropy part-of-speech
tagger, In Proceedings of the Empirical Methods in
Natural Language Processing Conference, University
of Pennsylvania, May 1996.
Salton, G., Yang, C., and Wong, A., A vector-space
model for information retrieval, Comm. of the ACM,
18, 1975.
Xu, J., Broglio, J. and Croft, W. B., The design and im-
plementation of a part of speech tagger for English,
Technical Report IR-52, Center for Intelligent Informa-
tion Retrieval, University of Massachusetts, Amherst,
1994.
Yang, Y., Carbonell, J., Brown, R., Pierce, T., Archibald,
B.T. and Liu, X., Learning Approaches for Detecting
and Tracking News Events. IEEE Intelligent Systems:
Special Issue on Applications of Intelligent Informa-
tion Retrieval, Vol 14(4), 32-43, 1999.
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 455?465, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Multi-instance Multi-label Learning for Relation Extraction
Mihai Surdeanu?, Julie Tibshirani?, Ramesh Nallapati?, Christopher D. Manning?
? Stanford University, Stanford, CA 94305
{mihais,jtibs,manning}@stanford.edu
? Artificial Intelligence Center, SRI International
nallapat@ai.sri.com
Abstract
Distant supervision for relation extraction
(RE) ? gathering training data by aligning a
database of facts with text ? is an efficient ap-
proach to scale RE to thousands of different
relations. However, this introduces a challeng-
ing learning scenario where the relation ex-
pressed by a pair of entities found in a sen-
tence is unknown. For example, a sentence
containing Balzac and France may express
BornIn or Died, an unknown relation, or no re-
lation at all. Because of this, traditional super-
vised learning, which assumes that each ex-
ample is explicitly mapped to a label, is not
appropriate. We propose a novel approach
to multi-instance multi-label learning for RE,
which jointly models all the instances of a pair
of entities in text and all their labels using
a graphical model with latent variables. Our
model performs competitively on two difficult
domains.
1 Introduction
Information extraction (IE), defined as the task of
extracting structured information (e.g., events, bi-
nary relations, etc.) from free text, has received re-
newed interest in the ?big data? era, when petabytes
of natural-language text containing thousands of dif-
ferent structure types are readily available. How-
ever, traditional supervised methods are unlikely to
scale in this context, as training data is either lim-
ited or nonexistent for most of these structures. One
of the most promising approaches to IE that ad-
dresses this limitation is distant supervision, which
generates training data automatically by aligning a
DB =
(
BornIn(Barack Obama,United States)
EmployedBy(Barack Obama,United States)
)
Sentence Latent Label
Barack Obama is the 44th and current President
of the United States.
EmployedBy
Obama was born in the United States just as he
has always said.
BornIn
United States President Barack Obama meets
with Chinese Vice President Xi Jinping today.
EmployedBy
Obama ran for the United States Senate in 2004. ?
Figure 1: Training sentences generated through distant
supervision for a database containing two facts.
database of facts with text (Craven and Kumlien,
1999; Bunescu and Mooney, 2007).
In this paper we focus on distant supervision for
relation extraction (RE), a subproblem of IE that ad-
dresses the extraction of labeled relations between
two named entities. Figure 1 shows a simple exam-
ple for a RE domain with two labels. Distant super-
vision introduces two modeling challenges, which
we highlight in the table. The first challenge is
that some training examples obtained through this
heuristic are not valid, e.g., the last sentence in Fig-
ure 1 is not a correct example for any of the known
labels for the tuple. The percentage of such false
positives can be quite high. For example, Riedel
et al2010) report up to 31% of false positives in
a corpus that matches Freebase relations with New
York Times articles. The second challenge is that
the same pair of entities may have multiple labels
and it is unclear which label is instantiated by any
textual mention of the given tuple. For example, in
Figure 1, the tuple (Barack Obama, United States)
has two valid labels: BornIn and EmployedBy, each
(latently) instantiated in different sentences. In the
455
instance
...
instance
label
instance
label
...
object
Figure 2: Overview of multi-instance multi-label learn-
ing. To contrast, in traditional supervised learning there
is one instance and one label per object. For relation ex-
traction the object is a tuple of two named entities. Each
mention of this tuple in text generates a different instance.
Riedel corpus, 7.5% of the entity tuples in the train-
ing partition have more than one label.
We summarize this multi-instance multi-label
(MIML) learning problem in Figure 2. In this pa-
per we propose a novel graphical model, which we
called MIML-RE, that targets MIML learning for re-
lation extraction. Our work makes the following
contributions:
(a) To our knowledge, MIML-RE is the first RE ap-
proach that jointly models both multiple instances
(by modeling the latent labels assigned to instances)
and multiple labels (by providing a simple method to
capture dependencies between labels). For example,
our model learns that certain labels tend to be gener-
ated jointly while others cannot be jointly assigned
to the same tuple.
(b) We show that MIML-RE performs competitively
on two difficult domains.
2 Related Work
Distant supervision for IE was introduced by Craven
and Kumlien (1999), who focused on the ex-
traction of binary relations between proteins and
cells/tissues/diseases/drugs using the Yeast Protein
Database as a source of distant supervision. Since
then, the approach grew in popularity (Bunescu and
Mooney, 2007; Bellare and McCallum, 2007; Wu
and Weld, 2007; Mintz et al2009; Riedel et al
2010; Hoffmann et al2011; Nguyen and Moschitti,
2011; Sun et al2011; Surdeanu et al2011a).
However, most of these approaches make one or
more approximations in learning. For example,
most proposals heuristically transform distant super-
vision to traditional supervised learning (i.e., single-
instance single-label) (Bellare and McCallum, 2007;
Wu and Weld, 2007; Mintz et al2009; Nguyen
and Moschitti, 2011; Sun et al2011; Surdeanu
et al2011a). Bunescu and Mooney (2007) and
Riedel et al2010) model distant supervision for
relation extraction as a multi-instance single-label
problem, which allows multiple mentions for the
same tuple but disallows more than one label per ob-
ject. Our work is closest to Hoffmann et al2011).
They address the same problem we do (binary rela-
tion extraction) with a MIML model, but they make
two approximations. First, they use a deterministic
model that aggregates latent instance labels into a
set of labels for the corresponding tuple by OR-ing
the classification results. We use instead an object-
level classifier that is trained jointly with the clas-
sifier that assigns latent labels to instances and can
capture dependencies between labels. Second, they
use a Perceptron-style additive parameter update ap-
proach, whereas we train in a Bayesian framework.
We show in Section 5 that these approximations gen-
erally have a negative impact on performance.
MIML learning has been used in fields other than
natural language processing. For example, Zhou
and Zhang (2007) use MIML for scene classifica-
tion. In this problem, each image may be assigned
multiple labels corresponding to the different scenes
captured. Furthermore, each image contains a set of
patches, which forms the bag of instances assigned
to the given object (image). Zhou and Zhang pro-
pose two algorithms that reduce the MIML problem
to a more traditional supervised learning task. In
one algorithm, for example, they convert the task to
a multi-instance single-label problem by creating a
separate bag for each label. Due to this, the pro-
posed approach cannot model inter-label dependen-
cies. Moreover, the authors make a series of approx-
imations, e.g., they assume that each instance in a
bag shares the bag?s overall label. We instead model
all these issues explicitly in our approach.
In general, our approach belongs to the category
of models that learn in the presence of incomplete or
incorrect labels. There has been interest among ma-
chine learning researchers in the general problem of
noisy data, especially in the area of instance-based
learning. Brodley and Friedl (1999) summarize
past approaches and present a simple, all-purpose
method to filter out incorrect data before training.
While potentially applicable to our problem, this ap-
proach is completely general and cannot incorporate
our domain-specific knowledge about how the noisy
456
data is generated.
3 Distant Supervision for Relation Extraction
Here we focus on distant supervision for the ex-
traction of relations between two entities. We de-
fine a relation as the construct r(e1, e2), where r is
the relation name, e.g., BornIn in Figure 1, and e1
and e2 are two entity names, e.g., Barack Obama
and United States. Note that there are entity tu-
ples (e1, e2) that participate in multiple relations,
r1, . . . , ri. In other words, the tuple (e1, e2) is the
object illustrated in Figure 2 and the different rela-
tion names are the labels. We define an entity men-
tion as a sequence of text tokens that matches the
corresponding entity name in some text, and relation
mention (for a given relation r(e1, e2)) as a pair of
entity mentions of e1 and e2 in the same sentence.
Relation mentions thus correspond to the instances
in Figure 2.1 As the latter definition indicates, we
focus on the extraction of relations expressed in a
single sentence. Furthermore, we assume that entity
mentions are extracted by a different process, such
as a named entity recognizer.
We define the task of relation extraction as a func-
tion that takes as input a document collection (C), a
set of entity mentions extracted from C (E), a set of
known relation labels (L) and an extraction model,
and outputs a set of relations (R) such that any of the
relations extracted is supported by at least one sen-
tence in C. To train the extraction model, we use a
database of relations (D) that are instantiated at least
once in C. Using distant supervision, D is aligned
with sentences in C, producing relation mentions for
all relations in D.
4 Model
Our model assumes that each relation mention in-
volving an entity pair has exactly one label, but al-
lows the pair to exhibit multiple labels across differ-
ent mentions. Since we do not know the actual re-
lation label of a mention in the distantly supervised
setting, we model it using a latent variable z that
can take one of the k pre-specified relation labels
as well as an additional NIL label, if no relation is
expressed by the corresponding mention. We model
the multiple relation labels an entity pair can assume
1For this reason, we use relation mention and relation in-
stance interchangeably in this paper.
. . .
. . . . . .
. . .
Figure 3: MIML model plate diagram. We unrolled the
y plate to emphasize that it is a collection of binary clas-
sifiers (one per relation label), whereas the z classifier is
multi-class. Each z and yj classifier has an additional
prior parameter, which is omitted here for clarity.
using a multi-label classifier that takes as input the
latent relation types of the all the mentions involving
that pair. The two-layer hierarchical model is shown
graphically in Figure 3, and is described more for-
mally below. The model includes one multi-class
classifier (for z) and a set of binary classifiers (for
each yj). The z classifier assigns latent labels from
L to individual relation mentions or NIL if no rela-
tion is expressed by the mention. Each yj classifier
decides if relation j holds for the given entity tu-
ple, using the mention-level classifications as input.
Specifically, in the figure:
? n is the number of distinct entity tuples in D;
? Mi is the set of mentions for the ith entity pair;
? x is a sentence and z is the latent relation clas-
sification for that sentence;
? wz is the weight vector for the multi-class
mention-level classifier;
? k is the number of known relation labels in L;
? yj is the top-level classification decision for the
entity pair as to whether the jth relation holds;
? wj is the weight vector for the binary top-level
classifier for the jth relation.
Additionally, we define Pi (Ni) as the set of all
known positive (negative) relation labels for the ith
entity tuple. In this paper, we construct Ni as L\Pi,
but, in general, other scenarios are possible. For
example, both Sun et al2011) and Surdeanu et
457
al. (2011a) proposed models where Ni for the ith tu-
ple (e1, e2) is defined as: {rj | rj(e1, ek) ? D, ek 6=
e2, rj /? Pi}, which is a subset of L\Pi. That is, en-
tity e2 is considered a negative example for relation
rj (in the context of entity e1) only if rj exists in the
training data with a different value.
The addition of the object-level layer (for y) is an
important contribution of this work. This layer can
capture information that cannot be modeled by the
mention-level classifier. For example, it can learn
that two relation labels (e.g., BornIn and SpouseOf)
cannot be generated jointly for the same entity tu-
ple. So, if the z classifier outputs both these la-
bels for different mentions of the same tuple, the y
layer can cancel one of them. Furthermore, the y
classifiers can learn when two labels tend to appear
jointly, e.g., CapitalOf and Contained between two
locations, and use this occurrence as positive rein-
forcement for these labels. We discuss the features
that implement these ideas in Section 5.
4.1 Training
We train the proposed model using hard discrimina-
tive Expectation Maximization (EM). In the Expec-
tation (E) step we assign latent mention labels us-
ing the current model (i.e., the mention and relation
level classifiers). In the Maximization (M) step we
retrain the model to maximize the log likelihood of
the data using the current latent assignments.
In the equations that follow, we refer to
w1, . . . ,wk collectively as wy for compactness.
The vector zi contains the latent mention-level clas-
sifications for the ith entity pair, while yi represents
the corresponding set of gold-standard labels (that
is, y(r)i = 1 if r ? Pi, and y
(r)
i = 0 for r ? Ni.)
Using these notations, the log-likelihood of the data
is given by:
LL(wy,wz) =
n?
i=1
log p(yi|xi,wy,wz)
=
n?
i=1
log
?
zi
p(yi, zi|xi,wy,wz)
The joint probability in the inner summation can be
broken up into simpler parts:
p(yi, zi|xi,wy,wz)
= p(zi|xi,wz)p(yi|zi,wy)
=
?
m?Mi
p(z(m)i |x
(m)
i ,wz)
?
r?Pi?Ni
p(y(r)i |zi,w
(r)
y )
where the last step follows from conditional inde-
pendence. Thus the log-likelihood for this problem
is not convex (it includes a sum of products). How-
ever, we can still use EM, but the optimization fo-
cuses on maximizing the lower bound of the log-
likelihood, i.e., we maximize the above joint proba-
bility for each entity pair in the database. Rewriting
this probability in log space, we obtain:
log p(yi, zi|xi,wy,wz) (1)
=
?
m?Mi
log p(z(m)i |x
(m)
i ,wz)+
?
r?Pi?Ni
log p(y(r)i |zi,w
(r)
y )
The algorithm proceeds as follows.
E-step: In this step we infer the mention-level
classifications zi for each entity tuple, given all its
mentions, the gold labels yi, and current model, i.e.,
wz and wy weights. Formally, we seek to find:
zi
? = argmax
z
p(z|yi,xi,wy,wz)
However it is computationally intractable to con-
sider all vectors z as there is an exponential num-
ber of possible assignments, so we approximate and
consider each mention separately. Concretely,
p(z(m)i |yi,xi,wy,wz)
? p(yi, z
(m)
i |xi,wy,wz)
? p(z(m)i |x
(m)
i ,wz)p(yi|z
?
i,wy)
= p(z(m)i |x
(m)
i ,wz)
?
r?Pi?Ni
p(y(r)i |z
?
i,w
(r)
y )
where z?i contains the previously inferred mention
labels for group i, with the exception of compo-
nent m whose label is replaced by z(m)i . So for
i = 1, . . . , n, and for each m ?Mi we calculate:
z(m)?i =argmax
z
p(z|x(m)i ,wz)? (2)
?
r?Pi?Ni
p(y(r)i |z
?
i,w
(r)
y )
458
Intuitively, the above equation indicates that men-
tion labels are chosen to maximize: (a) the prob-
abilities assigned by the mention-level model; (b)
the probability that the correct relation labels are as-
signed to the corresponding tuple; and (c) the prob-
ability that the labels known to be incorrect are not
assigned to the tuple. For example, if a particular
mention label receives a high mention-level proba-
bility but it is known to be a negative label for that
tuple, it will receive a low overall score.
M-step: In this step we find wy,wz that maxi-
mize the lower bound of the log-likelihood, i.e., the
probability in equation (1), given the current assign-
ments for zi. From equation (1) it is clear that this
can be maximized separately with respect to wy and
wz. Intuitively, this step amounts to learning the
weights for the mention-level classifier (wz) and the
weights for each of the k top-level classifiers (wy).
The updates are given by:
w?z = argmax
w
n?
i=1
?
m?Mi
log p(z(m)?i |x
(m)
i ,w) (3)
w(r)?y = argmax
w
?
1?i?n s.t. r?Pi?Ni
log p(y(r)i |z
?
i ,w) (4)
Note that these are standard updates for logistic re-
gression. We obtained these weights using k + 1
logistic classifiers: one multi-class classifier for wz
and k binary classifiers for each relation label r ? L.
We implemented all using the L2-regularized logis-
tic regression from the publicly-downloadable Stan-
ford CoreNLP package.2 The main difference be-
tween the classifiers is how features are generated:
the mention-level classifier computes its features
based on xi, whereas the relation-level classifiers
generate features based on the current assignments
for zi and the corresponding relation label r. We
discuss the actual features used in our experiments
in Section 5.
4.2 Inference
Given an entity tuple, we obtain its relation labels as
follows. We first classify its mentions:
z(m)?i = argmaxz
p(z|x(m)i ,wz) (5)
2nlp.stanford.edu/software/corenlp.shtml
then decide on the final relation labels using the top-
level classifiers:
y(r)?i = arg max
y?{0,1}
p(y|z?i ,w
(r)
y ) (6)
4.3 Implementation Details
We discuss next several details that are crucial for
the correct implementation of the above model.
Initialization: Since EM is not guaranteed to con-
verge at the global maximum of the observed data
likelihood, it is important to provide it with good
starting values. In our context, the initial values are
labels assigned to zi, which are required to compute
equation (2) in the first iteration (z?i). We generate
these values using a local logistic regression classi-
fier that uses the same features as the mention-level
classifier in the joint model but treats each relation
mention independently. We train this classifier using
?traditional? distant supervision: for each relation in
the databaseD we assume that all the corresponding
mentions are positive examples for the correspond-
ing label (Mintz et al2009). Note that this heuris-
tic repeats relation mentions with different labels for
the tuples that participate in multiple relations. For
example, all the relation mentions in Figure 1 will
yield datums with both the EmployedBy and BornIn
labels. Despite this limitation, we found that this is
a better initialization heuristic than random assign-
ment.
For the second part of equation (2), we initial-
ize the relation-level classifier with a model that
replicates the at least one heuristic of Hoffmann et
al. (2011). Each w(r)y model has a single feature with
a high positive weight that is triggered when label r
is assigned to any of the mentions in z?i .
Avoiding overfitting: A na??ve implementation of
our approach leads to an unrealistic training scenario
where the z classifier generates predictions (in equa-
tion (2)) for the same datums it has seen in training
in the previous iteration. To avoid this overfitting
problem we used cross validation: we divided the
training tuples in K distinct folds and trained K dif-
ferent mention-level classifiers. Each classifier out-
puts p(z|x(m)i ,wz) for tuples in a given fold during
the E-step (equation (2)) and is trained (equation (3))
using tuples from all other folds.
459
At testing time, we compute p(z|x(m)i ,wz) in
equation (5) as the average of the probabilities of
the above set of mention classifiers:
p(z|x(m)i ,wz) =
?K
j=1 p(z|x
(m)
i ,w
j
z)
K
where wjz are the weights of the mention classifier
responsible for fold j. We found that this simple
bagging model performs slightly better in practice
(a couple of tenths of a percent) than training a sin-
gle mention classifier on the latent mention labels
generated in the last training iteration.
Inference during training: During the inference
process in the E-step, the algorithm incrementally
?flips? mention labels based on equation (2), for
each group of mentions Mi. Thus, z?i changes as the
algorithm progresses, which may impact the label
assigned to the remaining mentions in that group. To
avoid any potential bias introduced by the arbitrary
order of mentions as seen in the data, we randomize
each group Mi before we inspect its mentions.
5 Experimental Results
5.1 Data
We evaluate our algorithm on two corpora. The first
was developed by Riedel et al2010) by aligning
Freebase3 relations with the New York Times (NYT)
corpus. They used the Stanford named entity recog-
nizer (Finkel et al2005) to find entity mentions in
text and constructed relation mentions only between
entity mentions in the same sentence.
Riedel et al2010) observes that evaluating on
this corpus underestimates true extraction accuracy
because Freebase is incomplete. Thus, some re-
lations extracted during testing will be incorrectly
marked as wrong, simply because Freebase has no
information on them. To mitigate this issue, Riedel
et al2010) and Hoffman et al2011) perform a
second evaluation where they compute the accuracy
of labels assigned to a set of relation mentions that
they manually annotated. To avoid any potential an-
notation biases, we instead evaluate on a second cor-
pus that has comprehensive annotations generated
by experts for all test relations.
We constructed this second dataset using mainly
resources distributed for the 2010 and 2011 KBP
3freebase.com
shared tasks (Ji et al2010; Ji et al2011). We gen-
erated training relations from the knowledge base
provided by the task organizers, which is a subset
of the English Wikipedia infoboxes from a 2008
snapshot. Similarly to the corpus of Riedel et al
these infoboxes contain open-domain relations be-
tween named entities, but with a different focus.
For example, more than half of the relations in
the evaluation data are alternate names of organi-
zations or persons (e.g., org:alternate names) or re-
lations associated with employment and member-
ship (e.g., per:employee of) (Ji et al2011). We
aligned these relations against a document collec-
tion that merges two distinct sources: (a) the col-
lection provided by the shared task, which contains
approximately 1.5 million documents from a vari-
ety of sources, including newswire, blogs and tele-
phone conversation transcripts; and (b) a complete
snapshot of the English Wikipedia from June 2010.
During training, for each entity tuple (e1, e2), we
retrieved up to 50 sentences that contain both en-
tity mentions.4 We used Stanford?s CoreNLP pack-
age to find entity mentions in text and, similarly to
Riedel et al2010), we construct relation mention
candidates only between entity mentions in the same
sentence. We analyzed a set of over 2,000 relation
mentions and we found that 39% of the mentions
where e1 is an organization name and 36% of men-
tions where e1 is a person name do not express the
corresponding relation.
At evaluation time, the KBP shared task requires
the extraction of all relations r(e1, e2) given a query
that contains only the first entity e1. To accommo-
date this setup, we adjusted our sentence extraction
component to use just e1 as the retrieval query and
we kept up to 50 sentences that contain a mention
of the input entity for each evaluation query. For
tuning and testing we used the 200 queries from the
2010 and 2011 evaluations. We randomly selected
40 queries for development and used the remaining
160 for the formal evaluation.
To address the large number of negative examples
in training, Riedel et alubsampled them randomly
with a retention probability of 10%. For the KBP
corpus, we followed the same strategy, but we used
4Sentences were ranked using the similarity between their
parent document and the query that concatenates the two entity
names. We used the default Lucene similarity measure.
460
# of gold # of gold % of gold entity tuples % of gold entity tuples % of mentions that
relations relations with more than one label with multiple mentions in text do not express # of relation labels
in training in testing in training in training their relation
Riedel 4,700 1,950 7.5% 46.4% up to 31% 51
KBP 183,062 3,334 2.8% 65.1% up to 39% 41
Table 1: Statistics about the two corpora used in this paper. Some of the numbers for the Riedel dataset is from (Riedel
et al2010; Hoffmann et al2011).
a subsampling probability of 5% because this led to
the best results in development for all models.
Table 1 provides additional statistics about the
two corpora. The table indicates that having mul-
tiple mentions for an entity tuple is a very common
phenomenon in both corpora, and that having mul-
tiple labels per tuple is more common in the Riedel
dataset than KBP (7.5% vs. 2.8%).
5.2 Features
Our model requires two sets of features: one for the
mention classifier (z) and one for the relation clas-
sifier (y). In the Riedel dataset, we used the same
features as Riedel et al2010) and Hoffmann et
al. (2011) for the mention classifier. In the KBP
dataset, we used a feature set that was developed in
our previous work (Surdeanu et al2011b). These
features can be grouped in three classes: (a) features
that model the two entities, such as their head words;
(b) features that model the syntactic context of the
relation mention, such as the dependency path be-
tween the two entity mentions; and (c) features that
model the surface context, such as the sequence of
part of speech tags between the two entity mentions.
We used these features for all the models evaluated
on the KBP dataset.5
For the relation-level classifier, we developed two
feature groups. The first models Hoffmann et al
at least one heuristic using a single feature, which
is set to true if at least one mention in zi has the la-
bel r, which is modeled by the current relation clas-
sifier. The second group models the dependencies
between relation labels. This is implemented by a
set of |L| ? 1 features, where feature j is instan-
tiated whenever the label modeled (r) is predicted
jointly with another label rj (rj ? L, rj 6= r) in zi.
These features learn both positive and negative re-
inforcements between labels. For example, if labels
5To avoid an excessive number of features in the KBP exper-
iments, we removed features seen less than five times in train-
ing.
r1 and r2 tend to be generated jointly, the feature for
the corresponding dependency will receive a posi-
tive weight in the models for r1 and r2. Similarly, if
r1 and r2 cannot be generated jointly, the model will
assign a negative weight to feature 2 in r1?s classi-
fier and to feature 1 in r2?s classifier. Note that this
feature is asymmetric, i.e., feature 1 in r2?s classi-
fier may have a different value than feature 2 in r1?s
classifier, depending on the accuracy of the individ-
ual predictions for r1 and r2.
5.3 Baselines
We compare our approach against three models:
Mintz++ ? This is the model used to initialize the
mention-level classifier in our model. As discussed
in Section 4.3, this model follows the ?traditional?
distant supervision heuristic, similarly to (Mintz et
al., 2009). However, our implementation has several
advantages over the original model: (a) we model
each relation mention independently, whereas Mintz
et alollapsed all the mentions of the same entity
tuple into a single datum; (b) we allow multi-label
outputs for a given entity tuple at prediction time
by OR-ing the predictions for the individual rela-
tion mentions corresponding to the tuple (similarly
to (Hoffmann et al2011))6; and (c) we use the
simple bagging strategy described in Section 4.3 to
combine multiple models. Empirically, we observed
that these changes yield a significant improvement
over the original proposal. For this reason, we con-
sider this model a strong baseline on its own.
Riedel ? This is the ?at-least-once? model reported
in (Riedel et al2010), which had the best perfor-
mance in that work. This approach models the task
as a multi-instance single-label problem. Note that
this is the only model shown here that does not allow
multi-label outputs for an entity tuple.
6We also allow multiple labels per tuple at training time,
in which case we replicate the corresponding datum for each
label. However, this did not improve performance significantly
compared to selecting a single label per datum during training.
461
Hoffmann ? This is the ?MultiR? model, which per-
formed the best in (Hoffmann et al2011). This
models RE as a MIML problem, but learns using
a Perceptron algorithm and uses a deterministic ?at
least one? decision instead of a relation classifier.
We used Hoffman?s publicly released code7 for the
experiments on the Riedel dataset and our own im-
plementation for the KBP experiments.8
5.4 Results
We tuned all models using three-fold cross valida-
tion for the Riedel dataset and using the develop-
ment queries for the KBP dataset. MIML-RE has
two parameters that require tuning: the number of
EM epochs (T ) and the number of folds for the men-
tion classifiers (K).9 The values obtained after tun-
ing are T = 15,K = 5 for the Riedel dataset and
T = 8,K = 3 for KBP. Similarly, we tuned the
number of epochs for the Hoffmann model on the
KBP dataset, obtaining an optimal value of 20.
On the Riedel dataset we evaluate all models us-
ing standard precision and recall measures. For the
KBP evaluation we used the official KBP scorer,10
with two changes: (a) we score with the parame-
ter anydoc set to true, which configures the scorer
to accept relation mentions as correct regardless of
their supporting document; and (b) we score only
on the subset of gold relations that have at least one
mention in our sentences. The first decision is neces-
sary because the gold KBP answers contain support-
ing documents only from the corpus provided by the
organizers but we retrieve candidate answers from
multiple collections. The second is required because
the focus of this work is not on sentence retrieval but
on RE, which should be evaluated in isolation.11
Similarly to previous work, we report preci-
sion/recall curves in Figure 4. We evaluate two
variants of MIML-RE: one that includes all the
features for the y model, and another (MIML-RE
7cs.washington.edu/homes/raphaelh/mr/
8The decision to reimplement the Hoffmann model was a
practical one, driven by incompatibilities between their imple-
mentation and our KBP framework.
9We could also tune the prior parameters for both our model
and Mintz++, but we found in early experiments that the default
value of 1 yields the best scores for all priors.
10nlp.cs.qc.cuny.edu/kbp/2011/scoring.html
11Due to these changes, the scores reported in this paper are
not directly comparable with the shared task scores.
At-Least-One) which has only the at least one
feature. For all the Bayesian models implemented
here, we sorted the predicted relations by the noisy-
or score of the top predictions for their mentions.
Formally, we rank a relation r predicted for group i,
i.e., r ? y?i , using:
noisyOri(r) = 1?
?
m?Mi
(1? s(m)i (r))
where s(m)i (r) = p(r|x
(m)
i ,wz) if r = z
(m)?
i or 0 oth-
erwise. The noisy-or formula performs well for
ranking because it integrates model confidence (the
higher the probabilities, the higher the score) and re-
dundancy (the more mentions are predicted with a
label, the higher that label?s score). Note that the
above ranking score does not include the probability
of the relation classifier (equation (6)) for MIML-RE.
While we use equation (6) to generate y?i , we found
that the corresponding probabilities are too coarse
to provide a good ranking score. This is caused by
the fact that our relation-level classifier works with
a small number of (noisy) features. Lastly, for our
implementation of the Hoffmann et alodel, we
used their ranking heuristic (sorting predictions by
the maximum extraction score for that relation).
6 Discussion
Figure 4 indicates that MIML-RE generally outper-
forms the current state of the art. In the Riedel
dataset, MIML-RE has higher overall recall than the
Riedel et alodel, and, for the same recall point,
MIML-RE?s precision is between 2 and 15 points
higher. For most of the curve, our model obtains
better precision for the same recall point than the
Hoffmann model, which currently has the best re-
ported results on this dataset. The difference is as
high as 5 precision points around the middle of the
curve. The Hoffmann model performs better close to
the extremities of the curve (low/high recall). Nev-
ertheless, we argue that our model is more stable
than Hoffmann?s: MIML-RE yields a smoother pre-
cision/recall curve, without most of the depressions
seen in the Hoffmann results. In the KBP dataset,
MIML-RE performs consistently better than our im-
plementation of Hoffmann?s model, with higher pre-
cision values for the same recall point, and much
higher overall recall. We believe that these dif-
ferences are caused by our Bayesian framework,
462
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0  0.05  0.1  0.15  0.2  0.25  0.3
Pr
ec
isi
on
Recall
Hoffmann
Riedel
Mintz++
MIML-RE
MIML-RE At-Least-One
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0  0.05  0.1  0.15  0.2  0.25  0.3
Pr
ec
isi
on
Recall
Hoffmann (our implementation)
Mintz++
MIML-RE
MIML-RE At-Least-One
Figure 4: Results in the Riedel dataset (top) and the KBP dataset (bottom). The Hoffmann scores in the KBP dataset
were generated using our implementation. The other Hoffmann and Riedel results were taken from their papers.
which provides a more formal implementation of the
MIML problem.
Figure 4 also indicates that MIML-RE yields a con-
sistent improvement over Mintz++ (with the excep-
tion of a few points in the low-recall portion of the
KBP curves). The difference in precision for the
same recall point is as high as 25 precision points in
the Riedel dataset and up to 5 points in KBP. Over-
all, the best F1 score of MIML-RE is slightly over 1
point higher than the best F1 score of Mintz++ in
the Riedel dataset and 3 points higher in KBP. Con-
sidering that Mintz++ is a strong baseline and we
evaluate on two challenging domains, we consider
these results proof that the correct modeling of the
MIML scenario is beneficial.
Lastly, Figure 4 shows that MIML-RE outper-
forms its variant without label-dependency fea-
tures (MIML-RE At-Least-One) in the higher-
recall part of the curve in the Riedel dataset. The im-
provement is approximately 1 F1 point throughout
the last segment of the curve. The overall increase
in F1 was found to be significant (p = 0.0296) in a
one-sided, paired t-test over randomly sampled test
data. We see a smaller improvement in KBP (con-
centrated around the middle of the curve), likely be-
cause the number of entity tuples with multiple la-
bels in training is small (see Table 1). Neverthe-
less, this exercise shows that, when dependencies
between labels exist in a dataset, modeling them,
which can be trivially done in MIML-RE, is useful.
463
P R F1
Hoffmann (our implementation) 48.6 29.8 37.0
Mintz++ 43.8 36.8 40.0
MIML-RE 64.8 31.6 42.6
MIML-RE At-Least-One 56.1 32.5 41.1
Table 2: Results at the highest F1 point in the preci-
sion/recall curve on the dataset that contains groups with
at least 10 mentions.
In a similar vein, we tested the models previ-
ously described on a subset of the Riedel evalua-
tion dataset that only includes groups with at least
10 mentions. This corpus contains approximately
2% of the groups from the original testing partition,
out of which 90 tuples have at least one known label
and 1410 groups serve as negative examples.
For conciseness, we do not include the entire
precision/recall curves for this experiment, but sum-
marize them in Table 2, which lists the performance
peak (highest F1 score) for each of the models
investigated. The table shows that MIML-RE obtains
the highest F1 score overall, 1.5 points higher than
MIML-RE At-Least-One and 2.6 points higher
than Mintz++. More importantly, for approximately
the same recall point, MIML-RE obtains a precision
that is over 8 percentage points higher than that of
MIML-RE At-Least-One. A post-hoc inspection
of the results indicates that, indeed, MIML-RE suc-
cessfully eliminates undesired labels when two
(or more) incompatible labels are jointly assigned
to the same tuple. Take for example the tuple
(Mexico City, Mexico), for which the correct re-
lation is /location/administrative division/country.
MIML-RE At-Least-One incorrectly predicts
the additional /location/location/contains relation,
while MIML-RE does not make this prediction
because it recognizes that these two labels are in-
compatible in general: one location cannot both be
within another location and contain it. Indeed, ex-
amining the weights assigned to label-dependency
features in MIML-RE, we see that the model has
assigned a large negative weight to the depen-
dency feature between /location/location/contains
and /location/administrative division/country
for the /location/location/contains class. We
also observe positive dependencies between la-
bels. For example, MIML-RE learns that the
relations /people/person/place lived and /peo-
ple/person/place of birth tend to co-occur and
assigns a positive weight to this dependency feature
for the corresponding classes.
These results strongly suggest that when all as-
pects of the MIML scenario are present, our model
can successfully capture them and make use of the
additional structure to improve performance.
7 Conclusion
In this paper we showed that distant supervision
for RE, which generates training data by aligning a
database of facts with text, poses a distinct multi-
instance multi-label learning scenario. In this set-
ting, each entity pair to be modeled typically has
multiple instances in the text and may have multiple
labels in the database. This is considerably differ-
ent from traditional supervised learning, where each
instance has a single, explicit label.
We argued that this MIML scenario should be
formally addressed. We proposed, to our knowl-
edge, the first approach that models all aspects of the
MIML setting, i.e., the latent assignment of labels to
instances and dependencies between labels assigned
to the same entity pair.
We evaluated our model on two challenging do-
mains and obtained state-of-the-art results on both.
Our model performs well even when not all aspects
of the MIML scenario are common, and as seen in
the discussion, shows significant improvement when
evaluated on entity pairs with many labels or men-
tions. When all aspects of the MIML scenario are
present, our model is well-equipped to handle them.
The code and data used in the experiments re-
ported in this paper are available at: http://nlp.
stanford.edu/software/mimlre.shtml.
Acknowledgments
We gratefully acknowledge the support of Defense Ad-
vanced Research Projects Agency (DARPA) Machine
Reading Program under Air Force Research Laboratory
(AFRL) prime contract no. FA8750-09-C-0181. Any
opinions, findings, and conclusion or recommendations
expressed in this material are those of the author(s) and
do not necessarily reflect the view of the DARPA, AFRL,
or the US government. We gratefully thank Raphael
Hoffmann and Sebastian Riedel for sharing their code
and data and for the many useful discussions.
464
References
Kedar Bellare and Andrew McCallum. 2007. Learn-
ing extractors from unlabeled text using relevant
databases. In Proceedings of the Sixth International
Workshop on Information Extraction on the Web.
Carla Brodley and Mark Friedl. 1999. Identifying mis-
labeled training data. Journal of Artificial Intelligence
Research (JAIR).
Razvan Bunescu and Raymond Mooney. 2007. Learning
to extract relations from the web using minimal super-
vision. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics.
Mark Craven and Johan Kumlien. 1999. Constructing
biological knowledge bases by extracting information
from text sources. In Proceedings of the Seventh Inter-
national Conference on Intelligent Systems for Molec-
ular Biology.
Jenny Rose Finkel, Trond Grenager, and Christopher D.
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of the 43nd Annual Meeting of the As-
sociation for Computational Linguistics.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-
based weak supervision for information extraction of
overlapping relations. In Proceedings of the Annual
Meeting of the Association for Computational Linguis-
tics (ACL).
Heng Ji, Ralph Grishman, Hoa T. Dang, Kira Griffitt, and
Joe Ellis. 2010. Overview of the TAC 2010 knowl-
edge base population track. In Proceedings of the Text
Analytics Conference.
Heng Ji, Ralph Grishman, and Hoa T. Dang. 2011.
Overview of the TAC 2011 knowledge base popula-
tion track. In Proceedings of the Text Analytics Con-
ference.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of the 47th
Annual Meeting of the Association for Computational
Linguistics.
Truc Vien T. Nguyen and Alessandro Moschitti. 2011.
End-to-end relation extraction using distant supervi-
sion from external semantic repositories. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions without
labeled text. In Proceedings of the European Confer-
ence on Machine Learning and Knowledge Discovery
in Databases (ECML PKDD ?10).
Ang Sun, Ralph Grishman, Wei Xu, and Bonan Min.
2011. New York University 2011 system for KBP slot
filling. In Proceedings of the Text Analytics Confer-
ence.
Mihai Surdeanu, Sonal Gupta, John Bauer, David Mc-
Closky, Angel X. Chang, Valentin I. Spitkovsky, and
Christopher D. Manning. 2011a. Stanford?s distantly-
supervised slot-filling system. In Proceedings of the
Text Analytics Conference.
Mihai Surdeanu, David McClosky, Mason R. Smith, An-
drey Gusev, and Christopher D. Manning. 2011b.
Customizing an information extraction system to a
new domain. In Proceedings of the Workshop on Re-
lational Models of Semantics, Portland, Oregon, June.
Fei Wu and Dan Weld. 2007. Autonomously semanti-
fying Wikipedia. In Proceedings of the International
Conference on Information and Knowledge Manage-
ment (CIKM).
Z.H. Zhou and M.L. Zhang. 2007. Multi-instance multi-
label learning with application to scene classification.
In Advances in Neural Information Processing Sys-
tems (NIPS).
465
