Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 439?446,
New York, June 2006. c?2006 Association for Computational Linguistics
Learning for Semantic Parsing with Statistical Machine Translation
Yuk Wah Wong and Raymond J. Mooney
Department of Computer Sciences
The University of Texas at Austin
1 University Station C0500
Austin, TX 78712-0233, USA
{ywwong,mooney}@cs.utexas.edu
Abstract
We present a novel statistical approach to
semantic parsing, WASP, for construct-
ing a complete, formal meaning represen-
tation of a sentence. A semantic parser
is learned given a set of sentences anno-
tated with their correct meaning represen-
tations. The main innovation of WASP
is its use of state-of-the-art statistical ma-
chine translation techniques. A word
alignment model is used for lexical acqui-
sition, and the parsing model itself can be
seen as a syntax-based translation model.
We show that WASP performs favorably
in terms of both accuracy and coverage
compared to existing learning methods re-
quiring similar amount of supervision, and
shows better robustness to variations in
task complexity and word order.
1 Introduction
Recent work on natural language understanding has
mainly focused on shallow semantic analysis, such
as semantic role labeling and word-sense disam-
biguation. This paper considers a more ambi-
tious task of semantic parsing, which is the con-
struction of a complete, formal, symbolic, mean-
ing representation (MR) of a sentence. Seman-
tic parsing has found its way in practical applica-
tions such as natural-language (NL) interfaces to
databases (Androutsopoulos et al, 1995) and ad-
vice taking (Kuhlmann et al, 2004). Figure 1 shows
a sample MR written in a meaning-representation
language (MRL) called CLANG, which is used for
((bowner our {4})
(do our {6} (pos (left (half our)))))
If our player 4 has the ball, then our player 6 should
stay in the left side of our half.
Figure 1: A meaning representation in CLANG
encoding coach advice given to simulated soccer-
playing agents (Kuhlmann et al, 2004).
Prior research in semantic parsing has mainly fo-
cused on relatively simple domains such as ATIS
(Air Travel Information Service) (Miller et al, 1996;
Papineni et al, 1997; Macherey et al, 2001), in
which a typcial MR is only a single semantic frame.
Learning methods have been devised that can gen-
erate MRs with a complex, nested structure (cf.
Figure 1). However, these methods are mostly
based on deterministic parsing (Zelle and Mooney,
1996; Kate et al, 2005), which lack the robustness
that characterizes recent advances in statistical NLP.
Other learning methods involve the use of fully-
annotated augmented parse trees (Ge and Mooney,
2005) or prior knowledge of the NL syntax (Zettle-
moyer and Collins, 2005) in training, and hence re-
quire extensive human efforts when porting to a new
domain or language.
In this paper, we present a novel statistical ap-
proach to semantic parsing which can handle MRs
with a nested structure, based on previous work on
semantic parsing using transformation rules (Kate et
al., 2005). The algorithm learns a semantic parser
given a set of NL sentences annotated with their
correct MRs. It requires no prior knowledge of
the NL syntax, although it assumes that an unam-
biguous, context-free grammar (CFG) of the target
MRL is available. The main innovation of this al-
439
answer(count(city(loc 2(countryid(usa)))))
How many cities are there in the US?
Figure 2: A meaning representation in GEOQUERY
gorithm is its integration with state-of-the-art statis-
tical machine translation techniques. More specif-
ically, a statistical word alignment model (Brown
et al, 1993) is used to acquire a bilingual lexi-
con consisting of NL substrings coupled with their
translations in the target MRL. Complete MRs are
then formed by combining these NL substrings and
their translations under a parsing framework called
the synchronous CFG (Aho and Ullman, 1972),
which forms the basis of most existing statisti-
cal syntax-based translation models (Yamada and
Knight, 2001; Chiang, 2005). Our algorithm is
called WASP, short for Word Alignment-based Se-
mantic Parsing. In initial evaluation on several
real-world data sets, we show that WASP performs
favorably in terms of both accuracy and coverage
compared to existing learning methods requiring the
same amount of supervision, and shows better ro-
bustness to variations in task complexity and word
order.
Section 2 provides a brief overview of the do-
mains being considered. In Section 3, we present
the semantic parsing model of WASP. Section 4 out-
lines the algorithm for acquiring a bilingual lexicon
through the use of word alignments. Section 5 de-
scribes a probabilistic model for semantic parsing.
Finally, we report on experiments that show the ro-
bustness of WASP in Section 6, followed by the con-
clusion in Section 7.
2 Application Domains
In this paper, we consider two domains. The first do-
main is ROBOCUP. ROBOCUP (www.robocup.org)
is an AI research initiative using robotic soccer as its
primary domain. In the ROBOCUP Coach Competi-
tion, teams of agents compete on a simulated soccer
field and receive coach advice written in a formal
language called CLANG (Chen et al, 2003). Fig-
ure 1 shows a sample MR in CLANG.
The second domain is GEOQUERY, where a func-
tional, variable-free query language is used for
querying a small database on U.S. geography (Zelle
and Mooney, 1996; Kate et al, 2005). Figure 2
shows a sample query in this language. Note that
both domains involve the use of MRs with a com-
plex, nested structure.
3 The Semantic Parsing Model
To describe the semantic parsing model of WASP,
it is best to start with an example. Consider the
task of translating the sentence in Figure 1 into its
MR in CLANG. To achieve this task, we may first
analyze the syntactic structure of the sentence us-
ing a semantic grammar (Allen, 1995), whose non-
terminals are the ones in the CLANG grammar. The
meaning of the sentence is then obtained by com-
bining the meanings of its sub-parts according to
the semantic parse. Figure 3(a) shows a possible
partial semantic parse of the sample sentence based
on CLANG non-terminals (UNUM stands for uni-
form number). Figure 3(b) shows the corresponding
CLANG parse from which the MR is constructed.
This process can be formalized as an instance of
synchronous parsing (Aho and Ullman, 1972), orig-
inally developed as a theory of compilers in which
syntax analysis and code generation are combined
into a single phase. Synchronous parsing has seen a
surge of interest recently in the machine translation
community as a way of formalizing syntax-based
translation models (Melamed, 2004; Chiang, 2005).
According to this theory, a semantic parser defines a
translation, a set of pairs of strings in which each
pair is an NL sentence coupled with its MR. To
finitely specify a potentially infinite translation, we
use a synchronous context-free grammar (SCFG) for
generating the pairs in a translation. Analogous to
an ordinary CFG, each SCFG rule consists of a sin-
gle non-terminal on the left-hand side (LHS). The
right-hand side (RHS) of an SCFG rule is a pair of
strings, ??, ??, where the non-terminals in ? are a
permutation of the non-terminals in ?. Below are
some SCFG rules that can be used for generating the
parse trees in Figure 3:
RULE ? ?if CONDITION 1 , DIRECTIVE 2 . ,
(CONDITION 1 DIRECTIVE 2 )?
CONDITION ? ?TEAM 1 player UNUM 2 has the ball ,
(bowner TEAM 1 {UNUM 2 })?
TEAM ? ?our , our?
UNUM ? ?4 , 4?
440
RULE
If CONDITION
TEAM
our
player UNUM
4
has the ball
...
(a) English
RULE
( CONDITION
(bowner TEAM
our
{ UNUM
4
})
...)
(b) CLANG
Figure 3: Partial parse trees for the CLANG statement and its English gloss shown in Figure 1
Each SCFG rule X ? ??, ?? is a combination of a
production of the NL semantic grammar, X ? ?,
and a production of the MRL grammar, X ? ?.
Each rule corresponds to a transformation rule in
Kate et al (2005). Following their terminology,
we call the string ? a pattern, and the string ? a
template. Non-terminals are indexed to show their
association between a pattern and a template. All
derivations start with a pair of associated start sym-
bols, ?S 1 , S 1 ?. Each step of a derivation involves
the rewriting of a pair of associated non-terminals
in both of the NL and MRL streams. Below is a
derivation that would generate the sample sentence
and its MR simultaneously: (Note that RULE is the
start symbol for CLANG)
?RULE 1 , RULE 1 ?
? ?if CONDITION 1 , DIRECTIVE 2 . ,
(CONDITION 1 DIRECTIVE 2 )?
? ?if TEAM 1 player UNUM 2 has the ball, DIR 3 . ,
((bowner TEAM 1 {UNUM 2 }) DIR 3 )?
? ?if our player UNUM 1 has the ball, DIR 2 . ,
((bowner our {UNUM 1 }) DIR 2 )?
? ?if our player 4 has the ball, DIRECTIVE 1 . ,
((bowner our {4}) DIRECTIVE 1 )?
? ...
? ?if our player 4 has the ball, then our player 6
should stay in the left side of our half. ,
((bowner our {4})
(do our {6} (pos (left (half our)))))?
Here the MR string is said to be a translation of the
NL string. Given an input sentence, e, the task of
semantic parsing is to find a derivation that yields
?e, f?, so that f is a translation of e. Since there may
be multiple derivations that yield e (and thus mul-
tiple possible translations of e), a mechanism must
be devised for discriminating the correct derivation
from the incorrect ones.
The semantic parsing model of WASP thus con-
sists of an SCFG, G, and a probabilistic model, pa-
rameterized by ?, that takes a possible derivation, d,
and returns its likelihood of being correct given an
input sentence, e. The output translation, f?, for a
sentence, e, is defined as:
f? = m
(
arg max
d?D(G|e)
Pr?(d|e)
)
(1)
where m(d) is the MR string that a derivation d
yields, and D(G|e) is the set of all possible deriva-
tions of G that yield e. In other words, the output
MR is the yield of the most probable derivation that
yields e in the NL stream.
The learning task is to induce a set of SCFG rules,
which we call a lexicon, and a probabilistic model
for derivations. A lexicon defines the set of deriva-
tions that are possible, so the induction of a proba-
bilistic model first requires a lexicon. Therefore, the
learning task can be separated into two sub-tasks:
(1) the induction of a lexicon, followed by (2) the
induction of a probabilistic model. Both sub-tasks
require a training set, {?ei, fi?}, where each training
example ?ei, fi? is an NL sentence, ei, paired with
its correct MR, fi. Lexical induction also requires
an unambiguous CFG of the MRL. Since there is no
lexicon to begin with, it is not possible to include
correct derivations in the training data. This is un-
like most recent work on syntactic parsing based on
gold-standard treebanks. Therefore, the induction of
a probabilistic model for derivations is done in an
unsupervised manner.
4 Lexical Acquisition
In this section, we focus on lexical learning, which
is done by finding optimal word alignments between
441
RULE ? (CONDITION DIRECTIVE)
TEAM ? our
UNUM ? 4
If
our
player
4
has
the
ball
CONDITION ? (bowner TEAM {UNUM})
Figure 4: Partial word alignment for the CLANG statement and its English gloss shown in Figure 1
NL sentences and their MRs in the training set. By
defining a mapping of words from one language to
another, word alignments define a bilingual lexicon.
Using word alignments to induce a lexicon is not a
new idea (Och and Ney, 2003). Indeed, attempts
have been made to directly apply machine transla-
tion systems to the problem of semantic parsing (Pa-
pineni et al, 1997; Macherey et al, 2001). However,
these systems make no use of the MRL grammar,
thus allocating probability mass to MR translations
that are not even syntactically well-formed. Here we
present a lexical induction algorithm that guarantees
syntactic well-formedness of MR translations by us-
ing the MRL grammar.
The basic idea is to train a statistical word align-
ment model on the training set, and then form a
lexicon by extracting transformation rules from the
K = 10 most probable word alignments between
the training sentences and their MRs. While NL
words could be directly aligned with MR tokens,
this is a bad approach for two reasons. First, not all
MR tokens carry specific meanings. For example, in
CLANG, parentheses and braces are delimiters that
are semantically vacuous. Such tokens are not sup-
posed to be aligned with any words, and inclusion of
these tokens in the training data is likely to confuse
the word alignment model. Second, MR tokens may
exhibit polysemy. For instance, the CLANG pred-
icate pt has three meanings based on the types of
arguments it is given: it specifies the xy-coordinates
(e.g. (pt 0 0)), the current position of the ball (i.e.
(pt ball)), or the current position of a player (e.g.
(pt our 4)). Judging from the pt token alone, the
word alignment model would not be able to identify
its exact meaning.
A simple, principled way to avoid these diffi-
culties is to represent an MR using a sequence of
productions used to generate it. Specifically, the
sequence corresponds to the top-down, left-most
derivation of an MR. Figure 4 shows a partial word
alignment between the sample sentence and the lin-
earized parse of its MR. Here the second produc-
tion, CONDITION ? (bowner TEAM {UNUM}), is
the one that rewrites the CONDITION non-terminal
in the first production, RULE ? (CONDITION DI-
RECTIVE), and so on. Note that the structure of a
parse tree is preserved through linearization, and for
each MR there is a unique linearized parse, since the
MRL grammar is unambiguous. Such alignments
can be obtained through the use of any off-the-shelf
word alignment model. In this work, we use the
GIZA++ implementation (Och and Ney, 2003) of
IBM Model 5 (Brown et al, 1993).
Assuming that each NL word is linked to at most
one MRL production, transformation rules are ex-
tracted in a bottom-up manner. The process starts
with productions whose RHS is all terminals, e.g.
TEAM ? our and UNUM ? 4. For each of these
productions, X ? ?, a rule X ? ??, ?? is ex-
tracted such that ? consists of the words to which
the production is linked, e.g. TEAM ? ?our, our?,
UNUM ? ?4, 4?. Then we consider productions
whose RHS contains non-terminals, i.e. predicates
with arguments. In this case, an extracted pattern
consists of the words to which the production is
linked, as well as non-terminals showing where the
arguments are realized. For example, for the bowner
predicate, the extracted rule would be CONDITION
? ?TEAM 1 player UNUM 2 has (1) ball, (bowner
TEAM 1 {UNUM 2 })?, where (1) denotes a word
gap of size 1, due to the unaligned word the that
comes between has and ball. A word gap, (g), can
be seen as a non-terminal that expands to at most
g words in the NL stream, which allows for some
flexibility in pattern matching. Rule extraction thus
proceeds backward from the end of a linearized MR
442
our
left
penalty
area
REGION ? (left REGION)
REGION ? (penalty-area TEAM)
TEAM ? our
Figure 5: A word alignment from which no rules can be extracted for the penalty-area predicate
parse (so that a predicate is processed only after its
arguments have all been processed), until rules are
extracted for all productions.
There are two cases where the above algorithm
would not extract any rules for a production r. First
is when no descendants of r in the MR parse are
linked to any words. Second is when there is a
link from a word w, covered by the pattern for r,
to a production r? outside the sub-parse rooted at
r. Rule extraction is forbidden in this case be-
cause it would destroy the link between w and r?.
The first case arises when a component of an MR
is not realized, e.g. assumed in context. The sec-
ond case arises when a predicate and its arguments
are not realized close enough. Figure 5 shows an
example of this, where no rules can be extracted
for the penalty-area predicate. Both cases can be
solved by merging nodes in the MR parse tree, com-
bining several productions into one. For example,
since no rules can be extracted for penalty-area,
it is combined with its parent to form REGION ?
(left (penalty-area TEAM)), for which the pat-
tern TEAM left penalty area is extracted.
The above algorithm is effective only when words
linked to an MR predicate and its arguments stay
close to each other, a property that we call phrasal
coherence. Any links that destroy this property
would lead to excessive node merging, a major cause
of overfitting. Since building a model that strictly
observes phrasal coherence often requires rules that
model the reordering of tree nodes, our goal is to
bootstrap the learning process by using a simpler,
word-based alignment model that produces a gen-
erally coherent alignment, and then remove links
that would cause excessive node merging before rule
extraction takes place. Given an alignment, a, we
count the number of links that would prevent a rule
from being extracted for each production in the MR
parse. Then the total sum for all productions is ob-
tained, denoted by v(a). A greedy procedure is em-
ployed that repeatedly removes a link a ? a that
would maximize v(a) ? v(a\{a}) > 0, until v(a)
cannot be further reduced. A link w ? r is never
removed if the translation probability, Pr(r|w), is
greater than a certain threshold (0.9). To replenish
the removed links, links from the most probable re-
verse alignment, a? (obtained by treating the source
language as target, and vice versa), are added to a, as
long as a remains n-to-1, and v(a) is not increased.
5 Parameter Estimation
Once a lexicon is acquired, the next task is to learn a
probabilistic model for the semantic parser. We pro-
pose a maximum-entropy model that defines a con-
ditional probability distribution over derivations (d)
given the observed NL string (e):
Pr?(d|e) =
1
Z?(e)
exp
?
i
?ifi(d) (2)
where fi is a feature function, and Z?(e) is a nor-
malizing factor. For each rule r in the lexicon there
is a feature function that returns the number of times
r is used in a derivation. Also for each word w there
is a feature function that returns the number of times
w is generated from word gaps. Generation of un-
seen words is modeled using an extra feature whose
value is the total number of words generated from
word gaps. The number of features is quite modest
(less than 3,000 in our experiments). A similar fea-
ture set is used by Zettlemoyer and Collins (2005).
Decoding of the model can be done in cubic time
with respect to sentence length using the Viterbi al-
gorithm. An Earley chart is used for keeping track
of all derivations that are consistent with the in-
put (Stolcke, 1995). The maximum conditional like-
lihood criterion is used for estimating the model pa-
rameters, ?i. A Gaussian prior (?2 = 1) is used for
regularizing the model (Chen and Rosenfeld, 1999).
Since gold-standard derivations are not available in
the training data, correct derivations must be treated
as hidden variables. Here we use a version of im-
443
proved iterative scaling (IIS) coupled with EM (Rie-
zler et al, 2000) for finding an optimal set of param-
eters.1 Unlike the fully-supervised case, the condi-
tional likelihood is not concave with respect to ?,
so the estimation algorithm is sensitive to initial pa-
rameters. To assume as little as possible, ? is initial-
ized to 0. The estimation algorithm requires statis-
tics that depend on all possible derivations for a sen-
tence or a sentence-MR pair. While it is not fea-
sible to enumerate all derivations, a variant of the
Inside-Outside algorithm can be used for efficiently
collecting the required statistics (Miyao and Tsujii,
2002). Following Zettlemoyer and Collins (2005),
only rules that are used in the best parses for the
training set are retained in the final lexicon. All
other rules are discarded. This heuristic, commonly
known as Viterbi approximation, is used to improve
accuracy, assuming that rules used in the best parses
are the most accurate.
6 Experiments
We evaluated WASP in the ROBOCUP and GEO-
QUERY domains (see Section 2). To build a cor-
pus for ROBOCUP, 300 pieces of coach advice were
randomly selected from the log files of the 2003
ROBOCUP Coach Competition, which were manu-
ally translated into English (Kuhlmann et al, 2004).
The average sentence length is 22.52. To build a
corpus for GEOQUERY, 880 English questions were
gathered from various sources, which were manu-
ally translated into the functional GEOQUERY lan-
guage (Tang and Mooney, 2001). The average sen-
tence length is 7.48, much shorter than ROBOCUP.
250 of the queries were also translated into Spanish,
Japanese and Turkish, resulting in a smaller, multi-
lingual data set.
For each domain, there was a minimal set of ini-
tial rules representing knowledge needed for trans-
lating basic domain entities. These rules were al-
ways included in a lexicon. For example, in GEO-
QUERY, the initial rules were: NUM ? ?x, x?, for
all x ? R; CITY ? ?c, cityid(?c?, )?, for all
city names c (e.g. new york); and similar rules for
other types of names (e.g. rivers). Name transla-
tions were provided for the multilingual data set (e.g.
1We also implemented limited-memory BFGS (Nocedal,
1980). Preliminary experiments showed that it typically reduces
training time by more than half with similar accuracy.
CITY ? ?nyuu yooku, cityid(?new york?, )? for
Japanese).
Standard 10-fold cross validation was used in our
experiments. A semantic parser was learned from
the training set. Then the learned parser was used
to translate the test sentences into MRs. Translation
failed when there were constructs that the parser did
not cover. We counted the number of sentences that
were translated into anMR, and the number of trans-
lations that were correct. For ROBOCUP, a trans-
lation was correct if it exactly matched the correct
MR. For GEOQUERY, a translation was correct if it
retrieved the same answer as the correct query. Us-
ing these counts, we measured the performance of
the parser in terms of precision (percentage of trans-
lations that were correct) and recall (percentage of
test sentences that were correctly translated). For
ROBOCUP, it took 47 minutes to learn a parser us-
ing IIS. For GEOQUERY, it took 83 minutes.
Figure 6 shows the performance of WASP com-
pared to four other algorithms: SILT (Kate et al,
2005), COCKTAIL (Tang and Mooney, 2001), SCIS-
SOR (Ge and Mooney, 2005) and Zettlemoyer and
Collins (2005). Experimental results clearly show
the advantage of extra supervision in SCISSOR and
Zettlemoyer and Collins?s parser (see Section 1).
However, WASP performs quite favorably compared
to SILT and COCKTAIL, which use the same train-
ing data. In particular, COCKTAIL, a determinis-
tic shift-reduce parser based on inductive logic pro-
gramming, fails to scale up to the ROBOCUP do-
main where sentences are much longer, and crashes
on larger training sets due to memory overflow.
WASP also outperforms SILT in terms of recall,
where lexical learning is done by a local bottom-up
search, which is much less effective than the word-
alignment-based algorithm in WASP.
Figure 7 shows the performance of WASP on
the multilingual GEOQUERY data set. The lan-
guages being considered differ in terms of word or-
der: Subject-Verb-Object for English and Spanish,
and Subject-Object-Verb for Japanese and Turkish.
WASP?s performance is consistent across these lan-
guages despite some slight differences, most proba-
bly due to factors other than word order (e.g. lower
recall for Turkish due to a much larger vocabulary).
Details can be found in a longer version of this pa-
per (Wong, 2005).
444
 0
 20
 40
 60
 80
 100
 0  50  100  150  200  250  300
Pr
ec
is
io
n 
(%
)
Number of training examples
WASP
SILT
COCKTAIL
SCISSOR
(a) Precision for ROBOCUP
 0
 20
 40
 60
 80
 100
 0  50  100  150  200  250  300
R
ec
al
l (%
)
Number of training examples
WASP
SILT
COCKTAIL
SCISSOR
(b) Recall for ROBOCUP
 0
 20
 40
 60
 80
 100
 0  100  200  300  400  500  600  700  800
Pr
ec
is
io
n 
(%
)
Number of training examples
WASP
SILT
COCKTAIL
SCISSOR
Zettlemoyer et al (2005)
(c) Precision for GEOQUERY
 0
 20
 40
 60
 80
 100
 0  100  200  300  400  500  600  700  800
R
ec
al
l (%
)
Number of training examples
WASP
SILT
COCKTAIL
SCISSOR
Zettlemoyer et al (2005)
(d) Recall for GEOQUERY
Figure 6: Precision and recall learning curves comparing various semantic parsers
 0
 20
 40
 60
 80
 100
 0  50  100  150  200  250
Pr
ec
is
io
n 
(%
)
Number of training examples
English
Spanish
Japanese
Turkish
(a) Precision for GEOQUERY
 0
 20
 40
 60
 80
 100
 0  50  100  150  200  250
R
ec
al
l (%
)
Number of training examples
English
Spanish
Japanese
Turkish
(b) Recall for GEOQUERY
Figure 7: Precision and recall learning curves comparing various natural languages
7 Conclusion
We have presented a novel statistical approach to
semantic parsing in which a word-based alignment
model is used for lexical learning, and the parsing
model itself can be seen as a syntax-based trans-
lation model. Our method is like many phrase-
based translation models, which require a simpler,
word-based alignment model for the acquisition of a
phrasal lexicon (Och and Ney, 2003). It is also sim-
ilar to the hierarchical phrase-based model of Chi-
ang (2005), in which hierarchical phrase pairs, es-
sentially SCFG rules, are learned through the use of
a simpler, phrase-based alignment model. Our work
shows that ideas from compiler theory (SCFG) and
machine translation (word alignment models) can be
successfully applied to semantic parsing, a closely-
related task whose goal is to translate a natural lan-
guage into a formal language.
Lexical learning requires word alignments that are
phrasally coherent. We presented a simple greedy
algorithm for removing links that destroy phrasal co-
herence. Although it is shown to be quite effective in
the current domains, it is preferable to have a more
principled way of promoting phrasal coherence. The
problem is that, by treating MRL productions as
atomic units, current word-based alignment models
have no knowledge about the tree structure hidden
in a linearized MR parse. In the future, we would
like to develop a word-based alignment model that
445
is aware of the MRL syntax, so that better lexicons
can be learned.
Acknowledgments
This research was supported by Defense Advanced
Research Projects Agency under grant HR0011-04-
1-0007.
References
A. V. Aho and J. D. Ullman. 1972. The Theory of Pars-
ing, Translation, and Compiling. Prentice Hall, Engle-
wood Cliffs, NJ.
J. F. Allen. 1995. Natural Language Understanding (2nd
Ed.). Benjamin/Cummings, Menlo Park, CA.
I. Androutsopoulos, G. D. Ritchie, and P. Thanisch.
1995. Natural language interfaces to databases: An
introduction. Journal of Natural Language Engineer-
ing, 1(1):29?81.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?312, June.
S. Chen and R. Rosenfeld. 1999. A Gaussian prior for
smoothing maximum entropy models. Technical re-
port, Carnegie Mellon University, Pittsburgh, PA.
M. Chen et al 2003. Users manual: RoboCup soc-
cer server manual for soccer server version 7.07 and
later. Available at http://sourceforge.net/
projects/sserver/.
D. Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. of ACL-05,
pages 263?270, Ann Arbor, MI, June.
R. Ge and R. J. Mooney. 2005. A statistical semantic
parser that integrates syntax and semantics. In Proc.
of CoNLL-05, pages 9?16, Ann Arbor, MI, July.
R. J. Kate, Y. W. Wong, and R. J. Mooney. 2005. Learn-
ing to transform natural to formal languages. In Proc.
of AAAI-05, pages 1062?1068, Pittsburgh, PA, July.
G. Kuhlmann, P. Stone, R. J. Mooney, and J. W. Shavlik.
2004. Guiding a reinforcement learner with natural
language advice: Initial results in RoboCup soccer. In
Proc. of the AAAI-04 Workshop on Supervisory Con-
trol of Learning and Adaptive Systems, San Jose, CA,
July.
K. Macherey, F. J. Och, and H. Ney. 2001. Natural lan-
guage understanding using statistical machine transla-
tion. In Proc. of EuroSpeech-01, pages 2205?2208,
Aalborg, Denmark.
I. D. Melamed. 2004. Statistical machine translation
by parsing. In Proc. of ACL-04, pages 653?660,
Barcelona, Spain.
S. Miller, D. Stallard, R. Bobrow, and R. Schwartz. 1996.
A fully statistical approach to natural language inter-
faces. In Proc. of ACL-96, pages 55?61, Santa Cruz,
CA.
Y. Miyao and J. Tsujii. 2002. Maximum entropy estima-
tion for feature forests. In Proc. of HLT-02, San Diego,
CA, March.
J. Nocedal. 1980. Updating quasi-Newton matrices
with limited storage. Mathematics of Computation,
35(151):773?782, July.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
K. A. Papineni, S. Roukos, and R. T. Ward. 1997.
Feature-based language understanding. In Proc. of
EuroSpeech-97, pages 1435?1438, Rhodes, Greece.
S. Riezler, D. Prescher, J. Kuhn, and M. Johnson. 2000.
Lexicalized stochastic modeling of constraint-based
grammars using log-linear measures and EM training.
In Proc. of ACL-00, pages 480?487, Hong Kong.
A. Stolcke. 1995. An efficient probabilistic context-free
parsing algorithm that computes prefix probabilities.
Computational Linguistics, 21(2):165?201.
L. R. Tang and R. J. Mooney. 2001. Using multiple
clause constructors in inductive logic programming for
semantic parsing. In Proc. of ECML-01, pages 466?
477, Freiburg, Germany.
Y. W. Wong. 2005. Learning for semantic parsing us-
ing statistical machine translation techniques. Techni-
cal Report UT-AI-05-323, Artificial Intelligence Lab,
University of Texas at Austin, Austin, TX, October.
K. Yamada and K. Knight. 2001. A syntax-based sta-
tistical translation model. In Proc. of ACL-01, pages
523?530, Toulouse, France.
J. M. Zelle and R. J. Mooney. 1996. Learning to parse
database queries using inductive logic programming.
In Proc. of AAAI-96, pages 1050?1055, Portland, OR,
August.
L. S. Zettlemoyer and M. Collins. 2005. Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In Proc.
of UAI-05, Edinburgh, Scotland, July.
446
Proceedings of NAACL HLT 2007, pages 172?179,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Generation by Inverting a Semantic Parser That Uses
Statistical Machine Translation
Yuk Wah Wong and Raymond J. Mooney
Department of Computer Sciences
The University of Texas at Austin
1 University Station C0500
Austin, TX 78712-0233, USA
{ywwong,mooney}@cs.utexas.edu
Abstract
This paper explores the use of statisti-
cal machine translation (SMT) methods
for tactical natural language generation.
We present results on using phrase-based
SMT for learning to map meaning repre-
sentations to natural language. Improved
results are obtained by inverting a seman-
tic parser that uses SMT methods to map
sentences into meaning representations.
Finally, we show that hybridizing these
two approaches results in still more accu-
rate generation systems. Automatic and
human evaluation of generated sentences
are presented across two domains and four
languages.
1 Introduction
This paper explores the use of statistical machine
translation (SMT) methods in natural language gen-
eration (NLG), specifically the task of mapping
statements in a formal meaning representation lan-
guage (MRL) into a natural language (NL), i.e. tacti-
cal generation. Given a corpus of NL sentences each
paired with a formal meaning representation (MR),
it is easy to use SMT to construct a tactical gener-
ator, i.e. a statistical model that translates MRL to
NL. However, there has been little, if any, research
on exploiting recent SMT methods for NLG.
In this paper we present results on using a re-
cent phrase-based SMT system, PHARAOH (Koehn
et al, 2003), for NLG.1 Although moderately effec-
1We also tried IBM Model 4/REWRITE (Germann, 2003), a
word-based SMT system, but it gave much worse results.
tive, the inability of PHARAOH to exploit the for-
mal structure and grammar of the MRL limits its ac-
curacy. Unlike natural languages, MRLs typically
have a simple, formal syntax to support effective au-
tomated processing and inference. This MRL struc-
ture can also be used to improve language genera-
tion.
Tactical generation can also be seen as the inverse
of semantic parsing, the task of mapping NL sen-
tences to MRs. In this paper, we show how to ?in-
vert? a recent SMT-based semantic parser, WASP
(Wong and Mooney, 2006), in order to produce a
more effective generation system. WASP exploits
the formal syntax of the MRL by learning a trans-
lator (based on a statistical synchronous context-
free grammar) that maps an NL sentence to a lin-
earized parse-tree of its MR rather than to a flat MR
string. In addition to exploiting the formal MRL
grammar, our approach also allows the same learned
grammar to be used for both parsing and genera-
tion, an elegant property that has been widely ad-
vocated (Kay, 1975; Jacobs, 1985; Shieber, 1988).
We present experimental results in two domains pre-
viously used to test WASP?s semantic parsing abil-
ity: mapping NL queries to a formal database query
language, and mapping NL soccer coaching instruc-
tions to a formal robot command language. WASP?1
is shown to produce a more accurate NL generator
than PHARAOH.
We also show how the idea of generating from
linearized parse-trees rather than flat MRs, used
effectively in WASP?1, can also be exploited in
PHARAOH. A version of PHARAOH that exploits
this approach is experimentally shown to produce
more accurate generators that are more competi-
tive with WASP?1?s. Finally, we also show how
172
((bowner our {4})
(do our {6} (pos (left (half our)))))
If our player 4 has the ball, then our player 6
should stay in the left side of our half.
(a) CLANG
answer(state(traverse 1(riverid(?ohio?))))
What states does the Ohio run through?
(b) GEOQUERY
Figure 1: Sample meaning representations
aspects of PHARAOH?s phrase-based model can be
used to improve WASP?1, resulting in a hybrid sys-
tem whose overall performance is the best.
2 MRLs and Test Domains
In this work, we consider input MRs with a hi-
erarchical structure similar to Moore (2002). The
only restriction on the MRL is that it be defined
by an available unambiguous context-free grammar
(CFG), which is true for almost all computer lan-
guages. We also assume that the order in which MR
predicates appear is relevant, i.e. the order can affect
the meaning of the MR. Note that the order in which
predicates appear need not be the same as the word
order of the target NL, and therefore, the content
planner need not know about the target NL grammar
(Shieber, 1993).
To ground our discussion, we consider two ap-
plication domains which were originally used to
demonstrate semantic parsing. The first domain is
ROBOCUP. In the ROBOCUP Coach Competition
(www.robocup.org), teams of agents compete in a
simulated soccer game and receive coach advice
written in a formal language called CLANG (Chen
et al, 2003). The task is to build a system that trans-
lates this formal advice into English. Figure 1(a)
shows a piece of sample advice.
The second domain is GEOQUERY, where a func-
tional, variable-free query language is used for
querying a small database on U.S. geography (Kate
et al, 2005). The task is to translate formal queries
into NL. Figure 1(b) shows a sample query.
3 Generation using SMT Methods
In this section, we show how SMT methods can be
used to construct a tactical generator. This is in con-
trast to existing work that focuses on the use of NLG
in interlingual MT (Whitelock, 1992), in which the
roles of NLG and MT are switched. We first con-
sider using a phrase-based SMT system, PHARAOH,
for NLG. Then we show how to invert an SMT-based
semantic parser, WASP, to produce a more effective
generation system.
3.1 Generation using PHARAOH
PHARAOH (Koehn et al, 2003) is an SMT system
that uses phrases as basic translation units. Dur-
ing decoding, the source sentence is segmented into
a sequence of phrases. These phrases are then re-
ordered and translated into phrases in the target lan-
guage, which are joined together to form the output
sentence. Compared to earlier word-based methods
such as IBM Models (Brown et al, 1993), phrase-
based methods such as PHARAOH are much more
effective in producing idiomatic translations, and
are currently the best performing methods in SMT
(Koehn and Monz, 2006).
To use PHARAOH for NLG, we simply treat the
source MRL as an NL, so that phrases in the MRL
are sequences of MR tokens. Note that the grammat-
icality of MRs is not an issue here, as they are given
as input.
3.2 WASP: The Semantic Parsing Algorithm
Before showing how generation can be performed
by inverting a semantic parser, we present a brief
overview of WASP (Wong and Mooney, 2006), the
SMT-based semantic parser on which this work is
based.
To describe WASP, it is best to start with an ex-
ample. Consider the task of translating the English
sentence in Figure 1(a) into CLANG. To do this,
we may first generate a parse tree of the input sen-
tence. The meaning of the sentence is then ob-
tained by combining the meanings of the phrases.
This process can be formalized using a synchronous
context-free grammar (SCFG), originally developed
as a grammar formalism that combines syntax anal-
ysis and code generation in compilers (Aho and Ull-
man, 1972). It has been used in syntax-based SMT
to model the translation of one NL to another (Chi-
ang, 2005). A derivation for a SCFG gives rise to
multiple isomorphic parse trees. Figure 2 shows a
partial parse of the sample sentence and its corre-
173
RULE
If CONDITION
TEAM
our
player UNUM
4
has the ball
...
(a) English
RULE
( CONDITION
(bowner TEAM
our
{ UNUM
4
})
...)
(b) CLANG
Figure 2: Partial parse trees for the CLANG statement and its English gloss shown in Figure 1(a)
sponding CLANG parse from which an MR is con-
structed. Note that the two parse trees are isomor-
phic (ignoring terminals).
Each SCFG rule consists of a non-terminal, X ,
on the left-hand side (LHS), and a pair of strings,
??, ??, on the right-hand side (RHS). The non-
terminals in ? are a permutation of the non-terminals
in ? (indices are used to show their correspondence).
In WASP, ? denotes an NL phrase, and X ? ? is
a production of the MRL grammar. Below are the
SCFG rules that generate the parses in Figure 2:
RULE ? ?if CONDITION 1 , DIRECTIVE 2 . ,
(CONDITION 1 DIRECTIVE 2 )?
CONDITION ? ?TEAM 1 player UNUM 2 has the
ball , (bowner TEAM 1 {UNUM 2 })?
TEAM ? ?our , our?
UNUM ? ?4 , 4?
All derivations start with a pair of co-indexed start
symbols of the MRL grammar, ?S 1 , S 1 ?, and each
step involves the rewriting of a pair of co-indexed
non-terminals (by ? and ?, respectively). Given an
input sentence, e, the task of semantic parsing is to
find a derivation that yields ?e, f?, so that f is an MR
translation of e.
Parsing with WASP requires a set of SCFG rules.
These rules are learned using a word alignment
model, which finds an optimal mapping from words
to MR predicates given a set of training sentences
and their correct MRs. Word alignment models have
been widely used for lexical acquisition in SMT
(Brown et al, 1993; Koehn et al, 2003). To use
a word alignment model in the semantic parsing
scenario, we can treat the MRL simply as an NL,
and MR tokens as words, but this often leads to
poor results. First, not all MR tokens carry spe-
cific meanings. For example, in CLANG, parenthe-
ses and braces are delimiters that are semantically
vacuous. Such tokens can easily confuse the word
alignment model. Second, MR tokens may exhibit
polysemy. For example, the CLANG predicate pt
has three meanings based on the types of arguments
it is given (Chen et al, 2003). Judging from the pt
token alone, the word alignment model would not be
able to identify its exact meaning.
A simple, principled way to avoid these difficul-
ties is to represent an MR using a list of productions
used to generate it. This list is used in lieu of the
MR in a word alignment. Figure 3 shows an exam-
ple. Here the list of productions corresponds to the
top-down, left-most derivation of an MR. For each
MR there is a unique linearized parse-tree, since
the MRL grammar is unambiguous. Note that the
structure of the parse tree is preserved through lin-
earization. This allows us to extract SCFG rules in a
bottom-up manner, assuming the alignment is n-to-1
(each word is linked to at most one production). Ex-
traction starts with productions whose RHS is all ter-
minals, followed by those with non-terminals. (De-
tails can be found in Wong and Mooney (2006).)
The rules extracted from Figure 3 would be almost
the same as those used in Figure 2, except the one for
bowner: CONDITION ? ?TEAM 1 player UNUM 2
has (1) ball, (bowner TEAM 1 {UNUM 2 })?. The
token (1) denotes a word gap of size 1, due to the un-
aligned word the that comes between has and ball.
It can be seen as a non-terminal that expands to at
most one word, allowing for some flexibility in pat-
tern matching.
In WASP, GIZA++ (Och and Ney, 2003) is used
to obtain the best alignments from the training ex-
amples. Then SCFG rules are extracted from these
alignments. The resulting SCFG, however, can be
174
RULE ? (CONDITION DIRECTIVE)
TEAM ? our
UNUM ? 4
If
our
player
4
has
the
ball
CONDITION ? (bowner TEAM {UNUM})
Figure 3: Partial word alignment for the CLANG statement and its English gloss shown in Figure 1(a)
ambiguous. Therefore, a maximum-entropy model
that defines the conditional probability of deriva-
tions (d) given an input sentence (e) is used for dis-
ambiguation:
Pr?(d|e) =
1
Z?(e)
exp
?
i
?ifi(d) (1)
The feature functions, fi, are the number of times
each rule is used in a derivation. Z?(e) is the
normalizing factor. The model parameters, ?i, are
trained using L-BFGS (Nocedal, 1980) to maxi-
mize the conditional log-likelihood of the training
examples (with a Gaussian prior). The decoding
task is thus to find a derivation d? that maximizes
Pr?(d?|e), and the output MR translation, f?, is the
yield of d?. This can be done in cubic time with re-
spect to the length of e using an Earley chart parser.
3.3 Generation by Inverting WASP
Now we show how to invert WASP to produce
WASP?1, and use it for NLG. We can use the same
grammar for both parsing and generation, a partic-
ularly appealing aspect of using WASP. Since an
SCFG is fully symmetric with respect to both gen-
erated strings, the same chart used for parsing can
be easily adapted for efficient generation (Shieber,
1988; Kay, 1996).
Given an input MR, f , WASP?1 finds a sentence
e that maximizes Pr(e|f). It is difficult to directly
model Pr(e|f), however, because it has to assign
low probabilities to output sentences that are not
grammatical. There is no such requirement for pars-
ing, because the use of the MRL grammar ensures
the grammaticality of all output MRs. For genera-
tion, we need an NL grammar to ensure grammati-
cality, but this is not available a priori.
This motivates the noisy-channel model for
WASP?1, where Pr(e|f) is divided into two smaller
components:
arg max
e
Pr(e|f) = arg max
e
Pr(e) Pr(f |e) (2)
Pr(e) is the language model, and Pr(f |e) is the
parsing model. The generation task is to find a sen-
tence e such that (1) e is a good sentence a priori,
and (2) its meaning is the same as the input MR. For
the language model, we use an n-grammodel, which
is remarkably useful in ranking candidate generated
sentences (Knight and Hatzivassiloglou, 1995; Ban-
galore et al, 2000; Langkilde-Geary, 2002). For the
parsing model, we re-use the one from WASP (Equa-
tion 1). Hence computing (2) means maximizing the
following:
max
e
Pr(e) Pr(f |e)
? max
d?D(f)
Pr(e(d)) Pr?(d|e(d))
= max
d?D(f)
Pr(e(d)) ? exp?i ?ifi(d)
Z?(e(d))
(3)
where D(f) is the set of derivations that are con-
sistent with f , and e(d) is the output sentence that
a derivation d yields. Compared to most exist-
ing work on generation, WASP?1 has the following
characteristics:
1. It does not require any lexical information in
the input MR, so lexical selection is an integral
part of the decoding algorithm.
2. Each predicate is translated to a phrase. More-
over, it need not be a contiguous phrase (con-
sider the SCFG rule for bowner in Section 3.2).
For decoding, we use an Earley chart generator
that scans the input MR from left to right. This im-
plies that each chart item covers a certain substring
of the input MR, not a subsequence in general. It
175
requires the order in which MR predicates appear
to be fixed, i.e. the order determines the meaning
of the MR. Since the order need not be identical to
the word order of the target NL, there is no need for
the content planner to know the target NL grammar,
which is learned from the training data.
Overall, the noisy-channel model is a weighted
SCFG, obtained by intersecting the NL side of the
WASP SCFG with the n-gram language model. The
chart generator is very similar to the chart parser, ex-
cept for the following:
1. To facilitate the calculation of Pr(e(d)), chart
items now include a list of (n?1)-grams that encode
the context in which output NL phrases appear. The
size of the list is 2N + 2, where N is the number of
non-terminals to be rewritten in the dotted rule.
2. Words are generated from word gaps through
special rules (g) ? ??, ??, where the word gap,
(g), is treated as a non-terminal, and ? is the NL
string that fills the gap (|?| ? g). The empty set
symbol indicates that the NL string does not carry
any meaning. There are similar constructs in Car-
roll et al (1999) that generate function words. Fur-
thermore, to improve efficiency, our generator only
considers gap fillers that have been observed during
training.
3. The normalizing factor in (3), Z?(e(d)), is not
a constant and varies across the output string, e(d).
(Note that Z?(e) is fixed for parsing.) This is un-
fortunate because the calculation of Z?(e(d)) is ex-
pensive, and it is not easy to incorporate it into the
chart generation algorithm. Normalization is done
as follows. First, compute the k-best candidate out-
put strings based on the unnormalized version of (3),
Pr(e(d)) ? exp?i ?ifi(d). Then re-rank the list by
normalizing the scores using Z?(e(d)), which is ob-
tained by running the inside-outside algorithm on
each output string. This results in a decoding al-
gorithm that is approximate?the best output string
might not be in the k-best list?and takes cubic time
with respect to the length of each of the k candidate
output strings (k = 100 in our experiments).
Learning in WASP?1 involves two steps. First, a
back-off n-gram language model with Good-Turing
discounting and no lexical classes2 is built from all
2This is to ensure that the same language model is used in
all systems that we tested.
training sentences using the SRILM Toolkit (Stolcke,
2002). We use n = 2 since higher values seemed to
cause overfitting in our domains. Next, the parsing
model is trained as described in Section 3.2.
4 Improving the SMT-based Generators
The SMT-based generation algorithms, PHARAOH
and WASP?1, while reasonably effective, can be
substantially improved by borrowing ideas from
each other.
4.1 Improving the PHARAOH-based Generator
A major weakness of PHARAOH as an NLG sys-
tem is its inability to exploit the formal structure of
the MRL. Like WASP?1, the phrase extraction al-
gorithm of PHARAOH is based on the output of a
word alignment model such as GIZA++ (Koehn et
al., 2003), which performs poorly when applied di-
rectly to MRLs (Section 3.2).
We can improve the PHARAOH-based generator
by supplying linearized parse-trees as input rather
than flat MRs. As a result, the basic translation units
are sequences of MRL productions, rather than se-
quences of MR tokens. This way PHARAOH can
exploit the formal grammar of the MRL to produce
high-quality phrase pairs. The same idea is used in
WASP?1 to produce high-quality SCFG rules. We
call the resulting hybrid NLG system PHARAOH++.
4.2 Improving the WASP-based Generator
There are several aspects of PHARAOH that can be
used to improve WASP?1. First, the probabilistic
model of WASP?1 is less than ideal as it requires
an extra re-ranking step for normalization, which is
expensive and prone to over-pruning. To remedy this
situation, we can borrow the probabilistic model of
PHARAOH, and define the parsing model as:
Pr(d|e(d)) =
?
d?d
w(r(d)) (4)
which is the product of the weights of the rules used
in a derivation d. The rule weight, w(X ? ??, ??),
is in turn defined as:
P (?|?)?1P (?|?)?2Pw(?|?)?3Pw(?|?)?4 exp(?|?|)?5
where P (?|?) and P (?|?) are the relative frequen-
cies of ? and ?, and Pw(?|?) and Pw(?|?) are
176
the lexical weights (Koehn et al, 2003). The word
penalty, exp(?|?|), allows some control over the
output sentence length. Together with the language
model, the new formulation of Pr(e|f) is a log-
linear model with ?i as parameters. The advantage
of this model is that maximization requires no nor-
malization and can be done exactly and efficiently.
The model parameters are trained using minimum
error-rate training (Och, 2003).
Following the phrase extraction phase in
PHARAOH, we eliminate word gaps by incorpo-
rating unaligned words as part of the extracted
NL phrases (Koehn et al, 2003). The reason is
that while word gaps are useful in dealing with
unknown phrases during semantic parsing, for
generation, using known phrases generally leads to
better fluency. For the same reason, we also allow
the extraction of longer phrases that correspond to
multiple predicates (but no more than 5).
We call the resulting hybrid system WASP?1++.
It is similar to the syntax-based SMT system of Chi-
ang (2005), which uses both SCFG and PHARAOH?s
probabilistic model. The main difference is that we
use the MRL grammar to constrain rule extraction,
so that significantly fewer rules are extracted, mak-
ing it possible to do exact inference.
5 Experiments
We evaluated all four SMT-based NLG systems in-
troduced in this paper: PHARAOH, WASP?1, and the
hybrid systems, PHARAOH++ and WASP?1++.
We used the ROBOCUP and GEOQUERY corpora
in our experiments. The ROBOCUP corpus consists
of 300 pieces of coach advice taken from the log files
of the 2003 ROBOCUP Coach Competition. The ad-
vice was written in CLANG and manually translated
to English (Kuhlmann et al, 2004). The average
MR length is 29.47 tokens, or 12.82 nodes for lin-
earized parse-trees. The average sentence length is
22.52. The GEOQUERY corpus consists of 880 En-
glish questions gathered from various sources. The
questions were manually translated to the functional
GEOQUERY language (Kate et al, 2005). The av-
erage MR length is 17.55 tokens, or 5.55 nodes for
linearized parse-trees. The average sentence length
is 7.57.
Reference: If our player 2, 3, 7 or 5 has the ball
and the ball is close to our goal line ...
PHARAOH++: If player 3 has the ball is in 2 5 the
ball is in the area near our goal line ...
WASP?1++: If players 2, 3, 7 and 5 has the ball
and the ball is near our goal line ...
Figure 4: Sample partial system output in the
ROBOCUP domain
ROBOCUP GEOQUERY
BLEU NIST BLEU NIST
PHARAOH 0.3247 5.0263 0.2070 3.1478
WASP?1 0.4357 5.4486 0.4582 5.9900
PHARAOH++ 0.4336 5.9185 0.5354 6.3637
WASP?1++ 0.6022 6.8976 0.5370 6.4808
Table 1: Results of automatic evaluation; bold type
indicates the best performing system (or systems)
for a given domain-metric pair (p < 0.05)
5.1 Automatic Evaluation
We performed 4 runs of 10-fold cross validation, and
measured the performance of the learned generators
using the BLEU score (Papineni et al, 2002) and the
NIST score (Doddington, 2002). Both MT metrics
measure the precision of a translation in terms of the
proportion of n-grams that it shares with the refer-
ence translations, with the NIST score focusing more
on n-grams that are less frequent and more informa-
tive. Both metrics have recently been used to eval-
uate generators (Langkilde-Geary, 2002; Nakanishi
et al, 2005; Belz and Reiter, 2006).
All systems were able to generate sentences for
more than 97% of the input. Figure 4 shows some
sample output of the systems. Table 1 shows the
automatic evaluation results. Paired t-tests were
used to measure statistical significance. A few
observations can be made. First, WASP?1 pro-
duced a more accurate generator than PHARAOH.
Second, PHARAOH++ significantly outperformed
PHARAOH, showing the importance of exploiting
the formal structure of the MRL. Third, WASP?1++
significantly outperformed WASP?1. Most of the
gain came from PHARAOH?s probabilistic model.
Decoding was also 4?11 times faster, despite ex-
act inference and a larger grammar due to extrac-
tion of longer phrases. Lastly, WASP?1++ signifi-
cantly outperformed PHARAOH++ in the ROBOCUP
177
ROBOCUP GEOQUERY
Flu. Ade. Flu. Ade.
PHARAOH++ 2.5 2.9 4.3 4.7
WASP?1++ 3.6 4.0 4.1 4.7
Table 2: Results of human evaluation
domain. This is because WASP?1++ allows dis-
contiguous NL phrases and PHARAOH++ does not.
Such phrases are commonly used in ROBOCUP
for constructions like: players 2 , 3 , 7 and 5;
26.96% of the phrases generated during testing were
discontiguous. When faced with such predicates,
PHARAOH++ would consistently omit some of the
words: e.g. players 2 3 7 5, or not learn any phrases
for those predicates at all. On the other hand, only
4.47% of the phrases generated during testing for
GEOQUERY were discontiguous, so the advantage of
WASP?1++ over PHARAOH++ was not as obvious.
Our BLEU scores are not as high as those re-
ported in Langkilde-Geary (2002) and Nakanishi et
al. (2005), which are around 0.7?0.9. However,
their work involves the regeneration of automati-
cally parsed text, and the MRs that they use, which
are essentially dependency parses, contain extensive
lexical information of the target NL.
5.2 Human Evaluation
Automatic evaluation is only an imperfect substitute
for human assessment. While it is found that BLEU
and NIST correlate quite well with human judgments
in evaluating NLG systems (Belz and Reiter, 2006),
it is best to support these figures with human evalu-
ation, which we did on a small scale. We recruited 4
native speakers of English with no previous experi-
ence with the ROBOCUP and GEOQUERY domains.
Each subject was given the same 20 sentences for
each domain, randomly chosen from the test sets.
For each sentence, the subjects were asked to judge
the output of PHARAOH++ and WASP?1++ in terms
of fluency and adequacy. They were presented with
the following definition, adapted from Koehn and
Monz (2006):
Score Fluency Adequacy
5 Flawless English All meaning
4 Good English Most meaning
3 Non-native English Some meaning
PHARAOH++ WASP?1++
BLEU NIST BLEU NIST
English 0.5344 5.3289 0.6035 5.7133
Spanish 0.6042 5.6321 0.6175 5.7293
Japanese 0.6171 4.5357 0.6585 4.6648
Turkish 0.4562 4.2220 0.4824 4.3283
Table 3: Results of automatic evaluation on the mul-
tilingual GEOQUERY data set
Score Fluency Adequacy
2 Disfluent English Little meaning
1 Incomprehensible No meaning
For each generated sentence, we computed the av-
erage of the 4 human judges? scores. No score
normalization was performed. Then we compared
the two systems using a paired t-test. Table 2
shows that WASP?1++ produced better generators
than PHARAOH++ in the ROBOCUP domain, con-
sistent with the results of automatic evaluation.
5.3 Multilingual Experiments
Lastly, we describe our experiments on the mul-
tilingual GEOQUERY data set. The 250-example
data set is a subset of the larger GEOQUERY cor-
pus. All English questions in this data set were
manually translated into Spanish, Japanese and
Turkish, while the corresponding MRs remain un-
changed. Table 3 shows the results, which are sim-
ilar to previous results on the larger GEOQUERY
corpus. WASP?1++ outperformed PHARAOH++
for some language-metric pairs, but otherwise per-
formed comparably.
6 Related Work
Numerous efforts have been made to unify the tasks
of semantic parsing and tactical generation. One of
the earliest espousals of the notion of grammar re-
versability can be found in Kay (1975). Shieber
(1988) further noted that not only a single gram-
mar can be used for parsing and generation, but the
same language-processing architecture can be used
for both tasks. Kay (1996) identified parsing charts
as such an architecture, which led to the develop-
ment of various chart generation algorithms: Car-
roll et al (1999) for HPSG, Bangalore et al (2000)
for LTAG, Moore (2002) for unification grammars,
178
White and Baldridge (2003) for CCG. More re-
cently, statistical chart generators have emerged, in-
cluding White (2004) for CCG, Carroll and Oepen
(2005) and Nakanishi et al (2005) for HPSG. Many
of these systems, however, focus on the task of sur-
face realization?inflecting and ordering words?
which ignores the problem of lexical selection. In
contrast, our SMT-based methods integrate lexical
selection and realization in an elegant framework
and automatically learn all of their linguistic knowl-
edge from an annotated corpus.
7 Conclusion
We have presented four tactical generation systems
based on various SMT-based methods. In particular,
the hybrid system produced by inverting the WASP
semantic parser shows the best overall results across
different application domains.
Acknowledgments
We would like to thank Kevin Knight, Jason
Baldridge, Razvan Bunescu, and the anonymous re-
viewers for their valuable comments. We also sin-
cerely thank the four annotators who helped us eval-
uate the SMT-based generators. This research was
supported by DARPA under grant HR0011-04-1-
0007 and a gift from Google Inc.
References
A. V. Aho and J. D. Ullman. 1972. The Theory of Parsing, Translation,
and Compiling. Prentice Hall, Englewood Cliffs, NJ.
S. Bangalore, O. Rambow, and S. Whittaker. 2000. Evaluation metrics
for generation. In Proc. INLG-00, pages 1?8, Mitzpe Ramon, Israel,
July.
A. Belz and E. Reiter. 2006. Comparing automatic and human evalu-
ation of NLG systems. In Proc. EACL-06, pages 313?320, Trento,
Italy, April.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and R. L. Mercer.
1993. The mathematics of statistical machine translation: Parameter
estimation. Computational Linguistics, 19(2):263?312, June.
J. Carroll and S. Oepen. 2005. High efficiency realization for a wide-
coverage unification grammar. In Proc. IJCNLP-05, pages 165?176,
Jeju Island, Korea, October.
J. Carroll, A. Copestake, D. Flickinger, and V. Poznan?ski. 1999. An
efficient chart generator for (semi-)lexicalist grammars. In Proc.
EWNLG-99, pages 86?95, Toulouse, France.
M. Chen et al 2003. Users manual: RoboCup soccer server man-
ual for soccer server version 7.07 and later. Available at http:
//sourceforge.net/projects/sserver/.
D. Chiang. 2005. A hierarchical phrase-based model for statistical
machine translation. In Proc. ACL-05, pages 263?270, Ann Arbor,
MI, June.
G. Doddington. 2002. Automatic evaluation of machine translation
quality using n-gram co-occurrence statistics. In Proc. ARPA Work-
shop on Human Language Technology, pages 128?132, San Diego,
CA.
U. Germann. 2003. Greedy decoding for statistical machine translation
in almost linear time. In Proc. HLT/NAACL-03, Edmonton, Canada.
P. S. Jacobs. 1985. PHRED: A generator for natural language inter-
faces. Computational Linguistics, 11(4):219?242.
R. J. Kate, Y. W. Wong, and R. J. Mooney. 2005. Learning to transform
natural to formal languages. In Proc. AAAI-05, pages 1062?1068,
Pittsburgh, PA, July.
M. Kay. 1975. Syntactic processing and functional sentence per-
spective. In Theoretical Issues in Natural Language Processing?
Supplement to the Proceedings, pages 12?15, Cambridge, MA,
June.
M. Kay. 1996. Chart generation. In Proc. ACL-96, pages 200?204, San
Francisco, CA.
K. Knight and V. Hatzivassiloglou. 1995. Two-level, many-paths gen-
eration. In Proc. ACL-95, pages 252?260, Cambridge, MA.
P. Koehn and C. Monz. 2006. Manual and automatic evaluation of
machine translation between European languages. In Proc. SMT-06
Workshop, pages 102?121, New York City, NY, June.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-based
translation. In Proc. HLT/NAACL-03, Edmonton, Canada.
G. Kuhlmann, P. Stone, R. J. Mooney, and J. W. Shavlik. 2004. Guiding
a reinforcement learner with natural language advice: Initial results
in RoboCup soccer. In Proc. of the AAAI-04 Workshop on Supervi-
sory Control of Learning and Adaptive Systems, San Jose, CA, July.
I. Langkilde-Geary. 2002. An empirical verification of coverage
and correctness for a general-purpose sentence generator. In Proc.
INLG-02, pages 17?24, Harriman, NY, July.
R. C. Moore. 2002. A complete, efficient sentence-realization algo-
rithm for unification grammar. In Proc. INLG-02, pages 41?48,
Harriman, NY, July.
H. Nakanishi, Y. Miyao, and J. Tsujii. 2005. Probabilistic models for
disambiguation of an HPSG-based chart generator. In Proc. IWPT-
05, pages 93?102, Vancouver, Canada, October.
J. Nocedal. 1980. Updating quasi-Newton matrices with limited stor-
age. Mathematics of Computation, 35(151):773?782, July.
F. J. Och and H. Ney. 2003. A systematic comparison of various statis-
tical alignment models. Computational Linguistics, 29(1):19?51.
F. J. Och. 2003. Minimum error rate training in statistical machine
translation. In Proc. ACL-03, pages 160?167, Sapporo, Japan, July.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002. BLEU: a
method for automatic evaluation of machine translation. In Proc.
ACL-02, pages 311?318, Philadelphia, PA, July.
S. M. Shieber. 1988. A uniform architecture for parsing and generation.
In Proc. COLING-88, pages 614?619, Budapest, Hungary.
S. M. Shieber. 1993. The problem of logical-form equivalence. Com-
putational Linguistics, 19(1):179?190.
A. Stolcke. 2002. SRILM?an extensible language modeling toolkit.
In Proc. ICSLP-02, pages 901?904, Denver, CO.
M. White and J. Baldridge. 2003. Adapting chart realization to CCG.
In Proc. EWNLG-03, Budapest, Hungary, April.
M. White. 2004. Reining in CCG chart realization. In Proc. INLG-04,
New Forest, UK, July.
P. Whitelock. 1992. Shake-and-bake translation. In Proc. COLING-92,
pages 784?791, Nantes, France.
Y. W. Wong and R. J. Mooney. 2006. Learning for semantic parsing
with statistical machine translation. In Proc. HLT/NAACL-06, pages
439?446, New York City, NY, June.
179
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 960?967,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Learning Synchronous Grammars for Semantic Parsing with
Lambda Calculus
Yuk Wah Wong and Raymond J. Mooney
Department of Computer Sciences
The University of Texas at Austin
{ywwong,mooney}@cs.utexas.edu
Abstract
This paper presents the first empirical results
to our knowledge on learning synchronous
grammars that generate logical forms. Using
statistical machine translation techniques, a
semantic parser based on a synchronous
context-free grammar augmented with ?-
operators is learned given a set of training
sentences and their correct logical forms.
The resulting parser is shown to be the best-
performing system so far in a database query
domain.
1 Introduction
Originally developed as a theory of compiling pro-
gramming languages (Aho and Ullman, 1972), syn-
chronous grammars have seen a surge of interest re-
cently in the statistical machine translation (SMT)
community as a way of formalizing syntax-based
translation models between natural languages (NL).
In generating multiple parse trees in a single deriva-
tion, synchronous grammars are ideal for model-
ing syntax-based translation because they describe
not only the hierarchical structures of a sentence
and its translation, but also the exact correspon-
dence between their sub-parts. Among the gram-
mar formalisms successfully put into use in syntax-
based SMT are synchronous context-free gram-
mars (SCFG) (Wu, 1997) and synchronous tree-
substitution grammars (STSG) (Yamada and Knight,
2001). Both formalisms have led to SMT sys-
tems whose performance is state-of-the-art (Chiang,
2005; Galley et al, 2006).
Synchronous grammars have also been used in
other NLP tasks, most notably semantic parsing,
which is the construction of a complete, formal
meaning representation (MR) of an NL sentence. In
our previous work (Wong and Mooney, 2006), se-
mantic parsing is cast as a machine translation task,
where an SCFG is used to model the translation
of an NL into a formal meaning-representation lan-
guage (MRL). Our algorithm, WASP, uses statistical
models developed for syntax-based SMT for lexical
learning and parse disambiguation. The result is a
robust semantic parser that gives good performance
in various domains. More recently, we show that
our SCFG-based parser can be inverted to produce a
state-of-the-art NL generator, where a formal MRL
is translated into an NL (Wong and Mooney, 2007).
Currently, the use of learned synchronous gram-
mars in semantic parsing and NL generation is lim-
ited to simple MRLs that are free of logical vari-
ables. This is because grammar formalisms such as
SCFG do not have a principled mechanism for han-
dling logical variables. This is unfortunate because
most existing work on computational semantics is
based on predicate logic, where logical variables
play an important role (Blackburn and Bos, 2005).
For some domains, this problem can be avoided by
transforming a logical language into a variable-free,
functional language (e.g. the GEOQUERY functional
query language inWong andMooney (2006)). How-
ever, development of such a functional language is
non-trivial, and as we will see, logical languages can
be more appropriate for certain domains.
On the other hand, most existing methods for
mapping NL sentences to logical forms involve sub-
stantial hand-written components that are difficult
to maintain (Joshi and Vijay-Shanker, 2001; Bayer
et al, 2004; Bos, 2005). Zettlemoyer and Collins
(2005) present a statistical method that is consider-
960
ably more robust, but it still relies on hand-written
rules for lexical acquisition, which can create a per-
formance bottleneck.
In this work, we show that methods developed for
SMT can be brought to bear on tasks where logical
forms are involved, such as semantic parsing. In par-
ticular, we extend the WASP semantic parsing algo-
rithm by adding variable-binding ?-operators to the
underlying SCFG. The resulting synchronous gram-
mar generates logical forms using ?-calculus (Mon-
tague, 1970). A semantic parser is learned given a
set of sentences and their correct logical forms us-
ing SMT methods. The new algorithm is called ?-
WASP, and is shown to be the best-performing sys-
tem so far in the GEOQUERY domain.
2 Test Domain
In this work, we mainly consider the GEOQUERY
domain, where a query language based on Prolog is
used to query a database on U.S. geography (Zelle
and Mooney, 1996). The query language consists
of logical forms augmented with meta-predicates
for concepts such as smallest and count. Figure 1
shows two sample logical forms and their English
glosses. Throughout this paper, we use the notation
x1, x2, . . . for logical variables.
Although Prolog logical forms are the main focus
of this paper, our algorithm makes minimal assump-
tions about the target MRL. The only restriction on
the MRL is that it be defined by an unambiguous
context-free grammar (CFG) that divides a logical
form into subformulas (and terms into subterms).
Figure 2(a) shows a sample parse tree of a logical
form, where each CFG production corresponds to a
subformula.
3 The Semantic Parsing Algorithm
Our work is based on the WASP semantic parsing al-
gorithm (Wong andMooney, 2006), which translates
NL sentences into MRs using an SCFG. In WASP,
each SCFG production has the following form:
A ? ??, ?? (1)
where ? is an NL phrase and ? is the MR translation
of ?. Both ? and ? are strings of terminal and non-
terminal symbols. Each non-terminal in ? appears
in ? exactly once. We use indices to show the cor-
respondence between non-terminals in ? and ?. All
derivations start with a pair of co-indexed start sym-
bols, ?S 1 , S 1 ?. Each step of a derivation involves
the rewriting of a pair of co-indexed non-terminals
by the same SCFG production. The yield of a deriva-
tion is a pair of terminal strings, ?e, f?, where e is
an NL sentence and f is the MR translation of e.
For convenience, we call an SCFG production a rule
throughout this paper.
While WASP works well for target MRLs that
are free of logical variables such as CLANG (Wong
and Mooney, 2006), it cannot easily handle various
kinds of logical forms used in computational seman-
tics, such as predicate logic. The problem is that
WASP lacks a principled mechanism for handling
logical variables. In this work, we extend the WASP
algorithm by adding a variable-binding mechanism
based on ?-calculus, which allows for compositional
semantics for logical forms.
This work is based on an extended version of
SCFG, which we call ?-SCFG, where each rule has
the following form:
A ? ??, ?x1 . . . ?xk.?? (2)
where ? is an NL phrase and ? is the MR trans-
lation of ?. Unlike (1), ? is a string of termi-
nals, non-terminals, and logical variables. The
variable-binding operator ? binds occurrences of
the logical variables x1, . . . , xk in ?, which makes
?x1 . . . ?xk.? a ?-function of arity k. When ap-
plied to a list of arguments, (xi1 , . . . , xik), the ?-
function gives ??, where ? is a substitution oper-
ator, {x1/xi1 , . . . , xk/xik}, that replaces all bound
occurrences of xj in ? with xij . If any of the ar-
guments xij appear in ? as a free variable (i.e. not
bound by any ?), then those free variables in ? must
be renamed before function application takes place.
Each non-terminal Aj in ? is followed by a list
of arguments, xj = (xj1 , . . . , xjkj ). During pars-
ing, Aj must be rewritten by a ?-function fj of ar-
ity kj . Like SCFG, a derivation starts with a pair
of co-indexed start symbols and ends when all non-
terminals have been rewritten. To compute the yield
of a derivation, each fj is applied to its correspond-
ing arguments xj to obtain an MR string free of ?-
operators with logical variables properly named.
961
(a) answer(x1,smallest(x2,(state(x1),area(x1,x2))))
What is the smallest state by area?
(b) answer(x1,count(x2,(city(x2),major(x2),loc(x2,x3),next to(x3,x4),state(x3),
equal(x4,stateid(texas)))))
How many major cities are in states bordering Texas?
Figure 1: Sample logical forms in the GEOQUERY domain and their English glosses.
(a)
smallest(x2,(FORM,FORM))
QUERY
answer(x1,FORM)
area(x1,x2)state(x1)
(b)
?x1.smallest(x2,(FORM(x1),FORM(x1, x2)))
QUERY
answer(x1,FORM(x1))
?x1.state(x1) ?x1.?x2.area(x1,x2)
Figure 2: Parse trees of the logical form in Figure 1(a).
As a concrete example, Figure 2(b) shows an
MR parse tree that corresponds to the English
parse, [What is the [smallest [state] [by area]]],
based on the ?-SCFG rules in Figure 3. To
compute the yield of this MR parse tree, we start
from the leaf nodes: apply ?x1.state(x1) to
the argument (x1), and ?x1.?x2.area(x1,x2)
to the arguments (x1, x2). This results in two
MR strings: state(x1) and area(x1,x2).
Substituting these MR strings for the FORM non-
terminals in the parent node gives the ?-function
?x1.smallest(x2,(state(x1),area(x1,x2))).
Applying this ?-function to (x1) gives the MR
string smallest(x2,(state(x1),area(x1,x2))).
Substituting this MR string for the FORM non-
terminal in the grandparent node in turn gives the
logical form in Figure 1(a). This is the yield of the
MR parse tree, since the root node of the parse tree
is reached.
3.1 Lexical Acquisition
Given a set of training sentences paired with their
correct logical forms, {?ei, fi?}, the main learning
task is to find a ?-SCFG, G, that covers the train-
ing data. Like most existing work on syntax-based
SMT (Chiang, 2005; Galley et al, 2006), we con-
structG using rules extracted fromword alignments.
We use the K = 5 most probable word alignments
for the training set given by GIZA++ (Och and Ney,
2003), with variable names ignored to reduce spar-
sity. Rules are then extracted from each word align-
ment as follows.
To ground our discussion, we use the word align-
ment in Figure 4 as an example. To represent
the logical form in Figure 4, we use its linearized
parse?a list of MRL productions that generate the
logical form, in top-down, left-most order (cf. Fig-
ure 2(a)). Since the MRL grammar is unambiguous,
every logical form has a unique linearized parse. We
assume the alignment to be n-to-1, where each word
is linked to at most one MRL production.
Rules are extracted in a bottom-up manner, start-
ing with MRL productions at the leaves of the
MR parse tree, e.g. FORM ? state(x1) in Fig-
ure 2(a). Given an MRL production, A ? ?, a
ruleA ? ??, ?xi1 . . . ?xik .?? is extracted such that:
(1) ? is the NL phrase linked to the MRL produc-
tion; (2) xi1 , . . . , xik are the logical variables that
appear in ? and outside the current leaf node in the
MR parse tree. If xi1 , . . . , xik were not bound by
?, they would become free variables in ?, subject to
renaming during function application (and therefore,
invisible to the rest of the logical form). For exam-
ple, since x1 is an argument of the state predicate
as well as answer and area, x1 must be bound
(cf. the corresponding tree node in Figure 2(b)). The
rule extracted for the state predicate is shown in
Figure 3.
The case for the internal nodes of the MR parse
tree is similar. Given an MRL production, A ? ?,
where ? contains non-terminals A1, . . . , An, a rule
A ? ??, ?xi1 . . . ?xik .??? is extracted such that: (1)
? is the NL phrase linked to the MRL production,
with non-terminals A1, . . . , An showing the posi-
tions of the argument strings; (2) ?? is ? with each
non-terminal Aj replaced with Aj(xj1 , . . . , xjkj ),
where xj1 , . . . , xjkj are the bound variables in the
?-function used to rewrite Aj ; (3) xi1 , . . . , xik are
the logical variables that appear in ?? and outside
the current MR sub-parse. For example, see the rule
962
FORM ? ?state , ?x1.state(x1)?
FORM ? ?by area , ?x1.?x2.area(x1,x2)?
FORM ? ?smallest FORM 1 FORM 2 , ?x1.smallest(x2,(FORM 1 (x1),FORM 2 (x1, x2)))?
QUERY ? ?what is (1) FORM 1 , answer(x1,FORM 1 (x1))?
Figure 3: ?-SCFG rules for parsing the English sentence in Figure 1(a).
smallest
the
is
what
state
by
area
QUERY ? answer(x1,FORM)
FORM ? smallest(x2,(FORM,FORM))
FORM ? state(x1)
FORM ? area(x1,x2)
Figure 4: Word alignment for the sentence pair in Figure 1(a).
extracted for the smallest predicate in Figure 3,
where x2 is an argument of smallest, but it does
not appear outside the formula smallest(...),
so x2 need not be bound by ?. On the other
hand, x1 appears in ??, and it appears outside
smallest(...) (as an argument of answer),
so x1 must be bound.
Rule extraction continues in this manner until the
root of the MR parse tree is reached. Figure 3 shows
all the rules extracted from Figure 4.1
3.2 Probabilistic Semantic Parsing Model
Since the learned ?-SCFG can be ambiguous, a
probabilistic model is needed for parse disambigua-
tion. We use the maximum-entropy model proposed
in Wong and Mooney (2006), which defines a condi-
tional probability distribution over derivations given
an observed NL sentence. The output MR is the
yield of the most probable derivation according to
this model.
Parameter estimation involves maximizing the
conditional log-likelihood of the training set. For
each rule, r, there is a feature that returns the num-
ber of times r is used in a derivation. More features
will be introduced in Section 5.
4 Promoting NL/MRL Isomorphism
We have described the ?-WASP algorithm which
generates logical forms based on ?-calculus. While
reasonably effective, it can be improved in several
ways. In this section, we focus on improving lexical
acquisition.
1For details regarding non-isomorphic NL/MR parse trees,
removal of bad links from alignments, and extraction of word
gaps (e.g. the token (1) in the last rule of Figure 3), see Wong
and Mooney (2006).
To see why the current lexical acquisition algo-
rithm can be problematic, consider the word align-
ment in Figure 5 (for the sentence pair in Fig-
ure 1(b)). No rules can be extracted for the state
predicate, because the shortest NL substring that
covers the word states and the argument string
Texas, i.e. states bordering Texas, contains the word
bordering, which is linked to an MRL production
outside the MR sub-parse rooted at state. Rule
extraction is forbidden in this case because it would
destroy the link between bordering and next to.
In other words, the NL and MR parse trees are not
isomorphic.
This problem can be ameliorated by transforming
the logical form of each training sentence so that
the NL and MR parse trees are maximally isomor-
phic. This is possible because some of the opera-
tors used in the logical forms, notably the conjunc-
tion operator (,), are both associative (a,(b,c)
= (a,b),c = a,b,c) and commutative (a,b =
b,a). Hence, conjuncts can be reordered and re-
grouped without changing the meaning of a conjunc-
tion. For example, rule extraction would be pos-
sible if the positions of the next to and state
conjuncts were switched. We present a method for
regrouping conjuncts to promote isomorphism be-
tween NL and MR parse trees.2 Given a conjunc-
tion, it does the following: (See Figure 6 for the
pseudocode, and Figure 5 for an illustration.)
Step 1. Identify the MRL productions that corre-
spond to the conjuncts and the meta-predicate that
takes the conjunction as an argument (count in
Figure 5), and figure them as vertices in an undi-
2This method also applies to any operators that are associa-
tive and commutative, e.g. disjunction. For concreteness, how-
ever, we use conjunction as an example.
963
QUERY ? answer(x1,FORM)howmany
major
cities
are
in
states
bordering
texas
FORM ? count(x2,(CONJ),x1)
CONJ ? city(x2),CONJ
CONJ ? major(x2),CONJ
CONJ ? loc(x2,x3),CONJ
CONJ ? next to(x3,x4),CONJ
CONJ ? state(x3),FORM
FORM ? equal(x4,stateid(texas))
O
rig
in
al
M
R
pa
rs
e
x2
x3
x4
how many
cities
in
states
bordering
texas
major
QUERY
answer(x1,FORM)
count(x2,(CONJ),x1)
major(x2),CONJ
city(x2),CONJ
loc(x2,x3),CONJ
state(x3),CONJ
next to(x3,x4),FORM
equal(x4,stateid(texas))
QUERY
answer(x1,FORM)
count(x2,(CONJ),x1)
major(x2),CONJ
city(x2),CONJ
loc(x2,x3),CONJ
equal(x4,stateid(texas))
next to(x3,x4),CONJ
state(x3),FORM
(sh
o
w
n
a
bo
ve
a
s
th
ic
k
ed
ge
s)
St
ep
5.
Fi
n
d
M
ST
Step 4. Assign edge weights
Step 6.
Construct MR parse
F
o
rm
g
raph
Step
s1
?3
.
Figure 5: Transforming the logical form in Figure 1(b). The step numbers correspond to those in Figure 6.
Input: A conjunction, c, of n conjuncts; MRL productions, p1, . . . , pn, that correspond to each conjunct; an MRL production,
p0, that corresponds to the meta-predicate taking c as an argument; an NL sentence, e; a word alignment, a.
Let v(p) be the set of logical variables that appear in p. Create an undirected graph, ?, with vertices V = {pi|i = 0, . . . , n}1
and edges E = {(pi, pj)|i < j,v(pi) ? v(pj) 6= ?}.
Let e(p) be the set of words in e to which p is linked according to a. Let span(pi, pj) be the shortest substring of e that2
includes e(pi) ? e(pj). Subtract {(pi, pj)|i 6= 0, span(pi, pj) ? e(p0) 6= ?} from E.
Add edges (p0, pi) to E if pi is not already connected to p0.3
For each edge (pi, pj) in E, set edge weight to the minimum word distance between e(pi) and e(pj).4
Find a minimum spanning tree, T , for ? using Kruskal?s algorithm.5
Using p0 as the root, construct a conjunction c? based on T , and then replace c with c?.6
Figure 6: Algorithm for regrouping conjuncts to promote isomorphism between NL and MR parse trees.
rected graph, ?. An edge (pi, pj) is in ? if and only
if pi and pj contain occurrences of the same logical
variables. Each edge in ? indicates a possible edge
in the transformed MR parse tree. Intuitively, two
concepts are closely related if they involve the same
logical variables, and therefore, should be placed
close together in the MR parse tree. By keeping oc-
currences of a logical variable in close proximity in
the MR parse tree, we also avoid unnecessary vari-
able bindings in the extracted rules.
Step 2. Remove edges from ? whose inclusion in
the MR parse tree would prevent the NL and MR
parse trees from being isomorphic.
Step 3. Add edges to ? to make sure that a spanning
tree for ? exists.
Steps 4?6. Assign edge weights based on word dis-
tance, find a minimum spanning tree, T , for ?, then
regroup the conjuncts based on T . The choice of T
reflects the intuition that words that occur close to-
gether in a sentence tend to be semantically related.
This procedure is repeated for all conjunctions
that appear in a logical form. Rules are then ex-
tracted from the same input alignment used to re-
group conjuncts. Of course, the regrouping of con-
juncts requires a good alignment to begin with, and
that requires a reasonable ordering of conjuncts in
the training data, since the alignment model is sen-
sitive to word order. This suggests an iterative algo-
rithm in which a better grouping of conjuncts leads
to a better alignment model, which guides further re-
grouping until convergence. We did not pursue this,
as it is not needed in our experiments so far.
964
(a) answer(x1,largest(x2,(state(x1),major(x1),river(x1),traverse(x1,x2))))
What is the entity that is a state and also a major river, that traverses something that is the largest?
(b) answer(x1,smallest(x2,(highest(x1,(point(x1),loc(x1,x3),state(x3))),density(x1,x2))))
Among the highest points of all states, which one has the lowest population density?
(c) answer(x1,equal(x1,stateid(alaska)))
Alaska?
(d) answer(x1,largest(x2,(largest(x1,(state(x1),next to(x1,x3),state(x3))),population(x1,x2))))
Among the largest state that borders some other state, which is the one with the largest population?
Figure 7: Typical errors made by the ?-WASP parser, along with their English interpretations, before any
language modeling for the target MRL was done.
5 Modeling the Target MRL
In this section, we propose two methods for model-
ing the target MRL. This is motivated by the fact that
many of the errors made by the ?-WASP parser can
be detected by inspecting the MR translations alone.
Figure 7 shows some typical errors, which can be
classified into two broad categories:
1. Type mismatch errors. For example, a state can-
not possibly be a river (Figure 7(a)). Also it is
awkward to talk about the population density of a
state?s highest point (Figure 7(b)).
2. Errors that do not involve type mismatch. For ex-
ample, a query can be overly trivial (Figure 7(c)),
or involve aggregate functions on a known single-
ton (Figure 7(d)).
The first type of errors can be fixed by type check-
ing. Each m-place predicate is associated with a list
ofm-tuples showing all valid combinations of entity
types that the m arguments can refer to:
point( ): {(POINT)}
density( , ):
{(COUNTRY, NUM), (STATE, NUM), (CITY, NUM)}
These m-tuples of entity types are given as do-
main knowledge. The parser maintains a set of
possible entity types for each logical variable in-
troduced in a partial derivation (except those that
are no longer visible). If there is a logical vari-
able that cannot refer to any types of entities
(i.e. the set of entity types is empty), then the par-
tial derivation is considered invalid. For exam-
ple, based on the tuples shown above, point(x1)
and density(x1, ) cannot be both true, because
{POINT} ? {COUNTRY, STATE, CITY} = ?. The
use of type checking is to exploit the fact that peo-
ple tend not to ask questions that obviously have no
valid answers (Grice, 1975). It is also similar to
Schuler?s (2003) use of model-theoretic interpreta-
tions to guide syntactic parsing.
Errors that do not involve type mismatch are
handled by adding new features to the maximum-
entropy model (Section 3.2). We only consider fea-
tures that are based on the MR translations, and
therefore, these features can be seen as an implicit
language model of the target MRL (Papineni et al,
1997). Of the many features that we have tried,
one feature set stands out as being the most effec-
tive, the two-level rules in Collins and Koo (2005),
which give the number of times a given rule is used
to expand a non-terminal in a given parent rule.
We use only the MRL part of the rules. For ex-
ample, a negative weight for the combination of
QUERY ? answer(x1,FORM(x1)) and FORM
? ?x1.equal(x1, ) would discourage any parse
that yields Figure 7(c). The two-level rules features,
along with the features described in Section 3.2, are
used in the final version of ?-WASP.
6 Experiments
We evaluated the ?-WASP algorithm in the GEO-
QUERY domain. The larger GEOQUERY corpus con-
sists of 880 English questions gathered from various
sources (Wong and Mooney, 2006). The questions
were manually translated into Prolog logical forms.
The average length of a sentence is 7.57 words.
We performed a single run of 10-fold cross
validation, and measured the performance of the
learned parsers using precision (percentage of trans-
lations that were correct), recall (percentage of test
sentences that were correctly translated), and F-
measure (harmonic mean of precision and recall).
A translation is considered correct if it retrieves the
same answer as the correct logical form.
Figure 8 shows the learning curves for the ?-
965
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  100  200  300  400  500  600  700  800  900
Pr
ec
is
io
n 
(%
)
Number of training examples
lambda-WASP
WASP
SCISSOR
Z&C
(a) Precision
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  100  200  300  400  500  600  700  800  900
R
ec
al
l (%
)
Number of training examples
lambda-WASP
WASP
SCISSOR
Z&C
(b) Recall
Figure 8: Learning curves for various parsing algorithms on the larger GEOQUERY corpus.
(%) ?-WASP WASP SCISSOR Z&C
Precision 91.95 87.19 92.08 96.25
Recall 86.59 74.77 72.27 79.29
F-measure 89.19 80.50 80.98 86.95
Table 1: Performance of various parsing algorithms on the larger GEOQUERY corpus.
WASP algorithm compared to: (1) the original
WASP algorithm which uses a functional query lan-
guage (FunQL); (2) SCISSOR (Ge and Mooney,
2005), a fully-supervised, combined syntactic-
semantic parsing algorithm which also uses FunQL;
and (3) Zettlemoyer and Collins (2005) (Z&C), a
CCG-based algorithm which uses Prolog logical
forms. Table 1 summarizes the results at the end
of the learning curves (792 training examples for ?-
WASP, WASP and SCISSOR, 600 for Z&C).
A few observations can be made. First, algorithms
that use Prolog logical forms as the target MRL gen-
erally show better recall than those using FunQL. In
particular, ?-WASP has the best recall by far. One
reason is that it allows lexical items to be combined
in ways not allowed by FunQL or the hand-written
templates in Z&C, e.g. [smallest [state] [by area]]
in Figure 3. Second, Z&C has the best precision, al-
though their results are based on 280 test examples
only, whereas our results are based on 10-fold cross
validation. Third, ?-WASP has the best F-measure.
To see the relative importance of each component
of the ?-WASP algorithm, we performed two abla-
tion studies. First, we compared the performance
of ?-WASP with and without conjunct regrouping
(Section 4). Second, we compared the performance
of ?-WASP with and without language modeling for
the MRL (Section 5). Table 2 shows the results.
It is found that conjunct regrouping improves recall
(p < 0.01 based on the paired t-test), and the use of
two-level rules in the maximum-entropy model im-
proves precision and recall (p < 0.05). Type check-
ing also significantly improves precision and recall.
A major advantage of ?-WASP over SCISSOR and
Z&C is that it does not require any prior knowl-
edge of the NL syntax. Figure 9 shows the perfor-
mance of ?-WASP on the multilingual GEOQUERY
data set. The 250-example data set is a subset of the
larger GEOQUERY corpus. All English questions in
this data set were manually translated into Spanish,
Japanese and Turkish, while the corresponding Pro-
log queries remain unchanged. Figure 9 shows that
?-WASP performed comparably for all NLs. In con-
trast, SCISSOR cannot be used directly on the non-
English data, because syntactic annotations are only
available in English. Z&C cannot be used directly
either, because it requires NL-specific templates for
building CCG grammars.
7 Conclusions
We have presented ?-WASP, a semantic parsing al-
gorithm based on a ?-SCFG that generates logical
forms using ?-calculus. A semantic parser is learned
given a set of training sentences and their correct
logical forms using standard SMT techniques. The
result is a robust semantic parser for predicate logic,
and it is the best-performing system so far in the
GEOQUERY domain.
This work shows that it is possible to use standard
SMT methods in tasks where logical forms are in-
volved. For example, it should be straightforward
to adapt ?-WASP to the NL generation task?all
one needs is a decoder that can handle input logical
forms. Other tasks that can potentially benefit from
966
(%) Precision Recall
?-WASP 91.95 86.59
w/o conj. regrouping 90.73 83.07
(%) Precision Recall
?-WASP 91.95 86.59
w/o two-level rules 88.46 84.32
and w/o type checking 65.45 63.18
Table 2: Performance of ?-WASP with certain components of the algorithm removed.
 0
 20
 40
 60
 80
 100
 0  50  100  150  200  250
Pr
ec
is
io
n 
(%
)
Number of training examples
English
Spanish
Japanese
Turkish
(a) Precision
 0
 20
 40
 60
 80
 100
 0  50  100  150  200  250
R
ec
al
l (%
)
Number of training examples
English
Spanish
Japanese
Turkish
(b) Recall
Figure 9: Learning curves for ?-WASP on the multilingual GEOQUERY data set.
this include question answering and interlingual MT.
In future work, we plan to further generalize the
synchronous parsing framework to allow different
combinations of grammar formalisms. For exam-
ple, to handle long-distance dependencies that occur
in open-domain text, CCG and TAG would be more
appropriate than CFG. Certain applications may re-
quire different meaning representations, e.g. frame
semantics.
Acknowledgments: We thank Rohit Kate, Raz-
van Bunescu and the anonymous reviewers for their
valuable comments. This work was supported by a
gift from Google Inc.
References
A. V. Aho and J. D. Ullman. 1972. The Theory of Pars-
ing, Translation, and Compiling. Prentice Hall, Englewood
Cliffs, NJ.
S. Bayer, J. Burger, W. Greiff, and B. Wellner. 2004.
The MITRE logical form generation system. In Proc. of
Senseval-3, Barcelona, Spain, July.
P. Blackburn and J. Bos. 2005. Representation and Inference
for Natural Language: A First Course in Computational Se-
mantics. CSLI Publications, Stanford, CA.
J. Bos. 2005. Towards wide-coverage semantic interpretation.
In Proc. of IWCS-05, Tilburg, The Netherlands, January.
D. Chiang. 2005. A hierarchical phrase-based model for sta-
tistical machine translation. In Proc. of ACL-05, pages 263?
270, Ann Arbor, MI, June.
M. Collins and T. Koo. 2005. Discriminative reranking
for natural language parsing. Computational Linguistics,
31(1):25?69.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,
W. Wang, and I. Thayer. 2006. Scalable inference and train-
ing of context-rich syntactic translation models. In Proc. of
COLING/ACL-06, pages 961?968, Sydney, Australia, July.
R. Ge and R. J. Mooney. 2005. A statistical semantic parser
that integrates syntax and semantics. In Proc. of CoNLL-05,
pages 9?16, Ann Arbor, MI, July.
H. P. Grice. 1975. Logic and conversation. In P. Cole and
J. Morgan, eds., Syntax and Semantics 3: Speech Acts, pages
41?58. Academic Press, New York.
A. K. Joshi and K. Vijay-Shanker. 2001. Compositional se-
mantics with lexicalized tree-adjoining grammar (LTAG):
How much underspecification is necessary? In H. Bunt et
al., eds., Computing Meaning, volume 2, pages 147?163.
Kluwer Academic Publishers, Dordrecht, The Netherlands.
R. Montague. 1970. Universal grammar. Theoria, 36:373?398.
F. J. Och and H. Ney. 2003. A systematic comparison of vari-
ous statistical alignment models. Computational Linguistics,
29(1):19?51.
K. A. Papineni, S. Roukos, and R. T. Ward. 1997. Feature-
based language understanding. In Proc. of EuroSpeech-97,
pages 1435?1438, Rhodes, Greece.
W. Schuler. 2003. Using model-theoretic semantic interpre-
tation to guide statistical parsing and word recognition in a
spoken language interface. In Proc. of ACL-03, pages 529?
536.
Y. W. Wong and R. J. Mooney. 2006. Learning for seman-
tic parsing with statistical machine translation. In Proc. of
HLT/NAACL-06, pages 439?446, New York City, NY.
Y. W. Wong and R. J. Mooney. 2007. Generation by inverting
a semantic parser that uses statistical machine translation. In
Proc. of NAACL/HLT-07, Rochester, NY, to appear.
D. Wu. 1997. Stochastic inversion transduction grammars and
bilingual parsing of parallel corpora. Computational Lin-
guistics, 23(3):377?403.
K. Yamada and K. Knight. 2001. A syntax-based statisti-
cal translation model. In Proc. of ACL-01, pages 523?530,
Toulouse, France.
J. M. Zelle and R. J. Mooney. 1996. Learning to parse database
queries using inductive logic programming. In Proc. of
AAAI-96, pages 1050?1055, Portland, OR, August.
L. S. Zettlemoyer and M. Collins. 2005. Learning to map sen-
tences to logical form: Structured classification with proba-
bilistic categorial grammars. In Proc. of UAI-05, Edinburgh,
Scotland, July.
967
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 185?188,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Maximum Entropy Word Segmentation of Chinese Text
Aaron J. Jacobs
Department of Linguistics
University of Texas at Austin
1 University Station B5100
Austin, TX 78712-0198 USA
aaronjacobs@mail.utexas.edu
Yuk Wah Wong
Department of Computer Sciences
University of Texas at Austin
1 University Station C0500
Austin, TX 78712-0233 USA
ywwong@cs.utexas.edu
Abstract
We extended the work of Low, Ng, and
Guo (2005) to create a Chinese word seg-
mentation system based upon a maximum
entropy statistical model. This system
was entered into the Third International
Chinese Language Processing Bakeoff and
evaluated on all four corpora in their re-
spective open tracks. Our system achieved
the highest F-score for the UPUC corpus,
and the second, third, and seventh high-
est for CKIP, CITYU, and MSRA respec-
tively. Later testing with the gold-standard
data revealed that while the additions we
made to Low et al?s system helped our re-
sults for the 2005 data with which we ex-
perimented during development, a number
of them actually hurt our scores for this
year?s corpora.
1 Segmenter
Our Chinese word segmenter is a modification of
the system described by Low et al (2005), which
they entered in the 2005 Second International Chi-
nese Word Segmentation Bakeoff. It uses a max-
imum entropy (Ratnaparkhi, 1998) model which
is trained on the training corpora provided for this
year?s bakeoff. The maximum entropy framework
used is the Python interface of Zhang Le?s maxi-
mum entropy modeling toolkit (Zhang, 2004).
1.1 Properties in common with Low et al
As with the system of Low et al, our system
treats the word segmentation problem as a tag-
ging problem. When segmenting a string of Chi-
nese text, each character can be assigned one of
four boundary tags: S for a character that stands
alone as a word, B for a character that begins a
multi-character word, M for a character in a multi-
character word which neither starts nor ends the
word, and E for a character that ends a multi-
character word. The optimal tag for a given char-
acter is chosen based on features derived from
the character?s surrounding context in accordance
with the decoding algorithm (see Section 1.2).
All of the feature templates of Low et al?s sys-
tem are utilized in our own (with a few slight mod-
ifications):
1. Cn (n = ?2,?1, 0, 1, 2)
2. CnCn+1 (n = ?2,?1, 0, 1)
3. C?1C1
4. Pu(C0)
5. T (C?2)T (C?1)T (C0)T (C1)T (C2)
6. Lt0
7. Cnt0 (n = ?1, 0, 1)
In the above feature templates, Ci refers to the
character i positions away from the character un-
der consideration, where negative values indicate
characters to the left of the present position. The
punctuation feature Pu(C0) is added only if the
current character is a punctuation mark, and the
function T maps characters to various numbers
representing classes of characters. In addition to
the numeral, date word, and English letter classes
of Low et al?s system, we added classes for punc-
tuation and likely decimal points (which are de-
fined by a period or the character ? occurring
between two numerals). L is defined to be the
length of the longest wordW in the dictionary that
matches some sequence of characters around C0
185
in the current context and t0 is the boundary tag of
C0 inW . The dictionary features are derived from
the use of the same online dictionary from Peking
University that was used by Low et al
In order to improve out-of-vocabulary (OOV)
recall rates, we followed the same procedure as
Low et al?s system in using the other three train-
ing corpora as additional training material when
training a model for a particular corpus:
1. Train a model normally using the given cor-
pus.
2. Use the resulting model to segment the other
training corpora from this year?s bakeoff, ig-
noring the pre-existing segmentation.
3. Let C be a character in one of the other cor-
pora D. If C is assigned a tag t by the model
with probability p, t is equivalent to the tag
assigned by the actual training corpus D, and
p is less than 0.8, then add C (along with
its associated features) as additional training
material.
4. Train a new model using all of the original
training data along with the new data derived
from the other corpora as described in the
previous step.
This procedure was carried out when training
models for all of the corpora except CKIP. The
model for that corpus was trained solely with its
own training data due to time and memory con-
cerns as well as the fact that our scores during de-
velopment for the corresponding corpus (AS) in
2005 did not seem to benefit from the addition of
data from the other corpora.
We adopt the same post-processing step as Low
et al?s system: after segmenting a body of text,
any sequence of 2 to 6 words whose total length
is at least 3 characters and whose concatenation
is found as a single word elsewhere in the seg-
menter?s output is joined together into that sin-
gle word. Empirical testing showed that this pro-
cess was actually detrimental to results in the 2005
CITYU data, so it was performed only for the
UPUC, MSRA, and CKIP corpora.
1.2 Decoding algorithm
When segmenting text, to efficiently compute the
most likely tag sequence our system uses the
Viterbi algorithm (Viterbi, 1967). Only legal tag
sequences are considered. This is accomplished
by ignoring illegal state transitions (e.g. from a B
tag to an S tag) during decoding. At each stage
the likelihood of the current path is estimated by
multiplying the likelihood of the path which it ex-
tends with the probability given by the model of
the assumed tag occurring given the surrounding
context and the current path. To keep the problem
tractable, only the 30 most likely paths are kept at
each stage.
The advantage of such an algorithm comes in
its ability to ?look ahead? compared to a simpler
algorithm which just chooses the most likely tag
at each step and goes on. Such an algorithm is
likely to run into situations where choosing the
most likely tag for one character forces the choice
of a very sub-optimal tag for a later character by
making impossible the choice of the best tag (e.g.
if S is the best choice but the tag assigned for the
previous character was B). In contrast, the Viterbi
algorithm entertains multiple possibilities for the
tagging of each character, allowing it to choose a
less likely tag now as a trade-off for a much more
likely tag later.
1.3 Other outcome-independent features
To the feature templates of Low et al?s system
described in Section 1.1, we added the following
three features which do not depend on previous
tagging decisions but only on the current charac-
ter?s context within the sentence:
1. The surname feature is set if the current
character is in our list of common surname
characters, as derived from the Peking Uni-
versity dictionary.
2. The redup-next feature is set if C1 is
equal to C0. This is to handle reduplication
within words, such as in the case of Z
Z ?particularly clear?.
3. The redup-prev feature is set if C?1 is
equal to C0.
These features were designed to give the system
hints in cases where we saw it make frequent er-
rors in the 2005 data.
1.4 Outcome-dependent features
In addition to the features previously discussed,
we added a number of features to our system that
are outcome-dependent in the sense that their re-
alization for a given character depends upon how
186
the previous characters were segmented. These
work in conjunction with the Viterbi algorithm
discussed in Section 1.2 to make it so that a given
character in a sentence can be assigned a different
set of features each time it is considered, depend-
ing on the path currently being extended.
1. If the current character is one of the place
characters such as Q or G which com-
monly occur at the end of a three-character
word and the length of the current word
(as determined by previous tagging decisions
on the current path) including the current
character is equal to three, then the feature
place-char-and-len-3 is set.
2. If the situation is as described above ex-
cept the next character in the current con-
text is the place character, then the feature
next-place-char-and-len-2 is set.
3. If the current character is I and the word
before the previous word is an enumerating
comma (), then the feature deng-list
is set. This is intended to capture situations
where a list of single-word items is presented,
followed byI to mean ?and so on?.
4. If the current character is I and the third
word back is an enumerating comma, then
the feature double-word-deng-list is
set.
5. If the length of the previous word is at least 2
and is equal to the length of the current word,
then the feature symmetry is set.
6. If the length of the previous word is at least 2
and is one more than the length of the current
word, then the feature almost-symmetry
is set.
7. Similar features are added if the length of the
current word is equal to (or one less than) the
length of the word before the last and the last
word is a comma.
These features were largely designed to help al-
leviate problems the model had with situations in
which it would otherwise be difficult to discern the
correct segmentation. For example, in one devel-
opment data set the model incorrectly groupedI
at the end of a list (which should be a word on its
own) with the following character to formI, a
word found in the dictionary.
1.5 Simplified normalization
To derive the most benefit from the additional
training data obtained as described in Section 1.1,
before generating any sort of features from char-
acters in training and test data, all characters are
normalized by the system to their simplified vari-
ants (if any) using data from version 4.1.0 of the
Unicode Standard. This is intended to improve the
utility of additional data from the traditional Chi-
nese corpora when training models for the sim-
plified corpora, and vice versa. Due to the re-
sults of some empirical testing, this normalization
was only performed when training models for the
UPUC and MSRA corpora; in our testing it did
not actually help with the scores for the traditional
Chinese corpora.
2 Results
Table 1 lists our official results for the bakeoff.
The columns show F scores, recall rates, precision
rates, and recall rates on out-of-vocabulary and
in-vocabulary words. Out of the participants in
the bakeoff whose scores were reported, our sys-
tem achieved the highest F score for UPUC, the
second-highest for CKIP, the seventh-highest for
MSRA, and the third-highest for CITYU.
Corpus F R P ROOV RIV
UPUC 0.944 0.949 0.939 0.768 0.966
CKIP 0.954 0.959 0.949 0.672 0.972
MSRA 0.960 0.959 0.961 0.711 0.968
CITYU 0.969 0.971 0.967 0.795 0.978
Table 1: Our 2006 SIGHAN bakeoff results.
The system?s F score forMSRAwas higher than
for UPUC or CKIP, but it did particularly poorly
compared to the rest of the contestants when one
considers how well it performed for the other cor-
pora. An analysis of the gold-standard files for
the MSRA test data show that out of all of the
corpora, MSRA had the highest percentage of
single-character words and the smallest percent-
age of two-character and three-character words.
Moreover, its proportion of words over 5 char-
acters in length was five times that of the other
corpora. Most of the errors our system made on
the MSRA test set involved incorrect groupings
of true single-character words. Another compar-
atively high proportion involved very long words,
especially names with internal syntactic structure
187
(e.g. -??Zi}?XTutorial Abstracts of ACL 2010, page 6,
Uppsala, Sweden, 11 July 2010. c?2010 Association for Computational Linguistics
Semantic Parsing: The Task, the State-of-the-Art and the Future
Rohit J. Kate
Department of Computer Science
The University of Texas at Austin
Austin, TX 78712, USA
rjkate@cs.utexas.edu
Yuk Wah Wong
Google Inc.
Pittsburgh, PA 15213, USA
ywwong@google.com
1 Introduction
Semantic parsing is the task of mapping natural
language sentences into complete formal mean-
ing representations which a computer can exe-
cute for some domain-specific application. This
is a challenging task and is critical for develop-
ing computing systems that can understand and
process natural language input, for example, a
computing system that answers natural language
queries about a database, or a robot that takes
commands in natural language. While the im-
portance of semantic parsing was realized a long
time ago, it is only in the past few years that the
state-of-the-art in semantic parsing has been sig-
nificantly advanced with more accurate and ro-
bust semantic parser learners that use a variety
of statistical learning methods. Semantic parsers
have also been extended to work beyond a single
sentence, for example, to use discourse contexts
and to learn domain-specific language from per-
ceptual contexts. Some of the future research di-
rections of semantic parsing with potentially large
impacts include mapping entire natural language
documents into machine processable form to en-
able automated reasoning about them and to con-
vert natural language web pages into machine pro-
cessable representations for the Semantic Web to
support automated high-end web applications.
This tutorial will introduce the semantic pars-
ing task and will bring the audience up-to-date
with the current research and state-of-the-art in se-
mantic parsing. It will also provide insights about
semantic parsing and how it relates to and dif-
fers from other natural language processing tasks.
It will point out research challenges and some
promising future directions for semantic parsing.
2 Content Overview
The proposed tutorial on semantic parsing will
start with an introduction to the task, giving ex-
amples of some application domains and meaning
representation languages. It will also point out its
distinctions from and relations to other NLP tasks.
Next, it will talk in depth about various semantic
parsers that have been built, starting with earlier
hand-built systems to the current state-of-the-art
statistical semantic parser learners. It will point
out the underlying commonalities and differences
between the learners. The next section of the tuto-
rial will talk about the recent advances in extend-
ing semantic parsing to work beyond parsing a sin-
gle sentence. Finally, the tutorial will point out
the current research challenges and some promis-
ing future directions for semantic parsing.
3 Outline
1. Introduction to the task of semantic parsing
(a) Definition of the task
(b) Examples of application domains and meaning
representation languages
(c) Distinctions from and relations to other NLP
tasks
2. Semantic parsers
(a) Earlier hand-built systems
(b) Learning for semantic parsing
i. Semantic parsing learning task
ii. Non-statistical semantic parser learners
iii. Statistical semantic parser learners
iv. Exploiting syntax for semantic parsing
v. Various forms of supervision: semi-
supervision, ambiguous supervision
(c) Underlying commonality and differences be-
tween different semantic parser learners
3. Semantic parsing beyond a sentence
(a) Using discourse contexts for semantic parsing
(b) Learning language from perceptual contexts
4. Research challenges and future directions
(a) Machine reading of documents: Connecting with
knowledge representation
(b) Applying semantic parsing techniques to the Se-
mantic Web
(c) Future research directions
5. Conclusions
6
