Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 442?449,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Improved Pronunciation Features for Construct-driven Assessment of
Non-native Spontaneous Speech
Lei Chen, Klaus Zechner, Xiaoming Xi
Educational Testing Service
Princeton, NJ, USA
{LChen,KZechner,XXi}@ets.org
Abstract
This paper describes research on automatic as-
sessment of the pronunciation quality of spon-
taneous non-native adult speech. Since the
speaking content is not known prior to the
assessment, a two-stage method is developed
to first recognize the speaking content based
on non-native speech acoustic properties and
then forced-align the recognition results with
a reference acoustic model reflecting native
and near-native speech properties. Features
related to Hidden Markov Model likelihoods
and vowel durations are extracted. Words with
low recognition confidence can be excluded
in the extraction of likelihood-related fea-
tures to minimize erroneous alignments due
to speech recognition errors. Our experiments
on the TOEFL R?Practice Online test, an En-
glish language assessment, suggest that the
recognition/forced-alignment method can pro-
vide useful pronunciation features. Our new
pronunciation features are more meaningful
than an utterance-based normalized acoustic
model score used in previous research from a
construct point of view.
1 Introduction
Automated systems for evaluating highly pre-
dictable speech (e.g. read speech or speech that
is quite constrained in the use of vocabulary and
syntactic structures) have emerged in the past
decade (Bernstein, 1999; Witt, 1999; Franco et al,
2000; Hacker et al, 2005) due to the growing matu-
rity of speech recognition and processing technolo-
gies. However, endeavors into automated scoring
for spontaneous speech have been sparse given the
challenge of both recognizing and assessing spon-
taneous speech. This paper addresses the develop-
ment and evaluation of pronunciation features for an
automated system for scoring spontaneous speech.
This system was deployed for the TOEFL R?Practice
Online (TPO) assessment used by prospective test
takers to prepare for the official TOEFL R?test.
A construct is a set of knowledge, skills, and abil-
ities measured by a test. The construct of the speak-
ing test is embodied in the rubrics that human raters
use to score the test. It consists of three key cat-
egories: delivery, language use, and topic devel-
opment. Delivery refers to the pace and the clar-
ity of the speech, including performance on into-
nation, rhythm, rate of speech, and degree of hesi-
tancy. Language use refers to the range, complex-
ity, and precision of vocabulary and grammar use.
Topic development refers to the coherence and full-
ness of the response. Most of the research on spon-
taneous speech assessment focuses on the delivery
aspect given the low recognition accuracy on non-
native spontaneous speech.
The delivery aspect can be measured on four di-
mensions: fluency, intonation, rhythm, and pronun-
ciation. For the TPO assessment, we have defined
pronunciation as the quality of vowels, consonants
and word-level stress (segmentals). Intonation and
sentence-level stress patterns (supra-segmentals) are
not defined as part of pronunciation. Pronuncia-
tion is one of the key factors that impact the intelli-
gibility and perceived comprehensibility of speech.
Because pronunciation plays an important role in
speech perception, features measuring pronuncia-
442
tion using speech technologies have been explored
in many previous studies. However, the bulk of the
research on automatic pronunciation evaluation con-
cerns read speech or highly predictable speech (Witt,
1999; Franco et al, 2000; Hacker et al, 2005),
where there is a high possibility of success in speech
recognition. Automatic pronunciation evaluation is
challenging for spontaneous speech and has been
under-explored.
In this paper, we will describe a method for
extracting pronunciation features based on sponta-
neous speech that is well motivated by theories and
supported by empirical evaluations of feature per-
formance. In conceptualizing and computing these
features, we draw on the literature on automatic pro-
nunciation evaluation for constrained speech. As de-
scribed in the related work in Section 2, the widely
used features for measuring pronunciation are (1)
likelihood (posterior probability) of a phoneme be-
ing spoken given the observed audio sample that
is computed in a Viterbi decoding process, and (2)
phoneme length measurements that are compared to
standard references based on native speech.
However, we have also come up with unique solu-
tions to address the issue of relatively low accuracy
in recognizing spontaneous speech. Our methods of
feature extraction are designed with considerations
of how to best capture the quality of pronunciation
given technological constraints.
The remainder of the paper is organized as fol-
lows: Section 2 reviews the related research; Sec-
tion 3 describes our method to extract a set of fea-
tures for measuring pronunciation; Section 4 de-
scribes the design of the experiments, including the
questions investigated, the data, the speech process-
ing technologies, and the measurement metrics; Sec-
tion 5 reports on the experimental results; Section 6
discusses the experimental results; and Section 7
summaries the findings and future research planned.
2 Related work
There is previous research on utilizing speech recog-
nition technology to automatically assess non-native
speakers? communicative competence (e.g., fluency,
intonation, and pronunciation). Witt (Witt, 1999)
developed the Goodness of Pronunciation (GOP)
measurement for measuring pronunciation based on
Hidden Markov Model (HMM) log likelihood. Us-
ing a similar method, Neumeyer et al (Neumeyer et
al., 2000) designed a series of likelihood related pro-
nunciation features, e.g., the local average likelihood
and global average likelihood. Hacker et al (Hacker
et al, 2005) utilized a relatively large feature vector
for scoring pronunciation.
Pronunciation has been the focus of assessment in
several automatic speech scoring systems. Franco et
al. (Franco et al, 2000) presented a system for au-
tomatic evaluation of pronunciation quality on the
phoneme level and the sentence level of speech by
native and non-native speakers of English and other
languages (e.g., French). A forced alignment be-
tween the speech read by subjects and the ideal path
through the HMM was computed. Then, the log
posterior probabilities for a certain position in the
signal were computed to achieve a local pronunci-
ation score. Cucchiarini et al (Cucchiarini et al,
1997a; Cucchiarini et al, 1997b) designed a system
for scoring Dutch pronunciation along a similar line.
Their pronunciation feature set was more extensive,
including various log likelihood HMM scores and
phoneme duration scores. In these two systems, the
speaking skill scores computed on features by ma-
chine are found to have good agreement with scores
provided by humans.
A limited number of studies have been conducted
on assessing speaking proficiency based on sponta-
neous speech. Moustroufas and Digalakis (Mous-
troufas and Digalakis, 2007) designed a system to
automatically evaluate the pronunciation of foreign
speakers using unknown text. The difference in the
recognition results between a recognizer trained on
speakers? native languages (L1) and another recog-
nizer trained on their learned languages (L2) was
used for pronunciation scoring. Zechner and Be-
jar (Zechner and Bejar, 2006) presented a system
to score non-native spontaneous speech using fea-
tures derived from the recognition results. Follow-
ing their work, an operational assessment system,
SpeechRaterTM , was implemented with further im-
provements (Zechner et al, 2007).
There are some issues with the method to extract
pronunciation features in the previous research on
automated assessment of spontaneous speech (Zech-
ner and Bejar, 2006; Zechner et al, 2007). For ex-
443
ample, the acoustic model (AM) that was used to es-
timate a likelihood of a phoneme being spoken was
well-fitted to non-native speech acoustic properties.
Further, other important aspects of pronunciation,
e.g., vowel duration, have not been utilized as a fea-
ture in the current SpeechRaterTMsystem. Likeli-
hoods estimated on non-words (such as silences and
fillers) that were not central to the measurement of
pronunciation were used in the feature extraction. In
addition, mis-recognized words lead to wrong like-
lihood estimation. Our paper attempts to address all
of these limitations described above.
3 Extraction of Pronunciation Features
Figure 1 depicts our new method for extracting an
expanded set of pronunciation features in a more
meaning way.
Figure 1: Two-stage pronunciation feature extraction
We used two different AMs for pronunciation fea-
ture extraction. First, we used an AM optimized
for speech recognition (typically an AM adapted
on non-native speech to better fit non-native speak-
ers? acoustics patterns) to generate word hypotheses;
then we used the other AM optimized for pronun-
ciation scoring (typically trained on native or near-
native speech to be a good reference model reflect-
ing expected speech characteristics) to force align
the speech signals to the word hypotheses and to
compute the likelihoods of individual words being
spoken and durations of phonemes; finally new pro-
nunciation features were extracted based on these
measurements.
Some notations used for computing the pronunci-
ation features are listed in Table 1. Based on these
notations, the proposed new pronunciation features
are described in Table 2. To address the limita-
tions of previous research on automated assessment
of pronunciation, which was described in Section 2,
our proposed method has achieved improvements on
(1) using the two-stage method to compute HMM
likelihoods using a reference acoustic model trained
on native and near-native speech, (2) expanding the
coverage of pronunciation features by using vowel
duration shifts that are compared to standard norms
of native speech, (3) and using likelihoods on the
audio portions that are recognized as words and ap-
plying various normalizations.
Table 1: Notations used for pronunciation feature extrac-
tion
Variable Meaning
L(xi) the likelihood of word xi being spo-
ken given the observed audio signal
ti the duration of word i in a response
Ts the duration of the entire response
T
n?
i=1
ti, the summation of the duration
of all words, where T ? Ts
n the number of words in a response
m the number of letters in a response
R mTs , the frequency of letters (as the rateof speech)
vi vowel i
Nv the total number of vowels
Pvi the duration of vowel vi
P? the average vowel duration (across all
vowels in the response being scored)
Dvi the standard average duration of
vowel vi (estimated on a native
speech corpus)
D? the averaged vowel duration (on all
vowels in a native speech corpus)
Svi |Pvi ? Dvi |, duration shift of vowel
vi (measured as the absolute value of
the difference between the duration of
vowel vi and its standard value)
Snvi |
Pvi
P? ?
Dvi
D? |, normalized duration shiftof vowel vi (measured as the absolute
value of the normalized difference be-
tween the duration of vowel vi and its
standard value)
4 Experiment design
We first raise three questions that we try to answer
with our experiments. Then, we describe the data
sets and the speech recognizers, especially the two
444
Table 2: A list of proposed pronunciation features
Feature Formula Meaning
L1
n?
i=1
L(xi) summation of likeli-
hoods of all the indi-
vidual words
L2 L1/n average likelihood
across all words
L3 L1/m average likelihood
across all letters
L4 L1/T average likelihood
per second
L5
n?
i=1
L(xi)
ti
n average likelihooddensity across all
words
L6 L4/R L4 normalized by the
rate of speech
L7 L5/R L5 normalized by the
rate of speech
S?
Nv?
i=1
Svi
Nv average vowel dura-tion shifts
S?n
Nv?
i=1
Snvi
Nv average normalizedvowel duration shifts
different acoustic models fitted to non-native and ex-
pected speech respectively. Finally, we describe the
evaluation criterion used in the experiment.
4.1 Research questions
In order to justify that the two-stage method for ex-
tracting pronunciation features is a valid method that
provides useful features for assessing pronunciation,
the following questions need to be answered:
Q1: Can the words hypothesized be used to approx-
imate the human transcripts in the forced align-
ment step?
Q2: Are the new pronunciation features effective
for assessment?
Q3: Can the likelihood-related features be im-
proved when using only words correctly recog-
nized?
4.2 Data
Table 3 lists the data sets used in the experiment.
Non-native speech collected in the TPO was used in
training a non-native AM. For feature evaluations,
we selected 1, 257 responses from the TPO data col-
lected in 2006. Within this set, 645 responses were
transcribed. Holistic scores were assigned by human
raters based on a score scale of 1 (the lowest profi-
ciency) to 4 (the highest proficiency).
In the TOEFL R?Native Speaker Study, native
speakers of primarily North American English
(NaE) took the TOEFL R?test and their speech files
were collected. This TOEFL R?native speech data
and some high-scored TPO responses were used
in the adaptation of an AM representing expected
speech properties. In addition, 1, 602 responses of
native speech, which had the highest speech profi-
ciency scores in NaE, were used to estimate standard
average vowel durations.
Type Function Source Size
non-
native
speech
AM training TPO ? 30 hrs
feature evalua-
tion
TPO col-
lected in
2006
1, 257
responses
(645 with
tran-
scripts)
native
or
near-
native
speech
AM adaptation TPO and
TOEFL
Native
? 2, 000
responses
estimation of
standard vowel
durations
TOEFL
Native
1, 602 re-
sponses
Table 3: Data sets used in the experiment
4.3 Speech technologies
For speech recognition and forced alignment, we
used a gender-independent fully continuous HMM
speech recognizer. Two different AMs were used in
the recognition and forced alignment steps respec-
tively.
The AM used in the recognition was trained
on about 30 hours of non-native speech from the
TPO. For language model training, a large corpus
of non-native speech (about 100 hours) was used
445
and mixed with a large general-domain language
model (trained from the Broadcast News (BN) cor-
pus (Graff et al, 1997) of the Linguistic Data Con-
sortium (LDC)). In the pronunciation feature extrac-
tion process depicted in Figure 1, this AM was used
to recognize non-native speech to generate the word
hypotheses.
The AM used in the forced alignment was trained
on native speech and high-scored non-native speech.
It was trained as follows: starting from a generic
recognizer, which was trained on a large and var-
ied native speech corpus, we adapted the AM using
batch-mode MAP adaptation. The adaptation corpus
contained about 2, 000 responses with high scores in
previous TPO tests and the TOEFL R?Native Speaker
Study. In addition, this AM was used to estimate
standard norms of vowels as described in Table 1.
4.4 Measurement metric
To measure the quality of the developed features,
a widely used metric is the Pearson correlation (r)
computed between the features and human scores.
In previous studies, human holistic scores of per-
ceived proficiency have been widely used in esti-
mating the correlations. In our experiment, we will
use the absolute value of Pearson correlation with
human holistic scores (|r|) to evaluate the features.
Given the close relationship between pronunciation
quality and overall speech proficiency, |r| is ex-
pected to approximate the strength of its relationship
with the human pronunciation scores.
5 Experimental Results
5.1 Results for Q1
When assessing read speech, the transcription of
the spoken content is known prior to the assess-
ment and used to forced-align the speech for fea-
ture extraction. However, when assessing sponta-
neous speech, we do not know the spoken content
and cannot provide a correct word transcription for
the forced alignment with imperfect speech recogni-
tion. A practical solution is to use the recognition
hypothesis to approximate the human transcript in
the forced alignment. Since the recognition word ac-
curacy on non-native spontaneous speech is not very
high (for example, a word accuracy of about 50% on
the TPO data was reported in (Zechner et al, 2007)),
it is critical to verify that the approximation can pro-
vide good enough pronunciation features compared
to the ones computed in an ideal scenario (by using
the human transcript in the forced alignment step).
We ran forced alignment on 645 TPO responses
with human transcriptions, using both the manual
transcription and the word hypotheses from the rec-
ognizer described in Section 4.3. Then, based on
these two forced alignment outputs, we extracted the
pronunciation features as described in Section 3.
Table 4 reports the |r|s between the proposed
pronunciation features and human holistic scores
when using the forced alignment results from ei-
ther transcriptions or recognition hypotheses. The
relative |r| reduction (defined as (|r|transcriptions ?
|r|hypotheses)/|r|transcriptions ? 100) is reported to
measure the magnitude reduction.
Based on the results shown in Table 4, we find that
the pronunciation features computed based on the
forced alignment results using transcriptions have
higher |r|s with the human holistic scores than the
corresponding features computed based on the FA
results using the recognition hypotheses. This is not
surprising given that only 50% ? 60% word accu-
racy can be achieved when recognizing non-native
spontaneous speech. However, the pronunciation
features computed using the recognition hypothe-
ses that is feasible in practice show some promising
correlations to human holistic scores. For example,
L3, L6, and L7 have |r|s larger than 0.45 and S?n
has an |r| larger than 0.35. Compared to the cor-
responding features computed using the FA results
based on transcriptions, these promising pronuncia-
tion features that can be obtained practically, show
some reduction in quality (from 13.4% to 21.1%)
but are still usable. Therefore, our proposed two-
stage method for pronunciation feature extraction is
proven to be a practical way for the computation of
features that have acceptable performance.
5.2 Result for Q2
Although our proposed modifications described in
Section 3 have improved the meaningfulness of the
features, an empirical study is needed to examine the
actual utility of these features for the assessment of
pronunciation.
In the experiment described in Section 5.1, four
pronunciation features (including L3, L6, L7, and
446
Feature |r| using
transcrip-
tion
|r| using
recog-
nition
hypothesis
relative |r|
reduction
(%)
L1 0.216 0.107 50.5
L2 0.443 0.416 6.1
L3 0.506 0.473 6.5
L4 0.363 0.294 19
L5 0.333 0.287 13.8
L6 0.549 0.475 13.5
L7 0.546 0.473 13.4
S? 0.396 0.296 25.3
S?n 0.451 0.356 21.1
Table 4: |r| between the pronunciation features and hu-
man holistic scores under two forced alignment input
conditions (using transcriptions vs. using recognition hy-
potheses) and relative |r| reduction
S?n) show promising correlations to human holistic
scores. To check the quality of the newly developed
pronunciation features, we compared these four fea-
tures with the amscore feature used in (Zechner et
al., 2007) on the TPO data set collected in 2006
(with 1, 257 responses). We first ran speech recog-
nition using the recognizer designed for non-native
speech. The recognition results were used to com-
pute the amscore, which is calculated by dividing
the likelihood over an entire response by the number
of letters. Then, we used the recognition hypothe-
ses to do the forced alignment using the other AM
trained on the native and near-native speech to ex-
tract those four pronunciation features. Finally, we
calculated the correlation coefficients between fea-
tures and the human holistic scores. The results are
reported in Table 5.
feature |r| to human holistic scores
amscore 0.434
L3 0.369
L6 0.444
L7 0.443
S?n 0.363
Table 5: A comparison of new pronunciation features to
amscore, the one used in SpeechRaterTM
Compared to the feature amscore, L6 and L7
have slightly higher |r|s with the human holistic
scores. This suggests that our construct-driven ap-
proach yields pronunciation features that are empiri-
cally comparable or even better than the amscore. In
addition, S?n, a new feature representing the vowel
production aspect of pronunciation, shows a rela-
tively high correlation with human holistic scores.
This suggests that our new pronunciation feature set
has an expanded coverage of pronunciation.
It is interesting to note that L3 has a lower |r|with
human holistic scores than the amscore does. Al-
though the computation of L3 is quite similar to that
of amscore, the major difference is that likelihoods
of non-word portions (such as silences and fillers)
are used to compute amscore but not L3. This sug-
gests that likelihood-related pronunciation features
that involve information related to non-words may
perform better in predicting human holistic scores.
For example, for amscore, the likelihoods measured
on those non-word units were involved in the feature
calculation; for L6 and L7, the temporal information
of those non-word units (e.g., duration of units) was
involved in the feature calculation 1.
5.3 Result for Q3
In the feature extraction, we used the words hy-
pothesized by the speech recognizer as the input for
the forced alignment. Since a considerable num-
ber of words are recognized incorrectly (especially
for non-native spontaneous speech), a natural way
to further improve the likelihood related features is
to only consider words which are correctly recog-
nized. A useful metric associated with the recog-
nition performance is the confidence score (CS) out-
put by the recognizer, which reflects the recognizer?s
estimation about the probability that a hypothesized
word is correctly recognized. The recognized words
with high confidence scores tend to be correctly rec-
ognized. Therefore, focusing on words recognized
with high confidence scores may reduce the negative
impact caused by recognition errors on the quality of
the likelihood related features.
On the TPO data with human transcripts, we used
the NIST?s sclite scoring tool (Fiscus, 2009) to mea-
sure the percentage of correct words (correct%),
which is defined as the ratio of the number of words
1L6 and L7 use R, which is computed as mTs , where Ts con-tains durations of non-words.
447
correctly recognized given the number of words in
the reference transcript. On all words (correspond-
ing to confidence scores ranging from 0.0 to 1.0), the
correct% is 53.3%. Figure 2 depicts the correct%
corresponding to ten confidence score bins ranging
from 0.0 to 1.0. Clearly, with the increase of the con-
fidence score, more words tend to be accurately rec-
ognized. Therefore, it is reasonable to only use like-
lihoods estimated on the hypothesized words with
high confidence scores for extracting likelihood re-
lated features.
 
0
 
10
 
20
 
30
 
40
 
50
 
60  0
 
0.2
 
0.4
 
0.6
 
0.8
 
1
Correct% of words hypothesized
Confid
ence s
core (C
S) bin
Figure 2: Correct% of words recognized across 10 confi-
dence score bins
On the TPO data set collected in 2006, we com-
puted three likelihood related features (including L3,
L6, and L7) only on words whose SC is equal to
or higher than a threshold (i.e., 0.5, 0.6, 0.7, 0.8,
and 0.9) and measured the |r| of a feature with the
human holistic scores. Table 6 lists the confidence
score cutting thresholds, the percentage of words
whose confidence scores are not lower than the cut-
ting threshold selected, and |r| between each like-
lihood feature to human holistic scores. In the Ta-
ble 6, we observe that only using words recognized
with high confidence improves the correlations be-
tween the features and the human holistic scores.
One issue about only using words recognized with
high confidence scores is that the number of words
used in the feature extraction has been reduced and
may reduce the robustness of the feature calculation.
Tc percentage
of words
whose CS
? Tc (%)
L3
|r|
L6
|r|
L7
|r|
0.0 100 0.369 0.444 0.443
0.5 84.21 0.38 0.462 0.461
0.6 77.07 0.377 0.465 0.464
0.7 69.31 0.363 0.461 0.461
0.8 60.86 0.371 0.466 0.466
0.9 50.76 0.426 0.477 0.475
Table 6: |r| between L3, L6, and L7 and human holistic
scores using only words recognized whose CSs are not
lower than a threshold (Tc)
6 Discussion
To assess the pronunciation of spontaneous speech,
we proposed a method for extracting a set of pro-
nunciation features. The method consists of two
stages: (1) recognizing speech using an AM well fit-
ted to non-native speech properties and (2) forced-
aligning the hypothesized words using the other
AM, which was trained on native and near-native
speech, and extracting features related to spectral
properties (HMM likelihood) and vowel production.
This method of using one AM optimized for speech
recognition and another AM optimized for pronun-
ciation evaluation is well motivated theoretically.
The derived pronunciation features have also been
found to have reasonably high correlations with hu-
man holistic scores. The results support the link-
age of the features to the construct of pronunciation
and their utility of being used in a scoring model to
predict human holistic judgments. Several contribu-
tions of this paper are described as below.
First, the two-stage method allows us to utilize
an AM trained on native and near-native speech as
a reference model when computing pronunciation
features. The decision to include high-scored non-
native speech was driven by the scoring rubrics de-
rived from the construct, where the pronunciation
quality of the highest level performance does not
necessarily require native-like accent, but highly in-
telligible speech. The way the reference model was
trained is consistent with the scoring rubrics, and
makes it an appropriate standard based on which the
pronunciation quality of non-native speech can be
448
evaluated. By using the recognition hypotheses from
the recognition step as input in the forced alignment
step, our experiments show a relatively small reduc-
tion in correlations with human holistic scores in
comparison to the features based on the human tran-
scriptions. This suggests that our method has po-
tential to be implemented in a real-time operational
setting.
Second, a few decisions we have made in com-
puting the pronunciation features are driven by
considerations of how these features are meaning-
fully linked to the construct of pronunciation as-
sessment. For example, we have excluded the
HMM likelihoods on non-words (such as pauses
and fillers) in the computations of likelihood-related
features. In addition, only using words recognized
with high confidence scores yields more informative
likelihood-related features for assessing the quality
of speech. The inclusion of vowel duration measures
in the feature set expanded the coverage of the qual-
ity of pronunciation.
7 Summary and future work
This paper presents a method for computing features
for assessing the pronunciation quality of non-native
spontaneous speech, guided by construct considera-
tions. We were able to show that using a two-stage
method of first recognizing speech with a non-native
AM and then forced aligning of the hypothesis using
a native or near-native speech AM we can generate
pronunciation features with promising correlations
with holistic scores assigned by human raters.
We plan to continue our research in the follow-
ing directions: (1) we will improve the native speech
norms for vowel durations, such as using the distri-
bution of vowel durations rather than just the mean
of durations in our feature computations; (2) we
will investigate other aspects of pronunciation, e.g.,
consonant quality and word stress; (3) we will add
other standard varieties of English (such as British,
Canadian, Australian, etc) to the training corpus for
the reference pronunciation model as the current
model is trained on primarily North American En-
glish (NaE).
References
J. Bernstein. 1999. PhonePass testing: Structure and
construct. Technical report, Ordinate Corporation.
C. Cucchiarini, H. Strik, and L. Boves. 1997a. Au-
tomatic evaluation of Dutch Pronunciation by us-
ing Speech Recognition Technology. In IEEE Auto-
matic Speech Recognition and Understanding Work-
shop (ASRU), Santa Barbara, CA.
C. Cucchiarini, H. Strik, and L. Boves. 1997b. Us-
ing Speech Recognition Technology to Assess Foreign
Speakers? Pronunciation of Dutch. In 3rd interna-
tional symosium on the acquision of second language
speech, Klagenfurt, Austria.
J. Fiscus. 2009. Speech Recognition Scoring Toolkit
(SCTK) Version 2.3.10.
H. Franco, V. Abrash, K. Precoda, H. Bratt, R. Rao, and
J. Butzberger. 2000. The SRI EduSpeak system:
Recognition and pronunciation scoring for language
learning. In InSTiLL (Intelligent Speech Technology
in Language Learning), Dundee, Stotland.
D. Graff, J. Garofolo, J. Fiscus, W. Fisher, and D. Pallett.
1997. 1996 English Broadcast News Speech (HUB4).
C. Hacker, T. Cincarek, R. Grubn, S. Steidl, E. Noth, and
H. Niemann. 2005. Pronunciation Feature Extraction.
In Proceedings of DAGM 2005.
N. Moustroufas and V. Digalakis. 2007. Automatic
pronunciation evaluation of foreign speakers using
unknown text. Computer Speech and Language,
21(6):219?230.
L. Neumeyer, H. Franco, V. Digalakis, and M. Weintraub.
2000. Automatic Scoring of Pronunciation Quality.
Speech Communication, 6.
S. M. Witt. 1999. Use of Speech Recognition in
Computer-assisted Language Learning. Ph.D. thesis,
University of Cambridge.
K. Zechner and I. Bejar. 2006. Towards Automatic Scor-
ing of Non-Native Spontaneous Speech. In NAACL-
HLT, NewYork NY.
K. Zechner, D. Higgins, and Xiaoming Xi. 2007.
SpeechRater: A Construct-Driven Approach to Scor-
ing Spontaneous Non-Native Speech. In Proc. SLaTE.
449
Proceedings of the Third ACL Workshop on Innovative Use of NLP for Building Educational Applications, pages 98?106,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
         Towards Automatic Scoring of a Test of Spoken Language with 
Heterogeneous Task Types 
 
 
 
Klaus Zechner     and     Xiaoming Xi 
Educational Testing Service 
Rosedale Road, Princeton, NJ 08541, USA 
{kzechner,xxi}@ets.org 
Abstract 
This paper describes a system aimed at auto-
matically scoring two task types of high and 
medium-high linguistic entropy from a spoken 
English test with a total of six widely differing 
task types. 
We describe the speech recognizer used for 
this system and its acoustic model and lan-
guage model adaptation; the speech features 
computed based on the recognition output; 
and finally the scoring models based on mul-
tiple regression and classification trees. 
For both tasks, agreement measures between 
machine and human scores (correlation, 
kappa) are close to or reach inter-human 
agreements. 
1 Introduction 
As demand for spoken language testing and cost of 
human scoring have increased in recent years, 
there is a growing interest in building both research 
and industrial systems for automatically scoring 
non-native speech (Bernstein, 1999, Zechner and 
Bejar, 2006, Zechner et al 2007).  
However, past approaches have focused typi-
cally only on one type of spoken language, or on a 
range of types similar in linguistic entropy. En-
tropy in this context can be seen as a measure for 
how predictable the language in the expected spo-
ken response is: Some tests, such as SET-10 (Bern-
stein 1999), are focused mostly on the lower 
entropy aspects of language, using tasks such as 
?reading? or ?repetition?, where the expected se-
quence of words is highly predictable. Other as-
sessments, such as the TOEFL? Practice Online 
Speaking test, on the other hand, focus on more 
spontaneous, high-entropy responses (Zechner et 
al., 2007). 
In this paper, we describe a spoken language test 
with heterogeneous task types, ranging from read 
speech to tasks that require candidates to give their 
opinions on an issue, whose goal is to assess com-
municative competence (Bachman, 1990; Bach-
man & Palmer, 1996); we call this test THT (Test 
with Heterogeneous Tasks). Communicative com-
petence, in this context, refers to a speaker's ability 
to use the language for communicative purposes.  
The effectiveness of the communication typically 
consists of a few aspects including comprehensibil-
ity, accuracy, clarity, coherence and appropriate-
ness, and is evident in a speaker's pronunciation, 
fluency, use of grammar and vocabulary, develop-
ment  of ideas, and sensitivity to the context of the 
communication.  
This test has the advantage of being able to as-
sess a wide range of non-native speakers? profi-
ciencies by using tasks of varying difficulty levels 
to allow even low proficiency speakers some de-
gree of success on easier task types. 
We select two tasks from this test, one of higher 
and one of medium to high entropy, and first adapt 
a non-native English speech recognizer (trained on 
TOEFL? Practice Online data) to transcribed THT 
task responses, then compute a set of relevant 
speech features based on the recognition output, 
and finally build a scoring model using a subset of 
these features to predict trained human rater scores. 
In this paper, we will demonstrate that the ma-
chine-human score agreements on these two task 
types come close to or even exceed the level of 
inter-human agreement. 
This paper is organized as follows: Section 2 
discusses related work, Section 3 describes the test 
and the challenges for automatic scoring involved, 
Section 4 discusses the speech recognizer and the 
acoustic and language model adaptations per-
98
formed, and Section 5 describes the speech fea-
tures selected for use in the scoring model. In Sec-
tion 6, we report the construction of the scoring 
model and its results, Section 7 contains a general 
discussion and Section 8 concludes the paper with 
a brief discussion of future research. 
2 Related work  
There has been previous work to automatically 
characterize aspects of communicative competence 
such as fluency, pronunciation, and prosody. 
Franco et al (2000) present a system for automatic 
evaluation of the pronunciation quality of both na-
tive and non-native speakers of English on a phone 
level and a sentence level (EduSpeak). Candidates 
read English texts and a forced alignment between 
the speech signal and the ideal path through the 
Hidden Markov Model (HMM) is computed. Next, 
the log posterior probabilities for pronouncing a 
certain phone at a certain position in the signal are 
computed to achieve a local pronunciation score. 
These scores are then combined with other auto-
matically derived measures such as the rate of 
speech (number of words per second) or the dura-
tion of phonemes to yield global pronunciation 
scores. 
Cucchiarini et al (1997a, 1997b) describe a sys-
tem for Dutch pronunciation scoring along similar 
lines. Their feature set, however, is more extensive 
and contains, in addition to log likelihood Hidden 
Markov Model scores, various duration scores, and 
information on pauses, word stress, syllable struc-
ture, and intonation. In an evaluation, correlations 
between four human scores and five machine 
scores range from 0.67 to 0.92. 
Bernstein (1999) presents a test for spoken Eng-
lish (SET-10) that uses the following types of task-
s: reading, sentence repetition, sentence building, 
opposites, short questions, and open-ended ques-
tions. All types except for the last are scored auto-
matically and a score is reported that can be 
interpreted as an indicator of how native-like a 
speaker?s speech is. In Bernstein et al (2000), an 
experiment is performed to investigate the per-
formance of the SET-10 test in predicting speak-
ers? oral proficiency.  It is shown that the SET-10 
test scores can predict different levels on the Oral 
Interaction Scale of the Council of Europe?s 
Framework (North, 2000) for describing oral pro-
ficiency of second/foreign language speakers with 
reasonable accuracy. This paper further reports on 
studies done to correlate the SET-10 automated 
scores with the human scores from two other tests 
of oral English communication skills. Correlations 
are found to be between 0.73 and 0.88.  
Zechner and Bejar (2006) investigate the auto-
mated scoring of unrestricted, spontaneous speech 
of non-native speakers. They focus on exploring a 
number of different fluency features for the auto-
mated scoring of short (one minute) responses to 
test questions in a TOEFL-related program. They 
explore scoring models based on classification and 
regression trees (CART) as well as support vector 
machines (SVM). Their findings are that the SVM 
models are more useful for a quantitative analysis, 
whereas the CART models allow for a more trans-
parent summary of the patterns underlying the 
data.  
In this paper, we use CART to build the scoring 
model for one task type. We also adopt multiple 
regression for another task type which has the ad-
vantage of being more easily interpreted than, for 
example, SVMs. Another major difference be-
tween previous work and the work reported in this 
paper is that we use feature normalization and 
transformation to obtain statistically more mean-
ingful input variables for the scoring model. In ad-
dition, we do not use the whole set of features in an 
exploratory fashion. Instead, we have carefully 
selected a subset of features that are both good pre-
dictors of human scores and maximize the repre-
sentation of the concept of communicative 
competence. 
3 The THT test 
3.1 Task types and scoring rubrics of the THT 
Speaking test 
There are six task types in the THT Speaking test, 
ranging from reading-aloud tasks to tasks that re-
quire short answers and tasks that require extended 
responses of one minute. The rubrics differ in both 
the dimensions of speaking skills measured and the 
possible score points. (Rubrics are characteriza-
tions of candidates? competence at given score lev-
els and are used by human raters to determine the 
appropriate score for a response.) Below is a brief 
description of the task types and the rubrics.  
 
99
Task type 1: Reading-aloud (Planning time: 45 
seconds; Response time: 45 seconds; zero/very-
low entropy) 
There are two read-aloud tasks. Each task requires 
the test-taker to read a short paragraph of 40-60 
words aloud. The reading materials include an-
nouncements, advertisements, introductions, etc. 
These two tasks are rated analytically on pronun-
ciation and intonation and stress on a 3-point scale. 
That is to say, two separate scores are given on 
each task ? one for pronunciation and one for into-
nation and stress.  
 
Task type 2: Picture description (Planning time: 
30 seconds; Response time: 45 seconds; me-
dium-high entropy) 
This task requires the test-taker to describe a pic-
ture in as much detail as possible.  
This task is rated holistically on the combined 
impact of delivery (fluency, pronunciation etc.), 
use of structures, vocabulary, content relevance 
and fullness on a 3-point scale.  
 
Task type 3: Open-ended short-answer ques-
tions (Planning time: none; Response time: 15-
30 seconds; low/low-medium entropy) 
The test-taker responds, without preparation, to 
three questions about familiar and accessible topics 
that draw on immediate personal experience. The 
first two questions each elicit a 15-second response 
that covers one or two pieces of information re-
lated to the specified topic. The third question re-
quires a 30-second response that expresses an 
opinion or gives an explanation related to the topic. 
This task is rated holistically on the combined im-
pact of delivery, use of structures, vocabulary, and 
task appropriateness on a 3-point scale.  
 
Task type 4: Constrained short-answer ques-
tions (Planning time: none; Response time: 15-
30 seconds; low/low-medium entropy) 
The test-taker responds to three questions about a 
schedule/agenda that is provided in written form. 
All the information needed to answer the questions 
should be included on or easily inferred from the 
schedule. The test-taker has 15 seconds to respond 
to each of the first two questions. These questions 
ask for specific information on the schedule or eas-
ily inferred information about the schedule. The 
test-taker has 30 seconds to respond to the last 
question which requires a summary of multiple 
events or multiple pieces of information on the 
schedule. This task is rated holistically on the 
combined impact of delivery, use of structures, 
vocabulary, task appropriateness and content accu-
racy on a 3-point scale.  
 
Task type 5: Respond to a voice mail (Planning 
time: 30 seconds; Response time: 60 seconds; 
high entropy) 
In this task, the test-taker listens to a voicemail that 
describes a problem, question or situation and then 
assumes a particular role (bank teller, office assis-
tant, etc.) to respond with a proposed solution or 
answer. This task is rated holistically on the com-
bined impact of fluency, pronunciation, intonation 
and stress, grammar, vocabulary, register, content 
relevance, and cohesion and idea progression on a 
5-point scale.  
 
Task type 6: Opinion task (Planning time: 15 
seconds; Response time: 60 seconds; high en-
tropy)  
In this task, the test-taker is expected to state an 
opinion or position on an issue that is familiar and 
accessible and to express support for the opinion or 
position with reasons, examples, arguments, etc. 
This task is rated holistically on the combined im-
pact of fluency, pronunciation, intonation and 
stress, grammar, vocabulary, content relevance, 
and cohesion and idea progression on a 5-point 
scale. 
3.2 Challenges of the THT test design to auto-
matic scoring 
1. Some of the tasks require responses that are ex-
pected to vary very little in vocabulary and content 
across examinees (e.g., Reading-aloud and Con-
strained short-answer questions) whereas others 
allow much more flexibility and variation in the 
use of vocabulary and grammatical structure and 
topical content (e.g. Respond to a voicemail and 
Opinion task). The predictability of the expected 
response will dictate what type of language model-
ing technique is preferable to optimize speech rec-
ognition results. Therefore, unlike in other systems 
focusing either on high or low entropy speech 
(e.g., Zechner and Bejar, 2006; Bernstein, 1999), 
in which a single speech recognizer is employed, it 
is anticipated that different types of speech recog-
nizers are needed to suit different THT task types. 
This may increase both the amount of development 
100
work and the complexity in integrating different 
types of recognizers into the real-time automated 
scoring system.   
2. Furthermore, the scoring criteria of these six 
different task types are somewhat different. This 
suggests that different scoring models may need to 
be developed for different task types since the 
relevant speech features to be included in the scor-
ing model for each task type may differ.   
3. THT speaking tasks use two kinds of score 
scales: 0-3 and 0-5. Classification techniques, such 
as classification trees or cumulative logit models 
(Agresti, 2002; Menard, 2001), may be more ap-
propriate for task types that use a 3-point scale. 
Prediction techniques such as multiple regression 
may be better suited for task types that are on a 5-
point scale. Training different types of scoring 
models will certainly increase the complexity and 
the amount of scoring model development and 
evaluation work.  
 In summary, the complexity of the design 
of the THT Speaking test is expected to have a ma-
jor impact on our efforts to develop an automated 
scoring system. Given these challenges and the 
research resources available, we decided on a strat-
egy of starting with high entropy task types and 
proceeding to low entropy task types. For this pa-
per, we selected the high entropy Opinion task and 
the medium-high entropy Picture tasks for system 
development.  
4 Adaptation of the speech recognizer  
For this work, we are using a state-of-the-art gen-
der-independent Hidden Markov Model speech 
recognizer whose acoustic model was trained on 
about 30 hours of non-native speech and whose 
language model was built on several hundred hours 
of both native and non-native speech. The non-
native data came from the TOEFL? Practice 
Online system, a web-based practice program for 
prospective takers of the Test Of English as a For-
eign Language (TOEFL) (Zechner et al, 2007). 
This data is somewhat different from the THT, as 
there are only high-entropy tasks in TOEFL Speak-
ing and as the speakers are generally more profi-
cient. Due to this difference, the baseline word 
accuracy was fairly low (see Table 1). 
Therefore, as a first step, we needed to adapt the 
automatic speech recognition engine to the THT 
speech data.  
We had approximately 1,000 responses each 
from the Picture and Opinion tasks transcribed. As 
mentioned above, while the Opinion task responses 
are generally more spontaneous, the Picture task 
requires the candidate to accurately describe a pic-
ture and thus restricts the possible answer space 
considerably. Still, there is more room for individ-
ual choice and variation in the vocabulary, gram-
mar and content produced than there is in the more 
restricted low-medium and low entropy task types 
in the THT Speaking test.  
When using our baseline automatic speech rec-
ognition (ASR)  engine without any adaptation to 
the THT speech data, we only obtained word accu-
racies between 25% and 33%, which was clearly 
inadequate, and far below a word accuracy where, 
at least for some speakers, meaningful information 
can be drawn from the ASR hypothesis. 
Therefore, we undertook a series of adaptation 
and optimization steps with the goal of maximizing 
the word accuracy on the two task types for the 
THT Speaking test. We first adapted the acoustic 
model in batch mode with supervised maximum a-
posteriori (MAP) adaptation using the combined 
data from both tasks, then the language model, op-
timized the filler cost parameter and finally con-
ducted unsupervised maximum likelihood linear 
regression (MLLR) acoustic model adaptation 
based on individual speakers. 
4.1 Acoustic model batch adaptation 
We randomly selected about 90% of Picture and 
Opinion task response data for acoustic model 
(AM) adaptation, which contained 1,800 response 
files (over 25 hours of speech, adult speakers with 
typically low to intermediate English proficiency). 
Results are always reported on the held-out evalua-
tion data containing 100 files for the Picture task 
and 80 files for the Opinion task. 
We performed supervised maximum a posteriori 
(MAP) adaptation which is the method of choice 
for larger amounts of data and is typically per-
formed in batch mode (Tomokiyo and Waibel, 
2001; Wang et al, 2003). After one cycle of adap-
tation, word accuracy improved by about 8%, as is 
shown in Table 1. We also performed unsupervised 
maximum likelihood linear regression (MLLR) 
adaptation, which is discussed in Section 4.4 be-
low. 
             
 
101
 Picture task word 
accuracy 
Opinion task word 
accuracy 
Method Absolute Increase 
from 
previous 
step 
Absolute Increase 
from 
previous 
step 
Baseline 
recognizer 
25.8% NA 
 
32.2% 
 
NA 
AM MAP 
adaptation 
33.6% 7.8% 
 
40.0% 
 
7.8% 
LM adap-
tation 
50.4% 16.8% 
 
51.0% 
 
11.0% 
Filler 
optimiza-
tion 
57.0% 6.6% 
 
56.3% 
 
5.3% 
 
Ignoring 
fillers 
60.5% 3.5% 
 
59.2% 
 
2.9% 
 
MLLR 
Speaker 
adaptation 
62.4% 1.9% 
 
61.2% 
 
2.0% 
 
Table 1.  Word accuracies after each incremental 
step of adaptation or optimization and performance 
improvement within each step for Picture and Opin-
ion task types.               
4.2 Language model adaptation 
The second step was language model (LM) adapta-
tion. The Picture and Opinion tasks were adapted 
separately using the same training sets as above. 
We built interpolated models between the task-
specific LM and the baseline LM (from the origi-
nal recognizer). 
We obtained the best results using only the task-
specific LM trained on the THT data set (given in 
Table 1). This indicates that the domain of each of 
the tasks is narrow enough that it can be suffi-
ciently described with a set of about 900 tran-
scribed examples each and it does not benefit from 
a larger LM such as our baseline LM.  
4.3 Filler cost optimization 
?Filler cost? is a recognizer-internal parameter that 
determines the likelihood of filler and noise words 
to be inserted into the hypothesis before or after 
?real? words. The higher the parameter?s value, the 
less likely fillers will be inserted. 
The experiments with the filler cost parameter 
grew out of an observation that the baseline recog-
nizer has a tendency to hypothesize too many 
words when faced with different kinds of ?uncer-
tain? audio, such as mumbled words, noises or fill-
ers. Therefore we conjectured that having the 
recognizer hypothesize more filler and noise words 
in these cases and be more restrictive with actual 
word hypotheses might increase the word accuracy 
overall. 
We varied the filler cost parameter from its de-
fault, 3, down to its lowest meaningful value, 0. 
Our experiments show that for fillercost=0, a 
maximum word accuracy was achieved (given in 
Table 1), albeit at the cost of more than doubling 
the length of the recognizer?s hypothesis by intro-
ducing a large amount of fillers (such as ?um? or 
?uh?, noises, mumbles etc.). We observe that using 
such a low filler cost parameter setting can nega-
tively affect some speech features which are can-
didates for being used in a scoring model, such as 
?language model score?. Therefore we have to 
carefully assess whether achieving a higher word 
accuracy is more beneficial to the overall perform-
ance of the feature set or whether it has too many 
negative effects on some important speech fea-
tures. In future work we will attempt to tune the 
recognizer in such a way that it is not only opti-
mized for a high word accuracy, but also for high 
accuracy in filler (and noise) prediction. 
Word accuracy was computed with the fillers 
included or excluded. Since fillers are not real 
words, and in this round of scoring model devel-
opment we did not use any features based on fill-
ers, it was reasonable to compute the overall word 
accuracy with the fillers removed from the human 
and recognizer transcriptions, resulting in a moder-
ate performance gain (see Table 1). 
4.4 Unsupervised speaker adaptation  
We used unsupervised maximum likelihood lin-
ear regression (MLLR) AM adaptation on top of 
the previous adaptation and optimization steps (To-
mokiyo and Waibel, 2001; Wang et al, 2003). In 
this step, all words whose confidence score was 
higher than a pre-set threshold were collected and 
their acoustic information was used to adapt the 
acoustic model. All adaptations were done based 
on the utterances of a single speaker and pertained 
to that speaker only, i.e., it was not incremental or 
cumulative. Since a second decoding run is needed 
after the actual MLLR adaptations, the recog-
nizer?s response time more than doubles when this 
method is employed. The unsupervised speaker 
adaptation led to an additional increase of  
 
102
Feature 
Number 
Feature  
Name 
Feature 
Class 
Description  Used in  
1 hmmscore Pronuncia-
tion  
Acoustic Model score: sum of the log probabilities of 
every frame, normalized for length 
Opinion & Picture  
2 typesper-
second 
Fluency & 
Vocabulary  
diversity  
Number of unique words in response (?types?) di-
vided by length of response 
Opinion & Picture 
3 
silences-
persecond Fluency  Number of silences per second Opinion & Picture 
4 repetitions Fluency  Number of repetitions divided by number of words Opinion  
5 relevance-
cos5 
Vocabulary 
& Content 
Cosine word vector product between a response and 
all responses in the training set that have the highest 
score (5 for the Opinion task) 
Opinion  
6 relevance-
cos3 
Vocabulary 
& Content 
Cosine word vector product between a response and 
all responses in the training set that have the highest 
score (3 for the Picture task) 
Picture 
Table 2. Final features used for the scoring models for the Opinion and Picture tasks 
 
 
approximately 2% for the Picture and Opinion 
tasks (see Table 1). There were large differences 
between different speakers in terms of the per-
formance gain of MLLR adaptation on our data 
set, however. There was also a large variation of 
word accuracies between speakers (13-100%). The 
variation in accuracy across speakers can be due to 
many different factors, including the degree of ac-
cent, the grammaticality of the response, the voice 
quality and the recording quality.  
5 Speech features  
Based on the output of the ASR engine, a feature 
computation module computes a set of about 40 
features for each response, mostly in the fluency 
domain (e.g.  ?average silence duration?), but also 
some features related to pronunciation, vocabulary 
diversity and content. 
Instead of using all of these features in a scoring 
model, we used a process of iterative refinement 
and selection to narrow down the feature set, based 
on both the coverage of the concept of communica-
tive competence and empirical performance (corre-
lations with human scores) of the features. 
Following this process, five features were selected 
to be included in developing the scoring models for 
the Opinion task type and four for the Picture task 
type (see Table 2). 
When we look at the correlations of these fea-
tures to the human scores, we find that hmmscore, 
after being transformed to improve normality, was 
the strongest predictor of human scores for both 
the Opinion and Picture tasks with typespersecond 
as the second strongest (0.5 <= Pearson r <= 0.7). 
6 Scoring models 
All the responses were double scored by a ran-
domly selected pair of raters who were trained for 
scoring this test. The agreements between the two 
ratings (both kappa and Pearson r correlation) were 
around 0.50 for the Picture and 0.72 for the Opin-
ion task. (Note that the fewer points a scale has, the 
lower correlation we can expect due to less score 
variability, everything else being equal.) 
While we use the same training sets for the scor-
ing model experiments as for the above ASR ex-
periments (sm-train), we add about 600 responses 
each to the evaluation sets (these responses were 
untranscribed) to yield a scoring model evaluation 
set size of about 700 responses each (sm-eval). 
Scoring models were developed and evaluated 
for the Opinion and Picture task types separately. 
The Opinion tasks are on a 0-5 point scale whereas 
the Picture tasks are on a 0-3 point scale. There 
were only a handful of 0s on each task and they 
were excluded in building the scoring models.  
For the Opinion tasks, multiple regression mod-
els employing different weights for the features 
were developed, namely an Equal Weights model, 
an Expert Weights model and an Optimal Weights 
model. In the Equal Weights model, each feature 
was assigned the same weight, indicating that all 
features are equally important in the prediction. In 
the Expert Weights model, different weights were 
assigned to different features that reflected our un-
derstanding of the different roles features play in 
indicating the overall speech quality. In the Opti-
mal Weights model, weights were determined by 
103
the least squares optimization procedure using the 
sm-train data.  All features were normalized to 
have a mean of 0 and a standard deviation of 1, 
such that their respective baseline influence on the 
model is comparable across features. 
For the Picture task type, CART was used to 
predict the score class each response should be 
assigned to. CART 5.0 (Steinberg & Colla, 1997) 
was used to build the classification trees. 
In addition, generic and task-specific models 
were developed for both task types. The task-
specific models made use of task-specific vocabu-
lary features (Features 5 and 6 in Table 2) which 
required using previous response data to each of 
the tasks within a particular task type. (Both task 
types had 4 different tasks each). The generic 
models, in contrast, used features that were the 
same across all tasks for a particular task type and 
did not use any task-specific vocabulary features.  
As it would be much more time-consuming and 
costly to build task-specific models, it is worth-
while to investigate how much more predictive 
power the task-specific vocabulary features could 
add over and beyond the features in the generic 
models. 
6.1    Opinion task type  
For the Opinion tasks, four features were used in 
building the generic models and five in developing 
the task-specific models. The following features  
Were used: hmmscore, typespersecond, silences-
persecond, repetitions and relevancecos5 (the latter 
only in the task-specific model). 
Table 3 shows the results on the sm-eval set. 
The Expert Weights model and the Optimal 
Weights models yielded very similar results 
(weighted kappa and correlation = 0.61-0.63) if we 
look at predicted scores that were rounded to the 
nearest integer. The agreements between regres-
sion model predicted scores and scores of human 
rater 1 were just a little below the agreements be-
tween two human raters (weighted kappa and cor-
relation = 0.72). However, the results for the Equal 
Weights model were inferior.  
The results for the task-specific models showed 
no improvement over the generic models, suggest-
ing that the task-specific vocabulary feature did not 
contribute more predictive power beyond the four 
features already in the generic models.  
 
 
Model 
Multiple 
Regres-
sion 
(Equal 
Weights)
Multiple 
Regression 
(Expert 
Weights) 
Multiple 
Regres-
sion (Op-
timal 
Weights)
 
Weighted ? 0.53 0.62 0.61 
Pearson r 
Correlation 
(unrounded)
0.62 0.68 0.69 
Pearson r 
Correlation 
(rounded) 
0.56 0.63 0.63 
Table 3. Performance of different weighting schemes 
on THT scoring model evaluation set for Opinion 
tasks (generic model)  
6.2    Picture task type  
As mentioned earlier, the Picture tasks are on a 0-3 
point scale and we removed a small number of 0-
scores from the analyses, making it a 3-point scale. 
Given this particular score scale, multiple regres-
sion may not be appropriate for this data as it re-
quires a continuous or a quasi-continuous 
dependent variable (i.e. a variable that has at least 
5 or more data points). Some classification tech-
niques such as CART (Brieman et al, 1984) or 
logistic regression, which can take ordered score 
categories as the outcome variable, are better 
suited for this data. In this study, we analyzed the 
data with CART models.  
CART 5.0 (Steinberg and Colla, 1997) was used 
to build the classification trees.  We built two sets 
of CART models, one set with the task-specific 
vocabulary feature (relevancecos3) and one set 
without it. We explored different model configura-
tions, i.e., different combinations of priors and 
splitting rules.  For each combination, a 10-fold 
cross-validation was conducted.  Subsequently, the 
optimal sub tree that was a relatively small tree 
with the highest or near-highest agreement with the 
human scores (weighted kappa) on the cross-
validation sample was identified. Then the cases in 
the sm-eval data set were dropped down the opti-
mal tree to obtain the evaluation results on the 
held-out data.  
The results for the generic model vs. task-
specific models are compared in Table 4. For both 
104
models, CART trees built using the Twoing1 split-
ting rule combined with mixed priors (average of 
equal priors for different score classes and sm-train 
sample priors) yielded the best kappa values on the 
cross-validation data and were selected as the op-
timal trees. The agreements between the CART 
model predicted scores and first rater scores 
slightly exceeded that between two human raters 
on the sm-eval data set. Another observation from 
Table 4 was that for this task type, the task-specific 
CART model did not demonstrate an advantage 
over the generic model; actually, its performance 
was slightly worse than that of the generic model, a 
finding in line with the Opinion task. 
 
 Generic Task-specific 
Inter-human 
agreement 
Weighted ? 0.51 0.50 0.49 
Pearson r 
Correlation  0.52 0.50 0.50 
Table 4. Performance of CART models on THT 
scoring model evaluation set for Picture tasks (ge-
neric model vs. task-specific model)  
7 Discussion 
This paper investigates the feasibility of develop-
ing an automatic scoring system for the THT 
Speaking test, focusing on the particular challenges 
posed by the design of the test. The main challenge 
posed by the test design is the high variability in 
task types -- ranging from low-entropy Reading-
aloud tasks to high-entropy Opinion tasks. While 
previous tests of spoken language have focused 
mainly on either high or low entropy tasks (Bern-
stein, 1999; Zechner and Bejar, 2006), we have 
made an attempt at starting to address the whole 
scale of entropy within a single test. 
In this paper, we selected one high entropy task 
(Opinion) and one medium-high entropy task (Pic-
ture) to start our explorations. While we found that 
we could, for the most part, use a similar set of 
features for both tasks, we had to address the dif-
ference in score scales between these two task 
types. While we could use multiple regression for 
scoring the 5-point-scale Opinion task, we had to 
                                                          
1 The Twoing rule divides the cases into two 
groups, gathers similar classes together, and at-
tempts to separate the two groups in descendant 
nodes.  
 
employ CART trees for the 3-point-scale Picture 
task, demonstrating that one can not necessarily 
use one type of scoring model for all tasks. 
When moving to low and low-medium entropy 
tasks, we expect further adaptations, both in terms 
of the feature set (e.g., the higher importance of 
pronunciation features in Reading-aloud tasks), 
and in speech recognition, where more restrictive 
language models will be needed. 
We have reported findings associated with the 
performance of the scoring models for the Opinion 
and Picture task types. Overall, the preliminary 
findings are quite promising: with a few key 
speech features, we were able to achieve prediction 
accuracies that could almost emulate or slightly 
exceed the agreements between two human raters 
at task level. Once we have developed scoring 
models for all task types, it is conceivable to ag-
gregate the task level scores to produce a total 
summary score at the test level and it is very likely 
we would see a much stronger association between 
human scores and automated scores for the whole 
test.  
The findings also suggest that task-specific 
modeling efforts did not seem to be necessary for 
the two task types investigated. This does not pre-
clude the possibility, though, that task-specific 
scoring models are superior for other task types in 
which the expected content is much more restricted 
(such as the Constrained short-answer questions). 
8 Conclusions and future work 
We have demonstrated that by using a three-stage 
architecture of automatic speech recognition, fea-
ture computation, and scoring models, we are able 
to achieve some degree of success in generating 
automated scores for two task types of a spoken 
language test with a wide variation in entropy in its 
tasks. The agreement between machine scores and 
human scores comes close to or reaches the inter-
human agreement levels for these two tasks. 
In future work, we will switch our focus to task 
types that elicit more constrained speech (such as 
the Reading-aloud tasks and Constrained short-
answer questions). In the meantime, we will con-
tinue to refine and evaluate the preliminary scoring 
models developed in this paper. In particular, we 
will explore cumulative logit models for tasks that 
are on a 0-3 point scale and compare the results to 
those of CART models. 
105
References  
Agresti, A. (2002). Categorial data analysis (2nd 
ed.).  New York: Wiley.  
Bachman, L.F. (1990). Fundamental Considera-
tions in Language Testing.  New York: Oxford 
University Press. 
Bachman, L. F., and Palmer, A. S. (1996). Lan-
guage testing in practice. Ox-
ford:OxfordUniversity Press.  
Bernstein, J. (1999). PhonePass testing: Structure 
and construct. Menlo Park, CA: Ordinate Corpo-
ration. 
Bernstein, J., DeJong, J., Pisoni, D., and Town-
shend, B. (2000). Two experiments in automatic 
scoring of spoken language proficiency. In-
STILL2000, Dundee, Scotland. 
Brieman, L., Jerome F., Olshen, R., and Stone, C. 
(1984). Classification and Regression Trees. Pa-
cific Grove: Wadsworth.  
Cucchiarini, C., Strik, H., & Boves, L. (1997a). 
Using speech recognition technology to assess 
foreign speakers' pronunciation of Dutch. Third 
international symposium on the acquisition of 
second language speech: NEW SOUNDS 97, 
Klagenfurt, Austria. 
Cucchiarini, C., Strik, S., and Boves, L. (1997b). 
Automatic evaluation of Dutch pronunciation by 
using speech recognition technology. IEEE 
Automatic Speech Recognition and Understand-
ing Workshop, Santa Barbara, CA. 
Franco, H., Abrash, V., Precoda, K., Bratt, H., 
Rao, R., and Butzberger, J. (2000). The SRI 
EduSpeak system: Recognition and pronuncia-
tion scoring for language learning. InSTiLL-
2000 (Intelligent Speech Technology in Lan-
guage Learning), Dundee, Scotland. 
Menard, S. (2001). Applied logistic regression 
analysis. Sage University Paper Series on Quan-
titative Applications in the Social Sciences 07-
106, Thousand Oaks, CA: Sage.  
North, B. (2000). The Development of a Common 
Framework Scale of Language Proficiency. 
New York, NY: Peter Lang. 
 
Steinberg, D., and Colla, P. (1997). CART -- Clas-
sification and Regression Trees. San Diego, CA: 
Salford Systems.  
Tomokiyo, L. M., and Waibel, A. (2001). Adapta-
tion methods for non-native speech. Multilin-
guality in Spoken Language Processing, 
Aalborg. 
Wang, Z., Schultz, T., and Waibel, A. (2003). 
Comparison of acoustic model adaptation tech-
niques on non-native speech. IEEE International 
Conference on Acoustics, Speech, and Signal Proc-
essing (ICASSP-2003), Hong Kong, China. 
Zechner, K., and Bejar, I. (2006). Towards Auto-
matic Scoring of Non-Native Spontaneous 
Speech. HLT-NAACL-06, New York, NY. 
Zechner, K., Higgins, D., and Xi, X. (2007). 
SpeechRater?: A Construct-Driven Approach to 
Score Spontaneous Non-Native Speech. Pro-
ceedings of the 2007 Workshop of the Interna-
tional Speech Communication Association 
(ISCA) Special Interest Group on Speech and 
Language Technology in Education (SLaTE), 
Farmington, PA, October. 
 
 
106
Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 74?79,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Towards Using Structural Events To Assess Non-native Speech
Lei Chen, Joel Tetreault, Xiaoming Xi
Educational Testing Service (ETS)
Princeton, NJ 08540, USA
{LChen,JTetreault,XXi}@ets.org
Abstract
We investigated using structural events, e.g.,
clause and disfluency structure, from tran-
scriptions of spontaneous non-native speech,
to compute features for measuring speaking
proficiency. Using a set of transcribed au-
dio files collected from the TOEFL Practice
Test Online (TPO), we conducted a sophisti-
cated annotation of structural events, includ-
ing clause boundaries and types, as well as
disfluencies. Based on words and the anno-
tated structural events, we extracted features
related to syntactic complexity, e.g., the mean
length of clause (MLC) and dependent clause
frequency (DEPC), and a feature related to
disfluencies, the interruption point frequency
per clause (IPC). Among these features, the
IPC shows the highest correlation with holis-
tic scores (r = ?0.344). Furthermore, we in-
creased the correlation with human scores by
normalizing IPC by (1) MLC (r = ?0.386),
(2) DEPC (r = ?0.429), and (3) both (r =
?0.462). In this research, the features derived
from structural events of speech transcriptions
are found to predict holistic scores measuring
speaking proficiency. This suggests that struc-
tural events estimated on speech word strings
provide a potential way for assessing non-
native speech.
1 Introduction
In the last decade, a breakthrough in speech pro-
cessing is the emergence of a lot of active research
work on automatic estimation of structural events,
e.g., sentence structure and disfluencies, on sponta-
neous speech (Shriberg et al, 2000; Liu, 2004; Os-
tendorf et al, 2008). The detected structural events
have been successfully used in many natural lan-
guage processing (NLP) applications (Ostendorf et
al., 2008).
However, the structural events in speech data
haven?t been largely utilized by the research on us-
ing automatic speech recognition (ASR) technology
to assess speech proficiency (Neumeyer et al, 2000;
Zechner et al, 2007), which mainly used cues de-
rived at the word level, such as timing information
of spoken words. The information beyond the word
level, e.g., clause/sentence structure of utterances
and disfluency structure, has not been or is poorly
represented. For example, in Zechner et al (2007),
only special words for filled pauses such as um and
uh were obtained from ASR results to represent dis-
fluencies.
Given the successful usage of structural events
on a wide range of NLP applications and the fact
that the usage of these events is missing in the auto-
matic speech assessment research, a research ques-
tion emerges: Can we use structural events of spon-
taneous speech to assess non-native speech profi-
ciency?
We will address this question in this paper. The
paper is organized as follows: Section 2 reviews
previous research. Section 3 describes our annota-
tion convention. Section 4 reports on the data col-
lection, annotation, and quality control. Section 5
reports on features based on structural event anno-
tations. Section 6 reports on our experiments. Sec-
tion 7 discusses our findings and plans for future re-
search work.
74
2 Previous Work
In the last decade, a large amount of research (Os-
tendorf et al, 2008) has been conducted on detection
of structural events, e.g., sentence structure and dis-
fluency structure, in spontaneous speech. In these
research works, the structural events were detected
with a quite high accuracy. Furthermore, the de-
tected sentence and disfluency structures have been
found to help many of the following NLP tasks,
e.g., speech parsing, information retrieval, machine
translation, and extractive speech summary (Osten-
dorf et al, 2008).
In the second language acquisition (SLA) and
child language development research fields, the lan-
guage development is measured according to flu-
ency, accuracy, and complexity (Iwashita, 2006).
The syntactic complexity of learners? writing data
has been extensively studied in the SLA commu-
nity (Ortega, 2003). Recently, this study has been
extended to the learner?s speaking data (Iwashita,
2006). Typical metrics for examining syntactic com-
plexity include: length of production unit (e.g., T-
unit, which is defined as essentially a main clause
plus any other clauses which are dependent upon
it (Hunt, 1970), clauses, verb phrases, and sen-
tences), amount of embedding, subordination and
coordination, range of structural types, and structure
sophistication.
Iwashita (2006) investigated several measures for
syntactic complexity on the data from learners of
Japanese. The author reported that some measure-
ments, e.g., T-unit length, the number of clauses per
T-unit, and the number of independent clauses per T-
Unit, were good at predicting learners? proficiency
levels.
In addition, some previous studies used measure-
ments related to disfluencies to assess speaking pro-
ficiency. For example, Lennon (1990) used a dozen
features related to speed, pauses, and several dis-
fluency markers, such as filler pauses per T-unit,
to measure four German-speaking women?s English
improvement during a half year study in England.
He found a significant change in filled pauses per
T-unit during the studying process.
The features related to syntactic complexity and
the features related to ?smoothness? (disfluency) of
speech were jointly used in some previous stud-
ies. For example, Mizera (2006) used fluency fac-
tors related to speed, voiced smoothness (frequen-
cies of repetitions or self-corrections), pauses, syn-
tactic complexity (mean length of T-units), and
accuracy, to measure speaking proficiency on 20
non-native English speakers. In this experiment,
disfluency-related factors, such as total voiced dis-
fluencies, had a high correlation with fluency (r =
?0.45). However, the syntactic complexity factor
only showed a moderate correlation (r = 0.310).
Yoon (2009) implemented an automated disfluency
detection method and found that the disfluency-
related features lead to the moderate improvement
in the automated speech proficiency scoring.
There were limitations on using the features re-
ported in these SLA studies on standard language
tests. For example, only a very limited number of
subjects (from 20 to 30 speakers) were used in these
studies. Second, the speaking content was narra-
tions of picture books or cartoon videos rather than
standard test questions. Therefore, we conducted a
study using a much larger data set obtained from real
speech tests to address these limitations.
3 Structural Event Annotation Convention
To annotate structural events of speech content, we
have developed a convention based on previous stud-
ies and our observations on the TOEFL Practice On-
line (TPO) test data. Defining clauses is a relatively
simple task; however, defining clause boundaries
and specifying which elements fall within a particu-
lar clause is a much more challenging task for spo-
ken discourse, due to the presence of grammatical
errors, fragments, repetitions, self corrections, and
conversation fillers.
Foster et al (Foster et al, 2000) review various
units for analyzing spoken language, including syn-
tactic, semantic and intonational units, and propose
a new analysis of speech unit (AS-Unit) that they
claim is appropriate for many different purposes. In
this study, we focused on clauses given the charac-
teristics of spontaneous speech. Also, we defined
clause types based on grammar books such as (Azar,
2003). The following clause types were defined:
? Simple sentence (SS) contains a subject and a
verb, and expresses a complete thought.
75
? Independent clause (I) is the main clause that
can stand along syntactically as a complete sen-
tence. It consists minimally a subject and a fi-
nite verb (a verb that shows tense, person, or
singular/plural, e.g., he goes, I went, and I was).
? Subordinate clause is a clause in a complex
sentence that cannot stand alone as a complete
sentence and that functions within the sentence
as a noun, a verb complement, an adjective or
an adverb. There are three types of subordi-
nate clauses: noun clause (NC), relative clause
that functions as an adjective (ADJ), adverbial
clause that functions as an adverb (ADV).
? Coordinate clause (CC) is a clause in a com-
pound sentence that is grammatically equiva-
lent to the main clause and that performs the
same grammatical function.
? Adverbial phrase (ADVP) is a separate clause
from the main clause that contains a non-finite
verb (a verb that does not show tense, person,
or singular/plural).
The clause boundaries and clause types were an-
notated on the word transcriptions. Round brack-
ets were used to indicate the beginning and end of a
clause. Then, the abbreviations described above for
clause types were added. Also, if a specific bound-
ary serves as the boundaries for both the local and
global clause, the abbreviation of the local clause
was followed by that of the global. Some examples
of clause boundaries and types are reported in Ta-
ble 1.
In our annotation manual, a speech disfluency
contains three parts:
? Reparandum: the speech portion that will be
repeated, corrected, or even abandoned. The
end of the reparandum is called the interruption
point (IP), which indicates the stop of a normal
fluent speech stream.
? Editing phrase: optional inserted words, e.g.,
um.
? Correction: the speech portion that repeats,
corrects, or even starts new content.
In our annotation manual, the reparandum was en-
closed by ?*?, the editing phrase was enclosed by
?%?, and the correction was enclosed by ?$?. For
example, in the following utterance, ?He is a * very
mad * % er % $ very bad $ cop?, ?very mad? was
corrected by ?very bad? and an editing phrase, er,
was inserted.
4 Data Collection and Annotation
4.1 Audio data collection and scoring
About 1300 speech responses from the TPO test
were collected and transcribed. Each item was
scored by two experienced human raters indepen-
dently using a 4-point holistic score based on the
scoring rubrics designed for the test.
In the TPO test, some tasks required test-takers to
provide information or opinions on familiar topics
based on their personal experience or background
knowledge. Others required them to summarize and
synthesize information presented in listening and/or
reading materials. Each test-taker was required to
finish six items in one test session. Each item has a
45 or 60 seconds response time.
4.2 Annotation procedure
Two annotators (who were not the human raters
mentioned above) with a linguistics background and
past linguistics annotation experience were first pre-
sented with a draft of the annotation convention.
After reading through it, the annotators, as well as
the second and third author completed four iterative
loops of rating 4 or 5 responses per meeting. All four
discussed differences in annotations and the conven-
tion was refined as needed. After the final iteration
of comparisons, the raters seemed to have very few
disagreement and thus began annotating sets of re-
sponses. Each set consisted of roughly 50-75 re-
sponses and then a kappa set of 30-50 responses
which both annotators completed. Accordingly, be-
tween the two annotators, a set comprised roughly
130 to 200 responses. Each response takes roughly
3-8 minutes to annotate. The annotators were in-
structed to listen to the corresponding audio file if
they needed the prosodic information to annotate a
particular speech disfluency event.
76
Clause type Example
SS (That?s right |SS)
I (He turned away |I) as soon as he saw me |ADV)
NC ((What he did |NC) shocked me |I)
ADJ (She is the woman (I told you about |ADJ)|I)
ADV (As soon as he saw me |ADV) (he turned away |I)
CC (I will go home |I) (and he will go to work |CC)
ADVP (While walking to class |ADVP) (I ran into a friend |I)
Table 1: Examples of clause boundary and type annotation
4.3 Evaluation of annotation
To evaluate the quality of structural event anno-
tation, we measured the inter-rater agreement on
clause boundary (CB) annotation and interruption
point (IP) of disfluencies1.
We used Cohen?s ? to calculate the annotator
agreement on each kappa set. ? is calculated on the
absence or presence of a boundary marker (either a
clause boundary (CB) or an interruption point (IP)
between consecutive words). For each consecutive
pair of words, we check for the existence of one or
more boundaries, and collapse the set into one term
?boundary? and then compute the agreement on this
reduced annotation.
In Table 2, we list the annotator agreement for
both boundary events over 4 kappa sets. The second
column refers to the number of speech responses in
the kappa set, the next two columns refer to the an-
notator agreement using the Cohen?s ? value on CB
and IP annotation results.
Set N ? CB ? IP
Set1 54 0.886 0.626
Set2 71 0.847 0.687
Set3 35 0.855 0.695
Set4 34 0.899 0.833
Table 2: Between-rater agreement of structural event an-
notation
In general, a ? of 0.8-1.0 represents excellent
agreement, 0.6-0.8 represents good agreement, and
so forth. Over each kappa set, ? for CB annota-
tions ranges between 0.8 and 0.9, which is an ex-
1Measurement on CBs and IPs can provide a rough qual-
ity measurement of annotations. In addition, doing so is more
important to us since automatic detection of these two types of
events will be investigated in future.
cellent agreement; ? for IP annotation ranges be-
tween 0.6 and 0.8, which is a good agreement. Com-
pared to annotating clauses, marking disfluencies is
more challenging. As a result, a lower between-rater
agreement is expected.
5 Features Derived On Structural Events
Based on the structural event annotations, including
clause boundaries and their types, as well as disflu-
encies, some features measuring syntactic complex-
ity and disfluency profile were derived.
Since simple sentence (SS), independent clause
(I), and conjunct clause (CC) represent a complete
idea, we treat them as an approximate to a T-unit (T).
The clauses that have no complete idea, are depen-
dent clauses (DEP), including noun clauses (N), rel-
ative clauses that function as adjective (ADJ), adver-
bial clauses (ADV), and adverbial phrases (ADVP).
The total number of clauses is a summation of the
number of T-units (T), dependent clauses (DEP), and
fragments2 (denoted as F). Therefore,
NT = NSS +NI +NCC
NDEP = NNC +NADJ +NADV +NADV P
NC = NT +NDEP +NF
Assuming Nw is the total number of words in
the speech response (without pruning speech re-
pairs), the following features, including mean length
of clause (MLC), dependent clauses per clause
(DEPC), and interruption points per clause (IPC),
are derived:
MLC = Nw/NC
2It is either a subordinate clause that does not have a cor-
responding independent clause or a string of words without a
subject or a verb that does not express a complete thought.
77
DEPC = NDEP /NC
IPC = NIP /NC
Furthermore, we elaborated the IPC feature. Dis-
fluency is a complex behavior and is influenced by
a variety of factors, such as proficiency level, speak-
ing rate, and familiarity with speaking content. The
complexity of utterances is also an important fac-
tor on the disfluency pattern. For example, Roll
et al (Roll et al, 2007) found that complexity of
expression computed based on the language?s pars-
ing tree structure influenced the frequency of disflu-
encies in their experiment on Swedish. Therefore,
since disfluency frequency was not only influenced
by the test-takers? speaking proficiency but also by
the utterance?s syntactic structure?s difficulty, we re-
duced the impact from the syntactic structure so that
we can focus on speakers? ability. For this purpose,
we normalized IPC by dividing by some features re-
lated to syntactic-structure?s complexity, including
MLC, DEPC, and both. Therefore, the following
elaborated disfluency-related features were derived:
IPCn1 = IPC/MLC
IPCn2 = IPC/DEPC
IPCn3 = IPC/MLC/DEPC
6 Experiment
For each item, two human raters rated it separately
with a score from 1 to 4. If these two scores are
consistent (the difference between two scores is ei-
ther zero or one), we put this item in an item-pool.
Finally, a total of 1, 257 audio items were included
in the pool. Following the score-handling protocol
used in the TPO test, we used the first human rater?s
score as the item score. From the obtained item-
pool, we selected speakers with more than three
items so that the averaged score per speaker can be
estimated on several items to achieve a robust score
computation3. As a result, 175 speakers4 were se-
lected.
3The mean holistic score of these speakers is 2.786, which
is close to the mean holistic score of the selected item-pool
(2.785), indicating that score distribution was kept after focus-
ing on speakers with more than three items.
4If a speaker was assigned in a Kappa set in the annotation
as described in Section 4, this speaker would have as many as 12
annotated items. Therefore, the minimum number of speakers
from the item-pool was about 105 (1257/12).
For each speaker, his or her annotations of words
and structural events were used to extract the fea-
tures described in Section 5. Then, we computed
the Pearson correlation among the obtained features
with the averaged holistic scores per speaker.
Feature r
MLC 0.211
DEPC 0.284
IPC -0.344
IPCn1 -0.386
IPCn2 -0.429
IPCn3 -0.462
Table 3: Correlation coefficients (rs) between the fea-
tures derived from structural events with human scores
averaged on test takers
Table 3 reports on the correlation coefficient
(r) between the proposed features derived from
structural events with holistic scores. Relying
on three simple structural event annotations, i.e.,
clause boundaries, dependent clauses, and interrup-
tion points in speech disfluencies, some promising
correlations between features with holistic scores
were found. Between the two syntactic complex-
ity features, the DEPC has a higher correlation with
holistic scores than the MLC (0.284 > 0.211). It ap-
pears that a measurement about clauses? embedding
profile is more informative about a speaker?s profi-
ciency level. Second, compared to features measur-
ing syntactic complexity, the feature measuring the
disfluency profile is better to predict human holis-
tic scores on this non-native data set. For example,
IPC has a r of ?0.344, which is better than the fea-
tures about clause lengths or embedding. Finally, by
jointly using the structural events related to clauses
and disfluencies, we can further achieve a further
improved r. Compared to IPC, IPCn3 has a relative
34.30% correlation increase. This is consistent with
our idea of reducing utterance-complexity?s impact
on disfluency-related features.
7 Discussion
In most current automatic speech assessment sys-
tems, features derived from recognized words, such
as delivery features about speaking rate, pause infor-
mation, and accuracy related to word identities, have
been widely used to assess non-native speech from
78
fluency and accuracy points of view. However, in-
formation beyond recognized words, e.g., the struc-
ture of clauses and disfluencies, has only received
limited attention. Although several previous SLA
studies used features derived from structural events
to measure speaking proficiency, these studies were
limited and the findings from them were difficult to
directly apply to on large-scale standard tests.
In this paper, using a large-sized data set col-
lected in the TPO speaking test, we conducted an
sophisticated annotation of structural events, includ-
ing boundaries and types of clauses and disfluen-
cies, from transcriptions of spontaneous speech test
responses. A series of features were derived from
these structural event annotations and were eval-
uated according to their correlations with holistic
scores. We found that disfluency-related features
have higher correlations to human holistic scores
than features about syntactic complexity, which con-
firms the result reported in (Mizera, 2006). In spon-
taneous speech utterances, simple syntactic structure
tends to be utilized by speakers. This is in contrast to
sophisticated syntactic structure appearing in writ-
ing. This may cause that complexity-related features
are poor at predicting fluency scores. On the other
hand, disfluencies, a pattern unique to spontaneous
speech, were found to play a more important role in
indicating speaking proficiency levels.
Although syntactic complexity features were not
highly indicative of holistic scores, they were useful
to further improve disfluency-related features? corre-
lation with holistic scores. By normalizing IPC us-
ing measurements representing syntactic complex-
ity, we can highlight contributions from speakers?
proficiency levels. Therefore, in our experiment,
IPCn3 shows a 34.30% relative improvement in its
correlation coefficient with human holistic scores
over the original IPC.
The study reported in this paper suggests promise
that structural events beyond speech recognition re-
sults can be utilized to measure non-native speaker
proficiency levels. Recently, in the NLP research
field, an increasing amount of effort has been
made on structural event detection in spontaneous
speech (Ostendorf et al, 2008). Therefore, such
progress can benefit the study of automatic estima-
tion of structural events on non-native speech data.
For our future research plan, first, we will inves-
tigate automatically detecting these structural events
from speech transcriptions and recognition hypothe-
ses. Second, the features derived from the obtained
structural events will be used to augment the features
in automatic speech assessment research to provide
a wider construct coverage than fluency and pronun-
ciation features do.
References
B. Azar. 2003. Fundamentals of English grammar.
Pearson Longman, White Plains, NY, 3rd edition.
P. Foster, A. Tonkyn, and G. Wigglesworth. 2000. Mea-
suring spoken language: A unit for all reasons. Ap-
plied Linguistics, 21(3):354.
K. W. Hunt. 1970. Syntactic maturity in school chil-
dren and adults. In Monographs of the Society for Re-
search in Child Development. University of Chicago
Press, Chicago, IL.
N. Iwashita. 2006. Syntactic complexity measures and
their relation to oral proficiency in Japanese as a for-
eign language. Language Assessment Quarterly: An
International Journal, 3(2):151?169.
P. Lennon. 1990. Investigating fluency in EFL: A quanti-
tative approach. Language Learning, 40(3):387?417.
Y. Liu. 2004. Structural Event Detection for Rich Tran-
scription of Speech. Ph.D. thesis, Purdue University.
G. J. Mizera. 2006. Working memory and L2 oral flu-
ency. Ph.D. thesis, University of Pittsburgh.
L. Neumeyer, H. Franco, V. Digalakis, and M. Weintraub.
2000. Automatic Scoring of Pronunciation Quality.
Speech Communication, 30:83?93.
L. Ortega. 2003. Syntactic complexity measures and
their relationship to L2 proficiency: A research syn-
thesis of college-level L2 writing. Applied Linguistics,
24(4):492.
M. Ostendorf et al 2008. Speech segmentation and spo-
ken document processing. Signal Processing Maga-
zine, IEEE, 25(3):59?69, May.
M. Roll, J. Frid, and M. Horne. 2007. Measuring syntac-
tic complexity in spontaneous spoken Swedish. Lan-
guage and Speech, 50(2):227.
E. Shriberg, A. Stolcke, D. Hakkani-Tur, and G. Tur.
2000. Prosody-based automatic segmentation of
speech into sentences and topics. Speech Communi-
cation, 32(1-2):127?154.
S. Yoon. 2009. Automated assessment of speech fluency
for L2 English learners. Ph.D. thesis, University of
Illinois at Urbana-Champaign.
K. Zechner, D. Higgins, and Xiaoming Xi. 2007.
SpeechRater: A Construct-Driven Approach to Scor-
ing Spontaneous Non-Native Speech. In Proc. SLaTE.
79
