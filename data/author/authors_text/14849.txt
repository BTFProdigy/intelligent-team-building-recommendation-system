Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 121?124,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Using Parallel Propbanks to enhance Word-alignments
Jinho D. Choi
Dept. of Computer Science
Univ. of Colorado at Boulder
choijd@colorado.edu
Martha Palmer
Dept. of Linguistics
Univ. of Colorado at Boulder
mpalmer@colorado.edu
Nianwen Xue
Dept. of Computer Science
Brandeis University
xuen@brandeis.edu
Abstract
This short paper describes the use of the
linguistic annotation available in paral-
lel PropBanks (Chinese and English) for
the enhancement of automatically derived
word alignments. Specifically, we sug-
gest ways to refine and expand word
alignments for verb-predicates by using
predicate-argument structures. Evalua-
tions demonstrate improved alignment ac-
curacies that vary by corpus type.
1 Introduction
Since verbs tend to be the roots of dependency re-
lations in a sentence (Palmer et al, 2005), when it
comes down to translations, finding correct map-
pings between verbs in a source and a target lan-
guage is very important. Many machine transla-
tion systems (Fraser and Marcu, 2007) use word-
alignment tools such as GIZA++ (Och and Ney,
2003) to retrieve word mappings between a source
and a target language. Although GIZA++ gives
well-structured alignments, it has limitations in
several ways. First, it is hard to verify if align-
ments generated by GIZA++ are correct. Second,
GIZA++ may not find alignments for low-frequent
words. Third, GIZA++ does not account for any
semantic information.
In this paper, we suggest a couple of ways to
enhance word-alignments for predicating expres-
sions such as verbs1. We restricted the source
and the target language to Chinese and English,
respectively. The goal is to use the linguistic
annotation available in parallel PropBanks (Xue
and Palmer, 2009) to refine and expand automatic
word-alignments. First, we check if the alignment
for each Chinese predicate, generated by GIZA++,
is also a predicate in English (Section 3). If it is,
we verify if the alignment is correct by matching
1Throughout the paper, all predicates refer to verbs.
their arguments (Section 4.1). If it is not, we find
an English predicate that has the maximum argu-
ment matching with the Chinese predicate (Sec-
tion 4.2). Finally, we evaluate the potential of the
enhanced word-alignments for providing a signif-
icant improvement over the GIZA++ baseline.
2 Parallel Corpus
We used the ?English Chinese Translation Tree-
bank? (ECTB), a parallel English-Chinese cor-
pus. In addition to the treebank syntactic struc-
ture, the corpus has also been annotated with
semantic role labels in the standard PropBank
style of Arg0, Arg1, etc., based on verb specific
frame file definitions (Xue and Palmer, 2009).
The corpus is divided into two parts: the Xin-
hua Chinese newswire with literal English trans-
lations (4,363 parallel sentences) and the Sino-
rama Chinese news magazine with non-literal En-
glish translations (12,600 parallel sentences). We
experimented with the two parts separately to
see how literal and non-literal translations affect
word-alignments.
3 Predicate Matching
For preprocessing, we ran GIZA++ on ECTB to
get word-alignments between Chinese and En-
glish. Then, for each Chinese predicate, we
checked if it is aligned to an English predicate by
using the gold-standard parallel Propbanks. Ta-
ble 1 shows how many Chinese predicates were
aligned to what kind of English words.
Only (45.3%-Xinhua, 19.1%-Sinorama) of Chi-
nese predicates were aligned to words that are
predicates in English. It is true that not all Chi-
nese verbs are supposed to be translated to verbs
in English, but that does not account for the num-
bers in Table 1. We therefore assume that there
are opportunities to enhance word-alignments for
Chinese and English predicates.
121
Alignment Xinhua Sinorama
Ch.pred? En.pred 5,842 7,643
Ch.pred? En.be 386 1,229
Ch.pred? En.else 2,489 8,726
Ch.pred? En.none 4,178 22,488
Total 12,895 40,086
Table 1: Results of predicate matching (Ch: Chi-
nese, En: English, pred: predicates, be: be-verbs,
else: non-verbs, none: no word). The numbers in-
dicate the amount of verb-tokens, not verb-types.
4 Argument Matching
For Chinese predicates aligned to English predi-
cates, we can verify the alignments by ?Top-down
argument matching?: given Chinese and English
predicates that are aligned, check if their argu-
ments are also aligned (arguments are found from
parallel Propbanks). The intuition is that if the
predicates are correctly aligned across the lan-
guages, their arguments should be aligned as well.
For Chinese predicates not aligned to any En-
glish words, we can find their potential English
alignments by ?Bottom-up argument matching?:
given a set of arguments for a such Chinese predi-
cate, find some English predicate whose set of ar-
guments has the most words aligned to words in
the Chinese arguments. If the words in the argu-
ments are mostly aligned (above a certain thresh-
old) across the languages, we suspect that the
predicates should be aligned as well.
4.1 Top-down Argument Matching (T-D)
Given a Chinese predicate pc aligned to an English
predicate pe, let Sc and Se be a set of arguments
for pc and pe, respectively. For each cai ? Sc, we
match it with some eaj ? Se that has the most
words aligned to words in cai. If such eaj ex-
ists, we count the number of aligned words, say
|cai ? eaj |; otherwise, the count is 0. Once the
matchings are done, we average the proportions
of the counts and if the average is above a certain
threshold, we consider the alignment is correct.
Let us look at the example in Table 2. Af-
ter the preprocessing, a Chinese predicate ????
is aligned to an English predicate ?set up? by
GIZA++. ???? has two arguments, Ch.Arg0 and
Ch.Arg1, retrieved from the Chinese Propbank.
For each Chinese argument, we search for some
argument of ?set? (from the English Propbank) that
? Chinese Sentence ?
: ?????????????????????
- Predicate: ??.01? set up
- Ch.Arg0: ????? those municipalities
- Ch.Arg1: ??????????
? fourteen border economic cooperation zones
? English Sentence ?
: At the same time it also sanctioned those municipalities
to set up fourteen border economic cooperation zones
- Predicate: set.03 (set up)
- En.Arg0: those municipalities
- En.Arg1: fourteen border economic cooperation zones
Table 2: Parallel sentences labelled with their se-
mantic roles
has the most words aligned. For instance, words
in Ch.Arg0, ??? ???, are aligned to ?those
municipalities? by GIZA++ so Ch.Arg0 finds
En.Arg0 as the one maximizes word-interscetions
(similar for Ch.Arg1 and En.Arg1). In this case,
the argument matchings for all pairs of arguments
are 100%, so we consider the alignment is correct.
Table 3 shows the average argument matching
scores for all pairs of Chinese and English predi-
cates. For each pair of predicates, ?macro-average?
measures the proportion of word-intersections for
each pair of Chinese and English arguments (with
the most words aligned) and averages the pro-
portions whereas ?micro-average? counts word-
intersections for all pairs of arguments (each pair
with the most words aligned) and divides it by the
total number of words in Chinese arguments.
? Sc = a set of Chinese arguments, cai ? Sc
? Se = a set of English arguments, eaj ? Se
? Macro average argument matching score
= 1|Sc|
?
?cai
(argmax(|cai ? eaj |)|cai| )
? Micro average argument matching score
=
?
?cai argmax(|cai ? eaj |)?
?cai |cai|
Xinhua Sinorama
Macro Avg. 80.55% 53.56%
Micro Avg. 83.91% 52.62%
Table 3: Average argument matching scores for
top-down argument matching
122
It is not surprising that Xinhua?s scores are
higher because the English sentences in Xinhua
are more literally translated than ones in Sinorama
so that it is easier to find correct alignments in Xin-
hua.
4.2 Bottom-Up Argument Matching (B-U)
A large portion of Chinese predicates are aligned
to no English words. For such Chinese predicate,
say pc, we check to see if there exists an English
predicate within the parallel sentence, say pe, that
is not aligned to any Chinese word and gives the
maximum micro-average score (Section 4.1) com-
pare to all other predicates in the English sen-
tence. If the micro-average score is above a certain
threshold, we align pc to pe.
The thresholds we used are 0.7 and 0.8. Thresh-
olds below 0.7 assumes too many alignments that
are incorrect and ones above 0.8 assumes too few
alignments to be useful. Table 4 shows the average
argument matching scores for alignments found by
bottom-up argument matching.
Xinhua Sinorama
Thresh. 0.7 0.8 0.7 0.8
Macro 80.74 83.99 77.70 82.86
Micro 82.63 86.46 79.45 85.07
Table 4: Average argument matching scores in
percentile for bottom-up argument matching
5 Evaluations
Evaluations are done by a Chinese-English bilin-
gual. We used a different English-Chinese paral-
lel corpus for evaluations. There are 100 paral-
lel sentences, 365 Chinese verb-tokens, and 273
Chinese verb-types in the corpus. We tested
word-alignments, refined and expanded by our ap-
proaches, on verb-types rather than verb-tokens
to avoid over-emphasizing multiple appearances
of a single type. Furthermore, we tested word-
alignments from Xinhua and Sinorama separately
to see how literal and non-literal translations affect
the outcomes.
5.1 Refining word-alignment
We used three kinds of measurements for compar-
isons: term coverage, term expansion, and align-
ment accuracy. ?Term coverage? shows how many
source terms (Chinese verb-types) are covered by
word-alignments found in each corpus. Out of
273 Chinese verb-types in the test corpus, (79-
Xinhua, 129-Sinorama) were covered by word-
alignments generated by GIZA++. ?Term expan-
sion? shows how many target terms (English verb-
types) are suggested for each of the covered source
terms. There are on average (1.77-Xinhua, 2.29-
Sinorama) English verb-types suggested for each
covered Chinese verb-type. ?Alignment accuracy?
shows how many of the suggested target terms are
correct. Among the suggested English verb-types,
(83.35%-Xinhua, 57.76%-Sinorama) were correct
on average.
The goal is to improve the alignment accu-
racy with minimum reduction of the term cov-
erage and expansion. To accomplish the goal,
we set a threshold for the T-D?s macro-average
score: for Chinese predicates aligned to English
predicates, we kept only alignments whose macro-
average scores meet or exceed a certain threshold.
The thresholds we chose are 0.4 and 0.5; lower
thresholds did not have much effect and higher
thresholds threw out too many alignments. Table 5
shows the results of three measurements with re-
spect to the thresholds (Note that all these align-
ments were generated by GIZA++).
Xinhua Sinorama
TH TC ATE AAA TC ATE AAA
0.0 79 1.77 83.35 129 2.29 57.76
0.4 76 1.72 83.54 93 1.8 65.88
0.5 76 1.68 83.71 62 1.58 78.09
Table 5: Results for alignment refinement (TH:
threshold, TC: term coverage, ATE: average term
expansion, AAA: average alignment accuracy in
percentage). The highest score for each measure-
ment is marked as bold.
As you can see, thresholds did not have much
effect on alignments found in Xinhua. This is
understandable because the translations in Xin-
hua are so literal that it was relatively easy for
GIZA++ to find correct alignments; in other
words, the alignments generated by GIZA++ were
already very accurate. However, for alignments
found in Sinorama, the average alignment accu-
racy increases radically as the threshold increases.
This implies that it is possible to refine word-
alignments found in a corpus containing many
non-literal translations by using T-D.
Notice that the term coverage for Sinorama de-
creases as the threshold increases. Considering
123
how much improvement it made for the average
alignment accuracy, we suspect that it filtered out
mostly ones that were incorrect alignments.
5.2 Expanding word-alignment
We used B-U to expand word-alignments for Chi-
nese predicates aligned to no English words. We
decided not to expand alignments for Chinese
predicates aligned to non-verb English words be-
cause GIZA++ generated alignments are more ac-
curate than ones found by B-U in general.
There are (22-Xinhua, 20-Sinorama) additional
verb-types covered by the expanded-alignments.
Note that these alignments are already filtered by
the micro-average score (Section 4.2). To refine
the alignments even more, we set a threshold on
the macro-average score as well. The thresholds
we used for the macro-average score are 0.6 and
0.7. Table 6 shows the results of the expanded-
alignments found in Xinhua and Sinorama.
Mac - 0.7 Mac - 0.8
TC ATE AAA TC ATE AAA
Mic Xinhua
0.0 22 4.27 50.38 20 3.35 57.50
0.6 21 3.9 54.76 18 3.39 63.89
0.7 19 3.47 55.26 17 3.12 61.76
Mic Sinorama
0.0 37 3.59 18.01 29 3.14 14.95
0.6 31 3.06 15.11 27 2.93 14.46
0.7 21 2.81 11.99 25 2.6 11.82
Table 6: Results for expanded-alignments found in
Xinhua and Sinorama (Mac: threshold on macro-
average score, Mic: threshold on micro-average
score)
The average alignment accuracy for Xinhua is
encouraging; it shows that B-U can expand word-
alignments for a corpus with literal translations.
The average alignment accuracy for Sinorama is
surprisingly low; it shows that B-U cannot func-
tion effectively given non-literal translations.
6 Summary and Future Works
We have demonstrated the potential for using par-
allel Propbanks to improve statistical verb transla-
tions from Chinese to English. Our B-U approach
shows promise for expanding the term-coverage
of GIZA++ alignments that are based on literal
translations. In contrast, our T-D is most effec-
tive with non-literal translations for verifying the
alignment accuracy, which has been proven diffi-
cult for GIZA++.
This is still a preliminary work but in the fu-
ture, we will try to enhance word-alignments
by using automatically labelled Propbanks, Nom-
banks (Meyers et al, 2004), Named-entity tag-
ging, and test the enhancement on bigger corpora.
Furthermore, we will also evaluate the integration
of our enhanced alignments with statistical ma-
chine translation systems.
Acknowledgments
Special thanks to Daniel Gildea, Ding Liu
(University of Rochester) who provided word-
alignments, Wei Wang (Information Sciences In-
stitute at University of Southern California) who
provided the test-corpus, and Hua Zhong (Uni-
versity of Colorado at Boulder) who performed
the evaluations. We gratefully acknowledge
the support of the National Science Foundation
Grants IIS-0325646, Domain Independent Seman-
tic Parsing, CISE-CRI-0551615, Towards a Com-
prehensive Linguistic Annotation, and a grant
from the Defense Advanced Research Projects
Agency (DARPA/IPTO) under the GALE pro-
gram, DARPA/CMO Contract No. HR0011-06-
C-0022, subcontract from BBN, Inc. Any contents
expressed in this material are those of the authors
and do not necessarily reflect the views of the Na-
tional Science Foundation.
References
Alexander Fraser and Daniel Marcu. 2007. Measuring
word alignment quality for statistical machine trans-
lation. Computational Linguistics, 33(3):293?303.
A. Meyers, R. Reeves, C. Macleod, R. Szekely,
V. Zielinska, B. Young, and R. Grishman. 2004.
The nombank project: An interim report. In HLT-
NAACL 2004 Workshop: Frontiers in Corpus Anno-
tation, pages 24?31.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Nianwen Xue and Martha Palmer. 2009. Adding se-
mantic roles to the chinese treebank. Natural Lan-
guage Engineering, 15(1):143?172.
124
Proceedings of the NAACL HLT 2010: Demonstration Session, pages 13?16,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Multilingual Propbank Annotation Tools:
Cornerstone and Jubilee
Jinho D. Choi, Claire Bonial, Martha Palmer
University of Colorado at Boulder, USA
{choijd;bonial;mpalmer}@colorado.edu
Abstract
This paper demonstrates two annotation
tools related to Propbank: Cornerstone
and Jubilee. Propbank is a corpus in
which the arguments of each verb pred-
icate are annotated with their semantic
roles. Propbank annotation also requires
the choice of a sense id for each predicate,
defined in the corresponding frameset file.
Jubilee expedites the annotation process
by displaying several resources of syntactic
and semantic information simultaneously;
easy access to each of these resources al-
lows the annotator to quickly absorb and
apply the necessary syntactic and semantic
information pertinent to each predicate for
consistent and efficient annotation. Cor-
nerstone is a user-friendly xml editor, cus-
tomized to allow frame authors to create
and edit frameset files. Both tools have
been successfully adapted to many Prop-
bank projects; they run platform indepen-
dently, are light enough to run as X11 ap-
plications and support multiple languages
such as Arabic, Chinese, English, Hindi
and Korean.
1 Introduction
Propbank is a corpus in which the arguments of
each verb predicate are annotated with their se-
mantic roles (Palmer et al, 2005). Propbank an-
notation also requires the choice of a sense id for
each predicate. Thus, for each predicate in the
Propbank, there exists a corresponding frame-
set file encompassing one or more senses of the
predicate. All frameset files are written in xml,
which is somewhat difficult to read and edit. Al-
though there already exist many xml editors,
most of them require some degree of knowledge
of xml, and none of them are specifically cus-
tomized for frameset files. This motivated the
development of our own frameset editor, Cor-
nerstone.
Jubilee is a Propbank instance editor. For
each verb predicate, we create a Propbank in-
stance that consists of the predicate?s sense id
and its arguments labeled with semantic roles.
Previously the allocation of tasks, the annota-
tion of argument labels and the frameset tagging
were all done as separate tasks. With Jubilee,
the entire annotation procedure can be done us-
ing one tool that simultaneously provides rich
syntactic information as well as comprehensive
semantic information.
Both Cornerstone and Jubilee are developed
in Java (Jdk 6.0), so they run on any plat-
form where the Java virtual machine is installed.
They are light enough to run as X11 applica-
tions. This aspect is important because Prop-
bank data are usually stored in a server, so
annotators need to update them remotely (via
ssh). One of the biggest advantages of using
these tools is that they accommodate several
languages; in fact, the tools have been used
for Propbank projects in Arabic (M.Diab et al,
2008), Chinese (Xue and Palmer, 2009), En-
glish (Palmer et al, 2005) and Hindi, and have
been tested in Korean (Han et al, 2002).
This demo paper details how to create Prop-
bank framesets in Cornerstone, and how to an-
notate Propbank instances using Jubilee. There
are two modes in which to run Cornerstone:
multi-lemma and uni-lemma mode. In multi-
lemma mode, a predicate can have multiple lem-
13
mas, whereas a predicate can have only one
lemma in uni-lemma mode. Jubilee also has
two modes: normal and gold mode. In normal
mode, annotators are allowed to view and edit
only tasks that have been claimed by themselves
or by one other annotator. In gold mode, adju-
dicators are allowed to view and edit all tasks
that have undergone at least single-annotation.
2 How to obtain the tools
Cornerstone and Jubilee are available as an open
source project on Google code.1 The webpage
gives detailed instructions of how to download,
install and launch the tools (Choi et al, 2009a;
Choi et al, 2009b).
3 Description of Cornerstone
3.1 Multi-lemma mode
Languages such as English and Hindi are ex-
pected to run in multi-lemma mode, due to the
nature of their verb predicates. In multi-lemma
mode, a predicate can have multiple lemmas
(e.g., ?run?, ?run out?, ?run up?). The xml struc-
ture of the frameset files for such langauges is
defined in a dtd file, frameset.dtd.
Figure 1 shows what appears when you open
a frameset file, run.xml, in multi-lemma mode.
The window consists of four panes: the frame-
set pane, predicate pane, roleset pane and roles
pane. The frameset pane contains a frameset
note reserved for information that pertains to all
predicate lemmas and rolesets within the frame-
set file. The predicate pane contains one or more
tabs titled by predicate lemmas that may in-
clude verb particle constructions. The roleset
pane contains tabs titled by roleset ids (e.g.,
run.01, run.02, corresponding to different senses
of the predicate) for the currently selected predi-
cate lemma (e.g., ?run?). The roles pane includes
one or more roles, which represent arguments
that the predicate requires or commonly takes
in usage.
3.2 Uni-lemma mode
Languages such as Arabic and Chinese are ex-
pected to run in uni-lemma mode. Unlike multi-
1http://code.google.com/p/propbank/
Figure 1: Open run.xml in multi-lemma mode
lemma mode, which allows a predicate to have
multiple lemmas, uni-lemma mode allows only
one lemma for a predicate. The xml structure
of the frameset files for such langauges is defined
in a dtd file, verb.dtd.
Figure 2: Open HAfaZ.xml in uni-lemma mode
Figure 2 shows what appears when you open a
frameset file, HAfaZ.xml, in uni-lemma mode.
The window consists of four panes: the verb
pane, frameset pane, frame pane and roles pane.
The verb pane contains a verb comment field
for information helpful to annotators about the
verb, as well as the attribute field, ID, which in-
dicates the predicate lemma of the verb, repre-
sented either in the Roman alphabet or charac-
ters in other languages. The frameset pane con-
tains several tabs titled by frameset ids (corre-
sponding to verb senses) for the predicate. The
frame pane contains a frame comment for op-
14
tional information about the frame and the map-
ping pane, which includes mappings between
syntactic constituents and semantic arguments.
The roles pane consists of a set of arguments
that the predicate requires or commonly takes.
4 Description of Jubilee
4.1 Normal mode
Annotators are expected to run Jubilee in
normal mode. In normal mode, annotators
are allowed to view and edit only tasks claimed
by themselves or one other annotator when
the max-number of annotators allowed is two.
Jubilee gives the option of assigning a different
max-number of annotators as well.
When you run Jubilee in normal mode, you
will see an open-dialog (Figure 3). There are
three components in the open-dialog. The
combo-box at the top shows a list of all Prop-
bank projects. Once you select a project (e.g.,
english.sample), both [New Tasks] and [My
Tasks] will be updated. [New Task] shows a
list of tasks that have either not been claimed,
or claimed by only one other annotator. [My
Tasks] shows a list of tasks that have been
claimed by the current annotator.
Figure 3: Open-dialog
Once you choose a task and click the [Enter]
button, Jubilee?s main window will be prompted
(Figure 4). There are three views available in
the main window: the treebank view, frame-
set view and argument view. By default, the
treebank view shows the first tree (in the Penn
Treebank format (Marcus et al, 1993)) in the
selected task. The frameset view displays role-
sets and allows the annotator to choose the sense
of the predicate with respect to the current tree.
The argument view contains buttons represent-
ing each of the Propbank argument labels.
Figure 4: Jubilee?s main window
4.2 Gold mode
Adjudicators are expected to run Jubilee in gold
mode. In gold mode, adjudicators are allowed to
view and edit all tasks that have undergone at
least single-annotation. When you run Jubilee
in gold mode, you will see the same open-dialog
as you saw in Figure. 3. The [New Tasks] shows
a list of tasks that have not been adjudicated,
and the [My Tasks] shows a list of tasks that
have been adjudicated. Gold mode does not al-
low adjudicators to open tasks that have not
been at least single-annotated.
5 Demonstrations
5.1 Cornerstone
We will begin by demonstrating how to view
frameset files in both multi-lemma and uni-
lemma mode. In each mode, we will open an
existing frameset file, compare its interface with
the actual xml file, and show how intuitive it is
to interact with the tool. Next, we will demon-
strate how to create and edit a new frameset file
either from scratch or using an existing frameset
file. This demonstration will reflect several ad-
vantages of using the tool. First, the xml struc-
ture is completely transparent to the frame au-
thors, so that no knowledge of xml is required to
manage the frameset files. Second, the tool au-
tomates some of the routine work for the frame
authors (e.g., assigning a new roleset/frameset
id) and gives lists of options to be chosen (e.g.,
15
a list of function tags) so that frameset creation,
and the entire annotation procedure in turn, be-
come much faster. Third, the tool checks for the
completion of required fields and formatting er-
rors so that frame authors do not have to check
them manually. Finally, the tool automatically
saves the changes so the work is never lost.
5.2 Jubilee
For the treebank view, we will compare Jubilee?s
graphical representation of the trees with the
parenthetical representation of former tools: the
clear visual representation of the phrase struc-
ture helps the annotator to better understand
the syntax of the instance and to annotate the
appropriate node within the correct span. For
the frameset view, we will detail what kind of
semantic information it provides as you choose
different rolesets. This will highlight how Ju-
bilee?s support of roleset id annotation not only
speeds up the annotation process, but also en-
sures consistent annotation because the roleset
information provides a guideline for the correct
annotation of a particular verb sense. For the
argument view, we will illustrate how to anno-
tate Propbank arguments and use the opera-
tors for concatenations and links; thereby also
demonstrating that having each of these labels
clearly visible helps the annotator to remember
and evaluate the appropriateness of each possi-
ble argument label. Finally, we will show how
intuitive it is to adjudicate the annotations in
gold mode.
6 Future work
Both Cornerstone and Jubilee have been suc-
cessfully adapted to Propbank projects in sev-
eral universities such as Brandeis University, the
University of Colorado at Boulder, and the Uni-
versity of Illinois at Urbana-Champaign. We
will continuously develop the tools by improv-
ing their functionalities through user-testing and
feedback, and also by applying them to more
languages.
Acknowledgments
Special thanks are due to Prof. Nianwen Xue of
Brandeis University for his very helpful insights
as well as Scott Cotton, the developer of RATS
and Tom Morton, the developer of WordFreak,
both previously used for PropBank annotation.
We gratefully acknowledge the support of the
National Science Foundation Grants CISE-CRI-
0551615, Towards a Comprehensive Linguistic
Annotation and CISE-CRI 0709167, Collabo-
rative: A Multi-Representational and Multi-
Layered Treebank for Hindi/Urdu, and a grant
from the Defense Advanced Research Projects
Agency (DARPA/IPTO) under the GALE pro-
gram, DARPA/CMO Contract No. HR0011-06-
C-0022, subcontract from BBN, Inc. Any opin-
ions, findings, and conclusions or recommenda-
tions expressed in this material are those of the
authors and do not necessarily reflect the views
of the National Science Foundation.
References
Jinho D. Choi, Claire Bonial, and Martha Palmer.
2009a. Cornerstone: Propbank frameset editor
guideline (version 1.3). Technical report, Institute
of Cognitive Science, the University of Colorado at
Boulder.
Jinho D. Choi, Claire Bonial, and Martha Palmer.
2009b. Jubilee: Propbank instance editor guide-
line (version 2.1). Technical report, Institute of
Cognitive Science, the University of Colorado at
Boulder.
C. Han, N. Han, E. Ko, and M. Palmer. 2002. Ko-
rean treebank: Development and evaluation. In
Proceedings of the 3rd International Conference on
Language Resources and Evaluation.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The penn treebank. Com-
putational Linguistics, 19(2):313?330.
M.Diab, A.Mansouri, M.Palmer, O.Babko-Malaya,
W Zaghouani, A.Bies, and M.Maamouri. 2008.
A pilot arabic propbank. In Proceedings of the 7th
International Conference on Language Resources
and Evaluation.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Nianwen Xue and Martha Palmer. 2009. Adding
semantic roles to the chinese treebank. Natural
Language Engineering, 15(1):143?172.
16
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 687?692,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Getting the Most out of Transition-based Dependency Parsing
Jinho D. Choi
Department of Computer Science
University of Colorado at Boulder
choijd@colorado.edu
Martha Palmer
Department of Linguistics
University of Colorado at Boulder
mpalmer@colorado.edu
Abstract
This paper suggests two ways of improving
transition-based, non-projective dependency
parsing. First, we add a transition to an exist-
ing non-projective parsing algorithm, so it can
perform either projective or non-projective
parsing as needed. Second, we present a boot-
strapping technique that narrows down dis-
crepancies between gold-standard and auto-
matic parses used as features. The new ad-
dition to the algorithm shows a clear advan-
tage in parsing speed. The bootstrapping
technique gives a significant improvement to
parsing accuracy, showing near state-of-the-
art performance with respect to other parsing
approaches evaluated on the same data set.
1 Introduction
Dependency parsing has recently gained consider-
able interest because it is simple and fast, yet pro-
vides useful information for many NLP tasks (Shen
et al, 2008; Councill et al, 2010). There are two
main dependency parsing approaches (Nivre and
McDonald, 2008). One is a transition-based ap-
proach that greedily searches for local optima (high-
est scoring transitions) and uses parse history as fea-
tures to predict the next transition (Nivre, 2003).
The other is a graph-based approach that searches
for a global optimum (highest scoring tree) from
a complete graph in which vertices represent word
tokens and edges (directed and weighted) represent
dependency relations (McDonald et al, 2005).
Lately, the usefulness of the transition-based ap-
proach has drawn more attention because it gener-
ally performs noticeably faster than the graph-based
approach (Cer et al, 2010). The transition-based ap-
proach has a worst-case parsing complexity of O(n)
for projective, and O(n2) for non-projective pars-
ing (Nivre, 2008). The complexity is lower for pro-
jective parsing because it can deterministically drop
certain tokens from the search space whereas that
is not advisable for non-projective parsing. Despite
this fact, it is possible to perform non-projective
parsing in linear time in practice (Nivre, 2009). This
is because the amount of non-projective dependen-
cies is much smaller than the amount of projective
dependencies, so a parser can perform projective
parsing for most cases and perform non-projective
parsing only when it is needed. One other advan-
tage of the transition-based approach is that it can
use parse history as features to make the next pre-
diction. This parse information helps to improve
parsing accuracy without hurting parsing complex-
ity (Nivre, 2006). Most current transition-based ap-
proaches use gold-standard parses as features dur-
ing training; however, this is not necessarily what
parsers encounter during decoding. Thus, it is desir-
able to minimize the gap between gold-standard and
automatic parses for the best results.
This paper improves the engineering of different
aspects of transition-based, non-projective depen-
dency parsing. To reduce the search space, we add a
transition to an existing non-projective parsing algo-
rithm. To narrow down the discrepancies between
gold-standard and automatic parses, we present a
bootstrapping technique. The new addition to the
algorithm shows a clear advantage in parsing speed.
The bootstrapping technique gives a significant im-
provement to parsing accuracy.
687
LEFT-POPL
( [?1|i], ?2, [j|?], E ) ? ( ?1 , ?2, [j|?], E ? {i
L
? j} )
?i 6= 0, j. i 6?? j ? @k ? ?. i? k
LEFT-ARCL
( [?1|i], ?2 , [j|?], E )? ( ?1 , [i|?2], [j|?], E ? {i
L
? j} )
?i 6= 0, j. i 6?? j
RIGHT-ARCL
( [?1|i], ?2 , [j|?], E )? ( ?1 , [i|?2], [j|?], E ? {i
L
? j} )
?i, j. i 6?? j
SHIFT
( ?1 , ?2, [j|?], E ) ? ( [?1 ? ?2|j], [ ] , ? , E )
DT: ?1 = [ ], NT: @k ? ?1. k ? j ? k ? j
NO-ARC
( [?1|i], ?2 , [j|?], E )? ( ?1 , [i|?2], [j|?], E )
default transition
Table 1: Transitions in our algorithm. For each row, the first line shows a transition and the second line shows
preconditions of the transition.
2 Reducing search space
Our algorithm is based on Choi-Nicolov?s approach
to Nivre?s list-based algorithm (Nivre, 2008). The
main difference between these two approaches is in
their implementation of the SHIFT transition. Choi-
Nicolov?s approach divides the SHIFT transition into
two, deterministic and non-deterministic SHIFT?s,
and trains the non-deterministic SHIFT with a classi-
fier so it can be predicted during decoding. Choi and
Nicolov (2009) showed that this implementation re-
duces the parsing complexity from O(n2) to linear
time in practice (a worst-case complexity is O(n2)).
We suggest another transition-based parsing ap-
proach that reduces the search space even more.
The idea is to merge transitions in Choi-Nicolov?s
non-projective algorithm with transitions in Nivre?s
projective algorithm (Nivre, 2003). Nivre?s projec-
tive algorithm has a worst-case complexity of O(n),
which is faster than any non-projective parsing al-
gorithm. Since the number of non-projective depen-
dencies is much smaller than the number of projec-
tive dependencies (Nivre and Nilsson, 2005), it is
not efficient to perform non-projective parsing for
all cases. Ideally, it is better to perform projective
parsing for most cases and perform non-projective
parsing only when it is needed. In this algorithm, we
add another transition to Choi-Nicolov?s approach,
LEFT-POP, similar to the LEFT-ARC transition in
Nivre?s projective algorithm. By adding this tran-
sition, an oracle can now choose either projective or
non-projective parsing depending on parsing states.1
1We also tried adding the RIGHT-ARC transition from
Nivre?s projective algorithm, which did not improve parsing
performance for our experiments.
Note that Nivre (2009) has a similar idea of per-
forming projective and non-projective parsing selec-
tively. That algorithm uses a SWAP transition to
reorder tokens related to non-projective dependen-
cies, and runs in linear time in practice (a worst-case
complexity is still O(n2)). Our algorithm is distin-
guished in that it does not require such reordering.
Table 1 shows transitions used in our algorithm.
All parsing states are represented as tuples (?1, ?2,
?, E), where ?1, ?2, and ? are lists of word tokens.
E is a set of labeled edges representing previously
identified dependencies. L is a dependency label and
i, j, k represent indices of their corresponding word
tokens. The initial state is ([0], [ ], [1,. . . ,n], ?). The
0 identifier corresponds to an initial token, w0, intro-
duced as the root of the sentence. The final state is
(?1, ?2, [ ], E), i.e., the algorithm terminates when
all tokens in ? are consumed.
The algorithm uses five kinds of transitions. All
transitions are performed by comparing the last to-
ken in ?1, wi, and the first token in ?, wj . Both
LEFT-POPL and LEFT-ARCL are performed when
wj is the head of wi with a dependency relation L.
The difference is that LEFT-POP removes wi from
?1 after the transition, assuming that the token is no
longer needed in later parsing states, whereas LEFT-
ARC keeps the token so it can be the head of some
token wj<k?n in ?. This wi ? wk relation causes
a non-projective dependency. RIGHT-ARCL is per-
formed whenwi is the head ofwj with a dependency
relation L. SHIFT is performed when ?1 is empty
(DT) or there is no token in ?1 that is either the head
or a dependent ofwj (NT). NO-ARC is there to move
tokens around so each token in ? can be compared
to all (or some) tokens prior to it.
688
It
1
was
2
in
3
my
4
interest
5
to
6
Root
0
see
7
you
8
SBJ
ROOT
PRD NMOD
PMOD
IM
NMOD
OBJ
Transition ?1 ?2 ? E
0 [0] [ ] [1|?] ?
1 SHIFT (NT) [?1|1] [ ] [2|?]
2 LEFT-ARC [0] [1] [2|?] E ? {1?SBJ? 2}
3 RIGHT-ARC [ ] [0|?2] [2|?] E ? {0 ?ROOT? 2}
4 SHIFT (DT) [?1|2] [ ] [3|?]
5 RIGHT-ARC [?1|1] [2] [3|?] E ? {2 ?PRD? 3}
6 SHIFT (NT) [?1|3] [ ] [4|?]
7 SHIFT (NT) [?1|4] [ ] [5|?]
8 LEFT-POP [?1|3] [ ] [5|?] E ? {4?NMOD? 5}
9 RIGHT-ARC [?1|2] [3] [5|?] E ? {3 ?PMOD? 5}
10 SHIFT (NT) [?1|5] [ ] [6|?]
11 NO-ARC [?1|3] [5] [6|?]
12 NO-ARC [?1|2] [3|?2] [6|?]
13 NO-ARC [?1|1] [2|?2] [6|?]
14 RIGHT-ARC [0] [1|?2] [6|?] E ? {1 ?NMOD? 6}
15 SHIFT (NT) [?1|6] [ ] [7|?]
16 RIGHT-ARC [?1|5] [6] [7|?] E ? {6 ?IM? 7}
17 SHIFT (NT) [?1|7] [ ] [8|?]
18 RIGHT-ARC [?1|6] [7] [8|?] E ? {7 ?OBJ? 8}
19 SHIFT (NT) [?1|8] [ ] [ ]
Table 2: Parsing states for the example sentence. After LEFT-POP is performed (#8), [w4 = my] is removed from the
search space and no longer considered in the later parsing states (e.g., between #10 and #11).
During training, the algorithm checks for the pre-
conditions of all transitions and generates training
instances with corresponding labels. During decod-
ing, the oracle decides which transition to perform
based on the parsing states. With the addition of
LEFT-POP, the oracle can choose either projective
or non-projective parsing by selecting LEFT-POP or
LEFT-ARC, respectively. Our experiments show that
this additional transition improves both parsing ac-
curacy and speed. The advantage derives from im-
proving the efficiency of the choice mechanism; it is
now simply a transition choice and requires no addi-
tional processing.
3 Bootstrapping automatic parses
Transition-based parsing has the advantage of using
parse history as features to make the next prediction.
In our algorithm, when wi and wj are compared,
subtree and head information of these tokens is par-
tially provided by previous parsing states. Graph-
based parsing can also take advantage of using parse
information. This is done by performing ?higher-
order parsing?, which is shown to improve parsing
accuracy but also increase parsing complexity (Car-
reras, 2007; Koo and Collins, 2010).2 Transition-
based parsing is attractive because it can use parse
information without increasing complexity (Nivre,
2006). The qualification is that parse information
provided by gold-standard trees during training is
not necessarily the same kind of information pro-
vided by automatically parsed trees during decod-
ing. This can confuse a statistical model trained only
on the gold-standard trees.
To reduce the gap between gold-standard and au-
tomatic parses, we use bootstrapping on automatic
parses. First, we train a statistical model using gold-
2Second-order, non-projective, graph-based dependency
parsing is NP-hard without performing approximation.
689
standard trees. Then, we parse the training data us-
ing the statistical model. During parsing, we ex-
tract features for each parsing state, consisting of
automatic parse information, and generate a train-
ing instance by joining the features with the gold-
standard label. The gold-standard label is achieved
by comparing the dependency relation between wi
and wj in the gold-standard tree. When the parsing
is done, we train a different model using the training
instances induced by the previous model. We repeat
the procedure until a stopping criteria is met.
The stopping criteria is determined by performing
cross-validation. For each stage, we perform cross-
validation to check if the average parsing accuracy
on the current cross-validation set is higher than the
one from the previous stage. We stop the procedure
when the parsing accuracy on cross-validation sets
starts decreasing. Our experiments show that this
simple bootstrapping technique gives a significant
improvement to parsing accuracy.
4 Related work
Daume? et al (2009) presented an algorithm, called
SEARN, for integrating search and learning to solve
complex structured prediction problems. Our boot-
strapping technique can be viewed as a simplified
version of SEARN. During training, SEARN itera-
tively creates a set of new cost-sensitive examples
using a known policy. In our case, the new examples
are instances containing automatic parses induced
by the previous model. Our technique is simpli-
fied because the new examples are not cost-sensitive.
Furthermore, SEARN interpolates the current policy
with the previous policy whereas we do not per-
form such interpolation. During decoding, SEARN
generates a sequence of decisions and makes a fi-
nal prediction. In our case, the decisions are pre-
dicted dependency relations and the final prediction
is a dependency tree. SEARN has been successfully
adapted to several NLP tasks such as named entity
recognition, syntactic chunking, and POS tagging.
To the best of our knowledge, this is the first time
that this idea has been applied to transition-based
parsing and shown promising results.
Zhang and Clark (2008) suggested a transition-
based projective parsing algorithm that keeps B dif-
ferent sequences of parsing states and chooses the
one with the best score. They use beam search and
show a worst-case parsing complexity ofO(n) given
a fixed beam size. Similarly to ours, their learn-
ing mechanism using the structured perceptron al-
gorithm involves training on automatically derived
parsing states that closely resemble potential states
encountered during decoding.
5 Experiments
5.1 Corpora and learning algorithm
All models are trained and tested on English and
Czech data using automatic lemmas, POS tags,
and feats, as distributed by the CoNLL?09 shared
task (Hajic? et al, 2009). We use Liblinear L2-L1
SVM for learning (L2 regularization, L1 loss; Hsieh
et al (2008)). For our experiments, we use the fol-
lowing learning parameters: c = 0.1 (cost), e = 0.1
(termination criterion), B = 0 (bias).
5.2 Accuracy comparisons
First, we evaluate the impact of the LEFT-POP tran-
sition we add to Choi-Nicolov?s approach. To make
a fair comparison, we implemented both approaches
and built models using the exact same feature set.
The ?CN? and ?Our? rows in Table 3 show accuracies
achieved by Choi-Nicolov?s and our approaches, re-
spectively. Our approach shows higher accuracies
for all categories. Next, we evaluate the impact of
our bootstrapping technique. The ?Our+? row shows
accuracies achieved by our algorithm using the boot-
strapping technique. The improvement from ?Our?
to ?Our+? is statistically significant for all categories
(McNemar, p < .0001). The improvment is even
more significant in a language like Czech for which
parsers generally perform more poorly.
English Czech
LAS UAS LAS UAS
CN 88.54 90.57 78.12 83.29
Our 88.62 90.66 78.30 83.47
Our+ 89.15? 91.18? 80.24? 85.24?
Merlo 88.79 (3) - 80.38 (1) -
Bohnet 89.88 (1) - 80.11 (2) -
Table 3: Accuracy comparisons between different pars-
ing approaches (LAS/UAS: labeled/unlabeled attachment
score). ? indicates a statistically significant improvement.
(#) indicates an overall rank of the system in CoNLL?09.
690
Finally, we compare our work against other state-of-
the-art systems. For the CoNLL?09 shared task, Ges-
mundo et al (2009) introduced the best transition-
based system using synchronous syntactic-semantic
parsing (?Merlo?), and Bohnet (2009) introduced the
best graph-based system using a maximum span-
ning tree algorithm (?Bohnet?). Our approach shows
quite comparable results with these systems.3
5.3 Speed comparisons
Figure 1 shows average parsing speeds for each
sentence group in both English and Czech eval-
uation sets (Table 4). ?Nivre? is Nivre?s swap
algorithm (Nivre, 2009), of which we use the
implementation from MaltParser (maltparser.
org). The other approaches are implemented in
our open source project, called ClearParser (code.
google.com/p/clearparser). Note that fea-
tures used in MaltParser have not been optimized
for these evaluation sets. All experiments are tested
on an Intel Xeon 2.57GHz machine. For general-
ization, we run five trials for each parser, cut off
the top and bottom speeds, and average the middle
three. The loading times for machine learning mod-
els are excluded because they are independent from
the parsing algorithms. The average parsing speeds
are 2.86, 2.69, and 2.29 (in milliseconds) for Nivre,
CN, and Our+, respectively. Our approach shows
linear growth all along, even for the sentence groups
where some approaches start showing curves.
   0 10 20 30 40 50 60 70
   
   2
6
10
14
18
22
Sentence length
Par
sing
 spe
ed 
(in 
ms)
Our+
CN
Nivre
Figure 1: Average parsing speeds with respect to sentence
groups in Table 4.
3Later, ?Merlo? and ?Bohnet? introduced more advanced
systems, showing some improvements over their previous ap-
proaches (Titov et al, 2009; Bohnet, 2010).
< 10 < 20 < 30 < 40 < 50 < 60 < 70
1,415 2,289 1,714 815 285 72 18
Table 4: # of sentences in each group, extracted from both
English/Czech evaluation sets. ?< n? implies a group
containing sentences whose lengths are less than n.
We also measured average parsing speeds for ?Our?,
which showed a very similar growth to ?Our+?. The
average parsing speed of ?Our? was 2.20 ms; it per-
formed slightly faster than ?Our+? because it skipped
more nodes by performing more non-deterministic
SHIFT?s, which may or may not have been correct
decisions for the corresponding parsing states.
It is worth mentioning that the curve shown by
?Nivre? might be caused by implementation details
regarding feature extraction, which we included as
part of parsing. To abstract away from these im-
plementation details and focus purely on the algo-
rithms, we would need to compare the actual num-
ber of transitions performed by each parser, which
will be explored in future work.
6 Conclusion and future work
We present two ways of improving transition-based,
non-projective dependency parsing. The additional
transition gives improvements to both parsing speed
and accuracy, showing a linear time parsing speed
with respect to sentence length. The bootstrapping
technique gives a significant improvement to parsing
accuracy, showing near state-of-the-art performance
with respect to other parsing approaches. In the fu-
ture, we will test the robustness of these approaches
in more languages.
Acknowledgments
We gratefully acknowledge the support of the Na-
tional Science Foundation Grants CISE-IIS-RI-0910992,
Richer Representations for Machine Translation, a sub-
contract from the Mayo Clinic and Harvard Children?s
Hospital based on a grant from the ONC, 90TR0002/01,
Strategic Health Advanced Research Project Area 4: Nat-
ural Language Processing, and a grant from the Defense
Advanced Research Projects Agency (DARPA/IPTO) un-
der the GALE program, DARPA/CMO Contract No.
HR0011-06-C-0022, subcontract from BBN, Inc. Any
opinions, findings, and conclusions or recommendations
expressed in this material are those of the authors and do
not necessarily reflect the views of the National Science
Foundation.
691
References
Bernd Bohnet. 2009. Efficient parsing of syntactic and
semantic dependency structures. In Proceedings of the
13th Conference on Computational Natural Language
Learning: Shared Task (CoNLL?09), pages 67?72.
Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In The 23rd In-
ternational Conference on Computational Linguistics
(COLING?10).
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In Proceedings of
the CoNLL Shared Task Session of EMNLP-CoNLL?07
(CoNLL?07), pages 957?961.
Daniel Cer, Marie-Catherine de Marneffe, Daniel Juraf-
sky, and Christopher D. Manning. 2010. Parsing
to stanford dependencies: Trade-offs between speed
and accuracy. In Proceedings of the 7th International
Conference on Language Resources and Evaluation
(LREC?10).
Jinho D. Choi and Nicolas Nicolov. 2009. K-best, lo-
cally pruned, transition-based dependency parsing us-
ing robust risk minimization. In Recent Advances in
Natural Language Processing V, pages 205?216. John
Benjamins.
Isaac G. Councill, Ryan McDonald, and Leonid Ve-
likovich. 2010. What?s great and what?s not: Learn-
ing to classify the scope of negation for improved sen-
timent analysis. In Proceedings of the Workshop on
Negation and Speculation in Natural Language Pro-
cessing (NeSp-NLP?10), pages 51?59.
Hal Daume?, Iii, John Langford, and Daniel Marcu. 2009.
Search-based structured prediction. Machine Learn-
ing, 75(3):297?325.
Andrea Gesmundo, James Henderson, Paola Merlo, and
Ivan Titov. 2009. A latent variable model of syn-
chronous syntactic-semantic parsing for multiple lan-
guages. In Proceedings of the 13th Conference on
Computational Natural Language Learning: Shared
Task (CoNLL?09), pages 37?42.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The conll-2009
shared task: Syntactic and semantic dependencies in
multiple languages. In Proceedings of the 13th Con-
ference on Computational Natural Language Learning
(CoNLL?09): Shared Task, pages 1?18.
Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin, S. Sathiya
Keerthi, and S. Sundararajan. 2008. A dual coordinate
descent method for large-scale linear svm. In Proceed-
ings of the 25th international conference on Machine
learning (ICML?08), pages 408?415.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics (ACL?10).
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings of
the Conference on Human Language Technology and
Empirical Methods in Natural Language Processing
(HLT-EMNLP?05), pages 523?530.
Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency parsers.
In Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies (ACL:HLT?08), pages 950?958.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL?05), pages 99?106.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of the
8th International Workshop on Parsing Technologies
(IWPT?03), pages 23?25.
Joakim Nivre. 2006. Inductive Dependency Parsing.
Springer.
Joakim Nivre. 2008. Algorithms for deterministic incre-
mental dependency parsing. Computational Linguis-
tics, 34(4):513?553.
Joakim Nivre. 2009. Non-projective dependency parsing
in expected linear time. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP (ACL-IJCNLP?09),
pages 351?359.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (ACL:HLT?08), pages 577?585.
Ivan Titov, James Henderson, Paola Merlo, and Gabriele
Musillo. 2009. Online graph planarisation for syn-
chronous parsing of semantic and syntactic depen-
dencies. In Proceedings of the 21st International
Joint Conference on Artificial Intelligence (IJCAI?09),
pages 1562?1567.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graph-
based and transition-based dependency parsing using
beam-search. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP?08), pages 562?571.
692
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 363?367,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Fast and Robust Part-of-Speech Tagging Using Dynamic Model Selection
Jinho D. Choi
Department of Computer Science
University of Colorado Boulder
choijd@colorado.edu
Martha Palmer
Department of Linguistics
University of Colorado Boulder
mpalmer@colorado.edu
Abstract
This paper presents a novel way of improv-
ing POS tagging on heterogeneous data. First,
two separate models are trained (generalized
and domain-specific) from the same data set
by controlling lexical items with different doc-
ument frequencies. During decoding, one of
the models is selected dynamically given the
cosine similarity between each sentence and
the training data. This dynamic model selec-
tion approach, coupled with a one-pass, left-
to-right POS tagging algorithm, is evaluated
on corpora from seven different genres. Even
with this simple tagging algorithm, our sys-
tem shows comparable results against other
state-of-the-art systems, and gives higher ac-
curacies when evaluated on a mixture of the
data. Furthermore, our system is able to tag
about 32K tokens per second. We believe that
this model selection approach can be applied
to more sophisticated tagging algorithms and
improve their robustness even further.
1 Introduction
When it comes to POS tagging, two things must be
checked. First, a POS tagger needs to be tested for
its robustness in handling heterogeneous data.1 Sta-
tistical POS taggers perform very well when their
training and testing data are from the same source,
achieving over 97% tagging accuracy (Toutanova et
al., 2003; Gime?nez and Ma`rquez, 2004; Shen et
al., 2007). However, the performance degrades in-
creasingly as the discrepancy between the training
1We use the term ?heterogeneous data? as a mixture of data
collected from several different sources.
and testing data gets larger. Thus, to ensure robust-
ness, a tagger needs to be evaluated on several dif-
ferent kinds of data. Second, a POS tagger should be
tested for its speed. POS tagging is often performed
as a pre-processing step to other tasks (e.g., pars-
ing, chunking) and it should not be a bottleneck for
those tasks. Moreover, recent NLP tasks deal with
very large-scale data where tagging speed is critical.
To improve robustness, we first train two separate
models; one is optimized for a general domain and
the other is optimized for a domain specific to the
training data. During decoding, we dynamically se-
lect one of the models by measuring similarities be-
tween input sentences and the training data. Our hy-
pothesis is that the domain-specific and generalized
models perform better for sentences similar and not
similar to the training data, respectively. In this pa-
per, we describe how to build both models using the
same training data and select an appropriate model
given input sentences during decoding. Each model
uses a one-pass, left-to-right POS tagging algorithm.
Even with the simple tagging algorithm, our system
gives results that are comparable to two other state-
of-the-art systems when coupled with this dynamic
model selection approach. Furthermore, our system
shows noticeably faster tagging speed compared to
the other two systems.
For our experiments, we use corpora from seven
different genres (Weischedel et al, 2011; Nielsen et
al., 2010). This allows us to check the performance
of each system on different kinds of data when run
individually or selectively. To the best of our knowl-
edge, this is the first time that a POS tagger has been
evaluated on such a wide variety of data in English.
363
2 Approach
2.1 Training generalized and domain-specific
models using document frequency
Consider training data as a collection of documents
where each document contains sentences focusing
on a similar topic. For instance, in the Wall Street
Journal corpus, a document can be an individual file
or all files within each section.2 To build a gener-
alized model, lexical features (e.g., n-gram word-
forms) that are too specific to individual documents
should be avoided so that a classifier can place more
weight on features common to all documents.
To filter out these document-specific features, a
threshold is set for the document frequency of each
lowercase simplified word-form (LSW) in the train-
ing data. A simplified word-form (SW) is derived by
applying the following regular expressions sequen-
tially to the original word-form, w. ?replaceAll? is a
function that replaces all matches of the regular ex-
pression in w (the 1st parameter) with the specific
string (the 2nd parameter). In a simplified word, all
numerical expressions are replaced with 0.
1. w.replaceAll(\d%, 0) (e.g., 1% ? 0)
2. w.replaceAll(\$\d, 0) (e.g., $1 ? 0)
3. w.replaceAll(?\.\d, 0) (e.g., .1 ? 0)
4. w.replaceAll(\d(,|:|-|\/|\.)\d, 0)
(e.g., 1,2|1:2|1-2|1/2|1.2 ? 0)
5. w.replaceAll(\d+, 0) (e.g., 1234 ? 0)
A LSW is a decapitalized SW. Given a set of LSW?s
whose document frequencies are greater than a cer-
tain threshold, a model is trained by using only lexi-
cal features associated with these LSW?s. For a gen-
eralized model, we use a threshold of 2, meaning
that only lexical features whose LSW?s occur in at
least 3 documents of the training data are used. For
a domain-specific model, we use a threshold of 1.
The generalized and domain-specific models are
trained separately; their learning parameters are op-
timized by running n-fold cross-validation where n
is the total number of documents in the training data
and grid search on Liblinear parameters c and B (see
Section 2.4 for more details about the parameters).
2For our experiments, we treat each section of the Wall
Street Journal as one document.
2.2 Dynamic model selection during decoding
Once both generalized and domain-specific models
are trained, alternative approaches can be adapted
for decoding. One is to run both models and merge
their outputs. This approach can produce output that
is potentially more accurate than output from either
model, but takes longer to decode because the merg-
ing cannot be processed until both models are fin-
ished. Instead, we take an alternative approach, that
is to select one of the models dynamically given the
input sentence. If the model selection is done ef-
ficiently, this approach runs as fast as running just
one model, yet can give more robust performance.
The premise of this dynamic model selection is
that the domain-specific model performs better for
input sentences similar to its training space, whereas
the generalized model performs better for ones that
are dissimilar. To measure similarity, a set of SW?s,
say T , used for training the domain-specific model
is collected. During decoding, a set of SW?s in each
sentence, say S, is collected. If the cosine similarity
between T and S is greater than a certain threshold,
the domain-specific model is selected for decoding;
otherwise, the generalized model is selected.
0.0710 0.02 0.04
190
0
40
80
120
160
Cosine Similarity
Occ
urre
nce
5%
Figure 1: Cosine similarity distribution: the y-axis shows
the number of occurrences for each cosine similarity dur-
ing cross-validation.
The threshold is derived automatically by running
cross-validation; for each fold, both models are run
simultaneously and cosine similarities of sentences
on which the domain-specific model performs bet-
ter are extracted. Figure 1 shows the distribution
of cosine similarities extracted during our cross-
validation. Given the cosine similarity distribution,
the similarity at the first 5% area (in this case, 0.025)
is taken as the threshold.
364
2.3 Tagging algorithm and features
Each model uses a one-pass, left-to-right POS tag-
ging algorithm. The motivation is to analyze how
dynamic model selection works with a simple algo-
rithm first and then apply it to more sophisticated
ones later (e.g., bidirectional tagging algorithm).
Our feature set (Table 1) is inspired by Gime?nez
and Ma`rquez (2004) although ambiguity classes are
derived selectively for our case. Given a word-form,
we count how often each POS tag is used with the
form and keep only ones above a certain threshold.
For both generalized and domain-specific models, a
threshold of 0.7 is used, which keeps only POS tags
used with their forms over 70% of the time. From
our experiments, we find this to be more useful than
expanding ambiguity classes with lower thresholds.
Lexical
fi?{0,1,2,3}, (mi?2,i?1), (mi?1,i), (mi?1,i+1),
(mi,i+1), (mi+1,i+2), (mi?2,i?1,i), (mi?1,i,i+1),
(mi,i+1,i+2), (mi?2,i?1,i+1), (mi?1,i+1,i+2)
POS
pi?{3,2,1}, ai+{0,1,2,3}, (pi?2,i?1), (ai+1,i+2),
(pi?1, ai+1), (pi?2, pi?1, ai), (pi?2, pi?1, ai+1),
(pi?1, ai, ai+1), (pi?1, ai+1, ai+2)
Affix c:1, c:2, c:3, cn:, cn?1:, cn?2:, cn?3:
Binary
initial uppercase, all uppercase/lowercase,
contains 1/2+ capital(s) not at the beginning,
contains a (period/number/hyphen)
Table 1: Feature templates. i: the index of the current
word, f : SW, m: LSW, p: POS, a: ambiguity class, c?:
character sequence in wi (e.g., c:2: the 1st and 2nd char-
acters of wi, cn?1:: the n-1?th and n?th characters of wi).
See Gime?nez and Ma`rquez (2004) for more details.
2.4 Machine learning
Liblinear L2-regularization, L1-loss support vector
classification is used for our experiments (Hsieh et
al., 2008). From several rounds of cross-validation,
learning parameters of (c = 0.2, e = 0.1, B = 0.4) and
(c = 0.1, e = 0.1, B = 0.9) are found for the gener-
alized and domain-specific models, respectively (c:
cost, e: termination criterion, B: bias).
3 Related work
Toutanova et al (2003) introduced a POS tagging
algorithm using bidirectional dependency networks,
and showed the best contemporary results. Gime?nez
and Ma`rquez (2004) used one-pass, left-to-right
and right-to-left combined tagging algorithm and
achieved near state-of-the-art results. Shen et al
(2007) presented a tagging approach using guided
learning for bidirectional sequence classification and
showed current state-of-the-art results.3
Our individual models (generalized and domain-
specific) are similar to Gime?nez and Ma`rquez (2004)
in that we use a subset of their features and take one-
pass, left-to-right tagging approach, which is a sim-
pler version of theirs. However, we use Liblinear for
learning, which trains much faster than their classi-
fier, Support Vector Machines.
4 Experiments
4.1 Corpora
For training, sections 2-21 of the Wall Street Jour-
nal (WSJ) from OntoNotes v4.0 (Weischedel et al,
2011) are used. The entire training data consists of
30,060 sentences with 731,677 tokens. For evalua-
tion, corpora from seven different genres are used:
the MSNBC broadcasting conversation (BC), the
CNN broadcasting news (BN), the Sinorama news
magazine (MZ), the WSJ newswire (NW), and the
GALE web-text (WB), all from OntoNotes v4.0. Ad-
ditionally, the Mipacq clinical notes (CN) and the
Medpedia articles (MD) are used for evaluation of
medical domains (Nielsen et al, 2010). Table 2
shows distributions of these evaluation sets.
4.2 Accuracy comparisons
Our models are compared with two other state-of-
the-art systems, the Stanford tagger (Toutanova et
al., 2003) and the SVMTool (Gime?nez and Ma`rquez,
2004). Both systems are trained with the same train-
ing data and use configurations optimized for their
best reported results. Tables 3 and 4 show tagging
accuracies of all tokens and unknown tokens, re-
spectively. Our individual models (Models D and
G) give comparable results to the other systems.
Model G performs better than Model D for BC, CN,
and MD, which are very different from the WSJ.
This implies that the generalized model shows its
strength in tagging data that differs from the train-
ing data. The dynamic model selection approach
(Model S) shows the most robust results across gen-
res, although Models D and G still can perform
3Some semi-supervised and domain-adaptation approaches
using external data had shown better performance (Daume III,
2007; Spoustova? et al, 2009; S?gaard, 2011).
365
BC BN CN MD MZ NW WB Total
Source MSNBC CNN Mipacq Medpedia Sinorama WSJ ENG -
Sentences 2,076 1,969 3,170 1,850 1,409 1,640 1,738 13,852
All tokens 31,704 31,328 35,721 34,022 32,120 39,590 34,707 239,192
Unknown tokens 3,077 1,284 6,077 4,755 2,663 983 2,609 21,448
Table 2: Distributions of evaluation sets. The Total column indicates a mixture of data from all genres.
BC BN CN MD MZ NW WB Total
Model D 91.81 95.27 87.36 90.74 93.91 97.45 93.93 92.97
Model G 92.65 94.82 88.24 91.46 93.24 97.11 93.51 93.05
Model S 92.26 95.13 88.18 91.34 93.88 97.46 93.90 93.21
G over D 50.63 36.67 68.80 40.22 21.43 9.51 36.02 41.74
Stanford 87.71 95.50 88.49 90.86 92.80 97.42 94.01 92.50
SVMTool 87.82 95.13 87.86 90.54 92.94 97.31 93.99 92.32
Table 3: Tagging accuracies of all tokens (in %). Models D and G indicate domain-specific and generalized models,
respectively and Model S indicates the dynamic model selection approach. ?G over D? shows how often Model G is
selected over Model D using the dynamic selection (in %).
BC BN CN MD MZ NW WB Total
Model S 60.97 77.73 68.69 67.30 75.97 88.40 76.27 70.54
Stanford 19.24 87.31 71.20 64.82 66.28 88.40 78.15 64.32
SVMTool 19.08 78.35 66.51 62.94 65.23 86.88 76.47 47.65
Table 4: Tagging accuracies of unknown tokens (in %).
better for individual genres (except for NW, where
Model S performs better than any other model).
For both all and unknown token experiments,
Model S performs better than the other systems
when evaluated on a mixture of the data (the Total
column). The differences are statistically significant
for both experiments (McNemar?s test, p < .0001).
The Stanford tagger gives significantly better results
for unknown tokens in BN; we suspect that this is
where their bidirectional tagging algorithm has an
advantage over our simple left-to-right algorithm.
4.3 Speed comparisons
Tagging speeds are measured by running each sys-
tem on the mixture of all data. Our system and the
Stanford system are both written in Java; the Stan-
ford tagger provides APIs that allow us to make fair
comparisons between the two systems. The SVM-
Tool is written in Perl, so there is a systematic dif-
ference between the SVMTool and our system.
Table 5 shows speed comparisons between these
systems. All experiments are evaluated on an In-
tel Xeon 2.57GHz machine. Our system tags about
32K tokens per second (0.03 milliseconds per to-
ken), which includes run-time for both POS tagging
and model selection.
Stanford SVMTool Model S
tokens / sec. 421 1,163 31,914
Table 5: Tagging speeds.
5 Conclusion
We present a dynamic model selection approach that
improves the robustness of POS tagging on hetero-
geneous data. We believe that this approach can
be applied to more sophisticated algorithms and im-
prove their robustness even further. Our system also
shows noticeably faster tagging speed against two
other state-of-the-art systems. For future work, we
will experiment with more diverse training and test-
ing data and also more sophisticated algorithms.
Acknowledgments
This work was supported by the SHARP program
funded by ONC: 90TR0002/01. The content is
solely the responsibility of the authors and does not
necessarily represent the official views of the ONC.
366
References
Hal Daume III. 2007. Frustratingly Easy Domain Adap-
tation. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
ACL?07, pages 256?263.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2004. SVMTool: A
general POS tagger generator based on Support Vec-
tor Machines. In Proceedings of the 4th International
Conference on Language Resources and Evaluation,
LREC?04.
Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin, S. Sathiya
Keerthi, and S. Sundararajan. 2008. A Dual Coordi-
nate Descent Method for Large-scale Linear SVM. In
Proceedings of the 25th international conference on
Machine learning, ICML?08, pages 408?415.
Rodney D. Nielsen, James Masanz, Philip Ogren, Wayne
Ward, James H. Martin, Guergana Savova, and Martha
Palmer. 2010. An architecture for complex clinical
question answering. In Proceedings of the 1st ACM
International Health Informatics Symposium, IHI?10,
pages 395?399.
Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
Guided Learning for Bidirectional Sequence Classi-
fication. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
ACL?07, pages 760?767.
Anders S?gaard. 2011. Semi-supervised condensed
nearest neighbor for part-of-speech tagging. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, ACL?11, pages 48?52.
Drahom??ra ?johanka? Spoustova?, Jan Hajic?, Jan Raab,
and Miroslav Spousta. 2009. Semi-supervised Train-
ing for the Averaged Perceptron POS Tagger. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics, EACL?09, pages 763?771.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Network.
In Proceedings of the Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology,
NAACL?03, pages 173?180.
Ralph Weischedel, Eduard Hovy, Martha Palmer, Mitch
Marcus, Robert Belvin, Sameer Pradhan, Lance
Ramshaw, and Nianwen Xue. 2011. OntoNotes: A
Large Training Corpus for Enhanced Processing. In
Joseph Olive, Caitlin Christianson, and John McCary,
editors, Handbook of Natural Language Processing
and Machine Translation. Springer.
367
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1052?1062,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Transition-based Dependency Parsing with Selectional Branching
Jinho D. Choi
Department of Computer Science
University of Massachusetts Amherst
Amherst, MA, 01003, USA
jdchoi@cs.umass.edu
Andrew McCallum
Department of Computer Science
University of Massachusetts Amherst
Amherst, MA, 01003, USA
mccallum@cs.umass.edu
Abstract
We present a novel approach, called selec-
tional branching, which uses confidence es-
timates to decide when to employ a beam,
providing the accuracy of beam search at
speeds close to a greedy transition-based
dependency parsing approach. Selectional
branching is guaranteed to perform a fewer
number of transitions than beam search yet
performs as accurately. We also present a
new transition-based dependency parsing
algorithm that gives a complexity of O(n)
for projective parsing and an expected lin-
ear time speed for non-projective parsing.
With the standard setup, our parser shows
an unlabeled attachment score of 92.96%
and a parsing speed of 9 milliseconds per
sentence, which is faster and more accurate
than the current state-of-the-art transition-
based parser that uses beam search.
1 Introduction
Transition-based dependency parsing has gained
considerable interest because it runs fast and per-
forms accurately. Transition-based parsing gives
complexities as low as O(n) and O(n2) for projec-
tive and non-projective parsing, respectively (Nivre,
2008).1 The complexity is lower for projective pars-
ing because a parser can deterministically skip to-
kens violating projectivity, while this property is
not assumed for non-projective parsing. Nonethe-
less, it is possible to perform non-projective parsing
in expected linear time because the amount of non-
projective dependencies is notably smaller (Nivre
and Nilsson, 2005) so a parser can assume projec-
tivity for most cases while recognizing ones for
which projectivity should not be assumed (Nivre,
2009; Choi and Palmer, 2011).
1We refer parsing approaches that produce only projective
dependency trees as projective parsing and both projective and
non-projective dependency trees as non-projective parsing.
Greedy transition-based dependency parsing has
been widely deployed because of its speed (Cer et
al., 2010); however, state-of-the-art accuracies have
been achieved by globally optimized parsers using
beam search (Zhang and Clark, 2008; Huang and
Sagae, 2010; Zhang and Nivre, 2011; Bohnet and
Nivre, 2012). These approaches generate multiple
transition sequences given a sentence, and pick one
with the highest confidence. Coupled with dynamic
programming, transition-based dependency parsing
with beam search can be done very efficiently and
gives significant improvement to parsing accuracy.
One downside of beam search is that it always
uses a fixed size of beam even when a smaller size
of beam is sufficient for good results. In our exper-
iments, a greedy parser performs as accurately as a
parser that uses beam search for about 64% of time.
Thus, it is preferred if the beam size is not fixed but
proportional to the number of low confidence pre-
dictions that a greedy parser makes, in which case,
fewer transition sequences need to be explored to
produce the same or similar parse output.
We first present a new transition-based parsing
algorithm that gives a complexity of O(n) for pro-
jective parsing and an expected linear time speed
for non-projective parsing. We then introduce se-
lectional branching that uses confidence estimates
to decide when to employ a beam. With our new ap-
proach, we achieve a higher parsing accuracy than
the current state-of-the-art transition-based parser
that uses beam search and a much faster speed.
2 Transition-based dependency parsing
We introduce a transition-based dependency pars-
ing algorithm that is a hybrid between Nivre?s arc-
eager and list-based algorithms (Nivre, 2003; Nivre,
2008). Nivre?s arc-eager is a projective parsing al-
gorithm showing a complexity of O(n). Nivre?s
list-based algorithm is a non-projective parsing al-
gorithm showing a complexity of O(n2). Table 1
shows transitions in our algorithm. The top 4 and
1052
Transition Current state ? Resulting state
LEFTl-REDUCE ( [?|i], ?, [j|?], A ) ? ( ?, ?, [j|?], A ? {i l? j} )
RIGHTl-SHIFT ( [?|i], ?, [j|?], A ) ? ( [?|i|?|j], [ ], ?, A ? {i l? j} )
NO-SHIFT ( [?|i], ?, [j|?], A ) ? ( [?|i|?|j], [ ], ?, A )
NO-REDUCE ( [?|i], ?, [j|?], A ) ? ( ?, ?, [j|?], A )
LEFTl-PASS ( [?|i], ?, [j|?], A ) ? ( ?, [i|?], [j|?], A ? {i l? j} )
RIGHTl-PASS ( [?|i], ?, [j|?], A ) ? ( ?, [i|?], [j|?], A ? {i l? j} )
NO-PASS ( [?|i], ?, [j|?], A ) ? ( ?, [i|?], [j|?], A )
Table 1: Transitions in our dependency parsing algorithm.
Transition Preconditions
LEFTl-? [i 6= 0] ? ?[?k. (i? k) ? A] ? ?[(i?? j) ? A]
RIGHTl-? ?[?k. (k ? j) ? A] ? ?[(i ?? j) ? A]
?-SHIFT ?[?k ? ?. (k 6= i) ? ((k ? j) ? (k ? j))]
?-REDUCE [?h. (h? i) ? A] ? ?[?k ? ?. (i? k)]
Table 2: Preconditions of the transitions in Table 1 (? is a wildcard representing any transition).
the bottom 3 transitions are inherited from Nivre?s
arc-eager and list-based algorithms, respectively.2
Each parsing state is represented as a tuple (?,
?, ?, A), where ? is a stack containing processed
tokens, ? is a deque containing tokens popped out
of ? but will be pushed back into ? in later parsing
states to handle non-projectivity, and ? is a buffer
containing unprocessed tokens. A is a set of labeled
arcs. (i, j) represent indices of their corresponding
tokens (wi, wj), l is a dependency label, and the 0
identifier corresponds to w0, introduced as the root
of a tree. The initial state is ([0], [ ], [1, . . . , n], ?),
and the final state is (?, ?, [ ], A). At any parsing
state, a decision is made by comparing the top of
?, wi, and the first element of ?, wj . This decision
is consulted by gold-standard trees during training
and a classifier during decoding.
LEFTl-? and RIGHTl-? are performed when wj
is the head ofwi with a dependency label l, and vice
versa. After LEFTl-? or RIGHTl-?, an arc is added
to A. NO-? is performed when no dependency is
found for wi and wj . ?-SHIFT is performed when
no dependency is found for wj and any token in
? other than wi. After ?-SHIFT, all tokens in ?
as well as wj are pushed into ?. ?-REDUCE is
performed when wi already has the head, and wi is
not the head of any token in ?. After ?-REDUCE,
wi is popped out of ?. ?-PASS is performed when
neither ?-SHIFT nor ?-REDUCE can be performed.
After ?-PASS, wi is moved to the front of ? so it
2The parsing complexity of a transition-based dependency
parsing algorithm is determined by the number of transitions
performed with respect to the number of tokens in a sentence,
say n (K?bler et al, 2009).
can be compared to other tokens in ? later. Each
transition needs to satisfy certain preconditions to
ensure the properties of a well-formed dependency
graph (Nivre, 2008); they are described in Table 2.
(i? j) and (i ?? j) indicate that wj is the head
and an ancestor of wi with any label, respectively.
When a parser is trained on only projective trees,
our algorithm learns only the top 4 transitions and
produces only projective trees during decoding. In
this case, it performs at most 2n ? 1 transitions
per sentence so the complexity is O(n). When a
parser is trained on a mixture of projective and non-
projective trees, our algorithm learns all transitions
and produces both kinds of trees during decoding.
In this case, it performs at most n(n+1)2 transitionsso the complexity is O(n2). However, because of
the presence of ?-SHIFT and ?-REDUCE, our al-
gorithm is capable of skipping or removing tokens
during non-projective parsing, which allows it to
show a linear time parsing speed in practice.
700 10 20 30 40 50 60
130
0
20
40
60
80
100
Sentence length
Tra
nsit
ions
Figure 1: The # of transitions performed during
training with respect to sentence lengths for Dutch.
1053
Transition ? ? ? A
0 Initialization [0] [ ] [1|?] ?
1 NO-SHIFT [?|1] [ ] [2|?]
2 NO-SHIFT [?|2] [ ] [3|?]
3 NO-SHIFT [?|3] [ ] [4|?]
4 LEFT-REDUCE [?|2] [ ] [4|?] A ? {3?NSUBJ? 4}
5 NO-PASS [?|1] [2] [4|?]
6 RIGHT-SHIFT [?|4] [ ] [5|?] A ? {1 ?RCMOD? 4}
7 NO-SHIFT [?|5] [ ] [6|?]
8 LEFT-REDUCE [?|4] [ ] [6|?] A ? {5?AUX? 6}
9 RIGHT-PASS [?|2] [4] [6|?] A ? {4 ?XCOMP? 6}
10 LEFT-REDUCE [?|1] [4] [6|?] A ? {2?DOBJ? 6}
11 NO-SHIFT [?|6] [ ] [7|?]
12 NO-REDUCE [?|4] [ ] [7|?]
13 NO-REDUCE [?|1] [ ] [7|?]
14 LEFT-REDUCE [0] [ ] [7|?] A ? {1?NSUBJ? 7}
15 RIGHT-SHIFT [?|7] [ ] [8] A ? {0 ?ROOT? 7}
16 RIGHT-SHIFT [?|8] [ ] [ ] A ? {7 ?ADV? 8}
Table 3: A transition sequence generated by our parsing algorithm using gold-standard decisions.
Figure 1 shows the total number of transitions per-
formed during training with respect to sentence
lengths for Dutch. Among all languages distributed
by the CoNLL-X shared task (Buchholz and Marsi,
2006), Dutch consists of the highest number of
non-projective dependencies (5.4% in arcs, 36.4%
in trees). Even with such a high number of non-
projective dependencies, our parsing algorithm still
shows a linear growth in transitions.
Table 3 shows a transition sequence generated
by our parsing algorithm using gold-standard deci-
sions. Afterw3 andw4 are compared, w3 is popped
out of ? (state 4) so it is not compared to any other
token in ? (states 9 and 13). After w2 and w4 are
compared, w2 is moved to ? (state 5) so it can be
compared to other tokens in ? (state 10). After w4
and w6 are compared, RIGHT-PASS is performed
(state 9) because there is a dependency between
w6 and w2 in ? (state 10). After w6 and w7 are
compared, w6 is popped out of ? (state 12) because
it is not needed for later parsing states.
3 Selectional branching
3.1 Motivation
For transition-based parsing, state-of-the-art accu-
racies have been achieved by parsers optimized on
multiple transition sequences using beam search,
which can be done very efficiently when it is cou-
pled with dynamic programming (Zhang and Clark,
2008; Huang and Sagae, 2010; Zhang and Nivre,
2011; Huang et al, 2012; Bohnet and Nivre, 2012).
Despite all the benefits, there is one downside of
this approach; it generates a fixed number of tran-
sition sequences no matter how confident the one-
best sequence is.3 If every prediction leading to
the one-best sequence is confident, it may not be
necessary to explore more sequences to get the best
output. Thus, it is preferred if the beam size is not
fixed but proportional to the number of low confi-
dence predictions made for the one-best sequence.
The selectional branching method presented here
performs at most d ? t? e transitions, where t is the
maximum number of transitions performed to gen-
erate a transition sequence, d = min(b, |?|+1), b is
the beam size, |?| is the number of low confidence
predictions made for the one-best sequence, and
e = d(d?1)2 . Compared to beam search that alwaysperforms b ? t transitions, selectional branching is
guaranteed to perform fewer transitions given the
same beam size because d ? b and e > 0 except for
d = 1, in which case, no branching happens. With
selectional branching, our parser shows slightly
3The ?one-best sequence? is a transition sequence gener-
ated by taking only the best prediction at each parsing state.
1054
higher parsing accuracy than the current state-of-
the-art transition-based parser using beam search,
and performs about 3 times faster.
3.2 Branching strategy
Figure 2 shows an overview of our branching strat-
egy. sij represents a parsing state, where i is the
index of the current transition sequence and j is
the index of the current parsing state (e.g., s12 rep-
resents the 2nd parsing state in the 1st transition
sequence). pkj represents the k?th best prediction
(in our case, it is a predicted transition) given s1j
(e.g., p21 is the 2nd-best prediction given s11).
s
11
s
12
p
11
s
22
? ? s
1t
p
12
? ? s
2t
p
21
s
33
p
22
? s
3t
s
dt
?
? ?
p
2j
T
1
 =
T
2
 =
T
3
 =
T
d
 =
p
1j
Figure 2: An overview of our branching strategy.
Each sequence Ti>1 branches from T1.
Initially, the one-best sequence T1 = [s11, ... , s1t]
is generated by a greedy parser. While generating
T1, the parser adds tuples (s1j , p2j), ... , (s1j , pkj)
to a list ? for each low confidence prediction p1j
given s1j .4 Then, new transition sequences are gen-
erated by using the b highest scoring predictions in
?, where b is the beam size. If |?| < b, all predic-
tions in ? are used. The same greedy parser is used
to generate these new sequences although it now
starts with s1j instead of an initial parsing state,
applies pkj to s1j , and performs further transitions.
Once all transition sequences are generated, a parse
tree is built from a sequence with the highest score.
For our experiments, we set k = 2, which gave
noticeably more accurate results than k = 1. We
also experimented with k > 2, which did not show
significant improvement over k = 2. Note that as-
signing a greater k may increase |?| but not the total
number of transition sequences generated, which
is restricted by the beam size, b. Since each se-
quence Ti>1 branches from T1, selectional branch-
ing performs fewer transitions than beam search:
at least d(d?1)2 transitions are inherited from T1,
4? is initially empty, which is hidden in Figure 2.
where d = min(b, |?| + 1); thus, it performs that
many transitions less than beam search (see the
left lower triangle in Figure 2). Furthermore, se-
lectional branching generates a d number of se-
quences, where d is proportional to the number of
low confidence predictions made by T1. To sum up,
selectional branching generates the same or fewer
transition sequences than beam search and each
sequence Ti>1 performs fewer transitions than T1;
thus, it performs faster than beam search in general
given the same beam size.
3.3 Finding low confidence predictions
For each parsing state sij , a prediction is made by
generating a feature vector xij ? X , feeding it into
a classifier C1 that uses a feature map ?(x, y) and
a weight vector w to measure a score for each label
y ? Y , and choosing a label with the highest score.
When there is a tie between labels with the highest
score, the first one is chosen. This can be expressed
as a logistic regression:
C1(x) = arg max
y?Y
{f(x, y)}
f(x, y) = exp(w ? ?(x, y))?
y??Y exp(w ? ?(x, y?))
To find low confidence predictions, we use the mar-
gins (score differences) between the best prediction
and the other predictions. If all margins are greater
than a threshold, the best prediction is considered
highly confident; otherwise, it is not. Given this
analogy, the k-best predictions can be found as
follows (m ? 0 is a margin threshold):
Ck(x,m) = K arg max
y?Y
{f(x, y)}
s.t. f(x,C1(x))? f(x, y) ? m
?K arg max? returns a set of k? labels whose mar-
gins to C1(x) are smaller than any other label?s
margin to C1(x) and also ? m, where k? ? k.
When m = 0, it returns a set of the highest scoring
labels only, including C1(x). When m = 1, it re-
turns a set of all labels. Given this, a prediction is
considered not confident if |Ck(x,m)| > 1.
3.4 Finding the best transition sequence
Let Pi be a list of all predictions that lead to gen-
erate a transition sequence Ti. The predictions in
Pi are either inherited from T1 or made specifi-
cally for Ti. In Figure 2, P3 consists of p11 as its
first prediction, p22 as its second prediction, and
1055
further predictions made specifically for T3. The
score of each prediction is measured by f(x, y) in
Section 3.3. Then, the score of Ti is measured by
averaging scores of all predictions in Pi.
score(Ti) =
?
p?Pi score(p)
|Pi|
Unlike Zhang and Clark (2008), we take the av-
erage instead of the sum of all prediction scores.
This is because our algorithm does not guarantee
the same number of transitions for every sequence,
so the sum of all scores would weigh more on se-
quences with more transitions. We experimented
with both the sum and the average, and taking the
average led to slightly higher parsing accuracy.
3.5 Bootstrapping transition sequences
During training, a training instance is generated
for each parsing state sij by taking a feature vec-
tor xij and its true label yij . To generate multiple
transition sequences during training, the bootstrap-
ping technique of Choi and Palmer (2011) is used,
which is described in Algorithm 1.5
Algorithm 1 Bootstrapping
Input: Dt: training set, Dd: development set.
Output: A model M .
1: r ? 0
2: I ? getTrainingInstances(Dt)
3: M0 ? buildModel(I)
4: S0 ? getScore(Dd,M0)
5: while (r = 0) or (Sr?1 < Sr) do
6: r ? r + 1
7: I ? getTrainingInstances(Dt,Mr?1)
8: Mr ? buildModel(I)
9: Sr ? getScore(Dd,Mr)
10: return Mr?1
First, an initial model M0 is trained on all data by
taking the one-best sequences, and its score is mea-
sured by testing on a development set (lines 2-4).
Then, the next model Mr is trained on all data but
this time, Mr?1 is used to generate multiple tran-
sition sequences (line 7-8). Among all transition
sequences generated by Mr?1, training instances
from only T1 and Tg are used to trainMr, where T1
is the one-best sequence and Tg is a sequence giv-
ing the most accurate parse output compared to the
gold-standard tree. The score of Mr is measured
(line 9), and repeat the procedure if Sr?1 < Sr;
otherwise, return the previous model Mr?1.
5Alternatively, the dynamic oracle approach of Goldberg
and Nivre (2012) can be used to generate multiple transition
sequences, which is expected to show similar results.
3.6 Adaptive subgradient algorithm
To build each model during bootstrapping, we use
a stochastic adaptive subgradient algorithm called
ADAGRAD that uses per-coordinate learning rates
to exploit rarely seen features while remaining scal-
able (Duchi et al, 2011).This is suitable for NLP
tasks where rarely seen features often play an im-
portant role and training data consists of a large
number of instances with high dimensional features.
Algorithm 2 shows our adaptation of ADAGRAD
with logistic regression for multi-class classifica-
tion. Note that when used with logistic regression,
ADAGRAD takes a regular gradient instead of a sub-
gradient method for updating weights. For our ex-
periments, ADAGRAD slightly outperformed learn-
ing algorithms such as average perceptron (Collins,
2002) or Liblinear SVM (Hsieh et al, 2008).
Algorithm 2 ADAGRAD + logistic regression
Input: D = {(xi, yi)}ni=1 s.t. xi ? X , yi ? Y
?(x, y) ? Rd s.t. d = dimension(X )? |Y|
T : iterations, ?: learning rate, ?: ridge
Output: A weight vector w ? Rd.
1: w? 0, where w ? Rd
2: G? 0, where G ? Rd
3: for t? 1 . . . T do
4: for i? 1 . . . n do
5: Q?y?Y ? I(yi, y)? f(xi, y), s.t. Q ? R|Y|
6: ? ??y?Y(?(xi, y) ?Qy)
7: G? G + ? ? ?
8: for j ? 1 . . . d do
9: wj ? wj + ? ? 1?+?Gj ? ?j
I(y, y?) =
{
1 y = y?
0 otherwise
The algorithm takes three hyper-parameters; T is
the number of iterations, ? is the learning rate, and
? is the ridge (T > 0, ? > 0, ? ? 0). G is our run-
ning estimate of a diagonal covariance matrix for
the gradients (per-coordinate learning rates). For
each instance, scores for all labels are measured
by the logistic regression function f(x, y) in Sec-
tion 3.3. These scores are subtracted from an output
of the indicator function I(y, y?), which forces our
model to keep learning this instance until the pre-
diction is 100% confident (in other words, until
the score of yi becomes 1). Then, a subgradient
is measured by taking all feature vectors together
weighted by Q (line 6). This subgradient is used to
update G and w, where ? is the Hadamard product
(lines 7-9). ? is a ridge term to keep the inverse
covariance well-conditioned.
1056
4 Experiments
4.1 Corpora
For projective parsing experiments, the Penn En-
glish Treebank (Marcus et al, 1993) is used with
the standard split: sections 2-21 for training, 22 for
development, and 23 for evaluation. All constituent
trees are converted with the head-finding rules of
Yamada and Matsumoto (2003) and the labeling
rules of Nivre (2006). For non-projective pars-
ing experiments, four languages from the CoNLL-
X shared task are used: Danish, Dutch, Slovene,
and Swedish (Buchholz and Marsi, 2006). These
languages are selected because they contain non-
projective trees and are publicly available from the
CoNLL-X webpage.6 Since the CoNLL-X data we
have does not come with development sets, the last
10% of each training set is used for development.
4.2 Feature engineering
For English, we mostly adapt features from Zhang
and Nivre (2011) who have shown state-of-the-art
parsing accuracy for transition-based dependency
parsing. Their distance features are not included
in our approach because they do not seem to show
meaningful improvement. Feature selection is done
on the English development set.
For the other languages, the same features are
used with the addition of morphological features
provided by CoNLL-X; specifically, morphological
features from the top of ? and the front of ? are
added as unigram features. Moreover, all POS tag
features from English are duplicated with coarse-
grained POS tags provided by CoNLL-X. No more
feature engineering is done for these languages; it
is possible to achieve higher performance by using
different features, especially when these languages
contain non-projective dependencies whereas En-
glish does not, which we will explore in the future.
4.3 Development
Several parameters need to be optimized during de-
velopment. For ADAGRAD, T , ?, and ? need to be
tuned (Section 3.6). For bootstrapping, the number
of iterations, say r, needs to be tuned (Section 3.5).
For selectional branching, the margin threshold m
and the beam size b need to be tuned (Section 3.3).
First, all parameters are tuned on the English devel-
opment set by using grid search on T = [1, . . . , 10],
? = [0, 01, 0, 02], ? = [0.1, 0.2], r = [1, 2, 3],
6http://ilk.uvt.nl/conll/
m = [0.83, . . . , 0.92], and b = [16, 32, 64, 80].
As a result, the following parameters are found:
? = 0.02, ? = 0.1, m = 0.88, and b = 64|80. For
this development set, the beam size of 64 and 80
gave the exact same result, so we kept the one with
a larger beam size (b = 80).
0.920.83 0.86 0.88 0.9
91.2
91
91.04
91.08
91.12
91.16
Margin
Acc
urac
y
64|803216
b =
Figure 3: Parsing accuracies with respect to mar-
gins and beam sizes on the English development set.
b = 64|80: the black solid line with solid circles,
b = 32: the blue dotted line with hollow circles,
b = 16: the red dotted line with solid circles.
Figure 3 shows parsing accuracies with respect to
different margins and beam sizes on the English de-
velopment set. These parameters need to be tuned
jointly because different margins prefer different
beam sizes. For instance, m = 0.85 gives the high-
est accuracy with b = 32, but m = 0.88 gives the
highest accuracy with b = 64|80.
140 2 4 6 8 10 12
92
88.5
89
89.5
90
90.5
91
91.5
Iteration
Acc
urac
y
UAS
LAS
Figure 4: Parsing accuracies with respect to ADA-
GRAD and bootstrap iterations on the English de-
velopment set when ? = 0.02, ? = 0.1, m = 0.88,
and b = 64|80. UAS: unlabeled attachment score,
LAS: labeled attachment score.
Figure 4 shows parsing accuracies with respect to
ADAGRAD and bootstrap iterations on the English
development set. The range 1-5 shows results of
5 ADAGRAD iterations before bootstrapping, the
range 6-9 shows results of 4 iterations during the
1057
first bootstrapping, and the range 10-14 shows re-
sults of 5 iterations during the second bootstrap-
ping. Thus, the number of bootstrap iteration is
2 where each bootstrapping takes a different num-
ber of ADAGRAD iterations. Using an Intel Xeon
2.57GHz machine, it takes less than 40 minutes
to train the entire Penn Treebank, which includes
times for IO, feature extraction and bootstrapping.
800 10 20 30 40 50 60 70
1,200,000
0
200,000
400,000
600,000
800,000
1,000,000
Beam size = 1, 2, 4, 8, 16, 32, 64, 80
Tra
nsit
ions
Figure 5: The total number of transitions performed
during decoding with respect to beam sizes on the
English development set.
Figure 5 shows the total number of transitions per-
formed during decoding with respect to beam sizes
on the English development set (1,700 sentences,
40,117 tokens). With selectional branching, the
number of transitions grows logarithmically as the
beam size increases whereas it would have grown
linearly if beam search were used. We also checked
how often the one best sequence is chosen as the
final sequence during decoding. Out of 1,700 sen-
tences, the one best sequences are chosen for 1,095
sentences. This implies that about 64% of time,
our greedy parser performs as accurately as our
non-greedy parser using selectional branching.
For the other languages, we use the same values
as English for ?, ?, m, and b; only the ADAGRAD
and bootstrap iterations are tuned on the develop-
ment sets of the other languages.
4.4 Projective parsing experiments
Before parsing, POS tags were assigned to the train-
ing set by using 20-way jackknifing. For the auto-
matic generation of POS tags, we used the domain-
specific model of Choi and Palmer (2012a)?s tagger,
which gave 97.5% accuracy on the English evalua-
tion set (0.2% higher than Collins (2002)?s tagger).
Table 4 shows comparison between past and cur-
rent state-of-the-art parsers and our approach. The
first block shows results from transition-based de-
pendency parsers using beam search. The second
block shows results from other kinds of parsing
approaches (e.g., graph-based parsing, ensemble
parsing, linear programming, dual decomposition).
The third block shows results from parsers using
external data. The last block shows results from
our approach. The Time column show how many
seconds per sentence each parser takes.7
Approach UAS LAS Time
Zhang and Clark (2008) 92.1
Huang and Sagae (2010) 92.1 0.04
Zhang and Nivre (2011) 92.9 91.8 0.03
Bohnet and Nivre (2012) 93.38 92.44 0.4
McDonald et al (2005) 90.9
Mcdonald and Pereira (2006) 91.5
Sagae and Lavie (2006) 92.7
Koo and Collins (2010) 93.04
Zhang and McDonald (2012) 93.06 91.86
Martins et al (2010) 93.26
Rush et al (2010) 93.8
Koo et al (2008) 93.16
Carreras et al (2008) 93.54
Bohnet and Nivre (2012) 93.67 92.68
Suzuki et al (2009) 93.79
bt = 80, bd = 80, m = 0.88 92.96 91.93 0.009
bt = 80, bd = 64, m = 0.88 92.96 91.93 0.009
bt = 80, bd = 32, m = 0.88 92.96 91.94 0.009
bt = 80, bd = 16, m = 0.88 92.96 91.94 0.008
bt = 80, bd = 8, m = 0.88 92.89 91.87 0.006
bt = 80, bd = 4, m = 0.88 92.76 91.76 0.004
bt = 80, bd = 2, m = 0.88 92.56 91.54 0.003
bt = 80, bd = 1, m = 0.88 92.26 91.25 0.002
bt = 1, bd = 1, m = 0.88 92.06 91.05 0.002
Table 4: Parsing accuracies and speeds on the En-
glish evaluation set, excluding tokens containing
only punctuation. bt and bd indicate the beam sizes
used during training and decoding, respectively.
UAS: unlabeled attachment score, LAS: labeled
attachment score, Time: seconds per sentence.
For evaluation, we use the model trained with b =
80 and m = 0.88, which is the best setting found
during development. Our parser shows higher ac-
curacy than Zhang and Nivre (2011), which is
the current state-of-the-art transition-based parser
that uses beam search. Bohnet and Nivre (2012)?s
transition-based system jointly performs POS tag-
ging and dependency parsing, which shows higher
accuracy than ours. Our parser gives a comparative
accuracy to Koo and Collins (2010) that is a 3rd-
order graph-based parsing approach. In terms of
speed, our parser outperforms all other transition-
based parsers; it takes about 9 milliseconds per
7Dhillon et al (2012) and Rush and Petrov (2012) also
have shown good results on this data but they are excluded
from our comparison because they use different kinds of
constituent-to-dependency conversion methods.
1058
Approach Danish Dutch Slovene SwedishLAS UAS LAS UAS LAS UAS LAS UAS
Nivre et al (2006) 84.77 89.80 78.59 81.35 70.30 78.72 84.58 89.50
McDonald et al (2006) 84.79 90.58 79.19 83.57 73.44 83.17 82.55 88.93
Nivre (2009) 84.2 - - - 75.2 - - -
F.-Gonz?lez and G.-Rodr?guez (2012) 85.17 90.10 - - - - 83.55 89.30
Nivre and McDonald (2008) 86.67 - 81.63 - 75.94 84.66
Martins et al (2010) - 91.50 - 84.91 - 85.53 - 89.80
bt = 80, bd = 1, m = 0.88 86.75 91.04 80.75 83.59 75.66 83.29 86.32 91.12
bt = 80, bd = 80, m = 0.88 87.27 91.36 82.45 85.33 77.46 84.65 86.80 91.36
Table 5: Parsing accuracies on four languages with non-projective dependencies, excluding punctuation.
sentence using the beam size of 80. Our parser is
implemented in Java and tested on an Intel Xeon
2.57GHz. Note that we do not include input/output
time for our speed comparison.
For a proof of concept, we run the same model,
trained with bt = 80, but decode with different
beam sizes using the same margin. Surprisingly,
our parser gives the same accuracy (0.01% higher
for labeled attachment score) on this data even with
bd = 16. More importantly, bd = 16 shows about
the same parsing speed as bd = 80, which indicates
that selectional branching automatically reduced
down the beam size by estimating low confidence
predictions, so even if we assigned a larger beam
size for decoding, it would have performed as effi-
ciently. This implies that we no longer need to be
so conscious about the beam size during decoding.
Another interesting part is that (bt = 80, bd = 1)
shows higher accuracy than (bt = 1, bd = 1); this
implies that our training method of bootstrapping
transition sequences can improve even a greedy
parser. Notice that our greedy parser shows higher
accuracy than many other greedy parsers (Hall et
al., 2006; Goldberg and Elhadad, 2010) because
it uses the non-local features of Zhang and Nivre
(2011) and the bootstrapping technique of Choi
and Palmer (2011) that had not been used for most
other greedy parsing approaches.
4.5 Non-projective parsing experiments
Table 5 shows comparison between state-of-the-art
parsers and our approach for four languages with
non-projective dependencies. Nivre et al (2006)
uses a pseudo-projective transition-based parsing
approach. McDonald et al (2006) uses a 2nd-order
maximum spanning tree approach. Nivre (2009)
and Fern?ndez-Gonz?lez and G?mez-Rodr?guez
(2012) use different non-projective transition-based
parsing approaches. Nivre and McDonald (2008)
uses an ensemble model between transition-based
and graph-based parsing approaches. Martins et
al. (2010) uses integer linear programming for the
optimization of their parsing model.
Some of these approaches use greedy parsers, so
we include our results from models using (bt = 80,
bd = 1, m = 0.88), which finds only the one-best
sequences during decoding although it is trained on
multiple transition sequences (see Section 4.4). Our
parser shows higher accuracies for most languages
except for unlabeled attachment scores in Danish
and Slovene. Our greedy approach outperforms
both Nivre (2009) and Fern?ndez-Gonz?lez and
G?mez-Rodr?guez (2012) who use different non-
projective parsing algorithms.
600 10 20 30 40 50
130
0
20
40
60
80
100
Sentence length
Tra
nsit
ions
Figure 6: The # of transitions performed during de-
coding with respect to sentence lengths for Dutch.
Figure 6 shows the number of transitions performed
during decoding with respect to sentence lengths
for Dutch using bd = 1. Our parser still shows a
linear growth in transition during decoding.
5 Related work
Our parsing algorithm is most similar to Choi and
Palmer (2011) who integrated our LEFT-REDUCE
transition into Nivre?s list-based algorithm. Our
algorithm is distinguished from theirs because ours
gives different parsing complexities of O(n) and
O(n2) for projective and non-projective parsing,
respectively, whereas their algorithm gives O(n2)
1059
for both cases; this is possible because of the new
integration of the RIGHT-SHIFT and NO-REDUCE
transitions. There are other transition-based de-
pendency parsing algorithms that take a similar ap-
proach; Nivre (2009) integrated a SWAP transition
into Nivre?s arc-standard algorithm (Nivre, 2004)
and Fern?ndez-Gonz?lez and G?mez-Rodr?guez
(2012) integrated a buffer transition into Nivre?s
arc-eager algorithm to handle non-projectivity.
Our selectional branching method is most rele-
vant to Zhang and Clark (2008) who introduced
a transition-based dependency parsing model that
uses beam search. Huang and Sagae (2010) later
applied dynamic programming to this approach
and showed improved efficiency. Zhang and Nivre
(2011) added non-local features to this approach
and showed improved parsing accuracy. Bohnet
and Nivre (2012) introduced a transition-based sys-
tem that jointly performed POS tagging and de-
pendency parsing. Our work is distinguished from
theirs because we use selectional branching instead.
6 Conclusion
We present selectional branching that uses confi-
dence estimates to decide when to employ a beam.
Coupled with our new hybrid parsing algorithm,
ADAGRAD, rich non-local features, and bootstrap-
ping, our parser gives higher parsing accuracy than
most other transition-based dependency parsers in
multiple languages and shows faster parsing speed.
It is interesting to see that our greedy parser out-
performed most other greedy dependency parsers.
This is because our parser used both bootstrapping
and Zhang and Nivre (2011)?s non-local features,
which had not been used by other greedy parsers.
In the future, we will experiment with more ad-
vanced dependency representations (de Marneffe
and Manning, 2008; Choi and Palmer, 2012b) to
show robustness of our approach. Furthermore, we
will evaluate individual methods of our approach
separately to show impact of each method on pars-
ing performance. We also plan to implement the
typical beam search approach to make a direct com-
parison to our selectional branching.8
Acknowledgments
Special thanks are due to Luke Vilnis of the Uni-
versity of Massachusetts Amherst for insights on
8Our parser is publicly available under an open source
project, ClearNLP (clearnlp.googlecode.com).
the ADAGRAD derivation. We gratefully acknowl-
edge a grant from the Defense Advanced Research
Projects Agency (DARPA) under the DEFT project,
solicitation #: DARPA-BAA-12-47.
References
Bernd Bohnet and Joakim Nivre. 2012. A Transition-
Based System for Joint Part-of-Speech Tagging and
Labeled Non-Projective Dependency Parsing. In
Proceedings of the 2012 Joint Conference on Em-
pirical Methods in Natural Language Processing
and Computational Natural Language Learning,
EMNLP?12, pages 1455?1465.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-
X shared task on multilingual dependency parsing.
In Proceedings of the Tenth Conference on Com-
putational Natural Language Learning, CoNLL?06,
pages 149?164.
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
TAG, Dynamic Programming, and the Perceptron
for Efficient, Feature-rich Parsing. In Proceedings
of the 12th Conference on Computational Natural
Language Learning, CoNLL?08, pages 9?16.
Daniel Cer, Marie-Catherine de Marneffe, Daniel Ju-
rafsky, and Christopher D. Manning. 2010. Pars-
ing to Stanford Dependencies: Trade-offs between
speed and accuracy. In Proceedings of the 7th In-
ternational Conference on Language Resources and
Evaluation, LREC?10.
Jinho D. Choi and Martha Palmer. 2011. Getting the
Most out of Transition-based Dependency Parsing.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, ACL:HLT?11, pages 687?
692.
Jinho D. Choi and Martha Palmer. 2012a. Fast and Ro-
bust Part-of-Speech Tagging Using Dynamic Model
Selection. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguistics,
ACL?12, pages 363?367.
Jinho D. Choi and Martha Palmer. 2012b. Guidelines
for the Clear Style Constituent to Dependency Con-
version. Technical Report 01-12, University of Col-
orado Boulder.
Michael Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Exper-
iments with Perceptron Algorithms. In Proceedings
of the conference on Empirical methods in natural
language processing, EMNLP?02, pages 1?8.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies
representation. In Proceedings of the COLING
workshop on Cross-Framework and Cross-Domain
Parser Evaluation.
1060
Paramveer S. Dhillon, Jordan Rodu, Michael Collins,
Dean P. Foster, and Lyle H. Ungar. 2012. Spectral
Dependency Parsing with Latent Variables. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing, EMNLP?12, pages
205?213.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive Subgradient Methods for Online Learning
and Stochastic Optimization. The Journal of Ma-
chine Learning Research, 12(39):2121?2159.
Daniel Fern?ndez-Gonz?lez and Carlos G?mez-
Rodr?guez. 2012. Improving Transition-Based
Dependency Parsing with Buffer Transitions. In
Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Processing
and Computational Natural Language Learning,
EMNLP?12, pages 308?319.
Yoav Goldberg and Michael Elhadad. 2010. An Effi-
cient Algorithm for Easy-First Non-Directional De-
pendency Parsing. In Human Language Technolo-
gies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, HLT:NAACL?10, pages 742?750.
Yoav Goldberg and Joakim Nivre. 2012. A Dynamic
Oracle for Arc-Eager Dependency Parsing. In Pro-
ceedings of the 24th International Conference on
Computational Linguistics, COLING?12.
Johan Hall, Joakim Nivre, and Jens Nilsson. 2006.
Discriminative Classifiers for Deterministic Depen-
dency Parsing. In In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, COLING-ACL?06, pages 316?
323.
Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin,
S. Sathiya Keerthi, and S. Sundararajan. 2008. A
Dual Coordinate Descent Method for Large-scale
Linear SVM. In Proceedings of the 25th interna-
tional conference on Machine learning, ICML?08,
pages 408?415.
Liang Huang and Kenji Sagae. 2010. Dynamic Pro-
gramming for Linear-Time Incremental Parsing. In
Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, ACL?10.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured Perceptron with Inexact Search. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
NAACL-HLT?12, pages 142?151.
Terry Koo and Michael Collins. 2010. Efficient Third-
order Dependency Parsers. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, ACL?10.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple Semi-supervised Dependency Parsing. In
Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics, ACL:HLT?08,
pages 595?603.
Sandra K?bler, Ryan T. McDonald, and Joakim Nivre.
2009. Dependency Parsing. Synthesis Lectures
on Human Language Technologies. Morgan & Clay-
pool Publishers.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a Large Anno-
tated Corpus of English: The Penn Treebank. Com-
putational Linguistics, 19(2):313?330.
Andr? F. T. Martins, Noah A. Smith, Eric P. Xing, Pe-
dro M. Q. Aguiar, and M?rio A. T. Figueiredo. 2010.
Turbo Parsers: Dependency Parsing by Approximate
Variational Inference. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP?10, pages 34?44.
Ryan Mcdonald and Fernando Pereira. 2006. Online
Learning of Approximate Dependency Parsing Algo-
rithms. In Proceedings of the Annual Meeting of the
European American Chapter of the Association for
Computational Linguistics, EACL?06, pages 81?88.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online Large-Margin Training of
Dependency Parsers. In Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics, pages 91?98.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual Dependency Analysis with a
Two-Stage Discriminative Parser. In Proceedings
of the Tenth Conference on Computational Natural
Language Learning, CoNLL?06, pages 216?220.
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing Graph-based and Transition-based Dependency
Parsers. In Proceedings of the 46th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, ACL:HLT?08,
pages 950?958.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-
Projective Dependency Parsing. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics, ACL?05, pages 99?106.
Joakim Nivre, Johan Hall, Jens Nilsson, G?ls?en Eryig?it,
and Svetoslav Marinov. 2006. Labeled pseudo-
projective dependency parsing with support vec-
tor machines. In Proceedings of the 10th Confer-
ence on Computational Natural Language Learning,
CoNLL?06, pages 221?225.
Joakim Nivre. 2003. An Efficient Algorithm for Pro-
jective Dependency Parsing. In Proceedings of the
8th International Workshop on Parsing Technolo-
gies, IWPT?03, pages 149?160.
1061
Joakim Nivre. 2004. Incrementality in Deterministic
Dependency Parsing. In Proceedings of the ACL?04
Workshop on Incremental Parsing: Bringing Engi-
neering and Cognition Together, pages 50?57.
Joakim Nivre. 2006. Inductive Dependency Parsing.
Springer.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Computational Lin-
guistics, 34(4):513?553.
Joakim Nivre. 2009. Non-Projective Dependency Pars-
ing in Expected Linear Time. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, ACL-
IJCNLP?09, pages 351?359.
Alexander M. Rush and Slav Petrov. 2012. Vine Prun-
ing for Efficient Multi-Pass Dependency Parsing. In
Proceedings of the 12th Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, NAACL:HLT?12.
Alexander M. Rush, David Sontag, Michael Collins,
and Tommi Jaakkola. 2010. On Dual Decomposi-
tion and Linear Programming Relaxations for Nat-
ural Language Processing. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, EMNLP?10, pages 1?11.
Kenji Sagae and Alon Lavie. 2006. Parser Combina-
tion by Reparsing. In In Proceedings of the Human
Language Technology Conference of the NAACL,
NAACL?06, pages 129?132.
Jun Suzuki, Hideki Isozaki, Xavier Carreras, and
Michael Collins. 2009. An Empirical Study of
Semi-supervised Structured Conditional Models for
Dependency Parsing. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP?09, pages 551?560.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chine. In Proceedings of the 8th International Work-
shop on Parsing Technologies, IWPT?03, pages 195?
206.
Yue Zhang and Stephen Clark. 2008. A Tale of
Two Parsers: investigating and combining graph-
based and transition-based dependency parsing us-
ing beam-search. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP?08, pages 562?571.
Hao Zhang and Ryan McDonald. 2012. Generalized
Higher-Order Dependency Parsing with Cube Prun-
ing. In Proceedings of the 2012 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
EMNLP-CoNLL?12, pages 320?331.
Yue Zhang and Joakim Nivre. 2011. Transition-based
Dependency Parsing with Rich Non-local Features.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, ACL?11, pages 188?193.
1062
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 91?99,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Retrieving Correct Semantic Boundaries in Dependency Structure
Jinho D. Choi
Department of Computer Science
University of Colorado at Boulder
choijd@colorado.edu
Martha Palmer
Department of Linguistics
University of Colorado at Boulder
martha.palmer@colorado.edu
Abstract
This paper describes the retrieval of cor-
rect semantic boundaries for predicate-
argument structures annotated by depen-
dency structure. Unlike phrase structure,
in which arguments are annotated at the
phrase level, dependency structure does
not have phrases so the argument labels are
associated with head words instead: the
subtree of each head word is assumed to
include the same set of words as the an-
notated phrase does in phrase structure.
However, at least in English, retrieving
such subtrees does not always guarantee
retrieval of the correct phrase boundaries.
In this paper, we present heuristics that
retrieve correct phrase boundaries for se-
mantic arguments, called semantic bound-
aries, from dependency trees. By apply-
ing heuristics, we achieved an F1-score
of 99.54% for correct representation of
semantic boundaries. Furthermore, error
analysis showed that some of the errors
could also be considered correct, depend-
ing on the interpretation of the annotation.
1 Introduction
Dependency structure has recently gained wide in-
terest because it is simple yet provides useful in-
formation for many NLP tasks such as sentiment
analysis (Kessler and Nicolov, 2009) or machine
translation (Gildea, 2004). Although dependency
structure is a kind of syntactic structure, it is quite
different from phrase structure: phrase structure
gives phrase information by grouping constituents
whereas dependency structure gives dependency
relations between pairs of words. Many depen-
dency relations (e.g., subject, object) have high
correlations with semantic roles (e.g., agent, pa-
tient), which makes dependency structure suit-
able for representing semantic information such as
predicate-argument structure.
In 2009, the Conference on Computational Nat-
ural Language Learning (CoNLL) opened a shared
task: the participants were supposed to take de-
pendency trees as input and produce semantic role
labels as output (Hajic? et al, 2009). The depen-
dency trees were automatically converted from the
Penn Treebank (Marcus et al, 1993), which con-
sists of phrase structure trees, using some heuris-
tics (cf. Section 3). The semantic roles were ex-
tracted from the Propbank (Palmer et al, 2005).
Since Propbank arguments were originally anno-
tated at the phrase level using the Penn Treebank
and the phrase information got lost during the con-
version to the dependency trees, arguments are an-
notated on head words instead of phrases in depen-
dency trees; the subtree of each head word is as-
sumed to include the same set of words as the an-
notated phrase does in phrase structure. Figure 1
shows a dependency tree that has been converted
from the corresponding phrase structure tree.
S
NP1
DT
The
NNS
results
VP
VBP
appear
PP1
IN
in
NP
NP
NN
today
POS
?s
NN
news
The results appear in today 's newsroot
NMOD SBJ LOC NMOD
NMOD
ROOT PMOD
Figure 1: Phrase vs. dependency structure
91
In the phrase structure tree, arguments of the verb
predicate appear are annotated on the phrases:
NP1 as ARG0 and PP1 as ARGM-LOC. In the de-
pendency tree, the arguments are annotated on the
head words instead: results as the ARG0 and in as
the ARGM-LOC. In this example, both PP1 and the
subtree of in consist of the same set of words {in,
today, ?s, news} (as is the case for NP1 and the
subtree of results); therefore, the phrase bound-
aries for the semantic arguments, called semantic
boundaries, are retrieved correctly from the depen-
dency tree.
Retrieving the subtrees of head words usually
gives correct semantic boundaries; however, there
are cases where the strategy does not work. For
example, if the verb predicate is a gerund or a past-
participle, it is possible that the predicate becomes
a syntactic child of the head word annotated as a
semantic argument of the predicate. In Figure 2,
the head word plant is annotated as ARG1 of the
verb predicate owned, where owned is a child of
plant in the dependency tree. Thus, retrieving the
subtree of plant would include the predicate it-
self, which is not the correct semantic boundary
for the argument (the correct boundary would be
only {The, plant}).
The plant owned by Mark
NMOD NMOD LGS PMOD
Figure 2: Past-participle example
For such cases, we need some alternative for re-
trieving the correct semantic boundaries. This is
an important issue that has not yet been thoroughly
addressed. In this paper, we first show how to con-
vert the Penn Treebank style phrase structure to
dependency structure. We then describe how to
annotate the Propbank arguments, already anno-
tated in the phrase structure, on head words in the
dependency structure. Finally, we present heuris-
tics that correctly retrieve semantic boundaries in
most cases. For our experiments, we used the en-
tire Penn Treebank (Wall Street Journal). Our ex-
periments show that it is possible to achieve an F1-
score of 99.54% for correct representation of the
semantic boundaries.
2 Related work
Ekeklint and Nivre (2007) tried to retrieve seman-
tic boundaries by adding extra arcs to dependency
trees, so the structure is no longer a tree but a
graph. They experimented with the same cor-
pus, the Penn Treebank, but used a different de-
pendency conversion tool, Penn2Malt.1 Our work
is distinguished from theirs because we keep the
tree structure but use heuristics to find the bound-
aries. Johansson (2008) also tried to find seman-
tic boundaries for evaluation of his semantic role
labeling system using dependency structure. He
used heuristics that apply to general cases whereas
we add more detailed heuristics for specific cases.
3 Converting phrase structure to
dependency structure
We used the same tool as the one used for the
CoNLL?09 shared task to automatically convert
the phrase structure trees in the Penn Treebank
to the dependency trees (Johansson and Nugues,
2007). The script gives several options for the con-
version; we mostly used the default values except
for the following options:2
? splitSlash=false: do not split slashes. This
option is taken so the dependency trees pre-
serve the same number of word-tokens as the
original phrase structure trees.
? noSecEdges=true: ignore secondary edges
if present. This option is taken so all sib-
lings of verb predicates in phrase structure
become children of the verbs in dependency
structure regardless of empty categories. Fig-
ure 3 shows the converted dependency tree,
which is produced when the secondary edge
(*ICH*) is not ignored, and Figure 4 shows
the one produced by ignoring the secondary
edge. This option is useful because NP? and
PP-2? are annotated as separate arguments of
the verb predicate paid in Propbank (NP? as
ARG1 and PP-2? as ARGM-MNR).
S
NP-1
He
VP
VBD
was
VP
VBN
paid
NP
*-1
NP*
NP
.. salary
PP
*ICH*-2
PP-2?
with ..
1http://stp.lingfil.uu.se/ nivre/research/Penn2Malt.html
2http://nlp.cs.lth.se/software/treebank converter/
92
paidHe wasroot
SBJ NMOD
ROOT NMOD
$342Ka salary with
VC NMOD
NMOD
$280Ka bonus
OBJ
NMOD
NMOD
Figure 3: When the secondary edge is not ignored
paidHe wasroot
SBJ NMOD
ROOT NMOD
$342Ka salary with
VC NMOD
NMOD
$280Ka bonus
OBJ
NMOD
ADV
Figure 4: When the secondary edge is ignored
Total 49,208 dependency trees were converted
from the Penn Treebank. Although it was pos-
sible to apply different values for other options,
we found them not helpful in finding correct se-
mantic boundaries of Propbank arguments. Note
that some of non-projective dependencies are re-
moved by ignoring the secondary edges. However,
it did not make all dependency trees projective;
our methods can be applied for either projective
or non-projective dependency trees.
4 Adding semantic roles to dependency
structure
4.1 Finding the head words
For each argument in the Propbank annotated on
a phrase, we extracted the set of words belonging
to the phrase. Let this set be Sp. In Figure 1, PP1
is the ARGM-LOC of appear so Sp is {in, today,
?s, news}. Next, we found a set of head words,
say Sd, whose subtrees cover all words in Sp (e.g.,
Sd = {in} in Figure 1). It would be ideal if there
existed one head word whose subtree covers all
words in Sp, but this is not always the case. It is
possible that Sd needs more than one head word to
cover all the words in Sp.
Figure 5 shows an algorithm that finds a set of
head words Sd whose subtrees cover all words in
Sp. For each word w in Sp, the algorithm checks
if w?s subtree gives the maximum coverage (if w?s
subtree contains more words than any other sub-
tree); if it does, the algorithm adds w to Sd, re-
moves all words in w?s subtree from Sp, then re-
peats the search. The search ends when all words
in Sp are covered by some subtree of a head word
in Sd. Notice that the algorithm searches for the
minimum number of head words by matching the
maximum coverages.
Input: Sp = a set of words for each argument
in the Propbank
Output: Sd = a set of head words whose
subtrees cover all words in Sp
Algorithm:getHeadWords(Sp)1
Sd = {}2
while Sp 6= ? do3
max = None4
foreach w ? Sp do5
if |subtree(w)| > |subtree(max)|6
then
max = w7
end8
Sd.add(max)9
Sp.removeAll(subtree(max))10
end11
return Sd12
Figure 5: Finding the min-set of head words
The algorithm guarantees to find the min-set Sd
whose subtrees cover all words in Sp. This gives
100% recall for Sd compared to Sp; however, the
precision is not guaranteed to be as perfect. Sec-
tion 5 illustrates heuristics that remove the over-
generated words so we could improve the preci-
sion as well.
4.2 Ignoring empty categories
As described in Figures 3 and 4, dependency trees
do not include any empty categories (e.g., null
elements, traces, PRO?s): the empty categories
are dropped during the conversion to the depen-
dency trees. In the Penn Treebank, 11.5% of the
Propbank arguments are annotated on empty cat-
egories. Although this is a fair amount, we de-
cided to ignore them for now since dependency
structure is not naturally designed to handle empty
categories. Nonetheless, we are in the process of
finding ways of automatically adding empty cate-
gories to dependency trees so we can deal with the
remaining of 11.5% Propbank arguments.
4.3 Handling disjoint arguments
Some Propbank arguments are disjoint in the
phrase structure so that they cannot be represented
as single head words in dependency trees. For ex-
ample in Figure 6, both NP-1? and S? are ARG1 of
the verb predicate continued but there is no head
word for the dependency tree that can represent
both phrases. The algorithm in Figure 5 naturally
93
handles this kind of disjoint arguments. Although
words in Sp are not entirely consecutive ({Yields,
on, mutual, funds, to, slide}), it iteratively finds
both head words correctly: Yields and to.
S
NP-1?
NP
Yields
PP
IN
on
NP
mutual funds
VP
VBD
continued
S?
NP
*-1
VP
TO
to
VP
slide
Yields on mutual toroot
NMOD OPRDNMOD
PMOD
ROOT
SBJ
funds continued slide
IM
Figure 6: Disjoint argument example
5 Retrieving fine-grained semantic
boundaries
There are a total of 292,073 Propbank arguments
in the Penn Treebank, and only 88% of them map
to correct semantic boundaries from the depen-
dency trees by taking the subtrees of head words.
The errors are typically caused by including more
words than required: the recall is still 100% for the
error cases whereas the precision is not. Among
several error cases, the most critical one is caused
by verb predicates whose semantic arguments are
the parents of themselves in the dependency trees
(cf. Figure 2). In this section, we present heuris-
tics to handle such cases so we can achieve preci-
sion nearly as good as the recall.
5.1 Modals
In the current dependency structure, modals (e.g.,
will, can, do) become the heads of the main verbs.
In Figure 7, will is the head of the verb predicate
remain in the dependency tree; however, it is also
an argument (ARGM-MOD) of the verb in Prop-
bank. This can be resolved by retrieving only the
head word, but not the subtree. Thus, only will is
retrieved as the ARGM-MOD of remain.
Modals can be followed by conjuncts that are
also modals. In this case, the entire coordination
is retrieved as ARGM-MOD (e.g., {may, or, may,
not} in Figure 8).
They will remain on the list
SBJ
root
VC PRD NMOD
PRD
ROOT
Figure 7: Modal example 1
He may or read the bookroot
SBJ COORD ADV NMOD
OBJ
ROOT
may not
CONJ
COORD
Figure 8: Modal example 2
5.2 Negations
Negations (e.g., not, no longer) are annotated as
ARGM-NEG in Propbank. In most cases, nega-
tions do not have any child in dependency trees,
so retrieving only the negations themselves gives
the correct semantic boundaries for ARGM-NEG,
but there are exceptions. One is where a negation
comes after a conjunction; in which case, the nega-
tion becomes the parent of the main verb. In Fig-
ure 9, not is the parent of the verb predicate copy
although it is the ARGM-NEG of the verb.
You may come but notroot
SBJ COORD
ROOT
to read
PRP
copy
VC IM CONJ COORD
Figure 9: Negation example 1
The other case is where a negation is modified by
some adverb; in which case, the adverb should
also be retrieved as well as the negation. In Fig-
ure 10, both no and longer should be retrieved as
the ARGM-NEG of the verb predicate oppose.
They no longer the legislationroot
SBJ
NMOD
OBJ
oppose
AMOD
TMP
ROOT
Figure 10: Negation example 2
5.3 Overlapping arguments
Propbank does not allow overlapping arguments.
For each predicate, if a word is included in one
argument, it cannot be included in any other argu-
ment of the predicate. In Figure 11, burdens and
in the region are annotated as ARG1 and ARGM-
LOC of the verb predicate share, respectively. The
arguments were originally annotated as two sepa-
rate phrases in the phrase structure tree; however,
94
in became the child of burdens during the conver-
sion, so the subtree of burdens includes the subtree
of in, which causes overlapping arguments.
S
NP
U.S.
VP
VBZ
encourages
S
NP
Japan
VP
TO
to
VP
VB
share
NP
NP
burdens
PP
in ..
U.S. encourages Japan
in
root
share
LOC
OPRD
to burdens
the region
NMOD
PMOD
OBJIMOBJSBJ
ROOT
Figure 11: Overlapping argument example 1
When this happens, we reconstruct the depen-
dency tree so in becomes the child of share instead
of burdens (Figure 12). By doing so, taking the
subtrees of burdens and in no longer causes over-
lapping arguments.3
U.S. encourages Japan
in
root
share
OPRD
to burdens
the region
NMOD
PMOD
OBJIMOBJSBJ
ROOT
LOC
Figure 12: Overlapping argument example 2
5.4 Verb predicates whose semantic
arguments are their syntactic heads
There are several cases where semantic arguments
of verb predicates become the syntactic heads of
the verbs. The modals and negations in the previ-
ous sections are special cases where the seman-
tic boundaries can be retrieved correctly with-
out compromising recall. The following sec-
tions describe other cases, such as relative clauses
(Section 5.4.2), gerunds and past-participles (Sec-
tion 5.4.3), that may cause a slight decrease in re-
call by finding more fine-grained semantic bound-
aries. In these cases, the subtree of the verb predi-
cates are excluded from the semantic arguments.
3This can be considered as a Treebank/Propbank dis-
agreement, which is further discussed in Sectino 6.2.
5.4.1 Verb chains
Three kinds of verb chains exist in the current
dependency structure: auxiliary verbs (including
modals and be-verbs), infinitive markers, and con-
junctions. As discussed in Section 5.1, verb chains
become the parents of their main verbs in depen-
dency trees. This indicates that when the subtree
of the main verb is to be excluded from semantic
arguments, the verb chain needs to be excluded as
well. This usually happens when the main verbs
are used within relative clauses. In addition, more
heuristics are needed for retrieving correct seman-
tic boundaries for relative clauses, which are fur-
ther discussed in Section 5.4.2.
The following figures show examples of each
kind of verb chain. It is possible that multiple verb
chains are joined with one main verb. In this case,
we find the top-most verb chain and exclude its
entire subtree from the semantic argument. In Fig-
ure 13, part is annotated as ARG1 of the verb pred-
icate gone, chained with the auxiliary verb be, and
again chained with the modal may. Since may is
the top-most verb chain, we exclude its subtree so
only a part is retrieved as the ARG1 of gone.
a part that
be
NMOD
may gone
PRDVCDEPNMOD
Figure 13: Auxiliary verb example
Figure 14 shows the case of infinitive markers.
those is annotated as ARG0 of the verb predicate
leave, which is first chained with the infinitive
marker to then chained with the verb required. By
excluding the subtree of required, only those is re-
trieved as the ARG0 of leave.
rules are
tough
root those
ROOT
on
required
SBJ
to
AMOD
leave
PRD PMOD APPO OPRD IM
Figure 14: Infinitive marker example
Figure 15 shows the case of conjunctions. people
is annotated as ARG0 of the verb predicate exceed,
which is first chained with or then chained with
meet. By excluding the subtree of meet, only peo-
ple is retrieved as the ARG0 of exceed.
When a verb predicate is followed by an ob-
ject complement (OPRD), the subtree of the object
complement is not excluded from the semantic ar-
gument. In Figure 16, distribution is annotated as
95
people
who meet
exceed
NMOD
or the
DEP NMOD
OBJ
expectation
CONJCOORD
Figure 15: Conjunction example
ARG1 of the verb predicate expected. By excluding
the subtree of expected, the object complement to
occur would be excluded as well; however, Prop-
bank annotation requires keeping the object com-
plement as the part of the argument. Thus, a dis-
tribution to occur is retrieved as the ARG1 of ex-
pected.
a distribution expected to occur
NMOD IMOPRDAPPO
Figure 16: Object complement example
5.4.2 Relative clauses
When a verb predicate is within a relative clause,
Propbank annotates both the relativizer (if present)
and its antecedent as part of the argument. For ex-
ample in Figure 15, people is annotated as ARG0
of both meet and exceed. By excluding the subtree
of meet, the relativizer who is also excluded from
the semantic argument, which is different from the
original Propbank annotation. In this case, we
keep the relativizer as part of the ARG0; thus, peo-
ple who is retrieved as the ARG0 (similarly, a part
that is retrieved as the ARG0 of gone in Figure 13).
It is possible that a relativizer is headed by a
preposition. In Figure 17, climate is annotated as
ARGM-LOC of the verb predicate made and the
relativizer which is headed by the preposition in.
In this case, both the relativizer and the preposi-
tion are included in the semantic argument. Thus,
the climate in which becomes the ARGM-LOC of
made.
the
climate in decisionsthe was
PMOD
madewhich
NMOD NMOD
LOC
DEP
VC
Figure 17: Relativizer example
5.4.3 Gerunds and past-participles
In English, when gerunds and past-participles are
used without the presence of be-verbs, they often
function as noun modifiers. Propbank still treats
them as verb predicates; however, these verbs be-
come children of the nouns they modify in the de-
pendency structure, so the heuristics discussed in
Section 5.4 and 5.4.1 need to be applied to find the
correct semantic boundaries. Furthermore, since
these are special kinds of verbs, they require even
more rigorous pruning.
When a head word, annotated to be a seman-
tic argument of a verb predicate, comes after the
verb, every word prior to the verb predicate needs
to be excluded from the semantic argument. In
Figure 18, group is annotated as ARG0 of the
verb predicate publishing, so all words prior to the
predicate (the Dutch) need to be excluded. Thus,
only group is retrieved as the ARG0 of publishing.
the Dutch publishing group
NMOD
NMOD
NMOD
Figure 18: Gerund example
When the head word comes before the verb pred-
icate, the subtree of the head word, excluding the
subtree of the verb predicate, is retrieved as the se-
mantic argument. In Figure 19, correspondence is
annotated as ARG1 of the verb predicate mailed,
so the subtree of correspondence, excluding the
subtree of mailed, is retrieved to be the argument.
Thus, correspondence about incomplete 8300s be-
comes the ARG1 of mailed.
correspondence mailed about
NMOD
NMOD
incomplete 8300s
NMOD
PMOD
Figure 19: Past-participle example 1
When the subtree of the verb predicate is imme-
diately followed by comma-like punctuation (e.g.,
comma, colon, semi-colon, etc.) and the head
word comes before the predicate, every word after
the punctuation is excluded from the semantic ar-
gument. In Figure 20, fellow is annotated as ARG1
of the verb predicate named, so both the subtree
of the verb (named John) and every word after the
comma (, who stayed for years) are excluded from
the semantic argument. Thus, only a fellow is re-
trieved as the ARG1 of named.
5.5 Punctuation
For evaluation, we built a model that excludes
punctuation from semantic boundaries for two rea-
sons. First, it is often not clear how punctuation
96
a named John who stayedfellow , for years
NMOD APPO OPRD
P
DEP TMP PMOD
NMOD
Figure 20: Past-participle example 2
needs to be annotated in either Treebank or Prop-
bank; because of that, annotation for punctuation
is not entirely consistent, which makes it hard to
evaluate. Second, although punctuation gives use-
ful information for obtaining semantic boundaries,
it is not crucial for semantic roles. In fact, some
of the state-of-art semantic role labeling systems,
such as ASSERT (Pradhan et al, 2004), give an
option for omitting punctuation from the output.
For these reasons, our final model ignores punctu-
ation for semantic boundaries.
6 Evaluations
6.1 Model comparisons
The following list describes six models used for
the experiments. Model I is the baseline approach
that retrieves all words in the subtrees of head
words as semantic boundaries. Model II to VI use
the heuristics discussed in the previous sections.
Each model inherits all the heuristics from the pre-
vious model and adds new heuristics; therefore,
each model is expected to perform better than the
previous model.
? I - all words in the subtrees (baseline)
? II - modals + negations (Sections 5.1, 5.2)
? III - overlapping arguments (Section 5.3)
? IV - verb chains + relative clauses (Sec-
tions 5.4.1, 5.4.2)
? V - gerunds + past-participles (Section 5.4.3)
? VI - excluding punctuations (Section 5.5)
The following list shows measurements used for
the evaluations. gold(arg) is the gold-standard
set of words for the argument arg. sys(arg) is
the set of words for arg produced by our system.
c(arg1, arg2) returns 1 if arg1 is equal to arg2;
otherwise, returns 0. T is the total number of ar-
guments in the Propbank.
Accuracy =
1
T
?
?
?arg
c(gold(arg), sys(arg))
Precision =
1
T
?
?
?arg
|gold(arg) ? sys(arg)|
|sys(arg)|
Recall =
1
T
?
?
?arg
|gold(arg) ? sys(arg)|
|gold(arg)|
F1 =
2 ? Precision ?Recall
Precision + Recall
Table 1 shows the results from the models us-
ing the measurements. As expected, each model
shows improvement over the previous one in
terms of accuracy and F1-score. The F1-score
of Model VI shows improvement that is statisti-
cally significant compared to Model I using t-test
(t = 149.00, p < 0.0001). The result from the
final model is encouraging because it enables us
to take full advantage of dependency structure for
semantic role labeling. Without finding the correct
semantic boundaries, even if a semantic role label-
ing system did an excellent job finding the right
head words, we would not be able to find the ac-
tual chunks for the arguments. By using our ap-
proach, finding the correct semantic boundaries is
no longer an issue for using dependency structure
for automatic semantic role labeling.
Model Accuracy Precision Recall F1
I 88.00 92.51 100 96.11
II 91.84 95.77 100 97.84
III 92.17 97.08 100 98.52
IV 95.89 98.51 99.95 99.23
V 97.00 98.94 99.95 99.44
VI 98.20 99.14 99.95 99.54
Table 1: Model comparisons (in percentage)
6.2 Error analysis
Although each model consistently shows improve-
ment on the precision, the recall is reduced a bit for
some models. Specifically, the recalls for Mod-
els II and III are not 100% but rather 99.9994%
and 99.996%, respectively. We manually checked
all errors for Models II and III and found that they
are caused by inconsistent annotations in the gold-
standard. For Model II, Propbank annotation for
ARGM-MOD was not done consistently with con-
97
junctions. For example in Figure 8, instead of an-
notating may or may not as the ARGM-MOD, some
annotations include only may and may not but not
the conjunction or. Since our system consistently
included the conjunctions, they appeared to be dif-
ferent from the gold-standard, but are not errors.
For Model III, Treebank annotation was not
done consistently for adverbs modifying nega-
tions. For example in Figure 10, longer is some-
times (but rarely) annotated as an adjective where
it is supposed to be an adverb. Furthermore,
longer sometimes becomes a child of the verb
predicate oppose (instead of being the child of no).
Such annotations made our system exclude longer
as a part of ARGM-NEG, but it would have found
them correctly if the trees were annotated consis-
tently.
There are a few cases that caused errors in Mod-
els IV and V. The most critical one is caused by PP
(prepositional phrase) attachment. In Figure 21,
enthusiasm is annotated as ARG1 of the verb pred-
icate showed, so our system retrieved the subtree
of enthusiasm, excluding the subtree of showed,
as the semantic boundary for the ARG1 (e.g., the
enthusiasm). However, Propbank originally an-
notated both the enthusiasm and for stocks as the
ARG1 in the phrase structure tree (so the preposi-
tional phrase got lost in our system).
the investors showed forenthusiasm stocks
NMOD
NMOD
SBJ ADV PMOD
Figure 21: PP-attachment example 1
This happens when there is a disagreement be-
tween Treebank and Propbank annotations: the
Treebank annotation attached the PP (for stocks)
to the verb (showed) whereas the Propbank anno-
tation attached the PP to the noun (enthusiasm).
This is a potential error in the Treebank. In this
case, we can trust the Propbank annotation and re-
construct the tree so the Treebank and Propbank
annotations agree with each other. After the re-
construction, the dependency tree would look like
one in Figure 22.
the investors showed forenthusiasm stocks
NMOD
NMOD
SBJ PMOD
ADV
Figure 22: PP-attachment example 2
7 Conclusion and future work
We have discussed how to convert phrase struc-
ture trees to dependency trees, how to find the
minimum-set of head words for Propbank argu-
ments in dependency structure, and heuristics for
retrieving fine-grained semantic boundaries. By
using our approach, we correctly retrieved the se-
mantic boundaries of 98.2% of the Propbank ar-
guments (F1-score of 99.54%). Furthermore, the
heuristics can be used to fix some of the incon-
sistencies in both Treebank and Propbank annota-
tions. Moreover, they suggest ways of reconstruct-
ing dependency structure so that it can fit better
with semantic roles.
Retrieving correct semantic boundaries is im-
portant for tasks like machine translation where
not only the head words but also all other words
matter to complete the task (Choi et al, 2009).
In the future, we are going to apply our approach
to other corpora and see how well the heuristics
work. In addition, we will try to find ways of auto-
matically adding empty categories to dependency
structure so we can deal with the full set of Prop-
bank arguments.
Acknowledgments
Special thanks are due to Professor Joakim Nivre
of Uppsala University and Claire Bonial of the
University of Colorado at Boulder for very helpful
insights. We gratefully acknowledge the support
of the National Science Foundation Grants CISE-
CRI-0551615, Towards a Comprehensive Lin-
guistic Annotation and CISE-CRI 0709167, Col-
laborative: A Multi-Representational and Multi-
Layered Treebank for Hindi/Urdu, and a grant
from the Defense Advanced Research Projects
Agency (DARPA/IPTO) under the GALE pro-
gram, DARPA/CMO Contract No. HR0011-06-C-
0022, subcontract from BBN, Inc. Any opinions,
findings, and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of the National
Science Foundation.
References
Jinho D. Choi, Martha Palmer, and Nianwen Xue.
2009. Using parallel propbanks to enhance word-
alignments. In Proceedings of ACL-IJCNLP work-
shop on Linguistic Annotation (LAW?09), pages
121?124.
98
Susanne Ekeklint and Joakim Nivre. 2007. A
dependency-based conversion of propbank. In
Proceedings of NODALIDA workshop on Building
Frame Semantics Resources for Scandinavian and
Baltic Languages (FRAME?07), pages 19?25.
Daniel Gildea. 2004. Dependencies vs. constituents
for tree-based alignment. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP?04), pages 214?221.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The conll-2009
shared task: Syntactic and semantic dependencies in
multiple languages. In Proceedings of the 13th Con-
ference on Computational Natural Language Learn-
ing (CoNLL?09), pages 1?18.
Richard Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for en-
glish. In Proceedings of the 16th Nordic Conference
of Computational Linguistics (NODALIDA?07).
Richard Johansson. 2008. Dependency-based Seman-
tic Analysis of Natural-language Text. Ph.D. thesis,
Lund University.
Jason S. Kessler and Nicolas Nicolov. 2009. Targeting
sentiment expressions through supervised ranking of
linguistic configurations. In Proceedings of the 3rd
International AAAI Conference on Weblogs and So-
cial Media (ICWSM?09).
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The penn treebank. Compu-
tational Linguistics, 19(2):313?330.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Sameer S. Pradhan, Wayne Ward, Kadri Hacioglu,
James H. Martin, and Daniel Jurafsky. 2004. Shal-
low semantic parsing using support vector machines.
In Proceedings of the Human Language Technology
Conference/North American chapter of the Associ-
ation for Computational Linguistics annual meeting
(HLT/NAACL?04).
99
Proceedings of the Fifth Law Workshop (LAW V), pages 21?29,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Analysis of the Hindi Proposition Bank using Dependency Structure
Ashwini Vaidya Jinho D. Choi Martha Palmer Bhuvana Narasimhan
Institute of Cognitive Science
University of Colorado at Boulder
{vaidyaa,choijd,mpalmer,narasimb}@colorado.edu
Abstract
This paper makes two contributions. First, we
describe the Hindi Proposition Bank that con-
tains annotations of predicate argument struc-
tures of verb predicates. Unlike PropBanks
in most other languages, the Hind PropBank
is annotated on top of dependency structure,
the Hindi Dependency Treebank. We explore
the similarities between dependency and pred-
icate argument structures, so the PropBank an-
notation can be faster and more accurate. Sec-
ond, we present a probabilistic rule-based sys-
tem that maps syntactic dependents to seman-
tic arguments. With simple rules, we classify
about 47% of the entire PropBank arguments
with over 90% confidence. These preliminary
results are promising; they show how well
these two frameworks are correlated. This can
also be used to speed up our annotations.
1 Introduction
Proposition Bank (from now on, PropBank) is a cor-
pus in which the arguments of each verb predicate
are annotated with their semantic roles (Palmer et
al., 2005). PropBank annotation has been carried
out in several languages; most of them are annotated
on top of Penn Treebank style phrase structure (Xue
and Palmer, 2003; Palmer et al, 2008). However, a
different grammatical analysis has been used for the
Hindi PropBank annotation, dependency structure,
which may be particularly suited for the analysis of
flexible word order languages such as Hindi.
As a syntactic corpus, we use the Hindi Depen-
dency Treebank (Bhatt et al, 2009). Using de-
pendency structure has some advantages. First, se-
mantic arguments1 can be marked explicitly on the
syntactic trees, so annotations of the predicate ar-
gument structure can be more consistent with the
dependency structure. Second, the Hindi Depen-
dency Treebank provides a rich set of dependency
relations that capture the syntactic-semantic infor-
mation. This facilitates mappings between syntac-
tic dependents and semantic arguments. A success-
ful mapping would reduce the annotation effort, im-
prove the inter-annotator agreement, and guide a full
fledged semantic role labeling task.
In this paper, we briefly describe our annotation
work on the Hindi PropBank, and suggest mappings
between syntactic and semantic arguments based on
linguistic intuitions. We also present a probabilistic
rule-based system that uses three types of rules to
arrive at mappings between syntactic and semantic
arguments. Our experiments show some promising
results; these mappings illustrate how well those two
frameworks are correlated, and can also be used to
speed up the PropBank annotation.
2 Description of the Hindi PropBank
2.1 Background
The Hindi PropBank is part of a multi-dimensional
and multi-layered resource creation effort for the
Hindi-Urdu language (Bhatt et al, 2009). This
multi-layered corpus includes both dependency an-
notation as well as lexical semantic information in
the form of PropBank. The corpus also produces
phrase structure representations in addition to de-
1The term ?semantic argument? is used to indicate all num-
bered arguments as well as modifiers in PropBank.
21
pendency structure. The Hindi Dependency Tree-
bank has created an annotation scheme for Hindi
by adapting labels from Panini?s Sanskrit gram-
mar (also known as CPG: Computational Paninian
Grammar; see Begum et al (2008)). Previous work
has demonstrated that the English PropBank tagset
is quite similar to English dependency trees anno-
tated with the Paninian labels (Vaidya et al, 2009).
PropBank has also been mapped to other depen-
dency schemes such as Functional Generative De-
scription (Cinkova, 2006).
2.2 Hindi Dependency Treebank
The Hindi Dependency Treebank (HDT) includes
morphological, part-of-speech and chunking infor-
mation as well as dependency relations. These are
represented in the Shakti Standard Format (SSF; see
Bharati et al (2007)). The dependency labels de-
pict relations between chunks, which are ?minimal
phrases consisting of correlated, inseparable enti-
ties? (Bharati et al, 2006), so they are not neces-
sarily individual words. The annotation of chunks
also assumes that intra-chunk dependencies can be
extracted automatically (Husain et al, 2010).
The dependency tagset consists of about 43 labels,
which can be grouped into three categories: depen-
dency relation labels, modifier labels, and labels for
non-dependencies (Bharati et al, 2009). PropBank
is mainly concerned with those labels depicting de-
pendencies in the domain of locality of verb predi-
cates. The dependency relation labels are based on
the notion of ?karaka?, defined as ?the role played by
a participant in an action?. The karaka labels, k1-5,
are centered around the verb?s meaning. There are
other labels such as rt (purpose) or k7t (location)
that are independent of the verb?s meaning.
2.3 Annotating the Hindi PropBank
The Hindi PropBank (HPB) contains the labeling of
semantic roles, which are defined on a verb-by-verb
basis. The description at the verb-specific level is
fine-grained; e.g., ?hitter? and ?hittee?. These verb-
specific roles are then grouped into broader cate-
gories using numbered arguments (ARG#). Each
verb can also have modifiers not specific to the verb
(ARGM*). The annotation process takes place in two
stages: the creation of frameset files for individual
verb types, and the annotation of predicate argu-
ment structures for each verb instance. As annota-
tion tools, we use Cornerstone and Jubilee (Choi et
al., 2010a; Choi et al, 2010b). The annotation is
done on the HDT; following the dependency anno-
tation, PropBank annotates each verb?s syntactic de-
pendents as their semantic arguments at the chunk
level. Chunked trees are conveniently displayed for
annotators in Jubilee. PropBank annotations gener-
ated in Jubilee can also be easily projected onto the
SSF format of the original dependency trees.
The HPB currently consists of 24 labels including
both numbered arguments and modifiers (Table 1).
In certain respects, the HPB labels make some dis-
tinctions that are not made in some other language
such as English. For instance, ARG2 is subdivided
into labels with function tags, in order to avoid
ARG2 from being semantically overloaded (Yi,
2007). ARGC and ARGA mark the arguments of mor-
phological causatives in Hindi, which is different
from the ARG0 notion of ?causer?. We also intro-
duce two labels to represent the complex predicate
constructions: ARGM-VLV and ARGM-PRX.
Label Description
ARG0 agent, causer, experiencer
ARG1 patient, theme, undergoer
ARG2 beneficiary
ARG3 instrument
ARG2-ATR attribute ARG2-GOL goal
ARG2-LOC location ARG2-SOU source
ARGC causer
ARGA secondary causer
ARGM-VLV verb-verb construction
ARGM-PRX noun-verb construction2
ARGM-ADV adverb ARGM-CAU cause
ARGM-DIR direction ARGM-DIS discourse
ARGM-EXT extent ARGM-LOC location
ARGM-MNR manner ARGM-MNS means
ARGM-MOD modal ARGM-NEG negation
ARGM-PRP purpose ARGM-TMP temporal
Table 1: Hindi PropBank labels.
2.4 Empty arguments in the Hindi PropBank
The HDT and HPB layers have different ways of
handling empty categories (Bhatia et al, 2010).
HPB inserts empty arguments such as PRO (empty
subject of a non-finite clause), RELPRO (empty
22
relative pronoun), pro (pro-drop argument), and
gap-pro (gapped argument). HPB annotates syn-
tactic relations between its semantic roles, notably
co-indexation of the empty argument PRO as well as
gap-pro. The example in Figure 1 shows that Mo-
han and PRO are co-indexed; thus, Mohan becomes
ARG0 of read via the empty argument PRO. There is
no dependency link between PRO and read because
PRO is inserted only in the PropBank layer.
Mohan wanted to read a book
????_?
PRO
?? ???
????
Mohan_ERG
k1
vmod
ARG1
ARG0
ARG0
????
PRO book read want
k2
ARG1
Figure 1: Empty argument example. The upper and lower
edges indicate HDT and HPB labels, respectively.
3 Comparisons between syntactic and
semantic arguments
In this section, we describe the mappings between
HDT and HPB labels based on our linguistic intu-
itions. We show that there are several broad similar-
ities between two tagsets. These mappings form the
basis for our linguistically motivated rules in Sec-
tion 4.2.3. In section 5.5, we analyze whether the
intuitions discussed in this section are borne out by
the results of our probabilistic rule-based system.
3.1 Numbered arguments
The numbered arguments correspond to ARG0-3,
including function tags associated with ARG2. In
PropBank, ARG0 and ARG1 are conceived as
framework-independent labels, closely associated
with Dowty?s Proto-roles (Palmer et al, 2010). For
instance, ARG0 corresponds to the agent, causer, or
experiencer, whether it is realized as the subject of
an active construction or as the object of an adjunct
(by phrase) of the corresponding passive. In this re-
spect, ARG0 and ARG1 are very similar to k1 and
k2 in HDT, which are annotated based on their se-
mantic roles, not their grammatical relation. On the
other hand, HDT treats the following sentences sim-
ilarly, whereas PropBank does not:
? The boy broke the window.
? The window broke.
The boy and the window are both considered k1 for
HDT, whereas PropBank labels the boy as ARG0 and
The window as ARG1. The window is not consid-
ered a primary causer as the verb is unaccusative for
Propbank. For HDT, the notion of unaccusativity is
not taken into consideration. This is an important
distinction that needs to be considered while carry-
ing out the mapping. k1 is thus ambiguous between
ARG0 and ARG1. Also, HDT makes a distinction
between Experiencer subjects of certain verbs, label-
ing them as k4a. As PropBank does not make such
a distinction, k4a maps to ARG0. The Experiencer
subject information is included in the corresponding
frameset files of the verbs. The mappings to ARG0
and ARG1 would be accurate only if they make use
of specific verb information. The mappings for other
numbered arguments as well as ARGC and ARGA are
given in Table 2.
HDT label HPB label
k1 (karta); k4a (experiencer) Arg0
k2 (karma) Arg1
k4 (beneficiary) Arg2
k1s (attribute) Arg2-ATR
k5 (source) Arg2-SOU
k2p (goal) Arg2-GOL
k3 (instrument) Arg3
mk1 (causer) ArgC
pk1 (secondary causer) ArgA
Table 2: Mappings to the HPB numbered arguments.
Note that in HDT annotation practice, k3 and k5
tend to be interpreted in a broad fashion such that
they map not only to ARG3 and ARG2-SOU, but also
to ARGM-MNS and ARGM-LOC (Vaidya and Husain,
2011). Hence, a one-to-one mapping for these la-
bels is not possible. Furthermore, the occurrence of
morphological causatives (ARGC and ARGA) is fairly
low so that we may not be able to test the accuracy
of these mappings with the current data.
3.2 Modifiers
The modifiers in PropBank are quite similar in their
definitions to certain HDT labels. We expect a fairly
high mapping accuracy, especially as these are not
verb-specific. Table 3 shows mappings between
23
HDT labels and HPB modifiers. A problematic map-
ping could be ARGM-MNR, which is quite coarse-
grained in PropBank, applying not only to adverbs
of manner, but also to infinitival adjunct clauses.
HDT label HPB label
sent-adv (epistemic adv) ArgM-ADV
rh (cause/reason) ArgM-CAU
rd (direction) ArgM-DIR
rad (discourse) ArgM-DIS
k7p (location) ArgM-LOC
adv (manner adv) ArgM-MNR
rt (purpose) ArgM-PRP
k7t (time) ArgM-TMP
Table 3: Mappings to the HPB modifiers.
3.3 Simple and complex predicates
HPB distinguishes annotations between simple and
complex predicates. Simple predicates consist of
only a single verb whereas complex predicates con-
sist of a light verb and a pre-verbal element. The
complex predicates are identified with a special label
ARGM-PRX (ARGument-PRedicating eXpresstion),
which is being used for all light verb annotations
in PropBank (Hwang et al, 2010). Figure 2 shows
an example of the predicating noun mention anno-
tated as ARGM-PRX, used with come. The predicat-
ing noun also has its own argument, matter of, in-
dicated with the HDT label r6-k1. The HDT has
two labels, r6-k1 and r6-k2, for the arguments of
the predicating noun. Hence, the argument span for
complex predicates includes not only direct depen-
dents of the verb but also dependents of the noun.
??????_?_????? ??????_?? ????_?? ?? ?_??
hearing_of_during Wed._of matter_of mention_to
k7t
k7t
pof
ARGM-PRX
ARG1
ARGM-TMP
come
???
r6-k1
ARGM-TMP
During the hearing on Wednesday, the matter was mentioned
Figure 2: Complex predicate example.
The ARGM-PRX label usually overlaps with the
HDT label pof, indicating a ?part of units? as pre-
verbal elements in complex predicates. However, in
certain cases, HPB has its own analysis for noun-
verb complex predicates. Hence, not all the nom-
inals labeled pof are labeled as ARGM-PRX. In
the example in Figure 3, the noun chunk important
progress is not considered to be an ARGM-PRX by
HPB (in this example, we have pragati hona; (lit)
progess be; to progress). The nominal for PropBank
is in fact ARG1 of the verb be, rather than a com-
posite on the verb. Additional evidence for this is
that neither the nominal nor the light verb seem to
project arguments of their own.
Important progress has been made in this work
??_???_? ?_?
k7p
pof
ARG1
ARGM-LOC
?????? ??_?? ??
this_work_LOC important_progress be_PRES
Figure 3: HDT vs. HPB on complex predicates.
4 Automatic mapping of HDT to HPB
Mapping between syntactic and semantic structures
has been attempted in other languages. The Penn
English and Chinese Treebanks consist of several se-
mantic roles (e.g., locative, temporal) annotated on
top of Penn Treebank style phrase structure (Marcus
et al, 1994; Xue and Palmer, 2009). The Chinese
PropBank specifies mappings between syntactic and
semantic arguments in frameset files (e.g., SBJ ?
ARG0) that can be used for automatic mapping (Xue
and Palmer, 2003). However, these Chinese map-
pings are limited to certain types of syntactic argu-
ments (mostly subjects and objects). Moreover, se-
mantic annotations on the Treebanks are done inde-
pendently from PropBank annotations, which causes
disagreement between the two structures.
Dependency structure transparently encodes rela-
tions between predicates and their arguments, which
facilitates mappings between syntactic and seman-
tic arguments. Hajic?ova? and Kuc?erova? (2002) tried
to project PropBank semantic roles onto the Prague
Dependency Treebank, and showed that the projec-
tion is not trivial. The same may be true to our case;
however, our goal is not to achieve complete map-
pings between syntactic and semantic arguments,
24
but to find a useful set of mappings that can speed
up our annotation. These mappings will be applied
to our future data as a pre-annotation stage, so that
annotators do not need to annotate arguments that
have already been automatically labeled by our sys-
tem. Thus, it is important to find mappings with high
precision and reasonably good recall.
In this section, we present a probabilistic rule-
based system that identifies and classifies semantic
arguments in the HPB using syntactic dependents in
the HDT. This is still preliminary work; our system
is expected to improve as we annotate more data and
do more error analysis.
4.1 Argument identification
Identifying semantic arguments of each verb pred-
icate is relatively easy given the dependency Tree-
bank. For each verb predicate, we consider all syn-
tactic dependents of the predicate as its semantic
arguments (Figure 4). For complex predicates, we
consider the syntactic dependents of both the verb
and the predicating noun (cf. Section 3.3).
?? ???? ? ?? ??_ ? ?? ??? ??_ ??
Kishori Haridwar_from Delhi come_be
k1
k5
k2p
ARG2-GOL
ARG2-SOU
ARG0
Kishori came from Haridwar to Delhi
Figure 4: Simple predicate example.
With our heuristics, we get a precision of 99.11%,
a recall of 95.50%, and an F1-score of 97.27% for
argument identification. Such a high precision is
expected as the annotation guidelines for HDT and
HPB generally follow the same principles of iden-
tifying syntactic and semantic arguments of a verb.
About 4.5% of semantic arguments are not identi-
fied by our method. Table 4 shows distributions of
the most frequent non-identified arguments.
Label Dist. Label Dist. Label Dist.
ARG0 3.21 ARG1 0.90 ARG2? 0.09
Table 4: Distributions of non-identified arguments caused
by PropBank empty categories (in %).
Most of the non-identified argument are antecedents
of PropBank empty arguments. As shown in Fig-
ure 1, the PropBank empty argument has no depen-
dency link to the verb predicate. Identifying such
arguments requires a task of empty category reso-
lution, which will be explored as future work. Fur-
thermore, we do not try to identify PropBank empty
arguments for now, which will also be explored later.
4.2 Argument classification
Given the identified semantic arguments, we classify
their semantic roles. Argument classification is done
by using three types of rules. Deterministic rules are
heuristics that are straightforward given dependency
structure. Empirically-derived rules are generated
by measuring statistics of dependency features in as-
sociation with semantic roles. Finally, linguistically-
motivated rules are derived from our linguistic intu-
itions. Each type of rule has its own strength; how
to combine them is the art we need to explore.
4.2.1 Deterministic rule
Only one deterministic rule is used in our system.
When an identified argument has a pof dependency
relation with its predicate, we classify the argu-
ment as ARGM-PRX. This emphasizes the advan-
tage of using our dependency structure: classifying
ARGM-PRX cannot be done automatically in most
other languages where there is no information pro-
vided for light verb constructions. This determin-
istic rule is applied before any other type of rule.
Therefore, we do not generate further rules to clas-
sify the ARGM-PRX label.
4.2.2 Empirically-derived rules
Three kinds of features are used for the generation of
empirically-derived rules: predicate ID, predicate?s
voice type, and argument?s dependency label. The
predicate ID is either the lemma or the roleset ID
of the predicate. Predicate lemmas are already pro-
vided in HDT. When we use predicate lemmas, we
assume no manual annotation of PropBank. Thus,
rules generated from predicate lemmas can be ap-
plied to any future data without modification. When
we use roleset ID?s, we assume that sense annota-
tions are already done. PropBank includes anno-
tations of coarse verb senses, called roleset ID?s,
that differentiate each verb predicate with different
25
senses (Palmer et al, 2005). A verb predicate can
form several argument structures with respect to dif-
ferent senses. Using roleset ID?s, we generate more
fine-grained rules that are specific to those senses.
The predicate?s voice type is either ?active? or
?passive?, also provided in HDT. There are not many
instances of passive construction in our current data,
which makes it difficult to generate rules general
enough for future data. However, even with the lack
of training instances, we find some advantage of us-
ing the voice feature in our experiments. Finally, the
argument?s dependency label is the dependency la-
bel of an identified argument with respect to its pred-
icate. This feature is straightforward for the case of
simple predicates. For complex predicates, we use
the dependency labels of arguments with respect to
their syntactic heads, which can be pre-verbal ele-
ments. Note that rules generated with complex pred-
icates contain slightly different features for predicate
lemmas as well; instead of using predicate lemmas,
we use joined tags of the predicate lemmas and the
lemmas of pre-verbal elements.
ID V Drel PBrel #
come a k1 ARG0 1
come a k5 ARG2-SOU 1
come a k2p ARG2-GOL 1
come mention a k7t ARGM-TMP 2
come mention a r6-k1 ARG1 1
Table 5: Rules generated by the examples in Figures 4 and
2. The ID, V, and Drel columns show predicate ID, predicate?s
voice type, and argument?s dependency label. The PBrel col-
umn shows the PropBank label of each argument. The # column
shows the total count of each feature tuple being associated with
the PropBank label. ?a? stands for active voice.
Table 5 shows a set of rules generated by the exam-
ples in Figures 4 (come) and 2 (come mention). No
rule is generated for ARGM-PRX because the label
is already covered by our deterministic rule (Sec-
tion 4.2.1). When roleset ID?s are used in place of
the predicate ID, come and come mention are re-
placed with A.03 and A.01, respectively. These
rules can be formulated as a function rule such that:
rule(id, v, drel) = argmax i P (pbreli)
where P (pbreli) is a probability of the predicted
PropBank label pbreli, given a tuple of features
(id, v, drel). The probability is measured by es-
timating a maximum likelihood of each PropBank
label being associated with the feature tuple. For
example, a feature tuple (come, active, k1) can be
associated with two PropBank labels, ARG0 and
ARG1, with counts of 8 and 2, respectively. In this
case, the maximum likelihoods of ARG0 and ARG1
being associated with the feature tuple is 0.8 and 0.2;
thus rule(come, active, k1) = ARG0.
Since we do not want to apply rules with low con-
fidence, we set a threshold to P (pbrel), so predic-
tions with low probabilities can be filtered out. Find-
ing the right threshold is a task of handling the pre-
cision/recall trade-off. For our experiments, we ran
10-fold cross-validation to find the best threshold.
4.2.3 Linguistically-motivated rules
Linguistically-motivated rules are applied to argu-
ments that the deterministic rule and empirically-
derived rules cannot classify. These rules capture
general correlations between syntactic and seman-
tic arguments for each predicate, so they are not as
fine-grained as empirically-derived rules, but can be
helpful for predicates not seen in the training data.
The rules are manually generated by our annota-
tors and specified in frameset files. Table 6 shows
linguistically-motivated rules for the predicate ?A
(come)?, specified in the frameset file, ?A-v.xml?.3
Roleset Usage Rule
A.01 to come
k1 ? ARG1
k2p ? ARG2-GOL
A.03 to arrive
k1 ? ARG1
k2p ? ARG2-GOL
k5 ? ARG2-SOU
A.02 light verb No rule provided
Table 6: Rules for the predicate ?A (come)?.
The predicate ?A? has three verb senses and each
sense specifies a different set of rules. For instance,
the first rule of A.01 maps a syntactic dependent
with the dependency label k1 to a semantic ar-
gument with the semantic label ARG1. Note that
frameset files include rules only for numbered ar-
guments. Most of these rules should already be in-
cluded in the empirically-derived rules as we gain
3See Choi et al (2010a) for details about frameset files.
26
more training data; however, for an early stage of
annotation, these rules provide useful information.
5 Experiments
5.1 Corpus
All our experiments use a subset of the Hindi Depen-
dency Treebank, distributed by the ICON?10 con-
test (Husain et al, 2010). Our corpus contains about
32,300 word tokens and 2,005 verb predicates, in
which 546 of them are complex predicates. Each
verb predicate is annotated with a verse sense speci-
fied in its corresponding frameset file. There are 160
frameset files created for the verb predicates. The
number may seem small compared to the number
of verb predicates. This is because we do not cre-
ate separate frameset files for light verb construc-
tions, which comprise about 27% of the predicate
instances (see the example in Table 6).
All verb predicates are annotated with argument
structures using PropBank labels. A total of 5,375
arguments are annotated. Since there is a relatively
small set of data, we do not make a separate set for
evaluations. Instead, we run 10-fold cross-validation
to evaluate our rule-based system.
5.2 Evaluation of deterministic rule
First, we evaluate how well our deterministic rule
classifies the ARGM-PRX label. Using the determin-
istic rule, we get a 94.46% precision and a 100%
recall on ARGM-PRX. The 100% recall is expected;
the precision implies that about 5.5% of the time,
light verb annotations in the HPB do not agree with
the complex predicate annotations (pof relation) in
the HDT (cf. Section 3.3). More analysis needs to
be done to improve the precision of this rule.
5.3 Evaluation of empirically-derived rules
Next, we evaluate our empirically-derived rules with
respect to the different thresholds set for P (pbreli).
In general, the higher the threshold is, the higher
and lower the precision and recall become, respec-
tively. Figure 5 shows comparisons between preci-
sion and recall with respect to different thresholds.
Notice that a threshold of 1.0, meaning that using
only rules with 100% confidence, does not give the
highest precision. This is because the model with
this high of a threshold overfits to the training data.
Rules that work well in the training data do not nec-
essarily work as well on the test data.
   0 0.2 0.4 0.6 0.8 1
   
   30
40
50
60
70
80
Threshold
Acc
ura
cy (
in %
)
R
F1
P
0.93
Figure 5: Accuracies achieved by the empirically derived
rules using (lemma, voice, label) features. P, R, and F1
stand for precisions, recalls, and F1-scores, respectively.
We need to find a threshold that gives a high preci-
sion (so annotators do not get confused by the au-
tomatic output) while maintaining a good recall (so
annotations can go faster). With a threshold of 0.93
using features (lemma, voice, dependency label), we
get a precision of 90.37%, a recall of 44.52%, and
an F1-score of 59.65%. Table 7 shows accuracies
for all PropBank labels achieved by a threshold of
0.92 using roleset ID?s instead of predicate?s lem-
mas. Although the overall precision stays about the
same, we get a noticeable improvement in the over-
all recall using roleset ID?s. Note that some labels
are missing in Table 7. This is because either they
do not occur in our current data (ARGC and ARGA)
or we have not started annotating them properly yet
(ARGM-MOD and ARGM-NEG).
5.4 Evaluation of linguistically-motivated rules
Finally, we evaluate the impact of the linguistically-
motivated rules. Table 8 shows accuracies achieved
by the linguistically motivated rules applied after the
empirically derived rules. As expected, the linguis-
tically motivated rules improve the recall of ARGN
significantly, but bring a slight decrease in the pre-
cision. This shows that our linguistic intuitions are
generally on the right track. We may combine some
of the empirically derived rules with linguistically
motivated rules together in the frameset files so an-
notators can take advantage of both kinds of rules in
the future.
27
Dist. P R F1
ALL 100.00 90.59 47.92 62.69
ARG0 17.50 95.83 67.27 79.05
ARG1 27.28 94.47 61.62 74.59
ARG2 3.42 81.48 37.93 51.76
ARG2-ATR 2.54 94.55 40.31 56.52
ARG2-GOL 1.61 64.29 21.95 32.73
ARG2-LOC 0.87 90.91 22.73 36.36
ARG2-SOU 0.83 78.26 42.86 55.38
ARG3 0.08 0.00 0.00 0.00
ARGM-ADV 3.50 31.82 3.93 7.00
ARGM-CAU 1.44 50.00 5.48 9.88
ARGM-DIR 0.43 100.00 18.18 30.77
ARGM-DIS 1.63 26.67 4.82 8.16
ARGM-EXT 1.42 0.00 0.00 0.00
ARGM-LOC 10.77 83.80 27.42 41.32
ARGM-MNR 6.00 57.14 9.18 15.82
ARGM-MNS 0.79 77.78 17.50 28.57
ARGM-PRP 2.15 65.52 17.43 27.54
ARGM-PRX 10.75 94.46 100.00 97.15
ARGM-TMP 7.01 74.63 14.04 23.64
Table 7: Labeling accuracies achieved by the empirically de-
rived rules using (roleset ID, voice, label) features and a thresh-
old of 0.92. The accuracy for ARGM-PRX is achieved by the
deterministic rule. The Dist. column shows a distribution of
each label.
Dist. P R F1
ALL 100.00 89.80 55.28 68.44
ARGN 54.12 91.87 72.36 80.96
ARGM 45.88 85.31 35.14 49.77
ARGN w/o LM 93.63 58.76 72.21
Table 8: Labeling accuracies achieved by the linguistically
motivated rules. The ARGN and ARGM rows show statistics of
all numbered arguments and modifiers combined, respectively.
The ?ARGN w/o LM? row shows accuracies of ARGN achieved
only by the empirically derived rules.
5.5 Error anlaysis
The precision and recall results for ARG0 and ARG1,
are better than expected, despite the complexity of
the mapping (Section 3.1). This is because they oc-
cur most often in the corpus, so enough rules can
be extracted. The other numbered arguments are
closely related to particular types of verbs (e.g., mo-
tion verbs for ARG2-GOL|SOU). Our linguistically
motivated rules are more effective for these types
of HPB labels. We would expect the modifiers to
be mapped independently of the verb, but our ex-
periments show that the presence of the verb lemma
feature enhances the performance of modifiers. Al-
though section 3.2 expects one-to-one mappings for
modifiers, it is not the case in practice.
We observe that the interpretation of labels in an-
notation practice is important. For example, our sys-
tem performs poorly for ARGM-ADV because the la-
bel is used for various sentential modifiers and can
be mapped to as many as four HDT labels. On the
other hand, HPB makes some fine-grained distinc-
tions. For instance, means and causes are distin-
guished using ARGM-CAU and ARGM-MNS labels, a
distinction that HDT does not make. In the example
in Figure 6, we find that aptitude with is assigned to
ARGM-MNS, but gets the cause label rh in HDT.
Rajyapal can call upon any party with his aptitude
???????
Rajyapal
???
his
?? ??_ ?
aptitude_with
?? ??_ ??
any_EMPH
????_ ??
party_DAT
????_ ????_ ?
call_can_be
Figure 6: Means vs. cause example.
6 Conclusion and future work
We provide an analysis of the Hindi PropBank anno-
tated on the Hindi Dependency Treebank. There is
an interesting correlation between dependency and
predicate argument structures. By analyzing the
similarities between the two structures, we find rules
that can be used for automatic mapping of syntactic
and semantic arguments, and achieve over 90% con-
fidence for almost half of the data. These rules will
be applied to our future data, which will make the
annotation faster and possibly more accurate.
We plan to use different sets of rules generated by
different thresholds to see which rule set leads to the
most effective annotation. We also plan to develop
a statistical semantic role labeling system in Hindi,
once we have enough training data. In addition, we
will explore the possibility of using existing lexical
resource such as WordNet (Narayan et al, 2002) to
improve our system.
Acknowledgements
This work is supported by NSF grants CNS- 0751089, CNS-
0751171, CNS-0751202, and CNS-0751213. Any opinions,
findings, and conclusions or recommendations expressed in this
material are those of the authors and do not necessarily reflect
the views of the National Science Foundation.
28
References
Rafiya Begum, Samar Husain, Arun Dhwaj, Dipti Misra
Sharma, Lakshmi Bai, and Rajeev Sangal. 2008. De-
pendency annotation scheme for indian languages. In
In Proceedings of the 3rd International Joint Confer-
ence on Natural Language Processing, IJCNLP?08.
Akshar Bharati, Dipti Misra Sharma, Lakshmi Bai, and
Rajeev Sangal. 2006. AnnCorra: Guidelines for POS
and Chunk Annotation for Indian Languages. Techni-
cal report, IIIT Hyderabad.
Akshar Bharati, Rajeev Sangal, and Dipti Misra Sharma.
2007. Ssf: Shakti standard format guide. Technical
report, IIIT Hyderabad.
Akshara Bharati, Dipti Misra Sharma, Samar Husain,
Lakshmi Bai, Rafiya Begam, and Rajeev Sangal.
2009. Anncorra : Treebanks for indian languages,
guidelines for annotating hindi treebank. Technical re-
port, IIIT Hyderabad.
Archna Bhatia, Rajesh Bhatt, Bhuvana Narasimhan,
Martha Palmer, Owen Rambow, Dipti Misra Sharma,
Michael Tepper, Ashwini Vaidya, and Fei Xia. 2010.
Empty categories in a hindi treebank. In Proceedings
of the 7th International Conference on Language Re-
sources and Evaluation (LREC?10), pages 1863?1870.
Rajesh Bhatt, Bhuvana Narasimhan, Martha Palmer,
Owen Rambow, Dipti Sharma, and Fei Xia. 2009. A
Multi-Representational and Multi-Layered Treebank
for Hindi/Urdu. In In the Proceedings of the Third Lin-
guistic Annotation Workshop held in conjunction with
ACL-IJCNLP 2009.
Jinho D. Choi, Claire Bonial, and Martha Palmer. 2010a.
Propbank frameset annotation guidelines using a ded-
icated editor, cornerstone. In Proceedings of the 7th
International Conference on Language Resources and
Evaluation, LREC?10, pages 3650?3653.
Jinho D. Choi, Claire Bonial, and Martha Palmer. 2010b.
Propbank instance annotation guidelines using a ded-
icated editor, jubilee. In Proceedings of the 7th In-
ternational Conference on Language Resources and
Evaluation, LREC?10, pages 1871?1875.
Silvie Cinkova. 2006. From PropBank to EngVALLEX:
Adapting PropBank-Lexicon to the Valency Theory of
Functional Generative Description. In Proceedings
of the fifth International conference on Language Re-
sources and Evaluation (LREC 2006), Genova, Italy.
Eva Hajic?ova? and Ivona Kuc?erova?. 2002. Argu-
ment/valency structure in propbank, lcs database and
prague dependency treebank: A comparative pi-
lot study. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation,
LREC?02, pages 846?851.
Samar Husain, Prashanth Mannem, Bharat Ram Ambati,
and Phani Gadde. 2010. The ICON-2010 tools contest
on Indian language dependency parsing. In Proceed-
ings of ICON-2010 Tools Contest on Indian Language
Dependency Parsing, ICON?10, pages 1?8.
Jena D. Hwang, Archna Bhatia, Claire Bonial, Aous
Mansouri, Ashwini Vaidya, Nianwen Xue, and Martha
Palmer. 2010. PropBank Annotation of Multilingual
Light Verb Constructions. In Proceedings of the Lin-
guistic Annotation Workshop at ACL 2010.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert Macintyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The penn tree-
bank: Annotating predicate argument structure. In
ARPA Human Language Technology Workshop, pages
114?119.
Dipak Narayan, Debasri Chakrabarti, Prabhakar Pande,
and Pushpak Bhattacharyya. 2002. An experience
in building the indo wordnet - a wordnet for hindi.
In Proceedings of the 1st International Conference on
Global WordNet.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71?
106.
Martha Palmer, Olga Babko-Malaya, Ann Bies, Mona
Diab, Mohamed Maamouri, Aous Mansouri, and Wa-
jdi Zaghouani. 2008. A pilot arabic propbank. In Pro-
ceedings of the 6th International Language Resources
and Evaluation, LREC?08, pages 28?30.
Martha Palmer, Daniel Gildea, and Nianwen Xue. 2010.
Semantic role labeling. In Graeme Hirst, editor, Syn-
thesis Lectures on Human Language Technologies.
Morgan and Claypool.
Ashwini Vaidya and Samar Husain. 2011. A classifica-
tion of dependencies in the Hindi/Urdu Treebank. In
Presented at the Workshop on South Asian Syntax and
Semantics, Amherst, MA.
Ashwini Vaidya, Samar Husain, and Prashanth Mannem.
2009. A karaka based dependency scheme for En-
glish. In Proceedings of the CICLing-2009, Mexico
City, Mexico.
Nianwen Xue and Martha Palmer. 2003. Annotating the
propositions in the penn chinese treebank. In Proceed-
ings of the 2nd SIGHAN workshop on Chinese lan-
guage processing, SIGHAN?03, pages 47?54.
Nianwen Xue and Martha Palmer. 2009. Adding seman-
tic roles to the chinese treebank. Natural Language
Engineering, 15(1):143?172.
Szu-Ting Yi. 2007. Automatic Semantic Role Labeling.
Ph.D. thesis, University of Pennsylvania.
29
Proceedings of the ACL 2011 Workshop on Relational Models of Semantics (RELMS 2011), pages 37?45,
Portland, Oregon, USA, June 23, 2011. c?2011 Association for Computational Linguistics
Transition-based Semantic Role Labeling
Using Predicate Argument Clustering
Jinho D. Choi
Department of Computer Science
University of Colorado at Boulder
choijd@colorado.edu
Martha Palmer
Department of Linguistics
University of Colorado at Boulder
mpalmer@colorado.edu
Abstract
This paper suggests two ways of improving
semantic role labeling (SRL). First, we intro-
duce a novel transition-based SRL algorithm
that gives a quite different approach to SRL.
Our algorithm is inspired by shift-reduce pars-
ing and brings the advantages of the transition-
based approach to SRL. Second, we present
a self-learning clustering technique that effec-
tively improves labeling accuracy in the test
domain. For better generalization of the sta-
tistical models, we cluster verb predicates by
comparing their predicate argument structures
and apply the clustering information to the
final labeling decisions. All approaches are
evaluated on the CoNLL?09 English data. The
new algorithm shows comparable results to
another state-of-the-art system. The cluster-
ing technique improves labeling accuracy for
both in-domain and out-of-domain tasks.
1 Introduction
Semantic role labeling (SRL) has sparked much in-
terest in NLP (Shen and Lapata, 2007; Liu and
Gildea, 2010). Lately, dependency-based SRL has
shown advantages over constituent-based SRL (Jo-
hansson and Nugues, 2008). Two main benefits can
be found. First, dependency parsing is much faster
than constituent parsing, whereas constituent pars-
ing is usually considered to be a bottleneck to SRL in
terms of execution time. Second, dependency struc-
ture is more similar to predicate argument struc-
ture than phrase structure because it specifically de-
fines relations between a predicate and its arguments
with labeled arcs. Unlike constituent-based SRL
that maps phrases to semantic roles, dependency-
based SRL maps headwords to semantic roles be-
cause there is no phrasal node in dependency struc-
ture. This may lead to a concern about getting the
actual semantic chunks back, but Choi and Palmer
(2010) have shown that it is possible to recover the
original chunks from the headwords with minimal
loss, using a certain type of dependency structure.
Traditionally, either constituent or dependency-
based, semantic role labeling is done in two steps,
argument identification and classification (Gildea
and Jurafsky, 2002). This is from a general be-
lief that each step requires a different set of fea-
tures (Xue and Palmer, 2004), and training these
steps in a pipeline takes less time than training them
as a joint-inference task. However, recent machine
learning algorithms can deal with large scale vector
spaces without taking too much training time (Hsieh
et al, 2008). Furthermore, from our experience in
dependency parsing, handling these steps together
improves accuracy in identification as well as clas-
sification (unlabeled and labeled attachment scores
in dependency parsing). This motivates the develop-
ment of a new semantic role labeling algorithm that
treats these two steps as a joint inference task.
Our algorithm is inspired by shift-reduce pars-
ing (Nivre, 2008). The algorithm uses several transi-
tions to identify predicates and their arguments with
semantic roles. One big advantage of the transition-
based approach is that it can use previously identi-
fied arguments as features to predict the next argu-
ment. We apply this technique to our approach and
achieve comparable results to another state-of-the-
art system evaluated on the same data sets.
37
NO-PRED
( ?1 , ?2, j, ?3, [i|?4], A )? ( [?1|j], ?2, i, ?3, ?4 , A )
?j. oracle(j) 6= predicate
SHIFT
( ?1 , ?2, j, [i|?3], ?4, A )? ( [?2|j], [ ] , i, [ ] , ?3, A )
?j. oracle(j) = predicate ? ?1 = [ ] ? ?4 = [ ]
NO-ARC?
( [?1|i], ?2 , j, ?3, ?4, A )? ( ?1 , [i|?2], j, ?3, ?4, A )
?j. oracle(j) = predicate ? ?i.oracle(i, j) = {i 6? j}
NO-ARC?
( ?1, ?2, j, ?3 , [i|?4], A )? ( ?1, ?2, j, [?3|i], ?4 , A )
?j. oracle(j) = predicate ? ?i.oracle(i, j) = {j 6? i}
LEFT-ARC?L
( [?1|i], ?2 , j, ?3, ?4, A )? ( ?1 , [i|?2], j, ?3, ?4, A ? {i
L
? j} )
?j. oracle(j) = predicate ? ?i.oracle(i, j) = {i
L
? j}
RIGHT-ARC?L
( ?1, ?2, j, ?3 , [i|?4], A )? ( ?1, ?2, j, [?3|i], ?4 , A ? {j
L
? i} )
?j. oracle(j) = predicate ? ?i.oracle(i, j) = {j
L
? i}
Table 1: Transitions in our bidirectional top-down search algorithm. For each row, the first line shows a transition and
the second line shows preconditions of the transition.
For better generalization of the statistical models,
we apply a self-learning clustering technique. We
first cluster predicates in test data using automati-
cally generated predicate argument structures, then
cluster predicates in training data by using the previ-
ously found clusters as seeds. Our experiments show
that this technique improves labeling accuracy for
both in-domain and out-of-domain tasks.
2 Transition-based semantic role labeling
Dependency-based semantic role labeling can be
viewed as a special kind of dependency parsing in
the sense that both try to find relations between
word pairs. However, they are distinguished in two
major ways. First, unlike dependency parsing that
tries to find some kind of relation between any word
pair, semantic role labeling restricts its search only
to top-down relations between predicate and argu-
ment pairs. Second, dependency parsing requires
one head for each word, so the final output is a tree,
whereas semantic role labeling allows multiple pred-
icates for each argument. Thus, not all dependency
parsing algorithms, such as a maximum spanning
tree algorithm (Mcdonald and Pereira, 2006), can be
naively applied to semantic role labeling.
Some transition-based dependency parsing algo-
rithms have been adapted to semantic role labeling
and shown good results (Henderson et al, 2008;
Titov et al, 2009). However, these algorithms are
originally designed for dependency parsing, so are
not necessarily customized for semantic role label-
ing. Here, we present a novel transition-based algo-
rithm dedicated to semantic role labeling. The key
difference between this algorithm and most other
transition-based algorithms is in its directionality.
Given an identified predicate, this algorithm tries to
find top-down relations between the predicate and
the words on both left and right-hand sides, whereas
other transition-based algorithms would consider
words on either the left or the right-hand side, but
not both. This bidirectional top-down search makes
more sense for semantic role labeling because predi-
cates are always assumed to be the heads of their ar-
guments, an assumption that cannot be generalized
to dependency parsing, and arguments can appear
either side of the predicate.
Table 1 shows transitions used in our algorithm.
All parsing states are represented as tuples (?1, ?2,
p, ?3, ?4, A), where ?1..4 are lists of word indices
and p is either a word index of the current predi-
cate candidate or @ indicating no predicate candi-
date. ?1,4 contain indices to be compared with p and
?2,3 contain indices already compared with p. A is a
set of labeled arcs representing previously identified
arguments with respect to their predicates. ? and
? indicate parsing directions. L is a semantic role
label, and i, j represent indices of their correspond-
ing word tokens. The initial state is ([ ], [ ], 1, [ ],
[2, . . . , n], ?), where w1 and wn are the first and the
last words in a sentence, respectively. The final state
is (?1, ?2, @, [ ], [ ],A), i.e., the algorithm terminates
when there is no more predicate candidate left.
38
John
1
wants
2
to
3
buy
4
a
5
car
6
Root
0
SBJ
ROOT
OPRD
OBJ
IM
NMOD
A0 A1
A0 A1
Figure 1: An example of a dependency tree with semantic roles. The upper and lower arcs stand for syntactic and
semantic dependencies, respectively. SBJ, OBJ, OPRD, IM, NMOD stand for a subject, object, object predicative,
infinitive marker, and noun-modifier. A0, A1 stand for ARG0, ARG1 in PropBank (Palmer et al, 2005).
Transition ?1 ?2 p ?3 ?4 A
0 [ ] [ ] 1 [ ] [2..6] ?
1 NO-PRED [1] [ ] 2 [ ] [3..6]
2 LEFT-ARC [ ] [1] 2 [ ] [3..6] A ? {1?A0? 2}
3 RIGHT-ARC [ ] [1] 2 [3] [4..6] A ? {2 ?A1? 3}
4 NO-ARC [ ] [1] 2 [3..4] [5..6]
5 NO-ARC [ ] [1] 2 [3..5] [6]
6 NO-ARC [ ] [1] 2 [3..6] [ ]
7 SHIFT [1..2] [ ] 3 [ ] [4..6]
8 NO-PRED [1..3] [ ] 4 [ ] [5..6]
9 NO-ARC [1..2] [3] 4 [ ] [5..6]
10 NO-ARC [1] [2..3] 4 [ ] [5..6]
11 LEFT-ARC [ ] [1..3] 4 [ ] [5..6] A ? {1?A0? 4}
12 NO-ARC [ ] [1..3] 4 [5] [6]
13 RIGHT-ARC [ ] [1..3] 4 [5..6] [ ] A ? {4 ?A1? 6}
14 SHIFT [1..4] [ ] 5 [ ] [6]
15 NO-PRED [1..5] [ ] 6 [ ] [ ]
16 NO-PRED [1..6] [ ] @ [ ] [ ]
Table 2: Parsing states generated by our algorithm for the example in Figure 1.
The algorithm uses six kinds of transitions. NO-
PRED is performed when an oracle identifies wj as
not a predicate. All other transitions are performed
when wj is identified as a predicate. SHIFT is per-
formed when both ?1 and ?4 are empty, meaning
that there are no more argument candidates left for
the predicate wj . NO-ARC is performed when wi
is identified as not an argument of wj . LEFT-ARCL
and RIGHT-ARCL are performed when wi is identi-
fied as an argument of wj with a label L. These tran-
sitions can be performed in any order as long as their
preconditions are satisfied. For our experiments, we
use the following generalized sequence:
[ (NO-PRED)? ? (LEFT-ARC?L |NO-ARC
?)? ?
(RIGHT-ARC?L |NO-ARC
?)? ? SHIFT ]?
Notice that this algorithm does not take separate
steps for argument identification and classification.
By adding the NO-ARC transitions, we successfully
merge these two steps together without decrease in
labeling accuracy.1 Since each word can be a predi-
cate candidate and each predicate considers all other
words as argument candidates, a worst-case com-
plexity of the algorithm is O(n2). To reduce the
complexity, Zhao et al (2009) reformulated a prun-
ing algorithm introduced by Xue and Palmer (2004)
for dependency structure by considering only direct
dependents of a predicate and its ancestors as ar-
gument candidates. This pruning algorithm can be
easily applied to our algorithm: the oracle can pre-
filter such dependents and uses the information to
perform NO-ARC transitions without consulting sta-
tistical models.
1We also experimented with the traditional approach of
building separate classifiers for identification and classification,
which did not lead to better performance in our case.
39
Table 2 shows parsing states generated by our al-
gorithm. Our experiments show that this algorithm
gives comparable results against another state-of-
the-art system.
3 Predicate argument clustering
Some studies showed that verb clustering informa-
tion could improve performance in semantic role la-
beling (Gildea and Jurafsky, 2002; Pradhan et al,
2008). This is because semantic role labelers usually
perform worse on verbs not seen during training, for
which the clustering information can provide useful
features. Most previous studies used either bag-of-
words or syntactic structure to cluster verbs; how-
ever, this may or may not capture the nature of predi-
cate argument structure, which is more semantically
oriented. Thus, it is preferable to cluster verbs by
their predicate argument structures to get optimized
features for semantic role labeling.
In this section, we present a self-learning clus-
tering technique that effectively improves labeling
accuracy in the test domain. First, we perform se-
mantic role labeling on the test data using the algo-
rithm in Section 2. Next, we cluster verbs in the test
data using predicate argument structures generated
by our semantic role labeler (Section 3.2). Then, we
cluster verbs in the training data using the verb clus-
ters we found in the test data (Section 3.3). Finally,
we re-run our semantic role labeler on the test data
using the clustering information. Our experiments
show that this technique gives improvement to la-
beling accuracy for both in and out-of domain tasks.
3.1 Projecting predicate argument structure
into vector space
Before clustering, we need to project the predicate
argument structure of each verb into vector space.
Two kinds of features are used to represent these
vectors: semantic role labels and joined tags of
semantic role labels and their corresponding word
lemmas. Figure 2 shows vector representations of
predicate argument structures of verbs, want and
buy, in Figure 1.
Initially, all existing and non-existing features are
assigned with a value of 1 and 0, respectively. How-
ever, assigning equal values to all existing features
is not necessarily fair because some features have
want 1 1 1 1 00s 0s
buy 1 1 1 0 10s 0s
A0 A1 john:A0 to:A1 car:A1... ...Verb
Figure 2: Projecting the predicate argument structure of
each verb into vector space.
higher confidence, or are more important than the
others; e.g., ARG0 and ARG1 are generally predicted
with higher confidence than modifiers, nouns give
more important information than some other gram-
matical categories, etc. Instead, we assign each ex-
isting feature with a value computed by the follow-
ing equations:
s(lj |vi) =
1
1 + exp(?score(lj |vi))
s(mj , lj) =
{
1 (wj 6= noun)
exp( count(mj ,lj)?
?k count(mk,lk)
)
vi is the current verb, lj is the j?th label of vi, and
mj is lj?s corresponding lemma. score(lj |vi) is a
score of lj being a correct argument label of vi; this
is always 1 for training data and is provided by our
statistical models for test data. Thus, s(lj |vi) is an
approximated probability of lj being a correct argu-
ment label of vi, estimated by the logistic function.
s(mj , lj) is equal to 1 if wj is not a noun. If wj is
a noun, it gets a value ? 1 given a maximum likeli-
hood of mj being co-occurred with lj .2
With the vector representation, we can apply any
kind of clustering algorithm (Hofmann and Puzicha,
1998; Kamvar et al, 2002). For our experiments,
we use k-best hierarchical clustering for test data,
and k-means clustering for training data.
3.2 Clustering verbs in test data
Given automatically generated predicate argument
structures in the test data, we apply k-best hierar-
chical clustering; that is, a relaxation of classical hi-
erarchical agglomerative clustering (from now on,
HAC; Ward (1963)), to find verb clusters. Unlike
HAC that merges a pair of clusters at each iteration,
k-best hierarchical clustering merges k-best pairs at
2Assigning different weights for nouns resulted in more
meaningful clusters in our experiments. We will explore addi-
tional grammatical category specific weighting schemes in fu-
ture work.
40
each iteration (Lo et al, 2009). Instead of merging a
fixed number of k-clusters, we use a threshold to dy-
namically determine the top k-clusters. Our studies
indicate that this technique produces almost as fine-
grained clusters as HAC, yet converges much faster.
Our algorithm for k-best hierarchical clustering is
presented in Algorithm 1. thup is a threshold that de-
termines which k-best pairs are to be merged (in our
case, kup = 0.8). sim(ci, cj) is a similarity between
clusters ci and cj . For our experiments, we use co-
sine similarity with average-linkage. It is possible
that other kinds of similarity metrics would work
better, which we will explore as future work. Con-
ditions in line 15 ensure that each cluster is merged
with at most one other cluster at each iteration, and
conditions in line 17 force at least one cluster to
be merged with one other cluster at each iteration.
Thus, the algorithm is guaranteed to terminate after
at most (n? 1) iterations.
When the algorithm terminates, it returns a set of
one cluster with different hierarchical levels. For
our experiments, we set another threshold, thlow, for
early break-out: if there is no cluster pair whose sim-
ilarity is greater than thlow, we terminate the algo-
rithm (in our case, thlow = 0.7). A cluster set gen-
erated by this early break-out contains several unit
clusters that are not merged with any other cluster.
All of these unit clusters are discarded from the set
to improve set quality. This is reasonable because
our goal is not to cluster all verbs but to find a useful
set of verb clusters that can be mapped to verbs in
training data, which can lead to better performance
in semantic role labeling.
3.3 Clustering verbs in training data
Given the verb clusters we found in the test data,
we search for verbs that are similar to these clusters
in the training data. K-means clustering (Hartigan,
1975) is a natural choice for this case because we
already know k-number of center clusters to begin
with. Each verb in the training data is compared with
all verb clusters in the test data, and merged with the
cluster that gives the highest similarity. To maintain
the quality of the clusters, we use the same thresh-
old, thlow, to filter out verbs in the training data that
are not similar enough to any verb cluster in the test
data. By doing so, we keep only verbs that are more
likely to be helpful for semantic role labeling.
input : C = [c1, .., cn]: ci is a unit cluster.
thup ? R: threshold.
output: C? = [c1, .., cm]: cj is a unit or merged
cluster, where m ? n.
begin1
while |C| > 1 do2
L? list()3
for i ? [1, |C| ? 1] do4
for j ? [i+ 1, |C|] do5
t? (i, j, sim(ci, cj))6
L.add(t)7
end8
end9
descendingSortBySimilarity(L)10
S ? set()11
for k ? [1, |L|] do12
t? L.get(k)13
i? t(0); j ? t(1); sim? t(2)14
if i ? S or j ? S then15
continue16
if k = 1 or sim > thup then17
C.add(ci ? cj); S.add(i, j)18
C.remove(ci, cj)19
else20
break21
end22
end23
end24
end25
Algorithm 1: k-best hierarchical clustering.
4 Features
4.1 Baseline features
For a baseline approach, we use features similar to
ones used by Johansson and Nugues (2008). All fea-
tures are assumed to have dependency structures as
input. Table 3 shows n-gram feature templates used
for our experiments (f: form, m: lemma, p: POS tag,
d: dependency label). warg andwpred are the current
argument and predicate candidates. hd(w) stands for
the head of w, lm(w), rm(w) stand for the leftmost,
rightmost dependents of w, and ls(w), rs(w) stand
for the left-nearest, right-nearest siblings of w, with
respect to the dependency structures. Some of these
features can be presented as a joined feature; e.g., a
combination of warg?s POS tag and lemma.
41
Word tokens Features
warg, wpred f,m,p,d
warg?1, hd, lm, rm, ls, rs (warg) m,p
wpred?1, hd, lm, rm (wpred) m,p
Table 3: N -gram feature templates.
Besides the n-gram features, we use several struc-
tural features such as dependency label set, subcat-
egorization, POS path, dependency path, and depen-
dency depth. Dependency label set features are de-
rived by collecting all dependency labels of wpred?s
direct dependents. Unlike Johansson and Nugues,
we decompose subcategorization features into two
parts: one representing the left-hand side and the
other representing the right-hand side dependencies
of wpred. For the predicate wants in Figure 3, we
generate ??SBJ and ????OPRD as separate subcategoriza-
tion features.
wants
PRP:John TO:to
VB:buy
SBJ OPRD
IM
Figure 3: Dependency structure used for subcategoriza-
tion, path, and depth features.
We also decompose path features into two parts:
given the lowest common ancestor (LCA) of warg
and wpred, we generate path features from warg to
the LCA and from the LCA to wpred, separately.
For example, the predicate buy and the argument
John in Figure 3 have a LCA at wants, so we gen-
erate two sets of path features, {?PRP, ?TO?VB}
with POS tags, and {?SBJ, ?OPRD?IM} with depen-
dency labels. Such decompositions allow more gen-
eralization of those features; even if one part is not
matched to the current parsing state, the other part
can still participate as a feature. Throughout our
experiments, these generalized features give slightly
higher labeling accuracy than ungeneralized features
although they form a smaller feature space.
In addition, we apply dependency path features to
wpred?s highest verb chain, which often shares ar-
guments with the predicate (e.g., John is a shared
argument of the predicate buy and its highest verb
chain wants). To retrieve the highest verb chain, we
apply a simple heuristic presented below. The func-
tion getHighestVerbChain takes a predicate,
pred, as input and returns its highest verb chain,
vNode, as output. If there is no verb chain for the
predicate, it returns null instead. Note that this
heuristic is designed to work with dependency rela-
tions and labels described by the CoNLL?09 shared
task (Hajic? et al, 2009).
func getHighestVerbChain(pred)
vNode = pred;
regex = "CONJ|COORD|IM|OPRD|VC";
while (regex.matches(vNode.deprel))
vNode = vNode.head;
if (vNode != pred) return vNode;
else return null;
Dependency depth features are a reduced form of
path features. Instead of specifying POS tags or de-
pendency labels, we indicate paths with their depths.
For instance, John and buy in Figure 3 have a depen-
dency depth feature of ?1?2, which implies that the
depth between John and its LCA (wants) is 1, and
the depth between the LCA and buy is 2.
Finally, we use four kinds of binary features: if
warg is a syntactic head of wpred, if wpred is a syn-
tactic head ofwarg, ifwpred is a syntactic ancestor of
warg, and if wpred?s verb chain has a subject. Each
feature gets a value of 1 if true; otherwise, it gets a
value of 0.
4.2 Dynamic and clustering features
All dynamic features are derived by using previ-
ously identified arguments. Two kinds of dynamic
features are used for our experiments. One is a la-
bel of the very last predicted numbered argument of
wpred. For instance, the parsing state 3 in Table 2
uses a label A0 as a feature to make its prediction,
wants
A1
? to, and the parsing states 4 to 6 use a label
A1 as a feature to make their predictions, NO-ARC?s.
With this feature, the oracle can narrow down the
scope of expected arguments of wpred. The other is
a previously identified argument label of warg. The
existence of this feature implies that warg is already
identified as an argument of some other predicate.
For instance, when warg = John and wpred = buy in
Table 2, a label A0 is used as a feature to make the
prediction, John
A0
? buy, because John is already
identified as an A0 of wants.
42
Finally, we use wpred?s cluster ID as a feature. The
dynamic and clustering features combine a very
small portion of the entire feature set, but still give a
fair improvement to labeling accuracy.
5 Experiments
5.1 Corpora
All models are trained on Wall Street Journal sec-
tions 2-21 and developed on section 24 using auto-
matically generated lemmas and POS tags, as dis-
tributed by the CoNLL?09 shared task (Hajic? et al,
2009). CoNLL?09 data contains semantic roles for
both verb and noun predicates, for which we use
only ones related to verb predicates. Furthermore,
we do not include predicate sense classification as a
part of our task, which is rather a task of word sense
disambiguation than semantic role labeling.
For in-domain and out-of-domain evaluations,
WSJ section 23 and the Brown corpus are used, also
distributed by CoNLL?09. To retrieve automatically
generated dependency trees as input to our semantic
role labeler, we train our open source dependency
parser, called ClearParser3, on the training set and
run the parser on the evaluation sets. ClearParser
uses a transition-based dependency parsing algo-
rithm that gives near state-of-the-art results (Choi
and Palmer, 2011), and mirrors our SRL algorithm.
5.2 Statistical models
We use Liblinear L2-L1 SVM for learning; a linear
classification algorithm using L2 regularization and
L1 loss function. This algorithm is designed to han-
dle large scale data: it assumes the data to be lin-
early separable so does not use any kind of kernel
space (Hsieh et al, 2008). As a result, it significantly
reduces training time compared to typical SVM, yet
performs accurately. For our experiments, we use
the following learning parameters: c = 0.1 (cost),
e = 0.2 (termination criterion), B = 0 (bias).
Since predicate identification is already provided
in the CoNLL?09 data, we do not train NO-PRED.
SHIFT does not need to be trained in general be-
cause the preconditions of SHIFT can be checked
deterministically without consulting statistical mod-
els. NO-ARC? and LEFT-ARC?L are trained to-
gether using the one-vs-all method as are NO-ARC?
3http://code.google.com/p/clearparser/
and RIGHT-ARC?L . Even with multi-classifications,
it takes less than two minutes for the entire training
using Liblinear.
5.3 Accuracy comparisons
Tables 4 and 5 show accuracy comparisons between
three models evaluated on the WSJ and Brown cor-
pora, respectively. ?Baseline? uses the features de-
scribed in Section 4.1. ?+Dynamic? uses all baseline
features and the dynamic features described in Sec-
tion 4.2. ?+Cluster? uses all previous features and the
clustering feature. Even though our baseline system
already has high performance, each model shows an
improvement over its previous model (very slight
for ?+Cluster?). The improvement is greater for the
out-of-domain task, implying that the dynamic and
clustering features help more on new domains. The
differences between ?Baseline? and ?+Dynamic? are
statistically significant for both in and out-of domain
tasks (Wilcoxon signed-rank test, treating each sen-
tence as an individual event, p ? 0.025).
Task P R F1
Baseline
AI 92.57 88.44 90.46
AI+AC 87.20 83.31 85.21
+Dynamic
AI 92.38 88.76 90.54
AI+AC 87.33 83.91 85.59?
+Cluster
AI 92.62 88.90 90.72
AI+AC 87.43 83.92 85.64
JN (2008) AI+AC 88.46 83.55 85.93
Table 4: Labeling accuracies evaluated on the WSJ (P:
precision, R: recall, F1: F1-score, all in %). ?AI? and
?AC? stand for argument identification and argument clas-
sification, respectively.
Task P R F1
Baseline
AI 90.96 81.57 86.01
AI+AC 77.11 69.14 72.91
+Dynamic
AI 90.90 82.25 86.36
AI+AC 77.41 70.05 73.55?
+Cluster
AI 90.87 82.43 86.44
AI+AC 77.47 70.28 73.70
JN (2008) AI+AC 77.67 69.63 73.43
Table 5: Labeling accuracies evaluated on the Brown.
We also compare our results against another state-
of-the-art system. Unfortunately, no other system
43
has been evaluated with our exact environmental set-
tings. However, Johansson and Nugues (2008), who
showed state-of-the-art performance in CoNLL?08,
evaluated their system with settings very similar to
ours. Their task was exactly the same as ours;
given predicate identification, they evaluated their
dependency-based semantic role labeler for argu-
ment identification and classification on the WSJ
and Brown corpora, distributed by the CoNLL?05
shared task (Carreras and Ma`rquez, 2005). Since
the CoNLL?05 data was not dependency-based, they
applied heuristics to build dependency-based predi-
cate argument structures. Their converted data may
appear to be a bit different from the CoNLL?09 data
we use (e.g., hyphenated words are tokenized by the
hyphens in CoNLL?09 data whereas they are not in
CoNLL?05 data), but semantic role annotations on
headwords should look very similar.
Johansson and Nugues?s results are presented as
JN (2008) in Tables 4 and 5. Our final system shows
comparable results against this system. These re-
sults are meaningful in two ways. First, JN used a
graph-based dependency parsing algorithm that gave
higher parsing accuracy for these test sets than the
transition-based dependency parsing algorithm used
in ClearParser (about 0.9% better in labeled attach-
ment score). Even with poorer parse output, our SRL
system performed as well as theirs. Furthermore,
our system used only one set of features, which
makes the feature engineering easier than JN?s ap-
proach that used different sets of features for argu-
ment identification and classification.
6 Conclusion and future work
This paper makes two contributions. First, we in-
troduce a transition-based semantic role labeling al-
gorithm that shows comparable performance against
another state-of-the-art system. The new algorithm
takes advantage of using previous predictions as fea-
tures to make the next predictions. Second, we
suggest a self-learning clustering technique that im-
proves labeling accuracy slightly in both the do-
mains. The clustering technique shows potential for
improving performance in other new domains.
These preliminary results are promising; however,
there is still much room for improvement. Since our
algorithm is transition-based, many existing tech-
niques such as k-best ranking (Zhang and Clark,
2008) or dynamic programming (Huang and Sagae,
2010) designed to improve transition-based parsing
can be applied. We can also apply different kinds of
clustering algorithms to improve the quality of the
verb clusters. Furthermore, more features, such as
named entity tags or dependency labels, can be used
to form a better representation of feature vectors for
the clustering.
One of the strongest motivations for designing our
transition-based SRL system is to develop a joint-
inference system between dependency parsing and
semantic role labeling. Since we have already de-
veloped a dependency parser, ClearParser, based
on a parallel transition-based approach, it will be
straightforward to integrate this SRL system with the
parser. We will also explore the possiblity of adding
empty categories during semantic role labeling.
7 Related work
Nivre (2008) introduced several transition-based de-
pendency parsing algorithms that have been widely
used. Johansson and Nugues (2008) and Zhao
et al (2009) presented dependency-based semantic
role labelers showing state-of-the-art performance
for the CoNLL?08 and ?09 shared tasks in English.
Scheible (2010) clustered predicate argument struc-
tures using EM training and the MDL principle.
Wagner et al (2009) used predicate argument clus-
tering to improve verb sense disambiguation.
Acknowledgments
We gratefully acknowledge the support of the
National Science Foundation Grants CISE-IIS-
RI-0910992, Richer Representations for Machine
Translation, a subcontract from the Mayo Clinic and
Harvard Children?s Hospital based on a grant from
the ONC, 90TR0002/01, Strategic Health Advanced
Research Project Area 4: Natural Language Pro-
cessing, and a grant from the Defense Advanced
Research Projects Agency (DARPA/IPTO) under
the GALE program, DARPA/CMO Contract No.
HR0011-06-C-0022, subcontract from BBN, Inc.
Any opinions, findings, and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the views
of the National Science Foundation.
44
References
X. Carreras and L. Ma`rquez. 2005. Introduction to the
conll-2005 shared task: semantic role labeling. In Pro-
ceedings of the Ninth Conference on Computational
Natural Language Learning.
J. D. Choi and M. Palmer. 2010. Retrieving correct se-
mantic boundaries in dependency structure. In Pro-
ceedings of ACL workshop on Linguistic Annotation.
J. D. Choi and M. Palmer. 2011. Getting the most out
of transition-based dependency parsing. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of
semantic roles. Computational Linguistics, 28(3).
J. Hajic?, M. Ciaramita, R. Johansson, D. Kawahara,
M. A. Mart??, L. Ma`rquez, A. Meyers, J. Nivre, S. Pado?,
J. S?te?pa?nek, P. Stran?a?k, M. Surdeanu, N. Xue, and
Y. Zhang. 2009. The conll-2009 shared task: Syntac-
tic and semantic dependencies in multiple languages.
In Proceedings of the 13th Conference on Computa-
tional Natural Language Learning: Shared Task.
J. A. Hartigan. 1975. Clustering Algorithms. New York:
John Wiley & Sons.
J. Henderson, P. Merlo, G. Musillo, and I. Titov. 2008.
A latent variable model of synchronous parsing for
syntactic and semantic dependencies. In Proceedings
of the Twelfth Conference on Computational Natural
Language Learning.
T. Hofmann and J. Puzicha. 1998. Statistical models for
co-occurrence data. Technical report, Massachusetts
Institute of Technology.
C. Hsieh, K. Chang, C. Lin, S. S. Keerthi, and S. Sun-
dararajan. 2008. A dual coordinate descent method
for large-scale linear svm. In Proceedings of the 25th
international conference on Machine learning.
L. Huang and K. Sagae. 2010. Dynamic programming
for linear-time incremental parsing. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics.
R. Johansson and P. Nugues. 2008. Dependency-based
semantic role labeling of PropBank. In Proceedings of
the 2008 Conference on Empirical Methods in Natural
Language Processing.
S. D. Kamvar, D. Klein, and C. D. Manning. 2002. Inter-
preting and extending classical agglomerative cluster-
ing algorithms using a model-based approach. In Pro-
ceedings of the 9th International Conference on Ma-
chine Learning.
D. Liu and D. Gildea. 2010. Semantic role features for
machine translation. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics.
C. Lo, J. Luo, and M. Shieh. 2009. Hardware/software
codesign of resource constrained real-time systems. In
Proceedings of the 5th International Conference on In-
formation Assurance and Security.
R. Mcdonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Pro-
ceedings of the Annual Meeting of the European Amer-
ican Chapter of the Association for Computational
Linguistics.
J. Nivre. 2008. Algorithms for deterministic incremen-
tal dependency parsing. Computational Linguistics,
34(4).
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1).
S. Pradhan, W. Ward, and J. H. Martin. 2008. Towards
robust semantic role labeling. Computational Linguis-
tics: Special Issue on Semantic Role Labeling, 34(2).
C. Scheible. 2010. An evaluation of predicate argument
clustering using pseudo-disambiguation. In Proceed-
ings of the 7th conference on International Language
Resources and Evaluation.
D. Shen and M. Lapata. 2007. Using semantic roles
to improve question answering. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing and on Computational Natural Lan-
guage Learning.
I. Titov, J. Henderson, P. Merlo, and G. Musillo. 2009.
Online graph planarisation for synchronous parsing of
semantic and syntactic dependencies. In Proceedings
of the 21st International Joint Conference on Artificial
Intelligence.
W. Wagner, H. Schmid, and S. Schulte im Walde.
2009. Verb sense disambiguation using a predicate-
argument-clustering model. In Proceedings of the
CogSci Workshop on Distributional Semantics beyond
Concrete Concepts.
J. H. Ward. 1963. Hierarchical grouping to optimize an
objective function. Journal of the American Statistical
Association, 58(301).
N. Xue and M. Palmer. 2004. Calibrating features for se-
mantic role labeling. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing.
Y. Zhang and S. Clark. 2008. A tale of two parsers: in-
vestigating and combining graph-based and transition-
based dependency parsing using beam-search. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing.
H. Zhao, W. Chen, and C. Kit. 2009. Semantic depen-
dency parsing of NomBank and PropBank: An effi-
cient integrated approach via a large-scale feature se-
lection. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing.
45
Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 153?162,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Dynamic Knowledge-Base Alignment for Coreference Resolution
Jiaping Zheng Luke Vilnis Sameer Singh Jinho D. Choi Andrew McCallum
School of Computer Science
University of Massachusetts
Amherst MA 01003
{jzheng,luke,sameer,jdchoi,mccallum}@cs.umass.edu
Abstract
Coreference resolution systems can benefit
greatly from inclusion of global context,
and a number of recent approaches have
demonstrated improvements when precom-
puting an alignment to external knowledge
sources. However, since alignment itself
is a challenging task and is often noisy, ex-
isting systems either align conservatively,
resulting in very few links, or combine the
attributes of multiple candidates, leading
to a conflation of entities. Our approach
instead performs joint inference between
within-document coreference and entity
linking, maintaining ranked lists of candi-
date entities that are dynamically merged
and reranked during inference. Further, we
incorporate a large set of surface string vari-
ations for each entity by using anchor texts
from the web that link to the entity. These
forms of global context enables our system
to improve classifier-based coreference by
1.09 B3 F1 points, and improve over the
previous state-of-art by 0.41 points, thus
introducing a new state-of-art result on the
ACE 2004 data.
1 Introduction
Coreference resolution is the task of identifying
sets of noun phrase mentions from a document
that refer to the same real-world entities. For ex-
ample, in the following excerpt: ?The Chicago
suburb of Arlington Heights is the first stop for
?George W. Bush?1 today. ?The Texas governor?2
stops in ?Gore?s home state?3 of ?Tennessee?4 this
afternoon. . . ?, (m1,m2) and (m3,m4) define the
coreferent pairs. Coreference resolution forms an
important component for natural language process-
ing and information extraction pipelines due to its
utility in relation extraction, cross-document coref-
erence, text summarization, and question answer-
ing. The task of coreference is challenging for
automated systems as the local information con-
tained in the document is often not enough to accu-
rately disambiguate mentions, for example, corefer-
encing (m1,m2) requires identifying that George
W. Bush (m1) is the governor of Texas (m2), and
similarly for (m3,m4). External knowledge-bases
such as FrameNet (Baker et al, 1998), Wikipedia,
Yago (Suchanek et al, 2007), and Freebase (Bol-
lacker et al, 2008), can be used to provide global
context, and there is a strong need for coreference
resolution systems to accurately use such sources
for disambiguation.
Incorporating external knowledge bases into
coreference has been the subject of active recent
research. Ponzetto and Strube (2006) and Ratinov
and Roth (2012) precompute a fixed alignment of
the mentions to the knowledge base entities. The
attributes of these entities are used during corefer-
ence by incorporating them in the mention features.
Since alignment of mentions to the external enti-
ties is itself a difficult task, these systems favor
high-precision linking. Unfortunately, this results
in fewer alignments, and improvements are only
shown on mentions that are easier to align and core-
fer (such as the non-transcript documents in Rati-
nov and Roth (2012)). Alternatively, Rahman and
Ng (2011) link each mention to multiple entities in
the knowledge base, improving recall at the cost
of lower precision; the attributes of all the linked
entities are aggregated as features. Although this
approach is more robust to noise in the documents,
the features of a mention merge the different as-
pects of the entities, for example a ?Michael Jordan?
mention will contain features for both the scientist
and basketball personas.
Instead of fixing the alignment of the mentions to
the knowledge base, our proposed approach main-
tains a ranked list of candidate entities for each
mention. To expand the set of surface strings that
153
may be used to refer to each entity, the attributes
of each candidate contain anchor texts (the visible
text) of the links on the web that refer to that entity
candidate. When mentions are compared during
inference, we use the features computed from the
top ranked entity candidate of the antecedent men-
tion. As mentions are merged, the ranked lists of
candidate entities are also merged and reranked, of-
ten changing the top-ranked entity candidate used
in subsequent comparisons. The large set of sur-
face string variations and constant reranking of the
entity candidates during inference allows our ap-
proach to correct mistakes in alignment and makes
external information applicable to a wider variety
of mentions.
Our paper provides the following contributions:
(1) an approach that jointly reasons about both
within-doc entities and their alignment to KB-
entities by dynamically adjusting a ranked list of
candidate alignments, during coreference, (2) Uti-
lization of a larger set of surface string variations
for each entity candidate by using links that appear
all over the web (Spitkovsky and Chang, 2012), (3)
A combination of these approaches that improves
upon a competitive baseline without a knowledge
base by 1.09 B3 F1 points on the ACE 2004 data,
and outperforms the state-of-the-art coreference
system (Stoyanov and Eisner, 2012) by 0.41 B3
F1 points, and (4) Accurate predictions on docu-
ments that are difficult for coreference, such as the
transcript documents that were omitted from the
evaluation in Ratinov and Roth (2012), and docu-
ments that contain a large number of mentions.
2 Baseline Pairwise System
In this section we describe a variant of a commonly-
used coreference resolution system that does not
utilize external knowledge sources. This widely
adopted model casts the problem as a series of
binary classifications (Soon et al, 2001; Ng and
Cardie, 2002; Ponzetto and Strube, 2006; Bengston
and Roth, 2008; Stoyanov et al, 2010). Given
a document with its mentions, the system itera-
tively checks each mention mj for coreference with
preceding mentions using a classifier. A corefer-
ence link may be created between mj and one of
these preceding mentions using one of the follow-
ing strategies. The CLOSESTLINK (Soon et al,
2001) method picks the closest mention to mj that
is positively classified, while the BESTLINK (Ng
and Cardie, 2002) method links mj to the preced-
Types Features
String-
Similarity
mention string match, head string match,
head substring match, head word pair, men-
tion substring match, acronym
Syntax number match, gender match, apposition,
relative pronoun, mention type, modifier
match, head word POS tags
Semantic synonym, antonym, hypernym, modifier re-
lations, both mentions are surrounded by a
verb meaning ?to say?, demonym match
Other predicted entity type, predicted entity type
match, both mentions in same sentence, sen-
tence/token distance, capitalization
Table 1: Features of the baseline model. Extensions
to Bengston and Roth (2008) are italicized.
ing mention that was scored the highest. If none
of the preceding mentions are classified as positive
(for CLOSESTLINK), or are above a threshold (for
BESTLINK), then mj is left unlinked. After all the
mentions have been processed, the links are used
to generate a transitive closure that corresponds to
the recognized entities in the document.
2.1 Pairwise Mention Features
The features used to train our classifier are similar
to those in Bengston and Roth (2008), including
lexical, syntactical, semantic, predicted NER types,
etc., with the exclusion of their ?learned features?
that require additional classifiers. Further, we in-
clude features that compare the mention strings, the
distance between the two mentions in terms of the
number of sentences and tokens, and the POS tags
of the head words. We also use the conjunctions of
these features as in Bengston and Roth (2008), as
well as the BESTLINK approach. The complete set
of features are listed in Table 1.
The training for our system is similar
to Bengston and Roth (2008). The positive train-
ing examples are generated from mentions and
their immediate preceding antecedent. The neg-
ative examples are generated from mentions and
all their preceding non-coreferent mentions. If the
mention is not a pronoun, preceding pronouns are
not used to create training examples, and they are
also excluded during inference. In contrast to aver-
aged perceptron used in Bengston and Roth (2008),
our baseline system is trained using hinge-loss, `2-
regularized SVM.
2.2 Merging Pairwise Features
When a mention mj is compared against a preced-
ing mention mi, information from other mentions
154
that are already coreferent with mi may be helpful
in disambiguating mj as they may contain infor-
mation that is not available from mi. Let M be
the mentions between mi and mj that are coref-
erent with mi. Let mq ? M be the mention that
is closest to mj . All the features from the pair
(mq,mj), except those that characterize one men-
tion (for example, mention type of mj), are added
to the features between (mi,mj). This extends a
similar approach by Lee et al (2011) that merges
only the attributes of mentions (such as gender, but
not all pairwise features).
2.3 Pruning Comparisons During Training
A potential drawback of including all the negative
examples as in Bengston and Roth (2008) is that
the negative instances far outnumber the positive
ones, which is challenging for training a classifier.
In their system, the positive training examples only
constitute 1.6% of the total training instances. By
contrast, Soon et al (2001) reduce the number of
negative instances by using only mentions between
the mention and its closest coreferent pair as neg-
ative examples. Instead of just using the closest
coreferent mention, we extend this approach to
use the k closest of coreferent preceding mentions,
where k is tuned using the development data.
3 Dynamic Linking to Knowledge-Base
In this section, we describe our approach to coref-
erence resolution that incorporates external knowl-
edge sources. The approach is an extension of the
pairwise model described earlier, with the inclusion
of a ranked list of entities, and using a larger set of
surface string variations.
3.1 Algorithm
We describe our overall approach in Algorithm 1.
The system assumes that the data is annotated with
true mention boundaries and mention types. We
additionally tokenize the document text and tag the
tokens with their parts of speech for use as features.
First, an empty entity candidate list is created for
each mention in the document. For each proper
noun mention, we query a knowledge base for an
ordered list of Wikipedia articles that may refer
to it, and add these to the mention?s candidate list.
Other mentions? candidates lists are left empty.
After this pre-processing, each mention mi
is compared against its preceding mentions
m1 . . .mi?1 and their top-ranked entity candi-
Algorithm 1 Dynamic Linking to Wikipedia
1: Input: Mentions {mj}
2: Initialize blank entity lists {Em} . Section 3.2
3: for m ? Proper Noun Mentions do
4: LINKWIKIPEDIA(m, Em) . Section 3.2
5: POPULATEENTITYATTRS(Em) . Section 3.3
6: end for
7: for mi ?Mentions do
8: Antecedents? {m1...mi?1}
9: for m? ? Antecedents do
10: t? TOPRANKEDATTRS(Em?) . Section 3.4
11: s? SCORE(m?, mi, t) . Section 3.4
12: Scoresm?? s
13: end for
14: m? ? argmaxm? Scoresm?15: if Scoresm? > threshold then
16: MARKCOREFERENT(m?, mi)
17: MERGEENTITYLISTS(Em? , Emi ) . Section 3.418: end if
19: end for
20: return Coreferent mention clusters
date using a classifier. Amongst antecedents
m1 . . .mi?1 that score above a threshold, the
highest-scoring one mj is marked as coreferent
with mi and the two candidate lists that correspond
to mi and mj are merged. Merging two mentions
results in the merging and reranking of their respec-
tive entity candidate lists, described below. If no
antecedents score above a threshold, we leave the
mention in its singleton cluster.
3.2 Linking to Wikipedia
To create the initial entity candidate lists for
proper noun mentions, we query a knowledge base
searcher (Dalton and Dietz, 2013) with the text
of these mentions. These queries return scored,
ranked lists of entity candidates (Wikipedia arti-
cles), which we associate with each proper noun
mention, leaving the rest of the candidate lists
empty. Linking is often noisy, so only selecting the
high-precision links as in Ratinov and Roth (2012)
results in too few matches, while picking an aggre-
gation of all links results in more noise due to lower
precision (Rahman and Ng, 2011). Additionally,
since linking is often performed in pre-processing,
two mentions that are determined coreferent dur-
ing inference could still be linked to different KB
entities. To avoid these problems, we keep a list of
candidate links for each mention, merging the lists
when two mentions are determined coreferent, and
rerank this list during inference.
3.3 Populating Entity Attributes
After linking to Wikipedia, we have a list of can-
didate KB entities for each mention. Each entity
155
has access to external information keyed on the
Wikipedia article, but this information could more
generally come from any knowledge base. Given
these entities, there are many possible features that
may be used for disambiguation of the mentions,
such as gender and fine-grained Wikipedia cate-
gories as used by Ratinov and Roth (2012), how-
ever most of these features may not be relevant to
the task of within-document coreference. Instead,
an important resource for linking non-proper men-
tions of an entity is to identify the possible name
variations of the entity. For example, it would be
useful to know that Massachusetts is also referred
to as ?The 6th State?, however this information is
not readily available from Wikipedia.1
We instead use the corpus described
in Spitkovsky and Chang (2012) that con-
sists of anchor texts of links to Wikipedia that
appear on web pages. This collection of anchor
texts is sufficiently extensive to cover many
common misspellings of entity names, as well as
many name variations missing from Wikipedia.
For example, for the entity ?Massachusetts?, our
anchor texts include misspellings like ?Massachus-
setts? and ?Messuchusetts?, and the (debatably)
affectionate nickname of ?Taxachusetts??none of
which are found in Wikipedia. Using these anchor
texts, each entity candidate provides a rich set of
name variations that we use for disambiguation, as
described in the next section.
3.4 Inference with Dynamic Linking
The input to our inference algorithm consists of a
number of mentions, a list of ranked entity candi-
dates for the proper noun mentions that are present
in the KB, and a list of attributes (in this case, name
variations) for each entity candidate.
Scoring: Our underlying model is a pairwise
classification approach as described in Section 2.
Similar to existing coreference systems such as
Bengston and Roth (2008) and Rahman and Ng
(2011), we perform coreference resolution using
greedy left-to-right pairwise mention classification,
clustering each mention with its highest-scoring
antecedent (or leaving it as a singleton temporarily
if no score is above a threshold). We add the same
additional features and perform feature merging
operation (Section 2.2) as in our baseline system.
1Some of this information is available as redirects and
from links within Wikipedia, however these do not accurately
reflect all the variations of the name.
The top-ranked entity candidate of the an-
tecedent mention is used during coreference to
provide additional features for the pairwise classi-
fier. Only using the top-ranked entity candidate al-
lows the system to maintain a consistent one entity
per cluster hypothesis, reducing the noise resulting
from conflated entities. The attributes for this top-
ranked entity consist of name variations. We add a
binary feature, and conjunctions of this with other
features, if the text of the right mention matches
one of these name variations.
Entity List Merging: Once a mention pair is
scored as coreferent, their corresponding entity can-
didates are merged. Merging is performed by sim-
ply combining the two lists of candidates. Note that
there is only one candidate list for a given group of
coreferent mentions at any point in inference: if m1
and m2 have been previously marked as coreferent,
and m3 is marked as coreferent with m2, m1?s en-
tity candidates will then contain those from m3 for
future classification decisions.
Re-Ranking: After the two entity candidate lists
are merged, we rerank the candidates to identify
the top-ranked one. We sort the new list of candi-
date entities by the number of times each candidate
occurs in the list, breaking ties by their original
relevance from the KB. For example, if two men-
tions disagree on the top-ranked KB search result,
but agree on the second one, after being clustered
they will both use the second search result when
creating feature vectors for future coreference de-
cisions. Even though other candidates besides the
top-ranked one are ignored for a single classifica-
tion decision, they may become top-ranked after
merging with later candidate sets.
This approach allows our system to use the inter-
mediate results of coreference resolution to re-link
mentions to KB entities, reducing the noise and
contradictory features from incorrect links. Addi-
tionally, features from the KB are added to non-
proper noun mentions once those mentions are
linked with a populated entity, allowing the results
of coreference to enrich non-proper noun mentions
with KB-based features. The initial proper noun
queries effectively seed the linking process, and
KB data is then dynamically spread to the other
mentions through coreference.
3.5 Example
We describe a run of our approach on an exam-
ple in Figure 1. Consider three mentions, each
156
?about navigation charts that he had 
ordered from a company based in the 
state of Washington. He assumed ?
?opened one of them to discover the 
absentee ballot of Steven H. Forrester 
of Bellevue, Wash?.
...were not meaningful because 
counting in Washington State has 
been completed...
(a) Example Excerpts with Mentions
Washington, DC
Washington State
...
Car Wash
The Wash
...
Washington State
Washington State
...
Washington
Wash
Washington 
State
(b) Initial Alignment (top-ranked in bold)
Washington State
Washington, DC
Car Wash
The Wash
...
Washington State
...
Washington
Wash
Washington 
State
(c) Merged and Reranked Alignment
Figure 1: Example of Dynamic Alignment
paired with a top-ranked KB candidate: ?Washing-
ton?, ?Wash?, and ?Washington State?. For the
first two mentions, clearly the top entity candidate
is incorrect; hence approaches that rely on a fixed
alignment will perform poorly. In particular, since
?Washington State? mention is not compatible with
the top-ranked entities of the first two mentions
(Washington, D.C. and Car Wash respectively), ap-
proaches that do not modify the ranking during
inference may not resolve them. However, the cor-
rect candidate Washington State does appear in the
candidate entities of the first two mentions, albeit
with a lower rank. In our approach, clustering
the first two mentions causes the shared candidate
Washington State to move to the top of the list. The
coreference system is now able to easily identify
that the ?Washington State? mention is compati-
ble with the Washington State entity formed by the
previous two mentions, providing evidence that the
final mention should be clustered with either of
them in subsequent comparisons.
4 Experiments
4.1 Setup
We evaluate our system on the ACE 2004 anno-
tated dataset (Doddington et al, 2004). Following
the setup in Bengston and Roth (2008), we split
the corpus into training, development, and test sets,
resulting in 268 documents in the train set, 107
documents in the test set, and 68 documents in the
development set. The data is processed using stan-
dard open source tools to segment the sentences
and tokenize the corpus, and using the OpenNLP2
tagger to obtain the POS tags. The hyperparame-
ters of our system, such as regularization, initial
number of candidates, and the number of compar-
2http://opennlp.apache.org/
isons during training (k in Section 2.3) are tuned
on the development data when trained on the train
set. The models we use to evaluate on the test data
set are trained on the training and development sets,
following the standard evaluation for coreference
first used by Culotta et al (2007).
To provide the initial ranked list of entity candi-
dates from Wikipedia, we query the KB Bridge sys-
tem (Dalton and Dietz, 2013) with the proper name
mentions. KB Bridge is an information-retrieval-
based entity linking system that connects the query
mentions to Wikipedia entities using a sequential
dependence model. This system has been shown to
match or outperform the top performing systems in
the 2012 TAC KBP entity linking task.
4.2 Methods
Our experiments investigate a number of baselines
that are similar or identical to existing approaches.
Wikipedia Linking: As a simple baseline, we
directly evaluate the quality of the alignment for
coreference by merging all pairs of proper noun
mentions that share at least one common candi-
date, as per KB bridge. Further, the non-pronoun
mentions are linked to these proper nouns if the
mention string matches any of the entity titles or
anchor texts.
Bengston and Roth (2008): A pairwise corefer-
ence model containing a rich set of features, as de-
scribed and evaluated in Bengston and Roth (2008).
Baseline: Our implementation of a pairwise
model that is similar to the approach in Bengston
and Roth (2008) with the differences described in
Section 2. This is our baseline system that performs
coreference without the use of external knowledge.
Incidentally, it outperforms Bengston and Roth
(2008).
Dynamic linking: This is our complete system as
157
described in Section 3, in which the list of candi-
dates associated with each mention is reranked and
modified during inference.
Static linking: Identical to dynamic linking ex-
cept that entity candidate lists are not merged dur-
ing inference (i.e., Algorithm 1 without line 17).
This approach is comparable to the fixed alignment
model, as in the approaches of Ponzetto and Strube
(2006) and Ratinov and Roth (2012).
4.3 Results
As in Bengston and Roth (2008), we evaluate our
system primarily using the B3 metric (Bagga and
Baldwin, 1998), but also include pairwise, MUC
and CEAF(m) metrics. The performance of our
systems on the test data set is shown in Table 2.
These results use true mentions provided in the
dataset, since, as suggested by Ng (2010), corefer-
ence resolvers that use different mention detectors
(extraction from parse tree, detector trained from
gold boundaries, etc.) should not be compared.
Our baseline system outperforms Bengston and
Roth (2008) by 0.32 B3 F1 points on this data set.
Incorporating Wikipedia and anchor text informa-
tion from the web with a fixed alignment (static
linking) further improves our performance by 0.54
B3 F1 points. Using dynamic linking, which im-
proves the alignment during inference, achieves
another 0.55 F1 point improvement, which is 1.09
F1 above our baseline, 1.41 F1 above the current
best pairwise classification system (corresponding
to an error reduction of 7.4%), and 0.4 F1 above the
current state-of-art on this dataset (Stoyanov and
Eisner, 2012). The improvement of the dynamic
linking approach over our baselines is consistent
across the various evaluation metrics.
5 Discussion
We also explore our system?s performance on sub-
sets of the ACE dataset, and on the OntoNotes
dataset.
5.1 Document Length
Coreference becomes more difficult as the number
of mentions is increased since the number of pair-
wise comparisons increases quadratically with the
number of mentions. We observe this phenomenon
in our dataset: the performance on the smallest
third of the documents (when sorted according to
number of mentions) is 8.5-10% higher than on the
largest third of the documents, as per the B3 metric.
55   10 15 20 25 30 35 40 45 50
   
   
0.6
0.8
1
1.2
1.4
1.6
1.8
Top X% of Docs by Number of Mentions
Im
pro
vem
en
t o
ver
 Ba
sel
ine
Dynamic Linking
Static Linking
Figure 2: Improvements on the top X% of docu-
ments ranked by the number of mentions.
Method Non-Transcripts Transcripts
Baseline 82.50 79.77
RR 2012 83.03 -
Static Linking 83.06 80.25
Dynamic Linking 83.32 81.13
Table 3: B3 F1 accuracy on transcripts and non-
transcripts from the ACE test data. RR 2012 only
evaluate on non-transcripts.
However, we expect dynamic linking of entities to
be more beneficial on these larger documents as
our system can use the information from a larger
number of mentions to improve the alignment dur-
ing inference. Static linking, on the other hand, is
unlikely to obtain higher improvements with the
larger number of mentions in the document as the
alignment is fixed.
We perform the following experiment to analyze
the performance with varying numbers of mentions.
We sort all the documents in the test set according
to their number of mentions, and evaluate on the top
X% of this list (where X is 10, 33, 40, 50). As the
results demonstrate in Figure 2, the improvement
of the static linking approach stays fairly even as
X is varied. Even though the experiments suggest
that the larger documents are tougher to corefer-
ence,3 dynamic linking provides higher improve-
ments when the documents contain a larger number
of mentions.
5.2 Performance on Transcripts
The quality of alignment and the coreference pre-
dictions for a document is influenced by the quality
of the mentions in the document. In particular,
3i.e., the absolute values are lower for these splits. The
baseline system obtains 83.08, 79.29, 79.64, and 79.77 respec-
tively for X = 10, 33, 40, 50.
158
Method Pairwise MUC CEAF B
3
P / R F1 P / R F1 P / R F1 P / R F1
Culotta et al (2007) - - - - - - 86.7 73.2 79.3
Raghunathan et al (2010) 71.6 46.2 56.1 80.4 71.8 75.8 - - 86.3 75.4 80.4
Stoyanov and Eisner (2012) - - - 80.1 - - - 81.8
Wiki-linking 64.15 14.99 24.30 74.41 28.39 41.10 58.54 58.4 58.47 92.89 57.21 70.81
Bengston and Roth (2008) - - 82.7 69.9 75.8 - - 88.3 74.5 80.8
Baseline 66.56 47.07 55.14 82.84 72.02 77.05 75.58 75.40 75.49 87.02 75.97 81.12
Static Linking 82.53 40.80 54.61 88.39 66.93 76.18 75.33 75.35 75.44 93.10 72.72 81.66
Dynamic Linking 72.20 47.40 57.23 85.07 72.02 78.01 76.55 76.37 76.46 89.37 76.12 82.21
Table 2: Evaluation on the ACE test data, with the system trained on the train and development sets.
   
   
0
0.2
0.4
0.6
0.8
1
1.2
1.4
Im
pro
vem
en
t o
ver
 Ba
sel
ine
non-trans
non-trans     
transcripts
Static Linking Dynamic Linking
trans
Figure 3: Comparison on the transcripts data.
ACE contains a large number of broadcast news
documents, many of which consist of transcribed
data containing noise in the form of incomplete
sentences and disfluencies. Since these transcripts
provide an additional challenge for alignment and
coreference, Ratinov and Roth (2012) only use the
set of non-transcripts for their evaluation.
Using dynamic linking and a large set of surface
string variations, our approach may be able to pro-
vide an improvement even on the transcripts. To
identify the transcripts in the test set, we use the
approximation from Ratinov and Roth (2012) that
considers a document to be non-transcribed if it
contains proper noun mentions and at least a third
of those start with a capital letter. The performance
is shown in Table 3, while the improvement over
our baseline is shown in Figure 3.
Our static linking matches the performance of
Ratinov and Roth (2012) on the non-transcripts.
Further, the improvement of static linking on the
transcripts over the baseline is lower than that on
the non-transcript data, suggesting that noisy men-
tions and text result in poor quality alignment. Dy-
namic linking, on the other hand, not only outper-
forms all other systems, but also shows a higher im-
provement over the baseline on the transcripts than
on non-transcripts. This indicates that dynamic
linking approach is robust to noise, and its wider
variety of surface strings and flexible alignments
are especially useful for transcripts.
5.3 OntoNotes
We also run our systems on the OntoNotes dataset,
which was used for evaluation in CoNLL 2011
Shared Task (Pradhan et al, 2011). The dataset
consists of 2083 documents from a much larger va-
riety of genres, such as conversations, magazines,
web text, etc. Further, the dataset alo consists of
mentions that refer to events, most of which do not
appear as Wikipedia pages. Since only the non-
singleton mentions are annotated in the training set,
we also include additional noun phrase mentions
during training. We obtain B3 F1 of 65.3, 67.6, and
67.7 for our baseline, static linking, and dynamic
linking respectively.4 When compared to the par-
ticipants of the closed task, the dynamic linking
system outperforms all but two on this metric, sug-
gesting that dynamic alignment is beneficial even
when the features have not been engineered for
events or for different genres.
6 Related Work
Within-document coreference has been well-
studied for a number of years. A variety of ap-
proaches incorporate linguistic knowledge as rules
iteratively applied to identify the chains, such
as Haghighi and Klein (2009), Raghunathan et
al. (2010), Stoyanov et al (2010). Alternatively
(and similar to our approach), others represent this
knowledge as features in a machine learning model.
Early applications of such models include Soon et
al. (2001), Ng and Cardie (2002) and (Bengston
and Roth, 2008). There are also a number of tech-
niques that represent entities explicitly (Culotta et
4with MUC 46.1, 49.9 & 50.1, and CEAF(m) 47.9, 49.6 &
49.8, respectively for baseline, static and dynamic linking.
159
al., 2007; Wick et al, 2009; Haghighi and Klein,
2010; Stoyanov and Eisner, 2012).
This work is an extension of recent approaches
that incorporate external knowledge sources to im-
prove within-document coreference. Ponzetto and
Strube (2006) identify Wikipedia candidates for
each mention as a preprocessing step, and incor-
porate them as features in a pairwise model. Our
method differs in that we draw such features from
entity candidates during inference, and also main-
tain and update a set of candidate entity links
instead of selecting only one. Rahman and Ng
(2011) introduce similar features from a more ex-
tensive set of knowledge sources (such as YAGO
and FrameNet) into a cluster-based model whose
features change as inference proceeds. However,
the features for each cluster come from a combina-
tion of all entities aligned to the cluster mentions.
We improve upon this approach by maintaining a
list of the candidate entities for each mention clus-
ter, modifying this list during the course of infer-
ence, and using features from only the top-ranked
candidate at any time. Further, they do not provide
a comparison on a standard dataset.
Ratinov and Roth (2012) extend the multi-sieve
coreference model (Raghunathan et al, 2010) by
identifying at most a single candidate for each men-
tion, and incorporating high-precision attributes
extracted from Wikipedia. The high-precision
mention-candidate pairings are precomputed and
fixed; additionally, the features for an entity are
based on the predictions of the previous sieves, thus
fixed while a sieve is applied. With these restric-
tions, they show improvements over the state-of-
the-art on a subset of ACE mentions that are more
easily aligned to Wikipedia, while our approach
demonstrates improvements on the complete set of
mentions including the tougher to link mentions
from the transcripts.
There are a number of approaches that provide
an alignment from mentions in a document to
Wikipedia. Wikifier (Ratinov et al, 2011) analyzes
the context around the mentions and the entities
jointly, and was used to align mentions for corefer-
ence in Ratinov and Roth (2012). Dalton and Dietz
(2013) introduce an approximation to the above ap-
proach, but incorporate retrieval-based supervised
reranking that provides multiple candidates and
scores; this approach performed competitively on
previous TAC-KBP entity linking benchmarks (Di-
etz and Dalton, 2012). Alignment to an external
knowledge-base has improved performance for a
number of NLP and information extraction tasks,
such as named-entity recognition (Cucerzan, 2007;
Han and Zhao, 2009), cross-document corefer-
ence (Finin et al, 2009; Singh et al, 2010), and
relation-extraction (Riedel et al, 2010; Hoffmann
et al, 2011).
7 Conclusions
In this paper, we incorporate external knowledge to
improve within-document coreference. Instead of
fixing the alignment a priori, our approach main-
tains a ranked list of candidate entities for each
mention, and merges and reranks the list during
inference. Further, we consider a large set of sur-
face string variations for each entity by using an-
chor texts from the web. These external sources
allow our system to achieve a new state-of-the-art
on the ACE data. We also demonstrate improve-
ments on documents that are difficult for alignment
and coreference, such as transcripts and documents
containing a large number of mentions.
A number of possible avenues for future study
are apparent. First, our alignment to a knowledge-
base can benefit from more document-aware link-
ing to entities, such as the Wikifier (Ratinov et al,
2011). Second, we would like to augment mention
features with additional information available from
the knowledge base, such as Wikipedia categoriza-
tion and gender attributes. We also want to investi-
gate a cluster ranking model, as used in (Rahman
and Ng, 2011; Stoyanov and Eisner, 2012), to ag-
gregate the features of all the coreferent mentions
as inference progresses.
Acknowledgments
This work was supported in part by the Center
for Intelligent Information Retrieval, in part by
DARPA under agreement number FA8750-13-2-
0020, in part by NSF medium IIS-0803847 and
in part by an award from Google. The U.S. Gov-
ernment is authorized to reproduce and distribute
reprint for Governmental purposes notwithstanding
any copyright annotation thereon. Any opinions,
findings and conclusions or recommendations ex-
pressed in this material are the authors? and neces-
sarily those of the sponsor.
160
References
Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In International
Conference on Language Resources and Evaluation
(LREC) Workshop on Linguistics Coreference, pages
563?566.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the 36th Annual Meeting of the Associa-
tion for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics -
Volume 1, ACL ?98, pages 86?90, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Eric Bengston and Dan Roth. 2008. Understanding
the value of features for coreference resolution. In
Empirical Methods in Natural Language Processing
(EMNLP).
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a collab-
oratively created graph database for structuring hu-
man knowledge. In Proceedings of the 2008 ACM
SIGMOD international conference on Management
of data, SIGMOD ?08, pages 1247?1250, New York,
NY, USA. ACM.
Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on Wikipedia data. In Empirical
Methods in Natural Language Processing (EMNLP),
pages 708?716.
Aron Culotta, Michael Wick, and Andrew McCallum.
2007. First-order probabilistic models for corefer-
ence resolution. In North American Chapter of the
Association for Computational Linguistics - Human
Language Technologies (NAACL HLT).
Jeffrey Dalton and Laura Dietz. 2013. A neighbor-
hood relevance model for entity linking. In Open
Research Areas in Information Retrieval (OAIR).
Laura Dietz and Jeffrey Dalton. 2012. Across-
document neighborhood expansion: UMass at TAC
KBP 2012 entity linking. In Text Analysis Confer-
ence (TAC).
G. Doddington, A. Mitchell, M. Przybocki,
L. Ramshaw, S. Strassel, and R. Weischedel.
2004. The Automatic Content Extraction (ACE)
program?tasks, data, and evaluation. In Pro-
ceedings of LREC, volume 4, pages 837?840.
Citeseer.
Tim Finin, Zareen Syed, James Mayfield, Paul Mc-
Namee, and Christine Piatko. 2009. Using Wiki-
tology for cross-document entity coreference resolu-
tion. In AAAI Spring Symposium on Learning by
Reading and Learning to Read.
Aria Haghighi and Dan Klein. 2009. Simple corefer-
ence resolution with rich syntactic and semantic fea-
tures. In Empirical Methods in Natural Language
Processing (EMNLP), pages 1152?1161.
Aria Haghighi and Dan Klein. 2010. Coreference reso-
lution in a modular, entity-centered model. In North
American Chapter of the Association for Computa-
tional Linguistics - Human Language Technologies
(NAACL HLT), pages 385?393.
Xianpei Han and Jun Zhao. 2009. Named entity disam-
biguation by leveraging Wikipedia semantic knowl-
edge. In Conference on Information and Knowledge
Management (CIKM), pages 215?224.
Raphael Hoffmann, Congle Zhang, Xiao Ling,
Luke Zettlemoyer, and Daniel S. Weld. 2011.
Knowledge-based weak supervision for information
extraction of overlapping relations. In Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL), pages 541?550, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Heeyoung Lee, Yves Peirsman, Angel Chang,
Nathanael Chambers, Mihai Surdeanu, and Dan
Jurafsky. 2011. Stanford?s multi-pass sieve
coreference resolution system at the CoNLL-2011
shared task. In Conference on Computational
Natural Language Learning (CoNLL), pages 28?34.
Association for Computational Linguistics.
Vincent Ng and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.
In Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 104?111.
Vincent Ng. 2010. Supervised noun phrase corefer-
ence research: the first fifteen years. In Proceedings
of the 48th Annual Meeting of the Association for
Computational Linguistics, ACL ?10, pages 1396?
1411, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In North
American Chapter of the Association for Computa-
tional Linguistics - Human Language Technologies
(NAACL HLT), pages 192?199.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen
Xue. 2011. CoNLL-2011 shared task: Modeling
unrestricted coreference in ontonotes. In Confer-
ence on Computational Natural Language Learning
(CoNLL), pages 1?27.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A multi-
pass sieve for coreference resolution. In Empirical
Methods in Natural Language Processing (EMNLP),
pages 492?501. Association for Computational Lin-
guistics.
Altaf Rahman and Vincent Ng. 2011. Coreference
resolution with world knowledge. In Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL), pages 814?824, Portland, Oregon, USA,
June.
161
L. Ratinov and D. Roth. 2012. Learning-based multi-
sieve co-reference resolution with knowledge. In
Empirical Methods in Natural Language Processing
(EMNLP).
L. Ratinov, D. Roth, D. Downey, and M. Anderson.
2011. Local and global algorithms for disambigua-
tion to wikipedia. In Annual Meeting of the Associa-
tion for Computational Linguistics (ACL).
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Collective cross-document relation extraction
without labelled data. In Empirical Methods in Nat-
ural Language Processing (EMNLP).
Sameer Singh, Michael L. Wick, and Andrew McCal-
lum. 2010. Distantly labeling data for large scale
cross-document coreference. Computing Research
Repository (CoRR), abs/1005.4298.
Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544, Dec.
Valentin I. Spitkovsky and Angel X. Chang. 2012. A
cross-lingual dictionary for english wikipedia con-
cepts. In International Conference on Language Re-
sources and Evaluation (LREC).
Veselin Stoyanov and Jason Eisner. 2012. Easy-first
coreference resolution. In Computational Linguis-
tics (COLING).
Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen
Riloff, David Buttler, and David Hysom. 2010.
Coreference resolution with reconcile. In Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 156?161, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
edge. In Proceedings of the 16th international con-
ference on World Wide Web, WWW ?07, pages 697?
706, New York, NY, USA. ACM.
Michael Wick, Aron Culotta, Khashayar Rohani-
manesh, and Andrew McCallum. 2009. An entity-
based model for coreference resolution. In SIAM In-
ternational Conference on Data Mining (SDM).
162
Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 146?182,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
Overview of the SPMRL 2013 Shared Task:
Cross-Framework Evaluation of Parsing Morphologically Rich Languages?
Djam? Seddaha, Reut Tsarfatyb, Sandra K?blerc,
Marie Canditod, Jinho D. Choie, Rich?rd Farkasf , Jennifer Fosterg, Iakes Goenagah,
Koldo Gojenolai, Yoav Goldbergj , Spence Greenk, Nizar Habashl, Marco Kuhlmannm,
Wolfgang Maiern, Joakim Nivreo, Adam Przepi?rkowskip, Ryan Rothq, Wolfgang Seekerr,
Yannick Versleys, Veronika Vinczet, Marcin Wolin?skiu,
Alina Wr?blewskav, Eric Villemonte de la Cl?rgeriew
aU. Paris-Sorbonne/INRIA, bWeizman Institute, cIndiana U., dU. Paris-Diderot/INRIA, eIPsoft Inc., f,tU. of Szeged,
gDublin City U., h,iU. of the Basque Country, jBar Ilan U., kStanford U., l,qColumbia U., m,oUppsala U., nD?sseldorf U.,
p,u,vPolish Academy of Sciences, rStuttgart U., sHeidelberg U., wINRIA
Abstract
This paper reports on the first shared task on
statistical parsing of morphologically rich lan-
guages (MRLs). The task features data sets
from nine languages, each available both in
constituency and dependency annotation. We
report on the preparation of the data sets, on
the proposed parsing scenarios, and on the eval-
uation metrics for parsing MRLs given dif-
ferent representation types. We present and
analyze parsing results obtained by the task
participants, and then provide an analysis and
comparison of the parsers across languages and
frameworks, reported for gold input as well as
more realistic parsing scenarios.
1 Introduction
Syntactic parsing consists of automatically assigning
to a natural language sentence a representation of
its grammatical structure. Data-driven approaches
to this problem, both for constituency-based and
dependency-based parsing, have seen a surge of inter-
est in the last two decades. These data-driven parsing
approaches obtain state-of-the-art results on the de
facto standard Wall Street Journal data set (Marcus et
al., 1993) of English (Charniak, 2000; Collins, 2003;
Charniak and Johnson, 2005; McDonald et al, 2005;
McClosky et al, 2006; Petrov et al, 2006; Nivre et
al., 2007b; Carreras et al, 2008; Finkel et al, 2008;
?Contact authors: djame.seddah@paris-sorbonne.fr,
reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu
Huang, 2008; Huang et al, 2010; Zhang and Nivre,
2011; Bohnet and Nivre, 2012; Shindo et al, 2012),
and provide a foundation on which many tasks oper-
ating on semantic structure (e.g., recognizing textual
entailments) or even discourse structure (coreference,
summarization) crucially depend.
While progress on parsing English ? the main
language of focus for the ACL community ? has in-
spired some advances on other languages, it has not,
by itself, yielded high-quality parsing for other lan-
guages and domains. This holds in particular for mor-
phologically rich languages (MRLs), where impor-
tant information concerning the predicate-argument
structure of sentences is expressed through word for-
mation, rather than constituent-order patterns as is the
case in English and other configurational languages.
MRLs express information concerning the grammati-
cal function of a word and its grammatical relation to
other words at the word level, via phenomena such
as inflectional affixes, pronominal clitics, and so on
(Tsarfaty et al, 2012c).
The non-rigid tree structures and morphological
ambiguity of input words contribute to the challenges
of parsing MRLs. In addition, insufficient language
resources were shown to also contribute to parsing
difficulty (Tsarfaty et al, 2010; Tsarfaty et al, 2012c,
and references therein). These challenges have ini-
tially been addressed by native-speaking experts us-
ing strong in-domain knowledge of the linguistic
phenomena and annotation idiosyncrasies to improve
the accuracy and efficiency of parsing models. More
146
recently, advances in PCFG-LA parsing (Petrov et al,
2006) and language-agnostic data-driven dependency
parsing (McDonald et al, 2005; Nivre et al, 2007b)
have made it possible to reach high accuracy with
classical feature engineering techniques in addition
to, or instead of, language-specific knowledge. With
these recent advances, the time has come for estab-
lishing the state of the art, and assessing strengths
and weaknesses of parsers across different MRLs.
This paper reports on the first shared task on sta-
tistical parsing of morphologically rich languages
(the SPMRL Shared Task), organized in collabora-
tion with the 4th SPMRL meeting and co-located
with the conference on Empirical Methods in Natural
Language Processing (EMNLP). In defining and exe-
cuting this shared task, we pursue several goals. First,
we wish to provide standard training and test sets for
MRLs in different representation types and parsing
scenarios, so that researchers can exploit them for
testing existing parsers across different MRLs. Sec-
ond, we wish to standardize the evaluation protocol
and metrics on morphologically ambiguous input,
an under-studied challenge, which is also present in
English when parsing speech data or web-based non-
standard texts. Finally, we aim to raise the awareness
of the community to the challenges of parsing MRLs
and to provide a set of strong baseline results for
further improvement.
The task features data from nine, typologically di-
verse, languages. Unlike previous shared tasks on
parsing, we include data in both dependency-based
and constituency-based formats, and in addition to
the full data setup (complete training data), we pro-
vide a small setup (a training subset of 5,000 sen-
tences). We provide three parsing scenarios: one in
which gold segmentation, POS tags, and morphologi-
cal features are provided, one in which segmentation,
POS tags, and features are automatically predicted
by an external resource, and one in which we provide
a lattice of multiple possible morphological analyses
and allow for joint disambiguation of the morpholog-
ical analysis and syntactic structure. These scenarios
allow us to obtain the performance upper bound of
the systems in lab settings using gold input, as well
as the expected level of performance in realistic pars-
ing scenarios ? where the parser follows a morpho-
logical analyzer and is a part of a full-fledged NLP
pipeline.
The remainder of this paper is organized as follows.
We first survey previous work on parsing MRLs (?2)
and provide a detailed description of the present task,
parsing scenarios, and evaluation metrics (?3). We
then describe the data sets for the nine languages
(?4), present the different systems (?5), and empiri-
cal results (?6). Then, we compare the systems along
different axes (?7) in order to analyze their strengths
and weaknesses. Finally, we summarize and con-
clude with challenges to address in future shared
tasks (?8).
2 Background
2.1 A Brief History of the SPMRL Field
Statistical parsing saw initial success upon the avail-
ability of the Penn Treebank (PTB, Marcus et al,
1994). With that large set of syntactically annotated
sentences at their disposal, researchers could apply
advanced statistical modeling and machine learning
techniques in order to obtain high quality structure
prediction. The first statistical parsing models were
generative and based on treebank grammars (Char-
niak, 1997; Johnson, 1998; Klein and Manning, 2003;
Collins, 2003; Petrov et al, 2006; McClosky et al,
2006), leading to high phrase-structure accuracy.
Encouraged by the success of phrase-structure
parsers for English, treebank grammars for additional
languages have been developed, starting with Czech
(Hajic? et al, 2000) then with treebanks of Chinese
(Levy and Manning, 2003), Arabic (Maamouri et
al., 2004b), German (K?bler et al, 2006), French
(Abeill? et al, 2003), Hebrew (Sima?an et al, 2001),
Italian (Corazza et al, 2004), Spanish (Moreno et al,
2000), and more. It quickly became apparent that
applying the phrase-based treebank grammar tech-
niques is sensitive to language and annotation prop-
erties, and that these models are not easily portable
across languages and schemes. An exception to that
is the approach by Petrov (2009), who trained latent-
annotation treebank grammars and reported good
accuracy on a range of languages.
The CoNLL shared tasks on dependency parsing
(Buchholz and Marsi, 2006; Nivre et al, 2007a) high-
lighted the usefulness of an alternative linguistic for-
malism for the development of competitive parsing
models. Dependency relations are marked between
input tokens directly, and allow the annotation of
147
non-projective dependencies that are parseable effi-
ciently. Dependency syntax was applied to the de-
scription of different types of languages (Tesni?re,
1959; Mel?c?uk, 2001), which raised the hope that in
these settings, parsing MRLs will further improve.
However, the 2007 shared task organizers (Nivre
et al, 2007a) concluded that: "[Performance] classes
are more easily definable via language characteris-
tics than via characteristics of the data sets. The
split goes across training set size, original data for-
mat [...], sentence length, percentage of unknown
words, number of dependency labels, and ratio of
(C)POSTAGS and dependency labels. The class
with the highest top scores contains languages with
a rather impoverished morphology." The problems
with parsing MRLs have thus not been solved by de-
pendency parsing, but rather, the challenge has been
magnified.
The first event to focus on the particular challenges
of parsing MRLs was a dedicated panel discussion
co-located with IWPT 2009.1 Work presented on
Hebrew, Arabic, French, and German made it clear
that researchers working on non-English parsing face
the same overarching challenges: poor lexical cover-
age (due to high level of inflection), poor syntactic
coverage (due to more flexible word ordering), and,
more generally, issues of data sparseness (due to
the lack of large-scale resources). Additionally, new
questions emerged as to the evaluation of parsers in
such languages ? are the word-based metrics used
for English well-equipped to capture performance
across frameworks, or performance in the face of
morphological complexity? This event provoked ac-
tive discussions and led to the establishment of a
series of SPMRL events for the discussion of shared
challenges and cross-fertilization among researchers
working on parsing MRLs.
The body of work on MRLs that was accumulated
through the SPMRL workshops2 and hosting ACL
venues contains new results for Arabic (Attia et al,
2010; Marton et al, 2013a), Basque (Bengoetxea
and Gojenola, 2010), Croatian (Agic et al, 2013),
French (Seddah et al, 2010; Candito and Seddah,
2010; Sigogne et al, 2011), German (Rehbein, 2011),
Hebrew (Tsarfaty and Sima?an, 2010; Goldberg and
1http://alpage.inria.fr/iwpt09/panel.en.
html
2See http://www.spmrl.org/ and related workshops.
Elhadad, 2010a), Hindi (Ambati et al, 2010), Ko-
rean (Chung et al, 2010; Choi and Palmer, 2011) and
Spanish (Le Roux et al, 2012), Tamil (Green et al,
2012), amongst others. The awareness of the model-
ing challenges gave rise to new lines of work on top-
ics such as joint morpho-syntactic processing (Gold-
berg and Tsarfaty, 2008), Relational-Realizational
Parsing (Tsarfaty, 2010), EasyFirst Parsing (Gold-
berg, 2011), PLCFRS parsing (Kallmeyer and Maier,
2013), the use of factored lexica (Green et al, 2013),
the use of bilingual data (Fraser et al, 2013), and
more developments that are currently under way.
With new models and data, and with lingering in-
terest in parsing non-standard English data, questions
begin to emerge, such as: What is the realistic per-
formance of parsing MRLs using today?s methods?
How do the different models compare with one an-
other? How do different representation types deal
with parsing one particular language? Does the suc-
cess of a parsing model on a language correlate with
its representation type and learning method? How to
parse effectively in the face of resource scarcity? The
first step to answering all of these questions is pro-
viding standard sets of comparable size, streamlined
parsing scenarios, and evaluation metrics, which are
our main goals in this SPMRL shared task.
2.2 Where We Are At: The Need for
Cross-Framework, Realistic, Evaluation
Procedures
The present task serves as the first attempt to stan-
dardize the data sets, parsing scenarios, and evalu-
ation metrics for MRL parsing, for the purpose of
gaining insights into parsers? performance across lan-
guages. Ours is not the first cross-linguistic task on
statistical parsing. As mentioned earlier, two previ-
ous CoNLL shared tasks focused on cross-linguistic
dependency parsing and covered thirteen different
languages (Buchholz and Marsi, 2006; Nivre et al,
2007a). However, the settings of these tasks, e.g.,
in terms of data set sizes or parsing scenarios, made
it difficult to draw conclusions about strengths and
weaknesses of different systems on parsing MRLs.
A key aspect to consider is the relation between
input tokens and tree terminals. In the standard sta-
tistical parsing setup, every input token is assumed
to be a terminal node in the syntactic parse tree (after
deterministic tokenization of punctuation). In MRLs,
148
morphological processes may have conjoined several
words into a single token. Such tokens need to be seg-
mented and their analyses need to be disambiguated
in order to identify the nodes in the parse tree. In
previous shared tasks on statistical parsing, morpho-
logical information was assumed to be known in ad-
vance in order to make the setup comparable to that
of parsing English. In realistic scenarios, however,
morphological analyses are initially unknown and are
potentially highly ambiguous, so external resources
are used to predict them. Incorrect morphological
disambiguation sets a strict ceiling on the expected
performance of parsers in real-world scenarios. Re-
sults reported for MRLs using gold morphological
information are then, at best, optimistic.
One reason for adopting this less-than-realistic
evaluation scenario in previous tasks has been the
lack of sound metrics for the more realistic scenario.
Standard evaluation metrics assume that the number
of terminals in the parse hypothesis equals the num-
ber of terminals in the gold tree. When the predicted
morphological segmentation leads to a different num-
ber of terminals in the gold and parse trees, standard
metrics such as ParsEval (Black et al, 1991) or At-
tachment Scores (Buchholz and Marsi, 2006) fail
to produce a score. In this task, we use TedEval
(Tsarfaty et al, 2012b), a metric recently suggested
for joint morpho-syntactic evaluation, in which nor-
malized tree-edit distance (Bille, 2005) on morpho-
syntactic trees allows us to quantify the success on
the joint task in realistic parsing scenarios.
Finally, the previous tasks focused on dependency
parsing. When providing both constituency-based
and dependency-based tracks, it is interesting to com-
pare results across these frameworks so as to better
understand the differences in performance between
parsers of different types. We are now faced with
an additional question: how can we compare pars-
ing results across different frameworks? Adopting
standard metrics will not suffice as we would be com-
paring apples and oranges. In contrast, TedEval is
defined for both phrase structures and dependency
structures through the use of an intermediate repre-
sentation called function trees (Tsarfaty et al, 2011;
Tsarfaty et al, 2012a). Using TedEval thus allows us
to explore both dependency and constituency parsing
frameworks and meaningfully compare the perfor-
mance of parsers of different types.
3 Defining the Shared-Task
3.1 Input and Output
We define a parser as a structure prediction function
that maps sequences of space-delimited input tokens
(henceforth, tokens) in a language to a set of parse
trees that capture valid morpho-syntactic structures
in that language. In the case of constituency parsing,
the output structures are phrase-structure trees. In de-
pendency parsing, the output consists of dependency
trees. We use the term tree terminals to refer to the
leaves of a phrase-structure tree in the former case
and to the nodes of a dependency tree in the latter.
We assume that input sentences are represented
as sequences of tokens. In general, there may be a
many-to-many relation between input tokens and tree
terminals. Tokens may be identical to the terminals,
as is often the case in English. A token may be
mapped to multiple terminals assigned their own POS
tags (consider, e.g., the token ?isn?t?), as is the case
in some MRLs. Several tokens may be grouped into
a single (virtual) node, as is the case with multiword
expressions (MWEs) (consider ?pomme de terre? for
?potatoe?). This task covers all these cases.
In the standard setup, all tokens are tree terminals.
Here, the task of a parser is to predict a syntactic
analysis in which the tree terminals coincide with the
tokens. Disambiguating the morphological analyses
that are required for parsing corresponds to selecting
the correct POS tag and possibly a set of morpho-
logical features for each terminal. For the languages
Basque, French, German, Hungarian, Korean, Polish,
and Swedish, we assume this standard setup.
In the morphologically complex setup, every token
may be composed of multiple terminals. In this case,
the task of the parser is to predict the sequence of tree
terminals, their POS tags, and a correct tree associ-
ated with this sequence of terminals. Disambiguating
the morphological analysis therefore requires split-
ting the tokens into segments that define the terminals.
For the Semitic languages Arabic and Hebrew, we
assume this morphologically complex setup.
In the multiword expression (MWEs) setup, pro-
vided here for French only, groupings of terminals
are identified as MWEs (non-terminal nodes in con-
stituency trees, marked heads in dependency trees).
Here, the parser is required to predict how terminals
are grouped into MWEs on top of predicting the tree.
149
3.2 Data Sets
The task features nine languages from six language
families, from Germanic languages (Swedish and
German) and Romance (French) to Slavic (Polish),
Koreanic (Korean), Semitic (Arabic, Hebrew), Uralic
(Hungarian), and the language isolate Basque.
These languages cover a wide range of morpho-
logical richness, with Arabic, Basque, and Hebrew
exhibiting a high degree of inflectional and deriva-
tional morphology. The Germanic languages, Ger-
man and Swedish, have greater degrees of phrasal
ordering freedom than English. While French is not
standardly classified as an MRL, it shares MRLs char-
acteristics which pose challenges for parsing, such as
a richer inflectional system than English.
For each contributing language, we provide two
sets of annotated sentences: one annotated with la-
beled phrase-structure trees, and one annotated with
labeled dependency trees. The sentences in the two
representations are aligned at token and POS levels.
Both representations reflect the predicate-argument
structure of the same sentence, but this information
is expressed using different formal terms and thus
results in different tree structures.
Since some of our native data sets are larger than
others, we provide the training set in two sizes: Full
containing all sentences in the standard training set
of the language, and 5k containing the number of
sentences that is equivalent in size to our smallest
training set (5k sentences). For all languages, the data
has been split into sentences, and the sentences are
parsed and evaluated independently of one another.
3.3 Parsing Scenarios
In the shared task, we consider three parsing scenar-
ios, depending on how much of the morphological
information is provided. The scenarios are listed
below, in increasing order of difficulty.
? Gold: In this scenario, the parser is provided
with unambiguous gold morphological segmen-
tation, POS tags, and morphological features for
each input token.
? Predicted: In this scenario, the parser is pro-
vided with disambiguated morphological seg-
mentation. However, the POS tags and mor-
phological features for each input segment are
unknown.
Scenario Segmentation PoS+Feat. Tree
Gold X X ?
Predicted X 1-best ?
Raw (1-best) 1-best 1-best ?
Raw (all) ? ? ?
Table 1: A summary of the parsing and evaluation sce-
narios. X depicts gold information, ? depicts unknown
information, to be predicted by the system.
? Raw: In this scenario, the parser is provided
with morphologically ambiguous input. The
morphological segmentation, POS tags, and
morphological features for each input token are
unknown.
The Predicted and Raw scenarios require predict-
ing morphological analyses. This may be done using
a language-specific morphological analyzer, or it may
be done jointly with parsing. We provide inputs that
support these different scenarios:
? Predicted: Gold treebank segmentation is given
to the parser. The POS tags assignment and mor-
phological features are automatically predicted
by the parser or by an external resource.
? Raw (1-best): The 1st-best segmentation and
POS tags assignment is predicted by an external
resource and given to the parser.
? Raw (all): All possible segmentations and POS
tags are specified by an external resource. The
parser selects jointly a segmentation and a tree.
An overview of all shown in table 1. For languages
in which terminals equal tokens, only Gold and Pre-
dicted scenarios are considered. For Semitic lan-
guages we further provide input for both Raw (1-
best) and Raw (all) scenarios. 3
3.4 Evaluation Metrics
This task features nine languages, two different repre-
sentation types and three different evaluation scenar-
ios. In order to evaluate the quality of the predicted
structures in the different tracks, we use a combina-
tion of evaluation metrics that allow us to compare
the systems along different axes.
3The raw Arabic lattices were made available later than the
other data. They are now included in the shared task release.
150
In this section, we formally define the different
evaluation metrics and discuss how they support sys-
tem comparison. Throughout this paper, we will be
referring to different evaluation dimensions:
? Cross-Parser Evaluation in Gold/Predicted
Scenarios. Here, we evaluate the results of dif-
ferent parsers on a single data set in the Gold
or Predicted setting. We use standard evalu-
ation metrics for the different types of anal-
yses, that is, ParsEval (Black et al, 1991)
on phrase-structure trees, and Labeled At-
tachment Scores (LAS) (Buchholz and Marsi,
2006) for dependency trees. Since ParsEval is
known to be sensitive to the size and depth of
trees (Rehbein and van Genabith, 2007b), we
also provide the Leaf-Ancestor metric (Samp-
son and Babarczy, 2003), which is less sensitive
to the depth of the phrase-structure hierarchy. In
both scenarios we also provide metrics to evalu-
ate the prediction of MultiWord Expressions.
? Cross-Parser Evaluation in Raw Scenarios.
Here, we evaluate the results of different parsers
on a single data set in scenarios where morpho-
logical segmentation is not known in advance.
When a hypothesized segmentation is not iden-
tical to the gold segmentation, standard evalua-
tion metrics such as ParsEval and Attachment
Scores break down. Therefore, we use TedEval
(Tsarfaty et al, 2012b), which jointly assesses
the quality of the morphological and syntactic
analysis in morphologically-complex scenarios.
? Cross-Framework Evaluation. Here, we com-
pare the results obtained by a dependency parser
and a constituency parser on the same set of sen-
tences. In order to avoid comparing apples and
oranges, we use the unlabeled TedEval metric,
which converts all representation types inter-
nally into the same kind of structures, called
function trees. Here we use TedEval?s cross-
framework protocol (Tsarfaty et al, 2012a),
which accomodates annotation idiosyncrasies.
? Cross-Language Evaluation. Here, we com-
pare parsers for the same representation type
across different languages. Conducting a com-
plete and faithful evaluation across languages
would require a harmonized universal annota-
tion scheme (possibly along the lines of (de
Marneffe and Manning, 2008; McDonald et al,
2013; Tsarfaty, 2013)) or task based evaluation.
As an approximation we use unlabeled TedEval.
Since it is unlabeled, it is not sensitive to label
set size. Since it internally uses function-trees,
it is less sensitive to annotation idiosyncrasies
(e.g., head choice) (Tsarfaty et al, 2011).
The former two dimensions are evaluated on the full
sets. The latter two are evaluated on smaller, compa-
rable, test sets. For completeness, we provide below
the formal definitions and essential modifications of
the evaluation software that we used.
3.4.1 Evaluation Metrics for Phrase Structures
ParsEval The ParsEval metrics (Black et al, 1991)
are evaluation metrics for phrase-structure trees. De-
spite various shortcomings, they are the de-facto stan-
dard for system comparison on phrase-structure pars-
ing, used in many campaigns and shared tasks (e.g.,
(K?bler, 2008; Petrov and McDonald, 2012)). As-
sume that G and H are phrase-structure gold and
hypothesized trees respectively, each of which is rep-
resented by a set of tuples (i, A, j) where A is a
labeled constituent spanning from i to j. Assume
that g is the same as G except that it discards the
root, preterminal, and terminal nodes, likewise for h
and H . The ParsEval scores define the accuracy of
the hypothesis in terms of the normalized size of the
intersection of the constituent sets.
Precision(g, h) = |g?h||h|
Recall(g, h) = |g?h||g|
F1(g, h) = 2?P?RP+R
We evaluate accuracy on phrase-labels ignoring any
further decoration, as it is in standard practices.
Evalb, the standard software that implements Par-
sEval,4 takes a parameter file and ignores the labels
specified therein. As usual, we ignore root and POS
labels. Contrary to the standard practice, we do take
punctuation into account. Note that, as opposed to the
official version, we used the SANCL?2012 version5
modified to actually penalize non-parsed trees.
4http://www.spmrl.org/
spmrl2013-sharedtask-metrics.html/#Evalb
5Modified by Petrov and McDonald (2012) to be less sensi-
tive to punctuation errors.
151
Leaf-Ancestor The Leaf-Ancestor metric (Samp-
son and Babarczy, 2003) measures the similarity be-
tween the path from each terminal node to the root
node in the output tree and the corresponding path
in the gold tree. The path consists of a sequence of
node labels between the terminal node and the root
node, and the similarity of two paths is calculated
by using the Levenshtein distance. This distance is
normalized by path length, and the score of the tree
is an aggregated score of the values for all terminals
in the tree (xt is the leaf-ancestor path of t in tree x).
LA(h, g) =
?
t?yield(g) Lv(ht,gt)/(len(ht)+len(gt))
|yield(g)|
This metric was shown to be less sensitive to dif-
ferences between annotation schemes in (K?bler et
al., 2008), and was shown by Rehbein and van Gen-
abith (2007a) to evaluate trees more faithfully than
ParsEval in the face of certain annotation decisions.
We used the implementation of Wagner (2012).6
3.4.2 Evaluation Metrics for Dependency
Structures
Attachment Scores Labeled and Unlabeled At-
tachment scores have been proposed as evaluation
metrics for dependency parsing in the CoNLL shared
tasks (Buchholz and Marsi, 2006; Nivre et al, 2007a)
and have since assumed the role of standard metrics
in multiple shared tasks and independent studies. As-
sume that g, h are gold and hypothesized dependency
trees respectively, each of which is represented by
a set of arcs (i, A, j) where A is a labeled arc from
terminal i to terminal j. Recall that in the gold and
predicted settings, |g| = |h| (because the number of
terminals determines the number of arcs and hence it
is fixed). So Labeled Attachment Score equals preci-
sion and recall, and it is calculated as a normalized
size of the intersection between the sets of gold and
parsed arcs.7
Precision(g, h) = |g?h||g|
Recall(g, h) = |g?h||h|
LAS(g, h) = |g?h||g| =
|g?h|
|h|
6The original version is available at
http://www.grsampson.net/Resources.
html, ours at http://www.spmrl.org/
spmrl2013-sharedtask-metrics.html/#Leaf.
7http://ilk.uvt.nl/conll/software.html.
3.4.3 Evaluation Metrics for Morpho-Syntactic
Structures
TedEval The TedEval metrics and protocols have
been developed by Tsarfaty et al (2011), Tsarfaty
et al (2012a) and Tsarfaty et al (2012b) for coping
with non-trivial evaluation scenarios, e.g., comparing
parsing results across different frameworks, across
representation theories, and across different morpho-
logical segmentation hypotheses.8 Contrary to the
previous metrics, which view accuracy as a normal-
ized intersection over sets, TedEval computes the ac-
curacy of a parse tree based on the tree-edit distance
between complete trees. Assume a finite set of (pos-
sibly parameterized) edit operations A = {a1....an},
and a cost function c : A ? 1. An edit script is the
cost of a sequence of edit operations, and the edit dis-
tance of g, h is the minimal cost edit script that turns
g into h (and vice versa). The normalized distance
subtracted from 1 provides the level of accuracy on
the task. Formally, the TedEval score on g, h is de-
fined as follows, where ted is the tree-edit distance,
and the |x| (size in nodes) discards terminals and root
nodes.
TedEval(g, h) = 1?
ted(g, h)
|g|+ |h|
In the gold scenario, we are not allowed to manipu-
late terminal nodes, only non-terminals. In the raw
scenarios, we can add and delete both terminals and
non-terminals so as to match both the morphological
and syntactic hypotheses.
3.4.4 Evaluation Metrics for
Multiword-Expression Identification
As pointed out in section 3.1, the French data set is
provided with tree structures encoding both syntactic
information and groupings of terminals into MWEs.
A given MWE is defined as a continuous sequence of
terminals, plus a POS tag. In the constituency trees,
the POS tag of the MWE is an internal node of the
tree, dominating the sequence of pre-terminals, each
dominating a terminal. In the dependency trees, there
is no specific node for the MWE as such (the nodes
are the terminals). So, the first token of a MWE is
taken as the head of the other tokens of the same
MWE, with the same label (see section 4.4).
8http://www.tsarfaty.com/unipar/
download.html.
152
To evaluate performance on MWEs, we use the
following metrics.
? R_MWE, P_MWE, and F_MWE are recall, pre-
cision, and F-score over full MWEs, in which
a predicted MWE counts as correct if it has the
correct span (same group as in the gold data).
? R_MWE +POS, R_MWE +POS, and F_MWE
+POS are defined in the same fashion, except
that a predicted MWE counts as correct if it has
both correct span and correct POS tag.
? R_COMP, R_COMP, and F_COMP are recall,
precision and F-score over non-head compo-
nents of MWEs: a non-head component of MWE
counts as correct if it is attached to the head of
the MWE, with the specific label that indicates
that it is part of an MWE.
4 The SPMRL 2013 Data Sets
4.1 The Treebanks
We provide data from nine different languages anno-
tated with two representation types: phrase-structure
trees and dependency trees.9 Statistics about size,
average length, label set size, and other character-
istics of the treebanks and schemes are provided in
Table 2. Phrase structures are provided in an ex-
tended bracketed style, that is, Penn Treebank brack-
eted style where every labeled node may be extended
with morphological features expressed. Dependency
structures are provided in the CoNLL-X format.10
For any given language, the dependency and con-
stituency treebanks are aligned at the token and ter-
minal levels and share the same POS tagset and mor-
phological features. That is, any form in the CoNLL
format is a terminal of the respective bracketed tree.
Any CPOS label in the CoNLL format is the pre-
terminal dominating the terminal in the bracketed
tree. The FEATS in the CoNLL format are repre-
sented as dash-features decorated on the respective
pre-terminal node in the bracketed tree. See Fig-
ure 1(a)?1(b) for an illustration of this alignment.
9Additionally, we provided the data in TigerXML format
(Brants et al, 2002) for phrase structure trees containing cross-
ing branches. This allows the use of more powerful parsing
formalisms. Unfortunately, we received no submissions for this
data, hence we discard them in the rest of this overview.
10See http://ilk.uvt.nl/conll/.
For ambiguous morphological analyses, we pro-
vide the mapping of tokens to different segmentation
possibilities through lattice files. See Figure 1(c) for
an illustration, where lattice indices mark the start
and end positions of terminals.
For each of the treebanks, we provide a three-way
dev/train/set split and another train set containing the
first 5k sentences of train (5k). This section provides
the details of the original treebanks and their anno-
tations, our data-set preparation, including prepro-
cessing and data splits, cross-framework alignment,
and the prediction of morphological information in
non-gold scenarios.
4.2 The Arabic Treebanks
Arabic is a morphologically complex language which
has rich inflectional and derivational morphology. It
exhibits a high degree of morphological ambiguity
due to the absence of the diacritics and inconsistent
spelling of letters, such as Alif and Ya. As a conse-
quence, the Buckwalter Standard Arabic Morpholog-
ical Analyzer (Buckwalter, 2004; Graff et al, 2009)
produces an average of 12 analyses per word.
Data Sets The Arabic data set contains two tree-
banks derived from the LDC Penn Arabic Treebanks
(PATB) (Maamouri et al, 2004b):11 the Columbia
Arabic Treebank (CATiB) (Habash and Roth, 2009),
a dependency treebank, and the Stanford version
of the PATB (Green and Manning, 2010), a phrase-
structure treebank. We preprocessed the treebanks
to obtain strict token matching between the treebanks
and the morphological analyses. This required non-
trivial synchronization at the tree token level between
the PATB treebank, the CATiB treebank and the mor-
phologically predicted data, using the PATB source
tokens and CATiB feature word form as a dual syn-
chronized pivot.
The Columbia Arabic Treebank The Columbia
Arabic Treebank (CATiB) uses a dependency repre-
sentation that is based on traditional Arabic grammar
and that emphasizes syntactic case relations (Habash
and Roth, 2009; Habash et al, 2007). The CATiB
treebank uses the word tokenization of the PATB
11The LDC kindly provided their latest version of the Arabic
Treebanks. In particular, we used PATB 1 v4.1 (Maamouri et al,
2005), PATB 2 v3.1 (Maamouri et al, 2004a) and PATB 3 v3.3.
(Maamouri et al, 2009)
153
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
train:
#Sents 15,762 7,577 14,759 40,472 8,146 23,010 6,578
#Tokens 589,220 96,368 443,113 719,532 170,141 351,184 68,424
Lex. Size 36,906 25,136 27,470 77,222 40,782 11,1540 22,911
Avg. Length 37.38 12.71 30.02 17.77 20.88 15.26 10.40
Ratio #NT/#Tokens 0.19 0.82 0.34 0.60 0.59 0.60 0.94
Ratio #NT/#Sents 7.40 10.50 10.33 10.70 12.38 9.27 9.84
#Non Terminals 22 12 32 25 16 8 34
#POS tags 35 25 29 54 16 1,975 29
#total NTs 116,769 79,588 152,463 433,215 100,885 213,370 64,792
Dep. Label Set Size 9 31 25 43 417 22 27
train5k:
#Sents 5,000 5,000 5,000 5,000 5,000 5,000 5,000 5,000 5,000
#Tokens 224,907 61,905 150,984 87,841 128,046 109,987 68,336 52,123 76,357
Lex. Size 19,433 18,405 15,480 17,421 15,975 29,009 29,715 18,632 14,110
Avg. Length 44.98 12.38 30.19 17.56 25.60 21.99 13.66 10.42 15.27
Ratio #NT/#Tokens 0.15 0.83 0.34 0.60 0.42 0.57 0.68 0.94 0.58
Ratio #NT/#Sents 7.18 10.33 10.32 10.58 10.97 12.57 9.29 9.87 8.96
#Non Terminals 22 12 29 23 60 16 8 34 8
#POS Tags 35 25 29 51 50 16 972 29 25
#total NTs 35,909 5,1691 51,627 52,945 54,856 62,889 46,484 49,381 44,845
Dep. Label Set Size 9 31 25 42 43 349 20 27 61
dev:
#Sents 1,985 948 1,235 5,000 500 1,051 2,066 821 494
#Tokens 73,932 13,851 38,820 76,704 11,301 29,989 30,480 8,600 9,341
Lex. Size 12,342 5,551 6,695 15,852 3,175 10,673 15,826 4,467 2,690
Avg. Length 37.24 14.61 31.43 15.34 22.60 28.53 14.75 10.47 18.90
Ratio #NT/#Tokens 0.19 0.74 0.33 0.63 0.47 047 0.63 0.94 0.48
Ratio #NT/#Sents 7.28 10.92 10.48 9.71 10.67 13.66 9.33 9.90 9.10
#Non Terminals 21 11 27 24 55 16 8 31 8
#POS Tags 32 23 29 50 47 16 760 29 24
#total NTs 14,452 10,356 12,951 48,560 5,338 14,366 19,283 8,132 4,496
Dep. Label Set Size 9 31 25 41 42 210 22 26 59
test:
#Sents 1959 946 2541 5000 716 1009 2287 822 666
#Tokens 73878 11457 75216 92004 16998 19908 33766 8545 10690
Lex. Size 12254 4685 10048 20149 4305 7856 16475 4336 3112
Avg. Length 37.71 12.11 29.60 18.40 23.74 19.73 14.76 10.39 16.05
Ratio #NT/#Tokens 0.19 0.83 0.34 0.60 0.47 0.62 0.61 0.95 0.57
Ratio #NT/#Sents 7.45 10.08 10.09 11.07 11.17 12.26 9.02 9.94 9.18
#Non Terminals 22 12 30 23 54 15 8 31 8
#POS Tags 33 22 30 52 46 16 809 27 25
#total NTs 14,610 9,537 25,657 55,398 8,001 12,377 20,640 8,175 6,118
Dep. Label Set Size 9 31 26 42 41 183 22 27 56
Table 2: Overview of participating languages and treebank properties. ?Sents? = number of sentences, ?Tokens? =
number of raw surface forms. ?Lex. size? and ?Avg. Length? are computed in terms of tagged terminals. ?NT? = non-
terminals in constituency treebanks, ?Dep Labels? = dependency labels on the arcs of dependency treebanks. ? A more
comprehensive table is available at http://www.spmrl.org/spmrl2013-sharedtask.html/#Prop.
154
(a) Constituency Tree
% % every line is a single tree in a bracketed Penn Treebank format
(ROOT (S (NP ( NNP-#pers=3|num=sing# John))(VP ( VB-#pers=3|num=sing# likes)(NP ( NNP-#pers=3|num=sing# Mary)))))
(b) Dependency Tree
%% every line describes a terminal: terminal-id form lemma CPOS FPOS FEATS Head Rel PHead PRel
1 John John NNP NNP pers=3|num=sing 2 sbj _ _
2 likes like VB VB pers=3|num=sing 0 root _ _
3 Mary Mary NNP NNP pers=3|num=sing 2 obj _ _
Input Lattice
0 1 2 3 4 5 6
1:AIF/NN
1:AIF/VB
1:AIF/NNT
2:LA/RB
3:NISH/VB
3:NISH/NN
4:L/PREP
4:LHSTIR/VB
4:HSTIR/VB
5:ZAT/PRP
%% every line describes a terminal: start-id end-id form lemma CPOS FPOS FEATS token-id
0 1 AIF AIF NN NN _ 1
0 1 AIF AIF NNT NNT _ 1
0 1 AIF AIF VB VB _ 1
1 2 LA LA RB RB _ 2
2 3 NISH NISH VB VB _ 3
2 3 NISH NISH NN NN _ 3
3 5 LHSTIR HSTIR VB VB _ 4
3 4 L L PREP PREP _ 4
4 5 HSTIR HSTIR VB VB _ 4
5 6 ZAT ZAT PRP PRP _ 5
Figure 1: File formats. Trees (a) and (b) are aligned constituency and dependency trees for a mockup English example.
Boxed labels are shared across the treebanks. Figure (c) shows an ambiguous lattice. The red part represents the yield
of the gold tree. For brevity, we use empty feature columns, but of course lattice arcs may carry any morphological
features, in the FEATS CoNLL format.
and employs a reduced POS tagset consisting of six
tags only: NOM (non-proper nominals including
nouns, pronouns, adjectives and adverbs), PROP
(proper nouns), VRB (active-voice verbs), VRB-
PASS (passive-voice verbs), PRT (particles such as
prepositions or conjunctions) and PNX (punctuation).
(This stands in extreme contrast with the Buckwalter
Arabic tagset (PATB official tagset) which is almost
500 tags.) To obtain these dependency trees, we used
the constituent-to-dependency tool (Habash and Roth,
2009). Additional CATiB trees were annotated di-
rectly, but we only use the portions that are converted
from phrase-structure representation, to ensure that
the constituent and dependency yields can be aligned.
The Stanford Arabic Phrase Structure Treebank
In order to stay compatible with the state of the art,
we provide the constituency data set with most of the
pre-processing steps of Green and Manning (2010),
as they were shown to improve baseline performance
on the PATB parsing considerably.12
To convert the original PATB to preprocessed
phrase-structure trees ? la Stanford, we first discard
all trees dominated by X, which indicates errors and
non-linguistic text. At the phrasal level, we collapse
unary chains with identical categories like NP? NP.
We finally remove all traces, but, unlike Green and
Manning (2010), we keep all function tags.
In the original Stanford instance, the pre-terminal
morphological analyses were mapped to the short-
ened Bies tag set provided with the treebank (where
Determiner markers, ?DT?, were added to definite
noun and adjectives, resulting in 32 POS tags). Here
we use the Kulick tagset (Kulick et al, 2006) for
12Both the corpus split and pre-processing code are available
with the Stanford parser at http://nlp.stanford.edu/
projects/arabic.shtml.
155
pre-terminal categories in the phrase-structure trees,
where the Bies tag set is included as a morphological
feature (stanpos) in our PATB instance.
Adapting the Data to the Shared Task We con-
verted the CATiB representation to the CoNLL rep-
resentation and added a ?split-from-previous? and
?split-from-next? markers as in LDC?s tree-terminal
fields.
A major difference between the CATiB treebank
and the Stanford treebank lies in the way they han-
dle paragraph annotations. The original PATB con-
tains sequences of annotated trees that belong to a
same discourse unit (e.g., paragraph). While the
CATiB conversion tool considers each sequence a
single parsing unit, the Stanford pre-processor treats
each such tree structure rooted at S, NP or Frag as
a tree spanning a single sentence. To be compati-
ble with the predicted morphology data which was
bootstrapped and trained on the CATiB interpretation,
we deterministically modified the original PATB by
adding pseudo XP root nodes, so that the Stanford
pre-proprecessor will generate the same tree yields
as the CATiB treebank.
Another important aspect of preprocessing (often-
delegated as a technicality in the Arabic parsing lit-
erature) is the normalization of token forms. Most
Arabic parsing work used transliterated text based on
the schemes proposed by Buckwalter (2002). The
transliteration schemes exhibit some small differ-
ences, but enough to increase the out-of-vocabulary
rate by a significant margin (on top of strictly un-
known morphemes). This phenomenon is evident in
the morphological analysis lattices (in the predicted
dev set there is a 6% OOV rate without normalization,
and half a point reduction after normalization is ap-
plied, see (Habash et al, 2009b; Green and Manning,
2010)). This rate is much lower for gold tokenized
predicted data (with an OOV rate of only 3.66%,
similar to French for example). In our data set, all
tokens are minimally normalized: no diacritics, no
normalization.13
Data Splits For the Arabic treebanks, we use the
data split recommended by the Columbia Arabic and
Dialect Modeling (CADiM) group (Diab et al, 2013).
13Except for the minimal normalization present in MADA?s
back-end tools. This script was provided to the participants.
The data of the LDC first three annotated Arabic Tree-
banks (ATB1, ATB2 and ATB3) were divided into
roughly a 10/80/10% dev/train/test split by word vol-
ume. When dividing the corpora, document bound-
aries were maintained. The train5k files are simply
the first 5,000 sentences of the training files.
POS Tagsets Given the richness of Arabic mor-
phology, there are multiple POS tag sets and tokeniza-
tion schemes that have been used by researchers, (see,
e.g., Marton et al (2013a)). In the shared task, we fol-
low the standard PATB tokenization which splits off
several categories of orthographic clitics, but not the
definite article Al+. On top of that, we consider three
different POS tag sets with different degrees of gran-
ularity: the Buckwalter tag set (Buckwalter, 2004),
the Kulick Reduced Tag set (Kulick et al, 2006), and
the CATiB tag set (Habash et al, 2009a), considering
that granularity of the morphological analyses may
affect syntactic processing. For more information see
Habash (2010).
Predicted Morphology To prepare input for the
Raw scenarios (?3.3), we used the MADA+TOKAN
system (Habash et al, 2009b). MADA is a system
for morphological analysis and disambiguation of
Arabic. It can predict the 1-best tokenization, POS
tags, lemmas and diacritization in one fell swoop.
The MADA output was also used to generate the
lattice files for the Raw-all scenario.
To generate input for the gold token / predicted
tag input scenario, we used Morfette (Chrupa?a et al,
2008), a joint lemmatization and POS tagging model
based on an averaged perceptron. We generated two
tagging models, one trained with the Buckwalter tag
set, and the other with the Kulick tag set. Both were
mapped back to the CATiB POS tag set such that all
predicted tags are contained in the feature field.14
4.3 The Basque Treebank
Basque is an agglutinative language with a high ca-
pacity to generate inflected wordforms, with free
constituent order of sentence elements with respect
to the main verb. Contrary to many other treebanks,
the Basque treebank was originally annotated with
dependency trees, which were later on converted to
constituency trees.
14A conversion script from the rich Buckwalter tagset to
CoNLL-like features was provided to the participants.
156
The Basque Dependency Treebank (BDT) is a
dependency treebank in its original design, due to
syntactic characteristics of Basque such as its free
word order. Before the syntactic annotation, mor-
phological analysis was performed, using the Basque
morphological analyzer of Aduriz et al (2000). In
Basque each lemma can generate thousands of word-
forms ? differing in morphological properties such
as case, number, tense, or different types of subordi-
nation for verbs. If only POS category ambiguity is
resolved, the analyses remain highly ambiguous.
For the main POS category, there is an average of
1.55 interpretations per wordform, which rises to 2.65
for the full morpho-syntactic information, resulting
in an overall 64% of ambiguous wordforms. The
correct analysis was then manually chosen.
The syntactic trees were manually assigned. Each
word contains its lemma, main POS category, POS
subcategory, morphological features, and the la-
beled dependency relation. Each form indicates mor-
phosyntactic features such as case, number and type
of subordination, which are relevant for parsing.
The first version of the Basque Dependency Tree-
bank, consisting of 3,700 sentences (Aduriz et al,
2003), was used in the CoNLL 2007 Shared Task on
Dependency Parsing (Nivre et al, 2007a). The cur-
rent shared task uses the second version of the BDT,
which is the result of an extension and redesign of the
original requirements, containing 11,225 sentences
(150,000 tokens).
The Basque Constituency Treebank (BCT) was
created as part of the CESS-ECE project, where the
main aim was to obtain syntactically annotated con-
stituency treebanks for Catalan, Spanish and Basque
using a common set of syntactic categories. BCT
was semi-automatically derived from the dependency
version (Aldezabal et al, 2008). The conversion pro-
duced complete constituency trees for 80% of the
sentences. The main bottlenecks have been sentence
connectors and non-projective dependencies which
could not be straightforwardly converted into projec-
tive tree structures, requiring a mechanism similar to
traces in the Penn English Treebank.
Adapting the Data to the Shared Task As the
BCT did not contain all of the original non-projective
dependency trees, we selected the set of 8,000 match-
ing sentences in both treebanks for the shared task.15
This implies that around 2k trees could not be gen-
erated and therefore were discarded. Furthermore,
the BCT annotation scheme does not contain attach-
ment for most of the punctuation marks, so those
were inserted into the BCT using a simple lower-left
attachment heuristic. The same goes for some con-
nectors that could not be aligned in the first phase.
Predicted Morphology In order to obtain pre-
dicted tags for the non-gold scenarios, we used the
following pipeline. First, morphological analysis as
described above was performed, followed by a dis-
ambiguation step. At that point, it is hard to obtain a
single interpretation for each wordform, as determin-
ing the correct interpretation for each wordform may
require knowledge of long-distance elements on top
of the free constituency order of the main phrasal el-
ements in Basque. The disambiguation is performed
by the module by Ezeiza et al (1998), which uses
a combination of knowledge-based disambiguation,
by means of Constraint Grammar (Karlsson et al,
1995; Aduriz et al, 1997), and a posterior statistical
disambiguation module, using an HMM.16
For the shared task data, we chose a setting that
disambiguates most word forms, and retains ? 97%
of the correct interpretations, leaving an ambiguity
level of 1.3 interpretations. For the remaining cases
of ambiguity, we chose the first interpretation, which
corresponds to the most frequent option. This leaves
open the investigation of more complex approaches
for selecting the most appropriate reading.17
4.4 The French Treebank
French is not a morphologically rich language per se,
though its inflectional system is richer than that of
English, and it also exhibits a limited amount of word
order variation occurring at different syntactic levels
including the word level (e.g. pre- or post-nominal
15We generated a 80/10/10 split, ? train/dev/test ? The first 5k
sentences of the train set were used as a basis for the train5k.
16Note that the statistical module can be parametrized accord-
ing to the level of disambiguation to trade off precision and
recall. For example, disambiguation based on the main cate-
gories (abstracting over morpho-syntactic features) maintains
most of the correct interpretations but still gives an output with
several interpretations per wordform.
17This is not an easy task. The ambiguity left is the hardest to
solve given that the knowledge-based and statistical disambigua-
tion processes have not been able to pick out a single reading.
157
adjective, pre- or post-verbal adverbs) and the phrase
level (e.g. possible alternations between post verbal
NPs and PPs). It also has a high degree of multi-
word expressions, that are often ambiguous with a
literal reading as a sequence of simple words. The
syntactic and MWE analysis shows the same kind of
interaction (though to a lesser extent) as morphologi-
cal and syntactic interaction in Semitic languages ?
MWEs help parsing, and syntactic information may
be required to disambiguate MWE identification.
The Data Set The French data sets were gener-
ated from the French Treebank (Abeill? et al, 2003),
which consists of sentences from the newspaper Le
Monde, manually annotated with phrase structures
and morphological information. Part of the treebank
trees are also annotated with grammatical function
tags for dependents of verbs. In the SPMRL shared
task release, we used only this part, consisting of
18,535 sentences,18 split into 14,759 sentences for
training, 1,235 sentences for development, and 2,541
sentences for the final evaluation.19
Adapting the Data to the Shared Task The con-
stituency trees are provided in an extended PTB
bracketed format, with morphological features at the
pre-terminal level only. They contain slight, auto-
matically performed, modifications with respect to
the original trees of the French treebank. The syntag-
matic projection of prepositions and complementiz-
ers was normalized, in order to have prepositions and
complementizers as heads in the dependency trees
(Candito et al, 2010).
The dependency representations are projective de-
pendency trees, obtained through automatic conver-
sion from the constituency trees. The conversion pro-
cedure is an enhanced version of the one described
by Candito et al (2010).
Both the constituency and the dependency repre-
sentations make use of coarse- and fine-grained POS
tags (CPOS and FPOS respectively). The CPOS are
the categories from the original treebank. The FPOS
18The process of functional annotation is still ongoing, the
objective of the FTB providers being to have all the 20000 sen-
tences annotated with functional tags.
19The first 9,981 training sentences correspond to the canoni-
cal 2007 training set. The development set is the same and the
last 1235 sentences of the test set are those of the canonical test
set.
are merged using the CPOS and specific morphologi-
cal information such as verbal mood, proper/common
noun distinction (Crabb? and Candito, 2008).
Multi-Word Expressions The main difference
with respect to previous releases of the bracketed
or dependency versions of the French treebank
lies in the representation of multi-word expressions
(MWEs). The MWEs appear in an extended format:
each MWE bears an FPOS20 and consists of a se-
quence of terminals (hereafter the ?components? of
the MWE), each having their proper CPOS, FPOS,
lemma and morphological features. Note though that
in the original treebank the only gold information
provided for a MWE component is its CPOS. Since
leaving this information blank for MWE components
would have provided a strong cue for MWE recog-
nition, we made sure to provide the same kind of
information for every terminal, whether MWE com-
ponent or not, by providing predicted morphological
features, lemma, and FPOS for MWE components
(even in the ?gold? section of the data set). This infor-
mation was predicted by the Morfette tool (Chrupa?a
et al, 2008), adapted to French (Seddah et al, 2010).
In the constituency trees, each MWE corresponds
to an internal node whose label is the MWE?s FPOS
suffixed by a +, and which dominates the component
pre-terminal nodes.
In the dependency trees, there is no ?node? for a
MWE as a whole, but one node (a terminal in the
CoNLL format) per MWE component. The first com-
ponent of a MWE is taken as the head of the MWE.
All subsequent components of the MWE depend on
the first one, with the special label dep_cpd. Further-
more, the first MWE component bears a feature mwe-
head equal to the FPOS of the MWE. For instance,
the MWE la veille (the day before) is an adverb, con-
taining a determiner component and a common noun
component. Its bracketed representation is (ADV+
(DET la) (NC veille)), and in the dependency repre-
sentation, the noun veille depends on the determiner
la, which bears the feature mwehead=ADV+.
Predicted Morphology For the predicted mor-
phology scenario, we provide data in which the
mwehead has been removed and with predicted
20In the current data, we did not carry along the lemma and
morphological features pertaining to the MWE itself, though this
information is present in the original trees.
158
FPOS, CPOS, lemma, and morphological features,
obtained by training Morfette on the whole train set.
4.5 The German Treebank
German is a fusional language with moderately free
word order, in which verbal elements are fixed in
place and non-verbal elements can be ordered freely
as long as they fulfill the ordering requirements of
the clause (H?hle, 1986).
The Data Set The German constituency data set
is based on the TiGer treebank release 2.2.21 The
original annotation scheme represents discontinuous
constituents such that all arguments of a predicate
are always grouped under a single node regardless of
whether there is intervening material between them
or not (Brants et al, 2002). Furthermore, punctua-
tion and several other elements, such as parentheses,
are not attached to the tree. In order to make the
constituency treebank usable for PCFG parsing, we
adapted this treebank as described shortly.
The conversion of TiGer into dependencies is a
variant of the one by Seeker and Kuhn (2012), which
does not contain empty nodes. It is based on the same
TiGer release as the one used for the constituency
data. Punctuation was attached as high as possible,
without creating any new non-projective edges.
Adapting the Data to the Shared Task For
the constituency version, punctuation and other
unattached elements were first attached to the tree.
As attachment target, we used roughly the respec-
tive least common ancestor node of the right and
left terminal neighbor of the unattached element (see
Maier et al (2012) for details), and subsequently, the
crossing branches were resolved.
This was done in three steps. In the first step, the
head daughters of all nodes were marked using a
simple heuristic. In case there was a daughter with
the edge label HD, this daughter was marked, i.e.,
existing head markings were honored. Otherwise, if
existing, the rightmost daughter with edge label NK
(noun kernel) was marked. Otherwise, as default, the
leftmost daughter was marked. In a second step, for
each continuous part of a discontinuous constituent,
a separate node was introduced. This corresponds
21This version is available from http://www.ims.
uni-stuttgart.de/forschung/ressourcen/
korpora/tiger.html
to the "raising" algorithm described by Boyd (2007).
In a third steps, all those newly introduced nodes
that did not cover the head daughter of the original
discontinuous node were deleted. For the second
and the third step, we used the same script as for the
Swedish constituency data.
Predicted Morphology For the predicted scenario,
a single sequence of POS tags and morphologi-
cal features has been assigned using the MATE
toolchain via a model trained on the train set via cross-
validation on the training set. The MATE toolchain
was used to provide predicted annotation for lem-
mas, POS tags, morphology, and syntax. In order to
achieve the best results for each annotation level, a
10-fold jackknifing was performed to provide realis-
tic features for the higher annotation levels. The pre-
dicted annotation of the 5k training set were copied
from the full data set.22
4.6 The Hebrew Treebank
Modern Hebrew is a Semitic language, characterized
by inflectional and derivational (templatic) morphol-
ogy and relatively free word order. The function
words for from/to/like/and/when/that/the are prefixed
to the next token, causing severe segmentation ambi-
guity for many tokens. In addition, Hebrew orthogra-
phy does not indicate vowels in modern texts, leading
to a very high level of word-form ambiguity.
The Data Set Both the constituency and the de-
pendency data sets are derived from the Hebrew
Treebank V2 (Sima?an et al, 2001; Guthmann et
al., 2009). The treebank is based on just over 6000
sentences from the daily newspaper ?Ha?aretz?, man-
ually annotated with morphological information and
phrase-structure trees and extended with head infor-
mation as described in Tsarfaty (2010, ch. 5). The
unlabeled dependency version was produced by con-
version from the constituency treebank as described
in Goldberg (2011). Both the constituency and depen-
dency trees were annotated with a set grammatical
function labels conforming to Unified Stanford De-
pendencies by Tsarfaty (2013).
22We also provided a predicted-all scenario, in which we
provided morphological analysis lattices with POS and mor-
phological information derived from the analyses of the SMOR
derivational morphology (Schmid et al, 2004). These lattices
were not used by any of the participants.
159
Adapting the Data to the Shared Task While
based on the same trees, the dependency and con-
stituency treebanks differ in their POS tag sets, as
well as in some of the morphological segmentation
decisions. The main effort towards the shared task
was unifying the two resources such that the two tree-
banks share the same lexical yields, and the same
pre-terminal labels. To this end, we took the layering
approach of Goldberg et al (2009), and included two
levels of POS tags in the constituency trees. The
lower level is lexical, conforming to the lexical re-
source used to build the lattices, and is shared by
the two treebanks. The higher level is syntactic, and
follows the tag set and annotation decisions of the
original constituency treebank.23 In addition, we uni-
fied the representation of morphological features, and
fixed inconsistencies and mistakes in the treebanks.
Data Split The Hebrew treebank is one of the
smallest in our language set, and hence it is provided
in only the small (5k) setting. For the sake of com-
parability with the 5k set of the other treebanks, we
created a comparable size of dev/test sets containing
the first and last 500 sentences respectively, where
the rest serve as the 5k training.24
Predicted Morphology The lattices encoding the
morphological ambiguity for the Raw (all) scenario
were produced by looking up the possible analyses
of each input token in the wide-coverage morpholog-
ical analyzer (lexicon) of the Knowledge Center for
Processing Hebrew (Itai and Wintner, 2008; MILA,
2008), with a simple heuristic for dealing with un-
known tokens. A small lattice encoding the possible
analyses of each token was produced separately, and
these token-lattices were concatenated to produce the
sentence lattice. The lattice for a given sentence may
not include the gold analysis in cases of incomplete
lexicon coverage.
The morphologically disambiguated input files for
the Raw (1-best) scenario were produced by run-
ning the raw text through the morphological disam-
23Note that this additional layer in the constituency treebank
adds a relatively easy set of nodes to the trees, thus ?inflating?
the evaluation scores compared to previously reported results.
To compensate, a stricter protocol than is used in this task would
strip one of the two POS layers prior to evaluation.
24This split is slightly different than the split in previous stud-
ies.
biguator (tagger) described in Adler and Elhadad
(2006; Goldberg et al (2008),Adler (2007). The
disambiguator is based on the same lexicon that is
used to produce the lattice files, but utilizes an extra
module for dealing with unknown tokens Adler et al
(2008). The core of the disambiguator is an HMM
tagger trained on about 70M unannotated tokens us-
ing EM, and being supervised by the lexicon.
As in the case of Arabic, we also provided data
for the Predicted (gold token / predicted morphol-
ogy) scenario. We used the same sequence labeler,
Morfette (Chrupa?a et al, 2008), trained on the con-
catenation of POS and morphological gold features,
leading to a model with respectable accuracy.25
4.7 The Hungarian Treebank
Hungarian is an agglutinative language, thus a lemma
can have hundreds of word forms due to derivational
or inflectional affixation (nominal declination and
verbal conjugation). Grammatical information is typ-
ically indicated by suffixes: case suffixes mark the
syntactic relationship between the head and its argu-
ments (subject, object, dative, etc.) whereas verbs
are inflected for tense, mood, person, number, and
the definiteness of the object. Hungarian is also char-
acterized by vowel harmony.26 In addition, there are
several other linguistic phenomena such as causa-
tion and modality that are syntactically expressed in
English but encoded morphologically in Hungarian.
The Data Set The Hungarian data set used in
the shared task is based on the Szeged Treebank,
the largest morpho-syntactic and syntactic corpus
manually annotated for Hungarian. This treebank
is based on newspaper texts and is available in
both constituent-based (Csendes et al, 2005) and
dependency-based (Vincze et al, 2010) versions.
Around 10k sentences of news domain texts were
made available to the shared task.27 Each word is
manually assigned all its possible morpho-syntactic
25POS+morphology prediction accuracy is 91.95% overall
(59.54% for unseen tokens). POS only prediction accuracy is
93.20% overall (71.38% for unseen tokens).
26When vowel harmony applies, most suffixes exist in two
versions ? one with a front vowel and another one with a back
vowel ? and it is the vowels within the stem that determine which
form of the suffix is selected.
27The original treebank contains 82,000 sentences, 1.2 million
words and 250,000 punctuation marks from six domains.
160
tags and lemmas and the appropriate one is selected
according to the context. Sentences were manu-
ally assigned a constituency-based syntactic struc-
ture, which includes information on phrase structure,
grammatical functions (such as subject, object, etc.),
and subcategorization information (i.e., a given NP
is subcategorized by a verb or an infinitive). The
constituency trees were later automatically converted
into dependency structures, and all sentences were
then manually corrected. Note that there exist some
differences in the grammatical functions applied to
the constituency and dependency versions of the tree-
bank, since some morpho-syntactic information was
coded both as a morphological feature and as dec-
oration on top of the grammatical function in the
constituency trees.
Adapting the Data to the Shared Task Origi-
nally, the Szeged Dependency Treebank contained
virtual nodes for elided material (ELL) and phonolog-
ically covert copulas (VAN). In the current version,
they have been deleted, their daughters have been
attached to the parent of the virtual node, and have
been given complex labels, e.g. COORD-VAN-SUBJ,
where VAN is the type of the virtual node deleted,
COORD is the label of the virtual node and SUBJ is
the label of the daughter itself. When the virtual node
was originally the root of the sentence, its daughter
with a predicative (PRED) label has been selected as
the new root of the sentence (with the label ROOT-
VAN-PRED) and all the other daughters of the deleted
virtual node have been attached to it.
Predicted Morphology In order to provide the
same POS tag set for the constituent and dependency
treebanks, we used the dependency POS tagset for
both treebank instances. Both versions of the tree-
bank are available with gold standard and automatic
morphological annotation. The automatic POS tag-
ging was carried out by a 10-fold cross-validation
on the shared task data set by magyarlanc, a natu-
ral language toolkit for processing Hungarian texts
(segmentation, morphological analysis, POS tagging,
and dependency parsing). The annotation provides
POS tags and deep morphological features for each
input token (Zsibrita et al, 2013).28
28The full data sets of both the constituency and de-
pendency versions of the Szeged Treebank are available at
4.8 The Korean Treebank
The Treebank The Korean corpus is generated by
collecting constituent trees from the KAIST Tree-
bank (Choi et al, 1994), then converting the con-
stituent trees to dependency trees using head-finding
rules and heuristics. The KAIST Treebank consists
of about 31K manually annotated constituent trees
from 97 different sources (e.g., newspapers, novels,
textbooks). After filtering out trees containing an-
notation errors, a total of 27,363 trees with 350,090
tokens are collected.
The constituent trees in the KAIST Treebank29 also
come with manually inspected morphological analy-
sis based on ?eojeol?. An eojeol contains root-forms
of word tokens agglutinated with grammatical affixes
(e.g., case particles, ending markers). An eojeol can
consist of more than one word token; for instance, a
compound noun ?bus stop? is often represented as
one eojeol in Korean, ????????????, which can be
broken into two word tokens,???? (bus) and????????
(stop). Each eojeol in the KAIST Treebank is sepa-
rated by white spaces regardless of punctuation. Fol-
lowing the Penn Korean Treebank guidelines (Han
et al, 2002), punctuation is separated as individual
tokens, and parenthetical notations surrounded by
round brackets are grouped into individual phrases
with a function tag (PRN in our corpus).
All dependency trees are automatically converted
from the constituent trees. Unlike English, which
requires complicated head-finding rules to find the
head of each phrase (Choi and Palmer, 2012), Ko-
rean is a head final language such that the rightmost
constituent in each phrase becomes the head of that
phrase. Moreover, the rightmost conjunct becomes
the head of all other conjuncts and conjunctions in
a coordination phrase, which aligns well with our
head-final strategy.
The constituent trees in the KAIST Treebank do
not consist of function tags indicating syntactic or
semantic roles, which makes it difficult to generate
dependency labels. However, it is possible to gener-
ate meaningful labels by using the rich morphology
in Korean. For instance, case particles give good
the following website: www.inf.u-szeged.hu/rgai/
SzegedTreebank, and magyarlanc is downloadable from:
www.inf.u-szeged.hu/rgai/magyarlanc.
29See Lee et al (1997) for more details about the bracketing
guidelines of the KAIST Treebank.
161
indications of what syntactic roles eojeols with such
particles should take. Given this information, 21
dependency labels were generated according to the
annotation scheme proposed by Choi (2013).
Adapting the Data to the Shared Task All details
concerning the adaptation of the KAIST treebank
to the shared task specifications are found in Choi
(2013). Importantly, the rich KAIST treebank tag set
of 1975 POS tag types has been converted to a list of
CoNLL-like feature-attribute values refining coarse
grained POS categories.
Predicted Morphology Two sets of automatic
morphological analyses are provided for this task.
One is generated by the HanNanum morphological
analyzer.30 The HanNanum morphological ana-
lyzer gives the same morphemes and POS tags as the
KAIST Treebank. The other is generated by the Se-
jong morphological analyzer.31 The Sejong morpho-
logical analyzer gives a different set of morphemes
and POS tags as described in Choi and Palmer (2011).
4.9 The Polish Treebank
The Data Set Sk?adnica is a constituency treebank
of Polish (Wolin?ski et al, 2011; S?widzin?ski and
Wolin?ski, 2010). The trees were generated with
a non-probabilistic DCG parser S?wigra and then
disambiguated and validated manually. The ana-
lyzed texts come from the one-million-token sub-
corpus of the National Corpus of Polish (NKJP,
(Przepi?rkowski et al, 2012)) manually annotated
with morpho-syntactic tags.
The dependency version of Sk?adnica is a re-
sult of an automatic conversion of manually disam-
biguated constituent trees into dependency structures
(Wr?blewska, 2012). The conversion was an entirely
automatic process. Conversion rules were based
on morpho-syntactic information, phrasal categories,
and types of phrase-structure rules encoded within
constituent trees. It was possible to extract dependen-
cies because the constituent trees contain information
about the head of the majority of constituents. For
other constituents, heuristics were defined in order to
select their heads.
30http://kldp.net/projects/hannanum
31http://www.sejong.or.kr
The version of Sk?adnica used in the shared task
comprises parse trees for 8,227 sentences.32
Predicted Morphology For the shared task Pre-
dicted scenario, an automatic morphological an-
notation was generated by the PANTERA tagger
(Acedan?ski, 2010).
4.10 The Swedish Treebank
Swedish is moderately rich in inflections, including
a case system. Word order obeys the verb second
constraint in main clauses but is SVO in subordinate
clauses. Main clause order is freer than in English
but not as free as in some other Germanic languages,
such as German. Also, subject agreement with re-
spect to person and number has been dropped in
modern Swedish.
The Data Set The Swedish data sets are taken
from the Talbanken section of the Swedish Treebank
(Nivre and Megyesi, 2007). Talbanken is a syntacti-
cally annotated corpus developed in the 1970s, orig-
inally annotated according to the MAMBA scheme
(Teleman, 1974) with a syntactic layer consisting
of flat phrase structure and grammatical functions.
The syntactic annotation was later automatically con-
verted to full phrase structure with grammatical func-
tions and from that to dependency structure, as de-
scribed by Nivre et al (2006).
Both the phrase structure and the dependency
version use the functional labels from the original
MAMBA scheme, which provides a fine-grained clas-
sification of syntactic functions with 65 different la-
bels, while the phrase structure annotation (which
had to be inferred automatically) uses a coarse set
of only 8 labels. For the release of the Swedish tree-
bank, the POS level was re-annotated to conform to
the current de facto standard for Swedish, which is
the Stockholm-Ume? tagset (Ejerhed et al, 1992)
with 25 base tags and 25 morpho-syntactic features,
which together produce over 150 complex tags.
For the shared task, we used version 1.2 of the
treebank, where a number of conversion errors in
the dependency version have been corrected. The
phrase structure version was enriched by propagating
morpho-syntactic features from preterminals (POS
32Sk?adnica is available from http://zil.ipipan.waw.
pl/Sklicense.
162
tags) to higher non-terminal nodes using a standard
head percolation table, and a version without crossing
branches was derived using the lifting strategy (Boyd,
2007).
Adapting the Data to the Shared Task Explicit
attribute names were added to the feature field and the
split was changed to match the shared task minimal
training set size.
Predicted Morphology POS tags and morpho-
syntactic features were produced using the Hun-
PoS tagger (Hal?csy et al, 2007) trained on the
Stockholm-Ume? Corpus (Ejerhed and K?llgren,
1997).
5 Overview of the Participating Systems
With 7 teams participating, more than 14 systems for
French and 10 for Arabic and German, this shared
task is on par with the latest large-scale parsing evalu-
ation campaign SANCL 2012 (Petrov and McDonald,
2012). The present shared task was extremely de-
manding on our participants. From 30 individuals or
teams who registered and obtained the data sets, we
present results for the seven teams that accomplished
successful executions on these data in the relevant
scenarios in the given the time frame.
5.1 Dependency Track
Seven teams participated in the dependency track.
Two participating systems are based on MaltParser:
MALTOPTIMIZER (Ballesteros, 2013) and AI:KU
(Cirik and S?ensoy, 2013). MALTOPTIMIZER uses
a variant of MaltOptimizer (Ballesteros and Nivre,
2012) to explore features relevant for the processing
of morphological information. AI:KU uses a combi-
nation of MaltParser and the original MaltOptimizer.
Their system development has focused on the inte-
gration of an unsupervised word clustering method
using contextual and morphological properties of the
words, to help combat sparseness.
Similarly to MaltParser ALPAGE:DYALOG
(De La Clergerie, 2013) also uses a shift-reduce
transition-based parser but its training and decoding
algorithms are based on beam search. This parser is
implemented on top of the tabular logic programming
system DyALog. To the best of our knowledge, this
is the first dependency parser capable of handling
word lattice input.
Three participating teams use the MATE parser
(Bohnet, 2010) in their systems: the BASQUETEAM
(Goenaga et al, 2013), IGM:ALPAGE (Constant et
al., 2013) and IMS:SZEGED:CIS (Bj?rkelund et al,
2013). The BASQUETEAM uses the MATE parser in
combination with MaltParser (Nivre et al, 2007b).
The system combines the parser outputs via Malt-
Blender (Hall et al, 2007). IGM:ALPAGE also uses
MATE and MaltParser, once in a pipeline architec-
ture and once in a joint model. The models are com-
bined via a re-parsing strategy based on (Sagae and
Lavie, 2006). This system mainly focuses on MWEs
in French and uses a CRF tagger in combination
with several large-scale dictionaries to handle MWEs,
which then serve as input for the two parsers.
The IMS:SZEGED:CIS team participated in both
tracks, with an ensemble system. For the depen-
dency track, the ensemble includes the MATE parser
(Bohnet, 2010), a best-first variant of the easy-first
parser by Goldberg and Elhadad (2010b), and turbo
parser (Martins et al, 2010), in combination with
a ranker that has the particularity of using features
from the constituent parsed trees. CADIM (Marton et
al., 2013b) uses their variant of the easy-first parser
combined with a feature-rich ensemble of lexical and
syntactic resources.
Four of the participating teams use exter-
nal resources in addition to the parser. The
IMS:SZEGED:CIS team uses external morpholog-
ical analyzers. CADIM uses SAMA (Graff et al,
2009) for Arabic morphology. ALPAGE:DYALOG
and IGM:ALPAGE use external lexicons for French.
IGM:ALPAGE additionally uses Morfette (Chrupa?a
et al, 2008) for morphological analysis and POS
tagging. Finally, as already mentioned, AI:KU clus-
ters words and POS tags in an unsupervised fashion
exploiting additional, un-annotated data.
5.2 Constituency Track
A single team participated in the constituency parsing
task, the IMS:SZEGED:CIS team (Bj?rkelund et al,
2013). Their phrase-structure parsing system uses a
combination of 8 PCFG-LA parsers, trained using a
product-of-grammars procedure (Petrov, 2010). The
50-best parses of this combination are then reranked
by a model based on the reranker by Charniak and
163
Johnson (2005).33
5.3 Baselines
We additionally provide the results of two baseline
systems for the nine languages, one for constituency
parsing and one for dependency parsing.
For the dependency track, our baseline system is
MaltParser in its default configuration (the arc-eager
algorithm and liblinear for training). Results marked
as BASE:MALT in the next two sections report the
results of this baseline system in different scenarios.
The constituency parsing baseline is based on the
most recent version of the PCFG-LA model of Petrov
et al (2006), used with its default settings and five
split/merge cycles, for all languages.34 We use this
parser in two configurations: a ?1-best? configura-
tion where all POS tags are provided to the parser
(predicted or gold, depending on the scenario), and
another configuration in which the parser performs
its own POS tagging. These baselines are referred to
as BASE:BKY+POS and BASE:BKY+RAW respec-
tively in the following results sections. Note that
even when BASE:BKY+POS is given gold POS tags,
the Berkeley parser sometimes fails to reach a perfect
POS accuracy. In cases when the parser cannot find a
parse with the provided POS, it falls back on its own
POS tagging for all tokens.
6 Results
The high number of submitted system variants and
evaluation scenarios in the task resulted in a large
number of evaluation scores. In the following evalu-
ation, we focus on the best run for each participant,
and we aim to provide key points on the different
dimensions of analysis resulting from our evaluation
protocol. We invite our interested readers to browse
the comprehensive representation of our results on
the official shared-task results webpages.35
33Note that a slight but necessary change in the configuration
of one of our metrics, which occurred after the system submis-
sion deadline, resulted in the IMS:SZEGED:CIS team to submit
suboptimal systems for 4 languages. Their final scores are ac-
tually slightly higher and can be found in (Bj?rkelund et al,
2013).
34For Semitic languages, we used the lattice based PCFG-LA
extension by Goldberg (2011).
35http://www.spmrl.org/
spmrl2013-sharedtask-results.html.
6.1 Gold Scenarios
This section presents the parsing results in gold sce-
narios, where the systems are evaluated on gold seg-
mented and tagged input. This means that the se-
quence of terminals, POS tags, and morphological
features are provided based on the treebank anno-
tations. This scenario was used in most previous
shared tasks on data-driven parsing (Buchholz and
Marsi, 2006; Nivre et al, 2007a; K?bler, 2008). Note
that this scenario was not mandatory. We thank our
participants for providing their results nonetheless.
We start by reviewing dependency-based parsing
results, both on the trees and on multi-word expres-
sion, and continue with the different metrics for
constituency-based parsing.
6.1.1 Dependency Parsing
Full Training Set The results for the gold parsing
scenario of dependency parsing are shown in the top
block of table 3.
Among the six systems, IMS:SZEGED:CIS
reaches the highest LAS scores, not only on aver-
age, but for every single language. This shows that
their approach of combining parsers with (re)ranking
provides robust parsing results across languages with
different morphological characteristics. The second
best system is ALPAGE:DYALOG, the third best sys-
tem is MALTOPTIMIZER. The fact that AI:KU is
ranked below the Malt baseline is due to their sub-
mission of results for 6 out of the 9 languages. Simi-
larly, CADIM only submitted results for Arabic and
ranked in the third place for this language, after the
two IMS:SZEGED:CIS runs. IGM:ALPAGE and
BASQUETEAM did not submit results for this setting.
Comparing LAS results across languages is prob-
lematic due to the differences between languages,
treebank size and annotation schemes (see section 3),
so the following discussion is necessarily tentative. If
we consider results across languages, we see that the
lowest results (around 83% for the best performing
system) are reached for Hebrew and Swedish, the
languages with the smallest data sets. The next low-
est result, around 86%, is reached for Basque. Other
languages reach similar LAS scores, around 88-92%.
German, with the largest training set, reaches the
highest LAS, 91.83%.
Interstingly, all systems have high LAS scores
on the Korean Treebank given a training set size
164
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 89.83 86.68 90.29 91.83 83.87 88.06 89.59 89.58 83.97 88.19
ALPAGE:DYALOG 85.87 80.39 87.69 88.25 80.70 79.60 88.23 86.00 79.80 84.06
MALTOPTIMIZER 87.03 82.07 85.71 86.96 80.03 83.14 89.39 80.49 77.67 83.61
BASE:MALT 82.28 69.19 79.86 79.98 76.61 72.34 88.43 77.70 75.73 78.01
AI:KU 86.39 86.98 79.42 83.67 85.16 78.87 55.61
CADIM 85.56 9.51
2) gold setting / 5k training set
IMS:SZEGED:CIS 87.35 85.69 88.73 87.70 83.87 87.21 83.38 89.16 83.97 86.34
ALPAGE:DYALOG 83.25 79.11 85.66 83.88 80.70 78.42 81.91 85.67 79.80 82.04
MALTOPTIMIZER 85.30 81.40 84.93 83.59 80.03 82.37 83.74 79.79 77.67 82.09
BASE:MALT 80.36 67.13 78.16 76.64 76.61 71.27 81.93 76.64 75.73 76.05
AI:KU 84.98 83.47 79.42 82.84 84.37 78.87 54.88
CADIM 82.67 9.19
3) predicted setting / full training set
IMS:SZEGED:CIS 86.21 85.14 85.24 89.65 80.89 86.13 86.62 87.07 82.13 85.45
ALPAGE:DYALOG 81.20 77.55 82.06 84.80 73.63 75.58 81.02 82.56 77.54 79.55
MALTOPTIMIZER 81.90 78.58 79.00 82.75 73.01 79.63 82.65 79.89 75.82 79.25
BASE:MALT 80.36 70.11 77.98 77.81 69.97 70.15 82.06 75.63 73.21 75.25
AI:KU 72.57 82.32 69.01 78.92 81.86 76.35 51.23
BASQUETEAM 84.25 84.51 88.66 84.97 80.88 47.03
IGM:ALPAGE 85.86 9.54
CADIM 83.20 9.24
4) predicted setting / 5k training set
IMS:SZEGED:CIS 83.66 83.84 83.45 85.08 80.89 85.24 80.80 86.69 82.13 83.53
MALTOPTIMIZER 79.64 77.59 77.56 79.22 73.01 79.00 75.90 79.50 75.82 77.47
ALPAGE:DYALOG 78.65 76.06 80.11 73.07 73.63 74.48 73.79 82.04 77.54 76.60
BASE:MALT 78.48 68.12 76.54 74.81 69.97 69.08 74.87 75.29 73.21 73.37
AI:KU 71.23 79.16 69.01 78.04 81.30 76.35 50.57
BASQUETEAM 83.19 82.65 84.70 84.01 80.88 46.16
IGM:ALPAGE 83.60 9.29
CADIM 80.51 8.95
Table 3: Dependency parsing: LAS scores for full and 5k training sets and for gold and predicted input. Results in bold
show the best results per language and setting.
of approximately 23,000 sentences, which is a little
over half of the German treebank. For German, on
the other hand, only the IMS:SZEGED:CIS system
reaches higher LAS scores than for Korean. This
final observation indicates that more than treebank
size is important for comparing system performance
across treebanks. This is the reason for introducing
the reduced set scenario, in which we can see how the
participating system perform on a common ground,
albeit small.
5k Training Set The results for the gold setting
on the 5k train set are shown in the second block
of Table 3. Compared with the full training, we
see that there is a drop of around 2 points in this
setting. Some parser/language pairs are more sensi-
tive to data sparseness than others. CADIM, for in-
stance, exhibit a larger drop than MALTOPTIMIZER
on Arabic, and MALTOPTIMIZER shows a smaller
drop than IMS:SZEGED:CIS on French. On average,
among all systems that covered all languages, MALT-
OPTIMIZER has the smallest drop when moving to
5k training, possibly since the automatic feature opti-
mization may differ for different data set sizes.
Since all languages have the same number of sen-
tences in the train set, these results can give us limited
insight into the parsing complexity of the different
treebanks. Here, French, Arabic, Polish, and Korean
reach the highest LAS scores while Swedish reaches
165
Team F_MWE F_COMP F_MWE+POS
1) gold setting / full training set
AI:KU 99.39 99.53 99.34
IMS:SZEGED:CIS 99.26 99.39 99.21
MALTOPTIMIZER 98.95 98.99 0
ALPAGE:DYALOG 98.32 98.81 0
BASE:MALT 68.7 72.55 68.7
2) predicted setting / full training set
IGM:ALPAGE 80.81 81.18 77.37
IMS:SZEGED:CIS 79.45 80.79 70.48
ALPAGE:DYALOG 77.91 79.25 0
BASQUE-TEAM 77.19 79.81 0
MALTOPTIMIZER 70.29 74.25 0
BASE:MALT 67.49 71.01 0
AI:KU 0 0 0
3) predicted setting / 5k training set
IGM:ALPAGE 77.66 78.68 74.04
IMS:SZEGED:CIS 77.28 78.92 70.42
ALPAGE:DYALOG 75.17 76.82 0
BASQUETEAM 73.07 76.58 0
MALTOPTIMIZER 65.76 70.42 0
BASE:MALT 62.05 66.8 0
AI:KU 0 0 0
Table 4: Dependency Parsing: MWE results
the lowest one. Treebank variance depends not only
on the language but also on annotation decisions,
such as label set (Swedish, interestingly, has a rela-
tively rich one). A more careful comparison would
then take into account the correlation of data size,
label set size and parsing accuracy. We investigate
these correlations further in section 7.1.
6.1.2 Multiword Expressions
MWE results on the gold setting are found at
the top of Table 4. All systems, with the excep-
tion of BASE:MALT, perform exceedingly well in
identifying the spans and non-head components of
MWEs given gold morphology.36 These almost per-
fect scores are the consequence of the presence of
two gold MWE features, namely MWEHEAD and
PRED=Y, which respectively indicate the node span
of the whole MWE and its dependents, which do not
have a gold feature field. The interesting scenario is,
of course, the predicted one, where these features are
not provided to the parser, as in any realistic applica-
tion.
36Note that for the labeled measure F_MWE+POS, both
MALTOPTIMIZER and ALPAGE:DYALOG have an F-score of
zero, since they do not attempt to predict the MWE label at all.
6.1.3 Constituency Parsing
In this part, we provide accuracy results for phrase-
structure trees in terms of ParsEval F-scores. Since
ParsEval is sensitive to the non-terminals-per-word
ratio in the data set (Rehbein and van Genabith,
2007a; Rehbein and van Genabith, 2007b), and given
the fact that this ratio varies greatly within our data
set (as shown in Table 2), it must be kept in mind that
ParsEval should only be used for comparing parsing
performance over treebank instances sharing the ex-
act same properties in term of annotation schemes,
sentence length and so on. When comparing F-Scores
across different treebanks and languages, it can only
provide a rough estimate of the relative difficulty or
ease of parsing these kinds of data.
Full Training Set The F-score results for the gold
scenario are provided in the first block of Table 5.
Among the two baselines, BASE:BKY+POS fares
better than BASE:BKY+RAW since the latter selects
its own POS tags and thus cannot benefit from the
gold information. The IMS:SZEGED:CIS system
clearly outperforms both baselines, with Hebrew as
an outlier.37
As in the dependency case, the results are not
strictly comparable across languages, yet we can
draw some insights from them. We see consider-
able differences between the languages, with Basque,
Hebrew, and Hungarian reaching F-scores in the low
90s for the IMS:SZEGED:CIS system, Korean and
Polish reaching above-average F-scores, and Ara-
bic, French, German, and Swedish reaching F-scores
below the average, but still in the low 80s. The per-
formance is, again, not correlated with data set sizes.
Parsing Hebrew, with one of the smallest training
sets, obtains higher accuracy many other languages,
including Swedish, which has the same training set
size as Hebrew. It may well be that gold morphologi-
cal information is more useful for combatting sparse-
ness in languages with richer morphology (though
Arabic here would be an outlier for this conjecture),
or it may be that certain treebanks and schemes are
inherently harder to parser than others, as we investi-
gate in section 7.
For German, the language with the largest training
37It might be that the easy layer of syntactic tags benefits from
the gold POS tags provided. See section 4 for further discussion
of this layer.
166
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 82.20 90.04 83.98 82.07 91.64 92.60 86.50 88.57 85.09 86.97
BASE:BKY+POS 80.76 76.24 81.76 80.34 92.20 87.64 82.95 88.13 82.89 83.66
BASE:BKY+RAW 79.14 69.78 80.38 78.99 87.32 81.44 73.28 79.51 78.94 78.75
2) gold setting / 5k training set
IMS:SZEGED:CIS 79.47 88.45 82.25 74.78 91.64 91.87 80.10 88.18 85.09 84.65
BASE:BKY+POS 77.54 74.06 78.07 71.37 92.20 86.74 72.85 87.91 82.89 80.40
BASE:BKY+RAW 75.22 67.16 75.91 68.94 87.32 79.34 60.40 78.30 78.94 74.61
3) predicted setting / full training set
IMS:SZEGED:CIS 81.32 87.86 81.83 81.27 89.46 91.85 84.27 87.55 83.99 85.49
BASE:BKY+POS 78.66 74.74 79.76 78.28 85.42 85.22 78.56 86.75 80.64 80.89
BASE:BKY+RAW 79.19 70.50 80.38 78.30 86.96 81.62 71.42 79.23 79.18 78.53
4) predicted setting / 5k training set
IMS:SZEGED:CIS 78.85 86.65 79.83 73.61 89.46 90.53 78.47 87.46 83.99 83.21
BASE:BKY+POS 74.84 72.35 76.19 69.40 85.42 83.82 67.97 87.17 80.64 77.53
BASE:BKY+RAW 74.57 66.75 75.76 68.68 86.96 79.35 58.49 78.38 79.18 74.24
Table 5: Constituent Parsing: ParsEval F-scores for full and 5k training sets and for gold and predicted input. Results in
bold show the best results per language and setting.
set and the highest scores in dependency parsing,
the F-scores are at the lower end. These low scores,
which are obtained despite the larger treebank and
only moderately free word-order, are surprising. This
may be due to case syncretism; gold morphological
information exhibits its own ambiguity and thus may
not be fully utilized.
5k Training Set Parsing results on smaller com-
parable test sets are presented in the second block
of Table 5. On average, IMS:SZEGED:CIS is less
sensitive than BASE:BKY+POS to the reduced size.
Systems are not equally sensitive to reduced training
sets, and the gaps range from 0.4% to 3%, with Ger-
man and Korean as outliers (Korean suffering a 6.4%
drop in F-score and German 7.3%). These languages
have the largest treebanks in the full setting, so it is
not surprising that they suffer the most. But this in
itself does not fully explain the cross-treebank trends.
Since ParsEval scores are known to be sensitive to
the label set sizes and the depth of trees, we provide
LeafAncestor scores in the following section.
6.1.4 Leaf-Ancestor Results
The variation across results in the previous subsec-
tion may have been due to differences across annota-
tion schemes. One way to neutralize this difference
(to some extent) is to use a different metric. We
evaluated the constituency parsing results using the
Leaf-Ancestor (LA) metric, which is less sensitive
to the number of nodes in a tree (Rehbein and van
Genabith, 2007b; K?bler et al, 2008). As shown in
Table 6, these results are on a different (higher) scale
than ParsEval, and the average gap between the full
and 5k setting is lower.
Full Training Set The LA results in gold setting
for full training sets are shown in the first block of Ta-
ble 6. The trends are similar to the ParsEval F-scores.
German and Arabic present the lowest LA scores
(in contrast to the corresponding F-scores, Arabic is
a full point below German for IMS:SZEGED:CIS).
Basque and Hungarian have the highest LA scores.
Hebrew, which had a higher F-score than Basque,
has a lower LA than Basque and is closer to French.
Korean also ranks worse in the LA analysis. The
choice of evaluation metrics thus clearly impacts sys-
tem rankings ? F-scores rank some languages suspi-
ciously high (e.g., Hebrew) due to deeper trees, and
another metric may alleviate that.
5k Training Set The results for the leaf-ancestor
(LA) scores in the gold setting for the 5k training set
are shown in the second block of Table 6. Across
167
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 88.61 94.90 92.51 89.63 92.84 95.01 91.30 94.52 91.46 92.31
BASE:BKY+POS 87.85 91.55 91.74 88.47 92.69 92.52 90.82 92.81 90.76 91.02
BASE:BKY+RAW 87.05 89.71 91.22 87.77 91.29 90.62 87.11 90.58 88.97 89.37
2) gold setting / 5k training set
IMS:SZEGED:CIS 86.68 94.21 91.56 85.74 92.84 94.79 88.87 94.17 91.46 91.15
BASE:BKY+POS 86.26 90.72 89.71 84.11 92.69 92.11 86.75 92.91 90.76 89.56
BASE:BKY+RAW 84.97 88.68 88.74 83.08 91.29 89.94 81.82 90.31 88.97 87.53
3) predicted setting / full training set
IMS:SZEGED:CIS 88.45 94.50 91.79 89.32 91.95 94.90 90.13 94.11 91.05 91.80
BASE:BKY+POS 86.60 90.90 90.96 87.46 89.66 91.72 89.10 92.56 89.51 89.83
BASE:BKY+RAW 86.97 89.91 91.11 87.46 90.77 90.50 86.68 90.48 89.16 89.23
4) predicted setting / 5k training set
IMS:SZEGED:CIS 86.69 93.85 90.76 85.20 91.95 94.05 87.99 93.99 91.05 90.61
BASE:BKY+POS 84.76 89.83 89.18 83.05 89.66 91.24 84.87 92.74 89.51 88.32
BASE:BKY+RAW 84.63 88.50 89.00 82.69 90.77 89.93 81.50 90.08 89.16 87.36
Table 6: Constituent Parsing: Leaf-Ancestor scores for full and 5k training sets and for gold and predicted input.
parsers, IMS:SZEGED:CIS again has a smaller drop
than BASE:BKY+POS on the reduced size. German
suffers the most from the reduction of the training
set, with a loss of approximately 4 points. Korean,
however, which was also severely affected in terms
of F-scores, only loses 1.17 points in the LA score.
On average, the LA seem to reflect a smaller drop
when reducing the training set ? this underscores
again the impact of the choice of metrics on system
evaluation.
6.2 Predicted Scenarios
Gold scenarios are relatively easy since syntactically
relevant morphological information is disambiguated
in advance and is provided as input. Predicted scenar-
ios are more difficult: POS tags and morphological
features have to be automatically predicted, by the
parser or by external resources.
6.2.1 Dependency Parsing
Eight participating teams submitted dependency
results for this scenario. Two teams submitted for a
single language. Four teams covered all languages.
Full Training Set The results for the predicted
scenario in full settings are shown in the third
block of Table 3. Across the board, the re-
sults are considerably lower than the gold sce-
nario. Again, IMS:SZEGED:CIS is the best per-
forming system, followed by ALPAGE:DYALOG and
MALTOPTIMIZER. The only language for which
IMS:SZEGED:CIS is outperformed is French, for
which IGM:ALPAGE reaches higher results (85.86%
vs. 85.24%). This is due to the specialized treatment
of French MWEs in the IGM:ALPAGE system, which
is thereby shown to be beneficial for parsing in the
predicted setting.
If we compare the results for the predicted set-
ting and the gold one, given the full training set,
the IMS:SZEGED:CIS system shows small differ-
ences between 1.5 and 2 percent. The only ex-
ception is French, for which the LAS drops from
90.29% to 85.24% in the predicted setting. The
other systems show somewhat larger differences than
IMS:SZEGED:CIS, with the highest drops for Ara-
bic and Korean. The AI:KU system shows a similar
problem as IMS:SZEGED:CIS for French.
5k Training Set When we consider the predicted
setting for the 5k training set, in the last block of
Table 3, we see the same trends as comparing with
the full training set or when comparing to the gold
setting. Systems suffer from not having gold stan-
dard data, and they suffer from the small training set.
Interestingly, the loss between the different training
set sizes in the predicted setting is larger than in the
168
gold setting, but only marginally so, with a differ-
ence < 0.5. In other words, the predicted setting
adds a challenge to parsing, but it only minimally
compounds data sparsity.
6.2.2 Multiword Expressions Evaluation
In the predicted setting, shown in the second
block of table 4 for the full training set and in the
third block of the same table for the 5k training set,
we see that only two systems, IGM:ALPAGE and
IMS:SZEGED:CIS can predict the MWE label when
it is not present in the training set. IGM:ALPAGE?s
approach of using a separate classifier in combination
with external dictionaries is very successful, reach-
ing an F_MWE+POS score of 77.37. This is com-
pared to the score of 70.48 by IMS:SZEGED:CIS,
which predicts this node label as a side effect of
their constituent feature enriched dependency model
(Bj?rkelund et al, 2013). AI:KU has a zero score
for all predicted settings, which results from an erro-
neous training on the gold data rather than the pre-
dicted data.38
6.2.3 Constituency Parsing
Full Training Set The results for the predicted set-
ting with the full training set are shown in the third
block of table 5. A comparison with the gold setting
shows that all systems have a lower performance in
the predicted scenario, and the differences are in the
range of 0.88 for Arabic and 2.54 for Basque. It is
interesting to see that the losses are generally smaller
than in the dependency framework: on average, the
loss across languages is 2.74 for dependencies and
1.48 for constituents. A possible explanation can be
found in the two-dimensional structure of the con-
stituent trees, where only a subset of all nodes is
affected by the quality of morphology and POS tags.
The exception to this trend is Basque, for which the
loss in constituents is a full point higher than for de-
pendencies. Another possible explanation is that all
of our constituent parsers select their own POS tags
in one way or another. Most dependency parsers ac-
cept predicted tags from an external resource, which
puts an upper-bound on their potential performance.
5k Training Set The results for the predicted set-
ting given the 5k training set are shown in the bottom
38Unofficial updated results are to to be found in (Cirik and
S?ensoy, 2013)
block of table 5. They show the same trends as the
dependency ones: The results are slightly lower than
the results obtained in gold setting and the ones uti-
lizing the full training set.
6.2.4 Leaf Ancestor Metrics
Full Training Set The results for the predicted sce-
nario with a full training set are shown in the third
block of table 6. In the LA evaluation, the loss
in moving from gold morphology are considerably
smaller than in F-scores. For most languages, the
loss is less than 0.5 points. Exceptions are French
with a loss of 0.72, Hebrew with 0.89, and Korean
with 1.17. Basque, which had the highest loss in
F-scores, only shows a minor loss of 0.4 points. Also,
the average loss of 0.41 points is much smaller than
the one in the ParsEval score, 1.48.
5k Training Set The results for the predicted set-
ting given the 5k training set are shown in the last
block of table 6. These results, though considerably
lower (around 3 points), exhibit the exact same trends
as observed in the gold setting.
6.3 Realistic Raw Scenarios
The previous scenarios assume that input surface to-
kens are identical to tree terminals. For languages
such as Arabic and Hebrew, this is not always the
case. In this scenario, we evaluate the capacity of a
system to predict both morphological segmentation
and syntactic parse trees given raw, unsegmented
input tokens. This may be done via a pipeline as-
suming a 1-st best morphological analysis, or jointly
with parsing, assuming an ambiguous morpholog-
ical analysis lattice as input. In this task, both of
these scenarios are possible (see section 3). Thus,
this section presents a realistic evaluation of the par-
ticipating systems, using TedEval, which takes into
account complete morpho-syntactic parses.
Tables 7 and 8 present labeled and unlabeled
TedEval results for both constituency and depen-
dency parsers, calculated only for sentence of length
<= 70.39 We firstly observe that labeled TedEval
scores are considerably lower than unlabeled Ted-
Eval scores, as expected, since unlabeled scores eval-
uate only structural differences. In the labeled setup,
39TedEval builds on algorithms for calculating edit distance
on complete trees (Bille, 2005). In these algorithms, longer
sentences take considerably longer to evaluate.
169
Arabic Arabic Hebrew All
full training set 5k training set
Acc (x100) Ex (%) Acc (x100) Ex (%) Acc (x100) Ex (%) Avg. Soft Avg.
IMS:SZEGED:CIS (Bky) 83.34 1.63 82.54 0.67 56.47 0.67 69.51 69.51
IMS:SZEGED:CIS 89.12 8.37 87.82 5.56 86.08 8.27 86.95 86.95
CADIM 87.81 6.63 86.43 4.21 - - 43.22 86.43
MALTOPTIMIZER 86.74 5.39 85.63 3.03 83.05 5.33 84.34 84.34
ALPAGE:DYALOG 86.60 5.34 85.71 3.54 82.96 6.17 41.48 82.96
ALPAGE:DYALOG (RAW) - - - - 82.82 4.35 41.41 82.82
AI:KU - - - - 78.57 3.37 39.29 78.57
Table 7: Realistic Scenario: Tedeval Labeled Accuracy and Exact Match for the Raw scenario.
The upper part refers to constituency results, the lower part refers to dependency results
Arabic Arabic Hebrew All
full training set 5k training set
Acc (x100) Ex (%) Acc (x100) Ex (%) Acc (x100) Ex (%) Avg. Soft Avg.
IMS:SZEGED:CIS (Bky) 92.06 9.49 91.29 7.13 89.30 13.60 90.30 90.30
IMS:SZEGED:CIS 91.74 9.83 90.85 7.30 89.47 16.97 90.16 90.16
ALPAGE:DYALOG 89.99 7.98 89.46 5.67 88.33 12.20 88.90 88.90
MALTOPTIMIZER 90.09 7.08 89.47 5.56 87.99 11.64 88.73 88.73
CADIM 90.75 8.48 89.89 5.67 - - 44.95 89.89
ALPAGE:DYALOG (RAW) - - - - 87.61 10.24 43.81 87.61
AI:KU - - - - 86.70 8.98 43.35 86.70
Table 8: Realistic Scenario: Tedeval Unlabeled Accuracy and Exact Match for the Raw scenario.
Top upper part refers to constituency results, the lower part refers to dependency results.
the IMS:SZEGED:CIS dependency parser are the
best for both languages and data set sizes. Table 8
shows that their unlabeled constituency results reach
a higher accuracy than the next best system, their
own dependency results. However, a quick look at
the exact match metric reveals lower scores than for
its dependency counterparts.
For the dependency-based joint scenarios, there
is obviously an upper bound on parser performance
given inaccurate segmentation. The transition-based
systems, ALPAGE:DYALOG & MALTOPTIMIZER,
perform comparably on Arabic and Hebrew, with
ALPAGE:DYALOG being slightly better on both lan-
guages. Note that ALPAGE:DYALOG reaches close
results on the 1-best and the lattice-based input set-
tings, with a slight advantage for the former. This is
partly due to the insufficient coverage of the lexical
resource we use: many lattices do not contain the
gold path, so the joint prediction can only as be high
as the lattice predicted path allows.
7 Towards In-Depth Cross-Treebank
Evaluation
Section 6 reported evaluation scores across systems
for different scenarios. However, as noted, these re-
sults are not comparable across languages, represen-
tation types and parsing scenarios due to differences
in the data size, label set size, length of sentences and
also differences in evaluation metrics.
Our following discussion in the first part of this
section highlights the kind of impact that data set
properties have on the standard metrics (label set size
on LAS, non-terminal nodes per sentence on F-score).
Then, in the second part of this section we use the
TedEval cross-experiment protocols for comparative
evaluation that is less sensitive to representation types
and annotation idiosyncrasies.
7.1 Parsing Across Languages and Treebanks
To quantify the impact of treebank characteristics on
parsing parsing accuracy we looked at correlations
of treebank properties with parsing results. The most
highly correlated combinations we have found are
shown in Figures 2, 3, and 4 for the dependency track
and the constituency track (F-score and LeafAnces-
170
21/09/13 03:00SPMRL charts
Page 3 sur 3http://pauillac.inria.fr/~seddah/updated_official.spmrl_results.html
Correlation between label set size, treebank size, and mean LAS
FrP
FrP
GeP
GeP
HuP
HuP
SwP
ArP
ArP
ArG
ArG
BaP
BaP
FrG
FrG
GeG
GeG
HeP
HeG
HuG
HuG
PoP
PoP
PoG
PoG
SwG
BaG
BaG
KoP
KoP
KoG
KoG
10 50 100 500 1 000
72
74
76
78
80
82
84
86
88
90
treebank size / #labels
L
A
S
 
(
%
)
Figure 2: The correlation between treebank size, label set size, and LAS scores. x: treebank size / #labels ; y: LAS (%)
01/10/13 00:43SPMRL charts: all sent.
Page 1 sur 5file:///Users/djame2/=Boulot/=PARSING-FTB/statgram/corpus/SPMRL-S?/SPMRL_FINAL/RESULTS/OFFICIAL/official_ptb-all.spmrl_results.html
SPMRL Results charts (Parseval): Const. Parsing Track (gold tokens, all sent.)
(13/10/01 00:34:34
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish Synthesis
pred/full pred/5k gold/full gold/5k Correlation charts
Correlation between treebank size (#Non terminal), number of sentences (#sent) and mean F1
Arabic
Basque
French
German
Hebrew
Hungarian
Korean
Polish
Swedish
ArP
ArG
BaP
BaG
FrP
FrG
GeP
GeG
HeP
HeG
HuP
HuG
KoP
KoG
PoP
PoG
SwP
SwG
8 9 10
72
74
76
78
80
82
84
86
88
90
92
treebank size (#Non terminal) / #sent
F
1
 
(
%
)
Figure 3: The correlation between the non terminals per sentence ratio and F-scores. x: #non terminal/ #sentence ; y:
F1 (%)
171
tor) respectively.
Figure 2 presents the LAS against the average num-
ber of tokens relative to the number of labels. The
numbers are averaged per language over all partici-
pating systems, and the size of the ?bubbles? is pro-
portional to the number of participants for a given
language setting. We provide ?bubbles? for all lan-
guages in the predicted (-P) and gold (-G) setting,
for both training set sizes. The lower dot in terms
of parsing scores always corresponds to the reduced
training set size.
Figure 2 shows a clear correlation between data-
set complexity and parsing accuracy. The simpler
the data set is (where ?simple" here translates into
large data size with a small set of labels), the higher
the results of the participating systems. The bubbles
reflects a diagonal that indicates correlation between
these dimensions. Beyond that, we see two interest-
ing points off of the diagonal. The Korean treebank
(pink) in the gold setting and full training set can be
parsed with a high LAS relative to its size and label
set. It is also clear that the Hebrew treebank (purple)
in the predicted version is the most difficult one to
parse, relative to our expectation about its complexity.
Since the Hebrew gold scenario is a lot closer to the
diagonal again, it may be that this outlier is due to the
coverage and quality of the predicted morphology.
Figure 340 shows the correlation of data complex-
ity in terms of the average number of non-terminals
per sentence, and parsing accuracy (ParsEval F-
score). Parsing accuracy is again averaged over all
participating systems for a given language. In this
figure, we see a diagonal similar to the one in figure 2,
where Arabic (dark blue) has high complexity of the
data (here interpreted as flat trees, low number of
non terminals per sentence) and low F-scores accord-
ingly. Korean (pink), Swedish (burgundy), Polish
(light green), and Hungarian (light blue) follow, and
then Hebrew (purple) is a positive outlier, possibly
due to an additional layer of ?easy" syntactic POS
nodes which increases tree size and inflates F-scores.
French (orange), Basque (red), and German (dark
green) are negative outliers, falling off the diago-
nal. German has the lowest F-score with respect to
40This figure was created from the IMS:SZEGED:CIS
(Const.) and our own PCFG-LA baseline in POS Tagged mode
(BASE:BKY+POS) so as to avoid the noise introduced by the
parser?s own tagging step (BASE:BKY+RAW).
what would be expected for the non-terminals per
sentence ratio, which is in contrast to the LAS fig-
ure where German occurs among the less complex
data set to parse. A possible explanation may be
the crossing branches in the original treebank which
were re-attached. This creates flat and variable edges
which might be hard predict accurately.
Figure 441 presents the correlation between parsing
accuracy in terms the LeafAncestor metrics (macro
averaged) and treebank complexity in terms of the
average number of non-terminals per sentence. As
in the correlation figures, the parsing accuracy is
averaged over the participanting systems for any lan-
guage. The LeafAncestor accuracy is calculated over
phrase structure trees, and we see a similar diago-
nal to the one in Figure 3 showing that flatter tree-
banks are harder (that is, are correlated with lower
averaged scores) But, its slope is less steep than for
the F-score, which confirms the observation that the
LeafAncestor metric is less sensitive than F-score to
the non-terminals-per-sentence ratio.
Similarly to Figure 3, German is a negative outlier,
which means that this treebank is harder to parse ? it
obtains lower scores on average than we would ex-
pect. As for Hebrew, it is much closer to the diagonal.
As it turns out, the "easy" POS layer that inflates the
scores does not affect the LA ratings as much.
7.2 Evaluation Across Scenarios, Languages
and Treebanks
In this section we analyze the results in cross-
scenario, cross-annotation, and cross-framework set-
tings using the evaluation protocols discussed in
(Tsarfaty et al, 2012b; Tsarfaty et al, 2011; Tsarfaty
et al, 2012a).
As a starting point, we select comparable sections
of the parsed data, based on system runs trained on
the small train set (train5k). For those, we selected
subsets containing the first 5,000 tree terminals (re-
specting sentence boundaries) of the test set. We only
used TedEval on sentences up to 70 terminals long,
and projectivized non-projective sentences in all sets.
We use the TedEval metrics to calculate scores on
both constituency and dependency structures in all
languages and all scenarios. Since the metric de-
fines one scale for all of these different cases, we can
41This figure was created under the same condition as the
F-score correlation in figure (Figure 3).
172
04/10/13 23:05SPMRL charts:
Page 1 sur 6file:///Users/djame2/=Boulot/=PARSING-FTB/statgram/corpus/SPMRL-SHAREDTASK/SPMRL_FINAL/RESULTS/TESTLEAF.spmrl_results.html
SPMRL Results charts (Parseval): Const. Parsing Track (gold tokens, )
(13/10/04 23:05:31
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
Synthesis
pred/full pred/5k gold/full gold/5k Correlation charts
Correlation between treebank size (#Non terminal), number of sentences (#sent) and mean Leaf Accuracy
ArP
ArG
BaP
BaG
FrP
FrG
GeP
HeP
HeG
HuP
HuG
KoP
KoG
PoP
PoG
SwP
SwG
GeG
8 9 10
74
76
78
80
82
84
86
88
90
92
94
treebank size (#Non terminal) / #sent
F
1
 
(
%
)
Figure 4: The correlation between the non terminals per sentence ratio and Leaf Accuracy (macro) scores. x: #non
terminal/ #sentence ; y: Acc.(%)
compare the performance across annotation schemes,
assuming that those subsets are representative of their
original source.42
Ideally, we would be using labeled TedEval scores,
as the labeled parsing task is more difficult, and la-
beled parses are far more informative than unlabeled
ones. However, most constituency-based parsers do
not provide function labels as part of the output, to
be compared with the dependency arcs. Furthermore,
as mentioned earlier, we observed a huge difference
between label set sizes for the dependency runs. Con-
sequently, labeled scores will not be as informative
across treebanks and representation types. We will
therefore only use labels across scenarios for the
same language and representation type.
42We choose this sample scheme for replicability. We first
tried sampling sentences, aiming at the same average sentence
length (20), but that seemed to create artificially difficult test sets
for languages as Polish and overly simplistic ones for French or
Arabic.
7.2.1 Cross-Scenario Evaluation: raw vs. gold
One novel aspect of this shared task is the evalu-
ation on non-gold segmentation in addition to gold
morphology. One drawback is that the scenarios are
currently not using the same metrics ? the metrics
generally applied for gold and predicted scenrios can-
not apply for raw. To assess how well state of the art
parsers perform in raw scenarios compared to gold
scenarios, we present here TedEval results comparing
raw and gold systems using the evaluation protocol
of Tsarfaty et al (2012b).
Table 9 presents the labeled and unlabeled results
for Arabic and Hebrew (in Full and 5k training set-
tings), and Table 10 presents unlabeled TedEval re-
sults (for all languages) in the gold settings. The
unlabeled TedEval results for the raw settings are
substantially lower then TedEval results on the gold
settings for both languages.
When comparing the unlabeled TedEval results for
Arabic and Hebrew on the participating systems, we
see a loss of 3-4 points between Table 9 (raw) and Ta-
ble 10 (gold). In particular we see that for the best per-
173
forming systems on Arabic (IMS:SZEGED:CIS for
both constituency and dependency), the gap between
gold and realistic scenarios is 3.4 and 4.3 points,
for the constituency and the dependency parser re-
spectively. These results are on a par with results
by Tsarfaty et al (2012b), who showed for different
settings, constituency and dependency based, that
raw scenarios are considerably more difficult to parse
than gold ones on the standard split of the Modern
Hebrew treebank.
For Hebrew, the performance gap between unla-
beled TedEval in raw (Table 9) and gold (Table 10)
is even more salient, with around 7 and 8 points of
difference between the scenarios. We can only specu-
late that such a difference may be due to the difficulty
of resolving Hebrew morpho-syntactic ambiguities
without sufficient syntactic information. Since He-
brew and Arabic now have standardized morpholog-
ically and syntactically analyzed data sets available
through this task, it will be possible to investigate
further how cross-linguistic differences in morpho-
logical ambiguity affect full-parsing accuracy in raw
scenarios.
This section compared the raw and gold parsing
results only on unlabeled TedEval metrics. Accord-
ing to what we have seen so far is expected that
for labeled TedEval metrics using the same protocol,
the gap between gold and raw scenario will be even
greater.
7.2.2 Cross-Framework Evaluation:
Dependency vs. Constituency
In this section, our focus is on comparing parsing
results across constituency and dependency parsers
based on the protocol of Tsarfaty et al (2012a) We
have only one submission from IMS:SZEGED:CIS
in the constituency track, and. from the same group,
a submission on the dependency track. We only com-
pare the IMS:SZEGED:CIS results on constituency
and dependency parsing with the two baselines we
provided. The results of the cross-framework evalua-
tion protocol are shown in Table 11.
The results comparing the two variants of the
IMS:SZEGED:CIS systems show that they are very
close for all languages, with differences ranging from
0.03 for German to 0.8 for Polish in the gold setting.
It has often been argued that dependency parsers
perform better than a constituency parser, but we
notice that when using a cross framework protocol,
such as TedEval, and assuming that our test set sam-
ple is representative, the difference between the in-
terpretation of both representation?s performance is
alleviated. Of course, here the metric is unlabeled, so
it simply tells us that both kind of parsing models are
equally able to provide similar tree structures. Said
differently, the gaps in the quality of predicting the
same underlying structure across representations for
MRLs is not as large as is sometimes assumed.
For most languages, the baseline constituency
parser performs better than the dependency base-
line one, with Basque and Korean as an exception,
and at the same time, the dependency version of
IMS:SZEGED:CIS performs slightly better than their
constituent parser for most languages, with the excep-
tion of Hebrew and Hungarian. It goes to show that,
as far as these present MRL results go, there is no
clear preference for a dependency over a constituency
parsing representation, just preferences among par-
ticular models.
More generally, we can say that even if the linguis-
tic coverage of one theory is shown to be better than
another one, it does not necessarily mean that the
statistical version of the formal theory will perform
better for structure prediction. System performance
is more tightly related to the efficacy of the learning
and search algorithms, and feature engineering on
top of the selected formalism.
7.2.3 Cross-Language Evaluation: All
Languages
We conclude with an overall outlook of the Ted-
Eval scores across all languages. The results on the
gold scenario, for the small training set and the 5k
test set are presented in Table 10. We concentrate
on gold scenarios (to avoid the variation in cover-
age of external morphological analyzers) and choose
unlabeled metrics as they are not sensitive to label
set sizes. We emphasize in bold, for each parsing
system (row in the table), the top two languages that
most accurately parsed by it (boldface) and the two
languages it performed the worse on (italics).
We see that the European languages German
and Hungarian are parsed most accurately in the
constituency-based setup, with Polish and Swedish
having an advantage in dependency parsing. Across
all systems, Korean is the hardest to parse, with Ara-
174
Arabic Hebrew AVG1 SOFT AVG Arabic Hebrew AVG2 SOFT AVG2
1) Constituency Evaluation
Labeled TedEval Unabeled TedEval
IMS:SZEGED:CIS (Bky) 83.59 56.43 70.01 70.01 92.18 88.02 90.1 90.1
2) Dependency Evaluation
Labeled TedEval Unabeled TedEval
IMS:SZEGED:CIS 88.61 84.74 86.68 86.68 91.41 88.58 90 90
ALPAGE:DYALOG 87.20 81.65 40.83 81.65 90.74 87.44 89.09 89.09
CADIM 87.99 - 44 87.99 91.22 - 45.61 91.22
MALTOPTIMIZER 86.62 81.74 43.31 86.62 90.26 87.00 45.13 90.26
ALPAGE:DYALOG (RAW) - 82.82 41.41 82.82 - 87.43 43.72 87.43
AI:KU - 77.8 38.9 77.8 - 85.87 42.94 85.87
Table 9: Labeled and Unlabeled TedEval Results for raw Scenarios, Trained on 5k sentences and tested on 5k terminals.
The upper part refers to constituency parsing and the lower part refers to dependency parsing.
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
1) Constituency Evaluation
IMS:SZEGED:CIS (Bky) 95.35 96.91 95.98 97.12 96.22 97.92 92.91 97.19 96.65
BASE:BKY+POS 95.11 94.69 95.08 97.01 95.85 97.08 90.55 96.99 96.38
BASE:BKY+RAW 94.58 94.32 94.72 96.74 95.64 96.15 87.08 95.93 95.90
2) Dependency Evaluation
IMS:SZEGED:CIS 95.76 97.63 96.59 96.88 96.29 97.56 94.62 98.01 97.22
ALPAGE:DYALOG 93.76 95.72 95.75 96.4 95.34 95.63 94.56 96.80 96.55
BASE:MALT 94.16 95.08 94.21 94.55 94.98 95.25 94.27 95.83 95.33
AI:KU - - 95.46 96.34 95.07 96.53 - 96.88 95.87
MALTOPTIMIZER 94.91 96.82 95.23 96.32 95.46 96.30 94.69 96.06 95.90
CADIM 94.66 - - - - - - - -
Table 10: Cross-Language Evaluation: Unlabeled TedEval Results in gold input scenario, On a 5k-sentences set set and
a 5k-terminals test set. The upper part refers to constituency parsing and the lower part refers to dependency parsing.
For each system we mark the two top scoring languages in bold and the two lowest scoring languages in italics.
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
1) gold setting
IMS:SZEGED:CIS (Bky) 95.82 97.30 96.15 97.43 96.37 98.25 94.07 97.22 96.89
IMS:SZEGED:CIS 95.87 98.06 96.61 97.46 96.31 97.93 94.62 98.04 97.24
BASE:BKY+POS 95.61 95.25 95.48 97.31 96.03 97.53 92.15 96.97 96.66
BASE:MALT 94.26 95.76 94.23 95.53 95.00 96.09 94.27 95.90 95.35
2) predicted setting
IMS:SZEGED:CIS (Bky) 95.74 97.07 96.21 97.31 96.10 98.03 94.05 96.92 96.90
IMS:SZEGED:CIS 95.18 97.67 96.15 97.09 96.22 97.63 94.43 97.50 97.02
BASE:BKY+POS 95.03 95.35 97.12 95.36 97.20 91.34 96.92 96.25
BASE:MALT 95.49 93.84 95.39 94.41 95.72 93.74 96.04 95.09
Table 11: Cross Framework Evaluation: Unlabeled TedEval on generalized gold trees in gold scenario, trained on 5k
sentences and tested on 5k terminals.
bic, Hebrew and to some extent French following. It
appears that on a typological scale, Semitic and Asian
languages are still harder to parse than a range of Eu-
ropean languages in terms of structural difficulty and
complex morpho-syntactic interaction. That said,
note that we cannot tell why certain treebanks appear
more challenging to parse then others, and it is still
unclear whether the difficulty is inherent on the lan-
guage, in the currently available models, or because
of the annotation scheme and treebank consistency.43
43The latter was shown to be an important factor orthogonal
to the morphologically-rich nature of the treebank?s language
175
8 Conclusion
This paper presents an overview of the first shared
task on parsing morphologically rich languages. The
task features nine languages, exhibiting different lin-
guistic phenomena and varied morphological com-
plexity. The shared task saw submissions from seven
teams, and results produced by more than 14 different
systems. The parsing results were obtained in dif-
ferent input scenarios (gold, predicted, and raw) and
evaluated using different protocols (cross-framework,
cross-scenario, and cross-language). In particular,
this is the first time an evaluation campaign reports
on the execution of parsers in realistic, morphologi-
cally ambiguous, setting.
The best performing systems were mostly ensem-
ble systems combining multiple parser outputs from
different frameworks or training runs, or integrat-
ing a state-of-the-art morphological analyzer on top
of a carefully designed feature set. This is con-
sistent with previous shared tasks such as ConLL
2007 or SANCL?2012. However, dealing with am-
biguous morphology is still difficult for all systems,
and a promising approach, as demonstrated by AL-
PAGE:DYALOG, is to deal with parsing and morphol-
ogy jointly by allowing lattice input to the parser. A
promising generalization of this approach would be
the full integration of all levels of analysis that are
mutually informative into a joint model.
The information to be gathered from the results of
this shared task is vast, and we only scratched the
surface with our preliminary analyses. We uncov-
ered and documented insights of strategies that make
parsing systems successful: parser combination is
empirically proven to reach a robust performance
across languages, though language-specific strategies
are still a sound avenue for obtaining high quality
parsers for that individual language. The integration
of morphological analysis into the parsing needs to
be investigated thoroughly, and new approaches that
are morphologically aware need to be developed.
Our cross-parser, cross-scenario, and cross-
framework evaluation protocols have shown that, as
expected, more data is better, and that performance
on gold morphological input is significantly higher
than that in more realistic scenarios. We have shown
that gold morphological information is more help-
(Schluter and van Genabith, 2007)
ful to some languages and parsers than others, and
that it may also interact with successful identification
of multiword expressions. We have shown that dif-
ferences between dependency and constituency are
smaller than previously assumed and that properties
of the learning model and granularity of the output
labels are more influential. Finally, we observed
that languages which are typologically farthest from
English, such as Semitic and Asian languages, are
still amongst the hardest to parse, regardless of the
parsing method used.
Our cross-treebank, in-depth analysis is still pre-
liminary, owing to the limited time between the end
of the shared task and the deadline for publication
of this overview. but we nonetheless feel that our
findings may benefit researchers who aim to develop
parsers for diverse treebanks.44
A shared task is an inspection of the state of the
art, but it may also accelerate research in an area
by providing a stable data basis as well as a set of
strong baselines. The results produced in this task
give a rich picture of the issues associated with pars-
ing MRLs and initial cues towards their resolution.
This set of results needs to be further analyzed to be
fully understood, which will in turn contribute to new
insights. We hope that this shared task will provide
inspiration for the design and evaluation of future
parsing systems for these languages.
Acknowledgments
We heartily thank Miguel Ballesteros and Corentin
Ribeire for running the dependency and constituency
baselines. We warmly thank the Linguistic Data Con-
sortium: Ilya Ahtaridis, Ann Bies, Denise DiPersio,
Seth Kulick and Mohamed Maamouri for releasing
the Arabic Penn Treebank for this shared task and
for their support all along the process. We thank
Alon Itai and MILA, the knowledge center for pro-
cessing Hebrew, for kindly making the Hebrew tree-
bank and morphological analyzer available for us,
Anne Abeill? for allowing us to use the French tree-
bank, and Key-Sun Choi for the Kaist Korean Tree-
bank. We thank Grzegorz Chrupa?a for providing
the morphological analyzer Morfette, and Joachim
44The data set will be made available as soon as possible under
the license distribution of the shared-task, with the exception
of the Arabic data, which will continue to be distributed by the
LDC.
176
Wagner for his LeafAncestor implementation. We
finally thank ?zlem ?etinog?lu, Yuval Marton, Benoit
Crabb? and Benoit Sagot who have been nothing but
supportive during all that time.
At the end of this shared task (though watch out
for further updates and analyses), what remains to be
mentioned is our deep gratitude to all people involved,
either data providers or participants. Without all of
you, this shared task would not have been possible.
References
Anne Abeill?, Lionel Cl?ment, and Fran?ois Toussenel.
2003. Building a treebank for French. In Anne Abeill?,
editor, Treebanks. Kluwer, Dordrecht.
Szymon Acedan?ski. 2010. A Morphosyntactic Brill Tag-
ger for Inflectional Languages. In Advances in Natural
Language Processing, volume 6233 of Lecture Notes
in Computer Science, pages 3?14. Springer-Verlag.
Meni Adler and Michael Elhadad. 2006. An unsupervised
morpheme-based HMM for Hebrew morphological dis-
ambiguation. In Proceedings COLING-ACL, pages
665?672, Sydney, Australia.
Meni Adler, Yoav Goldberg, David Gabay, and Michael
Elhadad. 2008. Unsupervised lexicon-based resolution
of unknown words for full morphological analysis. In
Proceedings of ACL-08: HLT, pages 728?736, Colum-
bus, OH.
Meni Adler. 2007. Hebrew Morphological Disambigua-
tion: An Unsupervised Stochastic Word-based Ap-
proach. Ph.D. thesis, Ben-Gurion University of the
Negev.
Itziar Aduriz, Jos? Mar?a Arriola, Xabier Artola, A D?az
de Ilarraza, et al 1997. Morphosyntactic disambigua-
tion for Basque based on the constraint grammar for-
malism. In Proceedings of RANLP, Tzigov Chark, Bul-
garia.
Itziar Aduriz, Eneko Agirre, Izaskun Aldezabal, I?aki
Alegria, Xabier Arregi, Jose Maria Arriola, Xabier Ar-
tola, Koldo Gojenola, Aitor Maritxalar, Kepa Sarasola,
et al 2000. A word-grammar based morphological
analyzer for agglutinative languages. In Proceedings
of COLING, pages 1?7, Saarbr?cken, Germany.
Itziar Aduriz, Maria Jesus Aranzabe, Jose Maria Arriola,
Aitziber Atutxa, A Diaz de Ilarraza, Aitzpea Garmen-
dia, and Maite Oronoz. 2003. Construction of a
Basque dependency treebank. In Proceedings of the
2nd Workshop on Treebanks and Linguistic Theories
(TLT), pages 201?204, V?xj?, Sweden.
Zeljko Agic, Danijela Merkler, and Dasa Berovic. 2013.
Parsing Croatian and Serbian by using Croatian depen-
dency treebanks. In Proceedings of the Fourth Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL), Seattle, WA.
I. Aldezabal, M.J. Aranzabe, A. Diaz de Ilarraza, and
K. Fern?ndez. 2008. From dependencies to con-
stituents in the reference corpus for the processing of
Basque. In Procesamiento del Lenguaje Natural, no
41 (2008), pages 147?154. XXIV edici?n del Congreso
Anual de la Sociedad Espa?ola para el Procesamiento
del Lenguaje Natural (SEPLN).
Bharat Ram Ambati, Samar Husain, Joakim Nivre, and
Rajeev Sangal. 2010. On the role of morphosyntactic
features in Hindi dependency parsing. In Proceedings
of the NAACL/HLT Workshop on Statistical Parsing of
Morphologically Rich Languages (SPMRL 2010), Los
Angeles, CA.
Mohammed Attia, Jennifer Foster, Deirdre Hogan,
Joseph Le Roux, Lamia Tounsi, and Josef van Gen-
abith. 2010. Handling unknown words in statistical
latent-variable parsing models for Arabic, English and
French. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL), Los Angeles, CA.
Miguel Ballesteros and Joakim Nivre. 2012. MaltOpti-
mizer: An optimization tool for MaltParser. In Pro-
ceedings of EACL, pages 58?62, Avignon, France.
Miguel Ballesteros. 2013. Effective morphological fea-
ture selection with MaltOptimizer at the SPMRL 2013
shared task. In Proceedings of the Fourth Workshop on
Statistical Parsing of Morphologically-Rich Languages,
pages 53?60, Seattle, WA.
Kepa Bengoetxea and Koldo Gojenola. 2010. Appli-
cation of different techniques to dependency parsing
of Basque. In Proceedings of the NAACL/HLT Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL 2010), Los Angeles, CA.
Philip Bille. 2005. A survey on tree edit distance and re-
lated problems. Theoretical Computer Science, 337(1?
3):217?239, 6.
Anders Bj?rkelund, Ozlem Cetinoglu, Rich?rd Farkas,
Thomas Mueller, and Wolfgang Seeker. 2013.
(Re)ranking meets morphosyntax: State-of-the-art re-
sults from the SPMRL 2013 shared task. In Proceed-
ings of the Fourth Workshop on Statistical Parsing
of Morphologically-Rich Languages, pages 134?144,
Seattle, WA.
Ezra Black, Steven Abney, Dan Flickinger, Claudia
Gdaniec, Ralph Grishman, Philip Harrison, Donald
Hindle, Robert Ingria, Frederick Jelinek, Judith Kla-
vans, Mark Liberman, Mitchell Marcus, Salim Roukos,
Beatrice Santorini, and Tomek Strzalkowski. 1991. A
procedure for quantitatively comparing the syntactic
coverage of English grammars. In Proceedings of the
DARPA Speech and Natural Language Workshop 1991,
pages 306?311, Pacific Grove, CA.
177
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Proceed-
ings of the EMNLP-CoNLL, pages 1455?1465, Jeju,
Korea.
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of COL-
ING, pages 89?97, Beijing, China.
Adriane Boyd. 2007. Discontinuity revisited: An im-
proved conversion to context-free representations. In
Proceedings of the Linguistic Annotation Workshop,
Prague, Czech Republic.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER treebank.
In Proceedings of the First Workshop on Treebanks
and Linguistic Theories (TLT), pages 24?41, Sozopol,
Bulgaria.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL, pages 149?164, New York, NY.
Tim Buckwalter. 2002. Arabic morphological analyzer
version 1.0. Linguistic Data Consortium.
Tim Buckwalter. 2004. Arabic morphological analyzer
version 2.0. Linguistic Data Consortium.
Marie Candito and Djam? Seddah. 2010. Parsing word
clusters. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2010), Los Angeles, CA.
Marie Candito, Benoit Crabb?, and Pascal Denis. 2010.
Statistical French dependency parsing: Treebank con-
version and first results. In Proceedings of LREC, Val-
letta, Malta.
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
TAG, dynamic programming, and the perceptron for ef-
ficient, feature-rich parsing. In Proceedings of CoNLL,
pages 9?16, Manchester, UK.
Eugene Charniak and Mark Johnson. 2005. Course-to-
fine n-best-parsing and maxent discriminative rerank-
ing. In Proceedings of ACL, pages 173?180, Barcelona,
Spain.
Eugene Charniak. 1997. Statistical parsing with a context-
free grammar and word statistics. In AAAI/IAAI, pages
598?603.
Eugene Charniak. 2000. A maximum entropy inspired
parser. In Proceedings of NAACL, pages 132?139, Seat-
tle, WA.
Jinho D. Choi and Martha Palmer. 2011. Statistical de-
pendency parsing in Korean: From corpus generation
to automatic parsing. In Proceedings of Second Work-
shop on Statistical Parsing of Morphologically Rich
Languages, pages 1?11, Dublin, Ireland.
Jinho D. Choi and Martha Palmer. 2012. Guidelines
for the Clear Style Constituent to Dependency Conver-
sion. Technical Report 01-12, University of Colorado
at Boulder.
Key-sun Choi, Young S. Han, Young G. Han, and Oh W.
Kwon. 1994. KAIST Tree Bank Project for Korean:
Present and Future Development. In In Proceedings
of the International Workshop on Sharable Natural
Language Resources, pages 7?14, Nara, Japan.
Jinho D. Choi. 2013. Preparing Korean data for the
shared task on parsing morphologically rich languages.
arXiv:1309.1649.
Grzegorz Chrupa?a, Georgiana Dinu, and Josef van Gen-
abith. 2008. Learning morphology with Morfette. In
Proceedings of LREC, Marrakech, Morocco.
Tagyoung Chung, Matt Post, and Daniel Gildea. 2010.
Factors affecting the accuracy of Korean parsing. In
Proceedings of the NAACL/HLT Workshop on Sta-
tistical Parsing of Morphologically Rich Languages
(SPMRL 2010), Los Angeles, CA.
Volkan Cirik and H?sn? S?ensoy. 2013. The AI-KU
system at the SPMRL 2013 shared task: Unsuper-
vised features for dependency parsing. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 68?75, Seat-
tle, WA.
Michael Collins. 2003. Head-driven statistical models for
natural language parsing. Computational Linguistics,
29(4):589?637.
Matthieu Constant, Marie Candito, and Djam? Seddah.
2013. The LIGM-Alpage architecture for the SPMRL
2013 shared task: Multiword expression analysis and
dependency parsing. In Proceedings of the Fourth
Workshop on Statistical Parsing of Morphologically-
Rich Languages, pages 46?52, Seattle, WA.
Anna Corazza, Alberto Lavelli, Giogio Satta, and Roberto
Zanoli. 2004. Analyzing an Italian treebank with
state-of-the-art statistical parsers. In Proceedings of
the Third Workshop on Treebanks and Linguistic Theo-
ries (TLT), T?bingen, Germany.
Benoit Crabb? and Marie Candito. 2008. Exp?riences
d?analyse syntaxique statistique du fran?ais. In Actes
de la 15?me Conf?rence sur le Traitement Automatique
des Langues Naturelles (TALN?08), pages 45?54, Avi-
gnon, France.
D?ra Csendes, J?nos Csirik, Tibor Gyim?thy, and Andr?s
Kocsor. 2005. The Szeged treebank. In Proceedings of
the 8th International Conference on Text, Speech and
Dialogue (TSD), Lecture Notes in Computer Science,
pages 123?132, Berlin / Heidelberg. Springer.
Eric De La Clergerie. 2013. Exploring beam-based
shift-reduce dependency parsing with DyALog: Re-
sults from the SPMRL 2013 shared task. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
178
Morphologically-Rich Languages, pages 81?89, Seat-
tle, WA.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies repre-
sentation. In Proceedings of the workshop on Cross-
Framework and Cross-Domain Parser Evaluation.
Mona Diab, Nizar Habash, Owen Rambow, and Ryan
Roth. 2013. LDC Arabic treebanks and associated cor-
pora: Data divisions manual. Technical Report CCLS-
13-02, Center for Computational Learning Systems,
Columbia University.
Eva Ejerhed and Gunnel K?llgren. 1997. Stockholm
Ume? Corpus. Version 1.0. Department of Linguis-
tics, Ume? University and Department of Linguistics,
Stockholm University.
Eva Ejerhed, Gunnel K?llgren, Ola Wennstedt, and Mag-
nus ?str?m. 1992. The linguistic annotation system
of the Stockholm?Ume? Corpus project. Technical
Report 33, University of Ume?: Department of Linguis-
tics.
Nerea Ezeiza, I?aki Alegria, Jos? Mar?a Arriola, Rub?n
Urizar, and Itziar Aduriz. 1998. Combining stochastic
and rule-based methods for disambiguation in aggluti-
native languages. In Proceedings of COLING, pages
380?384, Montr?al, Canada.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL, pages
959?967, Columbus, OH.
Alexander Fraser, Helmut Schmid, Rich?rd Farkas, Ren-
jing Wang, and Hinrich Sch?tze. 2013. Knowledge
sources for constituent parsing of German, a morpho-
logically rich and less-configurational language. Com-
putational Linguistics, 39(1):57?85.
Iakes Goenaga, Koldo Gojenola, and Nerea Ezeiza. 2013.
Exploiting the contribution of morphological informa-
tion to parsing: the BASQUE TEAM system in the
SPRML?2013 shared task. In Proceedings of the Fourth
Workshop on Statistical Parsing of Morphologically-
Rich Languages, pages 61?67, Seattle, WA.
Yoav Goldberg and Michael Elhadad. 2010a. Easy-first
dependency parsing of Modern Hebrew. In Proceed-
ings of the NAACL/HLT Workshop on Statistical Pars-
ing of Morphologically Rich Languages (SPMRL 2010),
Los Angeles, CA.
Yoav Goldberg and Michael Elhadad. 2010b. An ef-
ficient algorithm for easy-first non-directional depen-
dency parsing. In Proceedings of HLT: NAACL, pages
742?750, Los Angeles, CA.
Yoav Goldberg and Reut Tsarfaty. 2008. A single frame-
work for joint morphological segmentation and syntac-
tic parsing. In Proceedings of ACL, Columbus, OH.
Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008.
EM can find pretty good HMM POS-taggers (when
given a good start). In Proc. of ACL, Columbus, OH.
Yoav Goldberg, Reut Tsarfaty, Meni Adler, and Michael
Elhadad. 2009. Enhancing unlexicalized parsing per-
formance using a wide coverage lexicon, fuzzy tag-set
mapping, and EM-HMM-based lexical probabilities. In
Proceedings of EALC, pages 327?335, Athens, Greece.
Yoav Goldberg. 2011. Automatic syntactic processing of
Modern Hebrew. Ph.D. thesis, Ben Gurion University
of the Negev.
David Graff, Mohamed Maamouri, Basma Bouziri, Son-
dos Krouna, Seth Kulick, and Tim Buckwalter. 2009.
Standard Arabic Morphological Analyzer (SAMA) ver-
sion 3.1. Linguistic Data Consortium LDC2009E73.
Spence Green and Christopher D. Manning. 2010. Better
Arabic parsing: Baselines, evaluations, and analysis.
In Proceedings of COLING, pages 394?402, Beijing,
China.
Nathan Green, Loganathan Ramasamy, and Zden?k
?abokrtsk?. 2012. Using an SVM ensemble system for
improved Tamil dependency parsing. In Proceedings
of the ACL 2012 Joint Workshop on Statistical Pars-
ing and Semantic Processing of Morphologically Rich
Languages, pages 72?77, Jeju, Korea.
Spence Green, Marie-Catherine de Marneffe, and Christo-
pher D. Manning. 2013. Parsing models for identify-
ing multiword expressions. Computational Linguistics,
39(1):195?227.
Noemie Guthmann, Yuval Krymolowski, Adi Milea, and
Yoad Winter. 2009. Automatic annotation of morpho-
syntactic dependencies in a Modern Hebrew Treebank.
In Proceedings of the Eighth International Workshop on
Treebanks and Linguistic Theories (TLT), Groningen,
The Netherlands.
Nizar Habash and Ryan Roth. 2009. CATiB: The
Columbia Arabic Treebank. In Proceedings of ACL-
IJCNLP, pages 221?224, Suntec, Singapore.
Nizar Habash, Ryan Gabbard, Owen Rambow, Seth
Kulick, and Mitch Marcus. 2007. Determining case in
Arabic: Learning complex linguistic behavior requires
complex linguistic features. In Proceedings of EMNLP-
CoNLL, pages 1084?1092, Prague, Czech Republic.
Nizar Habash, Reem Faraj, and Ryan Roth. 2009a. Syn-
tactic Annotation in the Columbia Arabic Treebank. In
Proceedings of MEDAR International Conference on
Arabic Language Resources and Tools, Cairo, Egypt.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009b.
MADA+TOKAN: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, POS tag-
ging, stemming and lemmatization. In Proceedings of
the Second International Conference on Arabic Lan-
guage Resources and Tools. Cairo, Egypt.
179
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publishers.
Jan Hajic?, Alena B?hmov?, Eva Hajic?ov?, and Barbora
Vidov?-Hladk?. 2000. The Prague Dependency Tree-
bank: A three-level annotation scenario. In Anne
Abeill?, editor, Treebanks: Building and Using Parsed
Corpora. Kluwer Academic Publishers.
P?ter Hal?csy, Andr?s Kornai, and Csaba Oravecz. 2007.
HunPos ? an open source trigram tagger. In Proceed-
ings of ACL, pages 209?212, Prague, Czech Republic.
Johan Hall, Jens Nilsson, Joakim Nivre, G?ls?en Eryig?it,
Be?ta Megyesi, Mattias Nilsson, and Markus Saers.
2007. Single malt or blended? A study in multilingual
parser optimization. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
933?939, Prague, Czech Republic.
Chung-hye Han, Na-Rae Han, Eon-Suk Ko, Martha
Palmer, and Heejong Yi. 2002. Penn Korean Treebank:
Development and evaluation. In Proceedings of the
16th Pacific Asia Conference on Language, Information
and Computation, Jeju, Korea.
Tilman H?hle. 1986. Der Begriff "Mittelfeld", Anmerkun-
gen ?ber die Theorie der topologischen Felder. In Ak-
ten des Siebten Internationalen Germanistenkongresses
1985, pages 329?340, G?ttingen, Germany.
Zhongqiang Huang, Mary Harper, and Slav Petrov. 2010.
Self-training with products of latent variable grammars.
In Proceedings of EMNLP, pages 12?22, Cambridge,
MA.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of ACL,
pages 586?594, Columbus, OH.
Alon Itai and Shuly Wintner. 2008. Language resources
for Hebrew. Language Resources and Evaluation,
42(1):75?98, March.
Mark Johnson. 1998. PCFG models of linguistic tree
representations. Computational Linguistics, 24(4):613?
632.
Laura Kallmeyer and Wolfgang Maier. 2013. Data-driven
parsing using probabilistic linear context-free rewriting
systems. Computational Linguistics, 39(1).
Fred Karlsson, Atro Voutilainen, Juha Heikkilae, and Arto
Anttila. 1995. Constraint Grammar: a language-
independent system for parsing unrestricted text. Wal-
ter de Gruyter.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL, pages
423?430, Sapporo, Japan.
Sandra K?bler, Erhard W. Hinrichs, and Wolfgang Maier.
2006. Is it really that difficult to parse German? In Pro-
ceedings of EMNLP, pages 111?119, Sydney, Australia,
July.
Sandra K?bler, Wolfgang Maier, Ines Rehbein, and Yan-
nick Versley. 2008. How to compare treebanks. In
Proceedings of LREC, pages 2322?2329, Marrakech,
Morocco.
Sandra K?bler. 2008. The PaGe 2008 shared task on
parsing German. In Proceedings of the Workshop on
Parsing German, pages 55?63, Columbus, OH.
Seth Kulick, Ryan Gabbard, and Mitch Marcus. 2006.
Parsing the Arabic Treebank: Analysis and Improve-
ments. In Proceedings of the Treebanks and Linguistic
Theories Conference, pages 31?42, Prague, Czech Re-
public.
Joseph Le Roux, Benoit Sagot, and Djam? Seddah. 2012.
Statistical parsing of Spanish and data driven lemmati-
zation. In Proceedings of the Joint Workshop on Statis-
tical Parsing and Semantic Processing of Morphologi-
cally Rich Languages, pages 55?61, Jeju, Korea.
Kong Joo Lee, Byung-Gyu Chang, and Gil Chang Kim.
1997. Bracketing Guidelines for Korean Syntactic Tree
Tagged Corpus. Technical Report CS/TR-97-112, De-
partment of Computer Science, KAIST.
Roger Levy and Christopher D. Manning. 2003. Is it
harder to parse Chinese, or the Chinese treebank? In
Proceedings of ACL, Sapporo, Japan.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Hubert Jin. 2004a. Arabic Treebank: Part 2 v 2.0.
LDC catalog number LDC2004T02.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004b. The Penn Arabic Treebank:
Building a large-scale annotated Arabic corpus. In
NEMLAR Conference on Arabic Language Resources
and Tools, pages 102?109, Cairo, Egypt.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Hubert Jin. 2005. Arabic Treebank: Part 1 v 3.0. LDC
catalog number LDC2005T02.
Mohamed Maamouri, Ann Bies, Seth Kulick, Fatma Gad-
deche, Wigdan Mekki, Sondos Krouna, and Basma
Bouziri. 2009. The Penn Arabic Treebank part 3 ver-
sion 3.1. Linguistic Data Consortium LDC2008E22.
Wolfgang Maier, Miriam Kaeshammer, and Laura
Kallmeyer. 2012. Data-driven PLCFRS parsing re-
visited: Restricting the fan-out to two. In Proceedings
of the Eleventh International Conference on Tree Ad-
joining Grammars and Related Formalisms (TAG+11),
Paris, France.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn TreeBank. Computational
Linguistics, 19(2):313?330.
Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar,
and Mario Figueiredo. 2010. Turbo parsers: Depen-
dency parsing by approximate variational inference. In
Proceedings of EMNLP, pages 34?44, Cambridge, MA.
180
Yuval Marton, Nizar Habash, and Owen Rambow. 2013a.
Dependency parsing of Modern Standard Arabic with
lexical and inflectional features. Computational Lin-
guistics, 39(1):161?194.
Yuval Marton, Nizar Habash, Owen Rambow, and Sarah
Alkhulani. 2013b. SPMRL?13 shared task system:
The CADIM Arabic dependency parser. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 76?80, Seat-
tle, WA.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of HLT:NAACL, pages 152?159, New York, NY.
Ryan T. McDonald, Koby Crammer, and Fernando C. N.
Pereira. 2005. Online large-margin training of depen-
dency parsers. In Proceedings of ACL, pages 91?98,
Ann Arbor, MI.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuzman
Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Oscar
Tackstrom, Claudia Bedini, Nuria Bertomeu Castello,
and Jungmee Lee. 2013. Universal dependency anno-
tation for multilingual parsing. In Proceedings of ACL,
Sofia, Bulgaria.
Igor Mel?c?uk. 2001. Communicative Organization in Nat-
ural Language: The Semantic-Communicative Struc-
ture of Sentences. J. Benjamins.
Knowledge Center for Processing Hebrew
MILA. 2008. Hebrew morphological analyzer.
http://mila.cs.technion.ac.il.
Antonio Moreno, Ralph Grishman, Susana Lopez, Fer-
nando Sanchez, and Satoshi Sekine. 2000. A treebank
of Spanish and its application to parsing. In Proceed-
ings of LREC, Athens, Greece.
Joakim Nivre and Be?ta Megyesi. 2007. Bootstrapping a
Swedish treeebank using cross-corpus harmonization
and annotation projection. In Proceedings of the 6th
International Workshop on Treebanks and Linguistic
Theories, pages 97?102, Bergen, Norway.
Joakim Nivre, Jens Nilsson, and Johan Hall. 2006. Tal-
banken05: A Swedish treebank with phrase structure
and dependency annotation. In Proceedings of LREC,
pages 1392?1395, Genoa, Italy.
Joakim Nivre, Johan Hall, Sandra K?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007a. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of the CoNLL Shared Task Ses-
sion of EMNLP-CoNLL 2007, pages 915?932, Prague,
Czech Republic.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
G?ls?en Eryig?it, Sandra K?bler, Svetoslav Marinov,
and Erwin Marsi. 2007b. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Slav Petrov and Ryan McDonald. 2012. Overview of the
2012 Shared Task on Parsing the Web. In Proceedings
of the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL), a NAACL-HLT 2012
workshop, Montreal, Canada.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of COLING-
ACL, Sydney, Australia.
Slav Petrov. 2009. Coarse-to-Fine Natural Language
Processing. Ph.D. thesis, University of California at
Bekeley, Berkeley, CA.
Slav Petrov. 2010. Products of random latent variable
grammars. In Proceedings of HLT: NAACL, pages 19?
27, Los Angeles, CA.
Adam Przepi?rkowski, Miros?aw Ban?ko, Rafa? L. G?rski,
and Barbara Lewandowska-Tomaszczyk, editors. 2012.
Narodowy Korpus Jkezyka Polskiego. Wydawnictwo
Naukowe PWN, Warsaw.
Ines Rehbein and Josef van Genabith. 2007a. Eval-
uating Evaluation Measures. In Proceedings of the
16th Nordic Conference of Computational Linguistics
NODALIDA-2007, Tartu, Estonia.
Ines Rehbein and Josef van Genabith. 2007b. Treebank
annotation schemes and parser evaluation for German.
In Proceedings of EMNLP-CoNLL, Prague, Czech Re-
public.
Ines Rehbein. 2011. Data point selection for self-training.
In Proceedings of the Second Workshop on Statistical
Parsing of Morphologically Rich Languages, pages 62?
67, Dublin, Ireland.
Kenji Sagae and Alon Lavie. 2006. Parser combination
by reparsing. In Proceedings of HLT-NAACL, pages
129?132, New York, NY.
Geoffrey Sampson and Anna Babarczy. 2003. A test of
the leaf-ancestor metric for parse accuracy. Natural
Language Engineering, 9(04):365?380.
Natalie Schluter and Josef van Genabith. 2007. Prepar-
ing, restructuring, and augmenting a French Treebank:
Lexicalised parsers or coherent treebanks? In Proc. of
PACLING 07, Melbourne, Australia.
Helmut Schmid, Arne Fitschen, and Ulrich Heid. 2004.
SMOR: A German computational morphology covering
derivation, composition and inflection. In Proceedings
of LREC, Lisbon, Portugal.
Djam? Seddah, Grzegorz Chrupa?a, Ozlem Cetinoglu,
Josef van Genabith, and Marie Candito. 2010.
Lemmatization and statistical lexicalized parsing of
morphologically-rich languages. In Proceedings of the
First Workshop on Statistical Parsing of Morphologi-
cally Rich Languages (SPMRL), Los Angeles, CA.
Wolfgang Seeker and Jonas Kuhn. 2012. Making el-
lipses explicit in dependency conversion for a German
181
treebank. In Proceedings of LREC, pages 3132?3139,
Istanbul, Turkey.
Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, and
Masaaki Nagata. 2012. Bayesian symbol-refined tree
substitution grammars for syntactic parsing. In Pro-
ceedings of ACL, pages 440?448, Jeju, Korea.
Anthony Sigogne, Matthieu Constant, and Eric Laporte.
2011. French parsing enhanced with a word clustering
method based on a syntactic lexicon. In Proceedings
of the Second Workshop on Statistical Parsing of Mor-
phologically Rich Languages, pages 22?27, Dublin,
Ireland.
Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altmann,
and Noa Nativ. 2001. Building a tree-bank of Modern
Hebrew text. Traitement Automatique des Langues,
42:347?380.
Marek S?widzin?ski and Marcin Wolin?ski. 2010. Towards
a bank of constituent parse trees for Polish. In Pro-
ceedings of Text, Speech and Dialogue, pages 197?204,
Brno, Czech Republic.
Ulf Teleman. 1974. Manual f?r grammatisk beskrivning
av talad och skriven svenska. Studentlitteratur.
Lucien Tesni?re. 1959. ?l?ments De Syntaxe Structurale.
Klincksieck, Paris.
Reut Tsarfaty and Khalil Sima?an. 2010. Modeling mor-
phosyntactic agreement in constituency-based parsing
of Modern Hebrew. In Proceedings of the First Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL), Los Angeles, CA.
Reut Tsarfaty, Djame Seddah, Yoav Goldberg, Sandra
K?bler, Marie Candito, Jennifer Foster, Yannick Vers-
ley, Ines Rehbein, and Lamia Tounsi. 2010. Statistical
parsing for morphologically rich language (SPMRL):
What, how and whither. In Proceedings of the First
workshop on Statistical Parsing of Morphologically
Rich Languages (SPMRL), Los Angeles, CA.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2011. Evaluating dependency parsing: Robust and
heuristics-free cross-framework evaluation. In Pro-
ceedings of EMNLP, Edinburgh, UK.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012a. Cross-framework evaluation for statistical pars-
ing. In Proceeding of EACL, Avignon, France.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012b. Joint evaluation for segmentation and parsing.
In Proceedings of ACL, Jeju, Korea.
Reut Tsarfaty, Djam? Seddah, Sandra K?bler, and Joakim
Nivre. 2012c. Parsing morphologically rich languages:
Introduction to the special issue. Computational Lin-
guistics, 39(1):15?22.
Reut Tsarfaty. 2010. Relational-Realizational Parsing.
Ph.D. thesis, University of Amsterdam.
Reut Tsarfaty. 2013. A unified morpho-syntactic scheme
of Stanford dependencies. In Proceedings of ACL,
Sofia, Bulgaria.
Veronika Vincze, D?ra Szauter, Attila Alm?si, Gy?rgy
M?ra, Zolt?n Alexin, and J?nos Csirik. 2010. Hungar-
ian Dependency Treebank. In Proceedings of LREC,
Valletta, Malta.
Joachim Wagner. 2012. Detecting Grammatical Errors
with Treebank-Induced Probabilistic Parsers. Ph.D.
thesis, Dublin City University.
Marcin Wolin?ski, Katarzyna G?owin?ska, and Marek
S?widzin?ski. 2011. A preliminary version of
Sk?adnica?a treebank of Polish. In Proceedings of
the 5th Language & Technology Conference, pages
299?303, Poznan?, Poland.
Alina Wr?blewska. 2012. Polish Dependency Bank. Lin-
guistic Issues in Language Technology, 7(1):1?15.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of ACL:HLT, pages 188?193, Portland,
OR.
J?nos Zsibrita, Veronika Vincze, and Rich?rd Farkas.
2013. magyarlanc: A toolkit for morphological and
dependency parsing of Hungarian. In Proceedings of
RANLP, pages 763?771, Hissar, Bulgaria.
182
