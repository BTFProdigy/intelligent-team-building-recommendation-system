Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 41?48,
New York, June 2006. c?2006 Association for Computational Linguistics
Learning to recognize features of valid textual entailments
Bill MacCartney, Trond Grenager, Marie-Catherine de Marneffe,
Daniel Cer, and Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305
{wcmac, grenager, mcdm, cerd, manning}@cs.stanford.edu
Abstract
This paper advocates a new architecture for tex-
tual inference in which finding a good alignment is
separated from evaluating entailment. Current ap-
proaches to semantic inference in question answer-
ing and textual entailment have approximated the
entailment problem as that of computing the best
alignment of the hypothesis to the text, using a lo-
cally decomposable matching score. We argue that
there are significant weaknesses in this approach,
including flawed assumptions of monotonicity and
locality. Instead we propose a pipelined approach
where alignment is followed by a classification
step, in which we extract features representing
high-level characteristics of the entailment prob-
lem, and pass the resulting feature vector to a statis-
tical classifier trained on development data. We re-
port results on data from the 2005 Pascal RTE Chal-
lenge which surpass previously reported results for
alignment-based systems.
1 Introduction
During the last five years there has been a surge in
work which aims to provide robust textual inference
in arbitrary domains about which the system has no
expertise. The best-known such work has occurred
within the field of question answering (Pasca and
Harabagiu, 2001; Moldovan et al, 2003); more re-
cently, such work has continued with greater focus
in addressing the PASCAL Recognizing Textual En-
tailment (RTE) Challenge (Dagan et al, 2005) and
within the U.S. Government AQUAINT program.
Substantive progress on this task is key to many
text and natural language applications. If one could
tell that Protestors chanted slogans opposing a free
trade agreement was a match for people demonstrat-
ing against free trade, then one could offer a form of
semantic search not available with current keyword-
based search. Even greater benefits would flow to
richer and more semantically complex NLP tasks.
Because full, accurate, open-domain natural lan-
guage understanding lies far beyond current capa-
bilities, nearly all efforts in this area have sought
to extract the maximum mileage from quite lim-
ited semantic representations. Some have used sim-
ple measures of semantic overlap, but the more in-
teresting work has largely converged on a graph-
alignment approach, operating on semantic graphs
derived from syntactic dependency parses, and using
a locally-decomposable alignment score as a proxy
for strength of entailment. (Below, we argue that
even approaches relying on weighted abduction may
be seen in this light.) In this paper, we highlight the
fundamental semantic limitations of this type of ap-
proach, and advocate a multi-stage architecture that
addresses these limitations. The three key limita-
tions are an assumption of monotonicity, an assump-
tion of locality, and a confounding of alignment and
evaluation of entailment.
We focus on the PASCAL RTE data, examples
from which are shown in table 1. This data set con-
tains pairs consisting of a short text followed by a
one-sentence hypothesis. The goal is to say whether
the hypothesis follows from the text and general
background knowledge, according to the intuitions
of an intelligent human reader. That is, the standard
is not whether the hypothesis is logically entailed,
but whether it can reasonably be inferred.
2 Approaching a robust semantics
In this section we try to give a unifying overview
to current work on robust textual inference, to
present fundamental limitations of current meth-
ods, and then to outline our approach to resolving
them. Nearly all current textual inference systems
use a single-stage matching/proof process, and differ
41
ID Text Hypothesis Entailed
59 Two Turkish engineers and an Afghan translator kidnapped
in December were freed Friday.
translator kidnapped in Iraq no
98 Sharon warns Arafat could be targeted for assassination. prime minister targeted for assassination no
152 Twenty-five of the dead were members of the law enforce-
ment agencies and the rest of the 67 were civilians.
25 of the dead were civilians. no
231 The memorandum noted the United Nations estimated that
2.5 million to 3.5 million people died of AIDS last year.
Over 2 million people died of AIDS last
year.
yes
971 Mitsubishi Motors Corp.?s new vehicle sales in the US fell
46 percent in June.
Mitsubishi sales rose 46 percent. no
1806 Vanunu, 49, was abducted by Israeli agents and convicted
of treason in 1986 after discussing his work as a mid-level
Dimona technician with Britain?s Sunday Times newspaper.
Vanunu?s disclosures in 1968 led experts
to conclude that Israel has a stockpile of
nuclear warheads.
no
2081 The main race track in Qatar is located in Shahaniya, on the
Dukhan Road.
Qatar is located in Shahaniya. no
Table 1: Illustrative examples from the PASCAL RTE data set, available at http://www.pascal-network.org/Challenges/RTE.
Though most problems shown have answer no, the data set is actually balanced between yes and no.
mainly in the sophistication of the matching stage.
The simplest approach is to base the entailment pre-
diction on the degree of semantic overlap between
the text and hypothesis using models based on bags
of words, bags of n-grams, TF-IDF scores, or some-
thing similar (Jijkoun and de Rijke, 2005). Such
models have serious limitations: semantic overlap is
typically a symmetric relation, whereas entailment
is clearly not, and, because overlap models do not
account for syntactic or semantic structure, they are
easily fooled by examples like ID 2081.
A more structured approach is to formulate the
entailment prediction as a graph matching problem
(Haghighi et al, 2005; de Salvo Braz et al, 2005).
In this formulation, sentences are represented as nor-
malized syntactic dependency graphs (like the one
shown in figure 1) and entailment is approximated
with an alignment between the graph representing
the hypothesis and a portion of the corresponding
graph(s) representing the text. Each possible align-
ment of the graphs has an associated score, and the
score of the best alignment is used as an approxi-
mation to the strength of the entailment: a better-
aligned hypothesis is assumed to be more likely to
be entailed. To enable incremental search, align-
ment scores are usually factored as a combination
of local terms, corresponding to the nodes and edges
of the two graphs. Unfortunately, even with factored
scores the problem of finding the best alignment of
two graphs is NP-complete, so exact computation is
intractable. Authors have proposed a variety of ap-
proximate search techniques. Haghighi et al (2005)
divide the search into two steps: in the first step they
consider node scores only, which relaxes the prob-
lem to a weighted bipartite graph matching that can
be solved in polynomial time, and in the second step
they add the edges scores and hillclimb the align-
ment via an approximate local search.
A third approach, exemplified by Moldovan et al
(2003) and Raina et al (2005), is to translate de-
pendency parses into neo-Davidsonian-style quasi-
logical forms, and to perform weighted abductive
theorem proving in the tradition of (Hobbs et al,
1988). Unless supplemented with a knowledge
base, this approach is actually isomorphic to the
graph matching approach. For example, the graph
in figure 1 might generate the quasi-LF rose(e1),
nsubj(e1, x1), sales(x1), nn(x1, x2), Mitsubishi(x2),
dobj(e1, x3), percent(x3), num(x3, x4), 46(x4).
There is a term corresponding to each node and arc,
and the resolution steps at the core of weighted ab-
duction theorem proving consider matching an indi-
vidual node of the hypothesis (e.g. rose(e1)) with
something from the text (e.g. fell(e1)), just as in
the graph-matching approach. The two models be-
come distinct when there is a good supply of addi-
tional linguistic and world knowledge axioms?as in
Moldovan et al (2003) but not Raina et al (2005).
Then the theorem prover may generate intermedi-
ate forms in the proof, but, nevertheless, individ-
ual terms are resolved locally without reference to
global context.
Finally, a few efforts (Akhmatova, 2005; Fowler
et al, 2005; Bos and Markert, 2005) have tried to
42
translate sentences into formulas of first-order logic,
in order to test logical entailment with a theorem
prover. While in principle this approach does not
suffer from the limitations we describe below, in
practice it has not borne much fruit. Because few
problem sentences can be accurately translated to
logical form, and because logical entailment is a
strict standard, recall tends to be poor.
The simple graph matching formulation of the
problem belies three important issues. First, the
above systems assume a form of upward monotonic-
ity: if a good match is found with a part of the text,
other material in the text is assumed not to affect
the validity of the match. But many situations lack
this upward monotone character. Consider variants
on ID 98. Suppose the hypothesis were Arafat tar-
geted for assassination. This would allow a perfect
graph match or zero-cost weighted abductive proof,
because the hypothesis is a subgraph of the text.
However, this would be incorrect because it ignores
the modal operator could. Information that changes
the validity of a proof can also exist outside a match-
ing clause. Consider the alternate text Sharon denies
Arafat is targeted for assassination.1
The second issue is the assumption of locality.
Locality is needed to allow practical search, but
many entailment decisions rely on global features of
the alignment, and thus do not naturally factor by
nodes and edges. To take just one example, drop-
ping a restrictive modifier preserves entailment in a
positive context, but not in a negative one. For exam-
ple, Dogs barked loudly entails Dogs barked, but No
dogs barked loudly does not entail No dogs barked.
These more global phenomena cannot be modeled
with a factored alignment score.
The last issue arising in the graph matching ap-
proaches is the inherent confounding of alignment
and entailment determination. The way to show that
one graph element does not follow from another is
to make the cost of aligning them high. However,
since we are embedded in a search for the lowest
cost alignment, this will just cause the system to
choose an alternate alignment rather than recogniz-
ing a non-entailment. In ID 152, we would like the
hypothesis to align with the first part of the text, to
1This is the same problem labeled and addressed as context
in Tatu and Moldovan (2005).
be able to prove that civilians are not members of
law enforcement agencies and conclude that the hy-
pothesis does not follow from the text. But a graph-
matching system will to try to get non-entailment
by making the matching cost between civilians and
members of law enforcement agencies be very high.
However, the likely result of that is that the final part
of the hypothesis will align with were civilians at
the end of the text, assuming that we allow an align-
ment with ?loose? arc correspondence.2 Under this
candidate alignment, the lexical alignments are per-
fect, and the only imperfect alignment is the subject
arc of were is mismatched in the two. A robust in-
ference guesser will still likely conclude that there is
entailment.
We propose that all three problems can be re-
solved in a two-stage architecture, where the align-
ment phase is followed by a separate phase of en-
tailment determination. Although developed inde-
pendently, the same division between alignment and
classification has also been proposed by Marsi and
Krahmer (2005), whose textual system is developed
and evaluated on parallel translations into Dutch.
Their classification phase features an output space
of five semantic relations, and performs well at dis-
tinguishing entailing sentence pairs.
Finding aligned content can be done by any search
procedure. Compared to previous work, we empha-
size structural alignment, and seek to ignore issues
like polarity and quantity, which can be left to a
subsequent entailment decision. For example, the
scoring function is designed to encourage antonym
matches, and ignore the negation of verb predicates.
The ideas clearly generalize to evaluating several
alignments, but we have so far worked with just
the one-best alignment. Given a good alignment,
the determination of entailment reduces to a simple
classification decision. The classifier is built over
features designed to recognize patterns of valid and
invalid inference. Weights for the features can be
hand-set or chosen to minimize a relevant loss func-
tion on training data using standard techniques from
machine learning. Because we already have a com-
plete alignment, the classifier?s decision can be con-
2Robust systems need to allow matches with imperfect arc
correspondence. For instance, given Bill went to Lyons to study
French farming practices, we would like to be able to conclude
that Bill studied French farming despite the structural mismatch.
43
ditioned on arbitrary global features of the aligned
graphs, and it can detect failures of monotonicity.
3 System
Our system has three stages: linguistic analysis,
alignment, and entailment determination.
3.1 Linguistic analysis
Our goal in this stage is to compute linguistic rep-
resentations of the text and hypothesis that contain
as much information as possible about their seman-
tic content. We use typed dependency graphs, which
contain a node for each word and labeled edges rep-
resenting the grammatical relations between words.
Figure 1 gives the typed dependency graph for ID
971. This representation contains much of the infor-
mation about words and relations between them, and
is relatively easy to compute from a syntactic parse.
However many semantic phenomena are not repre-
sented properly; particularly egregious is the inabil-
ity to represent quantification and modality.
We parse input sentences to phrase structure
trees using the Stanford parser (Klein and Manning,
2003), a statistical syntactic parser trained on the
Penn TreeBank. To ensure correct parsing, we pre-
process the sentences to collapse named entities into
new dedicated tokens. Named entities are identi-
fied by a CRF-based NER system, similar to that
described in (McCallum and Li, 2003). After pars-
ing, contiguous collocations which appear in Word-
Net (Fellbaum, 1998) are identified and grouped.
We convert the phrase structure trees to typed de-
pendency graphs using a set of deterministic hand-
coded rules (de Marneffe et al, 2006). In these rules,
heads of constituents are first identified using a mod-
ified version of the Collins head rules that favor se-
mantic heads (such as lexical verbs rather than aux-
iliaries), and dependents of heads are typed using
tregex patterns (Levy and Andrew, 2006), an exten-
sion of the tgrep pattern language. The nodes in the
final graph are then annotated with their associated
word, part-of-speech (given by the parser), lemma
(given by a finite-state transducer described by Min-
nen et al (2001)) and named-entity tag.
3.2 Alignment
The purpose of the second phase is to find a good
partial alignment between the typed dependency
graphs representing the hypothesis and the text. An
alignment consists of a mapping from each node
(word) in the hypothesis graph to a single node in
the text graph, or to null.3 Figure 1 gives the align-
ment for ID 971.
The space of alignments is large: there are
O((m + 1)n) possible alignments for a hypothesis
graph with n nodes and a text graph with m nodes.
We define a measure of alignment quality, and a
procedure for identifying high scoring alignments.
We choose a locally decomposable scoring function,
such that the score of an alignment is the sum of
the local node and edge alignment scores. Unfor-
tunately, there is no polynomial time algorithm for
finding the exact best alignment. Instead we use an
incremental beam search, combined with a node or-
dering heuristic, to do approximate global search in
the space of possible alignments. We have exper-
imented with several alternative search techniques,
and found that the solution quality is not very sensi-
tive to the specific search procedure used.
Our scoring measure is designed to favor align-
ments which align semantically similar subgraphs,
irrespective of polarity. For this reason, nodes re-
ceive high alignment scores when the words they
represent are semantically similar. Synonyms and
antonyms receive the highest score, and unrelated
words receive the lowest. Our hand-crafted scor-
ing metric takes into account the word, the lemma,
and the part of speech, and searches for word relat-
edness using a range of external resources, includ-
ing WordNet, precomputed latent semantic analysis
matrices, and special-purpose gazettes. Alignment
scores also incorporate local edge scores, which are
based on the shape of the paths between nodes in
the text graph which correspond to adjacent nodes
in the hypothesis graph. Preserved edges receive the
highest score, and longer paths receive lower scores.
3.3 Entailment determination
In the final stage of processing, we make a deci-
sion about whether or not the hypothesis is entailed
by the text, conditioned on the typed dependency
graphs, as well as the best alignment between them.
3The limitations of using one-to-one alignments are miti-
gated by the fact that many multiword expressions (e.g. named
entities, noun compounds, multiword prepositions) have been
collapsed into single nodes during linguistic analysis.
44
rose
sales
Mitsubishi
percent
46
nsubj dobj
nn num
Alignment
rose ? fell
sales ? sales
Mitsubishi ? Mitsubishi Motors Corp.
percent ? percent
46 ? 46
Alignment score: ?0.8962
Features
Antonyms aligned in pos/pos context ?
Structure: main predicate good match +
Number: quantity match +
Date: text date deleted in hypothesis ?
Alignment: good score +
Entailment score: ?5.4262
Figure 1: Problem representation for ID 971: typed dependency graph (hypothesis only), alignment, and entailment features.
Because we have a data set of examples that are la-
beled for entailment, we can use techniques from su-
pervised machine learning to learn a classifier. We
adopt the standard approach of defining a featural
representation of the problem and then learning a
linear decision boundary in the feature space. We
focus here on the learning methodology; the next
section covers the definition of the set of features.
Defined in this way, one can apply any statistical
learning algorithm to this classification task, such
as support vector machines, logistic regression, or
naive Bayes. We used a logistic regression classifier
with a Gaussian prior parameter for regularization.
We also compare our learning results with those
achieved by hand-setting the weight parameters for
the classifier, effectively incorporating strong prior
(human) knowledge into the choice of weights.
An advantage to the use of statistical classifiers
is that they can be configured to output a proba-
bility distribution over possible answers rather than
just the most likely answer. This allows us to get
confidence estimates for computing a confidence
weighted score (see section 5). A major concern in
applying machine learning techniques to this clas-
sification problem is the relatively small size of the
training set, which can lead to overfitting problems.
We address this by keeping the feature dimensional-
ity small, and using high regularization penalties in
training.
4 Feature representation
In the entailment determination phase, the entail-
ment problem is reduced to a representation as a
vector of 28 features, over which the statistical
classifier described above operates. These features
try to capture salient patterns of entailment and
non-entailment, with particular attention to contexts
which reverse or block monotonicity, such as nega-
tions and quantifiers. This section describes the most
important groups of features.
Polarity features. These features capture the pres-
ence (or absence) of linguistic markers of negative
polarity contexts in both the text and the hypothesis,
such as simple negation (not), downward-monotone
quantifiers (no, few), restricting prepositions (with-
out, except) and superlatives (tallest).
Adjunct features. These indicate the dropping or
adding of syntactic adjuncts when moving from the
text to the hypothesis. For the common case of
restrictive adjuncts, dropping an adjunct preserves
truth (Dogs barked loudly |= Dogs barked), while
adding an adjunct does not (Dogs barked 6|= Dogs
barked today). However, in negative-polarity con-
texts (such as No dogs barked), this heuristic is
reversed: adjuncts can safely be added, but not
dropped. For example, in ID 59, the hypothesis
aligns well with the text, but the addition of in Iraq
indicates non-entailment.
We identify the ?root nodes? of the problem: the
root node of the hypothesis graph and the corre-
sponding aligned node in the text graph. Using de-
pendency information, we identify whether adjuncts
have been added or dropped. We then determine
the polarity (negative context, positive context or
restrictor of a universal quantifier) of the two root
nodes to generate features accordingly.
Antonymy features. Entailment problems might
involve antonymy, as in ID 971. We check whether
an aligned pairs of text/hypothesis words appear to
be antonymous by consulting a pre-computed list
of about 40,000 antonymous and other contrasting
pairs derived from WordNet. For each antonymous
pair, we generate one of three boolean features, in-
dicating whether (i) the words appear in contexts of
matching polarity, (ii) only the text word appears in
a negative-polarity context, or (iii) only the hypoth-
esis word does.
45
Modality features. Modality features capture
simple patterns of modal reasoning, as in ID 98,
which illustrates the heuristic that possibility does
not entail actuality. According to the occurrence
(or not) of predefined modality markers, such as
must or maybe, we map the text and the hypoth-
esis to one of six modalities: possible, not possi-
ble, actual, not actual, necessary, and not necessary.
The text/hypothesis modality pair is then mapped
into one of the following entailment judgments: yes,
weak yes, don?t know, weak no, or no. For example:
(not possible |= not actual)? ? yes
(possible |= necessary)? ? weak no
Factivity features. The context in which a verb
phrase is embedded may carry semantic presuppo-
sitions giving rise to (non-)entailments such as The
gangster tried to escape 6|= The gangster escaped.
This pattern of entailment, like others, can be re-
versed by negative polarity markers (The gangster
managed to escape |= The gangster escaped while
The gangster didn?t manage to escape 6|= The gang-
ster escaped). To capture these phenomena, we
compiled small lists of ?factive? and non-factive
verbs, clustered according to the kinds of entail-
ments they create. We then determine to which class
the parent of the text aligned with the hypothesis
root belongs to. If the parent is not in the list, we
only check whether the embedding text is an affir-
mative context or a negative one.
Quantifier features. These features are designed
to capture entailment relations among simple sen-
tences involving quantification, such as Every com-
pany must report |= A company must report (or
The company, or IBM). No attempt is made to han-
dle multiple quantifiers or scope ambiguities. Each
quantifier found in an aligned pair of text/hypothesis
words is mapped into one of five quantifier cate-
gories: no, some, many, most, and all. The no
category is set apart, while an ordering over the
other four categories is defined. The some category
also includes definite and indefinite determiners and
small cardinal numbers. A crude attempt is made to
handle negation by interchanging no and all in the
presence of negation. Features are generated given
the categories of both hypothesis and text.
Number, date, and time features. These are de-
signed to recognize (mis-)matches between num-
bers, dates, and times, as in IDs 1806 and 231. We
do some normalization (e.g. of date representations)
and have a limited ability to do fuzzy matching. In
ID 1806, the mismatched years are correctly iden-
tified. Unfortunately, in ID 231 the significance of
over is not grasped and a mismatch is reported.
Alignment features. Our feature representation
includes three real-valued features intended to rep-
resent the quality of the alignment: score is the
raw score returned from the alignment phase, while
goodscore and badscore try to capture whether the
alignment score is ?good? or ?bad? by computing
the sigmoid function of the distance between the
alignment score and hard-coded ?good? and ?bad?
reference values.
5 Evaluation
We present results based on the First PASCAL RTE
Challenge, which used a development set contain-
ing 567 pairs and a test set containing 800 pairs.
The data sets are balanced to contain equal num-
bers of yes and no answers. The RTE Challenge
recommended two evaluation metrics: raw accuracy
and confidence weighted score (CWS). The CWS is
computed as follows: for each positive integer k up
to the size of the test set, we compute accuracy over
the k most confident predictions. The CWS is then
the average, over k, of these partial accuracies. Like
raw accuracy, it lies in the interval [0, 1], but it will
exceed raw accuracy to the degree that predictions
are well-calibrated.
Several characteristics of the RTE problems
should be emphasized. Examples are derived from a
broad variety of sources, including newswire; there-
fore systems must be domain-independent. The in-
ferences required are, from a human perspective,
fairly superficial: no long chains of reasoning are
involved. However, there are ?trick? questions ex-
pressly designed to foil simplistic techniques. The
definition of entailment is informal and approx-
imate: whether a competent speaker with basic
knowledge of the world would typically infer the hy-
pothesis from the text. Entailments will certainly de-
pend on linguistic knowledge, and may also depend
on world knowledge; however, the scope of required
46
Algorithm RTE1 Dev Set RTE1 Test Set
Acc CWS Acc CWS
Random 50.0% 50.0% 50.0% 50.0%
Jijkoun et al 05 61.0% 64.9% 55.3% 55.9%
Raina et al 05 57.8% 66.1% 55.5% 63.8%
Haghighi et al 05 ? ? 56.8% 61.4%
Bos & Markert 05 ? ? 57.7% 63.2%
Alignment only 58.7% 59.1% 54.5% 59.7%
Hand-tuned 60.3% 65.3% 59.1% 65.0%
Learning 61.2% 74.4% 59.1% 63.9%
Table 2: Performance on the RTE development and test sets.
CWS stands for confidence weighted score (see text).
world knowledge is left unspecified.4
Despite the informality of the problem definition,
human judges exhibit very good agreement on the
RTE task, with agreement rate of 91?96% (Dagan
et al, 2005). In principle, then, the upper bound
for machine performance is quite high. In practice,
however, the RTE task is exceedingly difficult for
computers. Participants in the first PASCAL RTE
workshop reported accuracy from 49% to 59%, and
CWS from 50.0% to 69.0% (Dagan et al, 2005).
Table 2 shows results for a range of systems and
testing conditions. We report accuracy and CWS on
each RTE data set. The baseline for all experiments
is random guessing, which always attains 50% accu-
racy. We show comparable results from recent sys-
tems based on lexical similarity (Jijkoun and de Ri-
jke, 2005), graph alignment (Haghighi et al, 2005),
weighted abduction (Raina et al, 2005), and a mixed
system including theorem proving (Bos and Mark-
ert, 2005).
We then show results for our system under several
different training regimes. The row labeled ?align-
ment only? describes experiments in which all fea-
tures except the alignment score are turned off. We
predict entailment just in case the alignment score
exceeds a threshold which is optimized on devel-
opment data. ?Hand-tuning? describes experiments
in which all features are on, but no training oc-
curs; rather, weights are set by hand, according to
human intuition. Finally, ?learning? describes ex-
periments in which all features are on, and feature
weights are trained on the development data. The
4Each RTE problem is also tagged as belonging to one of
seven tasks. Previous work (Raina et al, 2005) has shown that
conditioning on task can significantly improve accuracy. In this
work, however, we ignore the task variable, and none of the
results shown in table 2 reflect optimization by task.
figures reported for development data performance
therefore reflect overfitting; while such results are
not a fair measure of overall performance, they can
help us assess the adequacy of our feature set: if
our features have failed to capture relevant aspects
of the problem, we should expect poor performance
even when overfitting. It is therefore encouraging
to see CWS above 70%. Finally, the figures re-
ported for test data performance are the fairest ba-
sis for comparison. These are significantly better
than our results for alignment only (Fisher?s exact
test, p < 0.05), indicating that we gain real value
from our features. However, the gain over compara-
ble results from other teams is not significant at the
p < 0.05 level.
A curious observation is that the results for hand-
tuned weights are as good or better than results for
learned weights. A possible explanation runs as fol-
lows. Most of the features represent high-level pat-
terns which arise only occasionally. Because the
training data contains only a few hundred exam-
ples, many features are active in just a handful of
instances; their learned weights are therefore quite
noisy. Indeed, a feature which is expected to fa-
vor entailment may even wind up with a negative
weight: the modal feature weak yes is an example.
As shown in table 3, the learned weight for this fea-
ture was strongly negative ? but this resulted from
a single training example in which the feature was
active but the hypothesis was not entailed. In such
cases, we shouldn?t expect good generalization to
test data, and human intuition about the ?value? of
specific features may be more reliable.
Table 3 shows the values learned for selected fea-
ture weights. As expected, the features added ad-
junct in all context, modal yes, and text is factive
were all found to be strong indicators of entailment,
while date insert, date modifier insert, widening
from text to hyp all indicate lack of entailment. Inter-
estingly, text has neg marker and text & hyp diff po-
larity were also found to disfavor entailment; while
this outcome is sensible, it was not anticipated or
designed.
6 Conclusion
The best current approaches to the problem of tex-
tual inference work by aligning semantic graphs,
47
Feature class & condition weight
Adjunct added adjunct in all context 1.40
Date date mismatch 1.30
Alignment good score 1.10
Modal yes 0.70
Modal no 0.51
Factive text is factive 0.46
. . . . . . . . .
Polarity text & hyp same polarity ?0.45
Modal don?t know ?0.59
Quantifier widening from text to hyp ?0.66
Polarity text has neg marker ?0.66
Polarity text & hyp diff polarity ?0.72
Alignment bad score ?1.53
Date date modifier insert ?1.57
Modal weak yes ?1.92
Date date insert ?2.63
Table 3: Learned weights for selected features. Positive weights
favor entailment. Weights near 0 are omitted. Based on training
on the PASCAL RTE development set.
using a locally-decomposable alignment score as a
proxy for strength of entailment. We have argued
that such models suffer from three crucial limita-
tions: an assumption of monotonicity, an assump-
tion of locality, and a confounding of alignment and
entailment determination.
We have described a system which extends
alignment-based systems while attempting to ad-
dress these limitations. After finding the best align-
ment between text and hypothesis, we extract high-
level semantic features of the entailment problem,
and input these features to a statistical classifier to
make an entailment decision. Using this multi-stage
architecture, we report results on the PASCAL RTE
data which surpass previously-reported results for
alignment-based systems.
We see the present work as a first step in a promis-
ing direction. Much work remains in improving the
entailment features, many of which may be seen as
rough approximations to a formal monotonicity cal-
culus. In future, we aim to combine more precise
modeling of monotonicity effects with better mod-
eling of paraphrase equivalence.
Acknowledgements
We thank Anna Rafferty, Josh Ainslie, and partic-
ularly Roger Grosse for contributions to the ideas
and system reported here. This work was supported
in part by the Advanced Research and Development
Activity (ARDA)?s Advanced Question Answering
for Intelligence (AQUAINT) Program.
References
E. Akhmatova. 2005. Textual entailment resolution via atomic
propositions. In Proceedings of the PASCAL Challenges
Workshop on Recognising Textual Entailment, 2005.
J. Bos and K. Markert. 2005. Recognising textual entailment
with logical inference. In EMNLP-05.
I. Dagan, O. Glickman, and B. Magnini. 2005. The PASCAL
recognising textual entailment challenge. In Proceedings of
the PASCAL Challenges Workshop on Recognising Textual
Entailment.
Marie-Catherine de Marneffe, Bill MacCartney, and Christo-
pher D. Manning. 2006. Generating typed dependency
parses from phrase structure parses. In LREC 2006.
R. de Salvo Braz, R. Girju, V. Punyakanok, D. Roth, and
M. Sammons. 2005. An inference model for semantic entail-
ment and question-answering. In Proceedings of the Twenti-
eth National Conference on Artificial Intelligence (AAAI).
C. Fellbaum. 1998. WordNet: an electronic lexical database.
MIT Press.
A. Fowler, B. Hauser, D. Hodges, I. Niles, A. Novischi, and
J. Stephan. 2005. Applying COGEX to recognize textual
entailment. In Proceedings of the PASCAL Challenges Work-
shop on Recognising Textual Entailment.
A. Haghighi, A. Ng, and C. D. Manning. 2005. Robust textual
inference via graph matching. In EMNLP-05.
J. R. Hobbs, M. Stickel, P. Martin, and D. D. Edwards. 1988.
Interpretation as abduction. In 26th Annual Meeting of the
Association for Computational Linguistics: Proceedings of
the Conference, pages 95?103, Buffalo, New York.
V. Jijkoun and M. de Rijke. 2005. Recognizing textual entail-
ment using lexical similarity. In Proceedings of the PAS-
CAL Challenge Workshop on Recognising Textual Entail-
ment, 2005, pages 73?76.
D. Klein and C. D. Manning. 2003. Accurate unlexicalized
parsing. In Proceedings of the 41st Meeting of the Associa-
tion of Computational Linguistics.
Roger Levy and Galen Andrew. 2006. Tregex and Tsurgeon:
tools for querying and manipulating tree data structures. In
LREC 2006.
E. Marsi and E. Krahmer. 2005. Classification of semantic re-
lations by humans and machines. In Proceedings of the ACL
2005 Workshop on Empirical Modeling of Semantic Equiva-
lence and Entailment.
A. McCallum and W. Li. 2003. Early results for named entity
recognition with conditional random fields, feature induction
and web-enhanced lexicons. In Proceedings of CoNLL 2003.
G. Minnen, J. Carroll, and D. Pearce. 2001. Applied morpho-
logical processing in English. In Natural Language Engi-
neering, volume 7(3), pages 207?233.
D. Moldovan, C. Clark, S. Harabagiu, and S. Maiorano. 2003.
COGEX: A logic prover for question answering. In NAACL-
03.
M. Pasca and S. Harabagiu. 2001. High performance ques-
tion/answering. In SIGIR-01, pages 366?374.
R. Raina, A .Ng, and C. D. Manning. 2005. Robust textual
inference via learning and abductive reasoning. In Proceed-
ings of the Twentieth National Conference on Artificial Intel-
ligence (AAAI).
M. Tatu and D. Moldovan. 2005. A semantic approach to rec-
ognizing textual entailment. In HLT/EMNLP 2005, pages
371?378.
48
Proceedings of ACL-08: HLT, pages 1039?1047,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Finding Contradictions in Text
Marie-Catherine de Marneffe,
Linguistics Department
Stanford University
Stanford, CA 94305
mcdm@stanford.edu
Anna N. Rafferty and Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305
{rafferty,manning}@stanford.edu
Abstract
Detecting conflicting statements is a foun-
dational text understanding task with appli-
cations in information analysis. We pro-
pose an appropriate definition of contradiction
for NLP tasks and develop available corpora,
from which we construct a typology of con-
tradictions. We demonstrate that a system for
contradiction needs to make more fine-grained
distinctions than the common systems for en-
tailment. In particular, we argue for the cen-
trality of event coreference and therefore in-
corporate such a component based on topical-
ity. We present the first detailed breakdown
of performance on this task. Detecting some
types of contradiction requires deeper inferen-
tial paths than our system is capable of, but
we achieve good performance on types arising
from negation and antonymy.
1 Introduction
In this paper, we seek to understand the ways con-
tradictions occur across texts and describe a system
for automatically detecting such constructions. As a
foundational task in text understanding (Condoravdi
et al, 2003), contradiction detection has many possi-
ble applications. Consider applying a contradiction
detection system to political candidate debates: by
drawing attention to topics in which candidates have
conflicting positions, the system could enable voters
to make more informed choices between candidates
and sift through the amount of available informa-
tion. Contradiction detection could also be applied
to intelligence reports, demonstrating which infor-
mation may need further verification. In bioinfor-
matics where protein-protein interaction is widely
studied, automatically finding conflicting facts about
such interactions would be beneficial.
Here, we shed light on the complex picture of con-
tradiction in text. We provide a definition of contra-
diction suitable for NLP tasks, as well as a collec-
tion of contradiction corpora. Analyzing these data,
we find contradiction is a rare phenomenon that may
be created in different ways; we propose a typol-
ogy of contradiction classes and tabulate their fre-
quencies. Contradictions arise from relatively obvi-
ous features such as antonymy, negation, or numeric
mismatches. They also arise from complex differ-
ences in the structure of assertions, discrepancies
based on world-knowledge, and lexical contrasts.
(1) Police specializing in explosives defused the rock-
ets. Some 100 people were working inside the plant.
(2) 100 people were injured.
This pair is contradictory: defused rockets cannot go
off, and thus cannot injure anyone. Detecting con-
tradictions appears to be a harder task than detecting
entailments. Here, it is relatively easy to identify the
lack of entailment: the first sentence involves no in-
juries, so the second is unlikely to be entailed. Most
entailment systems function as weak proof theory
(Hickl et al, 2006; MacCartney et al, 2006; Zan-
zotto et al, 2007), but contradictions require deeper
inferences and model building. While mismatch-
ing information between sentences is often a good
cue of non-entailment (Vanderwende et al, 2006),
it is not sufficient for contradiction detection which
requires more precise comprehension of the conse-
quences of sentences. Assessing event coreference
is also essential: for texts to contradict, they must
1039
refer to the same event. The importance of event
coreference was recognized in the MUC information
extraction tasks in which it was key to identify sce-
narios related to the same event (Humphreys et al,
1997). Recent work in text understanding has not
focused on this issue, but it must be tackled in a suc-
cessful contradiction system. Our system includes
event coreference, and we present the first detailed
examination of contradiction detection performance,
on the basis of our typology.
2 Related work
Little work has been done on contradiction detec-
tion. The PASCAL Recognizing Textual Entailment
(RTE) Challenges (Dagan et al, 2006; Bar-Haim
et al, 2006; Giampiccolo et al, 2007) focused on
textual inference in any domain. Condoravdi et al
(2003) first recognized the importance of handling
entailment and contradiction for text understanding,
but they rely on a strict logical definition of these
phenomena and do not report empirical results. To
our knowledge, Harabagiu et al (2006) provide the
first empirical results for contradiction detection, but
they focus on specific kinds of contradiction: those
featuring negation and those formed by paraphrases.
They constructed two corpora for evaluating their
system. One was created by overtly negating each
entailment in the RTE2 data, producing a bal-
anced dataset (LCC negation). To avoid overtrain-
ing, negative markers were also added to each non-
entailment, ensuring that they did not create con-
tradictions. The other was produced by paraphras-
ing the hypothesis sentences from LCC negation, re-
moving the negation (LCC paraphrase): A hunger
strike was not attempted ? A hunger strike was
called off. They achieved very good performance:
accuracies of 75.63% on LCC negation and 62.55%
on LCC paraphrase. Yet, contradictions are not lim-
ited to these constructions; to be practically useful,
any system must provide broader coverage.
3 Contradictions
3.1 What is a contradiction?
One standard is to adopt a strict logical definition of
contradiction: sentences A and B are contradictory
if there is no possible world in which A and B are
both true. However, for contradiction detection to be
useful, a looser definition that more closely matches
human intuitions is necessary; contradiction occurs
when two sentences are extremely unlikely to be true
simultaneously. Pairs such as Sally sold a boat to
John and John sold a boat to Sally are tagged as con-
tradictory even though it could be that each sold a
boat to the other. This definition captures intuitions
of incompatiblity, and perfectly fits applications that
seek to highlight discrepancies in descriptions of the
same event. Examples of contradiction are given in
table 1. For texts to be contradictory, they must in-
volve the same event. Two phenomena must be con-
sidered in this determination: implied coreference
and embedded texts. Given limited context, whether
two entities are coreferent may be probable rather
than certain. To match human intuitions, compatible
noun phrases between sentences are assumed to be
coreferent in the absence of clear countervailing ev-
idence. In the following example, it is not necessary
that the woman in the first and second sentences is
the same, but one would likely assume it is if the two
sentences appeared together:
(1) Passions surrounding Germany?s final match turned
violent when a woman stabbed her partner because
she didn?t want to watch the game.
(2) A woman passionately wanted to watch the game.
We also mark as contradictions pairs reporting con-
tradictory statements. The following sentences refer
to the same event (de Menezes in a subway station),
and display incompatible views of this event:
(1) Eyewitnesses said de Menezes had jumped over the
turnstile at Stockwell subway station.
(2) The documents leaked to ITV News suggest that
Menezes walked casually into the subway station.
This example contains an ?embedded contradic-
tion.? Contrary to Zaenen et al (2005), we argue
that recognizing embedded contradictions is impor-
tant for the application of a contradiction detection
system: if John thinks that he is incompetent, and his
boss believes that John is not being given a chance,
one would like to detect that the targeted information
in the two sentences is contradictory, even though
the two sentences can be true simultaneously.
3.2 Typology of contradictions
Contradictions may arise from a number of different
constructions, some overt and others that are com-
1040
ID Type Text Hypothesis
1 Antonym Capital punishment is a catalyst for more crime. Capital punishment is a deterrent to
crime.
2 Negation A closely divided Supreme Court said that juries and
not judges must impose a death sentence.
The Supreme Court decided that only
judges can impose the death sentence.
3 Numeric The tragedy of the explosion in Qana that killed more
than 50 civilians has presented Israel with a dilemma.
An investigation into the strike in Qana
found 28 confirmed dead thus far.
4 Factive Prime Minister John Howard says he will not be
swayed by a warning that Australia faces more terror-
ism attacks unless it withdraws its troops from Iraq.
Australia withdraws from Iraq.
5 Factive The bombers had not managed to enter the embassy. The bombers entered the embassy.
6 Structure Jacques Santer succeeded Jacques Delors as president
of the European Commission in 1995.
Delors succeeded Santer in the presi-
dency of the European Commission.
7 Structure The Channel Tunnel stretches from England to
France. It is the second-longest rail tunnel in the
world, the longest being a tunnel in Japan.
The Channel Tunnel connects France
and Japan.
8 Lexical The Canadian parliament?s Ethics Commission said
former immigration minister, Judy Sgro, did nothing
wrong and her staff had put her into a conflict of in-
terest.
The Canadian parliament?s Ethics
Commission accuses Judy Sgro.
9 Lexical In the election, Bush called for U.S. troops to be with-
drawn from the peacekeeping mission in the Balkans.
He cites such missions as an example of
how America must ?stay the course.?
10 WK Microsoft Israel, one of the first Microsoft branches
outside the USA, was founded in 1989.
Microsoft was established in 1989.
Table 1: Examples of contradiction types.
plex even for humans to detect. Analyzing contra-
diction corpora (see section 3.3), we find two pri-
mary categories of contradiction: (1) those occur-
ring via antonymy, negation, and date/number mis-
match, which are relatively simple to detect, and
(2) contradictions arising from the use of factive or
modal words, structural and subtle lexical contrasts,
as well as world knowledge (WK).
We consider contradictions in category (1) ?easy?
because they can often be automatically detected
without full sentence comprehension. For exam-
ple, if words in the two passages are antonyms and
the sentences are reasonably similar, especially in
polarity, a contradiction occurs. Additionally, little
external information is needed to gain broad cover-
age of antonymy, negation, and numeric mismatch
contradictions; each involves only a closed set of
words or data that can be obtained using existing
resources and techniques (e.g., WordNet (Fellbaum,
1998), VerbOcean (Chklovski and Pantel, 2004)).
However, contradictions in category (2) are more
difficult to detect automatically because they require
precise models of sentence meaning. For instance,
to find the contradiction in example 8 (table 1),
it is necessary to learn that X said Y did nothing
wrong and X accuses Y are incompatible. Presently,
there exist methods for learning oppositional terms
(Marcu and Echihabi, 2002) and paraphrase learn-
ing has been thoroughly studied, but successfully
extending these techniques to learn incompatible
phrases poses difficulties because of the data dis-
tribution. Example 9 provides an even more dif-
ficult instance of contradiction created by a lexical
discrepancy. Structural issues also create contradic-
tions (examples 6 and 7). Lexical complexities and
variations in the function of arguments across verbs
can make recognizing these contradictions compli-
cated. Even when similar verbs are used and ar-
gument differences exist, structural differences may
indicate non-entailment or contradiction, and distin-
guishing the two automatically is problematic. Con-
sider contradiction 7 in table 1 and the following
non-contradiction:
(1) The CFAP purchases food stamps from the govern-
ment and distributes them to eligible recipients.
(2) A government purchases food.
1041
Data # contradictions # total pairs
RTE1 dev1 48 287
RTE1 dev2 55 280
RTE1 test 149 800
RTE2 dev 111 800
RTE3 dev 80 800
RTE3 test 72 800
Table 2: Number of contradictions in the RTE datasets.
In both cases, the first sentence discusses one en-
tity (CFAP, The Channel Tunnel) with a relationship
(purchase, stretch) to other entities. The second sen-
tence posits a similar relationship that includes one
of the entities involved in the original relationship
as well as an entity that was not involved. However,
different outcomes result because a tunnel connects
only two unique locations whereas more than one
entity may purchase food. These frequent interac-
tions between world-knowledge and structure make
it hard to ensure that any particular instance of struc-
tural mismatch is a contradiction.
3.3 Contradiction corpora
Following the guidelines above, we annotated the
RTE datasets for contradiction. These datasets con-
tain pairs consisting of a short text and a one-
sentence hypothesis. Table 2 gives the number of
contradictions in each dataset. The RTE datasets are
balanced between entailments and non-entailments,
and even in these datasets targeting inference, there
are few contradictions. Using our guidelines,
RTE3 test was annotated by NIST as part of the
RTE3 Pilot task in which systems made a 3-way de-
cision as to whether pairs of sentences were entailed,
contradictory, or neither (Voorhees, 2008).1
Our annotations and those of NIST were per-
formed on the original RTE datasets, contrary to
Harabagiu et al (2006). Because their corpora are
constructed using negation and paraphrase, they are
unlikely to cover all types of contradictions in sec-
tion 3.2. We might hypothesize that rewriting ex-
plicit negations commonly occurs via the substitu-
tion of antonyms. Imagine, e.g.:
H: Bill has finished his math.
1Information about this task as well as data can be found at
http://nlp.stanford.edu/RTE3-pilot/.
Type RTE sets ?Real? corpus
1 Antonym 15.0 9.2
Negation 8.8 17.6
Numeric 8.8 29.0
2 Factive/Modal 5.0 6.9
Structure 16.3 3.1
Lexical 18.8 21.4
WK 27.5 13.0
Table 3: Percentages of contradiction types in the
RTE3 dev dataset and the real contradiction corpus.
Neg-H: Bill hasn?t finished his math.
Para-Neg-H: Bill is still working on his math.
The rewriting in both the negated and the para-
phrased corpora is likely to leave one in the space of
?easy? contradictions and addresses fewer than 30%
of contradictions (table 3). We contacted the LCC
authors to obtain their datasets, but they were unable
to make them available to us. Thus, we simulated the
LCC negation corpus, adding negative markers to
the RTE2 test data (Neg test), and to a development
set (Neg dev) constructed by randomly sampling 50
pairs of entailments and 50 pairs of non-entailments
from the RTE2 development set.
Since the RTE datasets were constructed for tex-
tual inference, these corpora do not reflect ?real-life?
contradictions. We therefore collected contradic-
tions ?in the wild.? The resulting corpus contains
131 contradictory pairs: 19 from newswire, mainly
looking at related articles in Google News, 51 from
Wikipedia, 10 from the Lexis Nexis database, and
51 from the data prepared by LDC for the distillation
task of the DARPA GALE program. Despite the ran-
domness of the collection, we argue that this corpus
best reflects naturally occurring contradictions.2
Table 3 gives the distribution of contradiction
types for RTE3 dev and the real contradiction cor-
pus. Globally, we see that contradictions in category
(2) occur frequently and dominate the RTE develop-
ment set. In the real contradiction corpus, there is a
much higher rate of the negation, numeric and lex-
ical contradictions. This supports the intuition that
in the real world, contradictions primarily occur for
two reasons: information is updated as knowledge
2Our corpora?the simulation of the LLC negation corpus,
the RTE datasets and the real contradictions?are available at
http://nlp.stanford.edu/projects/contradiction.
1042
of an event is acquired over time (e.g., a rising death
toll) or various parties have divergent views of an
event (e.g., example 9 in table 1).
4 System overview
Our system is based on the stage architecture of the
Stanford RTE system (MacCartney et al, 2006), but
adds a stage for event coreference decision.
4.1 Linguistic analysis
The first stage computes linguistic representations
containing information about the semantic content
of the passages. The text and hypothesis are con-
verted to typed dependency graphs produced by
the Stanford parser (Klein and Manning, 2003; de
Marneffe et al, 2006). To improve the dependency
graph as a pseudo-semantic representation, colloca-
tions in WordNet and named entities are collapsed,
causing entities and multiword relations to become
single nodes.
4.2 Alignment between graphs
The second stage provides an alignment between
text and hypothesis graphs, consisting of a mapping
from each node in the hypothesis to a unique node
in the text or to null. The scoring measure uses
node similarity (irrespective of polarity) and struc-
tural information based on the dependency graphs.
Similarity measures and structural information are
combined via weights learned using the passive-
aggressive online learning algorithm MIRA (Cram-
mer and Singer, 2001). Alignment weights were
learned using manually annotated RTE development
sets (see Chambers et al, 2007).
4.3 Filtering non-coreferent events
Contradiction features are extracted based on mis-
matches between the text and hypothesis. Therefore,
we must first remove pairs of sentences which do not
describe the same event, and thus cannot be contra-
dictory to one another. In the following example, it
is necessary to recognize that Pluto?s moon is not the
same as the moon Titan; otherwise conflicting diam-
eters result in labeling the pair a contradiction.
T: Pluto?s moon, which is only about 25 miles in di-
ameter, was photographed 13 years ago.
H: The moon Titan has a diameter of 5100 kms.
This issue does not arise for textual entailment: el-
ements in the hypothesis not supported by the text
lead to non-entailment, regardless of whether the
same event is described. For contradiction, however,
it is critical to filter unrelated sentences to avoid
finding false evidence of contradiction when there
is contrasting information about different events.
Given the structure of RTE data, in which the
hypotheses are shorter and simpler than the texts,
one straightforward strategy for detecting coreferent
events is to check whether the root of the hypothesis
graph is aligned in the text graph. However, some
RTE hypotheses are testing systems? abilities to de-
tect relations between entities (e.g., John of IBM . . .
? John works for IBM). Thus, we do not filter verb
roots that are indicative of such relations. As shown
in table 4, this strategy improves results on RTE
data. For real world data, however, the assumption
of directionality made in this strategy is unfounded,
and we cannot assume that one sentence will be
short and the other more complex. Assuming two
sentences of comparable complexity, we hypothe-
size that modeling topicality could be used to assess
whether the sentences describe the same event.
There is a continuum of topicality from the start to
the end of a sentence (Firbas, 1971). We thus orig-
inally defined the topicality of an NP by nw where
n is the nth NP in the sentence. Additionally, we
accounted for multiple clauses by weighting each
clause equally; in example 4 in table 1, Australia
receives the same weight as Prime Minister because
each begins a clause. However, this weighting was
not supported empirically, and we thus use a sim-
pler, unweighted model. The topicality score of a
sentence is calculated as a normalized score across
all aligned NPs.3 The text and hypothesis are topi-
cally related if either sentence score is above a tuned
threshold. Modeling topicality provides an addi-
tional improvement in precision (table 4).
While filtering provides improvements in perfor-
mance, some examples of non-coreferent events are
still not filtered, such as:
T: Also Friday, five Iraqi soldiers were killed and nine
3Since dates can often be viewed as scene setting rather than
what the sentence is about, we ignore these in the model. How-
ever, ignoring or including dates in the model creates no signif-
icant differences in performance on RTE data.
1043
Strategy Precision Recall
No filter 55.10 32.93
Root 61.36 32.93
Root + topic 61.90 31.71
Table 4: Precision and recall for contradiction detection
on RTE3 dev using different filtering strategies.
wounded in a bombing, targeting their convoy near
Beiji, 150 miles north of Baghdad.
H: Three Iraqi soldiers also died Saturday when their
convoy was attacked by gunmen near Adhaim.
It seems that the real world frequency of events
needs to be taken into account. In this case, attacks
in Iraq are unfortunately frequent enough to assert
that it is unlikely that the two sentences present mis-
matching information (i.e., different location) about
the same event. But compare the following example:
T: President Kennedy was assassinated in Texas.
H: Kennedy?s murder occurred in Washington.
The two sentences refer to one unique event, and the
location mismatch renders them contradictory.
4.4 Extraction of contradiction features
In the final stage, we extract contradiction features
on which we apply logistic regression to classify the
pair as contradictory or not. The feature weights are
hand-set, guided by linguistic intuition.
5 Features for contradiction detection
In this section, we define each of the feature sets
used to capture salient patterns of contradiction.
Polarity features. Polarity difference between the
text and hypothesis is often a good indicator of con-
tradiction, provided there is a good alignment (see
example 2 in table 1). The polarity features cap-
ture the presence (or absence) of linguistic mark-
ers of negative polarity contexts. These markers are
scoped such that words are considered negated if
they have a negation dependency in the graph or are
an explicit linguistic marker of negation (e.g., sim-
ple negation (not), downward-monotone quantifiers
(no, few), or restricting prepositions). If one word is
negated and the other is not, we may have a polarity
difference. This difference is confirmed by checking
that the words are not antonyms and that they lack
unaligned prepositions or other context that suggests
they do not refer to the same thing. In some cases,
negations are propagated onto the governor, which
allows one to see that no bullet penetrated and a bul-
let did not penetrate have the same polarity.
Number, date and time features. Numeric mis-
matches can indicate contradiction (example 3
in table 1). The numeric features recognize
(mis-)matches between numbers, dates, and times.
We normalize date and time expressions, and rep-
resent numbers as ranges. This includes expression
matching (e.g., over 100 and 200 is not a mismatch).
Aligned numbers are marked as mismatches when
they are incompatible and surrounding words match
well, indicating the numbers refer to the same entity.
Antonymy features. Aligned antonyms are a very
good cue for contradiction. Our list of antonyms
and contrasting words comes from WordNet, from
which we extract words with direct antonymy links
and expand the list by adding words from the same
synset as the antonyms. We also use oppositional
verbs from VerbOcean. We check whether an
aligned pair of words appears in the list, as well as
checking for common antonym prefixes (e.g., anti,
un). The polarity of the context is used to determine
if the antonyms create a contradiction.
Structural features. These features aim to deter-
mine whether the syntactic structures of the text and
hypothesis create contradictory statements. For ex-
ample, we compare the subjects and objects for each
aligned verb. If the subject in the text overlaps with
the object in the hypothesis, we find evidence for a
contradiction. Consider example 6 in table 1. In the
text, the subject of succeed is Jacques Santer while
in the hypothesis, Santer is the object of succeed,
suggesting that the two sentences are incompatible.
Factivity features. The context in which a verb
phrase is embedded may give rise to contradiction,
as in example 5 (table 1). Negation influences some
factivity patterns: Bill forgot to take his wallet con-
tradicts Bill took his wallet while Bill did not forget
to take his wallet does not contradict Bill took his
wallet. For each text/hypothesis pair, we check the
(grand)parent of the text word aligned to the hypoth-
esis verb, and generate a feature based on its factiv-
1044
ity class. Factivity classes are formed by clustering
our expansion of the PARC lists of factive, implica-
tive and non-factive verbs (Nairn et al, 2006) ac-
cording to how they create contradiction.
Modality features. Simple patterns of modal rea-
soning are captured by mapping the text and hy-
pothesis to one of six modalities ((not )possible,
(not )actual, (not )necessary), according to the
presence of predefined modality markers such as
can or maybe. A feature is produced if the
text/hypothesis modality pair gives rise to a con-
tradiction. For instance, the following pair will
be mapped to the contradiction judgment (possible,
not possible):
T: The trial court may allow the prevailing party rea-
sonable attorney fees as part of costs.
H: The prevailing party may not recover attorney fees.
Relational features. A large proportion of the
RTE data is derived from information extraction
tasks where the hypothesis captures a relation be-
tween elements in the text. Using Semgrex, a pat-
tern matching language for dependency graphs, we
find such relations and ensure that the arguments be-
tween the text and the hypothesis match. In the fol-
lowing example, we detect that Fernandez works for
FEMA, and that because of the negation, a contra-
diction arises.
T: Fernandez, of FEMA, was on scene when Martin
arrived at a FEMA base camp.
H: Fernandez doesn?t work for FEMA.
Relational features provide accurate information but
are difficult to extend for broad coverage.
6 Results
Our contradiction detection system was developed
on all datasets listed in the first part of table 5. As
test sets, we used RTE1 test, the independently an-
notated RTE3 test, and Neg test. We focused on at-
taining high precision. In a real world setting, it is
likely that the contradiction rate is extremely low;
rather than overwhelming true positives with false
positives, rendering the system impractical, we mark
contradictions conservatively. We found reasonable
inter-annotator agreement between NIST and our
post-hoc annotation of RTE3 test (? = 0.81), show-
ing that, even with limited context, humans tend to
Precision Recall Accuracy
RTE1 dev1 70.37 40.43 ?
RTE1 dev2 72.41 38.18 ?
RTE2 dev 64.00 28.83 ?
RTE3 dev 61.90 31.71 ?
Neg dev 74.07 78.43 75.49
Neg test 62.97 62.50 62.74
LCC negation ? ? 75.63
RTE1 test 42.22 26.21 ?
RTE3 test 22.95 19.44 ?
Avg. RTE3 test 10.72 11.69 ?
Table 5: Precision and recall figures for contradiction de-
tection. Accuracy is given for balanced datasets only.
?LCC negation? refers to performance of Harabagiu et al
(2006); ?Avg. RTE3 test? refers to mean performance of
the 12 submissions to the RTE3 Pilot.
agree on contradictions.4 The results on the test sets
show that performance drops on new data, highlight-
ing the difficulty in generalizing from a small corpus
of positive contradiction examples, as well as under-
lining the complexity of building a broad coverage
system. This drop in accuracy on the test sets is
greater than that of many RTE systems, suggesting
that generalizing for contradiction is more difficult
than for entailment. Particularly when addressing
contradictions that require lexical and world knowl-
edge, we are only able to add coverage in a piece-
meal fashion, resulting in improved performance on
the development sets but only small gains for the
test sets. Thus, as shown in table 6, we achieve
13.3% recall on lexical contradictions in RTE3 dev
but are unable to identify any such contradictions in
RTE3 test. Additionally, we found that the preci-
sion of category (2) features was less than that of
category (1) features. Structural features, for exam-
ple, caused us to tag 36 non-contradictions as con-
tradictions in RTE3 test, over 75% of the precision
errors. Despite these issues, we achieve much higher
precision and recall than the average submission to
the RTE3 Pilot task on detecting contradictions, as
shown in the last two lines of table 5.
4This stands in contrast with the low inter-annotator agree-
ment reported by Sanchez-Graillet and Poesio (2007) for con-
tradictions in protein-protein interactions. The only hypothesis
we have to explain this contrast is the difficulty of scientific ma-
terial.
1045
Type RTE3 dev RTE3 test
1 Antonym 25.0 (3/12) 42.9 (3/7)
Negation 71.4 (5/7) 60.0 (3/5)
Numeric 71.4 (5/7) 28.6 (2/7)
2 Factive/Modal 25.0 (1/4) 10.0 (1/10)
Structure 46.2 (6/13) 21.1 (4/19)
Lexical 13.3 (2/15) 0.0 (0/12)
WK 18.2 (4/22) 8.3 (1/12)
Table 6: Recall by contradiction type.
7 Error analysis and discussion
One significant issue in contradiction detection is
lack of feature generalization. This problem is es-
pecially apparent for items in category (2) requiring
lexical and world knowledge, which proved to be
the most difficult contradictions to detect on a broad
scale. While we are able to find certain specific re-
lationships in the development sets, these features
attained only limited coverage. Many contradictions
in this category require multiple inferences and re-
main beyond our capabilities:
T: The Auburn High School Athletic Hall of Fame re-
cently introduced its Class of 2005 which includes
10 members.
H: The Auburn High School Athletic Hall of Fame has
ten members.
Of the types of contradictions in category (2), we are
best at addressing those formed via structural differ-
ences and factive/modal constructions as shown in
table 6. For instance, we detect examples 5 and 6 in
table 1. However, creating features with sufficient
precision is an issue for these types of contradic-
tions. Intuitively, two sentences that have aligned
verbs with the same subject and different objects (or
vice versa) are contradictory. This indeed indicates
a contradiction 55% of the time on our development
sets, but this is not high enough precision given the
rarity of contradictions.
Another type of contradiction where precision fal-
ters is numeric mismatch. We obtain high recall for
this type (table 6), as it is relatively simple to deter-
mine if two numbers are compatible, but high preci-
sion is difficult to achieve due to differences in what
numbers may mean. Consider:
T: Nike Inc. said that its profit grew 32 percent, as the
company posted broad gains in sales and orders.
H: Nike said orders for footwear totaled $4.9 billion,
including a 12 percent increase in U.S. orders.
Our system detects a mismatch between 32 percent
and 12 percent, ignoring the fact that one refers to
profit and the other to orders. Accounting for con-
text requires extensive text comprehension; it is not
enough to simply look at whether the two numbers
are headed by similar words (grew and increase).
This emphasizes the fact that mismatching informa-
tion is not sufficient to indicate contradiction.
As demonstrated by our 63% accuracy on
Neg test, we are reasonably good at detecting nega-
tion and correctly ascertaining whether it is a symp-
tom of contradiction. Similarly, we handle single
word antonymy with high precision (78.9%). Never-
theless, Harabagiu et al?s performance demonstrates
that further improvement on these types is possible;
indeed, they use more sophisticated techniques to
extract oppositional terms and detect polarity differ-
ences. Thus, detecting category (1) contradictions is
feasible with current systems.
While these contradictions are only a third of
those in the RTE datasets, detecting such contra-
dictions accurately would solve half of the prob-
lems found in the real corpus. This suggests that
we may be able to gain sufficient traction on contra-
diction detection for real world applications. Even
so, category (2) contradictions must be targeted to
detect many of the most interesting examples and to
solve the entire problem of contradiction detection.
Some types of these contradictions, such as lexi-
cal and world knowledge, are currently beyond our
grasp, but we have demonstrated that progress may
be made on the structure and factive/modal types.
Despite being rare, contradiction is foundational
in text comprehension. Our detailed investigation
demonstrates which aspects of it can be resolved and
where further research must be directed.
Acknowledgments
This paper is based on work funded in part by
the Defense Advanced Research Projects Agency
through IBM and by the Disruptive Technology
Office (DTO) Phase III Program for Advanced
Question Answering for Intelligence (AQUAINT)
through Broad Agency Announcement (BAA)
N61339-06-R-0034.
1046
References
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo
Giampiccolo, Bernardo Magnini, and Idan Szpektor.
2006. The second PASCAL recognising textual en-
tailment challenge. In Proceedings of the Second
PASCAL Challenges Workshop on Recognising Tex-
tual Entailment, Venice, Italy.
Nathanael Chambers, Daniel Cer, Trond Grenager,
David Hall, Chloe Kiddon, Bill MacCartney, Marie-
Catherine de Marneffe, Daniel Ramage, Eric Yeh, and
Christopher D. Manning. 2007. Learning alignments
and leveraging natural logic. In Proceedings of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing.
Timothy Chklovski and Patrick Pantel. 2004. Verbo-
cean: Mining the web for fine-grained semantic verb
relations. In Proceedings of EMNLP-04.
Cleo Condoravdi, Dick Crouch, Valeria de Pavia, Rein-
hard Stolle, and Daniel G. Bobrow. 2003. Entailment,
intensionality and text understanding. Workshop on
Text Meaning (2003 May 31).
Koby Crammer and Yoram Singer. 2001. Ultraconser-
vative online algorithms for multiclass problems. In
Proceedings of COLT-2001.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL recognising textual entailment
challenge. In Quinonero-Candela et al, editor, MLCW
2005, LNAI Volume 3944, pages 177?190. Springer-
Verlag.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of the 5th International Conference on Lan-
guage Resources and Evaluation (LREC-06).
Christiane Fellbaum. 1998. WordNet: an electronic lexi-
cal database. MIT Press.
Jan Firbas. 1971. On the concept of communicative dy-
namism in the theory of functional sentence perspec-
tive. Brno Studies in English, 7:23?47.
Danilo Giampiccolo, Ido Dagan, Bernardo Magnini, and
Bill Dolan. 2007. The third PASCAL recognizing tex-
tual entailment challenge. In Proceedings of the ACL-
PASCAL Workshop on Textual Entailment and Para-
phrasing.
Sanda Harabagiu, Andrew Hickl, and Finley Lacatusu.
2006. Negation, contrast, and contradiction in text
processing. In Proceedings of the Twenty-First Na-
tional Conference on Artificial Intelligence (AAAI-06).
Andrew Hickl, John Williams, Jeremy Bensley, Kirk
Roberts, Bryan Rink, and Ying Shi. 2006. Recog-
nizing textual entailment with LCC?s GROUNDHOG
system. In Proceedings of the Second PASCAL Chal-
lenges Workshop on Recognising Textual Entailment.
Kevin Humphreys, Robert Gaizauskas, and Saliha Az-
zam. 1997. Event coreference for information extrac-
tion. In Proceedings of the Workshop on Operational
Factors in Pratical, Robust Anaphora Resolution for
Unrestricted Texts, 35th ACL meeting.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Annual Meeting of the Association of Computational
Linguistics.
Bill MacCartney, Trond Grenager, Marie-Catherine de
Marneffe, Daniel Cer, and Christopher D. Manning.
2006. Learning to recognize features of valid textual
entailments. In Proceedings of the North American
Association of Computational Linguistics (NAACL-
06).
Daniel Marcu and Abdessamad Echihabi. 2002. An
unsupervised approach to recognizing discourse rela-
tions. In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics.
Rowan Nairn, Cleo Condoravdi, and Lauri Karttunen.
2006. Computing relative polarity for textual infer-
ence. In Proceedings of ICoS-5.
Olivia Sanchez-Graillet and Massimo Poesio. 2007. Dis-
covering contradiction protein-protein interactions in
text. In Proceedings of BioNLP 2007: Biological,
translational, and clinical language processing.
Lucy Vanderwende, Arul Menezes, and Rion Snow.
2006. Microsoft research at rte-2: Syntactic contri-
butions in the entailment task: an implementation. In
Proceedings of the Second PASCAL Challenges Work-
shop on Recognising Textual Entailment.
Ellen Voorhees. 2008. Contradictions and justifications:
Extensions to the textual entailment task. In Proceed-
ings of the 46th Annual Meeting of the Association for
Computational Linguistics.
Annie Zaenen, Lauri Karttunen, and Richard S. Crouch.
2005. Local textual inference: can it be defined or
circumscribed? In ACL 2005 Workshop on Empirical
Modeling of Semantic Equivalence and Entailment.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Alessandro Moschitti. 2007. Shallow semantics in
fast textual entailment rule learners. In Proceedings
of the ACL-PASCAL Workshop on Textual Entailment
and Paraphrasing.
1047
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 165?170,
Prague, June 2007. c?2007 Association for Computational Linguistics
Learning Alignments and Leveraging Natural Logic
Nathanael Chambers, Daniel Cer, Trond Grenager, David Hall, Chloe Kiddon
Bill MacCartney, Marie-Catherine de Marneffe, Daniel Ramage
Eric Yeh, Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305
{natec,dcer,grenager,dlwh,loeki,wcmac,mcdm,dramage,yeh1,manning}@stanford.edu
Abstract
We describe an approach to textual infer-
ence that improves alignments at both the
typed dependency level and at a deeper se-
mantic level. We present a machine learning
approach to alignment scoring, a stochas-
tic search procedure, and a new tool that
finds deeper semantic alignments, allowing
rapid development of semantic features over
the aligned graphs. Further, we describe a
complementary semantic component based
on natural logic, which shows an added gain
of 3.13% accuracy on the RTE3 test set.
1 Introduction
Among the many approaches to textual inference,
alignment of dependency graphs has shown utility
in determining entailment without the use of deep
understanding. However, discovering alignments
requires a scoring function that accurately scores
alignment and a search procedure capable of approx-
imating the optimal mapping within a large search
space. We address the former requirement through
a machine learning approach for acquiring lexical
feature weights, and we address the latter with an
approximate stochastic approach to search.
Unfortunately, the most accurate aligner can-
not capture deeper semantic relations between two
pieces of text. For this, we have developed a tool,
Semgrex, that allows the rapid development of de-
pendency rules to find specific entailments, such as
familial or locative relations, a common occurence
in textual entailment data. Instead of writing code by
hand to capture patterns in the dependency graphs,
we develop a separate rule-base that operates over
aligned dependency graphs. Further, we describe a
separate natural logic component that complements
our textual inference system, making local entail-
ment decisions based on monotonic assumptions.
The next section gives a brief overview of the sys-
tem architecture, followed by our proposal for im-
proving alignment scoring and search. New coref-
erence features and the Semgrex tool are then de-
scribed, followed by a description of natural logic.
2 System Overview
Our system is a three stage architecture that con-
ducts linguistic analysis, builds an alignment be-
tween dependency graphs of the text and hypothesis,
and performs inference to determine entailment.
Linguistic analysis identifies semantic entities, re-
lationships, and structure within the given text and
hypothesis. Typed dependency graphs are passed
to the aligner, as well as lexical features such as
named entities, synonymity, part of speech, etc. The
alignment stage then performs dependency graph
alignment between the hypothesis and text graphs,
searching the space of possible alignments for the
highest scoring alignment. Improvements to the
scorer, search algorithm, and automatically learned
weights are described in the next section.
The final inference stage determines if the hy-
pothesis is entailed by the text. We construct a set
of features from the previous stages ranging from
antonyms and polarity to graph structure and seman-
tic relations. Each feature is weighted according to a
set of hand-crafted or machine-learned weights over
165
the development dataset. We do not describe the fea-
tures here; the reader is referred to de Marneffe et al
(2006a) for more details. A novel component that
leverages natural logic is also used to make the final
entailment decisions, described in section 6.
3 Alignment Model
We examine three tasks undertaken to improve the
alignment phase: (1) the construction of manu-
ally aligned data which enables automatic learning
of alignment models, and effectively decouples the
alignment and inference development efforts, (2) the
development of new search procedures for finding
high-quality alignments, and (3) the use of machine
learning techniques to automatically learn the pa-
rameters of alignment scoring models.
3.1 Manual Alignment Annotation
While work such as Raina et al (2005) has tried
to learn feature alignment weights by credit assign-
ment backward from whether an item is answered
correctly, this can be very difficult, and here we fol-
low Hickl et al (2006) in using supervised gold-
standard alignments, which help us to evaluate and
improve alignment and inference independently.
We built a web-based tool that allows annotators
to mark semantic relationships between text and hy-
pothesis words. A table with the hypothesis words
on one axis and the text on the other allows re-
lationships to be marked in the corresponding ta-
ble cell with one of four options. These relation-
ships include text to hypothesis entailment, hypothe-
sis to text entailment, synonymy, and antonymy. Ex-
amples of entailment (from the RTE 2005 dataset)
include pairs such as drinking/consumption, coro-
navirus/virus, and Royal Navy/British. By distin-
guishing between these different types of align-
ments, we can capture some limited semantics in the
alignment process, but full exploitation of this infor-
mation is left to future work.
We annotated the complete RTE2 dev and
RTE3 dev datasets, for a total of 1600 aligned
text/hypothesis pairs (the data is available at
http://nlp.stanford.edu/projects/rte/).
3.2 Improving Alignment Search
In order to find ?good? alignments, we define both a
formal model for scoring the quality of a proposed
alignment and a search procedure over the alignment
space. Our goal is to build a model that maximizes
the total alignment score of the full datasetD, which
we take to be the sum of the alignment scores for all
individual text/hypothesis pairs (t, h).
Each of the text and hypothesis is a semantic de-
pendency graph; n(h) is the set of nodes (words)
and e(h) is the set of edges (grammatical relations)
in a hypothesis h. An alignment a : n(h) 7? n(t) ?
{null} maps each hypothesis word to a text word
or to a null symbol, much like an IBM-style ma-
chine translation model. We assume that the align-
ment score s(t, h, a) is the sum of two terms, the first
scoring aligned word pairs and the second the match
between an edge between two words in the hypoth-
esis graph and the corresponding path between the
words in the text graph. Each of these is a sum, over
the scoring function for individual word pairs sw and
the scoring function for edge path pairs se:
s(t, h, a) =
?
hi?n(h)
sw(hi, a(hi))
+
?
(hi,hj)?e(h)
se((hi, hj), (a(hi), a(hj)))
The space of alignments for a hypothesis with m
words and a text with n words contains (n + 1)m
possible alignments, making exhaustive search in-
tractable. However, since the bulk of the alignment
score depends on local factors, we have explored
several search strategies and found that stochastic
local search produces better quality solutions.
Stochastic search is inspired by Gibbs sampling
and operates on a complete state formulation of the
search problem. We initialize the algorithm with the
complete alignment that maximizes the greedy word
pair scores. Then, in each step of the search, we
seek to randomly replace an alignment for a single
hypothesis word hi. For each possible text word tj
(including null), we compute the alignment score if
we were to align hi with tj . Treating these scores as
log probabilities, we create a normalized distribution
from which we sample one alignment. This Gibbs
sampler is guaranteed to give us samples from the
posterior distribution over alignments defined im-
plicitly by the scoring function. As we wish to find a
maximum of the function, we use simulated anneal-
ing by including a temperature parameter to smooth
166
the sampling distribution as a function of time. This
allows us to initially explore the space widely, but
later to converge to a local maximum which is hope-
fully the global maximum.
3.3 Learning Alignment Models
Last year, we manually defined the alignment scor-
ing function (de Marneffe et al, 2006a). However,
the existence of the gold standard alignments de-
scribed in section 3.1 enables the automatic learning
of a scoring function. For both the word and edge
scorers, we choose a linear model where the score is
the dot product of a feature and a weight vector:
sw(hi, tj) = ?w ? f(hi, tj), and
se((hi, hj), (tk, t`)) = ?e ? f((hi, hj), (tk, t`)).
Recent results in machine learning show the ef-
fectiveness of online learning algorithms for struc-
ture prediction tasks. Online algorithms update their
model at each iteration step over the training set. For
each datum, they use the current weight vector to
make a prediction which is compared to the correct
label. The weight vector is updated as a function
of the difference. We compared two different up-
date rules: the perceptron update and the MIRA up-
date. In the perceptron update, for an incorrect pre-
diction, the weight vector is modified by adding a
multiple of the difference between the feature vector
of the correct label and the feature vector of the pre-
dicted label. We use the adaptation of this algorithm
to structure prediction, first proposed by (Collins,
2002). TheMIRA update is a proposed improvement
that attempts to make the minimal modification to
the weight vector such that the score of the incorrect
prediction for the example is lower than the score of
the correct label (Crammer and Singer, 2001).
We compare the performance of the perceptron
and MIRA algorithms on 10-fold cross-validation
on the RTE2 dev dataset. Both algorithms improve
with each pass over the dataset. Most improve-
ment is within the first five passes. Table 1 shows
runs for both algorithms over 10 passes through the
dataset. MIRA consistently outperforms perceptron
learning. Moreover, scoring alignments based on the
learned weights marginally outperforms our hand-
constructed scoring function by 1.7% absolute.
A puzzling problem is that our overall per-
formance decreased 0.87% with the addition of
Perfectly aligned
Individual words Text/hypothesis pairs
Perceptron 4675 271
MIRA 4775 283
Table 1: Perceptron and MIRA results on 10-fold cross-
validation on RTE2 dev for 10 passes.
RTE3 dev alignment data. We believe this is due
to a larger proportion of ?irrelevant? and ?relation?
pairs. Irrelevant pairs are those where the text and
hypothesis are completely unrelated. Relation pairs
are those where the correct entailment judgment re-
lies on the extraction of relations such as X works
for Y, X is located in Y, or X is the wife of Y. Both
of these categories do not rely on alignments for en-
tailment decisions, and hence introduce noise.
4 Coreference
In RTE3, 135 pairs in RTE3 dev and 117 in
RTE3 test have lengths classified as ?long,? with
642 personal pronouns identified in RTE3 dev and
504 in RTE3 test. These numbers suggest that re-
solving pronomial anaphora plays an important role
in making good entailment decisions. For exam-
ple, identifying the first ?he? as referring to ?Yunus?
in this pair from RTE3 dev can help alignment and
other system features.
P: Yunus, who shared the 1.4 million prize Friday with the
Grameen Bank that he founded 30 years ago, pioneered the con-
cept of ?microcredit.?
H: Yunus founded the Grameen Bank 30 years ago.
Indeed, 52 of the first 200 pairs from RTE3 dev
were deemed by a human evaluator to rely on ref-
erence information. We used the OpenNLP1 pack-
age?s maximum-entropy coreference utility to per-
form resolution on parse trees and named-entity data
from our system. Found relations are stored and
used by the alignment stage for word similarity.
We evaluated our system with and without coref-
erence over RTE3 dev and RTE3 test. Results are
shown in Table 3. The presence of reference infor-
mation helped, approaching significance on the de-
velopment set (p < 0.1, McNemar?s test, 2-tailed),
but not on the test set. Examination of alignments
and features between the two runs shows that the
alignments do not differ significantly, but associated
1http://opennlp.sourceforge.net/
167
weights do, thus affecting entailment threshold tun-
ing. We believe coreference needs to be integrated
into all the featurizers and lexical resources, rather
than only with word matching, in order to make fur-
ther gains.
5 Semgrex Language
A core part of an entailment system is the ability to
find semantically equivalent patterns in text. Pre-
viously, we wrote tedious graph traversal code by
hand for each desired pattern. As a remedy, we
wrote Semgrex, a pattern language for dependency
graphs. We use Semgrex atop the typed dependen-
cies from the Stanford Parser (de Marneffe et al,
2006b), as aligned in the alignment phase, to iden-
tify both semantic patterns in a single text and over
two aligned pieces of text. The syntax of the lan-
guage was modeled after tgrep/Tregex, query lan-
guages used to find syntactic patterns in trees (Levy
and Andrew, 2006). This speeds up the process of
graph search and reduces errors that occur in com-
plicated traversal code.
5.1 Semgrex Features
Rather than providing regular expression match-
ing of atomic tree labels, as in most tree pattern
languages, Semgrex represents nodes as a (non-
recursive) attribute-value matrix. It then uses regular
expressions for subsets of attribute values. For ex-
ample, {word:run;tag:/?NN/} refers to any
node that has a value run for the attribute word and
a tag that starts with NN, while {} refers to any node
in the graph.
However, the most important part of Semgrex is
that it allows you to specify relations between nodes.
For example, {} <nsubj {} finds all the depen-
dents of nsubj relations. Logical connectives can
be used to form more complex patterns and node
naming can help retrieve matched nodes from the
patterns. Four base relations, shown in figure 1, al-
low you to specify the type of relation between two
nodes, in addition to an alignment relation (@) be-
tween two graphs.
5.2 Entailment Patterns
A particularly useful application of Semgrex is to
create relation entailment patterns. In particular, the
IE subtask of RTE has many characteristics that are
Semgrex Relations
Symbol #Description
{A} >reln {B} A is the governor of a reln relation
with B
{A} <reln {B} A is the dependent of a reln relation
with B
{A} >>reln {B} A dominates a node that is the
governor of a reln relation with B
{A} <<reln {B} A is the dependent of a node that is
dominated by B
{A} @ {B} A aligns to B
Figure 1: Semgrex relations between nodes.
not well suited to the core alignment features of our
system. We began integrating Semgrex into our sys-
tem by creating semantic alignment rules for these
IE tasks.
T: Bill Clinton?s wife Hillary was in Wichita today, continuing
her campaign.
H: Bill Clinton is married to Hillary. (TRUE)
Pattern:
({}=1
<nsubjpass ({word:married} >pp to {}=2))
@ ({} >poss ({lemma:/wife/} >appos {}=3))
This is a simplified version of a pattern that looks
for marriage relations. If it matches, additional pro-
grammatic checks ensure that the nodes labeled 2
and 3 are either aligned or coreferent. If they are,
then we add a MATCH feature, otherwise we add a
MISMATCH. Patterns included other familial rela-
tions and employer-employee relations. These pat-
terns serve both as a necessary component of an IE
entailment system and as a test drive of Semgrex.
5.3 Range of Application
Our rules for marriage relations correctly matched
six examples in the RTE3 development set and one
in the test set. Due to our system?s weaker per-
formance on the IE subtask of the data, we ana-
lyzed 200 examples in the development set for Sem-
grex applicability. We identified several relational
classes, including the following:
? Work: works for, holds the position of
? Location: lives in, is located in
? Relative: wife/husband of, are relatives
? Membership: is an employee of, is part of
? Business: is a partner of, owns
? Base: is based in, headquarters in
These relations make up at least 7% of the data, sug-
gesting utility from capturing other relations.
168
6 Natural Logic
We developed a computational model of natural
logic, the NatLog system, as another inference en-
gine for our RTE system. NatLog complements our
core broad-coverage system by trading lower recall
for higher precision, similar to (Bos and Markert,
2006). Natural logic avoids difficulties with translat-
ing natural language into first-order logic (FOL) by
forgoing logical notation and model theory in favor
of natural language. Proofs are expressed as incre-
mental edits to natural language expressions. Edits
represent conceptual contractions and expansions,
with truth preservation specified natural logic. For
further details, we refer the reader to (Sa?nchez Va-
lencia, 1995).
We define an entailment relation v between
nouns (hammer v tool), adjectives (deafening v
loud), verbs (sprint v run), modifiers, connectives
and quantifiers. In ordinary (upward-monotone)
contexts, the entailment relation between compound
expressions mirrors the entailment relations be-
tween their parts. Thus tango in Paris v dance
in France, since tango v dance and in Paris v in
France. However, many linguistic constructions cre-
ate downward-monotone contexts, including nega-
tion (didn?t sing v didn?t yodel), restrictive quanti-
fiers (few beetles v few insects) and many others.
NatLog uses a three-stage architecture, compris-
ing linguistic pre-processing, alignment, and entail-
ment classification. In pre-processing, we define a
list of expressions that affect monotonicity, and de-
fine Tregex patterns that recognize each occurrence
and its scope. This monotonicity marking can cor-
rectly account for multiple monotonicity inversions,
as in no soldier without a uniform, and marks each
token span with its final effective monotonicity.
In the second stage, word alignments from our
RTE system are represented as a sequence of atomic
edits over token spans, as entailment relations
are described across incremental edits in NatLog.
Aligned pairs generate substitution edits, unaligned
premise words yield deletion edits, and unaligned
hypothesis words yield insertion edits. Where pos-
sible, contiguous sequences of word-level edits are
collected into span edits.
In the final stage, we use a decision-tree classi-
fier to predict the elementary entailment relation (ta-
relation symbol in terms of v RTE
equivalent p = h p v h, h v p yes
forward p < h p v h, h 6v p yes
reverse p = h h v p, p 6v h no
independent p # h p 6v h, h 6v p no
exclusive p | h p v ?h, h v ?p no
Table 2: NatLog?s five elementary entailment relations. The last
column indicates correspondences to RTE answers.
ble 2) for each atomic edit. Edit features include
the type, effective monotonicity at affected tokens,
and their lexical features, including syntactic cate-
gory, lemma similarity, and WordNet-derived mea-
sures of synonymy, hyponymy, and antonymy. The
classifier was trained on a set of 69 problems de-
signed to exercise the feature space, learning heuris-
tics such as deletion in an upward-monotone context
yields<, substitution of a hypernym in a downward-
monotone context yields =, and substitution of an
antonym yields |.
To produce a top-level entailment judgment, the
atomic entailment predictions associated with each
edit are composed in a fairly obvious way. If r is any
entailment relation, then (= ? r) ? r, but (# ? r) ?
#. < and= are transitive, but (< ? =) ? #, and so
on.
We do not expect NatLog to be a general-purpose
solution for RTE problems. Many problems depend
on types of inference that it does not address, such
as paraphrase or relation extraction. Most pairs have
large edit distances, and more atomic edits means
a greater chance of errors propagating to the final
output: given the entailment composition rules, the
system can answer yes only if all atomic-level pre-
dictions are either< or =. Instead, we hope to make
reliable predictions on a subset of the RTE problems.
Table 3 shows NatLog performance on RTE3. It
makes positive predictions on few problems (18%
on development set, 24% on test), but achieves good
precision relative to our RTE system (76% and 68%,
respectively). For comparison, the FOL-based sys-
tem reported in (Bos and Markert, 2006) attained a
precision of 76% on RTE2, but made a positive pre-
diction in only 4% of cases. This high precision sug-
gests that superior performance can be achieved by
hybridizing NatLog with our core RTE system.
The reader is referred to (MacCartney and Man-
169
ID Premise(s) Hypothesis Answer
518 The French railway company SNCF is cooperating in
the project.
The French railway company is called SNCF. yes
601 NUCOR has pioneered a giant mini-mill in which steel
is poured into continuous casting machines.
Nucor has pioneered the first mini-mill. no
Table 4: Illustrative examples from the RTE3 test suite
RTE3 Development Set (800 problems)
System % yes precision recall accuracy
Core +coref 50.25 68.66 66.99 67.25
Core -coref 49.88 66.42 64.32 64.88
NatLog 18.00 76.39 26.70 58.00
Hybrid, bal. 50.00 69.75 67.72 68.25
Hybrid, opt. 55.13 69.16 74.03 69.63
RTE3 Test Set (800 problems)
System % yes precision recall accuracy
Core +coref 50.00 61.75 60.24 60.50
Core -coref 50.00 60.25 58.78 59.00
NatLog 23.88 68.06 31.71 57.38
Hybrid, bal. 50.00 64.50 62.93 63.25
Hybrid, opt. 54.13 63.74 67.32 63.62
Table 3: Performance on the RTE3 development and test sets.
% yes indicates the proportion of yes predictions made by the
system. Precision and recall are shown for the yes label.
ning, 2007) for more details on NatLog.
7 System Results
Our core systemmakes yes/no predictions by thresh-
olding a real-valued inference score. To construct
a hybrid system, we adjust the inference score by
+x if NatLog predicts yes, ?x otherwise. x is cho-
sen by optimizing development set accuracy when
adjusting the threshold to generate balanced predic-
tions (equal numbers of yes and no). As another
experiment, we fix x at this value and adjust the
threshold to optimize development set accuracy, re-
sulting in an excess of yes predictions. Results for
these two cases are shown in Table 3. Parameter
values tuned on development data yielded the best
performance. The optimized hybrid system attained
an absolute accuracy gain of 3.12% over our RTE
system, corresponding to an extra 25 problems an-
swered correctly. This result is statistically signifi-
cant (p < 0.01, McNemar?s test, 2-tailed).
The gain cannot be fully attributed to NatLog?s
success in handling the kind of inferences about
monotonicity which are the staple of natural logic.
Indeed, such inferences are quite rare in the RTE
data. Rather, NatLog seems to have gained primarily
by being more precise. In some cases, this precision
works against it: NatLog answers no to problem 518
(table 4) because it cannot account for the insertion
of called. On the other hand, it correctly rejects the
hypothesis in problem 601 because it cannot account
for the insertion of first, whereas the less-precise
core system was happy to allow it.
Acknowledgements
This material is based upon work supported in
part by the Disruptive Technology Office (DTO)?s
AQUAINT Phase III Program.
References
Johan Bos and Katja Markert. 2006. When logical inference
helps determining textual entailment (and when it doesn?t).
In Proceedings of the Second PASCAL RTE Challenge.
Michael Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with per-
ceptron algorithms. In Proceedings of EMNLP-2002.
Koby Crammer and Yoram Singer. 2001. Ultraconservative
online algorithms for multiclass problems. In Proceedings
of COLT-2001.
Marie-Catherine de Marneffe, Bill MacCartney, Trond Grena-
ger, Daniel Cer, Anna Rafferty, and Christopher D. Manning.
2006a. Learning to distinguish valid textual entailments. In
Second Pascal RTE Challenge Workshop.
Marie-Catherine de Marneffe, Bill MacCartney, and Christo-
pher D. Manning. 2006b. Generating typed dependency
parses from phrase structure parses. In 5th Int. Conference
on Language Resources and Evaluation (LREC 2006).
Andrew Hickl, John Williams, Jeremy Bensley, Kirk Roberts,
Bryan Rink, and Ying Shi. 2006. Recognizing textual entail-
ment with LCC?s GROUNDHOG system. In Proceedings of
the Second PASCAL RTE Challenge.
Roger Levy and Galen Andrew. 2006. Tregex and Tsurgeon:
tools for querying and manipulating tree data structures. In
Proceedings of the Fifth International Conference on Lan-
guage Resources and Evaluation.
Bill MacCartney and Christopher D. Manning. 2007. Natu-
ral logic for textual inference. In ACL Workshop on Textual
Entailment and Paraphrasing.
Rajat Raina, Andrew Y. Ng, and Christopher D. Manning. 2005.
Robust textual inference via learning and abductive reason-
ing. In AAAI 2005, pages 1099?1105.
Victor Sa?nchez Valencia. 1995. Parsing-driven inference: Nat-
ural logic. Linguistic Analysis, 25:258?285.
170
Proceedings of the 2009 Workshop on Applied Textual Inference, ACL-IJCNLP 2009, pages 1?9,
Suntec, Singapore, 6 August 2009.
c?2009 ACL and AFNLP
Multi-word expressions in textual inference: Much ado about nothing?
Marie-Catherine de Marneffe
Linguistics Department
Stanford University
Stanford, CA
mcdm@stanford.edu
Sebastian Pad
?
o
Institut f?ur Maschinelle
Sprachverarbeitung
Stuttgart University, Germany
pado@ims.uni-stuttgart.de
Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA
manning@stanford.edu
Abstract
Multi-word expressions (MWE) have seen much at-
tention from the NLP community. In this paper, we
investigate their impact on the recognition of tex-
tual entailment (RTE). Using the manual Microsoft
Research annotations, we first manually count and
classify MWEs in RTE data. We find few, most
of which are arguably unlikely to cause processing
problems. We then consider the impact of MWEs on
a current RTE system. We are unable to confirm that
entailment recognition suffers from wrongly aligned
MWEs. In addition, MWE alignment is difficult
to improve, since MWEs are poorly represented in
state-of-the-art paraphrase resources, the only avail-
able sources for multi-word similarities. We con-
clude that RTE should concentrate on other phe-
nomena impacting entailment, and that paraphrase
knowledge is best understood as capturing general
lexico-syntactic variation.
1 Introduction
Multi-word expressions (MWEs) can be defined as
?idiosyncratic interpretations that cross word bound-
aries?, such as traffic light or kick the bucket. Called
a ?pain in the neck for NLP?, they have received
considerable attention in recent years and it has
been suggested that proper treatment could make
a significant difference in various NLP tasks (Sag
et al, 2002). The importance attributed to them is
also reflected in a number of workshops (Bond et
al., 2003; Tanaka et al, 2004; Moir?on et al, 2006;
Gr?egoire et al, 2007). However, there are few de-
tailed breakdowns of the benefits that improved
MWE handling provides to applications.
This paper investigates the impact of MWEs
on the ?recognition of textual entailment? (RTE)
task (Dagan et al, 2006). Our analysis ties in with
the pivotal question of what types of knowledge
are beneficial for RTE. A number of papers have
suggested that paraphrase knowledge plays a very
important role (Bar-Haim et al, 2005; Marsi et al,
2007; Dinu and Wang, 2009). For example, Bar-
Haim et al (2005) conclude: ?Our analysis also
shows that paraphrases stand out as a dominant
contributor to the entailment task.?
The term ?paraphrase? is however often con-
strued broadly. In Bar-Haim et al (2005), it refers
to the ability of relating lexico-syntactic reformula-
tions such as diathesis alternations, passivizations,
or symmetrical predicates (X lent his BMW to Y/Y
borrowed X?s BMW). If ?paraphrase? simply refers
to the use of a language?s lexical and syntactic
possibilities to express equivalent meaning in dif-
ferent ways, then paraphrases are certainly impor-
tant to RTE. But such a claim means little more
than that RTE can profit from good understand-
ing of syntax and semantics. However, given the
abovementioned interest in MWEs, there is another
possibility: does success in RTE involve proper
handling of MWEs, such as knowing that take a
pass on is equivalent to aren?t purchasing, or kicked
the bucket to died? This seems not too far-fetched:
Knowledge about MWEs is under-represented in
existing semantic resources like WordNet or dis-
tributional thesauri, but should be present in para-
phrase resources, which provide similarity judg-
ments between phrase pairs, including MWEs.
The goal of our study is to investigate the merits
of this second, more precise, hypothesis, measur-
ing the impact of MWE processing on RTE. In
the absence of a universally accepted definition
of MWEs, we define MWEs in the RTE setting
as multi-word alignments, i.e., words that partici-
pate in more than one word alignment link between
premise and hypothesis:
(1)
PRE: He died.
HYP: He kicked the bucket.
The exclusion of MWEs that do not lead to multi-
word alignments (i.e., which can be aligned word
by word) is not a significant loss, since these cases
are unlikely to cause significant problems for RTE.
In addition, an alignment-based approach has the
advantage of generality: Almost all existing RTE
models align the linguistic material of the premise
1
and hypothesis and base at least part of their de-
cision on properties of this alignment (Burchardt
et al, 2007; Hickl and Bensley, 2007; Iftene and
Balahur-Dobrescu, 2007; Zanzotto et al, 2007).
We proceed in three steps. First, we analyze
the Microsoft Research (MSR) manual word align-
ments (Brockett, 2007) for the RTE2 dataset (Bar-
Haim et al, 2006), shedding light on the rela-
tionship between alignments and multi-word ex-
pressions. We provide frequency estimates and
a coarse-grained classification scheme for multi-
word expressions on textual entailment data. Next,
we analyze two widely used types of paraphrase
resources with respect to their modeling of MWEs.
Finally, we investigate the impact of MWEs and
their handling on practical entailment recognition.
2 Multi-Word Expressions in Alignment
Almost all textual entailment recognition models
incorporate an alignment procedure that establishes
correspondences between the premise and the hy-
pothesis. The computation of word alignments
is usually phrased as an optimization task. The
search space is based on lexical similarities, but
usually extended with structural biases in order to
obtain alignments with desirable properties, such
as the contiguous alignment of adjacent words, or
the mapping of different source words on to differ-
ent target words. One prominent constraint of the
IBM word alignment models (Brown et al, 1993)
is functional alignment, that is each target word
is mapped onto at most one source word. Other
models produce only one-to-one alignments, where
both alignment directions must be functional.
MWEs that involve many-to-many or one-to-
many alignments like Ex. (1) present a problem
for such constrained word alignment models. A
functional alignment model can still handle cases
like Ex. (1) correctly in one direction (from bottom
to top), but not in the other one. One-to-one align-
ments manage neither. Various workarounds have
been proposed in the MT literature, such as comput-
ing word alignments in both directions and forming
the union or intersection. Even if an alignment is
technically within the search space, accurate knowl-
edge about plausible phrasal matches is necessary
for it to be assigned a high score and thus identified.
3 MWEs in the RTE2 Dataset
In the first part of our study, we estimate the extent
to which the inability of aligners to model one-to-
CARDINALITY
M-to-M 1-to-M
DECOM- yes (1) (3)
POSABLE? no (2) (4)
OTHER (5), (6), (7)
Table 1: MWEs categories and definition criteria
(M-to-M: many-to-many; 1-to-M: one-to-many).
many and many-to-many correspondences is an
issue. To do so, we use the Microsoft Research
manual alignments for the RTE2 data. To date, the
MSR data constitutes the only gold standard align-
ment corpus publicly available. Since annotators
were not constrained to use one-to-one alignments,
we assume that the MSR alignments contain multi-
word alignments where appropriate.
From the MSR data, we extract all multi-word
alignments that fall outside the scope of ?func-
tional? alignments, i.e., alignments of the form
?many-to-many? or ?one-to-many? (in the direction
hypothesis-premise). We annotate them according
to the categories defined below. The MSR data
distinguishes between SURE and POSSIBLE align-
ments. We only take the SURE alignments into
account. While this might mean missing some
multi-word alignments, we found many ?possible?
links to be motivated by the desire to obtain a high-
coverage alignment, as Ex. 2 shows:
(2)
PRE: ECB spokeswoman, Regina Schueller, ...
HYP: Regina Schueller ...
Here, the hypothesis words ?Regina Schueller? are
individually ?sure?-aligned to the premise words
?Regina Schueller? (solid lines), but are also both
?possible?-linked to ?ECB spokeswoman? (dashed
lines). This ?possible? alignment can be motivated
on syntactic or referential grounds, but does not
indicate a correspondence in meaning (as opposed
to reference).
3.1 Analysis of Multi-Word Expressions
Table 1 shows the seven categories we define to
distinguish the different types of multi-word align-
ments. We use two main complementary criteria
for our annotation. The first one is the cardinality
of the alignment: does it involve phrases proper
on both sides (many-to-many), or just on one side
(one-to-many)? The second one is decomposabil-
ity: is it possible to create one or more one-to-one
alignments that capture the main semantic contribu-
tion of the multi-word alignment? Our motivation
2
for introducing this criterion is that even aligners
that are unable to recover the complete MWE have
a chance to identify the links crucial for entailment
if the MWE is decomposable (categories (1) and
(3)). This is not possible for the more difficult
non-decomposable categories (2) and (4). The re-
maining categories, (5) to (7), involve auxiliaries,
multiple mentions, and named entities, which are
not MWEs in the narrow sense. We will henceforth
use the term ?true MWEs? to refer to categories
(1)?(4), as opposed to (5)?(7).
The criteria we use for MWE categorization are
different from the ones adopted by Sag et al (2002).
Sag et al?s goal is to classify constructions by their
range of admissible variation, and thus relies heav-
ily on syntactic variability. Since we are more inter-
ested in semantic properties, we base our classes on
alignment patterns, complemented by semantic de-
composability judgments (which reflect the severity
of treating MWEs like compositional phrases). As
mentioned in Section 1, our method misses MWEs
aligned with one-to-one links; however, the use of
a one-to-one link by the annotation can be seen as
evidence for decomposability.
A. Multiple words on both sides
(1) Compositional phrases (CP):
Each word in the left phrase can be aligned to one
word in the right phrase, e.g., capital punishment
? death penalty for which capital can be aligned
to death and punishment to penalty.
(2) Non-compositional phrases (NCP):
There is no simple way to align words between the
two phrases, such as in poorly represented? very
few or illegally entered? broke into.
B. One word to multiple words
(3) Headed multi-word expressions (MWEH):
A single word can be aligned with one token of
an MWE: e.g., vote? cast ballots where ballots
carries enough of the semantics of vote.
(4) Non-headed MWEs (MWENH):
The MWE as a whole is necessary to capture the
meaning of the single word, which doesn?t align
well to any individual word of the MWE: e.g., ferry
? passenger vessel.
(5) Multiple mentions (MENTION):
These alignments link one word to multiple occur-
rences of the same or related word(s) in the text,
e.g., military? forces ... Marines, antibiotics?
Status Category RTE2 dev RTE2 test
decomp. CP 5 0
MWEH 40 31
non- NCP 6 0
decomp. MWENH 30 29
Subtotal: True MWEs 81 60
other MENTION 26 48
PART 82 54
AUX 0 2
Total: All MWEs 189 164
Table 2: Frequencies of sentences with different
multi-word alignment categories in MSR data.
antibiotics ... drug.
(6) Parts of named entities (PART):
Each element of a named entity is aligned to the
whole named entity: e.g., Shukla? Nidhi Shukla.
This includes the use of acronyms or abbreviations
on one side and their spelled-out forms on the other
side, such as U.S.? United States.
(7) Auxiliaries (AUX):
The last category involves the presence of an auxil-
iary: e.g., were? are being.
Initially, one of the authors used these categories
to analyze the complete RTE2 MSR data (dev and
test sets). The most difficult distinction to draw
was, not surprisingly, the decision between decom-
posable multi-word alignments (categories (1) and
(3)) and non-decomposable ones (categories (2)
and (4)). To ascertain that a reliable distinction
can be made, another author did an independent
second analysis of the instances from categories
(1) through (4). We found moderate inter-annotator
agreement (? = 0.60), indicating that not all, but
most annotation decisions are uncontroversial.
3.2 Distribution of Multi-Word Expressions
Table 2 shows the distribution in the MSR data
of all alignment categories. Our evaluation will
concentrate on the ?true MWE? categories (1) to
(4): CP, NCP, MWEH and MWENH.
1
1
The OTHER categories (5) to (7) can generally be dealt
with during pre- or post-processing: Auxiliary-verb combi-
nations (cat. 7) are usually ?headed? so that it is sufficient to
align the main verb; multiple occurrences of words referring
to the same entity (cat. 5) is an anaphor resolution problem;
and named-entity matches (cat. 6) are best solved by using a
named entity recognizer to collapse NEs into a single token.
3
In RTE2 dev and test, we find only 81 and 60
true MWEs, respectively. Out of the 1600 sentence
pairs in the two datasets, 8.2% involve true MWEs
(73 in RTE2 dev and 58 in RTE2 test). On the level
of word alignments, the ratio is even smaller: only
1.2% of all SURE alignments involve true MWEs.
Furthermore, more than half of them are decom-
posable (MWEH/CP). Some examples from this
category are (?heads? marked in boldface):
sue? file lawsuits against
diseases? liver cancer
Barbie? Barbie doll
got? was awarded with
works? executive director
military? naval forces
In particular when light verbs are involved (file
lawsuits) or when modification adds just minor
meaning aspects (executive director), we argue that
it is sufficient to align the left-hand expression to
the ?head? in order to decide entailment.
Consider, in contrast, these examples from the
non-decomposable categories (MWENH/NCP):
politician? presidential candidate
killed? lost their lives
shipwreck? sunken ship
ever? in its history
widow? late husband
sexes? men and women
These cases span a broad range of linguistic rela-
tions from pure associations (widow/late husband)
to collective expressions (sexes/men and women).
Arguably, in these cases aligning the left-hand word
to any single word on the right can seriously throw
off an entailment recognition system. However,
they are fairly rare, occurring only in 65 out of
1600 sentences.
3.3 Conclusions from the MSR Analysis
Our analysis has found that 8% of the sentences
in the MSR dataset involve true MWEs. At the
word level, the fraction of true MWEs of all SURE
alignment links is just over 1%.
Of course, if errors in the alignment of these
MWEs had a high probability to lead to entailment
recognition errors, MWEs would still constitute a
major factor in determining entailment. However,
we have argued that about half of the true MWEs
are decomposable, that is, the part of the alignment
that is crucial for entailment can be recovered with
a one-to-one alignment link that can be identified
even by very limited alignment models.
This leaves considerably less than 1% of all word
alignments (or ?4% of sentence pairs) where im-
perfect MWE alignments are able at all to exert a
negative influence on entailment. However, this is
just an upper bound ? their impact is by no means
guaranteed. Thus, our conclusion from the annota-
tion study is that we do not expect MWEs to play a
large role in actual entailment recognition.
4 MWEs in Paraphrase Resources
Before we come to actual experiments on the au-
tomatic recognition of MWEs in a practical RTE
system, we need to consider the prerequisites for
this task. As mentioned in Section 2, if an RTE
system is to establish multi-word alignments, it re-
quires a knowledge source that provides accurate
semantic similarity judgments for ?many-to-many?
alignments (capital punishment ? death penalty)
as well as for ?one-to-many? alignments (vote ?
cast ballots). Such similarities are not present in
standard lexical resources like WordNet or Dekang
Lin?s thesaurus (Lin, 1998).
The best class of candidate resources to provide
wide-coverage of multi-word similarities seems to
be paraphrase resources. In this section, we ex-
amine to what extent two of the most widely used
paraphrase resource types provide supporting ev-
idence for the true MWEs in the MSR data. We
deliberately use corpus-derived, noisy resources,
since we are interested in the real-world (rather
than idealized) prospects for accurate MWE align-
ment.
Dependency-based paraphrases. Lin and Pan-
tel (2002)?s DIRT model collects lexicalized de-
pendency paths with two slots at either end. Paths
with similar distributions over slot fillers count as
paraphrases, with the quality measured by a mutual
information-based similarity over the slot fillers.
The outcome of their study is the DIRT database
which lists paraphrases for around 230,000 depen-
dency paths, extracted from about 1 GB of mis-
cellaneous newswire text. We converted the DIRT
paraphrases
2
into a resource of semantic similari-
ties between raw text phrases. We used a heuristic
mapping from dependency relations to word or-
der, and obtained similarity ratings by rescaling the
DIRT paraphrase ratings, which are based on a mu-
tual information-based measure of filler similarity,
onto the range [0,1].
2
We thank Patrick Pantel for granting us access to DIRT.
4
Parallel corpora-based paraphrases. An alter-
native approach to paraphrase acquisition was pro-
posed by Bannard and Callison-Burch (2005). It
exploits the variance inherent in translation to ex-
tract paraphrases from bilingual parallel corpora.
Concretely, it observes translational relationships
between a source and a target language and pairs
up source language phrases with other source lan-
guage phrases that translate into the same target
language phrases. We applied this method to
the large Chinese-English GALE MT evaluation
P3/P3.5 corpus (?2 GB text per language, mostly
newswire). The large number of translations makes
it impractical to store all observed paraphrases. We
therefore filtered the list of paraphrases against the
raw text of the RTE corpora, acquiring the 10 best
paraphrases for around 100,000 two- and three-
word phrases. The MLE conditional probabilities
were scaled onto [0,1] for each target.
Analysis. We checked the two resources for the
presence of the true MWEs identified in the MSR
data. We found that overall 34% of the MWEs ap-
pear in these resources, with more decomposable
MWEs (MWEH/CP) than non-decomposable ones
(MWENH/NCP) (42.1% vs. 24.6%). However, we
find that almost all of the MWEs that are covered
by the paraphrase resources are assigned very low
scores, while erroneous paraphrases (expressions
with clearly different meanings) have higher scores.
This is illustrated in Table 3 for the case of poorly
represented, which is aligned to very few in one
RTE2 sentence. This paraphrase is on the list, but
with a lower similarity than unsuitable paraphrases
such as representatives or good. This problem is
widespread. Other examples of low-scoring para-
phrases are: another step? measures, quarantine
? in isolation, punitive measures ? sanctions,
held a position? served as, or inability? could
not.
The noise in the rankings means that any align-
ment algorithm faces a dilemma: either it uses a
high threshold and misses valid MWE alignments,
or it lowers its threshold and risks constructing
incorrect alignments.
5 Impact of MWEs on Practical
Entailment Recognition
This section provides the final step in our study: an
evaluation of the impact of MWEs on entailment
recognition in a current RTE system, and of the
benefits of explicit MWE alignment. While the
poorly represented
represented 0.42
poorly 0.07
rarely 0.06
good 0.05
representatives 0.04
very few 0.04
well 0.02
representative 0.01
Table 3: Paraphrases of ?poorly represented? with
scores (semantic similarities).
results of this experiment are not guaranteed to
transfer to other RTE system architectures, or to
future, improved paraphrase resources, it provides
a current snapshot of the practical impact of MWE
handling.
5.1 The Stanford RTE System
We base our experiments on the Stanford RTE sys-
tem which uses a staged architecture (MacCartney
et al, 2006). After the linguistic analysis which
produces dependency graphs for premise and hy-
pothesis, the alignment stage creates links between
the nodes of the two dependency trees. In the infer-
ence stage, the system produces roughly 70 features
for the aligned premise-hypothesis pair, almost all
of which are implementations of ?small linguistic
theories? whose activation indicates lexical, syn-
tactic and semantic matches and mismatches of
different types. The entailment decision is com-
puted using a logistic regression on these features.
The Stanford system supports the use of dif-
ferent aligners without touching the rest of the
pipeline. We compare two aligners: a one-to-one
aligner, which cannot construct MWE alignments
(UNIQ), and a many-to-many aligner (MANLI)
(MacCartney et al, 2008), which can. Both align-
ers use around 10 large-coverage lexical resources
of semantic similarities, both manually compiled
resources (such as WordNet and NomBank) and
automatically induced resources (such as Dekang
Lin?s distributional thesaurus or InfoMap).
UNIQ: A one-to-one aligner. UNIQ constructs
an alignment between dependency graphs as the
highest-scoring mapping from each word in the
hypothesis to one word in the premise, or to null.
Mappings are scored by summing the alignment
scores of all individual word pairs (provided by the
lexical resources), plus edge alignment scores that
5
use the syntactic structure of premise and hypoth-
esis to introduce a bias for syntactic parallelism.
The large number of possible alignments (expo-
nential in the number of hypothesis words) makes
exhaustive search intractable. Instead, UNIQ uses a
stochastic search based on Gibbs sampling, a well-
known Markov Chain Monte Carlo technique (see
de Marneffe et al (2007) for details).
Since it does not support many-to-many align-
ments, the UNIQ aligner cannot make use of the
multi-word information present in the paraphrase
resources. To be able to capture some common
MWEs, the Stanford RTE system was originally
designed with a facility to concatenate MWEs
present in WordNet into a single token (mostly
particle verbs and collocations, e.g., treat as or
foreign minister). However, we discovered that
WordNet collapsing always has a negative effect.
Inspection of the constructed alignments suggests
that the lexical resources that inform the alignment
process do not provide scores for most collapsed
tokens (such as wait for), and precision suffers.
MANLI: A phrase-to-phrase aligner. MANLI
aims at finding an optimal alignment between
phrases, defined as contiguous spans of one or mul-
tiple words. MANLI characterizes alignments as
edit scripts, sets of edits (substitutions, deletions,
and insertions) over phrases. The quality of an
edit script is the sum of the quality of the individ-
ual edit steps. Individual edits are scored using a
feature-based scoring function that takes edit type
and size into consideration.
3
The score for substi-
tution edits also includes a lexical similarity score
similar to UNIQ, plus potential knowledge about
the semantic relatedness of multi-word phrases not
expressible in UNIQ. Substitution edits also use
contextual features, including a distortion score
and a matching-neighbors feature.
4
Due to the
dependence between alignment and segmentation
decisions, MANLI uses a simulated annealing strat-
egy to traverse the resulting large search space.
Even though MANLI is our current best candi-
date at recovering MWE alignments, it currently
has an important architectural limitation: it works
on textual phrases rather than dependency tree frag-
ments, and therefore misses all MWEs that are not
contiguous (e.g., due to inserted articles or adver-
3
Positive weights for all operation types ensure that
MANLI prefers small over large edits where appropriate.
4
An adaptation of the averaged perceptron algorithm
(Collins, 2002) is used to tune the model parameters.
micro-avg
P R F
1
UNIQ w/o para 80.4 80.8 80.6
MANLI w/o para 77.0 85.5 81.0
w/ para 76.7 85.4 80.8
Table 4: Evaluation of aligners and resources
against the manual MSR RTE2 test annotations.
bials). This accounts for roughly 9% of the MWEs
in RTE2 data. Other work on RTE has targeted
specifically this observation and has described para-
phrases on a dependency level (Marsi et al, 2007;
Dinu and Wang, 2009).
Setup. To set the parameters of the two models
(i.e., the weights for different lexical resources for
UNIQ, and the weights for the edit operation for
MANLI), we use the RTE2 development data. Test-
ing takes place on the RTE2 test and RTE4 datasets.
For MANLI, we performed this procedure twice,
with the paraphrase resources described in Sec-
tion 4 once deactivated and once activated. We
evaluated the output of the Stanford RTE system
both on the word alignment level, and on the entail-
ment decision level.
5.2 Evaluation of Alignment Accuracy
The results for evaluating the MANLI and UNIQ
alignments against the manual alignment links in
the MSR RTE2 test set are given in Table 4. We
present micro-averaged numbers, where each align-
ment link counts equally (i.e., longer problems have
a larger impact). The overall difference is not large,
but MANLI produces a slightly better alignment.
The ability of MANLI to construct many-to-
many alignments is reflected in a different position
on the precision/recall curve: the MANLI aligner
is less precise than UNIQ, but has a higher recall.
Examples for UNIQ and MANLI alignments are
shown in Figures 1 and 2. A comparison of the
alignments shows the pattern to be expected from
Table 4: MANLI has a higher recall, but contains
occasional questionable links, such as at President
? President in Figure 1.
However, the many-to-many alignments that
MANLI produces do not correspond well to the
MWE alignments. The overall impact of the para-
phrase resources is very small, and their addition
actually hurts MANLI?s performance slightly. A
more detailed analysis revealed two contrary trends.
On the one hand, the paraphrase resources provide
6
Aligner w/o para w/ para
UNIQ 63.8 ?
MANLI 60.6 60.6
Table 5: Entailment recognition accuracy of the
Stanford system on RTE2 test (two-way task).
Aligner w/o para w/ para TAC system
UNIQ 63.3 ? 61.4
MANLI 59.0 57.9 57.0
Table 6: Entailment recognition accuracy of the
Stanford system on RTE4 (two-way task).
beneficial information, maybe surprisingly, in the
form of broad distributional similarities for single
words that were not available from the standard lex-
ical resources (e.g., the alignment ?the company?s
letter?? ?the company?s certificate?).
On the other hand, MANLI captures not one of
the true MWEs identified in the MSR data. It only
finds two many-to-many alignments which belong
to the CP category: aimed criticism ? has criti-
cised, European currency ? euro currency. We
see this as the practical consequences of our ob-
servation from Section 4: The scores in current
paraphrase resources are too noisy to support accu-
rate MWE recognition (cf. Table 3).
5.3 Evaluation of Entailment Recognition
We finally evaluated the performance of the Stan-
ford system using UNIQ and MANLI alignments
on the entailment task. We consider two datasets:
RTE2 test, the alignment evaluation dataset, and
the most recent RTE4 dataset, where current num-
bers for the Stanford system are available from last
year?s Text Analysis Conference (TAC).
A reasonable conjecture would be that better
alignments translate into better entailment recog-
nition. However, as the results in Tables 5 and 6
show, this is not the case. Overall, UNIQ outper-
forms MANLI by several percent accuracy despite
MANLI?s better alignments. This ?baseline? differ-
ence should not be overinterpreted, since it may be
setup-specific: the features computed in the infer-
ence stage of the Stanford system were developed
mainly with the UNIQ aligner in mind. A more sig-
nificant result is that the integration of paraphrase
knowledge in MANLI has no effect on RTE2 test,
and even decreases performance on RTE4.
The general picture that we observe is that
there is only a loose coupling between alignments
and the entailment decision: individual align-
ments seldom matter. This is shown, for exam-
ple, by the alignments in Figures 1 and 2. Even
though MANLI provides a better overall alignment,
UNIQ?s alignment is ?good enough? for entailment
purposes. In Figure 1, the two words UNIQ leaves
unaligned are a preposition (at) and a light verb
(aimed), both of which are not critical to determine
whether or not the premise entails the hypothesis.
This interpretation is supported by another analy-
sis, where we tested whether entailments involving
at least one true MWE are more difficult to rec-
ognize. We computed the entailment accuracy for
all applicable RTE2 test pairs (7%, 58 sentences).
The accuracy on this subset is 62% for the MANLI
model without paraphrases, 64% for the MANLI
model with paraphrases, and 74% for UNIQ. The
differences from the numbers in Table 5 are not
significant due to the small size of the MWE sam-
ple, but we observe that the accuracy on the MWE
subset tends to be higher than on the whole set
(rather than lower). Futhermore, even though we fi-
nally see a small beneficial effect of paraphrases on
the MANLI aligner, the UNIQ aligner, which com-
pletely ignores MWEs, still performs substantially
better.
Our conclusion is that wrong entailment deci-
sions rarely hinge on wrongly aligned MWEs, at
least with a probabilistic architecture like the Stan-
ford system. Consequently, it suffices to recover
the most crucial alignment links to predict entail-
ment, and the benefits associated with the use of
a more restricted alignment formulation, like the
one-to-one alignment formulation of UNIQ, out-
weighs those of more powerful alignment models,
like MANLI?s phrasal alignments.
6 Conclusions
We have investigated the influence of multi-word
expressions on the task of recognizing textual en-
tailment. In contrast to the widely held view that
proper treatment of MWEs could bring about a sub-
stantial improvement in NLP tasks, we found that
the importance of MWEs in RTE is rather small.
Among the MWEs that we identified in the align-
ments, more than half can be captured by one-to-
one alignments, and should not pose problems for
entailment recognition.
Furthermore, we found that the remaining
MWEs are rather difficult to model faithfully. The
MSR MWEs are poorly represented in state-of-the-
7
Former
South
African
President
aimed
criticismat
President
Bush
Form
er
SouthAfrica
n
Presid
ent
has critic
ised
Presid
ent
Geor
ge
Bush NULL Former
South
African
President
aimed
criticismat
President
Bush
Form
er
SouthAfrica
n
Presid
ent
has critic
ised
Presid
ent
Geor
ge
Bush NULL
Figure 1: UNIQ (left) and MANLI (right) alignments for problem 483 in RTE2 test. The rows represent
the hypothesis words, and the columns the premise words.
Forme
Sor
uetrheueA
furi
caum
inPosoahe
deuiaBeBs
GgBP
derdG
e
Sor mguh
nhe
caumiaN mgffe
udeuia
BeBs
GgBPAaiaP
e
UL??
AaiaPe
???
Forme
Sor
uetrheueA
furi
caum
inPosoahe
deuiaBeBs
GgBP
derdG
e
Sor mguh
nhe
caumiaN mgffe
udeuia
BeBs
GgBPAaiaP
e
UL??
AaiaPe
???
Figure 2: UNIQ (left) and MANLI (right) alignments for problem 1 in RTE2 test.
art lexical resources, and when they are present,
scoring issues arise. Consequently, at least in
the Stanford system, the integration of paraphrase
knowledge to enable MWE recognition has made
almost no difference either in terms of alignment
accuracy nor in entailment accuracy. Furthermore,
it is not the case that entailment recognition accu-
racy is worse for sentences with ?true? MWEs. In
sum, we find that even though capturing and repre-
senting MWEs is an interesting problem in itself,
MWEs do not seem to be such a pain in the neck ?
at least not for textual entailment.
Our results may seem to contradict the results
of many previous RTE studies such as (Bar-Haim
et al, 2005) which found paraphrases to make an
important contribution. However, the beneficial ef-
fect of paraphrases found in these studies refers not
to an alignment task, but to the ability of relating
lexico-syntactic reformulations such as diathesis
alternations or symmetrical predicates (buy/sell).
In the Stanford system, this kind of knowledge
is already present in the features of the inference
stage. Our results should therefore rather be seen
as a clarification of the complementary nature of
the paraphrase and MWE issues.
In our opinion, there is much more potential
for improvement from better estimates of semantic
similarity. This is true for phrasal similarity, as our
negative results for multi-word paraphrases show,
but also on the single-word level. The 2% gain
in accuracy for the Stanford system here over the
reported TAC RTE4 results stems merely from ef-
forts to clean up and rescale the lexical resources
used by the system, and outweighs the effect of
MWEs. One possible direction of research is con-
ditioning semantic similarity on context: Most cur-
rent lexical resources characterize similarity at the
lemma level, but true similarities of word or phrase
pairs are strongly context-dependent: obtain and
be awarded are much better matches in the context
of a degree than in the context of data.
Acknowledgments
We thank Bill MacCartney for his help with the
MANLI aligner, and Michel Galley for the parallel
corpus-based paraphrase resource. This paper is
based on work funded in part by DARPA through
IBM. The content does not necessarily reflect the
views of the U.S. Government, and no official en-
dorsement should be inferred.
8
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of the 43rd Annual Meeting of the Association
for Computational Linguistics, pages 597?604, Ann
Arbor, MI.
Roy Bar-Haim, Idan Szpecktor, and Oren Glickman.
2005. Definition and analysis of intermediate entail-
ment levels. In Proceedings of the ACL Workshop on
Empirical Modeling of Semantic Equivalence and
Entailment, pages 55?60, Ann Arbor, MI.
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,
Danilo Giampiccolo, Bernardo Magnini, and Idan
Szpektor. 2006. The second PASCAL recognising
textual entailment challenge. In Proceedings of the
Second PASCAL Challenges Workshop on Recognis-
ing Textual Entailment, Venice, Italy.
Francis Bond, Anna Korhonen, Diana McCarthy, and
Aline Villavicencio, editors. 2003. Proceedings of
the ACL 2003 workshop on multiword expressions:
Analysis, acquisition and treatment.
Chris Brockett. 2007. Aligning the RTE 2006 corpus.
Technical Report MSR-TR-2007-77, Microsoft Re-
search.
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathe-
matic of statistical machine translation: Parameter
estimation. Computational Linguistics, 19(2):263?
311.
Aljoscha Burchardt, Nils Reiter, Stefan Thater, and
Anette Frank. 2007. A semantic approach to tex-
tual entailment: System evaluation and task analy-
sis. In Proceedings of the ACL-PASCAL Workshop
on Textual Entailment and Paraphrasing, pages 10?
15, Prague, Czech Republic.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL recognising textual entail-
ment challenge. In J. Quinonero-Candela, I. Da-
gan, B. Magnini, and F. d?Alch Buc, editors, Ma-
chine Learning Challenges. Lecture Notes in Com-
puter Science, Vol. 3944, pages 177?190. Springer.
Marie-Catherine de Marneffe, Trond Grenager, Bill
MacCartney, Daniel Cer, Daniel Ramage, Chlo?e
Kiddon, and Christopher D. Manning. 2007. Align-
ing semantic graphs for textual inference and ma-
chine reading. In Proceedings of the AAAI Spring
Symposium.
Georgiana Dinu and Rui Wang. 2009. Inference rules
and their application to recognizing textual entail-
ment. In Proceedings of the 12th Conference of the
European Chapter of the ACL (EACL 2009), pages
211?219, Athens, Greece.
Nicole Gr?egoire, Stefan Evert, and Su Nam Kim, edi-
tors. 2007. Proceedings of the ACL workshop: A
broader perspective on multiword expressions.
Andrew Hickl and Jeremy Bensley. 2007. A discourse
commitment-based framework for recognizing tex-
tual entailment. In Proceedings of the ACL-PASCAL
Workshop on Textual Entailment and Paraphrasing,
pages 171?176, Prague, Czech Republic.
Adrian Iftene and Alexandra Balahur-Dobrescu. 2007.
Hypothesis transformation and semantic variability
rules used in recognizing textual entailment. In
Proceedings of the ACL-PASCAL Workshop on Tex-
tual Entailment and Paraphrasing, pages 125?130,
Prague, Czech Republic.
Dekang Lin and Patrick Pantel. 2002. Discovery of
inference rules for question answering. Journal of
Natural Language Engineering, 7(4):343?360.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the joint An-
nual Meeting of the Association for Computational
Linguistics and International Conference on Com-
putational Linguistics, pages 768?774, Montr?eal,
Canada.
Bill MacCartney, Trond Grenager, Marie-Catherine
de Marneffe, Daniel Cer, and Christopher D. Man-
ning. 2006. Learning to recognize features of valid
textual entailments. In Proceedings of NAACL.
Bill MacCartney, Michel Galley, and Christopher D.
Manning. 2008. A phrase-based alignment model
for natural language inference. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, Honolulu, Hawaii.
Erwin Marsi, Emiel Krahmer, and Wauter Bosma.
2007. Dependency-based paraphrasing for recogniz-
ing textual entailment. In Proceedings of the ACL-
PASCAL Workshop on Textual Entailment and Para-
phrasing, pages 83?88, Prague, Czech Republic.
Begona Villada Moir?on, Aline Villavicencio, Diana
McCarthy, Stefan Evert, and Suzanne Stevenson, ed-
itors. 2006. Proceedings of the ACL Workshop on
Multiword Expressions: Identifying and Exploiting
Underlying Properties.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multi-word
expressions: a pain in the neck for NLP. In Proceed-
ings of CICLing.
Takaaki Tanaka, Aline Villavicencio, Francis Bond,
and Anna Korhonen, editors. 2004. Proceedings of
the second ACL workshop on multiword expressions:
Integrating processing.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Alessandro Moschitti. 2007. Shallow semantic in
fast textual entailment rule learners. In Proceed-
ings of the ACL-PASCAL Workshop on Textual En-
tailment and Paraphrasing, pages 72?77, Prague,
Czech Republic.
9
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 136?143,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Not a simple yes or no: Uncertainty in indirect answers
Marie-Catherine de Marneffe, Scott Grimm and Christopher Potts
Linguistics Department
Stanford University
Stanford, CA 94305
{mcdm,sgrimm,cgpotts}@stanford.edu
Abstract
There is a long history of using logic to
model the interpretation of indirect speech
acts. Classical logical inference, how-
ever, is unable to deal with the combina-
tions of disparate, conflicting, uncertain
evidence that shape such speech acts in
discourse. We propose to address this by
combining logical inference with proba-
bilistic methods. We focus on responses
to polar questions with the following prop-
erty: they are neither yes nor no, but
they convey information that can be used
to infer such an answer with some de-
gree of confidence, though often not with
enough confidence to count as resolving.
We present a novel corpus study and asso-
ciated typology that aims to situate these
responses in the broader class of indirect
question?answer pairs (IQAPs). We then
model the different types of IQAPs using
Markov logic networks, which combine
first-order logic with probabilities, empha-
sizing the ways in which this approach al-
lows us to model inferential uncertainty
about both the context of utterance and in-
tended meanings.
1 Introduction
Clark (1979), Perrault and Allen (1980), and Allen
and Perrault (1980) study indirect speech acts,
identifying a wide range of factors that govern how
speakers convey their intended messages and how
hearers seek to uncover those messages. Prior dis-
course conditions, the relationship between the lit-
eral meaning and the common ground, and spe-
cific lexical, constructional, and intonational cues
all play a role. Green and Carberry (1992, 1994)
provide an extensive computational model that in-
terprets and generates indirect answers to polar
questions. Their model focuses on inferring cat-
egorical answers, making use of discourse plans
and coherence relations.
This paper extends such work by recasting the
problem in terms of probabilistic modeling. We
focus on the interpretation of indirect answers
where the respondent does not answer with yes or
no, but rather gives information that can be used
by the hearer to infer such an answer only with
some degree of certainty, as in (1).
(1) A: Is Sue at work?
B: She is sick with the flu.
In this case, whether one can move from the re-
sponse to a yes or no is uncertain. Based on typical
assumptions about work and illness, A might take
B?s response as indicating that Sue is at home, but
B?s response could be taken differently depending
on Sue?s character ? B could be reproaching Sue
for her workaholic tendencies, which risk infect-
ing the office, or B could be admiring Sue?s stead-
fast character. What A actually concludes about
B?s indirect reply will be based on some combi-
nation of this disparate, partially conflicting, un-
certain evidence. The plan and logical inference
model of Green and Carberry falters in the face of
such collections of uncertain evidence. However,
natural dialogues are often interpreted in the midst
of uncertain and conflicting signals. We therefore
propose to enrich a logical inference model with
probabilistic methods to deal with such cases.
This study addresses the phenomenon of indi-
rect question?answer pairs (IQAP), such as in (1),
from both empirical and engineering perspectives.
136
First, we undertake a corpus study of polar ques-
tions in dialogue to gather naturally occurring in-
stances and to determine how pervasive indirect
answers that indicate uncertainty are in a natu-
ral setting (section 2). From this empirical base,
we provide a classification of IQAPs which makes
a new distinction between fully- and partially-
resolving answers (section 3). We then show how
inference in Markov logic networks can success-
fully model the reasoning involved in both types
of IQAPs (section 4).
2 Corpus study
Previous corpus studies looked at how pervasive
indirect answers to yes/no questions are in dia-
logue. Stenstro?m (1984) analyzed 25 face-to-face
and telephone conversations and found that 13%
of answers to polar questions do not contain an
explicit yes or no term. In a task dialogue, Hockey
et al (1997) found 38% of the responses were
IQAPs. (This higher percentage might reflect the
genre difference in the corpora used: task dialogue
vs. casual conversations.) These studies, how-
ever, were not concerned with how confidently one
could infer a yes or no from the response given.
We therefore conducted a corpus study to ana-
lyze the types of indirect answers. We used the
Switchboard Dialog Act Corpus (Jurafsky et al,
1997) which has been annotated for approximately
60 basic dialog acts, clustered into 42 tags. We
are concerned only with direct yes/no questions,
and not with indirect ones such as ?May I remind
you to take out the garbage?? (Clark, 1979; Per-
rault and Allen, 1980). From 200 5-minute con-
versations, we extracted yes/no questions (tagged
?qy?) and their answers, but discarded tag ques-
tions as well as disjunctive questions, such as in
(2), since these do not necessarily call for a yes
or no response. We also did not take into account
questions that were lost in the dialogue, nor ques-
tions that did not really require an answer (3). This
yielded a total of 623 yes/no questions.
(2) [sw 0018 4082]
A: Do you, by mistakes, do you mean just
like honest mistakes
A: or do you think they are deliberate sorts
of things?
B: Uh, I think both.
(3) [sw 0070 3435]
A: How do you feel about your game?
A: I guess that?s a good question?
B: Uh, well, I mean I?m not a serious
golfer at all.
To identify indirect answers, we looked at the
answer tags. The distribution of answers is given
in Table 1. We collapsed the tags into 6 categories.
Category I contains direct yes/no answers as well
as ?agree? answers (e.g., That?s exactly it.). Cate-
gory II includes statement?opinion and statement?
non-opinion: e.g., I think it?s great, Me I?m in the
legal department, respectively. Affirmative non-
yes answers and negative non-no answers form
category III. Other answers such as I don?t know
are in category IV. In category V, we put utterances
that avoid answering the question: by holding (I?m
drawing a blank), by returning the question ? wh-
question or rhetorical question (Who would steal
a newspaper?) ? or by using a backchannel in
question form (Is that right?). Finally, category
VI contains dispreferred answers (Schegloff et al,
1977; Pomerantz, 1984).
We hypothesized that the phenomenon we are
studying would appear in categories II, III and VI.
However, some of the ?na/ng? answers are dis-
guised yes/no answers, such as Right, I think so,
or Not really, and as such do not interest us. In the
case of ?sv/sd? and ?nd? answers, many answers
include reformulation, question avoidance (see 4),
or a change of framing (5). All these cases are not
really at issue for the question we are addressing.
(4) [sw 0177 2759]
A: Have you ever been drug tested?
B: Um, that?s a good question.
(5) [sw 0046 4316]
A: Is he the guy wants to, like, deregulate
heroin, or something?
B: Well, what he wants to do is take all the
money that, uh, he gets for drug
enforcement and use it for, uh, drug
education.
A: Uh-huh.
B: And basically, just, just attack the
problem at the demand side.
137
Definition Tag Total
I yes/no answers ny/nn/aa 341
II statements sv/sd 143
III affirmative/negative non-yes/no answers na/ng 91
IV other answers no 21
V avoid answering ?h/qw/qh/bh 18
VI dispreferred answers nd 9
Total 623
Table 1: Distribution of answer tags to yes/no questions.
(6) [sw 0046 4316]
A: That was also civil?
B: The other case was just traffic, and you
know, it was seat belt law.
We examined by hand all yes/no questions for
IQAPs and found 88 examples (such as (6), and
(7)?(11)), which constitutes thus 14% of the total
answers to direct yes/no questions, a figure simi-
lar to those of Stenstro?m (1984). The next section
introduces our classification of answers.
3 Typology of indirect answers
We can adduce the general space of IQAPs from
the data assembled in section 2 (see also Bolinger,
1978; Clark, 1979). One point of departure is that,
in cooperative dialogues, a response to a ques-
tion counts as an answer only when some relation
holds between the content of the response and the
semantic desiderata of the question. This is suc-
cinctly formulated in the relation IQAP proposed
by Asher and Lascarides (2003), p. 403:
IQAP(?,?) holds only if there is a true
direct answer p to the question J?K, and
the questioner can infer p from J?K in
the utterance context.
The apparent emphasis on truth can be set aside
for present purposes; Asher and Lascarides?s no-
tions of truth are heavily relativized to the current
discourse conditions. This principle hints at two
dimensions of IQAPs which must be considered,
and upon which we can establish a classification:
(i) the type of answer which the proffered response
provides, and (ii) the basis on which the inferences
are performed. The typology established here ad-
heres to this, distinguishing between fully- and
partially-resolving answers as well as between the
types of knowledge used in the inference (logical,
linguistic, common ground/world).
3.1 Fully-resolving responses
An indirect answer can fully resolve a question
by conveying information that stands in an inclu-
sion relation to the direct answer: if q ? p (or
?p), then updating with the response q also re-
solves the question with p (or ?p), assuming the
questioner knows that the inclusion relation holds
between q and p. The inclusion relation can be
based on logical relations, as in (7), where the re-
sponse is an ?over-answer?, i.e., a response where
more information is given than is strictly neces-
sary to resolve the question. Hearers supply more
information than strictly asked for when they rec-
ognize that the speaker?s intentions are more gen-
eral than the question posed might suggest. In (7),
the most plausible intention behind the query is
to know more about B?s family. The hearer can
also identify the speaker?s plan and any necessary
information for its completion, which he then pro-
vides (Allen and Perrault, 1980).
(7) [sw 0001 4325]
A: Do you have kids?
B: I have three.
While logical relations between the content of
the question and the response suffice to treat exam-
ples such as (7), other over-answers often require
substantial amounts of linguistic and/or world-
knowledge to allow the inference to go through,
as in (8) and (9).
(8) [sw 0069 3144]
A: Was that good?
B: Hysterical. We laughed so hard.
(9) [sw 0057 3506]
A: Is it in Dallas?
B: Uh, it?s in Lewisville.
138
In the case of (8), a system must recognize
that hysterical is semantically stronger than good.
Similarly, to recognize the implicit no of (9), a sys-
tem must recognize that Lewisville is a distinct
location from Dallas, rather than, say, contained
in Dallas, and it must include more general con-
straints as well (e.g., an entity cannot be in two
physical locations at once). Once the necessary
knowledge is in place, however, the inferences are
properly licensed.
3.2 Partially-resolving responses
A second class of IQAPs, where the content of
the answer itself does not fully resolve the ques-
tion, known as partially-resolved questions (Groe-
nendijk and Stokhof, 1984; Zeevat, 1994; Roberts,
1996; van Rooy, 2003), is less straightforward.
One instance is shown in (10), where the gradable
adjective little is the source of difficulty.
(10) [sw 0160 3467]
A: Are they [your kids] little?
B: I have a seven-year-old and a
ten-year-old.
A: Yeah, they?re pretty young.
The response, while an answer, does not, in and
of itself, resolve whether the children should be
considered little. The predicate little is a grad-
able adjective, which inherently possesses a de-
gree of vagueness: such adjectives contextually
vary in truth conditions and admit borderline cases
(Kennedy, 2007). In the case of little, while some
children are clearly little, e.g., ages 2?3, and some
clearly are not, e.g., ages 14?15, there is another
class in between for which it is difficult to as-
sess whether little can be truthfully ascribed to
them. Due to the slippery nature of these predi-
cates, there is no hard-and-fast way to resolve such
questions in all cases. In (10), it is the questioner
who resolves the question by accepting the infor-
mation proffered in the response as sufficient to
count as little.
The dialogue in (11) shows a second example of
an answer which is not fully-resolving, and inten-
tionally so.
(11) [sw 0103 4074]
A: Did he raise him [the cat] or
something1?
1The disjunct or something may indicate that A is open
B: We bought the cat for him and so he?s
been the one that you know spent the
most time with him.
Speaker B quibbles with whether the relation
his son has to the cat is one of raising, instead cit-
ing two attributes that go along with, but do not
determine, raising. Raising an animal is a com-
posite relation, which typically includes the rela-
tions owning and spending time with. However,
satisfying these two sub-relations does not strictly
entail satisfying the raising relation as well. It
is not obvious whether a system would be mis-
taken in attributing a fully positive response to the
question, although it is certainly a partially posi-
tive response. Similarly, it seems that attributing
a negative response would be misguided, though
the answer is partly negative. The rest of the dia-
logue does not determine whetherA considers this
equivalent to raising, and the dialogue proceeds
happily without this resolution.
The preceding examples have primarily hinged
upon conventionalized linguistic knowledge, viz.
what it means to raise X or for X to be little. A
further class of partially-resolving answers relies
on knowledge present in the common ground. Our
initial example (1) illustrates a situation where dif-
ferent resolutions of the question were possible de-
pending on the respondent?s intentions: no if sym-
pathetic, yes if reproachful or admiring.
The relationship between the response and
question is not secured by any objective world
facts or conventionalized meaning, but rather
is variable ? contingent on specialized world
knowledge concerning the dialogue participants
and their beliefs. Resolving such IQAPs positively
or negatively is achieved only at the cost of a de-
gree of uncertainty: for resolution occurs against
the backdrop of a set of defeasible assumptions.
3.3 IQAP classification
Table 2 is a cross-classification of the examples
discussed by whether the responses are fully- or
partially-resolving answers and by the types of
knowledge used in the inference (logical, linguis-
tic, world). It gives, for each category, the counts
of examples we found in the corpus. The partially-
resolved class contains more than a third of the an-
swers.
to hearing about alternatives to raise. We abstract away from
this issue for present purposes and treat the more general case
by assuming A?s contribution is simply equivalent to ?Did he
raise him??
139
Logic Linguistic World Total
Fully-Resolved 27 (Ex. 7) 18 (Ex. 8) 11 (Ex. 9) 56
Partially-Resolved ? 20 (Ex. 10;11) 12 (Ex. 1) 32
Table 2: Classification of IQAPs by knowledge type and resolvedness: counts and examples.
The examples given in (7)?(9) are fully resolv-
able via inferences grounded in logical relations,
linguistic convention or objective facts: the an-
swer provides enough information to fully resolve
the question, and the modeling challenge is secur-
ing and making available the correct information.
The partially-resolved pairs are, however, qualita-
tively different. They involve a degree of uncer-
tainty that classical inference models do not ac-
commodate in a natural way.
4 Towards modeling IQAP resolution
To model the reasoning involved in all types of
IQAPs, we can use a relational representation, but
we need to be able to deal with uncertainty, as
highlighted in section 3. Markov logic networks
(MLNs; Richardson and Domingos, 2006) exactly
suit these needs: they allow rich inferential reason-
ing on relations by combining the power of first-
order logic and probabilities to cope with uncer-
tainty. A logical knowledge-base is a set of hard
constraints on the set of possible worlds (set of
constants and grounded predicates). In Markov
logic, the constraints are ?soft?: when a world vi-
olates a relation, it becomes less probable, but not
impossible. A Markov logic network encodes a
set of weighted first-order logic constraints, such
that a higher weight implies a stronger constraint.
Given constants in the world, the MLN creates a
network of grounded predicates which applies the
constraints to these constants. The network con-
tains one feature fj for each possible grounding of
each constraint, with a value of 1 if the grounded
constraint is true, and 0 otherwise. The probability
of a world x is thus defined in terms of the con-
straints j satisfied by that world and the weights w
associated with each constraint (Z being the parti-
tion function):
P (X = x) =
1
Z
?
j
wjfj(x)
In practice, we use the Alchemy implemen-
tation of Markov logic networks (Kok et al,
2009). Weights on the relations can be hand-set
or learned. Currently, we use weights set by hand,
which suffices to demonstrate that an MLN han-
dles the pragmatic reasoning we want to model,
but ultimately we would like to learn the weights.
In this section, we show by means of a few
examples how MLNs give a simple and elegant
way of modeling the reasoning involved in both
partially- and fully-resolved IQAPs.
4.1 Fully-resolved IQAPs
While the use of MLNs is motivated by partially-
resolved IQAPs, to develop the intuitions behind
MLNs, we show how they model fully-resolved
cases, such as in (9). We define two distinct places,
Dallas and Lewisville, a relation linking a per-
son to a place, and the fact that person K is in
Lewisville. We also add the general constraint that
an individual can be in only one place at a time,
to which we assign a very high weight. Markov
logic allows for infinite weights, which Alchemy
denotes by a closing period. We also assume that
there is another person L, whose location is un-
known.
Constants and facts:
Place = {Dallas, Lewisville}
Person = {K,L}
BeIn(Person,Place)
BeIn(K,Lewisville)
Constraints:
// ?If you are in one place, you are not in another.?
(BeIn(x,y) ? (y != z))? !BeIn(x,z).
Figure 1 represents the grounded Markov network
obtained by applying the constraint to the con-
stants K, L, Dallas and Lewisville. The graph
contains a node for each predicate grounding, and
an arc between each pair of nodes that appear to-
gether in some grounding of the constraint. Given
that input, the MLN samples over possible worlds,
and infers probabilities for the predicate BeIn,
based on the constraints satisfied by each world
and their weights. The MLN returns a very low
probability for K being in Dallas, meaning that the
answer to the question Is it in Dallas? is no:
BeIn(K,Dallas): 4.9995e-05
140
BeIn(K, Dallas) BeIn(K, Lewisville)
BeIn(L, Lewisville)BeIn(L, Dallas)
Figure 1: Grounded Markov network obtained by applying the constraints to the constants K, L, Dallas
and Lewisville.
Since no information about L?s location has been
given, the probabilities of L being in Dallas or
Lewisville will be equal and low (0.3), which is
exactly what one would hope for. The probabili-
ties returned for each location will depend on the
number of locations specified in the input.
4.2 Partially-resolved IQAPs
To model partially-resolved IQAPs appropriately,
we need probabilities, since such IQAPs feature
reasoning patterns that involve uncertainty. We
now show how we can handle three examples of
partially-resolved IQAPs.
Gradable adjectives. Example (10) is a bor-
derline case of gradable adjectives: the question
bears on the predicate be little for two children of
ages 7 and 10. We first define the constants and
facts about the world, which take into account the
relations under consideration, ?BeLittle(X)? and
?Age(X, i)?, and specify which individuals we are
talking about, K and L, as well as their ages.
Constants and facts:
age = {0 . . . 120}
Person = {K, L}
Age(Person,age)
BeLittle(Person)
Age(K,7)
Age(L,10)
The relation between age and being little involves
some uncertainty, which we can model using a lo-
gistic curve. We assume that a 12-year-old child
lies in the vague region for determining ?little-
ness? and therefore 12 will be used as the center
of the logistic curve.
Constraints:
// ?If you are under 12, you are little.?
1.0 (Age(x,y) ? y < 12)? BeLittle(x)
// ?If you are above 12, you are not little.?
1.0 (Age(x,y) ? y > 12)? !BeLittle(x)
// The constraint below links two instances of Be-
Little.
(Age(x,u)?Age(y,v)? v>u?BeLittle(y))?Be-
Little(x).
Asking the network about K being little and L
being little, we obtain the following results, which
lead us to conclude that K and L are indeed little
with a reasonably high degree of confidence, and
that the indirect answer to the question is heavily
biased towards yes.
BeLittle(K): 0.92
BeLittle(L): 0.68
If we now change the facts, and say that K and L
are respectively 12 and 16 years old (instead of 7
and 10), we see an appropriate change in the prob-
abilities:
BeLittle(K): 0.58
BeLittle(L): 0.16
L, the 16-year-old, is certainly not to be consid-
ered ?little? anymore, whereas the situation is less
clear-cut for K, the 12-year-old (who lies in the
vague region of ?littleness? that we assumed).
Ideally, we would have information about the
speaker?s beliefs, which we could use to update
the constraints? weights. Absent such information,
we could use general knowledge from the Web to
learn appropriate weights. In this specific case, we
could find age ranges appearing with ?little kids?
in data, and fit the logistic curve to these.
This probabilistic model adapts well to cases
where categorical beliefs fit uneasily: for border-
line cases of vague predicates (whose interpreta-
tion varies by participant), there is no determinis-
tic yes or no answer.
141
Composite relations. In example (11), we want
to know whether the speaker?s son raised the cat
inasmuch as he owned and spent time with him.
We noted that raise is a composite relation, which
entails simpler relations, in this case spend time
with and own, although satisfying any one of the
simpler relations does not suffice to guarantee the
truth of raise itself. We model the constants, facts,
and constraints as follows:
Constants and Facts:
Person = {K}
Animal = {Cat}
Raise(Person,Animal)
SpendTime(Person,Animal)
Own(Person,Animal)
SpendTime(K,Cat)
Own(K,Cat)
Constraints:
// ?If you spend time with an animal, you help
raise it.?
1.0 SpendTime(x,y)? Raise(x,y)
// ?If you own an animal, you help raise it.?
1.0 Own(x,y)? Raise(x,y)
The weights on the relations reflect how central we
judge them to be in defining raise. For simplicity,
here we let the weights be identical. Clearly, the
greater number of relevant relations a pair of en-
tities fulfills, the greater the probability that the
composite relation holds of them. Considering
two scenarios helps illustrate this. First, suppose,
as in the example, that both relations hold. We will
then have a good indication that by owning and
spending time with the cat, the son helped raise
him:
Raise(K,Cat): 0.88
Second, suppose that the example is different in
that only one of the relations holds, for instance,
that the son only spent time with the cat, but did
not own it, and accordingly the facts in the net-
work do not contain Own(K,Cat). The probability
that the son raised the cat decreases:
Raise(K,Cat): 0.78
Again this can easily be adapted depending on the
centrality of the simpler relations to the composite
relation, as well as on the world-knowledge con-
cerning the (un)certainty of the constraints.
Speaker beliefs and common ground knowl-
edge. The constructed question?answer pair
given in (1), concerning whether Sue is at work,
demonstrated that how an indirect answer is mod-
eled depends on different and uncertain evidence.
The following constraints are intended to capture
some background assumptions about how we re-
gard working, being sick, and the connections be-
tween those properties:
// ?If you are sick, you are not coming to work.?
Sick(x)? !AtWork(x)
// ?If you are hardworking, you are at work.?
HardWorking(x)? AtWork(x)
// ?If you are malicious and sick, you come to
work.?
(Malicious(x) ? Sick(x))? AtWork(x)
// ?If you are at work and sick, you are malicious
or thoughtless.?
(AtWork(x) ? Sick(x)) ? (Malicious(x) ?
Thoughtless(x))
These constraints provide different answers about
Sue being at work depending on how they are
weighted, even while the facts remain the same
in each instance. If the first constraint is heavily
weighted, we get a high probability for Sue not
being at work, whereas if we evenly weight all the
constraints, Sue?s quality of being a hard-worker
dramatically raises the probability that she is at
work. Thus, MLNs permit modeling inferences
that hinge upon highly variable common ground
and speaker beliefs.
Besides offering an accurate treatment of fully-
resolved inferences, MLNs have the ability to deal
with degrees of certitude. This power is required
if one wants an adequate model of the reasoning
involved in partially-resolved inferences. Indeed,
for the successful modeling of such inferences, it
is essential to have a mechanism for adding facts
about the world that are accepted to various de-
grees, rather than categorically, as well as for up-
dating these facts with speakers? beliefs if such in-
formation is available.
5 Conclusions
We have provided an empirical analysis and ini-
tial treatment of indirect answers to polar ques-
tions. The empirical analysis led to a catego-
rization of IQAPs according to whether their an-
swers are fully- or partially-resolving and accord-
ing to the types of knowledge used in resolving
142
the question by inference (logical, linguistic, com-
mon ground/world). The partially-resolving indi-
rect answers injected a degree of uncertainty into
the resolution of the predicate at issue in the ques-
tion. Such examples highlight the limits of tradi-
tional logical inference and call for probabilistic
methods. We therefore modeled these exchanges
with Markov logic networks, which combine the
power of first-order logic and probabilities. As
a result, we were able to provide a robust model
of question?answer resolution in dialogue, one
which can assimilate information which is not cat-
egorical, but rather known only to a degree of cer-
titude.
Acknowledgements
We thank Christopher Davis, Dan Jurafsky, and
Christopher D. Manning for their insightful com-
ments on earlier drafts of this paper. We also thank
Karen Shiells for her help with the data collection
and Markov logic.
References
James F. Allen and C. Raymond Perrault. 1980. Ana-
lyzing intention in utterances. Artificial Intelligence,
15:143?178.
Nicholas Asher and Alex Lascarides. 2003. Logics of
Conversation. Cambridge University Press, Cam-
bridge.
Dwight Bolinger. 1978. Yes?no questions are not al-
ternative questions. In Henry Hiz, editor, Questions,
pages 87?105. D. Reidel Publishing Company, Dor-
drecht, Holland.
Herbert H. Clark. 1979. Responding to indirect speech
acts. Cognitive Psychology, 11:430?477.
Nancy Green and Sandra Carberry. 1992. Conver-
sational implicatures in indirect replies. In Pro-
ceedings of the 30th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 64?
71, Newark, Delaware, USA, June. Association for
Computational Linguistics.
Nancy Green and Sandra Carberry. 1994. A hybrid
reasoning model for indirect answers. In Proceed-
ings of the 32nd Annual Meeting of the Association
for Computational Linguistics, pages 58?65, Las
Cruces, New Mexico, USA, June. Association for
Computational Linguistics.
Jeroen Groenendijk and Martin Stokhof. 1984. Studies
in the Semantics of Questions and the Pragmatics of
Answers. Ph.D. thesis, University of Amsterdam.
Beth Ann Hockey, Deborah Rossen-Knill, Beverly
Spejewski, Matthew Stone, and Stephen Isard.
1997. Can you predict answers to Y/N questions?
Yes, No and Stuff. In Proceedings of Eurospeech
1997.
Daniel Jurafsky, Elizabeth Shriberg, and Debra Bi-
asca. 1997. Switchboard SWBD-DAMSL shallow-
discourse-function annotation coders manual, draft
13. Technical Report 97-02, University of Colorado,
Boulder Institute of Cognitive Science.
Christopher Kennedy. 2007. Vagueness and grammar:
The semantics of relative and absolute gradable ad-
jectives. Linguistics and Philosophy, 30(1):1?45.
Stanley Kok, Marc Sumner, Matthew Richardson,
Parag Singla, Hoifung Poon, Daniel Lowd, Jue
Wang, and Pedro Domingos. 2009. The Alchemy
system for statistical relational AI. Technical report,
Department of Computer Science and Engineering,
University of Washington, Seattle, WA.
C. Raymond Perrault and James F. Allen. 1980. A
plan-based analysis of indirect speech acts. Amer-
ican Journal of Computational Linguistics, 6(3-
4):167?182.
Anita M. Pomerantz. 1984. Agreeing and dis-
agreeing with assessment: Some features of pre-
ferred/dispreferred turn shapes. In J. M. Atkinson
and J. Heritage, editors, Structure of Social Action:
Studies in Conversation Analysis. Cambridge Uni-
versity Press.
Matt Richardson and Pedro Domingos. 2006. Markov
logic networks. Machine Learning, 62(1-2):107?
136.
Craige Roberts. 1996. Information structure: To-
wards an integrated formal theory of pragmatics. In
Jae Hak Yoon and Andreas Kathol, editors, OSU
Working Papers in Linguistics, volume 49: Papers
in Semantics, pages 91?136. The Ohio State Uni-
versity Department of Linguistics, Columbus, OH.
Revised 1998.
Robert van Rooy. 2003. Questioning to resolve
decision problems. Linguistics and Philosophy,
26(6):727?763.
Emanuel A. Schegloff, Gail Jefferson, and Harvey
Sacks. 1977. The preference for self-correction
in the organization of repair in conversation. Lan-
guage, 53:361?382.
Anna-Brita Stenstro?m. 1984. Questions and re-
sponses in English conversation. In Claes Schaar
and Jan Svartvik, editors, Lund Studies in English
68, Malmo? Sweden. CWK Gleerup.
Henk Zeevat. 1994. Questions and exhaustivity in up-
date semantics. In Harry Bunt, Reinhard Muskens,
and Gerrit Rentier, editors, Proceedings of the In-
ternational Workshop on Computational Semantics,
pages 211?221. ITK, Tilburg.
143
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 725?735,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Multiword Expression Identification with Tree Substitution Grammars:
A Parsing tour de force with French
Spence Green*, Marie-Catherine de Marneffe?, John Bauer*, and Christopher D. Manning*?
*Computer Science Department, Stanford University
?Linguistics Department, Stanford University
{spenceg,mcdm,horatio,manning}@stanford.edu
Abstract
Multiword expressions (MWE), a known nui-
sance for both linguistics and NLP, blur the
lines between syntax and semantics. Previous
work onMWE identification has relied primar-
ily on surface statistics, which perform poorly
for longer MWEs and cannot model discontin-
uous expressions. To address these problems,
we show that even the simplest parsing mod-
els can effectively identify MWEs of arbitrary
length, and that Tree Substitution Grammars
achieve the best results. Our experiments show
a 36.4% F1 absolute improvement for French
over an n-gram surface statistics baseline, cur-
rently the predominant method for MWE iden-
tification. Our models are useful for several
NLP tasks in which MWE pre-grouping has
improved accuracy.
1 Introduction
Multiword expressions (MWE) have long been a
challenge for linguistic theory and NLP. There is
no universally accepted definition of the term, but
MWEs can be characterized as ?idiosyncratic inter-
pretations that cross word boundaries (or spaces)?
(Sag et al, 2002) such as traffic light, or as ?fre-
quently occurring phrasal units which are subject
to a certain level of semantic opaqueness, or non-
compositionality? (Rayson et al, 2010).
MWEs are often opaque fixed expressions, al-
though the degree to which they are fixed can vary.
Some MWEs do not allow morphosyntactic varia-
tion or internal modification (e.g., in short, but *in
shorter or *in very short). Other MWEs are ?semi-
fixed,? meaning that they can be inflected or undergo
internal modification. The type of modification is of-
ten limited, but not predictable, so it is not possible
to enumerate all variants (Table 1).
French English
? terme in the near term
? court terme in the short term
? tr?s court terme in the very short term
? moyen terme in the mediumterm
? long terme in the long term
? tr?s long terme in the very long term
Table 1: Semi-fixed MWEs in French and English. The
French adverb ? terme ?in the end? can be modified by
a small set of adjectives, and in turn some of these ad-
jectives can be modified by an adverb such as tr?s ?very?.
Similar restrictions appear in English.
Merging known MWEs into single tokens has
been shown to improve accuracy for a variety of
NLP tasks: dependency parsing (Nivre and Nilsson,
2004), constituency parsing (Arun andKeller, 2005),
sentence generation (Hogan et al, 2007), and ma-
chine translation (Carpuat andDiab, 2010). Most ex-
periments use gold MWE pre-grouping or language-
specific resources like WordNet. For unlabeled text,
the best MWE identification methods, which are
based on surface statistics (Pecina, 2010), suffer
from sparsity induced by longer n-grams (Ramisch
et al, 2010). A dilemma thus exists: MWE knowl-
edge is useful, but MWEs are hard to identify.
In this paper, we show the effectiveness of statis-
tical parsers for MWE identification. Specifically,
Tree Substitution Grammars (TSG) can achieve a
36.4% F1 absolute improvement over a state-of-the-
art surface statistics method. We choose French,
which has pervasive MWEs, for our experiments.
Parsing models naturally accommodate discontinu-
ous MWEs like phrasal verbs, and provide syntac-
tic subcategorization. By contrast, surface statistics
methods are usually limited to binary judgements for
contiguous n-grams or dependency bigrams.
725
FTB (train) WSJ (train)
Sentences 13,449 39,832
Tokens 398,248 950,028
#Word Types 28,842 44,389
#Tag Types 30 45
#Phrasal Types 24 27
Per Sentence
Depth (?/?2) 4.03 / 0.360 4.18 / 0.730
Breadth (?/?2) 13.5 / 6.79 10.7 / 4.59
Length (?/?2) 29.6 / 17.3 23.9 / 11.2
Constituents (?) 20.3 19.6
? Constituents / ? Length 0.686 0.820
Table 2: Gross corpus statistics for the pre-processed FTB
(training set) andWSJ (sec. 2-21). The FTB sentences are
longer with broader syntactic trees. The FTB POS tag set
has 33% fewer types than theWSJ. The FTB dev set OOV
rate is 17.77% vs. 12.78% for the WSJ.
Type #Total #Single %Single %Total
MWN noun 9,680 2,737 28.3 49.7
MWADV adverb 3,852 449 11.7 19.8
MWP prep. 3,526 342 9.70 18.1
MWC conj. 814 73 8.97 4.18
MWV verb. 585 243 41.5 3.01
MWD det. 328 69 21.0 1.69
MWA adj. 324 126 38.9 1.66
MWPRO pron. 266 33 12.4 1.37
MWCL clitic 59 1 1.69 0.30
MWET foreign 24 18 0.75 0.12
MWI interj. 4 2 0.50 0.02
19,462 4,093 21.0% 100.0%
Table 3: Frequency distribution of the 11 MWE subcate-
gories in the FTB (training set). MWEs account for 7.08%
of the bracketings and 13.0% of the tokens in the treebank.
Only 21% of the MWEs occur once (?single?).
We first introduce a new instantiation of the
French Treebank that, unlike previous work, does not
use gold MWE pre-grouping. Consequently, our ex-
perimental results also provide a better baseline for
parsing raw French text.
2 French Treebank Setup
The corpus used in our experiments is the French
Treebank (Abeill? et al (2003), version from June
2010, hereafter FTB). In French, there is a linguis-
tic tradition of lexicography which compiles lists
of MWEs occurring in the language. For exam-
ple, Gross (1986) shows that dictionaries contain
about 1,500 single-word adverbs but that French con-
tains over 5,000 multiword adverbs. MWEs occur
in every part-of-speech (POS) category (e.g., noun
trousse de secours ?first-aid kit?; verb faire main-
basse [do hand-low] ?seize?; adverb comme dans du
beurre [as in butter] ?easily?; adjective ?? part en-
ti?re? ?wholly?).
The FTB explicitly annotates MWEs (also called
compounds in prior work). We used the subset of
the corpus with functional annotations, not for those
annotations but because this subset is known to be
more consistently annotated. POS tags for MWEs
are given not only at the MWE level, but also inter-
nally: most tokens that constitute an MWE also have
a POS tag. Table 2 compares this part of the FTB to
the WSJ portion of the Penn Treebank.
2.1 Preprocessing
The FTB requires significant pre-processing prior to
parsing.
Tokenization We changed the default tokenization
for numbers by fusing adjacent digit tokens. For ex-
ample, 500 000 is tagged as an MWE composed of
two words 500 and 000. We made this 500000 and
retained the MWE POS, although we did not mark
the new token as an MWE. For consistency, we used
one token for punctuated numbers like ?17,9?.
MWE Tagging We marked MWEs with a flat
bracketing in which the phrasal label is the MWE-
level POS tag with an ?MW? prefix, and the preter-
minals are the internal POS tags for each terminal.
The resulting POS sequences are not always unique
to MWEs: they appear in abundance elsewhere in
the corpus. However, some MWEs contain normally
ungrammatical POS sequences (e.g., adverb ? la va
vite ?in a hurry?: PDVADV [at the goes quick]), and
some words appear only as part of an MWE, such as
insu in ? l?insu de ?to the ignorance of?.
Labels We augmented the basic FTB label set?
which contains 14 POS tags and 19 phrasal tags?in
two ways. First, we added 16 finer-grained POS tags
for punctuation.1 Second, we added the 11 MWE
1Punctuation tag clusters?as used in the WSJ?did not im-
prove accuracy. Enriched tag sets like that of Crabb? and Can-
dito (2008) could also be investigated and compared to our re-
sults since Evalb is insensitive to POS tags.
726
labels shown in Table 3, resulting in 24 total phrasal
categories.
Corrections Historically, the FTB suffered from
annotation errors such as missing POS and phrasal
tags (Arun and Keller, 2005). We found that this
problem has been largely resolved in the current re-
lease. However, 1,949 tokens and 36 MWE spans
still lacked tags. We restored the labels by first as-
signing each token its most frequent POS tag else-
where in the treebank, and then assigning the most
frequent MWE phrasal category for the resulting
POS sequence.2
Split We used the 80/10/10 split described by
Crabb? and Candito (2008). However, they used a
previous release of the treebank with 12,531 trees.
3,391 trees have been added to the present version.
We appended these extra trees to the training set, thus
retaining the same development and test sets.
2.2 Comparison to Prior FTB Representations
Our pre-processing approach is simple and auto-
matic3 unlike the three major instantiations of the
FTB that have been used in previous work:
Arun-Cont and Arun-Exp (Arun and Keller,
2005): Two instantiations of the full 20,000 sentence
treebank that differed principally in their treatment of
MWEs: (1) Cont, in which the tokens of eachMWE
were concatenated into a single token (en moyenne
? en_moyenne); (2)Exp, in which theyweremarked
with a flat structure. For both representations, they
also gave results in which coordinated phrase struc-
tures were flattened. In the published experiments,
they mistakenly removed half of the corpus, believ-
ing that the multi-terminal (per POS tag) annotations
of MWEs were XML errors (Schluter and Genabith,
2007).
MFT (Schluter andGenabith, 2007): Manual revi-
sion to 3,800 sentences. Major changes included co-
ordination raising, an expanded POS tag set, and the
273 of the unlabeled word types did not appear elsewhere
in the treebank. All but 11 of these were nouns. We manually
assigned the correct tags, but we would not expect a negative
effect by deterministically labeling all of them as nouns.
3We automate tree manipulation with Tregex/Tsurgeon
(Levy and Andrew, 2006). Our pre-processing package is avail-
able at http://nlp.stanford.edu/software/lex-parser.shtml.
correction of annotation errors. Like Arun-Cont,
MFT contains concatenated MWEs.
FTB-UC (Candito and Crabb?, 2009): An in-
stantiation of the functionally annotated section that
makes a distinction between MWEs that are ?syn-
tactically regular? and those that are not. Syntacti-
cally regular MWEs were given internal structure,
while all other MWEs were concatenated into sin-
gle tokens. For example, nouns followed by ad-
jectives, such as loi agraire ?land law? or Union
mon?taire et ?conomique ?monetary and economic
Union? were considered syntactically regular. They
are MWEs because the choice of adjective is arbi-
trary (loi agraire and not *loi agricole, similarly to
?coal black? but not *?crow black? for example), but
their syntactic structure is not intrinsic to MWEs.
In such cases, FTB-UC gives the MWE a conven-
tional analysis of an NP with internal structure. Such
analysis is indeed sufficient to recover the mean-
ing of these semantically compositional MWEs that
are extremely productive. On the other hand, the
FTB-UC loses information about MWEs with non-
compositional semantics.
Almost all work on the FTB has followed Arun-
Cont and used goldMWEpre-grouping. As a result,
most results for French parsing are analogous to early
results for Chinese, which used gold word segmen-
tation, and Arabic, which used gold clitic segmenta-
tion. Candito et al (2010) were the first to acknowl-
edge and address this issue, but they still used FTB-
UC (with some pre-grouped MWEs). Since the syn-
tax and definition of MWEs is a contentious issue,
we take a more agnostic view?which is consistent
with that of the FTB annotators?and leave them to-
kenized. This permits a data-oriented approach to
MWE identification that is more robust to changes
to the status of specific MWE instances.
To set a baseline prior to grammar development,
we trained the Stanford parser (Klein and Manning,
2003) with no grammar features, achieving 74.2%
labeled F1 on the development set (sentences ? 40
words). This is lower than the most recent results ob-
tained by Seddah (2010). However, the results are
not comparable: the data split was different, they
made use of morphological information, and more
importantly they concatenated MWEs. The focus of
727
our work is on models and data representations that
enable MWE identification.
3 MWEs in Lexicon-Grammar
The MWE representation in the FTB is close to
the one proposed in the Lexicon-Grammar (Gross,
1986). In the Lexicon-Grammar, MWEs are classi-
fied according to their global POS tags (noun, verb,
adverb, adjective), and described in terms of the se-
quence of the POS tags of the words that constitute
the MWE (e.g., ?N de N? garde d?enfant [guard of
child] ?daycare?, pied de guerre [foot of war] ?at the
ready?). In other words, MWEs are represented by a
flat structure. The Lexicon-Grammar distinguishes
between units that are fixed and have to appear as is
(en tout et pour tout [in all and for all] ?in total?) and
units that accept some syntactic variation such as ad-
mitting the insertion of an adverb or adjective, or the
variation of one of the words in the expression (e.g.,
a possessive as in ?from the top of one?s hat?). It also
notes whether the MWE displays some selectional
preferences (e.g., it has to be preceded by a verb or
by an adjective).
Our FTB instantiation is largely consistent with
the Lexicon-Grammar. Recall that we defined differ-
ent MWE categories based on the global POS. We
now detail three of the categories.
MWN The MWN category consists of proper
nouns (1a), foreign common nouns (1b), as well as
common nouns. The common nouns appear in sev-
eral syntactically regular sequences of POS tags (2).
Multiword nouns allow inflection (singular vs. plu-
ral) but no insertion.
(1) a. London Sunday Times, Los Angeles
b. week - end, mea culpa, joint - venture
(2) a. NA: corpsm?dical ?medical staff?, dette
publique ?public debt?
b. N PN:mode d?emploi ?instruction man-
ual?
c. N N: num?ro deux ?number two?, mai-
sonm?re [housemother] ?headquarters?,
gr?ve surprise ?sudden strike?
d. N P D N: imp?t sur le revenu ?income
tax?, ministre de l??conomie ?finance
minister?
MWA Multiword adjectives appear with different
POS sequences (3). They include numbers such as
vingt et uni?me ?21st?. Some items in (3b) allow in-
ternal variation: some adverbs or adjectives can be
added to both examples given (? tr?s haut risque, de
toute derni?re minute).
(3) a. P N: d?antan [from before] ?old?, en
question ?under discussion?
b. P A N: ? haut risque ?high-risk?, de
derni?re minute [from the last minute]
?at the eleventh hour?
c. A C A: pur et simple [pure and simple]
?straightforward?, noir et blanc ?black
and white?
MWV Multiword verbs also appear in several POS
sequences (4). All verbs allow number and tense in-
flections. Some MWVs containing a noun or an ad-
jective allow the insertion of a modifier (e.g., don-
ner grande satisfication ?give great satisfaction?),
whereas others do not. When an adverb intervenes
between the main verb and its complement, the FTB
marks the two parts of the MWV discontinuously
(e.g., [MWV [V prennent]] [ADV d?j?] [MWV [P en] [N
cause]] ?already take into account?).
(4) a. V N: avoir lieu ?take place?, donner sat-
isfaction ?give satisfaction?
b. V P N: mettre en place ?put in place?,
entrer en vigueur ?to come into effect?
c. V P ADV: mettre ? mal [put at bad]
?harm?, ?tre ? m?me [be at same] ?be
able?
d. V D N P N: tirer la sonnette d?alarme
?ring the alarm bell?, avoir le vent en
poupe ?to have the wind astern?
4 Parsing Models
We develop two parsers for French with the goal
of improving MWE identification. The first is a
manually-annotated grammar that we incorporate
into the Stanford parser. Manual annotation results in
human interpretable grammars that can inform future
treebank annotation decisions. Moreover, the gram-
mar can be used as the base distribution in our sec-
ond model, a Probabilistic Tree Substitution Gram-
mar (PTSG) parser. PTSGs learn parameters for tree
728
Feature States Tags F1 ?F1
? 4325 31 74.21
tagPA 4509 215 76.94 +2.73
markInf 4510 216 77.42 +0.48
markPart 4511 217 77.73 +0.31
markVN 5986 217 78.32 +0.59
markCoord 7361 217 78.45 +0.13
markDe 7521 233 79.11 +0.66
markP 7523 235 79.34 +0.23
markMWE 7867 235 79.23 ?0.11
Table 4: Effects on grammar size and labeled F1 for each
of the manual state splits (development set, sentences ?
40 words). markMWE decreases overall accuracy, but
increases both the number of correctly parsed trees (by
0.30%) and per category MWE accuracy.
fragments larger than basic CFG rules. PTSG rules
may also be lexicalized. This means that commonly
observed collocations?some of which areMWEs?
can be stored in the grammar.
4.1 Stanford Parser
We configure the Stanford parser with settings that
are effective for other languages: selective parent an-
notation, lexicon smoothing, and factored parsing.
We use the head-finding rules of Dybro-Johansen
(2004), which we find to yield an approximately
1.0% F1 development set improvement over those of
Arun (2004). Finally, we include a simple unknown
word model consisting entirely of surface features:
- Nominal, adjectival, verbal, adverbial, and plu-
ral suffixes
- Contains a digit or punctuation
- Is capitalized (except the first word in a sen-
tence)
- Consists entirely of capital letters
- If none of the above, add a one- or two-character
suffix
Combined with the grammar features, this unknown
word model yields 97.3% tagging accuracy on the
development set.
4.1.1 Grammar Development
Table 4 lists the symbol refinements used in our
grammar. Most of the features are POS splits as
many phrasal tag splits did not lead to any improve-
ment. Parent annotation of POS tags (tagPA) cap-
tures information about the external context. mark-
Inf and markPart accomplish a finite/nonfinite dis-
tinction: they respectively specify whether the verb
is an infinitive or a participle based on the type of
the grandparent node. markVN captures the notion
of verbal distance as in Klein and Manning (2003).
We opted to keep the COORD phrasal tag, and
to capture parallelism in coordination, we mark CO-
ORD with the type of its child (NP, AP, VPinf, etc.).
markDe identifies the preposition de and its variants
(du, des, d?) which is very frequent and appears in
several different contexts. markP identifies preposi-
tions which introduce PPs modifying a noun. Mark-
ing other kinds of prepositional modifiers (e.g., verb)
did not help. markMWE adds an annotation to sev-
eral MWE categories for frequently occuring POS
sequences. For example, we mark MWNs that occur
more than 600 times (e.g., ?N P N? and ?N N?).
4.2 DP-TSG Parser
A shortcoming of CFG-based grammars is that they
do not explicitly capture idiomatic usage. For exam-
ple, consider the two utterances:
(5) a. He [MWV kicked the bucket] .
b. He [VP kicked [NP the pail]] .
The examples in (5) may be equally probable and re-
ceive the same analysis under a PCFG; words are
generated independently. However, recall that in
our representation, (5a) should receive a flat analysis
as MWV, whereas (5b) should have a conventional
analysis of the verb kicked and its two arguments.
An alternate view of parsing is one in which new
utterances are built from previously observed frag-
ments. This is the original motivation for data ori-
ented parsing (DOP) (Bod, 1992), in which ?id-
iomaticity is the rule rather than the exception?
(Scha, 1990). If we have seen the collocation kicked
the bucket several times before, we should store that
whole fragment for later use.
We consider a variant of the non-parametric PTSG
model of Cohn et al (2009) in which tree fragments
are drawn from a Dirichlet process (DP) prior.4
The DP-TSG can be viewed as a DOP model with
Bayesian parameter estimation. A PTSG is a 5-tuple
?V,?, R,?,?? where c ? V are non-terminals;
4Similar models were developed independently by
O?Donnell et al (2009) and Post and Gildea (2009).
729
?c DP concentration parameter for each c ? V
P0(e|c) CFG base distribution
x Set of non-terminal nodes in the treebank
S Set of sampling sites (one for each x ? x)
S A block of sampling sites, where S ? S
b = {bs}s?S Binary variables to be sampled (bs = 1 ?
frontier node)
z Latent state of the segmented treebank
m Number of sites s ? S s.t. bS = 1
n = {nc,e} Sufficient statistics of z
?nS:m Change in counts by setting m sites in S
Table 5: DP-TSG model notation. For consistency, we
largely follow the notation of Liang et al (2010). Note
that z = (b,x), and as such z = ?c, e?.
t ? ? are terminals; e ? R are elementary trees;5
? ? V is a unique start symbol; and ?c,e ? ? are
parameters for each tree fragment. A PTSG deriva-
tion is created by successively applying the substitu-
tion operator to the leftmost frontier node (denoted
by c+). All other nodes are internal (denoted by c?).
In the supervised setting, DP-TSG grammar ex-
traction reduces to a segmentation problem. We have
a treebank T that we segment into the set R, a pro-
cess that we model with Bayes? rule:
p(R | T ) ? p(T | R) p(R) (1)
Since the tree fragments completely specify each
tree, p(T | R) is either 0 or 1, so all work is per-
formed by the prior over the set of elementary trees.
The DP-TSG contains a DP prior for each c ? V
(Table 5 defines further notation). We generate ?c, e?
tuples as follows:
?c|c, ?c, P0(?|c) ? DP (?c, P0)
e|?c ? ?c
The data likelihood is given by the latent state z and
the parameters ?: p(z|?) =?z?z ?
nc,e(z)
c,e . Integrat-
ing out the parameters, we have:
p(z) =
?
c?V
?
e(?cP0(e|c))nc,e(z)
?nc,?(z)c
(2)
where xn = x(x + 1) . . . (x + n ? 1) is the rising
factorial. (?A.1 contains ancillary details.)
Base Distribution The base distribution P0 is the
same maximum likelihood PCFG used in the Stan-
5We use the terms tree fragment and elementary tree inter-
changeably.
NP+
PUNC-(1)
?
N+
Jacques
N-
Chirac
PUNC+(2)
?
Figure 1: Example of two conflicting sites of the same
type. Define the type of a site t(z, s) def= (?ns:0,?ns:1).
Sites (1) and (2) above have the same type since t(z, s1) =
t(z, s2). However, the two sites conflict since the prob-
abilities of setting bs1 and bs2 both depend on counts for
the tree fragment rooted at NP. Consequently, sites (1) and
(2) are not exchangeable: the probabilities of their assign-
ments depend on the order in which they are sampled.
ford parser.6,7 After applying the manual state splits,
we perform simple right binarization, collapse unary
rules, and replace rare words with their signatures
(Petrov et al, 2006).
For each non-terminal type c, we learn a stop prob-
ability sc ? Beta(1, 1). Under P0, the probability of
generating a rule A+ ? B? C+ composed of non-
terminals is
P0(A+ ? B? C+) = pMLE(A ? B C)sB(1?sC)
(3)
For lexical insertion rules, we add a penalty propor-
tional to the frequency of the lexical item:
P0(c ? t) = pMLE(c ? t)p(t) (4)
where p(t) is equal to the MLE unigram probabil-
ity of t in the treebank. Lexicalizing a rule makes it
very specific, so we generally want to avoid lexical-
ization with rare words. Empirically, we found that
this penalty reduces overfitting.
Type-based Inference Algorithm To learn the pa-
rameters ? we use the collapsed, block Gibbs sam-
pler of Liang et al (2010). We sample binary vari-
ables bs associated with each non-terminal node/site
in the treebank. The key idea is to select a block
of exchangeable sites S of the same type that do not
conflict (Figure 1). Since the sites in S are exchange-
able, we can set bS randomly so long as we know m,
the number of sites with bs = 1. Because this algo-
rithm is a not a contribution of this paper, we refer
the reader to Liang et al (2010).
6The Stanford parser is a product model, so the results in ?5.1
include the contribution of a dependency parser.
7Bansal and Klein (2010) also experimented with symbol re-
finement in an all-fragments (parametric) TSG for English.
730
After each Gibbs iteration, we sample each sc di-
rectly using binomial-Beta conjugacy. We re-sample
the DP concentration parameters ?c with the auxil-
iary variable procedure of West (1995).
Decoding We compute the rule score of each tree
fragment from a single grammar sample as follows:
?c,e =
nc,e(z) + ?cP0(e|c)
nc,?(z) + ?c
(5)
To make the grammar more robust, we also include
all CFG rules in P0 with zero counts inn. Scores for
these rules follow from (5) with nc,e(z) = 0.
For decoding, we note that the derivations of a
TSG are a CFGparse forest (Vijay-Shanker andWeir,
1993). As such, we can use a Synchronous Context
Free Grammar (SCFG) to translate the 1-best parse
to its derivation. Consider a unique tree fragment ei
rooted at X with frontier ?, which is a sequence of
terminals and non-terminals. We encode this frag-
ment as an SCFG rule of the form
[X ? ? , X ? i, Y1, . . . , Yn] (6)
where Y1, . . . , Yn is the sequence of non-terminal
nodes in ?.8 During decoding, the input is re-
written as a sequence of tree fragment (rule) indices
{i, j, k, . . . }. Because the TSG substitution operator
always applies to the leftmost frontier node, we can
deterministically recover the monolingual parse with
top-down re-writes of ?.
The SCFG formulation has a practical benefit: we
can take advantage of the heavily-optimized SCFG
decoders for machine translation. We use cdec
(Dyer et al, 2010) to recover the Viterbi derivation
under a DP-TSG grammar sample.
5 Experiments
5.1 Standard Parsing Experiments
We evaluate parsing accuracy of the Stanford and
DP-TSG models (Table 6). For comparison, we also
include the Berkeley parser (Petrov et al, 2006).9
For the DP-TSG, we initialized all bs with fair coin
tosses and ran for 400 iterations, after which likeli-
hood stopped improving.
8This formulation is due to Chris Dyer.
9Training settings: right binarization, no parent annotation,
six split-merge cycles, and random initialization.
Leaf Ancestor Evalb
Corpus Sent LP LR F1 EX%
PA-PCFG 0.793 0.812 68.1 67.0 67.6 10.5
DP-TSG 0.823 0.842 75.6 76.0 75.8 15.1
Stanford 0.843 0.861 77.8 79.0 78.4 17.5
Berkeley 0.880 0.891 82.4 82.0 82.2 21.4
Table 6: Standard parsing experiments (test set, sentences
? 40 words). All parsers exceed 96% tagging accuracy.
Berkeley and DP-TSG results are the average of three in-
dependent runs.
We report two different parsing metrics. Evalb
is the standard labeled precision/recall metric.10
Leaf Ancestor measures the cost of transforming
guess trees to the reference (Sampson and Babar-
czy, 2003). It was developed in response to the non-
terminal/terminal ratio bias of Evalb, which penal-
izes flat treebanks like the FTB. The range of the
score is between 0 and 1 (higher is better). We report
micro-averaged (whole corpus) and macro-averaged
(per sentence) scores.
In terms of parsing accuracy, the Berkeley parser
exceeds both Stanford and DP-TSG. This is consis-
tent with previous experiments for French by Sed-
dah et al (2009), who show that the Berkeley parser
outperforms other models. It also matches the or-
dering for English (Cohn et al, 2010; Liang et al,
2010). However, the standard baseline for TSGmod-
els is a simple parent-annotated PCFG (PA-PCFG).
For English, Liang et al (2010) showed that a similar
DP-TSG improved over PA-PCFG by 4.2% F1. For
French, our gain is a more substantial 8.2% F1.
5.2 MWE Identification Experiments
Table 7 lists overall and per-category MWE identifi-
cation results for the parsing models. Although DP-
TSG is less accurate as a general parsing model, it is
more effective at identifying MWEs.
The predominant approach to MWE identification
is the combination of lexical association measures
(surface statistics) with a binary classifier (Pecina,
2010). A state-of-the-art, language independent
package that implements this approach for higher
order n-grams is mwetoolkit (Ramisch et al,
2010).11 In Table 8 we compare DP-TSG to both
10Available at http://nlp.cs.nyu.edu/evalb/ (v.20080701).
11Available at http://multiword.sourceforge.net/. See ?A.2 for
731
#gold Stanford DP-TSG Berkeley
MWET 3 0.0 0.0 0.0
MWV 26 64.0 57.7 50.7
MWA 8 26.1 32.2 29.8
MWN 456 64.1 67.6 67.1
MWD 15 70.3 65.5 70.1
MWPRO 17 73.7 78.0 76.2
MWADV 220 74.6 72.7 70.4
MWP 162 81.3 80.5 77.7
MWC 47 83.5 83.5 80.8
954 70.1 71.1 69.6
Table 7: MWE identification per category and overall re-
sults (test set, sentences ? 40 words). MWI and MWCL
do not occur in the test set.
Model F1
mwetoolkit All 15.4
PA-PCFG 32.6
mwetoolkit Filter 34.7
PA-PCFG+Features 63.1
DP-TSG 71.1
Table 8: MWE identification F1 of the best parsing model
vs. the mwetoolkit baseline (test set, sentences ? 40
words). PA-PCFG+Features includes the grammar fea-
tures in Table 4, which is the CFG from which the TSG is
extracted. For mwetoolkit, All indicates the inclusion
of all n-grams in the training corpus. Filter indicates pre-
filtering of the training corpus by removing rare n-grams
(see ?A.2 for details).
mwetoolkit and the CFG from the which the TSG
is extracted. The TSG-based parsing model outper-
forms mwetoolkit by 36.4% F1 while providing
syntactic subcategory information.
6 Discussion
Automatic learning methods run the risk of produc-
ing uninterpretable models. However, the DP-TSG
model learns useful generalizations over MWEs. A
sample of the rules is given in Table 9. Some spe-
cific sequences like ?[MWN [coup de N]]? are part of
the grammar: such rules can indeed generate quite
a few MWEs, e.g., coup de pied ?kick?, coup de
coeur, coup de foudre ?love at first sight?, coup de
main ?help?, coup d??tat, coup de gr?ce (note that
only some of these MWEs are seen in the training
configuration details.
MWN MWV MWP
soci?t?s de N sous - V de l?ordre de
prix de N faire N y compris
coup de N V les moyens au N de
N d??tat V de N en N de
N de N V en N ADV de
N ? N
Table 9: Sample of the TSG rules learned.
MWN
N
tour
P
de
N
passe
-
-
N
passe
(a) Reference
NP
N
tour
PP
P
de
NP
MWN
N
passe
-
-
N
passe
(b) DP-TSG
Figure 2: Example of an MWE error for tour de passe-
passe ?magic trick?. (dev set)
data). For MWV, ?V de N? as in avoir de cesse ?give
no peace?, perdre de vue [lose from sight] ?forget?,
prendre de vitesse [take from speed] ?outpace?), is
learned. For prepositions, the grammar stores full
subtrees of MWPs, but can also generalize the struc-
ture of very frequent sequences: ?en N de? occurs in
manymultiword prepositions (e.g., en compagnie de,
en face de, en mati?re de, en terme de, en cours de,
en faveur de, en raison de, en fonction de). The TSG
grammar thus provides a categorization of MWEs
consistent with the Lexicon-Grammar. It also learns
verbal phrases which contain discontinuous MWVs
due to the insertion of an adverb or negation such as
?[VN [MWV va] [MWADV d?ailleurs] [MWV bon train]]?
[go indeed well], ?[VN [MWV a] [ADV jamais] [MWV
?t? question d?]]? [has never been in question].
A significant fraction of errors for MWNs occur
with adjectives that are not recognized as part of the
MWE. For example, since ?tablissements priv?s ?pri-
vate corporation? is unseen in the training data, it is
not found. Sometimes the parser did not recognize
the whole structure of an MWE. Figure 2 shows an
example where the parser only found a subpart of the
MWN tour de passe-passe ?magic trick?.
Other DP-TSG errors are due to inconsistencies in
the FTB annotation. For example, sous pr?texte que
732
MWC
P
sous
N
pr?texte
C
que
(a) Reference
PP
P
sous
NP
N
pr?texte
Ssub
C
que
(b) Reference
Figure 3: Example of an inconsistent FTB annotation for
sous pr?texte que ?on the pretext of?.
?on the pretext of? is tagged as both MWC and as a
regular PP structure (Figure 3). However, the parser
always assigns a MWC structure, which is a better
analysis than the gold annotation. We expect that
more consistent annotation would help the DP-TSG
more than the CFG-based parsers.
The DP-TSG is not immune to false positives: in
Le march? national, fait-on remarquer, est enfin en
r?gression . . . ?The national economy, people at last
note, is going down? the parser tags march? national
asMWN. As noted, the boundary of what should and
should not count as an MWE can be fuzzy, and it is
therefore hard to assess whether or not this should be
an MWE. The FTB does not mark it as one.
There are multiple examples were the DP-TSG
found the MWE whereas Stanford (its base distribu-
tion) did not, such as in Figure 4. Note that the ?N
P N? structure is quite frequent for MWNs, but the
TSG correctly identifies the MWADV in emplois ?
domicile [jobs at home] ?homeworking?.
7 Related Work
There is a voluminous literature on MWE identi-
fication. Here we review closely related syntax-
based methods.12 The linguistic and computa-
tional attractiveness of lexicalized grammars for
modeling idiosyncratic constructions in French was
identified by Abeill? (1988) and Abeill? and Sch-
abes (1989). They manually developed a small
Tree Adjoining Grammar (TAG) of 1,200 elemen-
tary trees and 4,000 lexical items that included
MWEs. The classic statistical approach to MWE
identification, Xtract (Smadja, 1993), used an in-
12See Seretan (2011) for a comprehensive survey of syntax-
based methods for MWE identification. For an overview of n-
gram methods like mwetoolkit, see Pecina (2010).
MWN
N
campagne
P
de
N
promotion
(a) DP-TSG
NP
N
campagne
PP
P
de
NP
N
promotion
(b) Stanford
NP
N
emplois
MWADV
P
?
N
domicile
(c) DP-TSG
NP
N
emplois
PP
P
?
NP
N
domicile
(d) Stanford
Figure 4: Correct analyses by DP-TSG. (dev set)
cremental parser in the third stage of its pipeline
to identify predicate-argument relationships. Lin
(1999) applied information-theoretic measures to
automatically-extracted dependency relations to find
MWEs. To our knowledge, Wehrli (2000) was the
first to use syntactically annotated corpora to im-
prove a parser for MWE identification. He pro-
posed to rank analyses of a symbolic parser based
on the presence of collocations, although details of
the ranking function were not provided.
The most similar work to ours is that of Nivre
and Nilsson (2004), who converted a Swedish cor-
pus into two versions: one in which MWEs were
left as tokens, and one in which they were merged.
On the first version, they showed that a deterministic
dependency parser could identify MWEs at 71.1%
F1, albeit without subcategory information. On
the second version?which simulated perfect MWE
identification?they showed that labeled attachment
improved by about 1%.
Recent statistical parsing work on French has in-
cluded Stochastic Tree Insertion Grammars (STIGs),
which are related to TAGs, but with a restricted ad-
junction operation.13 Seddah et al (2009) and Sed-
dah (2010) showed that STIGs underperform CFG-
based parsers on the FTB. In their experiments,
MWEs were concatenated.
13TSGs differ from TAGs and STIGs in that they do not in-
clude an adjunction operator.
733
8 Conclusion
The main result of this paper is that an existing sta-
tistical parser can achieve a 36.4% F1 absolute im-
provement for MWE identification over a state-of-
the-art n-gram surface statistics package. Parsers
also provide syntactic subcategorization, and do not
require pre-filtering of the training data. We have
also demonstrated that TSGs can capture idiomatic
usage better than a PCFG.While the DP-TSG, which
is a relatively new parsing model, still lags state-of-
the-art parsers in terms of overall labeling accuracy,
we have shown that it is already very effective for
other tasks like MWE identification. We plan to im-
prove the DP-TSG by experimenting with alternate
parsing objectives (Cohn et al, 2010), lexical rep-
resentations, and parameterizations of the base dis-
tribution. A particularly promising base distribution
is the latent variable PCFG learned by the Berkeley
parser. However, initial experiments with this distri-
bution were negative, so we leave further develop-
ment to future work.
We chose French for these experiments due to the
pervasiveness ofMWEs and the availability of an an-
notated corpus. However, MWE lists and syntactic
treebanks exist for many of the world?s major lan-
guages. We will investigate automatic conversion of
these treebanks (by flattening MWE bracketings) for
MWE identification.
A Appendix
A.1 Notes on the Rising Factorial
The rising factorial?also known as the ascending
factorial or Pochhammer symbol?arises in the con-
text of samples from a Dirichlet process (see Prop.
3 of Antoniak (1974) for details). For a positive in-
teger n and a complex number x, the rising factorial
xn is defined14 by
xn = x(x + 1) . . . (x + n? 1)
=
n?
j=1
(x + j ? 1) (7)
The rising factorial can be generalized to a com-
plex number ? with the gamma function:
x? = ?(x + ?)?(x) (8)
14We adopt the notation of Knuth (1992).
where x0 ? 1.
In our type-based sampler, we computed (7) di-
rectly in a dynamic program. We found that (8) was
prohibitively slow for sampling.
A.2 mwetoolkit Configuration
We configured mwetoolkit15 with the four stan-
dard lexical features: the maximum likelihood esti-
mator, Dice?s coefficient, pointwise mutual informa-
tion (PMI), and Student?s t-score. We added the POS
sequence for each n-gram as a single feature. We re-
moved the web counts features to make the experi-
ments comparable. To compensate for the absence
of web counts, we computed the lexical features us-
ing the gold lemmas from the FTB instead of using
an automatic lemmatizer.
Since MWE n-grams only account for a small
fraction of the n-grams in the corpus, we filtered the
training and test sets by removing all n-grams that
occurred once. To further balance the proportion of
MWEs, we trained on all valid MWEs plus 10x ran-
domly selected non-MWE n-grams. This proportion
matches the fraction of MWE/non-MWE tokens in
the FTB. Since we generated a random training set,
we reported the average of three independent runs.
We created feature vectors for the training n-
grams and trained a binary Support Vector Machine
(SVM) classifier with Weka (Hall et al, 2009). Al-
though mwetoolkit defaults to a linear kernel,
we achieved higher accuracy on the development set
with an RBF kernel.
The FTB is sufficiently large for the corpus-based
methods implemented in mwetoolkit. Ramisch
et al (2010)?s experiments were on Genia, which
contains 18k sentences and 490k tokens, similar to
the FTB. Their test set had 895 sentences, smaller
than ours. They reported 30.6% F1 for their task
against an Xtract baseline, which only obtained 7.3%
F1. These results are comparable inmagnitude to our
FTB results.
Acknowledgments We thank Marie Candito, Chris Dyer,
Dan Flickinger, Percy Liang, Carlos Ramisch, Djam?
Seddah, and Val Spitkovsky for their helpful comments.
The first author is supported by a National Defense Sci-
ence and Engineering Graduate (NDSEG) fellowship.
15We re-implemented mwetoolkit in Java for compatibil-
ity with Weka and our pre-processing routines.
734
References
A. Abeill? and Y. Schabes. 1989. Parsing idioms in lexicalized
TAGs. In EACL.
A. Abeill?, L. Cl?ment, and A. Kinyon, 2003. Building a tree-
bank for French, chapter 10. Kluwer.
A. Abeill?. 1988. Parsing Frenchwith TreeAdjoiningGrammar:
some linguistic accounts. In COLING.
C. E. Antoniak. 1974. Mixtures of Dirichlet processes with ap-
plications to Bayesian nonparametric problems. The Annals
of Statistics, 2(6):1152?1174.
A. Arun and F. Keller. 2005. Lexicalization in crosslinguistic
probabilistic parsing: The case of French. In ACL.
A. Arun. 2004. Statistical parsing of the French treebank. Tech-
nical report, University of Edinburgh.
M. Bansal and D. Klein. 2010. Simple, accurate parsing with
an all-fragments grammar. In ACL.
R. Bod. 1992. A computation model of language performance:
Data-Oriented Parsing. In COLING.
M. Candito and B. Crabb?. 2009. Improving generative statisti-
cal parsing with semi-supervised word clustering. In IWPT.
M. Candito, B. Crabb?, and P. Denis. 2010. Statistical French
dependency parsing: treebank conversion and first results. In
LREC.
M. Carpuat and M. Diab. 2010. Task-based evaluation of mul-
tiword expressions: a pilot study in statistical machine trans-
lation. In HLT-NAACL.
T. Cohn, S. Goldwater, and P. Blunsom. 2009. Inducing compact
but accurate tree-substitution grammars. In HLT-NAACL.
T. Cohn, P. Blunsom, and S. Goldwater. 2010. Inducing tree-
substitution grammars. JMLR, 11:3053?3096, Nov.
B. Crabb? and M. Candito. 2008. Exp?riences d?analyse syn-
taxique statistique du fran?ais. In TALN.
A. Dybro-Johansen. 2004. Extraction automatique de gram-
maires ? partir d?un corpus fran?ais. Master?s thesis, Univer-
sit? Paris 7.
C. Dyer, A. Lopez, J. Ganitkevitch, J.Weese, F. Ture, et al 2010.
cdec: A decoder, alignment, and learning framework for
finite-state and context-free translation models. In ACL Sys-
tem Demonstrations.
M. Gross. 1986. Lexicon-Grammar: the representation of com-
pound words. In COLING.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and
I. H. Witten. 2009. The WEKA data mining software: an
update. SIGKDD Explorations Newsletter, 11:10?18.
D. Hogan, C. Cafferkey, A. Cahill, and J. van Genabith. 2007.
Exploiting multi-word units in history-based probabilistic
generation. In EMNLP-CoNLL.
D. Klein and C. D. Manning. 2003. Accurate unlexicalized
parsing. In ACL.
D. E. Knuth. 1992. Two notes on notation. American Mathe-
matical Monthly, 99:403?422, May.
R. Levy and G. Andrew. 2006. Tregex and Tsurgeon: tools for
querying and manipulating tree data structures. In LREC.
P. Liang, M. I. Jordan, and D. Klein. 2010. Type-based MCMC.
In HLT-NAACL.
D. Lin. 1999. Automatic identification of non-compositional
phrases. In ACL.
J. Nivre and J. Nilsson. 2004. Multiword units in syntactic pars-
ing. In Methodologies and Evaluation of Multiword Units in
Real-World Applications (MEMURA).
T. J. O?Donnell, J. B. Tenenbaum, and N. D. Goodman. 2009.
Fragment grammars: Exploring computation and reuse in
language. Technical report, MIT Computer Science and Arti-
ficial Intelligence Laboratory Technical Report Series, MIT-
CSAIL-TR-2009-013.
P. Pecina. 2010. Lexical association measures and collocation
extraction. Language Resources and Evaluation, 44:137?
158.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006. Learning
accurate, compact, and interpretable tree annotation. In ACL.
M. Post and D. Gildea. 2009. Bayesian learning of a tree sub-
stitution grammar. In ACL-IJCNLP, Short Papers.
C. Ramisch, A. Villavicencio, and C. Boitet. 2010. mwe-
toolkit: a framework for multiword expression identifi-
cation. In LREC.
P. Rayson, S. Piao, S. Sharoff, S. Evert, and B. Moir?n. 2010.
Multiword expressions: hard going or plain sailing? Lan-
guage Resources and Evaluation, 44:1?5.
I. A. Sag, T. Baldwin, F. Bond, A. Copestake, and D. Flickinger.
2002. Multiword expressions: A pain in the neck for NLP.
In CICLing.
G. Sampson and A. Babarczy. 2003. A test of the leaf-ancestor
metric for parse accuracy. Natural Language Engineering,
9:365?380.
R. Scha, 1990. Taaltheorie en taaltechnologie: competence en
performance, pages 7?22. Landelijke Vereniging van Neer-
landici (LVVNjaarboek).
N. Schluter and J. Genabith. 2007. Preparing, restructuring,
and augmenting a French treebank: Lexicalised parsers or
coherent treebanks? In Pacling.
D. Seddah, M. Candito, and B. Crabb?. 2009. Cross parser
evaluation and tagset variation: a French treebank study. In
IWPT.
D. Seddah. 2010. Exploring the Spinal-STIG model for parsing
French. In LREC.
V. Seretan. 2011. Syntax-Based Collocation Extraction, vol-
ume 44 of Text, Speech, and Language Technology. Springer.
F. Smadja. 1993. Retrieving collocations from text: Xtract.
Computational Linguistics, 19:143?177.
K. Vijay-Shanker and D. J. Weir. 1993. The use of shared forests
in tree adjoining grammar parsing. In EACL.
E. Wehrli. 2000. Parsing and collocations. In Natural Lan-
guage Processing?NLP 2000, volume 1835 of Lecture Notes
in Computer Science, pages 272?282. Springer.
M. West. 1995. Hyperparameter estimation in Dirichlet process
mixture models. Technical report, Duke University.
735
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1625?1630,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Deriving adjectival scales from continuous space word representations
Joo-Kyung Kim
Department of Computer Science and Engineering
The Ohio State University
Columbus, OH 43210, USA
kimjook@cse.ohio-state.edu
Marie-Catherine de Marneffe
Department of Linguistics
The Ohio State University
Columbus, OH 43210, USA
mcdm@ling.ohio-state.edu
Abstract
Continuous space word representations ex-
tracted from neural network language mod-
els have been used effectively for natural lan-
guage processing, but until recently it was not
clear whether the spatial relationships of such
representations were interpretable. Mikolov
et al (2013) show that these representations
do capture syntactic and semantic regularities.
Here, we push the interpretation of continuous
space word representations further by demon-
strating that vector offsets can be used to de-
rive adjectival scales (e.g., okay < good < ex-
cellent). We evaluate the scales on the indirect
answers to yes/no questions corpus (de Marn-
effe et al, 2010). We obtain 72.8% accuracy,
which outperforms previous results (?60%)
on this corpus and highlights the quality of the
scales extracted, providing further support that
the continuous space word representations are
meaningful.
1 Introduction
There has recently been a surge of interest for deep
learning in natural language processing. In particu-
lar, neural network language models (NNLMs) have
been used to learn distributional word vectors (Ben-
gio et al, 2003; Schwenk, 2007; Mikolov et al,
2010): the models jointly learn an embedding of
words into an n-dimensional feature space. One of
the advantages put forth for such distributed rep-
resentations compared to traditional n-gram mod-
els is that similar words are likely to have similar
vector representations in a continuous space model,
whereas the discrete units of an n-gram model do
not exhibit any inherent relation with one another.
It has been shown that the continuous space repre-
sentations improve performance in a variety of NLP
tasks, such as POS tagging, semantic role labeling,
named entity resolution, parsing (Collobert and We-
ston, 2008; Turian et al, 2010; Huang et al, 2012).
Mikolov et al (2013) show that there are some
syntactic and semantic regularities in the word rep-
resentations learned, such as the singular/plural rela-
tion (the difference of singular and plural word vec-
tors are equivalent: apple ? apples ? car ? cars ?
family ? families) or the gender relation (a mascu-
line noun can be transformed into the feminine form:
king ? man + woman ? queen).
We extend Mikolov et al (2013)?s approach and
explore further the interpretation of the vector space.
We show that the word vectors learned by NNLMs
are meaningful: we can extract scalar relationships
between adjectives (e.g., bad < okay < good < ex-
cellent), which can not only serve to build a senti-
ment lexicon but also be used for inference. To eval-
uate the quality of the scalar relationships learned
by NNLMs, we use the indirect yes/no question an-
swer pairs (IQAP) from (de Marneffe et al, 2010),
where scales between adjectives are needed to infer
a yes/no answer from a reply without explicit yes or
no such as Was the movie good? It was excellent.
Our method reaches 72.8% accuracy, which is the
best result reported so far when scales are used.
2 Previous work
We use the continuous word representations from
(Mikolov et al, 2011), extracted from a recurrent
neural network language model (RNNLM), whose
1625
three-layer architecture is represented in Figure 1.
sigmoid
softmax
U
W
w(t)
s(t-1)
s(t) y(t)
V
Figure 1: The architecture of the RNNLM.
In the input layer, w(t) is the input word repre-
sented by 1-of-N coding at time t when the vocabu-
lary size is N . When there are M nodes in the hid-
den layer, the number of connections between the
input layer and the hidden layer is NM and the con-
nections can be represented by a matrix U .
The hidden layer is also connected recurrently to
the context s(t ? 1) at time t ? 1 (s(0) is initial-
ized with small values like 0.1). The connections
between the previous context and the hidden layer
are represented by a matrix W . The dimension-
ality of the word representations is controlled by
the size of W . The output of the hidden layer is
s(t) = f(Uw(t) + Ws(t ? 1)), where f is a sig-
moid function.
Because the inputs of the hidden layer consist of
the word w(t) and the previous hidden layer output
s(t ? 1), the current context of the RNN is influ-
enced by the current word and the previous context.
Therefore, we can regard that the continuous repre-
sentations from the RNNLM exploit the context im-
plicitly considering the word sequence information
(Mikolov et al, 2010).
V is a N by M matrix representing the connec-
tions between the hidden layer and the output layer.
The final output is y(t) = g(V s(t)), where g is a
softmax function to represent the probability distri-
bution over all the words in the vocabulary.
When the RNN is trained by the back propagation
algorithm, we can regard the ith column vector of U
as the continuous representation of the ith word in
the vocabulary since the column was adjusted corre-
spondingly to the ith element of w(t). Because the
s(t) outputs of two input words will be similar when
they have similar s(t? 1) values, the corresponding
column vectors of the words will also be similar.
Mikolov et al (2013) showed that constant vector
offsets of word pairs can represent linguistic regu-
larities. Let wa and wb denote the vectors for the
words a and b, respectively. Then the vector offset
of the word pair is wa ? wb. If a and b are syn-
tactically or semantically related, the vector offset
can be interpreted as a transformation of the syn-
tactic form or the meaning. The offset can also be
added to another word vector c. The word vector
nearest to wa ? wb + wc would be related to word c
with the syntactic or semantic difference as the dif-
ference between a and b, as it is the case for the
king, man, and woman example, where king ? man
+ woman would approximately represent king with
feminine gender (i.e., queen). They also tried to use
the continuous representations generated by Latent
Semantic Analysis (LSA) (Landauer et al, 1998).
However, the results using LSA were worse because
LSA is a bag-of-words model, in which it is difficult
to exploit word sequence information as the context.
For all the experiments in this paper, we use the
precomputed word representations generated by the
RNNLM from (Mikolov et al, 2013). Their RNN is
trained with 320M words from the Broadcast News
data (the vocabulary size is 82,390 words), and we
used word vectors with a dimensionality of 1,600
(the highest dimensionality provided).1 We stan-
dardized the dataset so that the mean and the vari-
ance of the representations are 0 and 1, respec-
tively.2
3 Deriving adjectival scales
Here we explore further the interpretation of word
vectors. Assuming that the transformation of form
or meaning represented by the vector offset is lin-
ear, an intermediate vector between two word vec-
tors would represent some ?middle? form or mean-
ing. For example, given the positive and superlative
forms of an adjective (e.g., good and best), we ex-
pect that the word representation in the middle of
1We also experimented with smaller dimensions, but con-
sistent with the analyses in (Mikolov et al, 2013), the highest
dimensionality gave better results.
2http://www.fit.vutbr.cz/?imikolov/
rnnlm/word_projections-1600.txt.gz
1626
Input words Words with highest cosine similarities to the mean vector
good:best better: 0.738 strong: 0.644 normal: 0.619 less: 0.609
bad:worst terrible: 0.726 great: 0.678 horrible: 0.674 worse: 0.665
slow:slowest slower: 0.637 sluggish: 0.614 steady: 0.558 brisk: 0.543
fast:fastest faster: 0.645 slower: 0.602 quicker: 0.542 harder: 0.518
Table 1: Words with corresponding vectors closest to the mean of positive:superlative word vectors.
First word (-) 1st quarter Half 3rd quarter Second word (+)
furious 1 angry 0.632 unhappy 0.640 pleased 0.516 happy 1
furious 1 angry 0.615 tense 0.465 quiet 0.560 calm 1
terrible 1 horrible 0.783 incredible 0.714 wonderful 0.772 terrific 1
cold 1 mild 0.348 warm 0.517 sticky 0.424 hot 1
ugly 1 nasty 0.672 wacky 0.645 lovely 0.715 gorgeous 1
Table 2: Adjectival scales extracted from the RNN: each row represent a scale, and for each intermediate point the
closest word in term of cosine similarity is given.
them will correspond to the comparative form (i.e.,
better). To extract the ?middle? word between two
word vectors wa and wb, we take the vector offset
wa?wb divided by 2, and addwb: wb+(wa?wb)/2.
The result corresponds to the midpoint between the
two words. Then, we find the word whose cosine
similarity to the midpoint is the highest.
Table 1 gives some positive:superlative pairs and
the top four closest words to the mean vectors, where
the distance metric is the cosine similarity. The
correct comparative forms (in bold) are quite close
to the mean vector of the positive and superlative
form vectors, highlighting the fact that there is some
meaningful interpretation of the vector space: the
word vectors are constituting a scale.
We can extend this idea of extracting an or-
dering between two words. For any two seman-
tically related adjectives, intermediate vectors ex-
tracted along the line connecting the first and sec-
ond word vectors should exhibit scalar properties, as
seen above for the positive-comparative-superlative
triplets. If we take two antonyms (furious and
happy), words extracted at the intermediate points
x1, x2 and x3 should correspond to words lying on
a scale of happiness (from ?less furious? to ?more
happy?), as illustrated in Figure 2. Table 2 gives
some adjectival scales that we extracted from the
continuous word space, using antonym pairs. We
picked three points with equal intervals on the line
from the first to the second word (1st quarter, half
and 3rd quarter). The extracted scales look quite
reasonable: the words form a continuum from more
negative to more positive meanings.
x2
a=furious
b=happy
x1
x3angry
unhappy
pleased
Figure 2: An example of vectors with the highest cosine
similarity to intermediate points on the line between furi-
ous and happy.
Tables 1 and 2 demonstrate that the word vector
space is interpretable: intermediate vectors between
two word vectors represent a semantic continuum.
4 Evaluation: Indirect answers to yes/no
questions
To evaluate the quality of the adjective scales learned
by the neural network approach, we use the cor-
pus of indirect answers to yes/no questions created
by (de Marneffe et al, 2010), which consists of
question-answer pairs involving gradable modifiers
to test scalar implicatures. We focus on the 125 pairs
in the corpus where both the question and answer
contain an adjective: e.g., Is Obama qualified? I
think he?s young.3 Each question-answer pair has
3These 125 pairs correspond to the ?Other adjective? cate-
gory in (de Marneffe et al, 2010).
1627
been annotated via Mechanical Turk for whether the
answer conveys yes, no or uncertain.
4.1 Method
The previous section showed that we can draw a line
passing through an adjective and its antonym and
that the words extracted along the line are roughly
semantically ordered. To infer a yes or no answer in
the case of the IQAP corpus, we use the following
approach illustrated with the Obama example above
(Figure 3). Using WordNet 3.1 (Fellbaum, 1998),
we look for an antonym of the adjective in the ques-
tion qualified: unqualified is retrieved. Since the
scales extracted are only roughly ordered, to infer
yes when the question and answer words are very
close, we set the decision boundary perpendicular
to the line connecting the two words and passing
through the midpoint of the line.
Since the answer word is young, we check
whether young is in the area including qualified or
in the other area. We infer a yes answer in the for-
mer case, and a no answer in the latter case. If young
is on the boundary, we infer uncertain. If a sentence
contains a negation (e.g., Are you stressed? I am
not peaceful.), we compute the scale for stressed-
peaceful and then reverse the answer obtained, sim-
ilarly to what is done in (de Marneffe et al, 2010).
qualified
unqualified
young
Figure 3: An example of the decision boundary given
qualified as the question and young as the answer.
Since a word can have multiple senses and differ-
ent antonyms for the senses, it is important to select
the most appropriate antonym to build a more accu-
rate decision boundary. We consider all antonyms
across senses4 and select the antonym that is most
collinear with the question and the answer. For the
word vectors of the question wq, the ith antonym
wanti , and the answer wa, we select anti where
argmaxanti |cos(wq ? wa, wq ? wanti)|. Figure 4
schematically shows antonym selection when the
4Antonyms in WordNet can be directly opposed to a given
word or indirectly opposed via other words. When there are
direct antonyms for the question word, we only consider those.
Macro
Acc P R F1
de Marneffe (2010) 60.00 59.72 59.40 59.56
Mohtarami (2011) ? 62.23 60.88 61.55
RNN model 72.80 69.78 71.39 70.58
Table 3: Score (%) comparison on the 125 scalar adjec-
tive pairs in the IQAP corpus.
question is good and the answer is excellent: bad
and evil are the antonym candidates of good.
Because the absolute cosine similarity of good-
excellent to good-bad is higher than to good-evil, we
choose bad as the antonym in this case.
bad
excellent
good
evil
Figure 4: An example of antonym selection.
4.2 Results and discussion
Table 3 compares our results with previous ones
where adjectival scales are considered: de Marn-
effe et al (2010) propose an unsupervised approach
where scales are learned from distributional infor-
mation in a Web corpus; Mohtarami et al (2011)?s
model is similar to ours but uses word represen-
tations obtained by LSA and a word sense disam-
biguation system (Zhong and Ng, 2010) to choose
antonyms. To compare with Mohtarami et al
(2011), we use macro-averaged precision and recall
for yes and no. For the given metrics, our model sig-
nificantly outperforms the previous ones (p < 0.05,
McNemar?s test).
Mohtarami et al (2011) present higher numbers
obtained by replacing the answer words with their
synonyms in WordNet. However, that approach fails
to capture orderings. Two words of different degree
are often regarded as synonyms: even though furi-
ous means extremely angry, furious and angry are
synonyms in WordNet. Therefore using synonyms,
the system will output the same answer irrespective
of the order in the pair. Mohtarami et al (2012)
also presented results on the interpretation of indi-
rect questions on the IQAP corpus, but their method
1628
-20 -15 -10 -5 0 5 10 15 20-25
-20
-15
-10
-5
0
5
10
15
20
goodbad
terrible confident
diffident
sure
happy
unhappy
delighted
qualified
unqualified
young
dim 1
dim
 2
Figure 5: Question words (bold), their antonyms (italic), and answer words (normal) of four pairs from the IQAP
dataset. The words are visualized by MDS.
did not involve learning or using scalar implicatures.
Figure 5 gives a qualitative picture: the question
words, antonyms and answer words for four of the
IQAP pairs are visualized in 2D space by multi-
dimensional scaling (MDS). Note that MDS intro-
duces some distortion in the lower dimensions. Bul-
let markers correspond to words in the same pair.
Question words, antonyms, and answer words are
displayed by bold, italic, and normal fonts, respec-
tively. In the Obama example previously mentioned
(Is Obama qualified? I think he?s young.), the ques-
tion word is qualified and the answer word is young.
In Figure 5, qualified is around (2,-20) while its
antonym unqualified is around (-6,-24). Since young
is around (-7,-8), we infer that young is semanti-
cally closer to unqualified which corroborates with
the Turkers? intuitions in this case. (1), (2) and (3)
give the other examples displayed in Figure 5.
(1) A: Do you think she?d be happy with this
book?
B: I think she?d be delighted by it.
(2) A: Do you think that?s a good idea?
B: It?s a terrible idea.
(3) A: The president is promising support for
Americans who have suffered from this
hurricane. Are you confident you are
going to be getting that?
B: I?m not so sure about my insurance
company.
In (1), delighted is stronger than happy, leading to
a yes answer, whereas in (2), terrible is weaker than
good leading to a no answer. In (3), the presence of
a negation will reverse the answer inferred, leading
to no.
5 Conclusion
In this paper we give further evidence that the rela-
tionships in the continuous vector space learned by
recurrent neural network models are interpretable.
We show that using vector offsets, we can success-
fully learn adjectival scales, which are useful for
scalar implicatures, as demonstrated by the high re-
sults we obtain on the IQAP corpus.
Acknowledgements
We thank Eric Fosler-Lussier and the anonymous re-
viewers for their helpful comments on previous ver-
sions of this paper.
1629
References
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. The Journal of Machine Learning Re-
search, 3:1137?1155.
Ronan Collobert and Jason Weston. 2008. A unified ar-
chitecture for natural language processing: deep neu-
ral networks with multitask learning. In Proceedings
of the 25th international conference on Machine learn-
ing, pages 160?167.
Marie-Catherine de Marneffe, Christopher D. Manning,
and Christopher Potts. 2010. Was it good? It was
provocative. Learning the meaning of scalar adjec-
tives. In Proceedings of the 48th Meeting of the Asso-
ciation for Computational Linguistics, pages 167?176.
Christiane Fellbaum. 1998. WordNet: An electronic lex-
ical database. MIT Press.
Eric H. Huang, Richard Socher, Christopher D. Manning,
and Andrew Y Ng. 2012. Improving word representa-
tions via global context and multiple word prototypes.
In Proceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 873?882.
Association for Computational Linguistics.
Thomas K. Landauer, Peter W. Foltz, and Darrell La-
ham. 1998. An introduction to latent semantic analy-
sis. Discourse Processes, 25:259?284.
Tomas Mikolov, Martin Karafia?t, Luka?s? Burget, Jan Cer-
nocky, and Sanjeev Khudanpur. 2010. Recurrent neu-
ral network based language model. In Proceedings of
Interspeech, pages 1045?1048.
Tomas Mikolov, Daniel Povey, Luka?s? Burget, and Jan
Cernocky. 2011. Strategies for training large scale
neural network language models. In Proceedings of
ASRU, pages 196?201.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic regularities in continuous space
word representations. In Proceedings of NAACL-HLT,
pages 746?751.
Mitra Mohtarami, Hadi Amiri, Man Lan, and Chew Lim
Tan. 2011. Predicting the uncertainty of sentiment
adjectives in indirect answers. In Proceedings of the
20th ACM international conference on Information
and knowledge management, pages 2485?2488.
Mitra Mohtarami, Hadi Amiri, Man Lan, Thanh Phu
Tran, and Chew Lim Tan. 2012. Sense sentiment sim-
ilarity: an analysis. In Proceedings of the 26th AAAI
Conference on Artificial Intelligence, pages 1706?
1712.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech & Language, 21(3):492?
518.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method for
semi-supervised learning. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 384?394.
Zhi Zhong and Hwee Tou Ng. 2010. It makes sense: a
wide-coverage word sense disambiguation system for
free text. In Proceedings of the ACL 2010 System
Demonstrations, pages 78?83.
1630
Parsing Models for Identifying
Multiword Expressions
Spence Green?
Stanford University
Marie-Catherine de Marneffe??
Stanford University
Christopher D. Manning?
Stanford University
Multiword expressions lie at the syntax/semantics interface and have motivated alternative
theories of syntax like Construction Grammar. Until now, however, syntactic analysis and
multiword expression identification have been modeled separately in natural language process-
ing. We develop two structured prediction models for joint parsing and multiword expression
identification. The first is based on context-free grammars and the second uses tree substitution
grammars, a formalism that can store larger syntactic fragments. Our experiments show that
both models can identify multiword expressions with much higher accuracy than a state-of-the-
art system based on word co-occurrence statistics.
We experiment with Arabic and French, which both have pervasive multiword expres-
sions. Relative to English, they also have richer morphology, which induces lexical sparsity
in finite corpora. To combat this sparsity, we develop a simple factored lexical representation
for the context-free parsing model. Morphological analyses are automatically transformed into
rich feature tags that are scored jointly with lexical items. This technique, which we call
a factored lexicon, improves both standard parsing and multiword expression identification
accuracy.
1. Introduction
Multiword expressions are groups of words which, taken together, can have un-
predictable semantics. For example, the expression part of speech refers not to some
aspect of speaking, but to the syntactic category of a word. If the expression is
altered in some ways?part of speeches, part of speaking, type of speech?then the
idiomatic meaning is lost. Other modifications, however, are permitted, as in the plural
parts of speech. These characteristics make multiword expressions (MWEs) difficult to
identify and classify. But if they can be identified, then the incorporation of MWE
knowledge has been shown to improve task accuracy for a range of NLP applications
? Department of Computer Science. E-mail: spenceg@stanford.edu.
?? Department of Linguistics. E-mail: mcdm@stanford.edu.
? Departments of Computer Science and Linguistics. E-mail: manning@stanford.edu.
Submission received: October 1, 2011; revised submission received: June 9, 2012; accepted for publication:
August 3, 2012.
No rights reserved. This work was authored as part of the Contributor?s official duties as an Employee of
the United States Government and is therefore a work of the United States Government. In accordance with
17 U.S.C. 105, no copyright protection is available for such works under U.S. law.
Computational Linguistics Volume 39, Number 1
including dependency parsing (Nivre and Nilsson 2004), supertagging (Blunsom and
Baldwin 2006), sentence generation (Hogan et al 2007), machine translation (Carpuat
and Diab 2010), and shallow parsing (Korkontzelos and Manandhar 2010).
The standard approach to MWE identification is n-gram classification. This tech-
nique is simple. Given a corpus, all n-grams are extracted, filtered using heuristics,
and assigned feature vectors. Each coordinate in the feature vector is a real-valued
quantity such as log likelihood or pointwise mutual information. A binary classifier
is then trained to render a MWE/non-MWE decision. All entries into the 2008 MWE
Shared Task (Evert 2008) utilized variants of this technique.
Broadly speaking, n-gram classification methods measure word co-occurrence. Sup-
pose that a corpus contains more occurrences of part of speech than parts of speech. Surface
statistics may erroneously predict that only the former is an MWE and the latter is not.
More worrisome is that the statistics for the two n-grams are separate, thus missing an
obvious generalization.
In this article, we show that statistical parsing models generalize more effectively
over arbitrary-length multiword expressions. This approach has not been previously
demonstrated. To show its effectiveness, we build two parsing models for MWE iden-
tification. The first model is based on a context-free grammar (CFG) with manual
rule refinements (Klein and Manning 2003). This parser also includes a novel lexical
model?the factored lexicon?that incorporates morphological features. The second
model is based on tree substitution grammar (TSG), a formalism with greater strong
generative capacity that can store larger structural tree fragments, some of which are
lexicalized.
We apply the models to Modern Standard Arabic (henceforth MSA, or simply
?Arabic?) and French, two morphologically rich languages (MRLs). The lexical sparsity
(in finite corpora) induced by rich morphology poses a particular challenge for n-gram
classification. Relative to English, French has a richer array of morphological features?
such as grammatical gender and verbal conjugation for aspect and voice. Arabic also
has richer morphology including gender and dual number. It has pervasive verb-
initial matrix clauses, although preposed subjects are also possible. For languages like
these it is well known that constituency parsing models designed for English often do
not generalize well. Therefore, we focus on the interplay among language, annotation
choices, and parsing model design for each language (Levy and Manning 2003; K?bler
2005, inter alia), although our methods are ultimately very general.
Our modeling strategy for MWEs is simple: We mark them with flat bracketings
in phrase structure trees. This representation implicitly assumes a locality constraint
on idioms, an assumption with a precedent in linguistics (Marantz 1997, inter alia).
Of course, it is easy to find non-local idioms that do not correspond to surface con-
stituents or even contiguous strings (O?Grady 1998). Utterances such as All hell seemed
to break loose and The cat got Mary?s tongue are clearly idiomatic, yet the idiomatic
elements are discontiguous. Our models cannot identify these MWEs, but then again,
neither can n-gram classification. Nonetheless, many common MWE types like nominal
compounds are contiguous and often correspond to constituent boundaries.
Consider again the phrasal compound part of speech,1 which is non-compositional:
The idiomatic meaning ?syntactic category? does not derive from any of the component
1 It is common to hyphenate some nominal compounds, e.g., part-of-speech. This practice invites a
words-with-spaces treatment of idioms. However, hyphens are inconsistently used in English.
Hyphenation is more common in French, but totally absent in Arabic.
196
Green, de Marneffe, and Manning Parsing Models for Identifying Multiword Expressions
words. This non-compositionality affects the syntactic environment of the compound as
shown by the addition of an attributive adjective:
(1) a. Noun is a part of speech.
b. *Noun is a big part of speech.
c. *Noun is a big part.
(2) a. Liquidity is a part of growth.
b. Liquidity is a big part of growth.
c. Liquidity is a big part.
In Example (1a) the copula predicate part of speech as a whole describes Noun. In
Examples (1b) and (1c) big clearly modifies only part and the idiomatic meaning is
lost. The attributive adjective cannot probe arbitrarily into the non-compositional com-
pound. In contrast, Example (2) contains parallel data without idiomatic semantics.
The conventional syntactic analysis of Example (2a) is identical to that of Example (1a)
except for the lexical items, yet part of growth is not idiomatic. Consequently, many pre-
modifiers are appropriate for part, which is semantically vacuous. In Example (2b), big
clearly modifies part, and of growth is just an optional PP complement, as shown by
Example (2c), which is still grammatical.
This article proposes different phrase structures for examples such as (1a) and
(2a). Figure 1a shows a Penn Treebank (PTB) (Marcus, Marcinkiewicz, and Santorini
1993) parse of Example (1a), and Figure 1b shows the parse of a paraphrase. The
phrasal compound part of speech functions syntactically like a single-word nominal
like category, and indeed Noun is a big category is grammatical. Single-word para-
phrasability is a common, though not mandatory, characteristic of MWEs (Baldwin
and Kim 2010). Starting from the paraphrase parse, we create a representation like
Figure (1c). The MWE is indicated by a label in the predicted structure, which is
flat. This representation explicitly models the idiomatic semantics of the compound
and is context-free, so we can build efficient parsers for it. Crucially, MWE identifi-
cation becomes a by-product of parsing as we can trivially extract MWE spans from
full parses.
We convert existing Arabic and French syntactic treebanks to the new MWE
representation. With this representation, the TSG model yields the best MWE iden-
tification results for Arabic (81.9% F1) and competitive results for French (71.3%),
even though its parsing results lag state-of-the-art probabilistic CFG (PCFG)-based
parsers. The TSG model also learns human-interpretable MWE rules. The fac-
tored lexicon model with gold morphological annotations achieves the best MWE
results for French (87.3% F1) and competitive results for Arabic (78.2% F1). For both
languages the factored lexicon model also approaches state-of-the-art basic parsing
accuracy.
The remainder of this article begins with linguistic background on common MWE
types in Arabic and French (Section 2). We then describe two constituency parsing
models that are tuned for MWE identification (Sections 3 and 4). These models are
supervised and can be trained on existing linguistic resources (Section 5). We evaluate
the models for both basic parsing and MWE identification (Section 6). Finally, we
compare our results with a state-of-the-art n-gram classification system (Section 7) and
to prior work (Section 8).
197
Computational Linguistics Volume 39, Number 1
2. Multiword Expressions in Arabic and French
In this section we provide a general definition and taxonomy of MWEs. Then we discuss
types of MWEs in Arabic and French.
2.1 Definition of Multiword Expressions
MWEs, a known nuisance for both linguistics and NLP, blur the lines between syntax
and semantics. Jackendoff (1997, page 156) comments that MWEs ?are hardly a marginal
part of our use of language,? and estimates that a native speaker knows at least as many
MWEs as single words. A linguistically adequate representation for MWEs remains an
active area of research, however. Baldwin and Kim (2010) define MWEs as follows:
Definition 1
Multiword expressions are lexical items that: (a) can be decomposed into multi-
ple lexemes; and (b) display lexical, syntactic, semantic, pragmatic, and/or statistical
idiomaticity.
S
NP
NN
Noun
VP
VBZ
is
NP
DT
a
NN
part
PP
IN
of
NP
NN
speech
S
NP
NN
Noun
VP
VBZ
is
NP
DT
a
NN
category
(a) Standard analysis of Example (1a) (b) Standard analysis of a paraphrase
S
NP
NN
Noun
VP
VBZ
is
NP
DT
a
MWN
NN
part
IN
of
NN
speech
(c) MWE analysis used in this article
Figure 1
(a) A standard PTB parse of Example (1a). (b) The MWE part of speech functions syntactically
like the ordinary nominal category, as shown by this paraphrase. (c) We incorporate the
presence of the MWE into the syntactic analysis by flattening the tree dominating part of speech
and introducing a new non-terminal label multiword noun (MWN) for the resulting span. The
new representation classifies an MWE according to a global syntactic type and assigns a POS
to each of the internal tokens. It makes no commitment to the internal syntactic structure of
the MWE, however.
198
Green, de Marneffe, and Manning Parsing Models for Identifying Multiword Expressions
Table 1
Semi-fixed MWEs in French and English. The French adverb ? terme (?in the end?) can be
modified by a small set of adjectives, and in turn some of these adjectives can be modified
by an adverb such as tr?s (?very?). Similar restrictions appear in English.
French English
? terme in the near term
? court terme in the short term
? tr?s court terme in the very short term
? moyen terme in the medium term
? long terme in the long term
? tr?s long terme in the very long term
MWEs fall into four broad categories (Sag et al 2002):
1. Fixed?do not allow morphosyntactic variation or internal modification (in short,
by and large).
2. Semi-fixed?can be inflected or undergo internal modification (Table 1).
3. Syntactically flexible?undergo syntactic variation such as inflection (e.g.,
phrasal verbs such as look up and write down).
4. Institutionalized phrases?fully compositional phrases that are statistically
idiosyncratic (traffic light, Secretary of State).
Statistical parsers are well-suited for coping with lexical, syntactic, and statistical
idiomaticity across all four MWE classes. However, to our knowledge, we are the first
to explicitly tune parsers for MWE identification.
2.2 Arabic MWEs
The most recent and most relevant work on Arabic MWEs was by Ashraf (2012), who
analyzed an 83-million-word Arabic corpus. He developed an empirical taxonomy of
six MWE types, which correspond to syntactic classes. The syntactic class is defined
by the projection of the purported syntactic head of the MWE. MWEs are further
subcategorized by observed POS sequences. For some of these classes, the syntactic
distinctions are debatable. For example, in the verb-object idiom 





	




 




Daraba cSfuurayn bi-Hajar (?he killed two birds with one stone?)2 the composition of

	




 (?two birds?) with 

 (?stone?) is at least as important as composition with the
verb 



 (?he killed?), yet Ashraf (2012) classifies the phrase as a verbal idiom.
The corpus in our experiments only marks three of the six Arabic MWE classes:
Nominal idioms (MWN) consist of proper nouns (Example 3a), noun compounds
(Example 3b), and construct NPs (Example 3c). MWNs typically correspond to NP
bracketings:
(3) a. N N: 




 


 abuu Dabii (?Abu Dhabi?)
2 For each Arabic example in this work, we provide native script, transliterations in italics according to the
phonetic scheme in Ryding (2005), and English translations in single quotes.
199
Computational Linguistics Volume 39, Number 1
b. D+N D+N: Proceedings of NAACL-HLT 2013, pages 627?633,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
The Life and Death of Discourse Entities: Identifying Singleton Mentions
Marta Recasens
Linguistics Department
Stanford University
Stanford, CA 94305
recasens@google.com
Marie-Catherine de Marneffe
Linguistics Department
The Ohio State University
Columbus, OH 43210
mcdm@ling.osu.edu
Christopher Potts
Linguistics Department
Stanford University
Stanford, CA 94305
cgpotts@stanford.edu
Abstract
A discourse typically involves numerous en-
tities, but few are mentioned more than once.
Distinguishing discourse entities that die out
after just one mention (singletons) from those
that lead longer lives (coreferent) would ben-
efit NLP applications such as coreference res-
olution, protagonist identification, topic mod-
eling, and discourse coherence. We build a lo-
gistic regression model for predicting the sin-
gleton/coreferent distinction, drawing on lin-
guistic insights about how discourse entity
lifespans are affected by syntactic and seman-
tic features. The model is effective in its own
right (78% accuracy), and incorporating it into
a state-of-the-art coreference resolution sys-
tem yields a significant improvement.
1 Introduction
Not all discourse entities are created equal. Some
lead long lives and appear in a variety of discourse
contexts (coreferent), whereas others never escape
their birthplaces, dying out after just one mention
(singletons). The ability to make this distinction
based on properties of the NPs used to identify these
referents (mentions) would benefit not only corefer-
ence resolution, but also topic analysis, textual en-
tailment, and discourse coherence.
The existing literature provides numerous gen-
eralizations relevant to answering the question of
whether a given discourse entity will be singleton
or coreferent. These involve the internal syntax and
morphology of the target NP (Prince, 1981a; Prince,
1981b; Wang et al, 2006), the grammatical function
and discourse role of that NP (Chafe, 1976; Hobbs,
1979; Walker et al, 1997; Beaver, 2004), and the in-
teraction of all of those features with semantic oper-
ators like negation, modals, and attitude predicates
(Karttunen, 1973; Karttunen, 1976; Kamp, 1981;
Heim, 1982; Heim, 1992; Roberts, 1990; Groe-
nendijk and Stokhof, 1991; Bittner, 2001).
The first step in our analysis is to bring these
insights together into a single logistic regression
model ? the lifespan model ? and assess their
predictive power on real data. We show that the
features generally behave as the existing literature
leads us to expect, and that the model itself is highly
effective at predicting whether a given mention is
singleton or coreferent. We then provide an initial
assessment of the engineering value of making the
singleton/coreferent distinction by incorporating our
lifespan model into the Stanford coreference resolu-
tion system (Lee et al, 2011). This addition results
in a significant improvement on the CoNLL-2012
Shared Task data, across the MUC, B3, CEAF, and
CoNLL scoring algorithms.
2 Data
All the data used throughout the paper come from
the CoNLL-2012 Shared Task (Pradhan et al,
2012), which included the 1.6M English words from
OntoNotes v5.0 (Hovy et al, 2006) that have been
annotated with different layers of annotation (coref-
erence, parse trees, etc.). We used the training, de-
velopment (dev), and test splits as defined in the
shared task (Table 1). Since the OntoNotes corefer-
ence annotations do not contain singleton mentions,
we automatically marked as singletons all the NPs
627
MENTIONS
Dataset Docs Tokens Coreferent Singletons
Training 2,802 1.3M 152,828 192,248
Dev 343 160K 18,815 24,170
Test 348 170K 19,392 24,921
Table 1: CoNLL-2012 Shared Task data statistics. We
added singletons (NPs not annotated as coreferent).
not annotated as coreferent. Thus, our singletons in-
clude non-referential NPs but not verbal mentions.
3 Predicting lifespans
Our lifespan model makes a binary distinction be-
tween discourse referents that are not part of a coref-
erence chain (singletons) and items that are part of
one (coreferent). The distribution of lifespans in our
data (Figure 1) suggests that this is a natural divi-
sion. The propensity of singletons also highlights
the relevance of detecting singletons for a coref-
erence system. We fit a binary logistic regression
model in R (R Core Team, 2012) on the training
data, coding singletons as ?0? and coreferent men-
tions as ?1?. Throughout the following tables of co-
efficient estimates, positive values favor coreferents
and negative ones favor singletons. We turn now to
describing and motivating the features of this model.
Singleton 2 3 4 5 6-10 11-15 16-20 >20
0
5K
15
K
25
K
Figure 1: Distribution of lifespans in the dev set. Single-
tons account for 56% of the data.
Internal morphosyntax of the mention Table 2
summarizes the features from our model that con-
cern the internal morphology and syntactic structure
of the mention. Many are common in coreference
systems (Recasens and Hovy, 2009), but our model
highlights their influence on lifespans. The picture
is expected on the taxonomy of given and new de-
fined by Prince (1981b) and assumed throughout dy-
namic semantics (Kamp, 1981; Heim, 1982): pro-
nouns depend on anaphoric connections to previous
mentions for disambiguation and thus are very likely
to be coreferent. This is corroborated by the pos-
itive coefficient estimate for ?Type = pronoun? in
Table 2. Few quantified phrases easily participate
in discourse anaphora (Partee, 1987; Wang et al,
2006), accounting for the association between quan-
tifiers and singletons (negative coefficient estimate
for ?Quantifier = quantified? in Table 2). The one
surprise is the negative coefficient for indefinites. In
theories stretching back to Karttunen (1976), indef-
inites function primarily to establish new discourse
entities, and should be able to participate in coref-
erence chains, but here the association with such
chains is negative. However, interactions explain
this fact (see Table 4 and our discussion of it).
The person, number, and animacy values suggest
that singular animates are excellent coreferent NPs,
a previous finding of Centering Theory (Grosz et al,
1995; Walker et al, 1998) and of cross-linguistic
work on obviative case-marking (Aissen, 1997).
Our model also includes named-entity features for
all of the eighteen OntoNotes entity-types (omitted
from Table 2 for space and clarity reasons). As a
rule, they behave like ?Type = proper noun? in asso-
ciating with coreferents. The exceptions are ORDI-
NAL, PERCENT, and QUANTITY, which seem intu-
itively unlikely to participate in coreference chains.
Estimate P-value
Type = pronoun 1.21 < 0.001
Type = proper noun 1.88 < 0.001
Animacy = inanimate ?1.36 < 0.001
Animacy = unknown ?0.38 < 0.001
Person = 1 1.05 < 0.001
Person = 2 0.13 < 0.001
Person = 3 1.62 < 0.001
Number = singular 0.61 < 0.001
Number = unknown 0.17 < 0.001
Quantifier = indefinite ?1.49 < 0.001
Quantifier = quantified ?1.23 < 0.001
Number of modifiers ?0.39 < 0.001
Table 2: Internal morphosyntactic features.
Grammatical role of the mention Synthesizing
much work in Centering Theory and information
structuring, we conclude that coreferent mentions
are likely to appear as core verbal arguments and
will favor sentence-initial (topic-tracking) positions
(Ward and Birner, 2004). The coefficient estimates
628
Estimate P-value
Sentence Position = end ?0.22 < 0.001
Sentence Position = first 0.04 0.07
Sentence Position = last ?0.31 < 0.001
Sentence Position = middle ?0.11 < 0.001
Relation = noun argument 0.56 < 0.001
Relation = other ?0.67 < 0.001
Relation = root ?0.61 < 0.001
Relation = subject 0.65 < 0.001
Relation = verb argument 0.32 < 0.001
In coordination ?0.48 < 0.001
Table 3: Grammatical role features.
in Table 3 corroborate these conclusions. To de-
fine the ?Relation? and ?In coordination? features, we
used the Stanford dependencies (de Marneffe et al,
2006) on the gold constituents.
Semantic environment of the mention Table 4
highlights the complex interactions between dis-
course anaphora and semantic operators. These
interactions have been a focus of logical seman-
tics since Karttunen (1976), whose guiding obser-
vation is semantic: an indefinite interpreted inside
the scope of a negation, modal, or attitude predicate
is generally unavailable for anaphoric reference out-
side of the scope of that operator, as in Kim didn?t
understand [an exam question]i. #Iti was too hard.
Of course, such discourses cohere if the indefinite
is interpreted as taking wide scope (?there is a ques-
tion Kim didn?t understand?). Such readings are of-
ten disfavored, but they become more salient when
modifiers like certain are included (Schwarzschild,
2002) or when the determiner is sensitive to the po-
larity or intensionality of its environment (Baker,
1970; Ladusaw, 1980; van der Wouden, 1997; Is-
rael, 1996; Israel, 2001; Giannakidou, 1999). Sub-
sequent research identified many other factors that
further extend or restrict the anaphoric potential of
an indefinite (Roberts, 1996).
We do not have direct access to semantic scope,
but we expect syntactic scope to correlate strongly
with semantic scope, so we used dependency rep-
resentations to define features capturing syntactic
scope for negation, modal auxiliaries, and a broad
range of attitude predicates. These features tend to
bias in favor of singletons because they so radically
restrict the possibilities for intersentential anaphora.
Interacting these features with those for the inter-
nal syntax of mentions is also informative. Since
proper names and pronouns are not scope-taking,
they are largely unaffected by the environment fea-
tures, whereas indefinites emerge as even more re-
stricted, just as Karttunen and others would predict.
Attitude predicates seem initially anomalous,
though. They share the relevant semantic proper-
ties with negation and modals, and yet they seem
to facilitate coreference. Here, the findings of de
Marneffe et al (2012) seem informative. Those au-
thors find that, in texts of the sort we are studying,
attitude predicates are used predominantly to mark
the source of information that is effectively asserted
despite being embedded (Rooryck, 2001; Simons,
2007). That is, though X said p does not semanti-
cally entail p, it is often interpreted as a commitment
to p, which correspondingly elevates mentions in p
to main-clause status (Harris and Potts, 2009).
Estimate P-value
Presence of negation ?0.18 < 0.001
Presence of modality ?0.22 < 0.001
Under an attitude verb 0.03 0.01
AttitudeVerb * (Type = pronoun) 0.29 < 0.001
AttitudeVerb * (Type = proper noun) 0.14 < 0.001
Modal * (Type = pronoun) 0.12 0.04
Modal * (Type = proper noun) 0.35 < 0.001
Negation * (Type = pronoun) 1.07 < 0.001
Negation * (Type = proper noun) 0.30 < 0.001
Negation * (Quantifier = indefinite) ?0.37 < 0.001
Negation * (Quantifier = quantified) ?0.36 0.23
Negation * (Number of modifiers) 0.11 < 0.001
Table 4: Semantic environment features and interactions.
Results The model successfully learns to tease
singletons and coreferent mentions apart. Table 5
summarizes its performance on the dev set. The
STANDARD model uses 0.5 as the decision bound-
ary, with 78% accuracy. The CONFIDENT model
predicts singleton if Pr < .2 and coreferent if Pr > .8,
which increases precision (P) at a cost to recall (R).
STANDARD CONFIDENT
Prediction R P F1 R P F1
Singleton 82.3 79.2 80.7 50.5 89.6 64.6
Coreferent 72.2 76.1 74.1 41.3 86.8 55.9
Table 5: Recall, precision, and F1 for the lifespan model.
629
MUC B3 CEAF-?3 CEAF-?4 CoNLL
System R P F1 R P F1 R / P / F1 R P F1 F1
Baseline 66.64* 64.72 65.67 68.05* 71.58 69.77* 58.31 45.49 47.55* 46.50 60.65
w/ Lifespan 66.08 67.33* 66.70* 66.40 73.14* 69.61 58.83* 47.77* 46.38 47.07* 61.13*
Table 6: Performance on the test set according to the official CoNLL-2012 scorer. Scores are on automatically pre-
dicted mentions. Stars indicate a statistically significant difference (paired Mann-Whitney U-test, p < 0.05).
B3 CEAF-?3 CoNLL
System R P F1 R P F1 F1
Baseline 58.53* 71.58 64.40 63.71* 58.31 60.89 58.86
w/ Lifespan 58.14 73.14* 64.78* 63.38 58.83* 61.02 59.52*
Table 7: B3, CEAF-?3 and CoNLL measures on the test set according to a modified CoNLL-2012 scorer that follows
Cai and Strube (2010). Scores are on automatically predicted mentions.
4 Application to coreference resolution
To assess the usefulness of the lifespan model in an
NLP application, we incorporate it into the Stanford
coreference resolution system (Lee et al, 2011),
which we take as our baseline. This was the highest-
scoring system in the CoNLL-2011 Shared Task,
and was also part of the highest-scoring system in
the CoNLL-2012 Shared Task (Fernandes et al,
2012). It is a rule-based system that includes a to-
tal of ten rules (or ?sieves?) for entity coreference,
such as exact string match and pronominal resolu-
tion. The sieves are applied from highest to lowest
precision, each rule adding coreference links.
Incorporating the lifespan model The lifespan
model can improve coreference resolution in two
different ways: (i) mentions classified as singletons
should not be considered as either antecedents or
coreferent, and (ii) mentions classified as coreferent
should be linked with another mention(s). By suc-
cessfully predicting singletons (i), we can enhance
the system?s precision; by successfully predicting
coreferent mentions (ii), we can improve the sys-
tem?s recall. Here we focus on (i) and use the lifes-
pan model for detecting singletons. This decision
is motivated by two factors. First, given the large
number of singletons (Figure 1), we are more likely
to see a gain in performance from discarding sin-
gletons. Second, the multi-sieve nature of the Stan-
ford coreference system does not make it straightfor-
ward to decide which antecedent a mention should
be linked to even if we know that it is coreferent.
We leave the incorporation of coreferent predictions
for future work.
To integrate the singleton model into the Stanford
coreference system, we let a sieve consider whether
a pair of mentions is coreferent only if neither of
the two mentions are classified as singletons by our
CONFIDENT model. Experiments on the dev set
showed that the model often made wrong predic-
tions for NEs. We do not trust the model for NE
mentions. Performance on coreference (on the dev
set) was higher with the CONFIDENT model than
with the STANDARD model.
Results and discussion To evaluate the corefer-
ence system with and without the lifespan model, we
used the English dev and test sets from the CoNLL-
2012 Shared Task, presented in Section 2. Although
the CoNLL shared task evaluated systems on only
multi-mention (i.e., non-singleton) entities, by stop-
ping singletons from being linked to multi-mention
entities, we expected the lifespan model to increase
the system?s precision. Our evaluation uses five
of the measures given by the CoNLL-2012 scorer:
MUC (Vilain et al, 1995), B3 (Bagga and Baldwin,
1998), CEAF-?3 and CEAF-?4 (Luo, 2005), and the
CoNLL official score (Denis and Baldridge, 2009).
We do not include BLANC (Recasens and Hovy,
2011) because it assumes gold mentions and so is
not suited for the scenario considered in this paper,
which uses automatically predicted mentions.
Table 6 summarizes the test set performance. All
the scores are on automatically predicted mentions.
We use gold POS, parse trees, and NEs. The base-
630
line is the Stanford system, and ?w/ Lifespan? is the
same system extended with our lifespan model to
discard singletons, as explained above.
As expected, the lifespan model increases preci-
sion but decreases recall. Overall, however, we ob-
tain a significant improvement of 0.5?1 points in the
F1 score of MUC, CEAF-?3, CEAF-?4 and CoNLL.
The drop in B3 traces to a bug in the CoNLL scorer?s
implementation of Cai and Strube (2010)?s algo-
rithm for aligning gold and automatically predicted
mentions, which affects the computation of B3 and
CEAF-?3.1 Table 7 presents the results after mod-
ifying the CoNLL-2012 scorer to compute B3 and
CEAF-?3 according to Cai and Strube (2010).2 We
do see an improvement in the precision and F1
scores of B3, and the overall CoNLL score remains
significant. The CEAF-?3 F1 score is no longer sig-
nificant, but is still in the expected direction.
5 Conclusion
We built a model to predict the lifespan of discourse
referents, teasing apart singletons from coreferent
mentions. The model validates existing linguistic
insights and performs well in its own right. This
alone has ramifications for tracking topics, identify-
ing protagonists, and modeling coreference and dis-
course coherence. We applied the lifespan model to
coreference resolution, showing how to incorporate
it effectively into a state-of-the-art rule-based coref-
erence system. We expect similar improvements
with machine-learning-based coreference systems,
where incorporating all the power of the lifespan
model would be easier.
Our lifespan model has been integrated into the
latest version of the Stanford coreference resolution
system.3
1At present, if the system links two mentions that do not
exist in the gold standard, the scorer adds two singletons to the
gold standard. This results in a higher B3 F1 score (when it
should be lower) because recall increases instead of staying the
same (precision goes up).
2In the modified scorer, twinless predicted mentions are
added to the gold standard to compute precision but not to com-
pute recall.
3http://nlp.stanford.edu/software/
dcoref.shtml
Acknowledgments
We thank Emili Sapena for modifying the CoNLL-
2012 scorer to follow Cai and Strube (2010).
This research was supported in part by ONR
grant No. N00014-10-1-0109 and ARO grant
No. W911NF-07-1-0216. The first author was sup-
ported by a Beatriu de Pino?s postdoctoral schol-
arship (2010 BP-A 00149) from Generalitat de
Catalunya.
References
Judith Aissen. 1997. On the syntax of obviation. Lan-
guage, 73(4):705?750.
Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In Proceedings of
the LREC 1998 Workshop on Linguistic Coreference,
pages 563?566.
C. L. Baker. 1970. Double negatives. Linguistic Inquiry,
1(2):169?186.
David Beaver. 2004. The optimization of discourse
anaphora. Linguistics and Philosophy, 27(1):3?56.
Maria Bittner. 2001. Surface composition as bridging.
Journal of Semantics, 18(2):127?177.
Jie Cai and Michael Strube. 2010. Evaluation metrics
for end-to-end coreference resolution systems. In Pro-
ceedings of SIGDIAL 2010, pages 28?36.
Wallace L. Chafe. 1976. Givenness, Contrastiveness,
Definiteness, Subjects, Topics, and Point of View. In
Charles N. Li, editor, Subject and Topic, pages 25?55.
Academic Press, New York.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of LREC 2006.
Marie-Catherine de Marneffe, Christopher D. Manning,
and Christopher Potts. 2012. Did it happen?
The pragmatic complexity of veridicality assessment.
Computational Linguistics, 38(2):301?333.
Pascal Denis and Jason Baldridge. 2009. Global joint
models for coreference resolution and named entity
classification. Procesamiento del Lenguaje Natural,
42:87?96.
Eraldo Fernandes, C??cero dos Santos, and Ruy Milidiu?.
2012. Latent structure perceptron with feature induc-
tion for unrestricted coreference resolution. In Pro-
ceedings of CoNLL-2012: Shared Task, pages 41?48.
Anastasia Giannakidou. 1999. Affective dependencies.
Linguistics and Philosophy, 22(4):367?421.
Jeroen Groenendijk and Martin Stokhof. 1991. Dynamic
predicate logic. Linguistics and Philosophy, 14(1):39?
100.
631
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203?225.
Jesse A. Harris and Christopher Potts. 2009.
Perspective-shifting with appositives and expressives.
Linguistics and Philosophy, 32(6):523?552.
Irene Heim. 1982. The Semantics of Definite and Indefi-
nite Noun Phrases. Ph.D. thesis, UMass Amherst.
Irene Heim. 1992. Presupposition projection and the
semantics of attitude verbs. Journal of Semantics,
9(2):183?221.
Jerry R. Hobbs. 1979. Coherence and coreference. Cog-
nitive Science, 3(1):67?90.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes:
The 90% solution. In Proceedings of HLT-
NAACL 2006, pages 57?60.
Michael Israel. 1996. Polarity sensitivity as lexical se-
mantics. Linguistics and Philosophy, 19(6):619?666.
Michael Israel. 2001. Minimizers, maximizers, and the
rhetoric of scalar reasoning. Journal of Semantics,
18(4):297?331.
Hans Kamp. 1981. A theory of truth and discourse
representation. In Jeroen Groenendijk, Theo M. V.
Janssen, and Martin Stockhof, editors, Formal Meth-
ods in the Study of Language, pages 277?322. Mathe-
matical Centre, Amsterdam.
Lauri Karttunen. 1973. Presuppositions and compound
sentences. Linguistic Inquiry, 4(2):169?193.
Lauri Karttunen. 1976. Discourse referents. In James D.
McCawley, editor, Syntax and Semantics, volume 7:
Notes from the Linguistic Underground, pages 363?
385. Academic Press, New York.
William A. Ladusaw. 1980. On the notion ?affective?
in the analysis of negative polarity items. Journal of
Linguistic Research, 1(1):1?16.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford?s multi-pass sieve coreference resolution sys-
tem at the CoNLL-2011 Shared Task. In Proceedings
of CoNLL-2011: Shared Task, pages 28?34.
Xiaoqiang Luo. 2005. On coreference resolution perfor-
mance metrics. In Proceedings of HLT-EMNLP 2005,
pages 25?32.
Barbara H. Partee. 1987. Noun phrase interpretation
and type-shifting principles. In Jeroen Groenendijk,
Dick de Jong, and Martin Stokhof, editors, Studies in
Discourse Representation Theory and the Theory of
Generalized Quantifiers, pages 115?143. Foris Publi-
cations, Dordrecht.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 Shared Task: Modeling multilingual unrestricted
coreference in OntoNotes. In Proceedings of EMNLP
and CoNLL-2012: Shared Task, pages 1?40.
Ellen Prince. 1981a. On the inferencing of indefi-
nite ?this? NPs. In Bonnie Lynn Webber, Ivan Sag,
and Aravind Joshi, editors, Elements of Discourse Un-
derstanding, pages 231?250. Cambridge University
Press, Cambridge.
Ellen F. Prince. 1981b. Toward a taxonomy of given?
new information. In Peter Cole, editor, Radical Prag-
matics, pages 223?255. Academic Press, New York.
R Core Team, 2012. R: A Language and Environment
for Statistical Computing. R Foundation for Statistical
Computing, Vienna, Austria.
Marta Recasens and Eduard Hovy. 2009. A deeper
look into features for coreference resolution. In Sobha
Lalitha Devi, Anto?nio Branco, and Ruslan Mitkov, ed-
itors, Anaphora Processing and Applications, volume
5847 of Lecture Notes in Computer Science, pages 29?
42. Springer.
Marta Recasens and Eduard Hovy. 2011. BLANC: Im-
plementing the Rand index for coreference evaluation.
Natural Language Engineering, 17(4):485?510.
Craige Roberts. 1990. Modal Subordination, Anaphora,
and Distributivity. Garland, New York.
Craige Roberts. 1996. Anaphora in intensional contexts.
In Shalom Lappin, editor, The Handbook of Contem-
porary Semantic Theory, pages 215?246. Blackwell
Publishers, Oxford.
Johan Rooryck. 2001. Evidentiality, Part II. Glot Inter-
national, 5(5):161?168.
Roger Schwarzschild. 2002. Singleton indefinites. Jour-
nal of Semantics, 19(3):289?314.
Mandy Simons. 2007. Observations on embedding
verbs, evidentiality, and presupposition. Lingua,
117(6):1034?1056.
Ton van der Wouden. 1997. Negative Contexts: Col-
location, Polarity and Multiple Negation. Routledge,
London and New York.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceedings
of MUC-6, pages 45?52.
Marilyn A. Walker, Aravind K. Joshi, and Ellen F. Prince,
editors. 1997. Centering in Discourse. Oxford Uni-
versity Press.
Marilyn A. Walker, Aravind K. Joshi, and Ellen F. Prince.
1998. Centering in naturally-occurring discourse: An
overview. In Marilyn A. Walker, Aravind K. Joshi,
and Ellen F. Prince, editors, Centering Theory in Dis-
course, pages 1?28, Oxford. Clarendon Press.
Linton Wang, Eric McCready, and Nicholas Asher. 2006.
Information dependency in quantificational subordina-
tion. In Klaus von Heusinger and Ken Turner, editors,
632
Where Semantics Meets Pragmatics, pages 267?304.
Elsevier Science, Amsterdam.
Gregory Ward and Betty Birner. 2004. Information
structure and non-canonical syntax. In Laurence R.
Horn and Gregory Ward, editors, The Handbook of
Pragmatics, pages 153?174. Blackwell, Oxford.
633
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 167?176,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
?Was it good? It was provocative.?
Learning the meaning of scalar adjectives
Marie-Catherine de Marneffe, Christopher D. Manning and Christopher Potts
Linguistics Department
Stanford University
Stanford, CA 94305
{mcdm,manning,cgpotts}@stanford.edu
Abstract
Texts and dialogues often express infor-
mation indirectly. For instance, speak-
ers? answers to yes/no questions do not
always straightforwardly convey a ?yes?
or ?no? answer. The intended reply is
clear in some cases (Was it good? It was
great!) but uncertain in others (Was it
acceptable? It was unprecedented.). In
this paper, we present methods for inter-
preting the answers to questions like these
which involve scalar modifiers. We show
how to ground scalar modifier meaning
based on data collected from the Web. We
learn scales between modifiers and infer
the extent to which a given answer conveys
?yes? or ?no?. To evaluate the methods,
we collected examples of question?answer
pairs involving scalar modifiers from CNN
transcripts and the Dialog Act corpus and
use response distributions from Mechani-
cal Turk workers to assess the degree to
which each answer conveys ?yes? or ?no?.
Our experimental results closely match the
Turkers? response data, demonstrating that
meanings can be learned from Web data
and that such meanings can drive prag-
matic inference.
1 Introduction
An important challenge for natural language pro-
cessing is how to learn not only basic linguistic
meanings but also how those meanings are system-
atically enriched when expressed in context. For
instance, answers to polar (yes/no) questions do
not always explicitly contain a ?yes? or ?no?, but
rather give information that the hearer can use to
infer such an answer in a context with some degree
of certainty. Hockey et al (1997) find that 27% of
answers to polar questions do not contain a direct
?yes? or ?no? word, 44% of which they regard as
failing to convey a clear ?yes? or ?no? response. In
some cases, interpreting the answer is straightfor-
ward (Was it bad? It was terrible.), but in others,
what to infer from the answer is unclear (Was it
good? It was provocative.). It is even common
for the speaker to explicitly convey his own uncer-
tainty with such answers.
In this paper, we focus on the interpretation
of answers to a particular class of polar ques-
tions: ones in which the main predication in-
volves a gradable modifier (e.g., highly unusual,
not good, little) and the answer either involves an-
other gradable modifier or a numerical expression
(e.g., seven years old, twenty acres of land). Inter-
preting such question?answer pairs requires deal-
ing with modifier meanings, specifically, learning
context-dependent scales of expressions (Horn,
1972; Fauconnier, 1975) that determine how, and
to what extent, the answer as a whole resolves the
issue raised by the question.
We propose two methods for learning the
knowledge necessary for interpreting indirect an-
swers to questions involving gradable adjectives,
depending on the type of predications in the ques-
tion and the answer. The first technique deals
with pairs of modifiers: we hypothesized that on-
line, informal review corpora in which people?s
comments have associated ratings would provide
a general-purpose database for mining scales be-
tween modifiers. We thus use a large collection of
online reviews to learn orderings between adjec-
tives based on contextual entailment (good < ex-
cellent), and employ this scalar relationship to in-
fer a yes/no answer (subject to negation and other
qualifiers). The second strategy targets numeri-
cal answers. Since it is unclear what kind of cor-
pora would contain the relevant information, we
turn to the Web in general: we use distributional
information retrieved via Web searches to assess
whether the numerical measure counts as a posi-
167
tive or negative instance of the adjective in ques-
tion. Both techniques exploit the same approach:
using texts (the Web) to learn meanings that can
drive pragmatic inference in dialogue. This paper
demonstrates to some extent that meaning can be
grounded from text in this way.
2 Related work
Indirect speech acts are studied by Clark (1979),
Perrault and Allen (1980), Allen and Perrault
(1980) and Asher and Lascarides (2003), who
identify a wide range of factors that govern how
speakers convey their intended messages and how
hearers seek to uncover those messages from
uncertain and conflicting signals. In the com-
putational literature, Green and Carberry (1994,
1999) provide an extensive model that interprets
and generates indirect answers to polar questions.
They propose a logical inference model which
makes use of discourse plans and coherence rela-
tions to infer categorical answers. However, to ad-
equately interpret indirect answers, the uncertainty
inherent in some answers needs to be captured (de
Marneffe et al, 2009). While a straightforward
?yes? or ?no? response is clear in some indirect an-
swers, such as in (1), the intended answer is less
certain in other cases (2):1
(1) A: Do you think that?s a good idea, that we
just begin to ignore these numbers?
B: I think it?s an excellent idea.
(2) A: Is he qualified?
B: I think he?s young.
In (2), it might be that the answerer does not
know about qualifications or does not want to talk
about these directly, and therefore shifts the topic
slightly. As proposed by Zeevat (1994) in his work
on partial answers, the speaker?s indirect answer
might indicate that he is deliberately leaving the
original question only partially addressed, while
giving a fully resolving answer to another one.
The hearer must then interpret the answer to work
out the other question. In (2) in context, we get a
sense that the speaker would resolve the issue to
?no?, but that he is definitely not committed to that
in any strong sense. Uncertainty can thus reside
both on the speaker and the hearer sides, and the
four following scenarios are attested in conversa-
tion:
1Here and throughout, the examples come from the corpus
described in section 3.
a. The speaker is certain of ?yes? or ?no? and
conveys that directly and successfully to the
hearer.
b. The speaker is certain of ?yes? or ?no? but
conveys this only partially to the hearer.
c. The speaker is uncertain of ?yes? or ?no? and
conveys this uncertainty to the hearer.
d. The speaker is uncertain of ?yes? or ?no?,
but the hearer infers one of those with con-
fidence.
The uncertainty is especially pressing for pred-
ications built around scalar modifiers, which are
inherently vague and highly context-dependent
(Kamp and Partee, 1995; Kennedy and McNally,
2005; Kennedy, 2007). For example, even if we
fix the basic sense for little to mean ?young for a
human?, there is a substantial amount of gray area
between the clear instances (babies) and the clear
non-instances (adults). This is the source of un-
certainty in (3), in which B?s children fall into the
gray area.
(3) A: Are your kids little?
B: I have a seven year-old and a ten
year-old.
3 Corpus description
Since indirect answers are likely to arise in in-
terviews, to gather instances of question?answer
pairs involving gradable modifiers (which will
serve to evaluate the learning techniques), we use
online CNN interview transcripts from five dif-
ferent shows aired between 2000 and 2008 (An-
derson Cooper, Larry King Live, Late Edition,
Lou Dobbs Tonight, The Situation Room). We
also searched the Switchboard Dialog Act corpus
(Jurafsky et al, 1997). We used regular expres-
sions and manual filtering to find examples of two-
utterance dialogues in which the question and the
reply contain some kind of gradable modifier.
3.1 Types of question?answer pairs
In total, we ended up with 224 question?answer
pairs involving gradable adjectives. However
our collection contains different types of answers,
which naturally fall into two categories: (I) in
205 dialogues, both the question and the answer
contain a gradable modifier; (II) in 19 dialogues,
the reply contains a numerical measure (as in (3)
above and (4)).
168
Modification in answer Example Count
I Other adjective (1), (2) 125
Adverb - same adjective (5) 55
Negation - same adjective (6), (7) 21
Omitted adjective (8) 4
II Numerical measure (3), (4) 19
Table 1: Types of question?answer pairs, and
counts in the corpus.
I Modification in answer Mean SD
Other adjective 1.1 0.6
Adverb - same adjective 0.8 0.6
Negation - same adjective 1.0 0.5
Omitted adjective 1.1 0.2
II Numerical measure 1.5 0.2
Table 2: Mean entropy values and standard devi-
ation obtained in the Mechanical Turk experiment
for each question?answer pair category.
(4) A: Have you been living there very long?
B: I?m in here right now about twelve and
a half years.
Category I, which consists of pairs of modifiers,
can be further divided. In most dialogues, the an-
swer contains another adjective than the one used
in the question, such as in (1). In others, the an-
swer contains the same adjective as in the ques-
tion, but modified by an adverb (e.g., very, basi-
cally, quite) as in (5) or a negation as in (6).
(5) A: That seems to be the biggest sign of
progress there. Is that accurate?
B: That?s absolutely accurate.
(6) A: Are you bitter?
B: I?m not bitter because I?m a soldier.
The negation can be present in the main clause
when the adjectival predication is embedded, as in
example (7).
(7) A: [. . . ] Is that fair?
B: I don?t think that?s a fair statement.
In a few cases, when the question contains an ad-
jective modifying a noun, the adjective is omitted
in the answer:
(8) A: Is that a huge gap in the system?
B: It is a gap.
Table 1 gives the distribution of the types ap-
pearing in the corpus.
3.2 Answer assignment
To assess the degree to which each answer con-
veys ?yes? or ?no? in context, we use response dis-
tributions from Mechanical Turk workers. Given a
written dialogue between speakers A and B, Turk-
ers were asked to judge what B?s answer conveys:
?definite yes?, ?probable yes?, ?uncertain?, ?proba-
ble no?, ?definite no?. Within each of the two ?yes?
and ?no? pairs, there is a scalar relationship, but
the pairs themselves are not in a scalar relationship
with each other, and ?uncertain? is arguably a sep-
arate judgment. Figure 1 shows the exact formu-
lation used in the experiment. For each dialogue,
we got answers from 30 Turkers, and we took the
dominant response as the correct one though we
make extensive use of the full response distribu-
tions in evaluating our approach.2 We also com-
puted entropy values for the distribution of an-
swers for each item. Overall, the agreement was
good: 21 items have total agreement (entropy of
0.0 ? 11 in the ?adjective? category, 9 in the
?adverb-adjective? category and 1 in the ?nega-
tion? category), and 80 items are such that a single
response got chosen 20 or more times (entropy <
0.9). The dialogues in (1) and (9) are examples of
total agreement. In contrast, (10) has response en-
tropy of 1.1, and item (11) has the highest entropy
of 2.2.
(9) A: Advertisements can be good or bad.
Was it a good ad?
B: It was a great ad.
(10) A: Am I clear?
B: I wish you were a little more forthright.
(11) A: 91 percent of the American people still
express confidence in the long-term
prospect of the U.S. economy; only 8
percent are not confident. Are they
overly optimistic, in your professional
assessment?
2120 Turkers were involved (the median number of items
done was 28 and the mean 56.5). The Fleiss? Kappa score for
the five response categories is 0.46, though these categories
are partially ordered. For the three-category response system
used in section 5, which arguably has no scalar ordering, the
Fleiss? Kappa is 0.63. Despite variant individual judgments,
aggregate annotations done with Mechanical Turk have been
shown to be reliable (Snow et al, 2008; Sheng et al, 2008;
Munro et al, 2010). Here, the relatively low Kappa scores
also reflect the uncertainty inherent in many of our examples,
uncertainty that we seek to characterize and come to grips
with computationally.
169
Indirect Answers to Yes/No Questions
In the following dialogue, speaker A asks a simple Yes/No
question, but speaker B answers with something more in-
direct and complicated.
dialogue here
Which of the following best captures what speaker B
meant here:
? B definitely meant to convey ?Yes?.
? B probably meant to convey ?Yes?.
? B definitely meant to convey ?No?.
? B probably meant to convey ?No?.
? (I really can?t tell whether B meant to convey ?Yes?
or ?No?.)
Figure 1: Design of the Mechanical Turk experi-
ment.
B: I think it shows how wise the American
people are.
Table 2 shows the mean entropy values for the
different categories identified in the corpus. Inter-
estingly, the pairs involving an adverbial modifi-
cation in the answer all received a positive answer
(?yes? or ?probable yes?) as dominant response.
All 19 dialogues involving a numerical measure
had either ?probable yes? or ?uncertain? as domi-
nant response. There is thus a significant bias for
positive answers: 70% of the category I items and
74% of the category II items have a positive an-
swer as dominant response. Examining a subset
of the Dialog Act corpus, we found that 38% of
the yes/no questions receive a direct positive an-
swers, whereas 21% have a direct negative answer.
This bias probably stems from the fact that people
are more likely to use an overt denial expression
where they need to disagree, whether or not they
are responding indirectly.
4 Methods
In this section, we present the methods we propose
for grounding the meanings of scalar modifiers.
4.1 Learning modifier scales and inferring
yes/no answers
The first technique targets items such as the ones
in category I of our corpus. Our central hypothesis
is that, in polar question dialogues, the semantic
relationship between the main predication PQ in
the question and the main predication PA in the an-
swer is the primary factor in determining whether,
and to what extent, ?yes? or ?no? was intended. If
PA is at least as strong as PQ, the intended answer
is ?yes?; if PA is weaker than PQ, the intended an-
swer is ?no?; and, where no reliable entailment re-
lationship exists between PA and PQ, the result is
uncertainty.
For example, good is weaker (lower on the rel-
evant scale) than excellent, and thus speakers in-
fer that the reply in example (1) above is meant to
convey ?yes?. In contrast, if we reverse the order
of the modifiers ? roughly, Is it a great idea?;
It?s a good idea ? then speakers infer that the
answer conveys ?no?. Had B replied with It?s a
complicated idea in (1), then uncertainty would
likely have resulted, since good and complicated
are not in a reliable scalar relationship. Negation
reverses scales (Horn, 1972; Levinson, 2000), so it
flips ?yes? and ?no? in these cases, leaving ?uncer-
tain? unchanged. When both the question and the
answer contain a modifier (such as in (9?11)), the
yes/no response should correlate with the extent to
which the pair of modifiers can be put into a scale
based on contextual entailment.
To ground such scales from text, we collected a
large corpus of online reviews from IMDB. Each
of the reviews in this collection has an associated
star rating: one star (most negative) to ten stars
(most positive). Table 3 summarizes the distribu-
tion of reviews as well as the number of words and
vocabulary across the ten rating categories.
As is evident from table 3, there is a signif-
icant bias for ten-star reviews. This is a com-
mon feature of such corpora of informal, user-
provided reviews (Chevalier and Mayzlin, 2006;
Hu et al, 2006; Pang and Lee, 2008). However,
since we do not want to incorporate the linguis-
tically uninteresting fact that people tend to write
a lot of ten-star reviews, we assume uniform pri-
ors for the rating categories. Let count(w, r) be
the number of tokens of word w in reviews in rat-
ing category r, and let count(r) be the total word
count for all words in rating category r. The prob-
ability of w given a rating category r is simply
Pr(w|r) = count(w, r)/ count(r). Then under the
assumption of uniform priors, we get Pr(r|w) =
Pr(w|r)/
?
r??R Pr(w|r?).
In reasoning about our dialogues, we rescale
the rating categories by subtracting 5.5 from each,
to center them at 0. This yields the scale R =
170
Rating Reviews Words Vocabulary Average words per review
1 124,587 25,389,211 192,348 203.79
2 51,390 11,750,820 133,283 228.66
3 58,051 13,990,519 148,530 241.00
4 59,781 14,958,477 156,564 250.22
5 80,487 20,382,805 188,461 253.24
6 106,145 27,408,662 225,165 258.22
7 157,005 40,176,069 282,530 255.89
8 195,378 48,706,843 313,046 249.30
9 170,531 40,264,174 273,266 236.11
10 358,441 73,929,298 381,508 206.25
Total 1,361,796 316,956,878 1,160,072 206.25
Table 3: Numbers of reviews, words and vocabulary size per rating category in the IMDB review corpus,
as well as the average number of words per review.
enjoyable
0.0
0.1
0.2
0.3
0.4
-4.
5
-3.
5
-2.
5
-1.
5
-0.
5 0.5 1.5 2.5 3.5 4.5
ER = 0.74
best
0.0
0.1
0.2
0.3
0.4
-4.
5
-3.
5
-2.
5
-1.
5
-0.
5 0.5 1.5 2.5 3.5 4.5
ER = 1.08
great
0.0
0.1
0.2
0.3
0.4
-4.
5
-3.
5
-2.
5
-1.
5
-0.
5 0.5 1.5 2.5 3.5 4.5
ER = 1.1
superb
0.0
0.1
0.2
0.3
0.4
-4.
5
-3.
5
-2.
5
-1.
5
-0.
5 0.5 1.5 2.5 3.5 4.5
ER = 2.18
disappointing
0.0
0.1
0.2
0.3
0.4
-4.
5
-3.
5
-2.
5
-1.
5
-0.
5 0.5 1.5 2.5 3.5 4.5
ER = -1.1
bad
0.0
0.1
0.2
0.3
0.4
-4.
5
-3.
5
-2.
5
-1.
5
-0.
5 0.5 1.5 2.5 3.5 4.5
ER = -1.47
awful
0.0
0.1
0.2
0.3
0.4
-4.
5
-3.
5
-2.
5
-1.
5
-0.
5 0.5 1.5 2.5 3.5 4.5
ER = -2.5
worst
0.0
0.1
0.2
0.3
0.4
-4.
5
-3.
5
-2.
5
-1.
5
-0.
5 0.5 1.5 2.5 3.5 4.5
ER = -2.56
Pr
(R
ati
ng
|W
ord
)
Rating (centered at 0)
Figure 2: The distribution of some scalar modifiers across the ten rating categories. The vertical lines
mark the expected ratings, defined as a weighted sum of the probability values (black dots).
??4.5,?3.5,?2.5,?1.5,?0.5, 0.5, 1.5, 2.5, 3.5, 4.5?.
Our rationale for this is that modifiers at the neg-
ative end of the scale (bad, awful, terrible) are
not linguistically comparable to those at the
positive end of the scale (good, excellent, superb).
Each group forms its own qualitatively different
scale (Kennedy and McNally, 2005). Rescaling
allows us to make a basic positive vs. negative
distinction. Once we have done that, an increase
in absolute value is an increase in strength. In
our experiments, we use expected rating values
to characterize the polarity and strength of mod-
ifiers. The expected rating value for a word w
is ER(w) =
?
r?R r Pr(r|w). Figure 2 plots these
values for a number of scalar terms, both positive
and negative, across the rescaled ratings, with
the vertical lines marking their ER values. The
weak scalar modifiers all the way on the left are
most common near the middle of the scale, with
a slight positive bias in the top row and a slight
negative bias in the bottom row. As we move
from left to right, the bias for one end of the scale
grows more extreme, until the words in question
are almost never used outside of the most extreme
rating category. The resulting scales correspond
well with linguistic intuitions and thus provide
an initial indication that the rating categories
are a reliable guide to strength and polarity for
scalar modifiers. We put this information to use
in our dialogue corpus via the decision procedure
171
Let D be a dialogue consisting of (i) a polar question
whose main predication is based on scalar predicate PQ
and (ii) an indirect answer whose main predication is
based on scalar predicate PA. Then:
1. if PA or PQ is missing from our data, infer ?Uncer-
tain?;
2. else if ER(PQ) and ER(PA) have different signs, in-
fer ?No?;
3. else if abs(ER(PQ)) 6 abs(ER(PA)), infer ?Yes?;
4. else infer ?No?.
5. In the presence of negation, map ?Yes? to ?No?, ?No?
to ?Yes?, and ?Uncertain? to ?Uncertain?.
Figure 3: Decision procedure for using the word
frequencies across rating categories in the review
corpus to decide what a given answer conveys.
described in figure 3.
4.2 Interpreting numerical answers
The second technique aims at determining
whether a numerical answer counts as a positive
or negative instance of the adjective in the ques-
tion (category II in our corpus).
Adjectives that can receive a conventional unit
of measure, such as little or long, inherently pos-
sess a degree of vagueness (Kamp and Partee,
1995; Kennedy, 2007): while in the extreme cases,
judgments are strong (e.g., a six foot tall woman
can clearly be called ?a tall woman? whereas a
five foot tall woman cannot), there are borderline
cases for which it is difficult to say whether the
adjectival predication can truthfully be ascribed
to them. A logistic regression model can capture
these facts. To build this model, we gather distri-
butional information from the Web.
For instance, in the case of (3), we can retrieve
from the Web positive and negative examples of
age in relation to the adjective and the modified en-
tity ?little kids?. The question contains the adjec-
tive and the modified entity. The reply contains the
unit of measure (here ?year-old?) and the numer-
ical answer. Specifically we query the Web using
Yahoo! BOSS (Academic) for ?little kids? year-
old (positive instances) as well as for ?not little
kids? year-old (negative instances). Yahoo! BOSS
is an open search services platform that provides a
query API for Yahoo! Web search. We then ex-
tract ages from the positive and negative snippets
obtained, and we fit a logistic regression to these
data. To remove noise, we discard low counts
(positive and negative instances for a given unit
< 5). Also, for some adjectives, such as little or
young, there is an inherent ambiguity between ab-
solute and relative uses. Ideally, a word sense dis-
ambiguation system would be used to filter these
cases. For now, we extract the largest contiguous
range for which the data counts are over the noise
threshold.3 When not enough data is retrieved for
the negative examples, we expand the query by
moving the negation outside the search phrase. We
also replace the negation and the adjective by the
antonyms given in WordNet (using the first sense).
The logistic regression thus has only one fac-
tor ? the unit of measure (age in the case of lit-
tle kids). For a given answer, the model assigns a
probability indicating the extent to which the ad-
jectival property applies to that answer. If the fac-
tor is a significant predictor, we can use the prob-
abilities from the model to decide whether the an-
swer qualifies as a positive or negative instance of
the adjective in the question, and thus interpret the
indirect response as a ?yes? or a ?no?. The prob-
abilistic nature of this technique adheres perfectly
to the fact that indirect answers are intimately tied
up with uncertainty.
5 Evaluation and results
Our primary goal is to evaluate how well we can
learn the relevant scalar and entailment relation-
ships from the Web. In the evaluation, we thus ap-
plied our techniques to a manually coded corpus
version. For the adjectival scales, we annotated
each example for its main predication (modifier, or
adverb?modifier bigram), including whether that
predication was negated. For the numerical cases,
we manually constructed the initial queries: we
identified the adjective and the modified entity in
the question, and the unit of measure in the answer.
However, we believe that identifying the requisite
predications and recognizing the presence of nega-
tion or embedding could be done automatically us-
ing dependency graphs.4
3Otherwise, our model is ruined by references to ?young
80-year olds?, using the relative sense of young, which are
moderately frequent on the Web.
4As a test, we transformed our corpus into the Stanford
dependency representation (de Marneffe et al, 2006), using
the Stanford parser (Klein and Manning, 2003) and were able
to automatically retrieve all negated modifier predications,
except one (We had a view of it, not a particularly good one),
172
Modification in answer Precision Recall
I Other adjective 60 60
Adverb - same adjective 95 95
Negation - same adjective 100 100
Omitted adjective 100 100
II Numerical 89 40
Total 75 71
Table 4: Summary of precision and recall (%) by
type.
Response Precision Recall F1
I Yes 87 76 81
No 57 71 63
II Yes 100 36 53
Uncertain 67 40 50
Table 5: Precision, recall, and F1 (%) per response
category. In the case of the scalar modifiers exper-
iment, there were just two examples whose dom-
inant response from the Turkers was ?Uncertain?,
so we have left that category out of the results. In
the case of the numerical experiment, there were
not any ?No? answers.
To evaluate the techniques, we pool the Me-
chanical Turk ?definite yes? and ?probable yes?
categories into a single category ?Yes?, and we
do the same for ?definite no? and ?probable no?.
Together with ?uncertain?, this makes for three-
response categories. We count an inference as
successful if it matches the dominant Turker re-
sponse category. To use the three-response scheme
in the numerical experiment, we simply catego-
rize the probabilities as follows: 0?0.33 = ?No?,
0.33?0.66 = ?Uncertain?, 0.66?1.00 = ?Yes?.
Table 4 gives a breakdown of our system?s per-
formance on the various category subtypes. The
overall accuracy level is 71% (159 out of 224 in-
ferences correct). Table 5 summarizes the results
per response category, for the examples in which
both the question and answer contain a gradable
modifier (category I), and for the numerical cases
(category II).
6 Analysis and discussion
Performance is extremely good on the ?Adverb ?
same adjective? and ?Negation ? same adjective?
cases because the ?Yes? answer is fairly direct for
them (though adverbs like basically introduce an
interesting level of uncertainty). The results are
because of a parse error which led to wrong dependencies.
Response Precision Recall F1
WordNet-based Yes 82 83 82.5
(items I) No 60 56 58
Table 6: Precision, recall, and F1 (%) per response
category for the WordNet-based approach.
somewhat mixed for the ?Other adjective? cate-
gory.
Inferring the relation between scalar adjectives
has some connection with work in sentiment de-
tection. Even though most of the research in that
domain focuses on the orientation of one term us-
ing seed sets, techniques which provide the ori-
entation strength could be used to infer a scalar
relation between adjectives. For instance, Blair-
Goldensohn et al (2008) use WordNet to develop
sentiment lexicons in which each word has a posi-
tive or negative value associated with it, represent-
ing its strength. The algorithm begins with seed
sets of positive, negative, and neutral terms, and
then uses the synonym and antonym structure of
WordNet to expand those initial sets and refine
the relative strength values. Using our own seed
sets, we built a lexicon using Blair-Goldensohn
et al (2008)?s method and applied it as in figure
3 (changing the ER values to sentiment scores).
Both approaches achieve similar results: for the
?Other adjective? category, the WordNet-based
approach yields 56% accuracy, which is not signif-
icantly different from our performance (60%); for
the other types in category I, there is no difference
in results between the two methods. Table 6 sum-
marizes the results per response category for the
WordNet-based approach (which can thus be com-
pared to the category I results in table 5). However
in contrast to the WordNet-based approach, we re-
quire no hand-built resources: the synonym and
antonym structures, as well as the strength values,
are learned from Web data alone. In addition, the
WordNet-based approach must be supplemented
with a separate method for the numerical cases.
In the ?Other adjective? category, 31 items
involve oppositional terms: canonical antonyms
(e.g., right/wrong, good/bad) as well as terms
that are ?statistically oppositional? (e.g., ready/
premature, true/preposterous, confident/nervous).
?Statistically oppositional? terms are not opposi-
tional by definition, but as a matter of contingent
fact. Our technique accurately deals with most
173
0 10 20 30 40 50 60
0.0
0.2
0.4
0.6
0.8
little kids
Age
Pro
ba
bili
ty 
of 
be
ing
 "li
ttle
"
0 10 20 30 40 50 60
0.2
0.4
0.6
0.8
young kids
Age
Pro
ba
bili
ty 
of 
be
ing
 "y
ou
ng
"
0 20 40 60 80 100 120
0.3
0.4
0.5
0.6
0.7
0.8
warm weather
Degree
Pro
ba
bili
ty 
of 
be
ing
 "w
arm
"
Figure 4: Probabilities of being appropriately described as ?little?, ?young? or ?warm?, fitted on data
retrieved when querying the Web for ?little kids?, ?young kids? and ?warm weather?.
of the canonical antonyms, and also finds some
contingent oppositions (qualified/young, wise/
neurotic) that are lacking in antonymy resources or
automatically generated antonymy lists (Moham-
mad et al, 2008). Out of these 31 items, our tech-
nique correctly marks 18, whereas Mohammad et
al.?s list of antonyms only contains 5 and Blair-
Goldensohn et al (2008)?s technique finds 11. Our
technique is solely based on unigrams, and could
be improved by adding context: making use of de-
pendency information, as well as moving beyond
unigrams.
In the numerical cases, precision is high but re-
call is low. For roughly half of the items, not
enough negative instances can be gathered from
the Web and the model lacks predictive power (as
for items (4) or (12)).
(12) A: Do you happen to be working for a
large firm?
B: It?s about three hundred and fifty
people.
Looking at the negative hits for item (12), one
sees that few give an indication about the num-
ber of people in the firm, but rather qualifications
about colleagues or employees (great people, peo-
ple?s productivity), or the hits are less relevant:
?Most of the people I talked to were actually pretty
optimistic. They were rosy on the job market
and many had jobs, although most were not large
firm jobs?. The lack of data comes from the fact
that the queries are very specific, since the adjec-
tive depends on the product (e.g., ?expensive ex-
ercise bike?, ?deep pond?). However when we
do get a predictive model, the probabilities corre-
Entropy of response distribution
Pro
bab
ility 
of c
orre
ct in
fere
nce
 by 
our 
sys
tem
0.0 0.5 1.0 1.5
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Figure 5: Correlation between agreement among
Turkers and whether the system gets the correct
answer. For each dialogue, we plot a circle at
Turker response entropy and either 1 = correct
inference or 0 = incorrect inference, except the
points are jittered a little vertically to show where
the mass of data lies. As the entropy rises (i.e., as
agreement levels fall), the system?s inferences be-
come less accurate. The fitted logistic regression
model (black line) has a statistically significant co-
efficient for response entropy (p < 0.001).
174
late almost perfectly with the Turkers? responses.
This happens for 8 items: ?expensive to call (50
cents a minute)?, ?little kids (7 and 10 year-old)?,
?long growing season (3 months)?, ?lot of land
(80 acres)?, ?warm weather (80 degrees)?, ?young
kids (5 and 2 year-old)?, ?young person (31 year-
old)? and ?large house (2450 square feet)?. In
the latter case only, the system output (uncer-
tain) doesn?t correlate with the Turkers? judgment
(where the dominant answer is ?probable yes? with
15 responses, and 11 answers are ?uncertain?).
The logistic curves in figure 4 capture nicely the
intuitions that people have about the relation be-
tween age and ?little kids? or ?young kids?, as
well as between Fahrenheit degrees and ?warm
weather?. For ?little kids?, the probabilities of be-
ing little or not are clear-cut for ages below 7 and
above 15, but there is a region of vagueness in be-
tween. In the case of ?young kids?, the probabil-
ities drop less quickly with age increasing (an 18
year-old can indeed still be qualified as a ?young
kid?). In sum, when the data is available, this
method delivers models which fit humans? intu-
itions about the relation between numerical mea-
sure and adjective, and can handle pragmatic in-
ference.
If we restrict attention to the 66 examples on
which the Turkers completely agreed about which
of these three categories was intended (again pool-
ing ?probable? and ?definite?), then the percent-
age of correct inferences rises to 89% (59 cor-
rect inferences). Figure 5 plots the relation-
ship between the response entropy and the accu-
racy of our decision procedure, along with a fit-
ted logistic regression model using response en-
tropy to predict whether our system?s inference
was correct. The handful of empirical points in
the lower left of the figure show cases of high
agreement between Turkers but incorrect infer-
ence from the system. The few points in the up-
per right indicate low agreement between Turk-
ers and correct inference from the system. Three
of the high-agreement/incorrect-inference cases
involve the adjectives right?correct. For low-
agreement/correct-inference, the disparity could
trace to context dependency: the ordering is clear
in the context of product reviews, but unclear in
a television interview. The analysis suggests that
overall agreement is positively correlated with our
system?s chances of making a correct inference:
our system?s accuracy drops as human agreement
levels drop.
7 Conclusion
We set out to find techniques for grounding ba-
sic meanings from text and enriching those mean-
ings based on information from the immediate lin-
guistic context. We focus on gradable modifiers,
seeking to learn scalar relationships between their
meanings and to obtain an empirically grounded,
probabilistic understanding of the clear and fuzzy
cases that they often give rise to (Kamp and Partee,
1995). We show that it is possible to learn the req-
uisite scales between modifiers using review cor-
pora, and to use that knowledge to drive inference
in indirect responses. When the relation in ques-
tion is not too specific, we show that it is also pos-
sible to learn the strength of the relation between
an adjective and a numerical measure.
Acknowledgments
This paper is based on work funded in part by
ONR award N00014-10-1-0109 and ARO MURI
award 548106, as well as by the Air Force Re-
search Laboratory (AFRL) under prime contract
no. FA8750-09-C-0181. Any opinions, findings,
and conclusion or recommendations expressed in
this material are those of the authors and do not
necessarily reflect the view of the Air Force Re-
search Laboratory (AFRL), ARO or ONR.
References
James F. Allen and C. Raymond Perrault. 1980. Ana-
lyzing intention in utterances. Artificial Intelligence,
15:143?178.
Nicholas Asher and Alex Lascarides. 2003. Logics of
Conversation. Cambridge University Press, Cam-
bridge.
Sasha Blair-Goldensohn, Kerry Hannan, Ryan McDon-
ald, Tyler Neylon, George A. Reis, and Jeff Reynar.
2008. Building a sentiment summarizer for local
service reviews. In WWW Workshop on NLP in the
Information Explosion Era (NLPIX).
Judith A. Chevalier and Dina Mayzlin. 2006. The
effect of word of mouth on sales: Online book re-
views. Journal of Marketing Research, 43(3):345?
354.
Herbert H. Clark. 1979. Responding to indirect speech
acts. Cognitive Psychology, 11:430?477.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
175
dependency parses from phrase structure parses. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation (LREC-2006).
Marie-Catherine de Marneffe, Scott Grimm, and
Christopher Potts. 2009. Not a simple ?yes? or
?no?: Uncertainty in indirect answers. In Proceed-
ings of the 10th Annual SIGDIAL Meeting on Dis-
course and Dialogue.
Gilles Fauconnier. 1975. Pragmatic scales and logical
structure. Linguistic Inquiry, 6(3):353?375.
Nancy Green and Sandra Carberry. 1994. A hybrid
reasoning model for indirect answers. In Proceed-
ings of the 32nd Annual Meeting of the Association
for Computational Linguistics, pages 58?65.
Nancy Green and Sandra Carberry. 1999. Interpret-
ing and generating indirect answers. Computational
Linguistics, 25(3):389?435.
Beth Ann Hockey, Deborah Rossen-Knill, Beverly
Spejewski, Matthew Stone, and Stephen Isard.
1997. Can you predict answers to Y/N questions?
Yes, No and Stuff. In Proceedings of Eurospeech
1997, pages 2267?2270.
Laurence R Horn. 1972. On the Semantic Properties of
Logical Operators in English. Ph.D. thesis, UCLA,
Los Angeles.
Nan Hu, Paul A. Pavlou, and Jennifer Zhang. 2006.
Can online reviews reveal a product?s true quality?:
Empirical findings and analytical modeling of online
word-of-mouth communication. In Proceedings of
Electronic Commerce (EC), pages 324?330.
Daniel Jurafsky, Elizabeth Shriberg, and Debra Bi-
asca. 1997. Switchboard SWBD-DAMSL shallow-
discourse-function annotation coders manual, draft
13. Technical Report 97-02, University of Colorado,
Boulder Institute of Cognitive Science.
Hans Kamp and Barbara H. Partee. 1995. Prototype
theory and compositionality. Cognition, 57(2):129?
191.
Christopher Kennedy and Louise McNally. 2005.
Scale structure and the semantic typology of grad-
able predicates. Language, 81(2):345?381.
Christopher Kennedy. 2007. Vagueness and grammar:
The semantics of relative and absolute gradable ad-
jectives. Linguistics and Philosophy, 30(1):1?45.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Meeting of the Association of Computational
Linguistics.
Stephen C. Levinson. 2000. Presumptive Meanings:
The Theory of Generalized Conversational Implica-
ture. MIT Press, Cambridge, MA.
Saif Mohammad, Bonnie Dorr, and Graeme Hirst.
2008. Computing word-pair antonymy. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing and Computational Nat-
ural Language Learning (EMNLP-2008).
Robert Munro, Steven Bethard, Victor Kuperman,
Vicky Tzuyin Lai, Robin Melnick, Christopher
Potts, Tyler Schnoebelen, and Harry Tily. 2010.
Crowdsourcing and language studies: The new gen-
eration of linguistic data. In NAACL 2010 Workshop
on Creating Speech and Language Data With Ama-
zon?s Mechanical Turk.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in In-
formation Retrieval, 2(1):1?135.
C. Raymond Perrault and James F. Allen. 1980. A
plan-based analysis of indirect speech acts. Amer-
ican Journal of Computational Linguistics, 6(3-
4):167?182.
Victor S. Sheng, Foster Provost, and Panagiotis G.
Ipeirotis. 2008. Get another label? improving data
quality and data mining using multiple, noisy label-
ers. In Proceedings of KDD-2008.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast ? but is it
good? evaluating non-expert annotations for natural
language tasks. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-2008).
Henk Zeevat. 1994. Questions and exhaustivity in up-
date semantics. In Harry Bunt, Reinhard Muskens,
and Gerrit Rentier, editors, Proceedings of the In-
ternational Workshop on Computational Semantics,
pages 211?221.
176
Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages 1?8
Manchester, August 2008
The Stanford typed dependencies representation
Marie-Catherine de Marneffe
Linguistics Department
Stanford University
Stanford, CA 94305
mcdm@stanford.edu
Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305
manning@stanford.edu
Abstract
This paper examines the Stanford typed
dependencies representation, which was
designed to provide a straightforward de-
scription of grammatical relations for any
user who could benefit from automatic text
understanding. For such purposes, we ar-
gue that dependency schemes must follow
a simple design and provide semantically
contentful information, as well as offer an
automatic procedure to extract the rela-
tions. We consider the underlying design
principles of the Stanford scheme from this
perspective, and compare it to the GR and
PARC representations. Finally, we address
the question of the suitability of the Stan-
ford scheme for parser evaluation.
1 Introduction
The Stanford typed dependencies representation
was designed to provide a simple description of
the grammatical relationships in a sentence that
could easily be understood and effectively used by
people without linguistic expertise who wanted to
extract textual relations. The representation was
not designed for the purpose of parser evaluation.
Nevertheless, we agree with the widespread senti-
ment that dependency-based evaluation of parsers
avoids many of the problems of the traditional Par-
seval measures (Black et al, 1991), and to the ex-
tent that the Stanford dependency representation
is an effective representation for the tasks envi-
sioned, it is perhaps closer to an appropriate task-
based evaluation than some of the alternative de-
pendency representations available. In this paper
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
we examine the representation and its underlying
design principles, look at how this representation
compares with other dependency representations
in ways that reflect the design principles, and con-
sider its suitability for parser evaluation.
A major problem for the natural language pro-
cessing (NLP) community is how to make the
very impressive and practical technology which
has been developed over the last two decades ap-
proachable to and usable by everyone who has text
understanding needs. That is, usable not only by
computational linguists, but also by the computer
science community more generally and by all sorts
of information professionals including biologists,
medical researchers, political scientists, law firms,
business and market analysts, etc. Thinking about
this issue, we were struck by two facts. First, we
noted how frequently WordNet (Fellbaum, 1998)
gets used compared to other resources, such as
FrameNet (Fillmore et al, 2003) or the Penn Tree-
bank (Marcus et al, 1993). We believe that much
of the explanation for this fact lies in the differ-
ence of complexity of the representation used by
the resources. It is easy for users not necessarily
versed in linguistics to see how to use and to get
value from the straightforward structure of Word-
Net. Second, we noted the widespread use of Mini-
Par (Lin, 1998) and the Link Parser (Sleator and
Temperley, 1993). This clearly shows that (i) it is
very easy for a non-linguist thinking in relation ex-
traction terms to see how to make use of a depen-
dency representation (whereas a phrase structure
representation seems much more foreign and for-
bidding), and (ii) the availability of high quality,
easy-to-use (and preferably free) tools is essential
for driving broader use of NLP tools.
1
1
On the other hand, evaluation seems less important; to the
best of our knowledge there has never been a convincing and
thorough evaluation of either MiniPar or the Link Grammar
1
This paper advocates for the Stanford typed de-
pendencies representation (henceforth SD) being a
promising vehicle for bringing the breakthroughs
of the last 15 years of parsing research to this broad
potential user community. The representation aims
to provide a simple, habitable design. All infor-
mation is represented as binary relations. This
maps straightforwardly on to common representa-
tions of potential users, including the logic forms
of Moldovan and Rus (Moldovan and Rus, 2001),
2
semantic web Resource Description Framework
(RDF) triples (http://www.w3.org/RDF/), and graph
representations (with labeled edges and nodes).
Unlike many linguistic formalisms, excessive de-
tail is viewed as a defect: information that users do
not understand or wish to process detracts from up-
take and usability. The user-centered design pro-
cess saw the key goal as representing semantically
contentful relations suitable for relation extraction
and more general information extraction uses. The
design supports this use by favoring relations be-
tween content words, by maintaining semantically
useful closed class word information while ignor-
ing linguistic decisions less relevant to users, and
by not representing less used material about lin-
guistic features such as tense and agreement. The
SD scheme thus provides a semantic representa-
tion simple and natural enough for people who are
not (computational) linguists but can benefit from
NLP tools.
2 Design choices and their implications
2.1 Design principles
The style of the SD representation bears a strong
intellectual debt to the framework of Lexical-
Functional Grammar (Bresnan, 2001), and, more
directly, it owes a debt to both the sets of gram-
matical relations and the naming defined in two
representations that follow an LFG style: the GR
(Carroll et al, 1999) and PARC (King et al, 2003)
schemes. These were used as a starting point for
developing the Stanford dependencies (de Marn-
effe et al, 2006). But where the SD scheme devi-
ates from GR, PARC, and its LFG roots is that it
has been designed to be a practical model of sen-
tence representation, particularly in the context of
relation extraction tasks.
parser.
2
The logic forms of Moldovan and Rus are in the form
of a predicate calculus representation, although not one that
represents such things as operator scope in a way that most
would expect of a predicate calculus representation.
SD makes available two options, suited to dif-
ferent use cases: in one, every word of the origi-
nal sentence is present as a node with relations be-
tween it and other nodes, whereas in the latter, cer-
tain words are ?collapsed? out of the representa-
tion, making such changes as turning prepositions
into relations. The former is useful when a close
parallelism to the source text words must be main-
tained, whereas the latter is intended to be more
useful for relation extraction and shallow language
understanding tasks. Here, we discuss only the lat-
ter representation; see (de Marneffe et al, 2006)
for a discussion of both options and the precise re-
lationship between them.
The intended use cases of usability by people
who are not (computational) linguists and suitabil-
ity for relation extraction applications led SD to try
to adhere to the following design principles (DPs):
1. Everything is represented uniformly as some
binary relation between two sentence words.
2. Relations should be semantically contentful
and useful to applications.
3. Where possible, relations should use notions
of traditional grammar for easier comprehen-
sion by users.
4. Underspecified relations should be available
to deal with the complexities of real text.
5. Where possible, relations should be between
content words, not indirectly mediated via
function words.
6. The representation should be spartan rather
than overwhelming with linguistic details.
We illustrate many of them in the rest of this sec-
tion, using example sentences which were made
available for the Parser Evaluation Shared Task.
The grammatical relations of SD are arranged in
a hierarchy, rooted with the most generic relation,
dependent. The hierarchy contains 56 grammatical
relations. When the relation between a head and
its dependent can be identified more precisely, re-
lations further down in the hierarchy are used, but
when it is unclear, more generic dependencies are
possible (DP1, DP4). For example, the dependent
relation can be specialized to aux (auxiliary), arg
(argument), or mod (modifier). The arg relation is
further divided into the subj (subject) relation and
the comp (complement) relation, and so on. The
backbone of this hierarchy is quite similar to that
in GR, but there are some crucial differences.
2
2.2 Comparison with GR and PARC
The SD scheme is not concerned with the argu-
ment/adjunct distinction which is largely useless in
practice. In contrast, NP-internal relations are an
inherent part of corpus texts and are critical in real-
world applications. The SD scheme therefore in-
cludes many relations of this kind: appos (apposi-
tive modifier), nn (noun compound), num (numeric
modifier), number (element of compound num-
ber) and abbrev (abbreviation), etc. (DP2). For
instance, in the sentence ?I feel like a little kid,?
says a gleeful Alex de Castro, a car salesman, who
has stopped by a workout of the Suns to slip six
Campaneris cards to the Great Man Himself to be
autographed (WSJ-R), we obtain the following re-
lations under the SD representation:
SD appos(Castro, salesman)
num(cards, six)
nn(cards, Campaneris)
The numeric modifier relation between cards and
six is also standard in the PARC and GR schemes.
PARC provides an apposition relation between
salesman and Alex de Castro, whereas GR only
identifies salesman as a text adjunct of Castro.
But on the whole, SD makes more fine-grained
distinctions in the relations, which are needed in
practice. The adjunct dependency of the PARC
scheme lumps together different relations. For ex-
ample, the adjectival modifier gleeful in the sen-
tence above will not be marked distinctively from
the preposition modifying workout, nor from the
relation between the verbs stop and slip:
PARC adjunct(Alex de Castro, gleeful)
adjunct(kid, little)
adjunct(stop, slip)
adjunct(workout, of)
The SD output for the relations between these
words looks as follows:
SD amod(Castro, gleeful)
amod(kid, little)
xcomp(stop, slip)
prep of(workout, Suns)
The comparison between the two outputs shows
that SD proposes a larger set of dependencies, cap-
turing relation differences which can play a role
in applications (DP2), while sticking to notions of
traditional grammar (DP3).
The SD scheme also chooses content words as
heads of the dependencies (DP5). Auxiliaries,
complementizers, and so on, are dependents of
them. This choice in design is driven by the kind of
information that is useful for applications. For in-
stance, in the sentence Considered as a whole, Mr.
Lane said, the filings required under the proposed
rules ?will be at least as effective, if not more so,
for investors following transactions? (WSJ-R), ef-
fective is chosen as the head of the quoted phrase.
This enables the representation to have a direct de-
pendency (nsubj for nominal subject) between the
key content words effective and filings. Such a
link is more difficult to infer from the GR scheme,
where be is chosen as the head. However the re-
lation between effective and filings is key to ex-
tracting the gist of the sentence semantics, and it
is therefore important for applications to be able
to retrieve it easily. Also, in the case of struc-
tures involving copular verbs, a direct link between
the subject and the complement enables equiva-
lent representations across languages (in Chinese,
for example, copulas are not explicitly expressed).
Such parallel representations should presumably
help machine translation, and this was a further
motivation for choosing content words as heads.
Another instance where direct links between
content words is useful is the case of prepositional
complements. The SD scheme offers the option
of ?collapsing? dependencies involving a preposi-
tion (DP5). In the example above, instead of hav-
ing two relations adjunct(workout, of) and obj(of,
Suns) as in PARC or ncmod(workout, of) and
dobj(of, Suns) as in GR, SD provides a direct rela-
tion between the content words: prep of (workout,
Suns). Prepositions often work as role markers,
and this type of link facilitates the extraction of
how the two content words are related; and thus
these links are often used by downstream applica-
tions (Lin and Pantel, 2001; Snow et al, 2005).
The usefulness of the representation is exemplified
in the sentence A similar technique is almost im-
possible to apply to other crops, such as cotton,
soybeans and rice (WSJ-R) for which SD gives di-
rect links between the entities joined through the
preposition such as:
SD prep such as(crops, cotton)
prep such as(crops, soybeans)
prep such as(crops, rice)
A similar collapsing treatment takes place for
conjuncts (DP5). Consider the following sentence:
Bell, based in Los Angeles, makes and distributes
3
SD nsubj(makes-8, Bell-1)
nsubj(distributes-10, Bell-1)
partmod(Bell-1, based-3)
nn(Angeles-6, Los-5)
prep in(based-3, Angeles-6)
conj and(makes-8, distributes-10)
amod(products-16, electronic-11)
conj and(electronic-11, computer-13)
amod(products-16, computer-13)
conj and(electronic-11, building-15)
amod(products-16, building-15)
dobj(makes-8, products-16)
Figure 1: SD representation for Bell, based in Los
Angeles, makes and distributes electronic, com-
puter and building products.
electronic, computer and building products (WSJ-
R). Figures 1 and 2 give the full dependency out-
put from SD and GR, respectively. The numbers
after the words in the SD representation indicate
the word position in the sentence.
3
From the SD
representation, one can easily see that the sentence
talks about electronic products and computer prod-
ucts as well as building products. By collapsing the
dependencies involving conjuncts, the output pro-
duced is closer to the semantics of the sentence,
and this facilitates information extraction (DP2).
This information is not straightforwardly apparent
in the GR scheme (see figure 2), nor in the PARC
scheme which follows a similar treatment of con-
juncts.
Another choice in the design has been to con-
sistently have binary relations (DP1). All the de-
pendencies form a triple: a grammatical relation
holding between two words (head and dependent).
This gives uniformity to the representation and
renders it very readable, critical features for a user-
centered design. Furthermore, all the information
can be represented by a directed graph, enabling
the creation of both a limpid visual representation
for humans and a canonical data structure for soft-
ware. Moreover, it maps straightforwardly on to
semantic web representations such as OWL and
RDF triples, as exploited in (Zouaq et al, 2006;
Zouaq et al, 2007).
This design choice limits the kind of informa-
tion offered by the SD scheme. For instance, the
PARC scheme contains much more information
3
Without word position, the representation is deficient if
the same word occurs more than once in a sentence.
GR (passive based)
(ncsubj based Bell obj)
(ta bal Bell based)
(iobj based in)
(dobj in Angeles)
(ncmod Angeles Los)
(conj and makes)
(conj and distributes)
(conj and electronic)
(conj and computer)
(conj and building)
(ncsubj and Bell )
(dobj and products)
(ncmod products and)
Figure 2: GR representation for Bell, based in Los
Angeles, makes and distributes electronic, com-
puter and building products.
about individual words, such as verb tense and
aspect, noun number and person, type of NE for
proper nouns, pronoun form, adjective degree, etc.
For the sentence in figures 1 and 2, the following
information is available for the word Los Angeles
in the PARC scheme:
PARC num(Los Angeles?5, sg)
pers(Los Angeles?5, 3)
proper(Los Angeles?5, location)
This kind of information is indubitably valuable,
but is often less used in practice, and does not per
se pertain to dependency data. Adding it lengthens
an output already complex enough, and impedes
readability and convenience. Thus, SD does not
provide such overwhelming detail (DP6).
2.3 Trading off linguistic fidelity and usability
We feel that turning prepositions into relations is
useful for 98% of users 98% of the time. Neverthe-
less opting for usability in this way causes the SD
scheme to sacrifice some linguistic fidelity. One
instance is that modifiers of prepositions are de-
pendent on the verb (or more precisely, on the head
of the clause in which they appear) and not on the
preposition itself. In Bill went over the river and
right through the woods, right will be an adverbial
modifier of went. In He had laughed, simultane-
ously mocking the stupidity of government by cos-
metics and confessing that he was also a part of it,
just as he was part of government by voice coach
and acting coach (BNC), just which modifies as
will be a dependent of the head of the adverbial
4
clause, i.e., part. This induces some distortion in
the exact semantics of the sentence.
The interaction between preposition collapsing
and PP conjunction is another instance in which
the SD treatment slightly alters the semantics of
the sentence. Consider again the sentence Bill
went over the river and right through the woods.
Both prepositions, over and through, are governed
by the verb went. To avoid disjoint subgraphs
when collapsing the relations, examples like this
are transformed into VP coordination, which re-
quires making a copy of the word went. This gives
the following representation, which corresponds to
a sentence like Bill went over the river and went
right through the woods:
SD prep over(went-2, river-5)
prep through(went-2?, woods-10)
conj and(went-2, went-2?)
Not collapsing the relations in such a case would
prevent the alteration of the semantics, but would
lead to a non-uniform treatment of prepositions.
Uniformity is key for readability and user con-
venience. It seems therefore reasonable to use a
representation which sacrifices the exact semantics
of the original sentence by producing a sentence
roughly equivalent, but which ensures uniformity
across relations.
3 The formalism and the tool
Two vital conditions for the success of a depen-
dency scheme are to provide a suitable represen-
tation for users as well as a tool that is easy to
use. Sagae et al (2008) note that the availability of
an automatic procedure to convert phrase structure
parses to SD is the reason for its use in evaluations
of parsers in the biomedical domain. The primary
focus of the SD scheme, however, has been to offer
grammatical relations appropriate for end-users.
The Stanford parser
4
comes with a tool, de-
scribed in (de Marneffe et al, 2006), which pro-
vides for the rapid extraction of the grammati-
cal relations from phrase structure parses. Struc-
tural configurations are used to define grammatical
roles: the semantic head of each constituent of the
parse is identified, using rules akin to the Collins
head rules, but modified to retrieve the semantic
head of the constituent rather than the syntactic
head. As mentioned, content words are chosen as
heads, and all the other words in the constituent
4
http://nlp.stanford.edu/software/lex-parser.shtml
depend on this head. To retrieve adequate heads
from a semantic point of view, heuristics are used
to inject more structure when the Penn Treebank
gives only flat constituents, as is often the case for
conjuncts, e.g., (NP the new phone book and tour
guide), and QP constituents, e.g., (QP more than
300). Then for each grammatical relation, patterns
are defined over the phrase structure parse tree us-
ing the tree-expression syntax defined by tregex
(Levy and Andrew, 2006). Conceptually, each pat-
tern is matched against every tree node, and the
matching pattern with the most specific grammati-
cal relation is taken as the type of the dependency.
The automatic extraction of the relations is not
infallible. For instance, in the sentence Behind
their perimeter walls lie freshly laundered flowers,
verdant grass still sparkling from the last shower,
yew hedges in an ecstasy of precision clipping
(BNC), the system will erroneously retrieve ap-
position relations between flowers and grass, as
well as between flowers and hedges whereas these
should be conj and relations. The system is clue-
less when there is no overt maker of conjunction.
Another limitation of the tool is the treat-
ment of long-distance dependencies, such as wh-
movement and control/raising: the system can-
not handle long-distance dependencies that cross
clauses. In a sentence like What does he think?,
the system will correctly find that what is a direct
object of think:
SD dobj(think-4, What-1)
aux(think-4, does-2)
nsubj(think-4, he-3)
However in a sentence such as Who the hell does
he think he?s kidding? (BNC), the automatic ex-
traction will fail to find that who is the direct ob-
ject of kidding. Here, it is vital to distinguish be-
tween SD as a representation versus the extant con-
version tool. Long-distance dependencies are not
absent from the formalism, but the tool does not
accurately deal with them.
5
4 Stanford dependencies in practice
SD has been successfully used by researchers in
different domains. In the PASCAL Recognizing
5
As possible future work, we have thought of using a tool
such as the one of Levy and Manning (2004) to correctly de-
termine long distance dependencies, as input to the current
dependency conversion system. This would presumably be
effective, but would make the conversion process much heav-
ier weight.
5
Textual Entailment (RTE) challenges (Dagan et al,
2006; Giampiccolo et al, 2007), the increase in
the use of SD is clearly apparent. The goal in
these challenges consists of identifying whether
one sentence follows from a piece of text and gen-
eral background knowledge, according to the intu-
itions of an intelligent human reader. In 2007, out
of the 21 systems which participated in the chal-
lenge, 5 used the SD representation, whereas the
year before only the Stanford entry was using it.
SD is also widely present in the bioinformatic
world where it is used with success (Erkan et al,
2007; Greenwood and Stevenson, 2007; Urbain et
al., 2007; Clegg, 2008). Fundel et al (2007) found
that, in extraction of relations between genes and
proteins, a system based on the SD scheme greatly
outperformed the previous best system on the LLL
challenge dataset (by an 18% absolute improve-
ment in F-measure). Airola et al (2008) provide
more systematic results on a number of protein-
protein interaction datasets. Their graph kernel ap-
proach uses an all-dependency-paths kernel which
allows their system to consider full dependency
graphs. Their system is based on the SD scheme,
and they demonstrate state-of-the-art performance
for this approach.
In the biomedical domain, SD has recently been
used in evaluations of parsers (Clegg and Shep-
herd, 2007; Pyysalo et al, 2007a). Pyysalo et al
(2007a) assessed the suitability of the SD scheme
over the Link Grammar dependency scheme in an
application-oriented evaluation. The Link Parser
indeed uses a very fine-grained set of relations,
which often makes distinctions of a structural
rather than a semantic nature. One example is the
MX relation which ?connects modifying phrases
with commas to preceding nouns (?The DOG, a
POODLE, was black?; ?JOHN, IN a black suit,
looked great?).? The Link Parser uses a different
set of dependency types for dependencies appear-
ing in questions and relative clauses. Another ex-
ample is the prepositional phrase where alterna-
tive attachment structures are indicated by differ-
ent relations. Many of these distinctions are too
fine and non-semantic to be of practical value. The
SD scheme, by aiming for an intermediate level of
granularity, and targeting semantic dependencies,
provides a more adequate representation for appli-
cations. Therefore, to increase the usability of the
BioInfer corpus (Pyysalo et al, 2007b), which pro-
vides manually annotated data for information ex-
traction in the biomedical domain and originally
followed the Link Grammar scheme, Pyysalo et
al. (2007a) developed a version of the corpus an-
notated with the SD scheme. They also made
available a program and conversion rules that they
used to transform Link Grammar relations into SD
graphs, which were then hand-corrected (Pyysalo
et al, 2007b). While a limited amount of gold stan-
dard annotated data was prepared for the Parser
Evaluation Shared Task, this is the main source of
gold-standard SD data which is currently available.
In other domains, Zhuang et al (2006) uses the
representation to extract opinions about features in
reviews and Meena and Prabhakar (2007) uses it
to improve the quality of sentence-level sentiment
analysis. The open information extraction system
TEXTRUNNER (Banko et al, 2007) also makes use
of the SD graph representation: its first module
uses the Stanford parser and the dependency tool
to automatically identify and label trustworthy and
untrustworthy extractions. Even in theoretical lin-
guistic work, SD has proven very useful: it has
hugely facilitated data extraction from corpora, in
the context of the NSF-funded project ?Dynamics
of probabilistic grammar? carried out at the Stan-
ford Linguistics department.
5 Suitability for parser evaluation
When seeking a gold-standard dependency scheme
for parser evaluation, the ultimate goal of such an
evaluation is an important question. It is necessary
to contrast the two different forms that evaluation
can take: extrinsic task-based evaluation and in-
trinsic evaluation. We tend to agree with Moll?a
and Hutchinson (2003) that intrinsic evaluations
have limited value and that task-based evaluation
is the correct approach. Some of the results of the
previous section at least broadly support the util-
ity of the SD scheme for practical use in higher-
level tasks. Nevertheless, given the current trend
in the NLP community as well as in other fields
such as bioinformatics, where the advantage of de-
pendency representations for shallow text under-
standing tasks has become salient, we would ar-
gue, following Clegg and Shepherd (2007), that
dependency-based evaluation is close to typical
user tasks. Moreover, it avoids some of the known
deficiencies of other parser evaluation measures
such as Parseval (Carroll et al, 1999).
Recent work on parser evaluation using depen-
dency graphs in the biomedical domain confirms
6
that researchers regard dependency-based evalu-
ation as a more useful surrogate for extrinsic
task-based evaluation (Clegg and Shepherd, 2007;
Pyysalo et al, 2007a). In their evaluation, Clegg
and Shepherd (2007) aimed at analyzing the ca-
pabilities of syntactic parsers with respect to se-
mantically important tasks crucial to biological
information extraction systems. To do so, they
used the SD scheme, which provides ?a de facto
standard for comparing a variety of constituent
parsers and treebanks at the dependency level,? and
they assessed its suitability for evaluation. They
found that the SD scheme better illuminates the
performance differences between higher ranked
parsers (e.g., Charniak-Lease parser (Lease and
Charniak, 2005)), and lower ranked parsers (e.g.,
the Stanford parser (Klein and Manning, 2003)).
Their parser evaluation accommodates user needs:
they used the collapsed version of the dependency
graphs offered by the SD scheme, arguing that this
is the kind of graph one would find most useful in
an information extraction project. Although Clegg
and Shepherd (2007) also favor dependency graph
representations for parser evaluation, they advo-
cate retention of parse trees so information lost in
the dependency structures can be accessed.
In essence, any existing dependency scheme
could be adopted as the gold-standard for evalu-
ation. However if one believes in ultimately valu-
ing extrinsic task-based evaluation, a dependency
representation which proposes a suitable design for
users and user tasks is probably the best surrogate
for intrinsic evaluation. Moreover, the existence
of tools for automatically generating and convert-
ing dependency representations has aided greatly
in making parser comparison possible across dif-
ferent formalisms. We believe that the SD scheme
approaches these goals. If one accepts the goals
set here, in order to enforce uniformity between
application and evaluation, it seems sensible to
have a unique scheme for both purposes. Some
of the positive results from use of the SD represen-
tation, as well as the evaluations carried out in the
biomedical field, point to the usability of the SD
scheme for both purposes.
Acknowledgments
We wish to thank Andrew Brian Clegg and Sampo
Pyysalo for their useful feedback on the depen-
dency extraction tool. Their comments enabled us
to improve the tool. We also thank the workshop
reviewers for their helpful comments.
References
Airola, Antti, Sampo Pyysalo, Jari Bj?orne, Tapio
Pahikkala, Filip Ginter, and Tapio Salakoski. 2008.
A graph kernel for protein-protein interaction ex-
traction. In Proceedings of BioNLP 2008: Current
Trends in Biomedical Natural Language Processing
(ACL08).
Banko, Michele, Michael J. Cafarella, Stephen Soder-
land, Matt Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Pro-
ceedings of the 20th International Joint Conference
on Artificial Intelligence (IJCAI 2007).
Black, E., S. Abney, D. Flickinger, C. Gdaniec, R. Gr-
ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. A proce-
dure for quantitatively comparing the syntactic cov-
erage of English grammars. In Proceedings, Speech
and Natural Language Workshop, pages 306?311,
Pacific Grove, CA. DARPA.
Bresnan, Joan. 2001. Lexical-Functional Syntax.
Blackwell, Oxford.
Carroll, John, Guido Minnen, and Ted Briscoe. 1999.
Corpus annotation for parser evaluation. In Proceed-
ings of the EACL workshop on Linguistically Inter-
preted Corpora (LINC).
Clegg, Andrew B. and Adrian J. Shepherd. 2007.
Benchmarking natural-language parsers for biolog-
ical applications using dependency graphs. BMC
Bioinformatics, 8:24.
Clegg, Andrew B. 2008. Computational-Linguistic
Approaches to Biological Text Mining. Ph.D. the-
sis, School of Crystallography, Birkbeck, University
of London.
Dagan, Ido, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL recognising textual entailment
challenge. In et al, Quinonero-Candela, editor,
MLCW 2005, LNAI Volume 3944, pages 177?190.
Springer-Verlag.
de Marneffe, Marie-Catherine, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC-06.
Erkan, Gunes, Arzucan Ozgur, and Dragomir R. Radev.
2007. Semi-supervised classification for extracting
protein interaction sentences using dependency pars-
ing. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL).
Fellbaum, Christiane. 1998. WordNet: an electronic
lexical database. MIT Press.
7
Fillmore, Charles J., Christopher R. Johnson, and Mir-
iam R.L. Petruck. 2003. Background to FrameNet.
International Journal of Lexicography, 16:235?250.
Fundel, Katrin, Robert K?uffner, and Ralf Zimmer.
2007. RelEx relation extraction using dependency
parse trees. Bioinformatics, 23.
Giampiccolo, Danilo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The third PASCAL recogniz-
ing textual entailment challenge. In Proceedings of
the ACL-PASCAL Workshop on Textual Entailment
and Paraphrasing, pages 1?9.
Greenwood, Mark A. and Mark Stevenson. 2007.
A semi-supervised approach to learning relevant
protein-protein interaction articles. In Proceedings
of the Second BioCreAtIvE Challenge Workshop,
Madrid, Spain.
King, Tracy H., Richard Crouch, Stefan Riezler, Mary
Dalrymple, and Ronald Kaplan. 2003. The PARC
700 dependency bank. In 4th International Work-
shop on Linguistically Interpreted Corpora (LINC-
03).
Klein, Dan and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Meeting of the Association for Computational
Linguistics.
Lease, Matthew and Eugene Charniak. 2005. Parsing
biomedical literature. In Proceedings of the Second
International Joint Conference on Natural Language
Processing (IJCNLP?05).
Levy, Roger and Galen Andrew. 2006. Tregex
and Tsurgeon: tools for querying and manipulating
tree data structures. In LREC 2006. http://www-
nlp.stanford.edu/software/tregex.shtml.
Levy, Roger and Christopher D. Manning. 2004. Deep
dependencies from context-free statistical parsers:
correcting the surface dependency approximation.
In ACL 42, pages 328?335.
Lin, Dekang and Patrick Pantel. 2001. Discovery of in-
ference rules for question answering. Natural Lan-
guage Engineering, 7(4):343?360.
Lin, Dekang. 1998. Dependency-based evaluation of
MINIPAR. In Workshop on the Evaluation of Pars-
ing Systems, Granada, Spain.
Marcus, Mitchell P., Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The Penn Treebank. Computa-
tional Linguistics, 19 (2).
Meena, Arun and T. V. Prabhakar. 2007. Sentence level
sentiment analysis in the presence of conjuncts using
linguistic analysis. In Advances in Information Re-
trieval, volume 4425 of Lecture Notes in Computer
Science. Springer.
Moldovan, Dan I. and Vasile Rus. 2001. Logic form
transformation of wordnet and its applicability to
question answering. In Meeting of the Association
for Computational Linguistics, pages 394?401.
Moll?a, Diego and Ben Hutchinson. 2003. Intrinsic ver-
sus extrinsic evaluations of parsing systems. In Pro-
ceedings of the Workshop on Evaluation Initiatives
in Natural Language Processing, pages 43?50. Eu-
ropean Association for Computational Linguistics.
Pyysalo, Sampo, Filip Ginter, Katri Haverinen, Juho
Heimonen, Tapio Salakoski, and Veronika Laippala.
2007a. On the unification of syntactic annotations
under the Stanford dependency scheme: A case
study on BioInfer and GENIA. In Proceedings of
BioNLP 2007: Biological, translational, and clini-
cal language processing (ACL07).
Pyysalo, Sampo, Filip Ginter, Juho Heimonen, Jari
Bj?orne, Jorma Boberg, Jouni J?arvinen, and Tapio
Salakoski. 2007b. BioInfer: A corpus for infor-
mation extraction in the biomedical domain. BMC
Bioinformatics, 8:50.
Sagae, Kenji, Yusuke Miyao, and Jun?ichi Tsujii. 2008.
Challenges in mapping of syntactic representations
for framework-independent parser evaluation. In
Proceedings of the Workshop on Automated Syntatic
Annotations for Interoperable Language Resources
at the First International Conference on Global In-
teroperability for Language Resources (ICGL?08).
Sleator, Daniel D. and Davy Temperley. 1993. Parsing
English with a link grammar. In Third International
Workshop on Parsing Technologies.
Snow, Rion, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In Proceedings of NIPS 2004.
Urbain, Jay, Nazli Goharian, and Ophir Frieder. 2007.
IIT TREC 2007 genomics track: Using concept-
based semantics in context for genomics literature
passage retrieval. In The Sixteenth Text REtrieval
Conference (TREC 2007) Proceedings.
Zhuang, Li, Feng Jing, Xiao yan Zhu, and Lei Zhang.
2006. Movie review mining and summarization. In
Proc. ACM Conference on Information and Knowl-
edge Management (CIKM).
Zouaq, Amal, Roger Nkambou, and Claude Frasson.
2006. The knowledge puzzle: An integrated ap-
proach of intelligent tutoring systems and knowledge
management. In Proceedings of the 18th IEEE Inter-
national Conference on Tools with Artificial Intelli-
gence (ICTAI 2006), pages 575?582.
Zouaq, Amal, Roger Nkambou, and Claude Frasson.
2007. Building domain ontologies from text for edu-
cational purposes. In Proceedings of the Second Eu-
ropean Conference on Technology Enhanced Learn-
ing: Creating new learning experiences on a global
scale.
8
