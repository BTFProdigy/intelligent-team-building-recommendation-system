Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 345?352
Manchester, August 2008
Modeling Chinese Documents 
 with Topical Word-Character Models  
Wei Hu1           Nobuyuki Shimizu2 
1Department of Computer Science 
Shanghai Jiao Tong University 
Shanghai, China 200240 
{no_bit,hysheng} 
@sjtu.edu.cn 
Hiroshi Nakagawa2           Huanye Sheng1 
2Information Technology Center 
The University of Tokyo 
Tokyo, Japan 113-0033 
{shimizu, nakagawa} 
@r.dl.itc.u-tokyo.ac.jp 
Abstract 
As Chinese text is written without word 
boundaries, effectively recognizing Chi-
nese words is like recognizing colloca-
tions in English, substituting characters 
for words and words for collocations. 
However, existing topical models that in-
volve collocations have a common limi-
tation. Instead of directly assigning a top-
ic to a collocation, they take the topic of 
a word within the collocation as the topic 
of the whole collocation. This is unsatis-
factory for topical modeling of Chinese 
documents. Thus, we propose a topical 
word-character model (TWC), which al-
lows two distinct types of topics: word 
topic and character topic. We evaluated 
TWC both qualitatively and 
quantitatively to show that it is a power-
ful and a promising topic model.  
1 Introduction 
Topic models (Blei et al, 2003; Griffiths & 
Steyvers 2004, 2007) are a class of statistical 
models in which documents are expressed as 
mixtures of topics, where a topic is a probability 
distribution over words. A topic model is a gen-
erative model for documents: it specifies a prob-
abilistic procedure for generating documents. To 
make a new document, we choose a distribution 
over topics. Then, for each word in this docu-
ment, we randomly select a topic from the distri-
bution, and draw a word from the topic. Once we 
have a topic model, we can invert the generating 
process, inferring the set of topics that was re-
sponsible  for  generating  a  collection  of  docu- 
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
ments.  
Although most topic models treat a document 
as a bag-of-words, the assumption has obvious 
shortcomings. Suppose there are many docu-
ments about art and musicals in New York. Then, 
we find a topic represented by words such as 
?art?, ?musical?, and ?New?, instead of getting 
?New York?. The bag-of-words assumption 
makes the topic model split a collocation?a 
phrase with meaning beyond the individual 
words?into individual words that have a differ-
ent meaning. One example of a collocation is the 
phrase ?white house?. In politics, it carries a spe-
cial meaning beyond a house that is white, 
whereas ?yellow house? does not.  
While it is reasonable for English, the equiva-
lent bag-of-characters assumption is especially 
troublesome for modeling Chinese documents, 
where almost all basic vocabularies are the equi-
valents of English collocations. In Chinese, some 
of the most commonly used couple-of-thousand 
characters are combined to make up a word, and 
no word boundary is given in the text. 
Effectively, Chinese words are like collocations 
in English. The difficulty is that there are over-
whelmingly more of them, enough to render a 
bag-of-character assumption unreasonable for 
Chinese. Therefore, a topical model for Chinese 
should be capable of detecting the boundary be-
tween two words, as well as assigning a topic to 
each word. 
While topic models for Chinese documents 
bear some similarity to collocation models in 
English, existing topical collocation discovery 
models, such as the LDA (latent Dirichelt alloca-
tion) Collocation model (LDACOL) (Griffiths et 
al., 2007) and the topical N-gram model (TNG) 
(Wang et al, 2007), do not directly assign a topic 
to a collocation. These models find the bounda-
ries of phrases and assign a topic to each word. 
The problem is in the next step?the topic of the 
collocation is exactly the same as one of the 
words. This is like saying that the topic of ?white 
345
house? is the same as either that of ?white? or 
?house?. We propose a new topical model, the 
topical word-character model (TWC), which 
aims to overcome these limitations.  
We evaluated the model both quantitatively 
and qualitatively. For the quantitative analysis, 
we compared the performance of TWC and TNG 
using a standard measure perplexity. For the 
qualitative analysis, we evaluated TWC?s ability 
to discover Chinese words and assign topics in 
comparison with TNG. 
The rest of the paper is organized as follows. 
Section 2 reviews topic models that aim to in-
clude collocations explicitly in the model and 
analyzes their limitations. Section 3 presents our 
new model TWC. Section 4 gives details of our 
consideration on inference for TWC. Section 5 
presents our qualitative and quantitative experi-
ments. Section 6 concludes with a summary and 
briefly mentions future work.  
2 Topic Models for Collocation Discov-
ery 
Since Chinese word discovery is similar to 
English collocation discovery, we first review 
some related topic models for collocation 
discovery. 
Although collocation discovery has long been 
studied, most methods are based on frequency or 
variance. LDACOL is an attempt to model collo-
cations in a topical scheme. Starting from the 
LDA topic model, LDACOL introduces special 
random variables xG . Variable xi = 1 implies that 
the  corresponding  word  wi  and  previous  word 
wi-1 belong to the same phrase, while xi = 0 im-
plies otherwise. Thus, LDACOL can decide the 
length of a phrase dynamically.  
TNG is a powerful generalization of LDACOL. 
Its graphical model is shown in Figure 1. 
 
Figure 1: Topical n-gram model. 
The model is defined in terms of three sets of  
variables: a sequence of words w
G
, a sequence of 
topics z
G
, and a sequence of indicators x
G
. TNG 
assumes the following generative process for 
documents. 
1. For each document d, draw ?d ~ 
Dirichlet(?). 
2. For each topic z, draw ?z ~ Dirichlet(?). 
3. For each topic z and each word w, draw ?zw 
~ Dirichlet(?).  
4. For each topic z and each word w, draw ?zw 
~ Beta(?). 
5. For each word wd,,i in document d: 
(a) draw xd,,i ~ Bernoulli(
d ,i 1 d ,i 1z ,w
? ? ? ), 
(b) draw zd,,i ~ Discrete(?d), 
(c) draw wd,,i ~ Discrete(
d ,iz
? )  if xd,,i=0, 
 draw wd,,i ~ Discrete(
d ,i d ,i 1z ,w
? ? )  if xd,,i=1, 
where ?, ?, ? are Dirichlet priors and ? is a Beta 
prior, zd,i denotes the ith topic assignment in 
document d, wd,i denotes the ith word in document 
d, and xd,i denotes the indicator between wd,i-1 and 
wd,i. Note that the variable xd,i = 1 implies that 
word wd,i-1 and its neighbor wd,i belong to the 
same phrase, while xd,i = 0 implies otherwise. 
However, the topics assigned to them (zd,i-1 and 
zd,i) are not required to be identical to each other. 
To decide the topic of a phrase, we can simply 
take the first (or last) word?s topic or the most 
common topic in the phrase. The authors of TNG 
prefer to choose the last word?s topic as the 
phrase topic because the last noun in a colloca-
tion is usually the ?head noun? in English.  
However, this simple strategy may be ineffec-
tive when we apply TNG to Chinese documents. 
The topics of ???? (game) and ????? 
(tournament) should be represented by their last 
characters while those of ???? (farmer) and 
???? (agriculture) should be represented by 
their first characters. And occasionally, the topic 
of a Chinese word is not identical to any topic of 
its component characters. For example ???? 
(Bluetooth) is neither a color nor a tooth.  
To overcome the limitation of TNG, we must 
discard its underlying assumption: that the topic 
of a whole word is the same as the topic of at 
least one of its components.  
3 Modeling Word Topic and Character 
Topic 
This section describes our topical word-character 
model (TWC), which models two distinct types 
of topics: word topic and character topic. 
 
? ?? ? 
    
? ? 
?? 
    
??? 
??? 
??? 
???
???
???
zi+1 zi+2 zi zi-1 
xi+2 xi+3xi-1xi xi-1 
  wi+1 wi+2 wi wi-1 
346
3.1 Word topic and character topic 
To solve the problem associated with the ??
?? (Bluetooth) example, we need to distinguish 
between the topics of characters and words. 
Therefore, we introduce a new type of topic for 
words in addition to the topics assigned to char-
acters. When generating a Chinese character, we 
first draw a word topic and then choose a charac-
ter topic. A schematic description of this model 
is shown in Figure 2.  
 
Figure 2: Schematic description of modeling 
Chinese documents with character and word top-
ics.  
Here, we use random variables z
G
 and t
G
 to 
denote word and character topics, respectively. 
Note that the word topic and character topics 
have a hierarchical tree-like structure (upper 
layer in Figure 2), whereas character topics and 
characters form a hidden Markov model (HMM) 
(lower layer in Figure 2). 
3.2 Topical word-character model (TWC)  
There are some indicators in the upper-right 
corner of each character topic in Figure 2. They 
help us to tell whether the current character be-
longs to the same word as the previous one. Now 
the question left is how to probabilistically draw 
these indicators, i.e., how to determine the length 
of the Markov chain. 
There are two ways to set the values of the in-
dicators. One is similar to that applied in the hid-
den semi-Markov model (HSMM), which gener-
ates the duration of a segment from the state. Ac-
cordingly, we could first choose the length of a 
word from the distribution associated with the 
word topic and then assign 0 or 1 to each indica-
tor. The other method is to directly draw indica-
tors from the distribution associated with the 
previous character and topic, just as LDACOL 
and TNG do. The difference between these two 
methods is that the former determines the length 
of a word in advance while the latter increases 
the length dynamically.  
We  prefer the  second choice  because it takes 
into consideration a lot of context information. In 
fact, our experimental results indicate that it has 
better performance. 
The formal definition of our model with word 
and character topics is as follows.  
 
Figure 3: Topical word-character model.  
TWC has four sets of variables: a sequence of 
characters c
G
, a sequence of character topics t
G
, a 
sequence of word topics z
G
, and a sequence of 
indicators x
G
. A document is generated via the 
following procedure.  
1. For each document d, draw ?d ~ 
Dirichlet(?);  
2. For each word topic z, draw ?z ~ 
Dirichlet(?); 
3. For each word topic z and each character 
topic t, draw ?zt ~ Dirichlet(?);  
4. For each word topic z, each character topic t 
and each character c, draw ?ztc ~ Beta(?); 
5. For each character topic t, draw ?t ~ 
Dirichlet(?); 
6. For each character cd,,i in document d: 
(a) draw xd,,i ~ Bernoulli( ? ? ?d ,i 1 d ,i 1 d ,i 1z ,t ,c? ); 
(b) draw zd,,i ~ Discrete (?d)  if xd,,i=0; 
     zd,,i= zd,,i-1    if xd,,i=1; 
(c) draw td,,i ~ Discrete(
d ,iz
? ) if xd,,i=0; 
    draw td,,i ~ Discrete( ?d ,i d ,i 1z ,t? )  if xd,,i=1; 
(d) draw cd,,i ~ Discrete(
d ,it
? ). 
Here, ?, ?, ?, ? are Dirichlet priors and ? is a 
Beta prior, zd,i denotes the ith word topic assignment 
in document d, td,i denotes the ith character topic as-
signment in document d, cd,i denotes the ith character 
in document d, and xd,i denotes the indicator between 
cd,i-1 and cd,i. 
Note that compared with the schematic model 
in Figure 2, each character has its corresponding 
Character
 topic
Word
 topic
 
? 
 
   
z2 z1 
t13 t21 t12 t11 
c13 c21 c12 c11 
 
 
t22 
c22 
1 1 0 1 0 
? ?? ?
 
  
??
?? ? ? 
  
??? 
??? 
??? 
??? 
??? 
???
??? 
??? 
  
zi+1 zi+2 zizi-1
ti+1 ti+2 titi-1
ci+1 ci+2 cici-1
xi+2 xi+3 xi-1xixi-1
347
word topic in the TWC model. This is because 
we cannot decide how many words there will be 
in a document and how many characters there 
will be in a certain word in advance. In other 
words, the structure of the ideal model is not 
fixed. Therefore, we duplicate word topic vari-
ables for each character. 
4 Inference with TWC 
Many approximate inference techniques such as 
variational methods, expectation propagation, 
and Gibbs sampling can be applied to graphical 
models. We use Gibbs sampling to perform our 
Bayesian inference in TWC.  
Gibbs sampling is a simple and widely appli-
cable Markov chain Monte Carlo (MCMC) algo-
rithm. In a traditional procedure, variables are 
sequentially sampled from their distributions 
conditioned on all other variables in the model. 
An extension of the basic approach is to 
choose blocks of variables first and then sample 
jointly from the variables in each block in turn, 
conditioned on the remaining variables; this is 
called blocking Gibbs sampling.  
When sampling for TWC, we separate vari-
ables into three types of blocks in the following 
manner (as shown in Figure 4). 
1. character variables ti 
2. indicators xi, whose value is 1 after n itera-
tions 
3. word topics zi, zi+1, ?, zi+l-1 and indicator xi, 
satisfying xi=xi+l=1 and xj=0 (j from i to i+l-1) 
after n iterations 
 
Figure 4: Illustration of partition in a certain it-
eration.  
Note that variables , , ,? ? ? ?
G G GG
 and ?
G
are not 
sampled. This is because we can integrate them 
out according to their conjugate priors. We only 
need to sample variables z
G
, x
G
, and t
G
. 
Before discussing the inference of conditional 
probabilities, let us analyze our partition strategy 
in detail. We will explain the reasons for (1) 
sampling zi, zi+1, ?, zi+l-1 together and (2) sam-
pling z
G
 and xi together 
1. Why do we sample zi, zi+1, ?, zi+l-1 together? 
Assume that we draw zi, zi+1, ?, zi+l-1 one by 
one, and it is now time to sample zi+1 according 
to the conditional probability 
( )( | , , , , , , , , )i 1 i 1P z j z x t c ? ? ? ? ?+ ? += GG GG , 
where ( )i 1z? +
G
denotes a word topic except i 1z + . 
Recall step 6-b in the generative TWC model: it 
says ?If xd,,i=1, then zd,,i= zd,,i-1?, which implies  
( | , ) ( )i 1 i i 1 i 1 iP z z x 1 I z z+ + += = = . 
As this probability is a factorial of the target 
probability, it follows that   
( )( | , , , , , , , , )i 1 i 1P z j z x t c ? ? ? ? ? 0+ ? += =GG GG   
for all ij z? . In other words, zi+1 should be equal 
to zi and not change during sampling.  
It seems that step 6-b in the generative model 
causes the problem. But supposing that we do not 
set zi+1 to zi; it is still more reasonable to sample 
z
G
 together. According to our partition principle, 
xi, xi+1, ?, xi+l-1, xi+l is a continuous indicator 
sequence whose head and tail are both 0 and the 
rest are 1, which implies that character string ci, 
ci+1, ?, ci+l-1 forms a word and has the same 
word topic. Recall the schematic model in Figure 
2: the word topic and character topics have a 
tree-like structure and each word has only one 
word topic node. We add some auxiliary dupli-
cates just because the ideal model is not fixed. 
Therefore, it is natural to sample the word topic 
together with its duplicates.  
2. Why do we sample z
G
 and xi together? 
Let us consider the probability of converting xi 
from 0 to 1 in the current sampling iteration. As-
sume that the number of word topics is 3, zi-1=2, 
and 
( ... | ) / ( ) ,
( | ... ) / ( ) ,
i i l 1 i
i i i l 1
P z z j x 0 1 3 1 j 3
P x k z z 2 1 2 0 k 1
+ ?
+ ?
= = = = = ? ?
= = = = = ? ?
where other variables and priors are omitted. If 
we first sample z
G
 and next sample xi, then the 
probability of drawing 1 for xi is 1/6, according 
to the multiplication principle. If we sample z
G
 
and xi together, the probability of drawing 1 for xi 
is ( ... , )i i l 1 iP z z 2 x 1+ ?= = = = . 
Since 
( ... , )
( ... , )
( ... , ) ( )
1 3
i i l 1 i
k 0 j 1
3
i i l 1 i
j 1
i i l 1 i i 1
1 P z z j x k
P z z j x 0
z z 2 x 1 z 2
+ ?
= =
+ ?
=
+ ? ?
=
==
= = = = =
= = = = =
+ = = = = =
??
?  
and  
( ... , )i i l 1 iP z z 2 x 1+ ?= = = =  
  
1 0 1 1 0
    
???
???
???ti ti+2 ti-1 ti-2 
??? 
zi zi+1 zi-1 zi-2 zi+2 
ti+2 
??? 
???  
0 
 
348
( ... , )
( ... , ) ( )
i i l 1 i
i i l 1 i
P z z 2 x 0
P z z j x 0 1 j 3
+ ?
+ ?
= = = = =
= = = = = ? ? , 
we get  
( ... , ) / /i i l 1 iP z z 2 x 1 1 4 1 6+ ?= = = = = > . 
In conclusion, the model is more likely to form 
long words, if we sample z
G
and xi together. This 
is preferred because both TNG and TWC tend to 
produce shorter words than we would like.  
For each type of block, we need to work out 
the corresponding conditional probability. 
, ,
, ,
, , , ,
, ( : ) ,
( | , , , , , , , , )
( | , , , , , , , , )
( ,
| , , , , , , , , )
d i d i
d i d i
d i d i 1 d i l 1 d i
d i i l 1 d i
P t s z x t c ? ? ? ? ?
P x k z x t c ? ? ? ? ?
P z z z j x k
z x t c ? ? ? ? ?P
?
?
+ + ?
? + ? ?
=
=
= = = = =
GG GG
GG GG
"
GG GG
 
where ,d it ?
G
 denotes the character topic assign-
ments except td,i, ,d ix ?
G
 denotes the indicators ex-
cept xd,i, and , ( : )d i i l 1z ? + ?
G
denotes the word topic 
assignments except ,d jz
G
 (j from i to i+l-1). De-
tails of the derivation of these conditional prob-
abilities are provided in Appendix A.1. 
5 Experiments 
In this section, we discuss our evaluation of 
TWC in Chinese document modeling and Chi-
nese word and topic discovery.  
5.1 Modeling documents 
To evaluate the generalization performance of 
our model, we trained both TWC and TNG on a 
Chinese corpus and computed the perplexity of 
the held-out test set. Perplexity, which indicates 
the uncertainty in predicting a single character, is 
a standard measure of performance for statistical 
models of natural language. A lower perplexity 
score indicates better generalization performance. 
Formally, the perplexities for TWC and TNG 
are defined as follows. 
( )
? ? ? ??log ( | , , , , )
exp{ }
TWC test
D
d
d 1
D
d
d 1
perplexity D
p c ? ? ? ? ?
N
=
=
= ?
?
?
GG G G GG
, 
where Dtest is the testing data, D is the number of 
documents in Dtest, Nd is the number of characters 
in document d, ,d? z? is simply set to 1/Z (Z is 
number of word topics), and ? ? ??, , ,? ? ? ?
G G GG
are poste-
rior estimates provided by applying TWC to 
training data. Details of the parameter estimation 
for TWC are provided in Appendix A.2. 
( )TNG testperplexity D  
? ? ??log ( | , , , )
exp{ }
D
d
d 1
D
d
d 1
p c ? ? ? ?
N
=
=
= ?
?
?
GG G GG
, 
where Dtest, D, and Nd are the same as defined for 
the TWC perplexity, ,d? z? is simply set to 1/T (T is 
number of topics), and ? ??, ,? ? ?
G GG
 are posterior esti-
mates provided by applying TNG to training data.  
Now, the remaining question is how to work 
out the likelihood function in the definition of 
perplexity. The likelihood function can be ob-
tained by marginalizing latent variables, but the 
time complexity is exponential. Therefore, we 
propose an efficient method of computing the 
likelihood that is similar to the forward algo-
rithm for an HMM. Details of the forward ap-
proach to computing likelihood for TWC and 
TNG are provided in Appendix B.  
In our experiments, we used a subset of Chi-
nese corpus LDC2005T14. The dataset contains 
6000 documents with 4476 unique characters and 
2,454,616 characters. We evaluated both TWC 
and TNG using 10-fold cross validation. In each 
experiment, both models ran for 500 iterations on 
90% of the data and computed the complexity for 
the remaining 10% of the data. 
TWC used fixed Dirichlet (Beta) priors ?=1, 
?=1, ?=1, ?=0.1 and ?=0.01 while TNG used 
?=1, ?=0.01, ?=0.01, and ?=0.1. 
0
50
100
150
200
250
0 20 40 60 80 100
          No. of topics (TNG)
No. of charcter topics *  no. of word topics
Pe
rp
le
xi
ty
TNG TWC
Figure 5: Perplexity results with LDC2005T14 
corpora for TNG and TWC. 
The results of these computations are shown in 
Figure 5. Note that the abscissa for TWC is the 
number of word topics (Z) multiplied by the 
(TWC)
349
number of character topics (T), while the ab-
scissa for TNG is the number of topics (T). They 
both represent the number of partitions into 
which the model classifies characters. 
Chance performance results in a perplexity 
equal to the number of unique characters, which 
was 4476 in our experiments. Therefore, both 
TWC and TNG are competitive models. And the 
lower curve shows that TWC is much better than 
TNG. 
We also found that both perplexity curves in-
creased with the number of partitions. In other 
words, both models suffer from overfitting issues. 
This is because the documents in a test set are 
very likely to contain words that do not appear in 
any of the documents in the training set. Such 
words will have a very low probability, which is 
inversely proportional to the number of partitions. 
Therefore, the perplexity of TWC increased from 
7.3513 (Z*T=2*2) to 8.9953 (Z*T=10*10), while 
that of TNG increased from 20.3789 (T=5) to 
193.6065 (T=100).  
5.2 Chinese word and topic discovery 
As shown in the previous subsection, TWC is 
a competitive method for topically modeling 
Chinese documents. Next, we show its ability to 
extract Chinese words and topics in comparison 
with TNG.  
In our qualitative experiments, the task of 
Chinese word and topic discovery was addressed 
as a supervised learning problem, where a set of 
words with their topical assignments was given 
as a seed set. Each seed can be viewed as some 
constraints imposed on the TWC and TNG mod-
els. For example, suppose that ???? (teacher) 
together with its assignment ?education? is a 
seed. This assumption implies that the indicator 
between characters ??? and ??? is 1 and that 
the (word) topic for each character is ?education?.  
We make use of such constraints in a simple 
but effective way. In each sampling iteration, we 
first sample all variables as usual and then reset 
observed variables according to the constraints. 
We used 8000 Chinese documents in the Chi-
nese Gigaword corpus (LDC2005T14) provided 
by the Linguistic Data Consortium for our ex-
periments. The dataset contains 4651 unique 
characters and 3,295,810 characters.  
The number of word topics in TWC, the num-
ber of character topics in TWC, and the number 
of topics in TNG were all set to 15. Furthermore, 
16 seeds scattered in 4 distinct topics were given, 
as listed in Table 1 column ?seed?. Dirichlet 
(Beta)   priors  were  set  to  the  same  values   as 
described in the previous subsection.  
Word and topic assignments were extracted af-
ter running 300 Gibbs sampling iterations on the 
training corpus together with the seed set. For the 
TNG model, we took the first character?s topic as 
the topic of the word. We omitted one-character 
words and ranked the extracted words using the 
following formula 
( )
( )
i
15
i
i 1
occ W
occ W
=
?
,  
where occi(W) represents how many words were 
assigned to (word) topic i. The top-50 extracted 
words are presented in Table 1.  
We find found that both TWC and TNG could 
assemble many common words used in corre-
sponding topics. And the TWC model had ad-
vantages over the TNG model in the following 
three respects. 
First, TNG drew more words related to the 
seeds. In Table 1, highly related words are 
marked in pink (underline) and partly related 
words are marked in blue (italic). It is clear that 
the TWC column is more colorful than the TNG 
column. 
Secondly, we found that many words extracted 
by TNG had the same prefix. For example, con-
sider the topic ?agriculture?: there are 14 words 
marked with superscript 1 in Table 1. They all 
have the prefix ???. This is because we took the 
first character?s topic as the topic of the word. 
Although this strategy is beneficial in some cases, 
such as for words with prefix ???, it is detri-
mental in other cases. For example, ??? ? 
(sugar cane) and ???? (Gansu) have the same 
prefix and topic assignment, but the latter is a 
name of a province in China and is not related to 
agriculture. Similarly, even though the character 
string ???? does not form a Chinese word, this 
string ???? and ???? (Iran) are classified in 
the same cluster. Compared with TNG, TWC can 
also extract words whose topics are identical to 
the topic of any character. For example, the top-
ics ????? (freestyle swimming), ????? 
(medley swimming), and ??? ? (butterfly 
stroke) depend on their suffixes.  
Thirdly, although TNG stands for ?topical n-
gram model?, it infrequently draws words con-
taining more than two characters. On the other 
hand, the TWC model extracts many n-character 
words, such as ???????? (president of 
United States, George Bush), ??????? 
(individual medley), and  ?????? (four  per- 
350
Seeds TNG TWC 
??(football) 
??(player) 
??(match) 
??
(championship) 
??,??,??,??,??,??,??,??,?
?,??,??,??,??,??,??,??,??,
??,??,??,??,??,??,??,??,?
?,??,??,??,??,??,??,??,??,
??,??,??,??,??,??,??,??,?
?,??,??,??,??,??,??,?? 
??,?? 2,??,???,???,??,??,??,?
?,??,??,??,??,??,??,???,??,?
?,??,???,??,??,??,?? 2,??,???,
??,???,??? 2,?????,??,??,??,?
?,??,??,??,??,??,???,???,??,?
?,??,??,??,??,??,???,?? 
?? (foodstuff) 
?? (country) 
?? (farmer) 
?? (paddies) 
??,?? 1,??,?? 1,??,?? 1,?? 1,??
1,??,?? 1,?? 1,??,?? 1,?? 1,??,?
?,?? 1,??,?? 1,??,?? 1,?? 1,??,?
? 1,??,??,??,??,??,??,??,??,
??,??,??,??,??,??,??,??,?
?,??,??,??,??,??,??,??,??,
?? 
??,??,??,??,??,??,??,??,??,?
?,??,???,??,??,??,??,??,??,??
?,??,?? 2,??,??? 2,??,????,?? 2,
??,??,??,??,??? 2,??,??,??,??,
??,??,??? 2,??,??,??,?? 2,?? 2,?
?,????,??,??,??,??,?? 
?? (school) 
?? (teacher) 
?? (student) 
?? (education) 
??,??,??,??,??,??,??,??,?
?,??,??,??,???,??,??,??,?
?,??,??,??,??,??,??,??,??,
??,??,??,??,??,??,??,??,?
?,??,??,??,??,??,??,??,??,
??,??,??,??,??,??,??,?? 
??,??,??,??,??,??,??,??,??,?
?,??,??,??,??,??,??,??,??,??,
??,????,??,???,??,??,??,??,?
?,??,??,???,??,??,??,??,??,?
?,??,??,??,??,??,??,??,??,??,
??,??,??,?? 
?? (war) 
?? (solider) 
?? (general) 
?? (weapon) 
??,??,??,??,??,??,??,??,?
?,??,??,??,??,??,??,??,??,
??,??,??,??,??,??,??,??,?
?,??,??,??,??,??,??,??,??,
??,??,??,??,??,??,??,??,?
?,??,??,???,??,??,??,??? 
??,??,??,??,???,??,??,??,??,?
?????,??,????,????? 2,??? 2,?
??,????,??,??,????,??,????,?
?,??,???,??,??,??,??,????,??,
???,??? 2,??,??,??,??,??,??,?
?,??,??,??,??,??,??,??,??,??,
??,?? 
Table 1: Top-50 words extracted by TWC and TNG.  
cent). This is partly due to our sampling strategy, 
discussed in subsection 4.1, which increases the 
probability of forming long words. 
We also found that some extracted character 
strings were very close to real Chinese words. 
For instance, ???? is a substring of ????? 
(tournament); ????? is a suffix of ????
?? (Chinese player), ?????? (American 
player), and ?????? (French player); and 
????? is a substring of  ?????? (100 
thousand kilograms) and ?????? (50 thou-
sand kilograms). (Such substrings are marked 
with superscript 2 in Table 1.) We believe that 
this result occurred because the training corpus 
was not large enough and that TWC will achieve 
better performance with a large dataset. 
6 Conclusion and Future Work 
In this paper, we presented a topical word-
character (TWC) model, which models two dis-
tinct types of topics: word topic and character 
topic. The experimental results show that TWC 
is a powerful approach to modeling Chinese 
documents according to the standard evaluation 
measure of perplexity. We also demonstrated 
TWC?s ability to detect words and assign topics. 
Since TWC is a straightforward improvement 
that removes the limitations of existing topical 
collocation models, we expect that its application 
to English collocation will also result in higher 
performance.  
Appendix A.1 Gibbs Sampling Derivation 
for TWC 
Symbols used here are defined as follows.  
C is the number of unique characters, T is the num-
ber of character topics, and Z is the number of word 
topics.  
Nd denotes the number of characters in a document 
d. 
( )I ?  is an indicator function, taking the value 1 
when its argument is true, and 0 otherwise. 
qd,z,0 represents how words are assigned to topic z in 
document d; pz,t,c,k represents how many times an indi-
cator is k given the previous character c, the previous 
character topic t, and the previous word topic z; nz,t,0 
represents how many times a character topic is t given 
a word topic z and the corresponding indicator 0; 
mz,v,t,1 represents how many times a character topic is t 
given a word topic z, the previous character topic v, 
and the corresponding indicator 1; and rt,c represents 
how many times character c is assigned to character 
topic t. 
'
'
', ' ', ' ', '
, ,
' ', ' ', ' ', ' '
' ' '
', ', ' ', ' , ,
' ' ' ' '
'
( | , , , , , , , , )
( | ) ( | , , )
( | ) ( | )
(
d
d
d i 1 d i 1 d i 1
d i d i
ND D
d d i d i d i 1 d
d 1 d 1 i 1
NZ T C D
z t c d i z t c
z 1 t 1 c 1 d 1 i 1
z
P t s z x t c ? ? ? ? ?
P ? ? P z x z ? d?
P ? ? P x ? d?
P ?
? ? ?
?
?
= = =
= = = = =
=
? ? ? ?
? ?
?
? ???
??? ???
GG GG
G G G
G G G
G '
', '', ' ', ' ', '
' ' ' ' '
| ) ( | ) ( | , ,
d
d i
NZ Z T D
z t d i d i z
z 1 z 1 t 1 d 1 i 1
? P ? ? P t x ?
= = = = =
? ?? ?? ???? GG
 
351
'', ' ', ' ', '
', ', ', '
', ', ' ', ', '
,
, ,, ,
, ' ', '
' ' '
' '
' ' ' ' '
) ( | ) ( | )
?( )
( ) ( )
?( )
?( )
?( )
d
d i d i 1 d i
z t c x
z t c z t c
d i 1
z s cd i d i
NT D
z t t d i t
t 1 d 1 i 1
Z T C 2 2
px ? 1 x
2
z 1 t 1 c 1 x 1 x 1
x
C
t
? d? d? P ? ? P c ? d?
2?
? ?
?
C?
? d?
?
?
+
= = =
?
= = = = =
? ? ? ? ?
? ? ? ?
? ?
? ???
??? ? ??
G G G GG G
G
', ' ,
' '
', ',
,', ', ',
, ,
' '
' ' '
' '
' '
' ' ' ' '
' '
', ' ', '
' ' ,
( ) ( )
?( ) ?( )
( ) ( )
?( ) ?( )
( ) ( )
t c d i
t t s
z t 0
d iz t v 1
d i d i 1
T C C
r cc ? 1 c
1 c 1 c 1
Z T T Z T
nt ? 1 t
z zT T
z 1 t 1 t 1 z 1 t 1
sT T
zmv ? 1 v
z t z t
v 1 v 1 z t
? ? ? d?
T? T?
? ?
? ?
?
? ?
? ?
?
= = =
?
= = = = =
?
= =
? ? ?
? ? ? ?
? ?
? ? ??
? ? ? ????
? ?
G
,
,, , , ,
, ,, ,
, ,
,
,
, ,
,
,*,, , , ,
, , ,, , ,* ,*
,
, ,*,
d i
d id i d i d i d i
d i d i 1d i d i
d i d i 1
d i
s
d i
z s 0
d i
z 0z s c x s c
z t s 1z s c s
d i
z t 1
x 0
d? d?
x 1
n ?
x 0
n T?p ? r ?
m ?p 2? r C?
x 1
m T?
?
?
? =? ? ?? =??
+? =? ++ + ?? ? ? ? ++ + ? =? +?
G G
  
Similarly, 
,
, , ,
, , ,
, ,
,
, , ,
, ,
, ,
, ,
, , ,
,*,
, , ,*
, ,
, ,
,*,
, , ,
, ,*,
( | , , , , , , , , )
( )
d i
d i 1 d i 1 d i 1
d i 1 d i 1 d i 1
d i d i
d i
d i d i 1 d i
d i d i 1
d i d i
d z 0
z t c k
d 0
z t c
d i d i 1
z t 0
z 0
z t t 1
z t
P x k z x t c ? ? ? ? ?
q ?
p ?k 0
q Z?
p 2?
I z z k 1
n ?
k 0
n T?
m ?
m
? ? ?
? ? ?
?
?
?
?
=
+? +=? +? ? ?? +? = =?
+
+?
=+
GG GG
1
k 1
T?
????? =? +?
 
, , ,
, , ,
, , ,
, , , , , ( : ) ,
, ,
, , ,
,*,
, , ,*
,
, , ,
( , | , , ,
, , , , , )
( )
d i 1 d i 1 d i 1
d i 1 d i 1 d i 1
d i u 2 d i u 2 d i u 1
d i d i 1 d i l 1 d i d i i l 1 d i
d j 0
z t c k
d 0
z t c
d i 1
j t c x
j
P z z z j x k z x t
c ? ? ? ? ?
q ?
k 0 p ?
q Z?
p 2?
I z j k
P
1
p ?
p
? ? ?
? ? ?
+ ? + ? + ?
+ + ? ? + ? ?
?
= = = = =
+? = +? +? ? ??
?
+? = =?
+
GGG"
G
, ,
, , ,
,
, ,
,
, , ,
, , ,* , ,*
, ,
,*,
, , ,
, ,*,
( )d i u 2 d i u 1
d i u 2 d i u 2 d i u 2
d i
d i 1 d i
d i 1
l 1 l
j t t 1
u 2 u 2t c j t 1
j t 0
j 0
j t t 1
j t 1
m ?
2? m T?
n ?
k 0
n T?
m ?
k 1
m T?
+ ? + ?
+ ? + ? + ?
?
?
+
= =
?
+? ?+ +
+? =? +?? +? =? +?
? ?
 
Appendix A.2 Parameter estimation for 
TWC 
After each Gibbs sampling iteration, we obtain pos-
terior estimates ? ? ??, , ,? ? ? ?
G G GG
 and r by 
, , , , ,
, , , ,
,*, , , ,*
, , , , ,
, , ,
, ,*, ,*,
,
,
,*
? ?
??
?
d z 0 z t c k
d z z t c k
d 0 z t c
z v t 1 z t 0
z v t z t
z v 1 z 0
t c
t c
t
q ? p ?
? ?
q Z? p 2?
m ? n ?
? ?
m T? n T?
r ?
?
r C?
+ += =+ +
+ += =+ +
+= +
, 
where the symbols are the same as those defined in 
Appendix A.1. These values correspond to the predic-
tive distribution over new word topics, new indicators, 
new character topics, and new characters. 
Appendix B. Likelihood Function Deriva-
tion for TWC and TNG 
To compute the likelihood function for TWC, a qua-
ternion function gi is defined as follows: (formula has 
a broken character) 
, , , , ,
, ,
( , , , ) ( , ,..., , , ,
? ? ? ??, | , , , , )
i d 1 d 2 d i d i d i
d i 1 d i
g r s u v P c c c z r x s
t u t v ? ? ? ? ??=====
= =
= ======

G G G GG . 
Then, it is clear that 
? ? ? ??( | , , , , ) ( , , , )
d
Z 1 T T
d N
r 1 s 0 u 1 v 1
P c ? ? ? ? ? g r s u v
= = = =
=????GG G G GG , 
where Z is the number of word topics and T is the 
number of character topics. The function gi can be 
rewritten in a recursive manner.  
,
,
,, ,
,
( , , , )
?( , , , )
? ?( , , , ) ( , , , )
? ?
?( )
d 1
d i 1
d i
1
cr v
1 d r v
Z 1 T
cs
i 1 i j u c v
j 1 k 0 l 1
vr
rd
v
r u
g r 1 u v 0
1
g r 0 u v ? ? ?
T
g r s u v g j k l u ? ?
s 0??
s 1?I r j
+
+
= = =
=
= ? ? ?
= ? ?
? =?? ?? ==??
===========
???  
Similarly we can define function hi to help compute 
the likelihood for TNG. (formula has a broken charac-
ter) 
,
,
, ,
,
, ,
,
,
? ? ??( , ) ( ,..., , , | , , , )
? ? ??( | , , , ) ( , )
( , )
? ?( , )
???( , ) ( , )
?
d
d 1
d i 1
d i d i 1
d i
i d 1 d i i i
Z 1
d N
r 1 s 0
1
cr
1 d r
c
Z 1
rs r
i 1 i j c d c
j 1 k 0 r c
h r s P c c z r x s ? ? ? ?
P c ? ? ? ? h r s
h r 1 0
h r 0 ? ?
? s 0
h r s h j k ? ?
s 1?
+
+
= =
+
= =
= =
=
=
= ?
? =?= ? ? ? ? =??
??
??
G G GG
GG G GG
 
References 
Blei, D. M., Ng, A. Y., and Jordan, M. J. 2003. Latent 
Dirichlet alocation. Journal of Machine Learning 
Research, 3:993?1022. 
Griffiths, T. L. and Steyvers, M. 2004. Finding Scien-
tific Topics. Proceedings of the National Academy 
of Sciences, 101 (suppl. 1), 5228-5235. 
Steyvers, M. and Griffiths, T. L. 2007. Probabilistic 
topic models. Latent Semantic Analysis: A Road to 
Meaning. Laurence Erlbaum. 
Griffiths, T. L., Steyvers, M., and Tenenbaum, J. B. T. 
2007. Topics in Semantic Representation Psycho-
logical Review, 114(2), 211-244.  
Wang, X., McCallum, A., and Wei, X. 2007. Topical 
N-grams: Phrase and Topic Discovery, with an 
Application to Information Retrieval. Proceedings 
of the 7th IEEE International Conference on Data 
Mining (ICDM 2007).  
352
A Semi-Supervised Approach to Build Annotated Corpus for Chinese Named 
Entity Recognition 
Xiaoshan FANG#, Jianfeng GAO*, Huanye SHENG# 
*Microsoft Research Asia, jfgao@microsoft.com 
#Shanghai Jiaotong University, China, {fang-xs, hysheng}@sjtu.edu.cn 
 
 
Abstract1 
This paper presents a semi-supervised ap-
proach to reduce human effort in building an 
annotated Chinese corpus. One of the disad-
vantages of many statistical Chinese named 
entity recognition systems is that training data 
may be in short supply, and manually building 
annotated corpus is expensive. In the proposed 
approach, we construct an 80M hand-
annotated corpus in three steps: (1) 
Automatically annotate training corpus; (2) 
Manually refine small subsets of the automati-
cally annotated corpus; (3) Combine small 
subsets and whole corpus in a bootstrapping 
process. Our approach is tested on a state-of-
the-art Chinese word segmentation system 
(Gao et al, 2003, 2004). Experiments show 
that only a small subset of hand-annotated 
corpus is sufficient to achieve a satisfying per-
formance of the named entity component in 
this system.  
1 Introduction 
The success of applying statistical methods to 
natural language processing tasks depends to a 
large degree upon the quality and amount of avail-
able training data.  
This paper presents our method of creating train-
ing data for the statistical Chinese word segmenter 
proposed in Gao et al (2003). The segmenter is 
based on improved source-channel models, which 
are trained on a large amount of annotated training 
data. Whereas the hand-annotation is a very expen-
sive task, creating the training data automatically 
remains an open research problem. Our approach 
falls somewhere between the two extremes of the 
spectrum. We try to minimize the human effort 
while keeping the quality of the annotation rea-
sonably good for model estimation.  The method to 
be presented has been discussed briefly in Gao et 
al. (2003). This paper presents an extended de-
scription with more details and experimental re-
sults. 
                                                     
1 This work was done while the author was visiting 
Microsoft Research Asia. 
The training data refer to a set of Chinese sen-
tences where word boundaries and types have been 
annotated. Our basic solution is the bootstrapping 
approach described in Gao et al (2002). It consists 
of three steps: (1) Initially, we use a greedy word 
segmenter to annotate the corpus, and obtain initial 
models based on the initial annotated corpus; (2) 
We re-annotate the corpus using the obtained mod-
els; (3) Re-train the models using the re-annotated 
corpus. Steps 2 and 3 are iterated until the per-
formance of the system converges. 
In this approach, the quality of the resulting 
models depends to a large degree upon the quality 
of the initial annotated corpus. Because there are 
many named entities that are not stored in a dic-
tionary, traditional dictionary-based forward 
maximum matching (FMM) algorithm is not suffi-
cient to create a good initial corpus. We thus 
manually annotate named entities on a small subset 
(call seed set) of the training data. Then, we obtain 
a model on the seed set (called seed model). We 
thus improve the initial model which is trained on 
the initial annotated training corpus by interpolat-
ing it with the seed model. Our experiments show 
that a relatively small seed set (e.g., 10 million 
characters, which takes approximately three weeks 
for 4 persons to annotate the NE tags) is enough to 
get a good improved model for initialization. 
The remainder of this paper is organized as fol-
lows: Section 2 summarizes the related work. Sec-
tion 3 deals with our approach to improve model 
estimation for Chinese word segmentation. The 
experiments are presented at Section 4. Finally we 
conclude in Section 5. 
2 Related work 
Traditional statistical approaches use a paramet-
ric model with maximum likelihood estimation 
(MLE), usually with smoothing methods to deal 
with data sparseness problems. These approaches 
have been introduced for the task of Chinese word 
segmentation. According to the training data used 
(word-segmented or not), the Chinese word seg-
mentation can be achieved in a supervised or unsu-
pervised manner. 
As an example of unsupervised training, Ge et al 
(1999) presents a simple zero-th order Markov 
model of the words in Chinese text. They devel-
oped an efficient algorithm to train their model on 
an unsegmented corpus. Their basic assumption is 
that Chinese words are usually 1 to 4 characters 
long. They however did not take into account a 
large amount of named entities (e.g. Chinese or-
ganization name, transliterate name and some per-
son names) most of which are longer than 4 char-
acters (e.g., ??????? Microsoft Research 
Asia, ????? California, ?????  a 
woman?s name which puts her husband?s surname 
ahead). 
An and Wong used Hidden Markov Models 
(HMM) for segmentation. Their system is solely 
trained on a corpus which has been manually anno-
tated with word boundaries and Part-of-Speech 
tags. Wu (2003) also used the training data to tune 
the segmentation parameters of their MSR-NLP 
Chinese system. He used the annotated training 
data to deal with the morphologically derived 
words. 
In this paper we present a semi-supervised train-
ing method where we use both an auto-segmented 
training corpus and a small hand-annotated subset 
of it. Comparing to unsupervised approaches, our 
approach leads to a better segmenter that can 
identify much more named entities which are not 
in the dictionary. Comparing to supervised ap-
proaches, our method requires much less human 
effort for data annotation. 
The Chinese word segmenter used in this study 
is described in Gao et al (2003). The segmenter 
provides a unified approach to word segmentation 
and named entity (NE) recognition. This unified 
approach is based on the improved source-channel 
models of Chinese sentence generation, with two 
components: a source model and a set of channel 
models. For each word class (e.g. a person name), 
there is a channel model (referred to as class model 
afterwards) that estimates the generative probabil-
ity of a character string given the word type. The 
source model is used to estimate the generative 
probability of a word sequence, in which each 
word belongs to one word class (e.g. a word in a 
lexicon or a named entity). In another word, it in-
dicates, given a context, how likely a word occurs. 
So the source model is also referred to as context 
model afterwards. This paper focuses the discus-
sion on how to create annotated corpus for context 
model estimation. 
3 A semi-supervised approach to improve 
context model estimation 
In this study the context model is a trigram 
model which estimates the probability of a word 
class. 
Ideally, given an annotated corpus, where each 
sentence is segmented into words which are tagged 
by their word types, the trigram word class prob-
abilities can be calculated using MLE, together 
with a backoff schema (Katz, 1987) to deal with 
the sparse data problem. Unfortunately, building 
such annotated training corpora is very expensive. 
Our basic solution is the bootstrapping approach 
described in Gao et al (2002). It consists of three 
steps: (1) Initially, a greedy word segmenter (i.e. 
FMM) is used to annotate the corpus, and an initial 
context model is obtained based on the initial an-
notated corpus; (2) Re-annotate the corpus using 
the obtained models; (3) Re-train the context 
model using the re-annotated corpus. Steps 2 and 3 
are iterated until the performance of the system 
converges. 
In the above approach, the quality of the context 
model depends to a large degree upon the quality 
of the initial annotated corpus, which is however 
not satisfied due to the fact that many named enti-
ties cannot be identifying using the greedy word 
segmenter which is based on the dictionary. As a 
consequence, the above approach achieves a low 
accuracy in detecting Chinese named entities. 
A straightforward solution to the above problem 
is to obtain large amount of high-quality annotated 
corpus for context model estimation. Unfortunately, 
manually creating such annotated corpus  is very 
expensive. For example, Douglas (1999) pointed 
out that at least up to about 1.2 million words of 
training data are necessary to train an HMM name 
recognizer. To guarantee a high degree of accuracy 
(e.g. 90% F-measure), it requires about 800 hours, 
or 20 person*weeks of labor to annotate and check 
the amount of data. This is almost certainly more 
time than would be required by a skilled rule writer 
to write a rule-based name recognizer achieving 
the same level of performance, assuming all the 
necessary resources, such as lexicons and name 
lists, are already available. 
Our training data contains approximately 80 
million Chinese characters from various domains 
of text. We are facing three questions in annotating 
the training data. (1) How to generate a high qual-
ity hand-annotated corpus? (2) How to best use the 
valuable hand-annotated corpus so as to achieve a 
satisfying performance? (3) What is the optimal 
size of the hand-annotated corpus, considering the 
tradeoff between the cost of human labor and the 
performance of the resulting segmenter? 
We leave the answers to the first and third ques-
tions to Section 4. In what follows, we describe our 
method of using small set of human-annotated cor-
pus to boost the quality of the annotation of the 
entire corpus. It consists of 6 steps. 
Step 1: Manually annotate named entities on a 
small subset (call seed set) of the training data. 
Step 2: Obtain a context model on the seed set 
(called seed model). 
Step 3: Re-annotate the training corpus using the 
seed model and then train an improved context 
model using the re-annotated corpus. 
Step 4: Manually annotate another small subset 
of the training data. Repeat Steps (2) and (3) until 
the entire training data have been annotated. 
Step 5: Repeat steps 1 to 4 using different seed 
sets (we used three seed sets in our experiments, as 
we shall describe in Section 4). 
Step 6: Combine all context models obtained in 
step 5 via linear interpolation: 
P(xyz) =? ?i? Pi(xyz) (1)
Here Pi(xyz) is the trigram probability of the i-th 
context model. ?s is the interpolation weights 
which vary from 0 to 1. 
4 Experiments 
In this section, we first present our experiments 
on the generation and evaluation of hand-annotated 
corpus to answer the first two questions. Then, the 
answer to the third question is given in subsection 
4.2. 
4.1 The generation and evaluation of hand-
annotated corpus 
4.1.1 The generation of hand-annotated corpus 
Four students, whose major is Chinese language, 
annotate the corpus according to a pre-defined 
MSRA?s guideline of Chinese named entities. We 
find that we have to revise the guideline when they 
were annotating the corpus. For example, Chinese 
character string ? ? ? ? ? ? (Shanghai 
Exposition)?can be tagged as either ?[L ?]???
?? or ?[L ??]????. Here ??? is the abbre-
viation of ???(Shanghai)?. ??? is the abbrevia-
tion of ???(city)?. L is the tag of location name. 
It is not clearly described in the guideline where 
the named entity?s right boundary is. 
We obtain in total three manually annotated sub-
sets (i.e. seed sets) by the following process: 
1. Annotate the training data using a greedy 
word segmenter. Highlight the NEs and their 
tags. 
2. Randomly select 10 million characters from 
the annotated training data and then ask the 
students to manually refine these 10 million 
characters. The refinement includes correcting 
the wrong NE tags and adding missing NE 
tags.  
3. Repeat the second step, and then combine the 
obtained new 10-million-character subset with 
the first one. Hence, a 20-million-character 
subset of the training data is obtained. 
4. Repeat the second step, and then combine the 
obtained new 10-million-character subset with 
the 20-million-character subset. Hence, a 30-
million-character subset of the training data is 
obtained. 
A manually annotated test set was developed as 
well. The text corpus contains approximately a half 
million Chinese characters that have been proof-
read and balanced in terms of domain, styles, and 
times. 
4.1.2 The evaluation of hand-annotated corpus 
To evaluate the quality of our annotated corpus, 
we trained a context model using the method de-
scribed in Section 3, with the first-obtained 10-
million-character seed set. We then compare the 
performance of the resulting segmenter with those 
of other state-of-the-art segmenters and the FMM 
segmenter. 
4.1.2.1 Evaluation metrics 
We conduct evaluations in terms of precision (P) 
and recall (R). 
NEsidentifiedofnumber
NEsidentifiedcorrectlyofnumberP ???
????=  (2)
NEsallofnumber
NEsidentifiedcorrectlyofnumberR ???
????=  (3)
4.1.2.2 Segmenters in Comparison 
1. The MSWS system is one of the best 
available products. It is released by Micro-
soft? (as a set of Windows APIs). MSWS first 
conducts the word-breaking using MM (aug-
mented by heuristic rules for disambiguation), 
and then conducts factoid detection and NER 
using rules. 
2. The LCWS system is one of the best re-
search systems in mainland China. It is re-
leased by Beijing Language University. The 
system works similarly to MSWS, but has a 
larger dictionary containing more PNs and 
LNs. 
3. The PBWS system is a rule-based Chinese 
parser which can also output the word seg-
mentation results. It explores high-level lin-
guistic knowledge, such as syntactic structure 
for Chinese word segmentation and NER. 
4.1.2.3 Results 
The performance of the resulting segmenter is 
compared with those of three state-of-the-art seg-
menters and FMM segmenter in Table 1. Here PN, 
LN and ON stand for person name, location name 
and organization name respectively. The first col-
umn lists the segmenters. 
As can be seen from Table 1, the resulting seg-
menter (SSSC.10m) achieves comparable results 
with those of the other three state-of-the-art word 
segmenters. From Table 1 we also find that our 
semi-supervised approach makes a 2.4%-49% im-
provement over FMM. 
 
Segmenter PN LN ON 
 R % P % R % P % R % P %
MSWS 74.4 90.7 44.2 93.5 46.9 64.2
LCWS 78.1 94.5 72.0 85.4 13.1 71.3
PBWS 78.7 78.0 73.6 76.7 21.6 81.7
FMM 65.7 84.4 82.7 76.0 56.6 38.6
SSSC.10m 73.6 86.6 80.7 89.5 84.3 56.8
Impr. (%) 12.0 2.6 2.4 17.7 49.0 47.1
Table 1: Results on different Chinese word seg-
menters 
The results show a moderate amount of hand-
annotated corpus leads our segmenter to a state-of-
the-art performance. 
4.2 The optimal size of the hand-annotated 
corpus 
Regarding the third question: what is the optimal 
size of the hand-annotated subset, considering the 
tradeoff between the cost of human labor and the 
performance of the resulting segmenter? We obtain 
a series of results using 10-30-million-character 
subsets as seed sets, and then plot three graphs 
showing the relationship between the performances 
and their corresponding human efforts of 
constructing the seed set. 
4.2.1 Baselines 
We use two baselines. 
One is the FMM method, which does not use 
annotated training data. It is used to evaluate the 
performances using 10-30-million-character sub-
sets (see Figure 1). 
The other is to use all the human effort of anno-
tating the whole training data, which takes about 
1920 person*hours human effort. It is used to cal-
culate how much labor we would save by using the 
semi-supervised approach described in section 3.  
4.2.2 Results 
The relationship between the performances and 
their corresponding human efforts of constructing 
the seed sets is shown in Figure 1. The X-axes give 
the human efforts on building 10, 20, and 30-
million-character subsets. They are 360, 720, and 
1080 person*hours. The Y-axes show the recall 
and precision results on person name, location 
name and organization name, separately.  
 
60
70
80
90
0 360 720 1080
PN
Recall (%) Precision (%)
30
45
60
75
90
0 360 720 1080
O
N
70
80
90
100
0 360 720 1080
Human effort (person*hour)
LN
 
Figure 1: The relationship between the perform-
ances and their corresponding human efforts 
We observe that both the recall and precision re-
sults first go upwards, and level off after the use of 
720 person*hours, which is the corresponding hu-
man effort of constructing 20 million characters. 
This means that 20 million characters is a satura-
tion point, because more human effort does not 
lead to any improvement in performance, and less 
human effort leads to lower performance. 
From the fact that manually annotating the 
whole training data costs 1920 person*hours, we 
indicate that by using our semi-supervised ap-
proach we save 62.5% human labor in corpus an-
notation. 
5 Conclusion 
This paper presents a semi-supervised method to 
save human effort in building annotated corpus. 
This method uses a small set of human-annotated 
corpus to boost the quality of the annotation of the 
entire corpus. We test this method on Gao?s Chi-
nese word segmentation system, which achieves a 
state-of-the-art performance on SIGHAN backoff 
data sets (Gao et al 2004). 
Several conclusions can be drawn from our ex-
periments: 
z The obtained corpus is of high quality. 
z 20-million-characters is the optimal size of 
hand-annotated subset to boost the 80-million-
character training data, considering the trade-
off between the cost of human labor and the 
performance of the resulting segmenter. 
z We save 62.5% human labor in corpus anno-
tation. 
References 
An, Q. and Wong, W. S. 1996. Automatic segmen-
tation and tagging of Hanzi text using a hybrid 
algorithm. In Proceedings of the 9th Interna-
tional Conference on Indus-trial & Engineering 
Applications of AI & Expert Systems. 
Borthwick, Andrew. 1999. A Maximum Entropy 
Approach to Named Entity Recognition. Ph.D. 
thesis, New York University. 
Douglas Appelt. 1999. Introduction to Information 
Extraction Technology. A Tutorial Prepared for 
IJCAI-99. 
Gao, Jianfeng, Joshua Goodman, Mingjing Li and 
Kai-Fu Lee. 2002. Toward a unified approach to 
statistical language modeling for Chinese. ACM 
TALIP, 1(1): 3-33. 
Gao, Jianfeng, Mu Li and Changning Huang. 2003. 
Improved source-channel models for Chinese 
word segmentation. In ACL-2003. Sapporo, Ja-
pan. 
Gao, Jianfeng, Andi Wu, Mu Li, Chang-Ning 
Huang, Hongqiao Li, Haowei Qin and Xinsong 
Xia. 2004. Adaptive Chinese Word Segmentation. 
In proceedings of ACL 2004. Barcelona, Spain. 
Ge, X., Pratt, W. and Smyth, P. 1999. Discovering 
Chinese Words from Unsegmented Text. SIGIR-
99, pages 271-272. 
Hockenmaier, J. and Brew, C. 1998. Error-driven 
learning of Chinese word segmentation. In J. 
Guo, K. T. Lua, and J. Xu, editors, 12th Pacific 
Conference on Language and Information, pp. 
218?229, Singapore. Chinese and Oriental Lan-
guages Processing Society. 
Katz, S. M. 1987. Estimation of probabilities from 
sparse data for the language model component 
of a speech recognizer. IEEE Trans. Acoustics, 
Speech Signal Process. ASSP-35, 3 (March), 
400-401. 
Palmer, David. 1997. A Trainable Rule-Based Al-
gorithm for Word Segmentation.  In Proceedings 
of the 35th Annual Meeting of the Association 
for Computational Linguistics (ACL ?97), Ma-
drid. 
Sun, Jian, Jianfeng Gao, Lei Zhang, Ming Zhou, 
and Changning Huang. 2002. Chinese named en-
tity identification using class-based language 
model. In: COLING 2002. Taipei, Taiwan. 
Wu, Andi. 2003. Chinese Word Segmentation in 
MSR-NLP. In Proceedings of the Second 
SIGHAN Workshop on Chinese Language Proc-
essing, Sapporo, Japan. 
