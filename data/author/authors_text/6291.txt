Word Sense Disambiguation by Learning from Unlabeled Data
Seong-Bae Park
y
, Byoung-Tak Zhang
y
and Yung Taek Kim
z
Articial Intelligence Lab (SCAI)
School of Computer Science and Engineering
Seoul National University
Seoul 151-742, Korea
y
fsbpark,btzhangg@scai.snu.ac.kr
z
ytkim@cse.snu.ac.kr
Abstract
Most corpus-based approaches to
natural language processing suer
from lack of training data. This
is because acquiring a large num-
ber of labeled data is expensive.
This paper describes a learning
method that exploits unlabeled data
to tackle data sparseness problem.
The method uses committee learn-
ing to predict the labels of unla-
beled data that augment the exist-
ing training data. Our experiments
on word sense disambiguation show
that predictive accuracy is signi-
cantly improved by using additional
unlabeled data.
1 Introduction
The objective of word sense disambiguation
(WSD) is to identify the correct sense of a
word in context. It is one of the most critical
tasks in most natural language applications,
including information retrieval, information
extraction, and machine translation. The
availability of large-scale corpus and various
machine learning algorithms enabled corpus-
based approach to WSD (Cho and Kim, 1995;
Hwee and Lee, 1996; Wilks and Stevenson,
1998),but a large scale sense-tagged corpus
or aligned bilingual corpus is needed for a
corpus-based approach.
However, most languages except English
do not have a large-scale sense-tagged cor-
pus. Therefore, any corpus-based approach
to WSD for such languages should consider
the following problems:
 There's no reliable and available sense-
tagged corpus.
 Most words are sense ambiguous.
 Annotating the large corpora requires
human experts, so that it is too expen-
sive.
Because it is expensive to construct sense-
tagged corpus or bilingual corpus, many re-
searchers tried to reduce the number of ex-
amples needed to learn WSD (Atsushi et al,
1998; Pedersen and Bruce, 1997). Atsushi et
al. (Atsushi et al, 1998) adopted a selec-
tive sampling method to use small number of
examples in training. They dened a train-
ing utility function to select examples with
minimum certainty, and at each training it-
eration the examples with less certainty were
saved in the example database. However, at
each iteration of training the similarity among
word property vectors must be calculated due
to their k-NN like implementation of training
utility.
While labeled examples obtained from a
sense-tagged corpus is expensive and time-
consuming, it is signicantly easier to ob-
tain the unlabeled examples. Yarowsky
(Yarowsky, 1995) presented, for the rst time,
the possibility that unlabeled examples can
be used for WSD. He used a learning algo-
rithm based on the local context under the
assumption that all instances of a word have
the same intended meaning within any xed
document and achieved good results with only
a few labeled examples and many unlabeled
ones. Nigam et al (Nigam et al, 2000) also
showed the unlabeled examples can enhance
the accuracy of text categorization.
Attribute Substance
GFUNC the grammatical function of w
PARENT the word of the node modied by w
SUBJECT whether or not PARENT of w has a subject
OBJECT whether or not PARENT of w has an object
NMODWORD the word of the noun modier of w
ADNWORD the head word of the adnominal phrase of w
ADNSUBJ whether or not the adnominal phrase of w has a subject
ADNOBJ whether or not the adnominal phrase of w has an object
Table 1: The properties used to distinguish the sense of an ambiguous Korean noun w.
In this paper, we present a new approach
to word sense disambiguation that is based
on selective sampling algorithm with commit-
tees. In this approach, the number of train-
ing examples is reduced, by determining by
weighted majority voting of multiple classi-
ers, whether a given training example should
be learned or not. The classiers of the com-
mittee are rst trained on a small set of la-
beled examples and the training set is aug-
mented by a large number of unlabeled exam-
ples. One might think that this has the pos-
sibility that the committee is misled by unla-
beled examples. But, the experimental results
conrm that the accuracy of WSD is increased
by using unlabeled examples when the mem-
bers of the committee are well trained with
labeled examples. We also theoretically show
that performance improvement is guaranteed
by a mild requirement, i.e., the base classi-
ers need to guess better than random selec-
tion. This is because the possibility misled by
unlabeled examples is reduced by integrating
outputs of multiple classiers. One advantage
of this method is that it eectively performs
WSD with only a small number of labeled ex-
amples and thus shows possibility of building
word sense disambiguators for the languages
which have no sense-tagged corpus.
The rest of this paper is organized as fol-
lows. Section 2 introduces the general proce-
dure for word sense disambiguation and the
necessity of unlabeled examples. Section 3 ex-
plains how the proposed method works using
both labeled and unlabeled examples. Section
4 presents the experimental results obtained
by using the KAIST raw corpus. Section 5
draws conclusions.
2 Word Sense Disambiguation
Let S 2 fs
1
; : : : ; s
k
g be the set of possible
senses of a word to be disambiguated. To
determine the sense of the word, we need
to consider the contextual properties. Let
x =< x
1
; : : : ; x
n
> be the vector for rep-
resenting selected contextual features. If we
have a classier f(x; ) parameterized with ,
then the sense of a word with property vec-
tor x can be determined by choosing the most
probable sense s

:
s

= argmax
s2S
f(x; ):
The parameters  are determined by training
the classier on a set of labeled examples, L =
f(x
1
; s
1
); : : : ; (x
N
; s
N
)g.
2.1 Property Sets
In general, the rst step of WSD is to extract
a set of contextual features. To select particu-
lar properties for Korean, the language of our
cencern, the following characteristics should
be considered:
 Korean is a partially free-order language.
The ordering information on the neigh-
bors of the ambiguous word, therefore,
does not give signicantly meaningful in-
formation in Korean.
 In Korean, ellipses appear very often
with a nominative case or objective case.
Therefore, it is di?cult to build a large
scale database of labeled examples with
case markers.
Considering both characteristics and re-
sults of previous work, we select eight prop-
erties for WSD of Korean nouns (Table 1).
Three of them (PARENT, NMODWORD,
ADNWORD) take morphological form as
their value, one (GFUNC) takes 11 values of
grammatical functions
1
, and others take only
true or false.
2.2 Unlabeled Data for WSD
Many researchers tried to develop automated
methods to reduce training cost in language
learning and found out that the cost can be
reduced by active learning which has control
over the training examples (Dagan and Engel-
son, 1997; Liere and Tadepalli, 1997; Zhang,
1994). Though the number of labeled exam-
ples needed is reduced by active learning, the
label of the selected examples must be given
by the human experts. Thus, active learn-
ing is still expensive and a method for auto-
matic labeling unlabeled examples is needed
to have the learner automatically gather in-
formation (Blum and Mitchell, 1998; Peder-
sen and Bruce, 1997; Yarowsky, 1995).
As the unlabeled examples can be obtained
with ease without human experts it makes
WSD robust. Yarowsky (Yarowsky, 1995)
presented the possibility of automatic label-
ing of training examples in WSD and achieved
good results with only a few labeled exam-
ples and many unlabeled examples. On the
other hand, Blum and Mitchell tried to clas-
sify Web pages, in which the description of
each example can be partitioned into distinct
views such as the words occurring on that
page and the words occurring in hyperlinks
(Blum and Mitchell, 1998). By using both
views together, they augmented a small set
of labeled examples with a lot of unlabeled
examples.
The unlabeled examples in WSD can pro-
vide information about the joint probability
1
These 11 grammatical functions are from
the parser, KEMTS (Korean-to-English Machine
Translation System) developed in Seoul National Uni-
versity, Korea.
distribution over properties but they also can
mislead the learner. However, the possibility
of being misled by the unlabeled examples is
reduced by the committee of classiers since
combining or integrating the outputs of sev-
eral classiers in general leads to improved
performance. This is why we use active learn-
ing with committees to select informative un-
labeled examples and label them.
3 Active Learning with
Committees for WSD
3.1 Active Learning Using Unlabeled
Examples
The algorithm for active learning using unla-
beled data is given in Figure 1. It takes two
sets of examples as inputs. A Set L is the one
with labeled examples and D = fx
1
; : : : ;x
T
g
is the one with unlabeled examples where x
i
is a property vector. First of all, the training
set L
(1)
j
(1  j  M) of labeled examples is
constructed for each base classier C
j
. This
is done by random resampling as in Bagging
(Breiman, 1996). Then, each base classier
C
j
is trained with the set of labeled examples
L
(1)
j
.
After the classiers are trained on labeled
examples, the training set is augmented by
the unlabeled examples. For each unlabeled
example x
t
2 D, each classier computes the
sense y
j
2 S which is the label associated with
it, where S is the set of possible sense of x
t
.
The distribution W over the base classi-
ers represents the importance weights. As
the distribution can be changed each iter-
ation, the distribution in iteration t is de-
noted by W
t
. The importance weight of clas-
sier C
j
under distribution W
t
is denoted by
W
t
(j). Initially, the base classiers have equal
weights, so that W
t
(j) = 1=M .
The sense of the unlabeled example x
t
is de-
termined by majority voting among C
j
's with
weight distribution W . Formally, the sense y
t
of x
t
is predicted by
y
t
(x
t
) = argmax
y2S
X
j:C
j
(x
t
)=y
W
t
(j):
If most classiers believe that y
t
is the correct
Given an unlabeled example set D = fx
1
; : : : ;x
T
g
and a labeled example set L
and a word sense set S 2 fs
1
; : : : ; s
k
g for x
i
,
Initialize W
1
(j) =
1
M
,
where M is the number of classiers in the
committee.
Resample L
(1)
j
from L for each classier C
j
,
where jL
(1)
j
j = jLj as done in Bagging.
Train base classier C
j
(1  j  M) from L
(1)
j
.
For t = 1; : : : ; T :
1. Each C
j
predicts the sense y
j
2 S for x
t
2 D.
Y =< y
1
; : : : ; y
M
>
2. Find the most likely sense y
t
from Y using
distribution W :
y
t
= argmax
y2S
X
j:C
j
(x
t
)=y
W
t
(j):
3. Set 
t
=
1 
t

t
, where

t
=
No. of C
j
's whose predictions are not y
t
M
:
4. If 
t
is larger than a certainty threshold ,
then update W
t
:
W
t+1
(j) =
W
t
(j)
Z
t

Text Chunking by Combining Hand-Crafted Rules and Memory-Based
Learning
Seong-Bae Park Byoung-Tak Zhang
School of Computer Science and Engineering
Seoul National University
Seoul 151-744, Korea
{sbpark,btzhang}@bi.snu.ac.kr
Abstract
This paper proposes a hybrid of hand-
crafted rules and a machine learning
method for chunking Korean. In the par-
tially free word-order languages such as
Korean and Japanese, a small number
of rules dominate the performance due
to their well-developed postpositions and
endings. Thus, the proposed method is
primarily based on the rules, and then the
residual errors are corrected by adopting a
memory-based machine learning method.
Since the memory-based learning is an
efficient method to handle exceptions in
natural language processing, it is good at
checking whether the estimates are excep-
tional cases of the rules and revising them.
An evaluation of the method yields the im-
provement in F-score over the rules or var-
ious machine learning methods alone.
1 Introduction
Text chunking has been one of the most interest-
ing problems in natural language learning commu-
nity since the first work of (Ramshaw and Marcus,
1995) using a machine learning method. The main
purpose of the machine learning methods applied to
this task is to capture the hypothesis that best deter-
mine the chunk type of a word, and such methods
have shown relatively high performance in English
(Kudo and Matsumoto, 2000; Zhang et. al, 2001).
In order to do it, various kinds of information, such
as lexical information, part-of-speech and grammat-
ical relation, of the neighboring words is used. Since
the position of a word plays an important role as a
syntactic constraint in English, the methods are suc-
cessful even with local information.
However, these methods are not appropriate for
chunking Korean and Japanese, because such lan-
guages have a characteristic of partially free word-
order. That is, there is a very weak positional con-
straint in these languages. Instead of positional con-
straints, they have overt postpositions that restrict
the syntactic relation and composition of phrases.
Thus, unless we concentrate on the postpositions,
we must enlarge the neighboring window to get
a good hypothesis. However, enlarging the win-
dow size will cause the curse of dimensionality
(Cherkassky and Mulier, 1998), which results in the
deficiency in the generalization performance.
Especially in Korean, the postpositions and the
endings provide important information for noun
phrase and verb phrase chunking respectively. With
only a few simple rules using such information,
the performance of chunking Korean is as good
as the rivaling other inference models such as ma-
chine learning algorithms and statistics-based meth-
ods (Shin, 1999). Though the rules are approxi-
mately correct for most cases drawn from the do-
main on which the rules are based, the knowledge
in the rules is not necessarily well-represented for
any given set of cases. Since chunking is usually
processed in the earlier step of natural language pro-
cessing, the errors made in this step have a fatal in-
fluence on the following steps. Therefore, the ex-
ceptions that are ignored by the rules must be com-
Training Phase
   w 1 ... w N
(PO S1 ... PO SN)
Rule Based
D eterm ination
Rule Base
For Each W ord w i
Correctly
D eterm ined?
Find Error Type
N o
Finish
Yes
E rror C ase Library
C lassification Phase
   w 1 ... w N
(PO S1 ... PO SN)
Rule Based
D eterm ination
Rule Base
For Each W ord w i
E rror C ase Library
M em ory Based
D eterm ination
 C 1 ... C N
Com bination
Figure 1: The structure of Korean chunking model. This figure describes a sentence-based learning and
classification.
pensated for by some special treatments of them for
higher performance.
To solve this problem, we have proposed a com-
bining method of the rules and the k-nearest neigh-
bor (k-NN) algorithm (Park and Zhang, 2001). The
problem in this method is that it has redundant k-
NNs because it maintains a separate k-NN for each
kind of errors made by the rules. In addition, be-
cause it applies a k-NN and the rules to each exam-
ples, it requires more computations than other infer-
ence methods.
The goal of this paper is to provide a new method
for chunking Korean by combining the hand-crafted
rules and a machine learning method. The chunk
type of a word in question is determined by the rules,
and then verified by the machine learning method.
The role of the machine learning method is to de-
termine whether the current context is an exception
of the rules. Therefore, a memory-based learning
(MBL) is used as a machine learning method that
can handle exceptions efficiently (Daelemans et. al,
1999).
The rest of the paper is organized as follows. Sec-
tion 2 explains how the proposed method works.
Section 3 describes the rule-based method for
chunking Korean and Section 4 explains chunking
by memory-based learning. Section 5 presents the
experimental results. Section 6 introduces the issues
for applying the proposed method to other problems.
Finally, Section 7 draws conclusions.
2 Chunking Korean
Figure 1 shows the structure of the chunking model
for Korean. The main idea of this model is to apply
rules to determine the chunk type of a word w
i
in a
sentence, and then to refer to a memory based clas-
sifier in order to check whether it is an exceptional
case of the rules. In the training phase, each sentence
is analyzed by the rules and the predicted chunk type
is compared with the true chunk type. In case of mis-
prediction, the error type is determined according to
the true chunk type and the predicted chunk type.
The mispredicted chunks are stored in the error case
library with their true chunk types. Since the error
case library accumulates only the exceptions of the
rules, the number of cases in the library is small if
the rules are general enough to represent the instance
space well.
The classification phase in Figure 1 is expressed
as a procedure in Figure 2. It determines the chunk
type of a word w
i
given with the context C
i
. First of
all, the rules are applied to determine the chunk type.
Then, it is checked whether C
i
is an exceptional case
of the rules. If it is, the chunk type determined by
the rules is discarded and is determined again by the
memory based reasoning. The condition to make a
decision of exceptional case is whether the similar-
ity between C
i
and the nearest instance in the error
Procedure Combine
Input : a word w
i
, a context C
i
, and the threshold t
Output : a chunk type c
[Step 1] c = Determine the chunk type of w
i
using rules.
[Step 2] e = Get the nearest instance of C
i
in error case
library.
[Step 3] If Similarity(C
i
, e) ? t,
then c = Determine chunk type of w
i
by memory-
based learning.
Figure 2: The procedure for combining the rules and
memory based learning.
case library is larger than the threshold t. Since the
library contains only the exceptional cases, the more
similar is C
i
to the nearest instance, the more prob-
able is it an exception of the rules.
3 Chunking by Rules
There are four basic phrases in Korean: noun phrase
(NP), verb phrase (VP), adverb phrase (ADVP), and
independent phrase (IP). Thus, chunking by rules is
divided into largely four components.
3.1 Noun Phrase Chunking
When the part-of-speech of w
i
is one of determiner,
noun, and pronoun, there are only seven rules to
determine the chunk type of w
i
due to the well-
developed postpositions of Korean.
1. If POS(w
i?1
) = determiner and w
i?1
does not have a
postposition Then y
i
= I-NP.
2. Else If POS(w
i?1
) = pronoun and w
i?1
does not have
a postposition Then y
i
= I-NP.
3. Else If POS(w
i?1
) = noun and w
i?1
does not have a
postposition Then y
i
= I-NP.
4. Else If POS(w
i?1
) = noun and w
i?1
has a possessive
postposition Then y
i
= I-NP.
5. Else If POS(w
i?1
) = noun andw
i?1
has a relative post-
fix Then y
i
= I-NP.
6. Else If POS(w
i?1
) = adjective and w
i?1
has a relative
ending Then y
i
= I-NP.
7. Else y
i
= B-NP.
Here, POS(w
i?1
) is the part-of-speech of w
i?1
.
B-NP represents the first word of a noun phrase,
while I-NP is given to other words in the noun
phrase.
Since determiners, nouns and pronouns play the
similar syntactic role in Korean, they form a noun
phrase when they appear in succession without post-
position (Rule 1?3). The words with postpositions
become the end of a noun phrase, but there are only
two exceptions. When the type of a postposition
is possessive, it is still in the mid of noun phrase
(Rule 4). The other exception is a relative postfix
? (jeok)? (Rule 5). Rule 6 states that a simple rela-
tive clause with no sub-constituent also constitutes a
noun phrase. Since the adjectives of Korean have no
definitive usage, this rule corresponds to the defini-
tive usage of the adjectives in English.
3.2 Verb Phrase Chunking
The verb phrase chunking has been studied for a
long time under the name of compound verb pro-
cessing in Korean and shows relatively high accu-
racy. Shin used a finite state automaton for verb
phrase chunking (Shin, 1999), while K.-C. Kim used
knowledge-based rules (Kim et. al, 1995). For the
consistency with noun phrase chunking, we use the
rules in this paper. The rules used are the ones pro-
posed by (Kim et. al, 1995) and the further explana-
tion on the rules is skipped. The number of the rules
used is 29.
3.3 Adverb Phrase Chunking
When the adverbs appear in succession, they have a
great tendency to form an adverb phrase. Though an
adverb sequence is not always one adverb phrase, it
usually forms one phrase. Table 1 shows this empiri-
cally. The usage of the successive adverbs is investi-
gated from STEP 2000 dataset1 where 270 cases are
observed. The 189 cases among them form a phrase
whereas the remaining 81 cases form two phrases in-
dependently. Thus, it can be said that the possibility
that an adverb sequence forms a phrase is far higher
than the possibility that it forms two phrases.
When the part-of-speech of w
i
is an adjective, its
chunk type is determined by the following rule.
1. If POS(w
i?1
) = adverb Then y
i
= I-ADVP.
2. Else y
i
= B-ADVP.
1This dataset will be explained in Section 5.1.
No. of Cases Probability
One Phrase 189 0.70
Two Phrases 81 0.30
Table 1: The probability that an adverb sequence
forms a chunk.
3.4 Independent Phrase Chunking
There is no special rule for independent phrase
chunking. It can be done only through knowledge
base that stores the cases where independent phrases
take place. We designed 12 rules for independent
phrases.
4 Chunking by Memory-Based Learning
Memory-based learning is a direct descent of the
k-Nearest Neighbor (k-NN) algorithm (Cover and
Hart, 1967). Since many natural language process-
ing (NLP) problems have constraints of a large num-
ber of examples and many attributes with different
relevance, memory-based learning uses more com-
plex data structure and different speedup optimiza-
tion from the k-NN.
It can be viewed with two components: a learning
component and a similarity-based performance com-
ponent. The learning component involves adding
training examples to memory, where all examples
are assumed to be fixed-length vectors of n at-
tributes. The similarity between an instance x and
all examples y in memory is computed using a dis-
tance metric, ?(x,y). The chunk type of x is then
determined by assigning the most frequent category
within the k most similar examples of x.
The distance from x and y, ?(x,y) is defined to
be
?(x,y) ?
n
?
i=1
?
i
?(x
i
, y
i
),
where ?
i
is the weight of i-th attribute and
?(x
i
, y
i
) =
{
0 if x
i
= y
i
,
1 if x
i
= y
i
.
When ?
i
is determined by information gain (Quin-
lan, 1993), the k-NN algorithm with this metric is
called IB1-IG (Daelemans et. al, 2001). All the ex-
periments performed by memory-based learning in
this paper are done with IB1-IG.
Table 2 shows the attributes of IB1-IG for chunk-
ing Korean. To determine the chunk type of a word
w
i
, the lexicons, POS tags, and chunk types of
surrounding words are used. For the surrounding
words, three words of left context and three words
of right context are used for lexicons and POS tags,
while two words of left context are used for chunk
types. Since chunking is performed sequentially, the
chunk types of the words in right context are not
known in determining the chunk type of w
i
.
5 Experiments
5.1 Dataset
For the evaluation of the proposed method, all exper-
iments are performed on STEP 2000 Korean Chunk-
ing dataset (STEP 2000 dataset)2. This dataset is
derived from the parsed corpus, which is a product
of STEP 2000 project supported by Korean govern-
ment. The corpus consists of 12,092 sentences with
111,658 phrases and 321,328 words, and the vocab-
ulary size is 16,808. Table 3 summarizes the infor-
mation on the dataset.
The format of the dataset follows that of CoNLL-
2000 dataset (CoNLL, 2000). Figure 3 shows an ex-
ample sentence in the dataset3. Each word in the
dataset has two additional tags, which are a part-of-
speech tag and a chunk tag. The part-of-speech tags
are based on KAIST tagset (Yoon and Choi, 1999).
Each phrase can have two kinds of chunk types: B-
XP and I-XP. In addition to them, there is O chunk
type that is used for words which are not part of any
chunk. Since there are four types of phrases and
one additional chunk type O, there exist nine chunk
types.
5.2 Performance of Chunking by Rules
Table 4 shows the chunking performance when only
the rules are applied. Using only the rules gives
97.99% of accuracy and 91.87 of F-score. In spite
of relatively high accuracy, F-score is somewhat low.
Because the important unit of the work in the appli-
cations of text chunking is a phrase, F-score is far
more important than accuracy. Thus, we have much
room to improve in F-score.
2The STEP 2000 Korean Chunking dataset is available in
http://bi.snu.ac.kr/?sbpark/Step2000.
3The last column of this figure, the English annotation, does
Attribute Explanation Attribute Explanation
W
i?3
word of w
i?3
POS
i?3
POS of w
i?3
W
i?2
word of w
i?2
POS
i?2
POS of w
i?2
W
i?1
word of w
i?1
POS
i?1
POS of w
i?1
W
i
word of w
i
POS
i
POS of w
i
W
i+1
word of w
i+1
POS
i+1
POS of w
i+1
W
i+2
word of w
i+2
POS
i+2
POS of w
i+2
W
i+3
word of w
i+3
POS
i+3
POS of w
i+3
C
i?3
chunk of w
i?3
C
i?2
chunk of w
i?2
C
i?1
chunk of w
i?1
Table 2: The attributes of IB1-IG for chunking Korean.
Information Value
Vocabulary Size 16,838
Number of total words 321,328
Number of chunk types 9
Number of POS tags 52
Number of sentences 12,092
Number of phrases 112,658
Table 3: The simple statistics on STEP 2000 Korean
Chunking dataset.
 nq B-NP Korea
? jcm I-NP Postposition : POSS
 nq I-NP Sejong
 ncn I-NP base
 jcj I-NP and
 mmd I-NP the
	
 ncn I-NP surrounding
 ncn I-NP base
	 jxt I-NP Postposition: TOPIC

 ncn B-NP western South Pole
 ncn B-NP south
	
 nq I-NP Shetland
? jcm I-NP Postposition : POSS
	
 nq I-NP King George Island
 jca I-NP Postposition : LOCA
 paa B-VP is located
 ef I-VP Ending : DECL
. sf O
Figure 3: An example of STEP 2000 dataset.
Type Precision Recall F-score
ADVP 98.67% 97.23% 97.94
IP 100.00% 99.63% 99.81
NP 88.96% 88.93% 88.94
VP 92.89% 96.35% 94.59
All 91.28% 92.47% 91.87
Table 4: The experimental results when the rules are
only used.
Error Type No. of Errors Ratio (%)
B-ADVP I-ADVP 89 1.38
B-ADVP I-NP 9 0.14
B-IP B-NP 9 0.14
I-IP I-NP 2 0.03
B-NP I-NP 2,376 36.76
I-NP B-NP 2,376 36.76
B-VP I-VP 3 0.05
I-VP B-VP 1,599 24.74
All 6,463 100.00
Table 5: The error distribution according to the mis-
labeled chunk type.
Table 5 shows the error types by the rules and
their distribution. For example, the error type ?B-
ADVP I-ADVP? contains the errors whose true la-
bel is B-ADVP and that are mislabeled by I-ADVP.
There are eight error types, but most errors are re-
lated with noun phrases. We found two reasons for
this:
1. It is difficult to find the beginning of noun
phrases. All nouns appearing successively
without postpositions are not a single noun
phrase. But, they are always predicted to be
single noun phrase by the rules, though they
can be more than one noun phrase.
2. The postposition representing a noun coordi-
nation, ? (wa)? is very ambiguous. When
? (wa)? is representing the coordination, the
chunk types of it and its next word should be
?I-NP I-NP?. But, when it is just an adverbial
postposition that implies ?with? in English, the
chunk types should be ?I-NP B-NP?.
Decision Tree SVM MBL
Accuracy 97.95?0.24% 98.15?0.20% 97.79?0.29%
Precision 92.29?0.94% 93.63?0.81% 91.41?1.24%
Recall 90.45?0.80% 91.48?0.70% 91.43?0.87%
F-score 91.36?0.85 92.54?0.72 91.38?1.01
Table 6: The experimental results of various ma-
chine learning algorithms.
5.3 Performance of Machine Learning
Algorithms
Table 6 gives the 10-fold cross validation result of
three machine learning algorithms. In each fold, the
corpus is divided into three parts: training (80%),
held-out (10%), test (10%). Since held-out set is
used only to find the best value for the threshold t
in the combined model, it is not used in measuring
the performance of machine learning algorithms.
The machine learning algorithms tested are (i)
memory-based learning (MBL), (ii) decision tree,
and (iii) support vector machines (SVM). We use
C4.5 release 8 (Quinlan, 1993) for decision tree in-
duction and SV Mlight (Joachims, 1998) for support
vector machines, while TiMBL (Daelemans et. al,
2001) is adopted for memory-based learning. De-
cision trees and SVMs use the same attributes with
memory-based learning (see Table 2). Two of the al-
gorithms, memory-based learning and decision tree,
show worse performance than the rules. The F-
scores of memory-based learning and decision tree
are 91.38 and 91.36 respectively, while that of the
rules is 91.87 (see Table 4). On the other hand, sup-
port vector machines present a slightly better perfor-
mance than the rules. The F-score of support vector
machine is 92.54, so the improvement over the rules
is just 0.67.
Table 7 shows the weight of attributes when
only memory-based learning is used. Each value
in this table corresponds to ?
i
in calculating
?(x,y). The more important is an attribute, the
larger is the weight of it. Thus, the most im-
portant attribute among 17 attributes is C
i?1
, the
chunk type of the previous word. On the other
hand, the least important attributes are W
i?3
and
C
i?3
. Because the words make less influence
on determining the chunk type of w
i
in ques-
tion as they become more distant from w
i
. That
not exist in the dataset. It is given for the explanation.
Attribute Weight Attribute Weight
W
i?3
0.03 POS
i?3
0.04
W
i?2
0.07 POS
i?2
0.11
W
i?1
0.17 POS
i?1
0.28
W
i
0.22 POS
i
0.38
W
i+1
0.14 POS
i+1
0.22
W
i+2
0.06 POS
i+2
0.09
W
i+3
0.04 POS
i+3
0.05
C
i?3
0.03 C
i?2
0.11
C
i?1
0.43
Table 7: The weights of the attributes in IB1-IG. The
total sum of the weights is 2.48.
fold Precision (%) Recall (%) F-score t
1 94.87 94.12 94.49 1.96
2 93.52 93.85 93.68 1.98
3 95.25 94.72 94.98 1.95
4 95.30 94.32 94.81 1.95
5 92.91 93.54 93.22 1.87
6 94.49 94.50 94.50 1.92
7 95.88 94.35 95.11 1.94
8 94.25 94.18 94.21 1.94
9 92.96 91.97 92.46 1.91
10 95.24 94.02 94.63 1.97
Avg. 94.47?1.04 93.96?0.77 94.21?0.84 1.94
Table 8: The final result of the proposed method by
combining the rules and the memory-based learning.
The average accuracy is 98.21?0.43.
is, the order of important lexical attributes is
?W
i
,W
i?1
,W
i+1
,W
i?2
,W
i+2
,W
i+3
,W
i?3
?. The
same phenomenon is found in part-of-speech
(POS) and chunk type (C). In comparing the part-
of-speech information with the lexical information,
we find out that the part-of-speech is more impor-
tant. One possible explanation for this is that the
lexical information is too sparse.
The best performance on English reported is
94.13 in F-score (Zhang et. al, 2001). The reason
why the performance on Korean is lower than that
on English is the curse of dimensionality. That is,
the wider context is required to compensate for the
free order of Korean, but it hurts the performance
(Cherkassky and Mulier, 1998).
5.4 Performance of the Hybrid Method
Table 8 shows the final result of the proposed
method. The F-score is 94.21 on the average which
is improvement of 2.34 over the rules only, 1.67 over
support vector machines, and 2.83 over memory-
based learning. In addition, this result is as high as
the performance on English (Zhang et. al, 2001).
80
82
84
86
88
90
92
94
96
98
100
ADVP IP NP VP
Phrases
F
-
s
c
o
re
Rule Only
Hybrid
Figure 4: The improvement for each kind of phrases
by combining the rules and MBL.
The threshold t is set to the value which produces
the best performance on the held-out set. The total
sum of all weights in Table 7 is 2.48. This implies
that when we set t > 2.48, only the rules are ap-
plied since there is no exception with this threshold.
When t = 0.00, only the memory-based learning is
used. Since the memory-based learning determines
the chunk type of w
i
based on the exceptional cases
of the rules in this case. the performance is poor with
t = 0.00. The best performance is obtained when t
is near 1.94.
Figure 4 shows how much F-score is improved for
each kind of phrases. The average F-score of noun
phrase is 94.54 which is far improved over that of the
rules only. This implies that the exceptional cases of
the rules for noun phrase are well handled by the
memory-based learning. The performance is much
improved for noun phrase and verb phrase, while it
remains same for adverb phrases and independent
phrases. This result can be attributed to the fact that
there are too small number of exceptions for adverb
phrases and independent phrases. Because the ac-
curacy of the rules for these phrases is already high
enough, most cases are covered by the rules. Mem-
ory based learning treats only the exceptions of the
rules, so the improvement by the proposed method
is low for the phrases.
6 Discussion
In order to make the proposed method practical and
applicable to other NLP problems, the following is-
sues are to be discussed:
1. Why are the rules applied before the
memory-based learning?
When the rules are efficient and accurate
enough to begin with, it is reasonable to ap-
ply the rules first (Golding and Rosenbloom,
1996). But, if they were deficient in some
way, we should have applied the memory-based
learning first.
2. Why don?t we use all data for the machine
learning method?
In the proposed method, memory-based learn-
ing is used not to find a hypothesis for inter-
preting whole data space but to handle the ex-
ceptions of the rules. If we use all data for both
the rules and memory-based learning, we have
to weight the methods to combine them. But, it
is difficult to know the weights of the methods.
3. Why don?t we convert the memory-based
learning to the rules?
Converting between the rules and the cases in
the memory-based learning tends to yield inef-
ficient or unreliable representation of rules.
The proposed method can be directly applied to
the problems other than chunking Korean if the
proper rules are prepared. The proposed method will
show better performance than the rules or machine
learning methods alone.
7 Conclusion
In this paper we have proposed a new method
to learn chunking Korean by combining the hand-
crafted rules and a memory-based learning. Our
method is based on the rules, and the estimates on
chunks by the rules are verified by a memory-based
learning. Since the memory-based learning is an
efficient method to handle exceptional cases of the
rules, it supports the rules by making decisions only
for the exceptions of the rules. That is, the memory-
based learning enhances the rules by efficiently han-
dling the exceptional cases of the rules.
The experiments on STEP 2000 dataset showed
that the proposed method improves the F-score of
the rules by 2.34 and of the memory-based learn-
ing by 2.83. Even compared with support vector
machines, the best machine learning algorithm in
text chunking, it achieved the improvement of 1.67.
The improvement was made mainly in noun phrases
among four kinds of phrases in Korean. This is
because the errors of the rules are mostly related
with noun phrases. With relatively many instances
for noun phrases, the memory-based learning could
compensate for the errors of the rules. We also em-
pirically found the threshold value t used to deter-
mine when to apply the rules and when to apply
memory-based learning.
We also discussed some issues in combining a
rule-based method and a memory-based learning.
These issues will help to understand how the method
works and to apply the proposed method to other
problems in natural language processing. Since the
method is general enough, it can be applied to other
problems such as POS tagging and PP attachment.
The memory-based learning showed good perfor-
mance in these problems, but did not reach the state-
of-the-art. We expect that the performance will be
improved by the proposed method.
Acknowledgement
This research was supported by the Korean Ministry
of Education under the BK21-IT program and by the
Korean Ministry of Science and Technology under
NRL and BrainTech programs.
References
V. Cherkassky and F. Mulier. 1998. Learning from Data:
Concepts, Theory, and Methods, John Wiley & Sons,
Inc.
CoNLL. 2000. Shared Task for Computational
Natural Language Learning (CoNLL), http://lcg-
www.uia.ac.be/conll2000/chunking.
T. Cover and P. Hart. 1967. Nearest Neighbor Pat-
tern Classification, IEEE Transactions on Information
Theory, Vol. 13, pp. 21?27.
W. Daelemans, A. Bosch and J. Zavrel. 1999. Forgetting
Exceptions is Harmful in Language Learning, Ma-
chine Learning, Vol. 34, No. 1, pp. 11?41.
W. Daelemans, J. Zavrel, K. Sloot and A. Bosch. 2001.
TiMBL: Tilburg Memory Based Learner, version 4.1,
Reference Guide, ILK 01-04, Tilburg University.
A. Golding and P. Rosenbloom. 1996. Improving Accu-
racy by Combining Rule-based and Case-based Rea-
soning, Artificial Intelligence, Vol. 87, pp. 215?254.
T. Joachims. 1998. Making Large-Scale SVM Learning
Practical, LS8, Universitaet Dortmund.
K.-C. Kim, K.-O. Lee, and Y.-S. Lee. 1995. Korean
Compound Verbals Processing driven by Morpholog-
ical Analysis, Journal of KISS, Vol. 22, No. 9, pp.
1384?1393.
Taku Kudo and Yuji Matsumoto. 2000. Use of Support
Vector Learning for Chunk Identification, In Proceed-
ings of the Fourth Conference on Computational Nat-
ural Language Learning, pp. 142?144.
S.-B. Park and B.-T. Zhang. 2001. Combining a Rule-
based Method and a k-NN for Chunking Korean Text,
In Proceedings of the 19th International Conference
on Computer Processing of Oriental Languages, pp.
225?230.
R. Quinlan. 1993. C4.5: Programs for Machine Learn-
ing, Morgan Kaufmann Publishers.
L. Ramshaw and M. Marcus. 1995. Text Chunking Us-
ing Transformation-Based Learning, In Proceedings
of the Third ACL Workshop on Very Large Corpora,
pp. 82?94.
H.-P. Shin. 1999. Maximally Efficient Syntatic Parsing
with Minimal Resources, In Proceedings of the Con-
ference on Hangul and Korean Language Infomration
Processing, pp. 242?244.
J.-T. Yoon and K.-S. Choi. 1999. Study on KAIST Cor-
pus, CS-TR-99-139, KAIST CS.
T. Zhang, F. Damerau and D. Johnson. 2001. Text
Chunking Using Regularized Winnow, In Proceed-
ings of the 39th Annual Meeting of the Association for
Computational Linguistics, pp. 539?546.
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 633?640,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Self-Organizing  -gram Model for Automatic Word Spacing
Seong-Bae Park Yoon-Shik Tae Se-Young Park
Department of Computer Engineering
Kyungpook National University
Daegu 702-701, Korea
 sbpark,ystae,sypark@sejong.knu.ac.kr
Abstract
An automatic word spacing is one of the
important tasks in Korean language pro-
cessing and information retrieval. Since
there are a number of confusing cases in
word spacing of Korean, there are some
mistakes in many texts including news ar-
ticles. This paper presents a high-accurate
method for automatic word spacing based
on self-organizing  -gram model. This
method is basically a variant of  -gram
model, but achieves high accuracy by au-
tomatically adapting context size.
In order to find the optimal context size,
the proposed method automatically in-
creases the context size when the contex-
tual distribution after increasing it dose
not agree with that of the current context.
It also decreases the context size when
the distribution of reduced context is sim-
ilar to that of the current context. This
approach achieves high accuracy by con-
sidering higher dimensional data in case
of necessity, and the increased compu-
tational cost are compensated by the re-
duced context size. The experimental re-
sults show that the self-organizing struc-
ture of  -gram model enhances the basic
model.
1 Introduction
Even though Korean widely uses Chinese charac-
ters, the ideograms, it has a word spacing model
unlike Chinese and Japanese. The word spacing of
Korean, however, is not a simple task, though the
basic rule for it is simple. The basic rule asserts
that all content words should be spaced. However,
there are a number of exceptions due to various
postpositions and endings. For instance, it is diffi-
cult to distinguish some postpositions from incom-
plete nouns. Such exceptions induce many mis-
takes of word spacing even in news articles.
The problem of the inaccurate word spacing is
that they are fatal in language processing and in-
formation retrieval. The incorrect word spacing
would result in the incorrect morphological analy-
sis. For instance, let us consider a famous Korean
sentence: ?  	 
.? The true
word spacing for this sentence is ? # 
	# 
.? whose meaning is that my fa-
ther entered the room. If the sentence is written
as ?
# 	# 
.?, it means that
my father entered the bag, which is totally dif-
ferent from the original meaning. That is, since
the morphological analysis is the first-step in most
NLP applications, the sentences with incorrect
word spacing must be corrected for their further
processing. In addition, the wrong word spacing
would result in the incorrect index for terms in in-
formation retrieval. Thus, correcting the sentences
with incorrect word spacing is a critical task in Ko-
rean information processing.
One of the most simple and strong models for
automatic word spacing is  -gram model. In spite
of the advantages of the  -gram model, its prob-
lem should be also considered for achieving high
performance. The main problem of the model is
that it is usually modeled with fixed window size,
 . The small value for   represents the narrow
context in modeling, which results in poor per-
formance in general. However, it is also difficult
to increase   for better performance due to data
sparseness. Since the corpus size is physically lim-
ited, it is highly possible that many  -grams which
do not appear in the corpus exist in the real world.
633
The goal of this paper is to provide a new
method for processing automatic word spacing
with an  -gram model. The proposed method au-
tomatically adapts the window size  . That is, this
method begins with a bigram model, and it shrinks
to an unigram model when data sparseness occurs.
It also grows up to a trigram, fourgram, and so
on when it requires more specific information in
determining word spacing. In a word, the pro-
posed model organizes the windows size   online,
and achieves high accuracy by removing both data
sparseness and information lack.
The rest of the paper is organized as follows.
Section 2 surveys the previous work on automatic
word spacing and the smoothing methods for  -
gram models. Section 3 describes the general way
to automatic word spacing by an  -gram model,
and Section 4 proposes a self-organizing  -gram
model to overcome some drawbacks of  -gram
models. Section 5 presents the experimental re-
sults. Finally, Section 6 draws conclusions.
2 Previous Work
Many previous work has explored the possibility
of automatic word spacing. While most of them
reported high accuracy, they can be categorized
into two parts in methodology: analytic approach
and statistical approach. The analytic approach
is based on the results of morphological analysis.
Kang used the fundamental morphological analy-
sis techniques (Kang, 2000), and Kim et al distin-
guished each word by the morphemic information
of postpositions and endings (Kim et al, 1998).
The main drawbacks of this approach are that (i)
the analytic step is very complex, and (ii) it is
expensive to construct and maintain the analytic
knowledge.
In the other hand, the statistical approach ex-
tracts from corpora the probability that a space is
put between two syllables. Since this approach can
obtain the necessary information automatically, it
does require neither the linguistic knowledge on
syllable composition nor the costs for knowledge
construction and maintenance. In addition, the
fact that it does not use a morphological analyzer
produces solid results even for unknown words.
Many previous studies using corpora are based on
bigram information. According to (Kang, 2004),
the number of syllables used in modern Korean is
about   , which implies that the number of bi-
grams reaches  . In order to obtain stable statis-
tics for all bigrams, a great large volume of cor-
pora will be required. If higher order  -gram is
adopted for better accuracy, the volume of corpora
required will be increased exponentially.
The main drawback of  -gram model is that
it suffers from data sparseness however large the
corpus is. That is, there are many  -grams of
which frequency is zero. To avoid this problem,
many smoothing techniques have been proposed
for construction of  -gram models (Chen and
Goodman, 1996). Most of them belongs to one
of two categories. One is to pretend each  -gram
occurs once more than it actually did (Mitchell,
1996). The other is to interpolate  -grams with
lower dimensional data (Jelinek and Mercer, 1980;
Katz, 1987). However, these methods artificially
modify the original distribution of corpus. Thus,
the final probabilities used in learning with  -
grams are the ones distorted by a smoothing tech-
nique.
A maximum entropy model can be considered
as another way to avoid zero probability in  -gram
models (Rosenfeld, 1996). Instead of construct-
ing separate models and then interpolate them, it
builds a single, combined model to capture all
the information provided by various knowledge
sources. Even though a maximum entropy ap-
proach is simple, general, and strong, it is com-
putationally very expensive. In addition, its per-
formance is mainly dependent on the relevance
of knowledge sources, since the prior knowledge
on the target problem is very important (Park and
Zhang, 2002). Thus, when prior knowledge is not
clear and computational cost is an important fac-
tor,  -gram models are more suitable than a maxi-
mum entropy model.
Adapting features or contexts has been an im-
portant issue in language modeling (Siu and Os-
tendorf, 2000). In order to incorporate long-
distance features into a language model, (Rosen-
feld, 1996) adopted triggers, and (Mochihashi and
Mastumoto, 2006) used a particle filter. However,
these methods are restricted to a specific language
model. Instead of long-distance features, some
other researchers tried local context extension. For
this purpose, (Schu?tze and Singer, 1994) adopted
a variable memory Markov model proposed by
(Ron et al, 1996), (Kim et al, 2003) applied se-
lective extension of features to POS tagging, and
(Dickinson and Meurers, 2005) expanded context
of  -gram models to find errors in syntactic anno-
634
tation. In these methods, only neighbor words or
features of the target  -grams became candidates
to be added into the context. Since they required
more information for better performance or detect-
ing errors, only the context extension was consid-
ered.
3 Automatic Word Spacing by  -gram
Model
The problem of automatic word spacing can be re-
garded as a binary classification task. Let a sen-
tence be given as   



   
 
. If i.i.d. sam-
pling is assumed, the data from this sentence are
given as    

 

      
 
 
 
 	 where
 

   
 and 

  
 . In this rep-
resentation,  

is a contextual representation of a
syllable 

. If a space should be put after 

, then


, the class of  

, is true. It is false otherwise.
Therefore, the automatic word spacing is to esti-
mate a function      
 . That
is, our task is to determine whether a space should
be put after a syllable 

expressed as  

with its
context.
The probabilistic method is one of the strong
and most widely used methods for estimating  .
That is, for each 

,
 

  	


 
 	

 

 


where  

 

 is rewritten as
 

 

 
  



 


  



Since   

 is independent of finding the class of
 

,  

 is determined by multiplying   




and  

. That is,
 

  	


 
 	

  



 


In  -gram model,  

is expressed with   neigh-
bor syllables around 

. Typically,   is taken
to be two or three, corresponding to a bigram or
trigram respectively.  

corresponds to 



when    . In the same way, it is 





when    . The simple and easy way to esti-
mate   



 is to use maximum likelihood esti-
mate with a large corpus. For instance, consider
the case    . Then, the probability   



 is
represented as  





, and is computed by
 





 
 






 


(1)












 0.7
 0.75
 0.8
 0.85
 0.9
 0  1e+06  2e+06  3e+06  4e+06  5e+06  6e+06  7e+06  8e+06
Ac
cu
ra
cy
 (%
)
No. of Training Examples
unigram
bigram
trigram
4-gram
5-gram
6-gram
7-gram
8-gram
9-gram
10-gram
Figure 1: The performance of  -gram models ac-
cording to the values of   in automatic word spac-
ing.
where  is a counting function.
Determining the context size, the value of  , in
 -gram models is closely related with the corpus
size. The larger is  , the larger corpus is required
to avoid data sparseness. In contrast, though low-
order  -grams do not suffer from data sparseness
severely, they do not reflect the language charac-
teristics well, either. Typically researchers have
used     or    , and achieved high perfor-
mance in many tasks (Bengio et al, 2003). Fig-
ure 1 supports that bigram and trigram outper-
form low-order (    ) and high-order (   )
 -grams in automatic word spacing. All the ex-
perimental settings for this figure follows those
in Section 5. In this figure, bigram model shows
the best accuracy and trigram achieves the second
best, whereas unigram model results in the worst
accuracy. Since the bigram model is best, a self-
organizing  -gram model explained below starts
from bigram.
4 Self-Organizing  -gram Model
To tackle the problem of fixed window size in  -
gram models, we propose a self-organizing struc-
ture for them.
4.1 Expanding  -grams
When  -grams are compared with    -grams,
their performance in many tasks is lower than that
of     -grams (Charniak, 1993). Simultane-
ously the computational cost for     -grams
is far higher than that for  -grams. Thus, it can
be justified to use     -grams instead of  -
635
Function HowLargeExpand( 

)
Input:  

:  -grams
Output: an integer for expanding size
1. Retrieve     -grams  

for  

.
2. Compute
    

  


3. If   EXP Then return 0.
4. return HowLargeExpand( 

) + 1.
Figure 2: A function that determines how large a
window size should be.
grams only when higher performance is expected.
In other words,     -grams should be different
from  -grams. Otherwise, the performance would
not be different. Since our task is attempted with
a probabilistic method, the difference can be mea-
sured with conditional distributions. If the condi-
tional distributions of  -grams and     -grams
are similar each other, there is no reason to adopt
    -grams.
Let  

 

 be a class-conditional probabil-
ity by  -grams and  

 

 that by   -
grams. Then, the difference        between
them is measured by Kullback-Leibler divergence.
That is,
         

 

 

 


which is computed by
 
 	

 

 	
 


 


 (2)
       that is larger than a predefined
threshold EXP implies that    is dif-
ferent from  

 

. In this case,   -grams
is used instead of  -grams.
Figure 2 depicts an algorithm that determines
how large  -grams should be used. It recursively
finds the optimal expanding window size. For in-
stance, let bigrams (   ) be used at first. When
the difference between bigrams and trigrams (  
) is larger than EXP, that between trigrams and
fourgrams (   ) is checked again. If it is less
than EXP, then this function returns 1 and tri-
grams are used instead of bigrams. Otherwise, it
considers higher  -grams again.
Function HowSmallShrink( 

)
Input:  

:  -grams
Output: an integer for shrinking size
1. If      Then return 0.
2. Retrieve    -grams  

for  

.
3. Compute
    

  


4. If  	 SHR Then return 0.
5. return HowSmallShrink( 

) - 1.
Figure 3: A function that determines how small a
window size should be used.
4.2 Shrinking  -grams
Shrinking  -grams is accomplished in the direc-
tion opposite to expanding  -grams. After com-
paring  -grams with   -grams,   -grams
are used instead of  -grams only when they are
similar enough. The difference        be-
tween  -grams and     -grams is, once again,
measured by Kullback-Leibler divergence. That
is,
        

 

 

 


If        is smaller than another predefined
threshold SHR, then     -grams are used in-
stead of  -grams.
Figure 3 shows an algorithm which determines
how deeply the shrinking is occurred. The main
stream of this algorithm is equivalent to that in
Figure 2. It also recursively finds the optimal
shrinking window size, but can not be further re-
duced when the current model is an unigram.
The merit of shrinking  -grams is that it can
construct a model with a lower dimensionality.
Since the maximum likelihood estimate is used in
calculating probabilities, this helps obtaining sta-
ble probabilities. According to the well-known
curse of dimensionality, the data density required
is reduced exponentially by reducing dimensions.
Thus, if the lower dimensional model is not differ-
ent so much from the higher dimensional one, it
is highly possible that the probabilities from lower
dimensional space are more stable than those from
higher dimensional space.
636
Function ChangingWindowSize( 

)
Input:  

:  -grams
Output: an integer for changing window size
1. Set exp := HowLargeExpand( 

).
2. If exp 	  Then return exp.
3. Set shr := HowSmallShrink( 

).
4. If shr   Then return shr.
5. return 0.
Figure 4: A function that determines the changing
window size of  -grams.
4.3 Overall Self-Organizing Structure
For a given i.i.d. sample  

, there are three pos-
sibilities on changing  -grams. First one is not
to change  -grams. It is obvious when  -grams
are not changed. This occurs when both    
   EXP and         SHR are met.
This is when the expanding results in too similar
distribution to that of the current  -grams and the
distribution after shrinking is too different from
that of the current  -grams.
The remaining possibilities are then expand-
ing and shrinking. The application order be-
tween them can affect the performance of the pro-
posed method. In this paper, an expanding is
checked prior to a shrinking as shown in Figure
4. The function ChangingWindowSize first calls
HowLargeExpand. The non-zero return value of
HowLargeExpand implies that the window size
of the current  -grams should be enlarged. Oth-
erwise, ChangingWindowSize checks if the win-
dow size should be shrinked by calling HowSmall-
Shrink. If HowSmallShrink returns a negative in-
teger, the window size should be shrinked to (  +
shr). If both functions return zero, the window
size should not be changed.
The reason why HowLargeExpand is called
prior to HowSmallShrink is that the expanded  -
grams handle more specific data. (    )-grams,
in general, help obtaining higher accuracy than  -
grams, since (    )-gram data are more specific
than  -gram ones. However, it is time-consuming
to consider higher-order data, since the number of
kinds of data increases. The time increased due
to expanding is compensated by shrinking. Af-
ter shrinking, only lower-oder data are considered,
and then processing time for them decreases.
4.4 Sequence Tagging
Since natural language sentences are sequential as
their nature, the word spacing can be considered
as a special POS tagging task (Lee et al, 2002) for
which a hidden Markov model is usually adopted.
The best sequence of word spacing for the sen-
tence is defined as


	 
 	


 
 
	 
 
	 

 	


 
  
	 

	 
 
	 

  
	 

 	


 
  
	 

	 
 
	 

by where  is a sentence length.
If we assume that the syllables are independent
of each other,   
	 

	 
 is given by
  
	 

	 
 
 


  




which can be computed using Equation (1). In ad-
dition, by Markov assumption, the probability of
a current tag 

conditionally depends on only the
previous Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1396?1404,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Device-Dependent Readability for Improved Text Understanding
A-Yeong Kim Hyun-Je Song Seong-Bae Park Sang-Jo Lee
School of Computer Science and Engineering
Kyungpook National University
Daegu, 702-701, Korea
{aykim,hjsong,sbpark}@sejong.knu.ac.kr, sjlee@knu.ac.kr
Abstract
Readability is used to provide users with high-
quality service in text recommendation or text
visualization. With the increasing use of hand-
held devices, reading device is regarded as
an important factor for readability. There-
fore, this paper investigates the relationship
between readability and reading devices such
as a smart phone, a tablet, and paper. We sug-
gest readability factors that are strongly related
with the readability of a specific device by
showing the correlations between various fac-
tors in each device and human-rated readabil-
ity. Our experimental results show that each
device has its own readability characteristics,
and thus different weights should be imposed
on readability factors according to the device
type. In order to prove the usefulness of the
results, we apply the device-dependent read-
ability to news article recommendation.
1 Introduction
Readability is a function that maps a given text into a
readability score by considering ?how easily the text is
read and understood? (Richards et al., 1992; Zamanian
and Heydari, 2012). Normally, the readability score is
formulated as a combination of various factors. These
factors reflect the easiness and understanding of the
text and include text presentation format, font size, av-
erage ratio of annotated images, and sentence length
(Hasegawa et al., 2008; Kitson, 1927; Ma et al., 2012;
?
Oquist, 2006). Therefore, readability can be used to
provide satisfiable services in text recommendation or
text visualization.
The study on readability has begun in the education
field to measure the level of a text. With the success
of using readability in education (Franc?ois and Fairon,
2012; Heilman et al., 2008; Ma et al., 2012), read-
ability has been used in a range of domains recently.
For example, in document retrieval, readability is used
to provide documents to non-expert users so that they
can read the retrieved documents easily (Jameel et al.,
2012; Yan et al., 2006). In text mining, readability has
been employed to analyze the characteristics of text.
Especially, Hillbom showed the differences in readabil-
ity between broadsheet newspapers and tabloids that
share a similar political stance (Hillbom, 2009).
There is one important issue of readability that has
not been studied in natural language processing. It is a
reading device. That is, previous studies focused only
on text printed on paper. However, with the increasing
use of hand-held devices, people in these days use var-
ious reading devices such as a tablet and a smart phone
as well as a paper. Readability score can be different
according to the device type, because each device has
its own idiosyncrasy. For example, assume that a sys-
tem recommends the same news article to both user A
who reads it in her smart phone and user B who reads
it on paper. Although both users read the same article,
user A might believe that her article is more difficult to
read than user B because of the screen size of her smart
phone.
This paper explores the relationship between reading
devices and readability. For this purpose, we first inves-
tigate whether readability changes according to device
type or not. Then, we analyze which readability fac-
tors are affected by reading devices. To see the rela-
tionship between readability factors and devices, var-
ious well-known readability factors are computed for
news articles collected from an Internet portal. At the
same time, the readability of each article is also man-
ually rated. When the readability is rated manually, it
is done three times for different reading devices of a
smart phone, a tablet, and paper. The factors that af-
fect the readability actually in each device are found
out through the correlations between the factors and the
manually-labeled readability. Some factors are impor-
tant to the readability of smart phone, but insignificant
to that of paper. Therefore, we discover the importance
of each readability factor for each device by analyzing
the correlations.
The usefulness of the device-dependent readability
is proven by applying it to news article recommenda-
tion. That is, different importance weights for read-
ability factors are considered according to device type
when recommending news articles. Our experimental
results show that the performance of news article rec-
ommendation gets best when the device used for read-
ing news articles is identical to the device used for mea-
suring readability. Therefore, it is essential to consider
different importance weights according to device type
1396
in news article recommendation. It also proves that
the proposed device-dependent readability reflects the
characteristics of reading devices well.
The rest of this paper is organized as follows. We
first review related studies on readability. Next, we
introduce various readability factors and propose the
device-dependent readability. Then, the news article
recommendation using the device-dependent readabil-
ity is explained. This recommendation is prepared to
prove the usefulness of the device-dependent readabil-
ity. In the experiments, we present the experimental
results on the relationship between reading devices and
readability. We also describe the experiments on news
recommendation using the device-dependent readabil-
ity and present their results. Finally, we summarize our
research.
2 Related work
The history of readability studies began in the 1800s.
Early studies focused on the frequency of easy words,
sentence length, and word length (Huld?en, 2004).
Flesch designed a formula to calculate ?reading ease?
using only the average word length and sentence length
(Flesch, 1948). He adjusted the relative importance
between word length and sentence length using 100
words selected randomly from a corpus. This formula
is called the Flesch-Kincaid formula, and is generally
used in measuring the readability of a textbook (Kin-
caid et al., 1975). Dale and Chall (1949) defined a list
of 3,000 easy words. Then, they used the average sen-
tence length and the percentage of words not included
in the list. These studies simply used superficial fac-
tors, and thus do not reflect syntactic factors.
Recent studies on readability use various factors in-
cluding syntactic ones, and combine them to produce
a highly predictive model of readability. Franc?ois and
Faircon (2012) proposed a readability formula with 46
textual factors for French as a foreign language. The
factors represent lexical, syntactic, and semantic char-
acteristics of sentences, and the specificities of French.
They are extracted from 28 French Foreign Language
(FFL) textbooks written for adults learning FFL. On the
other hand, Pitler and Nenkova (2008) showed the rela-
tion between readability factors and readability. They
used human ratings from the Wall Street Journal cor-
pus, and computed the correlations between the read-
ability factors and the average human ratings. Accord-
ing to their results, the average number of verb phrases
in a sentence, the number of words in an article, the
likelihood of the vocabulary, and the likelihood of the
discourse relations are highly correlated with human
ratings. However, these studies did not consider the
reading devices, but focused on how well a text is writ-
ten. Since the readability can be differentiated accord-
ing to reading device, a reading device should be con-
sidered when computing the readability of a given text.
To the best of our knowledge, there are few studies
on the readability on mobile devices that do not con-
sider language-related aspects. Most studies on mobile
devices focused on the development of new text format
and layout to help users read documents easily.
?
Oquist
(2006) proposed a new text presentation format called
the dynamic Rapid Serial Visual Presentation. Accord-
ing to his experimental results, this format helps to re-
duce eye movements. On the other hand, Hasegawa
et al. (2008) evaluated the readability of documents
on mobile devices with regard to screen and font size.
They reported that the readability is improved when the
characters are vertically enlarged. Readability on mo-
bile devices is not reflected only by the visualization
factors, but also by textual factors. Therefore, this pa-
per explores the readability factors that reflect the lexi-
cal and grammatical complexity of text and are affected
by reading devices.
3 Readability Factors
Table 1 lists the readability factors used in this paper.
Basically, they are based on the factors proposed by
Pitler and Nenkova (2008). However, some factors are
excluded and some new factors are added. This is be-
cause some of their factors are computationally infeasi-
ble and language-dependent. As a result, we have thir-
teen readability factors. These readability factors are
divided into four types: superficial, lexical, syntactic
factors, and lexical cohesion.
3.1 Superficial Factors
Superficial factors were used in most early readability
studies (Dale and Chall, 1949; Flesch, 1948; Kincaid et
al., 1975), and reflect the construction of a text. We in-
vestigate four factors: text length (TL), sentence length
(SL), average number of words per sentence (WS), and
average number of characters per word (CW). Since
longer text is perceived as ?harder-to-read? than short
one, these factors are all reciprocally related with read-
ability.
The first two factors are related to length. TL counts
the number of characters in a text, whereas SL com-
putes the number of sentences. When a writer attempts
to write many topics in a text, she tends to use many
kinds of words simultaneously. As a result, the text be-
comes longer and more complex. Such long length of
text disturbs a reader?s comprehension of the text, and
then it is more difficult for the reader to read the text
(Heilman et al., 2008).
WS counts the average number of words per sen-
tence, and CW reflects the average number of characters
per word. When they are large, the sentence is diffi-
cult to read, which leads to difficulties in understanding
the text. Especially, CW reflects compound nouns and
technical words. For instance, compound nouns in Ko-
rean are usually long, because there is no spacing be-
tween words in a compound noun. For example, let us
consider a compound noun, ?Daehanmingukjungboo,?
which means the Korean government. Actually this
compound noun consists of two independent nouns.
1397
Type of Factors Abbr. Description
Superficial factors
TL The number of characters in a text
SL The number of sentences in a text
WS Average number of words per sentence
CW Average number of characters per word
Lexical factor LL Article likelihood estimated by language model
Syntactic factors
PTD Average parse tree depths per sentence
NP Average number of noun phrases per sentence
VP Average number of verb phrases per sentence
SBAR Average number of subordinate clauses per sentence
Lexical cohesion
COS Average cosine similarity between pairs of adjacent sentences
WO Average word overlap between pairs of adjacent sentences
NPO Average word overlap over noun and pronoun only
PRP Average number of pronouns per sentence
Table 1: Description of readability factors
One is ?Daehanminguk? meaning Korea and the other
is ?Jungboo? meaning a government. The two are con-
catenated to form a compound noun and become a long
single word. In addition, many difficult words such as
domain-specific terms tend to be long. Such lengthy
words make it difficult to read a text.
3.2 Lexical Factor
Lexical factor determines whether a given text con-
sists of frequent words. Texts that express a new trend
in various fields often use many newly coined words.
Such neologisms make it difficult to read and under-
stand a text. Therefore, an easily-understandable text
is composed of widely-used words rather than unusual
words.
In order to compute the use of frequent words in a
text, a unigram language model is used as in the work
of Pitler and Nenkova (2008). In this model, the log
likelihood of text t is computed by
?
w?t
C(w) ? logP (w|B). (1)
where P (w|B) is the probability of a word w according
to a background corpus B, and C(w) is the number of
times that w appears in t.
This factor examines the familiarity of the words
used in the text. The more frequently a word appears
in the background corpus, the more familiar it is re-
garded. The frequency of a word w is then reflected
into P (w|B) computed from the independent back-
ground corpus B. Therefore, the factor LL is positively
related with readability.
3.3 Syntactic Factors
Syntactic factors reflect sentence complexity directly
that affects human processing of a sentence. We con-
sider the average parse tree depth per sentence (PTD),
the average number of noun phrases per sentence (NP),
the average number of verb phrases per sentence (VP),
and the average number of subordinate clauses per sen-
tence (SBAR) as syntactic factors. These four factors
were defined by Schwarm and Ostendorf (2005).
A reader regards a text as difficult when the sen-
tences in the text have large parse tree depths or many
subordinate clauses. Thus, PTD and SBAR are related
negatively with readability. On the other hand, the re-
lationship of NP and VP to readability are not one way.
The large number of noun phrases in a text requires
a reader to remember more items (Barzilay and Lap-
ata, 2008; Pitler and Nenkova, 2008). However, it also
makes the text more interesting. The texts written for
adults actually contain more entities than those writ-
ten for children (Barzilay and Lapata, 2008). The same
is true for VP. The large number of verb phrases in a
sentence makes the sentence more complex. However,
people feel that a text is more easier to comprehend
when related clauses are grouped together (Bailin and
Grafstein, 2001).
3.4 Lexical Cohesion
Lexical cohesion denotes how the sentences in a text
are semantically connected. People usually bring con-
tinuous sentences into their mind at the same time, and
interpret them as a single unit (Okazaki et al., 2005). In
other words, a reader prefers text whose sentences are
smoothly connected to text whose sentences are inde-
pendent of one another. Therefore, sentence continuity
plays a primary role in understanding an entire text.
In the classic study of cohesion, various uses of
cohesive elements such as pronouns, definite articles,
and topic continuity have been discussed (Halliday and
Hasan, 1976). This paper uses the average cosine sim-
ilarity (COS), word overlap (WO), word overlap over
just nouns and pronouns (NPO) between pairs of adja-
cent sentences, and the average number of pronouns per
sentence (PRP). COS, WO, and NPO are superficial mea-
sures of topic continuity, whereas PRP is an indicative
feature of sentence continuity. High values for these
factors imply that the sentences in the text are related
somehow. Therefore, these factors are believed to be
related positively with readability.
1398
3.5 Measurement of Readability
When a reading device d is given, the readability of
text t, represented as R(t|d), is formulated as a com-
bination of readability factors with their corresponding
weight in the device. We assume that w
i|d
, the weight
of a readability factor f
i
, is dependent on the reading
device d. Following the previous work of Pitler and
Nenkova (2008), we also assume that each readabil-
ity factor affects readability independently. Therefore,
readability is calculated as a weighted linear sum of all
readability factors. That is, R(t|d) is computed by
R(t|d) =
?
i?{1,2,...,M}
w
i|d
? f
i
(t) (2)
where M is the number of readability factors.
Each weight w
i|d
is determined from a set of news
articles T . We collected a large number of news arti-
cles from an Internet news portal. The readability of
each article was manually labeled. This is done three
times, since we have three different devices of a smart
phone, a tablet, and paper. Since human rating of each
article t ? T is available for each device, w
i|d
?s can
be estimated by linear regression. These weights are
different according to the devices.
4 News Article Recommendation by
Device-Dependent Readability
The fact that the weights w
i|d
in Equation (2) are differ-
ent for each device d implies that the readability mea-
surement should be different depending on the device
type. In order to see the usefulness of this device-
dependent readability, we apply it to news article rec-
ommendation. News article recommendation aims to
provide a user with news articles that interest the user.
Thus, it selects a few articles that meet user preference
from a gigantic amount of news events. Various meth-
ods have reported notable results in news article rec-
ommendation (Das et al., 2007; Li et al., 2010; Liu et
al., 2010). In addition, with the recent interest in hand-
held devices, the demand for news recommendation on
hand-held devices is increasing. However, there has
been, at least as far as we know, no study on the read-
ability of hand-held devices.
Device-dependent readability is reflected into news
article recommendation through a re-ranking frame-
work. Figure 1 depicts the overall process of suggest-
ing news articles for a specific device with the device-
dependent readability. The point of this figure is to
measure how appropriate a news article is for a spe-
cific reading device. For this, a news recommendation
system first chooses a set of news articles from a news
repository based on its own criterion. Then, we re-rank
them by the device-dependent readability to obtain the
final set of ranked news articles for the device.
Formally, a news article recommendation ranks a set
of articles, A = {a
1
, a
2
, ..., a
m
}, where a
i
represents
the i-th article. The order between ranks a
1
 a
2

Min Max Average
Article length 68 610 346.5
# of sentences 1 14 6.24
# of words per sentence 8 33 16.93
# of words per article 17 178 99.34
Table 2: Statistics of the news article data
...  a
m
should be satisfied by the criterion of the
recommendation system. That is, assuming that the
system has a score function score(a
i
), score(a
i
) >
score(a
j
) has to be met if a
i
 a
j
. Then, the top
k(k ? m) articles of A by the score function are sug-
gested as appropriate news articles. After that, the se-
lected articles are re-ranked by another criterion, the
device-dependent readability. That is, the final rank of
an article within the selected set is determined by an-
other function, rerank. Since this function has to re-
flect the device-dependent readability, it takes two pa-
rameters. One is an article, and the other is a device
type. The re-rank function is modeled as
rerank(a, d) = R(a|d)
=
?
i?{1,2,...,M}
w
i|d
? f
i
(a). (3)
As a result, the readability-based re-ranking module
suggests the news articles based on how easily the ar-
ticles are read on a specific reading device. Note that
even the same article would be ranked differently ac-
cording to the device type because the article is re-
ranked by the device-dependent readability. At last, the
top k
?
(k
?
? k) re-ranked articles among them are sug-
gested as final news articles.
5 Experiments
5.1 Experiments on Readability Factors
5.1.1 Experiment Settings
For the experiments of analyzing relationship between
readability factors and readability, we collected a Ko-
rean news corpus from Naver News
1
. This corpus con-
tains news articles from June 10, 2013 to June 25,
2013. We selected 74 articles randomly from the cor-
pus which were used for readability formula and show-
ing the relationships between readability factors. All
selected articles belong to one of three categories: ?Pol-
itics?, ?Entertainment?, and ?Sports?. A set of these 74
news articles becomes T , and is used to compute the
weights in Equation (2). Table 2 describes a simple
statistics of the selected news articles. The shortest ar-
ticle consists of 68 characters, whereas the longest one
has 610 characters. The average length of article is
346.5. The shortest article is written in one sentence,
and the longest has 14 sentences. One article has ap-
proximately 6.24 sentences on average. In addition, the
1
A Korean news portal of which web address is
http://news.naver.com.
1399
News
Repository
Device
dependent
re-ranking
News
Articles
News
Recommendation
System
Readability
Figure 1: Overall process of re-ranking news articles based on device-dependent readability
number of words per sentence ranges from 8 to 33, and
the average is 16.93. The minimum number of words
in an article is 17, and the maximum number of words
is 178. An article is composed of 99.34 words on aver-
age.
In order to compute the lexical factor LL by Equa-
tion (1), a background corpus B is required. Since this
corpus should be independent from the news articles
explained above, the Naver News is adopted again to
generate B. For the background corpus B, we col-
lected news articles from January 1, 2013 to September
6, 2013, but excluded the articles from June 10 to June
25, because they are already used. This corpus consists
of 298,729 articles with 3,264,104 distinct words.
The readability score for each article was manually
labeled by three undergraduate students. To investigate
the relationship between reading devices and readabil-
ity, each article was read using three different reading
devices. The Galaxy Note 1 with a 5-inch screen is
used as the smart phone, Galaxy Tab 10.1 with a 10.1-
inch screen is used as the tablet, and A4-size paper
is used for the paper. That is, the human annotators
read and rated 74 articles per device. The order of the
devices where the annotators evaluated readability is
smart phone, tablet, and paper. This order was main-
tained for all the experiments. All aspects but content
texts were under control. For instance, font = ?Gothic,
12 pt? (this is most commonly used font and size that
most Korean web pages and textbooks use), font color
= ?black?, alignment = ?both? were used for all three
devices. In addition, the non-content aspects were ex-
actly same for devices because the annotators of read-
ability and the recommended articles shared the read-
ing devices. Although these aspects affect readability
and many previous studies already proved it, it is not
our concern. We only attempt to capture how read-
Reading device Min Max Average
Smart phone 1.67 5 3.423 ? 0.741
Tablet 1.33 5 3.531 ? 0.837
Paper 2 5 3.360 ? 0.594
Table 3: Readability scores given by human annotators
ability is affected by the content in different types of
devices.
Human annotators can remember the content of
news articles when they read articles with three de-
vices. The human annotators were asked to read and
evaluate many articles within a relatively short period.
Therefore, before the main experiments, we performed
a pilot experiment on the memory effects of previously
read articles and verified it empirically. We hired three
undergraduate students who were not involved in our
main experiments. The students read the same 250 ar-
ticles four times, and these also come from Naver News
corpus which are not included the previous 74 articles.
After their first reading, they read the articles again in
3, 7, and 14 days later. After 3 days, two students re-
membered the articles somewhat, but one student re-
membered them vaguely. Since they almost forgot the
articles after 7 days, we placed 7 days interval between
devices.
The readability score of an article was rated by the
annotators using the questions in the work of Pitler and
Nenkova (2008). We use only two of the questions,
while they used four questions for the annotators. Their
questions are intended to measure the extent of how
well a text is written, how it fits together, how easy
it is to understand, and how interesting it is. We can
consider ?well-written? and ?fit-together? as a syntac-
tic perspective, whereas ?easy to understand? and ?in-
teresting? belong to a content perspective. For such a
1400
Smart phone Tablet Paper
Factor Value Factor Value Factor Value
SL -0.394 SL -0.370 NP 0.298
TL -0.293 WS 0.321 WS 0.278
WS 0.288 LL 0.253 LL 0.268
LL 0.249 NP 0.240 VP 0.244
Table 4: Pearson correlation coefficients of important
readability factors
reason, four questions can be summarized in two ques-
tions. The two questions used are
? How well-written is this article?
? How interesting is this article?
For these two questions, each annotator assigns a score
between 1 and 5 to each article. Here, 1 point means
that the article is worst and 5 point implies that it is
best. A readability score of one human annotator is
composed with the average of two questions (well-
written, interesting). We used the average of three hu-
man annotators? readability scores in our experiments.
Table 3 shows the readability scores of the articles for
each device. According to this table, the readability
score ranges from 1.67 to 5 for the smart phone, 1.33
to 5 for the tablet, and ranges from 2 to 5 for the paper.
The average readability is 3.423 for the smart phone,
3.531 for the tablet, and 3.360 for the paper. To see
the inter-judge agreement among annotators, the Kappa
coefficient (Fleiss, 1971) is used. The Kappa values
for the ?smart phone?, ?tablet?, and ?paper? are 0.342,
0.333, and 0.361, respectively. All these values corre-
spond to fair agreement.
5.1.2 Experimental Results
In order to see the importance of each factor in a spe-
cific device, we adopt the Pearson correlation coeffi-
cients between readability factors and reading devices.
Table 4 lists the four most important factors in each
device and their Pearson correlation coefficients. Espe-
cially, p-value is smaller than 0.05 for all factors in this
table.
For the smart phone, SL, the number of sentences in
a text, is the most important readability factor. Its cor-
relation with the smart phone is -0.394. TL, the number
of characters, is the second important factor and has a
negative correlation of -0.293. These results imply that
readers are negatively sensitive to the length of an arti-
cle because of the small display size of a smart phone.
That is, in the smart phone, longer articles are recog-
nized as difficult to read compared to shorter ones. The
number of words per sentence, WS, is the third impor-
tant factor with correlation of 0.288. The log-likelihood
of an article, LL, is also positively related with the read-
ability, which proves that widely-used words make it
easy to understand an article. The top three factors are
superficial with regard to text length. Therefore, the su-
perficial factors are more important than other types of
factors for the smart phone.
SL is the most critical readability factor even for the
tablet. It affects readability with high correlation of -
0.370. The second important factor is WS with correla-
tion of 0.321. Both of these factors are superfical. The
third important factor, LL, is positively related with
readability as expected. The fourth factor that affects
readability is the number of noun phrases, NP. It is nat-
ural for NP to be positively related with the readability.
Finally, for the paper, NP is most strongly related to
readability with correlation of 0.298. The second im-
portant factor is WS, whose correlation is 0.278. LL is
the third important factor and shows a positive relation-
ship. Note that WS and LL are important readability
factors for all devices. The next important readabil-
ity factor for the paper is the average number of verb
phrases (VP). The articles with many noun phrases and
verb phrases are perceived as easier-to-read for the pa-
per. Note that the importance of superficial factors is
limited for the paper. We expected that WS is negatively
related, but, it is positively related with readability for
all three devices. The reason for this could be that the
annotators thought the articles with higher WS are more
interesting.
The important factors for the smart phone are differ-
ent from those for the paper. On the other hand, the
tablet shares many factors with both the smart phone
and the paper. Because the screen size of a tablet is
similar to the size of an A4 paper, the tablet and the pa-
per share readability factors. However, length-related
factors play a more important role than syntactic fac-
tors in the smart phone because a smart phone has a
smaller screen.
5.2 Experiments on News Recommendation
5.2.1 Experiment Settings
Experiments for news article recommendation were
performed to see the effectiveness of device-dependent
readability. The process of news recommendation with
device-dependent readability is as follows. For a spe-
cific device,
1. Select top-k news articles from a news repository
by the criterion of the recommendation system.
2. Re-rank the k articles by the readability of the de-
vice using Equation (3).
3. Select top-k
?
news articles by the new rank.
4. Human annotators read and rate the k
?
articles
with the device.
5. Compare the ranks of k
?
articles by device-
dependent readability with those by human rat-
ings.
Since we have three types of devices, this process is
performed three times with a different device.
The news articles from September 10, 2013 to
September 12, 2013 collected from Naver News were
1401
Min Max Average
Article length 277 6,077 990.68
# of sentences 4 199 22.85
# of words per sentence 4 100 15.73
# of words per article 71 2,034 301.61
Table 5: Statistics of news data for recommendation
Reading device Min Max Average
Smart phone 1 5 3.513 ? 0.962
Tablet 1 5 3.344 ? 0.852
Paper 1 5 3.250 ? 0.907
Table 6: Scores of news articles by human annotators
in news recommendation
used as the news repository. The number of times that
a news article was actually read by its anonymous read-
ers at the portal site is used as the criterion for the rec-
ommendation system. Since this criterion is provided
on a daily basis and news articles were collected for
three days, the process explained above is performed
three times. The top twenty articles were selected by
the criterion every day. That is, k = 20. Table 5 shows
the statistics of the total 60 articles. The shortest arti-
cle consists of 277 characters, and the longest article
has 6,077 characters. On average, an article is writ-
ten with 990.68 characters. The minimum number of
sentences in an article is 4, and the maximum number
of sentences is 199. An article is composed of 22.85
sentences on average. The average number of words in
a sentence is 15.73, whereas a sentence length ranges
from 4 to 100 words. The shortest article has 71 words,
and the longest article has 2,034 words. One article has
approximately 301.61 words on average.
Three human annotators labeled the scores of the
news articles manually. The annotators were the same
persons who labeled the readability scores. Similar to
the previous experiments, 7 days intervals was placed
among devices to reduce the memory effect. The same
two questions used in the previous section were used
again for this experiment. The annotators assigned a
score between 1 and 5 to every article for each ques-
tion. The final score of an article was obtained by aver-
aging six scores (two questions from three annotators).
Table 6 summarizes the scores of the articles by the
human annotators. As shown in this table, the article
scores vary for all reading devices. The average scores
for smart phone, tablet, and paper are 3.513, 3.344,
and 3.250 respectively. The Kappa value for the ?smart
phone? is 0.402, and that for both the ?tablet? and the
?paper? is 0.393. Thus, the value of ?smart phone? falls
into moderate agreement, whereas those of the ?tablet?
and ?paper? correspond to fair agreement. The perfor-
mance of the news article recommendation is evaluated
with the Normalized Discounted Cumulative Gain at
top P (NDCG@P ) (J?arvelin and Kek?al?ainen, 2002).
Figure 2: NDCG@k
?
scores with various k
?
for the
smart phone.
Figure 3: NDCG@k
?
scores with various k
?
for the
tablet.
5.2.2 Experimental Results
For the a baseline criterion, we use the news article
recommendation system in Naver, which recommends
news article by the number of article hits. Figures 2 to 4
show the NDCG@k
?
scores with 1 ? k
?
? 10 for the
three devices. Each graph in these figures compares the
performance of various devices when the readability
for a specific device is used. That is, Figure 2 depicts
the NDCG@k
?
scores for the recommended news arti-
cles when the articles are shown in the smart phone, the
tablet, and the paper respectively. In computing their
NDCG@k
?
scores, the news articles are re-ranked by
readability for the smart phone. Therefore, in this fig-
ure we expect that the NDCG@k
?
score for using the
smart phone is higher than those for using the tablet and
paper. In the same way, Figure 3 and Figure 4 compare
the NDCG@k
?
scores when the readabilities for the
tablet and paper are used.
In all three graphs, the best news recommendation
performance is achieved when the device used to read
1402
Figure 4: NDCG@k
?
scores with various k
?
for the
paper.
news articles is the same as the device used for read-
ability. In Figure 2, the use of the smart phone outper-
forms those of other devices when k
?
? 6. This proves
that the quality of highly ranked news articles is much
better for the smart phone than for other devices, when
the readability for smart phone is used.
Figure 3 shows the NDCG@k
?
scores for using var-
ious devices when the news articles are re-ranked by
readability for the tablet. In this figure, the use of
the tablet as a reading device is better than using the
smart phone or the paper. The performance difference
is largest at k
?
= 3. The difference becomes smaller
as k
?
increases up to 10, but the performance of tablet
is still higher than those of others. In Figure 2 and 3,
when k
?
= 1, the baseline outperforms other devices.
We believe this happens because the baseline chooses
news articles by user-hit. Therefore, many articles rec-
ommended by the baseline are interesting because peo-
ple tend to click more often when an article is inter-
esting. As noted, readability reflects users? interests,
which leads to high performance of the baseline. The
performance of paper is best in Figure 4, since the ar-
ticles are re-ranked by the readability for paper. Paper
outperforms all other devices for all k
?
s. Note that the
performances of the baseline are always lowest regard-
less of reading device.
From all results above, we can infer that the use of
device-dependent readability is helpful to news article
recommendation. This is because the readability fac-
tors that affect the readers of news articles are different
according to the reading device. Therefore, it is im-
portant to reflect the characteristics of a reading device
when recommending news articles.
6 Conclusion
In this paper, we have proposed a device-dependent
readability. Since a reading device is one of the most
important features of readability, different weights have
been assigned to the readability factors according to de-
vice type. We have shown that the important readabil-
ity factors are distinct according to the reading device
by investigating the correlation between the readability
factors and the reading device. Through the correlation,
we found that tablet shares many important factors with
both smart phone and paper.
The experiments on the news articles collected from
an Internet portal proved that readability is actually af-
fected by the reading device. In addition, the validity of
the device-dependent readability was shown by apply-
ing it to the news article recommendation. The news
articles were first ranked by the criterion of the recom-
mendation system. Then, they were re-ranked by the
device-dependent readability. Our experiments showed
that the recommendation performance of the re-ranked
articles gets best when the device used for readability is
the same as the reading device. These two types of ex-
periments proved the importance and effectiveness of
the device-dependent readability.
Acknowledgments
This work was supported by the IT R&D program of
MSIP/KEIT (10044494, WiseKB: Big data based self-
evolving knowledge base and reasoning platform) and
the Industrial Strategic Technology Development Pro-
gram (10035348, Development of a Cognitive Planning
and Learning Model for Mobile Platforms) funded by
the Ministry of Knowledge Economy(MKE, Korea).
References
Alan Bailin and Ann Grafstein. 2001. The linguistic
assumptions underlying readability formulae: A cri-
tique. Language & Communication, 21(3):285?301.
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Compu-
tational Linguistics, 34(1):1?34.
Edgar Dale and Jeanne Chall. 1949. The concept of
readability. Elementary English, 26(1):19?26.
Abhinandan Das, Mayur Datar, Ashutosh Garg, and
Shyam Rajaram. 2007. Google news personaliza-
tion: scalable online collaborative filtering. In Pro-
ceedings of the 16th International Conference on
World Wide Web, pages 271?280.
Joseph Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. Psychological bulletin,
76(5):378?382.
Rudolph Flesch. 1948. A new readability yardstick.
Journal of Applied Psychology, 32(3):221?233.
Thomas Franc?ois and C?edrick Fairon. 2012. An
AI readability formula for French as a foreign lan-
guage. In Proceedings of the 2012 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 466?477.
1403
Michael Halliday and Ruqaiya Hasan. 1976. Cohesion
in English. Longman Group Ltd.
Satoshi Hasegawa, Kazuhiro Fujikake, Masako Omori,
and Masaru Miyao. 2008. Readability of charac-
ters on mobile phone liquid crystal displays. In-
ternational Journal of Occupational Safety and Er-
gonomics (JOSE), 14(3):293?304.
Michael Heilman, Kevyn Collins-Thompson, and
Maxine Eskenazi. 2008. An analysis of statistical
models and features for reading difficulty prediction.
In Proceedings of the Third Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 71?79.
Kristina Hillbom. 2009. Newspaper Readability: a
Broadsheet vs. a Tabloid. Ph.D. thesis, University of
G?avle.
M?ans Huld?en. 2004. Linguistic complexity in
two major american newspapers and the associated
press newswire, 1900?2000. Master?s thesis,
?
Abo
Akademi University.
Shoaib Jameel, Wai Lam, and Xiaojun Qian. 2012.
Ranking text documents based on conceptual dif-
ficulty using term embedding and sequential dis-
course cohesion. In Proceedings of the The 2012
IEEE/WIC/ACM International Joint Conferences on
Web Intelligence and Intelligent Agent Technology-
Volume 01, pages 145?152.
Kalervo J?arvelin and Jaana Kek?al?ainen. 2002. Cu-
mulated gain-based evaluation of IR techniques.
ACM Transactions on Information Systems (TOIS),
20(4):422?446.
J. Peter Kincaid, Robert Fishburne Jr., Richard Rogers,
and Brad Chissom. 1975. Derivation of new read-
ability formulas (automated readability index, fog
count and flesch reading ease formula) for navy en-
listed personnel. Technical report, DTIC Document.
Harry Kitson. 1927. The mind of the buyer. MacMil-
lan Company.
Lihong Li, Wei Chu, John Langford, and Robert E.
Schapire. 2010. A contextual-bandit approach to
personalized news article recommendation. In Pro-
ceedings of the 19th International Conference on
World Wide Web, pages 661?670.
Jiahui Liu, Peter Dolan, and Elin R. Pedersen. 2010.
Personalized news recommendation based on click
behavior. In Proceedings of the 15th International
Conference on Intelligent User Interfaces, pages 31?
40.
Yi Ma, Eric Fosler-Lussier, and Robert Lofthus. 2012.
Ranking-based readability assessment for early pri-
mary children?s literature. In Proceedings of the
2012 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 548?552.
Naoaki Okazaki, Yutaka Matsuo, and Mitsuru
Ishizuka. 2005. Improving chronological ordering
of sentences extracted from multiple newspaper ar-
ticles. ACM Transactions on Asian Language Infor-
mation Processing (TALIP), 4(3):321?339.
Gustav
?
Oquist. 2006. Evaluating readability on mo-
bile devices. Ph.D. thesis, Uppsala University.
Emily Pitler and Ani Nenkova. 2008. Revisiting read-
ability: A unified framework for predicting text qual-
ity. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
186?195.
Jack Richards, John Platt, Heidi Platt, and Christophe
Candlin. 1992. Longman Dictionary of Language
Teaching and Applied Linguistics, volume 78. Long-
man London.
Sarah Schwarm and Mari Ostendorf. 2005. Reading
level assessment using support vector machines and
statistical language models. In Proceedings of the
43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 523?530.
Xin Yan, Dawei Song, and Xue Li. 2006. Concept-
based document readability in domain specific infor-
mation retrieval. In Proceedings of the 15th ACM In-
ternational Conference on Information and Knowl-
edge Management, pages 540?549.
Mostafa Zamanian and Pooneh Heydari. 2012. Read-
ability of texts: State of the art. Theory and Practice
in Language Studies, 2(1):43?53.
1404
Proceedings of NAACL-HLT 2013, pages 888?896,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
A Just-In-Time Keyword Extraction from Meeting Transcripts
Hyun-Je Song Junho Go Seong-Bae Park Se-Young Park
School of Computer Science and Engineering
Kyungpook National University
Daegu, Korea
{hjsong,jhgo,sbpark,sypark}@sejong.knu.ac.kr
Abstract
In a meeting, it is often desirable to extract
keywords from each utterance as soon as it is
spoken. Thus, this paper proposes a just-in-
time keyword extraction from meeting tran-
scripts. The proposed method considers two
major factors that make it different from key-
word extraction from normal texts. The first
factor is the temporal history of preceding ut-
terances that grants higher importance to re-
cent utterances than old ones, and the sec-
ond is topic relevance that forces only the pre-
ceding utterances relevant to the current utter-
ance to be considered in keyword extraction.
Our experiments on two data sets in English
and Korean show that the consideration of the
factors results in performance improvement in
keyword extraction from meeting transcripts.
1 Introduction
A meeting is generally accomplished by a number
of participants and a wide range of subjects are dis-
cussed. Therefore, it would be helpful to meeting
participants to provide them with some additional
information related to the current subject. For in-
stance, assume that a participant is discussing a spe-
cific topic with other participants at a meeting. The
summary of previous meetings on the topic is then
one of the most important resources for her discus-
sion.
In order to provide information on a topic to par-
ticipants, keywords should be first generated for the
topic since keywords are often representatives of a
topic. A number of techniques have been proposed
for automatic keyword extraction (Frank et al, 1999;
Turney, 2000; Mihalcea and Tarau, 2004; Wan et al,
2007), and they are designed to extract keywords
from a written document. However, they are not
suitable for meeting transcripts. In a meeting, it is
often desirable to extract keywords at the time at
which a new utterance is made for just-in-time ser-
vice of additional information. Otherwise, the ex-
tracted keywords become just the important words
at the end of the meeting.
Two key factors for just-in-time keyword extrac-
tion from meeting transcripts are time of preceding
utterances and topic of current utterance. First, cur-
rent utterance is affected by temporal history of pre-
ceding utterances. That is, when a new utterance
is made it is likely to be related more closely with
latest utterances than old ones. Second, the preced-
ing utterances which carry similar topics to current
utterance are more important than irrelevant utter-
ances. Since a meeting consists of several topics,
the utterances that have nothing to do with current
utterance are inappropriate as a history of the cur-
rent utterance.
This paper proposes a graph-based keyword ex-
traction to reflect these factors. The proposed
method represents an utterance as a graph of which
nodes are candidate keywords. The preceding utter-
ances are also expressed as a history graph in which
the weight of an edge is the temporal importance
of the keywords connected by the edge. To reflect
the temporal history of utterances, forgetting curve
(Wozniak, 1999) is adopted in updating the weights
of edges in the history graph. It expresses effectively
not only the reciprocal relation between memory re-
888
tention and time, but also active recall that makes
frequent words more consequential in keyword ex-
traction. Then, a subgraph that is relevant to the
current utterance is derived from the history graph,
and used as an actual history of the current utterance.
The keywords of the current utterance are extracted
by TextRank (Mihalcea and Tarau, 2004) from the
merged graph of the current utterance and the his-
tory graphs.
The proposed method is evaluated with two kinds
of data sets: the National Assembly transcripts
in Korean and the ICSI meeting corpus (Janin et
al., 2003) in English. The experimental results
show that it outperforms both the TFIDF frame-
work (Frank et al, 1999; Liu et al, 2009) and the
PageRank-based graph model (Wan et al, 2007).
One thing to note is that the proposed method im-
proves even the supervised methods that do not re-
flect utterance time and topic relevance for the ICSI
corpus. This proves that it is critical to consider time
and content of utterances simultaneously in keyword
extraction from meeting transcripts.
The rest of the paper is organized as follows. Sec-
tion 2 reviews the related studies on keyword extrac-
tion. Section 3 explains the overall process of the
proposed method, and Section 4 addresses its de-
tailed description how to reflect meeting character-
istics. Experimental results are presented in Section
5. Finally, Section 6 draws some conclusions.
2 Related Work
Keyword extraction has been of interest for a long
time in various fields such as information retrieval,
document clustering, summarization, and so on.
Thus, there have been many studies on automatic
keyword extraction. The frequency-based key-
word extraction with TFIDF weighting (Frank et al,
1999) and the graph-based keyword extraction (Mi-
halcea and Tarau, 2004) are two base models for this
task. Many studies recently tried to extend them by
incorporating specific information such as linguistic
knowledge (Hulth, 2003), web-based resource (Tur-
ney, 2003), and semantic knowledge (Chen et al,
2010). As a result, they show good performance on
written text. However, it is difficult to use them di-
rectly for spoken genres, since spoken genres have
significantly different characteristics from written
text.
There have been a few studies focused on key-
word extraction from spoken genres. Among them,
the extraction from meetings has attracted more con-
cern, since the need for grasping important points
of a meeting or an opinion of each participant has
increased. The studies on meetings focused on
the exterior features of meeting dialogues such as
unstructured and ill-formed sentences. Liu et al
(2009) used some knowledge sources such as Part-
of-Speech (POS) filtering, word clustering, and sen-
tence salience to reflect dialogue features, and they
found out that a simple TFIDF-based keyword ex-
traction using these knowledge sources works rea-
sonably well. They also extended their work by
adopting various features such as decision making
sentence features, speech-related features, and sum-
mary features that reflect meeting transcripts better
(Liu et al, 2011). Chen et al (2010) extracted key-
words from spoken course lectures. In this study,
they considered prosodic information from HKT
forced alignment and topics in a lecture generated
by Probabilistic Latent Semantic Analysis (pLSA).
These studies focused on the exterior characteris-
tics of spoken genres, since they assumed that entire
scripts are given in advance and then they extracted
keywords that best describe the scripts. However, to
the best of our knowledge, there is no previous study
considered time of utterances which is an intrinsic
element of spoken genres.
The relevance between current utterance and pre-
ceding utterances is also a critical feature in keyword
extraction from meeting transcripts. The study that
considers this relevance explicitly is CollabRank
proposed by Wan and Xiao (2008). This is collabo-
rative approach to extract keywords in a document.
In this study, it is assumed that a few neighbor doc-
uments close to a current document can help extract
keywords. Therefore, they applied a clustering al-
gorithm to a document set and then extracted words
that are reinforced by the documents within a clus-
ter. However, this method also does not consider the
utterance time, since it is designed to extract key-
words from normal documents. As a result, if it is
applied to meeting transcripts, all preceding utter-
ances would affect the current utterance uniformly,
which leads to a poor performance.
889
Current
utterance 
graph (G1)
History 
graph (G2)
Subgraph (G3)
Expanded graph (G4)
Keyword 
graph (G5)
Subgraph
extraction
Expand
Keyword
extraction
Merge
Keywords
Current 
utterance
Filter
Figure 1: The overall process of the just-in-time keyword extraction from meeting transcripts.
3 Just-In-Time Keyword Extraction for a
Meeting
Figure 1 depicts the overall process of extracting
keywords from an utterance as soon as it is spo-
ken. We represent all the components in a meeting
as graphs. This is because graphs are effective to ex-
press the relationship between words, and the graph
operations that are required for keyword extraction
are also efficiently performed. That is, whenever an
utterance is spoken, it is represented as a graph (G1)
of which nodes are the potential keywords in the ut-
terance. This graph is named as current utterance
graph.
The summary of all preceding utterances is also
represented as a history graph (G2). We assume that
only the preceding utterances that are directly re-
lated with the current utterance are important for ex-
tracting keywords from the current utterance. There-
fore, a subgraph of G2 that maximally covers the
current utterance graph (G1) is extracted. This sub-
graph is labeled as G3 in Figure 1. Then, the current
utterance graph G1 is expanded by merging it and
G3. This expanded graph (G4) is a combined rep-
resentation of the current and preceding utterances,
and then the keywords of the current utterance is ex-
tracted from this graph. The keywords are so-called
hub nodes of G4.
After keywords are extracted from the current ut-
terance, the current utterance becomes a part of the
history graph for the next utterance. For this, the
extracted keywords are also represented as a graph
(G5), and it is merged into the current history G2.
This merged graph becomes a new history graph
for the next utterance. In merging two graphs, the
weight of each edge in G2 is updated to reflect the
temporal history. If an edge is connecting two nouns
from an old utterance, its weight becomes small. In
the same way, the weights for the edges from recent
utterances get large. The weights of the edges from
G5 are 1, the largest possible value.
4 Graph Representation and Weight
Update
4.1 Current Utterance Graph and History
Graph
Current utterance graph is a graph-representation of
the current utterance. When current utterance con-
sists of m words, we first extract the potential key-
890
words from the current utterance. Since all words
within the current utterance are not keywords, some
words are filtered out. For this filtering out, we fol-
low the POS filtering approach proposed by Liu et
al. (2009). This approach filters out non-keywords
using a stop-word list and POS tags of the words.
Assume that n words remain after the filtering out,
where n ? m. These n words become the vertices
of the current utterance graph.
Formally, the current utterance graph G1 =
(V1, E1) is an undirected graph, where |V1| = n.
E1 is a set of edges and each edge implies that the
nouns connected by the edge co-occur within a win-
dow sizedW . For each e1ij ? E1 that connects nodes
v1i and v
1
j , its weight is given by
w1ij =
{
1 if v1i &v
1
j cooccur within the window,
0 otherwise.
(1)
In a meeting, preceding utterances affect the cur-
rent utterance. We assume that only the keywords
of preceding utterances are effective. Therefore, the
history graph G2 = (V2, E2) is an undirected graph
of keywords in the preceding utterances. That is,
all vertices in V2 are keywords extracted from one
or more previous utterances, and the edge between
two keywords implies that they co-occurred at least
once. Every edge in E2 has a weight that represents
its temporal importance.
The history graph is updated whenever keywords
are extracted from a new utterance. This is because
the current utterance becomes a part of the history
graph for the next utterance. As a history, old ut-
terances are less important than recent ones. Thus,
the temporal importance should decrease gradually
according to the passage of time. In addition, the
keywords which occur frequently at a meeting are
more important than those mentioned just once or
twice. Since the frequently-mentioned keywords are
normally major topics of the meeting, their influence
should last for a long time.
To model these characteristics, the forgetting
curve (Wozniak, 1999) is adopted in updating the
history graph. It models the decline of memory re-
tention in time. Figure 2 shows a typical represen-
tation of the forgetting curve. The X-axis of this
figure is time and the Y-axis is memory retention.
As shown in this figure, memory retention of new
Time
Me
mo
ry
Re
ten
tio
n
Figure 2: Memory retention according to time.
information decreases gradually by the exponential
nature of forgetting. However, whenever the infor-
mation is repeated, it is recalled longer. This is for-
mulated as
R = e?
t
S ,
where R is memory retention, t is time, and S is the
relative strength of memory.
Based on the forgetting curve, the weight of each
edge e2ij ? E2 between keywords v
2
i and v
2
j is set as
w2ij = exp
? tf(vi,vj) , (2)
where t is the elapse of utterance time and f(vi, vj)
is the frequency that vi and vj co-occur from the
beginning of the meeting to now. According to
this equation, the temporal importance between key-
words decreases gradually as time passes by, but the
keyword relations repeated during the meeting are
remembered for a long time in the history graph.
4.2 Keyword Extraction by Merging Current
Utterance and History Graphs
All words within the history graph are not equally
important in extracting keywords from the current
utterance. In general, many participants discuss a
wide range of topics in a meeting. Therefore, some
preceding utterances that shares topics with the cur-
rent utterance are more significant. We assume that
the preceding utterances that contain the nouns in
the current utterance share topics with the current
utterance. Thus, only a subgraph of G2 that contain
words in G1 is relevant for keyword extraction from
G1.
891
Given the current utterance graph G1 = (V1, E1)
and the history graph G2 = (V2, E2), the relevant
graph G3 = (V3, E3) is a subgraph of G2. Here,
V3 = (V1?V2)?adjacency(V1) and adjacency(V1)
is a set of vertices from G2 which are directly con-
nected to the words in V1. That is, V3 contains
the words of G1 and their direct neighbor words in
G2. E3 is a subset of E2. Only the edges that ap-
pear in E2 are included in E3. The weight w3ij of
each e3ij ? E3 is also borrowed from G2. That is,
w3ij = w
2
ij . Therefore, G3 is a 1-walk subgraph
1 of
G2 in which words in G1 and their neighbor words
appear.
The keywords of the current utterance should re-
flect the relevant history as well as the current utter-
ance itself. For this purpose, G1 is expanded with
respect to G3. The expanded graph G4 = (V4, E4)
of G1 is defined as
V4 = V1 ? V3,
E4 = E1 ? E3.
For each edge e4ij ? E4, its weightw
4
ij is determined
to be the larger value between w1ij and w
3
ij if it ap-
pears in both G1 and G3. When it appears in only
one of the graphs, w4ij is set to be the weight of its
corresponding graph. That is,
w4ij =
?
???
???
max(w1ij , w
3
ij) if e
4
ij ? E1 and e
4
ij ? E3,
w1ij if e
4
ij ? E1 and e
4
ij /? E3,
w3ij otherwise.
From this expanded graph G4, the keywords are
extracted by TextRank (Mihalcea and Tarau, 2004).
TextRank is an unsupervised graph-based method
for keyword extraction. It singles out the key ver-
tices of a graph by providing a ranking mechanism.
In order to rank the vertices, it computes the score
of each vertex v4i ? V4 by
S(v4i ) = (1? d)+ d ?
?
v4j?adj(v
4
i )
w4ji
?
v4k?adj(v
4
j )
w4jk
S(v4j ),
(3)
1If a m-walk subgraph (m > 1) is used, more affluent his-
tory is used. However, this graph contains some words irrel-
evant to the current utterance. According to our experiments,
1-walk subgraph outperforms other m-walk subgraphs where
m > 1. In addition, extracting G3 becomes expensive for large
m.
where 0 ? d ? 1 is a damping factor and adj(vi)
denotes vi?s neighbors. Finally, the words whose
score is larger than a specific threshold ? are cho-
sen as keywords. Especially when the current utter-
ance is the first utterance of a meeting, the history
graph does not exist. In this case, the current utter-
ance graph becomes the expanded graph (G4 = G1),
and keywords are extracted from the current utter-
ance graph.
The proposed method extracts keywords when-
ever an utterance is spoken. Thus, it tries to extract
keywords even if the current utterance is not related
to the topics of a meeting or is too short. However,
if the current utterance is irrelevant to the meeting,
it has just a few connections with other previous ut-
terances, and thus the potential keywords in this ut-
terance are apt to have a low score. The proposed
method, however, does not select the words whose
score is smaller than the threshold ? as keywords.
As a result, it extracts only the relevant keywords
during the meeting.
Since the keywords for the current utterance
should be the history for the next utterance, they
have to be reflected into the history graph. There-
fore, a keyword graph G5 = (V5, E5) is constructed
from the keywords. Here, V5 is a set of keywords
extracted from G4, and E5 is a subset of E4 that
corresponds to V5. The weights of edges in E5 are
same with those in E4. That is, w5ij = w
4
ij . The key-
word graph G5 is then merged into the history graph
G2 in the same way that G1 and G3 are merged. As
stated above, the weights of the edges in the history
graph G2 are updated by Equation (2). Therefore,
before merging G5 and G2, all weights of G2 are
updated by increasing t as t + 1 to reflect temporal
importance of preceding utterances.
5 Experiments
The proposed method is evaluated with two kinds of
data sets: the National Assembly transcripts in Ko-
rean and the ICSI meeting corpus in English. Both
data sets are the records of meetings that are manu-
ally dictated by human transcribers.
892
Table 1: Simple statistics of the National Assembly transcripts
the first meeting the second meeting
No. of utterances 1,280 573
Average No. of words per utterance 7.22 10.17
5.1 National Assembly Transcripts in Korean
The first corpus used to evaluate our method is the
National Assembly transcripts2. This corpus is ob-
tained from the Knowledge Management System
of the National Assembly of KoreaIt is transcribed
from the 305th assembly record of the Knowledge
Economy Committee in 2012. Table 1 summa-
rizes simple statistics of the National Assembly tran-
scripts. The 305th assembly record actually consists
of two meetings. The first meeting contains 1,280
utterances and the second has 573 utterances. The
average number of words per utterance in the first
meeting is 7.22 while the second meeting contains
10.17 words per utterance on average. The second
meeting transcript is used as a development data set
to determine window size W of Equation (1), the
damping factor d of Equation (3), and the threshold
?. For all experiments below, d is set 0.85, W is 10,
and ? is 0.28. The remaining first meeting transcript
is used as a data set to extract keywords since this
transcript contains more utterances. Only nouns are
considered as potential keywords. That is, only the
words whose POS tag is NNG (common noun) or
NNP (proper noun) can be a keyword.
Three annotators are engaged to extract keywords
manually for each utterance in the first meeting
transcript, since the Knowledge Management Sys-
tem does not provide the keywords3. The aver-
age number of keywords per utterance is 2.58. To
see the inter-judge agreement among the annotators,
the Kappa coefficient (Carletta, 1996) was investi-
gated. The kappa agreement of the National Assem-
bly transcript is 0.31 that falls under the category of
?Fair?. Even though all congressmen in the transcript
belong to the same committee, they discussed vari-
ous topics at the meeting. As a result, the keywords
are difficult to be agreed unanimously by all three
2The data set is available: http://ml.knu.ac.kr/
dataset/keywordextraction.html
3A guideline was given to the annotators that keywords must
be a single word and the maximum number of keywords per
utterance is five.
annotators. Therefore, in this paper the words that
are recommended by more than two annotators are
chosen as keywords.
The evaluation is done with two metics: F-
measure and the weighted relative score (WRS).
Since the previous work by Liu et al (2009) re-
ported only F-measure and WRS, F-measure instead
of precision/recall are used for the comparison with
their method. The weighted relative score is de-
rived from Pyramid metric (Nenkova and Passon-
neau, 2004). When a keyword extraction system
generates keywords which many annotators agree,
a higher score is given to it. On the other hand, a
lower score is given if fewer annotators agree.
The proposed method is compared with two base-
line models to see its relative performance. One is
the frequency-based keyword extraction with TFIDF
weighting (Frank et al, 1999) and the other is Tex-
tRank in which the weight of edges is mutual in-
formation between vertices (Wan et al, 2007). In
TFIDF, each utterance is considered as a document,
and thus all utterances including the current one
are regarded as whole documents. The frequency-
based TFIDF chooses top-K words according to
their TFIDF value from the set of words appearing in
the meeting transcript. Since the human annotators
are restricted to extract up to five keywords, the key-
word extraction systems including our method are
also requested to select top-5 keywords when more
than five keywords are produced.
In order to see the effect of preceding utterances in
baseline models, the performances are measured ac-
cording to the number of preceding utterances used.
Figure 3 shows the results. The X-axis of this fig-
ure is the number of preceding utterances and the Y-
axis represents F-measures. As shown in this figure,
the performance of the baseline models improves
monotonically at first as the number of preceding
utterances increases. However, the performance im-
provement stops when many preceding utterances
are involved, and the performance begins to drop
893
Figure 3: The performance of baseline models according
to the number of preceding utterances
Table 2: The experimental results on the National Assem-
bly transcripts
Methods F-measure WRS
TextRank 0.478 0.387
TFIDF 0.481 0.394
Proposed method 0.533 0.421
when too many utterances are considered. The per-
formance of TextRank model drops from 20 preced-
ing utterances, while that of TFIDF model begins to
drops at 50 utterances. When too many preceding
utterances are taken into account, it is highly pos-
sible that some of their topics are irrelevant to the
current utterance, which leads to performance drop.
Table 2 compares our method with the baseline
models on the National Assembly transcripts. The
performances of baseline models are obtained when
they show the best performance for various number
of preceding utterances. TextRank model achieves
F-measure of 0.478 and weighted relative score of
0.387, while TFIDF reports its best F-measure of
0.481 and weighted relative score of 0.394. Thus,
the difference between TFIDF and TextRank is not
significant. However, F-measure and weighted rel-
ative score of the proposed method are 0.533 and
0.421 respectively, and they are much higher than
those of baseline models. In addition, our method
achieves precision of 0.543 and recall of 0.523 and
Table 3: The importance of temporal history
F-measure WRS
With Temporal History 0.533 0.421
Without Temporal History 0.518 0.413
this is much higher performance than TextRank
whose precision is just 0.510. Since the proposed
method uses, as history, the preceding utterances
relevant to the current utterance, its performance is
kept high even if whole utterances are used. There-
fore, it could be inferred that it is important to adopt
only the relevant history in keyword extraction from
meeting transcripts.
One of the key factors of our method is the tem-
poral history. Its importance is given in Table 3. As
explained above, the temporal history is achieved by
Equation (2). Thus, the proposed model does not
reflect the temporal importance of preceding utter-
ances if w2ij = 1 always. That is, under w
2
ij = 1,
old utterances are regarded as important as recent ut-
terances. Without temporal history, F-measure and
weighted relative score are just 0.518 and 0.413 re-
spectively. These poor performances prove the im-
portance of the temporal history in keyword extrac-
tion from meeting transcripts.
5.2 ICSI Meeting Corpus in English
The proposed method is also evaluated on the ICSI
meeting corpus (Janin et al, 2003) which consists of
naturally occurring meetings recordings. This cor-
pus is widely used for summarizing and extracting
keywords of meetings. We followed all the exper-
imental settings proposed by Liu et al (2009) for
this corpus. Among 26 meeting transcripts chosen
by Liu et al from 161 transcripts of the ICSI meet-
ing corpus, 6 transcripts are used as development
data and the remaining transcripts are used as data
to extract keywords. The parameters for the ICSI
meeting corpus are set to be d = 0.85,W = 10,
and ? = 0.20. Each meeting of the corpus consists
of several topic segments, and every topic segment
contains three sets of keywords that are annotated by
three annotators. Up to five keywords are annotated
for a topic segment.
Table 4 shows simple statistics of the ICSI meet-
ing data. Total number of topic segments in the 26
meetings is originally 201, but some of them do not
894
Table 4: Simple statistics of the ICSI meeting data
Information Value
# of meetings 26
# of topic segments 201
# of topic segments used actually 140
Average # of utterances per topic segment 260
Average # of words per utterance 7.22
Table 5: The experimental results on the ICSI corpus
Methods F-measure WRS
TFIDF-Liu 0.290 0.404
TextRank-Liu 0.277 0.380
ME model 0.312 0.401
Proposed method 0.334 0.533
have keywords. Such segments are discarded, and
the remaining 140 topic segments are actually used.
The average number of utterances in a topic segment
is 260 and the average number of words per utter-
ance is 7.22.
Unlike the National Assembly transcripts, the
keywords of the ICSI meeting corpus are annotated
at the topic segment level, not the utterance level.
Therefore, the proposed method which extracts key-
words at the utterance level can not be applied di-
rectly to this corpus. In order to obtain keywords
for a topic segment with the proposed method, the
keywords are first extracted from each utterance in
the segment by the proposed method and then they
are all accumulated. The proposed method extracts
keywords for a topic segment from these accumu-
lated utterance-level keywords as follows. Assume
that a topic segment consists of l utterances. Since
our method can extract up to 5 keywords for each
utterance, the number of keywords for the segment
can reach to 5 ? l. From these keywords, we select
top-5 keywords ranked by Equation (3).
The proposed method is compared with three pre-
vious studies. The first two are the methods pro-
posed by Liu et al (2009) One is the frequency-
based method of TFIDF weighting with the fea-
tures such as POS filtering, word clustering, and sen-
tence salience score, and the other is the graph-based
method with POS filtering. The last method is a
maximum entropy model applied to this task (Liu
et al, 2008). Note that the maximum entropy is a
supervised learning model.
Table 6: The effect of considering topic relevance
Methods F-measure WRS
With topic relevance 0.334 0.533
Without topic relevance 0.291 0.458
Table 5 summarizes the comparison results. As
shown in this table, the proposed method outper-
forms all previous methods. Our method achieves
precision of 0.311 and recall of 0.361, and thus
the F-score is 0.334. The weight relative score
of the proposed method is 0.533. This is the im-
provement of up to 0.044 in F-measure and 0.129
in weighted relative score over other unsupervised
methods (TFIDF-Liu and TextRank-Liu). It should
be also noted that the proposed method outperforms
even the supervised method (ME model). The differ-
ence between our method and the maximum entropy
model in weighted relative score is 0.132.
One possible variant of the proposed method for
the ICSI corpus is to simply merge the current utter-
ance graph (G1) with the history graph (G2) rather
than to extract keywords from each utterance. Af-
ter the current utterance graph of the last utterance
in a topic segment is merged into the history graph,
the keywords for the segment are extracted from the
history graph. This variant and the proposed method
both rely on the temporal history, but the difference
is that the history graph of the variant accumulates
all information within the topic segment. Thus, the
keywords extracted from the history graph by this
variant are those without consideration of topic rel-
evance.
Table 6 compares the proposed method with the
variant. The performance of the variant is higher
than those of TFIDF-Liu and TextRank-Liu. This
proves the importance of the temporal history in
keyword extraction from meeting transcripts. How-
ever, the proposed method still outperforms the vari-
ant, and it demonstrates the importance of topic rel-
evance. Therefore, it can be concluded that the con-
sideration of temporal history and topic relevance
is critical in keyword extraction from meeting tran-
scripts.
895
6 Conclusion
In this paper, we have proposed a just-in-time key-
word extraction from meeting transcripts. Whenever
an utterance is spoken, the proposed method extracts
keywords from the utterance that best describe the
utterance. Based on the graph representation of all
components in a meeting, the proposed method ex-
tracts keywords by TextRank with some graph oper-
ations.
Temporal history and topic of the current utter-
ance are two major factors especially in keyword ex-
traction from meeting transcripts. This is because re-
cent utterances are more important than old ones and
only the preceding utterances of which topic is rele-
vant to the current utterance are important. To model
the temporal importance of the preceding utterances,
the concept of forgetting curve is used in updating
the history graph of preceding utterances. In addi-
tion, the subgraph of the history graph that shares
words appearing in the current utterance graph is
used to extract keywords rather than whole history
graph. The proposed method was evaluated with the
National Assembly transcripts and the ICSI meeting
corpus. According to our experimental results on
these data sets, the performance of keyword extrac-
tion is improved when we consider temporal history
and topic relevance.
Acknowledgments
This research was supported by the Converging Re-
search Center Program funded by the Ministry of
Education, Science and Technology (2012K001342)
References
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: The kappa statistic. Computational Lin-
guistics, 22(2):249?254.
Yun-Nung Chen, Yu Huang, Sheng-Yi Kong, , and Lin-
Shan Lee. 2010. Automatic key term extraction from
spoken course lectures using branching entropy and
prosodic/semantic features. In Proceedings of IEEE
Workshop on Spoken Language Technology, pages
265?270.
Eibe Frank, Gordon W. Paynter, Ian H. Witten, Carl
Gutwin, and Craig G. Nevill-Manning. 1999.
Domain-specific keyphrase extraction. In Proceedings
of the 18th International Joint Conference on Artificial
intelligence, pages 668?671.
Anette Hulth. 2003. Improved automatic keyword ex-
traction given more linguistic knowledge. In Proceed-
ings of International Conference on Empirical Meth-
ods in Natural Language Processing, pages 216?223.
Adam Janin, Don Baron, Jane Edwards, Dan Ellis,
David Gelbart, Nelson Morgan, Barbara Peskin, Thilo
Pfau, Elizabeth Shriberg, Andreas Stolcke, and Chuck
Wooters. 2003. The icsi meeting corpus. In Proceed-
ings of International Conference on Acoustics, Speech,
and Signal Processing, pages 364?367.
Fei Liu, Feifan Liu, and Yang Liu. 2008. Automatic key-
word extraction for the meeting corpus using super-
vised approach and bigram expansion. In Proceedings
of IEEE Spoken Language Technology, pages 181?
184.
Feifan Liu, Deana Pennell, Fei Liu, and Yang Liu. 2009.
Unsupervised approaches for automatic keyword ex-
traction using meeting transcripts. In Proceedings of
Annual Conference of the North American Chapter of
the ACL, pages 620?628.
Fei Liu, Feifan Liu, and Yang Liu. 2011. A super-
vised framework for keyword extraction from meeting
transcripts. IEEE Transactions on Audio, Speech, and
Language Processing, 19(3):538?548.
Rada Mihalcea and Paul Tarau. 2004. Textrank: Bring-
ing order into texts. In Proceedings of International
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 404?411.
Ani Nenkova and Rebecca Passonneau. 2004. Evaluat-
ing content selection in summarization: The pyramid
method. In Proceedings of Annual Conference of the
North American Chapter of the ACL, pages 145?152.
Peter D. Turney. 2000. Learning algorithms for
keyphrase extraction. Information Retrieval, 2:303?
336.
Peter D. Turney. 2003. Coherent keyphrase extrac-
tion via web mining. In Proceedings of the 18th In-
ternational Joint Conference on Artificial intelligence,
pages 434?439.
Xiaojun Wan and Jianguo Xiao. 2008. Collabrank:
Towards a collaborative approach to single-document
keyphrase extraction. In Proceedings of International
Conference on Computational Linguistics, pages 969?
976.
Xiaojun Wan, Jianwu Yang, and Jianguo Xiao. 2007. To-
wards an iterative reinforcement approach for simulta-
neous document summarization and keyword extrac-
tion. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 552?
559.
Robert H. Wozniak. 1999. Classics in Psychology,
1855?1914: Historical Essays. Thoemmes Press.
896
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1025?1034,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Cost Sensitive Part-of-Speech Tagging:
Differentiating Serious Errors from Minor Errors
Hyun-Je Song1 Jeong-Woo Son1 Tae-Gil Noh2 Seong-Bae Park1,3 Sang-Jo Lee1
1School of Computer Sci. & Eng. 2Computational Linguistics 3NLP Lab.
Kyungpook Nat?l Univ. Heidelberg University Dept. of Computer Science
Daegu, Korea Heidelberg, Germany University of Illinois at Chicago
{hjsong,jwson,tgnoh}@sejong.knu.ac.kr sbpark@uic.edu sjlee@knu.ac.kr
Abstract
All types of part-of-speech (POS) tagging er-
rors have been equally treated by existing tag-
gers. However, the errors are not equally im-
portant, since some errors affect the perfor-
mance of subsequent natural language pro-
cessing (NLP) tasks seriously while others do
not. This paper aims to minimize these serious
errors while retaining the overall performance
of POS tagging. Two gradient loss functions
are proposed to reflect the different types of er-
rors. They are designed to assign a larger cost
to serious errors and a smaller one to minor
errors. Through a set of POS tagging exper-
iments, it is shown that the classifier trained
with the proposed loss functions reduces se-
rious errors compared to state-of-the-art POS
taggers. In addition, the experimental result
on text chunking shows that fewer serious er-
rors help to improve the performance of sub-
sequent NLP tasks.
1 Introduction
Part-of-speech (POS) tagging is needed as a pre-
processor for various natural language processing
(NLP) tasks such as parsing, named entity recogni-
tion (NER), and text chunking. Since POS tagging is
normally performed in the early step of NLP tasks,
the errors in POS tagging are critical in that they
affect subsequent steps and often lower the overall
performance of NLP tasks.
Previous studies on POS tagging have shown
high performance with machine learning techniques
(Ratnaparkhi, 1996; Brants, 2000; Lafferty et al,
2001). Among the types of machine learning ap-
proaches, supervised machine learning techniques
were commonly used in early studies on POS tag-
ging. With the characteristics of a language (Rat-
naparkhi, 1996; Kudo et al, 2004) and informa-
tive features for POS tagging (Toutanova and Man-
ning, 2000), the state-of-the-art supervised POS tag-
ging achieves over 97% of accuracy (Shen et al,
2007; Manning, 2011). This performance is gen-
erally regarded as the maximum performance that
can be achieved by supervised machine learning
techniques. There have also been many studies on
POS tagging with semi-supervised (Subramanya et
al., 2010; S?gaard, 2011) or unsupervised machine
learning methods (Berg-Kirkpatrick et al, 2010;
Das and Petrov, 2011) recently. However, there still
exists room to improve supervised POS tagging in
terms of error differentiation.
It should be noted that not all errors are equally
important in POS tagging. Let us consider the parse
trees in Figure 1 as an example. In Figure 1(a),
the word ?plans? is mistagged as a noun where it
should be a verb. This error results in a wrong parse
tree that is severely different from the correct tree
shown in Figure 1(b). The verb phrase of the verb
?plans? in Figure 1(b) is discarded in Figure 1(a)
and the whole sentence is analyzed as a single noun
phrase. Figure 1(c) and (d) show another tagging er-
ror and its effect. In Figure 1(c), a noun is tagged as
a NNS (plural noun) where its correct tag is NN (sin-
gular or mass noun). However, the error in Figure
1(c) affects only locally the noun phrase to which
?physics? belongs. As a result, the general structure
of the parse tree in Figure 1(c) is nearly the same as
1025
SVP
VP
NP
The treasury 
to
raise 150 billion in cash.
DT NNP
TO
VB CD CD IN NN
S
plans
NNS
(a) A parse tree with a serious error.
S
VPNP
The   treasury 
DT NNP
S
VP
VPto
raise 150 billion in cash.
TO
VB CD CD IN NN
plans
VBZ
(b) The correct parse tree of the sentence?The treasury
plans . . .?.
S
NP VP
We
PRP
altered
VBN
NP
NP PP
the chemistry and physics
DT
of the atmosphere
NN CC NNS INDT NN
(c) A parse tree with a minor error.
S
NP VP
We
PRP
altered
VBN
NP
NP PP
the chemistry and physics
DT
of the atmosphere
NN CC NN INDT NN
(d) The correct parse tree of the sentence ?We altered
. . .?.
Figure 1: An example of POS tagging errors
the correct one in Figure 1(d). That is, a sentence
analyzed with this type of error would yield a cor-
rect or near-correct result in many NLP tasks such
as machine translation and text chunking.
The goal of this paper is to differentiate the seri-
ous POS tagging errors from the minor errors. POS
tagging is generally regarded as a classification task,
and zero-one loss is commonly used in learning clas-
sifiers (Altun et al, 2003). Since zero-one loss con-
siders all errors equally, it can not distinguish error
types. Therefore, a new loss is required to incorpo-
rate different error types into the learning machines.
This paper proposes two gradient loss functions to
reflect differences among POS tagging errors. The
functions assign relatively small cost to minor er-
rors, while larger cost is given to serious errors.
They are applied to learning multiclass support vec-
tor machines (Tsochantaridis et al, 2004) which is
trained to minimize the serious errors. Overall accu-
racy of this SVM is not improved against the state-
of-the-art POS tagger, but the serious errors are sig-
nificantly reduced with the proposed method. The
effect of the fewer serious errors is shown by apply-
ing it to the well-known NLP task of text chunking.
Experimental results show that the proposed method
achieves a higher F1-score compared to other POS
taggers.
The rest of the paper is organized as follows. Sec-
tion 2 reviews the related studies on POS tagging. In
Section 3, serious and minor errors are defined, and
it is shown that both errors are observable in a gen-
eral corpus. Section 4 proposes two new loss func-
tions for discriminating the error types in POS tag-
ging. Experimental results are presented in Section
5. Finally, Section 6 draws some conclusions.
2 Related Work
The POS tagging problem has generally been solved
by machine learning methods for sequential label-
1026
Tag category POS tags
Substantive NN, NNS, NNP, NNPS, CD, PRP, PRP$
Predicate VB, VBD, VBG, VBN, VBP, VBZ, MD, JJ, JJR, JJS
Adverbial RB, RBR, RBS, RP, UH, EX, WP, WP$, WRB, CC, IN, TO
Determiner DT, PDT, WDT
Etc FW, SYM, POS, LS
Table 1: Tag categories and POS tags in Penn Tree Bank tag set
ing. In early studies, rich linguistic features and su-
pervised machine learning techniques are applied by
using annotated corpora like the Wall Street Journal
corpus (Marcus et al, 1994). For instance, Ratna-
parkhi (1996) used a maximum entropy model for
POS tagging. In this study, the features for rarely
appearing words in a corpus are expanded to im-
prove the overall performance. Following this direc-
tion, various studies have been proposed to extend
informative features for POS tagging (Toutanova
and Manning, 2000; Toutanova et al, 2003; Man-
ning, 2011). In addition, various supervised meth-
ods such as HMMs and CRFs are widely applied to
POS tagging. Lafferty et al (2001) adopted CRFs
to predict POS tags. The methods based on CRFs
not only have all the advantages of the maximum
entropy markov models but also resolve the well-
known problem of label bias. Kudo et al (2004)
modified CRFs for non-segmented languages like
Japanese which have the problem of word boundary
ambiguity.
As a result of these efforts, the performance of
state-of-the-art supervised POS tagging shows over
97% of accuracy (Toutanova et al, 2003; Gime?nez
and Ma`rquez, 2004; Tsuruoka and Tsujii, 2005;
Shen et al, 2007; Manning, 2011). Due to the high
accuracy of supervised approaches for POS tagging,
it has been deemed that there is no room to im-
prove the performance on POS tagging in supervised
manner. Thus, recent studies on POS tagging focus
on semi-supervised (Spoustova? et al, 2009; Sub-
ramanya et al, 2010; S?gaard, 2011) or unsuper-
vised approaches (Haghighi and Klein, 2006; Gold-
water and Griffiths, 2007; Johnson, 2007; Graca et
al., 2009; Berg-Kirkpatrick et al, 2010; Das and
Petrov, 2011). Most previous studies on POS tag-
ging have focused on how to extract more linguistic
features or how to adopt supervised or unsupervised
approaches based on a single evaluation measure,
accuracy. However, with a different viewpoint for
errors on POS tagging, there is still some room to
improve the performance of POS tagging for subse-
quent NLP tasks, even though the overall accuracy
can not be much improved.
In ordinary studies on POS tagging, costs of er-
rors are equally assigned. However, with respect
to the performance of NLP tasks relying on the re-
sult of POS tagging, errors should be treated differ-
ently. In the machine learning community, cost sen-
sitive learning has been studied to differentiate costs
among errors. By adopting different misclassifica-
tion costs for each type of errors, a classifier is op-
timized to achieve the lowest expected cost (Elkan,
2001; Cai and Hofmann, 2004; Zhou and Liu, 2006).
3 Error Analysis of Existing POS Tagger
The effects of POS tagging errors to subsequent
NLP tasks vary according to their type. Some errors
are serious, while others are not. In this paper, the
seriousness of tagging errors is determined by cat-
egorical structures of POS tags. Table 1 shows the
Penn tree bank POS tags and their categories. There
are five categories in this table: substantive, pred-
icate, adverbial, determiner, and etc. Serious tag-
ging errors are defined as misclassifications among
the categories, while minor errors are defined as mis-
classifications within a category. This definition fol-
lows the fact that POS tags in the same category
form similar syntax structures in a sentence (Zhao
and Marcus, 2009). That is, inter-category errors are
treated as serious errors, while intra-category errors
are treated as minor errors.
Table 2 shows the distribution of inter-category
and intra-category errors observed in section 22?
24 of the WSJ corpus (Marcus et al, 1994) that is
tagged by the Stanford Log-linear Part-Of-Speech
1027
Predicted category
Substantive Predicate Adverbial Determiner Etc
Substantive 614 479 32 10 15
Predicate 585 743 107 2 14
True category Adverbial 41 156 500 42 2
Determiner 13 7 47 24 0
Etc 23 11 3 1 0
Table 2: The distribution of tagging errors on WSJ corpus by Stanford Part-Of-Speech Tagger.
Tagger (Manning, 2011) (trained with WSJ sections
00?18). In this table, bold numbers denote inter-
category errors while all other numbers show intra-
category errors. The number of total errors is 3,471
out of 129,654 words. Among them, 1,881 errors
(54.19%) are intra-category, while 1,590 of the er-
rors (45.81%) are inter-category. If we can reduce
these inter-category errors under the cost of mini-
mally increasing intra-category errors, the tagging
results would improve in quality.
Generally in POS tagging, all tagging errors are
regarded equally in importance. However, inter-
category and intra-category errors should be distin-
guished. Since a machine learning method is opti-
mized by a loss function, inter-category errors can
be efficiently reduced if a loss function is designed
to handle both types of errors with different cost. We
propose two loss functions for POS tagging and they
are applied to multiclass Support Vector Machines.
4 Learning SVMs with Class Similarity
POS tagging has been solved as a sequential labeling
problem which assumes dependency among words.
However, by adopting sequential features such as
POS tags of previous words, the dependency can be
partially resolved. If it is assumed that words are
independent of one another, POS tagging can be re-
garded as a multiclass classification problem. One
of the best solutions for this problem is by using an
SVM.
4.1 Training SVMs with Loss Function
Assume that a training data set D =
{(x1, y1), (x2, y2), . . . , (xl, yl)} is given where
xi ? Rd is an instance vector and yi ? {+1,?1}
is its class label. SVM finds an optimal hyperplane
satisfying
xi ? w + b ? +1 for yi = +1,
xi ? w + b ? ?1 for yi = ?1,
where w and b are parameters to be estimated from
training data D. To estimate the parameters, SVMs
minimizes a hinge loss defined as
?i = Lhinge(yi, w ? xi + b)
= max{0, 1 ? yi ? (w ? xi + b)}.
With regularizer ||w||2 to control model complexity,
the optimization problem of SVMs is defined as
min
w,?
1
2
||w||2 + C
l
?
i=1
?i,
subject to
yi(xi ? w + b) ? 1? ?i, and ?i ? 0 ?i,
where C is a user parameter to penalize errors.
Crammer et al (2002) expanded the binary-class
SVM for multiclass classifications. In multiclass
SVMs, by considering all classes the optimization
of SVM is generalized as
min
w,?
1
2
?
k?K
||wk||2 + C
l
?
i=1
?i,
with constraints
(wyi ? ?(xi, yi))? (wk ? ?(xi, k)) ? 1? ?i,
?i ? 0 ?i, ?k ? K \ yi,
where ?(xi, yi) is a combined feature representation
of xi and yi, and K is the set of classes.
1028
POS
SUBSTANTIVE
PREDICATE ADVERBIAL
OTHERS
NOUN
PRONOUN
DETERMINER
DT
PDT
NNS
NN NNP
NNPS
CD
PRP PRP$
VERB
VBD
VB
VBG
VBN
VBP
VBZ
MD
ADJECT
JJR
JJ JJS
SYM
FW POS
LS
ADVERB
WH- CONJUNCTION
RBR
RB RBS
RP
UH
EX
WP
WP$
WRB
IN
CC TO
WDT
Figure 2: A tree structure of POS tags.
Since both binary and multiclass SVMs adopt a
hinge loss, the errors between classes have the same
cost. To assign different cost to different errors,
Tsochantaridis et al (2004) proposed an efficient
way to adopt arbitrary loss function, L(yi, yj) which
returns zero if yi = yj , otherwise L(yi, yj) > 0.
Then, the hinge loss ?i is re-scaled with the inverse
of the additional loss between two classes. By scal-
ing slack variables with the inverse loss, margin vi-
olation with high loss L(yi, yj) is more severely re-
stricted than that with low loss. Thus, the optimiza-
tion problem with L(yi, yj) is given as
min
w,?
1
2
?
k?K
||wk||2 + C
l
?
i=1
?i, (1)
with constraints
(wyi ? ?(xi, yi))? (wk ? ?(xi, k)) ? 1?
?i
L(yi, k)
,
?i ? 0 ?i, ?k ? K \ yi,
With the Lagrange multiplier ?, the optimization
problem in Equation (1) is easily converted to the
following dual quadratic problem.
min
?
1
2
l
?
i,j
?
ki?K\yi
?
kj?K\yj
?i,ki?j,kj ?
J(xi, yi, ki)J(xj , yj, kj)?
l
?
i
?
ki?K\yi
?i,ki ,
with constraints
? ? 0 and
?
ki?K\yi
?i,ki
L(yi, ki)
? C, ?i = 1, ? ? ? , l,
where J(xi, yi, ki) is defined as
J(xi, yi, ki) = ?(xi, yi)? ?(xi, ki).
4.2 Loss Functions for POS tagging
To design a loss function for POS tagging, this paper
adopts categorical structures of POS tags. The sim-
plest way to reflect the structure of POS tags shown
in Table 1 is to assign larger cost to inter-category
errors than to intra-category errors. Thus, the loss
function with the categorical structure in Table 1 is
defined as
Lc(yi, yj) =
?
?
?
?
?
?
?
0 if yi = yj ,
? if yi 6= yj but they belong
to the same POS category,
1 otherwise,
(2)
where 0 < ? < 1 is a constant to reduce the value of
Lc(yi, yj) when yi and yj are similar. As shown in
this equation, inter-category errors have larger cost
than intra-category errors. This loss Lc(yi, yj) is
named as category loss.
The loss function Lc(yi, yj) is designed to reflect
the categories in Table 1. However, the structure
of POS tags can be represented as a more complex
structure. Let us consider the category, predicate.
1029
?
Class NN Class NNS
Class VB
(a) Multiclass SVMs with hinge loss
Class NN Class NNS
Class VB
?
L(NN, VB)
?
L(NN, NNS)
(b) Multiclass SVMs with the proposed loss
function
Figure 3: Effect of the proposed loss function in multiclass SVMs
This category has ten POS tags, and can be further
categorized into two sub-categories: verb and ad-
ject. Figure 2 represents a categorical structure of
POS tags as a tree with five categories of POS tags
and their seven sub-categories.
To express the tree structure of Figure 2 as a loss,
another loss function Lt(yi, yj) is defined as
Lt(yi, yj) =
1
2
[Dist(Pi,j , yi) +Dist(Pi,j, yj)]? ?, (3)
where Pi,j denotes the nearest common parent of
both yi and yj , and the function Dist(Pi,j, yi) re-
turns the number of steps from Pi,j to yi. The user
parameter ? is a scaling factor of a unit loss for a
single step. This loss Lt(yi, yj) returns large value
if the distance between yi and yj is far in the tree
structure, and it is named as tree loss.
As shown in Equation (1), two proposed loss
functions adjust margin violation between classes.
They basically assign less value for intra-category
errors than inter-category errors. Thus, a classi-
fier is optimized to strictly keep inter-category er-
rors within a smaller boundary. Figure 3 shows a
simple example. In this figure, there are three POS
tags and two categories. NN (singular or mass noun)
and NNS (plural noun) belong to the same cate-
gory, while VB (verb, base form) is in another cat-
egory. Figure 3(a) shows the decision boundary of
NN based on hinge loss. As shown in this figure, a
single ? is applied for the margin violation among
all classes. Figure 3(b) also presents the decision
boundary of NN, but it is determined with the pro-
posed loss function. In this figure, the margin vio-
lation is applied differently to inter-category (NN to
VB) and intra-category (NN to NNS) errors. It re-
sults in reducing errors between NN and VB even if
the errors between NN and NNS could be slightly
increased.
5 Experiments
5.1 Experimental Setting
Experiments are performed with a well-known stan-
dard data set, the Wall Street Journal (WSJ) corpus.
The data is divided into training, development and
test sets as in (Toutanova et al, 2003; Tsuruoka and
Tsujii, 2005; Shen et al, 2007). Table 3 shows some
simple statistics of these data sets. As shown in
this table, training data contains 38,219 sentences
with 912,344 words. In the development data set,
there are 5,527 sentences with about 131,768 words,
those in the test set are 5,462 sentences and 129,654
words. The development data set is used only to se-
lect ? in Equation (2) and ? in Equation (3).
Table 4 shows the feature set for our experiments.
In this table, wi and ti denote the lexicon and POS
tag for the i-th word in a sentence respectively. We
use almost the same feature set as used in (Tsuruoka
and Tsujii, 2005) including word features, tag fea-
1030
Training Develop Test
Section 0?18 19?21 22?24
# of sentences 38,219 5,527 5,462
# of terms 912,344 131,768 129,654
Table 3: Simple statistics of experimental data
Feature Name Description
Word features wi?2, wi?1, wi, wi+1, wi+2wi?1 ? wi, wi ? wi+1
Tag features
ti?2, ti?1, ti+1, ti+2
ti?2 ? ti?1, ti+1 ? ti+2
ti?2 ? ti?1 ? ti+1, ti?1 ? ti+1 ? ti+2
ti?2 ? ti?1 ? ti+1 ? ti+2
Tag/Word
combination
ti?2?wi, ti?1 ?wi, ti+1?wi, ti+2?wi
ti?1 ? ti+1 ? wi
Prefix features prefixes of wi (up to length 9)
Suffix features suffixes of wi (up to length 9)
Lexical features
whether wi contains capitals
whether wi has a number
whether wi has a hyphen
whether wi is all capital
whether wi starts with capital and
locates at the middle of sentence
Table 4: Feature template for experiments
tures, word/tag combination features, prefix and suf-
fix features as well as lexical features. The POS tags
for words are obtained from a two-pass approach
proposed by Nakagawa et al (2001).
In the experiments, two multiclass SVMs with the
proposed loss functions are used. One is CL-MSVM
with category loss and the other is TL-MSVM with
tree loss. A linear kernel is used for both SVMs.
5.2 Experimental Results
CL-MSVM with ? = 0.4 shows the best overall per-
formance on the development data where its error
rate is as low as 2.71%. ? = 0.4 implies that the
cost of intra-category errors is set to 40% of that of
inter-category errors. The error rate of TL-MSVM
is 2.69% when ? is 0.6. ? = 0.4 and ? = 0.6 are set
in the all experiments below.
Table 5 gives the comparison with the previous
work and proposed methods on the test data. As can
be seen from this table, the best performing algo-
rithms achieve near 2.67% error rate (Shen et al,
2007; Manning, 2011). CL-MSVM and TL-MSVM
Error
(%)
# of Intra
error
# of Inter
error
(Gime?nez and Ma`rquez,
2004) 2.84
1,995
(54.11%)
1,692
(45.89%)
(Tsuruoka and Tsujii,
2005) 2.85 - -
(Shen et al, 2007) 2.67 1,856(53.52%)
1,612
(46.48%)
(Manning, 2011) 2.68 1,881(54.19%)
1,590
(45.81%)
CL-MSVM (? = 0.4) 2.69 1,916(55.01%)
1,567
(44.99%)
TL-MSVM (? = 0.6) 2.68 1,904(54.74%)
1,574
(45.26%)
Table 5: Comparison with the previous works
achieve an error rate of 2.69% and 2.68% respec-
tively. Although overall error rates of CL-MSVM
and TL-MSVM are not improved compared to the
previous state-of-the-art methods, they show reason-
able performance.
For inter-category error, CL-MSVM achieves the
best performance. The number of inter-category er-
ror is 1,567, which shows 23 errors reduction com-
pared to previous best inter-category result by (Man-
ning, 2011). TL-MSVM also makes 16 less inter-
category errors than Manning?s tagger. When com-
pared with Shen?s tagger, both CL-MSVM and TL-
MSVM make far less inter-category errors even if
their overall performance is slightly lower than that
of Shen?s tagger. However, the intra-category er-
ror rate of the proposed methods has some slight
increases. The purpose of proposed methods is to
minimize inter-category errors but preserving over-
all performance. From these results, it can be found
that the proposed methods which are trained with the
proposed loss functions do differentiate serious and
minor POS tagging errors.
5.3 Chunking Experiments
The task of chunking is to identify the non-recursive
cores for various types of phrases. In chunking, the
POS information is one of the most crucial aspects in
identifying chunks. Especially inter-category POS
errors seriously affect the performance of chunking
because they are more likely to mislead the chunk
compared to intra-category errors.
Here, chunking experiments are performed with
1031
POS tagger Accuracy (%) Precision Recall F1-score
(Shen et al, 2007) 96.08 94.03 93.75 93.89
(Manning, 2011) 96.08 94 93.8 93.9
CL-MSVM (? = 0.4) 96.13 94.1 93.9 94.00
TL-MSVM (? = 0.6) 96.12 94.1 93.9 94.00
Table 6: The experimental results for chunking
a data set provided for the CoNLL-2000 shared
task. The training data contains 8,936 sentences
with 211,727 words obtained from sections 15?18
of the WSJ. The test data consists of 2,012 sentences
and 47,377 words in section 20 of the WSJ. In order
to represent chunks, an IOB model is used, where
every word is tagged with a chunk label extended
with B (the beginning of a chunk), I (inside a chunk),
and O (outside a chunk). First, the POS informa-
tion in test data are replaced to the result of our POS
tagger. Then it is evaluated using trained chunking
model. Since CRFs (Conditional Random Fields)
has been shown near state-of-the-art performance in
text chunking (Fei Sha and Fernando Pereira, 2003;
Sun et al, 2008), we use CRF++, an open source
CRF implementation by Kudo (2005), with default
feature template and parameter settings of the pack-
age. For simplicity in the experiments, the values
of ? in Equation (2) and ? in Equation (3) are set
to be 0.4 and 0.6 respectively which are same as the
previous section.
Table 6 gives the experimental results of text
chunking according to the kinds of POS taggers in-
cluding two previous works, CL-MSVM, and TL-
MSVM. Shen?s tagger and Manning?s tagger show
nearly the same performance. They achieve an ac-
curacy of 96.08% and around 93.9 F1-score. On the
other hand, CL-MSVM achieves 96.13% accuracy
and 94.00 F1-score. The accuracy and F1-score of
TL-MSVM are 96.12% and 94.00. Both CL-MSVM
and TL-MSVM show slightly better performances
than other POS taggers. As shown in Table 5, both
CL-MSVM and TL-MSVM achieve lower accura-
cies than other methods, while their inter-category
errors are less than that of other experimental meth-
ods. Thus, the improvement of CL-MSVM and TL-
MSVM implies that, for the subsequent natural lan-
guage processing, a POS tagger should considers
different cost of tagging errors.
6 Conclusion
In this paper, we have shown that supervised POS
tagging can be improved by discriminating inter-
category errors from intra-category ones. An inter-
category error occurs by mislabeling a word with
a totally different tag, while an intra-category error
is caused by a similar POS tag. Therefore, inter-
category errors affect the performances of subse-
quent NLP tasks far more than intra-category errors.
This implies that different costs should be consid-
ered in training POS tagger according to error types.
As a solution to this problem, we have proposed
two gradient loss functions which reflect different
costs for two error types. The cost of an error type is
set according to (i) categorical difference or (ii) dis-
tance in the tree structure of POS tags. Our POS
experiment has shown that if these loss functions
are applied to multiclass SVMs, they could signif-
icantly reduce inter-category errors. Through the
text chunking experiment, it is shown that the multi-
class SVMs trained with the proposed loss functions
which generate fewer inter-category errors achieve
higher performance than existing POS taggers.
We have shown that cost sensitive learning can be
applied to POS tagging only with multiclass SVMs.
However, the proposed loss functions are general
enough to be applied to other existing POS taggers.
Most supervised machine learning techniques are
optimized on their loss functions. Therefore, the
performance of POS taggers based on supervised
machine learning techniques can be improved by ap-
plying the proposed loss functions to learn their clas-
sifiers.
Acknowledgments
This research was supported by the Converg-
ing Research Center Program funded by the
Ministry of Education, Science and Technology
(2011K000659).
References
Yasemin Altun, Mark Johnson, and Thomas Hofmann.
2003. Investigating Loss Functions and Optimiza-
tion Methods for Discriminative Learning of Label Se-
quences. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing. pp.
145?152.
1032
Talyor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?,
John DeNero, and Dan Klein. 2010. Painless Un-
supervised Learning with Features. In Proceedings
of the North American Chapter of the Association for
Computational Linguistics. pp. 582?590.
Thorsten Brants. 2000. TnT-A Statistical Part-of-Speech
Tagger. In Proceedings of the Sixth Applied Natural
Language Processing Conference. pp. 224?231.
Lijuan Cai and Thomas Hofmann. 2004. Hierarchi-
cal Document Categorization with Support Vector Ma-
chines. In Proceedings of the Thirteenth ACM Inter-
national Conference on Information and Knowledge
Management. pp. 78?87.
Koby Crammer, Yoram Singer. 2002. On the Algorith-
mic Implementation of Multiclass Kernel-based Vec-
tor Machines. Journal of Machine Learning Research,
Vol. 2. pp. 265?292.
Dipanjan Das and Slav Petrov. 2011. Unsupervised Part-
of-Speech Tagging with Bilingual Graph-Based Pro-
jections. In Proceedings of the 49th Annual Meeting
of the Association of Computational Linguistics. pp.
600?609.
Charles Elkan. 2001. The Foundations of Cost-Sensitive
Learning. In Proceedings of the Seventeenth Interna-
tional Joint Conference on Artificial Intelligence. pp.
973?978.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2004. SVMTool: A
general POS tagger generator based on Support Vector
Machines. In Proceedings of the Fourth International
Conference on Language Resources and Evaluation.
pp. 43?46.
Sharon Goldwater and Thomas T. Griffiths. 2007. A
fully Bayesian Approach to Unsupervised Part-of-
Speech Tagging. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguis-
tics. pp. 744?751.
Joao Graca, Kuzman Ganchev, Ben Taskar, and Fernando
Pereira. 2009. Posterior vs Parameter Sparsity in La-
tent Variable Models. In Advances in Neural Informa-
tion Processing Systems 22. pp. 664?672.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
Learning for Sequence Models. In Proceedings of the
North American Chapter of the Association for Com-
putational Linguistics. pp. 320?327.
Mark Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers? In Proceedings of the 2007 Joint Meet-
ing of the Conference on Empirical Methods in Natu-
ral Language Processing and the Conference on Com-
putational Natural Language Learning. pp. 296?305.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying Conditional Random Fields to
Japanese Morphological Analysis. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing. pp. 230?237.
Taku Kudo. 2005. CRF++: Yet another CRF toolkit.
http://crfpp.sourceforge.net.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional Random Fields: Probabilistic Mod-
els for Segmenting and Labeling Sequence Data. In
Proceedings of the Eighteenth International Confer-
ence on Machine Learning. pp. 282?289.
Christopher D. Manning. 2011. Part-of-Speech Tagging
from 97% to 100%: Is It Time for Some Linguistics?.
In Proceedings of the 12th International Conference
on Intelligent Text Processing and Computational Lin-
guistics. pp. 171?189.
Tetsuji Nakagawa, Taku Kudo, and Yuji Matsumoto.
2001. Unknown Word Guessing and Part-of-Speech
Tagging Using Support Vector Machines. In Proceed-
ings of the Sixth Natural Language Processing Pacific
Rim Symposium. pp. 325?331.
Adwait Ratnaparkhi. 1996. A Maximum Entropy Model
for Part-Of-Speech Tagging. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing. pp. 133?142.
Fei Sha and Fernando Pereira. 2003. Shallow Parsing
with Conditional Random Fields. In Proceedings of
the Human Language Technology and North American
Chapter of the Association for Computational Linguis-
tics. pp. 213?220.
Libin Shen, Giorgio Satta, and Aravind K. Joshi 2007.
Guided Learning for Bidirectional Sequence Classifi-
cation. In Proceedings of the 45th Annual Meeting
of the Association of Computational Linguistics. pp.
760?767.
Anders S?gaard 2011. Semisupervised condensed near-
est neighbor for part-of-speech tagging. In Proceed-
ings of the 49th Annual Meeting of the Association of
Computational Linguistics. pp. 48?52.
Drahom??ra ?johanka? Spoustova`, Jan Hajic?, Jan Raab,
and Miroslav Spousta 2009. Semi-supervised training
for the averaged perceptron POS tagger. In Proceed-
ings of the European Chapter of the Association for
Computational Linguistics. pp. 763?771.
Amarnag Subramanya, Slav Petrov and Fernando Pereira
2010. Efficient Graph-Based Semi-Supervised Learn-
ing of Structured Tagging Models. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing. pp. 167?176.
Xu Sun, Louis-Philippe Morency, Daisuke Okanohara
and Jun?ichi Tsujii 2008. Modeling Latent-Dynamic
in Shallow Parsing: A Latent Conditional Model with
Improved Inference. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics.
pp. 841?848.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Network.
1033
In Proceedings of the Human Language Technology
and North American Chapter of the Association for
Computational Linguistics. pp. 252?259.
Kristina Toutanova and Christopher D. Manning. 2000.
Enriching the Knowledge Sources Used in a Maxi-
mum Entropy Part-of-Speech Tagger. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing. pp. 63?70.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemi Altun. 2004. Support Vec-
tor Learning for Interdependent and Structured Output
Spaces. In Proceedings of the 21st International Con-
ference on Machine Learning. pp. 104?111.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2005. Bidi-
rectional Inference with the Easiest-First Strategy for
Tagging Sequence Data. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing. pp. 467?474.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, Vol. 19, No.2 . pp. 313?330.
Qiuye Zhao and Mitch Marcus. 2009. A Simple Un-
supervised Learner for POS Disambiguation Rules
Given Only a Minimal Lexicon. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing. pp. 688?697.
Zhi-Hua Zhou and Xu-Ying Liu 2006. On Multi-Class
Cost-Sensitive Learning. In Proceedings of the AAAI
Conference on Artificial Intelligence. pp. 567?572.
1034
