An Example-based Decoder for Spoken Language Machine Transla-
tion 
 
 
Zhou-Jun Li Wen-Han Chao 
Abstract 
In this paper, we propose an example-based 
decoder for a statistical machine translation 
(SMT) system, which is used for spoken 
language machine translation. In this way, 
it will help to solve the re-ordering problem 
and other problems for spoken language 
MT, such as lots of omissions, idioms etc. 
Through experiments, we show that this 
approach obtains improvements over the 
baseline on a Chinese-English spoken lan-
guage translation task. 
1 Introduction 
The state-of-the-art statistical machine translation 
(SMT) model is the log-linear model (Och and Ney, 
2002), which provides a framework to incorporate 
any useful knowledge for machine translation, 
such as translation model, language model etc.  
In a SMT system, one important problem is the 
re-ordering between words and phrases, especially 
when the source language and target language are 
very different in word order, such as Chinese and 
English.  
For the spoken language translation, the re-
ordering problem will be more crucial, since the 
spoken language is more flexible in word order. In 
addition, lots of omissions and idioms make the 
translation more difficult. 
However, there exists some "useful" features, 
such as, most of the spoken text is shorter than the 
written text and there are some fixed translation 
structures. For example,  ( ???? / Would you 
please ? ? ), (???/May I??). 
We can learn these fixed structures and take 
them as rules, Chiang (2005) presents a method to 
learn these rules, and uses them in the SMT. Gen-
erally, the number of these rules will be very large. 
In this paper, we propose an example-based de-
coder in a SMT model, which will use the transla-
tion examples to keep the translation structure, i.e. 
constraint the reordering, and make the omitted 
words having the chance to be translated. 
The rest of this paper is organized as follows: 
Since our decoder is based on the inversion trans-
duction grammars (ITG) (Wu, 1997), we introduce 
the ITG in Section 2 and describe the derived SMT 
model. In Section 3, we design the example-based 
decoder. In Section 4, we test our model and com-
pare it with the baseline system. Then, we con-
clude in Section 5 and Section 6. 
2 The SMT model 
ITG is a synchronous context-free grammar, which 
generates two output streams simultaneously. It 
consists of the following five types of rules: 
jiji
p ececAAAAA /|/|/||][ ??><??? (1)
Where A is the non-terminal symbol, [] and <> 
represent the two operations which generate out-
puts in straight and inverted orientation respec-
tively.  and  are terminal symbols, which rep-
resent the words in both languages, 
ic je
?  is the null 
National Laboratory for 
Parallel and Distributed 
Processing, Changsha, 
China 
School of Computer Sci-
ence and Engineering, 
Beihang University, 
China 
lizj@buaa.edu.cn  
National Laboratory for 
Parallel and Distributed 
Processing, Changsha, 
China 
cwh2k@163.com 
Yue-Xin Chen 
National Laboratory for 
Parallel and Distributed 
Processing, Changsha, 
China 
 
1
Sixth SIGHAN Workshop on Chinese Language Processing
words. The last three rules are called lexical rules. 
is the probability of the rule. p
In this paper, we consider the phrase-based SMT, 
so the  and  represent phrases in both lan-
guages, which are consecutive words. And a pair 
of   and  is called a phrase-pair, or a block. 
ic je
ic je
During the process of decoding, each phrase  
in the source sentence is translated into a target 
phrase  through lexical rules, and then rules [] 
or <>  are used to merge two adjacent blocks into a 
larger block in straight or inverted orientation, until 
the whole source sentence is covered. In this way, 
we will obtain a binary branching tree, which is 
different from the traditional syntactical tree, since 
each constituent in the branching tree is not a syn-
tactical constituent.  
ic
je
Thus, the model achieves a great flexibility to 
interpret alost arbitrary reordering during the de-
coding, while keeping a weak but effective con-
straint. Figure 1(a) gives an example to illustrate a 
derivation from the ITG model. 
 
?? 1 ? 2 ?? 3 ? 4 ?? 5 ?6 
where1 ?s2 the3 nearest4 cassino5 ?6 
(b)  A word alignment 
(a)  An ITG tree  
?/ the ? ?? / where ?s?? ? / nearest ?? / cassino ? / ? 
 
Figure 1. (a) An ITG tree derived from the ITG 
where the line between the branches means an in-
verted orientation, otherwise a straight one, (b) A 
word alignment corresponds to the ITG tree in (a). 
 
Since we regard the process of the decoding as a 
sequence of applications of rules in (1), i.e., the 
output sentence pair (C,E) will be a derivation D of 
the ITG, where C represents the source sentence 
and E is the target sentence. 
Following Och and Ney (2002), we define the 
probability for each rule as:  
?=
i
i
irulehrule ?)()Pr(  (2)
Where the hi represents the feature and ?i is the 
corresponding weight of the feature. 
We will consider mainly the following features 
for rules: 
z Translation Models: , , 
 and . The first two mod-
els consider the probability of phrase transla-
tion; and the latter two consider the lexical 
translation, i.e., the probability that the words 
in source (or target) phrase translate to the 
ones in the target (or source) phrase.  
)|( ceP )|( ecP
)|( cePlex )|( ecPlex
z Reordering model: , where o is 
the output orientation and b
),|( 21 bboP
1, b2 are the two 
blocks in the rule. 
z Language model: )(Pr elm? , which considers 
the increment of the language model for each 
rule.  
 
And the probability for the derivation will be: 
?=
?Dr
rD )Pr()Pr(  (3)
So the decoder searches the best E* derived 
from the best derivation D*, when given a source 
sentence C. 
)Pr(maxarg*
)(
DD
CDc =
=  (4)
2.1 Building the models 
In our SMT model, we use the translation models 
and reordering model. They will be built from the 
training corpus, which is a word-aligned bilingual 
corpus satisfying the ITG constraint.  
We define the word alignment A for the sen-
tence pair (C,E) in the following ways:  
z A region : )..,..( tsji ji..  represents a sequence 
of position index in sentence C, i.e. 
jii ,...,1, +  and  represents a sequence of 
position index in sentence E, i.e. 
ts..
tss ,...,1, + . 
We also call the  and ji.. ts..  are regions in 
monolingual sentences. The region corre-
sponds to a phrase pair, which we called as a 
block. The length of the block is 
|)1||,1max(| +?+? stij . 
2
Sixth SIGHAN Workshop on Chinese Language Processing
z A link : And each link represents 
the alignment between the consecutive words 
in both of the sentences, which position in-
dexes are in  and 
)..,..( tsjil =
ji.. ts.. . If one of the  
and 
ji..
ts..  is ?, i.e. an empty region, we call the 
link a null-align. 
z A word alignment A: a set of links 
. },...,,{ 21 nlllA =
We can merge two links  and 
 to form a larger link, if the two 
links are adjacent in both of the sentences, i.e.  
 is adjacent to  where 
)..,..( 11111 tsjil =
)..,..( 22222 tsjil =
11.. ji 22.. ji 112 += ji  or  
, or  (or ) is ? , so do the  
to . If the region can be formed by 
merging two adjacent links gradually, we call the 
region is independent, and the corresponding block 
is also independent. 
121 += ji 11.. ji 22.. ji 11..ts
22..ts )..,..( tsji
In our system, the word alignment must satisfy 
the ITG constraint, i.e. the word alignment is able 
to form a binary branching tree. Figure 1(b) illus-
trates a word alignment example; the number be-
low the word is the position index. In the example, 
the region (1..3, 3..5) is independent, and the block 
(   ?? ? ???the nearest cassino) is also inde-
pendent. 
In order to obtain the word alignment satisfying 
the ITG constraint, Wu(1997) propose a DP algo-
rithm, and we (Chao and Li, 2007) have transferred 
the constraint to four simple position judgment 
procedures in an explicit way, so that we can in-
corporate the ITG constraint as a feature into a log-
linear word alignment model (Moore, 2005).  
After obtaining the word-aligned corpus, in 
which each word alignment satisfy the ITG con-
straint, we can extract the blocks in a straight-
forward way. For the word alignment forms a hier-
archical binary tree, we choose each constituent as 
a block. Each block is formed by combining one or 
more links, and must be independent. Considering 
the data sparseness, we limit the length of each 
block as N (here N=3~5). 
We can also collect the reordering information 
between two blocks according to the orientation of 
the branches.  
Thus, we will build the translation models 
, ,  and , using 
the frequencies of the blocks, and the re-ordering 
model , 
)|( ceP )|( ecP )|( cePlex )|( ecPlex
),|( 21 bboP },{ invertstraighto?  in the 
following way:  
 ),( of freq.
)),(( of freq.
),|(
21
21
21 bbcooccur
obbO
bbop
==  (5)
Considering the data sparseness, we transfer the 
re-ordering model in the following way: 
)*,|(,*)|(),|( 2121 bopbopbbop ?=  (6)
where * represents any block, repre-
sents the probability when , i.e., when 
 occurs, the orientation it merges with any other 
block is o . So we can estimate the merging orien-
tation through the two blocks respectively.  
,*)|( 1bop
obO =,*)( 1
1b
2.2 A Baseline Decoder 
In order to evaluate the example-based decoder, we 
develop a CKY style decoder as a baseline (Chao 
et al 2007), which will generate a derivation from 
the ITG in a DP way. And it is similar with the 
topical phrase-based SMT system, while maintain-
ing the ITG constraint. 
3 The Example-based Decoder 
The SMT obtains the translation models during 
training, and does not need the training corpus 
when decoding; while the example-based machine 
translation system (EBMT) using the similar ex-
amples in the training corpus when decoding.  
However, both of them use the same corpus; we 
can generate a hybrid MT, which is a SMT system 
while using an example-based decoder, to benefit 
from the advantages within the two systems. 
Our example-based decoder consists of two 
components: retrieval of examples and decoding. 
Figure 2 shows the structure of the decoder.  
 
Training Corpus 
SMT Models 
Input sentence
Decoding 
Merging 
Retrieval of examples 
Matching 
Output  
Figure 2. The structure of the example-based de-
coder. 
3
Sixth SIGHAN Workshop on Chinese Language Processing
3.1 Retrieval of Examples 
Our training corpus is a sentence-aligned bilingual 
corpus. For each sentence pair (C,E), we obtained 
the word alignment A, satisfying the ITG constaint 
through the methods described in section 2. We 
call the triple (C,A,E) as an example.  
So, the problem of retrieval of examples is: 
given the input source sentence C0 and the training 
corpus, collecting a set of translation examples 
{( C1, A1, E1) , ( C2, TA2, E2),....} from the corpus, 
where each translation example (Ci, Ai, Ei)  is 
similar to the input sentence C0.  
The quality of the retrieval of the similar exam-
ples is very import to the hybrid MT. For the trans-
lating may run in a large-scale corpus and in a real-
time way, we divide the retrieval of similar exam-
ples into two phases:  
z Fast Retrieval Phase: retrieving the similar 
examples from the corpus quickly, and take 
them as candidates. The complexity should 
not be too high. 
z Refining Phase: refining the candidates to 
find the most similar examples. 
3.1.1 The Similarity Metric for Fast Retrieval 
Given an input sentence  and an ex-
ample (C, A, E), we calculate the number of the 
matched source words between the input sentence 
and the source sentence C  in the example firstly. 
nwwwI ...21=
),,()(
*2
),(
EACLenILen
Match
ExamISim ww +=
 (7)
where  is the number of the matched 
words and  is the number of words in 
wMatch
)(ILen I , 
and is the number of the words in the  
in C . 
),,( EACLen
Given an input sentence , we ob-
tain the relative blocks in the translation model for 
each word . We use to 
represent the blocks, in which for each block , 
the source phrase c  use the word as the first 
word, and the length of  c   is , i.e. the 
. For each c , there may exists more 
than one blocks with c  as the source phrase, so we 
will sort them by the probability and keep the best 
N (here set N=5) blocks. Now we represent the 
input sentence as: 
nwwwI ...21=
},...2,1{( niwi ? i gramkB ?
),( ec
iw
k
)1..( ?+= kiiwc
}1,1,|{)( nkniBbbI i gramk ?????= ??  (8)
 For example, in an input sentence ?   ??????,  
)},(),,(),,(),,{(11 MinemymeiB gram ????=?  
Note, some  may be empty, e.g. 
, since no blocks with ?  ?? ??? as 
the source phrase.  
i
gramkB ?
?=?22 gramB
In the same way, we represent the example 
 as:  ),,( EAC
*},|{),,( AbBbbEAC i gramk ??= ??  (9)
where *A  represents the blocks which are links in 
the alignment  or can be formed by merging ad-
jacent links independently. In order to accelerate 
the retrieval of similar examples, we generate the 
block set for the example during the training proc-
ess and store them in the corpus. 
A
Now, we can use the number of the matched 
blocks to measure the similarity of the input and 
the example: 
Exam
gram
I
gram
b
b
BB
Match
ExamISim +=
*2
),(  
(10)
where  is the number of the matched 
blocks and  is the number of  
( ) in 
bMatch
I
gramB
i
gramkB ?
???i gramkB )(I? , and is the number 
of the blocks in 
Exam
gramB
),,( EAC? .  
Since each block is attached a probability, we 
can compute the similarity in the following way: 
Exam
gram
I
gram
Matchb
p
BB
bob
ExamISim b+
?
= ?
)(Pr*2
),(  
(11)
So the final similarity metric for fast retrieval of 
the candidates is: 
pbwfast SimSimSimExamISim ??? ++=),(  (12)
where 11,,0 =++?? ?????? . Here we use 
mean values, i.e. 3/1=== ??? . During the fast 
retrieval phase, we first filter out the examples us-
ing the , then calculate the  for each 
example left, and retrieve the best N examples. 
wSim fastSim
4
Sixth SIGHAN Workshop on Chinese Language Processing
3.1.2 The Alignment Structure Metric 
After retrieving the candidate similar examples, we 
refine the candidates using the word alignment 
structure with the example, to find the best M simi-
lar examples (here set M=10). The word alignment 
in the example satisfies the ITG constraint, which 
provides a weak structure constraint. 
Given the input sentence I  and an example 
, we first search the matched blocks, at 
this moment the order of the source phrases in the 
blocks must correspond with the order of the words 
in the input.  
),,( EAC
As Figure 3 shows, the matching divides the in-
put and the example respectively into several re-
gions, where some regions are matched and some 
un-matched. And we take each region as a whole 
and align them between the input and the example 
according to the order of the matched regions. For 
example, the region (1..3,3..5) in  is un-
matched, which aligns to the region (1..1) in 
),,( EAC
I . In 
this way, we can use a similar edit distance method 
to measure the similarity. We count the number of 
the Deletion / Insertion / Substitution operations, 
which take the region as the object. 
 
 ?? 1 ? 2 ?? 3 ? 4 ?? 5 ?6
where1 ?s2 the3 nearest4 cassino5 ?6
(a)  An example 
??? 1 ? 2 ?? 3 ?4 
(b)  An input  
Figure 3. An input and an example. After matching, 
there are three regions in both sides, which are in-
cluded in the line box, where the region (4..5,1..2) 
in the example matches the region (2..3) in the in-
put, so do (6..6,6..6) to (4..4). And the region  
(1..3,3..5) in the example should be substituted to 
(1..1) in the input. 
 
We set the penalty for each deletion and inser-
tion operation as 1, while considering the un-
matched region in the example may be independ-
ent or not, we set the penalty for substitution as 0.5 
if the region is independent, otherwise as 1. E.g., 
the distance is 0.5 for substituting the region  
(1..3,3..5) to (1..1).  
We get the metric for measuring the structure 
similarity of the I  and : ),,( EAC
exmapleinput
align
RR
SID
ExamISim
+
++?=1),( (13)
where D, I, S are the deletion, insertion and substi-
tution distances, respectively. And the  and 
are the region numbers in the input and 
example. 
inputR
exmapleR
In the end, we obtain the similarity metric, 
which considers all of the above metrics: 
alignfastfinal SimSimExamISim ''),( ?? += (14)
where  1''1','0 =+?? ???? . Here we also 
use mean values 2/1'' == ?? . 
After the two phrases, we obtain the most simi-
lar examples with the input sentence.  
3.2 Decoding 
After retrieving the translation examples, our goal 
is to use these examples to constrain the order of 
the output words. During the decoding, we iterate 
the following two steps. 
3.2.1 Matching 
For each translation example (Ck, Ak, Ek) consists 
of the constituent structure tree, we can match the 
input sentence with the tree as in Section 3.1.2.  
After matching, we obtain a translation of the 
input sentence, in which some input phrases are 
matched to blocks in the tree, i.e. they are trans-
lated, and some phrases are un-translated. The or-
der of the matched blocks must be the same as the 
input phrases. We call the translation as a transla-
tion template for the input.  
If we take each un-translated phrase as a null-
aligned block, the translation template will be able 
to form a new constituent tree. And the matched 
blocks in the template will restrict the translation 
structure.  
Figure 4(a-c) illustrates the matching process, 
and Figure 4(c) is a translation template, in which "
? ?" and "? " have been translated and "? ?
? ?? ?" is not translated. And the translation 
5
Sixth SIGHAN Workshop on Chinese Language Processing
template can be derived from the ITG as follows 
(here we remove the un-matched phrase): 
 
couldA
youA
A
AAA
AAA
/
/
/?
][
4
3
2
431
21
?
?
??
>?
>?
>?
>><?
>?
 
(15)
Since we have M (here M=10) similar examples, 
we will get more than one translation template for 
the input sentence. So we define the evaluation 
function f for each translation template as :  
)(log)(log)( untranstrans CHDPtempf +=  (16)
Where  is the probability for the new 
ITG tree without the un-translated phrases, which  
is a derivation from the ITG, so we can calculate it 
using the SMT model in Section 2 ( formula 3).  
)( transDP
And the  is the estimated score for 
the un-translated phrases. In order to ob-
tain , we estimate the score for each 
un-translated phrase  in the following way: 
)( untransCH
)( untransCH
nmc ..
)}|*(),()(max{)( ..
*
...... maxmax nm
e
nkkm
k
nm cepcHcHcH ?= (17)
That is, using the best translation to estimate the 
translation score. Thus we can estimate the 
 as: )( untransCH
?=
c
nmuntrans cHCH )()( ..  (18)
We call the un-translated phrases as child inputs, 
and try to translate them literately, i.e., decoding 
them using the examples. If there are no un-
translated phrases in the input, the decoding is 
completed, and the decoder returns the translation 
template with the best score as the result. 
3.2.2 Merging 
If one child input is translated completely, i.e. no 
phrase is un-translated. Then, it should be merged 
into the parent translation template to form a new 
template. When merging, we must satisfy the ITG 
constraint, so we use the rules [] and <> to merge 
the child input with the adjacent blocks. Figure 
4(c-f) illustrates a merging process.  
 
(b) Example A 
? ? ? ?? ? ?
could you spell it ? ? / spell ?/ could ??/ ? ??/? ?/ it?/you
? ? ?? ?? ? ? ? 
(a) Input 
(c) Translation Tempate after match input with Example A 
? ? ?? ?? ? ?
could you ? ?? ?? ? ?/ could ??/ ? ?/you
?
(d) Example B 
? ?? ?? ? ?
please open your bag .. ?? / your ??/ open ?/ . ?/bag?/please
(e) Translation Tempate after match the child input with Example B 
?? / your??/ open ?/bag
?? ?? ?
open your bag
(f) Final translation after merged (c) and (e) 
? ?
? 
? ?
could you
?? ?? ?
open your bag ?/ could ??/ ? ?/you ?? / your??/ open ?/bag
 
Figure 4. An example to illustrate the example-
based decoding process, in which there are two 
translation examples. 
 
When merging, it may modify some rules which 
are adjacent to the child inputs. For example, when 
merging Figure 4(c) and (e), we may add a new 
rule:  
]  [ 1
'
1 childAAA >? (19)
Achild is the root non-terminal for the child input. 
And we should modify the rule  as: ][ 21AAA >?
][ 2
'
1AAA >?  (20)
The merged template may vary due to the fol-
lowing situations: 
z The orientation may vary. The orientation be-
tween the new block formed from the child 
6
Sixth SIGHAN Workshop on Chinese Language Processing
template and the preceding or posterior 
blocks may be straight or inverted. 
z The position to merge may vary. We may 
merge the new block with either the whole 
preceding or posterior blocks, or only the 
child blocks of them respectively, i.e. we 
may take the preceding or posterior blocks 
as the whole blocks or not. 
Thus, we will obtain a collection of the merged 
translation templates, the decoder will evaluate 
them using the formualte (16). If all the templates 
have no un-translated phrases, return the template 
with the best score. 
3.2.3 Decoding Algorithm 
The decoding algorithm is showed in Figure 5.  
In line 5~8, we match the input sentence with 
each similar example, and generate a collection of 
translation templates, using the formular (16) to 
evaluate the templates.  
In line 9~11, we verify whether the set of the 
templates for the input is null: If it is null, 
decoding the input using the normal CKY decoder, 
and return the translations.  
In lin 12~23, we decode the un-matched phrase 
in each template, and merge it with the parent 
template, until all of the template are translated 
completely.  
In line 24, we return the best N translations. 
4 Experiments 
We carried out experiments on an open Chinese-
English translation task IWSLT2007, which con-
sisting of sentence-aligned spoken language text 
for traveling. There are five development set, and 
we take the third development set, i.e. the 
IWSLT07_devset3_*, to tune the feature weights. 
 Chinese English 
stemmed 
Sentences 39,963 
Words 351,060 377,890 
Train. 
cor-
pus Vocabu-
lary 
11,302 7,610 
Sentences 506 Dev. 
Set Words 3,826  
Sentences 489 Test 
Set Words 3,189  
Table 1. The statistics of the corpus 
 
1: Function Example_Decoder(I,examples) 
2: Input: Input sentence I?Similar Examples examples 
3: Output: The best N tranlsations 
4: Begin 
5:   For each exampleA in examples Do 
6:     templates = Match(exampleA,I);    
7:     AddTemplate(templates,I);  
8:  End {For} 
9:  If templates is null then   
10:    templates = CYK_Decoder(I);  
11:    return templates; 
12: For each templateA in templates Do 
13:   If templateA is complete then 
14:      AddTemplate_Complete(templateA,I); 
15:   Else  
16:      RemoveTemplate(templateA,I); 
17:      For each untranslated phraseB in templateA do 
18:        childTemplates = Example_Decoder(phraseB);  
19:        For each childTemplateC in childTemplates Do 
20:          templateD=MergeTemplate(templateA,childTemplateC); 
21:    End{If} 
22:    AddTemplate(templateD,I);  
23:  End{For} 
24:  return BEST_N(complete_templates); 
28: End 
Figure 5. The decoding algorithm. 
 
Considering the size of the training corpus is 
relatively small, and the words in Chinese have no  
morphological changes, we stemmed the words in 
the English sentences. 
Table 1 shows the statistics for the training cor-
pus, development set and test set. 
In order to compare with the other SMT systems, 
we choose the Moses1, which is an extension to the 
state-of-the-art SMT system Pharaoh (Koehn, 
2004). We use the default tool in the Moses to train 
the model and tune the weights, in which the word 
alignment tool is Giza++ (Och and Ney 2003) and 
the language model tool is SRILM(Stolcke, 2002). 
The test results are showed in Table2. 
The first column lists the different MT systems, 
and the second column lists the Bleu scores (Pap-
ineni et. al, 2002) for the four decoders.  
The first system is the Moses, and the second is 
our SMT system described in section 2, which 
using a CKY-style decoder. We take them as base-
line systems. The third is the hybrid system but 
                                                 
1 http://www.statmt.org/moses/. 
7
Sixth SIGHAN Workshop on Chinese Language Processing
only using the fast retrieval module and the fourth 
is the hybrid system with refined retrieval module. 
Considering the result from the Moses, we 
think that maybe the size of the training corpus is 
too small, so that the word alignment obtained by 
Giza++ is poor.  
The results show that the example-based de-
coder achieves an improvement over the baseline 
decoders.  
Decoder Bleu 
Moses 22.61 
SMT-CKY 28.33 
Hybrid MT with fast retrieval 30.03 
Hybrid MT with refined retrieval 33.05 
Table 2. Test results for several systems. 
5 Related works 
There is some works about the hybrid machine 
translation. One way is to merge EBMT and SMT 
resources, such as Groves and Way (2005).  
Another way is to implement an exmaple-based 
decoder, Watanabe and Sumita (2003) presents an 
example-based decoder, which using a information 
retrieval framework to retrieve the examples; and 
when decoding, which runs a hill-climbing algo-
rithm to modify the translation example ( Ck, Ek, 
Ak) to obtain an alignment ( C0, E'k, A'k).  
6 Conclusions 
In this paper, we proposed a SMT system with an 
example-based decoder for the spoken language 
machine translation. This approach will take ad-
vantage of the constituent tree within the transla-
tion examples to constrain the flexible word re-
ordering in the spoken language, and it will also 
make the omitted words have the chance to be 
translated. Combining with the re-ordering model 
and the translation models in the SMT, the exam-
ple-based decoder obtains an improvement over 
the baseline phrase-based SMT system. 
In the future, we will test our method in the 
written text corpus. In addition, we will improve 
the methods to handle the morphological changes 
from the stemmed English words.  
Acknowledgements 
This work is supported by the National Science 
Foundation of China under Grants No. 60573057, 
60473057 and 90604007. 
References 
Wen-Han Chao and Zhou-Jun Li.(2007). Incorporating 
Constituent Structure Constraint into Discriminative 
Word Alignment. MT Summit XI, Copenhagen, 
Denmark, September 10-14, 2007. pp.97-103. 
Wen-Han Chao, Zhou-Jun Li, and Yue-Xin Chen.(2007) 
An Integrated Reordering Model for Statistical Ma-
chine Translation. In proceedings of MICAI 2007, 
LNAI 4827, pp. 955?965, 2007. 
David Chiang. (2005). A Hierarchical Phrase-Based 
Model for Statistical Machine Translation. In Proc. 
of ACL 2005, pages 263?270. 
Declan Groves and Andy Way: Hybrid Example-Based 
SMT: the Best of Both Worlds?  In Proceedings of the 
ACL Workshop on Building and Using Parallel Texts, 
pp. 183-190(2005) 
P. Koehn.(2004) Pharaoh: a beam search decoder for 
phrase-based statistical machine translation models. 
In: Proceedings of the Sixth Conference of the Asso-
ciation for Machine Translation in the Americas, pp. 
115?124. 
R. Moore. (2005). A discriminative framework for bilin-
gual word alignment. In Proceedings of HLT-
EMNLP, pages 81?88, Vancouver, Canada, October. 
Franz Joseph Och and Hermann Ney.(2002). Discrimi-
native training and maximum entropy models for sta-
tistical machine translation. In Proceedings of the 
40th Annual Meeting of the ACL, pp. 295?302. 
Franz Joseph Och and Hermann Ney. (2003) A System-
atic Comparison of Various Statistical Alignment 
Models. Computational Linguistics 29(1), 19?52  
Kishore Papineni, Salim Roukos, Todd Ward and Wei-
Jing Zhu. (2002). BLEU: a Method for Automatic 
Evaluation of Machine Translation. In Proceedings 
of the 40th Annual Meeting of the Association fo 
Computational Linguistics (ACL), Philadelphia, July 
2002, pp. 311-318. 
A. Stolcke. (2002). SRILM ? An extensible language 
modeling toolkit. In Proceedings of the International 
Conference on Spoken Language Processing, Denver, 
Colorado, 2002, pp. 901?904. 
Taro Watanabe and Eiichiro Sumita. (2003). Example-
based Decoding for Statistical Machine Translation. 
In Machine Translation Summit IX pp. 410-417. 
Dekai Wu. (1997). Stochastic Inversion Transduction 
Grammars and Bilingual Parsing of Parallel Cor-
pora. Computational Linguistics, 23(3):374. 
8
Sixth SIGHAN Workshop on Chinese Language Processing
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 650?658,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Comparable Entity Mining from Comparative Questions 
 
 
Shasha Li1?Chin-Yew Lin2?Young-In Song2?Zhoujun Li3 
1National University of Defense Technology, Changsha, China 
2Microsoft Research Asia, Beijing, China 
3Beihang University, Beijing, China 
shashali@nudt.edu.cn1, {cyl,yosong}@microsoft.com2, 
lizj@buaa.edu.cn3 
  
 
Abstract 
Comparing one thing with another is a typical 
part of human decision making process. How-
ever, it is not always easy to know what to 
compare and what are the alternatives. To ad-
dress this difficulty, we present a novel way to 
automatically mine comparable entities from 
comparative questions that users posted on-
line. To ensure high precision and high recall, 
we develop a weakly-supervised bootstrapping 
method for comparative question identification 
and comparable entity extraction by leveraging 
a large online question archive. The experi-
mental results show our method achieves F1-
measure of 82.5% in comparative question 
identification and 83.3% in comparable entity 
extraction. Both significantly outperform an 
existing state-of-the-art method.  
1 Introduction 
Comparing alternative options is one essential 
step in decision-making that we carry out every 
day. For example, if someone is interested in cer-
tain products such as digital cameras, he or she 
would want to know what the alternatives are 
and compare different cameras before making a 
purchase. This type of comparison activity is 
very common in our daily life but requires high 
knowledge skill. Magazines such as Consumer 
Reports and PC Magazine and online media such 
as CNet.com strive in providing editorial com-
parison content and surveys to satisfy this need.  
In the World Wide Web era, a comparison ac-
tivity typically involves: search for relevant web 
pages containing information about the targeted 
products, find competing products, read reviews, 
and identify pros and cons. In this paper, we fo-
cus on finding a set of comparable entities given 
a user?s input entity. For example, given an enti-
ty, Nokia N95 (a cellphone), we want to find 
comparable entities such as Nokia N82, iPhone 
and so on.  
In general, it is difficult to decide if two enti-
ties are comparable or not since people do com-
pare apples and oranges for various reasons.  For 
example, ?Ford? and ?BMW? might be compa-
rable as ?car manufacturers? or as ?market seg-
ments that their products are targeting?, but we 
rarely see people comparing ?Ford Focus? (car 
model) and ?BMW 328i?.   Things also get more 
complicated when an entity has several functio-
nalities. For example, one might compare 
?iPhone? and ?PSP? as ?portable game player? 
while compare ?iPhone? and ?Nokia N95? as 
?mobile phone?. Fortunately, plenty of compara-
tive questions are posted online, which provide 
evidences for what people want to compare, e.g. 
?Which to buy, iPod or iPhone??. We call ?iPod? 
and ?iPhone? in this example as comparators.  In 
this paper, we define comparative questions and 
comparators as: 
 
? Comparative question: A question that in-
tends to compare two or more entities and it 
has to mention these entities explicitly in the 
question. 
? Comparator: An entity which is a target of 
comparison in a comparative question.  
 
According to these definitions, Q1 and Q2 be-
low are not comparative questions while Q3 is. 
?iPod Touch? and ?Zune HD? are comparators. 
 
Q1: ?Which one is better?? 
Q2: ?Is Lumix GH-1 the best camera?? 
Q3: ?What?s the difference between iPod 
Touch and Zune HD?? 
 
The goal of this work is mining comparators 
from comparative questions. The results would 
be very useful in helping users? exploration of 
650
alternative choices by suggesting comparable 
entities based on other users? prior requests.  
To mine comparators from comparative ques-
tions, we first have to detect whether a question 
is comparative or not. According to our defini-
tion, a comparative question has to be a question 
with intent to compare at least two entities. 
Please note that a question containing at least 
two entities is not a comparative question if it 
does not have comparison intent. However, we 
observe that a question is very likely to be a 
comparative question if it contains at least two 
entities. We leverage this insight and develop a 
weakly supervised bootstrapping method to iden-
tify comparative questions and extract compara-
tors simultaneously. 
To our best knowledge, this is the first attempt 
to specially address the problem on finding good 
comparators to support users? comparison activi-
ty. We are also the first to propose using com-
parative questions posted online that reflect what 
users truly care about as the medium from which 
we mine comparable entities. Our weakly super-
vised method achieves 82.5% F1-measure in 
comparative question identification, 83.3% in 
comparator extraction, and 76.8% in end-to-end 
comparative question identification and compa-
rator extraction which outperform the most rele-
vant state-of-the-art method by Jindal & Liu 
(2006b) significantly.   
The rest of this paper is organized as follows. 
The next section discusses previous works. Sec-
tion 3 presents our weakly-supervised method for 
comparator mining. Section 4 reports the evalua-
tions of our techniques, and we conclude the pa-
per and discuss future work in Section 5. 
 
2 Related Work 
2.1 Overview 
In terms of discovering related items for an enti-
ty, our work is similar to the research on recom-
mender systems, which recommend items to a 
user. Recommender systems mainly rely on simi-
larities between items and/or their statistical cor-
relations in user log data (Linden et al, 2003). 
For example, Amazon recommends products to 
its customers based on their own purchase histo-
ries, similar customers? purchase histories, and 
similarity between products. However, recom-
mending an item is not equivalent to finding a 
comparable item. In the case of Amazon, the 
purpose of recommendation is to entice their cus-
tomers to add more items to their shopping carts 
by suggesting similar or related items. While in 
the case of comparison, we would like to help 
users explore alternatives, i.e. helping them make 
a decision among comparable items. 
For example, it is reasonable to recommend 
?iPod speaker? or ?iPod batteries? if a user is 
interested in ?iPod?, but we would not compare 
them with ?iPod?. However, items that are com-
parable with ?iPod? such as ?iPhone? or ?PSP? 
which were found in comparative questions post-
ed by users are difficult to be predicted simply 
based on item similarity between them. Although 
they are all music players, ?iPhone? is mainly a 
mobile phone, and ?PSP? is mainly a portable 
game device. They are similar but also different 
therefore beg comparison with each other. It is 
clear that comparator mining and item recom-
mendation are related but not the same.  
Our work on comparator mining is related to 
the research on entity and relation extraction in 
information extraction (Cardie, 1997; Califf and 
Mooney, 1999; Soderland, 1999; Radev et al, 
2002; Carreras et al, 2003). Specifically, the 
most relevant work is by Jindal and Liu (2006a 
and 2006b) on mining comparative sentences and 
relations. Their methods applied class sequential 
rules (CSR) (Chapter 2, Liu 2006) and label se-
quential rules (LSR) (Chapter 2, Liu 2006) 
learned from annotated corpora to identify com-
parative sentences and extract comparative rela-
tions respectively in the news and review do-
mains. The same techniques can be applied to 
comparative question identification and compa-
rator mining from questions. However, their me-
thods typically can achieve high precision but 
suffer from low recall (Jindal and Liu, 2006b) 
(J&L). However, ensuring high recall is crucial 
in our intended application scenario where users 
can issue arbitrary queries. To address this prob-
lem, we develop a weakly-supervised bootstrap-
ping pattern learning method by effectively leve-
raging unlabeled questions.  
Bootstrapping methods have been shown to be 
very effective in previous information extraction 
research (Riloff, 1996; Riloff and Jones, 1999; 
Ravichandran and Hovy, 2002; Mooney and Bu-
nescu, 2005; Kozareva et al, 2008). Our work is 
similar to them in terms of methodology using 
bootstrapping technique to extract entities with a 
specific relation. However, our task is different 
from theirs in that it requires not only extracting 
entities (comparator extraction) but also ensuring 
that the entities are extracted from comparative 
questions (comparative question identification), 
which is generally not required in IE task. 
651
2.2 Jindal & Liu 2006 
In this subsection, we provide a brief summary 
of the comparative mining method proposed by 
Jindal and Liu (2006a and 2006b), which is used 
as baseline for comparison and represents the 
state-of-the-art in this area.  We first introduce 
the definition of CSR and LSR rule used in their 
approach, and then describe their comparative 
mining method. Readers should refer to J&L?s 
original papers for more details. 
CSR and LSR 
CSR is a classification rule. It maps a sequence 
pattern S(?1?2???) to a class C.  In our problem, 
C is either comparative or non-comparative. 
Given a collection of sequences with class in-
formation, every CSR is associated to two para-
meters: support and confidence. Support is the 
proportion of sequences in the collection contain-
ing S as a subsequence. Confidence is the propor-
tion of sequences labeled as C in the sequences 
containing the S. These parameters are important 
to evaluate whether a CSR is reliable or not. 
LSR is a labeling rule. It maps an input se-
quence pattern ?(?1?2??? ???)  to a labeled 
sequence ??(?1?2? ?? ???) by replacing one to-
ken (??) in the input sequence with a designated 
label (?? ). This token is referred as the anchor. 
The anchor in the input sequence could be ex-
tracted if its corresponding label in the labeled 
sequence is what we want (in our case, a compa-
rator). LSRs are also mined from an annotated 
corpus, therefore each LSR also have two para-
meters: support and confidence. They are simi-
larly defined as in CSR. 
Supervised Comparative Mining Method 
J&L treated comparative sentence identification 
as a classification problem and comparative rela-
tion extraction as an information extraction prob-
lem. They first manually created a set of 83 key-
words such as beat, exceed, and outperform that 
are likely indicators of comparative sentences. 
These keywords were then used as pivots to 
create part-of-speech (POS) sequence data. A 
manually annotated corpus with class informa-
tion, i.e. comparative or non-comparative, was 
used to create sequences and CSRs were mined. 
A Na?ve Bayes classifier was trained using the 
CSRs as features. The classifier was then used to 
identify comparative sentences. 
Given a set of comparative sentences, J&L 
manually annotated two comparators with labels 
$ES1 and $ES2 and the feature compared with 
label $FT for each sentence. J&L?s method was 
only applied to noun and pronoun. To differen-
tiate noun and pronoun that are not comparators 
or features, they added the fourth label $NEF, i.e. 
non-entity-feature. These labels were used as 
pivots together with special tokens li & rj
1 (token 
position), #start (beginning of a sentence), and 
#end (end of a sentence) to generate sequence 
data, sequences with single label only and mini-
mum support greater than 1% are retained, and 
then LSRs were created. When applying the 
learned LSRs for extraction, LSRs with higher 
confidence were applied first. 
J&L?s method have been proved effective in 
their experimental setups. However, it has the 
following weaknesses:  
 
? The performance of J&L?s method relies 
heavily on a set of comparative sentence in-
dicative keywords. These keywords were 
manually created and they offered no guide-
lines to select keywords for inclusion. It is 
also difficult to ensure the completeness of 
the keyword list.  
? Users can express comparative sentences or 
questions in many different ways. To have 
high recall, a large annotated training corpus 
is necessary. This is an expensive process.  
? Example CSRs and LSRs given in Jindal & 
Liu (2006b) are mostly a combination of 
POS tags and keywords. It is a surprise that 
their rules achieved high precision but low 
recall. They attributed most errors to POS 
tagging errors. However, we suspect that 
their rules might be too specific and overfit 
their small training set (about 2,600 sen-
tences). We would like to increase recall, 
avoid overfitting, and allow rules to include 
discriminative lexical tokens to retain preci-
sion. 
 
In the next section, we introduce our method to 
address these shortcomings. 
3 Weakly Supervised Method for Com-
parator Mining 
Our weakly supervised method is a pattern-based 
approach similar to J&L?s method, but it is dif-
ferent in many aspects: Instead of using separate 
CSRs and LSRs, our method aims to learn se-
                                                 
1 li marks a token is at the i
th 
position to the left of the pivot 
and rj marks a token is at j
th position to the right of the 
pivot where i and j are between 1 and 4 in J&L (2006b). 
652
quential patterns which can be used to identify 
comparative question and extract comparators 
simultaneously.  
In our approach, a sequential pattern is defined 
as a sequence S(s1s2? si ? sn) where si can be a 
word, a POS tag, or a symbol denoting either a 
comparator ($C), or the beginning (#start) or the 
end of a question (#end). A sequential pattern is 
called an indicative extraction pattern (IEP) if it 
can be used to identify comparative questions 
and extract comparators in them with high relia-
bility. We will formally define the reliability 
score of a pattern in the next section.  
Once a question matches an IEP, it is classified 
as a comparative question and the token se-
quences corresponding to the comparator slots in 
the IEP are extracted as comparators.  When a 
question can match multiple IEPs, the longest 
IEP is used 2 . Therefore, instead of manually 
creating a list of indicative keywords, we create a 
set of IEPs. We will show how to acquire IEPs 
automatically using a bootstrapping procedure 
with minimum supervision by taking advantage 
of a large unlabeled question collection in the 
following subsections. The evaluations shown in 
section 4 confirm that our weakly supervised 
method can achieve high recall while retain high 
precision. 
This pattern definition is inspired by the work 
of Ravichandran and Hovy (2002). Table 1 
shows some examples of such sequential pat-
terns. We also allow POS constraint on compara-
tors as shown in the pattern ?<, $C/NN or $C/NN 
? #end>?. It means that a valid comparator must 
have a NN POS tag. 
3.1 Mining Indicative Extraction Patterns 
Our weakly supervised IEP mining approach is 
based on two key assumptions:  
 
                                                 
2 It is because the longest IEP is likely to be the most specif-
ic and relevant pattern for the given question. 
 
Figure 1: Overview of the bootstrapping alogorithm  
 
? If a sequential pattern can be used to extract 
many reliable comparator pairs, it is very likely 
to be an IEP.  
? If a comparator pair can be extracted by an 
IEP, the pair is reliable. 
 
Based on these two assumptions, we design 
our bootstrapping algorithm as shown in Figure 1. 
The bootstrapping process starts with a single 
IEP. From it, we extract a set of initial seed com-
parator pairs. For each comparator pair, all ques-
tions containing the pair are retrieved from a 
question collection and regarded as comparative 
questions. From the comparative questions and 
comparator pairs, all possible sequential patterns 
are generated and evaluated by measuring their 
reliability score defined later in the Pattern Eval-
uation section. Patterns evaluated as reliable ones 
are IEPs and are added into an IEP repository.  
Then, new comparator pairs are extracted from 
the question collection using the latest IEPs. The 
new comparators are added to a reliable compa-
rator repository and used as new seeds for pattern 
learning in the next iteration. All questions from 
which reliable comparators are extracted are re-
moved from the collection to allow finding new 
patterns efficiently in later iterations. The 
process iterates until no more new patterns can 
be found from the question collection.  
There are two key steps in our method: (1) 
pattern generation and (2) pattern evaluation. In 
the following subsections, we will explain them 
in details.   
Pattern Generation 
To generate sequential patterns, we adapt the 
surface text pattern mining method introduced in 
(Ravichandran and Hovy, 2002). For any given 
comparative question and its comparator pairs, 
comparators in the question are replaced with 
symbol $Cs. Two symbols, #start and #end, are 
attached to the beginning and the end of a sen-
Sequential Patterns 
<#start which city is better, $C or $C ? #end> 
<, $C or $C ? #end> 
<#start $C/NN or $C/NN ? #end> 
<which NN is better, $C or $C ?> 
<which city is JJR, $C or $C ?>  
<which NN is JJR, $C or $C ?> 
... 
Table 1: Candidate indicative extraction pattern (IEP) 
examples of the question ?which city is better, NYC or 
Paris?? 
 
653
tence in the question. Then, the following three 
kinds of sequential patterns are generated from 
sequences of questions: 
 
 
? Lexical patterns: Lexical patterns indicate 
sequential patterns consisting of only words 
and symbols ($C, #start, and #end). They are 
generated by suffix tree algorithm (Gusfield, 
1997) with two constraints: A pattern should 
contain more than one $C, and its frequency 
in collection should be more than an empiri-
cally determined number ?. 
? Generalized patterns: A lexical pattern can 
be too specific. Thus, we generalize lexical 
patterns by replacing one or more words with 
their POS tags. 2? ? 1 generalized patterns 
can be produced from a lexical pattern con-
taining N words excluding $Cs.  
? Specialized patterns: In some cases, a pat-
tern can be too general. For example, al-
though a question ?ipod or zune?? is com-
parative, the pattern ?<$C or $C>? is too 
general, and there can be many non-
comparative questions matching the pattern, 
for instance, ?true or false??. For this reason, 
we perform pattern specialization by adding 
POS tags to all comparator slots. For exam-
ple, from the lexical pattern ?<$C or $C>? 
and the question ?ipod or zune??, ?<$C/NN 
or $C/NN?>? will be produced as a specia-
lized pattern.  
 
Note that generalized patterns are generated from 
lexical patterns and the specialized patterns are 
generated from the combined set of generalized 
patterns and lexical patterns. The final set of 
candidate patterns is a mixture of lexical patterns, 
generalized patterns and specialized patterns. 
Pattern Evaluation  
According to our first assumption, a reliability 
score ??(??) for a candidate pattern ??  at itera-
tion k can be defined as follows: 
 
?? ?? =
 ?? (????? ? )??? ????
??1
?? (????)
        (1) 
 
, where ??  can extract known reliable comparator 
pairs ??? . ??
??1 indicates the reliable compara-
tor pair repository accumulated until the 
(? ? 1)?? iteration. ??(?) means the number of 
questions satisfying a condition x. The condition 
?? ? ???  denotes that ???  can be extracted from 
a question by applying pattern ??  while the con-
dition ?? ??  denotes any question containing 
pattern ?? .  
However, Equation (1) can suffer from in-
complete knowledge about reliable comparator 
pairs. For example, very few reliable pairs are 
generally discovered in early stage of bootstrap-
ping. In this case, the value of Equation (1) 
might be underestimated which could affect the 
effectiveness of equation (1) on distinguishing 
IEPs from non-reliable patterns. We mitigate this 
problem by a lookahead procedure. Let us denote 
the set of candidate patterns at the iteration k by 
? ? . We define the support ? for comparator pair  
?? ?  which can be extracted by ? 
?   and does not 
exist in the current reliable set:  
 
? ?? ? = ??( ? 
?
? ?? ?)     (2) 
 
where ? ? ? ?? ?  means that one of the patterns in 
? ?  can extract ???  in certain questions. Intuitive-
ly, if  ?? ?  can be extracted by many candidate 
patterns in ? ? , it is likely to be extracted as a 
reliable one in the next iteration. Based on this 
intuition, a pair ???  whose support S is more than 
a threshold ? is regarded as a likely-reliable pair. 
Using likely-reliable pairs, lookahead reliability 
score ?  ??  is defined: 
 
? ? ?? =
 ?? (????? i )??? ???? ???
?
?? (????)
      (3) 
 
, where ?? ???
?  indicates a set of likely-reliable 
pairs based on ? ? .  
By interpolating Equation (1) and (3), the final 
reliability score ?(??)?????
?  for a pattern is de-
fined as follows: 
 
?(??)?????
? = ? ? ?? ?? + (1? ?) ? ? 
?(??)     (4) 
 
Using Equation (4), we evaluate all candidate 
patterns and select patterns whose score is more 
than threshold ? as IEPs. All necessary parame-
ter values are empirically determined. We will 
explain how to determine our parameters in sec-
tion 4. 
4 Experiments 
4.1 Experiment Setup 
Source Data 
All experiments were conducted on about 60M 
questions mined from Yahoo! Answers? question 
title field. The reason that we used only a title 
654
field is that they clearly express a main intention 
of an asker with a form of simple questions in 
general.  
Evaluation Data 
Two separate data sets were created for evalua-
tion. First, we collected 5,200 questions by sam-
pling 200 questions from each Yahoo! Answers 
category3. Two annotators were asked to label 
each question manually as comparative, non-
comparative, or unknown. Among them, 139 
(2.67%) questions were classified as comparative,  
4,934 (94.88%) as non-comparative, and 127 
(2.44%) as unknown questions which are diffi-
cult to assess. We call this set SET-A. 
Because there are only 139 comparative ques-
tions in SET-A, we created another set which 
contains more comparative questions. We ma-
nually constructed a keyword set consisting of 53 
words such as ?or? and ?prefer?, which are good 
indicators of comparative questions. In SET-A, 
97.4% of comparative questions contains one or 
more keywords from the keyword set. We then 
randomly selected another 100 questions from 
each Yahoo! Answers category with one extra 
condition that all questions have to contain at 
least one keyword. These questions were labeled 
in the same way as SET-A except that their com-
parators were also annotated. This second set of 
questions is referred as SET-B. It contains 853 
comparative questions and 1,747 non-
comparative questions. For comparative question 
identification experiments, we used all labeled 
questions in SET-A and SET-B. For comparator 
extraction experiments, we used only SET-B. All 
the remaining unlabeled questions (called as 
SET-R) were used for training our weakly super-
vised method. 
As a baseline method, we carefully imple-
mented J&L?s method. Specifically, CSRs for 
comparative question identification were learned 
from the labeled questions, and then a statistical 
classifier was built by using CSR rules as fea-
tures. We examined both SVM and Na?ve Bayes 
(NB) models as reported in their experiments.  
For the comparator extraction, LSRs were 
learned from SET-B and applied for comparator 
extraction.  
To start the bootstrapping procedure, we ap-
plied the IEP ?<#start nn/$c vs/cc nn/$c ?/. 
#end>? to all the questions in SET-R and ga-
thered 12,194 comparator pairs as the initial 
seeds.  For our weakly supervised method, there 
                                                 
3 There are 26 top level categories in Yahoo! Answers. 
are four parameters, i.e. ?, ?, ?, and ?, need to be 
determined empirically. We first mined all poss-
ible candidate patterns from the suffix tree using 
the initial seeds. From these candidate patterns, 
we applied them to SET-R and got a new set of 
59,410 candidate comparator pairs. Among these 
new candidate comparator pairs, we randomly 
selected 100 comparator pairs and manually clas-
sified them into reliable or non-reliable compara-
tors. Then we found ? that maximized precision 
without hurting recall by investigating frequen-
cies of pairs in the labeled set. By this method, ? 
was set to 3 in our experiments. Similarly, the 
threshold parameters ? and ? for pattern evalua-
tion were set to 10 and 0.8 respectively. For the 
interpolation parameter ?  in Equation (3), we 
simply set the value to 0.5 by assuming that two 
reliability scores are equally important.  
As evaluation measures for comparative ques-
tion identification and comparator extraction, we 
used precision, recall, and F1-measure. All re-
sults were obtained from 5-fold cross validation. 
Note that J&L?s method needs a training data but 
ours use the unlabeled data (SET-R) with weakly 
supervised method to find parameter setting. 
This 5-fold evaluation data is not in the unla-
beled data. Both methods were tested on the 
same test split in the 5-fold cross validation. All 
evaluation scores are averaged across all 5 folds. 
For question processing, we used our own sta-
tistical POS tagger developed in-house4.  
4.2 Experiment Results 
Comparative Question Identification and 
Comparator Extraction 
Table 2 shows our experimental results. In the 
table, ?Identification only? indicates the perfor-
mances in comparative question identification, 
?Extraction only? denotes the performances of 
comparator extraction when only comparative 
questions are used as input, and ?All? indicates 
the end-to-end performances when question 
identification results were used in comparator 
extraction. Note that the results of J&L?s method 
on our collections are very comparable to what is 
reported in their paper.  
In terms of precision, the J&L?s method is 
competitive to our method in comparative ques-
                                                 
4  We used NLC-PosTagger which is developed by NLC 
group of Microsoft Research Asia. It uses the modified 
Penn Treebank POS set for its output; for example, NNS 
(plural nouns), NN (nouns), NP (noun phrases), NPS (plural 
noun phrases), VBZ (verb, present tense, 3rd person singu-
lar), JJ (adjective), RB(adverb), and so on. 
655
tion identification. However, the recall is signifi-
cantly lower than ours. In terms of recall, our 
method outperforms J&L?s method by 35% and 
22% in comparative question identification and 
comparator extraction respectively. In our analy-
sis, the low recall of J&L?s method is mainly 
caused by low coverage of learned CSR patterns 
over the test set.  
In the end-to-end experiments, our weakly su-
pervised method performs significantly better 
than J&L?s method. Our method is about 55% 
better in F1-measure. This result also highlights 
another advantage of our method that identifies 
comparative questions and extracts comparators 
simultaneously using one single pattern. J&L?s 
method uses two kinds of pattern rules, i.e. CSRs 
and LSRs. Its performance drops significantly 
due to error propagations. F1-measure of J&L?s 
method in ?All? is about 30% and 32% worse 
than the scores of ?Identification only? and ?Ex-
traction? only respectively, our method only 
shows small amount of performance decrease 
(approximately 7-8%).  
We also analyzed the effect of pattern genera-
lization and specialization. Table 3 shows the 
results. Despite of the simplicity of our methods, 
they significantly contribute to performance im-
provements. This result shows the importance of 
learning patterns flexibly to capture various 
comparative question expressions. Among the 
6,127 learned IEPs in our database, 5,930 pat-
terns are generalized ones, 171 are specialized 
ones, and only 26 patterns are non-generalized 
and specialized ones.  
To investigate the robustness of our bootstrap-
ping algorithm for different seed configurations, 
we compare the performances between two dif-
ferent seed IEPs. The results are shown in Table 
4. As shown in the table, the performance of our 
bootstrapping algorithm is stable regardless of 
significantly different number of seed pairs gen-
erated by the two IEPs. This result implies that 
our bootstrapping algorithm is not sensitive to 
the choice of IEP.  
Table 5 also shows the robustness of our boot-
strapping algorithm. In Table 5, ?All? indicates 
the performances that all comparator pairs from a 
single seed IEP is used for the bootstrapping, and 
?Partial? indicate the performances using only 
1,000 randomly sampled pairs from ?All?. As 
shown in the table, there is no significant per-
formance difference.  
In addition, we conducted error analysis for 
the cases where our method fails to extract cor-
rect comparator pairs: 
 
? 23.75% of errors on comparator extraction 
are due to wrong pattern selection by our 
simple maximum IEP length strategy.  
? The remaining 67.63% of errors come from 
comparative questions which cannot be cov-
ered by the learned IEPs. 
 
 
 Recall Precision F-score 
Original Patterns 0.689  0. 449 0.544 
+ Specialized 0.731  0.602 0.665 
+ Generalized 0.760  0.776 0.768 
Table 3: Effect of pattern specialization and Generali-
zation in the end-to-end experiments.  
 
Seed patterns # of resulted 
seed pairs 
F-score 
<#start nn/$c vs/cc nn/$c 
?/. #end>  
12,194 0.768 
<#start which/wdt is/vb 
better/jjr , nn/$c or/cc 
nn/$c ?/. #end> 
1,478 0.760 
Table 4: Performance variation over different initial 
seed IEPs in the end-to-end experiments 
 
Set  (# of seed pairs) Recall Precision F-score 
All (12,194) 0.760 0.774 0.768 
Partial (1,000) 0.724 0.763 0.743 
Table 5: Performance variation over different sizes of 
seed pairs generated from a single initial seed IEP 
?<#start nn/$c vs/cc nn/$c ?/. #end>?. 
 
 
Identification only 
(SET-A+SET-B) 
Extraction only 
(SET-B) 
All 
(SET-B) 
J&L (CSR) Our  
Method 
J&L 
(LSR) 
Our  
Method 
J&L Our  
Method SVM NB SVM NB 
Recall 0.601 0.537 0.817* 0.621 0.760* 0.373 0.363 0.760* 
Precision 0.847 0.851 0.833 0.861 0.916* 0.729 0.703 0.776* 
F-score 0.704 0.659 0.825* 0.722 0.833* 0.493 0.479 0.768* 
Table 2: Performance comparison between our method and Jindal and Bing?s Method (denoted as J&L). 
The values with * indicate statistically significant improvements over J&L (CSR) SVM or J&L (LSR) 
according to t-test  at p < 0.01 level. 
 
656
Examples of Comparator Extraction  
By applying our bootstrapping method to the 
entire source data (60M questions), 328,364 
unique comparator pairs were extracted from 
679,909 automatically identified comparative 
questions.  
Table 6 lists top 10 frequently compared enti-
ties for a target item, such as Chanel, Gap, in our 
question archive. As shown in the table, our 
comparator mining method successfully discov-
ers realistic comparators. For example, for ?Cha-
nel?, most results are high-end fashion brands 
such as ?Dior? or ?Louis Vuitton?, while the rank-
ing results for ?Gap? usually contains similar ap-
parel brands for young people, such as ?Old Navy? 
or ?Banana Republic?. For the basketball player 
?Kobe?, most of the top ranked comparators are 
also famous basketball players. Some interesting 
comparators are shown for ?Canon? (the compa-
ny name). It is famous for different kinds of its 
products, for example, digital cameras and prin-
ters, so it can be compared to different kinds of 
companies. For example, it is compared to ?HP?, 
?Lexmark?, or ?Xerox?, the printer manufacturers, 
and also compared to ?Nikon?, ?Sony?, or ?Kodak?, 
the digital camera manufactures.  Besides gener-
al entities such as a brand or company name, our 
method also found an interesting comparable 
entity for a specific item in the experiments. For 
example, our method recommends ?Nikon d40i?, 
?Canon rebel xti?, ?Canon rebel xt?, ?Nikon 
d3000?, ?Pentax k100d?, ?Canon eos 1000d? as 
comparators for the specific camera product ?Ni-
kon 40d?. 
Table 7 can show the difference between our 
comparator mining and query/item recommenda-
tion. As shown in the table, ?Google related 
searches? generally suggests a mixed set of two 
kinds of related queries for a target entity: (1) 
queries specified with subtopics for an original 
query (e.g., ?Chanel handbag? for ?Chanel?) and 
(2) its comparable entities (e.g., ?Dior? for ?Cha-
nel?). It confirms one of our claims that compara-
tor mining and query/item recommendation are 
related but not the same. 
5 Conclusion 
In this paper, we present a novel weakly super-
vised method to identify comparative questions 
and extract comparator pairs simultaneously. We 
rely on the key insight that a good comparative 
question identification pattern should extract 
good comparators, and a good comparator pair 
should occur in good comparative questions to 
bootstrap the extraction and identification 
process. By leveraging large amount of unla-
beled data and the bootstrapping process with 
slight supervision to determine four parameters, 
we found 328,364 unique comparator pairs and 
6,869 extraction patterns without the need of 
creating a set of comparative question indicator 
keywords.  
The experimental results show that our me-
thod is effective in both comparative question 
identification and comparator extraction. It sig-
 Chanel Gap iPod Kobe Canon 
1 Dior Old Navy Zune Lebron Nikon 
2 Louis Vuitton American Eagle mp3 player Jordan Sony 
3 Coach Banana Republic PSP MJ Kodak 
4 Gucci Guess by Marciano cell phone Shaq Panasonic 
5 Prada ACP Ammunition iPhone Wade Casio 
6 Lancome Old Navy brand Creative Zen T-mac Olympus 
7 Versace Hollister Zen Lebron James Hp 
8 LV Aeropostal iPod nano Nash Lexmark 
9 Mac American Eagle outfitters iPod touch KG Pentax 
10 Dooney Guess iRiver Bonds Xerox 
Table 6: Examples of comparators for different entities  
Chanel Gap iPod Kobe Canon 
Chanel handbag Gap coupons iPod nano Kobe Bryant stats Canon t2i 
Chanel sunglass Gap outlet iPod touch Lakers Kobe Canon printers 
Chanel earrings Gap card iPod best buy Kobe espn Canon printer drivers 
Chanel watches Gap careers iTunes Kobe Dallas Mavericks Canon downloads 
Chanel shoes Gap casting call Apple Kobe NBA Canon copiers 
Chanel jewelry Gap adventures iPod shuffle Kobe 2009 Canon scanner 
Chanel clothing Old navy iPod support Kobe san Antonio Canon lenses 
Dior Banana republic iPod classic Kobe Bryant 24 Nikon 
Table 7: Related queries returned by Google related searches for the same target entities in Table 6. The bold 
ones indicate overlapped queries to the comparators in Table 6. 
 
657
nificantly improves recall in both tasks while 
maintains high precision. Our examples show 
that these comparator pairs reflect what users are 
really interested in comparing. 
Our comparator mining results can be used for 
a commerce search or product recommendation 
system. For example, automatic suggestion of 
comparable entities can assist users in their com-
parison activities before making their purchase 
decisions. Also, our results can provide useful 
information to companies which want to identify 
their competitors.  
In the future, we would like to improve extrac-
tion pattern application and mine rare extraction 
patterns. How to identify comparator aliases such 
as ?LV? and ?Louis Vuitton? and how to separate 
ambiguous entities such ?Paris vs. London? as 
location and ?Paris vs. Nicole? as celebrity are 
all interesting research topics. We also plan to 
develop methods to summarize answers pooled 
by a given comparator pair.  
6 Acknowledgement  
This work was done when the first author 
worked as an intern at Microsoft Research Asia. 
References  
Mary Elaine Califf and Raymond J. Mooney. 1999. 
Relational learning of pattern-match rules for in-
formation extraction. In Proceedings of AAAI?99 
/IAAI?99. 
Claire Cardie. 1997. Empirical methods in informa-
tion extraction. AI magazine, 18:65?79.  
Dan Gusfield. 1997. Algorithms on strings, trees, and 
sequences: computer science and computational 
biology. Cambridge University Press, New York, 
NY, USA 
Taher H. Haveliwala. 2002. Topic-sensitive pagerank. 
In Proceedings of WWW ?02, pages 517?526. 
Glen Jeh and Jennifer Widom. 2003. Scaling persona-
lized web search. In Proceedings of WWW ?03, 
pages 271?279. 
Nitin Jindal and Bing Liu. 2006a. Identifying compar-
ative sentences in text documents. In Proceedings 
of SIGIR ?06, pages 244?251. 
Nitin Jindal and Bing Liu. 2006b. Mining compara-
tive sentences and relations. In Proceedings of 
AAAI ?06. 
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 
2008. Semantic class learning from the web with 
hyponym pattern linkage graphs. In Proceedings of 
ACL-08: HLT, pages 1048?1056.  
Greg Linden, Brent Smith and Jeremy York. 2003. 
Amazon.com Recommendations: Item-to-Item 
Collaborative Filtering. IEEE Internet Computing, 
pages 76-80.  
Raymond J. Mooney and Razvan Bunescu. 2005. 
Mining knowledge from text using information ex-
traction. ACM SIGKDD Exploration Newsletter, 
7(1):3?10. 
Dragomir Radev, Weiguo Fan, Hong Qi, and Harris 
Wu and Amardeep Grewal. 2002. Probabilistic 
question answering on the web. Journal of the 
American Society for Information Science and 
Technology, pages 408?419. 
Deepak Ravichandran and Eduard Hovy. 2002. 
Learning surface text patterns for a question ans-
wering system. In Proceedings of ACL ?02, pages 
41?47. 
Ellen Riloff and Rosie Jones. 1999. Learning dictio-
naries for information extraction by multi-level 
bootstrapping. In Proceedings of AAAI ?99 
/IAAI ?99, pages 474?479. 
Ellen Riloff. 1996. Automatically generating extrac-
tion patterns from untagged text. In Proceedings of 
the 13th National Conference on Artificial Intelli-
gence, pages 1044?1049. 
Stephen Soderland. 1999. Learning information ex-
traction rules for semi-structured and free text. Ma-
chine Learning, 34(1-3):233?272.  
 
658
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 923?933,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Exploiting Timelines to Enhance Multi-document Summarization
Jun-Ping Ng
1,2
, Yan Chen
3
, Min-Yen Kan
2,4
, Zhoujun Li
3
1
DSO National Laboratories, Singapore
2
School of Computing, National University of Singapore, Singapore
3
State Key Laboratory of Software Development Environment, Beihang University, China
4
Interactive and Digital Media Institute, National University of Singapore, Singapore
njunping@dso.org.sg
Abstract
We study the use of temporal information
in the form of timelines to enhance multi-
document summarization. We employ a
fully automated temporal processing sys-
tem to generate a timeline for each in-
put document. We derive three features
from these timelines, and show that their
use in supervised summarization lead to a
significant 4.1% improvement in ROUGE
performance over a state-of-the-art base-
line. In addition, we propose TIMEMMR,
a modification to Maximal Marginal Rel-
evance that promotes temporal diversity
by way of computing time span similar-
ity, and show its utility in summarizing
certain document sets. We also propose a
filtering metric to discard noisy timelines
generated by our automatic processes, to
purify the timeline input for summariza-
tion. By selectively using timelines guided
by filtering, overall summarization perfor-
mance is increased by a significant 5.9%.
1 Introduction
There has been a good amount of research in-
vested into improving the temporal interpretation
of text. Besides the increasing availability of an-
notation standards (e.g., TIMEML (Pustejovsky et
al., 2003a)) and corpora (e.g., TIDES (Ferro et
al., 2000), TimeBank (Pustejovsky et al, 2003b)),
the community has also organized three success-
ful evaluation workshops ? TempEval-1 (Verha-
gen et al, 2009), -2 (Verhagen et al, 2010), and
-3 (Uzzaman et al, 2013). As the state-of-the-
art improves, these workshops have moved away
from the piecemeal evaluation of individual tem-
poral processing tasks and towards the evaluation
of complete end-to-end systems in TempEval-3.
We believe our understanding of the temporal in-
formation found in text is sufficiently robust, and
that there is an opportunity to now leverage this in-
formation in downstream applications. In this pa-
per, we present our work in incorporating the use
of such temporal information into multi-document
summarization.
The goal of multi-document summarization is
to generate a summary which includes the main
points from an input collection of documents with
minimal repetition of similar points. We hope to
improve the quality of the summaries that are gen-
erated by considering temporal information found
in the input text. To motivate how temporal in-
formation can be useful in summarization, let us
refer to Figure 1. The three sentences describe a
recent cyclone and a previous one which happened
in 1991. Recognizing that sentence (3) is about a
storm that had happened in the past is important
when writing a summary about the recent storm,
as it is not relevant and can likely be excluded.
It is reasonable to expect that a collection of
documents about the recent storm will contain
more references to it, compared with the earlier
one that happened in 1991. Visualized on a time-
line, this will translate to more events (bolded in
Figure 1) around the time when the recent storm
occurred. There should be fewer events mentioned
in the collection for the earlier 1991 time period.
Figure 2 illustrates a possible timeline laid out
with the events found in Figure 1. The events
from the more recent storm are found together at
the same time. There are fewer events which talk
about the previous storm. Thus, temporal informa-
tion does assist in identifying which sentences are
more relevant to the final summary.
Our work is significant as it addresses an im-
portant gap in the exploitation of temporal infor-
mation. While there has been prior work making
use of temporal information for multi-document
923
(1) A fierce cyclone packing extreme winds and torrential rain smashed into Bangladesh?s southwestern coast Thursday,
wiping out homes and trees in what officials described as the worst storm in years.
(2) More than 100,000 coastal villagers have been evacuated before the cyclone made landfall.
(3) The storm matched one in 1991 that sparked a tidal wave that killed an estimated 138,000 people, Karmakar told AFP.
Figure 1: Modified extract from a news article which describes a cyclone landfall. Several events which
appear in Figure 2 are bolded.
smashed
packing
wiping
describedsparked killed
...
Storm in 1991 Latest cyclone
evacuated
2013-Feb-13 11:32 +0000
Figure 2: Possible timeline for events in Figure 1.
summarization, they 1) have been largely con-
fined to helping to chronologically order content
within summaries (Barzilay et al, 1999), or 2)
focus only on the use of recency as an indicator
of saliency (Goldstein et al, 2000; Wan, 2007).
In this work we construct timelines (as a repre-
sentation of temporal information) automatically
and incorporate them into a state-of-the-art multi-
document summarization system. This is achieved
with 1) three novel features derived from time-
lines to help measure the saliency of sentences,
as well as 2) TIMEMMR, a modification to the
traditional Maximal Marginal Relevance (MMR)
(Carbonell and Goldstein, 1998). TimeMMR pro-
motes diversity by additionally considering tem-
poral information instead of just lexical similari-
ties. Through these, we demonstrate that temporal
information is useful for multi-document summa-
rization. Compared to a competitive baseline, sig-
nificant improvements of up to 4.1% are obtained.
Automatic temporal processing systems are not
perfect yet, and this may have an impact on their
use for downstream applications. This work ad-
ditionally proposes the use of the lengths of time-
lines as a metric to gauge the usefulness of time-
lines. Together with the earlier described contribu-
tions, this metric further improves summarization,
yielding an overall 5.9% performance increase.
2 Related Work
Barzilay et al (1999) were one of the first to use
time for multi-document summarization. They
recognized the importance of generating a sum-
mary which presents the time perspective of the
summarized documents correctly. They estimated
the chronological ordering of events with a small
set of heuristics, and also made use of lexical pat-
terns to perform basic time normalization on terms
like ?today? relative to the document creation
time. The induced ordering is used to present the
selected summary content, following the chrono-
logical order in the original documents.
In another line of work, Goldstein et al (2000)
made use of the temporal ordering of documents
to be summarized. In computing the relevance of a
passage for inclusion into the final summary, they
considered the recency of the passage?s source
document. Passages from more recent documents
are deemed to be more important. Wan (2007)
and Demartini et al (2010) made similar assump-
tions in their work on TIMEDTEXTRANK and en-
tity summarization, respectively.
Instead of just considering the notion of re-
cency, Liu et al (2009) proposed an interesting
approach using a temporal graph. Events within
a document set correspond to vertices in their pro-
posed graph, while edges are determined by the
temporal ordering of events. From the resulting
weakly-connected graph, the largest forests are as-
sumed to contain key topics within the document
set and used to influence a scoring mechanism
which prefers sentences touching on these topics.
Wu (2008) also made use of the relative or-
dering of events. He assigned complete times-
tamps to events extracted from text. After lay-
ing out these events onto a timeline by making
use of these timestamps, the number of events that
happen within the same day is used to influence
sentence scoring. The motivation behind this ap-
proach is that days which have a large number of
events should be more important and more worthy
of reporting than others.
These prior works target either 1) sentence re-
ordering, or 2) the use of recency as an indicator of
saliency. In sentence re-ordering, final summaries
are re-arranged so that the extracted sentences that
form the summary are in a chronological order.
We argue that this may not be appropriate for all
summaries. Depending on the style of writing or
journalistic guidelines, a summary can arguably be
written in a number of ways. The use of recency
924
as an indicator of saliency is useful, yet disregards
other accessible temporal information. If a sum-
mary of a whole sequence of events is desired, re-
cency becomes less useful.
The work of Wu (2008) is closely related to one
of the features proposed in this paper. He had also
made use of temporal information to weight sen-
tences to generate summaries. However his ap-
proach is guided by the number of events hap-
pening within the same time span, and relies on
event co-referencing. In this work, we have sim-
plified this idea by dropping the need for event co-
referencing (removing a source of propagated er-
ror), and augmented it with two additional features
derived from timelines. By doing so, we are able
to make better use of the available temporal infor-
mation, taking into account all known events and
the time in which they occur.
A useful note here is that this work is ar-
guably different from the Temporal Summariza-
tion (TmpSum) track at the Text Retrieval Confer-
ence (Aslam et al, 2013). Given a large stream
of data in real-time, the purpose of the TmpSum
track is to look out for a query event, and retrieve
specific details about the event over a period of
time. Systems are also expected to identify the
source sentences from which these details are re-
trieved. This is not the same as our approach here,
which makes use of temporal information encoded
in timelines to generate prose summaries.
3 Methodology
To incorporate temporal information into multi-
document summarization, we adopt the workflow
in Figure 3, which has two key processes: 1) tem-
poral processing, and 2) summarization.
Input 
Documents
Input 
Documents
Input 
Documents
E-T
Temporal 
Classification
E-E
Temporal 
Classification
Event and Timex
Extraction
Sentence
Scoring
Sentence
Re-ordering
Pre-processing
Summary
Summarization Pipeline
Temporal Processing
Timeline
Generation
Timex
Normalization
Timelines
Timelines
Figure 3: Incorporating temporal information into
the SWING summarization pipeline.
Temporal Processing generates timelines from
text, one for each input document. Timelines are
well-understood constructs which have often been
used to represent temporal information (Denis and
Muller, 2011; Do et al, 2012). They indicate the
temporal relationships between two basic tempo-
ral units: 1) events, and 2) time expressions (or
timexes for short). In this work, we adopt the
definitions proposed in the standardized TIMEML
annotation (Pustejovsky et al, 2003a). An event
refers to an eventuality, a situation that occurs or
an action; while a timex is a reference to a partic-
ular date or time (e.g. ?2013 December 31?).
Following the ?divide-and-conquer? approach
described in Verhagen et al (2010), results from
the three temporal processing steps: 1) timex nor-
malization, 2) event-timex temporal relationship
classification, and 3) event-event temporal rela-
tionship classification, are merged to obtain time-
lines (top half of Figure 3). We tap on existing
systems for each of these steps (Ng and Kan, 2012;
Str?otgen and Gertz, 2013; Ng et al, 2013).
Summarization. We make use of a state-of-
the-art summarization system, SWING (Ng et al,
2012) (bottom half of Figure 3). SWING is a su-
pervised, extractive summarization system which
ranks sentences based on scores computed using
a set of features in the Sentence Scoring phase.
The Maximal Marginal Relevance (MMR) algo-
rithm is then used in the Sentence Re-ordering
phase to re-order and select sentences to form the
final summary. The timelines built in the ear-
lier temporal processing can be incorporated into
this pipeline by deriving a set of features used to
score sentences in Sentence Scoring, and as input
to the MMR algorithm when computing similarity
in Sentence Re-ordering.
3.1 Timelines from Temporal Processing
A typical timeline used in this work has been
shown earlier in Figure 2. The arrowed, horizon-
tal axis is the timeline itself. The timeline can
be viewed as a continuum of time, with points on
the timeline referring to specific moments of time.
Small solid blocks on the timeline itself are ref-
erences to absolute timestamps along the timeline
(e.g., ?2013-Feb-13 11:32 +0000? in the figure).
The black square boxes above the timeline de-
note events. Events can either occur at a specific
instance of time (e.g., an explosion), or over a pe-
riod of time (e.g. a football match). Generalizing,
we refer to the time period an event takes place in
as its time span (vertical dotted lines). As a simpli-
925
left peak of e right peak of e
b
i
g
g
e
s
t
 
c
l
u
s
t
e
r
Time Span A Time Span A+4
e
Figure 4: A simplified timeline illustrating how
the various timeline features can be derived.
fying assumption, events are laid out on the time-
line based on the starting time of their time span.
Note that in our work, time spans may not cor-
respond to specific instances of time, but instead
help in inferring an ordering of events. Events
which appear to the left of others take place ear-
lier, while events within the same time span hap-
pen together over the same time period.
3.2 Sentence Scoring with Timelines
We derive three features from the constructed
timelines, which are then used for downstream
Sentence Scoring. Figure 4 shows a simplified
timeline, along with annotations that are refer-
enced in this section to help explain how these
timeline features are derived.
1. Time Span Importance (TSI). We hypothe-
size that when more events happen within a partic-
ular time span, that time span is potentially more
relevant for summarization. Sentences that men-
tion events found in such a time span should be as-
signed higher scores. Referring to Figure 1, whose
timeline is shown in Figure 2, we see that the time
span with the most number of events is when the
latest cyclone made landfall. Assigning higher
scores for sentences which contain events in this
time span will help us to select more relevant sen-
tences if we want a summary about the cyclone.
Let TS
L
be the time span with the largest num-
ber of events in a timeline. The importance of
a time span TS
i
is computed by normalizing the
number of events in TS
i
against the number of
events in TS
L
. The TSI of a sentence s is then
the sum of the time span importance associated to
all the words in s:
TSI(s) =
?
w?s
|TS
w
|
|TS
L
|
|s|
(1)
where TS
w
denotes the time span which a word
w is associated with, and |TS
w
| is the number of
events within the time span.
2. Contextual Time Span Importance
(CTSI). The importance of a time span may not
depend solely on the number of events that hap-
pen within it. If it is near time spans which are
?important? (i.e., one that has a large number of
events), it should also be of relative importance. A
more concrete illustration of this can also be seen
in Figure 1. Sentence (2) explains that a lot of peo-
ple have been evacuated prior to the cyclone mak-
ing landfall. It is imaginable that this can be use-
ful information to be included in a summary, even
though from looking at the corresponding timeline
in Figure 2, the ?evacuated? event falls in a time
span with a low importance score (i.e., the time
span only has one event). CTSI seeks to promote
sentences such as this.
We derive the CTSI of a sentence by first com-
puting the contextual importance of words in the
sentence. We define the contextual importance of
a word found in time span TS
i
as a weighted sum
of the time span importance of the two nearest
peaks TS
lp
and TS
rp
found to the left and right
of TS
i
, respectively. In Figure 4, taking reference
from event e (shaded in black), the left peak to the
time span which e is in happens to be time span
A, while the right peak is time span A + 4. The
contribution of each peak to the weighted sum is
decayed by its distance from TS
i
. Formally, the
contextual time span importance of a word w can
be expressed as:
?(w) = ?
(
I
lp
|TS
w
? TS
lp
|
)
? (1? ?)
(
I
rp
|TS
rp
? TS
w
|
)
(2)
where TS
w
is the time span associated with w. I
lp
and I
rp
are the time span importance of the peaks
to the left and right of TS
w
respectively, while
|TS
w
? TS
lp
| and |TS
rp
? TS
w
| are the num-
ber of time spans between the left and right peaks
of TS
w
respectively. ? balances the importance of
the left and right peaks, intuitively set to 0.5. The
CTSI of a sentence is computed as:
CTSI(s) =
?
e?E
s
?(e)
|E
s
|
(3)
where E
s
denotes the set of events words in s.
3. Sentence Temporal Coverage Density
(TCD). We first define the temporal coverage of a
sentence. This corresponds to the number of time
spans that the events in a sentence talk about. Sup-
pose a sentence contains events which are associ-
ated with time spans TS
a
, TS
b
, TS
c
. The time
spans are ordered in the sequence they appear on
926
the timeline. Then the temporal coverage of a sen-
tence is defined as the number of time spans be-
tween the earliest time span TS
a
and the latest
time span TS
c
. Referring to Figure 4, suppose
a sentence contains the three events which have
been shaded black. The temporal coverage in this
case includes all the time spans from time span A
to time span A+ 4, inclusive.
The constraint on the number of sentences that
can be included in a summary requires us to select
compact sentences which contain as many rele-
vant facts as possible. Traditional lexical measures
may attempt to achieve this by computing the ra-
tio of keyphrases to the number of words in a sen-
tence (Gong and Liu, 2001). Stated equivalently,
when two sentences are of the same length, if one
contains more keyphrases, it should contain more
useful facts. TCD parallels this idea with the use
of temporal information, i.e. if two sentences are
of the same temporal coverage, then the one with
more events should carry more useful facts.
Formally, if a sentence s contains events E
s
=
{e
1
, . . . , e
n
}, where each event is associated with
a time span TS
i
, then TCD is computed using:
TCD(s) =
|E
s
|
|TS
n
? TS
1
|
(4)
where |E
s
| is the number of events found in s, and
|TS
n
? TS
1
| is the temporal coverage of s.
3.3 Enhancing MMR with TimeMMR
In the sentence re-ordering stage of the SWING
pipeline, the iterative MMR algorithm is used to
adjust the score of a candidate sentence, s. In each
iteration, s is penalized if it is lexically similar to
other sentences that have already been selected to
form the eventual summary S = {s
1
, s
2
, . . .}. The
motivating idea is to reduce repeated information
by preferring sentences which bring in new facts.
Incorporating temporal information can poten-
tially improve this. In Figure 5, the sentences de-
scribe many events which took place within the
same time span. They describe the destruction
caused by a hurricane with trees uprooted and
buildings blown away. A summary about the hur-
ricane need not contain all of these sentences as
they are all describing the same thing. However
it is not trivial for the lexically-motivated MMR
algorithm to detect that events like ?passed?, ?up-
rooted? or ?damaged? are in fact repetitive.
Thus, we propose further penalizing the score
of s if it contains events that happen in similar
time spans as those contained in sentences within
S. We refer to this as TIMEMMR. Modifying the
MMR equation from Ng et al (2012):
TimeMMR(s) = Score(s)? ?R2(s, S)? (1? ?)T (s, S) (5)
where Score(s) is the score of s, S is the set of
sentences already selected to be in the summary
from previous iterations, and R2 is the predicted
ROUGE-2 score of s with respect to the already
selected sentences (S). ? is a weighting parameter
which is empirically set to 0.9 after tuning over a
development dataset. T is the proportion of events
in swhich happen in the same time span as another
event in any other sentence in S. Two events are
said to be in the same time span if one happens
within the time period the other happens in. For
example, an event that takes place in ?2014 June?
is said to take place within the year ?2014?.
While TIMEMMR is proposed here as an im-
provement over MMR, the premise is that incor-
porating temporal information can be helpful to
minimize redundancy in summaries. In future
work, one could apply it to other state-of-the-art
lexical-based approaches including that of Hen-
drickx et al (2009) and Celikyilmaz and Hakkani-
Tur (2010). We also believe the same idea can be
transplanted even to non-lexical motivated tech-
niques such as the corpus-based similarity mea-
sure proposed by Xie and Liu (2008). We chose
to use MMR here as a proof-of-concept to demon-
strate the viability of such a technique, and to eas-
ily integrate our work into SWING.
3.4 Gauging Usefulness of Timelines
Temporal processing is imperfect. Together with
the simplifying assumptions that were made in
timeline construction, our generated timelines
have errors which propagate into the summariza-
tion process. With this in mind, we selectively em-
ploy timelines to generate summaries only when
we are confident of their accuracy. This can be
done by computing a metric which can be used to
decide whether or not timelines should be used for
a particular input document collection. We refer to
this as reliability filtering.
We postulate that the length of a timeline can
serve as a simple reliability filtering metric. The
intuition for this is that for longer timelines (which
contain more events), possible errors are spread
over the entire timeline, and do not overpower any
useful signal that can be obtained from the time-
line features outlined earlier. Errors are however
927
(1) An official in Barisal, 120 kilometres south of Dhaka, spoke of severe destruction as the 500 kilometre-wide mass of cloud
passed overhead.
(2) ?Many trees have been uprooted and houses and schools blown away,? Mostofa Kamal, a district relief and rehabilitation
officer, told AFP by telephone.
(3) ?Mud huts have been damaged and the roofs of several houses blown off,? said the state?s relief minister, Mortaza Hossain.
Figure 5: Extract from a news article which describes several events (bolded) happening at the same
time.
very easily propagated into summary generation
for shorter timelines, leading to less useful results.
We incorporate this into our process as follows:
given an input document collection (which con-
sists of 10 documents), the average size of all the
timelines for each of these 10 documents is com-
puted. Only when this value is larger than a thresh-
old value are the timelines used.
4 Experiments and Results
The proposed timeline features and TIMEMMR
were implemented on top of SWING, and eval-
uated on the test documents from TAC-2011
(Owczarzak and Dang, 2011). SWING makes use
of three generic features and two features targeted
specifically at guided summarization. Since the
focus of this paper is on multi-document summa-
rization, we employ only the three generic fea-
tures, i.e., 1) sentence position, 2) sentence length,
and 3) interpolated n-gram document frequency
in our experiments below. Summarization evalua-
tion is done using ROUGE-2 (R-2) (Lin and Hovy,
2003), as it has previously been shown to correlate
well with human assessment (Lin, 2004) and is of-
ten used to evaluate automatic text summarization.
The results obtained are shown in Table 1. In
the table, each row refers to a specific summariza-
tion system configuration. We also show the re-
sults of two reference systems, CLASSY (Conroy
et al, 2011) and POLYCOM (Zhang et al, 2011),
as benchmarks. CLASSY and POLYCOM are top
performing systems at TAC-2011 (ranked 2nd and
3rd by R-2 in TAC 2011, respectively; the full ver-
sion of SWING was ranked 1st with a R-2 score
of 0.1380). From these results, we can see that
SWING is a very competitive baseline.
Rows 9 to 16 additionally incorporate our time-
line reliability filtering. We assume that the var-
ious input document sets to be summarized are
available at the time of processing. Hence in these
experiments, the threshold for filtering is set to be
the average of all the timeline sizes over the whole
input dataset. In a production environment where
this assumption may not hold, this threshold could
Configuration R-2 Sig
R SWING 0.1339 NA
B1 CLASSY 0.1278 -
B2 POLYCOM 0.1227 **
Without Filtering
1 SWING+TSI+CTSI+TCD 0.1394 *
2 SWING+TSI+CTSI 0.1372 -
3 SWING+TSI+TCD 0.1372 -
4 SWING+CTSI+TCD 0.1387 *
5 SWING+TSI+CTSI+TCD+TMMR 0.1389 -
6 SWING+TSI+CTSI+TMMR 0.1374 -
7 SWING+TSI+TCD+TMMR 0.1343 -
8 SWING+CTSI+TCD+TMMR 0.1363 -
With Filtering
9 SWING+TSI+CTSI+TCD 0.1418 **
10 SWING+TSI+CTSI 0.1378 **
11 SWING+TSI+TCD 0.1389 **
12 SWING+CTSI+TCD 0.1401 **
13 SWING+TSI+CTSI+TCD+TMMR 0.1402 **
14 SWING+TSI+CTSI+TMMR 0.1397 **
15 SWING+TSI+TCD+TMMR 0.1376 *
16 SWING+CTSI+TCD+TMMR 0.1390 **
Table 1: R-2 scores after incorporating temporal
information into SWING. ?**? and ?*? denotes sig-
nificant differences with respect to Row R (paired
one-tailed Student?s t-test; p < 0.05 and p < 0.1
respectively), and TMMR denotes TIMEMMR.
be set by empirical tuning over a development set.
Row 1 shows the usefulness of the proposed
timeline-based features. A statistically significant
improvement of 4.1% is obtained with the use of
all three features over SWING. When we use re-
liability filtering (Row 9), this improvement in-
creases to 5.9%.
The ablation test results in Rows 2 to 4 show
a drop in R-2 each time a feature is left out. With
the exception of Row 4, removing a feature lessens
the improvement in R-2 to be insignificant from
SWING?s. The same drop occurs even when reli-
ability filtering is used (Rows 9 to 12). These in-
dicate that all the proposed features are important
and need to be used together to be effective.
Rows 5 to 8 and Rows 13 to 16 show the ef-
fect of TIMEMMR. While the results do not uni-
formly show that TIMEMMR is effective, it can be
helpful, such as when comparing Rows 2 and 6, or
Rows 10 and 14, where R-2 improves marginally.
Looking at Rows 1 to 8, and Rows 9 to 16, we
see the importance of reliability filtering. It is able
928
to guide the use of timelines such that significant
improvements in R-2 over SWING are obtained.
To help visualize what the differences in these
ROUGE scores mean, Figure 7 shows two sum-
maries
1
generated for document set D1117C of the
TAC-2011 dataset. The left one is produced by the
configuration in Row 9, and the right one is pro-
duced by SWING without the use of any temporal
information.
0.0	

0.2	

0.4	

0.6	

0.8	

1.0	

SP	
 Length	
 INDF	
 TSI	
 CTSI	
 TCD	

Fea
ture
 Sco
re	

Features	

L2	
 R2	

Figure 6: Breakdown of raw feature scores for sen-
tences (L2) and (R2) from Figure 7.
The higher R-2 score obtained by the summary
on the left (0.0873) compared to the one on the
right (0.0723) suggests that temporal information
can help to identify salient sentences more accu-
rately. A closer look at sentences (L2) and (R2)
and their R-2 scores (0.0424 and 0.0249, respec-
tively) is instructive. Figure 6 shows the raw fea-
ture scores of both sentences. Both sentences
score similarly for the SWING features of sen-
tence position (SP), sentence length (Length), and
interpolated n-gram document frequency (INDF);
however, the scores for all three timeline features
higher for (L2) than (R2). This helps our time sen-
sitive system prefer (L2).
5 Discussion
We now examine the proposed 1) timeline fea-
tures, 2) TIMEMMR algorithm, and 3) reliabil-
ity filtering metric in greater detail to gain insight
into their efficacy. For the analysis on timeline
features, we only present an analysis for TSI and
CTSI due to space constraints.
Time Span Importance. Figure 8 shows the
last sentences from a pair of summaries generated
with and without the use of TSI (all other sen-
tences were the same). The original articles de-
scribe an accident where casualties were suffered
when a crane toppled onto a building. It is easy to
see why (L1) scores higher for R-2 ? it describes
the cause of the accident just as it occurred. (R1)
however talks about events which happened before
1
The produced summaries are truncated to fit within a
100-word limit imposed by the TAC-2011 guidelines.
the accident itself (e.g., how much of the tower had
already been erected). In this case time span im-
portance is able to correctly guide summary gen-
eration by favoring time spans containing events
related to the actual toppling.
Contextual Time Span Importance. CTSI
recognizes that events which happen around the
time of a big cluster of other events can be im-
portant too. The benefits of this feature can be
clearly seen in Figure 9. The summary on the left
achieved a R-2 score of 0.1215 while the one on
the right achieved 0.0861. (L2) and (L3) were
both boosted by the use of the contextual impor-
tance feature.
Figure 10 shows an extract of the timeline gen-
erated for the source document from which (L3)
is extracted. The two events inside (L3) fall in
time spans A and B marked in the figure. Their
proximity to the peak P between them gives the
sentence a higher score for CTSI. This boost re-
sults in the sentence being selected for inclusion
in the final summary. It turns out that this sentence
was lifted exactly in one of the model summaries
for this document set, resulting in a very good R-2
score when contextual importance is used.
warn disappear
Peak here affects time span contextual importance of A and B
A BP
Figure 10: Extract of timeline generated for doc-
ument APW ENG 20070615.0356 from the TAC-
2011 dataset.
Is TIMEMMR Useful? The experimental re-
sults do not conclusively affirm the usefulness of
TIMEMMR. However we believe it is because
the ROUGE measures that are used for evalua-
tion are not suited for this purpose. Recall that
TIMEMMR seeks to eliminate redundancy based
on time span similarities and not lexical likeness.
ROUGE, however, measures the latter.
An interesting case in point is given in Fig-
ure 11. The summary on the left is generated
using TIMEMMR and achieved a lower ROUGE
score. The one on the right is generated with-
out TIMEMMR and scores higher, suggesting that
TIMEMMR is not helpful. The key difference in
929
R-2: 0.0873 R-2: 0.0723
(L1,R1) The Army?s surgeon general criticized stories in The Washington Post disclosing problems at Walter
Reed Army Medical Center, saying the series unfairly characterized the living conditions and care for soldiers
recuperating from wounds at the hospital?s facilities.
(L2) Defense Secretary Robert Gates says people
found to have been responsible for allowing sub-
standard living conditions for soldier outpatients at
Walter Reed Army Medical Center in Washington
will be ?held accountable,? although so far no one
in the Army chain of command has offered to re-
sign.
6= 6= (R2) A top Army general vowed to personally
oversee the upgrading of Walter Reed Army Medi-
cal Center?s Building 18, a dilapidated former hotel
that houses wounded soldiers as outpatients.
(L3) Top Army officials visited Building 18, the
decrepit former hotel housing more than 80 recov-
ering soldiers, outside
6= 6= (R3) ?I?m not sure it was an accurate representa-
tion,? Lt. Gen. Kevin Kiley, chief of the Army
Medical Command which oversees Walter Reed
and all Army health care, told reporters during a
news conference.
>>
(R4) The Washington
Figure 7: Generated summaries for document set D1117C from the TAC-2011 dataset. Left summary is
generated by SWING+TSI+CTSI+TCD with filtering; right summary is by SWING.
R-2: 0.1683 R-2: 0.1533
. . . . . . . . . . . .
(L1) A piece of steel fell and sheared off one of the
ties holding it to the building, causing it to detach
and topple, said Stephen Kaplan
6= 6=
(R1) About 19 of the 44 stories of the crane had
been erected and it was to be extended when a
piece of steel fell and sheared
Figure 8: Extract from summaries for document set D1137G from the TAC-2011 dataset. Left extract is
generated by SWING+TSI+CTSI+TCD; right extract is by SWING+CTSI+TCD.
the two summaries is (R3). (L3) is the equivalent
of (R4), while (L4) is the full version of the trun-
cated (R5). TIMEMMR penalizes (R3). (R3) re-
ports that the shoe-throwing incident happened as
the U.S. President Bush appeared together with the
Iraqi Prime Minister Nouri al-Maliki. However
their joint appearance is already reported in (R1)
(and similarly (L1)). (R3) repeats what had been
presented earlier. Since (R1) and (R3) talk about
the same time span, TIMEMMR down-weights
(R3). We argue that this is better even though the
ROUGE scores indicate otherwise. In future work
it will be worthwhile to consider the use of metrics
like Pyramid (Passonneau et al, 2005) which are
less bound to superficial lexicons.
Reliability Filtering. Table 2 shows the ef-
fect of varying the filtering threshold on R-2 for
the best performing configuration from Table 1
(i.e., SWING+TSI+CTSI+TCD). The result ob-
tained in Row 9 using a threshold of 42.68 is also
re-produced for reference. T=0 means that time-
lines are used for all input document sets, whereas
T=100 means that no timelines are used, as the
length of the longest timeline is less than 100.
As the threshold increases from 0 to 40?50,
summarization performance improves while the
T R-2 Sig # T R-2 Sig #
0 0.1394 * 44 50 0.1386 ** 13
10 0.1382 - 43 60 0.1361 * 7
20 0.1377 - 41 70 0.1351 - 3
30 0.1393 ** 35 80 0.1351 - 2
40 0.1426 ** 22 90 0.1353 - 1
42.68 0.1418 ** 21 100 0.1339 - 0
Table 2: Effect of different reliability filtering
thresholds for SWING+TSI+CTSI+TCD. ?T? is
the threshold used; ?#? is the number of input col-
lections (out of 44) where timelines are used; ?**?
and ?*? is statistical significance over SWING of
p < 0.05 and p < 0.1, respectively.
number of document sets where temporal informa-
tion is used is reduced. This suggests that filtering
is successful in identifying timelines that are not
sufficiently accurate to be useful for summariza-
tion. R-2 performance peaks around a threshold
of 40. This affirms our use of the average length
of timelines as the threshold value in our earlier
experiments. Beyond 60, the R-2 scores are still
higher than that obtained by SWING, but no longer
significantly different. At these higher thresholds,
temporal information is still able to help get an im-
provement in R-2. However as this affects only
very few out of the 44 document sets, statistical
variances mean that these R-2 scores are no longer
930
R-2: 0.1215 R-2: 0.0861
((L1,R1) Caribbean coral species essential to the region?s reef ecosystems are at risk of extinction as a result of
climate change.
(L2) But destructive fishing methods and over-
harvesting have reduced worldwide catches by 90
percent in the past two decades.
6= 6= (R2) The Coral Reef Task Force, created in the
Clinton administration, regularly assesses coral
health.
(L3) Scientists warn that up to half of the world?s
coral reefs could disappear by 2045.
6= 6= (R3) With a finished necklace retailing for up
to 20,000 dollars (15,000 euros), red corals are
among the world?s most expensive wildlife com-
modities.
. . . . . . . . . . . .
Figure 9: Extract from summaries for document set D1131F from the TAC-2011 dataset. Left extract is
generated by SWING+TSI+CTSI+TCD; right extract is by SWING+TSI+TCD.
R-2: 0.2643 R-2: 0.2772
(L1,R1) ? An Iraqi reporter threw his shoes at visiting U.S. President George W. Bush and called him a ?dog? in
Arabic during a news conference with Iraqi Prime Minister Nuri al-Maliki in Baghdad
(L2,R2) ?All I can report is it is a size 10,.
(L3) Muntadhar al-Zaidi, reporter of Baghdadiya
television jumped and threw his two shoes one by
one at the president, who ducked and thus narrowly
missed being struck, raising chaos in the hall in
Baghdad?s heavily fortified green Zone.
6= 6= (R3) The incident occurred as Bush was appearing
with Iraqi Prime Minister Nouri al-Maliki.
(L4) The president lowered his head and the first
shoe hit the American and Iraqi flags behind the
two leaders.
6= 6=
(R4) Muntadhar al-Zaidi, reporter of Baghdadiya
television jumped and threw his two shoes one by
one at the president, who ducked and thus narrowly
missed being struck, raising chaos in the hall in
Baghdad?s heavily fortified green Zone.
(L5) The 6= 6=
(R5) The president lowered his head and the
Figure 11: Summaries for document set D1126E from the TAC-2011 dataset. Left summary is generated
by SWING+TSI+CTSI+TCD+TIMEMMR; right summary is by SWING+TSI+CTSI+TCD.
significant from that produced by SWING.
6 Conclusion
We have shown in this work how temporal in-
formation in the form of timelines can be incor-
porated into multi-document summarization. We
achieve this through two means, using: 1) three
novel features derived from timelines to mea-
sure the saliency of sentences, and 2) TIMEMMR
which considers time span similarity to enhance
the traditional MMR?s lexical diversity measure.
To overcome errors propagated from the under-
lying temporal processing systems, we proposed
a reliability filtering metric which can be used to
help decide when temporal information should be
used for summarization. The use of this metric
leads to an overall 5.9% gain in R-2 over the com-
petitive SWING baseline.
In future work, we are keen to study our pro-
posed timeline-related features more intrinsically
in the context of human-generated summaries.
This can help us better understand their value in
improving content selection. As noted earlier,
it will be also be useful to repeat our experi-
ments with less lexicon-influenced measures like
the Pyramid method (Passonneau et al, 2005).
Manual assessment of the generated summaries
can also be done to give a better picture of the
quality of the summaries generated with the use
of timelines. Finally, given the importance of re-
liability filtering, a natural question is if there are
other metrics that can be used to get better results.
Acknowledgments
This research is supported by the Singapore Na-
tional Research Foundation under its International
Research Centre @ Singapore Funding Initiative
and administered by the IDM Programme Office.
This work is also partially supported by the
National Natural Science Foundation of China
(Grant Nos. 61170189, 61370126, 61202239),
the Fund of the State Key Laboratory of Software
Development Environment (Grant No. SKLSDE-
2013ZX-19), and the Innovation Foundation of
Beihang University for Ph.D. Graduates (YWF-
13-T-YJSY-024).
931
References
Javed Aslam, Matthew Ekstrand-Abueg, Virgil Pavlu,
Fernado Diaz, and Tetsuya Sakai. 2013. TREC
2013 Temporal Summarization. In Proceedings of
the 22nd Text Retrieval Conference (TREC), Novem-
ber.
Regina Barzilay, Kathleen McKeown, and Michael El-
hadad. 1999. Information Fusion in the Context of
Multi-document Summarization. In Proceedings of
the 37th Annual Meeting of the Association for Com-
putational Linguistics on Computational Linguistics
(ACL), pages 550?557, June.
Jaime Carbonell and Jade Goldstein. 1998. The Use
of MMR, Diversity-based Reranking for Reordering
Documents and Producing Summaries. In Proceed-
ings of the 21st Annual International ACM Confer-
ence on Research and Development in Information
Retrieval (SIGIR), pages 335?336, August.
Asli Celikyilmaz and Dilek Hakkani-Tur. 2010. A Hy-
brid Hierarchical Model for Multi-document Sum-
marization. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 815?824, July.
John M. Conroy, Judith D. Schlesinger, Jeff Kubina,
Peter A. Rankel, and Dianne P. O?Leary. 2011.
CLASSY 2011 at TAC: Guided and Multi-lingual
Summaries and Evaluation Metrics. In Proceedings
of the Text Analysis Conference (TAC), November.
Gianluca Demartini, Malik Muhammad Saad Missen,
Roi Blanco, and Hugo Zaragoza. 2010. Entity
Summarization of News Articles. In Proceedings of
the 33rd Annual International ACM Conference on
Research and Development in Information Retrieval
(SIGIR), pages 798?796, July.
Pascal Denis and Philippe Muller. 2011. Predicting
Globally-Coherent Temporal Structures from Texts
via Endpoint Inference and Graph Decomposition.
In Proceedings of the 22nd International Joint Con-
ference on Artificial Intelligence (IJCAI), July.
Quang Xuan Do, Wei Lu, and Dan Roth. 2012. Joint
Inference for Event Timeline Construction. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP),
pages 677?689, July.
Lisa Ferro, Laurie Gerber, Inderjeet Mani, Beth Sund-
heim, and George Wilson. 2000. Instruction Man-
ual for the Annotation of Temporal Expressions.
Technical report, The MITRE Corporation.
Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and
Mark Kantrowitz. 2000. Multi-document Sum-
marization by Sentence Extraction. In Proceedings
of the 2000 NAACL-ANLP Workshop on Automatic
Summarization, volume 4, pages 40?48, April.
Yihong Gong and Xin Liu. 2001. Generic Text Sum-
marization Using Relevance Measure and Latent Se-
mantic Analysis. In Proceedings of the 24th Annual
International ACM Conference on Research and De-
velopment in Information Retrieval (SIGIR), pages
19?25, September.
Iris Hendrickx, Walter Daelemans, Erwin Marsi, and
Emiel Krahmer. 2009. Reducing Redundancy in
Multi-document Summarization using Lexical Se-
mantic Similarity. In Proceedings of the Workshop
on Language Generation and Summarisation (UC-
NLG+Sum), pages 63?66, August.
Chin-Yew Lin and Eduard Hovy. 2003. Auto-
matic Evaluation of Summaries Using N-gram Co-
occurrence Statistics. In Proceedings of the Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics on Human Lan-
guage Technology (NAACL), volume 1, pages 71?
78, May.
Chin-Yew Lin. 2004. Looking for a Few Good Met-
rics: ROUGE and its Evaluation. In Working Notes
of the 4th NTCIR Workshop Meeting, June.
Maofu Liu, Wenjie Li, and Huijun Hu. 2009. Extrac-
tive Summarization Based on Event Term Temporal
Relation Graph and Critical Chain. In Information
Retrieval Technology, volume 5839 of Lecture Notes
in Computer Science, pages 87?99. Springer Berlin
Heidelberg.
Jun-Ping Ng and Min-Yen Kan. 2012. Improved
Temporal Relation Classification using Dependency
Parses and Selective Crowdsourced Annotations.
In Proceedings of the International Conference on
Computational Linguistics (COLING), pages 2109?
2124, December.
Jun-Ping Ng, Praveen Bysani, Ziheng Lin, Min-Yen
Kan, and Chew-Lim Tan. 2012. Exploiting
Category-Specific Information for Multi-Document
Summarization. In Proceedings of the International
Conference on Computational Linguistics (COL-
ING), pages 2093?2108, December.
Jun-Ping Ng, Min-Yen Kan, Ziheng Lin, Wei Feng, Bin
Chen, Jian Su, and Chew-Lim Tan. 2013. Exploit-
ing Discourse Analysis for Article-Wide Temporal
Classification. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 12?23, October.
Karolina Owczarzak and Hoa Dang. 2011. Overview
of the TAC 2011 Summarization Track: Guided
Task and AESOP Task. In Proceedings of the Text
Analysis Conference (TAC), November.
Rebecca J. Passonneau, Ani Nenkova, Kathleen McK-
eown, and Sergey Sigelman. 2005. Applying the
Pyramid Method in DUC 2005. In Proceedings of
the Document Understanding Conference Workshop
on Text Summarization, October.
932
James Pustejovsky, Jos?e Castano, Robert Ingria, Roser
Saur??, Robert Gaizauskas, Andrea Setzer, and Gra-
ham Katz. 2003a. TimeML: Robust Specification
of Event and Temporal Expressions in Text. In Pro-
ceedings of the 5th International Workshop on Com-
putational Semantics (IWCS), January.
James Pustejovsky, Patrick Hanks, Roser Sauri, An-
drew See, Robert Gaizauskas, Andrea Setzer,
Dragomir Radev, Beth Sundheim, David Day, Lisa
Ferro, and Marcia Lazo. 2003b. The TIMEBANK
corpus. In Proceedings of Corpus Linguistics, pages
647?656, March.
Jannik Str?otgen and Michael Gertz. 2013. Multilin-
gual and Cross-domain Temporal Tagging. Lan-
guage Resources and Evaluation, 47(2):269?298.
Naushad Uzzaman, Hector Llorens, Leon Derczynski,
Marc Verhagen, James F. Allen, and James Puste-
jovsky. 2013. SemEval-2013 Task 1: TEMPEVAL-
3: Evaluating Time Expressions, Events, and Tem-
poral Relations. In Proceedings of the 7th Interna-
tional Workshop on Semantic Evaluation (SemEval),
June.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Jessica Moszkowicz, and James
Pustejovsky. 2009. The TempEval Challenge: Iden-
tifying Temporal Relations in Text. Language Re-
sources and Evaluation, 43(2):161?179.
Marc Verhagen, Roser Saur??, Tommaso Caselli, and
James Pustejovsky. 2010. SemEval-2010 Task 13:
TempEval-2. In Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation (SemEval),
pages 57?62, July.
Xiaojun Wan. 2007. TimedTextRank: Adding the
Temporal Dimension to Multi-Document Summa-
rization. In Proceedings of the 30th Annual Interna-
tional ACM Conference on Research and Develop-
ment in Information Retrieval (SIGIR), pages 867?
868, July.
Mingli Wu. 2008. Investigations on Temporal-
Oriented Event-Based Extractive Summarization.
Ph.D. thesis, Hong Kong Polytechnic University.
Shasha Xie and Yang Liu. 2008. Using Corpus
and Knowledge-based Similarity Measure in Max-
imum Marginal Relevance for Meeting Summariza-
tion. In Proceedings of the International Confer-
ence on Acoustics, Speech, and Signal Processing
(ICASSP), pages 4985?4988, March.
Renxian Zhang, You Ouyang, and Wenjie Li. 2011.
Guided Summarization with Aspect Recognition. In
Proceedings of the Text Analysis Conference (TAC),
November.
933
