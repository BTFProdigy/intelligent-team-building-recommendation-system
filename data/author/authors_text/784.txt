A Finite State and Data-Oriented Method for Grapheme to 
Phoneme Conversion 
Gosse Bouma 
Al fa - in fo rmat ica  
R i jksun ivers i te i t  Gron ingen 
Postbus  716 
9700 AS Gron ingen 
The  Nether lands  
gosse@let, rug. nl 
Abst ract  
A finite-state method, based on leftmost longest- 
match replacement, is presented for segmenting 
words into graphemes, and for converting raphemes 
into phonemes. A small set of hand-crafted conver- 
sion rules for Dutch achieves a phoneme accuracy of 
over 93%. The accuracy of the system is further im- 
proved by using transformation-based learning. The 
phoneme accuracy of the best system (using a large 
rule and a 'lazy' variant of Brill's algoritm), trained 
on only 40K words, reaches 99%. 
1 In t roduct ion  
Automatic grapheme to phoneme conversion (i.e. 
the conversion of a string of characters into a string 
of phonemes) is essential for applications of text 
to speech synthesis dealing with unrestricted text, 
where the input may contain words which do not 
occur in the system dictionary. Furthermore, a
transducer for grapheme to phoneme conversion 
can be used to generate candidate replacements in
a (pronunciation-sensitive) spelling correction sys- 
tem. When given the pronunciation of a misspelled 
word, the inverse of the grapheme to phoneme trans- 
ducer will generate all identically pronounced words. 
Below, we present a method for developing such 
grapheme to phoneme transducers based on a com- 
bination of hand-crafted conversion rules, imple- 
mented using finite state calculus, and automatically 
induced rules. 
The hand-crafted system is defined as a two- 
step procedure: segmentation of the input into a 
sequence of graphemes (i.e. sequences of one or 
more characters typically corresponding to a sin- 
gle phoneme) and conversion of graphemes into (se- 
quences of) phonemes. The composition of the 
transducer which performs segmentation and the 
transducer defined by the conversion rules, is a 
transducer which converts sequences of characters 
into sequences of phonemes. 
Specifying the conversion rules is a difficult task. 
Although segmentation of the input can in princi- 
ple be dispensed with, we found that writing con- 
version rules for segmented input substantially re- 
duces the context-sensitivity and order-dependence 
of such rules. We manually developed a grapheme to 
phoneme transducer for Dutch data obtained from 
CELEX (Baayen et al, 1993) and achieved a word ac- 
curacy of 60.6% and a phoneme accuracy of 93.6%. 
To improve the performance of our system, we 
used transformation-based l arning (TBL) (Brill, 
1995). Training data are obtained by aligning the 
output of the hand-crafted finite state transducer 
with the correct phoneme strings. These data can 
then be used as input for TBL, provided that suit- 
able rule templates are available. We performed sev- 
eral experiments, in which the amount of' training 
data, the algorithm (Brill's original formulation and 
'lazy' variants (Samuel et al, 1998)), and the num- 
ber of rule templates varied. The best experiment 
(40K words, using a 'lazy' strategy with a large set 
of rule templates) induces over 2000 transformation 
rules, leading to 92.6% word accuracy and 99.0% 
phoneme accuracy. This result, obtained using a 
relatively small set of training data, compares well 
with that of other systems. 
2 F in i te  State Calculus 
As argued in Kaplan and Kay (1994), Karttunen 
(1995), Karttunen et al (1997), and elsewhere, 
many of the rules used in phonology and morphol- 
ogy can be analysed as special cases of regular ex- 
pressions. By extending the language of regular ex- 
pressions with operators which capture the interpre- 
tation of linguistic rule systems, high-level inguis- 
tic descriptions can be compiled into finite state au- 
tomata directly. Furthermore, such automata can be 
combined with other finite state automata perform- 
ing low-level tasks such as tokenization or lexical- 
lookup, or more advanced tasks such as shallow pars- 
ing. Composition of the individual components into 
a single transducer may lead to highly efficient pro- 
cessing. 
The system described below was implemented us- 
ing FSA Utilities, 1 a package for implementing and 
manipulating finite state automata, which provides 
possibilities for defining new regular expression oper- 
I www. let. rug. n l / -vannoord/fs  a/ 
303 
\[\] 
\[R~,..., R,,\] 
{R1,... ,Rn} 
R - 
ignore (A,B) 
AxB 
identity(A) 
ToU 
macro  (Term, R) 
the empty string 
concatenation 
disjunction 
optionality 
ignore: A interspersed with elements of B 
cross-product: he transducer which maps 
all strings in A to all strings in B. 
identity: the transducer which maps each 
element, in A onto itself. 
composition of the transducers T and U. 
use Term as an abbreviation for R (where Term and R may contain variables). 
Figure 1: A fragment of FSA regular expression syntax. A and B are regular expressions denoting recognizers, 
T and U transducers, and R can be either. 
ators. The part of FSA's built-in regular expression 
syntax relevant o this paper, is listed in figure 1. 
One particular useful extension of the basic syn- 
tax of regular expressions i the replace-operator. 
Karttunen (1995) argues that many phonological 
and morphological rules can be interpreted as rules 
which replace a certain portion of the input string. 
Although several implementations of the replace- 
operator are proposed, the most relevant case for 
our purposes is so-called 'leftmost longest-match' re- 
placement. In case of overlapping rule targets in the 
input, this operator will replace the leftmost arget, 
and in cases where a rule target contains a prefix 
which is also a potential target, the longer sequence 
will be replaced. Gerdemann and van Noord (1999) 
implement leftmost longest-match replacement in 
FSA as the operator 
replace(Target, LeftContext,RightContext), 
where Target is a transducer defining the actual re- 
placement, and LeftContext and RightContext are 
regular expressions defining the left- and rightcon- 
text of the rule, respectively. 
An example where leftmost replacement is use- 
ful is hyphenation. Hyphenation of (non-compound) 
words in Dutch amounts to segmenting a word 
into syllables, separated by hyphens. In cases 
where (the written form of) a word can in prin- 
ciple be segmented in several ways (i.e. the se- 
quence a l fabet  can be segmented as a l - fa -bet ,  
a l - fab -e t ,  a l l -a -bet ,  or a l f -ab -e t ) ,  the seg- 
mentation which maximizes onsets is in general the 
correct one (i.e. a l - fa -bet ) .  This property of hy- 
phenation is captured by leftmost replacement: 
macro(hyphenate,  
rep lace( \ [ \ ]  x - ,  sy l lab le ,  sy l lab le ) ) .  
Leftmost replacement ensures that hyphens are in- 
troduced 'eagerly', i.e. as early as possible. Given 
a suitable definition of sy l lab le ,  this ensures that 
wherever a consonant can be final in a coda or initial 
in the next onset, it is in fact added to the onset. 
The segmentation task discussed below makes cru- 
cial use of longest match. 
3 A f in i te s tate  method for 
grapheme to phoneme convers ion  
Grapheme to phoneme conversion is implemented as 
the composition of four transducers: 
macro  (graph2phon, 
segmentat ion 7, segment he input 
o mark_begin_end 7, add ' #' 
o convers ion 7. apply ru les  
o clean_up ) .  Z remove markers 
An example of conversion including the in- 
termediate steps is given below for the word 
aanknopingspunt (connection-point). 
input: aanknopingspunt 
s: aa-n-k-n-o-p-i-ng-s-p-u-n-t- 
m: #-aa-n-k-n-o-p-i-ng-s-p-u-n-t-# 
co: #-a+N+k-n-o -p - I+N+s-p-}+n- t -# 
cl: aNknopINsp}nt 
The first transducer (segmentation) takes as 
its input a sequence of characters and groups 
these into segments. The second transducer 
(mark_begin_end) adds a marker ( '~')  to the be- 
ginning and end of the sequence of segments. The 
third transducer (convers ion)  performs the actual 
conversion step. It converts each segment into a 
sequence of (zero or more) phonemes. The final 
step (clean_up) removes all markers. The output 
is a list of phonemes in the notation used by CELEX 
(which can be easily translated into the more com- 
mon SAMPA-notation). 
3.1 Segmentat ion  
The goal of segmentation is to divide a word into a 
sequence of graphemes, providing a convenient input 
304 
level of representation for the actual grapheme to 
phoneme conversion rules. 
While there are many letter-combinations which 
are realized as a single phoneme (ch, ng, aa, bb, 
.. ), it is only rarely the case that a single letter is 
mapped onto more than one phoneme (x), or that a 
letter receives no pronunciation at all (such as word- 
final n in Dutch, which is elided if it is proceeded by 
a schwa). As the number of cases where multiple 
letters have to be mapped onto a single phoneme 
is relatively high, it is natural to model a letter to 
phoneme system as involving two subtasks: segmen- 
tation and conversion. Segmentation splits an input 
string into graphemes, where each grapheme typ- 
ically, but not necessarily, corresponds to a single 
phoneme. 
Segmentation is defined as: 
macro(segmentat ion,  
rep lace(  
\ [ ident i ty (graphemes) ,  
) .  
\[\]x - \ ] , \ [ \ ] , \ [ \ ] )  
The macro graphemes defines the set of graphemes. 
It contains 77 elements, some of which are: 
a, aa, au, ai, aai, e, ee, el, eu, eau, 
eeu, i, ie, lee, ieu, ij, o, oe, oei,.. 
Segmentation attaches the marker ' - '  to each 
grapheme. Segmentation, as it is defined here, 
is not context-sensitive, and thus the second and 
third arguments of replace are simply empty. As 
the set of graphemes contains many elements which 
are substrings of other graphemes (i.e. e is a 
substring of ei, eau, etc.), longest-match is es- 
sential: the segmentation of be iaard ie r  (caril- 
lon player) should be b -e i -aa - r -d - ie - r -  and not 
b -e - i -a -a - r -d - i -e - r - .  This effect can be ob- 
tained by making the segment itself part of the tar- 
get of the replace statement. Targets are identi- 
fied using leftmost longest-match, and thus at each 
point in the input, only the longest valid segment is 
marked. 
The set of graphemes contains a number of ele- 
ments which might seem superfluous. The grapheme 
aa?, for instance, translates as aj, a sequence which 
could also be derived on the basis of two graphemes 
aa and ?. However, if we leave out the segment 
aa?, segmentation (using leftmost longest match) of 
words such as waaien (to blow) would lead to the 
segmentation w-aa- ie -n ,  which is unnatural, as it 
would require an extra conversion rule for ?e. Us- 
ing the grapheme aai  allows for two conversion rules 
which always map aai  to aj and ?e goes to ?. 
Segmentation as defined above provides the in- 
tuitively correct result in almost all cases, given a 
suitably defined set of graphemes. There are some 
cases which are less natural, but which do not nec- 
essarily lead to errors. The grapheme u, for in- 
stance, almost always goes to ' l ' ,  but translates 
as 'e , j ,} '  in (loan-) words such as museum and 
petroleum. One might argue that a segmentation 
e-u-  is therefore required, but a special conver- 
sion rule which covers these exceptional cases (i.e. 
eu followed by m) can easily be formulated. Simi- 
larly, ng almost always translates as N, but in some 
cases actually represents the two graphemes n-g- ,  
as in aaneenges loten (connected), where it should 
be translated as NG. This case is harder to detect, 
and is a potential source of errors. 
3.2 The  Convers ion  Ru les  
The g2p operator is designed to facilitate the formu- 
lation of conversion rules for segmented input: 
macro(g2p(Target,LtCont,RtCont), 
replace(\[Target, - x +\],  
\[ignore(LtCont,{+,-}), {-,+}\], 
ignore(RtCont,{+,-}) 
) 
). 
The g2p-operator implements a special pro:pose ver- 
sion of the replace-operator. The replacement of the 
marker '-' by '+ '  in the target ensures that g2p- 
conversion rules cannot apply in sequence to the 
same grapheme. 2 Second, each target of the g2p- 
operator must be a grapheme (and not some sub- 
string of it). This is a consequence of the fact that 
the final element of the left-context must be a marker 
and the target itself ends in '-'. Finally, the ig- 
nore statements in the left and right context imply 
that the rule contexts can abstract over the potential 
presence of markers. 
An overview of the conversion rules we used for 
Dutch is given in Figure 2. As the rules are ap- 
plied in sequence, exceptional rules can be ordered 
before the regular cases, thus allowing the regular 
cases to be specified with little or no context. The 
special_vowel_rules deal with exceptional trans- 
lations of graphemes such as eu or cases where i or 
i j  goes to '?'. The short_vowel_rules treat sin- 
gle vowels preceding two consonants, or a word final 
consonant. One problematic ase is e, which can 
be translated either as 'E '  or '~ ' .  Here, an ap- 
proximation is attempted which specifies tile con- 
text where e goes 'E ' ,  and subsumes the other 
case under the general rule for short vowels. Tile 
specia l_consonant_ru les address devoicing and a 
few other exceptional cases. The de fau l t _ ru les  
supply a default mapping for a large number of 
2Note that the input and output alphabet are not disjoint, 
and thus rules applying in sequence to the same part of the 
input are not excluded in principle. 
305 
graphemes. The target of this rule is a long disjunc- 
tion of grapheme-phoneme appings. As this rule- 
set applies after all more specific: cases have been 
dealt with, no context restrictions need to be speci- 
fied. 
Depending somewhat on how one counts, the full 
set of conversion rules for Dutch contains approxi- 
mately 80 conversion rules, more than 40 of which 
are default mappings requiring no context. 3 Compi- 
lation of the complete system results in a (minimal, 
deterministic) transducer with 747 states and 20,123 
transitions. 
3.3 Test  resu l ts  and discussion 
The accuracy of the hand-crafted system was evM- 
uated by testing it on all of tile words wihtout di- 
acritics in the CELEX lexical database which have a 
phonetic transcription. After several development 
cycles, we achieved a word accuracy of 60.6% and 
a phonenle accuracy (measured as the edit distance 
between the phoneme string produced by the sys- 
tem and the correct string, divided by the number 
of phonemes in the correct string) of 93.6%. 
There have been relatively few attempts at devel- 
oping grapheme to phoneme conversion systems us- 
ing finite state technology alone. Williams (1994) re- 
ports on a system for Welsh, which uses no less than 
700 rules implemented in a rather estricted environ- 
ment. The rules are also implemented in a two-level 
system, PC-KIMMO, (Antworth, 1990), but this still 
requires over 400 rules. MSbius et al (1997) report 
on full-fledged text-to-speech system for German, 
containing around 200 rules (which are compiled into 
a weighted finite state transducer) for the grapheme- 
to-phoneme conversion step. These numbers uggest 
that our implementation (which contains around 80 
rules in total) benefits considerably from the flexibil- 
ity and high-level of abstraction made available by 
finite state calculus. 
One might suspect hat a two-level approach to 
grapheme to phoneme conversion is more appropri- 
ate than the sequential approach used here. Some- 
what surprisingly, however, Williams concludes that 
a sequential approach is preferable. The formulation 
of rules in the latter approach is more intuitive, and 
rule ordering provides a way of dealing with excep- 
tional cases which is not easily available in a two- 
level system. 
While further improvements would definitely have 
been possible at this point, it becomes increasingly 
difficult to do this on the basis of linguistic knowl- 
edge alone. That is, most of the rules which have 
to be added deal with highly idiosyncratic cases (of- 
ten related to loan-words) which can only be discov- 
3It should be noted that  we only considered words which 
do not contain diacritics. Including those is unproblematic  n
principle, but  would lead to a slight increase of the number  
of rules. 
ered by browsing through the test results of previ- 
ous runs. At this point, switching from a linguistics- 
oriented to a data-oriented methodology, seemed ap- 
propriate. 
4 T rans format ion-based  grapheme 
to  phoneme convers ion  
Brill (1995) demonstrates that accurate part-of- 
speech tagging can be learned by using a two-step 
process. First, a simple system is used which as- 
signs the most probable tag to each word. The re- 
sults of the system are aligned with the correct ags 
for some corpus of training data. Next, (context- 
sensitive) transformation rules are selected from a 
pool of rule patterns, which replace erroneous tags 
by correct ags. The rule with the largest benefit on 
the training data (i.e. the rule for which the number 
of corrections minus the number of newly introduced 
mistakes, is the largest) is learned and applied to 
the training data. This process continues until no 
more rules can be found which lead to improvement 
(above a certain threshold). 
Transformation-based l arning (TBL) can be ap- 
plied to the present problem as well. 4 In this case, 
the base-line system is the finite state transducer de- 
scribed above, which can be used to produce a set 
of phonemic transcriptions for a word list. Next, 
these results are aligned with the correct transcrip- 
tions. In combination with suitable rule patterns, 
these data can be used as input for a TBL process. 
4.1 Alignment 
TBL requires aligned data for training and testing. 
While alignment is mostly trivial for part-of-speech 
tagging, this is not the case for the present task. 
Aligning data for grapheme-to-phoneme conversion 
amounts to aligning each part of the input (a se- 
quence of characters) with a part of the output (a 
sequence of phonemes). As the length of both se- 
quences is not guaranteed to be equal, it must be 
possible to align more than one character with a 
single phoneme (the usual case) or a single character 
with more than one phoneme (the exceptional case, 
i.e. 'x'). The alignment problem is often solved (Du- 
toit, 1997; Daelemans and van den Bosch, 1996) by 
allowing 'null' symbols in the phoneme string, and 
introducing 'compound' phonemes, such as 'ks' to 
account for exceptional cases where a single charac- 
ter must be aligned with two phonemes. 
As our finite state system already segments the 
input into graphemes, we have adopted a strategy 
where graphemes instead of characters are aligned 
with phoneme strings (see Lawrence and Kaye 
(1986) for a similar approach). The correspondence 
4Hoste et al (2000b) compare TBL  to C5.0 (Quinlan, 
1993) on a similar task, i.e. the mapping  of the pronunciat ion 
of one regional variant of Dutch into another.  
306 
macro(conversion, special_vowel_rules o short_vowel_rules 
o special_consonant_rules o default_rules 
macro (special_vowel_rules, 
g2p( \ [e ,u \ ]  x \ [e , j ,} \ ] ,  \ [ \ ] ,  m) 
o g2p( i  x ?, \ [ \ ] ,  g) 
o g2p( \ [ i , j \ ]  x @, 1, k) 
. . . .  . 
macro (shor t_vowe l_ ru les ,  
g2p(e x 'E ' ,  \ [ \ ] ,  { \ [ t , t \ ] , \ [k ,k \ ] ,x , . . .} )  
g2p({ a x 'h '  , e x 0, i x ' I '  , o x '0 '  
) .  
macro (special_consonant_rules, 
g2p(b x p, \ [ \ ] ,  {s , t ,#})  
o g2p(\[d,t ~\] x t ,  \[\], {s,g,k,j,v,h,z,#}) 
o g2p({ f x v, s x z}, \[\], {b,d}) 
o g2p(g x 'G', vowel, vowel) 
o g2p(n x 'N', \[\], {k,q}) 
o g2p(n x \ [ \ ] ,  \ [?\] , \ [#\] )  
. . . ) .  
macro (de fau l t ru les ,  
%% museum 
%% moedig(st) 
%% mogelijkheid 
. 
, u x ' } '  }, \ [ \ ] ,  \ [cons, {cons , #}\] ) 
g2p({ \ [a ,a \ ]  x a, \ [a ,a , i \ ]  x \ [a , j \ ] ,  \ [a,u\]  x 'M', \ [e ,a ,u \ ]  x o . . . . .  , 
\ [b,b\]  x b, \[d,d\] x d . . . . . .  \ [c,h\]  x ' x ' ,  \ [ s , c ,h \ ]  x I s ,x \ ] ,  \ [n,g\]  x 'N ' ,  
}, \[2, \[\] ) 
) .  
Figure 2: Conversion Rules 
between graphemes and phonemes is usually one to 
one, but it is no problem to align a grapheme with 
two or more phonemes. Null symbols are only intro- 
duced in the output if a grapheme, such as word-final 
'n ' ,  is not realized phonologically. 
For TBL, the input actually has to be aligned both 
with the system output as well as with the correct 
phoneme string. The first task can be solved triv- 
ially: since our finite state system proceeds by first 
segmenting the input into graphemes (sequences of 
characters), and then transduces each grapheme into 
a sequence of phonemes, we can obtain aligned data 
by simply aligning each grapheme with its con'e- 
sponding phoneme string. The input is segmented 
into graphemes by doing the segmentation step of 
the finite state transducer only. The corresponding 
phoneme strings can be identified by applying the 
conversion transducer to the segmented input, while 
keeping the boundary symbols '-' and '+' .  As a con- 
sequence of the design of the conversion-rules, the 
resulting sequence of separated phonemes equences 
stands in a one-to-one relationship to the graphemes. 
An example is shown in figure 3, where GR represents 
the grapheme segmented string, and sP the (system) 
phoneme strings produced by the finite state trans- 
ducer. Note that the final sP cell contains only a 
boundary marker, indicating that the grapheme 'n' 
is translated into the null phoneme. 
For the alignment between graphemes (and, idi- 
Word aalbessen (currants) 
GR aa-  1-  b -  e - ss -  e - n -  
sP a + 1- b-  @+ s + ~@+ 
cp a 1 b E s @ 
Figure 3: Alignment 
rectly, the system output) and the correct phoneme 
strings (as found in Celex), we used the 'hand- 
seeded' probabilistic alignment procedure described 
by Black et al (1998) ~. From the finite state conver- 
sion rules, a set of possible grapheme --+ phoneme se- 
quence mappings can be derived. This allowables-set 
was extended with (exceptional) mappings present 
in the correct data, but not in the haml-crafted sys- 
tem. We computed all possible aligmnents between 
(segmented) words and correct phoneme strings li- 
cenced by the allowables-set. Next, probabilities for 
all allowed mappings were estimated on the basis 
of all possible alignments, and the data was parsed 
again, now picking the most probable alignment for 
each word. To minimize the number of words that 
could not be aligned, a maximum of one unseen map- 
ping (which was assigned a low probability) was al- 
lowed per word. With this modification, only one 
out of 1000 words on average could not be aligned. '~ 
These words were discarded.The aJigned phoneme 
5Typical cases are loan words (umpires) and letter words 
(i.e. abbreviations) (abe). 
307 
method training data phonenm word induced CPU time 
(words) accuracy accuracy rules (in minutes) 
Base-line 93.6 60.6 
Brill 20K 98.0 86.1 447 162 
Brill 40K 98.4 88.9 812 858 
lazy(5) 20K 97.6 83.5 337 43 
lazy(5) 40K 98.2 87.0 701 190 
lazy(5) 60K 98.4 88.3 922 397 
lazy(10) 20K 97.7 84.3 368 83 
lazy(10) 40K 98.2 87.5 738 335 
lazy(10) 60K 98.4 88.9 974 711 
lazy(5)+ 20K 98.6 89.8 1225 186 
lazy(5)+ 40K 99.0 92.6 2221 603 
Figure 4: Experimental Results using training data produced by graph2phon 
string for the example in figure 3 is shown in the 
bottom line. Note that the final cell is empty, rep- 
resenting the null phoneme. 
4.2 The  exper iments  
For the experiments with TBL we used the #-TBL- 
package (Lager, 1999). This Prolog implementation 
of TBL is considerably more efficient (up to ten 
times faster) than Brill's original (C) implementa- 
tion. The speed-up results mainly from using Pro- 
log's first-argument indexing to access large quanti- 
ties of data efficiently. 
We constructed a set of 22 rule templates which 
replace a predicted phoneme with a (corrected) 
phoneme on the basis of the underlying segment, 
and a context consisting either of phoneme strings, 
with a maximum length of two on either side, or 
a context consisting of graphemes, with a maximal 
length of 1 on either side. Using only 20K words 
(which corresponds to almost 180K segments), and 
Brill's algorithm, we achieved a phoneme accuracy 
of 98.0% (see figure 4) on a test set of 20K words of 
unseen data. 6 Going to 40K words resulted in 98.4% 
phoneme accuracy. Note, however, that in spite of 
the relative efficiency of the implementation, CPU 
time also goes up sharply. 
The heavy computation costs of TBL are due to 
the fact that for each error in the training data, all 
possible instantiations of the rule templates which 
correct his error are generated, and for each of these 
instantiated rules the score on the whole training set 
has to be computed. Samuel et al (1998) there- 
fore propose an efficient, 'lazy', alternative, based 
on Monte Carlo sampling of the rules. For each er- 
ror in the training set, only a sample of the rules 
is considered which might correct it. As rules which 
correct a high number of errors have a higher chance 
6The statistics for less time consuming experiments were 
obtained by 10-fold cross-validation a d for the more expen- 
sive experiments by 5-fold cross-validation. 
of being sampled at some point, higher scoring rules 
are more likely to be generated than lower scoring 
rules, but no exhaustive search is required. We ex- 
perimented with sampling sizes 5 and 10. As CPU 
requirements are more modest, we managed to per- 
form experiments on 60K words in this case, which 
lead to results which are comparable with Brill's al- 
goritm applied to 40K words. 
Apart from being able to work with larger data 
sets, the 'lazy' strategy also has the advantage that 
it can cope with larger sets of rule templates. Brill's 
algorithm slows down quickly when the set of rule 
templates i extended, but for an algorithm based on 
rule sampling, this effect is much less severe. Thus, 
we also constructed a set of 500 rule templates, con- 
taining transformation rules which allowed up to 
three graphemes or phoneme sequences as left or 
right context, and also allowed for disjunctive con- 
texts (i.e. the context must contain an 'a '  at the 
first or second position to the right). We used this 
rule set in combination with a 'lazy' strategy with 
sampling size 5 (lazy(5)+ in figure 4). This led to a 
further improvement ofphoneme accuracy to 99.0%, 
and word accuracy of 92.6%, using only 40K words 
of training material. 
Finally, we investigated what the contribution was 
of using a relatively accurate training set. To this 
end, we constructed an alternative training set, in 
which every segment was associated with its most 
probable phoneme (where frequencies were obtained 
from the aligned CELEX data). As shown in figure 5, 
the initial accuracy for such as system is much lower 
than that of the hand-crafted system. The exper- 
imental results, for the 'lazy' algorithm with sam- 
pling size 5, show that the phoneme accuracy for 
training on 20K words is 0.3% less than for the cor- 
responding experiment in figure 4. For 40K words, 
the difference is still 0.2%, which, in both cases, cor- 
responds to a difference in error rate of around 10%. 
As might be expected, the number of induced rules 
308 
method training data phoneme word induced CPU time 
(words) accuracy accuracy rules (i n minutes) 
Base-line 72.9 10.8 
lazy(5) 20K 97.3 81.6 691 133 
lazy(5) 40K 98.0 86.0 1075 705 
Figure 5: Experimental results using data based on frequency. 
is much higher now, and thus cPu-requirements al o 
increase substantially. 
5 Conc lud ing  remarks  
We have presented a method for grapheme to 
phoneme conversion, which combines a hand-crafted 
finite state transducer with rules induced by a 
transformation-based learning. An advantage ofthis 
method is that it is able to achieve a high level of 
accuracy using relatively small training sets. Busser 
(1998), for instance, uses a memory-based learning 
strategy to achieve 90.1% word accuracy on the same 
task, but used 90% of the CELEX data (over 300K 
words) as training set and a (character/phoneme) 
window size of 9. Hoste et al (2000a) achieve a 
word accuracy of 95.7% and a phoneme accuracy of 
99.5% on the same task, using a combination ofma- 
chine learning techniques, as well as additional data 
obtained from a second ictionary. 
Given the result of Roche and Schabes (1997), an 
obvious next step is to compile the induced rules into 
an actual transducer, and to compose this with the 
hand-crafted transducer. It should be noted, how- 
ever, that the number of induced rules is quite large 
in some of the experiments, sothat the compilation 
procedure may require some attention. 
References 
Evan L. Antworth. 1990. PC-KIMMO : a two-level 
processor for morphological nalysis. Summer In- 
stitute of Linguistics, Dallas, Tex. 
R. H. Baayen, R. Piepenbrock, and H. van Rijn. 
1993. The CELEX Lexical Database (CD-ROM). 
Linguistic Data Consortium, University of Penn- 
sylvania, Philadelphia, PA. 
Alan Black, Kevin Lenzo, and Vincent Pagel. 1998. 
Issues in building general letter to sound rules. In 
Proceedings of the 3rd ESCA/COCSADA Work- 
shop on Speech Synthesis, pages 77-81, Jenolan 
Caves, Australia. 
Eric Brill. 1995. Transformation-based error-driven 
learning and natural anguage processing: A case 
study in part-of-speech tagging. Computational 
Linguistics, 21:543-566. 
Bertjan Busser. 1998. TreeTalk-D: a machine learn- 
ing approach to Dutch word pronunciation. In 
Proceedings TSD Conference, pages 3-8, Masaryk 
University, Czech Republic. 
W. Daelemans and A. van den Bosch. 1996. 
Language-independent da a-oriented grapheme- 
to-phoneme conversion. In Progress in Speech 
Synthesis, pages 77-90, New York. Springer Ver- 
lag. 
Thierry Dutoit. 1997. An Introduction to Tezt-to- 
Speech Synthesis. Kluwer, Dordrecht. 
Dale Gerdemann and Gertjan van Noord. 1999. 
Transducers from rewrite rules with backrefer- 
ences. In Proceedings of the Ninth Conference o/ 
the European Chapter of the Association for Com- 
putational Linguistics, pages 126-133, Bergen. 
Veronique Hoste, Walter Daelemans, Erik 
Tjong Kim Sang, and Steven Gillis. 2000a. 
Meta-learning ofphonemic annotation ofcorpora. 
ms., University of Antwerp. 
Veronique Hoste, Steven Gillis, and Walter Daele- 
mans. 2000b. A rule induction approach to mod- 
elling regional pronunciation variation, ms., Uni- 
versity of Antwerp. 
Ronald Kaplan and Martin Kay. 1994. Regular 
models of phonological rule systems. Computa- 
tional Linguistics, 20(3). 
L. Karttunen, J.P. Chanod, G. Grefenstette, and 
A. Schiller. 1997. Regular expressions tbr lan- 
guage engineering. Natural Larl.'u, age Engineering, 
pages 1-24. 
Lauri Karttunen. 1995. The replace operator. In 
33th Annual Meeting o/ the Association/or Com- 
putational Linguistics, pages 16-23, Boston, Mas- 
sachusetts. 
Torbj6rn Lager. 1999. The #-TBL System: 
Logic programming tools for transformation- 
based learning. In Proceedings of the Th, ird In- 
ternational Workshop on Computational Natu- 
ral Language Learning (CoNLL '99), pages 33-42, 
Bergen. 
S. C. G. Lawrence and G. Kaye. 1986. Alignment of 
phonemes with their corresponding orthography. 
Computer Speech and Language, 1(2):153 -165. 
Bernd MSbius, Richard Sproat, Jan van Santen, and 
Joseph Olive. 1997. The Bell L~0~s German text- 
to-speech system: An overview. In Proceedings of
the European Conference on Speech Communica- 
tion and Technology, pages 2443-2446, Rhodes. 
J. R. Quinlan. 1993. C4.5: programs for machine 
learning. Morgan Kaufmann Publishers, San Ma- 
rco. 
309 
Emmanuel Roche and Yves Schabes. 1997. Deter- 
ministic part-of-speech tagging with finite-state 
transducers. In Emmanuel Roche aml Yves Sdl- 
abes, editors, Finite state language processing, 
pages 205-239. MIT Press, Cambridge, Mass. 
Ken Samuel, Sandra Carberry, and K. Vijay- 
Shanker. 1998. Dialogue act tagging with 
transformation-based learning. In Proceedings of 
the 17th International Con\]erence on Computa- 
tional Linguistics (COLING-ACL '98), Montreal. 
Briony Williams. 1994. Welsh letter-to-sound rules: 
rewrite rules and two-level rules compared. Com- 
puter Speech and Language, 8:261-277. 
310 
76
77
78
79
80
81
82
83
84
Linguistic Knowledge and Question Answering
Gosse Bouma
Information Science
Groningen University
{g.bouma}@rug.nl
Abstract
The availability of robust and deep syntac-
tic parsing can improve the performance
of Question Answering systems. This is
illustrated using examples from Joost, a
Dutch QA system which has been used for
both open (CLEF) and closed domain QA.
1 Linguistically Informed IR
Information retrieval is used in most QA systems
to filter out relevant passages from large docu-
ment collections to narrow down the search for an-
swer extraction modules in a QA system. Given
a full syntactic analysis of the text collection, it
becomes feasible to exploit linguistic information
as a knowledge source for IR. Using Apache?s IR
system Lucene, we can index the document col-
lection along various linguistic dimensions, such
as part of speech tags, named entity classes, and
dependency relations. Tiedemann (2005) uses a
genetic algorithm to optimize the use of such an
extended IR index, and shows that it leads to sig-
nificant improvements of IR performance.
2 Acquisition of Lexical Knowledge
Syntactic similarity measures can be used for au-
tomatic acquisition of lexical knowledge required
for QA, as well as for answer extraction and rank-
ing. For instance, in van der Plas and Bouma
(2005) it is shown that automatically acquired
class-labels for named entities improve the ac-
curacy of answering general WH-questions (i.e.
Which ferry sank in the Baltic Sea?) and questions
which ask for the definition of a named entity (i.e.
Who is Nelson Mandela? or What is MTV?).
3 Off-line answer extraction
Off-line extraction of answers to frequent ques-
tion types can be based on dependency patterns
and coreference resolution (Bouma et al, 2005;
Mur and van der Plas, 2006), leading to higher
recall (compared to systems using surface pat-
terns). Closed-domain (medical) QA can bene-
fit from the fact that dependency relations allow
answers to be identified for questions which are
not restricted to specific named entity classes, i.e.
definitions, causes, symptoms, etc. Answering
definition questions, for instance, is a task which
has motivated approaches that go well beyond the
techniques used for answering factoid questions.
In Fahmi and Bouma (2006) it is shown that syn-
tactic patterns can be used to extract potential def-
inition sentences from Wikipedia, and that syn-
tactic features of these sentences (in combination
with obvious clues such as the position of the sen-
tence in the document) can be used to improve the
accuracy of an automatic classifier which distin-
guishes definitions from non-definitions in the ex-
tracted data set.
4 Joost
Joost is a QA system for Dutch which incorporates
the features mentioned above, using the Alpino
parser for Dutch to parse (offline) the document
collections as well as (interactively) user ques-
tions. It has been used for the open-domain mono-
lingual QA task of CLEF 2005, as well as for
closed domain medical QA. For CLEF, the full
Dutch text collection (4 years of newspaper text,
approximately 80 million words) has been parsed.
For the medical QA system, we have been using
a mixture of texts from general and medical ency-
clopedia, medical reference works, and web pages
2 KRAQ06
dedicated to medical topics. The medical data are
from mixed sources and contain a fair amount of
domain specific terminology. Although the Alpino
system is robust enough to deal with such material,
we believe that the accuracy of linguistic analysis
on this task can be further improved by incorporat-
ing domain specific terminological resources. We
are currently investigating methods for acquiring
such knowledge automatically from the encyclo-
pedia sources.
References
Gosse Bouma, Jori Mur, and Gertjan van Noord. 2005.
Reasoning over dependency relations for QA. In
Proceedings of the IJCAI workshop on Knowledge
and Reasoning for Answering Questions (KRAQ),
pages 15?21, Edinburgh.
Ismail Fahmi and Gosse Bouma. 2006. Learning
to identify definitions using syntactic features. In
Roberto Basili and Alessandro Moschitti, editors,
Proceedings of the EACL workshop on Learning
Structured Information in Natural Language Appli-
cations, Trento, Italy.
Jori Mur and Lonneke van der Plas. 2006. Anaphora
resolution for off-line answer extraction using in-
stances. submitted.
Jo?rg Tiedemann. 2005. Integrating linguistic knowl-
edge in passage retrieval for question answering. In
Proceedings of EMNLP 2005, pages 939?946, Van-
couver.
Lonneke van der Plas and Gosse Bouma. 2005. Auto-
matic acquisition of lexico-semantic knowledge for
question answering. In Proceedings of Ontolex 2005
? Ontologies and Lexical Resources, Jeju Island,
South Korea.
3 KRAQ06
Learning to Identify Definitions using Syntactic Features
Ismail Fahmi and Gosse Bouma
Information Science
Groningen University
{i.fahmi,g.bouma}@rug.nl
Abstract
This paper describes an approach to learn-
ing concept definitions which operates on
fully parsed text. A subcorpus of the
Dutch version of Wikipedia was searched
for sentences which have the syntactic
properties of definitions. Next, we ex-
perimented with various text classifica-
tion techniques to distinguish actual defi-
nitions from other sentences. A maximum
entropy classifier which incorporates fea-
tures referring to the position of the sen-
tence in the document as well as various
syntactic features, gives the best results.
1 Introduction
Answering definition questions is a challenge for
question answering systems. Much work in QA
has focused on answering factoid questions, which
are characterized by the fact that given the ques-
tion, one can typically make strong predictions
about the type of expected answer (i.e. a date,
name of a person, amount, etc.). Definition ques-
tions require a different approach, as a definition
can be a phrase or sentence for which only very
global characteristics hold.
In the CLEF 2005 QA task, 60 out of 200 ques-
tions were asking for the definition of a named
entity (a person or organization) such as Who is
Goodwill Zwelithini? or What is IKEA? Answers
are phrases such as current king of the Zulu nation,
or Swedish home furnishings retailer. For answer-
ing definition questions restricted to named enti-
ties, it generally suffices to search for noun phrases
consisting of the named entity and a preceding or
following nominal phrase. Bouma et al (2005) ex-
tract all such noun phrases from the Dutch CLEF
corpus off-line, and return the most frequent heads
of co-occurring nominal phrases expanded with
adjectival or prepositional modifiers as answer to
named entity definition questions. The resulting
system answers 50% of the CLEF 2005 definition
questions correctly.
For a Dutch medical QA system, which is being
developed as part of the IMIX project1, several sets
of test questions were collected. Approximately
15% of the questions are definition questions, such
as What is a runner?s knee? and What is cere-
brovascular accident?. Answers to such questions
(asking for the definition of a concept) are typi-
cally found in sentences such as A runner?s knee
is a degenerative condition of the cartilage sur-
face of the back of the knee cap, or patella or A
cerebrovascular accident is a decrease in the num-
ber of circulating white blood cells (leukocytes)
in the blood. One approach to finding answers to
concept definitions simply searches the corpus for
sentences consisting of a subject, a copular verb,
and a predicative phrase. If the concept matches
the subject, the predicative phrase can be returned
as answer. A preliminary evaluation of this tech-
nique in Tjong Kim Sang et al (2005) revealed
that only 18% of the extracted sentences (from
a corpus consisting of a mixture of encyclopedic
texts and web documents) is actually a definition.
For instance, sentences such as RSI is a major
problem in the Netherlands, every suicide attempt
is an emergency or an infection of the lungs is the
most serious complication are of the relevant syn-
tactic form, but do not constitute definitions.
In this paper, we concentrate on a method for
improving the precision of recognizing definition
sentences. In particular, we investigate to what
1www.let.rug.nl/?gosse/Imix
64
extent machine learning techniques can be used
to distinguish definitions from non-definitions in
a corpus of sentences containing a subject, copu-
lar verb, and predicative phrase. A manually an-
notated subsection of the corpus was divided into
definition and non-definition sentences. Next, we
trained various classifiers using unigram and bi-
gram features, and various syntactic features. The
best classifier achieves a 60% error reduction com-
pared to our baseline system.
2 Previous work
Work on identifying definitions from free text ini-
tially relied on manually crafted patterns without
applying any machine learning technique. Kla-
vans and Muresan (2000) set up a pattern extractor
for their Definder system using a tagger and a fi-
nite state grammar. Joho and Sanderson (2000) re-
trieve descriptive phrases (dp) of query nouns (qn)
from text to answer definition questions like Who
is qn? Patterns such as ?dp especially qn?, as uti-
lized by Hearst (1992), are used to extract names
and their descriptions.
Similar patterns are also applied by Liu et al
(2003) to mine definitions of topic-specific con-
cepts on the Web. As an additional assumption,
specific documents dedicated to the concepts can
be identified if they have particular HTML and hy-
perlink structures.
Hildebrandt et al (2004) exploit surface pat-
terns to extract as many relevant ?nuggets? of in-
formation of a concept as possible. Similar to our
work, a copular pattern NP1 be NP2 is used as
one of the extraction patterns. Nuggets which do
not begin with a determiner are discarded to fil-
ter out spurious nuggets (e.g., progressive tense).
Nuggets extracted from every article in a corpus
are then stored in a relational database. In the end,
answering definition questions becomes as simple
as looking up relevant terms from the database.
This strategy is similar to our approach for answer-
ing definition questions.
The use of machine learning techniques can be
found in Miliaraki and Androutsopoulos (2004)
and Androutsopoulos and Galanis (2005) They
use similar patterns as (Joho and Sanderson,
2000) to construct training attributes. Sager and
L?Homme (1994) note that the definition of a
term should at least always contain genus (term?s
category) and species (term?s properties). Blair-
Goldensohn et al (2004) uses machine learn-
ing and manually crafted lexico-syntactic patterns
to match sentences containing both a genus and
species phrase for a given term.
There is an intuition that most of definition
sentences are located at the beginning of docu-
ments. This lead to the use of sentence num-
ber as a good indicator of potential definition sen-
tences. Joho and Sanderson (2000) use the posi-
tion of the sentences as one of their ranking crite-
ria, while Miliaraki and Androutsopoulos (2004),
Androutsopoulos and Galanis (2005) and Blair-
Goldensohn et al (2004) apply it as one of their
learning attributes.
3 Syntactic properties of potential
definition sentences
To answer medical definition sentences, we used
the medical pages of Dutch Wikipedia2 as source.
Medical pages were selected by selecting all pages
mentioned on the Healthcare index page, and re-
cursively including pages mentioned on retrieved
pages as well.
The corpus was parsed syntactically by Alpino,
a robust wide-coverage parser for Dutch (Malouf
and van Noord, 2004). The result of parsing (il-
lustrated in Figure 1) is a dependency graph. The
Alpino-parser comes with an integrated named en-
tity classifier which assigns distinct part-of-speech
tags to person, organization, and geographical
named entities.
Potential definition sentences are sentences con-
taining a form of the verb zijn3 (to be) with a
subject and nominal predicative phrase as sisters.
The syntactic pattern does not match sentences in
which zijn is used as a possessive pronoun (his)
and sentences where a form of zijn is used as an
auxiliary. In the latter case, no predicative phrase
complement will be found. On the other hand,
we do include sentences in which the predicative
phrase precedes the subject, as in Onderdeel van
de testis is de Leydig-cel (the Leydig cel is part of
the testis). As word order in Dutch is less strict
than in English, it becomes relevant to include
such non-canonical word orders as well.
A number of non-definition sentences that will
be extracted using this method can be filtered by
simple lexical methods. For instance, if the subject
is headed by (the Dutch equivalents of) cause, con-
2nl.wikipedia.org
3Note that the example uses ben (the first person singular
form of the verb) as root for zijn.
65
?
smain
su
noun
stikstof0
hd
verb
ben1
predc
np
det
det
een2
mod
adj
scheikundig3
hd
noun
element4
mod
pp
hd
prep
met5
obj1
conj
cnj
np
hd
noun
symbool6
app
name
N7
crd
vg
en8
cnj
np
hd
noun
atoom nummer9
app
num
710
Figure 1: Parse of (the Dutch equivalent of) Nitrogen is a chemical element with symbol N and atomic
number 7. Nodes are labelled with depedency relations and categories or part-of-speech tags, root forms,
and string positions.
sequence, example, problem, result, feature, pos-
sibility, symptom, sign, etc., or contains the deter-
miner geen (no), the sentence will not be included
in the list of potential definitions.
However, even after applying the lexical filter,
not all extracted sentences are definitions. In the
next sections, we describe experiments aimed at
increasing the accuracy of the extraction method.
4 Annotating training examples
To create evaluation and training data, 2500 ex-
tracted sentences were manually annotated as def-
inition, non-definition, or undecided. One of the
criteria for undecided sentences is that it mentions
a characteristic of a definition but is not really
a (complete) definition, for example, Benzeen is
carcinogeen (Benzene is a carcinogen). The result
of this annotation is given in Table 1. The anno-
tated data was used both to evaluate the accuracy
of the syntactic extraction method, and to training
and evaluate material for the machine learning ex-
periments as discussed in the next sections.
After discarding the undecided sentences, we
are left with 2299 sentences, 1366 of which are
definitions. This means that the accuracy of the
extraction method using only syntax was 59%.4
4This is considerably higher than the estimated accuracy
of 18% reported in Tjong Kim Sang et al (2005). This is
probably partly due to the fact that the current corpus con-
sists of encyclopedic material only, whereas the corpus used
If we take sentence postion into account as well,
and classify all first sentences as definitions and
all other sentences as non-definitions, a baseline
accuracy of 75,9% is obtained.
It is obvious from Table 1 that the first sen-
tences of Wikipedia lemmas that match the syn-
tactic pattern are almost always definitions. It
seems that e.g. Google?s5 define query feature,
when restricted to Dutch at least, relies heavily
on this fact to answer definition queries. How-
ever it is also obvious that definition sentences can
also be found in other positions. For documents
from other sources, which are not as structured as
Wikipedia, the first position sentence is likely to
be an even weaker predictor of definition vs. non-
definition sentences.
5 Attributes of definition sentences
We aim at finding the best attributes for classifying
definition sentences. We experimented with com-
binations of the following attributes:
Text properties: bag-of-words, bigrams, and
root forms. Punctuation is included as Klavans
and Muresan (2000) observe that it can be used to
recognize definitions (i.e. definitions tend to con-
in Tjong Kim Sang et al (2005) contained web material
from various sources, such as patient discussion groups, as
well. The latter tends to contain more subjective and context-
dependent material.
5google.com
66
Sentence Def Non-def Undecided
position
first 831 18 31
other 535 915 170
Total 1366 933 201
Table 1: Number of sentences in the first and
other position of documents annotated as defini-
tion, non-definition, and undecided.
tain parentheses more often than non-definitions).
No stopword filtering is applied as in our exper-
iments it consistently decreased accuracy. Note
that we include all bigrams in a sentence as fea-
ture. A different use of n-grams has been explored
by Androutsopoulos and Galanis (2005) who add
only n-grams (n ? {1,2,3}) occurring frequently
either directly before or after a target term.
Document property: the position of each sen-
tence in the document. This attribute has been fre-
quently used in previous work and is motivated by
the observation that definitions are likely to be lo-
cated in the beginning of a document.
Syntactic properties: position of each sub-
ject in the sentence (initial, e.g. X is Y; or non-
initial, e.g. Y is X), and of each subject and
predicative complement: type of determiner (def-
inite, indefinite, other). These attributes have not
been investigated in previous work. In our exper-
iments, sentence-initial subjects appear in 92% of
the definition sentences and and 76% of the non-
definition sentences. These values show that a
definition sentence with a copular pattern tends
to put its subject in the beginning. Two other
attributes are used to encode the type of deter-
miner of the subject and predicative compelement.
As shown in Table 2, the majority of subjects in
definition sentences have no determiner (62%),
e.g. Paracetamol is een pijnstillend en koortsver-
lagend middel (Paracetamol is an pain alleviat-
ing and a fever reducing medicine), while in non-
definition sentences subject determiners tend to be
definite (50%), e.g. De werkzame stof is acetyl-
salicylzuur (The operative substance is acetylsal-
icylacid). Predicative complements, as shown in
Table 3, tend to contain indefinite determiners in
definition sentences (64%), e.g. een pijnstillend
. . . medicijn (a pain alleviating. . . medicine), while
in non-definition the determiner tends to be def-
inite (33%), e.g. Een fenomeen is de Landsge-
meinde (A phenomenon is the Landsgemeinde).
Type Definition Non-def
definite 23 50
indefinite 13 12
nodeterminer 62 29
other 2 9
Table 2: Percentage of determiner types of sub-
jects in definition and non-definition sentences.
Type Definition Non-def
definite 23 33
indefinite 64 29
nodeterminer 8 1
other 4 28
Table 3: Percentage of determiner types of
predicative complements in definition and non-
definition sentences.
Named entity tags: named entity class (NEC)
of subjects, e.g. location, person, organization,
or no-class. A significant difference in the dis-
tribution of this feature between definition and
non-definition sentences can be observed in Table
4. More definition sentences have named entity
classes contained in their subjects (40.63%) com-
pared to non-definition sentences (11.58%). We
also experimented with named entity classes con-
tained in predicative complements but it turned
out that very few predicates contained named en-
tities, and thus no significant differences in distri-
bution between definition and non-definition sen-
tences could be observed.
Features for lexical patterns, as used in (An-
droutsopoulos and Galanis, 2005), e.g. qn which
(is|was|are|were) dp, are not added because in this
experiment we investigate only a copular pattern.
WordNet-based attributes are also excluded, given
that coverage for Dutch (using EuroWordNet)
tends to be less good than for English, and even for
English their contribution is sometimes insignifi-
cant (Miliaraki and Androutsopoulos, 2004).
Type Definition Non-def
no-nec 59 88
location 10 4
organization 8 3
person 22 4
Table 4: Percentage of named-entity classes of
subjects in definition and non-definition sentences.
67
word bigrams only bigram + synt + pos
is a first sent
a other sent
are is a
is indef pred
) is no det subj
the init subj
is DIGITS a
are the are
this is
or other det pred
is of ) is
this/these noninit subj
atomic number def subj
atomic number DIGITS the
with symbol is DIGITS
and atomic number are the
that this
chemical or
a chemical other det subj
chemical element is of
Table 5: 20 most informative features for the sys-
tems using word bigrams only and word bigrams
in combination with syntactic and sentence posi-
tion features (word features have been translated
into English).
We use the text classification tool Rainbow6
(McCallum, 2000) to perform most of our experi-
ments. Each sentence is represented as a string of
words, possibly followed by bigrams, root forms,
(combinations of) syntactic features, etc.
All experiments were performed by selecting
only the 2000 highest ranked features according
to information gain. In the experiments which in-
clude syntactic features, the most informative fea-
tures tend to contain a fair number of syntactic fea-
tures. This is illustrated for the configuration using
bigrams, sentence position, and syntax in table 5.
It supports our intuition that the position of sub-
jects and the type of determiner of subjects and
predicative complements are clues to recognizing
definition sentences.
To investigate the effect of each attribute, we
set up several configurations of training examples
as described in Table 6. We start with using only
bag-of-words or bigrams, and then combine them
with other attribute sets.
6www.cs.cmu.edu/?mccallum/bow/rainbow/
Cfg Description
1 using only bag-of-words
2 using only bigrams
3 combining bigrams & bag-of-words
4 adding syntactic properties to
config. 3
5 adding syntactic properties
& NEC to config. 3
6 adding sentence position to
config. 3
7 adding root forms to
config. 3
8 adding syntactic properties &
sentence position to config. 3
9 adding syntactic properties, sentence
position & NEC to config. 3
10 adding syntactic properties, sentence
position & root forms to config. 3)
11 using all attributes (adding NEC
to configuration 10)
Table 6: The description of the attribute configu-
rations.
6 Learning-based methods
We apply three supervised learning methods to
each of the attribute configurations in Table 6,
namely naive Bayes, maximum entropy, and sup-
port vector machines (SVMs). Naive Bayes is a
fast and easy to use classifier based on the prob-
abilistic model of text and has often been used in
text classification tasks as a baseline. Maximum
entropy is a general estimation technique that has
been used in many fields such as information re-
trieval and machine learning. Some experiments
in text classification show that maximum entropy
often outperforms naive Bayes, e.g. on two of
three data sets in Nigam et al (1999). SVMs are
a new learning method but have been reported by
Joachims (1998) to be well suited for learning in
text classification.
We experiment with three kernel types of
SVMs: linear, polynomial, and radial base func-
tion (RBF). Rainbow (McCallum, 2000) is used to
examine these learning methods, except the RBF
kernel for which libsvm (Chang and Lin, 2001)
is used. Miliaraki and Androutsopoulos (2004)
use a SVM with simple inner product (polyno-
mial of first degree) kernel because higher degree
polynomial kernels were reported as giving no im-
provement. However we want to experiment with
68
Cfg NB ME svm1a svm2b svm3c
1 85.75 ? 0.57 85.35 ? 0.77 77.65 ? 0.87 78.39 ? 0.67 81.95 ? 0.82
2 87.77 ? 0.51 88.65 ? 0.54 84.02 ? 0.47 84.26 ? 0.52 85.38 ? 0.77
3 89.82 ? 0.53 88.82 ? 0.66 83.93 ? 0.57 84.24 ? 0.54 87.04 ? 0.95
4 85.22 ? 0.35 89.08 ? 0.50 84.93 ? 0.57 85.57 ? 0.53 87.77 ? 0.89
5 85.44 ? 0.45 91.38 ? 0.42 86.90 ? 0.48 86.90 ? 0.53 87.60 ? 0.87
6 90.26 ? 0.71 90.70 ? 0.48 85.26 ? 0.56 86.05 ? 0.64 88.52 ? 0.92
7 88.60 ? 0.81 88.99 ? 0.51 83.38 ? 0.38 84.69 ? 0.43 87.08 ? 0.87
8 86.40 ? 0.51 92.21 ? 0.27 86.57 ? 0.42 87.29 ? 0.47 88.77 ? 0.77
9 87.12 ? 0.52 90.83 ? 0.43 87.21 ? 0.42 87.99 ? 0.53 89.04 ? 0.67
10 87.60 ? 0.38 91.16 ? 0.43 86.68 ? 0.40 86.97 ? 0.41 88.91 ? 0.68
11 86.72 ? 0.46 91.16 ? 0.35 87.47 ? 0.40 87.05 ? 0.63 89.47 ? 0.67
aSVM with linear kernel (Rainbow)
bSVM with polynomial kernel (Rainbow)
cSVM with RBF kernel (libsvm)
Table 7: Accuracy and standard error (%) estimates for the dataset using naive Bayes (NB), maximum
entropy (ME), and three SVM settings at the different attribute configurations.
the RBF (gaussian) kernel by selecting model pa-
rameters C (penalty for misclassification) and ?
(function of the deviation of the Gaussian Kernel)
so that the classifier can accurately predict testing
data. This experiment is based on the argument
that if a complete model selection using the gaus-
sian kernel has been conducted, there is no need
to consider linear SVM, because the RBF kernel
with certain parameters (C , ?) has the same per-
formance as the linear kernel with a penalty pa-
rameter C? (Keerthi and Lin, 2003).
Given the finite dataset, we use k-fold cross-
validation (k = 20) to estimate the future perfor-
mance of each classifier induced by its learning
method and dataset. This estimation method intro-
duces lower bias compared to a bootstrap method
which has extremely large bias on some problems
(Kohavi, 1995).
7 Evaluation
We evaluated each configuration of Section 5 and
each learning method of Section 6 on the dataset
which consists of 1336 definitions and 963 non-
definitions sentences. Table 7 reports the accuracy
and standard error estimated from this experiment.
In all experiment runs, all of the classifiers in all
configurations outperform our baseline (75.9%).
The best accuracy of each classifier (bold) is be-
tween 11.57% to 16.31% above the baseline.
The bigram only attributes (config. 2) clearly
outperform the simplest setting (bag-of-word only
attributes) for all classifiers. The combination of
both attributes (config. 3) achieves some improve-
ment between 0.17% to 4.41% from configuration
2. It is surprising that naive Bayes shows the best
and relatively high accuracy in this base config-
uration (89.82%) and even outperforms all other
settings.
Adding syntactic properties (config. 4) or posi-
tion of sentences in documents (config. 6) to the
base configuration clearly gives some improve-
ment (in 4 and 5 classifiers respectively for each
configuration). But, adding root forms (config.
7) does not significantly contribute to an improve-
ment. These results show that in general, syntactic
properties can improve the performance of most
classifiers. The results also support the intuition
that the position of sentences in documents plays
important role in identifying definition sentences.
Moreover, this intuition is also supported by the
result that the best performance of naive Bayes is
achieved at configuration 6 (90.26%). Compared
to the syntactic features, sentence positions give
better accuracy in all classifiers.
The above results demonstrate an interesting
finding that a simple attribute set which consists of
bag-of-words, bigrams, and sentence position un-
der a fast and simple classifier (e.g. naive Bayes)
could give a relatively high accuracy. One expla-
nation that we can think of is that candidate sen-
tences have been syntactically very well extracted
with our filter. Thus, the sentences are biased by
the filter from which important words and bigrams
of definitions can be found in most of the sen-
69
tences. For example, the word and bigrams is een
(is a), een (a), zijn (are), is (is), zijn de (are the),
and is van (is of) are good clues to definitions and
consequently have high information gain. We have
to test this result in a future work on candidate def-
inition sentences which are extracted by filters us-
ing various other syntactic patterns.
More improvement is shown when both syntac-
tic properties and sentence position are added to-
gether (config. 8). All of the classifiers in this con-
figuration obtain more error reduction compared
to the base configuration. Moreover, the best ac-
curacy of this experiment is shown by maximum
entropy at this configuration (92.21%). This may
be a sign that our proposed syntactic properties are
good indicators to identify definition sentences.
Other interesting findings can be found in the
addition of named entity classes to configuration
3 (config. 5), to configuration 8 (config. 9) and
to configuration 10 (config. 11). In these con-
figurations, adding NEC increases accuracies of
almost all classifiers. On the other hand, adding
root forms to configuration 3 (config. 7) and to
configuration 8 (config. 10) does not improve ac-
curacies. However, the best accuracies of naive
Bayes (90.26%) and maximum entropy (92.21%)
are achieved when named entity and root forms are
not included as attributes.
We now evaluate the classifiers. It is clear
from the table that SVM1 and SVM2 settings can
not achieve better accuracy compared to the naive
Bayes setting, while SVM3 setting marginally out-
performs naive Bayes (on 6 out of 11 configura-
tions). This result is contrary to the superiority of
SVMs in many text classification tasks. Huang et
al. (2003) reported that both classifiers show sim-
ilar predictive accuracy and AUC (area under the
ROC (Receiver Operating Characteristics) curve)
scores. This performance of naive Bayes supports
the motivation behind its renaisance in machine
learning (Lewis, 1998).
From the three SVM settings, SVM with RBF
kernel appears as the best classifier for our task
in which it outperforms other SVMs settings in
all configurations. This result supports the above
mentioned argument that if the bestC and ? can be
selected, we do not need to consider linear SVM
(e.g. the svm1 setting).
Among all of the classifiers, maximum entropy
shows the best accuracy. It wins at 9 out of 11
configurations in all experiments. This result con-
firms previous reports e.g. in Nigam et al (1999)
that maximum entropy performs better than naive
Bayes in some text classification tasks.
8 Conclusions and future work
We have presented an experiment in identifying
definition sentences using syntactic properties and
learning-based methods. Our method is concen-
trated on improving the precision of recognizing
definition sentences. The first step is extracting
candidate definition sentences from a fully parsed
text using syntactic properties of definitions. To
distinguish definition from non-definition sen-
tences, we investigated several machine learning
methods, namely naive Bayes, maximum entropy,
and SVMs. We also experimented with several at-
tribute configurations. In this selection, we com-
bine text properties, document properties, and syn-
tactic properties of the sentences. We have shown
that adding syntactic properties, in particular the
position of subjects in the sentence, type of de-
terminer of each subject and predicative comple-
ment, improves the accuracy of most machine
learning techniques, and leads to the most accu-
rate result overall.
Our method has been evaluated on a subset of
manually annotated data from Wikipedia. The
combination of highly structured text material and
a syntactic filter leads to a relatively high initial
baseline.
Our results on the performance of SVMs do not
confirm the superiority of this learning method for
(text) classification tasks. Naive Bayes, which is
well known from its simplicity, appears to give
reasonably high accuracy. Moreover, it achieves
a high accuracy on simple attribute configuration
sets (containing no syntactic properties). In gen-
eral, our method will give the best result if all
properties except named entity classes and root
forms are used as attributes and maximum entropy
is applied as a classifier.
We are currently working on using more syn-
tactic patterns to extract candidate definition sen-
tences. This will increase the number of definition
sentences that we can identify from text.
References
I. Androutsopoulos and D. Galanis. 2005. A prac-
tically unsupervised learning method to identify
single-snippet answers to definition questions on the
web. In Human Language Technology Conference
70
and Conference on Empirical Methods in Natural
Language Processing (HLT-EMNLP 2005), Vancou-
ver, Canada.
S. Blair-Goldensohn, K. McKeown, and A.H. Schlaik-
jer. 2004. Answering definitional questions: A hy-
brid approach. In New Directions in Question An-
swering, pages 47?58.
Gosse Bouma, Jori Mur, Gertjan van Noord, Lonneke
van der Plas, and Jo?rg Tiedemann. 2005. Question
answering for Dutch using dependency relations. In
Working Notes for the CLEF 2005 Workshop, Vi-
enna.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines.
Software available at http://www.csie.ntu.
edu.tw/?cjlin/libsvm.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proc. of the 14th
COLING, pages 539?545, Nantes, France.
W. Hildebrandt, B. Katz, and J.J. Lin. 2004. An-
swering definition questions with multiple knowl-
edge sources. In HLT-NAACL, pages 49?56.
Jin Huang, Jingjing Lu, and Charles X. Ling. 2003.
Comparing naive bayes, decision trees, and svm
with auc and accuracy. In ICDM ?03: Proceedings
of the Third IEEE International Conference on Data
Mining, Washington, DC, USA. IEEE Computer So-
ciety.
Thorsten Joachims. 1998. Text categorization with
support vector machines: learning with many rele-
vant features. In Claire N?edellec and C?eline Rou-
veirol, editors, Proceedings of ECML-98, 10th Euro-
pean Conference on Machine Learning, pages 137?
142, Chemnitz, DE. Springer Verlag, Heidelberg,
DE.
H. Joho and M. Sanderson. 2000. Retrieving descrip-
tive phrases from large amounts of free text. In
CIKM, pages 180?186.
S. Sathiya Keerthi and Chih-Jen Lin. 2003. Asymp-
totic behaviors of support vector machines with
gaussian kernel. Neural Comput., 15(7):1667?1689.
J.L. Klavans and S. Muresan. 2000. Definder: Rule-
based methods for the extraction of medical termi-
nology and their associated definitions from on-line
text. In American Medical Informatics Assoc 2000.
Ron Kohavi. 1995. A study of cross-validation and
bootstrap for accuracy estimation and model selec-
tion. In IJCAI, pages 1137?1145.
David D. Lewis. 1998. Naive (Bayes) at forty: The in-
dependence assumption in information retrieval. In
Claire Ne?dellec and Ce?line Rouveirol, editors, Pro-
ceedings of ECML-98, 10th European Conference
on Machine Learning, pages 4?15, Chemnitz, DE.
Springer Verlag, Heidelberg, DE.
B. Liu, C.W. Chin, and H.T. Ng. 2003. Mining topic-
specific concepts and definitions on the web. In
WWW ?03: Proceedings of the 12th international
conference on World Wide Web, pages 251?260,
New York, NY, USA. ACM Press.
Robert Malouf and Gertjan van Noord. 2004. Wide
coverage parsing with stochastic attribute value
grammars. In IJCNLP-04 Workshop Beyond Shal-
low Analyses - Formalisms and statistical modeling
for deep analyses, Hainan.
A McCallum. 2000. Bow: A toolkit for statisti-
cal language modeling, text retrieval, classification
and clustering. http://www.cs.cmu.edu/
?mccallum/bow.
S. Miliaraki and I. Androutsopoulos. 2004. Learning
to identify single-snippet answers to definition ques-
tions. In 20th International Conference on Compu-
tational Linguistics (COLING 2004), pages 1360?
1366, Geneva, Switzerland. COLING 2004.
K. Nigam, J. Lafferty, and A. McCallum. 1999. Using
maximum entropy for text classification. In IJCAI-
99 Workshop on Machine Learning for Information
Filtering, pages 61?67.
Juan C. Sager and M.C. L?Homme. 1994. A model
for definition of concepts. Terminology, pages 351?
374.
Erik Tjong Kim Sang, Gosse Bouma, and Maarten
de Rijke. 2005. Developing offline strategies for
answering medical questions. In Diego Molla? and
Jose? Luis Vicedo, editors, AAAI 2005 workshop on
Question Answering in Restricted Domains.
71
Proceedings of the Linguistic Annotation Workshop, pages 17?24,
Prague, June 2007. c?2007 Association for Computational Linguistics
Mining Syntactically Annotated Corpora with XQuery
Gosse Bouma and Geert Kloosterman
Information Science
University of Groningen
The Netherlands
g.bouma|g.j.kloosterman@rug.nl
Abstract
This paper presents a uniform approach to
data extraction from syntactically annotated
corpora encoded in XML. XQuery, which
incorporates XPath, has been designed as
a query language for XML. The combina-
tion of XPath and XQuery offers flexibility
and expressive power, while corpus specific
functions can be added to reduce the com-
plexity of individual extraction tasks. We il-
lustrate our approach using examples from
dependency treebanks for Dutch.
1 Introduction
Manually annotated treebanks have played an im-
portant role in the development of robust and ac-
curate syntactic analysers. Now that such parsers
are available for various languages, there is a grow-
ing interest in research that uses automatically an-
notated corpora. While such corpora are not error-
free, the fact that they can be constructed rela-
tively easily, and the fact that they can be an order
of magnitude larger than manually corrected tree-
banks, makes them attractive for several types of re-
search. Syntactically annotated corpora have suc-
cesfully been used to acquire lexico-semantic infor-
mation (Lin and Pantel, 2001; Snow et al, 2005), for
relation extraction (Bunescu and Mooney, 2005), in
IR (Cui et al, 2005), and in QA (Katz and Lin, 2003;
Molla? and Gardiner, 2005).
What these tasks have in common is the fact that
they all operate on large amounts of data extracted
from syntactically annotated text. Tools to perform
this task are often developed with only a single ap-
plication in mind (mostly corpus linguistics) or are
developed in an ad-hoc fashion, as part of a specific
application.
We propose a more principled approach, based on
two observations:
? XML is widely used to encode syntactic anno-
tation. Syntactic annotation is not more com-
plex that some other types of information that
is routinely stored in XML. This suggests that
XML technology can be used to process syn-
tactically annotated corpora.
? XQuery is a query language for XML data. As
such, it is the obvious choice for mining syn-
tactically annotated corpora.
The remainder of this paper is organised as fol-
lows. In the next section, we present the Alpino tree-
bank format, which we use for syntactic annotation.
The Alpino parser has been used to annotate large
corpora, and the results have been used in a number
of research projects.1
In section 3, we discuss the existing approaches
to data extraction from Alpino corpora. We note that
all of these have drawbacks, either because they lack
expressive power, or because they require a serious
amount of programming overhead.
In section 4, we present our approach, starting
from a relatively straightforward corpus linguistics
task, that requires little more than XPath, and end-
ing with a more advanced relation extraction task,
1See www.let.rug.nl/?vannoord/research.
html
17
that requires XQuery. We demonstrate that much of
the complexity of advanced tasks can be avoided by
providing users with a corpus specific module, that
makes available common concepts and functions.
2 The Alpino Treebank format
As part of the development of the Alpino parser
(Bouma et al, 2001), a number of manually an-
notated dependency treebanks have been created
(van der Beek et al, 2002). Annotation guidelines
were adopted from the Corpus of Spoken Dutch
(Oostdijk, 2000), a large corpus annotation project
for Dutch. In addition, large corpora (e.g. the 80M
word Dutch CLEF2 corpus, the 500M word Twente
News corpus3, and Dutch Wikipedia4) have been
annotated automatically. Both types of treebanks
have been used for corpus linguistics (van der Beek,
2005; Villada Moiro?n, 2005; Bouma et al, 2007).
The automatically annoted treebanks have been used
for lexical acquisition (van der Plas and Bouma,
2005), and form the core of a Dutch QA system
(Bouma et al, 2005).
The format of Alpino dependency trees is illus-
trated in figure 1. The (somewhat simplified) XML
for this tree is in fig. 2. Nodes in the tree are labeled
with a dependency relation and a category or POS-
tag. Furthermore, the begin and end position of con-
stituents is represented in attributes,5 and the root
and word form of terminal nodes is encoded. Note
that heads do not have their dependents as children,
as is the case in most dependency tree formats. In-
stead, the head is a child of the constituent node if
which it is the head, and its dependents are siblings
of the head. Finally, trees may contain index nodes
(indicated by indices in bold in the graphical repre-
sentation and by the index attribute in the XML) to
indicate ?secondary? edges. The subject Alan Turing
in fig. 2 is a subject of the passive auxiliary word,
but also a direct object of the verb aan tref. Thus,
Alpino dependency trees are actually graphs.
A large syntactically annotated corpus tends to
2www.clef-campaign.org
3www.vf.utwente.nl/?druid/TwNC/
TwNC-main.html
4nl.wikipedia.org
5Note that constituents may be discontinuous, and thus, the
yield of a constituent may not contain every terminal node be-
tween begin and end. See also section 4.2.
?
smain
hd
verb
word4
su
1
mwu
mwp
name
Alan5
mwp
name
Turing6
vc
ppart
mod
pp
hd
prep
op0
obj1
mwu
mwp
noun
71
mwp
noun
juni2
mwp
noun
19543
obj1
1
predc
adj
dood7
hd
verb
tref aan8
Figure 1: Op 7 juni 1954 werd Alan Turing dood
aangetroffen (On June 7, 1954, Alan Turing was
found dead)
give rise to even larger volumes of XML. To sup-
port efficient storage and retrieval of XML data, a
set of tools has been developed for compression of
XML data (using dictzip6) and for efficient visuali-
sation and search of data in compressed XML files.
The tools are described in more detail at the Alpino
website.7
3 Existing approaches to extraction
Users have taken quite different approaches to cor-
pus exploration and data extraction.
? For corpus exploration, Alpino dtsearch is
the most widely used tool. It allows XPath
queries to be matched against trees in a tree-
bank. The result can be a visual display of trees
with matching nodes highlighted, but alterna-
tive outputs are possible as well. Examples of
how XPath can be used for extraction are pre-
sented in the next section.
? For relation extraction (i.e. finding symptoms
of diseases), the Alpino system itself has been
6www.dict.org
7www.let.rug.nl/?vannoord/alp/Alpino/
TreebankTools.html
18
<node begin="0" cat="smain" end="9" rel="--">
<node begin="4" end="5" pos="verb" rel="hd" root="word" word="werd"/>
<node begin="5" cat="mwu" end="7" index="1" rel="su">
<node begin="5" end="6" pos="name" rel="mwp" neclass="PER" root="Alan" word="Alan"/>
<node begin="6" end="7" pos="name" rel="mwp" neclass="PER" root="Turing" word="Turing"/>
</node>
<node begin="0" cat="ppart" end="9" rel="vc">
<node begin="0" cat="pp" end="4" rel="mod">
<node begin="0" end="1" pos="prep" rel="hd" root="op" word="Op"/>
<node begin="1" cat="mwu" end="4" rel="obj1">
<node begin="1" end="2" pos="noun" rel="mwp" root="7" word="7"/>
<node begin="2" end="3" pos="noun" rel="mwp" root="juni" word="juni"/>
<node begin="3" end="4" pos="noun" rel="mwp" root="1954" word="1954"/>
</node>
</node>
<node begin="5" end="7" index="1" rel="obj1"/>
<node begin="7" end="8" pos="adj" rel="predc" root="dood" word="dood"/>
<node begin="8" end="9" pos="verb" rel="hd" root="tref_aan" word="aangetroffen"/>
</node>
</node>
Figure 2: XML encoding of the Alpino depedency tree in fig. 1
used. It provides functionality for converting
dependency trees in XML into a Prolog list of
dependency triples. The full functionality of
Prolog can then be used to do the actual extrac-
tion.
? Alternatively, one can use XSLT to extract data
from the XML directly. As XSLT is primarily
intended for transformations, this tends to give
rise to complex code.
? Alternatively, a general purpose scripting or
programming language such as Perl or Python,
with suitable XML support, can be used. As in
the Alpino/Prolog case, this has the advantage
that one has a full programming language avail-
able. A disadvantage is that there is no specific
support for working with dependency trees or
triples.
None of the approaches listed above is optimal.
XPath is suitable only for identifying syntactic pat-
terns, and does not offer the possibility of extraction
of elements (i.e. it has no capturing mechanism).
The other three approaches do allow for both match-
ing and extraction, but they all require skills that go
considerably beyond conceptual knowledge of the
treebank and some basic knowledge of XML.
Another disadvantage of the current situation is
that there is little or no sharing of solutions be-
tween users. Yet, different applications tend to en-
counter the same problems. For instance, multiword
expressions (such as Alan Turing or 7 juni 1954)
are encoded as trees, dominated by a cat=?mwu?
node. An extraction task that requires names
to be extracted must thus take into account the
fact that names can be both nodes with a label
pos=?name? as well as cat=?mwu? nodes (dom-
inating a pos=?name?). The situation is further
complicated by the fact that individual parts of a
name, such as Alan in Alan Turing, should nor-
mally not be matched. Similar problems arise if
one wants to match e.g. finite verbs (there is no
single attribute which expresses tense) or NPs (the
cat=?np? attribute is only present on complex
NPs, not on single words). A very frequent issue
is the proper handling of index nodes. Searching
for the object of the verb tref aan in fig. 2 requires
that one finds the node in the tree that is coindexed
with the rel=?obj1? node with index 1. This is
a challenge in all approaches listed above, except
for Alpino/Prolog, which solves the problem by con-
verting trees to sets of dependency triples.
Some of the problems mentioned above could be
solved by introducing more and more fine-grained
attributes (i.e. a separate attribute for tense, as-
signing both a category and a POS-tag to (non-
head) terminal-nodes, etc.) or by introducing unary
branching nodes. This has the obvious drawback
of introducing redundancy in the encoding, would
19
mean another departure from the usual conception
of dependency trees (in the case unary branching is
introduced), and may still not cover all distinctions
that users need to make. Also, finding the content of
an index-node cannot be solved in this way.
One might consider moving to a radically differ-
ent treebank format, such as Tiger XML8 for in-
stance, in which trees are basically a listing of nodes,
with non-terminal nodes dominating a number of
edge elements that take (the index of) other nodes
as value. Note, however, that most of the problems
mentioned above refer to linguistic concepts, and
thus are unlikely to be solved by changing the ar-
chitecture of the underlying XML representation.
4 XQuery and XPath
Two closely related standards for processing XML
documents are XSLT9 and XQuery10 . Both make
use of XPath11, the XML language for locating parts
of XML documents. While XSLT is primarily in-
tended for transformations of documents, XQuery
is primarily intended for extraction of information
from XML databases. XQuery is in many respects
similar to SQL and is rapidly becoming the standard
for XML database systems.12 A distinctive differ-
ence between the XSLT and XQuery is the fact that
XSLT documents are themselves XML documents,
whereas this is not the case for XQuery. This typi-
cally makes XQuery more concise and easier to read
than XSLT.13
These considerations made us experiment with
XQuery as a language for data extraction from syn-
tactically annotated corpora. Similar studies were
carried out by Cassidy (2002) (for an early version
of XQuery) and Mayo et al (2006), who compare
the NITE Query Language and XQuery. Below, we
first illustrate a task that requires use of XPath only,
and then move on to tasks that require the additional
functionality of XQuery.
8www.ims.uni-stuttgart.de/projekte/
TIGER/
9www.w3.org/TR/xslt20
10www.w3.org/TR/xquery
11www.w3.org/TR/xpath20
12e.g. exist.sourceforge.net, monetdb.cwi.nl,
www.oracle.com/database/berkeley-db/xml
13See Kay (2005) for a thorough comparison.
4.1 Corpus exploration with XPath
As argued in Bouma and Kloosterman (2002),
XPath provides a powerful query language for for-
mulating linguistically relevant queries, provided
that the XML encoding of the treebank reflects the
syntactic structure of the trees.
Inherent reflexive verbs, for instance, are verbal
heads with a rel=?se? dependent. A verb with an
inherently reflexive can therefore be found as fol-
lows (remember that in Alpino dependency trees,
dependents are actually siblings of the head):
//node[@pos="verb"
and @rel="hd"
and ../node[@rel="se"]
]
The double slash (?//?) ensures that we search for
nodes anywhere within the XML document. The
material in brackets ([ ]) can be used to specify
additional constraints that matching nodes have to
meet. The @-sign is used to refer to attributes of an
element. The double dots (?..?) locate the parent el-
ement of an XML element. Children of an element
are located using the single slash (?/?) operator. The
two can be combined to locate siblings.
Comparison operators are available to compare
e.g. attributes that have a numeric value. The follow-
ing XPath query identifies cases where the reflexive
precedes the subject:
//node[@pos="verb"
and @rel="hd"
and ../node[@rel="se"]/@begin <
../node[@rel="su"]/@begin
]
Note that we can also use the ?/? to locate attributes
of an element, and that the begin attribute encodes
the initial string position of a constituent.
Reflexives preceding the subject are a marked op-
tion in Dutch. We may contrast matching verbs with
verbs matching the following expression:
//node[@pos="verb"
and @rel="hd"
and ../node[@rel="se"]/@begin >
../node[@rel="su"]/@begin
and not(../node[@rel="su"]/@begin="0")
]
Here we have simply reversed the comparison op-
erator. As we want to exclude from considera-
tion cases where the subject precedes the finite verb
(e.g. is in sentence-initial position), we have added a
negative constraint with this effect.
20
REFL-SU SU-REFL verb (gloss)
% # % #
94.3 33 5.7 2 vorm (to shape)
91.7 11 8.3 1 ontvouw (to unfold)
74.1 234 25.9 82 doe voor (to happen)
73.5 36 26.5 13 teken af (to form)
58.8 10 41.2 7 wreek (to take revenge)
57.1 44 42.9 33 voltrek (to take place)
56.0 42 44.0 33 verzamel (to assemble)
54.6 309 45.4 257 bevind (to be located)
50.0 18 50.0 18 dring op (to impose)
48.3 58 51.7 62 dien aan (to announce)
Table 1: Relative frequency of REFL-SU vs SU-REFL
word order
Using the two queries above to search one year
of newspaper text, we can collect the outcome and
compute, for a given verb, the relative frequency of
REFL-SU vs. SU-REFL order for non-subject initial
sentences in Dutch. A sample of verbs that have a
high percentage of REFL-SU occurrences, is given in
table 1. The result confirms an observation in Hae-
sereyn et al (1997), that REFL-SU word order occurs
especially with verbs having a somewhat ?bleeched
semantics? and expressing that something exists or
comes into existence.
It should be noted that XPath offers consider-
able more possibilities than what is illustrated here.
XPath 2.0 in particular is an important step forward
for linguistic search, as it includes far more func-
tionality for string processing (i.e. tokenization and
regular expressions) than its predecessors. Bird et al
(2006) propose an extension of XPath 1.0 for lin-
guistic queries. The intuitive notation they intro-
duce might be useful for some users. However,
the examples they concentrate on (all having to do
with linear order) presuppose trees without ?cross-
ing branches?. The introduction of begin and end
attributes in the Alpino format makes it possible
to handle such queries for dependency trees (with
crossing branches) as well, and furthermore, does
not require an extension of XPath.
4.2 Data Extraction with XQuery
The kind of explorative corpus search for which
XPath is ideally suited is supported by most other
treebank query languages as well, although not all
alternatives offer the same expressive power. There
are many applications, however, in which it is neces-
sary to extract more than just (root forms of) match-
ing nodes. XQuery offers the functionality that is
required to perform arbitrary extraction.
XQuery programs consist of so-called FLWOR
expressions (for, let, where, order by,
return, not all parts are required). The example
below illustrates this. Assume we want to extract
from a treebank all occurrences of names, along with
their named entity class. The following XQuery
script covers the base case.
for $name in
collection(?ad1994?)//node[@pos="name"]
let $nec := string($node/@neclass)
return
<term nec="{$nec}">
{string($name/@word)}
</term>
The for-statement locates the nodes to be pro-
cessed. Nodes are located by XPath expressions.
The collection-predicate defines the directory to be
processed. For every document in the collection,
nodes with a POS-attribute name are processed. We
use a let-statement to assign the variable $nec is
assigned the string value of the neclass-attibute
(which indicates the named entity class of the name).
The return-statement returns for each matching
node an XML element containing the string value of
the word attribute of the name, as well as an attribute
indicating the named entity class.
The complexity of XQuery scripts can increase
considerably, depending on the complexity of the
underlying XML data and the task being performed.
One of the most interesting features of XQuery is the
possibility to define functions. They can be used to
enhance the readibility of code. Furthermore, func-
tions can be collected in modules, and thus can be
reused across applications.
For Alpino treebanks, for instance, we have
implemented a module that covers concepts and
tasks that are needed frequently. As pointed
out above, names in the Alpino treebank are not
just single nodes, but, in case a name consists
of two or more words, can also consist of mul-
tiple node[@pos=?name?] elements, with a
node[@cat=?mwu?] as parent. This motivates
the introduction of a name and neclass function,
21
as shown in fig. 3. Assuming that the alpinomod-
ule has been imported, we can now write a better
name extraction script:
for $name in
collection(?ad1994?)//node
where alpino:name($name)
return
<term nec="{alpino:neclass($name)}">
{alpino:yield{$name)}
</term>
As we are matching with non-terminal nodes as
well, we need to take into account that it no longer
suffices to return the value of word to obtain the
yield of a node. As this situation arises frequently
as well, we added a yield function (see fig. 3).
It takes a node as argument, collects all descen-
dant node/@word attribute values in the variable
$words, sorted by the begin value of their node
element. The yield function returns the string con-
catenation of the elements in $words, separated by
blanks. Note that this solution also gives the correct
result for discontinuous constituents.
We used a wrapper around the XQuery processor
Saxon14 to execute XQuery scripts directly on com-
pacted corpora. The result is output such as:
<term nec="ORG">PvdA</term>
<term nec="LOC">Atlantische Oceaan</term>
A more advanced relation extraction example is
given in fig. 4. It is a script for extraction of events
involving the death of a person from a syntactically
annotated corpus (Dutch wikipedia in this case). It
will return the name of the person who died, and,
if these can be found in the same sentence, the
date, location, and cause of death.15 The script
makes heavy use of functions from the alpino
module that were added to facilitate relation extrac-
tion. The selector-of function defines the ?se-
mantic head? of a phrase. This is either the sibling
marked rel=?hd?, or (for nodes that are them-
selves heads) the head of the mother. For apposi-
tions and conjuncts, it it the selector of the head.
Note that the last case involves a recursive function
call. Similarly, the semantic role is normally iden-
tical to the value of the rel-attribute, but we go up
14www.saxonica.com
15Questions about such facts are relatively frequent in Ques-
tion Answering evaluation tasks.
one additional level for heads, appositions and con-
juncts. The value of $resolved is given by the
resolve-index function shown in fig. 3, i.e. if a
node is just an index (as is the case for the object of
aan tref in fig. 1), the ?antecedent? node is returned.
In all other cases, the node itself is returned. Date
and place are found using functions for locating the
date and place dependents of the verb. Finally, rel-
evant events are found using the die-verb and
kill-verb functions.
Some examples of the output of the extraction
script are (i.e. John Lennon was killed on Decem-
ber 8, 1980, and Erasmus died in Basel on July, 12,
1536):
<died-how place="nil" file="1687-98"
person="John Lennon" cause="vermoord"
date="op 8 december 1980"/>
<died-how place="in Bazel" file="20336-37"
person="Erasmus" cause="overlijd"
date="op 12 juli 1536"/>
The functions illustrated in the two examples can
be used for a range of similar data extraction tasks,
whether these are intended for corpus linguistics re-
search or as part of an information extraction sys-
tem. The definition of corpus specific functions that
cover frequently used syntactic and semantic con-
cepts allows the application specific code to be rel-
atively compact and straightforward. In addition,
code which builds upon well tested corpus specific
functions tends to give more accurate results than
code developed from scratch.
5 Conclusions
In this paper, we have presented an approach to min-
ing syntactically corpora that uses standard XML
technology. It can be used both for corpus explo-
ration as well as for information extraction tasks. By
providing a corpus specific module, the complexity
of such tasks can be reduced. By adopting standard
XML languages, we can benefit optimally from the
fact that these are far more expressive than what is
provided in application specific languages or tools.
In addition, there is no shortage of tools or platforms
supporting these languages. Thus, development of
corpus specific tools can be kept at a minimum.
22
module namespace alpino="alpino.xq" ;
declare function name($constituent as element(node)) as xs:boolean
{ if ( $constituent[@pos=?name?] or
$constituent[@cat = ?mwu?]/node[@neclass=?PER?] )
then fn:true()
else fn:false()
};
declare function neclass($constituent as element(node)) as xs:string
{ if $constituent[@neclass]
then fn:string($constituent/@neclass)
else if $constituent/node[@neclass]
then fn:string($constituent/node[1]/@neclass)
};
declare function alpino:yield($constituent as element(node)) as xs:string
{ let $words :=
for $leaf in $constituent/descendant-or-self::node[@word]
order by number($leaf/@begin)
return $leaf/@word
return string-join($words," ")
};
declare function alpino:resolve-index($constituent as element(node))
as element(node)
{ if ( $constituent[@index and not(@pos or @cat)] )
then $constituent/ancestor::alpino_ds/
descendant::node
[@index = $constituent/@index and (@pos or @cat)]
else $constituent
};
Figure 3: XQuery module (fragment) for Alpino treebanks
for $node in collection(?wikipedia?)/alpino_ds//node
let $verb := alpino:selector-of($node)
let $date := if ( exists(alpino:date-dependents($verb)) )
then alpino:yield(alpino:date-dependents($verb)[1])
else ?nil?
let $place := if ( exists(alpino:location-dependents($verb)) )
then alpino:yield(alpino:location-dependents($verb)[1])
else ?nil?
let $cause := if ( $verb/../node[@rel="pc"]/node[@root="aan"] )
then alpino:yield($verb/../node[@rel="pc"])
else [[omitted]]
let $role := alpino:semantic-role($node)
let $resolved := alpino:resolve-index($node)
where alpino:person-node($resolved)
and ( ( $role="su" and alpino:die-verb($verb) )
or ( $role="obj1" and alpino:kill-verb($verb) )
)
return
<died-how file="{alpino:file-id($node)}" person="{alpino:root-string($resolved)}"
cause="{$cause}" date="{$date}" place = "{$place}" />
Figure 4: Extracting circumstances of the death of a person
23
References
Steven Bird, Yi Chen, Susan B. Davidson, Haejoong
Lee, and Yifeng Zheng. Designing and evalu-
ating an XPath dialect for linguistic queries. In
Proceedings of 22nd International Conference on
Data Engineering (ICDE), 2006.
Gosse Bouma and Geert Kloosterman. Querying de-
pendency treebanks in XML. In Proceedings of
the 3rd conference on Language Resources and
Evaluation (LREC), Gran Canaria, 2002.
Gosse Bouma, Gertjan van Noord, and Robert Mal-
ouf. Alpino: Wide-coverage computational analy-
sis of Dutch. In Computational Linguistics in The
Netherlands 2000. Rodopi, Amsterdam, 2001.
Gosse Bouma, Ismail Fahmi, Jori Mur, Gertjan van
Noord, Lonneke van der Plas, and Jo?rg Tiede-
man. Linguistic knowledge and question answer-
ing. Traitement Automatique des Langues, 2(46):
15?39, 2005.
Gosse Bouma, Petra Hendriks, and Jack Hoeksema.
Focus particles inside prepositional phrases: A
comparison of Dutch, English, and German. Jour-
nal of Comparative Germanic Linguistics, 10(1),
2007.
Razvan Bunescu and Raymond Mooney. A short-
est path dependency kernel for relation extraction.
In Proceedings of HLT/EMNLP, pages 724?731,
Vancouver, 2005.
Steve Cassidy. XQuery as an annotation query lan-
guage: a use case analysis. In Language Re-
sources and Evaluation Conference (LREC), Gran
Canaria, 2002.
Hang Cui, Renxu Sun, Keya Li, Min-Yen Kan, and
Tat-Seng Chua. Question answering passage re-
trieval using dependency relations. In Proceed-
ings of SIGIR 05, Salvador, Brazil, 2005.
W. Haesereyn, K. Romijn, G. Geerts, J. De Rooy,
and M.C. Van den Toorn. Algemene Nederlandse
Spraakkunst. Martinus Nijhoff Uitgevers Gronin-
gen / Wolters Plantyn Deurne, 1997.
Boris Katz and Jimmy Lin. Selectively using rela-
tions to improve precision in question answering.
In Proceedings of the workshop on Natural Lan-
guage Processing for Question Answering (EACL
2003), pages 43?50, Budapest, 2003. EACL.
Michael Kay. Comparing XSLT and XQuery.
In Proceedings of XTech 2005, Amsterdam,
2005. URL www.idealliance.org/
proceedings/xtech05.
Dekan Lin and Patrick Pantel. Discovery of infer-
ence rules for question answering. Natural Lan-
guage Engineering, 7:343?360, 2001.
Neil Mayo, Jonathan Kilgour, and Jean Carletta.
Towards an alternative implementation of NXT
query language via XQuery. In Proceedings of the
EACL Workshop on Multi-dimensional Markup in
NLP, Trento, 2006.
D. Molla? and M. Gardiner. Answerfinder - ques-
tion answering by combining lexical, syntactic
and semantic information. In Australasian Lan-
guage Technology Workshop (ALTW) 2004, Syd-
ney, 2005.
Nelleke Oostdijk. The Spoken Dutch Corpus:
Overview and first evaluation. In Proceedings of
LREC 2000, pages 887?894, 2000.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng.
Learning syntactic patterns for automatic hyper-
nym discovery. In Lawrence K. Saul, Yair Weiss,
and Lon Bottou, editors, Advances in Neural In-
formation Processing Systems 17, pages 1297?
1304. MIT Press, Cambridge, MA, 2005.
L. van der Beek, G. Bouma, R. Malouf, and
G. van Noord. The Alpino dependency treebank.
In Computational Linguistics in the Netherlands
(CLIN) 2001, Twente University, 2002.
Leonoor van der Beek. Topics in Corpus Based
Dutch Syntax. PhD thesis, University of Gronin-
gen, Groningen, 2005.
Lonneke van der Plas and Gosse Bouma. Auto-
matic acquisition of lexico-semantic knowledge
for question answering. In Proceedings of On-
tolex 2005 ? Ontologies and Lexical Resources,
Jeju Island, South Korea, 2005.
Begon?a Villada Moiro?n. Linguistically enriched
corpora for establishing variation in support verb
constructions. In Proceedings of the 6th Inter-
national Workshop on Linguistically Interpreted
Corpora (LINC-2005), Jeju Island, Republic of
Korea, 2005.
24
Proceedings of the EACL 2009 Workshop on the Interaction between Linguistics and Computational Linguistics, pages 33?39,
Athens, Greece, 30 March, 2009. c?2009 Association for Computational Linguistics
Parsed Corpora for Linguistics
Gertjan van Noord
University of Groningen
G.J.M.van.noord@rug.nl
Gosse Bouma
University of Groningen
G.Bouma@rug.nl
Abstract
Knowledge-based parsers are now accu-
rate, fast and robust enough to be used to
obtain syntactic annotations for very large
corpora fully automatically. We argue that
such parsed corpora are an interesting new
resource for linguists. The argument is
illustrated by means of a number of re-
cent results which were established with
the help of parsed corpora.
1 Introduction
Once upon a time, knowledge-based parsers were
slow, inaccurate and fragile. This is no longer
true. In the last decade, enormous improvements
have been achieved in this area. Parsers based on
constraint-based formalisms such as HPSG, LFG,
and CCG are now fast enough for many appli-
cations; they are robust; and they perform much
more accurately than previously by incorporat-
ing, typically, a statistical disambiguation compo-
nent. As a consequence, such parsers now obtain
competitive, if not superior, performance. Zae-
nen (2004), for instance, points out that the (LFG-
based) XLE parser is fast, has a statistical disam-
biguation component, and is robust, and thus al-
lows full parsing to be incorporated in many appli-
cations. Clark and Curran (2007) show that both
accurate and highly efficient parsing is possible
using a CCG.
As a consequence of this development, massive
amounts of parsed sentences now become avail-
able. Such large collections of syntactically an-
notated but not manually verified syntactic analy-
ses are a very useful resource for many purposes.
In this position paper we focus on one purpose:
linguistic analysis. Our claim is, that very large
parsed corpora are an important resource for lin-
guists. Such very large parsed corpora can be
used to search systematically for specific infre-
quent syntactic configurations of interest, and also
to obtain quantitative data about specific syntac-
tic configurations. Although parsed corpora obvi-
ously contain a certain amount of noise, for many
applications the abundant size of these corpora
compensates for this.
In this paper, we illustrate our position by a nu-
mer of recent linguistic studies in which very large
corpora of Dutch have been employed, which
were syntactically annotated by the freely avail-
able Alpino parser (Bouma et al, 2001; van No-
ord, 2006).
The Alpino system incorporates a linguistically
motivated, wide-coverage grammar for Dutch in
the tradition of HPSG. It consists of over 800
grammar rules and a large lexicon of over 300,000
lexemes (including very many person names, geo-
graphical names, and organization names) and var-
ious rules to recognize special constructs such as
named entities, temporal expressions, etc. Since
we use Alpino to parse large amounts of data, it
is crucial that the parser is capable to treat sen-
tences with unknown words. A large set of heuris-
tics have been implemented carefully to deal with
unknown words and word sequences.
Based on the categories assigned to words, and
the set of grammar rules compiled from the HPSG
grammar, a left-corner parser finds the set of all
parses, and stores this set compactly in a packed
parse forest. All parses are rooted by an instance
of the top category, which is a category that gen-
eralizes over all maximal projections (S, NP, VP,
ADVP, AP, PP and some others). If there is no
parse covering the complete input, the parser finds
all parses for each substring. In such cases, the
robustness component will then select the best se-
quence of non-overlapping parses (i.e., maximal
projections) from this set.
In order to select the best parse from the parse
forest, a best-first search algorithm is applied. The
algorithm consults a Maximum Entropy disam-
biguation model to judge the quality of (partial)
33
parses. The disambiguation model is trained on
the manually verified Alpino treebank (about 7100
sentences from newspaper texts).
Although Alpino is not a dependency grammar
in the traditional sense, dependency structures are
generated by the lexicon and grammar rules as
the value of a dedicated feature. The dependency
structures are based on CGN (Corpus Gesproken
Nederlands, Corpus of Spoken Dutch) (Hoekstra
et al, 2003), D-Coi and LASSY (van Noord et al,
2006).
Dependency structures are stored in XML. Ad-
vantages of the use of XML include the avail-
ability of general purpose search and visualiza-
tion software. For instance, we exploit XPATH
(standard XML query language) to search in large
sets of dependency structures, and Xquery to ex-
tract information from such large sets of depen-
dency structures (Bouma and Kloosterman, 2002;
Bouma and Kloosterman, 2007).
2 Extraposition of comparative objects
out of topic
The first illustration of our thesis that parsed cor-
pora provide an interesting new resource for lin-
guists, constitutes more of an anecdote than a sys-
tematic study. We include the example, presented
earlier in van Noord (2009), because it is fairly
easy to explain, and because it was how we be-
came aware ourselves of the potential of parsed
corpora for the purpose of linguistics.
In van der Beek et al (2002), the grammar un-
derlying the Alpino parser is presented in some de-
tail. As an example of how the various specific
rules of the grammar interact with the more gen-
eral principles, the analysis of comparatives and
the interaction with generic principles for (right-
ward) extraposition is illustrated. In short, com-
paratives such as comparative adjectives and the
adverb anders as in the following example (1)
license corresponding comparative phrases (such
as phrases headed by dan (than)) by means of a
feature which percolates according to the extrapo-
sition principle. The analysis is illustrated in fig-
ure 1.
(1) . . . niks
. . . nothing
anders
else
doen
do
dan
than
almaar
continuously
ruw
raw
materiaal
material
verzamelen
collect
do nothing else but collect raw material (cdbl-
7)
Figure 2: Dependency structure for Lager was de
koers dan gisteren
An anonymous reviewer criticized the anal-
ysis, because the extraposition principle would
also allow the rightward extraction of comparative
phrases licensed by comparatives in topic position.
The extraposition principle would have to allow
for this in the light of examples such as
(2) De
The
vraag
question
is
is
gerechtvaardigd
justified
waarom
why
de
the
regering
government
niets
nothing
doet
does
The question is justified why the goverment
does not act
However, the reviewer claimed that comparative
phrases cannot be extraposed out of topic, as ex-
amples such as the following indicate:
(3) ?Lager
Lower
was
was
de
the
koers
rate
dan
than
gisteren
yesterday
The rate never was lower than yesterday
Since the Alpino grammar allows such cases, it
is possible to investigate if genuine examples of
this type occur in parsed corpora. In order to un-
derstand how we can specify a search query for
such cases, it is instructive to consider the de-
pendency structure assigned to such examples in
figure 2. As can be observed in the dependency
graph, the left-right order of nodes does not rep-
resent the left-right ordering in the sentence. The
word-order of words and phrases is indicated with
XML attributes begin and end (not shown in fig-
ure 2) which indicate for each node the begin and
end position in the sentence respectively.
The following XPATH query enumerates all ex-
34
vproj[extra ? ??
vproj[extra ?ompp[dan???
np[extra ?ompp[dan???
n
niks
adj-s[extra ?ompp[dan???
anders
v
doen
ompp[dan?
omp[dan?
dan
sbar
almaar ... verzamelen
Figure 1: Derivation of extraposed comparative object
amples of extraposition of comparative phrases
out of topic. We can then inspect the resulting list
to check whether the examples are genuine.
//node[
@cat="smain"
and
node[
node[@rel="obcomp"]/@end
>
../node[@rel="hd"]/@begin
]/@begin = @begin
]
The query can be read as: find root sentences
in which there is a daughter node, which itself has
a daughter node with relation label obcomp (the
label used for comparative complements). The
daughter node should begin at the same position
as the root sentence. Finally, the end position of
the obcomp node must be larger than the end po-
sition of the head of the root sentence (i.e. the finite
verb).
In addition to many mis-parsed sentences, we
found quite a few genuine cases. A mis-parse
can for instance occur if a sentence contains two
potential licensers for the comparative phrase, as
in the following example in which verder can be
wrongly analysed as a comparative adjective.
(4) Verder
Further
wil
want
ik
I
dat
that
mijn
my
backhand
backhand
even
just-as
goed
good
wordt
becomes
als
as
mijn
my
forehand
forehand
Furthermore, I want my backhand to become
as good as my forehand
More interestingly for the present discussion are
the examples which were parsed correctly. Not
only do we find such examples, but informants
agree that nothing is wrong with such cases. Some
examples are listed in figure 3. It is striking that
many examples involve the comparative adjectives
liever and eerder. Also, the list involves exam-
ples where adverbials such as zo, zozeer, zoveel are
related with an extraposed subordinate sentence
headed by dat which according to the annotation
guidelines are also treated as comparative comple-
ments.
The examples show that at least in some cases,
the possibility of extraposition of comparative
complements out of topic must be allowed; we hy-
pothesize that the acceptability of such cases is not
a binary decision, but rather a preference which
depends on the choice of comparative on the one
hand, and the heaviness of the comparative com-
plement on the other hand.
For the purpose of this paper, we hope to have
illustrated how parsed corpora can be helpful to
find new empirical evidence for fairly complicated
and suble linguistic issues. Note that for a con-
struction of this type, manually verified treebanks
are much too small. We estimated that it takes
about 5 million words to find a single, good, ex-
ample. It appears unrealistic to assume that tree-
banks of the required order of magnitude of tens
of millions of words will become available soon.
3 Frequency versus Complexity
Our second illustration is of a different nature, and
taken from a study related to agrammatic Broca?s
aphasia.
In Bastiaanse et al (to appear), potential causes
are discussed of the problems that patients suffer-
ing from agrammatic Broca?s aphasia encounter.
The Derived Order Problem Hypothesis (Basti-
aanse and van Zonneveld, 2005) assumes that the
linguistic representations of agrammatic patients
are intact, but due to processing disorders, some
representations are harder to retrieve than oth-
35
(5) Liever
Rather
betaalden
paid
werkgevers
employers
een
a
(
(
hoge
high
)
)
verzekeringspremie
insurance-fee
,
,
dan
than
opgescheept
left
te
to
zitten
be
met
with
niet
not
volwaardig
fully
functionerende
functioning
medewerkers
employees
Rather, employers pay a high insurance fee, than be left with not fully functioning employees (Alge-
meen Dagblad, January 15, 1999)
(6) Beter
Better
is
is
het
it
te
to
zorgen
ensure
dat
that
ziekenhuizen
hospitals
hun
their
verplichtingen
obligations
volgens
according-to
de
the
huidige
current
BOPZ
BOPZ
gaan
start
nakomen
meet
,
,
dan
than
de
the
rechten
rights
van
of
pati???12 ten
patients
nog
yet
verder
further
aan
PART
te
to
tasten
violate
It is better to ensure that hospitals start to meet their obligations according to the current BOZP,
than to violate rights of patients even further (Algemeen Dagblad, August 18, 2001)
(7) Dus
So
wat
what
anders
else
konden
could
de
the
LPF?ers
LPF-representatives
de
the
afgelopen
last
week
week
dan
than
zich
self
stil
quiet
houden
keep
?
?
What else could the LPF-representatives do last week , than keep quiet? (Volkskrant June 1, 2002)
(8) Sneller
Faster
kennen
know
ze
they
hun
their
tafels
tables
van
of
vermenigvuldiging
multiplication
dan
than
de
the
handelingen
acts
van
of
de
the
groet
greeting
They know the tables of multiplication faster than the acts of greeting (De Morgen March 27, 2006)
Figure 3: Some genuine examples of extraposition of comparative objects from topic. The examples are
identified automatically using an XPATH query applied to a large parsed corpus.
ers, due to differences in linguistic complexity.
This hypothesis thus assumes that agrammatic pa-
tients have difficulty with constructions of higher
linguistic complexity. An alternative hypothesis
states, that agrammatic patients have more diffi-
culty with linguistic constructions of lower fre-
quency.
In order to compare the two hypotheses, Bas-
tiaanse et al perform three corpus studies. In
three earlier experimental studies it was found that
agrammatic patients have more difficulty with (a)
finite verbs in verb-second position versus finite
verbs in verb-final position; (b) scrambled direct
objects versus non-scrambled direct objects; and
(c) transitive verbs used as unaccusative versus
transitive verbs used as transitive.
The three pairs of constructions are illustrated
as follows.
(9) a. de
the
jongen
boy
die
who
een
a
boek
book
leest
reads
the boy who reads a book
b. de
the
jongen
boy
leest
reads
een
a
boek
book
the boy reads a book
(10) a. dit
this
is
is
de
the
jongen
boy
die
who
vandaag
today
het
the
boek
book
leest
reads
this is the boy who reads the book today
b. dit
this
is
is
de
the
jongen
boy
die
who
het
the
boek
book
vandaag
today
leest
reads
this is the boy who reads the book today
(11) a. de
the
jongen
boy
breekt
breaks
het
the
glas
glass
the boy breaks the glass
b. het
the
glas
glass
breekt
breaks
the glass breaks
In each of the three cases, corpus data is used
to estimate the frequency of both syntactic con-
figurations. Two corpora were used: the manu-
ally verified syntactically annotated CGN corpus
(spoken language, approx. 1M words), and the the
automatically parsed TwNC corpus (Ordelman et
al., 2007) (the newspapers up to 2001, a parsed
corpus of 300 million words). For the first two
experiments, manual inspection revealed that the
parsed corpus material was of high enough quality
to be used directly. Furthermore, the relevant con-
structions are highly frequent, and thus even rela-
tively small corpora (such as the syntactically an-
36
notated part of CGN) provide sufficient data. For
the third experiment (unaccusative versus transi-
tive usage of verbs), an additional layer of manual
verification was used, and furthermore, as the sub-
categorization frequencies of individual verbs are
estimated, the full TwNC was searched in order to
obtain reasonably reliable estimates.
The outcome of the three experiments was the
same in each case: frequency information cannot
explain the difficulty encountered by agrammatic
patients. Verb-second is more frequent than verb-
final word order for lexical verbs and transitive
lexical verbs (the verbs used in the experiments
were all transitive). Finite verbs occur slightly
more often in verb-second position than in verb-
final position, but the difference is quite small.
Scrambled word order is more frequent than the
basic word order. The difference between the two
corpora (CGN and TwNC) is quite small in both
cases. Figure 4 gives an overview of the number
of occurrences of the transitive and unaccusative
use of the verbs used in the experiments in the
full TwNC. The data suggest that the relative fre-
quency of unaccusative depends strongly on the
verb, but that it is not in general the case that the
unaccusative use is less frequent than the transitive
use.
The three ?difficult? constructions used in the
experiments with aphasia patients are by no means
infrequent in Dutch. The authors conclude that the
hypothesis that processing difficulties are corre-
lated with higher linguistic complexity cannot be
falsified by an appeal to frequency.
What is interesting for the purposes of the cur-
rent paper, is that parsed corpora are used to es-
timate frequencies of syntactic constructions, and
that these are used to support claims about the role
of linguistic complexity in processing difficulties
of aphasia patients. Also note that figure 4 shows
that even in a large (300M word) corpus, the num-
ber of occurrences of a specific verb used with a
specific valency frame can be quite small. Thus,
it is unlikely that reliable frequency estimates can
be obtained for these cases from manually verified
treebanks.
Roland et al (2007) report on closely related
work for English. In particular, they give fre-
quency counts for a range of syntactic construc-
tions in English, and subcategorization frequen-
cies for specific verbs. They demonstrate that
these frequencies are highly dependent on corpus
and genre in a number of cases. They use their data
to verify claims in the psycholinguistic literature
about the processing of subject vs. object clefts,
relative clauses and sentential complements.
4 The distribution of zelf and zichzelf
As a further example of the use of parsed corpora
to further linguistic insights, we consider a recent
study (Bouma and Spenader, 2009) of the distribu-
tion of weak and strong reflexive objects in Dutch.
If a verb is used reflexively in Dutch, two forms
of the reflexive pronoun are available. This is il-
lustrated for the third person form in the examples
below.
(12) Brouwers
Brouwers
schaamt
shames
zich/?zichzelf
self1/self2
voor
for
zijn
his
schrijverschap.
writing
Brouwers is ashamed of his writing
(13) Duitsland
Germany
volgt
follows
?zich/zichzelf
self1/self2
niet
not
op
PART
als
as
Europees
European
kampioen.
Champion
Germany does not succeed itself as Euro-
pean champion
(14) Wie
Who
zich/zichzelf
self1/self2
niet
not
juist
properly
introduceert,
introduces,
valt
is
af.
out
Everyone who does not introduce himself
properly, is out.
The choice between zich and zichzelf depends on
the verb. Generally three groups of verbs are
distinguished. Inherent reflexives are claimed to
never occur with a non-reflexive argument, and as
a reflexive argument are claimed to use zich exclu-
sively, (12). Non-reflexive verbs seldom, if ever
occur with a reflexive argument. If they do how-
ever, they can only take zichzelf as a reflexive ar-
gument (13). Accidental reflexives can be used
with both zich and zichzelf, (14). Accidental re-
flexive verbs vary widely as to the frequency with
which they occur with both arguments. Bouma
and Spenader (2009) set out to explain this dis-
tribution.
The influential theory of Reinhart and Reuland
(1993) explains the distribution as the surface real-
ization of two different ways of reflexive coding.
An accidental reflexive that can be realized with
37
verb unacc trans
# % # %
luiden to ring/sound 269 26.6 743 73.4
scheuren to rip 332 28.8 819 71.2
breken to break 1969 31.2 4341 68.8
verbrand to burn 479 43.5 623 56.5
oplossen to (dis)solve 296 59.2 204 40.8
draaien to turn 2709 59.4 1852 40.6
smelten to melt 723 71.4 290 28.6
rollen to roll 3500 93.5 244 6.5
verdrinken to drown 1397 94.6 80 5.4
stuiteren to bounce 334 97.9 7 2.1
Figure 4: Estimated number of occurrences in TwNC of unaccusative and transitive uses of Dutch verbs
which may undergo the causative alternation
both zich and zichzelf is actually ambiguous be-
tween an inherent reflexive and an accidental re-
flexive (which always is realized with zichzelf).
An alternative approach is that of Haspelmath
(2004), Smits et al (2007), and Hendriks et al
(2008), who have claimed that the distribution of
weak vs. strong reflexive object pronouns corre-
lates with the proportion of events described by
the verb that are self-directed vs. other-directed.
In the course of this investigation, a first inter-
esting observation is, that many inherently reflex-
ive verbs, which are claimed not to occur with
zichzelf, actually often do combine with this pro-
noun. Here are a number of examples (simplified
for expository purposes):
(15) Nederland
Netherlands
moet
must
stoppen
stop
zichzelf
self2
op
on
de
the
borst
chest
te
to
slaan
beat
The Netherlands must stop beating itself on
the chest
(16) Hunze
Hunze
wil
want
zichzelf
self2
niet
not
al
all
te
too
zeer
much
op
on
de
the
borst
chest
kloppen
knock
Hunze doesn?t want to knock itself on the
chest too much
(17) Ze
They
verloren
lost
zichzelf
self2
soms
sometimes
in
in
het
tactical
gegoochel
variants
met alerlei tactische varianten
They sometimes lost themselves in tactical
variants
With regards to the main hypothesis of their
study, (Bouma and Spenader, 2009) use linear re-
gression to determine the correlation between re-
flexive use of a (non-inherently reflexive) verb and
the relative preference for a weak or strong re-
flexive pronoun. Frequency counts are collected
from the parsed TwNC corpus (almost 500 mil-
lion words). They limit the analysis to verbs that
occur at least 10 times with a reflexive meaning
and at least 50 times in total, distinguishing uses
by subcategorization frames. The statistical analy-
sis shows a significant correlation, which accounts
for 30% of the variance of the ratio of nonreflexive
over reflexive uses.
5 Conclusion
Knowledge-based parsers are now accurate, fast
and robust enough to be used to obtain syntactic
annotations for very large corpora fully automati-
cally. We argued that such parsed corpora are an
interesting new resource for linguists. The argu-
ment is illustrated by means of a number of re-
cent results which were established with the help
of huge parsed corpora.
Huge parsed corpora are especially crucial (1)
to obtain evidence concerning infrequent syntac-
tic configurations, and (2) to obtain more reliable
quantitative data about particular syntactic config-
urations.
Acknowledgments
This research was carried out in part in the
context of the STEVIN programme which is
funded by the Dutch and Flemish governments
38
(http://taalunieversum.org/taal/technologie/stevin/).
References
Roelien Bastiaanse and Ron van Zonneveld. 2005.
Sentence production with verbs of alternating tran-
sitivity in agrammatic Broca?s aphasia. Journal of
Neurolinguistics, 18(1):57?66, January.
Roelien Bastiaanse, Gosse Bouma, and Wendy Post.
to appear. Frequency and linguistic complexity in
agrammatic speech production. Brain and Lan-
guage.
Gosse Bouma and Geert Kloosterman. 2002. Query-
ing dependency treebanks in XML. In Proceedings
of the Third international conference on Language
Resources and Evaluation (LREC), pages 1686?
1691, Gran Canaria, Spain.
Gosse Bouma and Geert Kloosterman. 2007. Mining
syntactically annotated corpora using XQuery. In
Proceedings of the Linguistic Annotation Workshop,
Prague, June. ACL.
Gosse Bouma and Jennifer Spenader. 2009. The distri-
bution of weak and strong object reflexives in Dutch.
In Frank van Eynde, Anette Frank, Koenraad De
Smedt, and Gertjan van Noord, editors, Proceed-
ings of the Seventh International Workshop on Tree-
banks and Linguistic Theories (TLT 7), number 12
in LOT Occasional Series, pages 103?114, Utrecht,
The Netherlands. Netherlands Graduate School of
Linguistics.
Gosse Bouma, Gertjan van Noord, and Robert Mal-
ouf. 2001. Wide coverage computational analysis
of Dutch. InW. Daelemans, K. Sima?an, J. Veenstra,
and J. Zavrel, editors, Computational Linguistics in
the Netherlands 2000.
S. Clark and J.R. Curran. 2007. Wide-Coverage Effi-
cient Statistical Parsing with CCG and Log-Linear
Models. Computational Linguistics, 33(4):493?
552.
Martin Haspelmath. 2004. A frequentist explanation
of some universals of reflexive marking. Draft of a
paper presented at the Workshop on Reciprocals and
Reflexives, Berlin.
Petra Hendriks, Jennifer Spenader, and Erik-Jan Smits.
2008. Frequency-based constraints on reflexive
forms in Dutch. In Proceedings of the 5th Interna-
tional Workshop on Constraints and Language Pro-
cessing, pages 33?47, Roskilde, Denmark.
Heleen Hoekstra, Michael Moortgat, Bram Renmans,
Machteld Schouppe, Ineke Schuurman, and Ton
van der Wouden, 2003. CGN Syntactische Anno-
tatie, December.
Roeland Ordelman, Franciska de Jong, Arjan van Hes-
sen, and Hendri Hondorp. 2007. TwNC: a mul-
tifaceted Dutch news corpus. ELRA Newsletter,
12(3/4):4?7.
Tanya Reinhart and Eric Reuland. 1993. Reflexivity.
Linguistic Inquiry, 24:656?720.
Douglas Roland, Frederic Dick, and Jeffrey L. El-
man. 2007. Frequency of basic english grammatical
structures: A corpus analysis. Journal of Memory
and Language, 57(3):348?379, October.
Erik-Jan Smits, Petra Hendriks, and Jennifer Spenader.
2007. Using very large parsed corpora and judge-
ment data to classify verb reflexivity. In Antonio
Branco, editor, Anaphora: Analysis, Algorithms and
Applications, pages 77?93, Berlin. Springer.
Leonoor van der Beek, Gosse Bouma, and Gertjan van
Noord. 2002. Een brede computationele grammat-
ica voor het Nederlands. Nederlandse Taalkunde,
7(4):353?374.
Gertjan van Noord, Ineke Schuurman, and Vincent
Vandeghinste. 2006. Syntactic annotation of large
corpora in STEVIN. In Proceedings of the 5th In-
ternational Conference on Language Resources and
Evaluation (LREC), Genoa, Italy.
Gertjan van Noord. 2006. At Last Parsing Is Now
Operational. In TALN 2006 Verbum Ex Machina,
Actes De La 13e Conference sur Le Traitement
Automatique des Langues naturelles, pages 20?42,
Leuven.
Gertjan van Noord. 2009. Huge parsed corpora in
Lassy. In Frank van Eynde, Anette Frank, Koen-
raad De Smedt, and Gertjan van Noord, editors, Pro-
ceedings of the Seventh International Workshop on
Treebanks and Linguistic Theories (TLT 7), num-
ber 12 in LOT Occasional Series, pages 115?126,
Utrecht, The Netherlands. Netherlands Graduate
School of Linguistics.
Annie Zaenen. 2004. but full parsing is impossible.
ELSNEWS, 13(2):9?10.
39
Proceedings of CLIAWS3, Third International Cross Lingual Information Access Workshop, pages 21?29,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Cross-lingual Alignment and Completion of Wikipedia Templates
Gosse Bouma
Information Science
University of Groningen
g.bouma@rug.nl
Sergio Duarte
Information Science
University of Groningen
sergio.duarte@gmail.com
Zahurul Islam
Information Science
University of Groningen
zaisdb@gmail.com
Abstract
For many languages, the size of Wikipedia is
an order of magnitude smaller than the En-
glish Wikipedia. We present a method for
cross-lingual alignment of template and in-
fobox attributes in Wikipedia. The alignment
is used to add and complete templates and
infoboxes in one language with information
derived from Wikipedia in another language.
We show that alignment between English and
Dutch Wikipedia is accurate and that the re-
sult can be used to expand the number of tem-
plate attribute-value pairs in Dutch Wikipedia
by 50%. Furthermore, the alignment pro-
vides valuable information for normalization
of template and attribute names and can be
used to detect potential inconsistencies.
1 Introduction
One of the more interesting aspects of Wikipedia
is that it has grown into a multilingual resource,
with Wikipedia?s for many languages, and system-
atic (cross-language) links between the information
in different language versions. Eventhough English
has the largest Wikipedia for any given language, the
amount of information present in Wikipedia exceeds
that of any single Wikipedia. One of the reasons for
this is that each language version of Wikipedia has
its own cultural and regional bias. It is likely, for
instance, that information about the Netherlands is
better represented in Dutch Wikipedia than in other
Wikipedia?s. Some indication that this is indeed
the case comes from the fact a Google search for
?Pim Fortuyn? in the DutchWikipedia gives 498 hits,
whereas the English Wikipedia gives only 292 hits.
Also, 21,697 pages in DutchWikipedia fall in a cate-
gory matching ?Nederlands(e)?, whereas only 9,494
pages in English Wikipedia fall in a category match-
ing ?Dutch?. This indicates that, apart from the ob-
vious fact that smaller Wikipedia?s can be expanded
with information found in the larger Wikipedia?s,
it is also true that even the larger Wikipedia?s can
be supplemented with information harvested from
smaller Wikipedia?s.
Wikipedia infoboxes are tabular summaries of the
most relevant facts contained in an article. They
represent an important source of information for
general users of the encyclopedia. Infoboxes (see
figure 1) encode facts using attributes and values,
and therefore are easy to collect and process au-
tomatically. For this reason, they are extremely
valuable for systems that harvest information from
Wikipedia automatically, such as DbPedia (Auer et
al., 2008). However, as Wu and Weld (2007) note,
infoboxes are missing for many pages, and not all
infoboxes are complete. This is particularly true for
Wikipedia?s in languages other than English.
Infoboxes are a subclass of Wikipedia templates,
which are used by authors of Wikipedia pages to ex-
press information in a systematic way, and to en-
sure that formatting of this information is consistent
across Wikipedia. Templates exist for referring to
multimedia content, external websites, news stories,
scientific sources, other on-line repositories (such as
the Internet Movie Database (IMDB), medical clas-
sification systems (ICD9 and ICD10), coordinates on
Google Maps, etc. Although we are primarily inter-
ested in infoboxes, in the experiments below we take
21
{{Infobox Philosopher |
region = Western Philosophy |
era = [[19th-century philosophy]] |
image_name = George Boole.jpg|
image_caption = George Boole |
name = George Lawlor Boole |
birth = November 2, 1815 |
death = December 8, 1864 |
school = Mathematical foundations
of [[computer science]] |
main_interests = [[Mathematics]], [[Logic]] |
ideas = [[Boolean algebra]]
}}
Figure 1: Infobox and (simplified) Wikimedia source
all templates into account.
We plan to use information mined fromWikipedia
for Question Answering and related tasks. In 2007
and 2008, the CLEF question answering track1 used
Wikipedia as text collection. While the usual ap-
proach to open domain question answering relies on
information retrieval for selecting relevant text snip-
pets, and natural language processing techniques for
answer extraction, an alternative stream of research
has focussed on the potential of on-line data-sets for
question answering (Lita et al, 2004; Katz et al,
2005). In Bouma et al (2008) it is suggested that
information harvested from infoboxes can be used
for question answering in CLEF. For instance, the
answer to questions such as How high is the Matter-
horn?, Where was Piet Mondriaan born?, and What
is the area of the country Suriname? can in princi-
ple be found in infoboxes. However, in practice the
number of questions that is answered by their Dutch
QA-system by means information from infoboxes is
small. One reason for this is the lack of coverage of
infoboxes in Dutch Wikipedia.
1http://clef-qa.itc.it
In the recent GIKICLEF task2 systems have to find
Wikipedia pages in a number of languages which
match descriptions such as Which Australian moun-
tains are higher than 2000 m?, French bridges which
were in construction between 1980 and 1990, and
African capitals with a population of two million
inhabitants or more. The emphasis in this task is
less on answer extraction from text (as in QA) and
more on accurate interpretation of (geographical)
facts known about an entity. GIKICLEF is closely
related to the entity ranking task for Wikipedia, as
organized by INEX.3 We believe systems partici-
pating in tasks like this could profit from large col-
lections of ?entity,attribute,value? triples harvested
from Wikipedia templates.
In this paper, we propose a method for automati-
cally expanding the amount of information present
in the form of templates. In our experiments,
we used English and Dutch Wikipedia as sources.
Given a page in English, and a matching page in
Dutch, we first find all English-Dutch attribute-value
2http://www.linguateca.pt/GikiCLEF
3http://inex.is.informatik.uni-duisburg.
de
22
tuples which have a matching value. Based on the
frequency with which attributes match, we create
a bidirectional, intersective, alignment of English-
Dutch attribute pairs. Finally, we use the set of
aligned attributes to expand the number of attribute-
value pairs in Dutch Wikipedia with information ob-
tained from matching English pages. We also show
that aligned attributes can be used to normalize at-
tribute names and to detect formatting issues and po-
tential inconsistencies in attribute values.
2 Previous Work
DbPedia (Auer et al, 2008) is a large, on-going,
project which concentrates on harvesting informa-
tion from Wikipedia automatically, on normaliza-
tion of the extracted information, on linking the in-
formation with other on-line data repositories, and
on interactive access. It contains 274M facts about
2.6M entities (November, 2008). An important com-
ponent of DbPedia is harvesting of the information
present in infoboxes. However, as Wu and Weld
(2007) note, not all relevant pages have (complete)
infoboxes. The information present in infoboxes
is typically also present in the running text of a
page. One line of research has concentrated on
using the information obtained from infoboxes as
seeds for systems that learn relation extraction pat-
terns (Nguyen et al, 2007). Wu and Weld (2007)
go one step further, and concentrate on learning to
complete the infoboxes themselves. They present
a system which first learns to predict the appropri-
ate infobox for a page (using text classification).
Next, they learn relation extraction patterns using
the information obtained from existing infoboxes as
seeds. Finally, the learned patterns are applied to
text of pages for which a new infobox has been pre-
dicted, to assign values to infobox attributes. A re-
cent paper by Adar et al (2009) (which only came
to our attention at the time of writing) starts from the
same observation as we do. It presents a system for
completing infoboxes for English, German, French,
and Spanish Wikipedia, which is based on learning a
mapping between infoboxes and attributes in multi-
ple languages. A more detailed comparison between
their approach and ours is given in section 6
The potential of the multilingual nature of
Wikipedia has been explored previously by several
researchers. Adafre and de Rijke (2006) explore ma-
chine translation and (cross-lingual) link structure
to find sentences in English and Dutch Wikipedia
which express the same content. Bouma et al (2006)
discuss a system for the English-Dutch QA task
of CLEF. They basically use a Dutch QA-system,
which takes questions automatically translated from
English (by the on-line Babelfish translation ser-
vice). To improve the quality of the translation
of named entities, they use, among others, cross-
language links obtained from Wikipedia. Erdmann
et al (2008) explore the potential of Wikipedia for
the extraction of bilingual terminology. They note
that apart from the cross-language links, page redi-
rects and anchor texts (i.e. the text that is used to
label a hypertext reference to another (wikipedia)
page) can be used to obtain large and accurate bilin-
gual term lists.
3 Data collection and Preparation
We used a dump of Dutch Wikipedia (June 2008)
and English Wikipedia (August 2007) made avail-
able by the University of Amsterdam4 and converted
to an XML-format particularly suitable for informa-
tion extraction tasks.
From these two collections, for each page, we
extracted all attribute-value pairs found in all tem-
plates. Results were stored as quadruples of the
form ?Page, TemplateName, Attribute, Value?. Each
TemplateName?Attribute pair expresses a specific
semantic relation between the entity or concept de-
scribed by the Page and a Value. Values can be
anything, but often refer to another Wikipedia page
(i.e. ?George Boole, Philosopher, notable ideas,
Boolean algebra?, where Boolean algebra is a link
to another page) or to numeric values, amounts, and
dates. Note that attributes by themselves tend to be
highly ambiguous, and often can only be interpreted
in the context of a given template. The attribute pe-
riod, for instance, is used to describe chemical el-
ements, royal dynasties, countries, and (historical)
means of transportation. Another source of attribute
ambiguity is the fact that many templates simply
number their attributes. As we are interested in find-
ing an alignment between semantically meaningful
relations in the two collections, we will therefore
4http://ilps.science.uva.nl/WikiXML
23
Dutch English
date June 2008 August 2007
pages 715,992 3,840,950
pages with template 290,964 757,379
cross-language links 126,555
templates 550,548 1,074,935
tuples 4,357,653 5,436,033
template names 2,350 7,783
attribute names 7,510 19,378
templ?attr pairs 23,399 81,671
Table 1: Statistics for the version of Dutch and English
Wikipedia used in the experiment.
concentrate on the problem of finding an alignment
between TemplateName?Attribute pairs in English
and Dutch Wikipedia.
Some statistics for the two collections are given
in table 1. The number of pages is the count for all
pages in the collection. It should be noted that these
contain a fair number of administrative pages, pages
for multimedia content, redirect pages, page stubs,
etc. The number of pages which contains content
that is useful for our purposes is therefore probably
a good deal lower, and is maybe closer to the num-
ber of pages containing at least one template. Cross-
language links (i.e. links from an English page to
the corresponding Dutch page) where extracted from
English Wikipedia. The fact that 0.5M templates in
Dutch give rise to 4.3M tuples, whereas 1.0M tem-
plates in English give rise to only 5.4M tuples is per-
haps a consequence of the fact that the two collec-
tions are not from the same date, and thus may re-
flect different stages of the development of the tem-
plate system.
We did spend some time on normalization of the
values found in extracted tuples. Our alignment
method relies on the fact that for a sufficient num-
ber of matching pages, tuples can be found with
matching values. Apart from identity and Wikipedia
cross-language links, we rely on the fact that dates,
amounts, and numerical values can often be rec-
ognized and normalized easily, thus increasing the
number of tuples which can be used for alignment.
Normalization addresses the fact that the use of
comma?s and periods (and spaces) in numbers is
different in English and Dutch Wikipedia, and that
dates need to be converted to a standard. English
Wikipedia expresses distances and heights in miles
and feet, weights in pounds, etc., whereas Dutch
Wikipedia uses kilometres, metres, and kilograms.
Where English Wikipedia mentions both miles and
kilometres we preserve only the kilometres. In other
situations we convert miles to kilometres. In spite
of this effort, we noted that there are still quite a
few situations which are not covered by our normal-
ization patterns. Sometimes numbers are followed
or preceded by additional text (approx.14.5 MB, 44
minutes per episode), sometimes there is irregular
formatting (October 101988), and some units sim-
ply are not handled by our normalization yet (i.e.
converting square miles to square kilometres). We
come back to this issue in section 6.
Kro?tzsch et al (2007) have aslo boserved that
there is little structure in the way numeric values,
units, dates, etc. are represented in Wikipedia. They
suggest a tagging system similar to the way links to
other Wikipedia pages are annotated, but now with
the aim of representing numeric and temporal values
systematically. If such a system was to be adopted
by the Wikipedia community, it would greatly fa-
cilitate the processing of such values found in in-
foboxes.
4 Alignment
In this section we present our method for aligning
English Template?Attribute pairs with correspond-
ing pairs in Dutch.
The first step is creating a list of matching tuples.
Step 1. Extract all matching template
tuples.
An English ?Pagee, Temple?Attre,
Vale? tuple matches a Dutch ?Paged,
Templd?Attrd, Vald? tuple if Pagee
matches Paged and Vale matches Vald and
there is no other tuple for either Pagee or
Paged with value Vale or Vald.
Two pages or values E and D match if
there exists a cross-language link which
links E and D, or if E=D.
We only take into account tuples for which there
is a unique (non-ambiguous) match between English
and Dutch. Many infoboxes contain attributes which
24
often take the same value (i.e. title and imdb title
for movies). Other cases of ambiguity are caused
by numerical values which incidentally may take
on identical values. Such ambiguous cases are ig-
nored. Step 1 gives rise to 149,825 matching tu-
ples.5 It might seem that we find matching tuples
for only about 3-4% of the tuples present in Dutch
Wikipedia. Note, however, that while there are 290K
pages with a template in Dutch Wikipedia, there are
only 126K cross-language links. The total numer
of tuples on Dutch pages for which a cross-language
link to English exists is 837K. If all these tuples have
a counterpart in English Wikipedia (which is highly
unlikely), our method finds a match for 18% of the
relevant template tuples.6
The second step consists of extracting matching
English-Dutch Template?Attribute pairs from the
list of matching tuples constructed in step 1.
Step 2. For each matching pair of tuples
?Pagee, Temple?Attre, Vale? and ?Paged,
Templd?Attrd, Vald?, extract the English-
Dutch pair of Template?Attributes
?Temple?Attre,Templd?Attrd?.
In total, we extracted 7,772 different English-
Dutch ?Temple?Attre,Templd?Attrd? tuples. In
547 cases Temple?Attre=Templd?Attrd. In 915
cases, Attre=Attrd. In the remaining 6,310 cases,
Attre 6=Attrd. The matches are mostly accurate. We
evaluated 5% of the matching template?attribute
pairs, that had been found at least 2 times. For 27%
of these (55 out 205), it was not immediately clear
whether the match was correct, because one of the
attributes was a number. Among the remaing 150
cases, the only clear error seemed to be a match be-
tween the attributes trainer and manager (for soc-
cer club templates). Other cases which are perhaps
not always correct were mappings between succes-
sor, successor1, successor2 on the one hand and af-
ter/next on the other hand. The attributes with a
5It is interesting to note that 51K of these matching tuples
are for pages that have an identical name in English and Dutch,
but were absent in the table of cross-language links. As a result,
we find 32K pages with an identical name in English and Dutch,
and at least one pair of matching tuples. We suspect that these
newly discovered cross-language links are highly accurate.
6If we also include English pages with a name identical to a
Dutch page, the maximum number of matching tuples is 1.1M,
and we find a match for 14% of the data
101 cite web title
27 voetnoot web titel
12 film titel
10 commons 1
7 acteur naam
6 game naam
5 ster naam
4 taxobox zoogdier w-naam
4 plaats naam
4 band band naam
3 taxobox w-naam
Table 2: Dutch template?attribute pairs matching En-
glish cite web?title. Counts refer to the number of pages
with a matching value.
number suffix probably refer to the nth successor,
whereas the attributes without suffix probably refer
to the immediate successor.
On the other hand, for some frequent
template?attribute pairs, ambiguity is clearly
an issue. For the English pair cite web?title for
instance, 51 different mappings are found. The
most frequent cases are shown in table 2. Note that
it would be incorrect to conclude from this that, for
every English page which contains a cite web?title
pair, the corresponding Dutch page should include,
for instance, a taxobox?w-naam tuple.
In the third and final step, the actual alignment
between English-Dutch template?attribute pairs is
established, and ambiguity is eliminated.
Step 3. Given the list of matching
template?attribute pairs computed in step
2 with a frequency ?5, find for each En-
glish Temple?Attre pair the most frequent
matching Dutch pair Templd?Attrd. Simi-
larly, for each Dutch pair Templd?Attrd,
find the most frequent English pair
Temple?Attre. Return the intersection of
both lists.
2,070 matching template?attribute tuples are
seen at least 5 times. Preference for the
most frequent bidirectional match leaves 1,305
template?attribute tuples. Examples of aligned tu-
ples are given in table 3. We evaluated 10% of
the tuples containing meaningful attributes (i.e. not
25
English Dutch
Template Attribute Template Attribute
actor spouse acteur partner
book series boek reeks
casino owner casino eigenaar
csi character portrayed csi personage acteur
dogbreed country hond land
football club ground voetbal club stadion
film writer film schrijver
mountain range berg gebergte
radio station airdate radiozender lancering
Table 3: Aligned template?attribute pairs
numbers or single letters). In 117 tuples, we dis-
covered two errors: ?aircraft specification?number
of props, gevechtsvliegtuig?bemanning? aligns the
number of engines with the number of crew
members (based on 10 matching tuples), and
?book?country, film?land? involves a mismatch of
templates as it links the country attribute for a book
to the country attribute for a movie.
Note that step 3 is similar to bidirectional inter-
sective word alignment as used in statistical machine
translation (see Ma et al (2008), for instance). This
method is known for giving highly precise results.
5 Expansion
We can use the output of step 3 of the alignment
method to check for each English tuple whether a
corresponding Dutch tuple can be predicted. If the
tuple does not exist yet, we add it. In total, this
gives rise to 2.2M new tuples for 382K pages for
Dutch Wikipedia (see table 4). We generate almost
300K new tuples for existing Dutch pages (250K for
pages for which a cross-language link already ex-
isted). This means we exand the total number of
tuples for existing pages by 27%. Most tuples, how-
ever, are generated for pages which do not yet exist
in Dutch Wikipedia. These are perhaps less useful,
although one could use the results as knowledge for
a QA-system, or to generate stubs for newWikipedia
pages which already contain an infobox and other
relevant templates.
The 100 most frequenty added template?attribute
pairs (ranging from music album?genre (added
31,392 times) to single?producer (added 5605
pages triples
existing pages 50,099 253,829
new cross-links 11,526 43,449
new dutch pages 321,069 1,931,277
total 382,694 2,228,555
Table 4: Newly inferred template tuples
times)) are dominated by templates for music al-
bums, geographical places, actors, movies, and tax-
onomy infoboxes.
We evaluated the accuracy of the newly gener-
ated tuples for 100 random existing Dutch wikipedia
pages, to which at least one new tuple was added.
The pages contained 802 existing tuples. 876
tuples were added by our automatic expansion
method. Of these newly added tuples, 62 con-
tained a value which was identical to the value of
an already existing tuple (i.e. we add the tuple
?Reuzenhaai, taxobox?naam, Reuzenhaai? where
there was already an existing tuple ?Reuzenhaai,
taxobox begin?name, Reuzenhaai? tuple ? note that
we add a properly translated attribute name, where
the original tuple contains a name copied from En-
glish!). The newly added tuples contained 60 tu-
ples of the form ?Aegna, plaats?lat dir, N (letter)?,
where the value should have been N (the symbol for
latitude on the Northern hemisphere in geographical
coordinates), and not the letter N. One page (Akira)
was expanded with an incoherent set of tuples, based
on tuples for the manga, anime, and music producer
with the same name. Apart from this failure, there
were only 5 other clearly incorrect tuples (adding
o.a. place?name to Albinism, adding name in Dutch
with an English value to Macedonian, and adding
community?name to Christopher Columbus). In
many cases, added tuples are based on a different
template for the same entity, often leading to almost
identical values (i.e. adding geographical coordi-
nates using slightly different notation). In one case,
Battle of Dogger Bank (1915), the system added new
tuples based on a template that was already in use for
the Dutch page as well, thus automatically updating
and expanding an existing template.
26
geboren population
23 birth date 49 inwoners
16 date of birth 9 population
8 date of birth 5 bevolking
8 dateofbirth 4 inwonersaantal
2 born 3 inwoneraantal
2 birth 2 town pop
1 date birth 2 population total
1 birthdate 1 townpop
1 inw.
1 einwohner
Table 6: One-to-many aligned attribute names. Counts
are for the number of (aligned) infoboxes that contain the
attribute.
6 Discussion
6.1 Detecting Irregularities
Instead of adding new information, one may also
search for attribute-value pairs in two Wikipedia?s
that are expected to have the same value, but do
not. Given an English page with attribute-value
pair ?Attre, Vale?, and a matching Dutch page with
?Attrd, Vald?, where Attre and Attrd have been
aligned, one expects Vale and Vald to match as
well. If this is not the case, something irregular
is observed. We have applied the above rule to
our dataset, and detected 79K irregularities. An
overview of the various types of irregularities is
given in table 5. Most of the non-matching values
are the result of formatting issues, lack of transla-
tions, one value being more specific than the other,
and finally, inconsistencies. Note that inconsisten-
cies may also mean that one value is more recent
than the other (population, (stadium) capacity, latest
release data, spouse, etc.). A number of formatting
issues (of numbers, dates, periods, amounts, etc.)
can be fixed easily, using the current list of irregu-
larities as starting point.
6.2 Normalizing Templates
It is interesting to note that alignment can also be
used to normalize template attribute names. Table 6
illustrates this for the Dutch attribute geboren and
the English attribute population. Both are aligned
with a range of attribute names in the other language.
Such information is extremely valuable for ap-
plications that attempt to harvest knowledge from
Wikipedia, and merge the result in an ontology, or
attempt to use the harvested information in an ap-
plication. For instance, a QA-system that has to an-
swer questions about birth dates or populations, has
to know which attributes are used to express this in-
formation. Alternatively, one can also use this infor-
mation to normalize attribute-names. In that case,
all attributes which express the birth date property
could be replaced by birth date (the most frequent
attribute currently in use for this relation).
This type of normalization can greatly reduce the
noisy character of the current infoboxes. For in-
stance, there are many infoboxes in use for geo-
graphic locations, people, works of art, etc. These
infoboxes often contain information about the same
properties, but, as illustrated above, there is no guar-
antee that these are always expressed by the same
attribute.
6.3 Alignment by means of translation
Template and attribute names in Dutch often are
straightforward translations of the English name,
e.g. luchtvaartmaatschappij/airline, voetbal-
club/football club, hoofdstad/capital, naam/name,
postcode/postalcode, netnummer/area code and
opgericht/founded. One might use this information
as an alternative for determining whether two
template?attribute pairs express the same relation.
We performed a small experiment on in-
foboxes expressing geographical information, using
Wikipedia cross-language links and an on-line dic-
tionary as multilingual dictionaries. We found that
10 to 15% of the attribute names (depending on the
exact subset of infoboxes taken into consideration)
could be connected using dictionaries. When com-
bined with the attributes found by means of align-
ment, coverage went up to maximally 38%.
6.4 Comparison
It is hard to compare our results with those of Adar
et al (2009). Their method uses a Boolean classi-
fier which is trained using a range of features to de-
termine whether two values are likely to be equiva-
lent (including identity, string overlap, link relations,
translation features, and correlation of numeric val-
ues). Training data is collected automatically by
27
Attributes Values Type
English Dutch English Dutch
capacity capaciteit 23,400 23 400 formatting
nm lat min 04 4 formatting
date date 1775-1783 1775&#8211;1783 formatting
name naam African baobab Afrikaanse baobab translation
artist artiest Various Artists Verschillende artiesten translation
regnum rijk Plantae Plantae (Planten) specificity
city naam, Comune di Adrara San Martino Adrara San Martino specificity
birth date geboren 1934 1934-8-25 specificity
imagepath coa wapenafbeelding coa missing.jpg Alvaneu wappen.svg specificity
population inwonersaantal 5345 5369 inconsistent
capacity capaciteit 13,152 14 400 inconsistent
dateofbirth geboortedatum 2 February 1978 1978-1-2 inconsistent
elevation hoogte 300 228 inconsistent
Table 5: Irregular values in aligned attributes on matching pages
selecting highly similar tuples (i.e. with identical
template and attribute names) as positive data, and a
random tuple from the same page as negative data.
The accuracy of the classifier is 90.7%. Next, for
each potential pairing of template?attribute pairs
from two languages, random tuples are presented
to the classifier. If the ratio of positively clas-
sified tuples exceeds a certain threshold, the two
template?attribute pairs are assumed to express the
same relation. The accuracy of result varies, with
matchings of template?attribute pairs that are based
on the most frequent tuple matches having an accu-
racy score of 60%. They also evaluate their system
by determining how well the system is able to pre-
dict known tuples. Here, recall is 40% and precision
is 54%. The recall figure could be compared to the
18% tuples (for pages related by means of a cross-
language link) for which we find a match. If we
use only properly aligned template?attribute pairs,
however, coverage will certainly go down some-
what. Precision could be compared to our obser-
vation that we find 149K matching tuples, and, af-
ter alignment, predict an equivalence for 79K tuples
which in the data collecting do not have a matching
value. Thus, for 228K tuples we predict an equiva-
lent values, whereas this is only the case for 149K
tuples. We would not like to conclude from this,
however, that the precision of our method is 65%, as
we observed in section 5 that most of the conflict-
ing values are not inconsistencies, but more often
the consequence of formatting irregularities, transla-
tions, variation in specificity, etc. It is clear that the
system of Adar et al (2009) has a higher recall than
ours. This appears to be mainly due to the fact that
their feature based approach to determining match-
ing values considers much more data to be equiva-
lent than our approach which normalizes values and
then requires identity or a matching cross-language
link. In future work, we would like to explore more
rigorous normalization (taking the data discussed in
section 5 as starting point) and inclusion of features
to determine approximate matching to increase re-
call.
7 Conclusions
We have presented a method for automatically com-
pletingWikipedia templates which relies on the mul-
tilingual nature of Wikipedia and on the fact that
systematic links exist between pages in various lan-
guages. We have shown that matching template tu-
ples can be found automatically, and that an accu-
rate set of matching template?attribute pairs can be
derived from this by using intersective bidirectional
alignment. The method extends the number of tu-
ples by 51% (27% for existing Dutch pages).
In future work, we hope to include more lan-
guages, investigate the value of (automatic) transla-
tion for template and attribute alignment, investigate
alternative alignment methods (using more features
and other weighting scheme?s), and incorporate the
expanded data set in our QA-system for Dutch.
28
References
S.F. Adafre and M. de Rijke. 2006. Finding similar
sentences across multiple languages in wikipedia. In
Proceedings of the 11th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 62?69.
E. Adar, M. Skinner, and D.S. Weld. 2009. Informa-
tion arbitrage across multi-lingual Wikipedia. In Pro-
ceedings of the Second ACM International Conference
on Web Search and Data Mining, pages 94?103. ACM
New York, NY, USA.
So?ren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ives. 2008.
Dbpedia: A nucleus for a web of open data. The Se-
mantic Web, pages 722?735.
Gosse Bouma, Ismail Fahmi, Jori Mur, Gertjan van No-
ord, Lonneke van der Plas, and Jo?rg Tiedemann. 2006.
The University of Groningen at QA@CLEF 2006: Us-
ing syntactic knowledge for QA. In Working Notes for
the CLEF 2006 Workshop, Alicante.
Gosse Bouma, Jori Mur, Gertjan van Noord, Lonneke
van der Plas, and Jo?rg Tiedemann. 2008. Question
answering with Joost at QA@CLEF 2008. In Working
Notes for the CLEF 2008 Workshop, Aarhus.
M. Erdmann, K. Nakayama, T. Hara, and S. Nishio.
2008. An approach for extracting bilingual terminol-
ogy from wikipedia. Lecture Notes in Computer Sci-
ence, 4947:380.
B. Katz, G. Marton, G. Borchardt, A. Brownell,
S. Felshin, D. Loreto, J. Louis-Rosenberg, B. Lu,
F. Mora, S. Stiller, et al 2005. External knowledge
sources for question answering. In Proceedings of the
14th Annual Text REtrieval Conference (TREC?2005),
November.
M. Kro?tzsch, D. Vrandec?ic?, M. Vo?lkel, H. Haller, and
R. Studer. 2007. Semantic wikipedia. Web Semantics:
Science, Services and Agents on the World Wide Web.
L.V. Lita, W.A. Hunt, and E. Nyberg. 2004. Resource
analysis for question answering. In Association for
Computational Linguistics Conference (ACL).
Y. Ma, S. Ozdowska, Y. Sun, and A. Way. 2008. Improv-
ing word alignment using syntactic dependencies. In
Proceedings of the ACL-08: HLT Second Workshop on
Syntax and Structure in Statistical Translation (SSST-
2), pages 69?77.
D.P.T. Nguyen, Y. Matsuo, and M. Ishizuka. 2007. Rela-
tion extraction from wikipedia using subtree mining.
In PROCEEDINGS OF THE NATIONAL CONFER-
ENCE ON ARTIFICIAL INTELLIGENCE, page 1414.
AAAI Press.
Fei Wu and Daniel S. Weld. 2007. Autonomously se-
mantifying wikipedia. In CIKM ?07: Proceedings of
the sixteenth ACM conference on Conference on in-
formation and knowledge management, pages 41?50,
New York, NY, USA. ACM.
29
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1328?1336,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
On Learning Subtypes of the Part-Whole Relation: Do Not Mix your
Seeds
Ashwin Ittoo
University of Groningen
Groningen, The Netherlands
r.a.ittoo@rug.nl
Gosse Bouma
University of Groningen
Groningen, The Netherlands
g.bouma@rug.nl
Abstract
An important relation in information ex-
traction is the part-whole relation. On-
tological studies mention several types of
this relation. In this paper, we show
that the traditional practice of initializ-
ing minimally-supervised algorithms with
a single set that mixes seeds of different
types fails to capture the wide variety of
part-whole patterns and tuples. The re-
sults obtained with mixed seeds ultimately
converge to one of the part-whole relation
types. We also demonstrate that all the
different types of part-whole relations can
still be discovered, regardless of the type
characterized by the initializing seeds. We
performed our experiments with a state-of-
the-art information extraction algorithm.
1 Introduction
A fundamental semantic relation in many dis-
ciplines such as linguistics, cognitive science,
and conceptual modelling is the part-whole rela-
tion, which exists between parts and the wholes
they compise (Winston et al, 1987; Gerstl and
Pribbenow, 1995). Different types of part-whole
relations, classified in various taxonomies, are
mentioned in literature (Winston et al, 1987;
Odell, 1994; Gerstl and Pribbenow, 1995; Keet
and Artale, 2008). The taxonomy of Keet and Ar-
tale (2008), for instance, distinguishes part-whole
relations based on their transitivity, and on the
semantic classes of entities they sub-categorize.
Part-whole relations are also crucial for many in-
formation extraction (IE) tasks (Girju et al, 2006).
Annotated corpora and semantic dictionaries used
in IE, such as the ACE corpus1 and WordNet (Fell-
baum, 1998), include examples of part-whole re-
lations. Also, previous relation extraction work,
1http://projects.ldc.upenn.edu/ace/
such as Berland and Charniak (1999) and Girju et
al. (2006), have specifically targeted the discovery
of part-whole relations from text. Furthermore,
part-whole relations are de-facto benchmarks for
evaluating the performance of general relation ex-
traction systems (Pantel and Pennacchiotti, 2006;
Beamer et al, 2008; Pyysalo et al, 2009). How-
ever, these relation extraction efforts have over-
looked the ontological distinctions between the
different types of part-whole relations. They as-
sume the existence of a single relation, subsuming
the different part-whole relation types.
In this paper, we show that enforcing the onto-
logical distinctions between the different types of
part-whole relations enable information extraction
systems to capture a wider variety of both generic
and specialised part-whole lexico-syntactic pat-
terns and tuples. Specifically, we address 3 major
questions.
1. Is information extraction (IE) harder when
learning the individual types of part-whole
relations? That is, we determine whether the
performance of state-of-the-art IE systems in
learning the individual part-whole relation
types increases (due to more coherency in
the relations? linguistic realizations) or drops
(due to fewer examples), compared to the tra-
ditional practice of considering a single part-
whole relation.
2. Are the patterns and tuples discovered when
focusing on a specific part-whole relation
type confined to that particular type? That
is, we investgate whether IE systems discover
examples representative of the different types
by targetting one particular part-whole rela-
tion type.
3. Are more distinct examples discovered when
IE systems learn the individual part-whole re-
lation types? That is, we determine whether
1328
a wider variety of unique patterns and tuples
are extracted when IE systems target the dif-
ferent types of part-whole relations instead of
considering a single part-whole relation that
subsumes all the different types.
To answer these questions, we bootstrapped
a minimally-supervised relation extraction algo-
rithm, based on Espresso (Pantel and Pennac-
chiotti, 2006), with different seed-sets for the vari-
ous types of part-whole relations, and analyzed the
harvested tuples and patterns.
2 Previous Work
Investigations on the part-whole relations span
across many disciplines, such as conceptual mod-
eling (Artale et al, 1996; Keet, 2006; Keet and
Artale, 2008), which focus on the ontological
aspects, and linguistics and cognitive sciences,
which focus on natural language semantics. Sev-
eral linguistically-motivated taxonomies (Odell,
1994; Gerstl and Pribbenow, 1995), based on the
work of Winston et al (1987), have been proposed
to clarify the semantics of the different part-whole
relations types across these various disciplines.
Keet and Artale (2008) developed a formal taxon-
omy, distinguishing transitive mereological part-
whole relations from intransitive meronymic ones.
Meronymic relations identified are: 1) member-
of, between a physical object (or role) and an ag-
gregation, e.g. player-team, 2) constituted-of, be-
tween a physical object and an amount of mat-
ter e.g. clay-statue, 3) sub-quantity-of, between
amounts of matter or units, e.g. oxygen-water
or m-km, and 4)participates-in, between an entity
and a process e.g. enzyme-reaction. Mereologi-
cal relations are: 1)involved-in, between a phase
and a process, e.g. chewing-eating, 2) located-
in, between an entity and its 2-dimensional re-
gion, e.g. city-region, 3)contained-in, between
an entity and its 3-dimensional region, e.g.tool-
trunk, and 4)structural part-of, between integrals
and their (functional) components, e.g. engine-
car. This taxonomy further discriminates between
part-whole relation types by enforcing semantical
selectional restrictions, in the form of DOLCE on-
tology (Gangemi et al, 2002) classes, on their en-
tities.
In NLP, information extraction (IE) techniques,
for discovering part-whole relations from text have
also been developed. Berland and Charniak (1999)
use manually-crafted patterns, similar to Hearst
(1992), and on initial ?seeds? denoting ?whole?
objects (e.g. building) to harvest possible ?part?
objects (e.g. room) from the North Americal
News Corpus (NANC) of 1 million words. They
rank their results with measures like log-likelihood
(Dunning, 1993), and report a maximum accuracy
of 70% over their top-20 results. In the super-
vised approaches in Girju et al (2003) and Girju
et al (2006), lexical patterns expressing part-
whole relations between WordNet concept pairs
are manually extracted from 20,000 sentences of
the L.A Times and SemCor corpora (Miller et
al., 1993), and used to generate a training cor-
pus, with manually-annotated positive and nega-
tive examples of part-whole relations. Classifica-
tion rules, induced over the training data, achieve
a precision of 80.95% and recall of 75.91% in pre-
dicting whether an unseen pattern encode a part-
whole relation. Van Hage et al (2006) acquire
503 part-whole pairs from dedicated thesauri (e.g.
AGROVOC2) to learn 91 reliable part-whole pat-
terns. They substituted the patterns? ?part? ar-
guments with known entities to formulate web-
search queries. Corresponding ?whole? entities
were then discovered from documents in the query
results with a precision of 74%. The part-whole
relation is also a benchmark to evaluate the perfor-
mance of general information extraction systems.
The Espresso algorithm (Pantel and Pennacchiotti,
2006) achieves a precision of 80% in learning part-
whole relations from the Acquaint (TREC-9) cor-
pus of nearly 6M words. Despite the reasonable
performance of the above IE systems in discov-
ering part-whole relations, they overlook the on-
tological distinctions between the different rela-
tion types. For example, Girju et al (2003) and
Girju et al (2006) assume a single part-whole re-
lation, encompassing all the different types men-
tioned in the taxonomy of Winston et al (1987).
Similarly, the minimally-supervised Espresso al-
gorithm (Pantel and Pennacchiotti, 2006) is ini-
tialized with a single set that mixes seeds of
heterogeneous types, such as leader-panel and
oxygen-water, which respectively correspond to
the member-of and sub-quantity-of relations in the
taxonomy of Keet and Artale (2008).
2http://aims.fao.org/website/
AGROVOC-Thesaurus/sub
1329
3 Methodology
Our aim is to compare the relations harvested
when a minimally-supervised IE algorithm is ini-
tialized with separate sets of seeds for each type of
part-whole relation, and when it is initialized fol-
lowing the traditional practice of a single set that
mixes seeds of the different types. To distinguish
between types of part-whole relations, we commit
to the taxonomy of Keet and Artale (2008) (Keet?s
taxonomy), which uses sound ontological for-
malisms to unambiguously discrimate the relation
types. Also, this taxonomy classifies the various
part-whole relations introduced in literature, in-
cluding ontologically-motivated mereological re-
lations and linguistically-motivated meronymic
ones. We adopt a 3-step approach to address our
questions from section 1.
1. Define prototypical seeds (part-whole tuples)
as follows:
? (Separate) sets of seeds for each type of
part-whole relation in Keet?s taxonomy.
? A single set that mixes seeds denot-
ing all the different part-whole relations
types.
2. Part-whole relations extraction from a corpus
by initializing a minimally-supervised IE al-
gorithm with the seed-sets
3. Evaluation of the harvested relations to de-
termine performance gain/loss, types of part-
whole relations extracted, and distinct and
unique patterns and tuples discovered.
The corpora and IE algorithm we used, and the
seed-sets construction are described below. Re-
sults are presented in the next section.
3.1 Corpora
We used the English and Dutch Wikipedia texts
since their broad-coverage and size ensures that
they include sufficient lexical realizations of the
different types of part-whole relations. Wikipedia
has also been targeted by recent IE efforts (Nguyen
et al, 2007; Wu and Weld, 2007). However, while
they exploited the structured features (e.g. in-
foboxes), we only consider the unstructured texts.
The English corpus size is approximately 470M
words (? 80% of the August 2007 dump), while
for Dutch, we use the full text collection (Febru-
ary 2008 dump) of approximately 110M words.
We parsed the English and Dutch corpora respec-
tively with the Stanford3 (Klein and Manning,
2003) and the Alpino4 (van Noord, 2006) parsers,
and formalized the relations between terms (enti-
ties) as dependency paths. A dependency path is
the shortest path of lexico-syntactic elements, i.e.
shortest lexico-syntactic pattern, connecting enti-
ties (proper and common nouns) in their parse-
trees. Such a formalization has been successfully
employed in previous IE tasks (see Stevenson and
Greenwood (2009) for an overview). Compared
to traditional surface-pattern representations, used
by Pantel and Pennacchiotti (2006), dependency
paths abstract from surface texts to capture long
range dependencies between terms. They also al-
leviate the manual authoring of large numbers of
surface patterns. In our formalization, we substi-
tute entities in the dependency paths with generic
placeholders PART and WHOLE. Below, we show
two dependency paths (1-b) and (2-b), respectively
derived from English and Dutch Wikipedia sen-
tences (1-a) and (2-a), and denoting the relations
between sample-song, and alkalo??de-plant.
(1) a. The song ?Mao Tse Tung Said? by
Alabama 3 contains samples of a
speech by Jim Jones
b. WHOLE+nsubj? contains? dobj+PART
(2) a. Alle delen van de planten bevatten al-
kalo??den en zijn daarmee giftig (All
parts of the plants contain alkaloids
and therefore are poisonous)
b. WHOLE+obj1+van+mod+deel+su ?
bevat? obj1+PART
In our experiments, we only consider those en-
tity pairs (tuples), patterns, and co-occuring pairs-
patterns with a minimum frequency of 10 in the
English corpus, and 5 in the Dutch corpus. Statis-
tics on the number of tuples and patterns preserved
after applying the frequency cut-off are given in
Table 1.
3.2 Information Extraction Algorithm
As IE algorithm for extracting part-whole rela-
tions from our texts, we relied on Espresso, a
minimally-supervised algorithm, as described by
Pantel and Pennacchiotti (2006). They show
3http://nlp.stanford.edu/software/
lex-parser.shtml
4http://www.let.rug.nl/?vannoord/alp/
Alpino
1330
English Dutch
words 470.0 110.0
pairs 328.0 28.8
unique pairs 6.7 1.4
patterns 238.0 54.0
unique patterns 2.0 0.9
Table 1: Corpus Statistics in millions
that the algorithm achieves state-of-the-art perfor-
mance when initialized with relatively small seed-
sets over the Acquaint corpus (? 6M words). Re-
call is improved with web search queries as addi-
tional source of information.
Espresso extracts surface patterns connecting
the seeds (tuples) in a corpus. The reliability of
a pattern p, r(p), given a set of input tuples I , is
computed using (3), as its average strength of as-
sociation with each tuple,i, weighted by each tu-
ple?s reliability, r?(i).
(3) rpi(p) =
?
i?I
(
pmi(i,p)
maxpmi
?r?(i)
)
|I|
In this equation, pmi(i, p) is the pointwise mutual
information score (Church and Hanks, 1990) be-
tween a pattern, p (e.g. consist-of), and a tuple,
i (e.g. engine-car), and maxpmi is the maximum
PMI score between all patterns and tuples. The re-
liability of the initializing seeds is set to 1.
The top-k most reliable patterns are selected to
find new tuples. The reliability of each tuple i,
r?(i) is computed according to (4), where P is the
set of harvested patterns. The top-m most reliable
tuples are used to infer new patterns.
(4) r?(i) =
?
i?I
(
pmi(i,p)
maxpmi
?rpi(p)
)
|P |
The recursive discovery of patterns from tuples
and vice-versa is repeated until a threshold num-
ber of patterns and/or tuples have been extracted.
In our implementation, we maintain the core of the
original Espresso algorithm, which pertains to es-
timating the reliability of patterns and tuples.
Pantel and Pennacchiotti (2006) mention that
their method is independent of the way patterns
are formulated. Thus, instead of relying on surface
patterns, we use dependency paths (as described
above). Another difference is that while Pantel and
Pennacchiotti (2006) complement their small cor-
pus with documents retrieved from the web, we
only rely on patterns extracted from our (much
larger) corpora. Finally, we did not apply the dis-
counting factor suggested in Pantel and Pennac-
chiotti (2006) to correct for the fact that PMI over-
estimates the importance of low-frequency events.
Instead, as explained above, we applied a general
frequency cut-off.5
3.3 Seed Selection
Initially,we selected seeds from WordNet (Fell-
baum, 1998) (for English) and EuroWordNet
(Vossen, 1998) (for Dutch) to initialize the IE al-
gorithm. However, we found that these pairs,
such as acinos-mother of thyme or radarscherm-
radarapparatuur (radar screen - radar equipment,
hardly co-occured with reasonable frequency in
Wikipedia sentences, hindering pattern extraction.
We therefore adopted the following strategy.
We searched our corpora for archetypal pat-
terns, e.g. contain , which characterize all the dif-
ferent types of part-whole relations. The tuples
sub-categorized by these patterns in the English
texts were automatically6 typed to appropriate
DOLCE ontology7 classes, corresponding to those
employed by Keet and Artale for constraining the
entity pairs participating in different types of part-
whole relations. The types of part-whole relations
instantiated by the tuples could then be determined
based on their ontological classes. Separate sets of
20 tuples, with each set corresponding to a specific
relation type in the taxonomy of Keet and Artale
(Keet?s taxonomy), were then created. For exam-
ple, the English Wikipedia tuple t1 =actor-cast
was used as a seed to discover member-of part-
whole relations since both its elements were typed
to the SOCIAL OBJECT class of the DOLCE ontol-
ogy, and according to Keet?s taxonomy, they in-
stantiate a member-of relation. Seeds for extract-
ing relations from the Dutch corpus were defined
in a similar way, except that we manually deter-
mined their ontological classes based on the class
glossary of DOLCE.
Below, we only report on the member-of and
sub-quantity-of meronymic relations, and on the
located-in, contained-in and structural part-of
mereological relations. We were unable to find
sufficient seeds for the constituted-of meronymic
5We experimented with the suggested discounting factor
for PMI, but were not able to improve over the accuracy scores
reported later.
6Using the Java-OWL API, from http://protege.
stanford.edu/plugins/owl/api/
7OWL Version 0.72, downloaded from http://www.
loa-cnr.it/DOLCE.html/
1331
Lg Part Whole # Type
EN grave church 155 contain
NL beeld kerk 120 contain
(statue) (church)
EN city region 3735 located
NL abdij gemeente 36 located
(abbey) (community)
EN actor cast 432 member
NL club voetbal bond 178 member
(club) (soccer union)
EN engine car 3509 structural
NL geheugen computer 14 structural
(memory) (computer)
EN alcohol wine 260 subquant
NL alcohol bier 28 subquant
(alcohol) (beer)
Table 2: Seeds used for learning part-whole rela-
tions (contained-in, located-in, member-of, struc-
tural part-of, sub-quantity-of).
relations (e.g. clay-statue). Also, we did not ex-
periment with the participates-in and involved-in
relations since their lexical realizations in our cor-
pora are sparse, and they contain at least one ver-
bal argument, whereas we only targeted patterns
connecting nomimals. Sample seeds, their corpus
frequency, and the part-whole relation type they
instantiate from the English (EN) and Dutch (NL)
corpora are illustrated in Table 2. Besides the
five specialized seed-sets of 20 prototypical tuples
for the aforementioned relations, we also defined
a general set of mixed seeds, which combines four
seeds from each of the specialized sets.
4 Experiments and Evaluation
We initialized our IE algorithm with the seed-sets
to extract part-whole relations from our corpora.
The same parameters as Pantel and Pennacchiotti
(2006) were used. That is, the 10 most reliable
patterns inferred from the initial seeds are boot-
strapped to induce 100 part-whole tuples. In each
subsequent iteration, we learn one additional pat-
tern and 100 additional tuples. We evaluated our
results after 5 iterations since the performance in
later iterations was almost constant. The results
are discussed next.
meronomic mereological
memb subq cont struc locat gen
EN 0.67 0.74 0.70 0.82 0.75 0.80
NL 0.68 0.60 0.60 0.60 0.70 0.71
Table 3: Precision for seed-sets representing spe-
cific types of part-whole relations (member-of,
sub-quantity-of, contained-in, structural part-of
and located-in), and for the general set composed
of all types.
4.1 Precision of Extracted Relations
Two human judges manually evaluated the tuples
extracted from the English and Dutch corpora per
seed-set in each iteration of our algorithm. Tuples
that unambiguously instantiated part-whole rela-
tions were considered true positives. Those that
did not were considered false positives. Ambigu-
ous tuples were discarded. The precision of the
tuples discovered by the different seed-sets in the
last iteration of our algorithm are in Table 3.
These results reveal that the precision of har-
vested tuples varies depending on the part-whole
relation type that the initializing seeds denote.
Mereological seeds (cont, struct, locat sets) out-
performed their meronymic counterparts (memb,
subq) in extracting relations with higher precision
from the English texts. This could be attributed to
their formal ontological grounding, making them
less ambiguous than the linguistically-motivated
meronymic relations (Keet, 2006; Keet and Ar-
tale, 2008). The precision variations were less dis-
cernible for tuples extracted from the Dutch cor-
pus, although the best precision was still achieved
with mereological located-in seeds. We also no-
ticed that the precision of tuples extracted from
both the English and Dutch corpora by the gen-
eral set of mixed seeds was as high as the max-
imum precision obtained by the individual sets
of specialized seeds over these two corpora, i.e.
0.80 (general seeds) vs. 0.82 (structural part-
of seeds) for English, and 0.71 (general seeds)
vs. 0.70 (located-in seeds) for Dutch. Based
on these findings, we address our first question,
and conclude that 1) the type of relation instan-
tiated by the initializing seeds affects the perfor-
mance of IE algorithms, with mereological seeds
being in general more fertile than their meronymic
counterparts, and generating higher-precision tu-
ples; 2) the precision achieved when initializing
IE algorithms with a general set, which mixes
1332
seeds of heterogeneous part-whole relation types,
is comparable to the best results obtained with in-
dividual sets of specialized seeds, denoting spe-
cific part-whole relations. An evaluation of the
patterns and tuples extracted indicated consider-
able precision drop between successive iterations
of our algorithm. This appears to be due to se-
mantic drift (McIntosh and Curran, 2009), where
highly-ambiguous patterns promote incorrect tu-
ples , which in turn, compound the precision loss.
4.2 Types of Extracted Relations
Initializing our algorithm with seeds of a particular
type always led to the discovery of tuples charac-
terizing other types of part-whole relations in the
English corpus. This can be explained by proto-
typical patterns, e.g. ?include?, generated regard-
less of the seeds? types, and which are highy cor-
related with, and hence, trigger tuples denoting
other part-whole relation types. An almost sim-
ilar observation was made for the Dutch corpus,
except that tuples instantiating the member-of re-
lation could only be learnt using initial seeds of
that particular type (i.e. member-of). Upon in-
specting our results, it was found that this phe-
nomenon was due to the distinct and specific pat-
terns, such as ?treedt toe tot? (?become member
of?), which linguistically realize the member-of re-
lations in the Dutch corpus. Thus, initializing our
IE algorithm with seeds that instantiate relations
other than member-of fails to detect these unique
patterns, and fails to subsequently discover part-
whole tuples describing the member-of relations.
Our findings are illustrated in Table 4, where each
cell lists a tuple of a particular type (column),
which was harvested from seeds of a given type
(row). These results answer our second question.
4.3 Distinct Patterns and Tuples
We address our third question by comparing the
output of our algorithm to determine whether the
results obtained by initializing with the individual
specialized seeds were (dis)similar and/or distinct.
Each result set consisted of maximally 520 tuples
(including 20 initializing seeds) and 15 lexico-
syntactic patterns, obtained after five iterations.
Tuples extracted from the English corpus using
the member-of and contained-in seed-sets exhib-
ited a high degree of similarity, with 465 com-
mon tuples discovered by both sets. These iden-
tical tuples were also assigned the same ranks (re-
liability) in the results generated by the member-
of and contained-in seeds, with a Spearman rank
correlation of 0.82 between their respective out-
puts. This convergence was also reflected in
the fact that the member-of and contained-in
seeds generated around 80% of common pat-
terns. These patterns were mostly prototypi-
cal ones indicative of part-whole relations, such
as WHOLE+nsubj? include? dobj+PART (?in-
clude?) and their cognates involving passive forms
and relative clauses. However, the specialized
seeds also generated distinct patterns, like ?joined
as? and ?released with? for the member-of and
contained-in seeds respectively.
The most distinct tuples and patterns were har-
vested with the sub-quantity-of, structural part-of,
and located-in seeds. Negative Spearman corre-
lation scores were obtained when comparing the
results of these three sets among themselves, and
with the results of the member-of and contained-
in seeds, indicating insignificant similarity and
overlap. Examining the patterns harvested by the
sub-quantity-of, structural part-of, and located-in
seeds revealed a high prominence of specialized
and unique patterns, which specifically character-
ize these relations. Examples of such patterns in-
clude ?made with?, ?released with? and ?found
in?, which lexically realize the sub-quantity-of,
structural part-of, and located-in relations respec-
tively.
For the Dutch corpus, the seeds that generated
the most similar tuples were those correspond-
ing to the sub-quantity-of, contained-in, and struc-
tural part-of relations, with 490 common tuples
discovered, and a Spearman rank correlation in the
range of 0.89-0.93 between their respective out-
puts. As expected, these seeds also led to the dis-
covery of a substantial number of common and
prototypical part-whole patterns. Examples in-
clude ?bevat? (?contain?), ?omvat? (?comprise?),
and their variants. The most distinct results were
harvested by the located-in and member-of seeds,
with negative Spearman correlation scores be-
tween the output tuples indicating hardly any over-
lap. We also found out that the patterns harvested
by the located-in and member-of seeds character-
istically pertained to these relations. Example of
such patterns include ?ligt in? (?lie in?), ?is gele-
gen in? (?is located in?), and ?treedt toe tot? (?be-
come member of?), respectively describing the
located-in and member-of relations.
Thus, we observed that 1) tuples harvested from
1333
meronomic mereological
Tuples? member subquant contained struct located
Seeds?
EN member ship-convoy alcohol-wine card-deck proton-nucleus lake-park
subquant aircraft-fleet moisture-soil building-complex engine-car commune-canton
contained aircraft-fleet alohol-wine relic-church base-spacecraft campus-city
structural brother-family mineral-bone library-building inlay-fingerboard hamlet-town
located performer-cast alcohol-blood artifact-museum chassis-car city-shore
NL member sporter-ploeg helium-atmosfeer stalagmieten-grot shirt-tenue boerderij-dorp
(athlete-team) (helium-atmosphere) (stalagnites-cave) (shirt-outfit) (farm-village)
subquant ? vet-kaas pijp orgel-kerk kam-gitaar paleis-stad
(fat-cheese) (pipe-organ-church) (bridge-guitar) (palace-city)
contained ? tannine-wijn kamer-toren atoom-molecule paleis-stad
(tannine-wine) (room-tower) (atom-molecule) (palace-city)
structural ? kinine-tonic beeld-kerk wervel-ruggengraat paleis-stad
(quinine-tonic) statue-church) (vertebra-backbone) (palace-city)
located ? ? kunst werk-kathedraal poort-muur metro station-wijk
(work of art-cathedral) (gate-wall) (metro station-quarter)
Table 4: Sample tuples found per relation type.
both the English and Dutch corpora by seeds in-
stantiating a single particular type of part-whole
relation highly correlated with tuples discovered
by at least one other type of seeds (member-of
and contained-in for English, and sub-quantity-
of, contained-in and structural part-of for Dutch);
2) some part-whole relations are manifested by a
wide variety of specialized patterns (sub-quantity-
of, structural part-of, and located-in for English,
and located-in and member-of for Dutch).
Finally, instead of a single set that mixes seeds
of different types, we created five such general
sets by picking four different seeds from each of
the specialized sets, and used them to initialize our
algorithm. When examining the results of each of
the five general sets, we found out that they were
unstable, and always correlated with the output of
a different specialized set.
Based on these findings, we believe that the tra-
ditional practice of initializing IE algorithms with
general sets that mix seeds denoting different part-
whole relation types leads to inherently unstable
results. As we have shown, the relations extracted
by combining seeds of heterogeneous types almost
always converge to one specific part-whole rela-
tion type, which cannot be conclusively predicted.
Furthermore, general seeds are unable to capture
the specific and distinct patterns that lexically re-
alize the individual types of part-whole relations.
5 Conclusions
In this paper, we have investigated the effect of
ontologically-motivated distinctions in part-whole
relations on IE systems that learn instances of
these relations from text.
We have shown that learning from specialized
seeds-sets, denoting specific types of the part-
whole relations, results in precision that is as high
as or higher than the precision achieved with a
general set that mixes seeds of different types.
By comparing the outputs generated by different
seed-sets, we observed that the tuples learnt with
seeds denoting a specific part-whole relation type
are not confined to that particular type. In most
case, we are still able to discover tuples across
all the different types of part-whole relations, re-
gardless of the type instantiated by the initializing
seeds. Most importantly, we demonstrated that IE
algorithms initialized with general sets of mixed
seeds harvest results that tend to converge towards
a specific type of part-whole relation. Conversely,
when starting with seeds representing a specific
type, it is likely to discover tuples and patterns
that are completely distinct from those found by
a mixed seed-set.
Our results also illustrate that the outputs of IE
algorithms are heavily influenced by the initializ-
ing seeds, concurring with the findings of McIn-
tosh and Curran (2009). We believe that our re-
sults show a drastic form of this phenomenon:
given a set of mixed seeds, denoting heteroge-
neous relations, the harvested tuples may converge
towards any of the relations instantiated by the
seeds. Predicting the convergent relation is in
usual cases impossible, and may depend on factors
pertaining to corpus characteristics. This instabil-
ity strongly suggests that seeds instantiating differ-
ent types of relations should not be mixed, partic-
1334
ularly when learning part-whole relations, which
are characterized by many subtypes. Seeds should
be defined such that they represent an ontologi-
cally well-defined class, for which one may hope
to find a coherent set of extraction patterns.
Acknowledgement
Ashwin Ittoo is part of the project ?Merging of In-
coherent Field Feedback Data into Prioritized De-
sign Information (DataFusion)? (http://www.
iopdatafusion.org//), sponsored by the
Dutch Ministry of Economic Affairs under the
IOP-IPCR program.
Gosse Bouma acknowledges support from the
Stevin LASSY project (www.let.rug.nl/
?vannoord/Lassy/).
References
A. Artale, E. Franconi, N. Guarino, and L. Pazzi.
1996. Part-whole relations in object-centered sys-
tems: An overview. Data & Knowledge Engineer-
ing, 20(3):347?383.
B. Beamer, A. Rozovskaya, and R. Girju. 2008. Au-
tomatic semantic relation extraction with multiple
boundary generation. In Proceedings of the 23rd na-
tional conference on Artificial intelligence-Volume
2, pages 824?829. AAAI Press.
Matthew Berland and Eugene Charniak. 1999. Find-
ing parts in very large corpora. In Proceedings of the
37th annual meeting of the Association for Compu-
tational Linguistics on Computational Linguistics,
pages 57?64, Morristown, NJ, USA. Association for
Computational Linguistics.
K.W. Church and P. Hanks. 1990. Word association
norms, mutual information, and lexicography. Com-
putational linguistics, 16(1):22?29.
T. Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational linguis-
tics, 19(1):74.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT, Cambridge.
A. Gangemi, N. Guarino, C. Masolo, A. Oltramari, and
L. Schneider. 2002. Sweetening ontologies with
DOLCE. Knowledge Engineering and Knowledge
Management: Ontologies and the Semantic Web,
Lecture Notes in Computer Science, pages 223?233.
P. Gerstl and S. Pribbenow. 1995. Midwinters, end
games, and body parts: a classification of part-whole
relations. International Journal of Human Com-
puter Studies, 43:865?890.
R. Girju, A. Badulescu, and D. Moldovan. 2003.
Learning semantic constraints for the automatic dis-
covery of part-whole relations. In Proceedings of
HLT/NAACL, volume 3, pages 80?87.
R. Girju, A. Badulescu, and D. Moldovan. 2006. Au-
tomatic discovery of part-whole relations. Compu-
tational Linguistics, 32(1):83?135.
M.A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 14th conference on Computational linguistics-
Volume 2, pages 539?545. Association for Compu-
tational Linguistics Morristown, NJ, USA.
C.M. Keet and A. Artale. 2008. Representing and
reasoning over a taxonomy of part?whole relations.
Applied Ontology, 3(1):91?110.
C.M. Keet. 2006. Part-whole relations in object-
role models. On the Move to Meaningful Internet
Systems 2006, Lecture Notes in Computer Science,
4278:1118?1127.
D. Klein and C.D. Manning. 2003. Accurate un-
lexicalized parsing. In Proceedings of the 41st
Annual Meeting on Association for Computational
Linguistics-Volume 1, pages 423?430. Associa-
tion for Computational Linguistics Morristown, NJ,
USA.
T. McIntosh and J.R. Curran. 2009. Reducing seman-
tic drift with bagging and distributional similarity.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Confe rence on Natural Language Processing
of the AFNLP, pages 396?404.
G.A. Miller, C. Leacock, R. Tengi, and R.T. Bunker.
1993. A semantic concordance. In Proceedings
of the 3rd DARPA workshop on Human Language
Technology, pages 303?308. New Jersey.
D.P.T. Nguyen, Y. Matsuo, and M. Ishizuka. 2007. Re-
lation extraction from wikipedia using subtree min-
ing. In Proceedings of the National Conference on
Artificial Intelligence, volume 22, page 1414. Menlo
Park, CA; Cambridge, MA; London; AAAI Press;
MIT Press; 1999.
J. Odell. 1994. Six different kinds of composition.
Journal of Object-Oriented Programming, 5(8):10?
15.
Patrick Pantel and Marco Pennacchiotti. 2006.
Espresso: Leveraging generic patterns for auto-
matically harvesting semantic relations. In Pro-
ceedings of Conference on Computational Linguis-
tics / Association for Computational Linguistics
(COLING/ACL-06), pages 113?120, Sydney, Aus-
tralia.
S. Pyysalo, T. Ohta, J.D. Kim, and J. Tsujii. 2009.
Static relations: a piece in the biomedical informa-
tion extraction puzzle. In Proceedings of the Work-
shop on BioNLP, pages 1?9. Association for Com-
putational Linguistics.
1335
Mark Stevenson and Mark Greenwood. 2009. De-
pendency pattern models for information extraction.
Research on Language and Computation, 3:13?39.
W.R. Van Hage, H. Kolb, and G. Schreiber. 2006. A
method for learning part-whole relations. The Se-
mantic Web - ISWC 2006, Lecture Notes in Com-
puter Science, 4273:723?735.
Gertjan van Noord. 2006. At last parsing is now oper-
ational. In Piet Mertens, Cedrick Fairon, Anne Dis-
ter, and Patrick Watrin, editors, TALN06. Verbum Ex
Machina. Actes de la 13e conference sur le traite-
ment automatique des langues naturelles, pages 20?
42. Presses univ. de Louvain.
P. Vossen, editor. 1998. EuroWordNet A Multilingual
Database with Lexical Semantic Networks. Kluwer
Academic publishers.
M.E. Winston, R. Chaffin, and D. Herrmann. 1987.
A taxonomy of part-whole relations. Cognitive sci-
ence, 11(4):417?444.
F. Wu and D.S. Weld. 2007. Autonomously seman-
tifying wikipedia. In Proceedings of the sixteenth
ACM conference on Conference on information and
knowledge management, pages 41?50. ACM.
1336
