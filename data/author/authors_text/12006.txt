Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 880?889,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Polylingual Topic Models
David Mimno Hanna M. Wallach Jason Naradowsky
University of Massachusetts, Amherst
Amherst, MA 01003
{mimno, wallach, narad, dasmith, mccallum}cs.umass.edu
David A. Smith Andrew McCallum
Abstract
Topic models are a useful tool for analyz-
ing large text collections, but have previ-
ously been applied in only monolingual,
or at most bilingual, contexts. Mean-
while, massive collections of interlinked
documents in dozens of languages, such
as Wikipedia, are now widely available,
calling for tools that can characterize con-
tent in many languages. We introduce a
polylingual topic model that discovers top-
ics aligned across multiple languages. We
explore the model?s characteristics using
two large corpora, each with over ten dif-
ferent languages, and demonstrate its use-
fulness in supporting machine translation
and tracking topic trends across languages.
1 Introduction
Statistical topic models have emerged as an in-
creasingly useful analysis tool for large text col-
lections. Topic models have been used for analyz-
ing topic trends in research literature (Mann et al,
2006; Hall et al, 2008), inferring captions for im-
ages (Blei and Jordan, 2003), social network anal-
ysis in email (McCallum et al, 2005), and expand-
ing queries with topically related words in infor-
mation retrieval (Wei and Croft, 2006). Much of
this work, however, has occurred in monolingual
contexts. In an increasingly connected world, the
ability to access documents in many languages has
become both a strategic asset and a personally en-
riching experience. In this paper, we present the
polylingual topic model (PLTM). We demonstrate
its utility and explore its characteristics using two
polylingual corpora: proceedings of the European
parliament (in eleven languages) and a collection
of Wikipedia articles (in twelve languages).
There are many potential applications for
polylingual topic models. Although research liter-
ature is typically written in English, bibliographic
databases often contain substantial quantities of
work in other languages. To perform topic-based
bibliometric analysis on these collections, it is
necessary to have topic models that are aligned
across languages. Such analysis could be sig-
nificant in tracking international research trends,
where language barriers slow the transfer of ideas.
Previous work on bilingual topic modeling
has focused on machine translation applications,
which rely on sentence-aligned parallel transla-
tions. However, the growth of the internet, and
in particular Wikipedia, has made vast corpora
of topically comparable texts?documents that are
topically similar but are not direct translations of
one another?considerably more abundant than
ever before. We argue that topic modeling is
both a useful and appropriate tool for leveraging
correspondences between semantically compara-
ble documents in multiple different languages.
In this paper, we use two polylingual corpora
to answer various critical questions related to
polylingual topic models. We employ a set of di-
rect translations, the EuroParl corpus, to evaluate
whether PLTM can accurately infer topics when
documents genuinely contain the same content.
We also explore how the characteristics of dif-
ferent languages affect topic model performance.
The second corpus, Wikipedia articles in twelve
languages, contains sets of documents that are not
translations of one another, but are very likely to
be about similar concepts. We use this corpus
to explore the ability of the model both to infer
similarities between vocabularies in different lan-
guages, and to detect differences in topic emphasis
between languages. The internet makes it possible
for people all over the world to access documents
from different cultures, but readers will not be flu-
ent in this wide variety of languages. By linking
topics across languages, polylingual topic mod-
els can increase cross-cultural understanding by
providing readers with the ability to characterize
880
the contents of collections in unfamiliar languages
and identify trends in topic prevalence.
2 Related Work
Bilingual topic models for parallel texts with
word-to-word alignments have been studied pre-
viously using the HM-bitam model (Zhao and
Xing, 2007). Tam, Lane and Schultz (Tam et
al., 2007) also show improvements in machine
translation using bilingual topic models. Both
of these translation-focused topic models infer
word-to-word alignments as part of their inference
procedures, which would become exponentially
more complex if additional languages were added.
We take a simpler approach that is more suit-
able for topically similar document tuples (where
documents are not direct translations of one an-
other) in more than two languages. A recent ex-
tended abstract, developed concurrently by Ni et
al. (Ni et al, 2009), discusses a multilingual topic
model similar to the one presented here. How-
ever, they evaluate their model on only two lan-
guages (English and Chinese), and do not use the
model to detect differences between languages.
They also provide little analysis of the differ-
ences between polylingual and single-language
topic models. Outside of the field of topic mod-
eling, Kawaba et al (Kawaba et al, 2008) use
a Wikipedia-based model to perform sentiment
analysis of blog posts. They find, for example,
that English blog posts about the Nintendo Wii of-
ten relate to a hack, which cannot be mentioned in
Japanese posts due to Japanese intellectual prop-
erty law. Similarly, posts about whaling often
use (positive) nationalist language in Japanese and
(negative) environmentalist language in English.
3 Polylingual Topic Model
The polylingual topic model (PLTM) is an exten-
sion of latent Dirichlet alocation (LDA) (Blei et
al., 2003) for modeling polylingual document tu-
ples. Each tuple is a set of documents that are
loosely equivalent to each other, but written in dif-
ferent languages, e.g., corresponding Wikipedia
articles in French, English and German. PLTM as-
sumes that the documents in a tuple share the same
tuple-specific distribution over topics. This is un-
like LDA, in which each document is assumed to
have its own document-specific distribution over
topics. Additionally, PLTM assumes that each
?topic? consists of a set of discrete distributions
D
N
1
T
N
L
...
w
? ?
wz
z
...
?
1
?
L
?
1
?
L
Figure 1: Graphical model for PLTM.
over words?one for each language l = 1, . . . , L.
In other words, rather than using a single set of
topics ? = {?
1
, . . . ,?
T
}, as in LDA, there are L
sets of language-specific topics, ?
1
, . . . ,?
L
, each
of which is drawn from a language-specific sym-
metric Dirichlet with concentration parameter ?
l
.
3.1 Generative Process
A new document tuplew = (w
1
, . . . ,w
L
) is gen-
erated by first drawing a tuple-specific topic dis-
tribution from an asymmetric Dirichlet prior with
concentration parameter ? and base measurem:
? ? Dir (?, ?m). (1)
Then, for each language l, a latent topic assign-
ment is drawn for each token in that language:
z
l
? P (z
l
|?) =
?
n
?
z
l
n
. (2)
Finally, the observed tokens are themselves drawn
using the language-specific topic parameters:
w
l
? P (w
l
| z
l
,?
l
) =
?
n
?
l
w
l
n
|z
l
n
. (3)
The graphical model is shown in figure 1.
3.2 Inference
Given a corpus of training and test document
tuples?W and W
?
, respectively?two possible
inference tasks of interest are: computing the
probability of the test tuples given the training
tuples and inferring latent topic assignments for
test documents. These tasks can either be accom-
plished by averaging over samples of ?
1
, . . . ,?
L
and ?m from P (?
1
, . . . ,?
L
, ?m |W
?
, ?) or by
evaluating a point estimate. We take the lat-
ter approach, and use the MAP estimate for ?m
and the predictive distributions over words for
?
1
, . . . ,?
L
. The probability of held-out docu-
ment tuples W
?
given training tuples W is then
approximated by P (W
?
|?
1
, . . . ,?
L
, ?m).
Topic assignments for a test document tuple
w = (w
1
, . . . ,w
L
) can be inferred using Gibbs
881
sampling. Gibbs sampling involves sequentially
resampling each z
l
n
from its conditional posterior:
P (z
l
n
= t |w, z
\l,n
,?
1
, . . . ,?
L
, ?m)
? ?
l
w
l
n
|t
(N
t
)
\l,n
+ ?m
t
?
t
N
t
? 1 + ?
, (4)
where z
\l,n
is the current set of topic assignments
for all other tokens in the tuple, while (N
t
)
\l,n
is
the number of occurrences of topic t in the tuple,
excluding z
l
n
, the variable being resampled.
4 Results on Parallel Text
Our first set of experiments focuses on document
tuples that are known to consist of direct transla-
tions. In this case, we can be confident that the
topic distribution is genuinely shared across all
languages. Although direct translations in multi-
ple languages are relatively rare (in contrast with
comparable documents), we use direct translations
to explore the characteristics of the model.
4.1 Data Set
The EuroParl corpus consists of parallel texts in
eleven western European languages: Danish, Ger-
man, Greek, English, Spanish, Finnish, French,
Italian, Dutch, Portuguese and Swedish. These
texts consist of roughly a decade of proceedings
of the European parliament. For our purposes we
use alignments at the speech level rather than the
sentence level, as in many translation tasks using
this corpus. We also remove the twenty-five most
frequent word types for efficiency reasons. The
remaining collection consists of over 121 million
words. Details by language are shown in Table 1.
Table 1: Average document length, # documents, and
unique word types per 10,000 tokens in the EuroParl corpus.
Lang. Avg. leng. # docs types/10k
DA 160.153 65245 121.4
DE 178.689 66497 124.5
EL 171.289 46317 124.2
EN 176.450 69522 43.1
ES 170.536 65929 59.5
FI 161.293 60822 336.2
FR 186.742 67430 54.8
IT 187.451 66035 69.5
NL 176.114 66952 80.8
PT 183.410 65718 68.2
SV 154.605 58011 136.1
Models are trained using 1000 iterations of
Gibbs sampling. Each language-specific topic?
word concentration parameter ?
l
is set to 0.01.
centralbank europ?iske ecb s l?n centralbanks 
zentralbank ezb bank europ?ischen investitionsbank darlehen 
??????? ???????? ???????? ??? ????????? ???????? 
bank central ecb banks european monetary 
banco central europeo bce bancos centrales 
keskuspankin ekp n euroopan keskuspankki eip 
banque centrale bce europ?enne banques mon?taire 
banca centrale bce europea banche prestiti 
bank centrale ecb europese banken leningen 
banco central europeu bce bancos empr?stimos 
centralbanken europeiska ecb centralbankens s l?n 
b?rn familie udnyttelse b?rns b?rnene seksuel 
kinder kindern familie ausbeutung familien eltern 
?????? ??????? ?????????? ??????????? ?????? ???????? 
children family child sexual families exploitation 
ni?os familia hijos sexual infantil menores 
lasten lapsia lapset perheen lapsen lapsiin 
enfants famille enfant parents exploitation familles 
bambini famiglia figli minori sessuale sfruttamento 
kinderen kind gezin seksuele ouders familie 
crian?as fam?lia filhos sexual crian?a infantil 
barn barnen familjen sexuellt familj utnyttjande 
m?l n? m?ls?tninger m?let m?ls?tning opn? 
ziel ziele erreichen zielen erreicht zielsetzungen 
??????? ????? ?????? ?????? ?????? ???????? 
objective objectives achieve aim ambitious set 
objetivo objetivos alcanzar conseguir lograr estos 
tavoite tavoitteet tavoitteena tavoitteiden tavoitteita tavoitteen 
objectif objectifs atteindre but cet ambitieux 
obiettivo obiettivi raggiungere degli scopo quello 
doelstellingen doel doelstelling bereiken bereikt doelen 
objectivo objectivos alcan?ar atingir ambicioso conseguir 
m?l m?let uppn? m?len m?ls?ttningar m?ls?ttning 
andre anden side ene andet ?vrige 
anderen andere einen wie andererseits anderer 
????? ???? ???? ????? ?????? ???? 
other one hand others another there 
otros otras otro otra parte dem?s 
muiden toisaalta muita muut muihin muun 
autres autre part c?t? ailleurs m?me 
altri altre altro altra dall parte 
andere anderzijds anderen ander als kant 
outros outras outro lado outra noutros 
andra sidan ? annat ena annan 
DA
DE
EL
EN
ES
FI
FR
IT
NL
PT
SV
 
DA
DE
EL
EN
ES
FI
FR
IT
NL
PT
SV
 
DA
DE
EL
EN
ES
FI
FR
IT
NL
PT
SV
 
DA
DE
EL
EN
ES
FI
FR
IT
NL
PT
SV
 
Figure 2: EuroParl topics (T=400)
The concentration parameter ? for the prior over
document-specific topic distributions is initialized
to 0.01T , while the base measure m is initialized
to the uniform distribution. Hyperparameters ?m
are re-estimated every 10 Gibbs iterations.
4.2 Analysis of Trained Models
Figure 2 shows the most probable words in all lan-
guages for four example topics, from PLTM with
400 topics. The first topic contains words relating
to the European Central Bank. This topic provides
an illustration of the variation in technical ter-
minology captured by PLTM, including the wide
array of acronyms used by different languages.
The second topic, concerning children, demon-
strates the variability of everyday terminology: al-
though the four Romance languages are closely
882
related, they use etymologically unrelated words
for children. (Interestingly, all languages except
Greek and Finnish use closely related words for
?youth? or ?young? in a separate topic.) The third
topic demonstrates differences in inflectional vari-
ation. English and the Romance languages use
only singular and plural versions of ?objective.?
The other Germanic languages include compound
words, while Greek and Finnish are dominated by
inflected variants of the same lexical item. The fi-
nal topic demonstrates that PLTM effectively clus-
ters ?syntactic? words, as well as more semanti-
cally specific nouns, adjectives and verbs.
Although the topics in figure 2 seem highly fo-
cused, it is interesting to ask whether the model
is genuinely learning mixtures of topics or simply
assigning entire document tuples to single topics.
To answer this question, we compute the posterior
probability of each topic in each tuple under the
trained model. If the model assigns all tokens in
a tuple to a single topic, the maximum posterior
topic probability for that tuple will be near to 1.0.
If the model assigns topics uniformly, the maxi-
mum topic probability will be near 1/T . We com-
pute histograms of these maximum topic prob-
abilities for T ? {50, 100, 200, 400, 800}. For
clarity, rather than overlaying five histograms, fig-
ure 3 shows the histograms converted into smooth
curves using a kernel density estimator.
1
Although
there is a small bump around 1.0 (for extremely
short documents, e.g., ?Applause?), values are
generally closer to, but greater than, 1/T .
0.0 0.2 0.4 0.6 0.8 1.0
0
2
4
6
8
10
12
Smoothed histograms of max(P(t))
Maximum topic probability in document
Den
sity 800 topics400 topics200 topics100 topics50 topics
Figure 3: Smoothed histograms of the probability of the
most probable topic in a document tuple.
Although the posterior distribution over topics
for each tuple is not concentrated on one topic,
it is worth checking that this is not simply be-
cause the model is assigning a single topic to the
1
We use the R density function.
tokens in each of the languages. Although the
model does not distinguish between topic assign-
ment variables within a given document tuple (so
it is technically incorrect to speak of different pos-
terior distributions over topics for different docu-
ments in a given tuple), we can nevertheless divide
topic assignment variables between languages and
use them to estimate a Dirichlet-multinomial pos-
terior distribution for each language in each tuple.
For each tuple we can then calculate the Jensen-
Shannon divergence (the average of the KL di-
vergences between each distribution and a mean
distribution) between these distributions. Figure 4
shows the density of these divergences for differ-
ent numbers of topics. As with the previous fig-
ure, there are a small number of documents that
contain only one topic in all languages, and thus
have zero divergence. These tend to be very short,
formulaic parliamentary responses, however. The
vast majority of divergences are relatively low (1.0
indicates no overlap in topics between languages
in a given document tuple) indicating that, for each
tuple, the model is not simply assigning all tokens
in a particular language to a single topic. As the
number of topics increases, greater variability in
topic distributions causes divergence to increase.
0.0 0.1 0.2 0.3 0.4 0.5
0
5
10
15
20
Smoothed histograms of inter?language JS divergence
Jensen?Shannon Divergence
Den
sity
800 topics400 topics200 topics100 topics50 topics
Figure 4: Smoothed histograms of the Jensen-Shannon
divergences between the posterior probability of topics be-
tween languages.
4.3 Language Model Evaluation
A topic model specifies a probability distribution
over documents, or in the case of PLTM, docu-
ment tuples. Given a set of training document tu-
ples, PLTM can be used to obtain posterior esti-
mates of ?
1
, . . . ,?
L
and ?m. The probability of
previously unseen held-out document tuples given
these estimates can then be computed. The higher
the probability of the held-out document tuples,
the better the generalization ability of the model.
883
Analytically calculating the probability of a set
of held-out document tuples given ?
1
, . . . ,?
L
and
?m is intractable, due to the summation over an
exponential number of topic assignments for these
held-out documents. However, recently developed
methods provide efficient, accurate estimates of
this probability. We use the ?left-to-right? method
of (Wallach et al, 2009). We perform five esti-
mation runs for each document and then calculate
standard errors using a bootstrap method.
Table 2 shows the log probability of held-out
data in nats per word for PLTM and LDA, both
trained with 200 topics. There is substantial varia-
tion between languages. Additionally, the predic-
tive ability of PLTM is consistently slightly worse
than that of (monolingual) LDA. It is important to
note, however, that these results do not imply that
LDA should be preferred over PLTM?that choice
depends upon the needs of the modeler. Rather,
these results are intended as a quantitative analy-
sis of the difference between the two models.
Table 2: Held-out log probability in nats/word. (Smaller
magnitude implies better language modeling performance.)
PLTM does slightly worse than monolingual LDA models,
but the variation between languages is much larger.
Lang PLTM sd LDA sd
DA -8.11 0.00067 -8.02 0.00066
DE -8.17 0.00057 -8.08 0.00072
EL -8.44 0.00079 -8.36 0.00087
EN -7.51 0.00064 -7.42 0.00069
ES -7.98 0.00073 -7.87 0.00070
FI -9.25 0.00089 -9.21 0.00065
FR -8.26 0.00072 -8.19 0.00058
IT -8.11 0.00071 -8.02 0.00058
NL -7.84 0.00067 -7.75 0.00099
PT -7.87 0.00085 -7.80 0.00060
SV -8.25 0.00091 -8.16 0.00086
As the number of topics is increased, the word
counts per topic become very sparse in mono-
lingual LDA models, proportional to the size of
the vocabulary. Figure 5 shows the proportion
of all tokens in English and Finnish assigned to
each topic under LDA and PLTM with 800 topics.
More than 350 topics in the Finnish LDA model
have zero tokens assigned to them, and almost all
tokens are assigned to the largest 200 topics. En-
glish has a larger tail, with non-zero counts in all
but 16 topics. In contrast, PLTM assigns a sig-
nificant number of tokens to almost all 800 top-
ics, in very similar proportions in both languages.
PLTM topics therefore have a higher granularity ?
i.e., they are more specific. This result is impor-
tant: informally, we have found that increasing the
granularity of topics correlates strongly with user
perceptions of the utility of a topic model.
0 200 400 600 800
0.00
0.01
0.02
0.03
0.04
Sorted topic rank
Perc
enta
ge o
f tok
ens
Figure 5: Topics sorted by number of words assigned.
Finnish is in black, English is in red; LDA is solid, PLTM is
dashed. LDA in Finnish essentially learns a 200 topic model
when given 800 topics, while PLTM uses all 800 topics.
4.4 Partly Comparable Corpora
An important application for polylingual topic
modeling is to use small numbers of comparable
document tuples to link topics in larger collections
of distinct, non-comparable documents in multiple
languages. For example, a journal might publish
papers in English, French, German and Italian. No
paper is exactly comparable to any other paper, but
they are all roughly topically similar. If we wish
to perform topic-based bibliometric analysis, it is
vital to be able to track the same topics across all
languages. One simple way to achieve this topic
alignment is to add a small set of comparable doc-
ument tuples that provide sufficient ?glue? to bind
the topics together. Continuing with the exam-
ple above, one might extract a set of connected
Wikipedia articles related to the focus of the jour-
nal and then train PLTM on a joint corpus consist-
ing of journal papers and Wikipedia articles.
In order to simulate this scenario we create a
set of variations of the EuroParl corpus by treat-
ing some documents as if they have no paral-
lel/comparable texts ? i.e., we put each of these
documents in a single-document tuple. To do this,
we divide the corpusW into two sets of document
tuples: a ?glue? set G and a ?separate? set S such
that |G| / |W| = p. In other words, the proportion
of tuples in the corpus that are treated as ?glue?
(i.e., placed in G) is p. For every tuple in S, we
assign each document in that tuple to a new single-
document tuple. By doing this, every document in
S has its own distribution over topics, independent
of any other documents. Ideally, the ?glue? doc-
884
uments in G will be sufficient to align the topics
across languages, and will cause comparable doc-
uments in S to have similar distributions over top-
ics even though they are modeled independently.
Table 3: The effect of the proportion p of ?glue? tuples on
mean Jensen-Shannon divergence in estimated topic distribu-
tions for pairs of documents in S that were originally part of
a document tuple. Lower divergence means the topic distri-
butions distributions are more similar to each other.
p Mean JS # of pairs Std. Err.
0.01 0.83755 487670 0.00018
0.05 0.79144 467288 0.00021
0.1 0.70228 443753 0.00026
0.25 0.38480 369608 0.00029
0.5 0.29712 246380 0.00030
Table 4: Topics are meaningful within languages but di-
verge between languages when only 1% of tuples are treated
as ?glue? tuples. With 25% ?glue? tuples, topics are aligned.
lang Topics at p = 0.01
DE ru?land russland russischen tschetschenien sicherheit
EN china rights human country s burma
FR russie tch?etch?enie union avec russe r?egion
IT ho presidente mi perch?e relazione votato
lang Topics at p = 0.25
DE ru?land russland russischen tschetschenien ukraine
EN russia russian chechnya cooperation region belarus
FR russie tch?etch?enie avec russe russes situation
IT russia unione cooperazione cecenia regione russa
We train PLTM with 100 topics on corpora with
p ? {0.01, 0.05, 0.1, 0.25, 0.5}. We use 1000 it-
erations of Gibbs sampling with ? = 0.01. Hy-
perparameters ?m are re-estimated every 10 it-
erations. We calculate the Jensen-Shannon diver-
gence between the topic distributions for each pair
of individual documents in S that were originally
part of the same tuple prior to separation. The
lower the divergence, the more similar the distri-
butions are to each other. From the results in fig-
ure 4, we know that leaving all document tuples
intact should result in a mean JS divergence of
less than 0.1. Table 3 shows mean JS divergences
for each value of p. As expected, JS divergence is
greater than that obtained when all tuples are left
intact. Divergence drops significantly when the
proportion of ?glue? tuples increases from 0.01 to
0.25. Example topics for p = 0.01 and p = 0.25
are shown in table 4. At p = 0.01 (1% ?glue? doc-
uments), German and French both include words
relating to Russia, while the English and Italian
word distributions appear locally consistent but
unrelated to Russia. At p = 0.25, the top words
for all four languages are related to Russia.
These results demonstrate that PLTM is appro-
priate for aligning topics in corpora that have only
a small subset of comparable documents. One area
for future work is to explore whether initializa-
tion techniques or better representations of topic
co-occurrence might result in alignment of topics
with a smaller proportion of comparable texts.
4.5 Machine Translation
Although the PLTM is clearly not a substitute for
a machine translation system?it has no way to
represent syntax or even multi-word phrases?it is
clear from the examples in figure 2 that the sets of
high probability words in different languages for a
given topic are likely to include translations. We
therefore evaluate the ability of the PLTM to gen-
erate bilingual lexica, similar to other work in un-
supervised translation modeling (Haghighi et al,
2008). In the early statistical translation model
work at IBM, these representations were called
?cepts,? short for concepts (Brown et al, 1993).
We evaluate sets of high-probability words in
each topic and multilingual ?synsets? by compar-
ing them to entries in human-constructed bilingual
dictionaries, as done by Haghighi et al (2008).
Unlike previous work (Koehn and Knight, 2002),
we evaluate all words, not just nouns. We col-
lected bilingual lexica mapping English words to
German, Greek, Spanish, French, Italian, Dutch
and Swedish. Each lexicon is a set of pairs con-
sisting of an English word and a translated word,
{w
e
, w
`
}. We do not consider multi-word terms.
We expect that simple analysis of topic assign-
ments for sequential words would yield such col-
locations, but we leave this for future work.
For every topic t we select a small number K
of the most probable words in English (e) and
in each ?translation? language (`): W
te
and W
t`
,
respectively. We then add the Cartesian product
of these sets for every topic to a set of candidate
translations C. We report the number of elements
of C that appear in the reference lexica. Results
for K = 1, that is, considering only the single
most probable word for each language, are shown
in figure 6. Precision at this level is relatively
high, above 50% for Spanish, French and Italian
with T = 400 and 800. Many of the candidate
pairs that were not in the bilingual lexica were
valid translations (e.g. EN ?comitology? and IT
885
?comitalogia?) that simply were not in the lexica.
We also do not count morphological variants: the
model finds EN ?rules? and DE ?vorschriften,? but
the lexicon contains only ?rule? and ?vorschrift.?
Results remain strong as we increase K. With
K = 3, T = 800, 1349 of the 7200 candidate
pairs for Spanish appeared in the lexicon.
l l
l
l
l
200 400 600 800
0
100
200
300
400
500
Translation pairs at K=1
Topics
Corr
ect t
rans
lation
s
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ESFRITDESVEL
Figure 6: Are the single most probable words for a given
topic in different languages translations of each other? The
number of such pairs that appear in bilingual lexica is shown
on the y-axis. For T = 800, the top English and Spanish
words in 448 topics were exact translations of one another.
4.6 Finding Translations
In addition to enhancing lexicons by aligning
topic-specific vocabulary, PLTM may also be use-
ful for adapting machine translation systems to
new domains by finding translations or near trans-
lations in an unstructured corpus. These aligned
document pairs could then be fed into standard
machine translation systems as training data. To
evaluate this scenario, we train PLTM on a set of
document tuples from EuroParl, infer topic distri-
butions for a set of held-out documents, and then
measure our ability to align documents in one lan-
guage with their translations in another language.
It is not necessarily clear that PLTM will be ef-
fective at identifying translations. In finding a low-
dimensional semantic representation, topic mod-
els deliberately smooth over much of the varia-
tion present in language. We are therefore inter-
ested in determining whether the information in
the document-specific topic distributions is suffi-
cient to identify semantically identical documents.
We begin by dividing the data into a training
set of 69,550 document tuples and a test set of
17,435 document tuples. In order to make the task
more difficult, we train a relatively coarse-grained
PLTM with 50 topics on the training set. We then
use this model to infer topic distributions for each
40
50
60
70
80
90
100
Min query doc length
% o
f tra
nsl 
at r
ank
0 50 100 200
Rank 1
Rank 5
Rank 10
Rank 20
Figure 7: Percent of query language documents for which
the target language translation is ranked at or above 1, 5, 10
or 20 by JS divergence, averaged over all language pairs.
of the 11 documents in each of the held-out doc-
ument tuples using a method similar to that used
to calculate held-out probabilities (Wallach et al,
2009). Finally, for each pair of languages (?query?
and ?target?) we calculate the difference between
the topic distribution for each held-out document
in the query language and the topic distribution for
each held-out document in the target language. We
use both Jensen-Shannon divergence and cosine
distance. For each document in the query language
we rank all documents in the target language and
record the rank of the actual translation.
Results averaged over all query/target language
pairs are shown in figure 7 for Jensen-Shannon
divergence. Cosine-based rankings are signifi-
cantly worse. It is important to note that the
length of documents matters. As noted before,
many of the documents in the EuroParl collection
consist of short, formulaic sentences. Restrict-
ing the query/target pairs to only those with query
and target documents that are both longer than 50
words results in significant improvement and re-
duced variance: the average proportion of query
documents for which the true translation is ranked
highest goes from 53.9% to 72.7%. Performance
continues to improve with longer documents, most
likely due to better topic inference. Results vary
by language. Table 5 shows results for all tar-
get languages with English as a query language.
Again, English generally performs better with Ro-
mance languages than Germanic languages.
5 Results on Comparable Texts
Directly parallel translations are rare in many lan-
guages and can be extremely expensive to pro-
duce. However, the growth of the web, and in par-
ticular Wikipedia, has made comparable text cor-
886
CY
DE
EL
EN
FA
FI
FR
HE
IT
PL
RU
TR
CY
DE
EL
EN
FA
FI
FR
HE
IT
PL
RU
TR
CY
DE
EL
EN
FA
FI
FR
HE
IT
PL
RU
TR
Figure 8: Squares represent the proportion of tokens in each language assigned to a topic. The left topic, world ski km won,
centers around Nordic counties. The center topic, actor role television actress, is relatively uniform. The right topic, ottoman
empire khan byzantine, is popular in all languages but especially in regions near Istanbul.
Table 5: Percent of English query documents for which the
translation was in the top n ? {1, 5, 10, 20} documents by JS
divergence between topic distributions. To reduce the effect
of short documents we consider only document pairs where
the query and target documents are longer than 100 words.
Lang 1 5 10 20
DA 78.0 90.7 93.8 95.8
DE 76.6 90.0 93.4 95.5
EL 77.1 90.4 93.3 95.2
ES 81.2 92.3 94.8 96.7
FI 76.7 91.0 94.0 96.3
FR 80.1 91.7 94.3 96.2
IT 79.1 91.2 94.1 96.2
NL 76.6 90.1 93.4 95.5
PT 80.8 92.0 94.7 96.5
SV 80.4 92.1 94.9 96.5
pora ? documents that are topically similar but are
not direct translations of one another ? consider-
ably more abundant than true parallel corpora.
In this section, we explore two questions re-
lating to comparable text corpora and polylingual
topic modeling. First, we explore whether com-
parable document tuples support the alignment of
fine-grained topics, as demonstrated earlier using
parallel documents. This property is useful for
building machine translation systems as well as
for human readers who are either learning new
languages or analyzing texts in languages they do
not know. Second, because comparable texts may
not use exactly the same topics, it becomes cru-
cially important to be able to characterize differ-
ences in topic prevalence at the document level (do
different languages have different perspectives on
the same article?) and at the language-wide level
(which topics do particular languages focus on?).
5.1 Data Set
We downloaded XML copies of all Wikipedia ar-
ticles in twelve different languages: Welsh, Ger-
man, Greek, English, Farsi, Finnish, French, He-
brew, Italian, Polish, Russian and Turkish. These
versions of Wikipedia were selected to provide a
diverse range of language families, geographic ar-
eas, and quantities of text. We preprocessed the
data by removing tables, references, images and
info-boxes. We dropped all articles in non-English
languages that did not link to an English article. In
the English version of Wikipedia we dropped all
articles that were not linked to by any other lan-
guage in our set. For efficiency, we truncated each
article to the nearest word after 1000 characters
and dropped the 50 most common word types in
each language. Even with these restrictions, the
size of the corpus is 148.5 million words.
We present results for a PLTM with 400 topics.
1000 Gibbs sampling iterations took roughly four
days on one CPU with current hardware.
5.2 Which Languages Have High Topic
Divergence?
As with EuroParl, we can calculate the Jensen-
Shannon divergence between pairs of documents
within a comparable document tuple. We can then
average over all such document-document diver-
gences for each pair of languages to get an over-
all ?disagreement? score between languages. In-
terestingly, we find that almost all languages in
our corpus, including several pairs that have his-
torically been in conflict, show average JS diver-
gences of between approximately 0.08 and 0.12
for T = 400, consistent with our findings for
EuroParl translations. Subtle differences of sen-
timent may be below the granularity of the model.
887
sadwrn blaned gallair at lloeren mytholeg 
space nasa sojus flug mission 
?????????? sts nasa ???? small 
space mission launch satellite nasa spacecraft 
??????? ??????? ???? ???? ??????? ?????  
sojuz nasa apollo ensimm?inen space lento 
spatiale mission orbite mars satellite spatial 
?????? ? ???? ??? ???? ????  
spaziale missione programma space sojuz stazione 
misja kosmicznej stacji misji space nasa 
??????????? ???? ???????????? ??????? ???????
uzay soyuz ay uzaya salyut sovyetler 
sbaen madrid el la jos? sbaeneg 
de spanischer spanischen spanien madrid la 
???????? ??????? de ??????? ??? ??????? 
de spanish spain la madrid y 
?????? ???? ????????? ???????  de ????  
espanja de espanjan madrid la real 
espagnol espagne madrid espagnole juan y 
???? ??????? ????? ?? ?????? ????  
de spagna spagnolo spagnola madrid el 
de hiszpa?ski hiszpanii la juan y 
?? ?????? ??????? ??????? ????????? de 
ispanya ispanyol madrid la k?ba real 
bardd gerddi iaith beirdd fardd gymraeg 
dichter schriftsteller literatur gedichte gedicht werk 
??????? ?????? ?????? ???? ??????? ???????? 
poet poetry literature literary poems poem 
???? ???? ????? ?????? ??? ????  
runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi 
po?te ?crivain litt?rature po?sie litt?raire ses 
?????? ????? ???? ???? ????? ?????
poeta letteratura poesia opere versi poema 
poeta literatury poezji pisarz in jego 
???? ??? ???????? ?????????? ?????? ????????? 
?air edebiyat ?iir yazar edebiyat? adl? 
CY
DE
EL
EN
FA
FI
FR
HE
IT
PL
RU
TR
 
CY
DE
EL
EN
FA
FI
FR
HE
IT
PL
RU
TR
 
CY
DE
EL
EN
FA
FI
FR
HE
IT
PL
RU
TR
Figure 9: Wikipedia topics (T=400).
Overall, these scores indicate that although indi-
vidual pages may show disagreement, Wikipedia
is on average consistent between languages.
5.3 Are Topics Emphasized Differently
Between Languages?
Although we find that if Wikipedia contains an ar-
ticle on a particular subject in some language, the
article will tend to be topically similar to the arti-
cles about that subject in other languages, we also
find that across the whole collection different lan-
guages emphasize topics to different extents. To
demonstrate the wide variation in topics, we cal-
culated the proportion of tokens in each language
assigned to each topic. Figure 8 represents the es-
timated probabilities of topics given a specific lan-
guage. Competitive cross-country skiing (left) ac-
counts for a significant proportion of the text in
Finnish, but barely exists in Welsh and the lan-
guages in the Southeastern region. Meanwhile,
interest in actors and actresses (center) is consis-
tent across all languages. Finally, historical topics,
such as the Byzantine and Ottoman empires (right)
are strong in all languages, but show geographical
variation: interest centers around the empires.
6 Conclusions
We introduced a polylingual topic model (PLTM)
that discovers topics aligned across multiple lan-
guages. We analyzed the characteristics of PLTM
in comparison to monolingual LDA, and demon-
strated that it is possible to discover aligned top-
ics. We also demonstrated that relatively small
numbers of topically comparable document tu-
ples are sufficient to align topics between lan-
guages in non-comparable corpora. Additionally,
PLTM can support the creation of bilingual lexica
for low resource language pairs, providing candi-
date translations for more computationally intense
alignment processes without the sentence-aligned
translations typically used in such tasks. When
applied to comparable document collections such
as Wikipedia, PLTM supports data-driven analysis
of differences and similarities across all languages
for readers who understand any one language.
7 Acknowledgments
The authors thank Limin Yao, who was involved
in early stages of this project. This work was
supported in part by the Center for Intelligent In-
formation Retrieval, in part by The Central In-
telligence Agency, the National Security Agency
and National Science Foundation under NSF grant
number IIS-0326249, and in part by Army prime
contract number W911NF-07-1-0216 and Uni-
versity of Pennsylvania subaward number 103-
548106, and in part by National Science Founda-
tion under NSF grant #CNS-0619337. Any opin-
ions, findings and conclusions or recommenda-
tions expressed in this material are the authors?
and do not necessarily reflect those of the sponsor.
References
David Blei and Michael Jordan. 2003. Modeling an-
notated data. In SIGIR.
David Blei, Andrew Ng, and Michael Jordan. 2003.
Latent Dirichlet alocation. JMLR.
Peter F Brown, Stephen A Della Pietra, Vincent J Della
Pietra, and Robert L Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. CL, 19(2):263?311.
888
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In ACL, pages 771?779.
David Hall, Daniel Jurafsky, and Christopher D. Man-
ning. 2008. Studying the history of ideas using
topic models. In EMNLP.
Mariko Kawaba, Hiroyuki Nakasaki, Takehito Utsuro,
and Tomohiro Fukuhara. 2008. Cross-lingual blog
analysis based on multilingual blog distillation from
multilingual Wikipedia entries. In ICWSM.
Philipp Koehn and Kevin Knight. 2002. Learn-
ing a translation lexicon from monolingual corpora.
In Proceedings of ACL Workshop on Unsupervised
Lexical Acquisition.
Gideon Mann, David Mimno, and Andrew McCal-
lum. 2006. Bibliometric impact measures leverag-
ing topic analysis. In JCDL.
Andrew McCallum, Andr?es Corrada-Emmanuel, and
Xuerui Wang. 2005. Topic and role discovery in
social networks. In IJCAI.
Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen.
2009. Mining multilingual topics from Wikipedia.
In WWW.
Yik-Cheung Tam, Ian Lane, and Tanja Schultz. 2007.
Bilingual LSA-based adaptation for statistical ma-
chine translation. Machine Translation, 28:187?
207.
Hanna Wallach, Iain Murray, Ruslan Salakhutdinov,
and David Mimno. 2009. Evaluation methods for
topic models. In ICML.
Xing Wei and Bruce Croft. 2006. LDA-based docu-
ment models for ad-hoc retrieval. In SIGIR.
Bing Zhao and Eric P. Xing. 2007. HM-BiTAM: Bilin-
gual topic exploration, word alignment, and transla-
tion. In NIPS.
889
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 227?237,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Bayesian Checking for Topic Models
David Mimno
Department of Computer Science
Princeton University Princeton, NJ 08540
mimno@cs.princeton.edu
David Blei
Department of Computer Science
Princeton University Princeton, NJ 08540
blei@cs.princeton.edu
Abstract
Real document collections do not fit the inde-
pendence assumptions asserted by most statis-
tical topic models, but how badly do they vi-
olate them? We present a Bayesian method
for measuring how well a topic model fits a
corpus. Our approach is based on posterior
predictive checking, a method for diagnosing
Bayesian models in user-defined ways. Our
method can identify where a topic model fits
the data, where it falls short, and in which di-
rections it might be improved.
1 Introduction
Probabilistic topic models are a suite of machine
learning algorithms that decompose a corpus into
a set of topics and represent each document with a
subset of those topics. The inferred topics often cor-
respond with the underlying themes of the analyzed
collection, and the topic modeling algorithm orga-
nizes the documents according to those themes.
Most topic models are evaluated by their predic-
tive performance on held out data. The idea is that
topic models are fit to maximize the likelihood (or
posterior probability) of a collection of documents,
and so a good model is one that assigns high likeli-
hood to a held out set (Blei et al, 2003; Wallach et
al., 2009).
But this evaluation is not in line with how
topic models are frequently used. Topic mod-
els seem to capture the underlying themes of a
collection?indeed the monicker ?topic model? is
retrospective?and so we expect that these themes
are useful for exploring, summarizing, and learning
about its documents (Mimno and McCallum, 2007;
Chang et al, 2009). In such exploratory data anal-
ysis, however, we are not concerned with the fit to
held out data.
In this paper, we develop and study new methods
for evaluating topic models. Our methods are based
on posterior predictive checking, which is a model
diagnosis technique from Bayesian statistics (Rubin,
1984; Gelman et al, 1996). The goal of a posterior
predictive check (PPC) is to assess the validity of a
Bayesian model without requiring a specific alterna-
tive model. Given data, we first compute a posterior
distribution over the latent variables. Then, we esti-
mate the probability of the observed data under the
data-generating distribution that is induced by the
posterior (the ?posterior predictive distribution?). A
data set that is unlikely calls the model into ques-
tion, and consequently the posterior. PPCs can show
where the model fits and doesn?t fit the observations.
They can help identify the parts of the posterior that
are worth exploring.
The key to a posterior predictive check is the dis-
crepancy function. This is a function of the data that
measures a property of the model which is impor-
tant to capture. While the model is often chosen
for computational reasons, the discrepancy function
might capture aspects of the data that are desirable
but difficult to model. In this work, we will design
a discrepancy function to measure an independence
assumption that is implicit in the modeling assump-
tions but is not enforced in the posterior. We will
embed this function in a posterior predictive check
and use it to evaluate and visualize topic models in
new ways.
227
Specifically, we develop discrepancy functions
for latent Dirichlet alocation (the simplest topic
model) that measure how well its statistical assump-
tions about the topics are matched in the observed
corpus and inferred topics. LDA assumes that each
observed word in a corpus is assigned to a topic, and
that the words assigned to the same topic are drawn
independently from the same multinomial distribu-
tion (Blei et al, 2003). For each topic, we mea-
sure the whether this assumption holds by comput-
ing the mutual information between the words as-
signed to that topic and which document each word
appeared in. If the assumptions hold, these two vari-
ables should be independent: low mutual informa-
tion indicates that the assumptions hold; high mu-
tual information indicates a mismatch to the model-
ing assumptions.
We embed this discrepancy in a PPC and study
it in several ways. First, we focus on topics that
model their observations well; this helps separate
interpretable topics from noisy topics (and ?boiler-
plate? topics, which exhibit too little noise). Sec-
ond, we focus on individual terms within topics; this
helps display a model applied to a corpus, and under-
stand which terms are modeled well. Third, we re-
place the document identity with an external variable
that might plausibly be incorporated into the model
(such as time stamp or author). This helps point the
modeler towards the most promising among more
complicated models, or save the effort in fitting one.
Finally, we validate this strategy by simulating data
from a topic model, and assessing whether the PPC
?accepts? the resulting data.
2 Probabilistic Topic Modeling
Probabilistic topic models are statistical models of
text that assume that a small number of distributions
over words, called ?topics,? are used to generate the
observed documents. One of the simplest topic mod-
els is latent Dirichlet alocation (LDA) (Blei et al,
2003). In LDA, a set of K topics describes a cor-
pus; each document exhibits the topics with different
proportions. The words are assumed exchangeable
within each document; the documents are assumed
exchangeable within the corpus.
More formally, let ?1, . . . , ?K be K topics, each
of which is a distribution over a fixed vocabulary.
For each document, LDA assumes the following
generative process
1. Choose topic proportions ?d ? Dirichlet(?).
2. For each word
(a) Choose topic assignment zd,n ? ?.
(b) Choose word wd,n ? ?zd,n .
This process articulates the statistical assumptions
behind LDA: Each document is endowed with its
own set of topic proportions ?d, but the same set of
topics ?1:K governs the whole collection.
Notice that the probability of a word is indepen-
dent of its document ?d given its topic assignment
zd,n (i.e., wd,n ? ?d | zd,n). Two documents might
have different overall probabilities of containing a
word from the ?vegetables? topic; however, all the
words in the collection (regardless of their docu-
ments) drawn from that topic will be drawn from the
same multinomial distribution.
The central computational problem for LDA is
posterior inference. Given a collection of docu-
ments, the problem is to compute the conditional
distribution of the hidden variables?the topics ?k,
topic proportions ?d, and topic assignments zd,n.
Researchers have developed many algorithms for
approximating this posterior, including sampling
methods (Griffiths and Steyvers, 2004) (used in this
paper), variational methods (Blei et al, 2003), dis-
tributed variants (Asuncion et al, 2008), and online
algorithms (Hoffman et al, 2010).
3 Checking Topic Models
Once approximated, the posterior distribution is
used for the task at hand. Topic models have been
applied to many tasks, such as classification, predic-
tion, collaborative filtering, and others. We focus
on using them as an exploratory tool, where we as-
sume that the topic model posterior provides a good
decomposition of the corpus and that the topics pro-
vide good summaries of the corpus contents.
But what is meant by ?good?? To answer this
question, we turn to Bayesian model checking (Ru-
bin, 1981; Gelman et al, 1996). The goal of
Bayesian model checking is to assess whether the
observed data matches the modeling assumptions in
the directions that are important to the analysis. The
228
Score
Ran
k
14
12
10
8
6
4
2
14
12
10
8
6
4
2
14
12
10
8
6
4
2
Topic850
weekendBroadwayTimeslisting
selective
noteworthy
critics ticketshighly
recommendeddenotesboothTicketsStreetTKTS
Topic628
IraqIraqi HusseinBaghdadSaddamShiitegovernment
al IraqisSunniKurdishforces
country
militarytroops
Topic87
Roberts GrantFortWorth BurkeHuntKravis BassKohlbergGraceRothschildBaronBordenTexasWilliam
1 2 3 4
Topic371
TicketsThroughStreetRoadSaturdaysSundaysNewFridaysJerseyHoursFreeTuesdaysMUSEUMThursdaysTHEATER
Topic178
agency
safety
reportFederalAdministrationproblemsinvestigationSafety
violationsfederalfailedinspector
reviewdepartmentgeneral
Topic750
Four FreemanSeasonsDaVinciCode ThomsonWolffLeonardoBrownThreeDanCliffHolyda
1 2 3 4
Topic760
WeekbookWarner
salesListWeeks
womanbookstoresdeathindicatesAdvicePutnamOF
reportNew
Topic632job jobs
working
officebusiness
career
worked
employeeshiredboss
managerfind
corporatehelp
experience
Topic274
LeonLevy HessBardLEVYBotsteinAtlas ShelbyPanetta Norman WieseltierHESSDavidAmerada Norma
1 2 3 4
Figure 1: Visualization of variability within topics. Nine randomly selected topics from the New York Times with
low (top row), medium (middle row) and high (bottom row) mutual information between words and documents. The
y-axis shows term rank within the topic, with size proportional to log probability. The x-axis represents divergence
from the multinomial assumption for each word: terms that are uniformly distributed across documents are towards
the left, while more specialized terms are to the right. Triangles represent real values, circles represent 20 replications
of this same plot from the posterior of the model.
229
intuition is that only when satisfied with the model
should the modeler use the posterior to learn about
her data. In complicated Bayesian models, such as
topic models, Bayesian model checking can point to
the parts of the posterior that better fit the observed
data set and are more likely to suggest something
meaningful about it.
In particular, we will develop posterior predictive
checks (PPC) for topic models. In a PPC, we spec-
ify a discrepancy function, which is a function of
the data that measures an important property that we
want the model to capture. We then assess whether
the observed value of the function is similar to val-
ues of the function drawn from the posterior, through
the distribution of the data that it induces. (This dis-
tribution of the data is called the ?posterior predic-
tive distribution.?)
An innovation in PPCs is the realized discrepancy
function (Gelman et al, 1996), which is a function
of the data and any hidden variables that are in the
model. Realized discrepancies induce a traditional
discrepancy by marginalizing out the hidden vari-
ables. But they can also be used to evaluate assump-
tions about latent variables in the posterior, espe-
cially when combined with techniques like MCMC
sampling that provide realizations of them. In topic
models, as we will see below, we use a realized dis-
crepancy to factor the observations and to check spe-
cific components of the model that are discovered by
the posterior.
3.1 A realized discrepancy for LDA
Returning to LDA, we design a discrepancy func-
tion that checks the independence assumption of
words given their topic assignments. As we men-
tioned above, given the topic assignment z the word
w should be independent of its document ?. Con-
sider a decomposition of a corpus from LDA, which
assigns every observed word wd,n to a topic zd,n.
Now restrict attention to all the words assigned to the
kth topic and form two random variables: W are the
words assigned to the topic and D are the document
indices of the words assigned to that topic. If the
LDA assumptions hold then knowing W gives no
information about D because the words are drawn
independently from the topic.
We measure this independence with the mutual
information between W and D:1
MI(W,D | k)
=
?
w
?
d
P (w, d | k) log P (w | d, k)P (d | k)P (w | k)P (d | k)
=
?
w
?
d
N(w, d, k)
N(k) log
N(w, d, k)N(k)
N(d, k)N(w, k) . (1)
Where N(w, d, k) is the number of tokens of type
w in topic k in document d, with N(w, k) =?
dN(w, d, k), N(d, k) =
?
wN(w, d, k), and
N(k) =
?
w,dN(w, d, k). This function mea-
sures the divergence between the joint distribution
over word and document index and the product of
the marginal distributions. In the limit of infinite
samples, independent random variables have mutual
information of zero, but we expect finite samples
to have non-zero values even for truly independent
variables. Notice that this is a realized discrepancy;
it depends on the latent assignments of observed
words to topics.
Eq. 1 is defined as a sum over a set of documents
and a set of words. We can rearrange this summa-
tion as a weighted sum of the instantaneous mutual
information between words and documents:
IMI(w,D | k) = H(D|k)?H(D |W = w, k).
(2)
This quantity can be understood by considering the
per-topic distribution of document labels, p(d|k).
This distribution is formed by normalizing the
counts of how many words assigned to topic k ap-
peared in each document. The first term of Eq. 2
is the entropy?some topics are evenly distributed
across many documents (high entropy); others are
concentrated in fewer documents (low entropy).
The second term conditions this distribution on
a particular word type w by normalizing the per-
document number of times w appeared in each doc-
ument (in topic k). If this distribution is close
to p(d|k) then H(D|W = w, k) will be close to
H(D|k) and IMI(w,D|k) will be low. If, on the
other hand, word w occurs many times in only a few
documents, it will have lower entropy over docu-
1There are other choices of discrepancies, such as word-
word point-wise mutual information scores (Newman et al,
2010).
230
ments than the overall distribution over documents
for the topic and IMI(w,D|k) will be high.
We illustrate this discrepancy in Figure 1, which
shows nine topics trained from the New York Times.2
Each row contains randomly selected topics from
low, middle, and high ranges of MI, respectively.
Each triangle represents a word. Its place on the y-
axis is its rank in the topic. Its place on the x-axis
is its IMI(w|k), with more uniformly distributed
words (low IMI) to the left and more specific words
(high IMI) to the right. (For now, ignore the other
points in this figure.) IMI varies between topics, but
tends to increase with rank as less frequent words
appear in fewer documents.
The discrepancy captures different kinds of struc-
ture in the topics. The top left topic represents for-
mulaic language, language that occurs verbatim in
many documents. In particular, it models the boil-
erplate text ?Here is a selective listing by critics of
The Times of new or noteworthy...? Identifying re-
peated phrases is a common phenomenon in topic
models. Most words show lower than expected IMI,
indicating that word use in this topic is less vari-
able than data drawn from a multinomial distribu-
tion. The middle-left topic is an example of a good
topic, according to this discrepancy, which is related
to Iraqi politics. The bottom-left topic is an example
of the opposite extreme from the top-left. It shows
a loosely connected series of proper names with no
overall theme.
3.2 Posterior Predictive Checks for LDA
Intuitively, the middle row of topics in Figure 1 are
the sort of topics we look for in a model, while the
top and bottom rows contain topics that are less use-
ful. Using a PPC, we can formally measure the dif-
ference between these topics. For each of the real
topics in Figure 1 we regenerated the same figure
20 times. We sampled new words for every token
from the posterior distribution of the topic, and re-
calculated the rank and IMI for each word. These
?shadow? figures are shown as gray circles. The
density of those circles creates a reference distribu-
tion indicating the expected IMI values at each rank
under the multinomial assumption.
2Details about the corpus and model fitting are in Section
4.2. Similar figures for two other corpora are in the supplement.
By themselves, IMI scores give an indication of
the distribution of a word between documents within
a topic: small numbers are better, large numbers in-
dicate greater discrepancy. These scores, however,
are based on the specific allocation of words to top-
ics. For example, lower-ranked, less frequent words
within a topic tend to have higher IMI scores than
higher-ranked, more frequent words. This difference
may be due to greater violation of multinomial as-
sumptions, but may also simply be due to smaller
sample sizes, as the entropy H(D|W = w, k) is es-
timated from fewer tokens. The reference distribu-
tions help distinguish between these two cases.
In more detail, we generate replications of the
data by considering a Gibbs sampling state. This
state assigns each observed word to a topic. We
first record the number of instances of each term as-
signed to each topic, N(w|k). Then for each word
wd,n in the corpus, we sample a new observed word
wrepd,n where P (w) ? N(w|zd,n). (We did not usesmoothing parameters.) Finally, we recalculate the
mutual information and instantenous mutual infor-
mation for each topic.
In the top-left topic, most of the words have much
lower IMI than the word at the same rank in repli-
cations, indicating lower than expected variability.
The exception is the word Broadway, which is more
variable than expected. In the middle-left topic,
IMI for the words Iraqi and Baghdad occur within
the expected range. These words fit the multino-
mial assumption: any word assigned to this topic
is equally likely to be Iraqi. Values for the words
Shiite, Sunni, and Kurdish are more specific to par-
ticular documents than we expect under the model.
In the bottom-left topic, almost all words occur with
greater variability than expected. This topic com-
bines many terms with only coincidental similarity,
such as Mets pitcher Grant Roberts and the firm
Kohlberg Kravis Roberts.
Turning to an analysis of the full mutual infor-
mation, Figure 2 shows the three left-hand topics
from Figure 1: Weekend, Iraq, and Roberts. The
histogram represents MI scores for 100 replications
of the topic, rescaled to have mean zero and unit
variance. The observed value, also rescaled, and
the mean replicated value (set to zero) are shown
with vertical lines. The formulaic Weekend topic
has significantly lower than expected MI. The Iraq
231
Deviance
cou
nt
05
1015
2025
30
05
1015
2025
30
05
1015
2025
30
Topic850
Topic628
Topic87
?20 0 20 40
Figure 2: News: Observed topic scores (vertical lines)
relative to replicated scores, rescaled so that replica-
tions have zero mean and unit variance. The Weekend
topic (top) has lower than expected MI. The Iraq (mid-
dle) and Roberts (bottom) topics both have MI greater
than expected.
and Roberts topics have significantly greater than
expected MI.
For most topics the actual discrepancy is outside
the range of any replicated discrepancies. In their
original formulation, PPCs prescribe computing a
tail probability of a replicated discrepancy being
greater than (or less than) the observed discrepancy
under the posterior predictive distribution. For ex-
ample if an observed value is greater than 70 of 100
replicated values, we report a PPC p-value of 0.7.
When the observed value is far outside the range
of any replicated values, as in Figure 2, that tail
probability will be degenerate at 0 or 1. So, we re-
port instead a deviance value, an alternative way of
comparing an observed value to a reference distri-
bution. We compute the distribution of the repli-
cated discrepancies and compute its standard devi-
ation. We then compute how many standard devia-
tions the observed discrepancy is from the mean of
the replicated discrepancies.
This score allows us to compare topics. The ob-
served value for the Weekend topic is 31.8 standard
deviations below the mean replicated value, and thus
has deviance of -31.8, which is lower than expected.
The Iraq topic has deviance of 16.8 and the Roberts
topic has deviance of 47.7. This matches our intu-
ition that the former topic is more useful than the
latter.
4 Searching for Systematic Deviations
We demonstrated that the mutual information dis-
crepancy function can detect violations of multi-
nomial assumptions, in which instances of a term
in a given topic are not independently distributed
among documents. One way to address this lack
of fit is to encode document-level extra-multinomial
variance (?burstiness?) into the model using Dirich-
let compound multinomial distributions (Doyle and
Elkan, 2009). If there is no pattern to the deviations
from multinomial word use across documents, this
method is the best we can do.
In many corpora, however, there are systematic
deviations that can be explained by additional vari-
ables. LDA is the simplest generative topic model,
and researchers have developed many variants of
LDA that account for a variety of variables that can
be found or measured with a corpus. Examples in-
clude models that account for time (Blei and Laf-
ferty, 2006), books (Mimno and McCallum, 2007),
and aspect or perspective (Mei and Zhai, 2006; Lin
et al, 2008; Paul et al, 2010). In this section, we
show how we can use the mutual information dis-
crepancy function of Equation 1 and PPCs to guide
our choice in which topic model to fit.
Greater deviance implies that a particular group-
ing better explains the variation in word use within
a topic. The discrepancy functions are large when
words appear more than expected in some groups
and less than expected in others. We know that
the individual documents show significantly more
variation than we expect from replications from the
model?s posterior distribution. If we combine docu-
ments randomly in a meaningless grouping, such de-
viance should decrease, as differences between doc-
uments are ?smoothed out.? If a grouping of docu-
ments shows equal or greater deviation, we can as-
sume that that grouping is maintaining the underly-
ing structure of the systematic deviation from the
multinomial assumption, and that further modeling
or visualization using that grouping might be useful.
4.1 PPCs for systematic discrepancy
The idea is that the words assigned to a topic should
be independent of both document and any other vari-
able that might be associated with the document. We
simply replace the document index d with another
232
Score
Ran
k
20
15
10
5
DocumentsIraqIraqi HusseinBaghdadSaddamShiitegovernmental IraqisSunniKurdishforcescountrymilitarytroopsleaderscity KurdssecuritySadr
0.0 0.5 1.0 1.5 2.0 2.5
MonthsIraqIraqiHusseinBaghdadSaddamShiitegovernmental IraqisSunniKurdishforcescountrymilitarytroopsleaderscityKurdssecurity Sadr
0.0 0.5 1.0 1.5 2.0 2.5
DesksIraqIraqiHusseinBaghdadSaddamShiitegovernmentalIraqisSunniKurdishforcescountrymilitarytroopsleaderscityKurdssecuritySadr
0.0 0.5 1.0 1.5 2.0 2.5
Figure 3: Groupings decrease MI, but values are still larger than expected. Three ways of grouping words in a
topic from the New York Times. The word leaders varies more between desks than by time, while Sadr varies more by
time than desk.
variable g in the discrepancy. For example, the New
York Times articles are each associated with a par-
ticular news desk and also associated with a time
stamp. If the topic modeling assumptions hold, the
words are independent of both these variables. If we
see a significant discrepancy relative to a grouping
defined by a metadata feature, this systematic vari-
ability suggests that we might want to take that fea-
ture into account in the model.
Let G be a set of groups and let ? ? GD be
a grouping of D documents. Let N(w, g, k) =?
dN(w, d, k)I?d=g, that is, the number of words of
typew in topic k in documents in group g, and define
the other count variables similarly. We can now sub-
stitute these group-specific counts for the document-
specific counts in the discrepancy function in Eq.
1. Note that the previous discrepancy functions are
equivalent to a trivial grouping, in which each docu-
ment is the only member of its own group. In the fol-
lowing experiments we explore groupings by pub-
lished volume, blog, preferred political candidate,
and newspaper desk, and evaluate the effect of those
groupings on the deviation between mean replicated
values and observed values of those functions.
4.2 Case studies
We analyze three corpora, each with its own meta-
data: the New York Times Annotated Corpus (1987?
2007)3, the CMU 2008 political blog corpus (Eisen-
stein and Xing, 2010), and speeches from the British
3http://www.ldc.upenn.edu
House of Commons from 1830?1891.4 Descriptive
statistics are presented in Table 1. The realization
is represented by a single Gibbs sampling state after
1000 iterations of Gibbs sampling.
Table 1: Statistics for models used as examples.
Name Docs Tokens Vocab Topics
News 1.8M 76M 121k 1000
Blogs 13k 2.2M 90k 100
Parliament 540k 55M 52k 300
New York Times articles. Figure 3 shows three
groupings of words for the middle-left topic in Fig-
ure 1: by document, by month of publication (e.g.
May of 2005), and by desk (e.g. Editorial, Foreign,
Financial). Instantaneous mutual information values
are significantly smaller for the larger groupings, but
the actual values are still larger than expected under
the model. We are interested in measuring the de-
gree to which word usage varies within topics as a
function of both time and the perspective of the ar-
ticle. For example, we may expect that word choice
may differ between opinion articles, which overtly
reflect an author?s views, and news articles, which
take a more objective, factual approach.
We summarize each grouping by plotting the dis-
tribution of deviance scores for all topics. Results
for all 1000 topics grouped by documents, months,
and desks are shown in Figure 4.
4http://www.hansard-archive.parliament.uk/
233
Deviance
Gro
upi
ng
Documents
Months
Desks
l lll l l ll ll l ll l
lll lll llll l ll ll lll lll ll
l lll lll l l lll l l ll lll l ll l
0 100 200 300 400
Figure 4: News: Lack of fit correlates best with desks.
We calculate the number of standard deviations between
the mean replicated discrepancy and the actual discrep-
ancy for each topic under three groupings. Boxes repre-
sent typical ranges, points represent outliers.
Month
Sco
re
0.00000.0005
0.00100.0015
0.00000.0005
0.00100.0015
0e+002e?04
4e?046e?04
0.00000.0005
0.00100.0015
0.00200.0025
0e+002e?04
4e?046e?04
8e?04
?2e?040e+00
2e?044e?04
6e?04
Kurdish
Hussein
Sunni
Sadr
Maliki
Shiite
1987 1992 1997 2002 2007
Figure 5: News: Events change word distributions.
Words with the largest MI from a topic on Iraq?s gov-
ernment are shown, with individual scores grouped by
month.
Finally, we can analyze how individual words in-
teract with groupings like time or desk. Figure 5
breaks down the per-word discrepancy shown in Fig-
ure 3 by month, for the words with the largest overall
discrepancy. Kurdish is prominent during the Gulf
War and the 1996 cruise missile strikes, but is less
significant during the Iraq War. Individuals (Hus-
sein, Sadr, and Maliki) move on and off the stage.
Political blogs. The CMU 2008 political blog cor-
pus consists of six blogs, three of which supported
Barack Obama and three of which supported John
McCain. This corpus has previously been consid-
ered in the context of aspect-based topic models
(Ahmed and Xing, 2010) that assign distinct word
distributions to liberal and conservative bloggers. It
is reasonable to expect that blogs with different po-
litical leanings will use measurably different lan-
guage to describe the same themes, suggesting that
there will be systematic deviations from a multino-
mial hypothesis of exchangeability of words within
topics. Indeed, Ahmed and Xing obtained improved
results with such a model. Figure 6 shows the dis-
tribution of standard deviations from the mean repli-
cated value for a set of 150 topics grouped by doc-
ument, blog, and preferred candidate. Deviance is
greatest for blogs, followed by candidates and then
documents.
Deviance
Gro
upi
ng
Documents
Blogs
Candidates
l
lll
lll lll
0 100 200 300 400
Figure 6: Blogs: Lack of fit correlates more with blog
than preferred candidate. Grouping by preferred can-
didate has only slightly higher average deviance than by
documents, but the variance is greater.
Grouping by blogs appears to show greater de-
viance from mean replicated values than group-
ing by candidates, indicating that there is fur-
ther structure in word choice beyond a simple lib-
eral/conservative split. Are these results, however,
comparable? It may be that this difference is ex-
plained by the fact that there are six blogs and only
234
two candidates. To determine whether this particular
assignment of documents to blogs is responsible for
the difference in discrepancy functions or whether
any such split would have greater deviance, we com-
pared random groupings to the real groupings and
recalculate the PPC. We generated 10 such group-
ings by permuting document blog labels and another
10 by permuting document candidate labels, each
time holding the topics fixed. The average number
of standard deviations across topics was 6.6 ? 14.4
for permuted ?candidates? compared to 37.9? 39.2
for the real corpus, and 10.6 ? 12.9 for permuted
?blogs? compared to 44.4? 29.6 for real blogs.
British parliament proceedings. The parliament
corpus is divided into 305 volumes, each comprising
about three weeks of debates, with between 600 and
4000 speeches per session. In addition to volumes,
10 Prime Ministers were in office during this period.
Deviance
Gro
upi
ng
Documents
Volumes
PMs
ll
0 100 200 300 400
Figure 7: Parliament: Lack-of-fit correlates with time
(publication volume). Correlation with prime ministers
is not significantly better than with volume.
Grouping by prime minister shows greater av-
erage deviance than grouping by volumes, even
though there are substantially fewer divisions. Al-
though such results would need to be accompanied
by permutation experiments as in the blog corpus,
this methodology may be of interest to historians.
In order to provide insight into the nature of tem-
poral variation, we can group the terms in the sum-
mation in Equation 1 by word and rank the words by
their contribution to the discrepancy function. Fig-
ure 8 shows the most ?mismatching? words for a
topic with the most probable words ships, vessels,
admiralty, iron, ship, navy, consistent with changes
in naval technology during the Victorian era (that
is, wooden ships to ?iron clads?). Words that oc-
cur more prominently in the topic (ships, vessels)
are also variable, but more consistent across time.
Volume
Sco
re
0.00000.0005
0.00100.0015
0.00000.0005
0.00100.0015
0.00000.0005
0.00100.0015
0.00000.0005
0.00100.0015
0.00000.0005
0.00100.0015
0.00000.0005
0.00100.0015
iron
turret
clads
wooden
vessels
ships
1830 1835 1840 1845 1850 1855 1860 1865 1870 1875 1880 1885 1890
Figure 8: Parliament: iron-clads introduced in 1860s.
High probability words (ships, vessels) are variable, but
show less concentrated discrepancy than iron, wooden.
5 Calibration on Synthetic Data
A posterior predictive check asks ?do observations
sampled from the learned model look like the origi-
nal data?? In the previous sections, we have consid-
ered PPCs that explore variability within a topic on
a per-word basis, measure discrepancy at the topic
level, and compare deviance over all topics between
groupings of documents. Those results show that
the PPC detects deviation from multinomial assump-
tions when it exists: as expected, variability in word
choice aligns with known divisions in corpora, for
example by time and author perspective. We now
consider the opposite direction. When documents
are generated from a multinomial topic model, PPCs
should not detect systematic deviation.
We must also distinguish between lack of fit due
to model misspecification and lack of fit due to ap-
proximate inference. In this section, we present syn-
thetic data experiments where the learned model is
precisely the model used to generate documents. We
show that there is significant lack of fit introduced
by approximate inference, which can be corrected
by considering only parts of the model that are well-
estimated.
We generated 10 synthetic corpora, each consist-
ing of 100,000 100-word documents, drawn from 20
235
pco
un
t
0
10
20
30
40
All
0.0 0.2 0.4 0.6 0.8 1.0
TopDocs
0.0 0.2 0.4 0.6 0.8 1.0
TopWords
0.0 0.2 0.4 0.6 0.8 1.0
TopWordsDocs
0.0 0.2 0.4 0.6 0.8 1.0
Figure 9: Replicating only documents with large allocation in the topic leads to more uniform p-values. p-values
for 200 topics estimated from synthetic data generated from an LDA model are either uniform or skewed towards 1.0.
Overly conservative p-values would be clustered around 0.5.
topics over a vocabulary of 100 terms. Hyperpa-
rameters for both the document-topic and topic-term
Dirichlet priors were 0.1 for each dimension. We
then trained a topic model with the same hyperpa-
rameters and number of topics on each corpus, sav-
ing a Gibbs sampling state.
We can measure the fit of a PPC by examining the
distribution of empirical p-values, that is, the propor-
tion of replications wrep that result in discrepancies
less than the observed value. p-values should be uni-
formly distributed on (0, 1). Non-uniform p-values
indicate a lack of calibration. Unlike real collec-
tions, in synthetic corpora the range of discrepan-
cies from these replicated collections often includes
the real values, so p-values are meaningful. A his-
togram of p-values for 200 synthetic topics after 100
replications is shown in the left panel of Figure 9.
PPCs have been criticized for reusing training
data for model checking. For some models, the
posterior distribution is too close to the data, so all
replicated values are close to the real value, leading
to p-values clustered around 0.5 (Draper and Krn-
jajic, 2006; Bayarri and Castellanos, 2007). We
test divergence from a uniform distribution with a
Kolmogorov-Smirnov test. Our results indicate that
LDA is not overfitting, but that the distribution is not
uniform (KS p < 0.00001).
The PPC framework allows us to choose discrep-
ancy functions that reflect the relative importance
of subsets of words and documents. The second
panel in Figure 9 sums only over the 20 documents
with the largest probability of the topic, the third
sums over all documents but only over the top 10
most probable words, and the fourth sums over only
the top words and documents. This test indicates
that the distribution of p-values for the subset Top-
Words is not uniform (KS p < 0.00001), but that a
uniform distribution is a good fit for TopDocs (KS
p = 0.358) and TopWordsDocs (KS p = 0.069).
6 Conclusions
We have developed a Bayesian model checking
method for probabilistic topic models. Conditioned
on their topic assignment, the words of the docu-
ments are independently and identically distributed
by a multinomial distribution. We developed a real-
ized discrepancy function?the mutual information
between words and document indices, conditioned
on a topic?that checks this assumption. We em-
bedded this function in a posterior predictive check.
We demonstrated that we can use this posterior
predictive check to identify particular topics that fit
the data, and particular topics that misfit the data in
different ways. Moreover, our method provides a
new way to visualize topic models.
We adapted the method to corpora with external
variables. In this setting, the PPC provides a way to
guide the modeler in searching through more com-
plicated models that involve more variables.
Finally, on simulated data, we demonstrated that
PPCs with the mutual information discrepancy func-
tion can identify model fit and model misfit.
Acknowledgments
David M. Blei is supported by ONR 175-6343, NSF
CAREER 0745520, AFOSR 09NL202, the Alfred P.
Sloan foundation, and a grant from Google. David
Mimno is supported by a Digital Humanities Re-
search grant from Google. Arthur Spirling and Andy
236
Eggers suggested the use of the Hansards corpus.
References
Amr Ahmed and Eric Xing. 2010. Staying informed: Su-
pervised and semi-supervised multi-view topical anal-
ysis of ideological perspective. In EMNLP.
Arthur Asuncion, Padhraic Smyth, and Max Welling.
2008. Asynchronous distributed learning of topic
models. In NIPS.
M.J. Bayarri and M.E. Castellanos. 2007. Bayesian
checking of the second levels of hierarchical models.
Statistical Science, 22(3):322?343.
David M. Blei and John D. Lafferty. 2006. Dynamic
topic models. In ICML.
David Blei, Andrew Ng, and Michael Jordan. 2003. La-
tent Dirichlet alocation. Journal of Machine Learning
Research, 3:993?1022, January.
Jonathan Chang, Jordan Boyd-Graber, Chong Wang,
Sean Gerrish, and David M. Blei. 2009. Reading tea
leaves: How humans interpret topic models. In Ad-
vances in Neural Information Processing Systems 22,
pages 288?296.
Gabriel Doyle and Charles Elkan. 2009. Accounting for
burstiness in topic models. In ICML.
David Draper and Milovan Krnjajic. 2006. Bayesian
model specification. Technical report, University of
California, Santa Cruz.
Jacob Eisenstein and Eric Xing. 2010. The CMU 2008
political blog corpus. Technical report, Carnegie Mel-
lon University.
A. Gelman, X.L. Meng, and H.S. Stern. 1996. poste-
rior predictive assessment of model fitness via realized
discrepancies. Statistica Sinica, 6:733?807.
Thomas L. Griffiths and Mark Steyvers. 2004. Finding
scientific topics. PNAS, 101(suppl. 1):5228?5235.
Matthew Hoffman, David Blei, and Francis Bach. 2010.
Online learning for latent dirichlet alocation. In NIPS.
Wei-Hao Lin, Eric Xing, and Alexander Hauptmann.
2008. A joint topic and perspective model for ideo-
logical discourse. In PKDD.
Qiaozhu Mei and ChengXiang Zhai. 2006. A mixture
model for contextual text mining. In KDD.
David Mimno and Andrew McCallum. 2007. Organizing
the OCA: learning faceted subjects from a library of
digital books. In JCDL.
David Newman, Jey Han Lau, Karl Grieser, and Timothy
Baldwin. 2010. Automatic evaluation of topic coher-
ence. In Human Language Technologies: The Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics.
Michael J. Paul, ChengXiang Zhai, and Roxana Girju.
2010. Summarizing contrastive viewpoints in opin-
ionated text. In EMNLP.
Donald B. Rubin. 1981. Estimation in parallel random-
ized experiments. Journal of Educational Statistics,
6:377?401.
D. Rubin. 1984. Bayesianly justifiable and relevant fre-
quency calculations for the applied statistician. The
Annals of Statistics, 12(4):1151?1172.
Hanna Wallach, Iain Murray, Ruslan Salakhutdinov, and
David Mimno. 2009. Evaluation methods for topic
models. In ICML.
237
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 262?272,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Optimizing Semantic Coherence in Topic Models
David Mimno
Princeton University
Princeton, NJ 08540
mimno@cs.princeton.edu
Hanna M. Wallach
University of Massachusetts, Amherst
Amherst, MA 01003
wallach@cs.umass.edu
Edmund Talley Miriam Leenders
National Institutes of Health
Bethesda, MD 20892
{talleye,leenderm}@ninds.nih.gov
Andrew McCallum
University of Massachusetts, Amherst
Amherst, MA 01003
mccallum@cs.umass.edu
Abstract
Latent variable models have the potential
to add value to large document collections
by discovering interpretable, low-dimensional
subspaces. In order for people to use such
models, however, they must trust them. Un-
fortunately, typical dimensionality reduction
methods for text, such as latent Dirichlet al
location, often produce low-dimensional sub-
spaces (topics) that are obviously flawed to
human domain experts. The contributions of
this paper are threefold: (1) An analysis of the
ways in which topics can be flawed; (2) an au-
tomated evaluation metric for identifying such
topics that does not rely on human annotators
or reference collections outside the training
data; (3) a novel statistical topic model based
on this metric that significantly improves topic
quality in a large-scale document collection
from the National Institutes of Health (NIH).
1 Introduction
Statistical topic models such as latent Dirichlet al
location (LDA) (Blei et al, 2003) provide a pow-
erful framework for representing and summarizing
the contents of large document collections. In our
experience, however, the primary obstacle to accep-
tance of statistical topic models by users the outside
machine learning community is the presence of poor
quality topics. Topics that mix unrelated or loosely-
related concepts substantially reduce users? confi-
dence in the utility of such automated systems.
In general, users prefer models with larger num-
bers of topics because such models have greater res-
olution and are able to support finer-grained distinc-
tions. Unfortunately, we have observed that there
is a strong relationship between the size of topics
and the probability of topics being nonsensical as
judged by domain experts: as the number of topics
increases, the smallest topics (number of word to-
kens assigned to each topic) are almost always poor
quality. The common practice of displaying only a
small number of example topics hides the fact that as
many as 10% of topics may be so bad that they can-
not be shown without reducing users? confidence.
The evaluation of statistical topic models has tra-
ditionally been dominated by either extrinsic meth-
ods (i.e., using the inferred topics to perform some
external task such as information retrieval (Wei
and Croft, 2006)) or quantitative intrinsic methods,
such as computing the probability of held-out doc-
uments (Wallach et al, 2009). Recent work has
focused on evaluation of topics as semantically-
coherent concepts. For example, Chang et al (2009)
found that the probability of held-out documents is
not always a good predictor of human judgments.
Newman et al (2010) showed that an automated
evaluation metric based on word co-occurrence
statistics gathered from Wikipedia could predict hu-
man evaluations of topic quality. AlSumait et al
(2009) used differences between topic-specific dis-
tributions over words and the corpus-wide distribu-
tion over words to identify overly-general ?vacuous?
topics. Finally, Andrzejewski et al (2009) devel-
oped semi-supervised methods that avoid specific
user-labeled semantic coherence problems.
The contributions of this paper are threefold: (1)
To identify distinct classes of low-quality topics,
some of which are not flagged by existing evalua-
tion methods; (2) to introduce a new topic ?coher-
ence? score that corresponds well with human co-
herence judgments and makes it possible to identify
262
specific semantic problems in topic models without
human evaluations or external reference corpora; (3)
to present an example of a new topic model that
learns latent topics by directly optimizing a metric
of topic coherence. With little additional computa-
tional cost beyond that of LDA, this model exhibits
significant gains in average topic coherence score.
Although the model does not result in a statistically-
significant reduction in the number of topics marked
?bad?, the model consistently improves the topic co-
herence score of the ten lowest-scoring topics (i.e.,
results in bad topics that are ?less bad? than those
found using LDA) while retaining the ability to iden-
tify low-quality topics without human interaction.
2 Latent Dirichlet Allocation
LDA is a generative probabilistic model for docu-
mentsW = {w(1),w(2), . . . ,w(D)}. To generate a
word token w(d)n in document d, we draw a discrete
topic assignment z(d)n from a document-specific dis-
tribution over the T topics ?d (which is itself drawn
from a Dirichlet prior with hyperparameter ?), and
then draw a word type for that token from the topic-
specific distribution over the vocabulary ?z(d)n . Theinference task in topic models is generally cast as in-
ferring the document?topic proportions {?1, ...,?D}
and the topic-specific distributions {?1 . . . ,?T }.
The multinomial topic distributions are usually
drawn from a shared symmetric Dirichlet prior with
hyperparameter ?, such that conditioned on {?t}Tt=1
and the topic assignments {z(1), z(2), . . . ,z(D)},
the word tokens are independent. In practice, how-
ever, it is common to deal directly with the ?col-
lapsed? distributions that result from integrating
over the topic-specific multinomial parameters. The
resulting distribution over words for a topic t is then
a function of the hyperparameter ? and the number
of words of each type assigned to that topic, Nw|t.
This distribution, known as the Dirichlet compound
multinomial (DCM) or Po?lya distribution (Doyle
and Elkan, 2009), breaks the assumption of condi-
tional independence between word tokens given top-
ics, but is useful during inference because the con-
ditional probability of a word w given topic t takes
a very simple form: P (w | t, ?) = Nw|t+?Nt+|V|? , where
Nt =
?
w? Nw?|t and |V| is the vocabulary size.
The process for generating a sequence of words
from such a model is known as the simple Po?lya urn
model (Mahmoud, 2008), in which the initial prob-
ability of word type w in topic t is proportional to
?, while the probability of each subsequent occur-
rence of w in topic t is proportional to the number
of times w has been drawn in that topic plus ?. Note
that this unnormalized weight for each word type de-
pends only on the count of that word type, and is in-
dependent of the count of any other word type w?.
Thus, in the DCM/Po?lya distribution, drawing word
type w must decrease the probability of seeing all
other word types w? 6= w. In a later section, we will
introduce a topic model that substitutes a general-
ized Po?lya urn model for the DCM/Po?lya distribu-
tion, allowing a draw of word type w to increase the
probability of seeing certain other word types.
For real-world data, documents W are observed,
while the corresponding topic assignments Z are
unobserved and may be inferred using either vari-
ational methods (Blei et al, 2003; Teh et al, 2006)
or MCMC methods (Griffiths and Steyvers, 2004).
Here, we use MCMC methods?specifically Gibbs
sampling (Geman and Geman, 1984), which in-
volves sequentially resampling each topic assign-
ment z(d)n from its conditional posterior given the
documents W , the hyperparameters ? and ?, and
Z\d,n (the current topic assignments for all tokens
other than the token at position n in document d).
3 Expert Opinions of Topic Quality
Concentrating on 300,000 grant and related jour-
nal paper abstracts from the National Institutes of
Health (NIH), we worked with two experts from
the National Institute of Neurological Disorders and
Stroke (NINDS) to collaboratively design an expert-
driven topic annotation study. The goal of this study
was to develop an annotated set of baseline topics,
along with their salient characteristics, as a first step
towards automatically identifying and inferring the
kinds of topics desired by domain experts.1
3.1 Expert-Driven Annotation Protocol
In order to ensure that the topics selected for anno-
tation were within the NINDS experts? area of ex-
pertise, they selected 148 topics (out of 500), all as-
sociated with areas funded by NINDS. Each topic
1All evaluated models will be released publicly.
263
t was presented to the experts as a list of the thirty
most probable words for that topic, in descending or-
der of their topic-specific ?collapsed? probabilities,
Nw|t+?
Nt+|V|? . In addition to the most probable words,the experts were also given metadata for each topic:
The most common sequences of two or more con-
secutive words assigned to that topic, the four topics
that most often co-occurred with that topic, the most
common IDF-weighted words from titles of grants,
thesaurus terms, NIH institutes, journal titles, and
finally a list of the highest probability grants and
PubMed papers for that topic.
The experts first categorized each topic as one
of three types: ?research?, ?grant mechanisms and
publication types? or ?general?.2 The quality of
each topic (?good?, ?intermediate?, or ?bad?) was
then evaluated using criteria specific to the type
of topic. In general, topics were only annotated
as ?good? if they contained words that could be
grouped together as a single coherent concept. Addi-
tionally, each ?research? topic was only considered
to be ?good? if, in addition to representing a sin-
gle coherent concept, the aggregate content of the
set of documents with appreciable allocations to that
topic clearly contained text referring to the concept
inferred from the topic words. Finally, for each topic
marked as being either ?intermediate? or ?bad?, one
or more of the following problems (defined by the
domain experts) was identified, as appropriate:
? Chained: every word is connected to every
other word through some pairwise word chain,
but not all word pairs make sense. For exam-
ple, a topic whose top three words are ?acids?,
?fatty? and ?nucleic? consists of two distinct
concepts (i.e., acids produced when fats are
broken down versus the building blocks of
DNA and RNA) chained via the word ?acids?.
? Intruded: either two or more unrelated sets
of related words, joined arbitrarily, or an oth-
erwise good topic with a few ?intruder? words.
? Random: no clear, sensical connections be-
tween more than a few pairs of words.
? Unbalanced: the top words are all logically
connected to each other, but the topic combines
very general and specific terms (e.g., ?signal
2Equivalent to ?vacuous topics? of AlSumait et al (2009).
transduction? versus ?notch signaling?).
Examples of a good general topic, a good research
topic, and a chained research topic are in Table 1.
3.2 Annotation Results
The experts annotated the topics independently and
then aggregated their results. Interestingly, no top-
ics were ever considered ?good? by one expert and
?bad? by the other?when there was disagreement
between the experts, one expert always believed the
topic to be ?intermediate.? In such cases, the ex-
perts discussed the reasons for their decisions and
came to a consensus. Of the 148 topics selected for
annotation, 90 were labeled as ?good,? 21 as ?inter-
mediate,? and 37 as ?bad.? Of the topics labeled as
?bad? or ?intermediate,? 23 were ?chained,? 21 were
?intruded,? 3 were ?random,? and 15 were ?unbal-
anced?. (The annotators were permitted to assign
more than one problem to any given topic.)
4 Automated Metrics for Predicting
Expert Annotations
The ultimate goal of this paper is to develop meth-
ods for building models with large numbers of spe-
cific, high-quality topics from domain-specific cor-
pora. We therefore explore the extent to which in-
formation already contained in the documents being
modeled can be used to assess topic quality.
In this section we evaluate several methods for
ranking the quality of topics and compare these
rankings to human annotations. No method is likely
to perfectly predict human judgments, as individual
annotators may disagree on particular topics. For
an application involving removing low quality top-
ics we recommend using a weighted combination of
metrics, with a threshold determined by users.
4.1 Topic Size
As a simple baseline, we considered the extent to
which topic ?size? (as measured by the number of
tokens assigned to each topic via Gibbs sampling) is
a good metric for assessing topic quality. Figure 1
(top) displays the topic size (number of tokens as-
signed to that topic) and expert annotations (?good?,
?intermediate?, ?bad?) for the 148 topics manually
labeled by annotators as described above. This fig-
ure suggests that topic size is a reasonable predic-
264
lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll
40000 60000 80000 120000 160000
Tokens
goo
d
inte
r
bad
lllllllllllllllllllllllllllllllllllllllllllllllllll
?600 ?500 ?400 ?300 ?200
Coherence
goo
d
inte
r
bad
Figure 1: Topic size is a good indicator of quality; the
new coherence metric is better. Top shows expert-rated
topics ranked by topic size (AP 0.89, AUC 0.79), bottom
shows same topics ranked by coherence (AP 0.94, AUC
0.87). Random jitter is added to the y-axis for clarity.
tor of topic quality. Although there is some overlap,
?bad? topics are generally smaller than ?good? top-
ics. Unfortunately, this observation conflicts with
the goal of building highly specialized, domain-
specific topic models with many high-quality, fine-
grained topics?in such models the majority of top-
ics will have relatively few tokens assigned to them.
4.2 Topic Coherence
When displaying topics to users, each topic t is gen-
erally represented as a list of theM=5, . . . , 20 most
probable words for that topic, in descending order
of their topic-specific ?collapsed? probabilities. Al-
though there has been previous work on automated
generation of labels or headings for topics (Mei et
al., 2007), we choose to work only with the ordered
list representation. Labels may obscure or detract
from fundamental problems with topic coherence,
and better labels don?t make bad topics good.
The expert-driven annotation study described in
section 3 suggests that three of the four types of
poor-quality topics (?chained,? ?intruded? and ?ran-
dom?) could be detected using a metric based on
the co-occurrence of words within the documents
being modeled. For ?chained? and ?intruded? top-
ics, it is likely that although pairs of words belong-
ing to a single concept will co-occur within a single
document (e.g., ?nucleic? and ?acids? in documents
about DNA), word pairs belonging to different con-
cepts (e.g., ?fatty? and ?nucleic?) will not. For ran-
dom topics, it is likely that few words will co-occur.
This insight can be used to design a new metric
for assessing topic quality. Letting D(v) be the doc-
ument frequency of word type v (i.e., the number
of documents with least one token of type v) and
D(v, v?) be co-document frequency of word types v
and v? (i.e., the number of documents containing one
or more tokens of type v and at least one token of
type v?), we define topic coherence as
C(t;V (t)) =
M?
m=2
m?1?
l=1
log D(v
(t)
m , v(t)l ) + 1
D(v(t)l )
, (1)
where V (t) =(v(t)1 , . . . , v(t)M ) is a list of the M most
probable words in topic t. A smoothing count of 1
is included to avoid taking the logarithm of zero.
Figure 1 shows the association between the expert
annotations and both topic size (top) and our coher-
ence metric (bottom). We evaluate these results us-
ing standard ranking metrics, average precision and
the area under the ROC curve. Treating ?good? top-
ics as positive and ?intermediate? or ?bad? topics as
negative, we get average precision values of 0.89 for
topic size vs. 0.94 for coherence and AUC 0.79 for
topic size vs. 0.87 for coherence. We performed a
logistic regression analysis on the binary variable ?is
this topic bad?. Using topic size alone as a predic-
tor gives AIC (a measure of model fit) 152.5. Co-
herence alone has AIC 113.8 (substantially better).
Both predictors combined have AIC 115.8: the sim-
pler coherence alone model provides the best perfor-
mance. We tried weighting the terms in equation 1
by their corresponding topic?word probabilities and
and by their position in the sorted list of the M most
probable words for that topic, but we found that a
uniform weighting better predicted topic quality.
Our topic coherence metric also exhibits good
qualitative behavior: of the 20 best-scoring topics,
18 are labeled as ?good,? one is ?intermediate? (?un-
balanced?), and one is ?bad? (combining ?cortex?
and ?fmri?, words that commonly co-occur, but are
conceptually distinct). Of the 20 worst scoring top-
ics, 15 are ?bad,? 4 are ?intermediate,? and only one
(with the 19th worst coherence score) is ?good.?
265
Our coherence metric relies only upon word co-
occurrence statistics gathered from the corpus being
modeled, and does not depend on an external ref-
erence corpus. Ideally, all such co-occurrence infor-
mation would already be accounted for in the model.
We believe that one of the main contributions of our
work is demonstrating that standard topic models
do not fully utilize available co-occurrence informa-
tion, and that a held-out reference corpus is therefore
not required for purposes of topic evaluation.
Equation 1 is very similar to pointwise mutual in-
formation (PMI), but is more closely associated with
our expert annotations than PMI (which achieves
AUC 0.64 and AIC 170.51). PMI has a long history
in language technology (Church and Hanks, 1990),
and was recently used by Newman et al (2010) to
evaluate topic models. When expressed in terms of
count variables as in equation 1, PMI includes an
additional term for D(v(t)m ). The improved perfor-
mance of our metric over PMI implies that what mat-
ters is not the difference between the joint probabil-
ity of words m and l and the product of marginals,
but the conditional probability of each word given
the each of the higher-ranked words in the topic.
In order to provide intuition for the behavior of
our topic coherence metric, table 1 shows three
example topics and their topic coherence scores.
The first topic, related to grant-funded training pro-
grams, is one of the best-scoring topics. All pairs
of words have high co-document frequencies. The
second topic, on neurons, is more typical of qual-
ity ?research? topics. Overall, these words occur
less frequently, but generally occur moderately in-
terchangeably: there is little structure to their co-
variance. The last topic is one of the lowest-scoring
topics. Its co-document frequency matrix is shown
in table 2. The top two words are closely related:
487 documents include ?aging? at least once, 122
include ?lifespan?, and 55 include both. Meanwhile,
the third word ?globin? occurs with only one of the
top seven words?the common word ?human?.
4.3 Comparison to word intrusion
As an additional check for both our expert annota-
tions and our automated metric, we replicated the
?word intrusion? evaluation originally introduced by
Chang et al (2009). In this task, one of the top ten
most probable words in a topic is replaced with a
l lll l lll ll l lll ll l lll lll l lll ll lll l l ll l llll
l
ll l lll l lll ll l lll
l
l ll ll lll ll
l
l l
l
l
ll l
40000 60000 80000 120000 160000
0
4
8
Comparison of Topic Size to Intrusion Detection
Tokens assigned to topic
Cor
rect
 Gu
ess
es
ll ll ll ll l ll ll ll l ll ll llll l lll llll ll ll ll lll l
l
l lll lllll ll l ll
l
lll ll ll ll
l
l l
l
l
ll l
?600 ?500 ?400 ?300 ?200
0
4
8
Comparison of Coherence to Intrusion Detection
Coherence
Cor
rect
 Gu
ess
es
Good Topics
Correct Guesses
Fre
que
ncy
0 2 4 6 8 10
0
15
35
Bad Topics
Correct Guesses
Fre
que
ncy
0 2 4 6 8 10
0
15
35
Figure 2: Top: results of the intruder selection task rel-
ative to two topic quality metrics. Bottom: marginal in-
truder accuracy frequencies of good and bad topics.
another word, selected at random from the corpus.
The resulting set of words is presented, in a random
order, to users, who are asked to identify the ?in-
truder? word. It is very unlikely that a randomly-
chosen word will be semantically related to any of
the original words in the topic, so if a topic is a
high quality representation of a semantically coher-
ent concept, it should be easy for users to select the
intruder word. If the topic is not coherent, there may
be words in the topic that are also not semantically
related to any other word, thus causing users to se-
lect ?correct? words instead of the real intruder.
We recruited ten additional expert annotators
from NINDS, not including our original annotators,
and presented them with the intruder selection task,
using the set of previously evaluated topics. Re-
sults are shown in figure 2. In the first two plots,
the x-axis is one of our two automated quality met-
266
Table 1: Example topics (good/general, good/research, chained/research) with different coherence scores (numbers
closer to zero indicate higher coherence). The chained topic combines words related to aging (indicated in plain text)
and words describing blood and blood-related diseases (bold). The only connection is the common word human.
-167.1 students, program, summer, biomedical, training, experience, undergraduate, career, minority, student, ca-
reers, underrepresented, medical students, week, science
-252.1 neurons, neuronal, brain, axon, neuron, guidance, nervous system, cns, axons, neural, axonal, cortical,
survival, disorders, motor
-357.2 aging, lifespan, globin, age related, longevity, human, age, erythroid, sickle cell, beta globin, hb, senes-
cence, adult, older, lcr
Table 2: Co-document frequency matrix for the top words in a low-quality topic (according to our coherence metric),
shaded to highlight zeros. The diagonal (light gray) shows the overall document frequency for each word w. The
column on the right is Nw|t. Note that ?globin? and ?erythroid? do not co-occur with any of the aging-related words.
aging 487 53 0 65 42 0 51 0 138 0 914
lifespan 53 122 0 15 28 0 15 0 44 0 205
globin 0 0 39 0 0 19 0 15 27 3 200
age related 65 15 0 119 12 0 25 0 37 0 160
longevity 42 28 0 12 73 0 6 0 20 1 159
erythroid 0 0 19 0 0 69 0 8 23 1 110
age 51 15 0 25 6 0 245 1 82 0 103
sickle cell 0 0 15 0 0 8 1 43 16 2 93
human 138 44 27 37 20 23 82 16 4347 157 91
hb 0 0 3 0 1 1 0 2 5 15 73
267
rics (topic size and coherence) and the y-axis is the
number of annotators that correctly identified the
true intruder word (accuracy). The histograms be-
low these plots show the number of topics with each
level of annotator accuracy for good and bad top-
ics. For good topics (green circles), the annotators
were generally able to detect the intruder word with
high accuracy. Bad topics (red diamonds) had more
uniform accuracies. These results suggest that top-
ics with low intruder detection accuracy tend to be
bad, but some bad topics can have a high accuracy.
For example, spotting an intruder word in a chained
topic can be easy. The low-quality topic recep-
tors, cannabinoid, cannabinoids, ligands, cannabis,
endocannabinoid, cxcr4, [virus], receptor, sdf1, is
a typical ?chained? topic, with CXCR4 linked to
cannabinoids only through receptors, and otherwise
unrelated. Eight out of ten annotators correctly iden-
tified ?virus? as the correct intruder. Repeating the
logistic regression experiment using intruder detec-
tion accuracy as input, the AIC value is 163.18?
much worse than either topic size or coherence.
5 Generalized Po?lya Urn Models
Although the topic coherence metric defined above
provides an accurate way of assessing topic quality,
preventing poor quality topics from occurring in the
first place is preferable. Our results in the previous
section show that we can identify low-quality top-
ics without making use of external supervision; the
training data by itself contains sufficient information
at least to reject poor combinations of words.
In this section, we describe a new topic model that
incorporates the corpus-specific word co-occurrence
information used in our coherence metric directly
into the statistical topic modeling framework. It
is important to note that simply disallowing words
that never co-occur from being assigned to the same
topic is not sufficient. Due to the power-law charac-
teristics of language, most words are rare and will
not co-occur with most other words regardless of
their semantic similarity. It is rather the degree
to which the most prominent words in a topic do
not co-occur with the other most prominent words
in that topic that is an indicator of topic incoher-
ence. We therefore desire models that guide topics
towards semantic similarity without imposing hard
constraints.
As an example of such a model, we present a new
topic model in which the occurrence of word type w
in topic t increases not only the probability of seeing
that word type again, but also increases the probabil-
ity of seeing other related words (as determined by
co-document frequencies for the corpus being mod-
eled). This new topic model retains the document?
topic component of standard LDA, but replaces the
usual Po?lya urn topic?word component with a gen-
eralized Po?lya urn framework (Mahmoud, 2008).
A sequence of i.i.d. samples from a discrete dis-
tribution can be imagined as arising by repeatedly
drawing a random ball from an urn, where the num-
ber of balls of each color is proportional to the prob-
ability of that color, replacing the selected ball af-
ter each draw. In a Po?lya urn, each ball is replaced
along with another ball of the same color. Samples
from this model exhibit the ?burstiness? property:
the probability of drawing a ball of colorw increases
each time a ball of that color is drawn. This process
represents the marginal distribution of a hierarchical
model with a Dirichlet prior and a multinomial like-
lihood, and is used as the distribution over words
for each topic in almost all previous topic models.
In a generalized Po?lya urn model, having drawn a
ball of color w, Avw additional balls of each color
v ? {1, . . . ,W} are returned to the urn. Given W
and Z , the conditional posterior probability of word
w in topic t implied by this generalized model is
P (w | t,W,Z, ?,A) =
?
vNv|tAvw + ?
Nt + |V|?
, (2)
where A is a W ? W real-valued matrix, known
as the addition matrix or schema. The simple Po?lya
urn model (and hence the conditional posterior prob-
ability of word w in topic t under LDA) can be re-
covered by setting the schema A to the identity ma-
trix. Unlike the simple Po?lya distribution, we do not
know of a representation of the generalized Po?lya
urn distribution that can be expressed using a con-
cise set of conditional independence assumptions. A
standard graphical model with plate notation would
therefore not be helpful in highlighting the differ-
ences between the two models, and is not shown.
Algorithm 1 shows pseudocode for a single Gibbs
sweep over the latent variables Z in standard LDA.
Algorithm 2 shows the modifications necessary to
268
1: for d ? D do
2: for wn ? w(d) do
3: Nzi|di ? Nzi|di ? 1
4: Nwi|zi ? Nwi|zi ? 1
5: sample zi ? (Nz|di + ?z)
Nwi|z+??
z? (Nwi|z?+?)6: Nzi|di ? Nzi|di + 1
7: Nwi|zi ? Nwi|zi + 1
8: end for
9: end for
Algorithm 1: One sweep of LDA Gibbs sampling.
1: for d ? D do
2: for wn ? w(d) do
3: Nzi|di ? Nzi|di ? 1
4: for all v do
5: Nv|zi ? Nv|zi ?Avwi
6: end for
7: sample zi ? (Nz|di + ?z)
Nwi|z+??
z? (Nwi|z?+?)8: Nzi|di ? Nzi|di + 1
9: for all v do
10: Nv|zi ? Nv|zi +Avwi
11: end for
12: end for
13: end for
Algorithm 2: One sweep of gen. Po?lya Gibbs sam-
pling, with differences from LDA highlighted in red.
support a generalized Po?lya urn model: rather than
subtracting exactly one from the count of the word
given the old topic, sampling, and then adding one
to the count of the word given the new topic, we sub-
tract a column of the schema matrix from the entire
count vector over words for the old topic, sample,
and add the same column to the count vector for the
new topic. As long as A is sparse, this operation
adds only a constant factor to the computation.
Another property of the generalized Po?lya urn
model is that it is nonexchangeable?the joint prob-
ability of the tokens in any given topic is not invari-
ant to permutation of those tokens. Inference of Z
givenW via Gibbs sampling involves repeatedly cy-
cling through the tokens in W and, for each one,
resampling its topic assignment conditioned on W
and the current topic assignments for all tokens other
than the token of interest. For LDA, the sampling
distribution for each topic assignment is simply the
product of two predictive probabilities, obtained by
treating the token of interest as if it were the last.
For a topic model with a generalized Po?lya urn for
the topic?word component, the sampling distribu-
tion is more complicated. Specifically, the topic?
word component of the sampling distribution is no
longer a simple predictive distribution?when sam-
pling a new value for z(d)n , the implication of each
possible value for subsequent tokens and their topic
assignments must be considered. Unfortunately, this
can be very computationally expensive, particularly
for large corpora. There are several ways around this
problem. The first is to use sequential Monte Carlo
methods, which have been successfully applied to
topic models previously (Canini et al, 2009). The
second approach is to approximate the true Gibbs
sampling distribution by treating each token as if it
were the last, ignoring implications for subsequent
tokens and their topic assignments. We find that
this approximate method performs well empirically.
5.1 Setting the Schema A
Inspired by our evaluation metric, we define A as
Avv ? ?vD(v) (3)
Avw ? ?vD(w, v)
where each element is scaled by a row-specific
weight ?v and each column is normalized to sum
to 1. Normalizing columns makes comparison to
standard LDA simpler, because the relative effect of
smoothing parameter ?=0.01 is equivalent. We set
?v = log (D/D(v)), the standard IDF weight used
in information retrieval, which is larger for less fre-
quent words. The column for word type w can be
interpreted as word types with significant associa-
tion with w. The IDF weighting therefore has the
effect of increasing the strength of association for
rare word types. We also found empirically that it is
helpful to remove off-diagonal elements for the most
common types, such as those that occur in more than
5% of documents (IDF < 3.0). Including nonzero
off-diagonal values in A for very frequent types
causes the model to disperse those types over many
topics, which leads to large numbers of extremely
similar topics. To measure this effect, we calcu-
lated the Jensen-Shannon divergence between all
pairs of topic?word distributions in a given model.
For a model using off-diagonal weights for all word
269
?
29
0
?
26
0
100 Topics
Co
he
ren
ce
50 300 550 800
?
29
0
?
26
0
200 Topics
Co
he
ren
ce
50 300 550 800
?
29
0
?
26
0
300 Topics
Co
he
ren
ce
50 300 550 800
?
29
0
?
26
0
400 Topics
Co
he
ren
ce
50 300 550 800
?
40
0
?
34
0
10
 W
or
st 
Co
he
r
50 300 550 800
?
40
0
?
34
0
10
 W
or
st 
Co
he
r
50 300 550 800
?
40
0
?
34
0
10
 W
or
st 
Co
he
r
50 300 550 800
?
40
0
?
34
0
10
 W
or
st 
Co
he
r
50 300 550 800
?
17
00
?
16
60
Iteration
HO
LP
50 300 550 800
?
17
00
?
16
60
Iteration
HO
LP
50 300 550 800
?
17
00
?
16
60
Iteration
HO
LP
50 300 550 800
?
17
00
?
16
60
Iteration
HO
LP
50 300 550 800
Figure 3: Po?lya urn topics (blue) have higher average coherence and converge much faster than LDA topics
(red). The top plots show topic coherence (averaged over 15 runs) over 1000 iterations of Gibbs sampling. Error bars
are not visible in this plot. The middle plot shows the average coherence of the 10 lowest scoring topics. The bottom
plots show held-out log probability (in thousands) for the same models (three runs each of 5-fold cross-validation).
Name Docs Avg. Tok. Tokens Vocab
NIH 18756 114.64 ? 30.41 2150172 28702
Table 3: Data set statistics.
types, the mean of the 100 lowest divergences was
0.29 ? .05 (a divergence of 1.0 represents distribu-
tions with no shared support) at T =200. The aver-
age divergence of the 100 most similar pairs of top-
ics for standard LDA (i.e.,A = I) is 0.67?.05. The
same statistic for the generalized Po?lya urn model
without off-diagonal elements for word types with
high document frequency is 0.822? 0.09.
Setting the off-diagonal elements of the schema
A to zero for the most common word types also has
the fortunate effect of substantially reducing prepro-
cessing time. We find that Gibbs sampling for the
generalized Po?lya model takes roughly two to three
times longer than for standard LDA, depending on
the sparsity of the schema, due to additional book-
keeping needed before and after sampling topics.
5.2 Experimental Results
We evaluated the new model on a corpus of NIH
grant abstracts. Details are given in table 3. Figure 3
shows the performance of the generalized Po?lya urn
model relative to LDA. Two metrics?our new topic
coherence metric and the log probability of held-out
documents?are shown over 1000 iterations at 50 it-
eration intervals. Each model was run over five folds
of cross validation, each with three random initial-
izations. For each model we calculated an overall
coherence score by calculating the topic coherence
for each topic individually and then averaging these
values. We report the average over all 15 models in
each plot. Held-out probabilities were calculated us-
ing the left-to-right method of Wallach et al (2009),
with each cross-validation fold using its own schema
A. The generalized Po?lya model performs very well
in average topic coherence, reaching levels within
the first 50 iterations that match the final score. This
model has an early advantage for held-out proba-
bility as well, but is eventually overtaken by LDA.
This trend is consistent with Chang et al?s observa-
tion that held-out probabilities are not always good
predictors of human judgments (Chang et al, 2009).
Results are consistent over T ? {100, 200, 300}.
In section 4.2, we demonstrated that our topic co-
herence metric correlates with expert opinions of
topic quality for standard LDA. The generalized
270
Po?lya urn model was therefore designed with the
goal of directly optimizing that metric. It is pos-
sible, however, that optimizing for coherence di-
rectly could break the association between coher-
ence metric and topic quality. We therefore repeated
the expert-driven evaluation protocol described in
section 3.1. We trained one standard LDA model
and one generalized Po?lya urn model, each with
T = 200, and randomly shuffled the 400 resulting
topics. The topics were then presented to the experts
from NINDS, with no indication as to the identity of
the model from which each topic came. As these
evaluations are time consuming, the experts evalu-
ated the only the first 200 topics, which consisted of
103 generalized Po?lya urn topics and 97 LDA top-
ics. AUC values predicting bad topics given coher-
ence were 0.83 and 0.80, respectively. Coherence
effectively predicts topic quality in both models.
Although we were able to improve the average
overall quality of topics and the average quality of
the ten lowest-scoring topics, we found that the gen-
eralized Po?lya urn model was less successful reduc-
ing the overall number of bad topics. Ignoring one
?unbalanced? topic from each model, 16.5% of the
LDA topics and 13.5% from the generalized Po?lya
urn model were marked as ?bad.? While this result
is an improvement, it is not significant at p = 0.05.
6 Discussion
We have demonstrated the following:
? There is a class of low-quality topics that can-
not be detected using existing word-intrusion
tests, but that can be identified reliably using a
metric based on word co-occurrence statistics.
? It is possible to improve the coherence score
of topics, both overall and for the ten worst,
while retaining the ability to flag bad topics, all
without requiring semi-supervised data or ad-
ditional reference corpora. Although additional
information may be useful, it is not necessary.
? Such models achieve better performance with
substantially fewer Gibbs iterations than LDA.
We believe that the most important challenges in fu-
ture topic modeling research are improving the se-
mantic quality of topics, particularly at the low end,
and scaling to ever-larger data sets while ensuring
high-quality topics. Our results provide critical in-
sight into these problems. We found that it should be
possible to construct unsupervised topic models that
do not produce bad topics. We also found that Gibbs
sampling mixes faster for models that use word co-
occurrence information, suggesting that such meth-
ods may also be useful in guiding online stochastic
variational inference (Hoffman et al, 2010).
Acknowledgements
This work was supported in part by the Center
for Intelligent Information Retrieval, in part by the
CIA, the NSA and the NSF under NSF grant # IIS-
0326249, in part by NIH:HHSN271200900640P,
and in part by NSF # number SBE-0965436. Any
opinions, findings and conclusions or recommenda-
tions expressed in this material are the authors? and
do not necessarily reflect those of the sponsor.
References
Loulwah AlSumait, Daniel Barbara, James Gentle, and
Carlotta Domeniconi. 2009. Topic significance rank-
ing of LDA generative models. In ECML.
David Andrzejewski, Xiaojin Zhu, and Mark Craven.
2009. Incorporating domain knowledge into topic
modeling via Dirichlet forest priors. In Proceedings of
the 26th Annual International Conference on Machine
Learning, pages 25?32.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022, January.
K.R. Canini, L. Shi, and T.L. Griffiths. 2009. Online
inference of topics with latent Dirichlet alocation. In
Proceedings of the 12th International Conference on
Artificial Intelligence and Statistics.
Jonathan Chang, Jordan Boyd-Graber, Chong Wang,
Sean Gerrish, and David M. Blei. 2009. Reading tea
leaves: How humans interpret topic models. In Ad-
vances in Neural Information Processing Systems 22,
pages 288?296.
Kenneth Church and Patrick Hanks. 1990. Word asso-
ciation norms, mutual information, and lexicography.
Computational Linguistics, 6(1):22?29.
Gabriel Doyle and Charles Elkan. 2009. Accounting for
burstiness in topic models. In ICML.
S. Geman and D. Geman. 1984. Stochastic relaxation,
Gibbs distributions, and the Bayesian restoration of
images. IEEE Transaction on Pattern Analysis and
Machine Intelligence 6, pages 721?741.
271
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101(suppl. 1):5228?5235.
Matthew Hoffman, David Blei, and Francis Bach. 2010.
Online learning for latent dirichlet alocation. In NIPS.
Hosan Mahmoud. 2008. Po?lya Urn Models. Chapman
& Hall/CRC Texts in Statistical Science.
Qiaozhu Mei, Xuehua Shen, and ChengXiang Zhai.
2007. Automatic labeling of multinomial topic mod-
els. In Proceedings of the 13th ACM SIGKDD Interna-
tional Conference on Knowledge Discovery and Data
Mining, pages 490?499.
David Newman, Jey Han Lau, Karl Grieser, and Timothy
Baldwin. 2010. Automatic evaluation of topic coher-
ence. In Human Language Technologies: The Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics.
Yee Whye Teh, Dave Newman, and Max Welling. 2006.
A collapsed variational Bayesian inference algorithm
for lat ent Dirichlet alocation. In Advances in Neural
Information Processing Systems 18.
Hanna Wallach, Iain Murray, Ruslan Salakhutdinov, and
David Mimno. 2009. Evaluation methods for topic
models. In Proceedings of the 26th Interational Con-
ference on Machine Learning.
Xing Wei and Bruce Croft. 2006. LDA-based document
models for ad-hoc retrival. In Proceedings of the 29th
Annual International SIGIR Conference.
272
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1319?1328,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Low-dimensional Embeddings for Interpretable
Anchor-based Topic Inference
Moontae Lee
Dept. of Computer Science
Cornell University
Ithaca, NY, 14853
moontae@cs.cornell.edu
David Mimno
Dept. of Information Science
Cornell University
Ithaca, NY, 14853
mimno@cornell.edu
Abstract
The anchor words algorithm performs
provably efficient topic model inference
by finding an approximate convex hull
in a high-dimensional word co-occurrence
space. However, the existing greedy al-
gorithm often selects poor anchor words,
reducing topic quality and interpretability.
Rather than finding an approximate con-
vex hull in a high-dimensional space, we
propose to find an exact convex hull in
a visualizable 2- or 3-dimensional space.
Such low-dimensional embeddings both
improve topics and clearly show users why
the algorithm selects certain words.
1 Introduction
Statistical topic modeling is useful in exploratory
data analysis (Blei et al., 2003), but model infer-
ence is known to be NP-hard even for the sim-
plest models with only two topics (Sontag and
Roy, 2011), and training often remains a black
box to users. Likelihood-based training requires
expensive approximate inference such as varia-
tional methods (Blei et al., 2003), which are deter-
ministic but sensitive to initialization, or Markov
chain Monte Carlo (MCMC) methods (Griffiths
and Steyvers, 2004), which have no finite conver-
gence guarantees. Recently Arora et al. proposed
the Anchor Words algorithm (Arora et al., 2013),
which casts topic inference as statistical recovery
using a separability assumption: each topic has
a specific anchor word that appears only in the
context of that single topic. Each anchor word
can be used as a unique pivot to disambiguate the
corresponding topic distribution. We then recon-
struct the word co-occurrence pattern of each non-
anchor words as a convex combination of the co-
occurrence patterns of the anchor words.
burgersaladpizza
chickengood
told
popcornstadiumviewstiremoviesscreen
sashimi
carcalledhotel yoga
bagels
shoppingdog movie
Figure 1: 2D t-SNE projection of a Yelp review
corpus and its convex hull. The words corre-
sponding to vertices are anchor words for topics,
whereas non-anchor words correspond to the inte-
rior points.
This algorithm is fast, requiring only one pass
through the training documents, and provides
provable guarantees, but results depend entirely on
selecting good anchor words. (Arora et al., 2013)
propose a greedy method that finds an approxi-
mate convex hull around a set of vectors corre-
sponding to the word co-occurrence patterns for
each vocabulary word. Although this method is
an improvement over previous work that used im-
practical linear programming methods (Arora et
al., 2012), serious problems remain. The method
greedily chooses the farthest point from the cur-
rent subspace until the given number of anchors
have been found. Particularly at the early stages
1319
of the algorithm, the words associated with the
farthest points are likely to be infrequent and id-
iosyncratic, and thus form poor bases for human
interpretation and topic recovery. This poor choice
of anchors noticeably affects topic quality: the an-
chor words algorithm tends to produce large num-
bers of nearly identical topics.
Besides providing a separability criterion, an-
chor words also have the potential to improve topic
interpretability. After learning topics for given text
collections, users often request a label that sum-
marizes each topic. Manually labeling topics is ar-
duous, and labels often do not carry over between
random initializations and models with differing
numbers of topics. Moreover, it is hard to con-
trol the subjectivity in labelings between annota-
tors, which is open to interpretive errors. There
has been considerable interest in automating the
labeling process (Mei et al., 2007; Lau et al., 2011;
Chuang et al., 2012). (Chuang et al., 2012) pro-
pose a measure of saliency: a good summary term
should be both distinctive specifically to one topic
and probable in that topic. Anchor words are by
definition optimally distinct, and therefore may
seem to be good candidates for topic labels, but
greedily selecting extreme words often results in
anchor words that have low probability.
In this work we explore the opposite of Arora et
al.?s method: rather than finding an approximate
convex hull for an exact set of vectors, we find an
exact convex hull for an approximate set of vec-
tors. We project the V ? V word co-occurrence
matrix to visualizable 2- and 3-dimensional spaces
using methods such as t-SNE (van der Maaten and
Hinton, 2008), resulting in an input matrix up to
3600 times narrower than the original input for
our training corpora. Despite this radically low-
dimensional projection, the method not only finds
topics that are as good or better than the greedy
anchor method, it also finds highly salient, in-
terpretable anchor words and provides users with
a clear visual explanation for why the algorithm
chooses particular words, all while maintaining
the original algorithm?s computational benefits.
2 Related Work
Latent Dirichlet allocation (LDA) (Blei et al.,
2003) models D documents with a vocabulary V
using a predefined number of topics by K. LDA
views both {A
k
}
K
k=1
, a set of K topic-word distri-
butions for each topic k, and {W
d
}
D
d=1
, a set of D
document-topic distributions for each document d,
and {z
d
}
D
d=1
, a set of topic-assignment vectors for
word tokens in the document d, as randomly gen-
erated from known stochastic processes. Merging
{A
k
} as k-th column vector of V ?K matrix A,
{W
d
} as d-th column vector of K ?D matrix W ,
the learning task is to estimate the posterior dis-
tribution of latent variables A, W , and {z
d
} given
V ? D word-document matrix
?
M , which is the
only observed variable where d-th column corre-
sponds to the empirical word frequencies in the
training documents d.
(Arora et al., 2013) recover word-topic matrix
A and topic-topic matrix R = E[WW
T
] instead
of W in the spirit of nonnegative matrix factoriza-
tion. Though the true underlying word distribu-
tion for each document is unknown and could be
far from the sample observation
?
M , the empirical
word-word matrix
?
Q converges to its expectation
AE[WW
T
]A
T
= ARA
T
as the number of docu-
ments increases. Thus the learning task is to ap-
proximately recover A and R pretending that the
empirical
?
Q is close to the true second-order mo-
ment matrix Q.
The critical assumption for this method is to
suppose that every topic k has a specific anchor
word s
k
that occurs with non-negligible probabil-
ity (> 0) only in that topic. The anchor word s
k
need not always appear in every document about
the topic k, but we can be confident that the doc-
ument is at least to some degree about the topic k
if it contains s
k
. This assumption drastically im-
proves inference by guaranteeing the presence of
a diagonal sub-matrix inside the word-topic ma-
trix A. After constructing an estimate
?
Q, the al-
gorithm in (Arora et al., 2013) first finds a set
S = {s
1
, ..., s
K
} of K anchor words (K is user-
specified), and recovers A and R subsequently
based on S. Due to this structure, overall perfor-
mance depends heavily on the quality of anchor
words.
In the matrix algebra literature this greedy
anchor finding method is called QR with row-
pivoting. Previous work classifies a matrix into
two sets of row (or column) vectors where the vec-
tors in one set can effectively reconstruct the vec-
tors in another set, called subset-selection algo-
rithms. (Gu and Eisenstat, 1996) suggest one im-
portant deterministic algorithm. A randomized al-
gorithm provided by (Boutsidis et al., 2009) is the
state-of-the art using a pre-stage that selects the
1320
candidates in addition to (Gu and Eisenstat, 1996).
We found no change in anchor selection using
these algorithms, verifying the difficulty of the an-
chor finding process. This difficulty is mostly be-
cause anchors must be nonnegative convex bases,
whereas the classified vectors from the subset se-
lection algorithms yield unconstrained bases.
The t-SNE model has previously been used to
display high-dimensional embeddings of words in
2D space by Turian.
1
Low-dimensional embed-
dings of topic spaces have also been used to sup-
port user interaction with models: (Eisenstein et
al., 2011) use a visual display of a topic embed-
ding to create a navigator interface. Although
t-SNE has been used to visualize the results of
topic models, for example by (Lacoste-Julien et
al., 2008) and (Zhu et al., 2009), we are not aware
of any use of the method as a fundamental compo-
nent of topic inference.
3 Low-dimensional Embeddings
Real text corpora typically involve vocabularies in
the tens of thousands of distinct words. As the
input matrix
?
Q scales quadratically with V , the
Anchor Words algorithm must depend on a low-
dimensional projection of
?
Q in order to be practi-
cal. Previous work (Arora et al., 2013) uses ran-
dom projections via either Gaussian random ma-
trices (Johnson and Lindenstrauss, 1984) or sparse
random matrices (Achlioptas, 2001), reducing the
representation of each word to around 1,000 di-
mensions. Since the dimensionality of the com-
pressed word co-occurrence space is an order of
magnitude larger than K, we must still approxi-
mate the convex hull by choosing extreme points
as before.
In this work we explore two projection meth-
ods: PCA and t-SNE (van der Maaten and Hinton,
2008). Principle Component Analysis (PCA) is a
commonly-used dimensionality reduction scheme
that linearly transforms the data to new coordi-
nates where the largest variances are orthogonally
captured for each dimension. By choosing only a
few such principle axes, we can represent the data
in a lower dimensional space. In contrast, t-SNE
embedding performs a non-linear dimensionality
reduction preserving the local structures. Given a
set of points {x
i
} in a high-dimensional space X ,
t-SNE allocates probability mass for each pair of
points so that pairs of similar (closer) points be-
1
http://metaoptimize.com/projects/wordreprs/
good
placegreatlove
chicken
store
riceshowerresponseto-gobroccoliteriyakiyogasalonlettuce
Figure 2: 2D PCA projections of a Yelp review
corpus and its convex hulls.
come more likely to co-occur than dissimilar (dis-
tant) points.
p
j|i
=
exp(?d(x
i
,x
j
)
2
/2?
2
i
)
?
k 6=i
exp(?d(x
i
,x
k
)
2
/2?
2
i
)
(1)
p
ij
=
p
j|i
+ p
i|j
2N
(symmetrized) (2)
Then it generates a set of new points {y
i
} in
low-dimensional space Y so that probability dis-
tribution over points in Y behaves similarly to the
distribution over points in X by minimizing KL-
divergence between two distributions:
q
ij
=
(1 + ?y
i
? y
j
?
2
)
?1
?
k 6=l
(1 + ?y
k
? y
l
?
2
)
?1
(3)
min KL(P ||Q) =
?
i 6=j
p
ij
log
p
ij
q
ij
(4)
Instead of approximating a convex hull in such
a high-dimensional space, we select the exact
vertices of the convex hull formed in a low-
dimensional projected space, which can be calcu-
lated efficiently. Figures 1 and 2 show the con-
vex hulls for 2D projections of
?
Q using t-SNE and
PCA for a corpus of Yelp reviews. Figure 3 il-
lustrates the convex hulls for 3D t-SNE projection
for the same corpus. Anchor words correspond to
the vertices of these convex hulls. Note that we
present the 2D projections as illustrative examples
only; we find that three dimensional projections
perform better in practice.
1321
staffatmospherelovebarbeerhourchickenlocationfoodhighlywinemexicanyearskidsmusic screenshop
rice
roomsmanagercalled
bbq dimshowergroundgroup
cheesecake
tires
bagelssashimiwaiter
enchilada
specialshotelbeers
donuts
glass
yoga
peaks
cupcakes
divemoviedog
cookiechorizo
starbucks
shopping
hummus
play hair
bottleprompt
Figure 3: 3D t-SNE projection of a Yelp review
corpus and its convex hull. Vertices on the convex
hull correspond to anchor words.
In addition to the computational advantages,
this approach benefits anchor-based topic model-
ing in two aspects. First, as we now compute the
exact convex hull, the number of topics depends
on the dimensionality of the embedding, v. For
example in the figures, 2D projection has 21 ver-
tices, whereas 3D projection supports 69 vertices.
This implies users can easily tune the granularity
of topic clusters by varying v = 2, 3, 4, ... with-
out increasing the number of topics by one each
time. Second, we can effectively visualize the the-
matic relationships between topic anchors and the
rest of words in the vocabulary, enhancing both
interpretability and options for further vocabulary
curation.
4 Experimental Results
We find that radically low-dimensional t-SNE pro-
jections are effective at finding anchor words that
are much more salient than the greedy method, and
topics that are more distinctive, while maintain-
ing comparable held-out likelihood and semantic
coherence. As noted in Section 1, the previous
greedy anchor words algorithm tends to produce
many nearly identical topics. For example, 37 out
of 100 topics trained on a 2008 political blog cor-
pus have obama, mccain, bush, iraq or palin as
their most probable word, including 17 just for
obama. Only 66% of topics have a unique top
word. In contrast, the t-SNE model on the same
dataset has only one topic whose most probable
word is obama, and 86% of topics have a unique
top word (mccain is the most frequent top word,
with five topics).
We use three real datasets: business reviews
from the Yelp Academic Dataset,
2
political blogs
from the 2008 US presidential election (Eisen-
stein and Xing, 2010), and New York Times ar-
ticles from 2007.
3
Details are shown in Table
1. Documents with fewer than 10 word tokens
are discarded due to possible instability in con-
structing
?
Q. We perform minimal vocabulary cu-
ration, eliminating a standard list of English stop-
words
4
and terms that occur below frequency cut-
offs: 100 times (Yelp, Blog) and 150 times (NYT).
We further restrict possible anchor words to words
that occur in more than three documents. As our
datasets are not artificially synthesized, we reserve
5% from each set of documents for held-out like-
lihood computation.
Name Documents Vocab. Avg. length
Yelp 20,000 1,606 40.6
Blog 13,000 4,447 161.3
NYT 41,000 10,713 277.8
Table 1: Statistics for datasets used in experiments
Unlike (Arora et al., 2013), which presents
results on synthetic datasets to compare perfor-
mance across different recovery methods given in-
creasing numbers of documents, we are are inter-
ested in comparing anchor finding methods, and
are mainly concerned with semantic quality. As
a result, although we have conducted experiments
on synthetic document collections,
5
we focus on
real datasets for this work. We also choose to com-
pare only anchor finding algorithms, so we do not
report comparisons to likelihood-based methods,
which can be found in (Arora et al., 2013).
For both PCA and t-SNE, we use three-
dimensional embeddings across all experiments.
This projection results in matrices that are 0.03%
as wide as the original V ? V matrix for the
NYT dataset. Without low-dimensional embed-
ding, each word is represented by a V-dimensional
vector where only several terms are non-zero due
to the sparse co-occurrence patterns. Thus a ver-
2
https://www.yelp.com/academic dataset
3
http://catalog.ldc.upenn.edu/LDC2008T19
4
We used the list of 524 stop words included in the Mallet
library.
5
None of the algorithms are particularly effective at find-
ing synthetically introduced anchor words possibly because
there are other candidates around anchor vertices that approx-
imate the convex hull to almost the same degree.
1322
tex captured by the greedy anchor-finding method
is likely to be one of many eccentric vertices in
very high-dimensional space. In contrast, t-SNE
creates an effective dense representation where a
small number of pivotal vertices are more clearly
visible, improving both performance and inter-
pretability.
Note that since we can find an exact convex hull
in these spaces,
6
there is an upper bound to the
number of topics that can be found given a partic-
ular projection. If more topics are desired, one can
simply increase the dimensionality of the projec-
tion. For the greedy algorithm we use sparse ran-
dom projections with 1,000 dimensions with 5%
negative entries and 5% positive entries. PCA and
t-SNE choose (49, 32, 47) and (69, 77, 107) an-
chors, respectively for each of three Yelp, Blog,
and NYTimes datasets.
4.1 Anchor-word Selection
We begin by comparing the behavior of low-
dimensional embeddings to the behavior of the
standard greedy algorithm. Table 2 shows ordered
lists of the first 12 anchor words selected by three
algorithms: t-SNE embedding, PCA embedding,
and the original greedy algorithm. Anchor words
selected by t-SNE (police, business, court) are
more general than anchor words selected by the
greedy algorithm (cavalry, al-sadr, yiddish). Ad-
ditional examples of anchor words and their asso-
ciated topics are shown in Table 3 and discussed
in Section 4.2.
# t-SNE PCA Greedy
1 police beloved cavalry
2 bonds york biodiesel
3 business family h/w
4 day loving kingsley
5 initial late mourners
6 million president pcl
7 article people carlisle
8 wife article al-sadr
9 site funeral kaye
10 mother million abc?s
11 court board yiddish
12 percent percent great-grandmother
Table 2: The first 12 anchor words selected by
three algorithms for the NYT corpus.
The Gram-Schimdt process used by Arora et
al. greedily selects anchors in high-dimensional
space. As each word is represented within V -
6
In order to efficiently find an exact convex hull, we use
the Quickhull algorithm.
Type # HR Top Words (Yelp)
t-SNE 16 0 mexican good service great eat restaurant authentic delicious
PCA 15 0 mexican authentic eat chinese don?t restaurant fast salsa
Greedy 34 35 good great food place service restaurant it?s mexican
t-SNE 6 0 beer selection good pizza great wings tap nice
PCA 39 6 wine beer selection nice list glass wines bar
Greedy 99 11 beer selection great happy place wine good bar
t-SNE 3 0 prices great good service selection price nice quality
PCA 12 0 atmosphere prices drinks friendly selection nice beer ambiance
Greedy 34 35 good great food place service restaurant it?s mexican
t-SNE 10 0 chicken salad good lunch sauce ordered fried soup
PCA 10 0 chicken salad lunch fried pita time back sauce
Greedy 69 12 chicken rice sauce fried ordered i?m spicy soup
Type # HR Top Words (Blog)
t-SNE 10 0 hillary clinton campaign democratic bill party win race
PCA 4 0 hillary clinton campaign democratic party bill democrats vote
Greedy 45 19 obama hillary campaign clinton obama?s barack it?s democratic
t-SNE 3 0 iraq war troops iraqi mccain surge security american
PCA 9 1 iraq iraqi war troops military forces security american
Greedy 91 8 iraq mccain war bush troops withdrawal obama iraqi
t-SNE 9 0 allah muhammad qur verses unbelievers ibn muslims world
PCA 18 14 allah muhammad qur verses unbelievers story time update
Greedy 4 5 allah muhammad people qur verses unbelievers ibn sura
t-SNE 19 0 catholic abortion catholics life hagee time biden human
PCA 2 0 people it?s time don?t good make years palin
Greedy 40 1 abortion parenthood planned people time state life government
Type # HR Top Words (NYT)
t-SNE 0 0 police man yesterday officers shot officer year-old charged
PCA 6 0 people it?s police way those three back don?t
Greedy 50 198 police man yesterday officers officer people street city
t-SNE 19 0 senator republican senate democratic democrat state bill
PCA 33 2 state republican republicans senate senator house bill party
Greedy 85 33 senator republican president state campaign presidential people
t-SNE 2 0 business chief companies executive group yesterday billion
PCA 21 0 billion companies business deal group chief states united
Greedy 55 10 radio business companies percent day music article satellite
t-SNE 14 0 market sales stock companies prices billion investors price
PCA 11 0 percent market rate week state those increase high
Greedy 77 44 companies percent billion million group business chrysler people
Table 3: Example t-SNE topics and their most
similar topics across algorithms. The Greedy algo-
rithm can find similar topics, but the anchor words
are much less salient.
dimensions, finding the word that has the next
most distinctive co-occurrence pattern tends to
prefer overly eccentric words with only short, in-
tense bursts of co-occurring words. While the
bases corresponding to these anchor words could
be theoretically relevant for the original space in
high-dimension, they are less likely to be equally
important in low-dimensional space. Thus project-
ing down to low-dimensional space can rearrange
the points emphasizing not only uniqueness, but
also longevity, achieving the ability to form mea-
surably more specific topics.
Concretely, neither cavalry, al-sadr, yiddish nor
police, business, court are full representations of
New York Times articles, but the latter is a much
better basis than the former due to its greater gen-
erality. We see the effect of this difference in the
specificity of the resulting topics (for example in
17 obama topics). Most words in the vocabulary
have little connection to the word cavalry, so the
probability p(z|w) does not change much across
different w. When we convert these distributions
into P (w|z) using the Bayes? rule, the resulting
topics are very close to the corpus distribution, a
1323
unigram distribution p(w).
p(w|z = k
cavalry
) ? p(z = k
cavalry
|w)p(w)
? p(w)
This lack of specificity results in the observed sim-
ilarity of topics.
4.2 Quantitative Results
In this section we compare PCA and t-SNE pro-
jections to the greedy algorithm along several
quantitative metrics. To show the effect of dif-
ferent values of K, we report results for varying
numbers of topics. As the anchor finding algo-
rithms are deterministic, the anchor words in a K-
dimensional model are identical to the first K an-
chor words in a (K + 1)-dimensional model. For
the greedy algorithm we select anchor words in
the order they are chosen. For the PCA and t-
SNE methods, which find anchors jointly, we sort
words in descending order by their distance from
their centroid.
Recovery Error. Each non-anchor word is ap-
proximated by a convex combination of the K
anchor words. The projected gradient algorithm
(Arora et al., 2013) determines these convex coef-
ficients so that the gap between the original word
vector and the approximation becomes minimized.
As choosing a good basis of K anchor words de-
creases this gap, Recovery Error (RE) is defined
by the average `
2
-residuals across all words.
RE =
1
V
V
?
i=1
?
?
Q
i
?
K
?
k=1
p(z
1
= k|w
1
= i)
?
Q
S
k
?
2
(5)
Recovery error decreases with the number of top-
Yelp Blog NYTimes
0.00
0.01
0.02
0.03
0.04
0.05
0 30 60 90 0 30 60 90 0 30 60 90Topics
Rec
ove
ry AlgorithmGreedy
PCA
tSNE
Figure 4: Recovery error is similar across algo-
rithms.
ics, and improves substantially after the first 10?15
anchor words for all methods. The t-SNE method
has slightly better performance than the greedy al-
gorithm, but they are similar. Results for recovery
with the original, unprojected matrix (not shown)
are much worse than the other algorithms, sug-
gesting that the initial anchor words chosen are es-
pecially likely to be uninformative.
Normalized Entropy. As shown previously, if
the probability of topics given a word is close to
uniform, the probability of that word in topics will
be close to the corpus distribution. Normalized
Entropy (NE) measures the entropy of this distri-
bution relative to the entropy of a K-dimensional
uniform distribution:
NE =
1
V
V
?
i=1
H(z|w = i)
logK
. (6)
The normalized entropy of topics given word dis-
Yelp Blog NYTimes
0.25
0.50
0.75
1.00
0 30 60 90 0 30 60 90 0 30 60 90Topics
Nor
ma
lize
dEn
trop
y
Algorithm
Greedy
PCA
tSNE
Figure 5: Words have higher topic entropy in the
greedy model, especially in NYT, resulting in less
specific topics.
tributions usually decreases as we add more top-
ics, although both t-SNE and PCA show a dip in
entropy for low numbers of topics. This result in-
dicates that words become more closely associated
with particular topics as we increase the number of
topics. The low-dimensional embedding methods
(t-SNE and PCA) have consistently lower entropy.
Topic Specificity and Topic Dissimilarity. We
want topics to be both specific (that is, not overly
general) and different from each other. When there
are insufficient number of topics, p(w|z) often re-
sembles the corpus distribution p(w), where high
frequency terms become the top words contribut-
ing to most topics. Topic Specificity (TS) is de-
fined by the average KL divergence from each
topic?s conditional distribution to the corpus dis-
tribution.
7
TS =
1
K
K
?
k=1
KL
(
p(w|z = k) || p(w)
)
(7)
7
We prefer specificity to (AlSumait et al., 2009)?s term
vacuousness because the metric increases as we move away
from the corpus distribution.
1324
One way to define the distance between multiple
points is the minimum radius of a ball that cov-
ers every point. Whereas this is simply the dis-
tance form the centroid to the farthest point in
the Euclidean space, it is an itself difficult opti-
mization problem to find such centroid of distri-
butions under metrics such as KL-divergence and
Jensen-Shannon divergence. To avoid this prob-
lem, we measure Topic Dissimilarity (TD) view-
ing each conditional distribution p(w|z) as a sim-
ple V -dimensional vector in R
V
. Recall a
ik
=
p(w = i|z = k),
TD = max
1?k?K
?
1
K
K
?
k
?
=1
a
?k
?
? a
?k
?
2
. (8)
Specificity and dissimilarity increase with the
Yelp Blog NYTimes
0.0
0.5
1.0
1.5
2.0
0 30 60 90 0 30 60 90 0 30 60 90Topics
Spe
cific
ity AlgorithmGreedy
PCA
tSNE
Yelp Blog NYTimes
0.0
0.2
0.4
0.6
0 30 60 90 0 30 60 90 0 30 60 90Topics
Dis
sim
ilar
ity AlgorithmGreedy
PCA
tSNE
Figure 6: Greedy topics look more like the corpus
distribution and more like each other.
number of topics, suggesting that with few anchor
words, the topic distributions are close to the over-
all corpus distribution and very similar to one an-
other. The t-SNE and PCA algorithms produce
consistently better specificity and dissimilarity, in-
dicating that they produce more useful topics early
with small numbers of topics. The greedy algo-
rithm produces topics that are closer to the corpus
distribution and less distinct from each other (17
obama topics).
Topic Coherence is known to correlate with the
semantic quality of topic judged by human anno-
tators (Mimno et al., 2011). LetW
(T )
k
be T most
probable words (i.e., top words) for the topic k.
TC =
?
w
1
6=w
2
?W
(T )
k
log
D(w
1
, w
2
) + 
D(w
1
)
(9)
Here D(w
1
, w
2
) is the co-document frequency,
which is the number of documents inD consisting
of two words w
1
and w
2
simultaneously. D(w)
is the simple document frequency with the word
w. The numerator contains smoothing count 
in order to avoid taking the logarithm of zero.
Coherence scores for t-SNE and PCA are worse
Yelp Blog NYTimes
?600
?550
?500
?450
?400
0 30 60 90 0 30 60 90 0 30 60 90Topics
Coh
ere
nce AlgorithmGreedy
PCA
tSNE
Figure 7: The greedy algorithm creates more co-
herent topics (higher is better), but at the cost of
many overly general or repetitive topics.
than those for the greedy method, but this result
must be understood in combination with the Speci-
ficity and Dissimilarity metrics. The most frequent
terms in the overall corpus distribution p(w) often
appear together in documents. Thus a model creat-
ing many topics similar to the corpus distribution
is likely to achieve high Coherence, but low Speci-
ficity by definition.
Saliency. (Chuang et al., 2012) define saliency
for topic words as a combination of distinctive-
ness and probability within a topic. Anchor words
are distinctive by construction, so we can increase
saliency by selecting more probable anchor words.
We measure the probability of anchor words in
two ways. First, we report the zero-based rank of
anchor words within their topics. Examples of this
metric, which we call ?hard? rank are shown in Ta-
ble 3. The hard rank of the anchors in the PCA and
t-SNE models are close to zero, while the anchor
words for the greedy algorithm are much lower
ranked, well below the range usually displayed to
users. Second, while hard rank measures the per-
ceived difference in rank of contributing words,
position may not fully capture the relative impor-
tance of the anchor word. ?Soft? rank quantifies
the average log ratio between probabilities of the
1325
prominent word w
?
k
and the anchor word s
k
.
SR =
1
K
K
?
k=1
log
p(w = w
?
k
|z = k)
p(w = s
k
|z = k)
(10)
Yelp Blog NYTimes
0
1
2
3
4
0 30 60 90 0 30 60 90 0 30 60 90Topics
Sof
tAn
cho
rRa
nk Algorithm
Greedy
PCA
tSNE
Figure 8: Anchor words have higher probability,
and therefore greater salience, in t-SNE and PCA
models (1 ? one third the probability of the top
ranked word).
Lower values of soft rank (Fig. 8 indicate that
the anchor word has greater relative probability to
occur within a topic. As we increase the num-
ber of topics, anchor words become more promi-
nent in topics learned by the greedy method, but
t-SNE anchor words remain relatively more prob-
able within their topics as measured by soft rank.
Held-out Probability. Given an estimate of
the topic-word matrix A, we can compute the
marginal probability of held-out documents under
that model. We use the left-to-right estimator in-
troduced by (Wallach et al., 2009), which uses a
sequential algorithm similar to a Gibbs sampler.
This method requires a smoothing parameter for
document-topic Dirichlet distributions, which we
set to ?
k
= 0.1. We note that the greedy algo-
Yelp Blog NYTimes
?6.65
?6.60
?6.55
?7.70
?7.65
?7.60
?7.55
?7.50
?7.45
?8.4
?8.3
?8.2
?8.1
0 25 50 75 100 0 25 50 75 100 0 30 60 90Topics
Hel
dOu
tLL AlgorithmGreedy
PCA
tSNE
Figure 9: t-SNE topics have better held-out prob-
ability than greedy topics.
rithm run on the original, unprojected matrix has
better held-out probability values than t-SNE for
the Yelp corpus, but as this method does not scale
to realistic vocabularies we compare here to the
sparse random projection method used in (Arora
et al., 2013). The t-SNE method appears to do
best, particularly in the NYT corpus, which has a
larger vocabulary and longer training documents.
The length of individual held-out documents has
no correlation with held-out probability.
We emphasize that Held-out Probability is sen-
sitive to smoothing parameters and should only be
used in combination with a range of other topic-
quality metrics. In initial experiments, we ob-
served significantly worse held-out performance
for the t-SNE algorithm. This phenomenon was
because setting the probability of anchor words to
zero for all but their own topics led to large neg-
ative values in held-out log probability for those
words. As t-SNE tends to choose more frequent
terms as anchor words, these ?spikes? significantly
affected overall probability estimates. To make the
calculation more fair, we added 10
?5
to any zero
entries for anchor words in the topic-word matrix
A across all models and renormalized.
Because t-SNE is a stochastic model, different
initializations can result in different embeddings.
To evaluate how steady anchor word selection is,
we ran five random initializations for each dataset.
For the Yelp dataset, the number of anchor words
varies from 59 to 69, and 43 out of those are shared
across at least four trials. For the Blog dataset, the
number of anchor words varies from 80 to 95, with
56 shared across at least four trials. For the NYT
dataset, this number varies between 83 and 107,
with 51 shared across at least four models.
4.3 Qualitative Results
Table 3 shows topics trained by three methods (t-
SNE, PCA, and greedy) for all three datasets. For
each model, we select five topics at random from
the t-SNE model, and then find the closest topic
from each of the other models. If anchor words
present in the top eight words, they are shown in
boldface.
A fundamental difference between anchor-
based inference and traditional likelihood-based
inference is that we can give an order to top-
ics according to their contribution to word co-
occurrence convex hull. This order is intrinsic to
the original algorithm, and we heuristically give
orders to t-SNE and PCA based on their contri-
butions. This order is listed as # in the previous
table. For all but one topic, the closest topic from
the greedy model has a higher order number than
1326
the associated t-SNE topic. As shown above, the
standard algorithm tends to pick less useful anchor
words at the initial stage; only the later, higher or-
dered topics are specific.
The most clear distinction between models is
the rank of anchor words represented by Hard
Rank for each topic. Only one topic correspond-
ing to (initial) has the anchor word which does
not coincide with the top-ranked word. For the
greedy algorithm, anchor words are often tens of
words down the list in rank, indicating that they
are unlikely to find a connection to the topic?s se-
mantic core. In cases where the anchor word is
highly ranked (unbelievers, parenthood) the word
is a good indicator of the topic, but still less deci-
sive.
t-SNE and PCA are often consistent in their se-
lection of anchor words, which provides useful
validation that low-dimensional embeddings dis-
cern more relevant anchor words regardless of lin-
ear vs non-linear projections. Note that we are
only varying the anchor selection part of the An-
chor Words algorithm in these experiments, recov-
ering topic-word distributions in the same manner
given anchor words. As a result, any differences
between topics with the same anchor word (for ex-
ample chicken) are due to the difference in either
the number of topics or the rest of anchor words.
Since PCA suffers from a crowding problem in
lower-dimensional projection (see Figure 2) and
the problem could be severe in a dataset with a
large vocabulary, t-SNE is more likely to find the
proper number of anchors given a specified granu-
larity.
5 Conclusion
One of the main advantages of the anchor words
algorithm is that the running time is largely inde-
pendent of corpus size. Adding more documents
would not affect the size of the co-occurrence ma-
trix, requiring more times to construct the co-
occurrence matrix at the beginning. While the
inference is scalable depending only on the size
of the vocabulary, finding quality anchor words is
crucial for the performance of the inference.
(Arora et al., 2013) presents a greedy anchor
finding algorithm that improves over previous lin-
ear programming methods, but finding quality an-
chor words remains an open problem in spec-
tral topic inference. We have shown that previ-
ous approaches have several limitations. Exhaus-
tively finding anchor words by eliminating words
that are reproducible by other words (Arora et
al., 2012) is impractical. The anchor words se-
lected by the greedy algorithm are overly eccen-
tric, particularly at the early stages of the algo-
rithm, causing topics to be poorly differentiated.
We find that using low-dimensional embeddings
of word co-occurrence statistics allows us to ap-
proximate a better convex hull. The resulting
anchor words are highly salient, being both dis-
tinctive and probable. The models trained with
these words have better quantitative and qualita-
tive properties along various metrics. Most im-
portantly, using radically low-dimensional projec-
tions allows us to provide users with clear visual
explanations for the model?s anchor word selec-
tions.
An interesting property of using low-
dimensional embeddings is that the number
of topics depends only on the projecting dimen-
sion. Since we can efficiently find an exact convex
hull in low-dimensional space, users can achieve
topics with their preferred level of granularities
by changing the projection dimension. We do
not insist this is the ?correct? number of topics
for a corpus, but this method, along with the
range of metrics described in this paper, provides
users with additional perspective when choosing a
dimensionality that is appropriate for their needs.
We find that the t-SNE method, besides its
well-known ability to produce high quality lay-
outs, provides the best overall anchor selection
performance. This method consistently selects
higher-frequency terms as anchor words, resulting
in greater clarity and interpretability. Embeddings
with PCA are also effective, but they result in less
well-formed spaces, being less effective in held-
out probability for sufficiently large corpora.
Anchor word finding methods based on low-
dimensional projections offer several important
advantages for topic model users. In addition to
producing more salient anchor words that can be
used effectively as topic labels, the relationship of
anchor words to a visualizable word co-occurrence
space offers significant potential. Users who can
see why the algorithm chose a particular model
will have greater confidence in the model and in
any findings that result from topic-based analy-
sis. Finally, visualizable spaces offer the poten-
tial to produce interactive environments for semi-
supervised topic reconstruction.
1327
Acknowledgments
We thank David Bindel and the anonymous re-
viewers for their valuable comments and sugges-
tions, and Laurens van der Maaten for providing
his t-SNE implementation.
References
Dimitris Achlioptas. 2001. Database-friendly random
projections. In SIGMOD, pages 274?281.
Loulwah AlSumait, Daniel Barbar, James Gentle, and
Carlotta Domeniconi. 2009. Topic significance
ranking of lda generative models. In ECML.
S. Arora, R. Ge, and A. Moitra. 2012. Learning topic
models ? going beyond svd. In FOCS.
Sanjeev Arora, Rong Ge, Yonatan Halpern, David
Mimno, Ankur Moitra, David Sontag, Yichen Wu,
and Michael Zhu. 2013. A practical algorithm for
topic modeling with provable guarantees. In ICML.
D. Blei, A. Ng, and M. Jordan. 2003. Latent dirichlet
allocation. Journal of Machine Learning Research,
pages 993?1022. Preliminary version in NIPS 2001.
Christos Boutsidis, Michael W. Mahoney, and Petros
Drineas. 2009. An improved approximation algo-
rithm for the column subset selection problem. In
SODA, pages 968?977.
Jason Chuang, Christopher D. Manning, and Jeffrey
Heer. 2012. Termite: Visualization techniques
for assessing textual topic models. In International
Working Conference on Advanced Visual Interfaces
(AVI), pages 74?77.
Jacob Eisenstein and Eric Xing. 2010. The CMU
2008 political blog corpus. Technical report, CMU,
March.
Jacob Eisenstein, Duen Horng Chau, Aniket Kittur, and
Eric P. Xing. 2011. Topicviz: Semantic navigation
of document collections. CoRR, abs/1110.6200.
T. L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. Proceedings of the National Academy of
Sciences, 101:5228?5235.
Ming Gu and Stanley C. Eisenstat. 1996. Efficient
algorithms for computing a strong rank-revealing qr
factorization. In SIAM J. Sci Comput, pages 848?
869.
William B. Johnson and Joram Lindenstrauss. 1984.
Extensions of lipschitz mappings into a hilbert
space. Contemporary Mathematics, 26:189?206.
Simon Lacoste-Julien, Fei Sha, and Michael I. Jordan.
2008. DiscLDA: Discriminative learning for dimen-
sionality reduction and classification. In NIPS.
Jey Han Lau, Karl Grieser, David Newman, and Tim-
othy Baldwin. 2011. Automatic labelling of topic
models. In HLT, pages 1536?1545.
Qiaozhu Mei, Xuehua Shen, and ChengXiang Zhai.
2007. Automatic labeling of multinomial topic
models. In KDD, pages 490?499.
David Mimno, Hanna Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. 2011.
Optimizing semantic coherence in topic models. In
EMNLP.
D. Sontag and D. Roy. 2011. Complexity of inference
in latent dirichlet allocation. In NIPS, pages 1008?
1016.
L.J.P. van der Maaten and G.E. Hinton. 2008. Visu-
alizing high-dimensional data using t-SNE. JMLR,
9:2579?2605, Nov.
Hanna Wallach, Iain Murray, Ruslan Salakhutdinov,
and David Mimno. 2009. Evaluation methods for
topic models. In ICML.
Jun Zhu, Amr Ahmed, and Eric P. Xing. 2009.
MedLDA: Maximum margin supervised topic mod-
els for regression and classication. In ICML.
1328
