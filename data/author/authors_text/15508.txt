Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 151?161,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Semi-Supervised Recursive Autoencoders
for Predicting Sentiment Distributions
Richard Socher Jeffrey Pennington? Eric H. Huang Andrew Y. Ng Christopher D. Manning
Computer Science Department, Stanford University, Stanford, CA 94305, USA
?SLAC National Accelerator Laboratory, Stanford University, Stanford, CA 94309, USA
richard@socher.org {jpennin,ehhuang,ang,manning}@stanford.edu ang@cs.stanford.edu
Abstract
We introduce a novel machine learning frame-
work based on recursive autoencoders for
sentence-level prediction of sentiment label
distributions. Our method learns vector space
representations for multi-word phrases. In
sentiment prediction tasks these represen-
tations outperform other state-of-the-art ap-
proaches on commonly used datasets, such as
movie reviews, without using any pre-defined
sentiment lexica or polarity shifting rules. We
also evaluate the model?s ability to predict
sentiment distributions on a new dataset based
on confessions from the experience project.
The dataset consists of personal user stories
annotated with multiple labels which, when
aggregated, form a multinomial distribution
that captures emotional reactions. Our al-
gorithm can more accurately predict distri-
butions over such labels compared to several
competitive baselines.
1 Introduction
The ability to identify sentiments about personal ex-
periences, products, movies etc. is crucial to un-
derstand user generated content in social networks,
blogs or product reviews. Detecting sentiment in
these data is a challenging task which has recently
spawned a lot of interest (Pang and Lee, 2008).
Current baseline methods often use bag-of-words
representations which cannot properly capture more
complex linguistic phenomena in sentiment analy-
sis (Pang et al, 2002). For instance, while the two
phrases ?white blood cells destroying an infection?
and ?an infection destroying white blood cells? have
the same bag-of-words representation, the former is
a positive reaction while the later is very negative.
More advanced methods such as (Nakagawa et al,
IndicesWords
Semantic Representations
Recursive Autoencoder
i         walked      into         a        parked     car
Sorry, Hugs      You Rock       Teehee    I Understand    Wow, Just Wow
Predicted Sentiment Distribution
Figure 1: Illustration of our recursive autoencoder archi-
tecture which learns semantic vector representations of
phrases. Word indices (orange) are first mapped into a
semantic vector space (blue). Then they are recursively
merged by the same autoencoder network into a fixed
length sentence representation. The vectors at each node
are used as features to predict a distribution over senti-
ment labels.
2010) that can capture such phenomena use many
manually constructed resources (sentiment lexica,
parsers, polarity-shifting rules). This limits the ap-
plicability of these methods to a broader range of
tasks and languages. Lastly, almost all previous
work is based on single, positive/negative categories
or scales such as star ratings. Examples are movie
reviews (Pang and Lee, 2005), opinions (Wiebe et
al., 2005), customer reviews (Ding et al, 2008) or
multiple aspects of restaurants (Snyder and Barzilay,
2007). Such a one-dimensional scale does not accu-
rately reflect the complexity of human emotions and
sentiments.
In this work, we seek to address three issues. (i)
Instead of using a bag-of-words representation, our
model exploits hierarchical structure and uses com-
positional semantics to understand sentiment. (ii)
Our system can be trained both on unlabeled do-
main data and on supervised sentiment data and does
not require any language-specific sentiment lexica,
151
parsers, etc. (iii) Rather than limiting sentiment to
a positive/negative scale, we predict a multidimen-
sional distribution over several complex, intercon-
nected sentiments.
We introduce an approach based on semi-
supervised, recursive autoencoders (RAE) which
use as input continuous word vectors. Fig. 1 shows
an illustration of the model which learns vector rep-
resentations of phrases and full sentences as well as
their hierarchical structure from unsupervised text.
We extend our model to also learn a distribution over
sentiment labels at each node of the hierarchy.
We evaluate our approach on several standard
datasets where we achieve state-of-the art perfor-
mance. Furthermore, we show results on the re-
cently introduced experience project (EP) dataset
(Potts, 2010) that captures a broader spectrum of
human sentiments and emotions. The dataset con-
sists of very personal confessions anonymously
made by people on the experience project website
www.experienceproject.com. Confessions are la-
beled with a set of five reactions by other users. Re-
action labels are you rock (expressing approvement),
tehee (amusement), I understand, Sorry, hugs and
Wow, just wow (displaying shock). For evaluation on
this dataset we predict both the label with the most
votes as well as the full distribution over the senti-
ment categories. On both tasks our model outper-
forms competitive baselines. A set of over 31,000
confessions as well as the code of our model are
available at www.socher.org.
After describing the model in detail, we evalu-
ate it qualitatively by analyzing the learned n-gram
vector representations and compare quantitatively
against other methods on standard datasets and the
EP dataset.
2 Semi-Supervised Recursive
Autoencoders
Our model aims to find vector representations for
variable-sized phrases in either unsupervised or
semi-supervised training regimes. These representa-
tions can then be used for subsequent tasks. We first
describe neural word representations and then pro-
ceed to review a related recursive model based on
autoencoders, introduce our recursive autoencoder
(RAE) and describe how it can be modified to jointly
learn phrase representations, phrase structure and
sentiment distributions.
2.1 Neural Word Representations
We represent words as continuous vectors of param-
eters. We explore two settings. In the first setting
we simply initialize each word vector x ? Rn by
sampling it from a zero mean Gaussian distribution:
x ? N (0, ?2). These word vectors are then stacked
into a word embedding matrix L ? Rn?|V |, where
|V | is the size of the vocabulary. This initialization
works well in supervised settings where a network
can subsequently modify these vectors to capture
certain label distributions.
In the second setting, we pre-train the word vec-
tors with an unsupervised neural language model
(Bengio et al, 2003; Collobert and Weston, 2008).
These models jointly learn an embedding of words
into a vector space and use these vectors to predict
how likely a word occurs given its context. After
learning via gradient ascent the word vectors cap-
ture syntactic and semantic information from their
co-occurrence statistics.
In both cases we can use the resulting matrix of
word vectors L for subsequent tasks as follows. As-
sume we are given a sentence as an ordered list of
m words. Each word has an associated vocabulary
index k into the embedding matrix which we use to
retrieve the word?s vector representation. Mathemat-
ically, this look-up operation can be seen as a sim-
ple projection layer where we use a binary vector b
which is zero in all positions except at the kth index,
xi = Lbk ? Rn. (1)
In the remainder of this paper, we represent a sen-
tence (or any n-gram) as an ordered list of these
vectors (x1, . . . , xm). This word representation is
better suited to autoencoders than the binary number
representations used in previous related autoencoder
models such as the recursive autoassociative mem-
ory (RAAM) model (Pollack, 1990; Voegtlin and
Dominey, 2005) or recurrent neural networks (El-
man, 1991) since sigmoid units are inherently con-
tinuous. Pollack circumvented this problem by hav-
ing vocabularies with only a handful of words and
by manually defining a threshold to binarize the re-
sulting vectors.
152
x1 x3 x4x2
y1=f(W(1)[x3;x4] + b)
y2=f(W(1)[x2;y1] + b)
y3=f(W(1)[x1;y2] + b)
Figure 2: Illustration of an application of a recursive au-
toencoder to a binary tree. The nodes which are not filled
are only used to compute reconstruction errors. A stan-
dard autoencoder (in box) is re-used at each node of the
tree.
2.2 Traditional Recursive Autoencoders
The goal of autoencoders is to learn a representation
of their inputs. In this section we describe how to
obtain a reduced dimensional vector representation
for sentences.
In the past autoencoders have only been used in
setting where the tree structure was given a-priori.
We review this setting before continuing with our
model which does not require a given tree structure.
Fig. 2 shows an instance of a recursive autoencoder
(RAE) applied to a given tree. Assume we are given
a list of word vectors x = (x1, . . . , xm) as described
in the previous section as well as a binary tree struc-
ture for this input in the form of branching triplets
of parents with children: (p ? c1c2). Each child
can be either an input word vector xi or a nontermi-
nal node in the tree. For the example in Fig. 2, we
have the following triplets: ((y1 ? x3x4), (y2 ?
x2y1), (y1 ? x1y2)). In order to be able to apply
the same neural network to each pair of children, the
hidden representations yi have to have the same di-
mensionality as the xi?s.
Given this tree structure, we can now compute the
parent representations. The first parent vector y1 is
computed from the children (c1, c2) = (x3, x4):
p = f(W (1)[c1; c2] + b(1)), (2)
where we multiplied a matrix of parameters W (1) ?
Rn?2n by the concatenation of the two children.
After adding a bias term we applied an element-
wise activation function such as tanh to the result-
ing vector. One way of assessing how well this n-
dimensional vector represents its children is to try to
reconstruct the children in a reconstruction layer:
[
c?1; c?2
]
= W (2)p+ b(2). (3)
During training, the goal is to minimize the recon-
struction errors of this input pair. For each pair, we
compute the Euclidean distance between the original
input and its reconstruction:
Erec([c1; c2]) =
1
2
????[c1; c2]?
[
c?1; c?2
]????2 . (4)
This model of a standard autoencoder is boxed in
Fig. 2. Now that we have defined how an autoen-
coder can be used to compute an n-dimensional vec-
tor representation (p) of two n-dimensional children
(c1, c2), we can describe how such a network can be
used for the rest of the tree.
Essentially, the same steps repeat. Now that y1
is given, we can use Eq. 2 to compute y2 by setting
the children to be (c1, c2) = (x2, y1). Again, after
computing the intermediate parent vector y2, we can
assess how well this vector capture the content of
the children by computing the reconstruction error
as in Eq. 4. The process repeat until the full tree
is constructed and we have a reconstruction error at
each nonterminal node. This model is similar to the
RAAM model (Pollack, 1990) which also requires a
fixed tree structure.
2.3 Unsupervised Recursive Autoencoder for
Structure Prediction
Now, assume there is no tree structure given for
the input vectors in x. The goal of our structure-
prediction RAE is to minimize the reconstruction er-
ror of all vector pairs of children in a tree. We de-
fine A(x) as the set of all possible trees that can be
built from an input sentence x. Further, let T (y) be
a function that returns the triplets of a tree indexed
by s of all the non-terminal nodes in a tree. Using
the reconstruction error of Eq. 4, we compute
RAE?(x) = argmin
y?A(x)
?
s?T (y)
Erec([c1; c2]s) (5)
We now describe a greedy approximation that con-
structs such a tree.
153
Greedy Unsupervised RAE. For a sentence with
m words, we apply the autoencoder recursively. It
takes the first pair of neighboring vectors, defines
them as potential children of a phrase (c1; c2) =
(x1;x2), concatenates them and gives them as in-
put to the autoencoder. For each word pair, we save
the potential parent node p and the resulting recon-
struction error.
After computing the score for the first pair, the
network is shifted by one position and takes as input
vectors (c1, c2) = (x2, x3) and again computes a po-
tential parent node and a score. This process repeats
until it hits the last pair of words in the sentence:
(c1, c2) = (xm?1, xm). Next, it selects the pair
which had the lowest reconstruction error (Erec) and
its parent representation p will represent this phrase
and replace both children in the sentence word list.
For instance, consider the sequence (x1, x2, x3, x4)
and assume the lowestErec was obtained by the pair
(x3, x4). After the first pass, the new sequence then
consists of (x1, x2, p(3,4)). The process repeats and
treats the new vector p(3,4) like any other input vec-
tor. For instance, subsequent states could be either:
(x1, p(2,(3,4))) or (p(1,2), p(3,4)). Both states would
then finish with a deterministic choice of collapsing
the remaining two states into one parent to obtain
(p(1,(2,(3,4)))) or (p((1,2),(3,4))) respectively. The tree
is then recovered by unfolding the collapsing deci-
sions.
The resulting tree structure captures as much of
the single-word information as possible (in order
to allow reconstructing the word vectors) but does
not necessarily follow standard syntactic constraints.
We also experimented with a method that finds bet-
ter solutions to Eq. 5 based on CKY-like beam
search algorithms (Socher et al, 2010; Socher et al,
2011) but the performance is similar and the greedy
version is much faster.
Weighted Reconstruction. One problem with
simply using the reconstruction error of both chil-
dren equally as describe in Eq. 4 is that each child
could represent a different number of previously
collapsed words and is hence of bigger importance
for the overall meaning reconstruction of the sen-
tence. For instance in the case of (x1, p(2,(3,4)))
one would like to give more importance to recon-
structing p than x1. We capture this desideratum
by adjusting the reconstruction error. Let n1, n2 be
the number of words underneath a current poten-
tial child, we re-define the reconstruction error to be
Erec([c1; c2]; ?) =
n1
n1 + n2
????c1 ? c?1
????2 + n2n1 + n2
????c2 ? c?2
????2 (6)
Length Normalization. One of the goals of
RAEs is to induce semantic vector representations
that allow us to compare n-grams of different
lengths. The RAE tries to lower reconstruction error
of not only the bigrams but also of nodes higher in
the tree. Unfortunately, since the RAE computes the
hidden representations it then tries to reconstruct, it
can just lower reconstruction error by making the
hidden layer very small in magnitude. To prevent
such undesirable behavior, we modify the hidden
layer such that the resulting parent representation al-
ways has length one, after computing p as in Eq. 2,
we simply set: p = p||p|| .
2.4 Semi-Supervised Recursive Autoencoders
So far, the RAE was completely unsupervised and
induced general representations that capture the se-
mantics of multi-word phrases.In this section, we
extend RAEs to a semi-supervised setting in order
to predict a sentence- or phrase-level target distribu-
tion t.1
One of the main advantages of the RAE is that
each node of the tree built by the RAE has associ-
ated with it a distributed vector representation (the
parent vector p) which could also be seen as fea-
tures describing that phrase. We can leverage this
representation by adding on top of each parent node
a simple softmax layer to predict class distributions:
d(p; ?) = softmax(W labelp). (7)
Assuming there are K labels, d ? RK is
a K-dimensional multinomial distribution and?
k=1 dk = 1. Fig. 3 shows such a semi-supervised
RAE unit. Let tk be the kth element of the multino-
mial target label distribution t for one entry. The
softmax layer?s outputs are interpreted as condi-
tional probabilities dk = p(k|[c1; c2]), hence the
cross-entropy error is
EcE(p, t; ?) = ?
K?
k=1
tk log dk(p; ?). (8)
1For the binary label classification case, the distribution is
of the form [1, 0] for class 1 and [0, 1] for class 2.
154
R e c o n s t r u c t i o n  e r r o r            C r o s s - e n t r o p y e r r o r
W(1)
W(2) W(l a be l )
Figure 3: Illustration of an RAE unit at a nonterminal tree
node. Red nodes show the supervised softmax layer for
label distribution prediction.
Using this cross-entropy error for the label and the
reconstruction error from Eq. 6, the final semi-
supervised RAE objective over (sentences,label)
pairs (x, t) in a corpus becomes
J = 1N
?
(x,t)
E(x, t; ?) + ?2 ||?||
2, (9)
where we have an error for each entry in the training
set that is the sum over the error at the nodes of the
tree that is constructed by the greedy RAE:
E(x, t; ?) =
?
s?T (RAE?(x))
E([c1; c2]s, ps, t, ?).
The error at each nonterminal node is the weighted
sum of reconstruction and cross-entropy errors,
E([c1; c2]s, ps, t, ?) =
?Erec([c1; c2]s; ?) + (1? ?)EcE(ps, t; ?).
The hyperparameter ? weighs reconstruction and
cross-entropy error. When minimizing the cross-
entropy error of this softmax layer, the error will
backpropagate and influence both the RAE param-
eters and the word representations. Initially, words
such as good and bad have very similar representa-
tions. This is also the case for Brown clusters and
other methods that use only cooccurrence statistics
in a small window around each word. When learn-
ing with positive/negative sentiment, the word em-
beddings get modified and capture less syntactic and
more sentiment information.
In order to predict the sentiment distribution of a
sentence with this model, we use the learned vector
representation of the top tree node and train a simple
logistic regression classifier.
3 Learning
Let ? = (W (1), b(1),W (2), b(1),W label, L) be the set
of our model parameters, then the gradient becomes:
?J
?? =
1
N
?
(x,t)
?E(x, t; ?)
?? + ??. (10)
To compute this gradient, we first greedily construct
all trees and then derivatives for these trees are com-
puted efficiently via backpropagation through struc-
ture (Goller and Ku?chler, 1996). Because the algo-
rithm is greedy and the derivatives of the supervised
cross-entropy error also modify the matrix W (1),
this objective is not necessarily continuous and a
step in the gradient descent direction may not nec-
essarily decrease the objective. However, we found
that L-BFGS run over the complete training data
(batch mode) to minimize the objective works well
in practice, and that convergence is smooth, with the
algorithm typically finding a good solution quickly.
4 Experiments
We first describe the new experience project (EP)
dataset, results of standard classification tasks on
this dataset and how to predict its sentiment label
distributions. We then show results on other com-
monly used datasets and conclude with an analysis
of the important parameters of the model.
In all experiments involving our model, we repre-
sent words using 100-dimensional word vectors. We
explore the two settings mentioned in Sec. 2.1. We
compare performance on standard datasets when us-
ing randomly initialized word vectors (random word
init.) or word vectors trained by the model of Col-
lobert and Weston (2008) and provided by Turian
et al (2010).2 These vectors were trained on an
unlabeled corpus of the English Wikipedia. Note
that alternatives such as Brown clusters are not suit-
able since they do not capture sentiment information
(good and bad are usually in the same cluster) and
cannot be modified via backpropagation.
2http://metaoptimize.com/projects/
wordreprs/
155
Corpus K Instances Distr.(+/-) Avg|W |
MPQA 2 10,624 0.31/0.69 3
MR 2 10,662 0.5/0.5 22
EP 5 31,675 .2/.2/.1/.4/.1 113
EP? 4 5 6,129 .2/.2/.1/.4/.1 129
Table 1: Statistics on the different datasets. K is the num-
ber of classes. Distr. is the distribution of the different
classes (in the case of 2, the positive/negative classes, for
EP the rounded distribution of total votes in each class).
|W | is the average number of words per instance. We use
EP? 4, a subset of entries with at least 4 votes.
4.1 EP Dataset: The Experience Project
The confessions section of the experience project
website3 lets people anonymously write short per-
sonal stories or ?confessions?. Once a story is on
the site, each user can give a single vote to one of
five label categories (with our interpretation):
1 Sorry, Hugs: User offers condolences to author.
2. You Rock: Indicating approval, congratulations.
3. Teehee: User found the anecdote amusing.
4. I Understand: Show of empathy.
5. Wow, Just Wow: Expression of surprise,shock.
The EP dataset has 31,676 confession entries, a to-
tal number of 74,859 votes for the 5 labels above, the
average number of votes per entry is 2.4 (with a vari-
ance of 33). For the five categories, the numbers of
votes are [14, 816; 13, 325; 10, 073; 30, 844; 5, 801].
Since an entry with less than 4 votes is not very well
identified, we train and test only on entries with at
least 4 total votes. There are 6,129 total such entries.
The distribution over total votes in the 5 classes
is similar: [0.22; 0.2; 0.11; 0.37; 0.1]. The average
length of entries is 129 words. Some entries con-
tain multiple sentences. In these cases, we average
the predicted label distributions from the sentences.
Table 1 shows statistics of this and other commonly
used sentiment datasets (which we compare on in
later experiments). Table 2 shows example entries
as well as gold and predicted label distributions as
described in the next sections.
Compared to other datasets, the EP dataset con-
tains a wider range of human emotions that goes far
beyond positive/negative product or movie reviews.
Each item is labeled with a multinomial distribu-
3http://www.experienceproject.com/
confessions.php
tion over interconnected response categories. This
is in contrast to most other datasets (including multi-
aspect rating) where several distinct aspects are rated
independently but on the same scale. The topics
range from generic happy statements, daily clumsi-
ness reports, love, loneliness, to relationship abuse
and suicidal notes. As is evident from the total num-
ber of label votes, the most common user reaction
is one of empathy and an ability to relate to the au-
thors experience. However, some stories describe
horrible scenarios that are not common and hence
receive more offers of condolence. In the following
sections we show some examples of stories with pre-
dicted and true distributions but refrain from listing
the most horrible experiences.
For all experiments on the EP dataset, we split the
data into train (49%), development (21%) and test
data (30%).
4.2 EP: Predicting the Label with Most Votes
The first task for our evaluation on the EP dataset is
to simply predict the single class that receives the
most votes. In order to compare our novel joint
phrase representation and classifier learning frame-
work to traditional methods, we use the following
baselines:
Random Since there are five classes, this gives 20%
accuracy.
Most Frequent Selecting the class which most fre-
quently has the most votes (the class I under-
stand).
Baseline 1: Binary BoW This baseline uses logis-
tic regression on binary bag-of-word represen-
tations that are 1 if a word is present and 0 oth-
erwise.
Baseline 2: Features This model is similar to tra-
ditional approaches to sentiment classification
in that it uses many hand-engineered resources.
We first used a spell-checker and Wordnet to
map words and their misspellings to synsets to
reduce the total number of words. We then re-
placed sentiment words with a sentiment cat-
egory identifier using the sentiment lexica of
the Harvard Inquirer (Stone, 1966) and LIWC
(Pennebaker et al, 2007). Lastly, we used tf-idf
weighting on the bag-of-word representations
and trained an SVM.156
KL Predicted&Gold V. Entry (Shortened if it ends with ...)
.03
.16 .16 .16 .33 .16
6 I reguarly shoplift. I got caught once and went to jail, but I?ve found that this was not a deterrent. I don?t buy
groceries, I don?t buy school supplies for my kids, I don?t buy gifts for my kids, we don?t pay for movies, and I
dont buy most incidentals for the house (cleaning supplies, toothpaste, etc.)...
.03
.38 .04 .06 .35 .14
165 i am a very succesfull buissnes man.i make good money but i have been addicted to crack for 13 years.i moved 1
hour away from my dealers 10 years ago to stop using now i dont use daily but once a week usally friday nights.
i used to use 1 or 2 hundred a day now i use 4 or 5 hundred on a friday.my problem is i am a funcational addict...
.05
.14 .28 .14 .28 .14
7 Hi there, Im a guy that loves a girl, the same old bloody story... I met her a while ago, while studying, she Is so
perfect, so mature and yet so lonely, I get to know her and she get ahold of me, by opening her life to me and so
did I with her, she has been the first person, male or female that has ever made that bond with me,...
.07
.27 .18 .00 .45 .09
11 be kissing you right now. i should be wrapped in your arms in the dark, but instead i?ve ruined everything. i?ve
piled bricks to make a wall where there never should have been one. i feel an ache that i shouldn?t feel because
i?ve never had you close enough. we?ve never touched, but i still feel as though a part of me is missing. ...
.05 23 Dear Love, I just want to say that I am looking for you. Tonight I felt the urge to write, and I am becoming more
and more frustrated that I have not found you yet. I?m also tired of spending so much heart on an old dream. ...
.05 5 I wish I knew somone to talk to here.
.06 24 I loved her but I screwed it up. Now she?s moved on. I?ll never have her again. I don?t know if I?ll ever stop
thinking about her.
.06 5 i am 13 years old and i hate my father he is alwas geting drunk and do?s not care about how it affects me or my
sisters i want to care but the truthis i dont care if he dies
.13 6 well i think hairy women are attractive
.35 5 As soon as I put clothings on I will go down to DQ and get a thin mint blizzard. I need it. It?ll make my soul
feel a bit better :)
.36 6 I am a 45 year old divoced woman, and I havent been on a date or had any significant relationship in 12
years...yes, 12 yrs. the sad thing is, Im not some dried up old granny who is no longer interested in men, I just
can?t meet men. (before you judge, no Im not terribly picky!) What is wrong with me?
.63 6 When i was in kindergarden i used to lock myself in the closet and eat all the candy. Then the teacher found out
it was one of us and made us go two days without freetime. It might be a little late now, but sorry guys it was
me haha
.92 4 My paper is due in less than 24 hours and I?m still dancing round my room!
Table 2: Example EP confessions from the test data with KL divergence between our predicted distribution (light blue,
left bar on each of the 5 classes) and ground truth distribution (red bar and numbers underneath), number of votes. The
5 classes are [Sorry, Hugs ;You Rock; Teehee;I Understand;Wow, Just Wow]. Even when the KL divergence is higher,
our model makes reasonable alternative label choices. Some entries are shortened.
Baseline 3: Word Vectors We can ignore the RAE
tree structure and only train softmax layers di-
rectly on the pre-trained words in order to influ-
ence the word vectors. This is followed by an
SVM trained on the average of the word vec-
tors.
We also experimented with latent Dirichlet aloca-
tion (Blei et al, 2003) but performance was very
low.
Table 3 shows the results for predicting the class
with the most votes. Even the approach that is based
on sentiment lexica and other resources is outper-
formed by our model by almost 3%, showing that
for tasks involving complex broad-range human sen-
timent, the often used sentiment lexica lack in cover-
age and traditional bag-of-words representations are
not powerful enough.
4.3 EP: Predicting Sentiment Distributions
We now turn to evaluating our distribution-
prediction approach. In both this and the previous
Method Accuracy
Random 20.0
Most Frequent 38.1
Baseline 1: Binary BoW 46.4
Baseline 2: Features 47.0
Baseline 3: Word Vectors 45.5
RAE (our method) 50.1
Table 3: Accuracy of predicting the class with most votes.
maximum label task, we backprop using the gold
multinomial distribution as a target. Since we max-
imize likelihood and because we want to predict a
distribution that is closest to the distribution of labels
that people would assign to a story, we evaluate us-
ing KL divergence: KL(g||p) = ?i gi log(gi/pi),
where g is the gold distribution and p is the predicted
one. We report the average KL divergence, where a
smaller value indicates better predictive power. To
get an idea of the values of KL divergence, predict-
157
Avg.Distr. BoW Features Word Vec. RAE0.6
0.7
0.8
0.83 0.81 0.72 0.73 0.70
Figure 4: Average KL-divergence between gold and pre-
dicted sentiment distributions (lower is better).
ing random distributions gives a an average of 1.2 in
KL divergence, predicting simply the average distri-
bution in the training data give 0.83. Fig. 4 shows
that our RAE-based model outperforms the other
baselines. Table 2 shows EP example entries with
predicted and gold distributions, as well as numbers
of votes.
4.4 Binary Polarity Classification
In order to compare our approach to other meth-
ods we also show results on commonly used sen-
timent datasets: movie reviews4 (MR) (Pang and
Lee, 2005) and opinions5 (MPQA) (Wiebe et al,
2005).We give statistical information on these and
the EP corpus in Table 1.
We compare to the state-of-the-art system of
(Nakagawa et al, 2010), a dependency tree based
classification method that uses CRFs with hidden
variables. We use the same training and testing regi-
men (10-fold cross validation) as well as their base-
lines: majority phrase voting using sentiment and
reversal lexica; rule-based reversal using a depen-
dency tree; Bag-of-Features and their full Tree-CRF
model. As shown in Table 4, our algorithm outper-
forms their approach on both datasets. For the movie
review (MR) data set, we do not use any hand-
designed lexica. An error analysis on the MPQA
dataset showed several cases of single words which
never occurred in the training set. Correctly classify-
ing these instances can only be the result of having
them in the original sentiment lexicon. Hence, for
the experiment on MPQA we added the same sen-
timent lexicon that (Nakagawa et al, 2010) used in
their system to our training set. This improved ac-
curacy from 86.0 to 86.4.Using the pre-trained word
vectors boosts performance by less than 1% com-
4www.cs.cornell.edu/people/pabo/
movie-review-data/
5www.cs.pitt.edu/mpqa/
Method MR MPQA
Voting with two lexica 63.1 81.7
Rule-based reversal on trees 62.9 82.8
Bag of features with reversal 76.4 84.1
Tree-CRF (Nakagawa et al?10) 77.3 86.1
RAE (random word init.) 76.8 85.7
RAE (our method) 77.7 86.4
Table 4: Accuracy of sentiment classification on movie
review polarity (MR) and the MPQA dataset.
0 0.2 0.4 0.6 0.8 10.83
0.84
0.85
0.86
0.87
Figure 5: Accuracy on the development split of the MR
polarity dataset for different weightings of reconstruction
error and supervised cross-entropy error: err = ?Erec+
(1? ?)EcE .
pared to randomly initialized word vectors (setting:
random word init). This shows that our method can
work well even in settings with little training data.
We visualize the semantic vectors that the recursive
autoencoder learns by listing n-grams that give the
highest probability for each polarity. Table 5 shows
such n-grams for different lengths when the RAE is
trained on the movie review polarity dataset.
On a 4-core machine, training time for the smaller
corpora such as the movie reviews takes around 3
hours and for the larger EP corpus around 12 hours
until convergence. Testing of hundreds of movie re-
views takes only a few seconds.
4.5 Reconstruction vs. Classification Error
In this experiment, we show how the hyperparame-
ter ? influences accuracy on the development set of
one of the cross-validation splits of the MR dataset.
This parameter essentially trade-off the supervised
and unsupervised parts of the objective. Fig. 5 shows
that a larger focus on the supervised objective is im-
portant but that a weight of ? = 0.2 for the recon-
struction error prevents overfitting and achieves the
highest performance.
158
n Most negative n-grams Most positive n-grams
1 bad; boring; dull; flat; pointless; tv; neither; pretentious; badly;
worst; lame; mediocre; lack; routine; loud; bore; barely; stupid;
tired; poorly; suffers; heavy;nor; choppy; superficial
touching; enjoyable; powerful; warm; moving; culture; flaws;
provides; engrossing; wonderful; beautiful; quiet; socio-political;
thoughtful; portrait; refreshingly; chilling; rich; beautifully; solid;
2 how bad; by bad; dull .; for bad; to bad; boring .; , dull; are bad;
that bad; boring ,; , flat; pointless .; badly by; on tv; so routine; lack
the; mediocre .; a generic; stupid ,; abysmally pathetic
the beautiful; moving,; thoughtful and; , inventive; solid and; a
beautiful; a beautifully; and hilarious; with dazzling; provides the;
provides.; and inventive; as powerful; moving and; a moving; a
powerful
3 . too bad; exactly how bad; and never dull; shot but dull; is more
boring; to the dull; dull, UNK; it is bad; or just plain; by turns
pretentious; manipulative and contrived; bag of stale; is a bad; the
whole mildly; contrived pastiche of; from this choppy; stale mate-
rial.
funny and touching; a small gem; with a moving; cuts, fast; , fine
music; smart and taut; culture into a; romantic , riveting; ... a solid;
beautifully acted .; , gradually reveals; with the chilling; cast of
solid; has a solid; spare yet audacious; ... a polished; both the
beauty;
5 boring than anything else.; a major waste ... generic; nothing i
hadn?t already; ,UNK plotting;superficial; problem ? no laughs.;
,just horribly mediocre .; dull, UNK feel.; there?s nothing exactly
wrong; movie is about a boring; essentially a collection of bits
reminded us that a feel-good; engrossing, seldom UNK,; between
realistic characters showing honest; a solid piece of journalistic;
easily the most thoughtful fictional; cute, funny, heartwarming;
with wry humor and genuine; engrossing and ultimately tragic.;
8 loud, silly, stupid and pointless.; dull, dumb and derivative horror
film.; UNK?s film, a boring, pretentious; this film biggest problem
? no laughs.; film in the series looks and feels tired; do draw easy
chuckles but lead nowhere.; stupid, infantile, redundant, sloppy
shot in rich , shadowy black-and-white , devils an escapist con-
fection that ?s pure entertainment .; , deeply absorbing piece that
works as a; ... one of the most ingenious and entertaining; film is a
riveting , brisk delight .; bringing richer meaning to the story ?s;
Table 5: Examples of n-grams (n = 1, 2, 3, 5, 8) from the test data of the movie polarity dataset for which our model
predicts the most positive and most negative responses.
5 Related Work
5.1 Autoencoders and Deep Learning
Autoencoders are neural networks that learn a re-
duced dimensional representation of fixed-size in-
puts such as image patches or bag-of-word repre-
sentations of text documents. They can be used to
efficiently learn feature encodings which are useful
for classification. Recently, Mirowski et al (2010)
learn dynamic autoencoders for documents in a bag-
of-words format which, like ours, combine super-
vised and reconstruction objectives.
The idea of applying an autoencoder in a recursive
setting was introduced by Pollack (1990). Pollack?s
recursive auto-associative memories (RAAMs) are
similar to ours in that they are a connectionst, feed-
forward model. However, RAAMs learn vector
representations only for fixed recursive data struc-
tures, whereas our RAE builds this recursive data
structure. More recently, (Voegtlin and Dominey,
2005) introduced a linear modification to RAAMs
that is able to better generalize to novel combina-
tions of previously seen constituents. One of the
major shortcomings of previous applications of re-
cursive autoencoders to natural language sentences
was their binary word representation as discussed in
Sec. 2.1.
Recently, (Socher et al, 2010; Socher et al, 2011)
introduced a max-margin framework based on recur-
sive neural networks (RNNs) for labeled structure
prediction. Their models are applicable to natural
language and computer vision tasks such as parsing
or object detection. The current work is related in
that it uses a recursive deep learning model. How-
ever, RNNs require labeled tree structures and use a
supervised score at each node. Instead, RAEs learn
hierarchical structures that are trying to capture as
much of the the original word vectors as possible.
The learned structures are not necessarily syntacti-
cally plausible but can capture more of the semantic
content of the word vectors. Other recent deep learn-
ing methods for sentiment analysis include (Maas et
al., 2011).
5.2 Sentiment Analysis
Pang et al (2002) were one of the first to experiment
with sentiment classification. They show that sim-
ple bag-of-words approaches based on Naive Bayes,
MaxEnt models or SVMs are often insufficient for
predicting sentiment of documents even though they
work well for general topic-based document classi-
fication. Even adding specific negation words, bi-
grams or part-of-speech information to these mod-
els did not add significant improvements. Other
document-level sentiment work includes (Turney,
2002; Dave et al, 2003; Beineke et al, 2004; Pang
and Lee, 2004). For further references, see (Pang
and Lee, 2008).
Instead of document level sentiment classifica-
tion, (Wilson et al, 2005) analyze the contextual
polarity of phrases and incorporate many well de-
signed features including dependency trees. They
also show improvements by first distinguishing be-
159
tween neutral and polar sentences. Our model natu-
rally incorporates the recursive interaction between
context and polarity words in sentences in a unified
framework while simultaneously learning the neces-
sary features to make accurate predictions. Other ap-
proaches for sentence-level sentiment detection in-
clude (Yu and Hatzivassiloglou, 2003; Grefenstette
et al, 2004; Ikeda et al, 2008).
Most previous work is centered around a given
sentiment lexicon or building one via heuristics
(Kim and Hovy, 2007; Esuli and Sebastiani, 2007),
manual annotation (Das and Chen, 2001) or machine
learning techniques (Turney, 2002). In contrast, we
do not require an initial or constructed sentiment lex-
icon of positive and negative words. In fact, when
training our approach on documents or sentences, it
jointly learns such lexica for both single words and
n-grams (see Table 5). (Mao and Lebanon, 2007)
propose isotonic conditional random fields and dif-
ferentiate between local, sentence-level and global,
document-level sentiment.
The work of (Polanyi and Zaenen, 2006; Choi and
Cardie, 2008) focuses on manually constructing sev-
eral lexica and rules for both polar words and re-
lated content-word negators, such as ?prevent can-
cer?, where prevent reverses the negative polarity of
cancer. Like our approach they capture composi-
tional semantics. However, our model does so with-
out manually constructing any rules or lexica.
Recently, (Velikovich et al, 2010) showed how to
use a seed lexicon and a graph propagation frame-
work to learn a larger sentiment lexicon that also in-
cludes polar multi-word phrases such as ?once in a
life time?. While our method can also learn multi-
word phrases it does not require a seed set or a large
web graph. (Nakagawa et al, 2010) introduced an
approach based on CRFs with hidden variables with
very good performance. We compare to their state-
of-the-art system. We outperform them on the stan-
dard corpora that we tested on without requiring
external systems such as POS taggers, dependency
parsers and sentiment lexica. Our approach jointly
learns the necessary features and tree structure.
In multi-aspect rating (Snyder and Barzilay, 2007)
one finds several distinct aspects such as food or ser-
vice in a restaurant and then rates them on a fixed
linear scale such as 1-5 stars, where all aspects could
obtain just 1 star or all aspects could obtain 5 stars
independently. In contrast, in our method a single
aspect (a complex reaction to a human experience)
is predicted not in terms of a fixed scale but in terms
of a multinomial distribution over several intercon-
nected, sometimes mutually exclusive emotions. A
single story cannot simultaneously obtain a strong
reaction in different emotional responses (by virtue
of having to sum to one).
6 Conclusion
We presented a novel algorithm that can accurately
predict sentence-level sentiment distributions. With-
out using any hand-engineered resources such as
sentiment lexica, parsers or sentiment shifting rules,
our model achieves state-of-the-art performance on
commonly used sentiment datasets. Furthermore,
we introduce a new dataset that contains distribu-
tions over a broad range of human emotions. Our
evaluation shows that our model can more accu-
rately predict these distributions than other models.
Acknowledgments
We gratefully acknowledge the support of the Defense
Advanced Research Projects Agency (DARPA) Machine
Reading Program under Air Force Research Laboratory
(AFRL) prime contract no. FA8750-09-C-0181. Any
opinions, findings, and conclusion or recommendations
expressed in this material are those of the author(s) and
do not necessarily reflect the view of DARPA, AFRL, or
the US government. This work was also supported in part
by the DARPA Deep Learning program under contract
number FA8650-10-C-7020.
We thank Chris Potts for help with the EP data set, Ray-
mond Hsu, Bozhi See, and Alan Wu for letting us use
their system as a baseline and Jiquan Ngiam, Quoc Le,
Gabor Angeli and Andrew Maas for their feedback.
References
P. Beineke, T. Hastie, C. D. Manning, and
S. Vaithyanathan. 2004. Exploring sentiment
summarization. In Proceedings of the AAAI Spring
Symposium on Exploring Attitude and Affect in Text:
Theories and Applications.
Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. 2003.
A neural probabilistic language model. Journal of Ma-
chine Learning Research, 3:1137?1155.
D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent
dirichlet alocation. Journal of Machine Learning Re-
search., 3:993?1022.
160
Y. Choi and C. Cardie. 2008. Learning with composi-
tional semantics as structural inference for subsenten-
tial sentiment analysis. In EMNLP.
R. Collobert and J. Weston. 2008. A unified archi-
tecture for natural language processing: deep neural
networks with multitask learning. In Proceedings of
ICML, pages 160?167.
S. Das and M. Chen. 2001. Yahoo! for Amazon: Ex-
tracting market sentiment from stock message boards.
In Proceedings of the Asia Pacific Finance Association
Annual Conference (APFA).
K. Dave, S. Lawrence, and D. M. Pennock. 2003. Min-
ing the peanut gallery: Opinion extraction and seman-
tic classification of product reviews. In Proceedings of
WWW, pages 519?528.
X. Ding, B. Liu, and P. S. Yu. 2008. A holistic lexicon-
based approach to opinion mining. In Proceedings of
the Conference on Web Search and Web Data Mining
(WSDM).
J. L. Elman. 1991. Distributed representations, simple
recurrent networks, and grammatical structure. Ma-
chine Learning, 7(2-3):195?225.
A. Esuli and F. Sebastiani. 2007. Pageranking word-
net synsets: An application to opinion mining. In
Proceedings of the Association for Computational Lin-
guistics (ACL).
C. Goller and A. Ku?chler. 1996. Learning task-
dependent distributed representations by backpropaga-
tion through structure. In Proceedings of the Interna-
tional Conference on Neural Networks (ICNN-96).
G. Grefenstette, Y. Qu, J. G. Shanahan, and D. A. Evans.
2004. Coupling niche browsers and affect analysis
for an opinion mining application. In Proceedings
of Recherche d?Information Assiste?e par Ordinateur
(RIAO).
D. Ikeda, H. Takamura, L. Ratinov, and M. Okumura.
2008. Learning to shift the polarity of words for senti-
ment classification. In IJCNLP.
S. Kim and E. Hovy. 2007. Crystal: Analyzing predic-
tive opinions on the web. In EMNLP-CoNLL.
A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng,
and C. Potts. 2011. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of ACL.
Y. Mao and G. Lebanon. 2007. Isotonic Conditional
Random Fields and Local Sentiment Flow. In NIPS.
P. Mirowski, M. Ranzato, and Y. LeCun. 2010. Dynamic
auto-encoders for semantic indexing. In Proceedings
of the NIPS 2010 Workshop on Deep Learning.
T. Nakagawa, K. Inui, and S. Kurohashi. 2010. Depen-
dency tree-based sentiment classification using CRFs
with hidden variables. In NAACL, HLT.
B. Pang and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In ACL.
B. Pang and L. Lee. 2005. Seeing stars: Exploiting class
relationships for sentiment categorization with respect
to rating scales. In ACL, pages 115?124.
B. Pang and L. Lee. 2008. Opinion mining and senti-
ment analysis. Foundations and Trends in Information
Retrieval, 2(1-2):1?135.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? Sentiment classification using machine learning
techniques. In EMNLP.
J. W. Pennebaker, R.J. Booth, and M. E. Francis. 2007.
Linguistic inquiry and word count: Liwc2007 opera-
tors manual. University of Texas.
L. Polanyi and A. Zaenen. 2006. Contextual valence
shifters.
J. B. Pollack. 1990. Recursive distributed representa-
tions. Artificial Intelligence, 46:77?105, November.
C. Potts. 2010. On the negativity of negation. In David
Lutz and Nan Li, editors, Proceedings of Semantics
and Linguistic Theory 20. CLC Publications, Ithaca,
NY.
B. Snyder and R. Barzilay. 2007. Multiple aspect rank-
ing using the Good Grief algorithm. In HLT-NAACL.
R. Socher, C. D. Manning, and A. Y. Ng. 2010. Learning
continuous phrase representations and syntactic pars-
ing with recursive neural networks. In Proceedings of
the NIPS-2010 Deep Learning and Unsupervised Fea-
ture Learning Workshop.
R. Socher, C. C. Lin, A. Y. Ng, and C. D. Manning.
2011. Parsing Natural Scenes and Natural Language
with Recursive Neural Networks. In ICML.
P. J. Stone. 1966. The General Inquirer: A Computer
Approach to Content Analysis. The MIT Press.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: a simple and general method for semi-
supervised learning. In Proceedings of ACL, pages
384?394.
P. Turney. 2002. Thumbs up or thumbs down? Seman-
tic orientation applied to unsupervised classification of
reviews. In ACL.
L. Velikovich, S. Blair-Goldensohn, K. Hannan, and
R. McDonald. 2010. The viability of web-derived po-
larity lexicons. In NAACL, HLT.
T. Voegtlin and P. Dominey. 2005. Linear Recursive Dis-
tributed Representations. Neural Networks, 18(7).
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating ex-
pressions of opinions and emotions in language. Lan-
guage Resources and Evaluation, 39.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recogniz-
ing contextual polarity in phrase-level sentiment anal-
ysis. In HLT/EMNLP.
H. Yu and V. Hatzivassiloglou. 2003. Towards answer-
ing opinion questions: Separating facts from opinions
and identifying the polarity of opinion sentences. In
EMNLP.
161
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 873?882,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Improving Word Representations via Global Context
and Multiple Word Prototypes
Eric H. Huang, Richard Socher?, Christopher D. Manning, Andrew Y. Ng
Computer Science Department, Stanford University, Stanford, CA 94305, USA
{ehhuang,manning,ang}@stanford.edu, ?richard@socher.org
Abstract
Unsupervised word representations are very
useful in NLP tasks both as inputs to learning
algorithms and as extra word features in NLP
systems. However, most of these models are
built with only local context and one represen-
tation per word. This is problematic because
words are often polysemous and global con-
text can also provide useful information for
learning word meanings. We present a new
neural network architecture which 1) learns
word embeddings that better capture the se-
mantics of words by incorporating both local
and global document context, and 2) accounts
for homonymy and polysemy by learning mul-
tiple embeddings per word. We introduce a
new dataset with human judgments on pairs of
words in sentential context, and evaluate our
model on it, showing that our model outper-
forms competitive baselines and other neural
language models. 1
1 Introduction
Vector-space models (VSM) represent word mean-
ings with vectors that capture semantic and syntac-
tic information of words. These representations can
be used to induce similarity measures by computing
distances between the vectors, leading to many use-
ful applications, such as information retrieval (Man-
ning et al, 2008), document classification (Sebas-
tiani, 2002) and question answering (Tellex et al,
2003).
1The dataset and word vectors can be downloaded at
http://ai.stanford.edu/?ehhuang/.
Despite their usefulness, most VSMs share a
common problem that each word is only repre-
sented with one vector, which clearly fails to capture
homonymy and polysemy. Reisinger and Mooney
(2010b) introduced a multi-prototype VSM where
word sense discrimination is first applied by clus-
tering contexts, and then prototypes are built using
the contexts of the sense-labeled words. However, in
order to cluster accurately, it is important to capture
both the syntax and semantics of words. While many
approaches use local contexts to disambiguate word
meaning, global contexts can also provide useful
topical information (Ng and Zelle, 1997). Several
studies in psychology have also shown that global
context can help language comprehension (Hess et
al., 1995) and acquisition (Li et al, 2000).
We introduce a new neural-network-based lan-
guage model that distinguishes and uses both local
and global context via a joint training objective. The
model learns word representations that better cap-
ture the semantics of words, while still keeping syn-
tactic information. These improved representations
can be used to represent contexts for clustering word
instances, which is used in the multi-prototype ver-
sion of our model that accounts for words with mul-
tiple senses.
We evaluate our new model on the standard
WordSim-353 (Finkelstein et al, 2001) dataset that
includes human similarity judgments on pairs of
words, showing that combining both local and
global context outperforms using only local or
global context alone, and is competitive with state-
of-the-art methods. However, one limitation of this
evaluation is that the human judgments are on pairs
873
Global ContextLocal Context
scorel scoreg
Document
he walks to the bank... ...
sum
score
river
water
shore
global semantic vector
?
play
weighted average
Figure 1: An overview of our neural language model. The model makes use of both local and global context to compute
a score that should be large for the actual next word (bank in the example), compared to the score for other words.
When word meaning is still ambiguous given local context, information in global context can help disambiguation.
of words presented in isolation, ignoring meaning
variations in context. Since word interpretation in
context is important especially for homonymous and
polysemous words, we introduce a new dataset with
human judgments on similarity between pairs of
words in sentential context. To capture interesting
word pairs, we sample different senses of words us-
ing WordNet (Miller, 1995). The dataset includes
verbs and adjectives, in addition to nouns. We show
that our multi-prototype model improves upon the
single-prototype version and outperforms other neu-
ral language models and baselines on this dataset.
2 Global Context-Aware Neural Language
Model
In this section, we describe the training objective of
our model, followed by a description of the neural
network architecture, ending with a brief description
of our model?s training method.
2.1 Training Objective
Our model jointly learns word representations while
learning to discriminate the next word given a short
word sequence (local context) and the document
(global context) in which the word sequence occurs.
Because our goal is to learn useful word representa-
tions and not the probability of the next word given
previous words (which prohibits looking ahead), our
model can utilize the entire document to provide
global context.
Given a word sequence s and document d in
which the sequence occurs, our goal is to discrim-
inate the correct last word in s from other random
words. We compute scores g(s, d) and g(sw, d)
where sw is swith the last word replaced by wordw,
and g(?, ?) is the scoring function that represents the
neural networks used. We want g(s, d) to be larger
than g(sw, d) by a margin of 1, for any other word
w in the vocabulary, which corresponds to the train-
ing objective of minimizing the ranking loss for each
(s, d) found in the corpus:
Cs,d =
?
w?V
max(0, 1? g(s, d) + g(sw, d)) (1)
Collobert and Weston (2008) showed that this rank-
ing approach can produce good word embeddings
that are useful in several NLP tasks, and allows
much faster training of the model compared to op-
timizing log-likelihood of the next word.
2.2 Neural Network Architecture
We define two scoring components that contribute
to the final score of a (word sequence, document)
pair. The scoring components are computed by two
neural networks, one capturing local context and the
other global context, as shown in Figure 1. We now
describe how each scoring component is computed.
The score of local context uses the local word se-
quence s. We first represent the word sequence s as
874
an ordered list of vectors x = (x1, x2, ..., xm) where
xi is the embedding of word i in the sequence, which
is a column in the embedding matrix L ? Rn?|V |
where |V | denotes the size of the vocabulary. The
columns of this embedding matrix L are the word
vectors and will be learned and updated during train-
ing. To compute the score of local context, scorel,
we use a neural network with one hidden layer:
a1 = f(W1[x1;x2; ...;xm] + b1) (2)
scorel = W2a1 + b2 (3)
where [x1;x2; ...;xm] is the concatenation of the
m word embeddings representing sequence s, f is
an element-wise activation function such as tanh,
a1 ? Rh?1 is the activation of the hidden layer with
h hidden nodes, W1 ? Rh?(mn) and W2 ? R1?h
are respectively the first and second layer weights of
the neural network, and b1, b2 are the biases of each
layer.
For the score of the global context, we represent
the document also as an ordered list of word em-
beddings, d = (d1, d2, ..., dk). We first compute the
weighted average of all word vectors in the docu-
ment:
c =
?k
i=1w(ti)di
?k
i=1w(ti)
(4)
where w(?) can be any weighting function that cap-
tures the importance of word ti in the document. We
use idf-weighting as the weighting function.
We use a two-layer neural network to compute the
global context score, scoreg, similar to the above:
a1
(g) = f(W (g)1 [c;xm] + b
(g)
1 ) (5)
scoreg = W
(g)
2 a
(g)
1 + b
(g)
2 (6)
where [c;xm] is the concatenation of the weighted
average document vector and the vector of the last
word in s, a1(g) ? Rh
(g)?1 is the activation of
the hidden layer with h(g) hidden nodes, W (g)1 ?
Rh
(g)?(2n) and W (g)2 ? R
1?h(g) are respectively the
first and second layer weights of the neural network,
and b(g)1 , b
(g)
2 are the biases of each layer. Note that
instead of using the document where the sequence
occurs, we can also specify a fixed k > m that cap-
tures larger context.
The final score is the sum of the two scores:
score = scorel + scoreg (7)
The local score preserves word order and syntactic
information, while the global score uses a weighted
average which is similar to bag-of-words features,
capturing more of the semantics and topics of the
document. Note that Collobert and Weston (2008)?s
language model corresponds to the network using
only local context.
2.3 Learning
Following Collobert and Weston (2008), we sample
the gradient of the objective by randomly choosing
a word from the dictionary as a corrupt example for
each sequence-document pair, (s, d), and take the
derivative of the ranking loss with respect to the pa-
rameters: weights of the neural network and the em-
bedding matrix L. These weights are updated via
backpropagation. The embedding matrix L is the
word representations. We found that word embed-
dings move to good positions in the vector space
faster when using mini-batch L-BFGS (Liu and No-
cedal, 1989) with 1000 pairs of good and corrupt ex-
amples per batch for training, compared to stochas-
tic gradient descent.
3 Multi-Prototype Neural Language
Model
Despite distributional similarity models? successful
applications in various NLP tasks, one major limi-
tation common to most of these models is that they
assume only one representation for each word. This
single-prototype representation is problematic be-
cause many words have multiple meanings, which
can be wildly different. Using one representa-
tion simply cannot capture the different meanings.
Moreover, using all contexts of a homonymous or
polysemous word to build a single prototype could
hurt the representation, which cannot represent any
one of the meanings well as it is influenced by all
meanings of the word.
Instead of using only one representation per word,
Reisinger and Mooney (2010b) proposed the multi-
prototype approach for vector-space models, which
uses multiple representations to capture different
senses and usages of a word. We show how our
875
model can readily adopt the multi-prototype ap-
proach. We present a way to use our learned
single-prototype embeddings to represent each con-
text window, which can then be used by clustering to
perform word sense discrimination (Schu?tze, 1998).
In order to learn multiple prototypes, we first
gather the fixed-sized context windows of all occur-
rences of a word (we use 5 words before and after
the word occurrence). Each context is represented
by a weighted average of the context words? vectors,
where again, we use idf-weighting as the weighting
function, similar to the document context represen-
tation described in Section 2.2. We then use spheri-
cal k-means to cluster these context representations,
which has been shown to model semantic relations
well (Dhillon and Modha, 2001). Finally, each word
occurrence in the corpus is re-labeled to its associ-
ated cluster and is used to train the word representa-
tion for that cluster.
Similarity between a pair of words (w,w?) us-
ing the multi-prototype approach can be computed
with or without context, as defined by Reisinger and
Mooney (2010b):
AvgSimC(w,w?) =
1
K2
k?
i=1
k?
j=1
p(c, w, i)p(c?, w?, j)d(?i(w), ?j(w
?))
(8)
where p(c, w, i) is the likelihood that word w is in
its cluster i given context c, ?i(w) is the vector rep-
resenting the i-th cluster centroid of w, and d(v, v?)
is a function computing similarity between two vec-
tors, which can be any of the distance functions pre-
sented by Curran (2004). The similarity measure can
be computed in absence of context by assuming uni-
form p(c, w, i) over i.
4 Experiments
In this section, we first present a qualitative analysis
comparing the nearest neighbors of our model?s em-
beddings with those of others, showing our embed-
dings better capture the semantics of words, with the
use of global context. Our model also improves the
correlation with human judgments on a word simi-
larity task. Because word interpretation in context is
important, we introduce a new dataset with human
judgments on similarity of pairs of words in senten-
tial context. Finally, we show that our model outper-
forms other methods on this dataset and also that the
multi-prototype approach improves over the single-
prototype approach.
We chose Wikipedia as the corpus to train all
models because of its wide range of topics and
word usages, and its clean organization of docu-
ment by topic. We used the April 2010 snapshot of
the Wikipedia corpus (Shaoul and Westbury, 2010),
with a total of about 2 million articles and 990 mil-
lion tokens. We use a dictionary of the 30,000 most
frequent words in Wikipedia, converted to lower
case. In preprocessing, we keep the frequent num-
bers intact and replace each digit of the uncommon
numbers to ?DG? so as to preserve information such
as it being a year (e.g. ?DGDGDGDG?). The con-
verted numbers that are rare are mapped to a NUM-
BER token. Other rare words not in the dictionary
are mapped to an UNKNOWN token.
For all experiments, our models use 50-
dimensional embeddings. We use 10-word windows
of text as the local context, 100 hidden units, and no
weight regularization for both neural networks. For
multi-prototype variants, we fix the number of pro-
totypes to be 10.
4.1 Qualitative Evaluations
In order to show that our model learns more seman-
tic word representations with global context, we give
the nearest neighbors of our single-prototype model
versus C&W?s, which only uses local context. The
nearest neighbors of a word are computed by com-
paring the cosine similarity between the center word
and all other words in the dictionary. Table 1 shows
the nearest neighbors of some words. The nearest
neighbors of ?market? that C&W?s embeddings give
are more constrained by the syntactic constraint that
words in plural form are only close to other words
in plural form, whereas our model captures that the
singular and plural forms of a word are similar in
meaning. Other examples show that our model in-
duces nearest neighbors that better capture seman-
tics.
Table 2 shows the nearest neighbors of our model
using the multi-prototype approach. We see that
the clustering is able to group contexts of different
876
Center
Word
C&W Our Model
markets firms, industries,
stores
market, firms,
businesses
American Australian,
Indian, Italian
U.S., Canadian,
African
illegal alleged, overseas,
banned
harmful, prohib-
ited, convicted
Table 1: Nearest neighbors of words based on cosine sim-
ilarity. Our model is less constrained by syntax and is
more semantic.
Center Word Nearest Neighbors
bank 1 corporation, insurance, company
bank 2 shore, coast, direction
star 1 movie, film, radio
star 2 galaxy, planet, moon
cell 1 telephone, smart, phone
cell 2 pathology, molecular, physiology
left 1 close, leave, live
left 2 top, round, right
Table 2: Nearest neighbors of word embeddings learned
by our model using the multi-prototype approach based
on cosine similarity. The clustering is able to find the dif-
ferent meanings, usages, and parts of speech of the words.
meanings of a word into separate groups, allowing
our model to learn multiple meaningful representa-
tions of a word.
4.2 WordSim-353
A standard dataset for evaluating vector-space mod-
els is the WordSim-353 dataset (Finkelstein et al,
2001), which consists of 353 pairs of nouns. Each
pair is presented without context and associated with
13 to 16 human judgments on similarity and re-
latedness on a scale from 0 to 10. For example,
(cup,drink) received an average score of 7.25, while
(cup,substance) received an average score of 1.92.
Table 3 shows our results compared to previous
methods, including C&W?s language model and the
hierarchical log-bilinear (HLBL) model (Mnih and
Hinton, 2008), which is a probabilistic, linear neu-
ral model. We downloaded these embeddings from
Turian et al (2010). These embeddings were trained
on the smaller corpus RCV1 that contains one year
of Reuters English newswire, and show similar cor-
relations on the dataset. We report the result of
Model Corpus ?? 100
Our Model-g Wiki. 22.8
C&W RCV1 29.5
HLBL RCV1 33.2
C&W* Wiki. 49.8
C&W Wiki. 55.3
Our Model Wiki. 64.2
Our Model* Wiki. 71.3
Pruned tf-idf Wiki. 73.4
ESA Wiki. 75
Tiered Pruned tf-idf Wiki. 76.9
Table 3: Spearman?s ? correlation on WordSim-353,
showing our model?s improvement over previous neural
models for learning word embeddings. C&W* is the
word embeddings trained and provided by C&W. Our
Model* is trained without stop words, while Our Model-
g uses only global context. Pruned tf-idf (Reisinger and
Mooney, 2010b) and ESA (Gabrilovich and Markovitch,
2007) are also included.
our re-implementation of C&W?s model trained on
Wikipedia, showing the large effect of using a dif-
ferent corpus.
Our model is able to learn more semantic word
embeddings and noticeably improves upon C&W?s
model. Note that our model achieves higher corre-
lation (64.2) than either using local context alone
(C&W: 55.3) or using global context alone (Our
Model-g: 22.8). We also found that correlation can
be further improved by removing stop words (71.3).
Thus, each window of text (training example) con-
tains more information but still preserves some syn-
tactic information as the words are still ordered in
the local context.
4.3 New Dataset: Word Similarity in Context
The many previous datasets that associate human
judgments on similarity between pairs of words,
such as WordSim-353, MC (Miller and Charles,
1991) and RG (Rubenstein and Goodenough, 1965),
have helped to advance the development of vector-
space models. However, common to all datasets is
that similarity scores are given to pairs of words in
isolation. This is problematic because the mean-
ings of homonymous and polysemous words depend
highly on the words? contexts. For example, in the
two phrases, ?he swings the baseball bat? and ?the
877
Word 1 Word 2
Located downtown along the east bank of the Des
Moines River ...
This is the basis of all money laundering , a track record
of depositing clean money before slipping through dirty
money ...
Inside the ruins , there are bats and a bowl with Pokeys
that fills with sand over the course of the race , and the
music changes somewhat while inside ...
An aggressive lower order batsman who usually bats at
No. 11 , Muralitharan is known for his tendency to back
away to leg and slog ...
An example of legacy left in the Mideast from these
nobles is the Krak des Chevaliers ? enlargement by the
Counts of Tripoli and Toulouse ...
... one should not adhere to a particular explanation ,
only in such measure as to be ready to abandon it if it
be proved with certainty to be false ...
... and Andy ?s getting ready to pack his bags and head
up to Los Angeles tomorrow to get ready to fly back
home on Thursday
... she encounters Ben ( Duane Jones ) , who arrives
in a pickup truck and defends the house against another
pack of zombies ...
In practice , there is an unknown phase delay between
the transmitter and receiver that must be compensated
by ? synchronization ? of the receivers local oscillator
... but Gilbert did not believe that she was dedicated
enough , and when she missed a rehearsal , she was
dismissed ...
Table 4: Example pairs from our new dataset. Note that words in a pair can be the same word and have different parts
of speech.
bat flies?, bat has completely different meanings. It
is unclear how this variation in meaning is accounted
for in human judgments of words presented without
context.
One of the main contributions of this paper is the
creation of a new dataset that addresses this issue.
The dataset has three interesting characteristics: 1)
human judgments are on pairs of words presented in
sentential context, 2) word pairs and their contexts
are chosen to reflect interesting variations in mean-
ings of homonymous and polysemous words, and 3)
verbs and adjectives are present in addition to nouns.
We now describe our methodology in constructing
the dataset.
4.3.1 Dataset Construction
Our procedure of constructing the dataset consists
of three steps: 1) select a list a words, 2) for each
word, select another word to form a pair, 3) for each
word in a pair, find a sentential context. We now
describe each step in detail.
In step 1, in order to make sure we select a diverse
list of words, we consider three attributes of a word:
frequency in a corpus, number of parts of speech,
and number of synsets according to WordNet. For
frequency, we divide words into three groups, top
2,000 most frequent, between 2,000 and 5,000, and
between 5,000 to 10,000 based on occurrences in
Wikipedia. For number of parts of speech, we group
words based on their number of possible parts of
speech (noun, verb or adjective), from 1 to 3. We
also group words by their number of synsets: [0,5],
[6,10], [11, 20], and [20, max]. Finally, we sam-
ple at most 15 words from each combination in the
Cartesian product of the above groupings.
In step 2, for each of the words selected in step
1, we want to choose the other word so that the pair
captures an interesting relationship. Similar to Man-
andhar et al (2010), we use WordNet to first ran-
domly select one synset of the first word, we then
construct a set of words in various relations to the
first word?s chosen synset, including hypernyms, hy-
ponyms, holonyms, meronyms and attributes. We
randomly select a word from this set of words as the
second word in the pair. We try to repeat the above
twice to generate two pairs for each word. In addi-
tion, for words with more than five synsets, we allow
the second word to be the same as the first, but with
different synsets. We end up with pairs of words as
well as the one chosen synset for each word in the
pairs.
In step 3, we aim to extract a sentence from
Wikipedia for each word, which contains the word
and corresponds to a usage of the chosen synset.
We first find all sentences in which the word oc-
curs. We then POS tag2 these sentences and filter out
those that do not match the chosen POS. To find the
2We used the MaxEnt Treebank POS tagger in the python
nltk library.
878
Model ?? 100
C&W-S 57.0
Our Model-S 58.6
Our Model-M AvgSim 62.8
Our Model-M AvgSimC 65.7
tf-idf-S 26.3
Pruned tf-idf-S 62.5
Pruned tf-idf-M AvgSim 60.4
Pruned tf-idf-M AvgSimC 60.5
Table 5: Spearman?s ? correlation on our new
dataset. Our Model-S uses the single-prototype approach,
while Our Model-M uses the multi-prototype approach.
AvgSim calculates similarity with each prototype con-
tributing equally, while AvgSimC weighs the prototypes
according to probability of the word belonging to that
prototype?s cluster.
word usages that correspond to the chosen synset,
we first construct a set of related words of the chosen
synset, including hypernyms, hyponyms, holonyms,
meronyms and attributes. Using this set of related
words, we filter out a sentence if the document in
which the sentence appears does not include one of
the related words. Finally, we randomly select one
sentence from those that are left.
Table 4 shows some examples from the dataset.
Note that the dataset alo includes pairs of the same
word. Single-prototype models would give the max
similarity score for those pairs, which can be prob-
lematic depending on the words? contexts. This
dataset requires models to examine context when de-
termining word meaning.
Using Amazon Mechanical Turk, we collected 10
human similarity ratings for each pair, as Snow et
al. (2008) found that 10 non-expert annotators can
achieve very close inter-annotator agreement with
expert raters. To ensure worker quality, we only
allowed workers with over 95% approval rate to
work on our task. Furthermore, we discarded all
ratings by a worker if he/she entered scores out of
the accepted range or missed a rating, signaling low-
quality work.
We obtained a total of 2,003 word pairs and their
sentential contexts. The word pairs consist of 1,712
unique words. Of the 2,003 word pairs, 1328 are
noun-noun pairs, 399 verb-verb, 140 verb-noun, 97
adjective-adjective, 30 noun-adjective, and 9 verb-
adjective. 241 pairs are same-word pairs.
4.3.2 Evaluations on Word Similarity in
Context
For evaluation, we also compute Spearman corre-
lation between a model?s computed similarity scores
and human judgments. Table 5 compares different
models? results on this dataset. We compare against
the following baselines: tf-idf represents words in
a word-word matrix capturing co-occurrence counts
in all 10-word context windows. Reisinger and
Mooney (2010b) found pruning the low-value tf-idf
features helps performance. We report the result
of this pruning technique after tuning the thresh-
old value on this dataset, removing all but the top
200 features in each word vector. We tried the
same multi-prototype approach and used spherical
k-means3 to cluster the contexts using tf-idf repre-
sentations, but obtained lower numbers than single-
prototype (55.4 with AvgSimC). We then tried using
pruned tf-idf representations on contexts with our
clustering assignments (included in Table 5), but still
got results worse than the single-prototype version
of the pruned tf-idf model (60.5 with AvgSimC).
This suggests that the pruned tf-idf representations
might be more susceptible to noise or mistakes in
context clustering.
By utilizing global context, our model outper-
forms C&W?s vectors and the above baselines on
this dataset. With multiple representations per
word, we show that the multi-prototype approach
can improve over the single-prototype version with-
out using context (62.8 vs. 58.6). Moreover, using
AvgSimC4 which takes contexts into account, the
multi-prototype model obtains the best performance
(65.7).
5 Related Work
Neural language models (Bengio et al, 2003; Mnih
and Hinton, 2007; Collobert and Weston, 2008;
Schwenk and Gauvain, 2002; Emami et al, 2003)
have been shown to be very powerful at language
modeling, a task where models are asked to ac-
curately predict the next word given previously
seen words. By using distributed representations of
3We first tried movMF as in Reisinger and Mooney (2010b),
but were unable to get decent results (only 31.5).
4probability of being in a cluster is calculated as the inverse
of the distance to the cluster centroid.
879
words which model words? similarity, this type of
models addresses the data sparseness problem that
n-gram models encounter when large contexts are
used. Most of these models used relative local con-
texts of between 2 to 10 words. Schwenk and Gau-
vain (2002) tried to incorporate larger context by
combining partial parses of past word sequences and
a neural language model. They used up to 3 previ-
ous head words and showed increased performance
on language modeling. Our model uses a similar
neural network architecture as these models and uses
the ranking-loss training objective proposed by Col-
lobert and Weston (2008), but introduces a new way
to combine local and global context to train word
embeddings.
Besides language modeling, word embeddings in-
duced by neural language models have been use-
ful in chunking, NER (Turian et al, 2010), parsing
(Socher et al, 2011b), sentiment analysis (Socher et
al., 2011c) and paraphrase detection (Socher et al,
2011a). However, they have not been directly eval-
uated on word similarity tasks, which are important
for tasks such as information retrieval and summa-
rization. Our experiments show that our word em-
beddings are competitive in word similarity tasks.
Most of the previous vector-space models use a
single vector to represent a word even though many
words have multiple meanings. The multi-prototype
approach has been widely studied in models of cat-
egorization in psychology (Rosseel, 2002; Griffiths
et al, 2009), while Schu?tze (1998) used clustering
of contexts to perform word sense discrimination.
Reisinger and Mooney (2010b) combined the two
approaches and applied them to vector-space mod-
els, which was further improved in Reisinger and
Mooney (2010a). Two other recent papers (Dhillon
et al, 2011; Reddy et al, 2011) present models
for constructing word representations that deal with
context. It would be interesting to evaluate those
models on our new dataset.
Many datasets with human similarity ratings on
pairs of words, such as WordSim-353 (Finkelstein
et al, 2001), MC (Miller and Charles, 1991) and
RG (Rubenstein and Goodenough, 1965), have been
widely used to evaluate vector-space models. Moti-
vated to evaluate composition models, Mitchell and
Lapata (2008) introduced a dataset where an intran-
sitive verb, presented with a subject noun, is com-
pared to another verb chosen to be either similar or
dissimilar to the intransitive verb in context. The
context is short, with only one word, and only verbs
are compared. Erk and Pado? (2008), Thater et al
(2011) and Dinu and Lapata (2010) evaluated word
similarity in context with a modified task where sys-
tems are to rerank gold-standard paraphrase candi-
dates given the SemEval 2007 Lexical Substitution
Task dataset. This task only indirectly evaluates sim-
ilarity as only reranking of already similar words are
evaluated.
6 Conclusion
We presented a new neural network architecture that
learns more semantic word representations by us-
ing both local and global context in learning. These
learned word embeddings can be used to represent
word contexts as low-dimensional weighted average
vectors, which are then clustered to form different
meaning groups and used to learn multi-prototype
vectors. We introduced a new dataset with human
judgments on similarity between pairs of words in
context, so as to evaluate model?s abilities to capture
homonymy and polysemy of words in context. Our
new multi-prototype neural language model outper-
forms previous neural models and competitive base-
lines on this new dataset.
Acknowledgments
The authors gratefully acknowledges the support of
the Defense Advanced Research Projects Agency
(DARPA) Machine Reading Program under Air
Force Research Laboratory (AFRL) prime contract
no. FA8750-09-C-0181, and the DARPA Deep
Learning program under contract number FA8650-
10-C-7020. Any opinions, findings, and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reflect
the view of DARPA, AFRL, or the US government.
References
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent,
Christian Jauvin, Jaz K, Thomas Hofmann, Tomaso
Poggio, and John Shawe-taylor. 2003. A neural prob-
abilistic language model. Journal of Machine Learn-
ing Research, 3:1137?1155.
880
Ronan Collobert and Jason Weston. 2008. A unified ar-
chitecture for natural language processing: deep neu-
ral networks with multitask learning. In Proceedings
of the 25th international conference on Machine learn-
ing, ICML ?08, pages 160?167, New York, NY, USA.
ACM.
James Richard Curran. 2004. From distributional to se-
mantic similarity. Technical report.
Inderjit S. Dhillon and Dharmendra S. Modha. 2001.
Concept decompositions for large sparse text data us-
ing clustering. Mach. Learn., 42:143?175, January.
Paramveer S. Dhillon, Dean Foster, and Lyle Ungar.
2011. Multi-view learning of word embeddings via
cca. In Advances in Neural Information Processing
Systems (NIPS), volume 24.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, EMNLP ?10, pages 1162?1172,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Ahmad Emami, Peng Xu, and Frederick Jelinek. 2003.
Using a connectionist model in a syntactical based lan-
guage model. In Acoustics, Speech, and Signal Pro-
cessing, pages 372?375.
Katrin Erk and Sebastian Pado?. 2008. A structured
vector space model for word meaning in context. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?08,
pages 897?906, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2001. Placing search in context: the con-
cept revisited. In Proceedings of the 10th international
conference on World Wide Web, WWW ?01, pages
406?414, New York, NY, USA. ACM.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting semantic relatedness using wikipedia-based
explicit semantic analysis. In Proceedings of the
20th international joint conference on Artifical intel-
ligence, IJCAI?07, pages 1606?1611, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Thomas L Griffiths, Kevin R Canini, Adam N Sanborn,
and Daniel J Navarro. 2009. Unifying rational models
of categorization via the hierarchical dirichlet process.
Brain, page 323328.
David J Hess, Donald J Foss, and Patrick Carroll. 1995.
Effects of global and local context on lexical process-
ing during language comprehension. Journal of Ex-
perimental Psychology: General, 124(1):62?82.
Ping Li, Curt Burgess, and Kevin Lund. 2000. The ac-
quisition of word meaning through global lexical co-
occurrences.
D. C. Liu and J. Nocedal. 1989. On the limited mem-
ory bfgs method for large scale optimization. Math.
Program., 45(3):503?528, December.
Suresh Manandhar, Ioannis P Klapaftis, Dmitriy Dligach,
and Sameer S Pradhan. 2010. Semeval-2010 task
14: Word sense induction & disambiguation. Word
Journal Of The International Linguistic Association,
(July):63?68.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schtze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, New York, NY,
USA.
George A Miller and Walter G Charles. 1991. Contextual
correlates of semantic similarity. Language & Cogni-
tive Processes, 6(1):1?28.
George A. Miller. 1995. Wordnet: A lexical database for
english. Communications of the ACM, 38:39?41.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In In Proceedings of
ACL-08: HLT, pages 236?244.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling. In
Proceedings of the 24th international conference on
Machine learning, ICML ?07, pages 641?648, New
York, NY, USA. ACM.
Andriy Mnih and Geoffrey Hinton. 2008. A scalable
hierarchical distributed language model. In In NIPS.
Ht Ng and J Zelle. 1997. Corpus-based approaches to
semantic interpretation in natural language processing.
AI Magazine, 18(4):45?64.
Siva Reddy, Ioannis Klapaftis, Diana McCarthy, and
Suresh Manandhar. 2011. Dynamic and static proto-
type vectors for semantic composition. In Proceedings
of 5th International Joint Conference on Natural Lan-
guage Processing, pages 705?713, Chiang Mai, Thai-
land, November. Asian Federation of Natural Lan-
guage Processing.
Joseph Reisinger and Raymond Mooney. 2010a. A mix-
ture model with sharing for lexical semantics. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?10,
pages 1173?1182, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Joseph Reisinger and Raymond J. Mooney. 2010b.
Multi-prototype vector-space models of word mean-
ing. In Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter of the
Association for Computational Linguistics, HLT ?10,
pages 109?117, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Yves Rosseel. 2002. Mixture models of categorization.
Journal of Mathematical Psychology, 46:178?210.
881
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Commun. ACM,
8:627?633, October.
Hinrich Schu?tze. 1998. Automatic word sense discrimi-
nation. Journal of Computational Linguistics, 24:97?
123.
Holger Schwenk and Jean-luc Gauvain. 2002. Connec-
tionist language modeling for large vocabulary con-
tinuous speech recognition. In In International Con-
ference on Acoustics, Speech and Signal Processing,
pages 765?768.
Fabrizio Sebastiani. 2002. Machine learning in auto-
mated text categorization. ACM Comput. Surv., 34:1?
47, March.
Cyrus Shaoul and Chris Westbury. 2010. The westbury
lab wikipedia corpus.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast?but is it good?:
evaluating non-expert annotations for natural language
tasks. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
?08, pages 254?263, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Richard Socher, Eric H. Huang, Jeffrey Pennington, An-
drew Y. Ng, and Christopher D. Manning. 2011a. Dy-
namic pooling and unfolding recursive autoencoders
for paraphrase detection. In Advances in Neural Infor-
mation Processing Systems 24.
Richard Socher, Cliff C. Lin, Andrew Y. Ng, and Christo-
pher D. Manning. 2011b. Parsing natural scenes and
natural language with recursive neural networks. In
Proceedings of the 26th International Conference on
Machine Learning (ICML).
Richard Socher, Jeffrey Pennington, Eric H. Huang, An-
drew Y. Ng, and Christopher D. Manning. 2011c.
Semi-supervised recursive autoencoders for predicting
sentiment distributions. In Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP).
Stefanie Tellex, Boris Katz, Jimmy Lin, Aaron Fernan-
des, and Gregory Marton. 2003. Quantitative evalu-
ation of passage retrieval algorithms for question an-
swering. In Proceedings of the 26th Annual Interna-
tional ACM SIGIR Conference on Search and Devel-
opment in Information Retrieval, pages 41?47. ACM
Press.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2011. Word meaning in context: a simple and effec-
tive vector model. In Proceedings of the 5th Interna-
tional Joint Conference on Natural Language Process-
ing, IJCNLP ?11.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics, ACL ?10, pages 384?394, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
882
