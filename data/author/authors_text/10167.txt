How to Take Advantage of the Limitations with Markov Clustering? 
The Foundations of Branching Markov Clustering (BMCL) 
Hiroyuki Akama 
Tokyo Institute of Technology 
W9-10, 2-12-1, 
O-okayama, Meguroku, 
152-8552 Tokyo, Japan 
akama.h.aa@m.titech.ac.jp 
Maki Miyake 
University of Osaka 
Machikane-machi, Toyo-
naka-shi, 
560-0043 Osaka, Japan 
mmiyake@lang.osaka-
u.ac.jp 
Jaeyoung Jung 
Tokyo Institute of Technology 
W9-10, 2-12-1, 
O-okayama, Meguroku, 
152-8552 Tokyo, Japan 
jung.j.aa@m.titech.ac.jp 
 
Abstract 
In this paper, we propose a novel approach 
to optimally employing the MCL (Markov 
Cluster Algorithm) by ?neutralizing? the 
trivial disadvantages acknowledged by its 
original proposer.  Our BMCL (Branching 
Markov Clustering) algorithm makes it 
possible to subdivide a large core cluster 
into appropriately resized sub-graphs.  Util-
izing three corpora, we examine the effects 
of the BMCL which varies according to the 
curvature (clustering coefficient) of a hub 
in a network. 
1 MCL limitations? 
1.1 MCL and modularity Q 
The Markov Cluster Algorithm (MCL) (Van Don-
gen, 2000) is well-recognized as an effective 
method of graph clustering.  It involves changing 
the values of a transition matrix toward either 0 or 
1 at each step in a random walk until the stochastic 
condition is satisfied.  When the hadamard power 
for each transition probability value is divided by 
the sum of each column, the rescaling process 
yields a transition matrix for the next stage.  After 
repeatedly alternating for about 20 times between 
two steps?random walk (expansion) and probabil-
ity modification (inflation)?the process will fi-
nally reach a convergence stage in which the whole 
graph is subdivided into a set of ?hard? clusters that 
have no overlap.  Although this method has been 
generally applied in various domains with notable 
successes (such as Tribe-MCL clustering of pro-
teins (Enright et al, 2002); Synonymy Network, 
created by the addition of noise data (Gfeller, 
2005); and Lexical Acquisition (Dorow et al, 
2005)), Van Dongen et al (2001) frankly acknowl-
edge that there are limitations or weaknesses.  For 
instance, the readme file, which is included with 
the free MCL software available via the Internet 
from Van Dongen?s group, remarks that ?MCL is 
probably not suited for clustering tree graphs?. 
It should also be noted, however, that the group 
has provided no mathematical evidence for their 
claim of the MCL?s unsuitability for hierarchical 
applications.  What prompts this subtle caveat in 
the first place?  Is this a limitation on the type of 
graph clustering that can employ random walks for 
spectral analysis?  Or, is it difficult for this tech-
nique to (re-)form or adjust graph clusters that 
have already been clustered into a kind of multi-
layered organization?  Such questions are very im-
portant when comparing the MCL with other graph 
clustering methods that employ (greedy) algo-
rithms developed step by step in a tree form. 
A tree graph is essentially a kind of dendrogram, 
which means clustering results can be generated 
solely by making a cross cut at some height be-
tween the root and the leaves.  In other words, as 
there is no horizontal connection at the same level, 
it is not possible to create triangle circulation paths 
in a single stroke.  However, the graph coefficient 
known as ?curvature? (Dorow, 2005) is appropri-
ate for defining such structures.  The curvature, or 
the cluster coefficient, of a vertex is defined as a 
fraction of existing links among a node?s neighbors 
out of all possible links between neighbors.  Thus, 
a tree graph may be regarded as a chain of star 
graphs where all the vertices have a curvature val-
ue of 0. 
901
It is certainly true that when a hub has a low 
curvature value, the corresponding cluster will be 
less cohesive and more sparse than usual.  The 
modularity Q value is very low in such cases when 
we try to measure the accuracy of results from 
MCL clustering.  Modularity Q indicates differ-
ences in edge distributions between a graph with 
meaningful partitions and a random graph for iden-
tical vertices conditions.  According to Newman 
and Girvan, ? ?=
i
iii aeQ )(
2 , where i is the clus-
ter number of cluster ic , iie is the proportion of 
internal links in the whole graph and ia  is the ex-
pected proportion of ic ?s edges calculated as the 
total number of degrees in ic divided by the total of 
all degrees (2*the number of all edges) in the 
whole graph.  This value has been widely used as 
an index to evaluate the accuracy of clustering re-
sults. 
1.2 Karate club simulation 
However, it would be an exaggeration to regard 
Modularity Q is an almighty tool for accurately 
determining the attribution value of each vertex in 
a graph cluster.  That is only true for modularity-
based greedy algorithms that select vertices pair-
ings be merged into a cluster at each step of the 
tree-form integration process based on modularity 
optimization criterion.  However, such methods 
suffer from the problem that once a merger is exe-
cuted based on a discrimination error, there is no 
chance of subsequently splitting pairings that be-
long to different subgroups. 
This fatal error can be illustrated as follows.  
Zachary?s famous ?Karate Club? is often used as 
supervised data for graph clustering, because the 
complex relationships among the club members are 
presented as a graph composed of edges represent-
ing acquaintances and vertices coded indicating 
final attachments to factions.  If the results of 
graph clustering were to match with the actual 
composition of sects within the club, one could 
claim that the tested method was capable of simu-
lating the social relationships. 
However, the real difficulties lie at boundary 
positions.  It is worth pointing out that the degree 
of ambiguity is the same (0.5) for both vertices 3 
and 10 in Figure I, indicating that they occupy neu-
tral positions while in reality they belong to differ-
ent subgroups.  All modularity-based greedy algo-
rithms would inevitably bind the two nodes at an 
earlier step in the dendrogram construction (at the 
first merging step in experiments conducted by 
Newman and Danon and at the second in Pujol?s 
experiment).  In contrast, MCL is one of the rare 
clustering methods that avoids this type of mis-
judgment (accurate results for the karate club net-
work were also obtained with the Ward method), 
even though the modularity Q value for MCL is a 
little lower (0.371) than values for greedy algo-
rithms (for example, 0.3807 for Newman et al?s 
fast algorithm and 0.418 for Danon et al?s modi-
fied algorithm). 
 
Figure I Karate club  
 
The karate club case suggests the possibility of 
using both graph clustering and modularity Q from 
different perspectives.  MCL allows us to regard 
both clustering and discrimination on the same 
plan if we do not treat modularity Q as an optimi-
zation index but rather as an index of structuring 
dynamics balancing assembly and division.  To the 
extent that a graph clustering method is evaluated 
in terms of its effectiveness in a variety of dis-
crimination analyses with learning data extracted 
from real situations, it should be useful as a simu-
lation tool.  For example, it is possible to test with 
the karate club network the effects of supplement-
ing the network by adding to the original graph 
another hub with the highest degree value.  As the 
curvature value of this new hub varies according to 
the selection of vertices which become adjacent to 
it, we can re-execute MCL for the overall graph to 
see how curvature is closely related with how it 
influences clustering results.  In general cases, the 
hub of a whole graph also tends to be the represen-
tative node for the large-sized Markov cluster 
called the ?core cluster? (Jung, 2006). 
902
Let us imagine that a highly influential new-
comer joins the karate club and tries to contact 
with half (17) of all the members, functioning as a 
hub within the network.  Even though this is a 
purely hypothetical situation, it is possible to pre-
dict the impact on the network with MCL.   
 
 
 
 
Figures II, III Hub to high or low degree nodes 
 
For example, one could classify the 34 vertices 
into higher and lower degree subgroups, and set a 
hub that is adjacent to all vertices for one subgroup 
but is far from the other subgroup.  MCL results 
would indicate that even when adding a hub with 
the highest curvature value, it would be ineffectual 
in preventing a split (Figure II). However, if the 
newcomer were to be a friend with less sociable 
members, the club would be saved from being torn 
apart.  A hub connected with the lower degree sub-
group, and thus having the lowest curvature value, 
would become part of the largest core cluster, be-
cause the MCL would not subdivide the graph 
(Figure III).  In short, the results of MCL computa-
tion hinge on the curvature value of the hub with 
the highest degree value. 
2 The basic concept of BMCL 
This connection-sensitive feature of MCL brings 
us back to the limitations that Van Dogen et al 
inform their software users of.  Do these limita-
tions really render the MCL unsuitable for tree 
graphs?  Should we not regard a low modularity Q 
value for a graph as a positive attribute if it is due 
to the low curvature value for a hub?  In a very real 
sense, these questions are actually asking about the 
same thing.  The point can be clearer if conceived 
of in relation to a non-directed and cascading type 
of three-layer graph, as depicted in Figure IV. 
Figure IV Three-layer tree-form network 
 
The root node at the top (the hub) is linked to all 
the vertices in the intermediate layer but to none at 
the bottom layer, even though there are moderate 
levels of connectivity between the layers.  Connec-
tions within a layer are extremely rare or absent.  
Clearly, the curvature of the hub would be influ-
enced by the very low connectivity within layer 2. 
 
 0.01 0.02 0.03 
0.1 
1core 
cluster & 
singleton 
clusters 
1core cluster & 
singleton clus-
ters 
1cluster 
(not divided) 
0.15 
1cluster or 2 
core 
clusters 
1cluster 
(not divided) 
1cluster 
(not divided) 
0.2 2core clusters 
2core 
clusters 
1cluster 
(not divided) 
Table I. MCL results for the structured Random Graph 
903
We have executed computations at least 10 
times under the same condition in order to generate 
this type of structured random graph with 500 ver-
tices in the two layers respectively.  A random 
graph was produced by using a binominal distribu-
tion.  Although between connection rates were var-
ied from 0.1 to 0.2 and within connection rates for 
the intermediate layer from 0.01 to 0.03, no edges 
were inserted into the lower layer.  MCL results 
obtained for this architecture are almost constant, 
as shown in Table I. 
In this experiment, all singleton clusters con-
sisted of vertices belonging to layer 2.  In cases 
where the whole graph was split into 2 core clus-
ters, one cluster would correspond to the hub plus 
layer 3 while the other would correspond to layer 2.  
There was no exception when the between connec-
tion rate was 0.2.  This means that, quite curiously, 
the hub formed a core cluster around itself with 
vertices that were not all adjacent to it, so that ones 
that were connected with it in the raw data were all 
segregated into the other cluster.  In this case, the 
Modularity Q value for each core cluster was zero 
or extremely low. 
Nevertheless, in spite of this inaccuracy, this 
type of network can easily be by modified by the 
BMCL method that we discuss later.  It can be 
indirectly subdivided by graph clustering, if inside 
the same cluster, a latent shortcut is set between 
one vertex and another.  Such a latent connection 
can be counted in place of a path of length 2 that is 
traced in the original adjacency as a detour via a 
vertex of another cluster.  If all latent adjacency 
relationships are enumerated in this way, except 
for those for the hub, the core cluster will be re-
clustered by a second application of the MCL to 
realize a sort of hierarchical clustering (in this case 
for a quasi-tree graph), which has been regarded as 
being a limitation with the MCL. 
This principle can be called Branching Markov 
Clustering (BMCL) in the sense that it makes it 
possible to correct for unbalances in cluster-sizes 
by dividing large Markov clusters into appropriate 
branches.  In other words, BMCL is a way of re-
building adjacency relationships "inside" MCL 
clusters, by making reference to "outside" path in-
formation.  It then becomes natural to realize that 
the lower the curvature value of the hub is?
reflecting sparse connectivity inside the hub?s clus-
ter?the more effective BMCL will be in subdivid-
ing the core cluster, which will augment the modu-
larity Q value for the clustering results. 
3 Applying BMCL corpora data 
3.1 The BMCL algorithm 
In this section, we apply our BMCL method to a 
semantic network that is almost exhaustively ex-
tracted from typical documents of a specific struc-
ture.  It is supposed that if the MCL is applied to 
word association or co-occurrence data it will yield 
concept clusters where words are classified accord-
ing to similar topics or similar meanings as para-
digms.  However, because the word distribution of 
a corpus approximately follows Zipf?s law and 
produces a small-world scale-free network (Stey-
vers et al, 2005), the MCL will result in a biased 
distribution of cluster sizes, with a few extraordi-
narily large core clusters that lack any particular 
features. 
In order to overcome such difficulties in build-
ing appropriate lexical graphs for corpus data, we 
propose an original way of appropriately subdivid-
ing core clusters by taking into account graph coef-
ficients, especially the curvature of a hub word.  
As mentioned above, BMCL is most effective for 
clusters that, containing a high-degree and low-
curvature vertex, display a local part of a network 
with highly sparse connectivity when a hub is 
eliminated.  This feature increases the efficiency of 
the BMCL by making it possible to introduce 
moderate connection rates for latent adjacencies. 
In contrast to a ?real? adjacency between the ver-
tices ki, represented here by 1),( =kid , the ?latent? 
adjacency 1),( =jid v  will subsequently be defined 
to closely adapt to the connection state for the 
dataset, which we will utilize in testing the BMCL.  
The hub hM of each Markov cluster M is supposed 
to be the vertex with the largest degree for M .  
Here, we set a sufficiently large core cluster C , a 
set of hubs H and the hub of C as hC .Under such 
conditions, we can formulize the set of external 
hubs bypassing the intra-core connections jiK ,  as; 
}1),(),(,,|{ ,,, ==?? kjdkidKkHKK jijiji , 
where C
hh CjCiji
ji
???
?
,,
, , HCh ? . We also propose an 
additional function called 
n
ArgTopn , which identi-
fies the set of n nodes that have the highest connec-
904
tion values.  This is to produce a moderate connec-
tion rate which allows us to execute appropriate 
MCL operations by appropriately setting two prun-
ing thresholds, ?p and ?q.  These are applied in the 
row direction by fixing i  in the intra-core connec-
tion matrix to the number of the shortest paths be-
tween ji, -- || , jiK -- to make the following prun-
ing rule: 
1),(
|)|&&|(| ,,
=>?
??
=
jid
KArgTopnjKif
v
ji
n
pji
q?
?
 
This rule extracts from the intra-core connec-
tion matrix a latent adjacency matrix to which the 
MCL is applied once again in order to obtain ap-
propriately resized sub-clusters from a huge core 
cluster. 
3.2 A range of corpus data 
In this section, three documents were selected tak-
ing into consideration the curvature value of a hub 
with the highest degree and the density of connec-
tions with or without this hub among the vertices 
of a core cluster at the level of a raw data graph. 
I. Associative Concept Dictionary of Japanese 
Words (Ishizaki et al, 2001), hereafter abbreviated 
as ACDJ, which consists of 33,018 words and 
240,093 word pairing collected in an association 
task involving 10 participants.  Of these, 9,373 
critical words were selected to create well-arranged 
semantic network by removing the rarest 1-degree 
dangling words and rarer words with a degree of 2 
but curvature values of 0. 
II. Gakken?s Large Dictionary of Japanese (Kin-
daichi & Ikeda, 1988), hereafter abbreviated as 
GLDJ, which is an authoritative Japanese diction-
ary with some features of an encyclopedia in terms 
of its rich explanatory texts and copious examples.  
We selected 98,083 words after removing noise 
words, functional words, and 1,321 isolated words 
to extract word pairs by combining every head-
word with every other headword included within 
an entry text. 
III. WordNet. We used only the "data.noun" file 
where the lexical information for each noun is de-
fined by a set of index numbers corresponding not 
with words themselves but with their senses. The 
co-occurrence relationships for 98,794 meanings 
were extracted from every data block that contains 
a series of indexes, which also covers other parts-
of-speech. 
The principle for building a semantic network 
for each of these documents was to select relevant 
?word pairs? or ?index pairs? indicating the lexical 
relationships of adjacency, association or co-
occurrence, respectively.  Table II presents graph 
information for the three data sets and the results 
of applying both the MCL and the BMCL to them. 
 
 
Table II Data about the three corpora 
 
Although the first data (ACDJ) is much smaller, 
it is worthwhile executing because it represents a 
concrete example of the network type discussed 
earlier, namely, a three-layer architecture around a 
hub (quasi-tree graph).  The connection rate in the 
core cluster is very low (0.002 with and 0 without 
the hub), as is the modularity Q value for the MCL 
(0.094).  However, subdivision of the core cluster 
in the BMCL results yielded a high modularity Q 
value (0.606) when latent adjacencies derived from 
bypassing connections with a threshold of q? =3 
were used. 
The last two data (GLDJ and WordNet) are di-
rectly comparable because they are quite similar in 
size and provide sharp contrast, particularly in 
terms of curvature values (GLDJ: 8.51106E-05 << 
WordNet: 0.0405), and modularity Q values for the 
MCL (GLDJ: 0.176 << WordNet: 0.841).  For 
WordNet, the high connection rate in the core clus-
ter (0.03) makes it difficult for it to be subdivided 
by any clustering method, even if the hub is elimi-
nated.  In terms of the GLDJ, the core cluster was 
repeatedly divided by the BMCL and the modular-
ity for the subdivision turned out to be 0.2214 with 
a threshold of p?  = 1. 
However, there is another way to split the core 
cluster into sub graphs, which does not require the 
use of the latent adjacency information which is 
crucial for the BMCL.  That other method, which 
can be called the ?Simply-Repeated MCL (SR-
MCL)?, involves applying the MCL once again to 
ACDJ GLDJ WordNet
Num of Vertices 9373 98083 98794
Degree Mean 19.963 13.8939 63.7155
Hub Word House Archaic Words Individual
Degree of Hub 563 12959 2773
Curvature of Hub 0.0398 8.51106E-05 0.0405
Core Cluster Size 158 8962 2597
Connection Rate of Core Cluster 0.0022 0.000328782 0.030539
Ibid (Without Hub) 0 0.000153119 0.03005
Q for the First MCL 0.0946409 0.176 0.841275
Q for the BMCL 0.606284 0.221 -0.094
905
the part of the original adjacency matrix that corre-
sponds to the vertices apart from the hub, and 
which become members of the core cluster as a 
result of the first MCL.  In the case of ACDJ, it is 
impossible to execute the SR-MCL, because there 
is no edge that is not connected to the hub within 
the core cluster, and so all the vertices apart from 
the hub would be isolated if the hub were removed. 
A similar problem is also encountered with the 
core cluster of the GLDJ, even though the SR-
MCL increases the modularity Q value (0.769) 
much more than the BMCL.  Vertices that dangle 
from the hub?37% of the core members?would 
be dropped from the second MCL computation if 
the latent adjacency is not used, which, on the 
other hand, assures a high recall rate (0.88).  Thus, 
we have adopted an eclectic way to maintain both 
relatively high recall (the proportion of non-
isolated nodes) and relatively high precision (the 
modularity Q of the intra-core clustering).  This is 
what we may call a ?Mixed BMCL? which in-
volves combining the latent adjacency matrix ex-
clusively for the vertices dangling to the hub and 
the raw adjacency part matrix for the remaining 
ones that are connected among them.  As Figure V 
highlights, the F-measure 
RP
PR
?? +? )1(
(R: recall; P: 
precision) underscores the effectiveness of the 
Mixed BMCL for the GLDJ. 
 
Figure V Comparison of the methods (? =0.4) 
 
4 Conclusion 
This paper has examined MCL outputs obtained 
for some rather problematic conditions, such as the 
clustering of a tree graph and clustering for a net-
work that contains a hub that has a very low curva-
ture value.  In such cases, many of the vertices ad-
jacent to the hub are removed from the cluster that 
it represents.  However, compensating for that, the 
hub cluster will absorb many other vertices?some 
of which are not directly connected to the hub it-
self?to form a large-sized core cluster.  That is 
when our proposed method of Branching MCL 
(BMCL) is most effective in adjusting cluster sizes 
by utilizing latent adjacency.  Subdivision of the 
core cluster can facilitate the interpretation of the 
classified concepts. 
When the curvature of the hub is a little higher 
than in such extreme conditions, the combination 
of the ordinary MCL and the BMCL (a Mixed 
BMCL) can work well in increasing the F-Measure 
score.  However, it is not possible to reapply the 
MCL to a dense core cluster that is organized 
around a hub with a very high curvature value.  A 
direction for further research will be to automati-
cally select from between the BMCL and the 
Mixed-BMCL.  The SR-MCL or similar modifica-
tions may yield the optimal approach to dividing 
massive Markov clusters into appropriate subsets. 
References 
Clauset, A, Newman M.E.J., and Moore, C. Find-
ing Community Structure in Very Large Net-
works, Phys. Rev. E 70, 066111 (2004) 
Danon, L., Diaz-Guilera, A., and Arenas, A. Effect 
of Size Heterogeneity on Community Identifica-
tion in Complex Networks, J. Stat. Mech. 
P11010 (2006) 
Dorow, B. et al Using Curvature and Markov 
Clustering in Graphs for Lexical Acquisition and 
Word Sence Discrimination, MEANING-
2005,2nd Workshop organized by the MEAN-
ING Project, February,3rd-4th.(2005) 
Kindaichi, H., Ikeda, Y. Gakken?s Large Diction-
ary of Japanese, GAKKEN CO, LTD. (1988) 
Newman M. E. J. and Girvan M., Finding and 
evaluating community structure in networks, 
Physical Review E 69. 026113, (2004) 
Okamoto, J., Ishizaki, S. Associative Concept Dic-
tionary and its Comparison with Electronic Con-
cept Dictionaries, 
http://afnlp.org/pacling2001/pdf/okamoto.pdf, 
(2001). 
Pujol, J.M., B?jar, J. and Delgado, J. "Clustering 
Algorithm for Determining Community Struc-
ture in Large Networks".Physical Review E 74 
(2007):016107 
Van Dongen, S. Graph Clustering by Flow Simula-
tion. PhD thesis, University of Utrecht. (2000) 
 
0
0.2
0.4
0.6
0.8
1
1.2
Recall Precision F-Measure
SR-MCL
BMCL
Mixed BMCL 
906
Coling 2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Processing, pages 57?60
Manchester, August 2008
Random Graph Model Simulations of Semantic Networks for Associative Concept Dictionaries 
Hiroyuki Akama Tokyo Institute of Technology 2-12-1 O-okayama Meguro-ku Tokyo 152-8550, Japan akama@dp.hum.titech.ac.jp Terry Joyce Tama University 802 Engyo Fujisawa-shi Kanagawa-ken 252-0805, Japan terry@tama.ac.jp 
Jaeyoung Jung Tokyo Institute of Technology 2-12-1 O-okayama Meguro-ku Tokyo 152-8550, Japan catherina@dp.hum.titech.ac.jp Maki Miyake Osaka University 1-8 Machikaneyama-cho Toyonaka-shi Osaka 560-0043, Japan mmiyake@lang.osaka-u.ac.jp 
 Abstract 
Word association data in dictionary form can be simulated through the combina-tion of three components: a bipartite graph with an imbalance in set sizes; a scale-free graph of the Barab?si-Albert model; and a normal distribution con-necting the two graphs.  Such a model makes it possible to simulate the complex features in degree distributions and the interesting graph clustering results that are typically observed for real data. 1 Modeling background Associative Concept Dictionaries (ACDs) consist of word pair data based on psychological ex-periments where the participants are typically asked to provide the semantically-related re-sponse word that comes to mind on presentation of a stimulus word. Two well-known ACDs for English are the University of South Florida word association, rhyme and word fragment norms (Nelson et al, 1998) and the Edinburgh Word Association Thesaurus of English (EAT; Kiss et al, 1973). Two ACDs for Japanese are Ishizaki?s Associative Concept Dictionary (IACD) (Oka-moto and Ishizaki, 2001) and the Japanese Word Association Database (JWAD) (Joyce, 2005, 2006, 2007). While there are a number of practical applica-tions for ACDs, three are singled out for mention                                                 ? 2008. Licensed under the Creative Commons Attri-bution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved.  
here. The first is in the area of artificial intelli-gence, where ACDs can contribute to the devel-opment of intelligent information retrieval sys-tems for societies requiring increasingly sophisti-cated navigation methods. A second application is in the field of medicine, where ACDs could be used in developing systems that seek to prevent dementia by checking higher brain functions with a brain dock. Finally, within educational settings, ACDs can greatly facilitate language learning through the manifestation of inherent cultural modes of thinking. The typical format of an ACD is to list the stimulus words (cue words) and their response words together with some statistics relating to the word pairing. The stimulus words are generally basic words determined in advance by the ex-perimenter, while the response words are seman-tically associated words provided by respondents on presentation of the stimulus word. The statis-tics for the word pairing include, for example, measured or calculated indices of distance or perhaps some classification of the semantic rela-tionship between the pair of words. In order to mathematically analyze the struc-ture of ACDs, the raw association data is often transformed into some form of graph or complex network representation, where the vertices stand for words and the edges indicate an associative relationship (Joyce and Miyake, 2007). However, to our knowledge, there have been no attempts at mathematically simulating an ACD as a way of determining in advance the architectural design of a dictionary. One reason is that it is a major challenge to compute maximum likelihood esti-mations (MLEs) or Monte-Carlo simulations for graph data (Snijder, 2005). Thus, it is extremely difficult to predict dependences for unknown 
57
factors such as the lexical distribution across a predetermined and controllable dictionary framework starting simply from a list of basic words. Accordingly, we propose an easier and more basic approach to constructing an ACD model by combining random graph models to simulate graph features in terms of degree distri-butions and clustering results.  2 Degree distributions for ACDs 2.1 Typical local skew It is widely known that Barab?si and Albert (1999) have suggested that the degree distributions of scale-free network structures correspond to a power law, expressed as 
r
ddxP
?
== )(  (where d stands for degree and 
r  is a small number, such as 2 or 3). This type of distribution is also known as Zipf's law describing the typical frequency distribution of words in a document and plots on a log scale as a falling diagonal stroke. However, in the degree distribution of ACDs, there is always a local skew, as a local peak or bump with a low hemline. Figure 1 presents two degree distributions; for the IACD (upper) ( r  = 1.8) and the JWAD (lower) ( r  = 2.3).                        Figure 1. Degree distributions for actual data  The plots indicate a combination of heteroge-neous distributions, consisting of a single degree 
distribution represented as a bell form with a steep slope on the right side. However, what is most interesting here is that throughout the dis-tribution range the curves remain regular and continuous, with an absence of any ruptures or fractures both before and after the local peaks. When actual ACD data is examined, one finds that as response words are not linked together, almost all the words located in the skewed part are stimulus words (which we refer to as peak words in this study), while the items before the local peak are less frequent response words that have a strong tendency to conform to a decaying distribution. It is therefore relatively natural to divide all word pairs into two types of graph: either a bipartite graph for new response words that are not already part of the stimulus list and a graph that conforms to Zipf's law for the fre-quencies of response words that are already pre-sent in the stimulus list. For the first type, new response words are represented as nodes only with incoming links, generating a bipartite graph with two sets of different sizes. This bipartite graph would exhibit the decaying distribution due to low-frequency response words prior to the local peak. In the second type of graph, response words are represented as nodes with both incom-ing and outgoing links. This second type is simi-lar to a scale-free graph, such as that incorpo-rated within the Barab?si-Albert (BA) model. 2.2 Bipartite Graph and BA Model A bipartite graph is a graph consisting of vertices that are divided into two independent sets, S and R, such that every edge connects to one S vertex and one R vertex. The graph can be represented by an adjacency matrix with diagonal zero sub-matrices, where the values of the lower right sub-matrices would all be zero were it not for the appearances of some stimulus words as response words. The lower right section is exactly where the extremely high degrees of hubs are produced, which far exceed the average numbers of response words. Thus, we adopt an approach to generating a scale-free graph that reflects Zipf's law for fre-quency distributions. According to the BA model, the probability that a node receives an additional link is proportional to its degree. Here, we im-plement the principle of preferential attachment formulated by Bollob?s (2003): 
?
=
+
=?
t
T
ttx
TdxmdNxP
1
1
)(/)()(    (1), 
0.00001
0.0001
0.001
0.01
0.1
1
1 10 100 1000
k
P
(
k
)
data
k^(-r)
0.00001
0.0001
0.001
0.01
0.1
1
1 10 100 1000
k
P
(
k
)
data
k^(-r)
58
with the addition of one condition that is specific to ACDs, which we explain below. The BA model starts with a small number, 
0
m  of vertices, and at each time step, T , a new vertex with m  edges is added and linked to m  different vertices that already exist in the graph. 
1+t
N  represents a random set of m  early vertices, )(id
t
 the degree of vertex 
i
 in the process at time t . The probability that a new vertex will be connected to a vertex 
i
 depends on the connectivity of that vertex, as expressed by Equation (1). However, we specifically assume that m  is a random natural number that is smaller than 
0
m , because in actual data the ratio of stimulus words among all responses words for each stimulus word is obviously far from constant. Moreover, the graph for the BA model here should be regarded as being a directed graph, because the very reason that hubs emerge within semantic network representations of ACDs is that the number of incoming edges is much larger than the expected number of nodes for each possible in-degree. In contrast, out-degree is limited by the number of responses for each stimulus word 
i
, which is represented as )(ic . Let )(ic  follow a normal distribution with a mean cm and a small variance value 2?  (which is not constant but nearly so) to smoothly combine the distribution of the bipartite graph and the power distribution. If a directed adjacency matrix for the network exclusively between stimulus words is expressed as 
)(
ij
BD
, then the sum of the non-zero values for each row in a random bipartite graph introducing new response words will be 
?
?
?
i
ij
BDiC )()(
 (The vertices of stimulus words with the subscript j are linked with the vertex of the stimulus word i). Thus, new response words?words that are not stimulus words?will be randomly allocated within a bipartite graph according to Equation (2): 
?
?
?
?==
i
ij
BDicrliP ))()(()1),((
1            (2), where r is the approximate number of such words. Equation (2) will yield the lower left and the upper right sections of the complete adja-cency matrix A  for the ACD model. The subse-quent sub-matrix tP  refers to the transposition of the prior sub-matrix P . The adjacency matrix in Equation (3) represents a pseudo bipartite graph structure where the upper left section is a zero sub-matrix (because there are no intercon-
nections among new response words), but the lower right section is not. Here, 
ij
B  (not )(
ij
BD , but the undirected counterpart to it), which corre-sponds to the BA model, is taken as a subsection of the adjacency matrix that must be non-directed for the whole composition. 
?
?
?
?
?
?
?
?
=
ij
t
BP
PO
A           (3) 
The key to understanding Equation (3) is to real-ize that P  is conditionally dependent on 
ij
B , because we assume a normal distribution for the number of non-zero values at each row in the lower section of A . 2.3 Simulation Results Taking into account the approximate numbers of possible new response words, in other words, the balance in sizes between the two sets in the bipartite graph, we built a composition of partial random graphs that could represent an adjacency matrix of the ACD model. Figure 2 presents one of the results obtained for the following conditions:
3000,1,5,3,10,90
0
====== rcmmt
m
? .  As the Figure shows, the local peak and the accompanying hemline in the degree distribution are clearly simulated by the complex combination of random graphs. 
10 20 30 40
-8
-6
-4
-2
  Figure 2. Degree distribution of an ACD model  The degree distribution for the artificial net-work is consistent with the features observed for actual ACD data, where more than 96% of the stimulus words in each data set are distributed across the peak section of the degree distribution, which is why we have referred to them as peak words. Moreover, it is easy to verify that without the assumption of a normal distribution for )(ic , distinct fractures emerge in the artificial curve where new response words in the bipartite struc-
Local peak 
59
ture would be distinguished from stimulus words located at initial points of the local peak. 3 Markov Clustering of ACDs 3.1 MCL This section introduces the graph clustering method that is applied to both the real and artifi-cial ACD data in order to compare them. Markov Clustering (MCL) proposed by Van Dongen (2001) is well known as a scalable unsupervised cluster algorithm for graphs that decomposes a whole graph into small coherent groups by simu-lating the probability movements of a random walker across the graph. It is believed that when MCL is applied to semantic networks, it yields clusters of words that share certain similarities in meaning or appear to be related to common con-cepts. 3.2 MCL Results The clustering results for the ACD model created by combining random graphs reveal that each of the resultant clusters contains only one stimulus word surrounded by several response words. This result is somewhat strange because there are dense connections between stimulus words, which would lead us to assume that clusters would have multiple stimulus word. However, the results of applying MCL clustering to the graph for the ACD model are in reality highly influenced by the sub-structure of the bipartite graph and less dependent on the scale-free structure. Nevertheless, the result is quite similar to results observed with real data. On examining MCL clustering results for different ACD semantic networks, we have observed that MCL clusters tend to consist of one word node with a relatively high degree and some other words with relatively low degrees. On closer inspection of the graph, it is possible to see several supporter nodes that gather around one leader node, forming a kind of small conceptual community. This suggests that the highest degree word for each cluster becomes a representative for that particular cluster consisting of some other low degree words. In short, MCL clustering is executed based on such high degree words that tend to have relatively low curvature values (Dorow, 2005) compared to their high average degree values. 
4 Conclusion In this paper, we have proposed a basic approach to simulating word association dictionary data through the application of graph methodologies. This modeling is expected not only to provide insights into the structures of real ACD data, but also to predict, by manipulating the model pa-rameters, possible forms for future ACDs. Future research will focus on constructing an exponen-tial random graph model for ACDs based on Markov Chain Monte Carlo (MCMC) methods. References Barab?si, Albert-L?szl? and R?ka Albert. 1999. Emergence of scaling in random networks, Science. 286:509-512. Bollob?s, B?la. 2003. Mathematical Results on Scale-free Random Graphs, http://www.stat.berk eley.edu/~aldous/Networks/boll1.pdf Dorow, Beate et al 2005. Using Curvature and Markov Clustering in Graphs for Lexical Acquisi-tion and Word Sense Discrimination, MEANING-2005,2nd Workshop organized by the MEANING Project, February,3rd-4th. Joyce, Terry and Maki Miyake. 2007. Capturing the Structures in Association Knowledge: Application of Network Analyses to Large-Scale Databases of Japanese Word Associations, Large-Scale Knowl-edge Resources. Construction and Application, Springer Verlag:116-131. Kiss, G.R., Armstrong, C., Milroy, R., and Piper, J. 1973. An associative thesaurus of English and its computer analysis, In Aitken, A.J., Bailey, R.W. and Hamilton-Smith, N. (Eds.), The Computer and Literary Studies, Edinburgh University Press. Nelson, Douglas L., Cathy L. McEvoy, & Thomas A. Schreiber. 1998. The University of South Florida word association, rhyme, and word fragment norms, Retrieved August 31, 2005, from http://www.usf.edu/FreeAssociation Okamato, Jun and Shun Ishizaki. 2001. Associative Concept Dictionary and its Comparison Electronic Concept Dictionaries. PACLING2001-4th Confer-ence of the Pacific Association for Computational Linguistics:214-220. Snijders, Tom A.B., Philippa E. Pattison, Garry L.Robins, Mark S. Handcock, 2005. New Specifi-cations for Exponential Random Graph Models, http://stat.gamma.rug.nl/SnijdersPattisonRobinsHandcock2006.pdf Steyvers, Mark and Josh Tenenbaum. 2005. The Large-Scale Structure of Semantic Networks, Sta-tistical Analyses and a Model of Semantic Growth, Cognitive Science. 29 (1):41-78. 
60
 61
Proceedings of the NAACL HLT 2010 First Workshop on Computational Neurolinguistics, pages 27?35,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Network Analysis of Korean Word Associations 
G
G
Jaeyoung Jung Li Na Hiroyuki Akama
Tokyo Institute of Technology Tokyo Institute of Technology Tokyo Institute of Technology 
2-12-1, O-okayama, Meguro-ku 2-12-1, O-okayama, Meguro-ku 2-12-1, O-okayama, Meguro-ku
Tokyo, 152-8552, Japan Tokyo, 152-8552, Japan Tokyo, 152-8552, Japan 
jung.j.aa@m.titech.ac.jp li.n.ad@m.titech.ac.jp akama.h.aa@m.titech.ac.jp 
 
 
Abstract
Korean Word Associations (KorWA) were 
collected to build a semantic network for the 
Korean language. A graphic representation 
approach of applying coefficients to complex 
networks allows us to discern the semantic 
structures within words. A semantic network 
of the KorWA was found to exhibit the scale-
free property in its degree distribution. The 
growth of the network around hub words was 
also confirmed through two experimental 
phases. As an issue for further research, we 
suggest that the present results may yield in-
sights for computational neurolinguistics, as a 
semantic network of word association norms 
can bridge the gap between information about 
lexical co-occurrences derived from a corpora 
and anatomical networks as a basis for map-
ping out neural activations. 
1 Introduction 
Language is an intricate cognitive system. The 
mental system, called a grammar by linguists, al-
lows human beings to form and interpret the 
sounds, words, and sentences of their language. 
The system is often broken down into several 
components, such as phonetics, phonology, mor-
phology, syntax, and semantics (O'Grady et al 
2005). Depending on one?s concerns, the basic 
elements of each level (i.e. phones, syllables, mor-
phemes, words, or sentences) become the constitu-
ents of linguistic networks of sound patterns, 
morphological structures, or syntactic organiza-
tions. Parse trees, for instance, which are often 
used in analyzing the syntactic structures of sen-
tences, employ links to represent the syntagmatic 
relationships between words. However, focusing 
on the processes of conceptualizing feelings, ex-
periences, and perceptions and of encoding them in 
words (namely, lexicalization), linguists have fre-
quently drawn another kind of linguistic network 
substantiated as a map of words projecting seman-
tic structures and relations onto an Euclidian space 
from a paradigmatic perspective. In that sense, 
word association data is attractive in terms of ease 
of data manipulation, especially when making a 
graph from a list of word pairs. Moreover, the tools 
for analyzing complex networks have been often 
applied to analyzing the structural features within 
large-scale word association data and to mining 
lexical knowledge from them. 
Since Galton (1880), word association has been 
used as an empirical method for observing thought 
processes, memory, and mental states within clini-
cal and cognitive psychology (Deese, 1965). From 
a linguistic perspective, word associations are un-
doubtedly valuable language resources because 
they are rich sources of linguistic knowledge and 
lexical information. The data has some unique 
characteristics that are very interesting and useful 
for cultural studies, reflecting the life styles, social, 
cultural and linguistic backgrounds of the native 
speakers who contributed to the data collections.  
Such information could be particularly useful for 
further applications not only within semantic stud-
ies but also for intelligent information retrieval, 
brain research, and language learning. 
In short, so-called word association norms are 
crucial as large-scale paradigmatic corpora. They 
consist of word pair data based on psychological 
experiments where the participants are typically 
asked to provide a semantically-related response 
word that comes to mind upon presentation of a 
stimulus word. Two well-known word association 
data for English are the University of South Florida 
word association, rhyme and word fragment norms 
27
(Nelson et al, 1998) and the Edinburgh Word As-
sociation Thesaurus of English (EAT; Kiss et al, 
1973). For Japanese there are Ishizaki's Associa-
tive Concept Dictionary (IACD) (Okamoto and 
Ishizaki, 2001) and the Japanese Word Association 
Database (JWAD) (Joyce, 2005, 2006, 2007). Util-
izing computational linguistic techniques that aim 
to mathematically analyze their structures, raw as-
sociation data is often transformed into some form 
of graph or complex network representation, where 
the vertices stand for words and the edges indicate 
an associative relationship (Miyake et al, 2007). 
Such techniques of graph representation and their 
analysis allow us to discern the patterns of connec-
tivity within large-scale resources of linguistic 
knowledge and to perceive the inherent relation-
ships between words and word groups. 
However, despite a long history of word associa-
tion studies and the valuable contributions of such 
data to cognitive science, comprehensive, large-
scale databases of Korean word association norms 
have been seriously inadequate. In one study, Lee 
(1970) surveyed word associations based on 30 
adjectives and 29 words representing colors, tar-
geting 40 university students and analyzed the re-
sponse words for associative tendencies in terms of 
gender and grammatical word classes. More re-
cently, Shin (1998) attempted to categorize words 
by conceptual systems in order to construct a lexi-
cal dictionary supporting foreign language learners. 
Although her data differs from word association 
norms and is not available as an accessible digital 
database for academic purpose, the semantic clas-
sification of the words can be exploited in com-
plementing the analysis of Korean semantic 
networks.  
A collection of Korean word associations (for 
short, KorWA) was planned and conducted with 
the strong motivation of constructing a worthwhile 
database of Korean word associations as a kind of 
resource that has multiple applications in a number 
of areas such as lexicography, language education, 
artificial intelligent, natural language processing, 
and cultural study.  Moreover, we intend to share 
the database on the web to foster these various po-
tential utilities. 
  In this paper, KorWA is represented into se-
mantic networks and examined by some combina-
torial methods in linguistics. The details are 
presented from the whole process of collecting the 
data to the results of the analysis based on the the-
ory of complex networks. Furthermore, this paper 
briefly discusses another important characteristic, 
dynamics in scale-free networks, which has re-
cently attracted much attention in this research 
field. Finally we will mention the applicability of 
the graph-based analysis developed here to the fu-
ture potential researches of the computational neu-
rolinguistics. 
2 Korean word associations 
2.1 Design of Experiment 
Preparation of an association experiment begins 
with the selection of a stimulus word set that is to 
be presented to the respondent in order to initiate 
their association process.  Determining the stimu-
lus word set is a crucial part in designing the ex-
periment, as associative responses are greatly 
influenced by the characteristics of the presented 
words, in particular, the stimulus word familiarity 
influences response heterogeneity, variability, rela-
tional categories, and reaction times (Deese 1965). 
For the experiment of Korean word associations, 
we referred to a list of 5,000 Korean basic words 
(Seo et al 1998), which was derived from the 
Yonsei Corpora consisting of 42,644,891 words as 
of 1998 [ILIS].  From the list, we compiled a list of 
3,951 words, consisting of 2,628 nouns, 1,006 
verbs, and 317 adjectives. 
One hundred and thirty-two native Korean stu-
dents (71 males and 61 females) at Daejon Univer-
sity, South Korea voluntarily participated in the 
experiment.  The students were mainly from the 
departments of Korean language and literature 
(54%), physical therapy (30%), and philosophy 
(11%).  More than 70% of the students had educa-
tional background in the humanities.  Most of the 
students (93%) were in their 20s; 82% between 20 
and 25 years old and 11% between 26 and 30 years 
old.  14% of students answered that on average 
they read more than five books in a month. The 
task was conducted on the campus of Daejeon 
University from September 2007 to February 2008. 
It was a traditional pen-and-paper based task. Un-
der the control of an instructor, each session of the 
task lasted for 30 minutes. 
In the task, participants were instructed to write 
down the response words that came to mind when 
they looked at the presented words. We asked the 
subjects to write down all the words that they 
28
could think of from the presented words. This pro-
cedure is called the continuous free association 
task, differing from the discrete association task 
where the subject is asked to only write down their 
first response (Cramer 1968). 
As a means of naturally displaying continuous 
associations, the respondents were asked to map 
out their responses. That is, they drew a kind of 
associative map for a given word, by adding a line 
when they made an association and numbering the 
responses according to the order in which they 
came to mind from the stimulus word. In the ex-
periment, an A5 size booklet was distributed to the 
participants. The booklet had 66 pages printed on 
one-side, including 2 front-back cover pages.  
 
 
Figure 1. Instructions about the task and example 
 
The first 4 pages contained instructional informa-
tion; (1) a brief description of the experiment?s 
purpose and its method, (2) a short survey for basic 
respondent information (gender, age, major, and 
number of books read in a month), (3) an example 
illustrating what to do in the task (shown in Figure 
1), and (4) one practice before the task. Then, the 
remaining 60 pages were for the word association 
task, printed with one word per page. Thus, each 
participant was asked to provide word association 
responses for 60 words. 
In total, 132 booklets were prepared for the task. 
A list of 60 words for each booklet was randomly 
extracted from the 3,951 stimulus set. Apart from 
six of the 132 sets, the lists included 40 nouns, 15 
verbs, and 5 adjectives. The others had slightly 
different numbers of the syntactic categories. 
However, eventually each stimulus word was 
planned to be presented to up to two subjects. 
As the result of approximately 6-month period 
of data collection, we obtained 28,755 responses in 
total for the 3,942 stimulus words (from the origi-
nal stimulus set, nine words failed to elicit any 
word associations). The 28,755 responses (tokens) 
consisted of 11,275 distinct words (types). Each 
item was presented to two respondents. 
The KorWA database (Figure 2) was con-
structed from the collected word association re-
sponses. The data is arranged into six fields; (1) the 
part of speech of the stimulus word, (2) the stimu-
lus word, (3) the part of speech of the response, (4) 
response order, (5) raw form of the response, and 
(6) response word in standard form. 
 
 
Figure 2. Contents of the KorWA database 
2.2 Basic Analysis 
The Korean word association data collected is 
briefly summarized here in terms of the relations 
between the stimulus words and the responses to-
gether with some basic statistics. The participants 
produced on average 218 responses (standard de-
viation = 63.8, ranging from 98 to 482) for the 
complete set of 60 words in the free association 
task, which corresponds to 3.6 responses per 
stimulus word. Because each stimulus item was 
presented to two respondents, each stimulus has on 
average 7.3 association responses. 
29
As already mentioned, our task was the continu-
ous free association task where the respondent was 
allowed to provide more than one response, so 
there is the possibility of chaining responses where 
some association responses are elicited by prior 
responses. The response set of 28,755 tokens in-
cludes all such responses. Furthermore, it is possi-
ble to extract the primary responses that were 
given as the first response produced for each 
stimulus word. In doing that, it is possible to con-
vert the continuous free association task condition 
to the discrete association task employed in other 
existing data. The primary associates for the 3,942 
stimulus are 7,550 word tokens (4,197 types). The 
associations seem to be related to the grammatical 
classes of the stimulus.  Ervin (1961) reports that 
many associations tend to have the same gram-
matical class as the stimulus word. Similarly, Jen-
kins (1954) and Saporta (1955) provide an 
interesting way of classifying association struc-
tures into two modes, i.e. paradigmatic associa-
tions and syntagmatic ones. In the former mode, 
the stimulus and response fit a common grammati-
cal paradigm. For example, the word ACTION 
yields the associates of WORDS, LIFE, 
MOVEMENT, MOTION, GAME, and so on, 
which are not likely to occur as sequences in eve-
ryday English. In the latter case, the stimulus and 
response are generally contiguous, occupying dif-
ferent positions within phrases or sentences. 
Namely, they often form sequences, as in the rela-
tions between the stimulus word of 
ADMINISTRATIVE and its common associates of 
DUTY, JOB, CONTROL, DISCIPLINE, POWER, 
BUREAUCRATS, POSITION, AGENCY, 
ENTITY, SCHOOL, BOSS, GOVERNMENT, 
RULE, etc. (Deese 1965). Deese (1962) clarified 
the relative frequencies of paradigmatic and syn-
tagmatic associations among the grammatical 
classes of English, especially with nouns, verbs, 
adjectives, and adverbs in his study. He observed 
that the tendency towards paradigmatic or syntag-
matic association varied with word class; nouns are 
dominantly paradigmatic, while adjectives and 
verbs tend to be both paradigmatic and syntag-
matic. In the case of adjectives, it is a particularly 
interesting tendency for the association types to 
have a strong correlation with frequency of usage. 
That is to say, for common adjectives, associations 
are more likely to be paradigmatic (e.g. for HOT, 
associates such as COLD, WARM, and COOL 
more frequently occur than WOMEN, WEATHER, 
and the like), while uncommon adjectives are more 
syntagmatic (e.g. for ADMINISTRATIVE, associ-
ates such as DUTY, GOVERNMENT, and RULE 
are more often produced than SUPERVISORY, 
EXECUTIVE, and so on). What is more, most 
paradigmatic associates to adjectives are either 
synonymous with the stimulus (COLD COOL) or 
the opposite of the stimulus (COLD HOT). Com-
mon adjectives overwhelmingly have more anto-
nyms as their response, but relatively low-frequent 
adjectives have more synonym associations. 
A similar tendency is observed in our data, 
which included three types of grammatical class 
among the stimulus items, with nouns, verbs, and 
adjectives, covering 66.5%, 25.5%, and 8% of the 
stimulus set respectively. The different proportions 
of the word classes reflects their frequencies within 
the Yonsei corpora, i.e. among the 5,000 most fre-
quent words, there is a much larger number of 
nouns, compared to verbs and adjectives. By tag-
ging the responses with parts of speech data during 
the course of constructing the database, we can 
analyze the distributions of grammatical categories 
among the responses. The responses were over-
whelmingly nouns (78%), followed by adjectives 
(7%), proper nouns (4.5%) and verbs (4.4%) in 
descending order. Within the primary response list, 
the distributions of word class are not greatly dif-
ferent, with 79% nouns, 6.7% adjectives, 4.8% 
verbs, 3.9% proper nouns, and around 6% others. 
Corresponding to the grammatical class of the 
stimulus specifically, nouns are also the dominant 
responses. When considering just the primary re-
sponses, noun stimulus elicited mostly noun re-
sponses (80%), followed by adjectives (6%), 
proper nouns (5%), and verbs (3%); verb stimulus 
produced around 80% noun associates, 10% verbs 
and 4% adjectives; while for adjective stimulus, 
there were 70% noun responses, 19% adjectives, 
and 2% verbs. In short, we found a majority of 
noun noun, verb noun, and adjective noun com-
binations within the stimulus response relations. 
This demonstrates the association tendency for 
nouns to strongly elicit paradigmatic associations, 
as seen from the principal noun noun relations, 
while verbs and adjectives tend to yield more syn-
tagmatic associations, as seen from the major rela-
tions of verb noun and adjective noun. 
2.3 Network Analysis (1) 
30
Degrees:  Recently, a number of studies have ap-
plied graph theory approaches in investigating lin-
guistic knowledge resources. For instance, instead 
of word frequency based computations, Dorow, et 
al (2005) utilize graph clustering techniques as 
methods of detecting lexical ambiguity and of ac-
quiring semantic classes. Steyvers and Tenenbaum 
(2005) conducted a noteworthy study that exam-
ined the structural features of three semantic net-
works (free association norms of Nelson et al, 
Roget's thesaurus, and WordNet). By calculating a 
range of statistical features, including the average 
shortest paths, diameters, clustering coefficients, 
and degree distributions, they observed interesting 
similarities between three networks in terms of 
their scale-free patterns of connectivity and small-
world structures. Following their basic approach, 
we analyze the characteristics of the semantic net-
work representation of KorWA by calculating the 
statistical features of the graph coefficients, such as 
degree and degree distribution. 
The semantic network representation of the 
word association network is constructed by repre-
senting the words as nodes and associative pairing 
information for words as edges. The degree (D) of 
a node denotes the number of edges that a node has. 
An undirected graph is structured by the edges, 
while a directed graph is structured by arcs that 
include the associative direction. The numbers of 
incoming and outgoing arcs from a node are re-
ferred to as the in-degree and out-degree of a node, 
respectively. The sum of the in-degree and out-
degree values of a node is equal to its total degree. 
This concept of graph analysis allows us to 
categorize the total words in the data into three 
types; one being words only found in the stimulus 
set (S-type), one being words occurred as both 
stimulus and responses (SR-type), and the last be-
ing words only observed among the response set 
(R-type). The proportion of S-type, SR-type, and 
R-type words in the total word set corresponds to 
12.2% (1,568 words), 18.5% (2,374 words), and 
69.3% (8,901 words) respectively. Here, it is worth 
focusing on the SR-type of words. These are words 
selected as the most frequent ones through a large-
scale corpus covering various fields. At the same 
time, they also are produced by people in the free 
association task. This may indicate, in some sense, 
the high usability or commonness of those words. 
Indeed, the most frequent words in this data all 
belong to the SR-type. 
2.4 Network Analysis (2) 
Scale-free: The most frequent words belonging to 
the SR-type play the role of hubs in semantic net-
works made from word association data. These 
hubs can be represented as nodes that have not 
only outgoing links but also possess ingoing links, 
which leads us think of a scale-free graph, such as 
that incorporated within the Barab?si-Albert (BA) 
model. It is widely known that Barab?si and Albert 
(1999) have suggested that the degree distributions 
of scale-free network structures correspond to a 
power law, expressed as rddxP  !! )(  (where d 
stands for degree and ?is a small integer, such as 
2 or 3). This type of distribution is also known as 
Zipf's law, which describes the typical frequency 
distributions of words in a document and plots on a 
log scale as a falling diagonal stroke. The degree 
distribution of nodes in the KorWA network also 
exhibits this scale-free property, which has also 
been observed in word association data for differ-
ent languages. 
 
 
Figure 3. Degree distribution on log-log scales for the 
KorWA semantic network. P(k) is the probability that a 
node has k degrees in the network. 
   
However, we should stress the importance of 
network dynamics and of microscopically examin-
ing the ongoing process of data accumulation to 
determine whether the scale-freeness observed for 
word association data is derived from the same 
mechanism as the BA model. Rather than being 
static, networks are recognized as evolving over 
time, with the adding or pruning of nodes and 
edges (Barab?si and Albert, 1999; Watts, 1999). 
Indeed, we can easily identify such networks in a 
number of areas, from the World Wide Web to the 
internet connections on a physical level, co-
authorships, friendships, and business transactions. 
31
According to the BA model, the probability that 
a node receives an additional link is proportional to 
its degree. The probability that a new vertex will 
be connected to a vertex (node) i  depends on the 
connectivity of that vertex. Barab?si and Albert 
(1999) explain with this idea of preferential at-
tachment in terms of the scale-free property and 
the presence of hubs within the network. Networks 
as dynamical systems which grow over time and 
have topological properties produce dynamical 
behaviors as well. In particular with research on 
the diffusion of a new trend or technology or the 
spread of a disease and virus, the structural proper-
ties of the network have presented a new approach 
to understanding epidemical behaviors over a net-
work, including issues about why contagion occurs 
in certain cases, how it spreads, and what is the 
most efficient and effective way to prevent it. 
Many researchers have tried to address and analyze 
such behaviors with small-world models (Ball et 
al., 1997; Watts and Strogatz, 1998) and scale-free 
models (Pastor-Satorras and Vespignani, 2001). 
The semantic networks that we have examined 
to date have similar structural properties to many 
other networks. So, it is also possible to explain the 
scale-free feature of semantic networks in terms of 
preferential attachment? How can such dynamic 
behavior be interpreted for semantic networks? In 
the next section, we would like to briefly discuss 
those questions a little further. 
2.5 Network Analysis (3) 
Network Dynamics: It is a matter of fact that lan-
guage evolves; especially from a lexical perspec-
tive, where new vocabularies are generated and old 
senses sometimes disappear over time. However, 
tracing and observing such changes is rather diffi-
cult because such natural language evolution oc-
curs over long periods of time. When considering 
the evolution of semantic networks, therefore, we 
assume that the growth of a semantic network may 
correspond to the increases in the numbers of 
words (nodes) and semantic relations (edges) in as 
more data is added in the construction of the net-
work. Particularly, for our semantic networks 
which are built from word association data, the 
networks grow as more word association data is 
added. 
In this sense, we can attempt to observe the 
growth process for semantic networks here. To that 
aim, the KorWA network is particularly suitable, 
as it is constructed from KorWA data collected 
from two sessions that used exactly the same task. 
We may see how the network evolves by taking 
the sessions as two separate points in time. 
From the beginning, the KorWA network starts 
with the 3,951 nodes that correspond to the set of 
stimulus words. It cannot be called a network at 
this stage because there are no links between these 
nodes. Then, as the word associations are collected, 
a network starts to appear by adding edges between 
the initial nodes and new nodes corresponding to 
the association responses. When the first session of 
data collection was complete, we found that the 
initially disconnected 3,951 nodes forming a large, 
well-connected network, as presented in Table 1. 
 
Table 1. Growth of the KorWA semantic network. 
 
 
The number of nodes had increased to 9,054, 
and 13,669 edges were generated between them. 
8,641 nodes corresponding to 95% of the total 
nodes are connected to each other, being the larg-
est component in the network, but, at the same 
time, there were also 126 small partitions with 2 to 
3 nodes connected to each. The pseudo diameter, 
the longest distance, of the largest component is 18, 
which indicates that the nodes within it are well 
connected to each other. In this network, a node 
has three links on average and the distribution of 
degrees in the network shows a power law distribu-
tion (P(k)~K-  with a degree exponent !=2.42), as 
in Figure 3 above. 
Then, additional word associations were col-
lected for the same set of stimulus words in the 
same manner as in the first session. When the new 
data was added to the first network, we obtained a 
larger network, as described in Table 1. The net-
work grew by 12,844 nodes and 26,931 edges. 
Through this process, more than 99.7% of nodes 
(12,807) became interconnected, leaving on 37 
words as elements disconnected from the whole 
graph. Moreover, the pseudo diameter of the larg-
32
est component became smaller despite the increase 
in its size. The discrepancy in the degrees of words 
became larger than before, with a degree range 
from 1 to 198. 
 
Table 2.  Top 20 words with the highest degrees be-
fore and after growth of the KorWA network. 
    Before growth After growth 
G
?G(?money?)/ 87 
??G(?love?)/ 79 
??G(?friend?)/ 56 
??G(?human?)/ 48 
?G(?water?)/ 48 
?G(?dream?)/45 
??G(?army?)/ 45 
??G(?mind?)/ 44 
?G(?house?)/ 43 
??G(?tear?)/ 43 
??G(?movie?)/39 
??G(?study?)/39 
?G(?eye/snow?)/ 36 
?G(?book?)/ 35 
?G(?alcohol?)/ 34 
??G(?woman?)/ 34 
?G(?myself?)/ 33 
??G(?thing?)/ 32 
???G(?car?)/ 32 
??G(?family?)/ 32 
G
?G(?money?)/198 
??G(?love?)/ 146 
??G(?friend?)/ 114 
??G(?human?)/ 106 
??G(?mind?)/ 85 
??G(?woman?)/80 
?G(?water?)/ 80 
??G(?study?)/ 74 
??G(?tear?)/ 73 
?G(?myself?)/ 73 
?G(?dream?)/ 70 
??G(?army?)/ 69 
?G(?house?)/ 69 
?G(?alcohol?)/ 69 
?G(?book?)/ 68 
?G(?eye/snow?)/ 65 
??G(?fight?)/64 
??G(?war?)/ 64 
??G(?movie?)/ 63 
??G(?school?)/ 63 
Note. The number after the slash indicates the degree for 
the word. 
 
Over time (as reflected in the first and second 
sessions of data collection), 3,790 nodes and 
13,262 edges newly appeared in the KorWA net-
work. Through this growth, the network became 
much more interconnected, as clearly evidenced by 
the size of the largest component and the pseudo 
diameter. What is particularly salient is the number 
of links that a word has through the growth process. 
Interestingly, regardless of the double increase in 
the connections within the network, around 60% of 
the total nodes were still poorly connected, having 
a degree of only 1 or 2. On the other hand, some of 
nodes that already had plenty of links became 
much richer, becoming linked to even more other 
nodes; with the average degree for 1% of the total 
nodes being over 60. Table 2 lists the top 20 words 
in terms of highest degree values before and after 
growth. The first four words do not change in order, 
while the shifts for the other top items are not so 
significant. However, for most of these items, the 
degree value roughly doubled. 
From these observations, we can assume that 
there are some words that attract more links from 
other nodes, while most of these other words have 
just a few connections. This phenomenon appears 
even more conspicuously through the growth proc-
ess. The scale-free nature of semantic networks 
also seems to reflect a kind of preferential attach-
ment. What kinds of words always attract links 
from new nodes? As suggested already, these seem 
to be basic concept words, closely related to daily 
life and culture, and these hubs form a kind of 
bridge between several different conceptual do-
mains. 
Such words contributing to the connectivity of 
the network are central to the dynamic behavior of 
across the networks, and are likely to be key con-
cept words for understanding a culture and for 
learning language within the contexts of semantic 
networks. Further study and exploration in the 
structural and dynamic characteristics within se-
mantic networks may open up a new approach to 
semantics, cultural studies, and language learning 
from a cognitive perspective. 
   
3 Conclusion and Further study 
This paper has described our dataset to represent 
human language in the form of a network. With 
much interest in language as a communication and 
thinking tool, we have sought to build a semantic 
network representing lexical knowledge and the 
conceptual relations between words. To that aim, 
word association data is particularly suitable in 
terms of its data format and its abundant and useful 
content. We presented a project to collect Korean 
word association norms given the high utility and 
urgent need of data of this kind. We have detailed 
the project from the design of free association ex-
periment to the basic analysis of the data collected. 
The application of the word association data to 
computational neurolinguistics is an issue for our 
future work. We believe that our study could po-
tentially represent a breakthrough for this research 
field. The methods of Mitchell et al (2008), for 
example, suggest to us strong connections between 
neural activation data and lexical co-occurrence 
information, obtained from text corpora which 
plays a role of intermediating within linguistics 
33
embodiment theory with a sensory-motor basis and 
amodal theory with computational models. Ac-
cording to Mitchell et al, the techniques of natural 
language processing combined with neural linguis-
tics can enable us to predict the patterns of neural 
activation for word stimuli for which fMRI data 
are not yet available. In short, the neural associa-
tions within firing patterns turn out to be correlated 
with word associations within co-occurrence pat-
terns. 
However, the similarity coefficient or the dis-
tance between any two words might be computed 
not only from a set of documents but also from 
graphic representations of associative concepts, 
such as the one presented in this paper. If it is true 
that a word can be represented not only by a three-
dimensional array of cerebral activation, but also in 
terms of the lexical relatedness that is incorporated 
as a linear combination of these patterns, it may 
not be an overstatement to say that there might be a 
structural homology between natural neural net-
works in the brain and semantic networks built 
from word association norms. This kind of meta-
network perspective within cognitive science has 
become all the more important because attempts to 
fill the gaps in the modeling of neural pathways are 
increasingly attracting wide interest. Sporns et al 
(2004), for instance, have tried to apply the con-
ceptual methods of complex networks, such as 
small word-scale free, to cortical networks and to 
the more dynamic, functional and effective con-
nectivity patterns that underlie human cognition. 
Similarly, Stam and Reijneveld (2007) have intro-
duced a graph analysis applied to multi-channel 
recordings of brain activity, by setting up vertices 
at the anatomical loci within a neural circuit and 
linking some that elicit high correlation patterns to 
the same stimulus. Also within the experiment 
paradigms used by Mitchell et al some techniques 
for constructing a network model could be effec-
tive for the distributional representation of cortical 
responses handled at the same level as meaning 
proximity, even though Mitchell et al treated each 
voxel (volumetric pixel value in a 3-dimensional 
regular grid) independently. If such models of net-
work settings could be applied to images of neural 
activation across all the voxels for a set of stimulus 
nouns, it is possible to assume, by a reverse proc-
ess of parameter estimation, the existence of hid-
den semantic layers composed of unknown 
semantic features. These intermediate factors could 
be compared with real vocabulary data, such as 
basic verbs (as in the experiment conducted by 
Mitchell et al) taking the stimulus nouns as sub-
jects or targets. 
Moreover, the merits of introducing graph 
analysis techniques to computational neurolinguis-
tics could possibly be found in the evolutionary 
dynamics of networks, to the extent that the degree 
of word nodes (or, more simply, their frequencies) 
could be weighted for the neural connectivity de-
duced from fMRI responses. The data formats of 
neural activation patterns could then assimilate 
diachronic data to represent how a network grows 
over time around the key concepts or hub words, in 
accordance with the learning processes of particu-
lar individuals. Future research from this perspec-
tive could also support the high accuracy of similar 
experiments regardless of distributional bias in 
word frequencies. Briefly, semantic networks con-
structed from word association data could convey 
the lexical co-occurrence of words within docu-
ments to a visual map of the human brain reacting 
to those words. 
34
References  
D. Mollison, F. Ball, and G. Scalia-Tomba. 1997. Epi-
demics with two levels of mixing, Annals of Applied 
Probability 7, pp. 46-89. 
A.-L. Barab?si, and R. Albert. October 15, 1999. Emer-
gence of scaling in random networks. Science, 
286:509-512. 
P. Cramer. 1968. Word association. New York and 
London: Academic Press. 
J. Deese. 1962. "Form class and the determinants of 
association", Journal of verbal learning and verbal 
behavior, vol. 1, pp. 79-84. 
B. Dorow, D. Sergi, D. Widdows, E. Moses, K. Ling, 
and J. Eckmann. 2005. "Using Curvature and 
Markov Clustering in Graphs for Lexical Acquisition 
and Word Sense Discrimination", in MEANING-
2005, 2nd Workshop organized by the MEANING 
Project. 
F. Galton. 1880. "Psychometric experiments", Brain 2, 
pp. 149-162. 
S. Saporta, 1955. Linguistic structure as a factor and as 
a measure in word association, in J. J. Jenkins (Ed.), 
Associative process in verbal behavior: A Report of 
Minnesota Conference, Minneapolis: University of 
Minnesota. 
T. Joyce. 2005. Lexical association network maps for 
basic Japanese vocabulary, in Words in Asia cultural 
contexts, V. B. Y. Ooi, A. Pakir, I. Talib, L. Tan, P. 
K. W. Tan, and Y. Y. Tan (Eds.). Singapore: Na-
tional University of Singapore, pp. 114-120. 
G. R. Kiss. 1968. Words associations and networks, 
Journal of Verbal Learning and Verbal Behavior, 
vol.7, pp. 707-13. 
Y. J. Lee. 1970. Comparative studies on word associa-
tions by male and female university students: based 
on adjectives and color-referring words (translated 
from ?? ???? ????? ?? ????:     
???? ???? ????), ??? ?? ?? 
? 9?. ??? 344. 
C. L. McEvoy and D. L. Nelson. 1982. Category name 
and instance norms for 106 categories of various 
sizes, American Journal of Psychology 95, pp. 581-
634. 
H. Akama, J. Jung, M. Miyake, and T. Joyce. 2007.  
Hierarchical Structure in Semantic Networks of 
Japanese Word Associations, PACLIC21 (Pacific 
Asia Conference on Language, Information, and 
Computation-2007), pp.321-328. 
A. Carlson, K. Chang, M. Just, R. Mason, S. Shinkareva, 
T. Mitchell, and V. Malave. 2008. Predicting human 
brain activity associated with the meanings of nouns. 
Science, 320:1191?1195. 
C. L. McEvoy and D. L. Nelson. 2005. Implicitly acti-
vated memories: The missing links of remembering, 
In  Human learning and memory: Advances in theory 
and application, C. Izawa and N. Ohta (Eds.). Mah-
wah, NJ and London: Lawrence Erlbaum Associates, , 
pp 177-198. 
J. Archibald, J. Rees-Miller, M. Aronoff, and W. 
O'Grady. 2005. Contemporary Linguistics: An intro-
duction, 5th ed. Boston & New York: Bedford/St. 
Martin's. 
J. Okamoto and S. Ishizaki. 2001. "Construction of 
associative concept dictionary with distance 
information, and comparison with electronic concept 
dictionary (translated from ??????????
??????????)", ??????, vol. 8, 
pp. 37-54. 
A. Vespignani, and R. Pastor-Satorras. 2001. Epidemic 
spreading in scale-free networks, Physical Review 
Letter. 86. pp. 3200-3203. 
H. S. Shin. 1998. "Korean vocabulary teaching and 
semantic dictionary (translated from ??? 
????? ????)", ????? vol. 9, no.2, 
pp. 85-104. 
G. H. Jin, N. J. Nam, and S. G. Seo. 1998. "Determina-
tion of basic vocabulary for Korean language educa-
tion as a foreign language (translated from 
?????? ??? ??? ?? ?? 
????)", 1st year of annual report (December 14, 
1998), Internationalization of Korean language pro-
motion committee, Ministry of Culture, Sports, and 
Tourism. 
C. C. Hilgetag, D. R. Chialvo, M. Kaiser, and O. Sporns. 
9 September 2004. "Organization, development and 
function of complex brain networks", TRENDS in 
Cognitive Sciences Vol.8 No. 
C. J. Stam, J. C. Reijneveld. 2007. "Graph theoretical 
analysis of complex networks in the brain". Non-
linear Biomedical Physics. 
J. B. Tenenbaum and M. Steyvers. 2005. ?The large-
scale Structure of Semantic Networks: Statistical 
Analysis and a Model of Semantic Growth?, Cogni-
tive Science 29, pp.41-78. 
D. J. Watts and S. Strogatz. June 1998. "Collective dy-
namics of 'small-world' networks". Nature 393, 
pp.440?442. 
35
