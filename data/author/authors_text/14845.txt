Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1577?1588,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
A Simple Word Trigger Method for Social Tag Suggestion
Zhiyuan Liu, Xinxiong Chen and Maosong Sun
Department of Computer Science and Technology
State Key Lab on Intelligent Technology and Systems
National Lab for Information Science and Technology
Tsinghua University, Beijing 100084, China
{lzy.thu, cxx.thu}@gmail.com, sms@tsinghua.edu.cn
Abstract
It is popular for users in Web 2.0 era to
freely annotate online resources with tags.
To ease the annotation process, it has been
great interest in automatic tag suggestion. We
propose a method to suggest tags according to
the text description of a resource. By consid-
ering both the description and tags of a given
resource as summaries to the resource written
in two languages, we adopt word alignment
models in statistical machine translation to
bridge their vocabulary gap. Based on the
translation probabilities between the words in
descriptions and the tags estimated on a large
set of description-tags pairs, we build a word
trigger method (WTM) to suggest tags accord-
ing to the words in a resource description.
Experiments on real world datasets show that
WTM is effective and robust compared with
other methods. Moreover, WTM is relatively
simple and efficient, which is practical for
Web applications.
1 Introduction
In Web 2.0, Web users often use tags to collect and
share online resources such as Web pages, photos,
videos, movies and books. Table 1 shows a book
entry annotated with multiple tags by users1. On
the top of Table 1 we list the title and a short
introduction of the novel ?The Count of Monte
Cristo?. The bottom half of Table 1 shows the
annotated tags, each of which is followed by a
number in bracket, the total number of users who
1The original record is obtained from the book review
website Douban (www.douban.com) in Chinese. Here we
translate it to English for comprehension.
use the tag to annotate this book. Since the tags of
a resource are annotated collaboratively by multiple
users, we also name these tags as social tags. For
a resource, we refer to the additional information,
such as the title and introduction of a book, as
description, and the user-annotated social tags as
annotation.
Description
Title: The Count of Monte Cristo
Intro: The Count of Monte Cristo is one of the most
popular fictions by Alexandre Dumas. The writing of
the work was completed in 1844. ...
Annotation
Dumas (2748), Count of Monte Cristo (2716), foreign
literature (1813), novel (1345), France (1096), classic
(1062), revenge (913), famous book (759), ...
Table 1: An example of social tagging. The number
in the bracket after each tag is the total count of users
that annotate the tag on this book.
Social tags concisely indicate the main content
of the given resource, and potentially reflect user
interests. Social tagging has thus been widely
studied and successfully applied in recommender
systems (Eck et al, 2007; Yanbe et al, 2007; Zhou
et al, 2010), trend detection and tracking (Hotho
et al, 2006), personalization (Wetzker et al, 2010),
advertising (Mirizzi et al, 2010), etc.
The task of automatic social tag suggestion is
to automatically recommend tags for a user when
he/she wants to annotate a resource. Social tag
suggestion, as a crucial component for social tag-
ging systems, can help users annotate resources.
Moreover, social tag suggestion is usually consid-
ered as an equivalent problem to modeling social
1577
tagging behaviors, which is playing a more and more
important role in social computing and information
retrieval (Wang et al, 2007).
Most online resources contain descriptions, which
usually contain much resource information. For
example, on a book review website, each book entry
contains a title, the author(s) and an introduction
of the book. Some researchers thus propose
to automatically suggest tags based on resource
descriptions, which are collectively known as the
content-based approach.
One may think to suggest tags by selecting
important words from descriptions. This is far from
enough because descriptions and annotations are
using diverse vocabularies, usually referred to as a
vocabulary gap problem. Take the book entry in
Table 1 for instance, the word ?popular? used in the
description contrasts the tags ?classic? and ?famous
book? in the annotation; the word ?novel? is used in
the description, while most users annotate with the
tag ?fiction?. The vocabulary gap usually reflects in
two main issues:
? Some tags in the annotation do appear in the
corresponding description, but they may not be
statistically significant.
? Some tags may even not appear in the descrip-
tion.
It is not trivial to reduce the vocabulary gap and
find the semantic correspondence between descrip-
tions and annotations. By regarding both the de-
scription and the annotation as parallel summaries
of a resource, we use word alignment models in
statistical machine translation (SMT) (Brown et
al., 1993) to estimate the translation probabilities
between the words in descriptions and annotations.
SMT has been successfully applied in many ap-
plications to bridge vocabulary gap. For detailed
descriptions of related work, readers can refer to
Section 2.2. In this paper, besides employing word
alignment models to social tagging, we also propose
a method to efficiently build description-annotation
pairs for sufficient learning translation probabilities
by word alignment models.
Based on the learned translation probabilities
between words in descriptions and annotations,
we regard the tagging behavior as a word trigger
process:
1. A user reads the resource description to realize
its substance by seeing some important words
in the description.
2. Triggered by these important words, the user
translates them into the corresponding tags, and
annotates the resource with these tags.
Based on this perspective, we build a simple word
trigger method (WTM) for social tag suggestion. In
Fig. 1, we use a simple example to show the basic
idea of using word trigger for social tag suggestion.
In this figure, some words in the first sentence of the
book description in Table 1 are triggered to the tags
in annotation.
Figure 1: An example of the word trigger method
for suggesting tags given a description.
2 Related Work
2.1 Social Tag Suggestion
Previous work has been proposed to automatic
social tag suggestion.
Many researchers built tag suggestion systems
based on collaborative filtering (CF) (Herlocker et
al., 1999; Herlocker et al, 2004), a widely used
technique in recommender systems (Resnick and
Varian, 1997). These collaboration-based methods
typically base their suggestions on the tagging
history of the given resource and user, without con-
sidering resource descriptions. FolkRank (Jaschke
et al, 2008) and Matrix Factorization (Rendle et al,
2009) are representative CF methods for social tag
suggestion. Most of these methods suffer from the
cold-start problem, i.e., they are not able to perform
effective suggestions for resources that no one has
annotated yet.
The content-based approach for social tag sug-
gestion remedies the cold-start problem of the
1578
collaboration-based approach by suggesting tags
according to resource descriptions. Therefore, the
content-based approach plays an important role in
social tag suggestion.
Some researchers regarded social tag suggestion
as a classification problem by considering each tag
as a category label (Ohkura et al, 2006; Mishne,
2006; Lee and Chun, 2007; Katakis et al, 2008;
Fujimura et al, 2008; Heymann et al, 2008).
Various classifiers such as Naive Bayes, kNN, SVM
and neural networks have been explored to solve the
social tag suggestion problem.
There are two issues emerging from the
classification-based methods:
? The annotations provided by users are noisy,
and the classification-based methods can not
handle the issue well.
? The training cost and classification cost of
many classification-based methods are usually
in proportion to the number of classification
labels. These methods may thus be inefficient
for a real-world social tagging system, where
hundreds of thousands of unique tags should be
considered as classification labels.
Inspired by the popularity of latent topic models
such as Latent Dirichlet Allocation (LDA) (Blei et
al., 2003), various methods have been proposed to
model tags using generative latent topic models.
One intuitive approach is assuming that both tags
and words are generated from the same set of latent
topics. By representing both tags and descriptions
as the distributions of latent topics, this approach
suggests tags according to their likelihood given
the description (Krestel et al, 2009; Si and Sun,
2009). Bundschus et al (2009) proposed a joint
latent topic model of users, words and tags. Iwata
et al (2009) proposed an LDA-based topic model,
Content Relevance Model (CRM), which aimed at
finding the content-related tags for suggestion. Em-
pirical experiments showed that CRM outperformed
both classification methods and Corr-LDA (Blei and
Jordan, 2003), a generative topic model for contents
and annotations.
Most latent topic models have to pre-specify the
number of topics before training. We can either use
cross validation to determine the optimal number
of topics or employ the infinite topic models, such
as Hierarchical Dirichlet Process (HDP) (Teh et al,
2006) and nested Chinese Restaurant Process (Blei
et al, 2010), to automatically adjust the number
of topics during training. Both solutions are
usually computationally complicated. What is more
important, topic-based methods suggest tags by
measuring the topical relevance of tags and resource
descriptions. The latent topics are of concept-level
which are usually too general to precisely suggest
those specific tags such as named entities, e.g.,
the tags ?Dumas? and ?Count of Monte Cristo? in
Table 1. To remedy the problem, Si et al (2010)
proposed a generative model, Tag Allocation Model
(TAM), which considers the words in descriptions
as the possible topics to generate tags. However,
TAM assumes each tag can only have at most one
word as its reason. This is against the fact that a tag
may be annotated triggered by multiple words in the
description.
It should also be noted that social tag suggestion is
different from automatic keyphrase extraction (Tur-
ney, 2000; Frank et al, 1999; Liu et al, 2009a; Liu
et al, 2010b; Liu et al, 2011). Keyphrase extraction
aims at selecting terms from the given document
to represent the main topics of the document. On
the contrary, in social tag suggestion, the suggested
tags do not necessarily appear in the given resource
description. We can thus regard social tag sugges-
tion as a task of selecting appropriate tags from
a controlled tag vocabulary for the given resource
description.
2.2 Applications of SMT
SMT techniques have been successfully used in
many tasks of information retrieval and natural
language processing to bridge the vocabulary gap
between two types of objects. Some typical tasks are
document information retrieval (Berger and Laffer-
ty, 1999; Murdock and Croft, 2004; Karimzadehgan
and Zhai, 2010), question answering (Berger et al,
2000; Echihabi and Marcu, 2003; Soricut and Brill,
2006; Riezler et al, 2007; Surdeanu et al, 2008;
Xue et al, 2008), query expansions (Riezler et al,
2007; Riezler et al, 2008; Riezler and Liu, 2010),
paraphrasing (Quirk et al, 2004; Zhao et al, 2010a;
Zhao et al, 2010b), summarization (Banko et al,
2000), collocation extraction (Liu et al, 2009b;
1579
Liu et al, 2010c), keyphrase extraction (Liu et
al., 2011), sentiment analysis (Dalvi et al, 2009),
computational advertising (Ravi et al, 2010), and
image/video annotation and retrieval (Duygulu et
al., 2002; Jeon et al, 2003).
3 Word Trigger Method for Social Tag
Suggestion
3.1 Method Framework
We describe the word trigger method (WTM) for
social tag suggestion as a 3-stage process:
1. Preparing description-annotation pairs.
Given a collection of annotated resources, we first
prepare description-annotation pairs for learning
translation probabilities using word alignment mod-
els.
2. Learning a translation model. Given a
collection of description-annotation pairs, we adopt
IBM Model-1, a widely used word alignment model,
to learn the translation probabilities between words
in descriptions and tags in annotations.
3. Suggesting tags given a resource description.
After building translation probabilities between
words and tags, given a resource description, we
first compute the trigger power of each word in the
description and then suggest tags according to their
translation probabilities from the triggered words.
Before introducing the method in details, we
introduce the notations. In a social tagging system,
a resource is denoted as r ? R, where R is the set of
all resources. Each resource contains a description
and an annotation containing a set of tags. The
description dr of resource r can be regarded as a bag
of words wr = {(wi, ei)}Nri=1, where ei is the count
of word wi and Nr is the number of unique words
in r. The annotation ar of resource r is represented
as tr = {(ti, ei)}Mri=1, where ei is the count of tag ti
and Mr is the number of unique tags for r.
3.2 Preparing Description-Annotation Pairs
Learning translation probabilities requires a parallel
training dataset consisting of a number of aligned
sentence pairs. We assume the description and the
annotation of a resource as being written in two
distinct languages. We thus prepare our parallel
training dataset by pairing descriptions with anno-
tations.
The annotation of a resource is a bag of tags with
no position information. We thus select IBM Model-
1 (Brown et al, 1993) for training, which does not
take word position information into account on both
sides for each aligned pair.
In a social tagging system, the length of a
resource description is usually limited to hundreds
of words. Meanwhile, it is common that some
popular resources are annotated by multiple users
with thousands of tags. For example, the tag
Dumas is annotated by 2, 748 users for the book
in Table 1. We have to deal with the length-
unbalance between a resource description and its
corresponding annotation for two reasons.
? It is impossible to list all annotated tags on
the annotation side of a description-annotation
pair. The performance of word alignment
models will also suffer from the unbalanced
length of sentence pairs in the parallel training
data set (Och and Ney, 2003).
? Moreover, the annotated tags may have differ-
ent importance for the resource. It would be
unfair to treat these tags without distinction.
Here we propose a sampling method to pre-
pare length-balanced description-annotation pairs
for word alignment. The basic idea is to sample
a bag of tags from the annotation according to tag
weights and make the generated bag of tags with
comparable length with the description.
We consider two parameters when sampling tags.
First, we have to select a tag weighting type for
sampling. In this paper, we investigate two straight-
forward sampling types, including tag frequen-
cy (TFt) within the annotation and tag-frequency
inverse-resource-frequency (TF-IRFt). Given re-
source r, TFt and TF-IRFt of tag t are defined
as TFt = et/
?
t et and TF-IRFt = et/
?
t et ?
log
(
|R|/|?r?R Iet>0|
)
, where |?r?R Iet>0| in-
dicates the number of resources that have been
annotated with tag t.
Another parameter is the length ratio between the
description and the sampled annotation. We denote
the ratio as ? = |wr|/|tr|, where |wr| is the number
of words in the description and |tr| is the number of
tags in the annotation.
1580
3.3 Learning Translation Probabilities Using
Word Alignment Models
Suppose the source language is resource description
and the target language is resource annotation.
In IBM Model-1, the relationship of the source
language w = wJ1 and the target language t = tI1
is connected via a hidden variable describing an
alignment mapping from source position j to target
position aj :
Pr(wJ1 |tI1) =
?
aJ1
Pr(wJ1 , aJ1 |tI1). (1)
The alignment aJ1 also contains empty-word align-
ments aj = 0 which align source words to the
an empty word. IBM Model-1 can be trained
using Expectation-Maximization (EM) algorithm in
an unsupervised fashion, and obtains the translation
probabilities of two vocabularies, i.e., Pr(w|t),
where t is a tag and w is a word.
IBM Model-1 only produces one-to-many align-
ments from source language to target language.
The learned model is thus asymmetric. We will
learn translation models on two directions: one is
regarding descriptions as the source language and
annotations as the target language, and the other is
in reverse direction of the pairs. We denote the first
model as Prd2a and the latter as Pra2d. We further
define Pr(t|w) as the harmonic mean of the two
models:
Pr(t|w) ?
(
?/Pr d2a(t|w)+(1??)/Pr a2d(t|w)
)?1
,
(2)
where ? is the harmonic factor to combine the two
models. When ? = 1 or ? = 0, it simply uses model
Prd2a or Pra2d correspondingly.
3.4 Tag Suggestion Using Triggered Words and
Translation Probabilities
When given the description of a resource, we can
rank tags by computing the scores:
Pr(t|d = wd) =
?
w?wd
Pr(t|w) Pr(w|d), (3)
in which Pr(w|d) is the trigger power of the word w
in the description, which indicates the importance of
the word. According to the ranking scores, we can
suggest the top-ranked tags to users.
Here we explore three methods to compute the
trigger power of a word in a resource description:
TF-IRFw, TextRank and their product. TF-IRFw and
TextRank are two most widely adopted methods for
keyword extraction.
Similar to TF-IRFt mentioned in Section 3.2, TF-
IRFw considers both the local importance (TFw) and
global specification (IRFw).
TextRank (Mihalcea and Tarau, 2004) is a graph-
based method to compute term importance. Given
a resource description, TextRank first builds a term
graph by connecting the terms in the description
according to their semantic relations, and then run
PageRank algorithm (Page et al, 1998) to measure
the importance of each term in the graph. Readers
can refer to (Mihalcea and Tarau, 2004) for detailed
information.
We also use the product of TF-IRFw and Tex-
tRank to weight terms, which potentially takes both
global information and term relations into account.
Emphasize Tags Appearing In Description for
WTM (EWTM) In some social tagging systems,
the tags that appear in the resource description are
more likely to be selected by users for annotation.
Therefore, we propose to emphasize the tags in the
description by ranking tags as follows
Pr(t|d) =
?
w?wd
(
?It(w)+(1??) Pr(t|w)
)
Pr(w|d),
(4)
where It(w) is an indicator function which gets
value 1 when t = w and 0 when t 6= w; and ? is
the smooth factor with range ? ? [0.0, 1.0]. When
? = 1.0, it suggests tags simply according to their
trigger powers within the description, while when
? = 0.0, it does not emphasize the tags appearing in
the description and just suggests according to their
translation probabilities. In Section 4.4, we will
show the performance of EWTM.
4 Experiments
4.1 Datasets and Evaluation Metrics
Datasets In our experiments, we select two real
world datasets which are of diverse properties to
evaluate our methods. In Table 2 we show the
detailed statistical information of the two datasets.
1581
Data R W T N?w N?t
BOOK 70, 000 174, 748 46, 150 211.6 3.5
BIBTEX 158, 924 91, 277 50, 847 5.8 2.7
Table 2: Statistical information of two datasets. R,
W , T , N?w and N?t are the number of resources, the
vocabulary of descriptions, the vocabulary of tags,
the average number of words in each description
and the average number of tags in each resource,
respectively.
The first dataset, denoted as BOOK, is obtained
from a popular Chinese book review website www.
douban.com, which contains the descriptions of
books and the tags collaboratively annotated by
users. The second dataset, denoted as BIBTEX, is
obtained from an English online bibliography web-
site www.bibsonomy.org2. The dataset contains
the descriptions for academic papers (including the
title and note for each paper) and the tags annotated
by users. As shown in Table 2, the average length of
descriptions in the BIBTEX dataset is much shorter
than the BOOK dataset. Moreover, the BIBTEX
dataset does not provide how many times each tag
is annotated to a resource.
Evaluation Metrics We use precision, recall and
F-measure to evaluate the performance of tag sug-
gestion methods. For a resource, we denote the
original tags (gold standard) as Ta, the suggested
tags as Ts, and the correctly suggested tags as Ts ?
Ta. Precision, recall and F-measure are defined as
p = |Ts ? Ta||Ts|
, r = |Ts ? Ta||Ta|
, F = 2pr(p + r) .
(5)
The final evaluation scores are computed by micro-
averaging (i.e., averaging on resources of test set).
We perform 5-fold cross validation for each method
on all two datasets. In experiments, the number of
suggested tags M ranges from 1 to 10.
4.2 Comparing Results
Baseline Methods We select four content-based
algorithms as the baselines for comparison: Naive
Bayes (NB) (Manning et al, 2008), k nearest
neighbor algorithm (kNN) (Manning et al, 2008),
2The dataset can be obtained from http://www.kde.
cs.uni-kassel.de/bibsonomy/dumps
Content Relevance (CRM) model (Iwata et al,
2009) and Tag Allocation Model (TAM) (Si et al,
2010).
NB and kNN are two representative classification
methods. NB is a simple generative model, which
models the probability of each tag t given descrip-
tion d as
Pr(t|d) ? Pr(t)
?
w?d
Pr(w|t). (6)
Pr(t) is estimated by the frequency of the resources
annotated with the tag t. Pr(w|t) is estimated by the
frequency of the word w in the resource descriptions
annotated with the tag t. kNN is a widely used
classification method for tag suggestion, which
recommends tags to a resource according to the
annotated tags of similar resources measured using
vector space models (Manning et al, 2008).
CRM and TAM are selected to represent topic-
based methods for tag suggestion. CRM is an LDA-
based generative model. The number of latent topics
K is the key parameter for CRM. In experiments, we
evaluated the performance of CRM with different K
values, and here we only show the best one obtained
by setting K = 1, 024. TAM is also a generative
model which considers the words in descriptions as
the topics to further generate tags for the resource.
We set parameters for TAM as in (Si et al, 2010).
For comparison, we denote our method as WTM.
Complexity Analysis We compare the complexity
of these methods. We denote the number of training
iterations in CRM, TAM and WTM as I 3, and
the number of topics in CRM as K. For the
training phase, the complexity of NB is O(RN?wN?t),
kNN is O(1), TAM is O(IRN?wN?t), CRM is
O(IKRN?wN?t), and WTM is O(IRN?wN?t)4. When
suggesting for a given resource description with
length Nw, the complexity of NB is O(NwT ),
kNN is O(RN?wN?t), CRM is O(IKNwT ), TAM
3In fact, the numbers of iterations of the three methods are
different from each other. For simplicity, here we denote them
using the same notation.
4In more detail, the training phase of WTM contains
preparing parallel training dataset with O(RN?t) and learning
translation probabilities using word alignment models with
O(IRN?wN?t), where I is the number of iterations for learning
translation probabilities, and N?t is the average number of tags
for each resource after sampling.
1582
is O(INwT ) and WTM is O(NwT ). From the
analysis, we can see that WTM is a relatively simple
method for both training and suggestion. This is
especially valuable because WTM also shows good
effectiveness for tag suggestion compared with other
methods as we will shown later.
Parameter Settings We use GIZA++ (Och and
Ney, 2003)5 as IBM Model-1 to learn transla-
tion probabilities using description-annotation pairs
for WTM. The experimental results of WTM are
obtained by setting parameters as follows: tag
weighting type as TF-IRFt, length ratio ? = 1,
harmonic factor ? = 0.5 and the type of word trigger
strength as TF-IRFw. The influence of parameters to
WTM can be found in Section 4.3.
Experiment Results and Analysis In Fig. 2 we
show the precision-recall curves of NB, kNN, CRM
and WTM on two datasets. Each point of a
precision-recall curve represents different numbers
of suggested tags from M = 1 (bottom right, with
higher precision and lower recall) to M = 10
(upper left, with higher recall but lower precision)
respectively. The closer the curve to the upper right,
the better the overall performance of the method.
From Fig. 2, we observe that:
? WTM consistently performs the best on both
datasets. This indicates that WTM is robust and
effective for tag suggestion.
? The advantage of WTM is more significant on
the BOOK dataset. The reason is that WTM
can take a good advantage of annotation count
information of tags compared to other methods.
? The average length of resource descriptions is
short in the BIBTEX dataset, which makes
it difficult to determine the trigger powers of
words. But even on the BIBTEX dataset
with no count information of tags, WTM still
outperforms other methods especially when
recommending first several tags.
To further demonstrate the performance of WTM
and other baseline methods, in Table 3 we show the
5GIZA++ is freely available on code.google.com/p/
giza-pp. The toolkit is widely used for word alignment in
SMT. In this paper, we use the default setting of parameters for
training.
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 0.15  0.2  0.25  0.3  0.35  0.4  0.45  0.5  0.55  0.6  0.65
R
ec
al
l
Precision
 WAM
 NB
 kNN
 CRM
 TAM
(a) BOOK
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
 0.55
 0.05  0.1  0.15  0.2  0.25  0.3  0.35  0.4
R
ec
al
l
Precision
 WAM
 NB
 kNN
 CRM
 TAM
(b) BIBTEX
Figure 2: Performance comparison between NB,
kNN, CRM, TAM and WTM on two datasets.
precision, recall and F-measure of NB, kNN, CRM,
TAM and WTM on BOOK dataset when suggesting
M = 3 tags6. Due to the limit of space, we only
show the variance of F-measure. In fact, WTM
achieves its best performance when M = 2, where
the F-measure of WTM is 0.370, outperforming
both CRM (F = 0.263) and TAM (F = 0.277) by
about 10%.
An Example In Table 4 we show top 10 tags
suggested by NB, CRM, TAM and WTM for the
book in Table 1. The number in bracket after
the name of each method is the count of correctly
suggested tags. The correctly suggested tags are
marked in bold face. We select not to show
6We select to show this number because it is near the average
number of tags for BOOK dataset
1583
Method Precision Recall F-measure
NB 0.271 0.302 0.247? 0.004
kNN 0.280 0.314 0.258? 0.002
CRM 0.292 0.323 0.266? 0.004
TAM 0.310 0.344 0.283? 0.001
WTM 0.368 0.452 0.355? 0.002
Table 3: Comparing results of NB, kNN, CRM,
TAM and WTM on BOOK dataset when suggesting
M = 3 tags.
the results of kNN because the tags suggested by
kNN are totally unrelated to the book due to the
insufficient finding of nearest neighbors.
From Table 4, we observe that NB, CRM and
TAM, as generative models, tend to suggest general
tags such as ?novel?, ?literature?, ?classic? and
?France?, and fail in suggesting specific tags such as
?Alexandre Dumas? and ?Count of Monte Cristo?.
On the contrary, WTM succeeds in suggesting both
general and specific tags related to the book.
NB (+6): novel, foreign literature, literature, his-
tory, Japan, classic, France, philosophy, America,
biography
CRM (+5): novel, foreign literature, literature, bi-
ography, philosophy, culture, France, British, comic,
history
TAM (+5): novel, sociology, finance, foreign liter-
ature, France, literature, biography, France litera-
ture, comic, China
WTM (+7): novel, Alexandre Dumas, history,
Count of Monte Cristo, foreign literature, biogra-
phy, suspense, comic, America, France
Table 4: Top 10 tags suggested by NB, CRM, TAM
and WTM for the book in Table 1.
In Table 5, we list four important words (using
TF-IRFw as weighting metric) of the description and
their corresponding tags with the highest translation
probabilities. The values in brackets are the proba-
bility of tag t given word w, Pr(t|w). For each word,
we eliminated the tags with the probability less than
0.1. We can see that the translation probabilities can
map the words in descriptions to their semantically
corresponding tags in annotations.
Count of Monte Cristo: Count of Monte Cristo
(0.728), Alexandre Dumas (0.270), . . .
Alexandre Dumas: Alexandre Dumas (0.966), . . .
revenge: foreign literature (0.168), classic (0.130),
martial arts (0.123), Alexandre Dumas (0.122), . . .
France: France (0.99), . . .
Table 5: Four important words (in bold face) in the
book description in Table 1 and their corresponding
tags with the highest translation probabilities.
4.3 Parameter Influences
We explore the parameter influences to WTM for
social tag suggestion. The parameters include
harmonic factor, length ratio, tag weighting types,
and types of word trigger strength. When inves-
tigating one parameter, we set other parameters
to be the values inducing the best performance
as mentioned in Section 4.2. Finally, we also
investigate the influence of training data size for
suggestion performance. In experiments we find
that WTM reveals similar trends on both the BOOK
dataset and the BIBTEX dataset. We thus only show
the experimental results on the BOOK dataset for
analysis.
Harmonic Factor In Fig. 3 we investigate the
influence of harmonic factor via the curves of F-
measure of WTM versus the number of suggested
tags on the BOOK dataset when harmonic factor ?
ranges from 0.0 to 1.0. As shown in Section 3.3,
harmonic factor ? controls the proportion between
model Prd2a and Pra2d.
From Fig. 3, we observe that neither single model
Prd2a (? = 1.0) nor Pra2d (? = 0.0) achieves
the best performance. When the two models are
combined by harmonic mean, the performance is
consistently better, especially when ? ranges from
0.2 to 0.6. This is reasonable because IBM Model-
1 constrains that only the term in source language
can be aligned to multiple terms in target language,
which makes the translation probability learned by a
single model be asymmetric.
Length Ratio Fig. 4 shows the influence of length
ratios on the BOOK dataset. From the figure, we
observe that the performance for tag suggestion is
robust as the length ratio varies, except when the
ratio breaks the default restriction of GIZA++ (i.e.,
1584
? = 10)7.
 0.22
 0.24
 0.26
 0.28
 0.3
 0.32
 0.34
 0.36
 0.38
1 2 3 4 5 6 7 8 9 10
F-
m
ea
su
re
Number of Suggested Tags
? = 0.0
? = 0.2
? = 0.4
? = 0.5
? = 0.6
? = 0.8
? = 1.0
Figure 3: F-measure of WTM versus the number of
suggested tags on the BOOK dataset when harmonic
factor ? ranges from 0.0 to 1.0.
 0.2
 0.22
 0.24
 0.26
 0.28
 0.3
 0.32
 0.34
 0.36
 0.38
1 2 3 4 5 6 7 8 9 10
F-
m
ea
su
re
Number of Suggested Tags
? = 10/1
? = 10/3
? = 10/5
? = 1/1
? = 1/2
? = 1/5
Figure 4: F-measure of WTM versus the number of
suggested tags on the BOOK dataset when length
ratio ? ranges from 10/1 to 1/5.
Tag Weighting Types The influence of two
weighting types, TFt and TF-IRFt, on social tag
suggestion when M = 3 on the BOOK dataset
is shown in Table 6. TF-IRFt tends to select the
tags more specific to the resource while TFt tends
to select the most popular tags, because the latter
does not consider global information (the IRFt part).
7GIZA++ restricts the values of length ratio within [ 19 , 9] by
setting parameter maxfertility=10. From Fig. 4, we can
see when ? = 10, the performance becomes much worse since
GIZA++ will cut off the sentences out of range.
Table 6 verifies the analysis, where TF-IRFt is
slightly better than TFt.
Weighting Precision Recall F-measure
TFt 0.356 0.437 0.342? 0.002
TF-IRFt 0.368 0.452 0.355? 0.002
Table 6: Evaluation results for different tag weight-
ing types when M = 3 on the BOOK dataset.
Methods for Computing Word Trigger Power
In Table 7, we show the performance of social tag
suggestions on the BOOK dataset with different
methods for computing word trigger power. From
the table, we can see that there is not significant
difference between TF-IRFw and the product of TF-
IRFw and TextRank, while TextRank itself performs
the worst. This indicates that TextRank is less
competitive to measure word trigger power since it
does not take global information into consideration.
Weighting Precision Recall F-measure
TF-IRFw 0.368 0.452 0.355? 0.002
TextRank 0.345 0.424 0.332? 0.002
Product 0.368 0.451 0.354? 0.002
Table 7: Evaluation results for different methods for
computing word trigger powers when M = 3 on the
BOOK dataset.
Training Data Size We investigate the influence
of training data size for social tag suggestion. As
shown in Fig. 5, we increased the training data size
from 8, 000 to 56, 000 step by 8, 000, and carried
out evaluation on 4, 000 resources. The figure shows
that:
? When the training data size is small (e.g.,
8, 000), WTM can still achieve good sugges-
tion performance.
? As the training data size increases, the perfor-
mance of WTM improves, while the improve-
ment speed declines.
The observation indicates that WTM does not
require huge-size dataset to achieve good perfor-
mance.
1585
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 0.1  0.2  0.3  0.4  0.5  0.6  0.7
R
ec
al
l
Precision
  8,000
 16,000
 24,000
 32,000
 40,000
 48,000
 56,000
Figure 5: Precision-recall curves when the training
data size increases from 8, 000 thousand to 56, 000
thousand on the BOOK dataset.
Conclusion By analyzing the influences of pa-
rameters on WTM, we find that WTM is robust to
parameter variations.
4.4 Performance of EWTM
At the end of this section, we investigate the
performance of EWTM for social tag suggestion.
Here we simply set the smooth factor ? = 0.5.
As shown in Table 8, EWTM improves the
performance of WTM (in Table 7) on the BOOK
dataset when using TF-IRFw and the product as the
methods for computing the word trigger powers,
but decays when using TextRank. This verifies
that TF-IRFw is the best method to measure word
trigger powers for WTM. Table 8 indicates that
emphasizing the tags appearing in the descriptions
may enhance the suggestion power of the word
trigger method.
Weighting Precision Recall F-measure
TF-IRFw 0.385 0.472 0.371? 0.001
TextRank 0.344 0.423 0.332? 0.002
Product 0.374 0.457 0.360? 0.001
Table 8: The evaluation results of EWTM with dif-
ferent methods for computing word trigger powers
when M = 3 on the BOOK dataset.
However, the performance of EWTM on the
BIBTEX dataset decays much compared to WTM.
The F-measure of EWTM is only F = 0.229
compared with WTM F = 0.267. The main reason
of the decay is that: the resource descriptions in
the BIBTEX dataset are usually too short to provide
sufficient information to precisely emphasize tags.
In this case, EWTM may emphasize wrong tags and
drop correct tags.
The experimental results on EWTM suggest that,
the performance of EWTM is heavily influenced by
the length of resource descriptions. Therefore, we
have to analyze the characteristics of social tagging
systems to decide whether to emphasize the tags that
appear in the corresponding resource descriptions.
As future work, we will investigate the influence
of the smooth factor ? to EWTM. It is also worth
to investigate the problem when combining with
collaboration-based methods for social tag sugges-
tion.
5 Conclusion and Future Work
In this paper, we present a new perspective to social
tagging and propose the word trigger method for
social tag suggestion based on word alignment in
statistical machine translation. Experiments show
that our method is effective and efficient for social
tag suggestion compared to other baselines.
There are still several open problems that should
be further investigated:
1. We can exploit other word alignment methods
like log-linear models (Liu et al, 2010a) for
social tag suggestion.
2. We will ensemble WTM with other content-
based and collaboration-based methods to build
a practical social tag suggestion system.
3. WTM and EWTM can only suggest the tags
that have appeared in translation models. In
future, we plan to incorporate keyphrase ex-
traction in social tag suggestion to make it
suggest more appropriate tags not only from
translation models but also from the resource
descriptions.
Acknowledgments
This work is supported by the National Natural
Science Foundation of China (NSFC) under Grant
No. 60873174. The authors would like to thank
Peng Li for his insightful suggestions and thank the
anonymous reviewers for their helpful comments.
1586
References
M. Banko, V.O. Mittal, and M.J. Witbrock. 2000.
Headline generation based on statistical translation. In
Proceedings of ACL, pages 318?325.
A. Berger and J. Lafferty. 1999. Information retrieval as
statistical translation. In Proceedings of SIGIR, pages
222?229.
A. Berger, R. Caruana, D. Cohn, D. Freitag, and
V. Mittal. 2000. Bridging the lexical chasm: statistical
approaches to answer-finding. In Proceedings of
SIGIR, pages 192?199.
D.M. Blei and M.I. Jordan. 2003. Modeling annotated
data. In Proceedings of SIGIR, pages 127?134.
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
dirichlet alocation. JMLR, 3:993?1022.
D.M. Blei, T.L. Griffiths, and M.I. Jordan. 2010.
The nested chinese restaurant process and bayesian
nonparametric inference of topic hierarchies. Journal
of the ACM, 57(2):7.
P.F. Brown, V.J.D. Pietra, S.A.D. Pietra, and R.L.
Mercer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational
linguistics, 19(2):263?311.
M. Bundschus, S. Yu, V. Tresp, A. Rettinger, M. Dejori,
and H.P. Kriegel. 2009. Hierarchical bayesian models
for collaborative tagging systems. In Proceedings of
ICDM, pages 728?733.
N. Dalvi, R. Kumar, B. Pang, and A. Tomkins. 2009. A
translation model for matching reviews to objects. In
Proceeding of CIKM, pages 167?176.
P. Duygulu, K. Barnard, J. De Freitas, and D. Forsyth.
2002. Object recognition as machine translation:
Learning a lexicon for a fixed image vocabulary.
Proceedings of ECCV, pages 97?112.
A. Echihabi and D. Marcu. 2003. A noisy-channel
approach to question answering. In Proceedings of
ACL, pages 16?23.
D. Eck, P. Lamere, T. Bertin-Mahieux, and S. Green.
2007. Automatic generation of social tags for music
recommendation. In Proceedings of NIPS, pages 385?
392.
E. Frank, G.W. Paynter, I.H. Witten, C. Gutwin, and C.G.
Nevill-Manning. 1999. Domain-specific keyphrase
extraction. In Proceedings of IJCAI, pages 668?673.
S. Fujimura, KO Fujimura, and H. Okuda. 2008.
Blogosonomy: Autotagging any text using bloggers?
knowledge. In Proceedings of WI, pages 205?212.
J.L. Herlocker, J.A. Konstan, A. Borchers, and J. Riedl.
1999. An algorithmic framework for performing
collaborative filtering. In Proceedings of SIGIR, pages
230?237.
J.L. Herlocker, J.A. Konstan, L.G. Terveen, and J.T.
Riedl. 2004. Evaluating collaborative filtering recom-
mender systems. ACM Transactions on Information
Systems, 22(1):5?53.
P. Heymann, D. Ramage, and H. Garcia-Molina. 2008.
Social tag prediction. In Proceedings of SIGIR, pages
531?538.
A. Hotho, R. Jaschke, C. Schmitz, and G. Stumme.
2006. Trend detection in folksonomies. Semantic
Multimedia, pages 56?70.
T. Iwata, T. Yamada, and N. Ueda. 2009. Modeling
social annotation data with content relevance using a
topic model. In Proceedings of NIPS, pages 835?843.
R. Jaschke, L. Marinho, A. Hotho, L. Schmidt-Thieme,
and G. Stumme. 2008. Tag recommendations in
social bookmarking systems. AI Communications,
21(4):231?247.
J. Jeon, V. Lavrenko, and R. Manmatha. 2003. Automat-
ic image annotation and retrieval using cross-media
relevance models. In Proceedings of SIGIR, pages
119?126.
M. Karimzadehgan and C.X. Zhai. 2010. Estimation of
statistical translation models based on mutual informa-
tion for ad hoc information retrieval. In Proceedings
of SIGIR, pages 323?330.
I. Katakis, G. Tsoumakas, and I. Vlahavas. 2008. Mul-
tilabel text classification for automated tag suggestion.
ECML PKDD Discovery Challenge 2008, page 75.
R. Krestel, P. Fankhauser, and W. Nejdl. 2009. Latent
dirichlet alocation for tag recommendation. In
Proceedings of ACM RecSys, pages 61?68.
S.O.K. Lee and A.H.W. Chun. 2007. Automatic
tag recommendation for the web 2.0 blogosphere
using collaborative tagging and hybrid ann semantic
structures. In Proceedings of WSEAS, pages 88?93.
Z. Liu, P. Li, Y. Zheng, and M. Sun. 2009a. Clustering
to find exemplar terms for keyphrase extraction. In
Proceedings of EMNLP, pages 257?266.
Z. Liu, H. Wang, H. Wu, and S. Li. 2009b. Collocation
extraction using monolingual word alignment method.
In Proceedings of EMNLP, pages 487?495.
Y. Liu, Q. Liu, and S. Lin. 2010a. Discriminative
word alignment by linear modeling. Computational
Linguistics, 36(3):303?339.
Z. Liu, W. Huang, Y. Zheng, and M. Sun. 2010b. Au-
tomatic keyphrase extraction via topic decomposition.
In Proceedings of EMNLP, pages 366?376.
Z. Liu, H. Wang, H. Wu, and S. Li. 2010c. Improving
statistical machine translation with monolingual collo-
cation. In Proceedings of ACL, pages 825?833.
Z. Liu, X. Chen, Y. Zheng, and M. Sun. 2011. Automatic
keyphrase extraction by bridging vocabulary gap. In
Proceedings of CoNLL, pages 135?144.
1587
C.D. Manning, P. Raghavan, and H. Schtze. 2008.
Introduction to information retrieval. Cambridge
University Press New York, NY, USA.
R. Mihalcea and P. Tarau. 2004. TextRank: Bringing
order into texts. In Proceedings of EMNLP, pages
404?411.
R. Mirizzi, A. Ragone, T. Di Noia, and E. Di Sciascio.
2010. Semantic tags generation and retrieval for
online advertising. In Proceedings of CIKM, pages
1089?1098.
G. Mishne. 2006. Autotag: a collaborative approach
to automated tag assignment for weblog posts. In
Proceedings of WWW, pages 953?954.
V. Murdock and W.B. Croft. 2004. Simple translation
models for sentence retrieval in factoid question an-
swering. In Proceedings of SIGIR.
F.J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
linguistics, 29(1):19?51.
T. Ohkura, Y. Kiyota, and H. Nakagawa. 2006. Browsing
system for weblog articles based on automated folk-
sonomy. In Proceedings of WWW.
L. Page, S. Brin, R. Motwani, and T. Winograd. 1998.
The pagerank citation ranking: Bringing order to
the web. Technical report, Stanford Digital Library
Technologies Project.
C. Quirk, C. Brockett, and W. Dolan. 2004. Monolingual
machine translation for paraphrase generation. In
Proceedings of EMNLP, volume 149.
S. Ravi, A. Broder, E. Gabrilovich, V. Josifovski,
S. Pandey, and B. Pang. 2010. Automatic generation
of bid phrases for online advertising. In Proceedings
of WSDM, pages 341?350.
S. Rendle, L. Balby Marinho, A. Nanopoulos, and
L. Schmidt-Thieme. 2009. Learning optimal ranking
with tensor factorization for tag recommendation. In
Proceedings of KDD, pages 727?736.
P. Resnick and H.R. Varian. 1997. Recommender
systems. Communications of the ACM, 40(3):56?58.
S. Riezler and Y. Liu. 2010. Query rewriting using
monolingual statistical machine translation. Compu-
tational Linguistics, 36(3):569?582.
S. Riezler, A. Vasserman, I. Tsochantaridis, V. Mittal, and
Y. Liu. 2007. Statistical machine translation for query
expansion in answer retrieval. In Proccedings of ACL,
pages 464?471.
S. Riezler, Y. Liu, and A. Vasserman. 2008. Translating
queries into snippets for improved query expansion. In
Proceedings of COLING, pages 737?744.
X. Si and M. Sun. 2009. Tag-LDA for scalable real-
time tag recommendation. Journal of Computational
Information Systems, 6(1):23?31.
X. Si, Z. Liu, and M. Sun. 2010. Modeling social
annotations via latent reason identification. IEEE
Intelligent Systems, 25(6):42 ? 49.
R. Soricut and E. Brill. 2006. Automatic question
answering using the web: Beyond the factoid. Infor-
mation Retrieval, 9(2):191?206.
M. Surdeanu, M. Ciaramita, and H. Zaragoza. 2008.
Learning to rank answers on large online qa collec-
tions. In Proceedings of ACL, pages 719?727.
Y.W. Teh, M.I. Jordan, M.J. Beal, and D.M. Blei.
2006. Hierarchical dirichlet processes. Journal of
the American Statistical Association, 101(476):1566?
1581.
P.D. Turney. 2000. Learning algorithms for keyphrase
extraction. Information Retrieval, 2(4):303?336.
F.Y. Wang, K.M. Carley, D. Zeng, and W. Mao. 2007.
Social computing: From social informatics to social
intelligence. IEEE Intelligent Systems, 22(2):79?83.
R. Wetzker, C. Zimmermann, C. Bauckhage, and S. Al-
bayrak. 2010. I tag, you tag: translating tags for
advanced user models. In Proceedings of WSDM,
pages 71?80.
X. Xue, J. Jeon, and W.B. Croft. 2008. Retrieval models
for question and answer archives. In Proceedings of
SIGIR, pages 475?482.
Y. Yanbe, A. Jatowt, S. Nakamura, and K. Tanaka. 2007.
Can social bookmarking enhance search in the web?
In Proceedings of JCDL, pages 107?116.
S. Zhao, H. Wang, X. Lan, and T. Liu. 2010a. Leverag-
ing multiple mt engines for paraphrase generation. In
Proceedings of COLING, pages 1326?1334.
S. Zhao, H. Wang, and T. Liu. 2010b. Paraphrasing with
search engine query logs. In Proceedings of COLING,
pages 1317?1325.
T.C. Zhou, H. Ma, M.R. Lyu, and I. King. 2010.
UserRec: A user recommendation framework in social
tagging systems. In Proceedings of AAAI, pages
1486?1491.
1588
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1025?1035,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
A Unified Model for Word Sense Representation and Disambiguation
Xinxiong Chen, Zhiyuan Liu, Maosong Sun
State Key Laboratory of Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Science and Technology
Tsinghua University, Beijing 100084, China
cxx.thu@gmail.com, {lzy, sms}@tsinghua.edu.cn
Abstract
Most word representation methods assume
that each word owns a single semantic vec-
tor. This is usually problematic because
lexical ambiguity is ubiquitous, which is
also the problem to be resolved by word
sense disambiguation. In this paper, we
present a unified model for joint word
sense representation and disambiguation,
which will assign distinct representation-
s for each word sense.
1
The basic idea is
that both word sense representation (WS-
R) and word sense disambiguation (WS-
D) will benefit from each other: (1) high-
quality WSR will capture rich informa-
tion about words and senses, which should
be helpful for WSD, and (2) high-quality
WSD will provide reliable disambiguat-
ed corpora for learning better sense rep-
resentations. Experimental results show
that, our model improves the performance
of contextual word similarity compared to
existing WSR methods, outperforms state-
of-the-art supervised methods on domain-
specific WSD, and achieves competitive
performance on coarse-grained all-words
WSD.
1 Introduction
Word representation aims to build vectors for each
word based on its context in a large corpus, usually
capturing both semantic and syntactic information
of words. These representations can be used as
features or inputs, which are widely employed in
information retrieval (Manning et al., 2008), doc-
ument classification (Sebastiani, 2002) and other
NLP tasks.
1
Our sense representations can be downloaded at http:
//pan.baidu.com/s/1eQcPK8i.
Most word representation methods assume each
word owns a single vector. However, this is usual-
ly problematic due to the homonymy and polyse-
my of many words. To remedy the issue, Reisinger
and Mooney (2010) proposed a multi-prototype
vector space model, where the contexts of each
word are first clustered into groups, and then each
cluster generates a distinct prototype vector for a
word by averaging over all context vectors with-
in the cluster. Huang et al. (2012) followed this
idea, but introduced continuous distributed vectors
based on probabilistic neural language models for
word representations.
These cluster-based models conduct unsuper-
vised word sense induction by clustering word
contexts and, thus, suffer from the following is-
sues:
? It is usually difficult for these cluster-based
models to determine the number of cluster-
s. Huang et al. (2012) simply cluster word
contexts into static K clusters for each word,
which is arbitrary and may introduce mis-
takes.
? These cluster-based models are typically off-
line , so they cannot be efficiently adapted to
new senses, new words or new data.
? It is also troublesome to find the sense that
a word prototype corresponds to; thus, these
cluster-based models cannot be directly used
to perform word sense disambiguation.
In reality, many large knowledge bases have
been constructed with word senses available
online, such as WordNet (Miller, 1995) and
Wikipedia. Utilizing these knowledge bases to
learn word representation and sense representation
is a natural choice. In this paper, we present a uni-
fied model for both word sense representation and
disambiguation based on these knowledge bases
and large-scale text corpora. The unified model
1025
can (1) perform word sense disambiguation based
on vector representations, and (2) learn continu-
ous distributed vector representation for word and
sense jointly.
The basic idea is that, the tasks of word sense
representation (WSR) and word sense disam-
biguation (WSD) can benefit from each other: (1)
high-quality WSR will capture rich semantic and
syntactic information of words and senses, which
should be helpful for WSD; (2) high-quality WS-
D will provide reliable disambiguated corpora for
learning better sense representations.
By utilizing these knowledge bases, the prob-
lem mentioned above can be overcome:
? The number of senses of a word can be de-
cided by the expert annotators or web users.
? When a new sense appears, our model can be
easily applied to obtain a new sense represen-
tation.
? Every sense vector has a corresponding sense
in these knowledge bases.
We conduct experiments to investigate the per-
formance of our model for both WSR and WS-
D. We evaluate the performance of WSR using a
contextual word similarity task, and results show
that out model can significantly improve the cor-
relation with human judgments compared to base-
lines. We further evaluate the performance on
both domain-specific WSD and coarse-grained all-
words WSD, and results show that our model
yields performance competitive with state-of-the-
art supervised approaches.
2 Methodology
We describe our method as a 3-stage process:
1. Initializing word vectors and sense vectors.
Given large amounts of text data, we first use
the Skip-gram model (Mikolov et al., 2013),
a neural network based language model, to
learn word vectors. Then, we assign vector
representations for senses based on their def-
initions (e.g, glosses in WordNet).
2. Performing word sense disambiguation.
Given word vectors and sense vectors, we
propose two simple and efficient WSD algo-
rithms to obtain more relevant occurrences
for each sense.
3. Learning sense vectors from relevant oc-
currences. Based on the relevant occur-
rences of ambiguous words, we modify the
training objective of Skip-gram to learn word
vectors and sense vectors jointly. Then, we
obtain the sense vectors directly from the
model.
Before illustrating the three stages of our
method in Sections 2.2, 2.3 and 2.4, we briefly
introduce our sense inventory, WordNet, in Sec-
tion 2.1. Note that, although our experiments will
use the WordNet sense inventory, our model is not
limited to this particular lexicon. Other knowledge
bases containing word sense distinctions and defi-
nitions can also serve as input to our model.
2.1 WordNet
WordNet (Miller, 1995) is the most widely used
computational lexicon of English where a concep-
t is represented as a synonym set, or synset. The
words in the same synset share a common mean-
ing. Each synset has a textual definition, or gloss.
Table 1 shows the synsets and the corresponding
glosses of the two common senses of bank.
Before introducing the method in detail, we in-
troduce the notations. The unlabeled texts are de-
noted as R, and the vocabulary of the texts is de-
noted as W . For a word w in W , w
s
i
is the ith
sense in WordNet WN. Each sense w
s
i
has a gloss
gloss(w
s
i
) in WN. The word embedding of w is
denoted as vec(w), and the sense embedding of its
ith sense w
s
i
is denoted as vec(w
s
i
).
2.2 Initializing Word Vectors and Sense
Vectors
Initializing word vectors. First, we use Skip-
gram to train the word vectors from large amounts
of text data. We choose Skip-gram for its sim-
plicity and effectiveness. The training objective of
Skip-gram is to train word vector representations
that are good at predicting its context in the same
sentence (Mikolov et al., 2013).
More formally, given a sequence of training
words w
1
, w
2
, w
3
,...,w
T
, the objective of Skip-
gram is to maximize the average log probability
1
T
T
?
t=1
(
?
?k? j?k, j 6=0
log p(w
t+ j
|w
t
)
)
(1)
where k is the size of the training window. The
inner summation spans from ?k to k to compute
1026
Sense Synset Gloss
bank
s
1
(sloping land (especially the slope beside a body of water))
bank ?they pulled the canoe up on the bank?;
?he sat on the bank of the river and watched the currents?
bank
s
2
depository institution, (a financial institution that accepts deposits and channels the
bank, money into lending activities)
banking concern, ?he cashed a check at the bank?;
banking company ?that bank holds the mortgage on my home?
Table 1: Example of a synset in WordNet.
the log probability of correctly predicting the word
w
t+ j
given the word in the middle w
t
. The outer
summation covers all words in the training data.
The prediction task is performed via softmax, a
multiclass classifier. There, we have
p(w
t+ j
|w
t
) =
exp(vec
?
(w
t+ j
)
>
vec(w
t
))
?
W
w=1
exp(vec
?
(w)
>
vec(w
t
))
(2)
where vec(w) and vec
?
(w) are the ?input? and
?output? vector representations of w. This formu-
lation is impractical because the cost of comput-
ing p(w
t+ j
|w
t
) is proportional to W , which is often
large( 10
5
?10
7
terms).
Initializing sense vectors. After learning the
word vectors using the Skip-gram model, we ini-
tialize the sense vectors based on the glosses of
senses. The basic idea of the sense vector initial-
ization is to represent the sense by using the sim-
ilar words in the gloss. From the content words
in the gloss, we select those words whose cosine
similarities with the original word are larger than
a similarity threshold ? . Formally, for each sense
w
s
i
in WN, we first define a candidate set from
gloss(w
s
i
)
cand(w
s
i
) = {u|u ? gloss(w
s
i
),u 6= w,
POS(u) ?CW,cos(vec(w),vec(u)) > ?} (3)
where POS(u) is the part-of-speech tagging of the
word u and CW is the set of all possible part-of-
speech tags that content words could have. In this
paper, CW contains the following tags: noun, verb,
adjective and adverb.
Then the average of the word vectors in
cand(w
s
i
) is used as the initialization value of the
sense vector vec(w
s
i
).
vec(w
s
i
) =
1
|cand(w
s
i
)|
?
u?cand(w
s
i
)
vec(u) (4)
For example, in WordNet, the gloss of the sense
bank
s
1
is ?sloping land (especially the slope beside
a body of water)) they pulled the canoe up on the
bank; he sat on the bank of the river and watched
the currents?. The gloss contains a definition of
the sense and two examples of the sense. The
content words and the cosine similarities with the
word ?bank? are listed as follows: (sloping, 0.12),
(land, 0.21), (slope, 0.17), (body, 0.01), (water,
0.10), (pulled, 0.01), (canoe, 0.09), (sat, 0.06),
(river, 0.43), (watch, -0.11), (currents, 0.01). If
the threshold, ? , is set to 0.05, then cand(bank
s
1
)
is {sloping, land, slope, water, canoe, sat, riv-
er}. Then the average of the word vectors in
cand(bank
s
i
) is used as the initialization value of
vec(bank
s
i
).
2.3 Performing Word Sense Disambiguation.
One of the state-of-the-art WSD results can be
obtained using exemplar models, i.e., the word
meaning is modeled by using relevant occurrences
only, rather than merging all of the occurrences in-
to a single word vector (Erk and Pado, 2010). In-
spired by this idea, we perform word sense disam-
biguation to obtain more relevant occurrences.
Here, we perform knowledge-based word sense
disambiguation for training data on an all-words
setting, i.e., we will disambiguate all of the con-
tent words in a sentence. Formally, the sentence S
is a sequence of words (w
1
,w
2
,...,w
n
), and we will
identify a mapping M from words to senses such
that M(i) ? Senses
WN
(w
i
), where Senses
WN
(w
i
) is
the set of senses encoded in the WN for word w
i
.
For sentence S, there are
?
n
i=1
|Sense
WN
(w
i
)| pos-
sible mapping answers, which are impractical to
compute. Thus, we design two simple algorithms,
L2R (left to right) algorithm and S2C (simple to
complex) algorithm, for word sense disambigua-
tion based on the sense vectors.
The main difference between L2R and S2C is
1027
the order of words when performing word sense
disambiguation. When given a sentence, the L2R
algorithm disambiguates the words from left to
right (the natural order of a sentence), whereas the
S2C algorithm disambiguates the words with few-
er senses first. The main idea of S2C algorithm
is that the words with fewer senses are easier to
disambiguate, and the disambiguation result can
be helpful to disambiguate the words with more
senses. Both of the algorithms have three steps:
Context vector initialization. Similar to the ini-
tialization of sense vectors, we use the average of
all of the content words? vectors in a sentence as
the initialization vector of context.
vec(context) =
1
|cand(S)|
?
u?cand(S)
vec(u) (5)
where cand(S) is the set of content words
cand(S) = {u|u ? S,POS(u) ?CW}.
Ranking words. For L2R, we do nothing in this
step. For S2C, we rank the words based on the
ascending order of |Senses
WN
(w
i
)|.
Word sense disambiguation. For both L2R and
S2C, we denote the order of words as L and per-
form word sense disambiguation according to L.
First, we skip a word if the word is not
a content word or the word is monosemous
(|Senses
WN
(w
i
)| = 1). Then, for each word in
L, we can compute the cosine similarities be-
tween the context vector and its sense vectors. We
choose the sense that yields the maximum cosine
similarity as its disambiguation result. If the s-
core margin between the maximum and the sec-
ond maximum is larger than the threshold ? , we
are confident with the disambiguation result of w
i
and then use the sense vector to replace the word
vector in the context vector. Thus, we obtain a
more accurate context vector for other words that
are still yet to be disambiguated.
For example, given a sentence ?He sat on the
bank of the lake?, we first explain how S2C work-
s. In the sentence, there are three content word-
s, ?sat?, ?bank? and ?lake?, to be disambiguated.
First, the sum of the three word vectors is used as
the initialization of the context vector. Then we
rank the words by |Senses
W
N(w
i
)|, in ascending
order, that is, lake (3 senses), bank (10 senses), sat
(10 senses). We first disambiguate the word ?lake?
based on the similarities between its sense vectors
and context vector. If the score margin is larger
bankinput
projection
output sat on the of the lakesit lake1 1
Figure 1: The architecture of our model. The
training objective of Skip-gram is to train word
vector representations that are not only good at
predicting its context words but are also good at
predicting its context words? senses. The center
word ?bank? is used to predict not only its context
words but also the sense ?sit
1
? and ?lake
1
?.
than the threshold ? , then we are confident with
this disambiguation result and replace the word
vector with the sense vector to update the contex-
t vector. It would be helpful to disambiguate the
next word, ?bank?. We repeat this process until all
three words are disambiguated.
For L2R, the order of words to be disambiguat-
ed will be ?sat?, ?bank? and ?lake?. In this time,
when disambiguating ?bank? (10 senses), we still
don?t know the sense of ?lake? (3 senses).
2.4 Learning Sense Vectors from Relevant
Occurrences.
Based on the disambiguation result, we modify the
training objective of Skip-gram and train the sense
vectors directly from the large-scale corpus. Our
training objective is to train the vector representa-
tions that are not only good at predicting its con-
text words but are also good at predicting its con-
text words? senses. The architecture of our model
is shown in Figure 1.
More formally, given the disambiguation result
M(w
1
), M(w
2
), M(w
3
),...,M(w
T
), the training ob-
jective is modified to
1
T
T
?
t=1
(
k
?
j=?k
log{p(w
t+ j
|w
t
)p(M(w
t+ j
)|w
t
)}
)
(6)
where k is the size of the training window. The
inner summation spans from ?k to k to compute
the log probability of correctly predicting the word
w
t+ j
and the log probability of correctly predicting
1028
the sense M(w
t+ j
) given the word in the middle
w
t
. The outer summation covers all words in the
training data.
Because not all of the disambiguation results are
correct, we only disambiguate the words that we
are confident in. Similar to step 3 of our WSD
algorithm, we only disambiguate words under the
condition that the score margin between the max-
imum and the second maximum is larger than the
score margin threshold, ? .
We also use the softmax function to define
p(w
t+ j
|w
t
) and p(M(w
t+ j
)|w
t
). Then, we use hi-
erarchical softmax (Morin and Bengio, 2005) to
greatly reduce the computational complexity and
learn the sense vectors directly from the relevant
occurrences.
3 Experiments
In this section, we first present the nearest neigh-
bors of some words and their senses, showing that
our sense vectors can capture the semantics of
words. Then, we use three tasks to evaluate our u-
nified model: a contextual word similarity task to
evaluate our sense representations, and two stan-
dard WSD tasks to evaluate our knowledge-based
WSD algorithm based on the sense vectors. Ex-
perimental results show that our model not only
improves the correlation with human judgments
on the contextual word similarity task but also out-
performs state-of-the-art supervised WSD system-
s on domain-specific datasets and competes with
them in a coarse-grained all-words setting.
We choose Wikipedia as the corpus to train
the word vectors because of its wide coverage
of topics and words usages. We use an English
Wikipedia database dump from October 2013
2
,
which includes roughly 3 million articles and 1
billion tokens. We use Wikipedia Extractor
3
to
preprocess the Wikipedia pages and only save the
content of the articles.
We use word2vec
4
to train Skip-gram. We use
the default parameters of word2vec and the dimen-
sion of the vector representations is 200.
We use WordNet
5
as our sense inventory. The
datasets for different tasks are tagged with differ-
ent versions of WordNet. The version of WordNet
2
http://download.wikipedia.org.
3
The tool is available from http://medialab.di.
unipi.it/wiki/Wikipedia_Extractor.
4
The code is available from https://code.
google.com/p/word2vec/.
5
http://wordnet.princeton.edu/.
Word or sense Nearest neighbors
bank banks, IDBI, CitiBank
bank
s
1
river, slope, Sooes
bank
s
2
mortgage, lending, loans
star stars, stellar, trek
star
s
1
photosphere, radiation,
gamma-rays
star
s
2
someone, skilled, genuinely
plant plants, glavaticevo, herbaceous
plant
s
1
factories, machinery,
manufacturing
plant
s
2
locomotion, organism,
organisms
Table 2: Nearest neighbors of word vectors and
sense vectors learned by our model based on co-
sine similarity. The subscript of each sense label
corresponds to the index of the sense in Word-
Net. For example, bank
s
2
is the second sense of
the word bank in WordNet.
is 1.7 for the domain-specific WSD task and 2.1
for the coarse-grained WSD task.
We use the S2C algorithm described in Section
2.3 to perform word sense disambiguation to ob-
tain more relevant occurrences for each sense. We
compare S2C and L2R on the coarse-grained WS-
D task in a all-words setting.
The experimental results of our model are ob-
tained by setting the similarity threshold as ? = 0
and the score margin threshold as ? = 0.1. The in-
fluence of parameters on our model can be found
in Section 3.5.
3.1 Examples for Sense Vectors
Table 2 shows the nearest neighbors of word vec-
tors and sense vectors based on cosine similari-
ty. We see that our sense representations can i-
dentify different meanings of a word, allowing our
model to capture more semantic and syntactic re-
lationships between words and senses. Note that
each sense vector in our model corresponds to a
sense in WordNet; thus, our sense vectors can be
used to perform knowledge-based word sense dis-
ambiguation, whereas the vectors of cluster-based
models cannot.
3.2 Contextual Word Similarity
Experimental setting. A standard dataset for e-
valuating a vector-space model is the WordSim-
353 dataset (Finkelstein et al., 2001), which con-
1029
Model ??100
C&W-S 57.0
Huang-S 58.6
Huang-M AvgSim 62.8
Huang-M AvgSimC 65.7
Our Model-S 64.2
Our Model-M AvgSim 66.2
Our Model-M AvgSimC 68.9
Table 3: Spearman?s ? on the SCWS dataset. Our
Model-S uses one representation per word to com-
pute similarities, while Our Model-M uses one
representation per sense to compute similarities.
AvgSim calculates the similarity with each sense
contributing equally, while AvgSimC weighs the
sense according to the probability of the word
choosing that sense in context c.
sists of 353 pairs of nouns. However, each pair of
nouns in WordSim-353 is presented without con-
text. This is problematic because the meanings
of homonymous and polysemous words depend
highly on the words? contexts. Thus we choose the
Stanford?s Contextual Word Similarities (SCWS)
dataset from (Huang et al., 2012)
6
. The SCWS
dataset contains 2003 pairs of words and each pair
is associated with 10 human judgments on similar-
ity on a scale from 0 to 10. In the SCWS dataset,
each word in a pair has a sentential context.
In our experiments, the similarity between a
pair of words (w, w
?
) is computed as follows:
AvgSimC(w,w
?
) =
1
MN
M
?
i=1
N
?
j=1
p(i|w,c)p( j|w
?
,c
?
)d(vec(w
s
i
),vec(w
?
s
j
)) (7)
where p(i|w,c) is the likelihood that word w
chooses its ith sense given context c. d(vec,vec
?
)
is a function computing the similarity between two
vectors, and here we use cosine similarity.
Results and discussion. For evaluation, we
compute the Spearman correlation between a
model?s computed similarity scores and human
judgements. Table 3 shows our results com-
pared to previous methods, including (Collobert
and Weston, 2008)?s language model (C&W), and
Huang?s model which utilize the global context
and multi-prototype to improve the word represen-
tations.
6
The dataset can be downloaded at http://ai.
stanford.edu/
?
ehhuang/.
From Table 3, we observe that:
? Our single-vector version outperforms
Huang?s single-vector version. This indi-
cates that, by training the word vectors and
sense vectors jointly, our model can better
capture the semantic relationships between
words and senses.
? With one representation per sense, our mod-
el can outperform the single-vector version
without using context (66.2 vs. 64.2).
? Our model obtains the best performance
(68.9) by using AvgSimC, which takes con-
text into account.
3.3 Domain-Specific WSD
Experimental setting. We use Wikipedia as
training data because of its wide coverage for spe-
cific domains. To test our performance on do-
main word sense disambiguation, we evaluated
our system on the dataset published in (Koeling
et al., 2005). This dataset consists of examples
retrieved from the Sports and Finance sections of
the Reuters corpus. 41 words related to the Sports
and Finance domains were selected, with an aver-
age polysemy of 6.7 senses, ranging from 2 to 13
senses.
Approximately 100 examples for each word
were annotated with senses from WordNet v.1.7
by three reviewers, yielding an inter-tagger agree-
ment of 65%. (Koeling et al., 2005) did not clarify
how to select the ?correct? sense for each word, so
we followed the work of (Agirre et al., 2009) and,
used the sense chosen by the majority of taggers
as the correct answer.
Baseline methods. As a baseline, we use the
most frequent WordNet sense (MFS), as well as
a random sense assignment. We also compare our
results with four systems
7
: Static PageRank (A-
girre et al., 2009), the k nearest neighbor algorith-
m (k-NN), Degree (Navigli and Lapata, 2010) and
Personalized PageRank (Agirre et al., 2009).
Static PageRank applies traditional PageRank
over the semantic graph based on WordNet and
obtains a context-independent ranking of word
senses.
k-NN is a widely used classification method,
where neighbors are the k labeled examples most
7
We compare only with those systems performing token-
based WSD, i.e., disambiguating each instance of a target
word separately.
1030
Algorithm
Sports Finance
Recall Recall
Random BL 19.5 19.6
MFS BL 19.6 37.1
k-NN 30.3 43.4
Static PR 20.1 39.6
Personalized PR 35.6 46.9
Degree 42.0 47.8
Our Model 57.3 60.6
Table 4: Performance on the Sports and Finance
sections of the dataset from (Koeling et al., 2005).
similar to the test example. The k-NN system is
trained on SemCor (Miller et al., 1993), the largest
publicly available annotated corpus.
Degree and Personalized PageRank are state-
of-the-art systems that exploit WordNet to build
a semantic graph and exploit the structural proper-
ties of the graph in order to choose the appropriate
senses of words in context.
Results and discussion. Similar to other work
on this dataset, we use recall (the ratio of correct
sense labels to the total labels in the gold standard)
as our evaluation measure. Table 4 shows the re-
sults of different WSD systems on the dataset, and
the best results are shown in bold. The differences
between other results and the best result in each
column of the table are statistically significant at
p < 0.05.
The results show that:
? Our model outperforms k-NN on the t-
wo domains by a large margin, support-
ing the findings from (Agirre et al., 2009)
that knowledge-based systems perform bet-
ter than supervised systems when evaluated
across different domains.
? Our model also achieves better results than
the state-of-the-art system (+15.3% recall on
Sports and +12.8% recall on Finance against
Degree). The reason for this is that when
dealing with short sentences or context words
that are not in WordNet, our model can still
compute similarity based on the context vec-
tor and sense vectors, whereas Degree will
have difficulty building the semantic graph.
? Moreover, our model achieves the best per-
formance by only using the unlabeled text da-
ta and the definitions of senses, whereas other
Algorithm Type
Nouns only All words
F
1
F
1
Random BL U 63.5 62.7
MFS BL Semi 77.4 78.9
SUSSX-FR Semi 81.1 77.0
NUS-PT S 82.3 82.5
SSI Semi 84.1 83.2
Degree Semi 85.5 81.7
Our Model
L2R
U 79.2 73.9
Our Model
S2C
U 81.6 75.8
Our Model
L2R
Semi 82.5 79.6
Our Model
S2C
Semi 85.3 82.6
Table 5: Performance on Semeval-2007 coarse-
grained all-words WSD. In the type column,
U, Semi and S stand for unsupervised, semi-
supervised and supervised, respectively. The dif-
ferences between the results in bold in each col-
umn of the table are not statistically significant at
p < 0.05.
methods rely greatly on high-quality seman-
tic relations or annotated data, which are hard
to acquire.
3.4 Coarse-grained WSD
Experimental setting. We also evaluate our
WSD model on the Semeval-2007 coarse-grained
all-words WSD task (Navigli et al., 2007). There
are multiple reasons that we perform experiments
in a coarse-grained setting: first, it has been ar-
gued that the fine granularity of WordNet is one
of the main obstacles to accurate WSD (cf. the
discussion in (Navigli, 2009)); second, the train-
ing corpus of word representations is Wikipedia,
which is quite different from WordNet.
Baseline methods. We compare our model with
the best unsupervised system SUSSX-FR (Koel-
ing and McCarthy, 2007), and the best supervised
system, NUS-PT (Chan et al., 2007), participat-
ing in the Semeval-2007 coarse-grained all-words
task. We also compare with SSI (Navigli and Ve-
lardi, 2005) and the state-of-the-art system De-
gree (Navigli and Lapata, 2010). We use different
baseline methods for the two WSD tasks because
we want to compare our model with the state-
of-the-art systems that are applicable to different
datasets and show that our WSD method can per-
form robustly in these different WSD tasks.
1031
Results and discussion. We report our results in
terms of F
1
-measure on the Semeval-2007 coarse-
grained all-words dataset (Navigli et al., 2007).
Table 5 reports the results for nouns (1,108 words)
and all words (2,269 words). The difference be-
tween unsupervised and semi-supervised methods
is whether the method uses MFS as a back-off s-
trategy.
We can see that the S2C algorithm outperforms
the L2R algorithm no matter on the nouns subset
or on the entire set. This indicates that words with
fewer senses are easier to disambiguate, and it can
be helpful to disambiguate the words with more
senses.
On the nouns subset, our model yields compa-
rable performance to SSI and Degree, and it out-
performs NUS-PT and SUSSX-FR. Moreover, our
unsupervised WSD method (S2C) beats the MF-
S baseline, which is notably a difficult competitor
for knowledge-based systems.
On the entire set, our semi-supervised model is
significantly better than SUSSX-FR, and it is com-
parable with SSI and Degree. In contrast to SSI,
our model is simple and does not rely on a cost-
ly annotation effort to engineer the set of semantic
relations.
Overall, our model achieves state-of-the-art per-
formance on the Semeval-2007 coarse-grained all-
words dataset compared to other systems, with a
simple WSD algorithm that only relies on a large-
scale unlabeled text corpora and a sense inventory.
3.5 Parameter Influence
We investigate the influence of parameters on our
model with coarse-grained all-words WSD task.
The parameters include the similarity threshold, ? ,
and the score margin threshold, ? .
Similarity threshold. In Table 6, we show the
performance of domain WSD when the similari-
ty threshold ? ranges from ?0.1 to 0.3. The co-
sine similarity interval is [-1, 1], and we focus on
the performance in the interval [-0.1, 0.3] for two
reasons: first, no words are removed from glosses
when ? < ?0.1; second, nearly half of the word-
s are removed when ? > 0.3 and the performance
drops significantly for the WSD task. From table
6, we can see that our model achieves the best per-
formance when ? = 0.0.
Score margin threshold. In Table 7, we show
the performance on the coarse-grained all-words
Parameter Nouns only All words
? =?0.10 79.8 74.3
? =?0.05 81.0 74.6
? = 0.00 81.6 75.8
? = 0.05 81.3 75.4
? = 0.10 80.8 75.2
? = 0.15 80.0 75.0
? = 0.20 77.1 73.3
? = 0.30 75.0 72.1
Table 6: Evaluation results on the coarse-grained
all-words WSD when the similarity threshold ?
ranges from ?0.1 to 0.3.
Parameter Nouns only All words
? = 0.00 78.2 72.9
? = 0.05 79.5 74.5
? = 0.10 81.6 75.8
? = 0.15 81.2 74.7
? = 0.20 80.9 75.1
? = 0.25 80.2 74.8
? = 0.30 80.4 74.9
Table 7: Evaluation results on the coarse-grained
all-words WSD when the score margin threshold
? ranges from 0.0 to 0.3.
WSD when the score margin threshold ? ranges
from 0.0 to 0.3. When ? = 0.0, we use every
disambiguation result to update the context vec-
tor. When ? 6= 0, we only use the confident disam-
biguation results to update the context vector if the
score margin is larger than ? . Our model achieves
the best performance when ? = 0.1.
4 Related Work
4.1 Word Representations
Distributed representations for words were pro-
posed in (Rumelhart et al., 1986) and have been
successfully used in language models (Bengio et
al., 2006; Mnih and Hinton, 2008) and many nat-
ural language processing tasks, such as word rep-
resentation learning (Mikolov, 2012), named enti-
ty recognition (Turian et al., 2010), disambigua-
tion (Collobert et al., 2011), parsing and tag-
ging (Socher et al., 2011; Socher et al., 2013).
They are very useful in NLP tasks because they
can be used as inputs to learning algorithms or as
extra word features in NLP systems. Hence, many
NLP applications, such as keyword extraction (Li-
1032
u et al., 2010; Liu et al., 2011b; Liu et al., 2012),
social tag suggestion (Liu et al., 2011a) and text
classification (Baker and McCallum, 1998), may
also potentially benefit from distributed word rep-
resentation. The main advantage is that the rep-
resentations of similar words are close in vector
space, which makes generalization to novel pat-
terns easier and model estimation more robust.
Word representations are hard to train due to the
computational complexity. Recently, (Mikolov et
al., 2013) proposed two particular models, Skip-
gram and CBOW, to learn word representations in
large amounts of text data. The training objective
of the CBOW model is to combine the representa-
tions of the surrounding words to predict the word
in the middle, while the Skip-gram model?s is to
learn word representations that are good at predict-
ing its context in the same sentence (Mikolov et
al., 2013). Our paper uses the model architecture
of Skip-gram.
Most of the previous vector-space models use
one representation per word. This is problematic
because many words have multiple senses. The
multi-prototype approach has been widely stud-
ied. (Reisinger and Mooney, 2010) proposed the
multi-prototype vector-space model. (Huang et
al., 2012) used the multi-prototype models to learn
the vector for different senses of a word. All of
these models use the clustering of contexts as a
word sense and can not be directly used in word
sense disambiguation.
After our paper was submitted, we perceive the
following recent advances: (Tian et al., 2014) pro-
posed a probabilistic model for multi-prototype
word representation. (Guo et al., 2014) explored
bilingual resources to learn sense-specific word
representation. (Neelakantan et al., 2014) pro-
posed an efficient non-parametric model for multi-
prototype word representation.
4.2 Knowledge-based WSD
The objective of word sense disambiguation (WS-
D) is to computationally identify the meaning of
words in context (Navigli, 2009). There are t-
wo approaches of WSD that assign meaning of
words from a fixed sense inventory, supervised and
knowledge-based methods. Supervised approach-
es require large labeled training sets, which are
time consuming to create. In this paper, we on-
ly focus on knowledge-based word sense disam-
biguation.
Knowledge-based approaches exploit knowl-
edge resources (such as dictionaries, thesauri, on-
tologies, collocations, etc.) to determine the
senses of words in context. However, it has
been shown in (Cuadros and Rigau, 2006) that
the amount of lexical and semantic information
contained in such resources is typically insuf-
ficient for high-performance WSD. Much work
has been presented to automatically extend ex-
isting resources, including automatically linking
Wikipedia to WordNet to include full use of the
first WordNet sense heuristic (Suchanek et al.,
2008), a graph-based mapping of Wikipedia cat-
egories to WordNet synsets (Ponzetto and Nav-
igli, 2009), and automatically mapping Wikipedia
pages to WordNet synsets (Ponzetto and Navigli,
2010).
It was recently shown that word representation-
s can capture semantic and syntactic information
between words (Mikolov et al., 2013). Some re-
searchers tried to incorporate WordNet senses in a
neural model to learn better word representation-
s (Bordes et al., 2011). In this paper, we have pro-
posed a unified method for word sense representa-
tion and disambiguation to extend the information
contained in the vector representations to the ex-
isting resources. Our method only requires a large
amount of unlabeled text to train sense representa-
tions and a dictionary to provide the definitions of
word meanings, which makes it easily applicable
to other resources.
5 Conclusion
In this paper, we present a unified model for word
sense representation and disambiguation that us-
es one representation per sense. Experimental re-
sults show that our model improves the perfor-
mance of contextual word similarity compared to
existing WSR methods, outperforms state-of-the-
art supervised methods on domain-specific WSD,
and achieves competitive performance on coarse-
grained all-words WSD. Our model only requires
large-scale unlabeled text corpora and a sense in-
ventory for WSD, thus it can be easily applied to
other corpora and tasks.
There are still several open problems that
should be investigated further:
1. Because the senses of words change over
time (new senses appear), we will incorpo-
rate cluster-based methods in our model to
find senses that are not in the sense inventory.
1033
2. We can explore other WSD methods based
on sense vectors to improve our performance.
For example, (Li et al., 2010) used LDA to
perform data-driven WSD in a manner simi-
lar to our model. We may integrate the advan-
tages of these models and our model together
to build a more powerful WSD system.
3. To learn better sense vectors, we can exploit
the semantic relations (such as the hypernym
and hyponym relations defined in WordNet)
between senses in our model.
Acknowledgments
This work is supported by National Key Ba-
sic Research Program of China (973 Program
2014CB340500) and National Natural Science
Foundation of China (NSFC 61133012).
References
Eneko Agirre, Oier Lopez De Lacalle, Aitor Soroa, and
Informatika Fakultatea. 2009. Knowledge-based
wsd and specific domains: Performing better than
generic supervised wsd. In Proceedings of IJCAI,
pages 1501?1506.
L Douglas Baker and Andrew Kachites McCallum.
1998. Distributional clustering of words for text
classification. In Proceedings of SIGIR, pages 96?
103.
Yoshua Bengio, Holger Schwenk, Jean-S?ebastien
Sen?ecal, Fr?ederic Morin, and Jean-Luc Gauvain.
2006. Neural probabilistic language models. In In-
novations in Machine Learning, pages 137?186.
Antoine Bordes, Jason Weston, Ronan Collobert,
Yoshua Bengio, et al. 2011. Learning structured
embeddings of knowledge bases. In Proceedings of
AAAI, pages 301?306.
Yee Seng Chan, Hwee Tou Ng, and Zhi Zhong. 2007.
Nus-pt: exploiting parallel texts for word sense dis-
ambiguation in the english all-words tasks. In Pro-
ceedings of SemEval, pages 253?256.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of ICML, pages 160?167.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. JMLR, 12:2493?2537.
Montse Cuadros and German Rigau. 2006. Quality
assessment of large scale knowledge resources. In
Proceedings of EMNLP, pages 534?541.
Katrin Erk and Sebastian Pado. 2010. Exemplar-based
models for word meaning in context. In Proceedings
of ACL, pages 92?97.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, E-
hud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2001. Placing search in context: The con-
cept revisited. In Proceedings of WWW, pages 406?
414.
Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting Li-
u. 2014. Learning sense-specific word embeddings
by exploiting bilingual resources. In Proceedings of
COLING, pages 497?507.
Eric H Huang, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of ACL, pages 873?882.
Rob Koeling and Diana McCarthy. 2007. Sussx: Ws-
d using automatically acquired predominant senses.
In Proceedings of SemEval, pages 314?317.
Rob Koeling, Diana McCarthy, and John Carroll.
2005. Domain-specific sense distributions and pre-
dominant sense acquisition. In Proceedings of HLT-
EMNLP, pages 419?426.
Linlin Li, Benjamin Roth, and Caroline Sporleder.
2010. Topic models for word sense disambiguation
and token-based idiom detection. In Proceedings of
ACL, pages 1138?1147.
Zhiyuan Liu, Wenyi Huang, Yabin Zheng, and
Maosong Sun. 2010. Automatic keyphrase extrac-
tion via topic decomposition. In Proceedings of
EMNLP, pages 366?376.
Zhiyuan Liu, Xinxiong Chen, and Maosong Sun.
2011a. A simple word trigger method for social tag
suggestion. In Proceedings of EMNLP, pages 1577?
1588.
Zhiyuan Liu, Xinxiong Chen, Yabin Zheng, and
Maosong Sun. 2011b. Automatic keyphrase extrac-
tion by bridging vocabulary gap. In Proceedings of
CoNLL, pages 135?144.
Zhiyuan Liu, Xinxiong Chen, and Maosong Sun. 2012.
Mining the interests of chinese microbloggers via
keyword extraction. Frontiers of Computer Science,
6(1):76?87.
Christopher D Manning, Prabhakar Raghavan, and
Hinrich Sch?utze. 2008. Introduction to information
retrieval. Cambridge University Press Cambridge.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. Proceedings of ICLR.
Tomas Mikolov. 2012. Statistical Language Model-
s Based on Neural Networks. Ph.D. thesis, Ph. D.
thesis, Brno University of Technology.
1034
George A Miller, Claudia Leacock, Randee Tengi, and
Ross T Bunker. 1993. A semantic concordance. In
Proceedings of the workshop on HLT, pages 303?
308.
George A Miller. 1995. Wordnet: a lexical
database for english. Communications of the ACM,
38(11):39?41.
Andriy Mnih and Geoffrey E Hinton. 2008. A s-
calable hierarchical distributed language model. In
Proceedings of NIPS, pages 1081?1088.
Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
Proceedings of the international workshop on artifi-
cial intelligence and statistics, pages 246?252.
Roberto Navigli and Mirella Lapata. 2010. An ex-
perimental study of graph connectivity for unsu-
pervised word sense disambiguation. IEEE PAMI,
32(4):678?692.
Roberto Navigli and Paola Velardi. 2005. Structural
semantic interconnections: a knowledge-based ap-
proach to word sense disambiguation. IEEE PAMI,
27(7):1075?1086.
Roberto Navigli, Kenneth C Litkowski, and Orin Har-
graves. 2007. Semeval-2007 task 07: Coarse-
grained english all-words task. In Proceedings of
SemEval, pages 30?35.
Roberto Navigli. 2009. Word sense disambiguation: A
survey. CSUR, 41(2):10.
Arvind Neelakantan, Jeevan Shankar, Alexandre Pas-
sos, and Andrew McCallum. 2014. Efficient non-
parametric estimation of multiple embeddings per
word in vector space. In Proceedings of EMNLP.
Simone Paolo Ponzetto and Roberto Navigli. 2009.
Large-scale taxonomy mapping for restructuring
and integrating wikipedia. In Proceedings of IJCAI,
volume 9, pages 2083?2088.
Simone Paolo Ponzetto and Roberto Navigli. 2010.
Knowledge-rich word sense disambiguation rivaling
supervised systems. In Proceedings of ACL, pages
1522?1531.
Joseph Reisinger and Raymond J Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In Proceedings of HLT-NAACL, pages 109?
117.
David E Rumelhart, Geoffrey E Hintont, and Ronald J
Williams. 1986. Learning representations by back-
propagating errors. Nature, 323(6088):533?536.
Fabrizio Sebastiani. 2002. Machine learning in auto-
mated text categorization. CSUR, 34(1):1?47.
Richard Socher, Cliff C Lin, Andrew Ng, and Chris
Manning. 2011. Parsing natural scenes and natural
language with recursive neural networks. In Pro-
ceedings of ICML, pages 129?136.
Richard Socher, John Bauer, Christopher D Manning,
and Andrew Y Ng. 2013. Parsing with composi-
tional vector grammars. In Proceedings of ACL.
Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2008. Yago: A large ontology from
wikipedia and wordnet. Web Semantics: Sci-
ence, Services and Agents on the World Wide Web,
6(3):203?217.
Fei Tian, Hanjun Dai, Jiang Bian, Bin Gao, Rui Zhang,
Enhong Chen, and Tie-Yan Liu. 2014. A probabilis-
tic model for learning multi-prototype word embed-
dings. In Proceedings of COLING, pages 151?160.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of A-
CL, pages 384?394.
1035
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 135?144,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Automatic Keyphrase Extraction by Bridging Vocabulary Gap ?
Zhiyuan Liu, Xinxiong Chen, Yabin Zheng, Maosong Sun
State Key Laboratory of Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Science and Technology
Tsinghua University, Beijing 100084, China
{lzy.thu, cxx.thu, yabin.zheng}@gmail.com, sms@tsinghua.edu.cn
Abstract
Keyphrase extraction aims to select a set of
terms from a document as a short summary
of the document. Most methods extract
keyphrases according to their statistical prop-
erties in the given document. Appropriate
keyphrases, however, are not always statis-
tically significant or even do not appear in
the given document. This makes a large
vocabulary gap between a document and its
keyphrases. In this paper, we consider that
a document and its keyphrases both describe
the same object but are written in two different
languages. By regarding keyphrase extraction
as a problem of translating from the language
of documents to the language of keyphrases,
we use word alignment models in statistical
machine translation to learn translation proba-
bilities between the words in documents and
the words in keyphrases. According to the
translation model, we suggest keyphrases giv-
en a new document. The suggested keyphrases
are not necessarily statistically frequent in the
document, which indicates that our method
is more flexible and reliable. Experiments
on news articles demonstrate that our method
outperforms existing unsupervised methods
on precision, recall and F-measure.
1 Introduction
Information on the Web is emerging with the
development of Internet. It is becoming more and
more important to effectively search and manage
information. Keyphrases, as a brief summary of a
document, provide a solution to help organize and
?Zhiyuan Liu and Xinxiong Chen have equal contribution
to this work.
retrieve documents, which have been widely used
in digital libraries and information retrieval (Turney,
2000; Nguyen and Kan, 2007). Due to the explosion
of information, it is ineffective for professional
human indexers to manually annotate documents
with keyphrases. How to automatically extract
keyphrases from documents becomes an important
research problem, which is usually referred to as
keyphrase extraction.
Most methods for keyphrase extraction try to
extract keyphrases according to their statistical prop-
erties. These methods are susceptible to low perfor-
mance because many appropriate keyphrases may
not be statistically frequent or even not appear in the
document, especially for short documents. We name
the phenomenon as the vocabulary gap between
documents and keyphrases. For example, a research
paper talking about ?machine transliteration? may
less or even not mention the phrase ?machine
translation?. However, since ?machine transliter-
ation? is a sub-field of ?machine translation?, the
phrase ?machine translation? is also reasonable to
be suggested as a keyphrase to indicate the topics
of this paper. Let us take another example: in a
news article talking about ?iPad? and ?iPhone?, the
word ?Apple? may rarely ever come up. However,
it is known that both ?iPad? and ?iPhone? are the
products of ?Apple?, and the word ?Apple? may thus
be a proper keyphrase of this article.
We can see that, the essential challenge of
keyphrase extraction is the vocabulary gap between
documents and keyphrases. Therefore, the task of
keyphrase extraction is how to capture the semantic
relations between the words in documents and in
keyphrases so as to bridge the vocabulary gap.
In this paper, we provide a new perspective to
135
documents and their keyphrases: each document
and its keyphrases are descriptions to the same
object, but the document is written using one lan-
guage, while keyphrases are written using another
language. Therefore, keyphrase extraction can be
regarded as a translation problem from the language
of documents into the language of keyphrases.
Based on the idea of translation, we use word
alignment models (WAM) (Brown et al, 1993) in
statistical machine translation (SMT) (Koehn, 2010)
and propose a unified framework for keyphrase
extraction: (1) From a collection of translation pairs
of two languages, WAM learns translation probabil-
ities between the words in the two languages. (2)
According to the translation model, we are able to
bridge the vocabulary gap and succeed in suggesting
appropriate keyphrases, which may not necessarily
frequent in their corresponding documents.
As a promising approach to solve the problem
of vocabulary gap, SMT has been widely ex-
ploited in many applications such as information
retrieval (Berger and Lafferty, 1999; Karimzade-
hgan and Zhai, 2010), image and video anno-
tation (Duygulu et al, 2002), question answer-
ing (Berger et al, 2000; Echihabi and Marcu, 2003;
Murdock and Croft, 2004; Soricut and Brill, 2006;
Xue et al, 2008), query expansion and rewrit-
ing (Riezler et al, 2007; Riezler et al, 2008; Riezler
and Liu, 2010), summarization (Banko et al, 2000),
collocation extraction (Liu et al, 2009b; Liu et al,
2010b) and paraphrasing (Quirk et al, 2004; Zhao
et al, 2010). Although SMT is a widely adopted
solution to vocabulary gap, for various applications
using SMT, the crucial and non-trivial problem is
to find appropriate and enough translation pairs for
SMT.
The most straightforward translation pairs for
keyphrase extraction is document-keyphrase pairs.
In practice, however, it is time-consuming to anno-
tate a large collection of documents with keyphrases
for sufficient WAM training. In order to solve
the problem, we use titles and summaries to build
translation pairs with documents. Titles and sum-
maries are usually accompanying with the corre-
sponding documents. In some special cases, titles
or summaries may be unavailable. We are also able
to extract one or more important sentences from
the corresponding documents to construct sufficient
translation pairs.
2 State of the Art
Some researchers (Frank et al, 1999; Witten et al,
1999; Turney, 2000) regarded keyphrase extraction
as a binary classification problem (is-keyphrase or
non-keyphrase) and learned models for classifica-
tion using training data. These supervised methods
need manually annotated training set, which is time-
consuming. In this paper, we focus on unsupervised
methods for keyphrase extraction.
The most simple unsupervised method for
keyphrase extraction is using TFIDF (Salton and
Buckley, 1988) to rank the candidate keyphrases and
select the top-ranked ones as keyphrases. TFIDF
ranks candidate keyphrases only according to their
statistical frequencies, which thus fails to suggest
keyphrases with low frequencies.
Starting with TextRank (Mihalcea and Tarau,
2004), graph-based ranking methods are becoming
the state-of-the-art methods for keyphrase extrac-
tion (Liu et al, 2009a; Liu et al, 2010a). Given
a document, TextRank first builds a word graph,
in which the links between words indicate their
semantic relatedness, which are estimated by the
word co-occurrences in the document. By executing
PageRank (Page et al, 1998) on the graph, we obtain
the PageRank score for each word to rank candidate
keyphrases.
In TextRank, a low-frequency word will benefit
from its high-frequency neighbor words and thus be
ranked higher as compared to using TFIDF. This
alleviates the problem of vocabulary gap to some
extent. TextRank, however, still tends to extract
high-frequency words as keyphrases because these
words have more opportunities to get linked with
other words and obtain higher PageRank scores.
Moreover, TextRank usually constructs a word
graph simply according to word co-occurrences as
an approximation of the semantic relations between
words. This will introduce much noise because of
connecting semantically unrelated words and highly
influence extraction performance.
Some methods have been proposed to improve
TextRank, of which ExpandRank (Wan and Xi-
ao, 2008b; Wan and Xiao, 2008a) uses a smal-
l number, namely k, of neighbor documents to
136
provide more information of word relatedness for
the construction of word graphs. Compared to
TextRank, ExpandRank performs better when facing
the vocabulary gap by borrowing the information on
document level. However, the finding of neighbor
documents are usually arbitrary. This process may
introduce much noise and result in topic drift when
the document and its so-called neighbor documents
are not exactly talking about the same topics.
Another potential approach to alleviate vocabu-
lary gap is latent topic models (Landauer et al,
1998; Hofmann, 1999; Blei et al, 2003), of which
latent Dirichlet alocation (LDA) (Blei et al, 2003)
is most popular. Latent topic models learn topics
from a collection of documents. Using a topic
model, we can represent both documents and words
as the distributions over latent topics. The semantic
relatedness between a word and a document can be
computed using the cosine similarities of their topic
distributions. The similarity scores can be used as
the ranking criterion for keyphrase extraction (Hein-
rich, 2005; Blei and Lafferty, 2009). On one hand,
latent topic models use topics instead of statistical
properties of words for ranking, which abates the
vocabulary gap problem on topic level. On the other
hand, the learned topics are usually very coarse, and
topic models tend to suggest general words for a
given document. Therefore, the method usually fails
to capture the specific topics of the document.
In contract to the above-mentioned methods, our
method addresses vocabulary gap on word level,
which prevents from topic drift and works out better
performance. In experiments, we will show our
method can better solve the problem of vocab-
ulary gap by comparing with TFIDF, TextRank,
ExpandRank and LDA.
3 Keyphrase Extraction by Bridging
Vocabulary Gap Using WAM
First, we give a formal definition of keyphrase
extraction: given a collection of documents D, for
each document d ? D, keyphrase extraction aims
to rank candidate keyphrases according to their
likelihood given the document d, i.e., Pr(p|d) for all
p ? P, where P is the candidate keyphrase set. Then
we select top-Md ones as keyphrases, where Md can
be fixed or automatically determined by the system.
The document d can be regarded as a sequence of
words wd = {wi}Nd1 , where Nd is the length of d.
In Fig. 1, we demonstrate the framework of
keyphrase extraction using WAM. We divide the
algorithm into three steps: preparing translation
pairs, training translation models and extracting
keyphrases for a given document. We will introduce
the three steps in details from Section 3.1 to
Section 3.3.
Input: A large collection of documents D for keyphrase
extraction.
Step 1: Prepare Translation Pairs. For each d ? D, we
may prepare two types of translation pairs:
? Title-based Pairs. Use the title td of each document
d and prepare translation pairs, denote as ?D,T ?.
? Summary-based Pairs. Use the summary sd of
each document d and prepare translation pairs,
denote as ?D,S?.
Step 2: Train Translation Model. Given translation
pairs, e.g., ?D,T ?, train word-word translation model
Pr?D,T ?(t|w) using WAM, where w is the word in docu-
ment language and t is the word in title language.
Step 3: Keyphrase Extraction. For a document d,
extract keyphrases according to a trained translation
model, e.g., Pr?D,T ?(t|w).
1. Measure the importance score Pr(w|d) of each word
w in document d.
2. Compute the ranking score of candidate keyphrase
p by
Pr(p|d) =
?t?p?w?d Pr?D,T ?(t|w)Pr(w|d) (1)
3. Select top-Md ranked candidate keyphrases accord-
ing to Pr(p|d) as the keyphrases of document d.
Figure 1: WAM for keyphrase extraction.
3.1 Preparing Translation Pairs
Training dataset for WAM consists of a number
of translation pairs written in two languages. In
keyphrase extraction task, we have to construct
sufficient translation pairs to capture the semantic
relations between documents and keyphrases. Here
we propose to construct two types of translation
pairs: title-based pairs and summary-based pairs.
137
3.1.1 Title-based Pairs
Title is usually a short summary of the given doc-
ument. In most cases, documents such as research
papers and news articles have corresponding titles.
Therefore, we can use title to construct translation
pairs for a document.
WAM assumes each translation pair should be of
comparable length. However, a document is usually
much longer than title. It will hurt the performance
if we fill the length-unbalanced pairs for WAM
training. We propose two methods to address the
problem: sampling method and split method.
In sampling method, we perform word sampling
for each document to make it comparable to the
length of its title. Suppose the lengths of a document
and its title are Nd and Nt , respectively. For
document d, we first build a bag of words bd =
{(wi,ei)}Wdi=1, where Wd is the number of unique
words in d, and ei is the weights of word wi in d.
In this paper, we use TFIDF scores as the weights
of words. Using bd , we sample words for Nt
times with replacement according to the weights of
words, and finally form a new bag with Nt words
to represent document d. In the sampling result,
we keep the most important words in document d.
We can thus construct a document-title pair with
balanced length.
In split method, we split each document into
sentences which are of comparable length to its
title. For each sentence, we compute its semantic
similarity with the title. There are various methods
to measure semantic similarities. In this paper, we
use vector space model to represent sentences and
titles, and use cosine scores to compute similarities.
If the similarity is smaller than a threshold ? , we
will discard the sentence; otherwise, we will regard
the sentence and title as a translation pair.
Sampling method and split method have their
own characteristics. Compared to split method,
sampling method loses the order information of
words in documents. While split method generates
much more translation pairs, which leads to longer
training time of WAM. In experiment section, we
will investigate the performance of the two methods.
3.1.2 Summary-based Pairs
For most research articles, authors usually pro-
vide abstracts to summarize the articles. Many news
articles also have short summaries. Suppose each
document itself has a short summary, we can use
the summary and document to construct translation
pairs using either sampling method or split method.
Because each summary usually consists of multiple
sentences, split method for constructing summary-
based pairs has to split both the document and
summary into sentences, and the sentence pairs with
similarity scores above the threshold are filled in
training dataset for WAM.
3.2 Training Translation Models
Without loss of generality, we take title-based pairs
as the example to introduce the training process
of translation models, and suppose documents are
written in one language and titles are written in
another language. In this paper, we use IBM Model-
1 (Brown et al, 1993) for WAM training. IBM
Model-1 is a widely used word alignment algorithm
which does not require linguistic knowledge for two
languages 1.
In IBM Model-1, for each translation pair
?wd ,wt?, the relationship of the document language
wd = {wi}Ldi=0 and the title language wt = {ti}
Lt
i=0
is connected via a hidden variable a = {ai}Ldi=1
describing an alignment mapping from words of
documents to words of titles,
Pr(wd |wt) = ?aPr(wd ,a|wt) (2)
For example, a j = i indicates word w j in wd at
position j is aligned to word ti in wt at position i.
The alignment a also contains empty-word align-
ments a j = 0 which align words of documents to
an empty word. IBM Model-1 can be trained using
Expectation-Maximization (EM) algorithm (Demp-
ster et al, 1977) in an unsupervised fashion. Using
IBM Model-1, we can obtain the translation prob-
abilities of two language-vocabularies, i.e., Pr(t|w)
and Pr(w|t), where w is a word in document
vocabulary and t is a word in title vocabulary.
IBM Model-1 will produce one-to-many align-
ments from one language to another language, and
the trained model is thus asymmetric. Hence, we can
1We have also employed more sophisticated WAM al-
gorithms such as IBM Model-3 for keyphrase extraction.
However, these methods did not achieve better performance
than the simple IBM Model-1. Therefore, in this paper we only
demonstrate the experimental results using IBM Model-1.
138
train two different translation models by assigning
translation pairs in two directions, i.e., (document?
title) and (title ? document). We denote the former
model as Prd2t and the latter as Prt2d. We define
Pr?D,T ?(t|w) in Eq.(1) as the harmonic mean of the
two models:
Pr?D,T ?(t|w) ?
(
?
Prd2t(t|w) +
(1?? )
Prt2d(t|w)
)?1
(3)
where ? is the harmonic factor to combine the two
models. When ? = 1.0 or ? = 0.0, it simply uses
model Prd2t or Prt2d, correspondingly. Using the
translation probabilities Pr(t|w) we can bridge the
vocabulary gap between documents and keyphrases.
3.3 Keyphrase Extraction
Given a document d, we rank candidate keyphrases
by computing their likelihood Pr(p|d). Each can-
didate keyphrase p may be composed of multiple
words. As shown in (Hulth, 2003), most keyphrases
are noun phrases. Following (Mihalcea and Tarau,
2004; Wan and Xiao, 2008b), we simply select
noun phrases from the given document as candidate
keyphrases with the help of POS tags. For each
word t, we compute its likelihood given d, Pr(t|d) =
?w?d Pr(t|w)Pr(w|d), where Pr(w|d) is the weight
of the word w in d, which is measured using
normalized TFIDF scores. Pr(t|w) is the translation
probabilities obtained from WAM training.
Using the scores of all words in candidate
keyphrases, we compute the ranking score of each
candidate keyphrase by summing up the scores
of each word in the candidate keyphrase, i.e.,
Pr(p|d) =
?t?pPr(t|d). In all, the ranking scores
of candidate keyphrases is formalized in Eq. (1)
of Fig. 1. According to the ranking scores, we can
suggest top-Md ranked candidates as the keyphrases,
where Md is the number of suggested keyphrases to
the document d pre-specified by users or systems.
We can also consider the number of words in the
candidate keyphrase as a normalization factor to Eq.
(1), which will be our future work.
4 Experiments
To perform experiments, we crawled a collection of
13,702 Chinese news articles 2 from www.163.
2The dataset can be obtained from http://nlp.csai.
tsinghua.edu.cn/?lzy/datasets/ke_wam.html.
com, one of the most popular news websites in Chi-
na. The news articles are composed of various topics
including science, technology, politics, sports, arts,
society and military. All news articles are manually
annotated with keyphrases by website editors, and
all these keyphrases come from the corresponding
documents. Each news article is also provided with
a title and a short summary.
In this dataset, there are 72,900 unique words in
documents, and 12,405 unique words in keyphrases.
The average lengths of documents, titles and sum-
maries are 971.7 words, 11.6 words, and 45.8 words,
respectively. The average number of keyphrases
for each document is 2.4. In experiments, we
use the annotated titles and summaries to construct
translation pairs.
In experiments, we select GIZA++ 3 (Och and
Ney, 2003) to train IBM Model-1 using translation
pairs. GIZA++, widely used in various applications
of statistical machine translation, implements IBM
Models 1-5 and an HMM word alignment model.
To evaluate methods, we use the annotated
keyphrases by www.163.com as the standard
keyphrases. If one suggested keyphrase exact-
ly matches one of the standard keyphrases, it
is a correct keyphrase. We use precision p =
ccorrect/cmethod , recall r = ccorrect/cstandard and F-
measure f = 2pr/(p + r) for evaluation, where
ccorrect is the number of keyphrases correctly sug-
gested by the given method, cmethod is the number
of suggested keyphrases, and cstandard is the number
of standard keyphrases. The following experiment
results are obtained by 5-fold cross validation.
4.1 Evaluation on Keyphrase Extraction
4.1.1 Performance Comparison and Analysis
We use four representative unsupervised methods
as baselines for comparison: TFIDF, TextRank (Mi-
halcea and Tarau, 2004), ExpandRank (Wan and
Xiao, 2008b) and LDA (Blei et al, 2003). We
denote our method as WAM for short.
In Fig. 2, we demonstrate the precision-recall
curves of various methods for keyphrase extraction
including TFIDF, TextRank, ExpandRank, LDA
and WAM with title-based pairs prepared using
3The website for GIZA++ package is http://code.
google.com/p/giza-pp/.
139
sampling method (Title-Sa) and split method (Title-
Sp), and WAM with summary-based pairs prepared
using sampling method (Summ-Sa) and split method
(Summ-Sp). For WAM, we set the harmonic factor
? = 1.0 and threshold ? = 0.1, which is the optimal
setting as shown in the later analysis on parameter
influence. For TextRank, LDA and ExpandRank, we
report their best results after parameter tuning, e.g.,
the number of topics for LDA is set to 400, and the
number of neighbor documents for ExpandRank is
set to 5 .
The points on a precision-recall curve represent
different numbers of suggested keyphrases from
Md = 1 (bottom right) to Md = 10 (upper left),
respectively. The closer the curve is to the upper
right, the better the overall performance of the
method is. In Table 1, we further demonstrate the
precision, recall and F-measure scores of various
methods when Md = 2 4. In Table 1, we also show
the statistical variances after ?. From Fig. 2 and
Table 1, we have the following observations:
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.05  0.1  0.15  0.2  0.25  0.3  0.35  0.4
Rec
all
Precision
TFIDFTextRankLDAExpandRankTitle-SaTitle-SpSumm-SaSumm-Sp
Figure 2: The precision-recall curves of various
methods for keyphrase extraction.
First, our method outperforms all baselines. It
indicates that the translation perspective is valid
for keyphrase extraction. When facing vocabu-
lary gap, TFIDF and TextRank have no solutions,
ExpandRank adopts the external information on
document level which may introduce noise, and
LDA adopts the external information on topic level
which may be too coarse. In contrast to these
baselines, WAM aims to bridge the vocabulary gap
on word level, which avoids topic drift effectively.
4We select Md = 2 because WAM gains the best F-measure
score when Md = 2, which is close to the average number of
annotated keyphrases for each document 2.4.
Method Precision Recall F-measure
TFIDF 0.187 0.256 0.208?0.005
TextRank 0.217 0.301 0.243?0.008
LDA 0.181 0.253 0.203?0.002
ExpandRank 0.228 0.316 0.255?0.007
Title-Sa 0.299 0.424 0.337?0.008
Title-Sp 0.300 0.425 0.339?0.010
Summ-Sa 0.258 0.361 0.289?0.009
Summ-Sp 0.273 0.384 0.307?0.008
Table 1: Precision, recall and F-measure of various
methods for keyphrase extraction when Md = 2.
Therefore, our method can better solve the problem
of vocabulary gap in keyphrase extraction.
Second, WAM with title-based pairs performs
better than summary-based pairs consistently, no
matter prepared using sampling method or split
method. This indicates the titles are closer to
the keyphrase language as compared to summaries.
This is also consistent with the intuition that titles
are more important than summaries. Meanwhile, we
can save training efforts using title-based pairs.
Last but not least, split method achieves better or
comparable performance as compared to sampling
method on both title-based pairs and summary-
based pairs. The reasons are: (1) the split method
generates more translation pairs for adequate train-
ing than sampling method; and (2) split method
also keeps the context of words, which helps to
obtain better word alignment, unlike bag-of-words
in sampling method.
4.1.2 Influence of Parameters
We also investigate the influence of parameters
to WAM with title-based pairs prepared using split
method, which achieves the best performance as
shown in Fig. 2. The parameters include: harmonic
factor ? (described in Eq. 3) and threshold factor
? . Harmonic factor ? controls the weights of the
translation models trained in two directions, i.e.,
Prd2t(t|w) and Prt2d(t|w) as shown in Eq. (3). As
described in Section 3.1.1, using threshold factor ?
we filter out the pairs with similarities lower than ? .
In Fig. 3, we show the precision-recall curves
of WAM for keyphrase extraction when harmonic
factor ? ranges from 0.0 to 1.0 stepped by 0.2. From
the figure, we observe that the translation model
Prd2t(t|w) (i.e., when ? = 1.0) performs better than
140
Prt2d(t|w) (i.e., when ? = 0.0). This indicates that
it is sufficient to simply train a translation model
in one direction (i.e., Prd2t(t|w)) for keyphrase
extraction.
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 0.05  0.1  0.15  0.2  0.25  0.3  0.35  0.4
Rec
all
Precision
? = 0.0? = 0.2? = 0.4? = 0.6? = 0.8? = 1.0
Figure 3: Precision-recall curves of WAM when
harmonic factor ? ranges from 0.0 to 1.0.
In Fig. 4, we show the precision-recall curves
of WAM for keyphrase extraction when threshold
factor ? ranges from 0.01 to 0.90. In title-
based pairs using split method, the total number
of pairs without filtering any pairs (i.e., ? = 0)
is 347,188. When ? = 0.01, 0.10 and 0.90, the
numbers of retained translation pairs are 165,023,
148,605 and 41,203, respectively. From Fig. 4,
we find that more translation pairs result in better
performance. However, more translation pairs also
indicate more training time of WAM. Fortunately,
we can see that the performance does not drop much
when discarding more translation pairs with low
similarities. Even when ? = 0.9, our method can
still achieve performance with precision p = 0.277,
recall r = 0.391 and F-measure f = 0.312 when
Md = 2. Meanwhile, we reduce the training efforts
by about 50% as compared to ? = 0.01.
In all, based on the above analysis on two
parameters, we demonstrate the effectiveness and
robustness of our method for keyphrase extraction.
4.1.3 When Titles/Summaries Are Unavailable
Suppose in some special cases, the titles or sum-
maries are unavailable, how can we construct trans-
lation pairs? Inspired by extraction-based document
summarization (Goldstein et al, 2000; Mihalcea and
Tarau, 2004), we can extract one or more important
sentences from the given document to construct
translation pairs. Unsupervised sentence extraction
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 0.05  0.1  0.15  0.2  0.25  0.3  0.35  0.4  0.45
Rec
all
Precision
? = 0.01? = 0.05? = 0.10? = 0.30? = 0.50? = 0.70? = 0.90
Figure 4: Precision-recall curves of WAM when
threshold ? ranges from 0.01 to 0.90.
for document summarization is a well-studied task
in natural language processing. As shown in Table 2,
we only perform two simple sentence extraction
methods to demonstrate the effectiveness: (1) Select
the first sentence of a document (denoted as ?First?);
and (2) Compute the cosine similarities between
each sentence and the whole document represented
as two bags-of-words (denoted as ?Importance?).
It is interesting to find that the method of using
the first sentence performs similar to using titles.
This profits from the characteristic of news articles
which tend to give a good summary for the whole
article using the first sentence. Although the second
method drops much on performance as compared to
using titles, it still outperforms than other existing
methods. Moreover, the second method will im-
prove much if we use more effective measures to
identify the most important sentence.
Method Precision Recall F-measure
First 0.290 0.410 0.327?0.013
Importance 0.260 0.367 0.293?0.010
Table 2: Precision, recall and F-measure of
keyphrase extraction when Md = 2 by extracting one
sentence to construct translation pairs.
4.2 Beyond Extraction: Keyphrase Generation
In Section 4.1, we evaluate our method on keyphrase
extraction by suggesting keyphrases from docu-
ments. In fact, our method is also able to suggest
keyphrases that have not appeared in the content of
given document. The ability is important especially
when the length of each document is short, which
141
itself may not contain appropriate keyphrases. We
name the new task keyphrase generation. To
evaluate these methods on keyphrase generation,
we perform keyphrase generation for the titles of
documents, which are usually much shorter than
their corresponding documents. The experiment
setting is as follows: the training phase is the
same to the previous experiment, but in the test
phase we suggest keyphrases only using the titles.
LDA and ExpandRank, similar to our method, are
also able to select candidate keyphrases beyond the
titles. We still use the annotated keyphrases of the
corresponding documents as standard answers. In
this case, about 59% standard keyphrases do not
appear in titles.
In Table 3 we show the evaluation results of vari-
ous methods for keyphrase generation when Md = 2.
ForWAM, we only show the results using title-based
pairs prepared with split method. From the table,
we have three observations: (1) WAM outperforms
other methods on keyphrase generation. Moreover,
there are about 10% correctly suggested keyphrases
by WAM do not appear in titles, which indicates the
effectiveness of WAM for keyphrase generation. (2)
The performance of TFIDF and TextRank is much
lower as compared to Table 1, because the titles are
so short that they do not provide enough candidate
keyphrases and even the statistical information to
rank candidate keyphrases. (3) LDA, ExpandRank
and WAM roughly keep comparable performance as
in Table 1 (The performance of ExpandRank drops
a bit). This indicates the three methods are able to
perform keyphrase generation, and verifies again the
effectiveness of our method.
Method Precision Recall F-measure
TFIDF 0.105 0.141 0.115?0.004
TextRank 0.107 0.144 0.118?0.005
LDA 0.180 0.256 0.204?0.008
ExpandRank 0.194 0.268 0.216?0.012
WAM 0.296 0.420 0.334?0.009
Table 3: Precision, recall and F-measure of various
methods for keyphrase generation when Md = 2.
To demonstrate the features of our method for
keyphrase generation, in Table 4 we list top-5
keyphrases suggested by LDA, ExpandRank and
WAM for a news article entitled Israeli Military
Claims Iran Can Produce Nuclear Bombs and
Considering Military Action against Iran (We trans-
late the original Chinese title and keyphrases into
English for comprehension.). We have the following
observations: (1) LDA suggests general words like
?negotiation? and ?sanction? as keyphrases because
the coarse-granularity of topics. (2) ExpandRank
suggests some irrelevant words like ?Lebanon? as
keyphrases, which are introduced by neighbor doc-
uments talking about other affairs related to Israel.
(3) Our method can generate appropriate keyphrases
with less topic-drift. Moreover, our method can find
good keyphrases like ?nuclear weapon? which even
do not appear in the title.
LDA: Iran, U.S.A., negotiation, Israel, sanction
ExpandRank: Iran, Israel, Lebanon, U.S.A., Israeli
Military
WAM: Iran, military action, Israeli Military, Israel,
nuclear weapon
Table 4: Top-5 keyphrases suggested by LDA,
ExpandRank and WAM.
5 Conclusion and Future Work
In this paper, we provide a new perspective to
keyphrase extraction: regarding a document and its
keyphrases as descriptions to the same object written
in two languages. We use IBM Model-1 to bridge
the vocabulary gap between the two languages for
keyphrase generation. We explore various methods
to construct translation pairs. Experiments show
that our method can capture the semantic relations
between words in documents and keyphrases. Our
method is also language-independent, which can be
performed on documents in any languages.
We will explore the following two future work:
(1) Explore our method on other types of articles
and on other languages. (2) Explore more com-
plicated methods to extract important sentences for
constructing translation pairs.
Acknowledgments
This work is supported by the National Natural
Science Foundation of China (NSFC) under Grant
No. 60873174. The authors would like to thank
Peng Li and Xiance Si for their suggestions.
142
References
M. Banko, V.O. Mittal, and M.J. Witbrock. 2000.
Headline generation based on statistical translation. In
Proceedings of ACL, pages 318?325.
A. Berger and J. Lafferty. 1999. Information retrieval as
statistical translation. In Proceedings of SIGIR, pages
222?229.
A. Berger, R. Caruana, D. Cohn, D. Freitag, and
V. Mittal. 2000. Bridging the lexical chasm: statistical
approaches to answer-finding. In Proceedings of
SIGIR, pages 192?199.
D.M. Blei and J.D. Lafferty, 2009. Text mining:
Classification, Clustering, and Applications, chapter
Topic models. Chapman & Hall.
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
dirichlet alocation. Journal of Machine Learning
Research, 3:993?1022, January.
P.F. Brown, V.J.D. Pietra, S.A.D. Pietra, and R.L.
Mercer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational
linguistics, 19(2):263?311.
A.P. Dempster, N.M. Laird, D.B. Rubin, et al 1977.
Maximum likelihood from incomplete data via the em
algorithm. Journal of the Royal Statistical Society.
Series B (Methodological), 39(1):1?38.
P. Duygulu, Kobus Barnard, J. F. G. de Freitas, and
David A. Forsyth. 2002. Object recognition as
machine translation: Learning a lexicon for a fixed
image vocabulary. In Proceedings of ECCV, pages
97?112.
A. Echihabi and D. Marcu. 2003. A noisy-channel
approach to question answering. In Proceedings of
ACL, pages 16?23.
E. Frank, G.W. Paynter, I.H. Witten, C. Gutwin, and C.G.
Nevill-Manning. 1999. Domain-specific keyphrase
extraction. In Proceedings of IJCAI, pages 668?673.
J. Goldstein, V. Mittal, J. Carbonell, and M. Kantrowitz.
2000. Multi-document summarization by sentence
extraction. In Proceedings of NAACL-ANLP 2000
Workshop on Automatic summarization, pages 40?48.
G. Heinrich. 2005. Parameter estimation for text anal-
ysis. Web: http://www. arbylon. net/publications/text-
est.
T. Hofmann. 1999. Probabilistic latent semantic
indexing. In Proceedings of SIGIR, pages 50?57.
A. Hulth. 2003. Improved automatic keyword extraction
given more linguistic knowledge. In Proceedings of
EMNLP, pages 216?223.
M. Karimzadehgan and C.X. Zhai. 2010. Estimation of
statistical translation models based on mutual informa-
tion for ad hoc information retrieval. In Proceedings
of SIGIR, pages 323?330.
P. Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
T.K. Landauer, P.W. Foltz, and D. Laham. 1998. An
introduction to latent semantic analysis. Discourse
Processes, 25:259?284.
Z. Liu, P. Li, Y. Zheng, and M. Sun. 2009a. Clustering
to find exemplar terms for keyphrase extraction. In
Proceedings of EMNLP, pages 257?266.
Z. Liu, H. Wang, H. Wu, and S. Li. 2009b. Collocation
extraction using monolingual word alignment method.
In Proceedings of EMNLP, pages 487?495.
Z. Liu, W. Huang, Y. Zheng, and M. Sun. 2010a. Au-
tomatic keyphrase extraction via topic decomposition.
In Proceedings of EMNLP, pages 366?376.
Z. Liu, H. Wang, H. Wu, and S. Li. 2010b. Improving
statistical machine translation with monolingual collo-
cation. In Proceedings of ACL, pages 825?833.
R. Mihalcea and P. Tarau. 2004. Textrank: Bringing
order into texts. In Proceedings of EMNLP, pages
404?411.
V. Murdock and W.B. Croft. 2004. Simple translation
models for sentence retrieval in factoid question an-
swering. In Proceedings of SIGIR.
T. Nguyen and M.Y. Kan. 2007. Keyphrase extraction
in scientific publications. In Proceedings of the 10th
International Conference on Asian Digital Libraries,
pages 317?326.
F.J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
linguistics, 29(1):19?51.
L. Page, S. Brin, R. Motwani, and T. Winograd. 1998.
The pagerank citation ranking: Bringing order to
the web. Technical report, Stanford Digital Library
Technologies Project, 1998.
C. Quirk, C. Brockett, andW. Dolan. 2004. Monolingual
machine translation for paraphrase generation. In
Proceedings of EMNLP, volume 149.
S. Riezler and Y. Liu. 2010. Query rewriting using
monolingual statistical machine translation. Compu-
tational Linguistics, 36(3):569?582.
S. Riezler, A. Vasserman, I. Tsochantaridis, V. Mittal, and
Y. Liu. 2007. Statistical machine translation for query
expansion in answer retrieval. In Proccedings of ACL,
pages 464?471.
S. Riezler, Y. Liu, and A. Vasserman. 2008. Translating
queries into snippets for improved query expansion. In
Proceedings of COLING, pages 737?744.
G. Salton and C. Buckley. 1988. Term-weighting
approaches in automatic text retrieval. Information
processing and management, 24(5):513?523.
R. Soricut and E. Brill. 2006. Automatic question
answering using the web: Beyond the factoid. Infor-
mation Retrieval, 9(2):191?206.
143
P.D. Turney. 2000. Learning algorithms for keyphrase
extraction. Information Retrieval, 2(4):303?336.
X. Wan and J. Xiao. 2008a. Collabrank: towards a
collaborative approach to single-document keyphrase
extraction. In Proceedings of COLING, pages 969?
976.
X. Wan and J. Xiao. 2008b. Single document
keyphrase extraction using neighborhood knowledge.
In Proceedings of AAAI, pages 855?860.
I.H. Witten, G.W. Paynter, E. Frank, C. Gutwin, and
C.G. Nevill-Manning. 1999. Kea: Practical automatic
keyphrase extraction. In Proceedings of DL, pages
254?255.
X. Xue, J. Jeon, and W.B. Croft. 2008. Retrieval models
for question and answer archives. In Proceedings of
SIGIR, pages 475?482.
S. Zhao, H. Wang, and T. Liu. 2010. Paraphrasing with
search engine query logs. In Proceedings of COLING,
pages 1317?1325.
144
