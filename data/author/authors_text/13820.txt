Clustering Syntactic Positions with
Similar Semantic Requirements
Pablo Gamallo
Universidade de Santiago de
Compostela
Alexandre Agustini.
Pontif??cia Universidade Cato?lica do Rio
Grande do Sul, Centro de Informa?tica e
Tecnologias de InformaZa?o
Gabriel P. Lopes-
Centro de Informa?tica e Tecnologias de
InformaZa?o
This article describes an unsupervised strategy to acquire syntactico-semantic requirements of
nouns, verbs, and adjectives from partially parsed text corpora. The linguistic notion of
requirement underlying this strategy is based on two specific assumptions. First, it is assumed
that two words in a dependency are mutually required. This phenomenon is called here
corequirement. Second, it is also claimed that the set of words occurring in similar positions
defines extensionally the requirements associated with these positions. The main aim of the
learning strategy presented in this article is to identify clusters of similar positions by
identifying the words that define their requirements extensionally. This strategy allows us to
learn the syntactic and semantic requirements of words in different positions. This information
is used to solve attachment ambiguities. Results of this particular task are evaluated at the end
of the article. Extensive experimentation was performed on Portuguese text corpora.
1. Introduction
Word forms, as atoms, cannot arbitrarily combine with each other. They form new
composites by both imposing and satisfying certain requirements. A word uses a
linguistic requirement (constraint or preference) in order to restrict the type of words
with which it can combine in a particular position. The requirement of a given word
is characterized by at least two different objects: the position occupied by the words
that can be combined with the given word and the condition that those words must
satisfy in order to be in that position. For a word w and a specific description of a
location loc, the pair bloc, w? represents a position with regard to w. In addition,
* 2005 Association for Computational Linguistics
 Departamento de L??ngua Espanhola, Faculdade de Filologia, Universidade de Santiago de Compostela,
Campus Universitario Norte, 15782 Santiago de Compostela, Spain. E-mail: gamallo@fct.unl.pt.
. Faculdade de Informa?tica, Pontif??cia Universidade Cato?lica do Rio Grande do Sul, Av. Ipiranga 6681
pre?dio 30 bloco 4, CEP 90619-900 Porto Alegre (RS), Brazil. E-mail: agustini@inf.pucrs.br.
- Department of Computer Science, Faculty of Science and Technology, Universidade Nova de Lisboa,
Quinta da Torre, 2829-516, Caparica, Portugal. E-mail: gpl@di.fct.unl.pt.
Submission received: 13th June 2004; Revised submission received: 4th May 2004; Accepted
for publication: 17th June 2004
condition cond represents the set of linguistic properties that words must satisfy in
order to be in position bloc, w?. So a linguistic requirement of w can be represented as
the pair:
bbloc, w?, cond? ?1?
Consider, for instance, position bof_right, ratification?, where of_right is a location
described as being to the right of preposition of. This position represents the argument slot
ratification of [_ ]. Considering also that cond stands for the specific property being a
nominal phrase (np) whose head denotes a legal document (abbreviated by doc), then the pair
bbof_right, ratification?, doc? means that the particular position ratification of [_ ] selects
for nouns denoting legal documents. In other words, ratification requires nominal
arguments denoting legal documents to appear after preposition of. Suppose that there
exist some words such as law, treaty, and constitution that are nouns denoting legal
documents. Then it follows that they fill the condition imposed by ratification in the
of_right location. An expression like the ratification of the treaty is then well-formed,
because treaty satisfies the required condition.
Let us look now more carefully at several linguistic issues we consider to be
important to characterize the notion of linguistic requirement: extensionality/
intensionality, soft/hard requirements, the scope of a condition, syntactic/semantic
requirements, and corequirements.
A condition can be defined either intentionally or extensionally. For example, the
two specific properties being the head of an np and being a legal document are used to
define intensionally the condition imposed by position bof_right, ratification?. However,
it is also possible to define it extensionally by enumerating all those words that
actually possess such properties: for example, law, treaty, and constitution.
Moreover, the process of satisfying a condition can be defined as a binary action
producing a Boolean (yes/no) value. From this point of view, a word either satisfies
or does not satisfy the condition imposed by another word in a specific location. This
is a hard requirement. By contrast, the satisfaction process can also be viewed as
a soft requirement, in which some words are ??preferred?? without completely
excluding other possibilities. In Beale, Niremburg, and Viegas (1998), hard
requirements are named constraints, whereas the term preferences is employed
for soft requirements. In the following, we use one of these two terms only if it is
necessary to distinguish between hard and soft requirements. Otherwise, require-
ment is taken as the default term.
Let?s describe now what we call the scope of a condition. A position imposes a
specific condition on the words that can appear in that position. Yet a specific
condition is generally imposed not by only one position, but by a large set of them. If a
condition were bound only to a particular position, every combination of words would
be a noncompositional idiomatic expression. So speakers could not combine words
easily, and new composite expressions would be difficult to learn. The scope of a
condition embraces the positions that use it to restrict word combination. For instance,
the condition imposed by ratification of [_ ] seems to be the same as the one imposed by
the verb ratify on the words appearing at its right: bright, ratify? (to ratify [_ ]). In
addition, these positions also share the same conditions as to approve [_ ], to sign [_ ], or
signatories to [_ ]. Each of these similar positions is within the scope of a specific
condition, namely, being an np whose head denotes a legal document. In this article, we
assume that every linguistic condition is associated with a set of similar positions. This
108
Computational Linguistics Volume 31, Number 1
109
set represents the scope of the condition. The larger the set of similar positions, the
larger the condition scope, and the more general the property used to characterize the
condition.
We distinguish syntactic and semantic requirements. A syntactic requirement is
characterized by both a position and a morpho-syntactic condition. For instance,
requirement bbof_right, ratification?, np? consists of a position, bof_right, ratification?,
which selects for a nominal phrase. Note that the different syntactic requirements of
a word can serve to identify the set of subcategorization frames of that word. Note also
that, in some cases, a particular position presupposes a particular morpho-syntactic
condition. In our example, position bof_right, ratification? requires only a np. So we can
use this position as a shorter form of the syntactic requirement bbof_right, ratification?,
np?. We call a syntactic position a position that presupposes a specific morpho-
syntactic condition. On the other hand, a semantic requirement (also known as
selection restriction) is characterized by both a position and a semantic condition,
which presupposes a syntactic one. So bbof_right, ratification?,doc? means that position
bof_right, ratification? selects for the head of a np denoting a legal document. Condition
doc presupposes then a np. Identifying a particular semantic requirement entails the
identification of the underlying syntactic one.
The final linguistic issue to be introduced is the phenomenon referred to as
corequirements. It is assumed that each syntactic dependency between two words
(which are the heads of two phrases) is composed of two complementary
requirements. For instance, it seems that two different requirements underlie the
expression ratification of the treaty: bof_right, ratification? (ratification of [_ ]) needs to be
filled by words like treaty, while bof_left, treaty? ([_ ] of the treaty) needs to appear with
words such as ratification.
The main objective of this article is to describe an unsupervised method for
learning syntactic and semantic requirements from large text corpora. For instance, our
method discovers that the word secretary is associated with several syntactic positions
(i.e., positions with morpho-syntactic conditions), such as secretary of [_ ], [_ ] of the
secretary, [_ ] to the secretary, and [_ ] with the secretary. The set of syntactic positions
defined by a word can be used to characterize a set of subcategorization frames. The
precise characterization of these frames remains, however, beyond the scope of this
article. In addition, for each syntactic position, we assess the specific semantic
condition a word needs to fill in order to appear in that position. Another important
objective of the article is to use the semantic requirements to capture contextually
relevant semantic similarities between words. In particular, we assume that two words
filling the same semantic requirement share the same contextual word sense.
Consequently, learning semantic requirements also leads us to induce word senses.
Suppose that the word organization fills the condition imposed by secretary of [_ ]. In this
syntactic context, the word denotes a social institution and not a temporal process or
an abstract setup.
To achieve our objectives, we follow a particular clustering strategy. Syntactic
positions (and not words) are compared according to their word distribution. Similar
syntactic positions are put in more clusters following some constraints that are
defined later. Each cluster of positions represents a semantic condition. The features of
each cluster are the words that can fill the common condition imposed by those
positions: They are the fillers. They are used to extensionally define the particular
condition they can fill. That is, a condition is defined by identifying those words likely
to appear in positions considered similar. Given that a condition is extensionally
defined by the words that are able to fill it, our method describes the process of
Gamallo, Agustini, and Lopes Clustering Syntactic Positions
satisfying a condition as a Boolean constraint (yes/no) and not as a probabilistic
preference. The similar positions defining a cluster are within the scope of a particular
semantic condition. The association between each position of the cluster and that
condition characterizes the semantic requirement of a word. This learning strategy
does not require hand-crafted external resources such as a WordNet-like thesaurus or
a machine-readable dictionary.
The information captured by this strategy is useful for two different NLP
disambiguation tasks: selecting contextual senses of words (word sense disambigu-
ation) and solving structural ambiguity (attachment resolution). This article is focused
on the latter application.
In sum, the main contribution of our work is the large amount of linguistic
information we learn for each lexical word. Given a word, we acquire, at least, three
types of information: (1) an unordered set of syntactic positions, which is a first
approximation to define the set of subcategorization frames of the given word, (2) the
semantic requirements the word imposes on its arguments, and (3) the different
contextual senses of the word. By contrast, related work focuses only on one or two
aspects of this linguistic information. Another contribution is the use of corequire-
ments to characterize the arguments of a word.
To conclude the introduction, let?s outline the organization of the article. In the
next section, we situate our approach with regard to related work on acquisition of
linguistic requirements. Later, in sections 3 and 4, we describe in detail the main
linguistic assumptions underlying our approach. Special attention will be paid to both
the relativized view on word sense (i.e., contextual sense) and corequirements. Then,
section 5 depicts a general overview of our strategy. Two particular aspects of this
strategy are analyzed next. More precisely, section 6 describes both how syntactic
positions are extracted and how they are clustered in larger classes (section 7). Finally,
in section 8, we evaluate the results by measuring their performance in a particular
NLP task: syntactic-attachment resolution.
2. Statistics-Based Methods for Learning Linguistic Requirements
During the last years, various stochastic approaches to linguistic requirements
acquisition have been proposed (Basili, Pazienza, and Velardi 1992; Hindle and Rooth
1993; Sekine et al 1992; Grishman and Sterling 1994; Framis 1995; Dagan, Marcus, and
Markovitch 1995; Resnik 1997; Dagan, Lee, and Pereira 1998; Marques, Lopes, and
Coelho 2000; Ciaramita and Johnson 2000). In general, they follow comparable learning
strategies, despite significant differences observed. In this section, we present first the
common strategy followed by these approaches, and then we focus on their
differences. Special attention is paid to lexical methods. At the end, we situate our
strategy with regard to the related work.
2.1 A Common Strategy
The main design of the strategy for automatically learning requirements is to compute
the association degree between argument positions and their respective linguistic
conditions. For this purpose, the first task is to count the frequency with which
bbloc, w?, cond? occurs in a large corpus:
F?bbloc, w?, cond?? ?2?
110
Computational Linguistics Volume 31, Number 1
111
where F counts the frequency of co-occurring bloc, w? with cond. Then this frequency is
used to compute the conditional probability of cond given position bloc, w? :
P?cond j bloc, w?? ?3?
This probability is then used to measure the strength of statistical association between
bloc, w? and cond. Association measures such as mutual information or log-likelihood are
used for measuring the degree of (in)dependence between these two linguistic objects.
Intuitively, a high value of the association measure is evidence of the existence of a true
requirement (i.e., a type of linguistic dependence).
The stochastic association values obtained by such a strategy turn out to be useful
for NLP disambiguation tasks such as attachment resolution in probabilistic parsing
and sense disambiguation.
2.2 Specific Aspects of the Common Strategy
Despite the apparent methodological unanimity, approaches to learning requirements
propose different definitions for the following objects: association measure, position
bloc, w?, and linguistic condition cond. Many approaches differ only in the way in
which the association measure is defined. Yet such differences are not discussed in this
article.
As regards position bloc, w?, we distinguish, at least, among three different
definitions. First, it can be considered as a mere word sequence (Dagan, Marcus, and
Markovitch 1995): For instance, bright, w?, where right means being to the right of.
Second, a position can also be defined in terms of co-occurrence within a fixed window
(Dagan, Lee, and Pereira 1998; Marques, Lopes, and Coelho 2000). Finally, it can be
identified as the head or the dependent role within a binary grammatical relationship
such as subject, direct object, or modifier (Sekine et al 1992; Grishman and Sterling
1994; Framis 1995). In section 4, we pay special attention to the grammatical
characterization of syntactic positions.
As far as cond is concerned, various types of information are used to define
a linguistic condition: syntactic, semantic, and lexical information. The approaches
to learning requirements are easily distinguished by how they define cond.
Table 1 displays three different ways of encoding the condition imposed by verb
approve to the nominal the law in the expression to approve the law.
Requirement conditions of the pairs in Table 1 represent three descriptive levels
for the linguistic information underlying the nominal expression the law when it
appears to the right of the verb approve.1 The properties np, doc, and law are situated at
different levels of abstraction. The morpho-syntactic tag np conveys more abstract
information than the semantic tag doc (document), which, in turn, is more general than
the lemma law. Some conditions can be inferred from other conditions. For instance,
doc is used only to tag nouns, which are the heads of nominal phrases. So the semantic
tag doc entails the syntactic requirement np. Likewise, the lemma law is associated only
with nouns. It entails, then, an np.
Some approaches describe linguistic conditions only at the syntactic level (Hindle
and Rooth 1993; Marques, Lopes, and Coelho 2000). They count the frequency of pairs
1 In case of Portuguese, for intransitive verbs the occurrence of an np to the right of the verb does not mean
that the verb is transitive. In fact, this is the standard position of the subject for intransitive verbs.
Gamallo, Agustini, and Lopes Clustering Syntactic Positions
like bbright, approve?, np? in order to calculate the probability of an np occurring given
bright, approve?. This probability is then used to compute the degree of association
between approve and an np located to the right. This association value may be useful in
different linguistic tasks. For instance, it may serve to solve structural ambiguities
(Hindle and Rooth 1993) or to build a subcategorization lexicon (Marques, Lopes, and
Coelho 2000). Most approaches to learning syntactic requirements assume that
syntactic properties can be identified by means of some specific morphological ??cues??
appearing in the corpus. For instance, the article a following a verb is a clear evidence
for an np appearing at the right of the verb; the preposition of following a verb is
evidence for an of_right complement; and the conjunction that after a verb introduces a
that_clause. Morphological cues are used to easily identify syntactic requirements. This
technique allows raw text to be worked on directly. Let us note that these techniques
do not allow the acquisition of complete subcategorization frames (Brent 1991;
Manning 1993). They are able to acquire that, for instance, approve subcategorizes an
np on two locations: both right and of_right locations (e.g., to approve the laws, to approve
of the decision). So they associate that verb with two syntactic arguments. However,
they are not able to learn that the two arguments are incompatible and must belong
to two different subcategorization frames of the verb. We return to this issue in
section 8.1.
In other approaches to requirement learning, linguistic conditions are defined in
semantic terms by means of specific tags (Basili, Pazienza, and Velardi 1992; Resnik
1997; Framis 1995). In order to calculate the degree of association between tag doc and
position bright, approve?, these approaches count the frequency of pairs like bbright,
approve?, doc? throughout the corpus. If the association value is higher than that for
other related cases, then one might learn that the verb approve requires nominal
phrases denoting doc entities to appear at the right.
According to other learning approaches, the linguistic conditions used to
characterize requirements may be situated at the lexical level (Dagan, Lee, and
Pereira 1998; Dagan, Marcus, and Markovitch 1995; Grishman and Sterling 1994;
Sekine et al 1992). A pair like bbright, approve?, law? matches those expressions
containing a form of lemma law (e.g., law, laws, Law, Laws) appearing to the right
of the verb approve (to be more precise, to the right of any form of lemma approve). The
frequency of this pair in the corpus serves to compute the degree of association
between law and the verb approve at the right. In these approaches, then, conditions are
learned from lexical co-occurrences. From now on, when it is not necessary to
distinguish between lemmas and word forms, we use the term ??word?? for both
objects.
To compare the three types of approaches more accurately, let?s analyze their
behavior regarding different quantitative aspects: (1) the continuum between
supervised and unsupervised learning, (2) the continuum between knowledge-poor
and knowledge-rich methodology, and (3) the continuum between general- and
specific-information acquisition.
112
Table 1
Various levels of encoding linguistic conditions.
Syntactic level bbright, approve?, np?
Semantic level bbright, approve?, doc?
Lexical level bbright, approve?, law?
Computational Linguistics Volume 31, Number 1
113
2.2.1 Supervised/Unsupervised Learning. The first continuum ranges over the degree
of human supervision that is needed to annotate the training corpus. Among the works
cited above, Basili, Pazienza, and Velardi (1992) has the highest degree of supervision.
This semantic approach requires hand-tagging text nouns using a fixed set of semantic
labels. The other approaches involve close to total nonsupervision, since they do not
require a training corpus to be annotated by hand. However, some degree of human
supervision could be involved in building automatic tools (e.g., a neural tagger in
Marques, Lopes, and Coelho [2000]) or linguistic external sources (e.g., WordNet in
Resnik [1997]; Framis [1995]; Ciaramita and Johnson [2000]), which are used to
annotate the corpus.
2.2.2 Knowledge-Rich/Knowledge-Poor Methods. The second continuum refers to the
notions introduced by Grefenstette (1994). He distinguishes the learning methods
according to the quantity of linguistic knowledge they require. The most knowledge-
rich approaches need a handcrafted thesaurus (WordNet) to semantically annotate
nouns of the training corpus (Resnik 1997; Framis 1995; Ciaramita and Johnson 2000).
At the opposite end of the continuum, the most knowledge-poor methods are
introduced in Dagan, Marcus, and Markovitch (1995) and Dagan, Lee, and Pereira
(1998); these merely need to identify lemmas in the corpus.
2.2.3 General/Specific Conditions. As regards the general/specific continuum, syn-
tactic methods, that is, approaches to learning syntactic requirements, are the learning
methods that use the most general linguistic information. At the opposite end of the
continuum, we find the lexical methods, that is, those strategies situated at the lexical
level. Methods using tags like doc, human, and institution are situated at an interme-
diate level and are known as semantic methods. One of the most difficult theoretical
problems is to choose the appropriate generalization level for learning requirement
information.
The syntactic level seems not to be appropriate for solving structural ambiguity.
Concerning the parsing task, syntactic information is not always enough to produce a
single parse. Consider the following analyses:
?vpcut ?npthe potato ?ppwith a knife ?4?
?vpcut ?npthe potato ?ppwith a hole ?5?
In order to decide which analysis, either (4) or (5), is correct, we must enlist the aid of
our world knowledge concerning cutting actions, use of knives, and the properties of
potatoes. In general, we know that knives are used for cutting and that potatoes are
objects likely to have holes. So the parser is able to propose a correct analysis only if the
lexicon is provided not only with syntactic requirements, but also with information on
semantico-pragmatic requirements (i.e., with selection restrictions). Selection restric-
tions are typically used to capture facts about the world that are generally, but not
necessarily, true (Androutsopoulos and Dale 2000). So the main goal of semantic and
lexical methods is precisely the acquisition of selection restrictions.
As has been mentioned previously, semantic methods use handcrafted sources of
linguistic knowledge such as WordNet. There are several disadvantages associated
Gamallo, Agustini, and Lopes Clustering Syntactic Positions
with these knowledge-rich approaches: Manually created thesauri contain many
words either having rare senses or missing domain-specific meanings. In sum, the
level of semantic information provided by handcrafted thesauri is either too specific
or too general, and it is usually incomplete. It seems not to be appropriate for most
NLP tasks (Grefenstette 1994). By contrast, lexical methods are able to acquire
information at the level of detail required by the corpus domain. They are domain-
dependent approaches. However, they are very sensitive to the problem of data
sparseness.
2.3 Lexical Methods and the Data Sparseness Problem
Most word co-occurrences (for instance, the co-occurrence of agreement with approve at
location right) have very small probabilities of occurring in the training corpus. Note
that if they were not observed in the corpus, they would have identical probabilities
(i.e., probability 0) to those of incorrect co-occurrences such as cow appearing to the
right of approve. This is what is known as the data sparseness problem. To solve this
problem, many lexical methods estimate the probabilities of unobserved pairs by
taking into account word similarity. Suppose that the pair bbright, approve?, agreement?
is not observed in the training corpus. To obtain an appropriate measure of the
association between agreement and bright, approve?, the degree of association between
bright, approve? and each word most similar to agreement is computed. The main
criterion for measuring word similarity is comparing the context distribution of words.
The total association value for the specific lexical co-occurrence is the average of these
association values.
Information on word similarity is used to generalize the pairs appearing in the
corpus and to smooth their co-occurrence probabilities. That is, very specific
requirements described at the lexical level can be generalized by means of word
similarity information.
For instance, the following pair:
bbright, approve?, MOST SIM?agreement?? ?6?
associates the information MOST SIM?agreement? with the position bright, approve?,
where MOST SIM?agreement? represents the most similar words to agreement: for
example, law, treaty, accordance, and conformity. The use of word similarity allows the
probabilities computed at the lexical level to be smoothed (generalized). Computations
involving similar words minimize the data sparseness problem to a certain extent.
Lexical methods provided with similarity-based generalizations are found in Sekine
et al (1992), Grishman and Sterling (1994), and Dagan, Lee, and Pereira (1998). Later,
in section 8.3.4, we use a lexical method with similarity-based generalization to solve
syntactic attachments. The results obtained using this method are explicitly compared
to those obtained by our clustering strategy.
The methodology for automatically measuring word similarity is often based on
Harris?s (1985) distributional hypothesis on word meaning. According to this
hypothesis, words occurring in similar syntactic contexts (i.e., in similar syntactic
positions) are semantically similar. A simple way of implementing this hypothesis is to
compute the similarity between words by comparing the whole information concerning
their context distribution. Allegrini, Montemagni, and Pirrelli (2003) call this strategy
the absolute view on word similarity. The absolute view leads to the characterization of
114
Computational Linguistics Volume 31, Number 1
115
word similarity as an intransitive relation (Dagan, Lee, and Pereira 1998). Let us
examine expressions (7)?(10), which show that even if treaty is similar to agreement, and
agreement is similar to conformity, it does not mean that treaty is similar to conformity:
to approve the agreement=treaty ?7?
to ratify the agreement=treaty ?8?
we are in agreement=conformity with your proposal ?9?
my signature indicates my agreement=conformity to the rules ?10?
Intransitivity makes this type of word similarity rather inefficient for identifying
contextual word senses. For instance, it does not help show that agreement is similar to
treaty in quite a different way than it is similar to conformity. Expressions (7) and (8)
introduce the linguistic contexts in which agreement denotes a document containing
legal information. This word is considered to be semantically similar to treaty with
regard to the contexts introduced by verbs approve and ratify. By contrast, (9) and (10)
introduce different linguistic contexts. There, agreement conveys a different sense: the
verbal act of agreeing. In these contexts, it becomes similar to conformity. Word
similarity methods based on the absolute view seem to be unable to distinguish such
contextual meanings. This shortcoming may disrupt the smoothing process defined
above. As conformity and accordance are part of the most similar words to agreement,
they are involved in the process of computing the degree of association between this
word and bright, approve?. Yet this is counterintuitive, since they are not semantically
required by the verb in such a particular position.
2.4 General Properties of Our Method
The objective of this article is to propose a new strategy for learning linguistic
requirements. This strategy is designed to overcome the main drawbacks underlying
the different approaches introduced above. Our method can be characterized as follows:
 The information it acquires is described at a semantically appropriate
level of generalization.
 It is defined as a knowledge-poor and unsupervised strategy.
As regards the first characteristic, we consider the method to be semantically
appropriate only if the acquired requirements are useful for solving disambiguation
problems such as those illustrated above by parses (4) and (5). So our acquisition
method is focused on more specific information than that contained in syntactic
requirements. Given a word, our aim is to learn not only the syntactic positions in
which that word appears, but also the semantico-pragmatic constraints (i.e., what are
broadly called selection restrictions associated with each syntactic requirement.
Selection restrictions are extracted from position-word co-occurrences. We thus follow
a lexical method. However, selection restrictions are defined in accordance with a
theory of word sense that is not based on the absolute view on word similarity. We use
a more relativized viewpoint on word senses. In sum, we follow a strategy slightly
Gamallo, Agustini, and Lopes Clustering Syntactic Positions
different from that described in section 2.3. In the next section, we describe our basic
assumptions on word sense and word similarity.
Concerning the second characteristic (i.e., knowledge-poor and unsupervised
strategy), our method does not rely on external structured sources of lexical
information (e.g., WordNet) or on a training corpus built and corrected by hand.
Unlike the semantic methods outlined above (in section 2.2), ours attempts to reduce
human intervention to a minimum.
3. The Foundations of Our Learning Strategy
In this section, we outline the basic assumptions underlying our learning strategy. This
strategy relies on a particular definition of semantic condition (sections 3.1 and 3.2)
and a relativized view on word similarity (section 3.3), as well as a specific viewpoint
on word sense (section 3.4).
3.1 Extensional Definition
Given a requirement bbloc, w?, cond?, we define a semantic condition, cond, as the set of
words that can occur in position bloc, w?. This means that linguistic conditions are
defined extensionally. For instance, consider again position bright, approve? and one of
its possible conditions, namely, doc, which, as has been shown, means being a noun
denoting a legal document. This condition is extensionally defined by enumerating the
set of words likely to occur with both bright, approve? and their similar positions.
Identifying such a word set is not a trivial task. This set is not a closed, fixed, and
predefined list of nouns. Rather, it turns out to be a set open to a great variety of
extensions, since it can be modified as time, domain, or speaker change. The aim of our
method is to learn, for each argument position, the open set (or sets) of words it
requires. Each word set represents, in extensional terms, a specific linguistic condition.
For this purpose, we opt for the following learning strategy.
The condition imposed by an argument position is represented by the set of words
actually appearing in this position in the training corpus. For instance, let?s suppose
that bright, approve? occurs with four different words: law, agreement, convention, and oil
(to simplify the explanation, frequencies are not taken into account). For the present,
we know only that these words are mere candidates to satisfy the condition imposed
by that position. In order to actually know whether or not the candidate fillers satisfy
such a condition, we select the most similar positions to bright, approve?. So we get
clusters of similar positions imposing the same condition. Consider, for instance, the
following cluster:
fbright, approve?, bright, ratify?,bto right, signatories?,
bby left, becertified?,bof right, ratification?g ?11?
which is made of positions sharing features such as
law, agreement, article, treaty, convention, document ?12?
So, cluster features in (12) are the words that may fill the specific condition imposed
by the similar positions in (11). These words can be viewed as fillers satisfying the
116
Computational Linguistics Volume 31, Number 1
117
intensional property being a noun denoting a legal document. Note that (12) contains some
words (e.g., article and treaty) that do not actually occur with position bright, approve? in
the corpus. However, as these words actually occur with most of the positions that are
similar to bright, approve?, we may assume that they satisfy the condition of this particular
position. This is the technique we use to generalize (smooth) occurrences of position-
word pairs that are not observed in the training corpus. Details of our method of
clustering are given in section 7.2. Notice also that the set of fillers does not include the
word oil. This word does not belong to the set of shared features because it does not
occur with any of the positions similar to bright, approve?. This is the method we use
to identify and remove invalid associations between a position and a word. It is
explained in section 7.1.
In sum, positions are considered to be similar to one another because they impose
the same condition (i.e., they share the same selection restrictions). As has been noted
earlier, similar positions are within the scope of one common requirement. The set of
similar positions in (11) represents the scope of condition (12). The fillers are those
words that characterize the extension of such a condition.
3.2 Hard Requirements
We assume that the process of condition satisfaction may be defined as a Boolean
function and not as a probabilistic one. The value of the association between, for
instance, the word treaty and the position bright, approve? is either yes or no. Our
method merely attempts to learn whether or not there is a true association between
them. If the association is actually true, then we learn that the word satisfies the
condition. Hard requirements can easily be used to constrain the grammar of a
symbolic parser. In particular, we use them to improve the parser described in Rocio,
de la Clergerie, and Lopes (2001). Although linguistic constraints are defined in
Boolean terms, they are open to potential changes. Clusters and their features are
supposed to be modified and extended as the training corpus grows and is
progressively annotated with more trustworthy syntactic information. Moreover, a
new domain-specific corpus can lead us not only to create new clusters, but also to
tune old ones. From this viewpoint, Boolean constraints cannot be considered
necessary and sufficient conditions. They evolve progressively.
3.3 Relativized Word Similarity
Our learning strategy relies on a specific assumption on word similarity. We are
interested in computing similarity between words with regard to a set of similar
positions. So we must first compute similarity between positions. As has been
mentioned before, similar positions impose the same linguistic condition. Hence, they
are likely to be filled by the same set of words. Statistically, this means that they have
similar word distribution. A definition of this similarity is given in section 7.1. Unlike
in the absolute view stated above, we are not interested in measuring similarity
between words on the basis of the distribution of all their corpus-based positions (their
whole context distribution). Our aim is, first, to compute the similarity between
positions via their word distribution. Positions are in fact less ambiguous than words.
Then, we consider two words to be similar if they occur with at least a pair of similar
positions. This way of using similar positions allows all possible dimensions of
similarity of a given word to be captured. This is close to the ??relativized view?? on
word similarity offered by Allegrini, Montemagni, and Pirrelli (2003).
Gamallo, Agustini, and Lopes Clustering Syntactic Positions
3.4 Contextual Hypothesis on Word Sense
Behind this account of similarity, there is a particular view of word sense that is not far
from that of Schu?tze (1998):
Contextual Hypothesis for Word Sense: A set of similar positions defines a
particular type of context. A word occurring with positions of the same
type keeps the same sense. The sense of a word changes if the word
appears in a different type of context.
For instance agreement refers to a legal document when it satisfies the require-
ment of similar positions such as to approve [_ ] or ratification of [_ ]. By contrast, this
word denotes a verbal act when it appears in positions such as in [_ ] with your proposal
or [_ ] to the rules.
According to this hypothesis, identifying word senses relies on identifying sets of
similar positions (i.e., types of contexts). The noun book, for instance, can denote at least
three different contextual senses provided it appears in three context types: for
example, physical actions (carrying the book, putting it on the table, etc.), symbolic
processes (writing or reading books), and economic transactions (selling or buying
books). This notion of word sense is dependent on the ability to grasp classes of
contexts, that is, the ability to learn clusters of similar positions. The more accurate is
this ability, the more precise are the senses identified in a particular corpus. This
means that the set of senses associated with a word cannot be predefined by an
external lexical resource like WordNet or any machine-readable dictionary. Word
senses are dynamically learned as the text is processed and positions are organized in
semantically homogenous clusters. Each cluster of similar positions (or context type)
represents a particular word sense. From this viewpoint, the set of contextual senses of
a word represents its whole meaning. Such a notion of word meaning is in accordance
with the encyclopedic hypothesis on lexical meaning within the cognitive grammar
framework (Langacker 1991). According to this hypothesis, every word or lexical unit
is associated with a continuum of encyclopedic knowledge (the word meaning). The
use of the word in a particular context makes a partial aspect of this continuum more
salient (a specific word sense).
Within a particular corpus, we assume that the meaning of a word is defined by
the context types that organize the different positions of the word. In other words, a
word?s meaning is described by identifying the types of requirements the word fulfills.
In the next section, we explore the notions of requirement and syntactic position.
4. Syntactic Positions and Corequirements
This section discusses the general properties of syntactic positions and their role in
extracting linguistic requirements. Syntactic positions are defined here as internal
elements of binary dependencies. Two aspects of dependencies are retained: the head-
dependent distinction and the predicate-argument structure. Special attention is paid
to corequirements.
4.1 Head-Dependent Distinction
The head-dependent pattern takes over the process of transferring morpho-syntactic
features to higher grammatical levels. A composite expression inherits the features of
118
Computational Linguistics Volume 31, Number 1
119
the headword. There are two different locations (or grammatical roles) within a binary
dependency: the head and the dependent. Consider the binary dependencies shown
in the first column of Table 2, which represent the expressions to ratify the law and long
dinner. The grammatical relations between the two words are expressed by both robj,
which stands for the nominal object appearing to the right of the verb,2 and mod, which
stands for the noun-adjective dependency. The word indexed by , (down location)
plays the role of head, whereas the word indexed by j (up location) plays the role of
dependent. Since a binary dependency is constituted by two grammatical locations, we
can split the dependency into two complementary syntactic positions.
Each pair of positions in the second column of Table 2 was extracted from a binary
dependency. We show below that the two positions extracted from a dependency are
associated with specific semantic conditions. Hence, they can be used to characterize
syntactico-semantic requirements. In our work, the different types of binary relations
from which we extract all positions are lobj, robj, iobj_prepname, aobj_prepname,
prepname, and mod. Relation lobj designates the nominal object appearing to the left
of verb, robj represents the nominal object appearing to the right of the verb,
iobj_prepname introduces a nominal after a verb and a preposition, aobj_prepname
represents a nominal after an adjective and a preposition, prepname corresponds to a
nominal following a noun and a preposition, and mod refers to the adjective
modification of nouns. Note that each relation conveys not only two argument
positions, but also specific morpho-syntactic conditions. robj, for instance, signals that
there is an np to the right of a vp. So brobj_down, ratify? contains the same information as
the syntactic requirement bbrobj_down, ratify?, np?, while brobj_up, law? is equivalent to
bbrobj_up, law?, vp?.
4.2 Predicate-Argument Structure
Besides the head-dependent pattern, binary dependencies are also organized as
predicative structures: Predicate(Argument). While the former pattern drives the
process of inheriting morpho-syntactic information throughout grammatical levels, the
latter is directly related to semantic requirements. This section starts by introducing
the standard account concerning the role of the Predicate(Argument) structure in the
process of imposing linguistic requirements. Then we make new assumptions about
what we consider to be a more accurate notion of requirement information. This notion
is modeled by means of what we call corequirements. Corequirements are used later,
in sections 6 and 7, to elaborate our learning method.
Table 2
Two binary dependencies and their positions.
Dependencies Contexts
(robj; ratify,, lawj) brobj_down, ratify?
brobj_up, law?
(mod; dinner,, longj) bmod_down, dinner?
bmod_up, long?
2 In Portuguese, a right object (without governing preposition) can be elaborated, under specific conditions,
as either a direct object or a subject.
Gamallo, Agustini, and Lopes Clustering Syntactic Positions
4.2.1 Simple Requirements. It is broadly assumed that a binary syntactic dependency
is constituted by both the word that imposes linguistic constraints (the predicate) and
the word that must fill such constraints (its argument). In a syntactic dependency, each
word is considered to play a fixed role. The argument is perceived as the word
specifying or modifying the syntactico-semantic constraints imposed by the predicate,
while the predicate is viewed as the word that is specified or modified by the former.
Notice that a predicate is not defined here in model-theoretic terms. We inherit the
intuitive definition assumed in the linguistic tradition of dependency grammar
(Hudson 2003). According to this tradition, a predicate is the semantic representation
of one of the two words taking part in a binary dependency. More precisely, it is the
representation of one word (either head or dependent) that actually imposes semantic
requirements on the other word.
In standard linguistic approaches, the Predicate(Argument) structure is the se-
mantic counterpart of the head-dependent pattern. The former relates to the latter in
the following way. Typically, the dependent word playing the role of Argument is
conceived as the complement or object of the head (see Figure 1). By contrast, when it
plays a more active role, behaving more like a Predicate, it is viewed as a modifier or
the adjunct of the head (Figure 2). In other words, the dependent of a head-dependent
structure is described either as a passive complement, if it satisfies the linguistic
requirements of the head (Argument role), or as an active modifier, when the
dependent itself requires a specific type of head (Predicative role).
The most typical case of a head being a predicate is when a verb is the head within
a direct-object dependency. The noun is viewed here as a complement, that is, as a
dependent expression fulfilling the conditions imposed by the verbal predicate. The
most typical case of a dependent taken as a predicate is the standard use of an
adjective or an adverb. In this case, it is the adjective (or adverb) that imposes the
selection restrictions on the noun (or verb), which is the head of the dependency.
By contrast, in case of dependencies such as prepositional relations, it is not
possible to distinguish between a complement and a modifier, unless we have access
to the specific semantico-pragmatic information conveyed by words. However, there
are many cases in which the borderline between complement and modifier is not clear.
In these cases, even semantico-pragmatic knowledge is not enough to enable a decision
to be made in favor of one particular predicative structure. For instance, consider the
expression to play with a doll. Which is the word that can be taken as the predicate, and
which as the argument?
Linguists have made a considerable effort to discriminate between complements
and modifiers (= adjuncts). The complement/modifier distinction is probably one of
the most unclear issues in linguistics (Langacker 1991). No linguistic proposal is able to
distinguish in absolute terms complements from external adjuncts; for example, in the
previous expression, is with a doll an internal complement or an adverbial modifier of
play? In other words, is position biobj_with_down,play? one that requires as argument the
noun doll (complement construction)? Or, on the contrary, is position biobj_with_up, doll?
one that requires as argument the verb play (modifier structure)? There is no reliable
evidence on which to choose between the two possible requirement structures. Most
120
Figure 1
Complement structure.
Computational Linguistics Volume 31, Number 1
121
linguistic proposals may be situated in one of two general trends: (1) Some linguists
interpolate finer distinctions between the two extremes (Pustejovsky 1995). So between
true or basic complements and completely optional adjuncts, it is possible to find
default complements, shadow complements, and so on which share properties of both
complements and adjuncts. (2) A more radical view is to consider the complement/
modifier opposition as a continuum in which it is not easy to fix borderlines between
what is entirely optional and what is obligatory (Langacker 1991).
The idea of a continuum entails that complements and modifiers cannot be defined
in absolute terms. All binary dependencies always contain a certain degree of both
complementarization and modification. That is, given a dependency, the head requires
the dependent (complementarization), and conversely, the dependent requires the
head (modification). We assume in this article that such corequirements underlie any
binary dependency.
4.2.2 Corequirements. Recent linguistic research assumes that two words related by a
syntactic dependency are mutually constrained (Pustejovsky 1995; Gamallo 2003).
Each word imposes linguistic requirements on the other. There does not exist a single,
pre-fixed predicate-argument pattern. Each related word is at the same time both a
predicate and an argument. We call such a phenomenon corequirement structure.
Consider again the expression potato with a hole. It does not seem obvious whether
hole is the complement or the modifier of potato within the with dependency. If it is
considered the complement, then it is the head potato that should be provided with the
appropriate requirements. Otherwise, it should be the modifier hole, the word
imposing specific requirements on the head. Following recent research, we claim,
however, that such a radical opposition is not useful for describing linguistic
requirements. It is assumed here that two syntactically related expressions presuppose
two complementary requirements. In other words, every binary dependency is
constituted by two compatible predicate-argument structures.
On the one hand, the noun potato requires words denoting parts or properties of
potatoes to appear in the with_down location. The noun hole satisfies such a requirement.
On the other hand, the noun hole is also provided with selective requirements in the
with_up location. Indeed, in this location, it requires nouns denoting material objects
that can have holes. The noun potato fulfills such a condition. Note that the expressions
cut with a knife and play with a doll could also be considered borderline cases.
Corequirements are not useful only for modeling borderline cases. We believe that
they are also pertinent for typical complement structures (e.g., the direct-object relation
between verbs and nouns), as well as for typical modifier constructions (i.e., adjective-
noun and verb-adverb dependencies). In long dinner, for instance, the noun seems to
behave as a predicate constraining the adjective to denote a temporal dimension (and
not a spatial one). So not only does the adjective disambiguate the noun, but the noun
also disambiguates the adjective.
Therefore, according to the assumption on corequirements, two syntactically de-
pendent expressions are no longer interpreted as a standard predicate-argument pair,
in which the predicate is the active function imposing semantic conditions on a passive
Figure 2
Modifier structure.
Gamallo, Agustini, and Lopes Clustering Syntactic Positions
argument, which matches these conditions. On the contrary, each word of a binary
dependency is perceived simultaneously as both a predicate and an argument. That is,
each word both imposes semantic requirements and matches semantic requirements in
return. Figure 3 depicts a standard syntactic dependency between two words, the head
and the modifier, with two Predicate(Argument) structures. Figure 4 illustrates the
two specific Predicate(Argument) structures extracted from the modifier relation be-
tween the noun dinner (the head) and the adjective long (the dependent).
The learning strategy described in the remainder of the article takes advantage of
the corequirement structure.
5. System Overview
To evaluate the hypotheses presented above, a software package was developed to
support the automatic acquisition of syntactic and semantic requirements. The sys-
tem is constituted by six main processes, which are displayed as rectangles with solid
lines in Figure 5. They are organized as a linear sequence of data transformations. In
Figure 5, solid ellipses are used to display the data transformed by these processes.
Two local subprocesses (dotted rectangles) build extra data (dotted ellipses), in order
to constrain some of the main transformation processes. In the remainder of this
section, we merely outline the overall functionalities of these processes. Then in
subsequent sections, we describe them in detail.
Tagging and Chunking: Raw text is tagged (Marques and Lopes 2001) and then
analyzed in chunks using some potentialities of the shallow parser introduced in
Rocio, de la Clergerie, and Lopes (2001). This parser is implemented using
tabulation capabilities of the DyALog system (de la Clergerie 2002). The output is a
sequence of basic chunks. For instance, the sentence The President sent the document
to the Minister is analyzed as a sequence of four basic chunks: np, vp, np, and pp.
These chunks contain neither dependencies nor recursivity.
Attachment Heuristic RA: An attachment heuristic based on right association
(RA) is applied to chunk sequences in order to combine pairs of chunks. The
headwords of two related chunks form a syntactic dependency. Section 6.1
describes some properties of the dependencies extracted using the RA strategy.
Extractor of Position Vectors: Dependencies are used to extract syntactic
positions, which are internally characterized as vectors of word frequencies. This
process is described in section 6.2.
122
Figure 3
Dependency with corequirements.
Computational Linguistics Volume 31, Number 1
123
Clustering 1: Position vectors are compared with one another using a specific
similarity measure. Pairs of similar positions are put in basic clusters. A basic
cluster is constituted by two similar positions whose features are the words they
share. Section 7.1 describes this process.
Clustering 2: Basic clusters are successively aggregated using a conceptual
clustering methodology to induce more-general classes of positions. A corpus-
based thesaurus, built on the basis of the extracted dependencies, is used to
constrain cluster aggregation. We present this process (together with the thesaurus
design subprocess) in section 7.2.
Attachment Heuristic CR: Finally, the resulting clusters are used to parse again
the chunks and propose new dependencies (section 8). This is accomplished in two
steps. First, a lexicon builder module organizes the information underlying the
Figure 4
Corequirements in long dinner.
Figure 5
System modules.
Gamallo, Agustini, and Lopes Clustering Syntactic Positions
learned clusters and builds a lexicon with syntactico-semantic corequirements (see
section 8.1). Then, the grammar underlying the parser is provided with a specific
attachment heuristic that uses corequirement (CR) information from the lexicon.
This heuristic allows the parser to propose a new set of dependencies (section 8.2).
We evaluate the resulting dependencies in section 8.3.
The system was implemented on two different Portuguese text corpora: PGR3 and
EC.4 The experiments that were conducted are described and some results given in
section 7.3.
6. Extracting Dependencies and Positions
In this section, we describe two modules of the method: the attachment heuristic RA
and the extractor of position vectors. These modules involve the extraction of
candidate binary dependencies and syntactic positions.
6.1 Attachment Heuristic RA
Attachment heuristic RA takes as input parses constituted by sequences of chunks. It
uses the right-association strategy. That is, a new chunk is preferentially attached to
the preceding chunk. The headwords of two attached chunks form a possible binary
dependency. Consider the expression
::: a lei citada em o anterior parecer::: ?the law cited in the previous opinion?
?13?
The RA heuristic allows us to identify three candidate dependencies, which are
illustrated in the left column of Table 3. These dependencies are not considered at this
point to be actual attachments, but only potential candidates. Later, the parser will be
provided with the learned requirements stored in the lexicon, in order to propose new
dependencies, which will be the output of the parsing strategy. Note that lobj denotes a
nominal object appearing to the left of the verb. This cannot be identified with the
subject grammatical function. The order of verbal objects is not the main feature by
means of which to identify the subject and direct-object functions in Portuguese (and
in most Latin languages). The position of verb complements is quite free in these
languages. We consider then that grammatical functions are semantic-dependent
objects, since we need semantico-pragmatic knowledge to identify them.
In addition, we also provide some dependencies with specific morpho-syntactic
information. For instance, verb citar (to cite) is annotated using the past participle
(vpp) tag. This morpho-syntactic information is relevant for defining the semantic
requirements of dependencies. As we show later, only semantic information enables
us to consider the dependency underlying a lei citada (the law that was cited) as
being semantically similar to the one underlying citar a lei (to cite the law). These
dependencies are not merely merged into one single relation using morpho-syntactic
rules. Such rules pose some important problems: First, they require specific knowledge
124
3 PGR (Portuguese General Attorney Opinions) consists of case law documents.
4 EC (European Commission) contains documents on different aspects (legislation in force, social policy,
environment, etc.) of the European Commission. This corpus is available at http://europa.eu.int/eur-lex/
pt/index.html.
Computational Linguistics Volume 31, Number 1
125
on particular languages; and second, they introduce a great amount of noise. In our
approach, these two dependencies are merged in one cluster only if our learning
process provides us with semantic evidence to justify such merging. In fact, one of the
objectives of our learning method is to use information on semantic requirements for
identifying morpho-syntactic alternations of dependencies: for example, citadapelo
ministro/o ministro citou (cited by the Minister/the Minister cited); mencionar a
lei/mencionou-se a lei (to mention the law/the law was mentioned); ratificar a
lei/ratificac$a? o da lei (to ratify the law/ratification of the law). If two morpho-
syntactic alternations are considered to share the same semantic requirements, then
they will automatically occur in the same cluster. This strategy allows us to reduce
language-dependent knowledge to a minimum.
It is also worth noticing that tag pre in Table 3 is used to annotate adjectives in the
left position with regard to the modified noun (i.e., in the mod relation). We distinguish
three different adjective relations: the left modifier, the right modifier, and the
prepositional object. It is assumed here that these three dependencies can stress
different semantic aspects of an adjective. For instance, our strategy led us to learn that
anterior (previous) is semantically similar to primeiro ( first) and seguinte
( following) when it takes the role of left modifier. However, when the adjective is to
the right of a noun and is followed by a prepositional object (anterior a, previous to),
it is clustered together with inferior (inferior) and igual (equal), which also appear
within prepositional dependencies: equal to, inferior to.
Since the right-association strategy is knowledge-poor, the attachments it proposes
give rise to a substantial amount of odd syntactic dependencies (25%), including those
caused by POS-tagging errors. The overall precision of the tagger is 96.2%. Yet
considering only those tags we use in the learning strategy (i.e., nouns, adjectives,
articles, verbs, etc.), the precision is close to 90%. To overcome such a noisy input, we
need a reliable learning method.
6.2 Position Vectors
Given that each dependency contains two complementary grammatical locations
(head and dependent), we split dependencies into two syntactic positions: the position
associated with the head (or down) location and the one associated with the dependent
(or up) location. The positions extracted from expression (13) are illustrated in the right
column of Table 3. Following the assumption on corequirements, each position must
be provided with a particular linguistic requirement.
We represent each syntactic position as a feature vector. Each feature corresponds
to a word occurring in the position. The value of the feature is the frequency of the
Table 3
Binary dependencies and syntactic positions extracted from expression (13).
Binary dependencies Syntactic positions
(lobj; citar : vpp,, leij)
(be cited, law)
blobj_down, citar : vpp?
blobj_up, lei?
(iobj_em; citar : vpp,, parecerj)
(be cited in report)
biobj_em_down, citar : vpp?
biobj_em_up, parecer?
(mod; parecer,, anterior : prej)
(opinion, previous)
bmod_down, parecer?
bmod_up, anterior : pre?
Gamallo, Agustini, and Lopes Clustering Syntactic Positions
word in that position. A position is thus defined by means of its word distribution. As
has been mentioned before, those words appearing in a particular position can be used
to represent, in extensional terms, a first approximation of the semantic condition the
position requires (i.e., its selection restrictions). Clustering enables us to enlarge the
scope of each condition. In Table 4, we illustrate the word distribution of the two
complementary positions underlying citada no parecer (be cited in the report).
Note that those words occurring once in a position are also considered as features.
This allows us to minimize the data sparseness problem. Linguistic corpora are sparse
in the sense that most co-occurrences occur few times in a given corpus. So, if co-
occurences with lower frequencies were not used by the learning strategy, pertinent
linguistic information would be missing, and coverage would remain low. In order to
minimize missing information and coverage reduction, we retain infrequent words in
position vectors.
Nevertheless, taking into account infrequent co-occurrences increases noise and
thus may disturb the learning task. There are a number of noise sources: words
missing from the dictionary, words incorrectly tagged, wrong attachments, etc. The
position shown in the first line of Table 4 occurs with at least two words that are not
syntactically required: apoio (support) and sentido (sense).5 Note that these words
have frequency 1 in that position. Retaining requirements with frequency 1 enables us
to retain other words that are syntactically and semantically appropriate for that
position, such as artigo (article) and regulamento (regulation), which also occur
only once. The next step of our method (Clustering 1) focuses on the automatic
removal of odd features introduced in position vectors.
7. Clustering of Positions
Positions that share similar features are combined into clusters. Clustering is divided
into two different agglomerative processes: Clustering 1 and Clustering 2.
7.1 Clustering 1
Clustering 1 builds pairs of similar positions called basic clusters. A basic cluster is the
result of merging two positions considered to be similar. The features associated with a
basic cluster are only those words appearing in both similar positions. This allows us
126
Table 4
Two position vectors.
biobj_em_down, citar : vpp?
(be cited in [_ ])
(nota 53) (parecer 7) (conclusa? o 3) (informac$a? o 2)
(regulamento 1) (artigo 1) (apoio 1) (sentido 1)
note, report, conclusion, information, regulation, article, support, sense
biobj_em_up, parecer? (afirmar:vpp 9) (defender:vpp 7) (citar:vpp 7)
(analisar:vpp 7) (escrever 3) (reafirmar:vpp 2)
(esclarecer:vpp 1) (notar:vpp 1) (publicar:vpp 1)
(concluir:vpp 1) (assinalar:vpp 1)
([_ ] in the report) be affirmed, be defended, be cited, be analyzed, writer, be affirmed again,
be clarified, be noted, be published, be concluded, be pointed out
5 Word sentido (sense) appears in that position, not as a verb complement, but as a member of the
prepositional locution no sentido de (in the sense that), which is attached to the whole sentence.
Computational Linguistics Volume 31, Number 1
127
to filter out odd features from clusters. Features defining a basic cluster are, then, the
most reliable fillers of the semantic condition imposed by the two similar positions.
Those words that are not required by both positions are removed from the cluster. The
algorithm of this process is the following:
Similarity: We calculate the similarity between each pair of positions. To do this,
we measure the distance between their word distributions (see the details of this
measure below).
Selection: Then, for each position, we select the N (where N = 20) most similar
ones.
Aggregation: Then, given a position and the list of N most similar positions, we
merge the position with each member of the list. So, given a position, we create N
aggregations of that position with one similar position.
Filtering: Finally, for each aggregation of two similar positions, we select the
intersection of their features; that is, the features of a basic cluster are those words
that appear in both positions.
As a result of this process, we obtain a set of basic clusters, each augmented by reliable
features. The aim is to automatically filter out noisy features from each pair of similar
syntactic positions. Many incorrectly tagged words are removed at the filtering step.
Let?s take an example. Consider the position shown in the first row of Table 4, that
is, biobj_em_down, citar : vpp?. According to our similarity measure, its word
distribution is similar to that of the following positions:6
biobj em down, mencionar : vpp? biobj em down, cite?
biobj em down, assinalar : vpp? bde down, leitura?
biobj em down, referir : vpp? biobj em down, referenciar : vpp?:::
?14?
Then, biobj_em_down, citar : vpp? is merged with each one of the above positions. Note
that this position is similar to the position associated with the active form, citar.
Finally, each pair of similar positions (i.e., each basic cluster) is defined by the words
they have in common. For instance, take the basic cluster shown in (15):
fbiobj em down, citar : vpp? ? biobj em down, mencionar : vpp?g ?
nota conclusa?o informac
$
a?o artigo
?note, conclusion, information, article?
Looking at those words appearing as prepositional objects of both cited in [_ ] and
mentioned in [_ ], one can see that they are semantically homogeneous. The filtered
features no longer include odd words such as support and sense (see Table 4). Indeed,
6 English translation of (14): mencionar = be mentioned in [_ ], cite = cite in [_ ], assinalar = be pointed out in [_ ],
leitura = reading of [_ ], referir = be referred to in [_ ], referenciar = be made reference to in [_ ].
?15?
Gamallo, Agustini, and Lopes Clustering Syntactic Positions
the process of selecting the words shared by two similar positions relies on the
contextual hypothesis stated above in section 3.4, as well as on the following corpus-
based observation: Those words whose appearance in a particular position would be
incorrect are not likely to occur in similar positions.
Merging two similar positions by intersecting their features allows a semantic
condition to be associated with two positions. In (15), a single set of words is associated
with the two positions, since the positions have in common the same semantic
condition (or selection restrictions). However, the scope of the condition is still too
narrow: It merely embraces two positions. In order to extend the scope of semantic
conditions, we cluster them using a less restrictive clustering process that allows us to
build more general classes of words and positions.
Before explaining the following process (Clustering 2), let us describe the measure
used to calculate the similarity between syntactic positions. We use a particular
weighted version of the Lin (1998) coefficient. Our version, however, does not use
pointwise mutual information to characterize the weight on position-word pairs. As
Manning and Schu?tze (1999) argued, this does not seem to be a good measure of the
strength of association between a word and a local position. When the similarity
between two positions is computed using our method, higher scores are assigned to
rare attributes (i.e., words in our case) of compared objects (positions). By contrast, the
pointwise mutual information measure is not sensitive to the fact that frequent pairs
can have a strong association. In order to resolve this problem, we use a weight very
similar to that proposed in Grefenstette (1994). Consequently, we employ, on the one
hand, the general structure of the Lin coefficient, and on the other, the weight
proposed by Grefenstette.
Words are weighted considering their dispersion (global weight) and their con-
ditional probability given a position (local weight). The weight Assoc, measuring the
degree of association between word w and position p, is computed by equation (16):
Assoc?p,w? ? log2?PMLE?wjp??  log2?disp?w?? ?16?
On the other hand, the conditional probability PMLE is estimated by using the
maximum likelihood estimate (MLE), which is calculated in (17):
PMLE?wjp? ?
f ?p, w?
F?p? ?17?
where f ?p, w? represents the frequency of word w appearing in position p, and F?p? is
defined, for a particular position, as the total sum of its word frequencies: ~i f ?p, wi?.
On the other hand, word dispersion, disp, is defined as the following mean:
disp?w? ? F?w?
number of positions f or w
?18?
where F?w? is defined as the total sum of position frequencies of w: ~i f ?pi, w?. Higher
values are assigned by equation (18) to those words that are not dispersed, that is, to
128
Computational Linguistics Volume 31, Number 1
129
those words frequently appearing in few positions. disp measures the ability of a word
to be semantically selective with regard to its positions. So the Lin coefficient (LIN)
between two positions is computed using equation (19):
LIN?p1, p2? ?
P
fw:M?p1, w?,M?p2, w?g
?Assoc?p1, w? ? Assoc?p2, w??
P
fw:M?p1, w?g
Assoc?p1, w? ?
P
fw:M?p2, w?g
Assoc?p2; w?
?19?
In the numerator of (19), the condition of the summation indicates that each word w
must be found with both positions p1 and p2. In the denominator, w varies over all
words found in p1 and p2.
7.2 Clustering 2
Basic clusters are the input objects of the second process of clustering. We use an
agglomerative (bottom-up) clustering for aggregating basic clusters into larger ones.
The clustering algorithm is described in Table 5. According to this algorithm, two
objects are clustered if they satisfy the following restrictions: (1) they have the same
number of features (i.e., words), (2) they share more than 80% common features, and
(3) the features that are different must be thesaurically related to at least one of the
common features. In order to provide words with thesaurical relations, we
automatically build a thesaurus of similar words. Details of the thesaurus design are
given in section 7.5.
Figure 6 shows how two basic clusters are merged into one more general class of
positions. For two basic clusters such as CL_00013, which contains the features note,
article, dispatch, document, and text, and CL_03202, whose features are article, dispatch,
document, text, and opinion, we obtain the more general cluster CL_04447, which is
constituted by all the different positions and words of its basic components. Note that
the two basic clusters are different with regard to two features: note and opinion.
According to our clustering restrictions, the two clusters can be merged if each
different feature (i.e., note and opinion) is thesaurically related to at least one of the
common features: article, dispatch, document, and text. A word is thesaurically related to
another if it belongs to the list of most similar words, a list that was automatically
generated and entered in our thesaurus. The thesaurus, then, is used to control and
constrain the construction of abstract classes of positions. In addition, the larger class,
CL_04447, allows us to induce collocation data that does not appear in the corpus. For
instance, we induce that the word parecer (opinion) may appear in position biobj_em,
mencionar : vpp? (mentioned in [_ ]). Similarly, we also learn that word nota (note) can
occur with biobj_em, referenciar : vpp? (made reference to in [_ ]).
7.3 Tests and Results
We tested our learning strategy over two training corpora, PGR and EC.7 Data
concerning the information extracted from these two corpora are presented in Table 6.
The clusters generated by Clustering 2 are used to build a lexicon of words along
with their syntactic and semantic requirements. Each corpus has its own lexicon. Later,
in section 8.1, we describe how this information is stored in the lexicon entries.
7 Some results can be consulted at http://di165.di.fct.unl.pt/~agustini/restr_web.
Gamallo, Agustini, and Lopes Clustering Syntactic Positions
Learned clusters represent linguistic requirements that cannot be reduced to a
smaller set of general syntactico-semantic roles, such as Agent, Patient, Theme, and
Instrument. On the other hand, they cannot be associated with word-specific roles like,
for instance, Reader, Eater, and Singer. The level of elaboration of these clusters ranges
from very abstract to very specific lexical levels. They are situated, in fact, at the
domain-specific level, which is considered more appropriate for use in computational
tasks (Gildea and Jurafsky 2002). However, given the too-restrictive constraints of the
algorithm, the clustering method also overgenerates redundant clusters. In future
work, we will attempt to reduce redundancy using clustering algorithms based on
concept lattices (Kovacs and Baranyi 2002).
130
Table 5
Algorithm of Clustering 2.
Input Set of basic clusters organized by number of features.
Output A list of larger clusters representing classes of semantic conditions.
Step 1 Prerestrictions on candidates to be clustered
For each obj, select those objects that
& have the same number of features than obj
AND
& share at least 80% of features
Step 2 Similarity restrictions
From candidates extracted in step 1, take those objects
& that share all features with obj
OR
& the different features of which are related by a thesaurus
Step 3 Merging objects and their features
obj is merged with all objects filling the conditions stated in steps 1 and 2.
The new object has the following properties:
& It is constituted by the union of the features defining the merged objects.
& It is put together with objects having the same number of features.
Iteration Repeat steps 1, 2, and 3, increasing the number of features, until no cluster fulfills
the restrictions.
Figure 6
Clustering 2.
Computational Linguistics Volume 31, Number 1
131
In order to evaluate the linguistic relevance of these clusters, we check in section 8
whether they are useful in a parsing task. The degree of efficiency in such a task
(parsing) may serve as a reliable evaluation for measuring the soundness of the
learning strategy.
7.4 Related Clustering Methods
There are other approaches to acquisition of word senses by clustering words
according to context-sensitive information. Similarly to our work, these approaches
assume, on the one hand, that a word can appear in different clusters (soft clustering),
and on the other hand, that each cluster represents a particular sense distinction of the
words that are elements of it. Different clustering methods can be distinguished.
First, some methods compare the similarity between pairs of syntactic positions
(and not pair of words) in order to generate clusters of syntactic positions, whose
features are set of words (Allegrini, Montemagni, and Pirrelli 2003; Faure and Ne?dellec
1998; Reinberger and Daelemans 2003). Similarly to our approach, these methods
follow both the relative view on word similarity and the assumption on contextual
word sense, which were introduced above in sections 3.3 and 3.4, respectively.
However, these methods differ from ours in several aspects. That of Reinberger and
Daelemans (2003) does not use any kind of filtering process. So given a cluster of
positions, the set of its features is basically defined as the union of their co-occurring
words. This method turns out not to be appropriate when extracted co-occurrences are
noisy. The cooperative system Asium presented in Faure and Ne?dellec (1998) filters
out incorrect words from clusters of positions. However, unlike in our work, this task
is not automatic. It requires manual removal of those words that have been incorrectly
tagged or analyzed. Similarly to our approach, Allegrini, Montemagni, and Pirrelli
(2000) developed an automatic procedure to remove odd words from clusters. It
consists in defining a first clustering step in which positions are aggregated in basic
clusters, which are called substitutability islands. As in Clustering 1 (section 7.1), each
basic cluster selects only those words occurring in all positions of the cluster.
However, Allegrini, Montemagni, and Pirrelli (2000) define a second clustering step
involving significant differences with regard to our Clustering 2. Given a position p,
they define a list of basic clusters containing p. This list is ranked and then used as the
input to a clustering strategy that aggregates only basic clusters belonging to that list.
So a cluster containing p cannot be aggregated with a cluster that does not contain p.
This is a very strong constraint. It reduces significantly the system?s ability to make
generalizations.
Second, other methods discover word senses by clustering words according to the
similarity of their whole distributions (Pantel and Lin 2002; Lin and Pantel 2001). These
Table 6
Corpus data.
Corpus PGR Corpus EC
Word occurrences 6,643,579 3,110,397
Binary dependencies 966,689 487,916
Syntactic positions 178,522 113,847
Basic clusters 370,853 166,886
Clusters (Clustering 2) 16,274 10,537
Gamallo, Agustini, and Lopes Clustering Syntactic Positions
methods, then, follow both the absolute view on word similarity and Harris?s
distributional hypothesis, which we introduced in section 2.3. However, in order to
make the absolute view more relative, a collection of small and tight clusters (called
committees) is proposed in a first step. These tight clusters are supposed to represent
different word senses. Then in a second step, each word is assigned to a set of
committees.
Finally, Pantel and Lin (2000) offer a hybrid method based on the two basic views
on semantic similarity, absolute and relative. Given a word w occurring in position p,
in any pair of type bverb, function? or bnoun, preposition?, the system, in a first step,
generates classes of contextually similar words. A contextual class results from the
intersection of the words occurring in p and the words similar to w. The definition of a
contextual class contains the two views on word similarity. On the one hand, the
words occurring in p are called the cohorts of w. The cohorts are similar to w only with
regard to position p (relativized view). On the other hand, a corpus-based thesaurus is
used to select words similar to w with regard to its whole position distribution
(absolute view). Note that a contextual class is not far from what we call a basic cluster.
In a second step, contextual classes are used to compute attachment association scores.
The aim of the method is not to discover word senses (as in the methods outlined
above), but to solve syntactic ambiguities. No clustering strategy is proposed to
generate more general contextual senses.
Our system could also be considered a hybrid method, since besides the contextual
hypothesis and the relative view, we also take into account the absolute view on word
similarity to design a corpus-based thesaurus.
7.5 Automatic Thesaurus Construction
Clustering 2 uses a thesaurus of similar words to avoid undesirable aggregations. To
design a corpus-based thesaurus, we follow the absolute view on word similarity: The
similarity between two words is computed by comparing their whole context
distribution. Our thesaurus is not specifically designed to be involved in the clustering
process. It is designed primarily with the aim of measuring the discriminative
capabilities of syntactic positions defined on the basis of corequirements (Gamallo
et al 2001). In particular, we check whether corequired positions are semantically
more selective than those used by Grefenstette (1994), which were defined in terms of
simple requirements. Experimental tests showed that corequirements permit a finer-
grained characterization of ??meaningful?? syntactic positions.
To compute word similarity, we used the weighted version of the binary Jaccard
measure defined in Grefenstette (1994). The weighted Jaccard similarity (WJ) between
two words, w1 and w2, is computed by
WJ?w1, w2? ?
P
i min?Assoc?w1, pi?, Assoc?w2, pi??P
i max?Assoc?w1, pi?, Assoc?w2, pi??
?20?
In (20), the weight Assoc is the result of multiplying a local and a global
weight, whose definitions are analogous to those given in formulas (17) and (18).
The major difference is that in (20), positions are taken as attributes and words as
objects.
We designed a particular thesaurus for each training corpus. As regards the
PGR corpus, we produced 42,362 entries: 20,760 nouns, 16,272 verbs, and 15,330
132
Computational Linguistics Volume 31, Number 1
133
adjectives. For each entry w, the thesaurus provides a list containing the 20 words most
similar to w. This is the list that was later used in the clustering process.
8. Application and Evaluation
The acquired classes are used to solve attachment ambiguities. For this purpose, first, a
lexicon is designed by using the linguistic information contained in the learned
clusters. Then a particular heuristic uses this information to propose correct
attachments. Some experiments are performed on two text corpora. The results are
evaluated in section 8.3.
Table 7
Excerpt of entry secreta?rio (secretary) (in the PGR corpus).
secreta? rio (secretary)
SUBCAT
& bde_up, secreta?rio? ([_ ] of secretary) =
cargo, carreira, categoria, compete? ncia, escala? o, estatuto, funa$a? o,
remuneraca? o, trabalho, vencimento
(post, career, category, qualification, rank, status, function, remuneration, job, salary)
& bde_down, secreta?rio? (secretary of [_ ]) =
administrac$a? o, assembleia, autoridade, conselho, direcc$a? o, empresa,
entidade, estado, governo, instituto, juiz, ministro, ministe? rio,
presidente, servic$o, tribunal o? rga? o
(administration, assembly, authority, council direction, company, entity, state, government, institute,
judge, minister, ministery, president, service, tribunal organ)
& biobj_a_up, secreta?rio? ([_ ] to the secretary) =
aludir, aplicar:ref1, atender, atribuir, concernir, corresponder,
determinar, presidir, recorrer, referir : ref1, respeitar
(allude, apply, attend, assign, concern, correspond, determine, resort, refer, relate)
& biobj_a_up, secreta?rio? ([_ ] to the secretary) =
caber, competir, conceder:vpp, conferir, confiar:vpp, dirigir:vpp,
incumbir, pertencer
(concern, be incumbent, be conceded, confer, be trusted, be sent, be incumbent, belong)
& biobj_por_up, secreta?rio? ([_ ] by the secretary) =
assinar:vpp, conceder:vpp, conferir:vpp, homologar:vpp, louvar:vpp,
subscrito
(be signed, be conceded, be conferred, be homologated, be complimented, subscribe)
& blobj_up, secreta?rio? (the secretary [_ ]) =
definir, estabelecer, fazer, fixar, indicar, prever, referir
(define, establish, make, fix, indicate, foresee, refer)
SENSE
& administrac$a? o, assembleia, autoridade, chefe, comandante, comissa? o,
conselho, director, direcc$a? o, entidade, estado, funciona? rio, gabinete,
governador, governo, instituto, juiz, membro, ministro, ministe? rio,
presidente, provedor, secreteria, secreta? rio, senhor, servic$o, tribunal,
o? rga? o
(administration, assembly, authority, chief, commander, commission, council, director, direction, entity,
state, official, cabinet, governor, government, institute, judge, member, minister, ministry, president,
purveyor, secretary, secretary, mister, service, tribunal, organ)
& primeiro-ministro, autoridade, entidade, estado, membro, ministro,
ministe? rio, presidente, secreta? rio
(prime minister, authority, entity, state, member, minister, ministry, president, secretary)
Gamallo, Agustini, and Lopes Clustering Syntactic Positions
8.1 Design of a Lexicon with Corequirements
The learning method provides a lexicon with syntactic and semantic information. A
word entry is divided into two types of information (see Table 7). SUBCAT is the
repository of syntactic and semantic requirements. SENSE contains the different word
sets to which the entry belongs. Each word set corresponds to a particular sense
distinction. However, only the SUBCAT information is used here for the purpose of
attachment resolution. Table 7 shows an excerpt of entry secreta? rio (secretary). This
entry is associated with a SUBCAT repository with six requirements and a SENSE
repository containing two word senses.
The word secreta? rio requires two nominal and four verbal arguments.
Concerning the nominal positions, we learn that secretary selects for nouns such as
post or rank in the de_up location, whereas it requires a class of nouns denoting
institutions or functions in the de_down location. Concerning the verbal positions, we
also learn that secretary requires various verb classes in different verbal positions: two
classes in location iobj_a_up, one class in iobj_ por_up, and one more in lobj_up.
A syntactic pattern of subcategorization arguments underlies the organization of
the SUBCAT repository in Table 7. This pattern can be represented as follows:
?Xv aprep an?vp _ ?Yv porprep an?vp _ ?Zn deprep an?np _ ?an deprep Wn?np _ ?an Uv?vp ?21?
where X, Y, Z, . . . stand for variables of subcategorized words, while a is the
subcategorizer. If a is secreta? rio, then the specific values of X, Y, Z, . . . can be found
in Table 7. For example, according to Table 7, the noun cargo instantiates Z, while the
verb pertencer instantiates X. The symbol ? stands for Boolean disjunction. We take
into consideration that at least in Portuguese, all word arguments are optional. Even
the subject of a verb may be omitted. Note, however, that the syntactic pattern in (21)
does not allow it to be distinguished whether arguments are compatible or not. For
instance, it is not able to predict that (Yv porprep an)vp and (an Uv)vp are argument
positions that cannot appear in the same sentence. Moreover, there are no restrictions
on the linear order of arguments. As we do not learn this type of syntactic information,
the pattern depicted in (21) can be viewed merely as a set of potential arguments of a
word. So our method does not allow a set of entirely organized subcategorization
frames to be captured for each word.
Note that it is the corequirement structure that allows us to acquire a great number
of requirement positions that are not usual in most standard approaches. Five
positions of secretary require not standard dependent complements, but different types
of heads. This is a significant novelty of our approach. Consider the positions that
impose nonstandard requirements (i.e., nonstandard predicates). According to the
standard definition of predicate given in section 4.2.1 (simple requirement definition),
only locations robj_down, lobj_down, and mod_up give rise to positions with
requirements.8 By contrast, positions defined by the complementary locations (robj_up,
lobj_up, mod_down) are considered mere complements of verbs or objects modified by
adjectives. So they cannot impose any requirement, and thereby they are not
semantically defined as predicates. In opposition to this viewpoint, our system learns
more classes of requirements imposed by positions considered nonstandard predicates
(5,192) than classes of requirements imposed by positions considered standard
134
8 Positions with prepositions are not taken into account in this analysis because they are ambiguous.
Computational Linguistics Volume 31, Number 1
135
predicates (4,600). These experimental results seem to prove that nonstandard
predicates correspond to positions with requirements. In sum, we may infer that
binary dependencies are structured by corequirements.
Consider now the SENSE repository in Table 7. It contains two word sets which
should represent two senses of secreta? rio. Unfortunately, our clustering algorithm
generates some redundancy. In this case, the two clusters should have been merged
into one, since they seem to refer to the same concept. Cluster redundancy is the major
problem of our learning strategy.
8.2 Attachment Heuristic CR
The syntactic and semantic requirements provided by the lexical entries are used to
improve a parser and the DCG grammar it is based on. The description of the parser
remains beyond the scope of this article; it has been described in Rocio, de la Clergerie,
and Lopes (2001). Details of a symbolic DCG grammar with information on linguistic
corequirements can be found in Gamallo, Agustini, and Lopes (2003). In this article, we
only outline how the grammar uses this information to resolve syntactic attachments.
Corequirements are at the center of attachment resolution. They are used to
characterize a particular heuristic on syntactic attachment. This heuristic referred to
as CR, is supposed to be more precise than RA. It states that two chunks are
syntactically and semantically attached only if one of the following two conditions is
verified: Either the dependent is semantically required by the head or the head is
semantically required by the dependent. Take the expression
:::compete a o secreta?rio::: ?is incumbent on the secretary? ?22?
This expression is analyzed as a vp?pp construction only if at least one of the two
following requirements is satisfied:
Down requirement: The context biobj_a_down, competir? (be-incumbent on [_ ])
requires a class of nouns to which secreta? rio (secretary) belongs.
Up requirement: The context biobj_a_up, secreta?rio? ([_ ] on secretary) requires a class
of verbs to which competir (be incumbent) belongs.
Corequirements are viewed here as constraints on the syntactic rules of a
symbolic grammar. Attachments are then solved by using Boolean, and not purely
probabilistic, constraints. According to the lexical information illustrated in Table 7,
expression (22) can be analyzed as a vp?pp construction because at least the up
requirement is satisfied. Note that even if we had no information on the verb re-
quirements, the attachment would be allowed, since the noun requirements in the
dependent (up) location were learned. So we learned that the noun secreta? rio
has as argument the verb competir in location biobj_a_up?. As we show in
the evaluation procedure, corequirements are also used to resolve long-distance
attachments.
8.3 Evaluating Performance of Attachment Resolution
We evaluated the performance of CR, that is, the attachment heuristic based on
Boolean corequirements. The general aim of this evaluation was to check whether the
Gamallo, Agustini, and Lopes Clustering Syntactic Positions
linguistic requirements we learned were adequate for use in a parsing task. The degree
of efficiency in such a task may serve as a reliable evaluation for measuring the
soundness of our learning strategy.
8.3.1 Test Data. The test data consisted of sequences of basic phrases (i.e., chunks).
The phrase sequences selected belong to three types: vp?np?pp, vp?pp?pp, and np?pp?
pp. They were randomly selected from two different (and already chunked) test
corpora: a group of 633 sequences was selected from the EC corpus and another group
of 633 was selected from PGR. Each group of 633 sequences was constrained to have
three equal partitions: 211 vp?np?pp sequences, 211 vp?pp?pp sequences, and 211 np?
pp?pp sequences. The test corpus from which each group was selected had previously
been separated from the training corpus, so the sequences used for the test were
excluded from the learning process. Then the annotators (the coauthors) manually
proposed the correct attachments for each phrase sequence, using the full linguistic
context. Some specific instructions were given to the annotators for the most
controversial cases. The following excerpts from these instructions are illustrative:
(1) if a pp seems to be a modifier of the verb, then it is attached to the vp; (2) if a pp is a
modifier of the sentence, no attachment is proposed; (3) if an np following a vp is either
the direct object or the subject of the verb, then the np is attached to the vp; (4) if a pp
seems to be attached to two phrases, two attachments are proposed (we retain the
ambiguity); (5) if a phrase contains a word that was not correctly tagged, no
attachment is proposed. Note that verbal modifiers and verbal complements are
treated in the same way (see subsection 4.2.2). Moreover, we consider a robj (i.e., an np
following a vp) as being able to be instantiated by two different functions: both a direct
object and a subject (section 6.1).
Most works on attachment resolution use as test data only phrase sequences of
type vp?np?pp (Sekine et al 1992; Hindle and Rooth 1993; Ratnaparkhi, Reymar, and
Roukos 1994; Collins and Brooks 1995; Li and Abe 1998; Niemann 1998; Grishman and
Sterling 1994). These approaches consider each sequence selected for evaluation as
having the potential to be syntactically ambiguous in two ways. For instance, the
seuqence of chunks
?VP cut?NP the potato?PP with a knife ?23?
can be elaborated either by the parse
?VP cut?NP the potato?PP with a knife ?24?
which represents a syntactic configuration based on proximity (phrase2 is attached to
phrase1 and phrase3 is attached to phrase2), or by
?VP cut?NP the potato?PP with a knife ?25?
which is here the correct configuration. It contains both a contiguous and a
long-distance attachment: phrase2 is attached to phrase1 and phrase3 is attached to
phrase1.
136
Computational Linguistics Volume 31, Number 1
137
We consider, however, that the process of attachment resolution can be
generalized to other syntactic sequences and ambiguity configurations. On the one
hand, we evaluated not only one, but three types of phrase sequences: vp?np?pp,
vp?pp?pp, and pp?pp?pp. On the other hand, these sequences cannot be reduced to only
two syntactic configurations (two parses). They can be syntactically ambiguous in
different ways. These ambiguities are introduced by adjective arguments and sentence
adjuncts (see Table 8).
Table 8 shows phrase sequences that cannot be analyzed by means of the two
standard configurations underlying parses (24) and (25). None of the sequences in that
table match the two standard configurations. For instance, a o decreto (to the decree),
which is the phrase2 of the first example, is not attached to the head of phrase1, but to
the adjective relativo (referring). Similarly, in the second expression, a o citado
diploma (to the referred diploma) is attached to the adjective anexos (appended) and not
to the head of phrase2. Subcategorization of adjectives introduces a new type of
structural ambiguity, which makes it more difficult to make attachment decisions.
Finally, in the third sequence, em a medida (insofar as) is the beginning of an adverbial
sentence, so it is not attached to one of the individual phrases but to the whole
previous sentence. In sum, resolving structural ambiguity cannot be reduced to a
binary choice between the two configurations depicted in (24) and (25). We return to
this matter below.
Another important property of test data is that they contain incorrectly tagged
words. We do not remove these cases, since they can give us significant information
about how (in)dependent of noisy data our learning method is.
8.3.2 The Evaluation of Protocol. Each sequence selected from the test corpus con-
tains three phrases and two candidate attachments. So given a test expression, two
different attachment decisions are evaluated:
Decision A: Is phrase2 attached to phrase1, or not attached at all?
Decision B: Is phrase3 attached to phrase2, attached to phrase1, or
not attached at all?
As we selected 633  2 test expressions, and each expression implicitly contains
two attachment decisions, the total number of decisions that we evaluated was 2,532.
By contrast, in most related approaches, test expressions are ambiguous in only two
senses: phrase3 is attached to either phrase2 or phrase1. Such approaches do not consider
Table 8
Different types of syntactic sequences and various types of syntactic ambiguities.
np?pp?pp [np o artigo relativo] [pp a o decreto] [pp de a lei]
(the article referring to the decree-law)
vp?pp?pp [vp publicou] [pp em os estatutos anexos] [pp a o citado diploma]
(published in the statutes appended to the referred diploma)
vp?np?pp [vp tem] [np acesso] [pp em a medida]
(has access insofar as)
Gamallo, Agustini, and Lopes Clustering Syntactic Positions
the attachment between phrase2 and phrase1. So in these approaches, Decision A is not
taken into account. Moreover, they do not evaluate those cases in which phrase3 is not
attached to phrase2 or to phrase1. In sum, only one decision per expression is evaluated,
namely, the decision concerning the PP-attachment. This type of evaluation, however,
is not appropriate to measure the capability of the system to identify the nonstandard
structural ambiguities described above (section 8.3.1). For instance, we expect the
system not to propose that the pp ao diploma (to the [referred] diploma) is attached to
the previous np, headed by estatutos, in the second example of Table 8. The correct
decision is to propose no attachment between the pp (phrase3) and either of the two
previous phrases taking part in the sequence vp?pp?pp. The attachment is actually to a
word, namely, the adjective anexo, which is not a direct constituent of the abstract
sequence vp?pp?pp.
Another important aspect of the evaluation protocol is that CR overgenerates
attachments. There are several cases in which the three phrases of a sequence are
semantically related. In those cases, CR often proposes three attachments even if only
two of them are syntactically allowed. For instance, take the following np?pp?pp
sequence:
?npa remunerac$ a?o ?ppde o cargo ?ppde secreta?rio
?the salary concerning the post of secretary? ?26?
which would be correctly analyzed by using the same configuration as in parse (24),
that is,
?npa remunerac$ a?o ?ppde o cargo ?ppde secreta?rio ?27?
Note that there exists a strong semantic relationship between phrase3 (de secre-
ta? rio) and phrase1 (a remunerac$a? o), even if they are not syntactically attached in
(27). Taking into account the semantic requirements stocked in the lexicon (see
Table 7), CR is induced to propose, in addition to the two correct attachments, a
long-distance dependency, which seems not to be syntactically correct in this
particular case. We call this phenomenon attachment overgeneration. When a
sequence contains two semantically related phrases that are not actually syntactically
dependent, CR overgenerates an additional attachment. Attachment overgeneration
was found in ,15% of expressions selected from the test corpus. In order to
overcome this problem, we use a default rule based on right association. The default
rule removes the long-distance attachment and proposes only the two contiguous
ones. This simple rule has an accuracy of more than 90% with regard to the 15% of
sequences containing overgeneration.
From a semantic viewpoint, attachment overgeneration seems not to be a real
problem. The semantic interpretation of sequence (26) needs to account for all
conceptual relations underlying the sequence. So the semantic requirements that
linked secreta? rio to remunerac$a? o (even if they are not syntactically dependent)
are useful for building a semantic representation of the sequence.
8.3.3 Baseline (RA). Concerning the ability to propose correct syntactic attachments,
we made a comparison between CR and a baseline strategy. As a baseline, we used the
138
Computational Linguistics Volume 31, Number 1
139
attachments proposed by right association. For each sequence of the test data, RA
always proposes the configuration underlying parses (27) and (24); that is, phrase2 is
attached to phrase1, and phrase3 is attached to phrase2.
8.3.4 Similarity-Based Lexical Association. We also compared CR to a very different
learning strategy: the similarity-based lexical method (Sekine et al 1992; Grishman and
Sterling 1994; Dagan, Marcus, and Markovitch 1995; Dagan, Lee, and Pereira 1998),
described in section 2.3. We simulated here a particular version of this strategy. First,
we used the log-likelihood ratio as a score of the association between pairs of syntactic
positions and words. We restricted the lexical association procedure to suggest
attachments only in cases in which the absolute value of the ratio was greater than an
empirically set threshold (3.00). Then, in order to generalize from unobserved pairs, a
list of similar words was used to compute nonzero association scores. For this purpose,
the thesaurus described in section 7.5 turned out to be useful.
According to Dagan, Marcus, and Markovitch (1995), the similarity-based lexical
association LAsim between position p and word w is obtained by computing the average
of the likelihood ratios between p and the k most similar words to w:
LAsim?p, w? ?
P k
i? 0 LA?p, wi?
NZ
?28?
where LA?p, wi? is the likelihood ratio between p and one of the k most similar words
to w. NZ represents the number of nonzero values among LA?p, w1?, LA?p, w2?,...,
LA?p, wk?.
Corequirements are also considered. Given dependency (robj; ratificar#, lei")
(ratify the law), we compute the two following lexical associations:
LAsim?brobj down, ratificar?,lei ?
LAsim?brobj up, lei?ratificar?
?29?
The scores of these two associations are taken into account in the evaluation procedure.
In particular, the sum of the two scores (if each of them is greater than the empirically
set threshold) will be used to make a decision on the attachment between an np headed
by lei and a vp headed by ratificar.
8.3.5 Precision and Recall. The evaluation of each attachment decision made by the
system can be
 True positive (tp): The system proposes a correct attachment;
 True negative (tn): The system proposes correctly that there is
no attachment;
 False positive ( fp): The system proposes an incorrect attachment;
 False negative ( fn): The system proposes incorrectly that there is
no attachment.
We refer to both tp and tn as true decisions (td). The evaluation test measures the
ability of the system to make true decisions. As far as our strategy and the similarity-
Gamallo, Agustini, and Lopes Clustering Syntactic Positions
based approach are concerned, a false negative ( fn) is interpreted as the situation in
which the system lacks sufficient subcategorization information to make a decision. By
contrast, the baseline always proposes an attachment.
Taking into account these variables, precision is defined as the number of true
decisions suggested by the system divided by the number of total suggestions. That is:
precision ? td
td ? fp ?30?
Recall is computed as the number of true decisions suggested by the system divided
by all the decisions that have been made (i.e., the total number of ambiguities):
recall ? td
td ? fp ? fn ?31?
To clarify the evaluation procedure, Table 9 displays the different attachment decisions
made on the following test sequence:
?vp assistir ?pp por o representante ?pp de o EstadoMembro
?assisted by the delegate of the MemberState? ?32?
The two correct attachments in (32), proposed by the human annotator, are compared
against the attachment decisions proposed by the three methods at stake: heuristic
with Boolean corequirements, similarity-based lexical association, and right associa-
tion, which is the baseline. Table 9 assesses the two different decisions (A and B) made
by each method. Note that both CR and LAsim take advantage of corequirements.
Indeed, each decision is made after two types of subcategorization information have
been considered: the requirements the dependent word must satisfy and the
requirements that the headword must satisfy.
Decision A concerns the first candidate attachment, that is, the dependency
between [vp assistir] and [pp por o representante]. Let us analyze the behavior
of the three methods. LAsim incorrectly suggests that there is no attachment. The score
of two internal requirements is zero, so the final decision is a false negative: fn. The
system has no information on requirements because on the one hand, the two phrases
at stake do not co-occur in the training corpus, and on the other, co-occurrences of
phrases with similar words were not attested (and then no generalization was
allowed). CR, by contrast, is endowed with the appropriate requirements to correctly
suggest an attachment (tp) between the two phrases, even though they are not attested
in the training corpus. The clustering strategy allowed the system to learn both that
biobj_ por_D, assistir? requires representante and that biobj_ por_H, representante?
requires assistir. Note that in order to suggest the attachment, it is not necessary
that the two complementary requirements be learned. As has been noted in section 8.2,
the learning of only one of them is enough to make the suggestion. Finally, RA also
suggests the correct attachment. Indeed, the two phrases in (32) are related by right
association.
140
Computational Linguistics Volume 31, Number 1
141
As regards Decision B, all three methods correctly suggest that there is an
attachment (tp) between [np o representate] and [pp de o Estado-Membro].
8.3.6 Results. Table 10 reports the test scores in regard to the precision and recall
from the experiments performed. These scores concern three methods, namely RA,
LAsim, and CR, two text corpora (EC and PGR), and three types of phrase
sequences. There are no significant differences between the scores obtained from
corpus EC and those from PGR, CR, for instance, obtains very similar F-scores over
the two corpora. However, there are important differences among the precision
values associated with the three phrase sequences. In particular, the scores on
sequence vp?np?pp are significatively higher than those on the other sequences,
regardless of the method employed. This is motivated by the fact that in most vp?np?
pp sequences (,95%), there is a true attachment between np and vp. So the precision
score achieved by the three methods with regard to this particular attachment
decision is very high. Prepositional-phrase attachments, by contrast, are more
ambiguous, which causes sequences vp?pp?pp and np?pp?pp to be less predictable.
Indeed, such sequences have two prepositional phrases involved in the attachment
decisions.
Table 9
Evaluation of a test sequence.
CR Decision A:
biobj_ por_D, assistir? requires representante: YES
biobj_ por_H, represantante? requires assistir: YES
Result: tp
Decision B:
biobj_ por_D, assistir? requires Estado-Membro: NO
biobj_ por_H, Estado?Membro? requires assistir: NO
bde_D, representante? requires Estado-Membro: YES
bde_H, Estado ? Membro? requires representante: YES
Result: tp
LAsim Decision A:
LAsim (biobj_ por_D, assistir?, representante): 0
LAsim (biobj_ por_H, representante?, assistir): 0
Result: np
Decision B:
LAsim (biobj_ por_D, assistir?, Estado-Membro): 0
LAsim (biobj_ por_H, Estado ? Membro?, assistir): 0
LAsim (bde_D, representante?, Estado-Membro): 136.70
LAsim (bde_H, Estado ? Membro?, representante): 176.38
Result: tp
RA Decision A:
[vp assistir [pp por o representate]]: YES
Result: tp
Decision B:
[pp por o representate [pp de os Estados-Membros]]: YES
Result: tp
Gamallo, Agustini, and Lopes Clustering Syntactic Positions
Concerning the differences among the three methods, Table 11 averages the results
of the three methods over the two corpora and the three phrase sequences. The total
precision of our method (CR) reaches 0.89, that is, four percentage points higher than
LAsim. Note that the precision value of LAsim is not far from the values reached by other
approaches to attachment resolution based on the similarity-based lexical association
strategy. For instance, the method described in Grishman and Sterling (1994) scores a
precision of ,0.84. Concerning recall, CR also reaches a level of precision that is four
points higher than that achieved by LAsim. This entails that on the one hand, the ability
of CR to learn accurate subcategorization information is higher than that of LAsim, and
on the other hand, the ability of CR to learn from sparse data and to generalize is at
least no lower than that of LAsim.
The baseline score informs us that about 76% of attachments are links by
proximity. The remainder (24%) are either long-distance attachments between phrase3
and phrase1, other types of attachments such as adjective complements, and sentence
modifiers, or finally, tagger errors. Note that there is no difference between the
baseline method?s precision and recall scores. Since RA always makes a (true or false)
positive decision, there cannot be (true or false) negative decisions.
142
Table 11
Total scores of the three methods. For each method, we compute the average of the three
sequences and the two corpora.
Precision Recall F-score
Baseline .76 .76 .76
LAsim .85 .71 .77
CR .89 .75 .81
Table 10
Evaluation taking into account three types of sequences and two corpora: EC and PGR.
Pr = precision, Rec = recall, and F-S = F-score.
Baseline (RA)
Sequences PrEC PrPGR RecEC RecPGR F-SRV F-SPGR
np?pp?pp .71 .72 .71 .72 .71 .72
vp?np?pp .83 .80 .83 .80 .83 .80
vp?pp?pp .75 .74 .75 .74 .75 .74
Lexical association (LAsim)
Sequences PrEC PrPGR RecEC RecPGR F-SRV F-SPGR
np?pp?pp .77 .82 .66 .72 .71 .77
vp?np?pp .90 .86 .75 .74 .82 .79
vp?pp?pp .85 .89 .65 .70 .74 .78
Boolean requirements (CR)
Sequences PrEC PrPGR RecEC RecPGR F-SRV F-SPGR
np?pp?pp .85 .86 .73 .76 .78 .81
vp?np?pp .92 .93 .75 .78 .83 .85
vp?pp?pp .86 .91 .69 .75 .77 .82
Computational Linguistics Volume 31, Number 1
143
Some tagger errors, especially those that appear systematically and regularly in the
training corpus, have a negative influence on the precision of both LAsim and CR. These
methods are sensitive to noisy data.
In order to measure recall and precision stability, we ran the clustering
process over six partitions (25%, 40%, 55%, 70%, 85%, and 100%) of the EC corpus.
Figure 7 shows that recall improves with corpus size. However, recall growth is more
significant in smaller partitions. In this particular corpus, recall stability seems to be
achieved when the corpus contains three million words. It follows that in order to
improve recall, we would have to use not only a bigger training corpus, but also a
more efficient clustering strategy, that is, a strategy that would be able to make
additional correct generalizations. Finally, note that precision neither increases nor
decreases with corpus size.
9. Conclusion and Future Work
This article has presented a particular unsupervised strategy to automatically acquire
syntactic and semantic requirements. Our aim was to learn two types of information
about a given word: the syntactic positions in which the word appears and the
semantic requirements associated with each syntactic position. Besides that, this
strategy also allowed us to discriminate word senses. The strategy is based on several
linguistic assumptions. First, it was assumed that not only does the syntactic head
impose restrictions on its dependent word, but also that the dependent word may
select for a specific type of head, a phenomenon referred to as corequirement. Second,
we claimed that similar syntactic positions share the same semantic requirements. So
we measured not similarity between words on the basis of their syntactic distribution,
but similarity between syntactic positions on the basis of their word distribution. It
was assumed that the latter kind of similarity conveys more pertinent information on
linguistic requirements than the former one. The learning process allowed us to
provide a lexicon with, among other information, both syntactic subcategorization and
selection restrictions. This information was used to constrain attachment heuristics.
In current work, we are using the learned clusters in other NLP applications than
attachment resolution. They are being used to automatically select word senses in
context (word sense disambiguation task). For this purpose, we are performing new
Figure 7
Variation of recall and precision as a function of corpus size.
Gamallo, Agustini, and Lopes Clustering Syntactic Positions
experiments on less domain-specific text corpora, since such corpora increase the
number of senses per word. On the other hand, these clusters turn out to be very
useful for checking whether two or more different morphological forms of a word are
semantically related. For instance, if ratification of [_ ] is similar to ratify [_ ], we may
infer that the verb ratify and the noun ratification are semantically related.
In future work, we aim at extending the lexicon in order to increase the coverage of
the parser. To do this, parsing and learning can be involved in a bootstrapping process.
The dependencies proposed by heuristic CR will be used as input to discover new
linguistic requirements. This new information will enable us to update the lexicon, and
then to propose new dependencies. At each cycle, the lexicon will be provided with
new requirements, and thereby the parser coverage will be higher. The successive
??learning + parsing?? cycles will stop as no more new information is acquired and no
more new dependencies are proposed.
144
Acknowledgments
The work by Pablo Gamallo was supported
by a grant from Portugal?s FundaZa?o para a
Cie?ncia e Tecnologia (ref: PRAXIS XXI/
BPD/2209/99). Alexandre Agustini was
supported by Brazil?s Federal Agency for
Post-Graduate Education (CAPES) and
Pontifical Catholic University of Rio Grande
do Sul (PUCRS).
References
Allegrini, Paolo, Simonetta Montemagni,
and Vito Pirrelli. 2000. Learning word
clusters from data types. In COLING-00,
pages 8?14, Saarbru?cken, Germany.
Allegrini, Paolo, Simonetta Montemagni,
and Vito Pirrelli. 2003. Example-based
automatic induction of semantic classes
through entropic scores. In A. Zampolli, N.
Calzolari, and L. Cignoni, editors,
Computational Linguistics in Piza?Linguistica
Computazionale a Pisa (special issue of
Linguistica Computazionale). Pisa and
Rome: IEPI, volume 1, pages 1?45.
Androutsopoulos, Ion and Robert Dale. 2000.
Selectional restrictions in HPSG. In
Eighteenth Conference on Computational
Linguistics (COLING), pages 15?20,
Saarbru?cken, Germany.
Basili, Roberto, Maria Pazienza, and Paola
Velardi. 1992. Computational lexicons: The
neat examples and the odd exemplars. In
Proceedings of the Third ANLP, Trento, Italy.
Beale, Stephen, Sergei Niremburg, and
Evelyn Viegas. 1998. Constraints in
computational semantics. In COLING-98,
Montreal.
Brent, Michel R. 1991. Automatic acquisition
of subcategorization frames from
untagged text. In 29th Annual Meeting of
ACL, pages 209?214, Berkeley, CA.
Ciaramita, Massimiliano and Mark Johnson.
2000. Explaining away ambiguity:
Learning verb selectional preference with
Bayesian networks. In COLING-00, pages
187?193, Saarbru?cken, Germany.
Collins, Michael and James Brooks. 1995.
Prepositional phrase attachment through a
backed-off model. In Proceedings of the
Third Workshop on Very Large Corpora,
pages 27?38, Cambridge, MA.
Dagan, Ido, Lillian Lee, and Fernando
Pereira. 1998. Similarity-based methods of
word cooccurrence probabilities. Machine
Learning, 43:56?63.
Dagan, Ido, Shaul Marcus, and Shaul
Markovitch. 1995. Contextual word
similarity and estimation from sparse data.
Computer Speech and Language, 9(2):123?152.
de la Clergerie, Eric. 2002. Construire des
analyseurs avec dyalog. In Proceedings of
TALN?02, Nancy, France.
Faure, David and Claire Ne?dellec. 1998.
Asium: Learning subcategorization
frames and restrictions of selection. In
ECML98, Workshop on Text Mining,
Chemnitz, Germany.
Framis, Francesc Ribas. 1995. On
learning more appropriate selectional
restrictions. In Proceedings of the Seventh
Conference of the European Chapter of the
Association for Computational Linguistics,
Dublin.
Gamallo, Pablo. 2003. Cognitive
characterisation of basic grammatical
structures. Pragmatics and Cognition,
11(2):209?240.
Gamallo, Pablo, Alexandre Agustini, and
Gabriel P. Lopes. 2003. Learning
subcategorisation information to
model a grammar with co-restrictions.
Traitement Automatic de la Langue,
44(1):93?117.
Computational Linguistics Volume 31, Number 1
145
Gamallo, Pablo, Caroline Gasperin,
Alexandre Agustini, and Gabriel P. Lopes.
2001. Syntactic-based methods for
measuring word similarity. In V. Mautner,
R. Moucek, and K. Moucek, editors, Text,
Speech, and Discourse (TSD-2001). Berlin:
Springer Verlag, pages 116?125.
Gildea, Daniel and Daniel Jurafsky. 2002.
Automatic labeling of semantic roles.
Computational Linguistics, 28(3):245?288.
Grefenstette, Gregory. 1994. Explorations in
Automatic Thesaurus Discovery. Kluwer
Academic, Norwell, MA.
Grishman, Ralph and John Sterling. 1994.
Generalizing automatically generated
selectional patterns. In Proceedings of the
15th International Conference on
Computational Linguistics (COLING-94),
Kyoto, Japan.
Harris, Zellig. 1985. Distributional structure.
In J. J. Katz, editor, The Philosophy of
Linguistics. New York: Oxford University
Press, pages 26?47.
Hindle, Donald and Mats Rooth. 1993.
Structural ambiguity and lexical
relations. Computational Linguistics,
19(1):103?120.
Hudson, Richard. 2003. The psychological
reality syntactic dependency relations. In
MTT2003, Paris.
Kovacs, Laszlo and Peter Baranyi. 2002.
Document clustering based on concept
lattice. In IEEE International Conference on
System, Man and Cybernetics (SMC?02),
Hammamet, Tunisia.
Langacker, Ronald W. 1991. Foundations of
Cognitive Grammar: Descriptive Applications,
volume 2. Stanford University Press,
Stanford.
Li, Hang and Naoki Abe. 1998. Word
clustering and disambiguation based on
co-occurrence data. In COLING-ACL?98,
pages 749?755, Montreal.
Lin, Dekang. 1998. Automatic retrieval
and clustering of similar words. In
COLING-ACL?98, pages 768?774, Montreal.
Lin, Dekang and Patrick Pantel. 2001.
Induction of semantic classes from
natural language text. In SIGKDD-01,
San Francisco.
Manning, Christopher. 1993. Automatic
acquisition of a large subcategorization
dictionary from corpora. In 31st Annual
Meeting of ACL, pages 235?242,
Columbus, OH.
Manning, Christopher and Hinrich Schu?tze.
1999. Foundations of Statistical Natural
Language Processing. MIT Press,
Cambridge, MA.
Marques, Nuno and Gabriel P. Lopes. 2001.
Tagging with small training corpora. In F.
Hoffmann, D. Hand, N. Adams, D. Fisher,
and G. Guimaraes, editors, Advances in
Intelligent Data Analysis. Lecture Notes in
Computer Science. Springer Verlag,
Berlin, pages 62?72.
Marques, Nuno, Gabriel P. Lopes, and Carlos
Coelho. 2000. Mining subcategorization
information by using multiple feature
loglinear models. In 10th CLIN,
pages 117?126, UILOTS Utrecht.
Niemann, Michael. 1998. Determining PP
attachment through semantic associations
and preferences. In ANLP Post Graduate
Workshop, pages 25?32, Sydney.
Pantel, Patrick and Dekan Lin. 2000. An
unsupervised approach to prepositional
phrase attachment using contextually
similar words. In ACL?00, pages 101?108,
Hong Kong.
Pantel, Patrick and Dekan Lin. 2002.
Discovering word senses from text. In
Proceedings of ACM SIGKDD Conference
on Knowledge Discovery and Data
Mining, pages 613?619, Edmonton,
Alberta, Canada.
Pustejovsky, James. 1995. The Generative
Lexicon. MIT Press, Cambridge, MA.
Ratnaparkhi, Adwait, Jeff Reymar, and Salim
Roukos. 1994. A maximum entropy model
for prepositional phrase attachment. In
Proceedings of the ARPA Human Language
Technology Workshop, pages 250?255,
Princeton, NJ.
Reinberger, Marie-Laure and Walter
Daelemans. 2003. Is shallow parsing
useful for unsupervised learning of
semantic clusters? In Fourth Conference on
Intelligent Text Processing and Computational
Linguistics (CICLing-03), pages 304?312,
Mexico City.
Resnik, Philip. 1997. Selectional preference
and sense disambiguation. In ACL-SIGLEX
Workshop on Tagging with Lexical Semantics,
Washington, DC.
Rocio, Vitor, Eric de la Clergerie, and Gabriel
Lopes. 2001. Tabulation for multi-purpose
partial parsing. Journal of Grammars,
4(1):41?65.
Schu?tze, Hinrich. 1998. Automatic word
sense discrimination. Computational
Linguistics, 24(1):97?124.
Sekine, Satoshi, Jeremy Carrol, Sofia
Ananiadou, and Jun?ichi Tsujii. 1992.
Automatic learning for semantic
collocation. In Proceedings of the Third
Conference on Applied Natural Language
Processing, pages 104?110, Trento, Italy.
Gamallo, Agustini, and Lopes Clustering Syntactic Positions

Using Confidence Bands for Parallel Texts Alignment
Ant?nio RIBEIRO
Departamento de Inform?tica
Faculdade de Ci?ncias e Tecnologia
Universidade Nova de Lisboa
Quinta da Torre
P-2825-114 Monte da Caparica
Portugal
ambar@di.fct.unl.pt
Gabriel LOPES
Departamento de Inform?tica
Faculdade de Ci?ncias e Tecnologia
Universidade Nova de Lisboa
Quinta da Torre
P-2825-114 Monte da Caparica
Portugal
gpl@di.fct.unl.pt
Jo?o MEXIA
Departamento de Matem?tica
Faculdade de Ci?ncias e Tecnologia
Universidade Nova de Lisboa
Quinta da Torre
P-2825-114 Monte da Caparica
Portugal
Abstract
This paper describes a language independent
method for alignment of parallel texts that
makes use of homograph tokens for each
pair of languages. In order to filter out
tokens that may cause misalignment, we use
confidence bands of linear regression lines
instead of heuristics which are not theoreti-
cally supported. This method was originally
inspired on work done by Pascale Fung and
Kathleen McKeown, and Melamed, provid-
ing the statistical support those authors
could not claim.
Introduction
Human compiled bilingual dictionaries do not
cover every term translation, especially when it
comes to technical domains. Moreover, we can
no longer afford to waste human time and effort
building manually these ever changing and in-
complete databases or design language specific
applications to solve this problem. The need for
an automatic language independent task for
equivalents extraction becomes clear in multi-
lingual regions like Hong Kong, Macao,
Quebec, the European Union, where texts must
be translated daily into eleven languages, or
even in the U.S.A. where Spanish and English
speaking communities are intermingled.
Parallel texts (texts that are mutual transla-
tions) are valuable sources of information for
bilingual lexicography. However, they are not of
much use unless a computational system may
find which piece of text in one language corre-
sponds to which piece of text in the other lan-
guage. In order to achieve this, they must be
aligned first, i.e. the various pieces of text must
be put into correspondence. This makes the
translations extraction task easier and more reli-
able. Alignment is usually done by finding
correspondence points ? sequences of characters
with the same form in both texts (homographs,
e.g. numbers, proper names, punctuation marks),
similar forms (cognates, like Region and Regi?o
in English and Portuguese, respectively) or even
previously known translations.
Pascale Fung and Kathleen McKeown (1997)
present an alignment algorithm that uses term
translations as correspondence points between
English and Chinese. Melamed (1999) aligns
texts using correspondence points taken either
from orthographic cognates (Michel Simard et
al., 1992) or from a seed translation lexicon.
However, although the heuristics both ap-
proaches use to filter noisy points may be intui-
tively quite acceptable, they are not theoretically
supported by Statistics.
The former approach considers a candidate
correspondence point reliable as long as, among
some other constraints, ?[...] it is not too far
away from the diagonal [...]? (Pascale Fung and
Kathleen McKeown, 1997, p.72) of a rectangle
whose sides sizes are proportional to the lengths
of the texts in each language (henceforth, ?the
golden translation diagonal?). The latter ap-
proach uses other filtering parameters: maxi-
mum point ambiguity level, point dispersion and
angle deviation (Melamed, 1999, pp. 115?116).
Ant?nio Ribeiro et al (2000a) propose a
method to filter candidate correspondence points
generated from homograph words which occur
only once in parallel texts (hapaxes) using linear
regressions and statistically supported noise
filtering methodologies. The method avoids
heuristic filters and they claim high precision
alignments.
In this paper, we will extend this work by de-
fining a linear regression line with all points
generated from homographs with equal frequen-
cies in parallel texts. We will filter out those
points which lie outside statistically defined
confidence bands (Thomas Wonnacott and
Ronald Wonnacott, 1990). Our method will
repeatedly use a standard linear regression line
adjustment technique to filter unreliable points
until there is no misalignment. Points resulting
from this filtration are chosen as correspondence
points.
The following section will discuss related
work. The method is described in section 2 and
we will evaluate and compare the results in sec-
tion 3. Finally, we present conclusions and fu-
ture work.
1 Background
There have been two mainstreams for parallel
text alignment. One assumes that translated texts
have proportional sizes; the other tries to use
lexical information in parallel texts to generate
candidate correspondence points. Both use some
notion of correspondence points.
Early work by Peter Brown et al (1991) and
William Gale and Kenneth Church (1991)
aligned sentences which had a proportional
number of words and characters, respectively.
Pairs of sentence delimiters (full stops) were
used as candidate correspondence points and
they ended up being selected while aligning.
However, these algorithms tended to break down
when sentence boundaries were not clearly
marked. Full stops do not always mark sentence
boundaries, they may not even exist due to OCR
noise and languages may not share the same
punctuation policies.
Using lexical information, Kenneth Church
(1993) showed that cheap alignment of text
segments was still possible exploiting ortho-
graphic cognates (Michel Simard et al, 1992),
instead of sentence delimiters. They became the
new candidate correspondence points. During
the alignment, some were discarded because
they lied outside an empirically estimated
bounded search space, required for time and
space reasons.
Martin Kay and Martin R?scheisen (1993)
also needed clearly delimited sentences. Words
with similar distributions became the candidate
correspondence points. Two sentences were
aligned if the number of correspondence points
associating them was greater than an empirically
defined threshold: ?[...] more than some mini-
mum number of times [...]? (Martin Kay and
Martin R?scheisen, 1993, p.128). In Ido Dagan
et al (1993) noisy points were filtered out by
deleting frequent words.
Pascale Fung and Kathleen McKeown (1994)
dropped the requirement for sentence boundaries
on a case-study for English-Chinese. Instead,
they used vectors that stored distances between
consecutive occurrences of a word (DK-vec?s).
Candidate correspondence points were identified
from words with similar distance vectors and
noisy points were filtered using some heuristics.
Later, in Pascale Fung and Kathleen McKeown
(1997), the algorithm used extracted terms to
compile a list of reliable pairs of translations.
Those pairs whose distribution similarity was
above a threshold became candidate correspon-
dence points (called potential anchor points).
These points were further constrained not to be
?too far away? from the ?translation diagonal?.
Michel Simard and Pierre Plamondon (1998)
aligned sentences using isolated cognates as
candidate correspondence points, i.e. cognates
that were not mistaken for others within a text
window. Some were filtered out if they either
lied outside an empirically defined search space,
named a corridor, or were ?not in line? with
their neighbours.
Melamed (1999) also filtered candidate corre-
spondence points obtained from orthographic
cognates. A maximum point ambiguity level
filters points outside a search space, a maximum
point dispersion filters points too distant from a
line formed by candidate correspondence points
and a maximum angle deviation filters points
that tend to slope this line too much.
Whether the filtering of candidate correspon-
dence points is done prior to alignment or during
it, we all want to find reliable correspondence
points. They provide the basic means for ex-
tracting reliable information from parallel texts.
However, as far as we learned from the above
papers, current methods have repeatedly used
statistically unsupported heuristics to filter out
noisy points. For instance, the ?golden transla-
tion diagonal? is mentioned in all of them but
none attempts filtering noisy points using statis-
tically defined confidence bands.
2 Correspondence Points Filters
2.1 Overview
The basic insight is that not all candidate corre-
spondence points are reliable. Whatever heuris-
tics are taken (similar word distributions, search
corridors, point dispersion, angle deviation,...),
we want to filter the most reliable points. We
assume that reliable points have similar charac-
teristics. For instance, they tend to gather some-
where near the ?golden translation diagonal?.
Homographs with equal frequencies may be
good alignment points.
2.2 Source Parallel Texts
We worked with a mixed parallel corpus con-
sisting of texts selected at random from the Offi-
cial Journal of the European Communities1
(ELRA, 1997) and from The Court of Justice of
the European Communities2 in eleven lan-
guages3.
Language Written Questions Debates Judgements Total
da 259k (52k) 2,0M (395k) 16k (3k) 2250k
de 234k (47k) 1,8M (368k) 15k (3k) 2088k
el 272k (54k) 1,9M (387k) 16k (3k) 2222k
en 263k (53k) 2,1M (417k) 16k (3k) 2364k
es 292k (58k) 2,2M (439k) 18k (4k) 2507k
fi --- --- 13k (3k) 13k
fr 310k (62k) 2,2M (447k) 19k (4k) 2564k
it 279k (56k) 1,9M (375k) 17k (3k) 2171k
nl 275k (55k) 2,1M (428k) 16k (3k) 2431k
pt 284k (57k) 2,1M (416k) 17k (3k) 2381k
sv --- --- 15k (3k) 15k
Total 2468k (55k) 18,4M (408k) 177k (3k) 21005k
Sub-corpus
Table 1: Words per sub-corpus (average per text
inside brackets; markups discarded)4.
For each language, we included:
? five texts with Written Questions asked by
members of the European Parliament to the
European Commission and their corre-
sponding answers (average: about 60k words
or 100 pages / text);
                                                  
1
 Danish (da), Dutch (nl), English (en), French (fr),
German (de), Greek (el), Italian (it), Portuguese (pt) and
Spanish (es).
2
 Webpage address: curia.eu.int
3
 The same languages as those in footnote 1 plus
Finnish (fi) and Swedish (sv).
4
 No Written Questions and Debates texts for Finnish
and Swedish are available in ELRA (1997) since the
texts provided are from the 1992-4 period and it was
not until 1995 that the respective countries became
part of the European Union.
? five texts with records of Debates in the
European Parliament (average: about 400k
words or more than 600 pages / text). These
are written transcripts of oral discussions;
? five texts with judgements of The Court of
Justice of the European Communities (aver-
age: about 3k words or 5 pages / text).
In order to reduce the number of possible pairs
of parallel texts from 110 sets (11 lan-
guages?10) to a more manageable size of 10
sets, we decided to take Portuguese as the kernel
language of all pairs.
2.3 Generating Candidate Correspon-
dence Points
We generate candidate correspondence points
from homographs with equal frequencies in two
parallel texts. Homographs, as a naive and par-
ticular form of cognate words, are likely transla-
tions (e.g. Hong Kong in various European lan-
guages). Here is a table with the percentages of
occurrences of these words in the used texts:
Pair Written Questions Debates Judgements Average
pt-da 2,8k (4,9%) 2,5k (0,6%) 0,3k (8,1%) 2,5k (1,1%) 
pt-de 2,7k (5,1%) 4,2k (1,0%) 0,4k (7,9%) 4,0k (1,5%) 
pt-el 2,3k (4,0%) 1,9k (0,5%) 0,3k (6,9%) 1,9k (0,8%) 
pt-en 2,7k (4,8%) 2,8k (0,7%) 0,3k (6,2%) 2,7k (1,1%) 
pt-es 4,1k (7,1%) 7,8k (1,9%) 0,7k (15,2%) 7,4k (2,5%) 
pt-fi --- --- 0,2k (5,2%) 0,2k (5,2%) 
pt-fr 2,9k (5,0%) 5,1k (1,2%) 0,4k (9,4%) 4,8k (1,6%) 
pt-it 3,1k (5,5%) 5,4k (1,3%) 0,4k (9,6%) 5,2k (1,8%) 
pt-nl 2,6k (4,5%) 4,9k (1,2%) 0,3k (8,3%) 4,7k (1,6%) 
pt-sv --- --- 0,3k (6,9%) 0,3k (6,9%) 
Average 2,9k (5,1%) 4,4k (1,1%) 0,4k (8,4%) 4,2k (1,5%) 
Sub-corpus
Table 2: Average number of homographs with
equal frequencies per pair of parallel texts (aver-
age percentage of homographs inside brackets).
For average size texts (e.g. the Written Ques-
tions), these words account for about 5% of the
total (about 3k words / text). This number varies
according to language similarity. For instance,
on average, it is higher for Portuguese?Spanish
than for Portuguese?English.
These words end up being mainly numbers
and names. Here are a few examples from a
parallel Portuguese?English text: 2002 (num-
bers, dates), ASEAN (acronyms), Patten (proper
names), China (countries), Manila (cities),
apartheid (foreign words), Ltd (abbreviations),
habitats (Latin words), ferry (common names),
global (common vocabulary).
In order to avoid pairing homographs that are
not equivalent (e.g. ?a?, a definite article in Por-
tuguese and an indefinite article in English), we
restricted ourselves to homographs with the
same frequencies in both parallel texts. In this
way, we are selecting words with similar distri-
butions. Actually, equal frequency words helped
Jean-Fran?ois Champollion to decipher the Ro-
setta Stone for there was a name of a King
(Ptolemy V) which occurred the same number of
times in the ?parallel texts? of the stone.
Each pair of texts provides a set of candidate
correspondence points from which we draw a
line based on linear regression. Points are de-
fined using the co-ordinates of the word posi-
tions in each parallel text. For example, if the
first occurrence of the homograph word Patten
occurs at word position 125545 in the
Portuguese text and at 135787 in the English
parallel text, then the point co-ordinates are
(125545,135787). The generated points may
adjust themselves well to a linear regression line
or may be dispersed around it. So, firstly, we use
a simple filter based on the histogram of the
distances between the expected and real posi-
tions. After that, we apply a finer-grained filter
based on statistically defined confidence bands
for linear regression lines.
We will now elaborate on these filters.
2.4 Eliminating Extreme Points
The points obtained from the positions of homo-
graphs with equal frequencies are still prone to
be noisy. Here is an example:
Noisy Candidate Correspondence Points
y = 0,9165x + 141,65
0
10000
20000
30000
40000
50000
0 10000 20000 30000 40000 50000
pt Word Positions
en
 W
or
d 
Po
sit
io
ns
Figure 1: Noisy versus ?well-behaved? (?in
line?) candidate correspondence points. The
linear regression line equation is shown on the
top right corner.
The figure above shows noisy points because
their respective homographs appear in positions
quite apart. We should feel reluctant to accept
distant pairings and that is what the first filter
does. It filters out those points which are clearly
too far apart from their expected positions to be
considered as reliable correspondence points.
We find expected positions building a linear
regression line with all points, and then deter-
mining the distances between the real and the
expected word positions:
pt en Positions
Position Word Real Expected Distance
3877 I 24998 3695 21303
9009 etc 22897 8399 14499
11791 I 25060 10948 14112
15248 As 3398 14117 10719
16965 As 3591 15690 12099
22819 volume 32337 21056 11281
Table 3: A sample of the distances between
expected and real positions of noisy points in
Figure 1.
Expected positions are computed from the lin-
ear regression line equation y = ax + b, where a
is the line slope and b is the Y-axis intercept (the
value of y when x is 0), substituting x for the
Portuguese word position. For Table 3, the ex-
pected word position for the word I at pt word
position 3877 is 0.9165 ? 3877 + 141.65 = 3695
(see the regression line equation in Figure 1)
and, thus, the distance between its expected and
real positions is | 3695 ? 24998 | = 21303.
If we draw a histogram ranging from the
smallest to the largest distance, we get:
Histogram of Distances
0
2
4
6
8
10
0
27
69
55
38
83
07
11
07
6
13
84
5
16
61
4
19
38
3
22
15
2
24
92
1
27
69
0
30
45
9
33
22
8
35
99
7
Distances between Real and Expected Word Positions
N
um
be
r o
f P
oi
nt
s
filtered points
3297
Figure 2: Histogram of the distances between
expected and real word positions.
In order to build this histogram, we use the
Sturges rule (see ?Histograms? in Samuel Kotz et
al. 1982). The number of classes (bars or bins) is
given by 1 + log2n, where n is the total number
of points. The size of the classes is given by
(maximum distance ? minimum distance) /
number of classes. For example, for Figure 1, we
have 3338 points and the distances between
expected and real positions range from 0 to
35997. Thus, the number of classes is
1 + log23338 ? 12.7 ? 13 and the size of the
classes is (35997 ? 0) / 13 ? 2769. In this way,
the first class ranges from 0 to 2769, the second
class from 2769 to 5538 and so forth.
With this histogram, we are able to identify
those words which are too far apart from their
expected positions. In Figure 2, the gap in the
histogram makes clear that there is a discontinu-
ity in the distances between expected and real
positions. So, we are confident that all points
above 22152 are extreme points. We filter them
out of the candidate correspondence points set
and proceed to the next filter.
2.5 Confidence Bands of Linear Regres-
sion Lines
Confidence bands of linear regression lines
(Thomas Wonnacott and Ronald Wonnacott,
1990, p. 384) help us to identify reliable points,
i.e. points which belong to a regression line with
a great confidence level (99.9%). The band is
typically wider in the extremes and narrower in
the middle of the regression line.
The figure below shows an example of filter-
ing using confidence bands:
Linear Regression Line Confidence Bands
8700
8800
8900
9000
9100
9400 9450 9500 9550 9600 9650 9700 9750 9800
pt Word Position
en
 W
or
d 
Po
sit
io
n
Expected y
Real yConfidence band
Figure 3: Detail of the filter based on confi-
dence bands. Point A lies outside the confidence
band. It will be filtered out.
We start from the regression line defined by
the points filtered with the Histogram technique,
described in the previous section, and then we
calculate the confidence band. Points which lie
outside this band are filtered out since they are
credited as too unreliable for alignment (e.g.
Point A in Figure 3). We repeat this step until no
pieces of text belong to different translations, i.e.
until there is no misalignment.
The confidence band is the error admitted at
an x co-ordinate of a linear regression line. A
point (x,y) is considered outside a linear regres-
sion line with a confidence level of 99.9% if its y
co-ordinate does not lie within the confidence
interval [ ax + b ? error(x); ax + b + error(x)],
where ax + b is the linear regression line equa-
tion and error(x) is the error admitted at the x
co-ordinate. The upper and lower limits of the
confidence interval are given by the following
equation (see Thomas Wonnacott & Ronald
Wonnacott, 1990, p. 385):
?
=
?
?
+?+=
n
i
i Xx
Xx
n
stbaxy
1
2
2
005.0
)(
)(1
  )(
where:
? t0.005 is the t-statistics value for a 99.9% con-
fidence interval. We will use the z-statistics
instead since t0.005 = z0.005 = 3.27 for large
samples of points (above 120);
? n is the number of points;
? s is the standard deviation from the expected
value y?  at co-ordinate x (see Thomas Won-
nacott & Ronald Wonnacott, 1990, p. 379):
baxy
n
yy
s
n
i
i
+=
?
?
=
?
=
?
 where,
2
)?(
1
? X is the average value of the various xi:
?
=
=
n
i
ix
n
X
1
1
3 Evaluation
We ran our alignment algorithm on the parallel
texts of 10 language pairs as described in section
2.2. The table below summarises the results:
Pair Written Questions Debates Judgements Average
pt-da 128 (5%) 56 (2%) 114 (35%) 63 (2%) 
pt-de 124 (5%) 99 (2%) 53 (15%) 102 (3%) 
pt-el 118 (5%) 115 (6%) 60 (20%) 115 (6%) 
pt-en 88 (3%) 102 (4%) 50 (19%) 101 (4%) 
pt-es 59 (1%) 55 (1%) 143 (21%) 56 (1%) 
pt-fi --- --- 60 (26%) 60 (26%) 
pt-fr 148 (5%) 113 (2%) 212 (49%) 117 (2%) 
pt-it 117 (4%) 104 (2%) 25 (6%) 105 (2%) 
pt-nl 120 (5%) 73 (1%) 53 (15%) 77 (2%) 
pt-sv --- --- 74 (23%) 74 (23%) 
Average 113 (4%) 90 (2%) 84 (23%) 92 (2%) 
Sub-corpus
Table 4: Average number of correspondence
points in the first non-misalignment (average
ratio of filtered and initial candidate correspon-
dence points inside brackets).
On average, we end up with about 2% of the
initial correspondence points which means that
we are able to break a text in about 90 segments
(ranging from 70 words to 12 pages per segment
A
for the Debates). An average of just three filtra-
tions are needed: the Histogram filter plus two
filtrations with the Confidence Bands.
The figure below shows an example of a mis-
aligning correspondence point.
Misalignments
(Crossed segments)
300
400
500
600
700
800
900
1000
300 400 500 600 700 800
pt Word Position
en
 W
or
d 
Po
sit
io
n
Figure 4: Bad correspondence points (? ? mis-
aligning points; ? ? FRUUHVSRQGHQFH SRLQWV
Had we restricted ourselves to using homo-
graphs which occur only once (hapaxes), we
would get about one third of the final points
(Ant?nio Ribeiro et al 2000a). Hapaxes turn out
to be good candidate correspondence points
because they work like cognates that are not
mistaken for others within the full text scope
(Michel Simard and Pierre Plamondon, 1998).
When they are in similar positions, they turn out
to be reliable correspondence points.
To compare our results, we aligned the BAF
Corpus (Michel Simard and Pierre Plamondon,
1998) which consists of a collection of parallel
texts (Canadian Parliament Hansards, United
Nations, literary, etc.).
Filename # Tokens # Segments Chars / Segment # Segments Chars / Segment Ratio
citi1.fr 17556 49 1860 742 120 6,6%
citi2.fr 33539 48 3360 1393 104 3,4%
cour.fr 49616 101 2217 1377 140 7,3%
hans.fr 82834 45 8932 3059 117 1,5%
ilo.fr 210342 68 15654 7129 137 1,0%
onu.fr 74402 27 14101 2559 132 1,1%
tao1.fr 10506 52 1019 365 95 14,2%
tao2.fr 9825 51 972 305 97 16,7%
tao3.fr 4673 44 531 176 62 25,0%
verne.fr 79858 29 12736 2521 127 1,2%
xerox.fr 66605 114 2917 3454 85 3,3%
Average 111883 60 10271 3924 123 1,5%
Equal Frequency Homographs BAF Analysis
Table 5: Comparison with the Jacal alignment
(Michel Simard and Pierre Plamondon, 1998).
The table above shows that, on average, we
got about 1.5% of the total segments, resulting
in about 10k characters per segment. This num-
ber ranges from 25% (average: 500 characters
per segment) for a small text (tao3.fr-en) to 1%
(average: 15k characters per segment) for a large
text (ilo.fr-en). Although these are small num-
bers, we should notice that, in contrast with Mi-
chel Simard and Pierre Plamondon (1998), we
are not including:
? words defined as cognate ?if their four first
characters are identical?;
? an ?isolation window? heuristics to reduce the
search space;
? heuristics to define a search corridor to find
candidate correspondence points;
We should stress again that the algorithm re-
ported in this paper is purely statistical and re-
curs to no heuristics. Moreover, we did not re-
apply the algorithm to each aligned parallel
segment which would result in finding more
correspondence points and, consequently, fur-
ther segmentation of the parallel texts. Besides,
if we use the methodology presented in Joaquim
da Silva et al (1999) for extracting relevant
string patterns, we are able to identify more sta-
tistically reliable cognates.
Ant?nio Ribeiro and Gabriel Lopes (1999) re-
port a higher number of segments using clusters
of points. However, the algorithm does not as-
sure 100% alignment precision and discards
some good correspondence points which end up
in bad clusters.
Our main critique to the use of heuristics is
that though they may be intuitively quite accept-
able and may significantly improve the results as
seen with Jacal alignment for the BAF Corpus,
they are just heuristics and cannot be theoreti-
cally explained by Statistics.
Conclusions
Confidence bands of linear regression lines help
us to identify reliable correspondence points
without using empirically found or statistically
unsupported heuristics. This paper presents a
purely statistical approach to the selection of
candidate correspondence points for parallel
texts alignment without recurring to heuristics as
in previous work. The alignment is not restricted
to sentence or paragraph level for which clearly
delimited boundaries markers would be needed.
It is made at whatever segment size as long as
reliable correspondence points are found. This
means that alignment can result at paragraph,
sentence, phrase, term or word level.
Moreover, the methodology does not depend
on the way candidate correspondence points are
generated, i.e. although we used homographs
with equal frequencies, we could have also boot-
strapped the process using cognates (Michel
Simard et al 1992) or a small bilingual lexicon
to identify equivalents of words or expressions
(Dekai Wu 1994; Pascale Fung and Kathleen
McKeown 1997; Melamed 1999). This is a par-
ticularly good strategy when it comes to distant
languages like English and Chinese where the
number of homographs is reduced. As Ant?nio
Ribeiro et al (2000b) showed, these tokens ac-
count for about 5% for small texts. Aligning
languages with such different alphabets requires
automatic methods to identify equivalents as
Pascale Fung and Kathleen McKeown (1997)
presented, increasing the number of candidate
correspondence points at the beginning.
Selecting correspondence points improves the
quality and reliability of parallel texts alignment.
As this alignment algorithm is not restricted to
paragraphs or sentences, 100% alignment preci-
sion may be degraded by language specific term
order policies in small segments. On average,
three filtrations proved enough to avoid crossed
segments which are a result of misalignments.
The method is language and character-set inde-
pendent and does not assume any a priori lan-
guage knowledge (namely, small bilingual lexi-
cons), text tagging, well defined sentence or
paragraph boundaries nor one-to-one translation
of sentences.
Future Work
At the moment, we are working on alignment of
sub-segments of parallel texts in order to find
more correspondence points within each aligned
segment in a recursive way. We are also plan-
ning to apply the method to large parallel Portu-
guese?Chinese texts. We believe we may sig-
nificantly increase the number of segments we
get in the end by using a more dynamic ap-
proach to the filtering using linear regression
lines, by selecting candidate correspondence
points at the same time that parallel texts tokens
are input. This approach is similar to Melamed
(1999) but, in contrast, it is statistically sup-
ported and uses no heuristics.
Another area for future experiments will use
relevant strings of characters in parallel texts
instead of using just homographs. For this pur-
pose, we will apply a methodology described in
Joaquim da Silva et al (1999). This method was
used to extract string patterns and it will help us
to automatically extract ?real? cognates.
Acknowledgements
Our thanks go to the anonymous referees for
their valuable comments on the paper. We
would also like to thank Michel Simard for pro-
viding us the aligned BAF Corpus. This research
was partially supported by a grant from Funda-
??o para a Ci?ncia e Tecnologia / Praxis XXI.
References
Peter Brown, Jennifer Lai and Robert Mercer (1991)
Aligning Sentences in Parallel Corpora. In ?Pro-
ceedings of the 29th Annual Meeting of the Asso-
ciation for Computational Linguistics?, Berkeley,
California, U.S.A., pp. 169?176.
Kenneth Church (1993)  Char_align: A Program for
Aligning Parallel Texts at the Character Level. In
?Proceedings of the 31st Annual Meeting of the
Association for Computational Linguistics?,
Columbus, Ohio, U.S.A., pp. 1?8.
Ido Dagan, Kenneth Church and William Gale (1993)
Robust Word Alignment for Machine Aided
Translation. In ?Proceedings of the Workshop on
Very Large Corpora: Academic and Industrial
Perspectives?, Columbus, Ohio, U.S.A., pp. 1?8.
ELRA (European Language Resources Association)
(1997)  Multilingual Corpora for Co-operation,
Disk 2 of 2. Paris, France.
Pascale Fung and Kathleen McKeown (1994)
Aligning Noisy Parallel Corpora across Language
Groups: Word Pair Feature Matching by Dynamic
Time Warping. In ?Technology Partnerships for
Crossing the Language Barrier: Proceedings of the
First Conference of the Association for Machine
Translation in the Americas?, Columbia, Maryland,
U.S.A., pp. 81?88.
Pascale Fung and Kathleen McKeown (1997)  A
Technical Word- and Term-Translation Aid Using
Noisy Parallel Corpora across Language Groups.
Machine Translation, 12/1?2 (Special issue),
pp. 53?87.
William Gale and Kenneth Church (1991)  A Pro-
gram for Aligning Sentences in Bilingual Corpora.
In ?Proceedings of the 29th Annual Meeting of the
Association for Computational Linguistics?,
Berkeley, California, U.S.A., pp. 177?184 (short
version). Also (1993) Computational Linguistics,
19/1, pp. 75?102 (long version).
Martin Kay and Martin R?scheisen (1993)  Text-
Translation Alignment. Computational Linguistics,
19/1, pp. 121?142.
Samuel Kotz, Norman Johnson and Campbell Read
(1982)  Encyclopaedia of Statistical Sciences. John
Wiley & Sons, New York Chichester Brisbane
Toronto Singapore.
I. Dan Melamed (1999)  Bitext Maps and Alignment
via Pattern Recognition. Computational Linguis-
tics, 25/1, pp. 107?130.
Ant?nio Ribeiro, Gabriel Lopes and Jo?o Mexia
(2000a)  Using Confidence Bands for Alignment
with Hapaxes. In ?Proceedings of the International
Conference on Artificial Intelligence (IC?AI
2000)?, Computer Science Research, Education
and Applications Press, U.S.A., volume II,
pp. 1089?1095.
Ant?nio Ribeiro, Gabriel Lopes and Jo?o Mexia
(2000b, in press)  Aligning Portuguese and Chi-
nese Parallel Texts Using Confidence Bands. In
?Proceedings of the Sixth Pacific Rim International
Conference on Artificial Intelligence (PRICAI
2000) ? Lecture Notes in Artificial Intelligence?,
Springer-Verlag.
Joaquim da Silva, Ga?l Dias, Sylvie Guillor?, Jos?
Lopes (1999)  Using Localmaxs Algorithms for the
Extraction of Contiguous and Non-contiguous
Multiword Lexical Units. In Pedro Barahona and
Jos? Alferes, eds., ?Progress in Artificial Intelli-
gence ? Lecture Notes in Artificial Intelligence?,
number 1695, Springer-Verlag, Berlin, Germany,
pp. 113?132.
Michel Simard, George Foster and Pierre Isabelle
(1992)  Using Cognates to Align Sentences in Bi-
lingual Corpora. In ?Proceedings of the Fourth
International Conference on Theoretical and
Methodological Issues in Machine Translation
TMI-92?, Montreal, Canada, pp. 67?81.
Michel Simard and Pierre Plamondon (1998)
Bilingual Sentence Alignment: Balancing Robust-
ness and Accuracy. Machine Translation, 13/1,
pp. 59?80.
Dekai Wu (1994)  Aligning a Parallel English?Chi-
nese Corpus Statistically with Lexical Criteria. In
?Proceedings of the 32nd Annual Conference of
the Association for Computational Linguistics?,
Las Cruces, New Mexico, U.S.A., pp. 80?87.
Thomas Wonnacott and Ronald Wonnacott (1990)
Introductory Statistics. 5th edition, John Wiley &
Sons, New York Chichester Brisbane Toronto
Singapore, 711 p..
Using Co-Composition for Acquiring Syntactic and Semantic
Subcategorisation
Pablo Gamallo Alexandre Agustini
Department of Computer Science
New University of Lisbon, Portugal
 
gamallo,aagustini,gpl  @di.fct.unl.pt
Gabriel P. Lopes
Abstract
Natural language parsing requires ex-
tensive lexicons containing subcategori-
sation information for specific sublan-
guages. This paper describes an unsuper-
vised method for acquiring both syntac-
tic and semantic subcategorisation restric-
tions from corpora. Special attention will
be paid to the role of co-composition in
the acquisition strategy. The acquired in-
formation is used for lexicon tuning and
parsing improvement.
1 Introduction
Recent lexicalist Grammars project the subcat-
egorisation information encoding in the lexicon
onto syntactic structures. These grammars use
accurate subcategorised lexicons to restrict potential
syntactic structures. In terms of parsing devel-
opment, it is broadly assumed that parsers need
such information in order to reduce the number of
possible analyses and, therefore, solve syntactic
ambiguity. Over the last years various methods
for acquiring subcategorisation information from
corpora has been proposed. Some of them induce
syntactic subcategorisation from tagged texts
(Brent, 1993; Briscoe and Carrol, 1997; Marques,
2000). Unfortunately, syntactic information is not
enough to solve structural ambiguity. Consider the
following verbal phrases:
(1) [peel [  the potato] [  with a knife]]
(2) [peel [  [  the potato] [  with a rough stain]]]
The attachment of ?with PP? to both the verb
?peel? in phrase (1) and to the NP ?the potato? in
(2) does not depend only on syntactic requirements.
Indeed, it is not possible to attach the PP ?with
a knife? to the verb ?peel? by asserting that this
verb subcategorises a ?with PP?. Such a subcate-
gorisation information cannot be used to explain
the analysis of phrase (2), where it is the NP ?the
potato? that is attached to the ?with PP?. In order
to decide the correct analysis in both phrases, we
are helped by our world knowledge about the action
of peeling, the use of knifes, and the attributes
of potatoes. In general, we know that knifes are
used for peeling, and potatoes can have different
kinds of stains. So, the parser is able to propose a
correct analysis only if the lexicon is provided with,
not only syntactic subcategorisation information,
but also with information on semantic-pragmatic
requirements (i.e., with selection restrictions).
Other works attempt to acquire selection restric-
tions requiring pre-existing lexical ressources. The
learning algorithm requires sample corpora to be
constituted by verb-noun, noun-verb, or verb-prep-
noun dependencies, where the nouns are semanti-
cally tagged by using lexical hierarchies such as
WordNet (Resnik, 1997; Framis, 1995). Selection
restrictions are induced by considering those depen-
dencies associated with the same semantic tags. For
instance, if verb ratify frequently appears with nouns
semantically tagged as ?legal documents? in the di-
rect object position (e.g., article, law, precept, . . . ),
then it follows that it must select for nouns denot-
ing legal documents. Unfortunately, if a pre-defined
                     July 2002, pp. 34-41.  Association for Computational Linguistics.
                     ACL Special Interest Group on the Lexicon (SIGLEX), Philadelphia,
                  Unsupervised Lexical Acquisition: Proceedings of the Workshop of the
set of semantic tags is used to annotate the training
corpus, it is not obvious that the tags available are
the more appropriate for extracting domain-specific
semantic restrictions. If the tags were created specif-
ically to capture corpus dependent restrictions, there
could be serious problems concerning portability to
a new specific domain.
By contrast, unsupervised strategies to acquire
selection restrictions do not require a training cor-
pus to be semantically annotated using pre-existing
lexical hierarchies (Sekine et al, 1992; Dagan et
al., 1998; Grishman and Sterling, 1994). They re-
quire only a minimum of linguistic knowledge in or-
der to identify ?meaningful? syntactic dependencies.
According to the Grefenstette?s terminology, they
can be classified as ?knowledge-poor approaches?
(Grefenstette, 1994). Semantic preferences are in-
duced by merely using co-occurrence data, i.e., by
using a similarity measure to identify words which
occur in the same dependencies. It is assumed that
two words are semantically similar if they appear in
the same contexts and syntactic dependencies. Con-
sider for instance that the verb ratify frequently ap-
pear with the noun organisation in the subject po-
sition. Moreover, suppose that this noun turns to
be similar in a particular corpus to other nouns:
e.g., secretary and council. It follows that ratify not
only selects for organisation, but also for its simi-
lar words. This seems to be right. However, suppose
that organisation also appears in expressions like the
organisation of society began to be disturbed in the
last decade, or they are involved in the actual organ-
isation of things, with a significant different word
meaning. In this case, the noun means a particu-
lar kind of process. It seems obvious that its sim-
ilar words, secretary and council, cannot appear in
such subcategorisation contexts, since they are re-
lated to the other sense of the word. Soft clusters,
in which words can be members of different clusters
to different degrees, might solve this problem to a
certain extent (Pereira et al, 1993). We claim, how-
ever, that class membership should be modeled by
boolean decisions. Since subcategorisation contexts
require words in boolean terms (i.e., words are either
required or not required), words are either members
or not members of specific subcagorisation classes.
Hence, we propose a clustering method in which a
word may be gathered into different boolean clus-
ters, each cluster representing the semantic restric-
tions imposed by a class of subcategorisation con-
texts.
This paper describes an unsupervised method for
acquiring information on syntactic and semantic
subcategorisation from partially parsed text corpora.
The main assumptions underlying our proposal will
be introduced in the following section. Then, sec-
tion 3 will present the different steps -extraction of
candidate subcategorisation restrictions and concep-
tual clustering- of our learning method. In section
4, we will show how the dictionary entries are pro-
vided with the learned information. The accuracy
and coverage of this information will be measured
in a particular application: attachment resolution.
The experiments presented in this paper were per-
formed on 1,5 million of words belonging to the
P.G.R. (Portuguese General Attorney Opinions) cor-
pus, which is a domain-specific Portuguese corpus
containing case-law documents.
2 Underlying Assumptions
Our acquisition method is based on two theoretical
assumptions. First, we assume a very general no-
tion of linguistic subcategorisation. More precisely,
we consider that in a ?head-complement? depen-
dency, not only the head imposes constraints on the
complement, but also the complement imposes lin-
guistic requirements on the head. Following Puste-
jovsky?s terminology, we call this phenomenon ?co-
composition? (Pustejovsky, 1995). So, for a particu-
lar word, we attempt to learn both what kind of com-
plements and what kind of heads it subcategorises.
For instance, consider the compositional behavior of
the noun republic in a domain-specific corpus. On
the one hand, this word appears in the head position
within dependencies such as republic of Ireland, re-
public of Portugal, and so on. On the other hand, it
appears in the complement position in dependencies
like president of the republic, government of the re-
public, etc. Given that there are interesting semantic
regularities among the words cooccurring with re-
public in such linguistic contexts, we attempt to im-
plement an algorithm letting us learn two different
subcategorisation contexts:

	ffColing 2010: Poster Volume, pages 1149?1157,
Beijing, August 2010
Towards Automatic Building of Document Keywords
Joaquim Silva
CITI/DI/FCT
Universidade Nova de Lisboa
jfs@di.fct.unl.pt
Gabriel Lopes
CITI/DI/FCT
Universidade Nova de Lisboa
gpl@di.fct.unl.pt
Abstract
Document keywords are associated to
documents as summarized versions of the
documents? content. Considering that the
number of documents is quickly growing
every day, the availability of these key-
words is very important. Although, usu-
ally keywords are manually written. This
motivated us to work on an approach to
change this manual procedure for an auto-
matic one.
This paper presents a language indepen-
dent approach that extracts the most rel-
evant Multiword Expressions and single
words from documents and propose them
to describe the core content of each docu-
ment.
1 Introduction
Keywords provide efficient and sharp access to
documents concerning their main topics, that is,
their core content. Keywords are semantically rel-
evant terms, usually being relevant noun-phrases
rather than long full phrases. Full phrases such
as ?John F Kennedy?s speechwriter hails Obama?s
address? can be extracted by summarization ap-
proaches, but it wouldn?t be appropriate if used
as keywords since it doesn?t mean any main
topic/subtopic. On the other hand, by using Local-
Maxs algorithm (Silva and Lopes, 1999) it is pos-
sible to extract Multiword Expressions (MWEs)
from documents and, some of the most relevant
ones relatively to each document can be used as
that document?s descriptor, if properly selected.
In this paper we will show that MWEs having
2, 3 our 4 words, that is, (2-4)-gram MWEs, are
the most appropriate ones to fit the typical key-
words? semantic sharpness, as would be the case
of ?climate change?, ?American Red Cross?, ?so-
cial and economic policy?, etc., rather than (5-7)-
grams and larger MWEs addressing more specific
meanings, such as ?skills for lifelong learning pro-
cess report? or ?Assessment of the use of Mag-
netic Resonans Tomography?.
On the other hand, although MWEs extracted
by LocalMaxs algorithm are usually relevant,
some of them are semantically vague or simply
not relevant, such as ?general use? or ?Annex I?,
not having the semantic relevance and sharpness
required to form keywords. Other MWEs such as
?in case of? or ?as soon as possible? may be useful
for lexicon enrichment to improve Natural Lan-
guage Processing, but they are not relevant MWEs
to be taken as keywords.
During our investigation, we discovered that the
median of the words? length in each MWE has
a strong influence in the MWE relevance. Thus,
combining this and other factors that influence rel-
evance, a metric, Mk, is proposed to better evalu-
ate the relevance of each MWE under the purpose
of obtaining keywords, and consequently its rele-
vance score in each document.
Although most document keywords are mul-
tiwords, there are some single words , that
is, 1-grams, whose strong and sharp meaning
make them good keywords, such as ?Agricul-
ture?, ?salmonella?, among others. Then, since
we wanted to include single words in the set of the
main keywords of each document, and because
LocalMaxs algorithm does not extracts 1-grams,
we had to select the most informative single words
1149
from documents using another metric, Sk, also
presented in this paper.
This paper proposes a statistical and language-
independent approach to generate document de-
scriptors based on the automatic extraction of
the most informative MWEs and single words,
in terms of document summarization, under the
purpose of keywords, taken from each document.
Next section analyzes related work. A brief expla-
nation of the LocalMaxs algorithm is presented in
section 3. In section 4 we propose the metrics Mk
and Sk and consider other measures. Results are
presented in section 5 and conclusion are made in
the last section.
2 Related Work
In (Cigarra?n et al, 2005; Liu et al, 2009; Hulth,
2004) authors propose extraction of noun phases
and keywords. However, these are not language-
independent approaches, since they use some
language-dependent tools such as stop-words re-
moving, lemmatization, part-of-speech tagging or
syntactic pattern recognition.
In (Delort et al, 2003), authors address the is-
sue of Web document summarization by context.
They consider the context of a Web document by
the textual content of all documents linking to it.
According to the authors, the efficiency of this
approach depends on the size of the content and
context of the target document. However, its ef-
ficiency also depends on the existence of links to
the target documents.
In (Aliguliyev, 2006) a generic summarization
method is proposed. It extracts the most rel-
evance sentences from the source document to
form a summary. The summary can contain the
main contents of different topics. This approach
is based on clustering of sentences and, although
results are not shown, it does not use language-
dependent tools.
Other Information Extraction methods rely on
predefined linguistic rules and templates to iden-
tify certain entities in text documents (Yangarber
and Grishman, 2000; Jacquemin, 2001). Again,
these are not language-independent approaches,
despite the good results that they give rise to.
Some approaches address specific-domain
problems. In (Alani et al, 2003), authors propose
a method to extract artist information, such as
name and date of birth from documents and
then generate his or her biography. It works
with meta-data triples such as (subject-relation-
object), using ontology-relation declarations and
lexical information. Clearly, this approach is not
language-independent. In (Velardi et al, 2001),
a method to extract a domain terminology from
available documents such as the Web pages is
proposed. The method is based on two measures:
Domain Relevance and Domain Consensus that
give the specificity of a terminological candi-
date. In (Mart??nez-Ferna?ndez et al, 2004) the
News specific-domain is addressed. Again, this
approach is not language-independent.
A supervised approach (Ercan and Cicekli,
2007) extracts keywords by using lexical chains
built from the WordNet ontology (Miller, 1991), a
tool which is not available for every language.
Rather than being dependent on specific lan-
guages, structured data or domain, we try to find
out more general and language-independent fea-
tures from free text data.
In (Silva and Lopes, 2009), a MWEs extractor
and a metric, LeastRvar, extracts keywords from
documents. However, single words are ignored
as possible keywords and their global results are
outperformed by our proposal.
3 Using LocalMaxs Algorithm to Extract
Keyword Candidates
We used the SCP f cohesion metric and the Lo-
calMaxs algorithm to extract MWEs from docu-
ment corpora. Although details about these tools
are given in (Silva and Lopes, 1999; Silva et al,
1999), here follows a brief description for paper
self-containment. Thus, LocalMaxs is based on
the idea that each n-gram1 has a kind of glue or
cohesion sticking the words together within the
n-gram. Different n-grams usually have different
cohesion values. One can intuitively accept that
there is a strong cohesion within the n-gram ?Gis-
card d?Estaing? i.e. between the words ?Giscard?
and ?d?Estaing?. However, one cannot say that
there is a strong cohesion within the 2-grams ?or
given? or within the ?of two?. Thus, in order to
1w1 . . . wn or (w1 . . . wn) are also used to denote an n-
gram of length n.
1150
measure the cohesion value not only of 2-grams,
but also for every n-gram of any size in the cor-
pus, we used the SCP f(.) metric:
SCP f(w1 . . . wn) =
p(w1 . . . wn)2
Avp (1)
Avp = 1n? 1
n?1?
i=1
p(w1 . . . wi) . p(wi+1 . . . wn)
(2)
where p(w1 . . . wn) is the probability of the n-
gram w1 . . . wn in the corpus. This way, any size
n-gram is transformed in a pseudo-bigram that re-
flects the average cohesion between any two ad-
jacent contiguous sub-n-gram of the original n-
gram. Now it is possible to compare cohesions
from n-grams of different sizes.
3.1 LocalMaxs Algorithm
LocalMaxs is a language independent algorithm
to filter out cohesive n-grams of text elements
(words, tags or characters), requiring no threshold
arbitrarily assigned.
Definition 1. Let W =w1 . . . wn be an n-gram
and g(.) a cohesion generic function. And let:
?n?1(W ) be the set of g(.) values for all con-
tiguous (n?1)-grams contained in the n-gram W ;
?n+1(W ) be the set of g(.) values for all con-
tiguous (n+1)-grams which contain the n-gram
W , and let len(W ) be the length (number of ele-
ments) of n-gram W . So, it is stated that
W is a Multi Element Unit (MEU) if and only if,
for ?x ? ?n?1(W ),?y ? ?n+1(W )
(len(W ) = 2 ? g(W ) > y) ?
(len(W ) > 2 ? g(W ) > x + y2 ) .
Then, for n-grams with n ? 3, LocalMaxs algo-
rithm elects every n-gram whose cohesion value
is greater than the average of two maxima: the
greatest cohesion value found in the contiguous
(n? 1)-grams contained in the n-gram, and the
greatest cohesion found in the contiguous (n+1)-
grams containing the n-gram. Thus, in the present
approach we used LocalMaxs as a MWEs extrac-
tor ? MWEs are MEUs where the elements are
words ? and used SCP f(.) cohesion measure
as the g(.) function referred in the algorithm defi-
nition above.
4 Selecting Keywords from MWEs
Not every MWE extracted by LocalMaxs has
equal relevance or semantic sharpness. Some
MWEs are vague in terms of semantic sharpness,
such as ?important meeting? or ?general use?;
other ones are very specific in terms of the topic
they point to, for example ?Assessment of the use
of Magnetic Resonans Tomografy?; some others
are (2-4)-gram strongly informative MWEs, fit-
ting the semantic sharpness of typical keywords
such as ?computer science? or ?Food and Agri-
culture Organization?, and will be privileged by
the metric we present in subsection 4.4.
Some single words have adequate semantic
sharpness to be included as keywords, such as
?Algebra? or ?Agriculture?, among others. How-
ever, most single words are not informative
enough for that purpose.
As a consequence, we felt the need to work
on adequate metrics to value and privilege the
strongly informative MWEs and single words in
order to find keywords in documents.
4.1 The Tf-Idf Metric
Tf?Idf (Term frequency?Inverse document fre-
quency) is a statistical metric often used in IR
and text mining. Usually, it is used to evaluate
how important a word is to a document in a cor-
pus. The importance increases proportionally to
the number of times a word/multiword appears in
the document but it is offset by its frequency in
the corpus. Thus, this is one of the metrics with
which we will try to privilege the most informa-
tive MWEs and 1-grams in each document.
Tf?Idf(W,dj) = p(W,dj) . Idf(W,dj) (3)
p(W,dj) =
f(W,dj)
Ndj
(4)
Idf(W,dj) = log
?D?
?{dj : W ? dj}?
(5)
where f(W,dj) if the frequency of word/multi-
word W in document dj and Ndj stands for the
number of words of dj ; ?D? is the number of doc-
uments of the corpus. So, Tf?Idf(W,dj) will
give a measure of the importance of W , that is a
MWE or a single word, within the particular doc-
ument dj . By the structure of term Idf we can see
1151
that it privileges MWEs and single words occur-
ring in less documents, particularly those occur-
ring in just one document.
4.2 The LeastRvar Metric
Most weakly relevant MWE and errors extracted
by LocalMaxs begin or end with a so called stop-
word, that is a highly frequent word appearing in
most documents. However, stop-words may ex-
ist in the middle of a relevant MWE, for example
?United States of America? or ?Life on Mars?; but
usually not in the leftmost or rightmost word of
the MWEs. By considering this, LeastRvar was
proposed in (Silva and Lopes, 2009):
LeastRvar(MWEi) = least(Lrv,Rrv) (6)
where Lrv = Rvar(leftmostw(MWEi)) ,
Rrv = Rvar(rightmostw(MWEi))
and
Rvar(W )= 1?D?
?
di?D
(p(W,di)? p(W, .)
p(W, .)
)2
.
(7)
p(W, .) means the average probability of the word
W considering all documents. Rvar(.) is ap-
plied to the leftmost and the rightmost word of the
MWE:
p(W, .) = 1?D?
?
di?D
p(W,di). (8)
Rvar(W ) measures the variation of the proba-
bility of the word W along all documents. Ap-
parently the usual formula of the variance (the
second moment about the mean), would measure
that variation; however, it would wrongly bene-
fit the very frequent words such as ?of?, ?the? or
?and?, among others. This happens because the
absolute differences between the occurrence prob-
abilities of any of those frequent words along all
documents is high, regardless of the fact that they
usually occur in every document. These differ-
ences are captured and over-valued by the vari-
ance since it measures the average value of the
quantity (distance frommean)2, ignoring the
order of magnitude of the individual probabilities.
Then, Rvar(.) divides each individual distance,
in the original formula of the variance, by the or-
der of magnitude of these probabilities, that is, the
mean probability, given by p(W, .); see equations
7 and 8.
Then, LeastRvar(MWEi) is given by the
least Rvar(.) values considering the leftmost
word and the rightmost word of MWEi. This
way, LeastRvar(.) tends to privilege informative
MWEs and penalize those multiword expressions
having semantically meaningless words in the be-
gin or in the end of it.
4.3 The LeastCv metric
In oder to try to obtain better results than those
produced by LeastRvar, we changed Rvar(.) to
an alternative to measure the relative variation
of the probability of the leftmost and rightmost
words in MWEs. Then we defined:
LeastCv(MWEi) = least(Lcv,Rcv) (9)
where Lcv = Cv(leftmostw(MWEi)) ,
Rcv = Cv(rightmostw(MWEi)) ,
Cv(W ) = ?(W )/?(W ) , (10)
?(W )=
???? 1?D?
?
di?D
(p(W,di) ? p(W, .))2 ,
(11)
and
?(W ) = p(W, .) ; (12)
p(W,di) and p(W, .) have the same meaning as in
equation 7. The reader may recognize Cv(.) as
the coefficient of variation, which is given by the
ratio of the standard deviation ? to the mean ?.
Results in section 5 will show that LeastCv also
tends to privilege informative MWEs.
4.4 Two New Metrics to Find Keywords
Considering the results obtained for LeastRvar
and LeastCv, as we will see in section 5, we
wanted to develop a better metric to find MWE
keywords and another one for single word key-
words. They were built by combining some im-
portant factors that we present next.
The Median of the MWE Words? Length:
Since most of the semantically meaningless words
1152
are small and long words usually have sharp
meaning, we considered the median length of the
words in each MWE to help on selecting the
most informative MWEs. By comparison, median
length showed better results than average length.
For example, MWE ?Language Institute? has an
average word length of 8.5 characters, but the
semantically equivalent ?Institute of Languages?
has a different average length of 6.66. On the con-
trary, the median length for both MWEs presents
more close values: ((8 + 9)/2 = 8.5) for ?Lan-
guage Institute? and 9 for ?Institute of languages?
(the middle number after sorting the MWE words
length: 2, 9 and 9). Thus, because the median
values is more robust to outliers than the aver-
age value, the length of the meaningless word
?of? was, say, ignored in the median calcula-
tion. In fact, those equivalent meaning MWEs
have similar median length values (8.5 and 9),
but not so similar average length values (8.5 and
6.66). Furthermore, the robustness of the median
length enables more similar values when consider-
ing MWEs in English and other equivalent MWEs
in other languages where stop words are more
used; for example ?e?coles de conduite? (driving
schools), ?produccio?n de batata? (potato produc-
tion), etc..
How Many Words for a Keyword? As the
reader may check in documents having associ-
ated keywords, we noticed that the main docu-
ment keywords are usually (2-4)-grams. So, we
defined a factor, Ckl(MWEi), to measure how
similar is the pseudo number of words of MWEi
to the typical number of words of keywords. We
define the pseudo number of words of a MWE:
Pnw(MWEi) =
NumChars(MWEi)
Med(MWEi)
.
(13)
NumChars(MWEi) stands for the number
of characters of MWEi and Med(MWEi) is
the median length of its words. Pnw(MWEi)
gives a value close to the number of mean-
ingful words of MWEi. For example,
Pnw(?Institute of Languages?) = 20/9 = 2.2
(close to 2); Pnw(?European Council?) =
15/7.5 = 2, etc.. Now, Ckl(.) is given by:
Ckl(MWEi) =
1
|Pnw(MWEi) ? T |+ 1
,
(14)
where T is the typical number of words of the key-
words. Maximum value for CkLen(MWEi) is 1;
it happens if Pnw(MWEi) equals to T . As we
will see by the results in section 5, we tried two T
values: 2.5 and 3.5; and compared results.
The Mk Metric for MWE Keywords: We
built Mk(.) metric by improving LeastRvar(.):
Mk(M)=LeastRvar(M).Med(M).Ckl(M)
(15)
Thus, Mk(.) privileges MWEs having not only
informative leftmost and rightmost words, but
also having long words and a pseudo number of
words close to the number of words of typical key-
words ? for reasons of lack of space, we used M
instead of MWEi in equation 15 ?.
The Sk Metric for Single Word Keywords:
We built Sk(.) from Rvar(.) ? see equation 7 ?
to measure how meaningful is each single word:
Sk(Wi) = Rvar(Wi).Len(Wi) . (16)
Len(Wi) means the length of words Wi. Thus,
Sk(.) privileges single words having, not only a
high relative variation of their probabilities along
all documents, but also being long words.
5 Results
We analyze the quality of the document descrip-
tors after applying the LocalMaxs extractor fol-
lowed by each of the six different metrics to three
different document corpora, each one for a differ-
ent language: English, French and Spanish. Met-
rics applied to MWEs were Tf?Idf , LeastCv,
LeastRvar, Mk [2.5] ? that is T = 2.5 in equa-
tion 14; and Mk [3.5]. Metrics applied to single
words were Tf?Idf and Sk.
5.1 The Document Descriptor
We decided to represent the core content of each
document by using its 15 most informative terms,
in the sense of keywords: 11 MWEs and 4 single
words. An independent evaluation criteria were
1153
defined by Prof. Francisca Xavier from the Lin-
guistics Department of Universidade Nova de Lis-
boa. It was considered that, for example, ?aim
of mission? and ?16 December 2003? are wrong
keywords, as the first one is a too vague noun
phrase and the second one, just a simple date. Rel-
evant MWEs such as ?nuclear weapons? and ?fi-
nancial crisis? were evaluated as keywords. How-
ever, although some proposed multi-word expres-
sions are not keywords, they are informative in the
context of the descriptor and correspond to well
formed morphosyntactic tags, for example, ?56%
of GDP? or ?comfort zone?: these near-miss cases
were classified as half-correct half-wrong terms;
the same classification was given to single words
such as ?macro-economic? ? see table 7 ? which,
although it?s not a noun, it?s an informative adjec-
tive.
Thus, for each document, the extracted MWEs
are sorted according to each metric and the top
11 MWEs are taken as the document?s MWEs de-
scriptor. The single words of the document are
also sorted according to one of the two applied
metrics (Tf?Idf or Sk). By ignoring the rest of
the MWEs and single words, there is document in-
formation which will be lost by these descriptors,
but they must be taken as core content descriptors,
not as complete/detailed reports of the documents.
Although descriptors are composed by MWEs and
single words, for better comparison of the metrics,
tables separately show MWE descriptors or single
word descriptors. Table 1 shows an example of a
document MWE descriptor resulting from the ap-
plication of one of the metrics (Mk) to the docu-
ment?s MWEs extracted by LocalMaxs algorithm:
5.2 The Multi-Language Corpora Test
We used the EUR-Lex corpora, http://eur-
lex.europa.eu/en/, containing European Union law
documents about several topics in several Euro-
pean languages. We took 60 documents written
in each language, English, French and Spanish to
form three different sub-corpora. These are un-
structured row text documents.
To evaluate the approach?s performance, we
used Precision and Recall concepts. Precision was
given by the number of keywords in the set of
Table 1: Example of an English Document MWE
Descriptor ? Application of the Mk [2.5] Metric.
enterprise profits
comfort zone
medium-sized enterprises
brain drain
cold war
Balance of Payment
56% of GDP
excessive deficit
looking ahead
exports and imports
Stability and Growth Pact
the 11 most scored MWEs proposed as descrip-
tor, by the combination LocalMaxs?metric used,
divided by 11. Recall was given by the number
of keywords that are simultaneously in the doc-
ument?s descriptor proposed and in the set made
of the 11 most informative keywords of the docu-
ment, divided by 11.
According to the criteria mentioned above, this
is the evaluation of the descriptor shown in ta-
ble 1, considering Precision: 8 MWEs can be ac-
cepted as keywords (1st, 3rd, 4th, 5th, 6th, 8th,
10th and 11th); 2 near-miss MWEs (2nd and 7th);
and 1 weak or wrong MWE (9th). So, precision
is (8 + 2 ? 0.5)/11 = 0.818. Concerning the
document this descriptor represents, there are 3
strong keywords that should be in the descriptor,
but they weren?t: ?financial crisis?, ?structural re-
forms? and ?macroeconomic imbalances?. Thus,
Recall is 8/11 = 72.7 for this case.
5.3 Results for Different Metrics and
Languages
By table 2 we may see that for the same metric,
Precision or Recall values are similar for English,
French and Spanish. So, this approach does not
seem to privilege any of these languages, and we
believe that probably this happens for many other
languages, as no specific morphosyntactic infor-
mation was used. Even the difference between
Recall values for Spanish and English produced
by LeastRvar (0.61 and 0.63) would probably
decrease if the test corpora had more documents.
Table 2 also shows that Tf?Idf presents the poor-
1154
Table 2: Precision and Recall Average Values for
the Document MWE Descriptors.
Language Metric Precision Recall
Tf?Idf 0.51 0.35
LeastCv 0.62 0.61
English LeastRvar 0.65 0.63
Mk [2.5] 0.76 0.72
Mk [3.5] 0.74 0.68
Tf?Idf 0.50 0.35
LeastCv 0.62 0.60
French LeastRvar 0.64 0.63
Mk [2.5] 0.75 0.71
Mk [3.5] 0.73 0.68
Tf?Idf 0.51 0.34
LeastCv 0.61 0.60
Spanish LeastRvar 0.64 0.61
Mk [2.5] 0.75 0.72
Mk [3.5] 0.74 0.67
est results. In fact, due to its structure ? see
equation 3 ? we can see that MWEs that occur
many times in just one document are the most val-
ued/privileged ones. This explains why the de-
scriptors made by this measure tend to include too
specific/local MWEs, regardless of some impor-
tant ones. Table 3 shows a document descriptor
generated by the combination LocalMaxs?Tf?
Idf : for example MWE ?new Members? occurs
in just one document, 10 times; however, ?new
Members? is not a keyword. This is the descriptor
of the same document from where other descrip-
tors were generated by the combinations including
LeastRvar and Mk [2.5], and shown in tables 4
and 1.
For reasons of space limitation we don?t show
descriptors produced by LeastCv and MK [3.5]
metrics. However, table 2 shows that LeastCv
was outperformed by LeastRvar. This table also
shows that Mk [2.5] metric presents the highest
Precision (0.76, 0.75 and 0.75 for English, French
and Spanish). The highest Recall values are also
obtained for the same metric: 0.72, 0.71 and 0.72
for the same languages.
Tables 5 and 6 show examples of MWE de-
scriptors of French and Spanish documents, by the
application of Mk [2.5] as it produced the best re-
Table 3: Example of an English Document MWE
Descriptor ? Application of the Tf?Idf Metric.
in the new Member States
in the new Member
new Members
Single Market
income convergence
some of the new Member
financial crisis
structural reforms
new and old
euro area
reap the full benefits of the Single Market
Table 4: Example of an English Document MWE
Descriptor ? Application of the LeastRvar Met-
ric.
five years
Cold War
old Members
enterprise profits
Central Bank
Excessive Deficit
medium-sized enterprises
comfort zone
56% of GDP
1.5% of GDP
brain drain
sults.
Tables 7 and 8 show examples of single word
descriptors for the same document described in ta-
ble 1. As we could expect, Precision and Recall
values for single word descriptors are lower than
the values for MWEs descriptors, since singles
words are usually semantically less sharp than
multiwords: see table 9. Sk shows better perfor-
mance than Tf?Idf , specially for Recall.
6 Conclusions
Keywords are semantic tags associated to docu-
ments, usually declared manually by users. These
tags form small document descriptors and enable
applications to access to the summarized docu-
ments? core content. This paper proposes an ap-
proach to automatically generate document de-
1155
Table 5: Example of a French Document MWE
Descriptor ? Application of the Mk [2.5] Metric.
moto-fraises et motofaucheuses
agrumeraies et oliveraies
hommes Travail
Fumier liquide
familiale occupe?e
Mieux le?gife?rer
d?arbres fruitiers
Superficie irrigue?e
Main-d?oeuvre non familiale
activite?s lucratives
Alignements d?arbres
Table 6: Example of a Spanish Document MWE
Descriptor ? Application of the Mk [2.5] Metric.
ingredientes de cosme?ticos
combinaciones de ingredientes
someter a ensayo
Sustancias y Preparados
toxicidad aguda
irritacio?n ocular
fototoxicidad aguda
explicaciones dadas
corrosio?n cuta?nea
animales utilizados
Sustancias y Preparados Qu??micos
Table 7: Example of an English Document Single
Word Descriptor ? Application of the Sk Metric.
vulnerabilities
growth-enhancing
post-enlargement
macro-economic
Table 8: Example of an English Document Sin-
gle Word Descriptor ? Application of the Tf?Idf
Metric.
economic
new
enlargement
reforms
Table 9: Precision and Recall Average Values for
the Document Single Word Descriptors.
Language Metric Precision Recall
English Tf?Idf 0.52 0.36
Sk 0.55 0.48
French Tf?Idf 0.51 0.37
Sk 0.54 0.47
Spanish Tf?Idf 0.52 0.37
Sk 0.56 0.48
scriptors, as a language-independent and domain-
independent alternative to related work from other
authors. This approach uses LocalMaxs algorithm
to extract MWEs, and two new statistical metrics,
Mk and Sk, to select the 15 most relevant MWEs
and single words from each document in order to
form document descriptors.
Comparing the results produced by Mk with
the second best metric, LeastRvar, we may con-
clude that the introduction of the median of the
words? length of each MWE and the preference
for (2-4)-grams, improve the quality of docu-
ment descriptors by about 11% and 9% for Pre-
cision and Recall, respectively. Furthermore, by
comparison of Mk [2.5] and Mk [3.5] results we
conclude that keywords are mostly (2-3)-grams,
rather than (3-4)-grams or longer n-grams.
Results also showed that Precision and Recall
values are similar for the three languages tested
(English, French and Spanish), which enable us
to expect similar performance to other languages.
Apart from the Precision and Recall values, doc-
ument descriptors made by this approach does in-
deed capture the core content of each document.
We believe this may contribute to improve doc-
ument summarization. Future work will include
tests in other languages and we will work to im-
prove results, specially for single words.
References
Alani, Harith, Kim Sanghee, David E. Millard, Mark J.
Weal, Paul H. Lewis, Wendy Hall and Nigel Shad-
bolt. 2003. Automatic Extraction of Knowledge
from Web Documents. In Proceedings of Workshop
of Human Language Technology for the Semantic
Web and Web Services, 2nd International Seman-
1156
tic Web Conference. October 20th, Sanibel Island,
Florida, USA.
Aliguliyev, Ramiz M. 2006. A Novel Partitioning-
Based Clustering Method and Generic Document
Summarization. In Proceedings of the 2006
IEEE/Web Intelligence/Association for Computing
Machinery and the Intelligent Agent Technology
International Conference (2006 Workshops)(WI-
IATW?06). December 18-22, Hong Kong, China.
Cigarra?n, Joan. M., Anselmo Peas, Julio Gonzalo and
Felisa Verdejo. 2005. Automatic Selection of Noun
Phrases as Document Descriptors in an FCA-Based
Information Retrieval System. B. Ganter and R.
Godin (Eds.). ICFCA 2005, Lecture Notes in Com-
puter Science 3403, pp. 49-63. Springer-Verlag.
Ciravegna, Fabio, Alexeie Dingli, David Guthrie and
Yorick Wilks. 2003. Mining Web Sites Using
Unsupervised Adaptive Information Extraction. In
Proceedings of the 10th Conference of the European
Chapter of the Association for Computational Lin-
guistics. April, 12-17. Budapest, Hungary.
Delort, J.-Y., B. Bouchon-Meunier and M. Rifqi.
2003. Enhanced Web Document Summarization
Using Hyperlinks. In Proceedings of the fourteenth
Association for Computing Machinery conference
on Hypertext and hypermedia. August 26-30, Not-
tingham, UK.
Ercan, Gonenc and Ilyas Cicekli. 2007. Using lexi-
cal chains for keyword extraction. Information Pro-
cessing and Management: an International Jour-
nal archive. Volume 43, Issue 6, November, Pages
1705-1714, Pergamon Press, Inc.. ISSN 0306-4573.
Hulth, Anette. 2004. Enhancing linguistically ori-
ented automatic keyword extraction. Proceedings of
Human Language Technology-North American As-
sociation for Computational Linguistics 2004 con-
ference. Pag.17-20. May 02-07. Boston, Mas-
sachusetts. Publisher: Association for Computa-
tional Linguistics, Morristown, NJ, USA.
Jacquemin Christian. 2001. Spotting and Discovering
Terms through Natural Language Processing. MIT
Press, ISBN 0262100851.
Liu, Feifan, Deana Pennell, Fei Liu and Yang Liu.
2009. Unsupervised approaches for automatic key-
word extraction using meeting transcripts. Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics. May 31-June 05. Boulder, Colorado.
Mart??nez-Ferna?ndez, J. L., A. Garc??a-Serrano, P.
Mart??nez, J. Villena. 2004. Automatic Keyword
Extraction for News Finder. Lectures Notes in Ar-
tificial Intelligence, Springer-Verlag, volume 3094,
pages 99?119.
Miller, George A. 1991. The science of words. Scien-
tific American Library, New York.
Silva, Joaquim and Gabriel Lopes. 1999. A Local
Maxima Method and a Fair Dispersion Normaliza-
tion for Extracting Multi-word Units. In Proceed-
ings of the 6th Meeting on the Mathematics of Lan-
guage, pages 369-381. 23-25 July, University of
Central Florida, Orlando.
Silva, Joaquim and Gabriel Lopes. 2009. A Docu-
ment Descriptor Extractor Based on Relevant Ex-
pressions. 14 Encontro Portugus para a Intelign-
cia Artificial (Fourteenth Portuguese Conference on
Artificial Intelligence). October 12-15. Univerity of
Aveiro. Lectures Notes in Artificial Intelligence,
Springer-Verlag, volume 5816, pages 646-657.
Silva, Joaquim, Gael Dias, Sylvie Guillore? and
Gabriel Lopes. 1999. Using LocalMaxs Al-
gorithm for the Extraction of Contiguous and
Non-contiguous Multi-word Lexical Units. 9th
Portuguese Conference on Artificial Intelligence.
September, vora,Portugal. Lectures Notes in Arti-
ficial Intelligence, Pedro Barahora and Jos Alferes
(Eds.). Springer-Verlag, volume 1695, pages 113-
132.
Yangarber, Roman and Ralph Grishman. 2000. Ma-
chine Learning of Extraction Patterns from Unan-
otated Corpora: Position Statement. Workshop on
Machine Learning for Information Extraction. Held
in conjunction with the 14th European Conference
on Artificial Intelligence (ECAI). 21 August. Berlin,
Humboldt University.
Velardi, Paula, Michele Missikoff, and Roberto Basili.
2001. Identification of relevant terms to support
the construction of Domain Ontogies. Associa-
tion for Computational Linguistics-European Asso-
ciation for Computational Linguistics Workshop on
Human Language Technologies. July 6-7. Toulouse,
France.
1157
Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 135?137,
October 25, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Context Sense Clustering for Translation
Jo?o Casteleiro
Universidade Nova de Lisboa
Departamento de Inform?tica
2829-516 Caparica, Portugal
casteleiroalves@gmail.com
Gabriel Lopes
Universidade Nova de Lisboa
Departamento de Inform?tica
2829-516 Caparica, Portugal
gpl@fct.unl.pt
Joaquim Silva
Universidade Nova de Lisboa
Departamento de Inform?tica
2829-516 Caparica, Portugal
jfs@fct.unl.pt
Extended Abstract
Word sense ambiguity is present in all words 
with more than one meaning in several natural 
languages and is a fundamental characteristic of 
human language. This has consequences in trans-
lation as it is necessary to find the right sense and 
the correct translation for each word. For this 
reason, the English word fair can mean reasona-
ble or market such as plant also can mean factory
or herb.
The disambiguation problem has been recog-
nize as a major problem in natural languages 
processing research. Several words have several 
meanings or senses. The disambiguation task 
seeks to find out which sense of an ambiguous 
word is invoked in a particular use of that word. 
A system for automatic translation from English
to Portuguese should know how to translate the 
word bank as banco (an institution for receiving, 
lending, exchanging, and safeguarding money), 
and as margem (the land alongside or sloping 
down to a river or lake), and also should know 
that the word banana may appear in the same 
context as acerola and that these two belongs to 
hyperonym fruit. Whenever a translation systems 
depends on the meaning of the text being pro-
cessed, disambiguation is beneficial or even nec-
essary. Word Sense Disambiguation is thus es-
sentially a classification problem; given a word X
and an inventory of possible semantic tags for 
that word that might be translation, we seek 
which tag is appropriate for each individual in-
stance of that word in a particularly context.
In recent years research in the field has 
evolved in different directions. Several studies 
that combine clustering processes with word 
senses has been assessed by several. Apidianaki 
in (2010) presents a clustering algorithm for 
cross-lingual sense induction that generates bi-
lingual semantic inventories from parallel corpo-
ra. Li and Church in (2007) state that should not 
be necessary to look at the entire corpus to know 
if two words are strongly associated or not, thus, 
they proposed an algorithm for efficiently com-
puting word associations. In (Bansal et al., 
2012), authors proposed an unsupervised method 
for clustering translations of words through 
point-wise mutual information, based on a mono-
lingual and a parallel corpora. Gamallo, Agustini 
and Lopes presented in (2005) an unsupervised 
strategy to partially acquire syntactic-semantic 
requirements of nouns, verbs and adjectives from 
partially parsed monolingual text corpora. The 
goal is to identify clusters of similar positions by 
identifying the words that define their require-
ments extensionally. In (1991) Brown et al. de-
scribed a statistical technique for assigning sens-
es to words based on the context in which they 
appear. Incorporating the method in a machine 
translation system, they have achieved to signifi-
cantly reduce translation error rate. Tufis et al. in
(2004) presented a method that exploits word 
clustering based on automatic extraction of trans-
lation equivalents, being supported by available 
aligned wordnets. In (2013), Apidianaki de-
scribed a system for SemEval-2013 Cross-
lingual Word Sense Disambiguation task, where 
word senses are represented by means of transla-
tion clusters in a cross-lingual strategy.
In this article, a Sense Disambiguation ap-
proach, using Context Sense Clustering, within a 
mono-lingual strategy of neighbor features is 
proposed. We described a semi-supervised meth-
od to classify words based on clusters of contexts 
strongly correlated. For this purpose, we used a 
covariance-based correlation measure (Equation 
1). Covariance (Equation 2) measure how much 
two random variables change together. If the 
values of one variable (sense x) mainly corre-
spond to the values of the other variable (sense 
y), the variables tend to show similar behavior 
135
and the covariance is positive. In the opposite 
case, covariance is negative. Note that this pro-
cess is computationally heavy. The system needs 
to compute all relations between all features of 
all left words. If the number of features is very 
large, the processing time increases proportional-
ly.
?) ????, ?) =  ? )???, ? )??)???, ?) + ??)???, ?)
(1)
?)???, ?) = 1? ? 1 ? (???)??, ?). ???)??, ?))
??
????
 
(2)
Our goal is to join similar senses of the same 
ambiguous word in the same cluster, based on 
features correlation. Through the analysis of cor-
relation data, we easily induce sense relations. In 
order to streamline the task of creating clusters, 
we opted to use WEKA tool (Hall et al., 2009)
with X-means (Pelleg et al., 2000) algorithm.
Clusters
fructose, glucose
football, chess
title, appendix, annex
telephone, fax
liver, hepatic, kidney
aquatic, marine
disciplinary, infringement, criminal
Table 1. Well-formed resulting clusters
In order to determine the consistence of the 
obtained clusters, all of these were evaluated 
with V-measure. V-measure introduce two crite-
ria presented in (Rosenberg and Hirschberg, 
2007), homogeneity (h) and completeness (c). A 
clustering process is considered homogeneously 
well-formed if all of its clusters contain only data 
points which are members of a single class. 
Comparatively, a clustering result satisfies com-
pleteness if all data points that are members of a 
given class are elements of the same cluster.
Analysing the results of context sense clusters 
obtained (Table 1) we easily understand that al-
most all clusters are generally well formed, get-
ting a final V-measure average rating of 67%.
Finally, in order to train a classifier we choose 
to use a training data set with 60 well formed 
clusters (with V-measure value ranging between 
0.9 and 1). Our testing data set is composed by 
60 words related to the clusters but which are not 
contained there. The classifier used was a Sup-
port Vector Machine (SVM) (2011). The kernel 
type applied was the Radial Basis Function
(RBF). This kernel non linearly maps samples 
into a higher dimensional space, so it can handle 
the case when the relation between class labels 
and attributes is nonlinear, that is the case. Each 
word of training and testing data sets were en-
coded according the frequency in a corpora of all 
characteristics contained in the clusters. Our pur-
pose was to classify each one of the new poten-
tial ambiguous words, and fit it in the corre-
sponding cluster (Table 2 and Table 3).
Test Words Label assigned by (SVM)
Fruit Cluster 29
Infectious Cluster 7
Kiwi Cluster 60
Back Cluster 57
Legislative Cluster 34
Grape Cluster 29
Russian Cluster 59
Table 2. Results generated by (SVM)
Clusters Content of Clusters 
Cluster 7 Viral, contagious, hepatic
Cluster 29 Banana, apple
Cluster 34 Legal, criminal, infringement
Cluster 57 Cervical, lumbar
Cluster 59 French, Italian, Belgian, German
Cluster 60 Thyroid, mammary
Table 3. Cluster correspondence
The obtained results showed that almost all 
words were tagged in the corresponding cluster. 
Evaluating system accuracy we obtained an av-
erage value of 78%, which means that from the 
60 tested words, 47 words were assigned to the 
corresponding context cluster.
136
References
Marianna Apidianaki, Yifan He, et al. 2010. An 
algorithm for cross-lingual sense-clustering 
tested in a mt evaluation setting. In Proceed-
ings of the International Workshop on Spoken 
Language Translation, pages 219?226.
Li, P., Church, K.W.: A sketch algorithm for es-
timating two-way and multi-way associations. 
Computational Linguistics 33 (3), 305 - 354 
(2007).
Bansal, M., DeNero, J., Lin, D.: Unsupervised 
translation sense clustering. In: Proceedings of 
the 2012 Conference of the North American 
Chapter of the Association for Computational 
Linguistics: Human Language Technologies. 
pp. 773-782. Association for Computational 
Linguistics (2012).
Gamallo, P., Agustini, A., Lopes, G.P.: Cluster-
ing syntactic positions with similar semantic 
requirements. Computational Linguistics 
31(1), 107-146 (2005).
Brown, P.F., Pietra, S.A.D., Pietra, V.J.D., Mer-
cer, R.L.: Word-sense disambiguation using 
statistical methods. In: Proceedings of the 29th 
annual meeting on Association for Computa-
tional Linguistics. pp. 264-270. Association 
for Computational Linguistics (1991).
TufiS, D., Ion, R., Ide, N.: Fine-grained word 
sense disambiguation based on parallel corpo-
ra, word alignment, word clustering and 
aligned wordnets. In: Proceedings of the 20th 
international conference on Computational 
Linguistics. p. 1312. Association for Compu-
tational Linguistics (2004).
Apidianaki, M.: Cross-lingual word sense dis-
ambiguation using translation sense clustering. 
In: Proceedings of the 7th International Work-
shop on Semantic Evaluation (SemEval 2013). 
pp. 178-182. *SEM and NAACL (2013)
Mark Hall, Eibe Frank, Geoffrey Holmes, Bern-
hard Pfahringer, Peter Reutemann, and Ian H 
Witten. 2009. The weka data mining software: 
an update. ACM SIGKDD Explorations 
Newsletter, 11(1):10?18.
Dan Pelleg, Andrew W Moore, et al. 2000. X-
means: Extending k-means with efficient es-
timation of the number of clusters. In ICML, 
pages 727?734.
Andrew Rosenberg and Julia Hirschberg. 2007. 
Vmeasure: A conditional entropy-based exter-
nal cluster evaluation measure. In EMNLP-
CoNLL, volume 7, pages 410?420.
Chih-Chung Chang and Chih-Jen Lin. 2011. 
Libsvm: a library for support vector machines. 
ACM Transactions on Intelligent Systems and 
Technology (TIST), 2(3):27.
137
