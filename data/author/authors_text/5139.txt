Integrated Feasibility Experiment for Bio-Security: IFE-Bio
A TIDES Demonstration
Lynette Hirschman, Kris Concepcion, Laurie Damianos, David Day, John Delmore, Lisa Ferro,
John Griffith, John Henderson, Jeff Kurtz, Inderjeet Mani, Scott Mardis, Tom McEntee, Keith
Miller, Beverly Nunan, Jay Ponte, Florence Reeder, Ben Wellner, George Wilson, Alex Yeh
The MITRE Corporation
Bedford, Massachusetts, USA and
McLean, Virginia, USA
781-271-7789
lynette@mitre.org
ABSTRACT
As part of MITRE?s work under the DARPA TIDES
(Translingual Information Detection, Extraction and
Summarization) program, we are preparing a series of
demonstrations to showcase the TIDES Integrated Feasibility
Experiment on Bio-Security (IFE-Bio).  The current
demonstration illustrates some of the resources that can be made
available to analysts tasked with monitoring infectious disease
outbreaks and other biological threats.
Keywords
Translation, information extraction, summarization, topic
detection and tracking, system integration.
1. INTRODUCTION
The long-term goal of TIDES is to provide delivery of
information on demand in real-time from live on-line sources. For
IFE-Bio, the resources made available to the analyst include e-
mail, news groups, digital library resources, and eventually (in
later versions), topic-specific segments from broadcast news.
Because of the emphasis on global monitoring, there is a need to
process incoming information in multiple languages.  The system
must deliver the appropriate information content in the
appropriate form and in the appropriate language (taken for now
to be English). This means that the IFE-Bio system will have to
deliver news stories, clusters of relevant documents, threaded
discussions, alerts on new events, tables, summaries (particularly
over document collections), answers to questions, graphs and geo-
spatial  temporal displays of information.
The demonstration system for the Human Language Technology
Conference in March 2001 represents an early stage of the full
IFE-Bio system, with an emphasis on end-to-end processing.
Future demonstrations will make use of MITRE?s Catalyst
architecture, providing an efficient, scalable architecture to
facilitate  integration of multiple stages of linguistic processing.
By June 2001, the IFE-Bio system will provide richer linguistic
processing through the integration of modules contributed by
other TIDES participants. By June 2002, the IFE-Bio system will
include additional functionality, such as real-time broadcast news
feeds, new machine translation components, support for question-
answering, cross-language information retrieval, multi-document
summarization, automatic extraction and normalization of
temporal and spatial information, and automated geospatial and
temporal displays.
2. The IFE-Bio System
The current demonstration (March 2001) highlights the basic
functionality required by an analyst, including:
? Capture of sources, including e-mail, digital library
material, news groups, and web-based resources;
? Categorizing of the sources into multiple orthogonal
hierarchies useful to the analyst, e.g., disease, region, news
source, language;
? Processing of the information through various stages,
including ?zoning? of the text to select the relevant portions
for processing; named entity detection, event detection,
extraction of temporal information, summarization, and
translation from Spanish, Portuguese, and Chinese into
English;
? Access to the information through use of any mail and news
group reader, which allows the analyst to organize, save, and
share the information in a familiar, readily accessible
environment;
? Display of the information in alternate forms, including
color-tagged documents, tables, summaries, graphs, and
geospatial, map-based displays.
Figure 1 below shows the overall functionality envisioned
for the IFE-Bio system, including capture, categorizing,
processing, access and display.
Collection capability for the current IFE-Bio system includes
email, news groups, journals, and Web resources. We have a
complete copy of the ProMED mailings (a moderated source
tracking global infectious disease outbreaks), and are routinely
collecting other information sources from the World Health
Organization and CDC.  In addition, we are collecting several
general global news feeds. Current volume is around 2000
messages per day; we estimate capacity for the current system at
around 4500 messages/day. Once we have integrated a filtering
capability, we expect the volume of messages saved in IFE-Bio
should drop significantly, since many of the global news services
report on a wide range of events and not all need to be passed on
to IFE-Bio analysts.  The categorizing of sources is done based on
the message header. The header is synthesized by extracting key
information about disease name, the country, and other relevant
information such as type of victim and source of information, as
well as date of message receipt.
The processing for the current demonstration system uses a
limited subset of the Catalyst architecture capabilities and a
number of in-house linguistic modules. The linguistic modules in
the current demonstration system include tokenization, sentence
segmentation, part-of-speech tagging, named entity detection,
temporal extraction (Mani and Wilson 2000) and source-specific
event detection.  In addition, we have incorporated the
CyberTrans embedded machine translation system which ?wraps?
available machine translation engines to make them available via
an e-mail or Web interface (Reeder 2000). Single document
summarization is performed by the MITRE WebSumm system
(Mani and Bloedorn 1999).
We carefully chose a light-weight interface mechanism for
delivery of the information to the analyst.  By treating the
incoming streams of data as feeds to a news server, the analyst can
inspect and organize the information using a familiar news and e-
mail browser. The analyst can subscribe to areas of interest, flag
important messages, watch specific threads, and create tailored
filters for monitoring outbreaks. The stories are crossed-posted to
multiple relevant news groups, based on the information in the
header, e.g., a story on Ebola in Africa would be cross posted to
the Africa regional newsgroup and to the Ebola disease
newsgroup. Search by subject and date allow the analyst to select
subsets of the messages for further processing, annotation or
sharing.  The news client provides notification of incoming
messages. In later versions, we plan to integrate topic detection
and tracking capabilities, to provide improved filtering and
routing of messages, as well as detection of new topics.  The use
of this simple delivery mechanism provides a familiar
environment with almost no learning curve, and it avoids issues of
platform and operating system dependence.
Finally, the system makes use of several different devices to
display the information appropriately. Figure 2 shows the layout
of the Netscape news browser interface.  It includes the list of
newsgroups that have been subscribed to (on the left), the list of
messages from the chosen newsgroup (on top), and a particular
message with color-coded named entities (including disease terms
displayed in red, so that they are easy to spot in the message).
What is the status of the
current Ebola outbreak?
The epidemic is contained;
as of 12/22/00, there were 
421 cases with 162 deaths
Interaction
CDC
WHO
Medical
literatureEmail:
ProMed
~ 2500
 stories/day
Internl
News
Sources  Capture
Translingual 
Information 
Detection 
Extraction 
Summarization 
U niden tif ied h emor rhagic  f
U niden tif ied h emor rhagic  f
Ebola hemorr hagic  fever  in
Re :  Ebo la hemorrhagi. ..
R e: Ebola hemo rrha gi...
ProMED
A nnotator
Ja ne Analyst
10 /17/00 1 9:37
10 /17/00 2 0:42
10 /18/00 7 :42
High
Norm al
Normal
read
rep lied
ProMED
10 /18/00 1 2:3 4 High un read
Ebola hemorr hagic  fever  in
Sour ce
D ate
Priority Status
10   99
0   105
1   57
0   10
2   34
0   50
1   1
0   25
5   200
0   45
0   0
0   0
0   0
0   0
0   6
0   32
0   3
0   1
High
Norm al
High
High
Ebola hemorrhagic feve r  -  Ugan da
U nf ilte red
O utbr eak
     C holer a
     D engue  Fe ve r
     Eb ola
I nfras tructure?
N atu ral  Di sas. ..
Spi lls
A cc id en ts
W M D Tra ckin. ..
Sus picious Il ln. ..
Sus picious De.. .
Pos sible  Biol o. ..
Pathogen threa ?
- --- --- ---------------------
W orkspa ce
      E bola
      D ra fts
      Re ports
D isease
R e: Ebola hemo rrha gi...
Location
U NK
U NK
Ebola
Ebola
Ebola
Ebola
Rabies
Rabies
U gan da
U gan da
U gan da
K eny a
U gan da
IHT
ProMED
WHO
Jo e Analyst
D ate
10 /14/00 2 3:06
10 /15/00 1 0:50
10 /16/00 2 1:45
10 /17/00 1 9:12
read
read
read
read
un read
Date: 10/16/00
Disease: Ebo la
Descripto r: hem orrh agic fever
Locatio n:          Ugan da
Disease Date:     10/14/00
Ho spital: mission ary hosp ital  in Gulu
New cases:  at least  7
Total  cases: 51
Total  dead:       31
Ebola hemorr hagic  fever  -
Ugand an M ini stry  ide ntif ies Eb ola  virus as t he c ause of  the outbreak.  KA MP ALA :
The  dreade d Eb ola  virus that struck over 300  peopl e i n Kikwit,  in  t he D emocratic
Rep ub lic  of Con go  in  1995, has ki lled 31  people in northe rn Ugan da.  A  U gandan
M ini s tr y of Heal th  sta tement  said l aboratory test s had r eveale d that  the Ebola vi rus
was  t he caus e of the  epidemi c hemorr hagic feve r whi ch has been r agi ng in the  G ulu
dis trict  since Septe mbe r.   Thr ee  of the dea d wer e s tud ent nur ses , who tre ated the first
Eb ola  patients admitt ed to a  Lac or  mis sionary hosp it al in  Gu lu  tow n.  A  task force
he ade d by G ul u dis trict adm ini str ator, Walte r O ch ora , has bee n se t up to co-or dina te
efforts to control the epi demi c.  F ie ld offic ials i n  Gul u tol d the Ka mpala-based Ne w
H ttp: //ti des2000.mi tre.org/
Pr oM ED /10162000/34n390h.ht ml
U gan da
News Repository
CATALYSTEntity Tagging
Event Extraction
Translation
Summarization
Alerting
Change detection
Threading
Cross-language IR
Topic clustering
Figure 1: Overview of the IFE-Bio Demonstration System
Local,
private
workspace
Documents
automatically
categorized
into shared,
tailorable
hierarchy
Sort by disease, location, source, date, etc.
Associated
meta-data:
header,
event,
summary,
named-entity
Figure 2: Screenshot of IFE-Bio Interface Using News Group Reader
Figure 3: Sample Summarization Automatically Generated by WebSumm
There are multiple display modalities available. The message in
Figure 2 contains a short tabular display in the beginning,
identifying disease, region and victim type. Below that is a URL
to a document summary, created by MITRE?s WebSumm system
(see Figure 3 for a sample summary).   If an incoming message is
in a language other than English, then CyberTrans is called to run
code set and language identification modules, and the language is
translated into English for further processing. Figure 4 below
shows a sample translated message; note that there are a number
of untranslated words, but it is still possible to get the gist of the
message.
In addition, we are working on a mechanism to provide
geographic and eventually, temporal display of outbreak
information. Figure 5 shows the stages of processing involved.
Stage 1 shows onamed entity and temporal tagging to identify the
items of interest. These are combined into disease events by
further linguistic processing; the result is shown in the table in
Stage 2. This spreadsheet of events serves as input for a map-
based display, shown in Stage 3. The graph plots number of new
cases and number of cumulative cases over time.  In the map, the
size of the outer dot represents total number of cases to date, and
the inner dot represents new cases.  This allows the analyst to
visualize spread of the disease, as well as the stage of the outbreak
(spreading or subsiding).
3. REFERENCES
[1] Mani, I. and Bloedorn, E. (1999). "Summarizing
Similarities and Among Related Documents".
Information Retrieval 1(1): 35-67.
[2] Mani, I. and Wilson, G. (2000). "Robust Temporal
Processing of News," Proceedings of the 38th Annual
Meeting of the Association for Computational
Linguistics (ACL'2000), 69-76. New Brunswick, New
Jersey. Association for Computational Linguistics.
[3] Reeder, F.  (2000) "At Your Service:  Embedded MT
as a Service",  NAACL Workshop on Embedded MT,
March, 2000.
Figure 4: Translation from Portuguese to English Produced by CyberTrans
1. Annotate entities of interest via XML
Dise a se Source Country City_na m eDa te Ca se s Ne w _ca se s De a d
Ebola PROM ED Uganda G ula 26-O ct-2000 182 17 64
Ebola PROM ED Uganda G ula 5-Nov-2000 280 14 89
Ebola PROM ED Uganda G ulu 13-O ct-2000 42 9 30
Ebola PROM ED Uganda G ulu 15-O ct-2000 51 7 31
Ebola PROM ED Uganda G ulu 16-O ct-2000 63 12 33
Ebola PROM ED Uganda G ulu 17-O ct-2000 73 2 35
Ebola PROM ED Uganda G ulu 18-O ct-2000 94 21 39
Ebola PROM ED Uganda G ulu 19-O ct-2000 111 17 41
2. Assemble entities into events
0
50
100
150
200
250
300
350
400
10/
13/
200
0
10/
20/
200
0
10/
27/
200
0
11/
3/2
000
11/
10/
200
0
11/
17/
200
0
11/
24/
200
0
T IME
Nu
m
be
r C
as
es
Cases
New_cases
Dead
3. Display events...
   Total Cases   New Cases
Figure 5: Steps in Extraction to Support Temporal and Geospatial Displays of Disease Outbreak
Inferring Temporal Ordering of Events in News 
Inderjeet Mani  
The MITRE Corporation 
7515 Colshire Drive 
McLean, VA 22102 
imani@mitre.org 
Barry Schiffman 
Columbia University 
1214 Amsterdam Avenue 
New York, NY 10027 
bschiff@cs.columbia.
edu 
Jianping Zhang 
The MITRE Corporation 
7515 Colshire Drive 
McLean, VA 22102 
 jzhang@mitre.org 
 
 
 
 
Abstract 
This paper describes a domain-independent, 
machine-learning based approach to tempo-
rally anchoring and ordering events in news. 
The approach achieves 84.6% accuracy in 
temporally anchoring events and 75.4% accu-
racy in partially ordering them.  
1 
2 
Introduction 
Practical NLP applications such as text summariza-
tion and question-answering place increasing demands 
on the processing of temporal information. In multi-
document summarization of news, it is important to 
know the relative order of events so as to correctly 
merge and present information. In question-answering, 
one would like to be able to ask when an event occurs, 
or what events occurred prior to a particular event. Such 
capabilities presuppose an ability to infer the temporal 
order of events in discourse.  
A number of different knowledge sources appear to 
be involved in inferring event ordering (Lascarides and 
Asher 1993), including tense and aspect (1), temporal 
adverbials (2), and world knowledge (3).  
(1) Max entered the room. He had drunk/was drink-
ing the wine. 
(2) A drunken man died in the central Phillipines 
when he put a firecracker under his armpit. 
(3) U. N. Secretary- General Boutros Boutros-Ghali 
Sunday opened a meeting of ....Boutros-Ghali ar-
rived in Nairobi from South Africa, ? 
As (Bell 1999) has pointed out, the temporal struc-
ture of news is dictated by perceived news value rather 
than chronology. Thus, the latest news is often pre-
sented first, instead of events being described in order of 
occurrence (the latter ordering is called the narrative 
convention).  
This paper describes a domain-independent ap-
proach to temporally anchoring and ordering events in 
news. The approach is motivated by a pilot experiment 
with 8 subjects providing news event-ordering judg-
ments which revealed that the narrative convention ap-
plied only 47% of the time in ordering the events in 
successive past-tense clauses. Our approach involves 
mixed-initiative corpus annotation, with automatic tag-
ging to identify clause structure, tense, aspect, and tem-
poral adverbials, as well as tagging of reference times 
and anchoring of events with respect to reference times.  
We report on machine learning results from event-time 
anchoring  judgments. 
Linguistic Processing 
The time expression tagger TempEx (Mani and Wil-
son 2000) tags and assigns values to temporal expres-
sions, both ?absolute? expressions like ?June 1, 2001? 
and relative expressions like ?Monday?. It was cited in 
(Mani and Wilson 2000) as achieving a .83 F-measure 
against hand-annotated data. Inter-annotator reliability 
across 5 annotators on 193 TDT2-documents was .79F 
for extent and .86F for time values, with  TempEx scor-
ing .76F (extent) and .82F (value) on these documents. 
The clause tagger (CLAUSE-IT) identifies top-level 
clauses (C), top-level clauses with gapped subjects 
(GC), e.g., ?<C>He returned the book</C> <GC>and 
went home</GC>?, relative clauses (RC), and comple-
ment clauses (CO), which include all non-finite clauses.  
Our pilot experiment also revealed that the propor-
tion of clauses with explicit time expressions (TIMEX2) 
is approximately 25%, suggesting that anchoring the 
events to just the explicit times wouldn?t be sufficient. 
The system accordingly computes a reference time 
(Reichenbach 1947) value (tval)  for each clause, de-
fined to be either the time value of an explicit temporal 
expression mentioned in the clause, or, when the ex-
plicit time expression is absent, an implicit time value 
inferred from context.  
To generate this tval   feature, the simple algorithm 
in Figure 1 was used. The system also anchors the 
event?s time with respect to the tval (at, before, or after) 
when the tval is an explicit reference time. This feature 
is called anchor-explicit. All in all, the features shown 
in Table 1 were computed for each clause. 
 
Set initial tval to document-creation-date. 
For each clause: 
1. If clause has explicit time, then set its tval to it. 
2. If clause-type is relative clause, assume its tval 
is inaccessible to later discourse. 
3. If clause verb is of type reporting verb, set tval 
to document-creation-date. 
4. If clause is inside quotes inherit tval from em-
bedding clause. 
5. Otherwise, pick most recent tval. 
 
Figure 1: Algorithm for Computing  
Reference Time (tval) 
 
CTYPE: clause is a regular clause, 
complement clause, or relative clause  
CINDEX: subclause index  
PARA: paragraph number  
SENT: sentence number  
SCONJ: subordinating conjunction 
(e.g., while, since, before)  
TPREP: preposition in a TIMEX2  PP 
TIMEX2: string in the TIMEX2 tag  
TMOD:  temporal modifier not at-
tached to a TIMEX2, (e.g., after [an 
altercation])  
QUOTE: number of words in quotes  
REPVERB-P: reporting verb in clause 
STATIVE-P: stative verb in clause  
ACCOMP-P:  accomplishment verb  
ASPECTSHIFT: shift in aspect from 
previous clause  
G-ASPECT: grammatical aspect 
{progessive, perfect,nil} 
TENSE: tense of clause {past, pre-
sent, future, nil} 
TENSESHIFT: shift in tense from 
previous clause  
ANCHOR_EXPLICIT: {<, >, =, un-
def} 
TVAL: reference time for clause, i.e., 
a time value  
 
Table 1: Linguistic Features for each Clause1,2 
                                                          
3 
1 The statives and accomplishments were computed from 
UMaryland?s LCS lexicon, based on (Dorr and Olsen 1997) 
Learning Anchoring Rules 
A human unconnected with our project corrected the 
tval, based on a set of annotation guidelines, on a sam-
ple of 2069 clauses extracted at random from the North 
American News Corpus. She also anchored the event?s 
time with respect to the tval (AT, BEF, AFT, or unde-
fined). This feature (not a machine feature) is called 
anchors.  
The corrections showed that the algorithm in Figure 
1 was right on tval for 1231 out of 2069, giving an accu-
racy of 59%.  Tracking the sequence of corrected tvals 
revealed that the tval of the previous clause was kept 
65.75% of the time, that it reverted to some other previ-
ous tval 22.99% of the time, and that it shifted to a new 
tval 11.26% of the times. Most of the errors in com-
puted tvals had to do with the tval being assigned erro-
neously the document date rather than reverting to a 
non-immediately previous tval. Finally, the anchor-
explicit relation is correct 83.8% of the time; however, 
just guessing ?at? for the explicit anchor will get an 
accuracy of 90.2%.  
 
 ANCHORS TVAL-
MOVES 
MAJORITY (AT) 76.9 (KEEP) 
65.75 
C5.0 Rules 80.2 (?1.8) 71.8 (?0.5) 
 
Table 2: Accuracy of Anchoring Rules 
 
We then used this training data to train a statistical 
classifier, C5.0 Rules (Quinlan 1997), to learn (1) an-
chors relation rules and (2) rules for tracking the tval 
moves (keep, revert, shift) across successive clauses. 
The accuracy of anchors rules as well as tval change 
rules are shown in Table 2. It can be seen that accuracy 
of machine learning here is significantly better than the 
majority class. The tval, tense, and tense shift play a 
useful role in anchoring, revealing that the tval is a use-
ful abstraction. Here are some of the rules learnt (here te 
is the clause index, assumed to stand for the event time 
of the clause): 
If no sconj and no tmod and no tprep and tval-class 
=day then anchors(AT, te,, tval) 80.4% accurate 
(156 examples). 
If tense is present and no sconj and tval-
class=month then anchors(AT, te,, tval) 77.8 (7). 
If  tense is present perfect and no sconj, then  
                                                                                           
See  www.umiacs.umd.edu/ ~bonnie/ LCS_ Data-
base_Documentation.html. 
2 Since the TIMEX2 and tval values form an open class, they 
were automatically grouped into classes based on the granular-
ity of the time expression, namely, {time-of-day, day, week, 
month, year, or non-specific}. 
anchors(BEF, te,, tval) 83 (4). 
If tense shift is present2past and no explicit time and 
no sconj, then anchors(AT, te,, tval) 90 (30) 
4 Partially Ordering Links 
Based on the best machine-learned rules for the 
anchors relation, anchors tuples are generated for each 
document. The tvals in the document?s anchor tuples are 
also partially ordered, yielding tuples consisting of or-
dered pairs of tvals. The two sets of tuples are then used 
to provide a partial ordering of events in the document, 
in the form of links tuples: links(R, ei, ej), where ei and 
ej are the events corresponding to clauses i and j, and R 
is in {at, bef, aft, or undefined}. One of the authors 
evaluated the partial ordering for accuracy, on seven 
documents3. The results of this evaluation are shown in 
Table 3. #Correct-anchor is the number of the anchors 
tuples correctly classified and #total is the total number 
of anchors tuples classified. Link Recall is the percent-
age of human generated links tuples (723 in all) that are 
correctly identified by machine learned rules. Link Pre-
cision is the percentage of the machine generated links 
tuples that are correct.  
 
#Cl
aus
es 
#Wo
rds 
#correct-
anchor  /  
#total-
anchor 
 Link 
Recall 
Link 
Precision 
40 525 15/18  
(83.3%) 
44/65 
(67.7%) 
53/63 
(84.1%) 
18 335 12/13 
(92.3%) 
59/59 
(100%) 
59/62 
(95.2%) 
27 509 17/22 
(77.2%) 
23/40 
(57.5%) 
23/58 
(39.7%) 
38 617 21/27 
(77.8%) 
94/172 
(54.7%) 
94/190 
(49.5%) 
22 296 11/12 
(91.7%) 
39/42 
(92.9%) 
39/49 
(79.6%) 
14 242 6/7 
(85.7%) 
6/6 
(100%) 
6/7 
(85.7%) 
35 447 28/31 
(90.3%) 
297/339 
(87.6%) 
289/335 
(86.3%) 
194 2971 110/130 
(84.6%) 
562/723  
(77.7%) 
563/764  
(73.7%) 
 
Table 3: Document-Level  Accuracy 
of Learnt Rules 
5 
                                                          
Conclusion 
3 Note that the na?ve algorithm for tval is only 59% correct. 
While improvements to the na?ve algorithm are clearly possi-
ble based on the corrected tval, to adequately test the machine 
learnt rules we use the corrected tval. 
Overall, our approach achieves 84.6% accuracy in 
anchoring events and 75.4% F-measure in partially or-
dering them. These numbers compare favorably with the 
previous literature: (Filatova and Hovy 2001) obtained 
82% accuracy on anchoring for a single type of 
event/topic on 172 clauses, while (Mani and Wilson 
2000) obtained accuracy of 59.4% on anchoring over 
663 verb contexts. Our approach is also distinct in its 
use of human experimentation, machine learning and 
the variety of linguistically motivated features (includ-
ing temporal adverbials) that are brought to bear.  
Future work will examine the role of aspectual fea-
tures, learning from skewed distributions dominated by 
AT (an overwhelming majority of news events occur at 
the reference times), and the incorporation of unsuper-
vised learning methods.  
References 
A. Bell. News Stories as Narratives. In A. Jaworski 
and N. Coupland, The Discourse Reader, Routledge, 
1999, 236-251. 
E. Filatova, and E. Hovy. Assigning Time-Stamps to 
Event-Clauses. Workshop on Temporal and Spatial In-
formation Processing, ACL?2001, Toulouse, 88-95. 
A. Lascarides and  N. Asher. Temporal Relations, 
Discourse Structure, and Commonsense Entailment. 
1993. Linguistics and Philosophy 16, 437-494. 
I. Mani and G. Wilson. Robust Temporal Processing 
of News. ACL'2000, 69-76.   
R. Quinlan. 1997. C5.0. www.rulequest.com. 
H. Reichenbach. The tenses of verbs. In H. Reichen-
bach, Elements of Symbolic Logic. Macmillan, 1947, 
Section 51, 287-298. 
Guidelines for Annotating Temporal Information
Inderjeet Mani, George Wilson
The MITRE Corporation, W640
11493 Sunset Hills Road
Reston, Virginia 20190-5214, USA
+1-703-883-6149
imani@mitre.org
Lisa Ferro
The MITRE Corporation, K329
202 Burlington Road, Rte. 62
Bedford, MA 01730-1420, USA
+1-781-271-5875
lferro@mitre.org
Beth Sundheim
SPAWAR Systems Center, D44208
53140 Gatchell Road, Room 424B
Sand Diego, CA 92152-7420, USA
+1-619-553-4195
sundheim@spawar.navy.mil
ABSTRACT
This paper introduces a set of guidelines for annotating time
expressions with a canonicalized representation of the times they
refer to. Applications that can benefit from such an annotated
corpus include information extraction (e.g., normalizing temporal
references for database entry), question answering (answering
?when? questions), summarization (temporally ordering
information), machine translation (translating and normalizing
temporal references), and information visualization (viewing
event chronologies).
Keywords
Annotation, temporal information, semantics, ISO-8601.
1.
 
INTRODUCTION
The processing of temporal information poses numerous
challenges for NLP. Progress on these challenges may be
accelerated through the use of corpus-based methods. This paper
introduces a set of guidelines for annotating time expressions with
a canonicalized representation of the times they refer to.
Applications that can benefit from such an annotated corpus
include information extraction (e.g., normalizing temporal
references for database entry), question answering (answering
?when? questions), summarization (temporally ordering
information), machine translation (translating and normalizing
temporal references), and information visualization (viewing
event chronologies).
Our annotation scheme, described in detail in [Ferro et al 2000],
has several novel features:
?
 
It goes well beyond the one used in the Message
Understanding Conference [MUC7 1998], not only in terms
of the range of expressions that are flagged, but, also, more
importantly, in terms of representing and normalizing the
time values that are communicated by the expressions.
?
 
In addition to handling fully-specified time expressions [e.g.,
September 3rd, 1997), it also handles context-dependent
expressions. This is significant because of the ubiquity of
context-dependent time expressions; a recent corpus study
[Mani and Wilson 2000] revealed that more than two-thirds
of time expressions in print and broadcast news were
context-dependent ones. The context can be local (within the
same sentence), e.g., In 1995, the months of June and July
were devilishly hot, or global (outside the sentence), e.g., The
hostages were beheaded that afternoon. A subclass of these
context-dependent expressions are ?indexical? expressions,
which require knowing when the speaker is speaking to
determine the intended time value, e.g., now, today,
yesterday, tomorrow, next Tuesday, two weeks ago, etc.
Our scheme differs from the recent scheme of [Setzer and
Gaizauskas 2000] in terms of our in-depth focus on
representations for the values of specific classes of time
expressions, and in the application of our scheme to a variety of
different genres, including print news, broadcast news, and
meeting scheduling dialogs.
The annotation scheme has been designed to meet the
following criteria:
Simplicity with precision: We have tried to keep the scheme
simple enough to be executed confidently by humans, and yet
precise enough for use in various natural language processing
tasks.
Naturalness: We assume that the annotation scheme should reflect
those distinctions that a human could be expected to reliably
annotate, rather than reflecting an artificially-defined smaller set
of distinctions that automated systems might be expected to make.
This means that some aspects of the annotation will be well
beyond the reach of current systems.
Expressiveness:  The guidelines require that one specify time
values as fully as possible, within the bounds of what can be
confidently inferred by annotators. The use of ?parameters? and
the representation of ?granularity? (described below) are tools to
help ensure this.
Reproducibility: In addition to leveraging the [ISO-8601 1997]
format for representing time values, we have tried to ensure
consistency among annotators by providing an example-based
approach, with each guideline closely tied to specific examples.
While the representation accommodates both points and intervals,
the guidelines are aimed at using the point representation to the
extent possible, further helping enforce consistency.
The annotation process is decomposed into two steps: flagging a
temporal expression in a document, and identifying the time value
that the expression designates, or that the speaker intends for it to
designate. The flagging of temporal expressions is restricted to
those temporal expressions which contain a reserved time word
used in a temporal sense, called a ?lexical trigger?, which include
words like day, week, weekend, now, Monday, current, future, etc.
2. SEMANTIC DISTINCTIONS
Three different kinds of time values are represented: points in
time (answering the question ?when??), durations (answering
?how long??), and frequencies (answering ?how often??).
Points in time are calendar dates and times-of-day, or a
combination of both, e.g., Monday 3 pm, Monday next week, a
Friday, early Tuesday morning, the weekend. These are all
represented with values (the tag attribute VAL) in the ISO format,
which allows for representation of date of the month, month of the
year, day of the week, week of the year, and time of day, e.g.,
<TIMEX2 VAL=?2000-11-29-T16:30?>4:30 p.m. yesterday
afternoon</TIMEX2>.
Durations also use the ISO format to represent a period of time.
When only the period of time is known, the value is represented
as a duration, e.g.,
<TIMEX2 VAL=?P3D?>a three-day</TIMEX2> visit.
Frequencies reference sets of time points rather than particular
points.   SET and GRANULARITY attributes are used for such
expressions, with the PERIODICITY attribute being used for
regularly recurring times, e.g., <TIMEX2 VAL=?XXXX-WXX-2?
SET=?YES? PERIODICITY=?F1W?
GRANULARITY=?G1D?>every Tuesday</TIMEX2>. Here
?F1W? means frequency of once a week, and the granularity
?G1D? means the set members are counted in day-sized units.
The annotation scheme also addresses several semantic problems
characteristic of temporal expressions:
Fuzzy boundaries. Expressions like Saturday morning and Fall
are fuzzy in their intended value with respect to when the time
period starts and ends; the early 60?s is fuzzy as to which part of
the 1960?s is included. Our format for representing time values
includes parameters such as FA (for Fall), EARLY (for early,
etc.), PRESENT_REF (for today, current, etc.), among others.
For example, we have <TIMEX2 VAL=?1990-SU?>Summer of
1990</TIMEX2>. Fuzziness in modifiers is also represented, e.g.,
<TIMEX2 VAL=?1990? MOD=?BEFORE?>more than a
decade ago</TIMEX2>. The intent here is that a given
application may choose to assign specific values to these
parameters if desired; the guidelines themselves don?t dictate the
specific values.
Non-Specificity. Our scheme directs the annotator to represent the
values, where possible, of temporal expressions that do not
indicate a specific time.  These non-specific expressions include
generics, which state a generalization or regularity of some kind,
e.g., <TIMEX2 VAL=?XXXX-04?
NON_SPECIFIC=?YES?>April</TIMEX2> is usually wet, and
non-specific indefinites, like <TIMEX2 VAL="1999-06-XX"
NON_SPECIFIC="YES? GRANULARITY="G1D">a sunny day
in <TIMEX2 VAL="1999-06">June</TIMEX2></TIMEX2>.
3. USEFULNESS
Based on the guidelines, we have annotated a small reference
corpus, consisting of 35,000 words of newspaper text and 78,000
words of broadcast news [TDT2 1999]. Portions of this corpus
were used to train and evaluate a time tagger with a reported F-
measure of .83 [Mani and Wilson 2000]; the corpus has also been
used to order events for summarization.
Others have used temporal annotation schemes for the much more
constrained domain of meeting scheduling, e.g., [Wiebe et al
1998], [Alexandersson et al 1997], [Busemann et al 1997]; our
scheme has been applied to such domains as well. In particular,
we have begun annotation of the ?Enthusiast? corpus of meeting
scheduling dialogs used at CMU and by [Wiebe et al 1998]. Only
minor revisions to the guidelines? rules for tag extent have so far
been required for these dialogs.
This annotation scheme is also being leveraged in the Automatic
Content Extraction (ACE) program of the U.S. Department of
Defense, whose focus is on extraction of time-dependent relations
between pairs of ?entities? (persons, organizations, etc.).
Finally, initial feedback from Machine Translation system
grammar writers [Levin, personal communication] indicates that
the guidelines were found to be useful in extending an existing
interlingua for machine translation.
4. CONCLUSION
The annotation scheme we have developed appears applicable to a
wide variety of different genres of text. The semantic
representation used is also highly language-independent. In
Spring 2001, we will be embarking on a large-scale annotation
effort using a merged corpus consisting of Enthusiast data as well
as additional TDT2 data (inter-annotator agreement will also be
measured then). An initial annotation exercise carried out on a
sample of this merged corpus by 20 linguistics students using our
guidelines has been encouraging, with 12 of the students
following the guidelines in a satisfactory manner. In the future, we
expect to extend this scheme to multilingual corpora.
5. ACKNOWLEDGMENTS
Our thanks to Lynn Carlson (Department of Defense), Lori Levin
(Carnegie Mellon University), and Janyce Wiebe (University of
Pittsburgh) for providing the Enthusiast corpus to us.
6. REFERENCES
[1] Alexandersson, J., Riethinger, N. and Maier, E.
Insights into the Dialogue Processing of VERBMOBIL.
Proceedings of the Fifth Conference on Applied Natural
Language Processing, 1997, 33-40.
[2] Busemann, S., Decleck, T., Diagne, A. K., Dini,
L., Klein, J. and Schmeier, S. Natural Language Dialogue
Service for Appointment Scheduling Agents. Proceedings of
the Fifth Conference on Applied Natural Language
Processing, 1997, 25-32.
[3] Ferro, L., Mani, I., Sundheim, B., and Wilson, G.
TIDES Temporal Annotation Guidelines. Draft Version
1.0. MITRE Technical Report MTR 00W0000094, October
2000.
[4] ISO-8601 ftp://ftp.qsl.net/pub/g1smd/8601v03.pdf
1997.
 [5] Mani, I. and Wilson, G. Robust Temporal
Processing of News, Proceedings of the ACL'2000
Conference, 3-6 October 2000, Hong Kong.
[6] MUC-7. Proceedings of the Seventh Message
Understanding Conference, DARPA. 1998.
[7] Setzer, A. and Gaizauskas, R. Annotating Events
and Temporal Information in Newswire Texts. Proceedings
of the Second International Conference On Language
Resources And Evaluation (LREC-2000), Athens, Greece,
31 May- 2 June 2000.
[8] TDT2
http://morph.ldc.upenn.edu/Catalog/LDC99T37.html 1999
[9] Wiebe,  J. M., O?Hara, T. P., Ohrstrom-Sandgren,
T. and McKeever, K. J. An Empirical Approach to
Temporal Reference Resolution. Journal of Artificial
Intelligence Research, 9, 1998, pp. 247-293.
Robust Temporal Processing of News
Inderjeet Mani and George Wilson
The MITRE Corporation, W640
11493 Sunset Hills Road
Reston, Virginia 22090
{imani, gwilson}@mitre.org
Abstract
We introduce an annotation scheme for
temporal expressions, and describe a
method for resolving temporal
expressions in print and broadcast news.
The system, which is based on both
hand-crafted and machine-learnt rules,
achieves an 83.2% accuracy (F-
measure) against hand-annotated data.
Some initial steps towards tagging event
chronologies are also described.
Introduction
The extraction of temporal information from
news offers many interesting linguistic
challenges in the coverage and
representation of temporal expressions. It is
also of considerable practical importance in
a variety of current applications. For
example, in question-answering, it is useful
to be able to resolve the underlined
reference in ?the next year, he won the
Open? in response to a question like ?When
did X win the U.S. Open??.  In multi-
document summarization, providing fine-
grained chronologies of events over time
(e.g., for a biography of a person, or a
history of a crisis) can be very useful. In
information retrieval, being able to index
broadcast news stories by event times allows
for powerful multimedia browsing
capabilities.
Our focus here, in contrast to previous work
such as (MUC 1998),  is on resolving time
expressions, especially indexical expressions
like ?now?, ?today?, ?tomorrow?, ?next
Tuesday?,  ?two weeks ago?, ?20 mins after
the next hour?, etc., which designate times
that are dependent on the speaker and some
?reference? time1. In this paper, we discuss
a temporal annotation scheme for
representing dates and times in temporal
expressions. This is followed by details and
performance measures for a tagger to extract
this information from news sources. The
tagger uses a variety of hand-crafted and
machine-discovered rules, all of which rely
on lexical features that are easily
recognized. We also report on a preliminary
effort towards constructing event
chronologies from this data.  
1 Annotation Scheme
Any annotation scheme should aim to be
simple enough to be executed by humans,
and yet precise enough for use in various
natural language processing tasks. Our
approach (Wilson et al 2000) has been to
annotate those things that a human could be
expected to tag.
Our representation of times uses the ISO
standard CC:YY:MM:DD:HH:XX:SS, with
an optional time zone (ISO-8601 1997). In
other words, time points are represented in
terms of a calendric coordinate system,
rather than a real number line. The standard
also supports the representation of weeks
and days of the week in the format
CC:YY:Wwwd where ww specifies which
week within the year (1-53) and d specifies
the day of the week (1-7). For example, ?last
week? might receive the VAL 20:00:W16.
A time (TIMEX) expression (of type TIME
or DATE) representing a particular point on
the ISO line, e.g., ?Tuesday, November 2,
2000? (or ?next Tuesday?) is represented
with the ISO time  Value (VAL),
20:00:11:02. Interval expressions like ?From
                                                          
1 Some of these indexicals have been called
?relative times? in the (MUC 1998) temporal
tagging task.
May 1999 to June 1999?, or ?from 3 pm to 6
pm? are represented as two separate TIMEX
expressions.
In addition to the values provided by the
ISO standard, we have added several
extensions, including a list of additional
tokens to represent some commonly
occurring temporal units; for example,
?summer of ?69? could be represented as
19:69:SU. The intention here is to capture
the information in the text while leaving
further interpretation of the Values to
applications using the markup.
It is worth noting that there are several kinds
of temporal expressions that are not to be
tagged,  and that other expressions tagged as
a time expression are not assigned a value,
because doing so would violate the
simplicity and preciseness requirements. We
do not tag unanchored intervals, such as
?half an hour (long)? or ?(for) one month?.
Non-specific time expressions like generics,
e.g., ?April? in ?April is usually wet?, or
?today? in ?today?s youth?,  and indefinites,
e.g., ?a Tuesday?, are tagged without a
value. Finally, expressions which are
ambiguous without a strongly preferred
reading are left without a value.
This representation treats points as primitive
(as do (Bennett and Partee 1972), (Dowty
1979), among others); other representations
treat intervals as primitive, e.g., (Allen
1983). Arguments can be made for either
position, as long as both intervals and points
are accommodated. The annotation scheme
does not force committing to end-points of
intervals, and is compatible with current
temporal ontologies such as (KSL-Time
1999); this may help eventually support
advanced inferential capabilities based on
temporal information extraction.
2 Tagging Method
Overall Architecture
The system architecture of the temporal
tagger is shown in Figure 1. The tagging
program takes in a document which has
been tokenized into words and sentences and
tagged for part-of-speech. The program
passes each sentence first to a module that
identifies time expressions, and then to
another module (SC) that resolves self-
contained time expressions. The program
then takes the entire document and passes it
to a discourse processing module (DP)
which resolves context-dependent time
expressions (indexicals as well as other
expressions). The DP module tracks
transitions in temporal focus, uses syntactic
clues, and various other knowledge sources.
The module uses a notion of Reference Time
to help resolve context-dependent
expressions. Here, the Reference Time is the
time a context-dependent expression is
relative to. In our work, the reference time is
assigned the value of either the Temporal
Focus or the document (creation) date. The
Temporal Focus is the time currently being
talked about in the narrative. The initial
reference time is the document date.
2.2 Assignment of time values
We now discuss the modules that assign
values to identified time expressions. Times
which are fully specified are tagged with
their value, e.g, ?June 1999? as 19:99:06 by
the SC module. The DP module uses an
ordered sequence of rules to handle the
context-dependent expressions. These cover
the following cases:
Explicit offsets from reference time:
indexicals like ?yesterday?, ?today?,
?tomorrow?, ?this afternoon?, etc., are
ambiguous between a specific and a non-
specific reading. The specific use
(distinguished from the generic one by
machine learned rules discussed below) gets
assigned a value based on an offset from  the
reference time, but the generic use does not.
Positional offsets from reference time:
Expressions like ?next month?, ?last year?
and ?this coming Thursday? use lexical
markers (underlined) to describe the
direction and magnitude of the offset from
the reference time.
Implicit offsets based on verb tense:
Expressions like ?Thursday? in ?the action
taken Thursday?, or bare month names like
?February? are passed to rules that try to
determine the direction of the offset from
the reference time. Once the direction is
determined, the magnitude of the offset can
be computed. The tense of a neighboring
verb is used to decide what direction to look
to resolve the expression. Such a verb is
found by first searching backward to the last
TIMEX, if any, in the sentence, then
forward to the end of the sentence and
finally backwards to the beginning of the
sentence. If the tense is past, then the
direction is backwards from the reference
time. If the tense is future, the direction is
forward. If the verb is present tense, the
expression is passed on to subsequent rules
for resolution. For example, in the following
passage, ?Thursday? is resolved to the
Thursday prior to the reference date because
?was?, which has a past tense tag, is found
earlier in the sentence:
The Iraqi news agency said the first shipment
of 600,000 barrels was loaded Thursday by the
oil tanker Edinburgh.
Further use of lexical markers:  Other
expressions lacking a value are examined for
the nearby presence of a few additional
markers, such as ?since? and ?until?, that
suggest the direction of the offset.
Nearby Dates:  If a direction from the
reference time has not been determined,
some dates, like ?Feb. 14?, and other
expressions that indicate a particular date,
like ?Valentine?s Day?, may still be
untagged because the year has not been
determined.  If the year can be chosen in a
way that makes the date in question less than
a month from the reference date, that year is
chosen. For example, if the reference date is
Feb. 20, 2000 and the expression ?Feb. 14?
has not been assigned a value, this rule
would assign it the value Feb. 14, 2000.
Dates more than a month away are not
assigned values by this rule.
3 Time Tagging Performance
3.1 Test Corpus
There were two different genres used in the
testing: print news and broadcast news
transcripts. The print news consisted of 22
New York Times (NYT) articles from
January 1998. The broadcast news data
consisted of 199 transcripts of Voice of
America  (VOA) broadcasts from January of
1998, taken from the TDT2 collection
(TDT2 1999).  The print data was much
cleaner than the transcribed broadcast data
in the sense that there were very few
typographical errors, spelling and grammar
were good. On the other hand, the print data
also had longer, more complex sentences
with somewhat greater variety in the words
used to represent dates. The broadcast
collection had a greater proportion of
expressions referring to time of day,
primarily due to repeated announcements of
the current time and the time of upcoming
shows.
The test data was marked by hand tagging
the time expressions and assigning value to
them where appropriate. This hand-marked
data was used to evaluate the performance
of a frozen version of the machine tagger,
which was trained and engineered on a
separate body of NYT, ABC News, and
CNN data. Only the body of the text was
included in the tagging and evaluation.
3.2 System performance
The system performance is shown in Table
12. Note that if the human said the TIMEX
had no value, and the system decided it had
a value,  this is treated as  an error. A
baseline of just tagging values of absolute,
fully specified TIMEXs (e.g., ?January 31st,
1999?) is shown for comparison in
parentheses. Obviously, we would prefer a
larger data sample; we are currently engaged
in an effort within the information extraction
community to annotate a large sample of the
TDT2 collection and to conduct an inter-
annotator reliability study.
Error Analysis
Table 2 shows the number of errors made by
the program classified by the type of error.
Only 2 of these 138 errors (5 on TIME, 133
on DATE) were due to errors in the source.
14 of the 138 errors (9 NYT vs. 5 VOA)
                                                          
2 The evaluated version of the system does not
adjust the Reference Time for subsequent
sentences.
were due to the document date being
incorrect as a reference time.
Part of speech tagging: Some errors, both in
the identification of time expressions and the
assignment of values, can be traced to
incorrect part of speech tagging in the
preprocessing; many of these errors should
be easily correctable.
TIMEX expressions
A total of 44 errors were made in the
identification of TIMEX expressions.
Not yet implemented: The biggest source
of errors in identifying time expressions was
formats that had not yet been implemented.
For example, one third (7 of 21, 5 of which
were of type TIME) of all missed time
expressions came from numeric expressions
being spelled out, e.g. ?nineteen seventy-
nine?. More than two thirds (11 of 16) of the
time expressions for which the program
incorrectly found the boundaries of the
expression (bad extent) were due to the
unimplemented pattern ?Friday the 13th?.
Generalization of the existing patterns
should correct these errors.
Proper Name Recognition: A few items
were spuriously tagged as time expressions
(extra TIMEX). One source of this that
should be at least partially correctable is in
the tagging of apparent dates in proper
names, e.g. ?The July 26 Movement?, ?The
Tonight Show?, ?USA Today?. The time
expression identifying rules assumed that
these had been tagged as lexical items, but
this lexicalization has not yet been
implemented.
Values assigned
A total of 94 errors were made in the
assignment of values to time expressions
that had been correctly identified.
Generic/Specific: In the combined data, 25
expressions were assigned a value when
they should have received none because the
expression was a generic usage that could
not be placed on a time line. This is the
single biggest source of errors in the value
assignments.   
 4 Machine Learning Rules
Our approach has been to develop initial
rules by hand, conduct an initial evaluation
on an unseen test set, determine major
errors, and then handling those errors by
augmenting the rule set with additional rules
discovered by machine learning. As noted
earlier, distinguishing between specific use
of a time expression and a generic use (e.g.,
?today?, ?now?, etc.) was and is a
significant source of error. Some of the other
problems that these methods could be
applied to distinguishing a calendar year
reference from a fiscal year one (as in ?this
year?), and distinguishing seasonal from
specific day references. For example,
?Christmas? has a seasonal use (e.g., ?I
spent Christmas visiting European capitals?)
distinct from its reference to a specific day
use as ?December 25th? (e.g., ?We went to a
great party on Christmas?).
Here we discuss machine learning results in
distinguishing specific use of ?today?
(meaning the day of the utterance) from its
generic use meaning ?nowadays?. In
addition to features based on words co-
occurring with ?today? (Said, Will, Even,
Most, and Some features below), some other
features (DOW and CCYY) were added
based on a granularity hypothesis.
Specifically, it seems possible that ?today?
meaning the day of the utterance sets a scale
of events at a day or a small number of days.
The generic use, ?nowadays?, seems to have
a broader scale. Therefore, terms that might
point to one of these scales such as the
names of days of the week, the word ?year?
and four digit years were also included in
the training features.  To summarize, the
features we used for the ?today? problem are
as follows (features are boolean except for
string-valued POS1 and POS2):
Poss: whether ?today? has a possessive
inflection
Qcontext: whether ?today? is inside a
quotation
Said: presence of ?said? in the same sentence
Will: presence of ?will? in the same sentence
Even: presence of ?even? in the same sentence
Most: presence of ?most? in the same sentence
Some: presence of ?some? in the same
sentence
Year: presence of ?year? in the same sentence
CCYY: presence of a four-digit year in the
same sentence
DOW: presence of a day of the week
expression (?Monday? thru ?Sunday?) in the
same sentence
FW:  ?today? is the first word of the sentence
POS1: part-of-speech of the word before
?today?
POS2: part-of-speech of the word after
?today?
Label: specific or non-specific (class label)
Table 3 shows the performance of different
classifiers in classifying occurrences of
?today? as generic versus specific. The
results are for 377 training vectors and 191
test vectors,  measured in terms of Predictive
Accuracy (percentage test vectors correctly
classified).
We  incorporated some of the rules learnt by
C4.5 Rules (the only classifier which
directly output rules) into the current version
of the program. These rules included
classifying ?today? as generic based on (1)
feature Most being true (74.1% accuracy) or
(2) based on feature FW being true and
Poss, Some and Most being false (67.4%
accuracy). The granularity hypothesis was
partly borne out in that C4.5 rules also
discovered that the mention of a day of a
week (e.g. ?Monday?), anywhere in the
sentence predicted specific use (73.3%
accuracy).
5 Towards Chronology Extraction
Event Ordering
Our work in this area is highly preliminary.
To extract temporal relations between
events, we have developed an event-
ordering component, following (Song and
Cohen 1991). We encode the tense
associated with each verb using their
modified Reichenbachian (Reichenbach
1947) representation based on the tuple
 <si, lge, ri, lge, ei>. Here si is an index for
the speech time, ri for the reference time,
and ei for the event time, with lge being the
temporal  relations precedes, follows, or
coincides. With each successive event, the
temporal focus is either maintained or
shifted, and a temporal ordering relation
between the event and the focus is asserted,
using heuristics defining coherent tense
sequences; see (Song and Cohen 1991) for
more details. Note that the tagged TIME
expressions aren't used in determining these
inter-event temporal relations, so this event-
ordering component could be used to order
events which don't have time VALs.
Event Time Alignment
In addition, we have also investigated the
alignment of events on a calendric line,
using the tagged TIME expressions. The
processing, applied to documents tagged by
the time tagger, is in two stages. In the first
stage, for each sentence, each ?taggable verb
occurrence? lacking a time expression is
given the VAL of the immediately previous
time expression in the sentence. Taggable
verb occurrences are all verb occurrences
except auxiliaries, modals and verbs
following  ?to?, ?not?, or specific modal
verbs. In turn, when a time expression is
found, the immediately previous verb
lacking a time expression is given that
expression's VAL as its TIME. In the second
stage, each taggable verb in a sentence
lacking a time expression is given the TIME
of the immediately previous verb in the
sentence which has one, under the default
assumption that the temporal focus is
maintained.
Of course, rather than blindly propagating
time expressions to events based on
proximity, we should try to represent
relationships expressed by temporal
coordinators like ?when?, ?since?, ?before?,
as well as  explicitly temporally anchored
events, like ?ate at 3 pm?. The event-aligner
component uses a very simple method,
intended to serve as a baseline method, and
to gain an understanding of the issues
involved. In the future, we expect to
advance to event-alignment algorithms
which rely on a syntactic analysis, which
will be compared against this baseline.
Assessment
An example of the chronological tagging of
events offered by these two components is
shown in Figure 2, along with the TIMEX
tags extracted by the time tagger. Here each
taggable verb is given an event index, with
the precedes attribute indicating one or more
event indices which it precedes temporally.
(Attributes irrelevant to the example aren't
shown). The information of the sort shown
in Figure 2 can be used to sort and cluster
events temporally, allowing for various
time-line based presentations of this
information in response to specific queries.
The event-orderer has not yet been
evaluated. Our evaluation of the event-
aligner checks the TIME of all correctly
recognized verbs (i.e., verbs recognized
correctly by the part-of-speech tagger). The
basic criterion for event TIME annotation is
that if the time of the event is obvious, it is
to be tagged as the TIME for that verb. (This
criterion excludes interval specifications for
events, as well as event references involving
generics, counterfactuals, etc. However, the
judgements are still delicate in certain
cases.) We score Correctness as number of
correct TIME fills for correctly recognized
verbs over total number of correctly
recognized verbs. Our total correctness
scores on a small sample of 8505 words of
text is 394 correct event times out of 663
correct verb tags, giving a correctness score
of 59.4%. Over half the errors were due to
propagation of spreading of an incorrect
event time to neighboring events; about 15%
of the errors were due to event times
preceding the initial TIMEX expression
(here the initial reference time should have
been used); and at least 10% of the errors
were due to explicitly marked tense
switches. This is a very small sample, so the
results are meant to be illustrative of the
scope and limitations of this baseline event-
aligning technique rather than present a
definitive result.
6 Related Work
The most relevant prior work is (Wiebe et
al. 98), who dealt with meeting scheduling
dialogs (see also (Alexandersson et al 97),
(Busemann et al 97)), where the goal is to
schedule a time for the meeting. The
temporal references in meeting scheduling
are somewhat more constrained than in
news, where (e.g., in a historical news piece
on toxic dumping) dates and times may be
relatively unconstrained. In addition, their
model requires the maintenance of a focus
stack. They obtained roughly .91 Precision
and .80 Recall on one test set, and .87
Precision and .68 Recall on another.
However, they adjust the reference time
during processing, which is something that
we have not yet addressed.
More recently, (Setzer and Gaizauskas
2000) have independently developed an
annotation scheme which represents both
time values and more fine-grained inter-
event and event-time temporal relations.
Although our work is much more limited in
scope, and doesn't exploit the internal
structure of events, their annotation scheme
may be leveraged in evaluating aspects of
our work.
 The MUC-7 task (MUC-7 98) did not
require VALs, but did test TIMEX
recognition accuracy. Our 98 F-measure on
NYT can be compared for just TIMEX with
MUC-7 (MUC-7 1998) results on similar
news stories, where the best performance
was .99 Precision and .88 Recall. (The MUC
task required recognizing a wider variety of
TIMEXs, including event-dependent ones.
However, at least 30% of the dates and
times in the MUC test were fixed-format
ones occurring in document headers, trailers,
and copyright notices. )
Finally, there is  a large body of work, e.g.,
(Moens and Steedman 1988), (Passoneau
1988), (Webber 1988), (Hwang 1992),
(Song and Cohen 1991), that has focused on
a computational analysis of tense and aspect.
While the work on event chronologies is
based on some of the notions developed in
that body of work, we hope to further
exploit insights from previous work.
Conclusion
We have developed a temporal annotation
specification, and an algorithm for resolving
a class of time expressions found in news.
The algorithm, which is relatively
knowledge-poor, uses a mix of hand-crafted
and machine-learnt rules and obtains
reasonable results.
In the future, we expect to improve the
integration of various modules, including
tracking the temporal focus in the time
resolver, and interaction between the event-
order and the event-aligner. We also hope to
handle a wider class of time expressions, as
well as further improve our extraction and
evaluation of event chronologies. In the long
run, this could include representing event-
time and inter-event relations expressed by
temporal coordinators, explicitly temporally
anchored events, and nominalizations.
Figure 1.  Time Tagger
Source
articles
number
of words
Type Human
Found
(Correct)
System
Found
System
Correct
PrecisionRecall F-
measure
NYT
22
35,555
TIMEX 302 302 296 98.0 98.0 98.0
Values 302 302 249 (129)82.5
(42.7)
82.5
(42.7)
82.5
(42.7)
Broadcast
199
42,616
TIMEX 426 417 400 95.9 93.9 94.9
Values 426 417 353 (105)84.7
(25.1)
82.9
(24.6)
83.8
(24.8)
Overall
221
78,171
TIMEX 728 719 696 96.8 95.6 96.2
Values 728 719 602 (234)83.7
(32.5)
82.7
(32.1)
83.2
(32.3)
Table 1. Performance of Time Tagging Algorithm
Print Broadcast Total
Missing Vals 10 29 39
Extra Vals 18 7 25
Wrong Vals 19 11 30
Missing
TIMEX
6 15 21
Extra
TIMEX
2 5 7
Bad TIMEX
extent
4 12 16
TOTAL 59 79 138
Table 2. High Level Analysis of Errors
Driver
Resolve
Self-contained
Identify
Expressions
Discourse
Processor
Context
Tracker
Algorithm Predictive Accuracy
MC4 Decision Tree3 79.8
C4.5 Rules 69.8
Na?ve Bayes 69.6
Majority Class (specific) 66.5
Table 3.  Performance of  ?Today? Classifiers
In the last step after years of preparation, the countries <lex eindex=?9?
precedes=?10|? TIME=?19981231?>locked</lex> in the exchange rates of
their individual currencies to the euro, thereby <lex eindex=?10?
TIME=?19981231?>setting</lex> the value at which the euro will begin <lex
eindex=?11? TIME=?19990104?>trading</lex> when financial markets open
around the world on <TIMEX VAL=?19990104?>Monday</TIMEX>??.
Figure 2.  Chronological Tagging
                                                          
3 Algorithm from the MLC++ package (Kohavi and Sommerfield 1996).
References
J. Alexandersson, N. Riethinger, and E. Maier.
Insights into the Dialogue Processing of
VERBMOBIL. Proceedings of the Fifth
Conference on Applied Natural Language
Processing, 1997, 33-40.
J. F.  Allen. Maintaining Knowledge About
Temporal Intervals. Communications of the
ACM, Volume 26, Number 11, 1983.
M. Bennett and B. H. Partee.  Towards the Logic
of Tense and Aspect in English, Indiana
University Linguistics Club, 1972.
S. Busemann, T. Decleck, A. K. Diagne, L. Dini,
J. Klein, and S. Schmeier. Natural Language
Dialogue Service for Appointment Scheduling
Agents. Proceedings of the Fifth Conference
on Applied Natural Language Processing,
1997, 25-32.
D. Dowty. ?Word Meaning and Montague
Grammar?, D. Reidel, Boston, 1979.
C. H. Hwang. A Logical Approach to Narrative
Understanding. Ph.D. Dissertation,
Department of Computer Science, U. of
Alberta, 1992.
ISO-8601
ftp://ftp.qsl.net/pub/g1smd/8601v03.pdf 1997.
R. Kohavy and D. Sommerfield. MLC++:
Machine Learning Library in C++.
http://www.sgi.com/Technology/mlc 1996.
KSL-Time 1999.
http://www.ksl.Stanford.EDU/ontologies/time/
1999.
M. Moens and M. Steedman. Temporal Ontology
and Temporal Reference. Computational
Linguistics, 14, 2, 1988, pp. 15-28.
MUC-7. Proceedings of the Seventh Message
Understanding Conference, DARPA. 1998.
R. J. Passonneau. A Computational Model of the
Semantics of Tense and Aspect. Computational
Linguistics, 14, 2, 1988, pp. 44-60.
H. Reichenbach. Elements of Symbolic Logic.
London, Macmillan. 1947.
A. Setzer and R. Gaizauskas. Annotating Events
and Temporal Information in Newswire Texts.
Proceedings of the Second International
Conference On Language Resources And
Evaluation (LREC-2000), Athens, Greece, 31
May- 2 June 2000.
F. Song and R. Cohen. Tense Interpretation in
the Context of Narrative. Proceedings of the
Ninth National Conference on Artifical
Intelligence (AAAI'91), pp.131-136. 1991.
TDT2
http://morph.ldc.upenn.edu/Catalog/LDC99T3
7.html 1999
B. Webber. Tense as Discourse Anaphor.
Computational Linguistics, 14, 2, 1988, pp.
61-73.
J. M. Wiebe, T. P. O?Hara, T. Ohrstrom-
Sandgren, and K. J. McKeever. An Empirical
Approach to Temporal Reference Resolution.
Journal of Artificial Intelligence Research, 9,
1998, pp. 247-293.
G. Wilson, I. Mani, B. Sundheim, and L. Ferro.
Some Conventions for Temporal Annotation of
Text. Technical Note (in preparation). The
MITRE Corporation, 2000.
Producing Biographical Summaries: Combining Linguistic
Knowledge with Corpus Statistics1
Barry Schiffman
Columbia University
1214 Amsterdam Avenue
New York, NY 10027, USA
Bschiff@cs.columbia.edu
Inderjeet Mani2
The MITRE Corporation
11493 Sunset Hills Road
Reston, VA 20190, USA
imani@mitre.org
Kristian J. Concepcion
The MITRE Corporation
11493 Sunset Hills Road
Reston, VA 20190, USA
kjc9@mitre.org
                                                       
1
 This work has been funded by DARPA?s Translingual Information Detection, Extraction, and Summarization (TIDES)
research program, under contract number DAA-B07-99-C-C201 and ARPA Order H049.
2
 Also at the Department of Linguistics, Georgetown University, Washington, D. C. 20037.
Abstract
We describe a biographical multi-
document summarizer that summarizes
information about people described in
the news.  The summarizer uses corpus
statistics along with linguistic
knowledge to select and merge
descriptions of people from a document
collection, removing redundant
descriptions. The summarization
components have been extensively
evaluated for coherence, accuracy, and
non-redundancy of the descriptions
produced.
1 Introduction
The explosion of the World Wide Web has
brought with it a vast hoard of information, most
of it relatively unstructured. This has created a
demand for new ways of managing this often
unwieldy body of dynamically changing
information. The goal of automatic text
summarization is to take a partially-structured
source text, extract information content from it,
and present the most important content in a
condensed form in a manner sensitive to the
needs of the user and task (Mani and Maybury
1999). Summaries can be ?generic?, i.e., aimed
at a broad audience, or topic-focused, i.e.,
tailored to the requirements of a particular user
or group of users. Multi-Document
Summarization (MDS) is, by definition, the
extension of single-document summarization to
collections of related documents. MDS can
potentially help the user to see at a glance what a
collection is about, or to examine similarities
and differences in the information content in the
collection.
Specialized multi-document
summarization systems can be constructed for
various applications; here we discuss a
biographical summarizer. Biographies can, of
course, be long, as in book-length biographies,
or short, as in an author?s description on a book
jacket. The nature of descriptions in the
biography can vary, from physical
characteristics (e.g., for criminal suspects) to
scientific or other achievements (e.g., a
speaker?s biography). The crucial point here is
that facts about a person?s life are selected,
organized, and presented so as to meet the
compression and task requirements.
 While book-quality biographies are out
of reach of computers, many other kinds can be
synthesized by sifting through large quantities of
on-line information, a task that is tedious for
humans to carry out. We report here on the
development of a biographical MDS summarizer
that summarizes information about people
described in the news. Such a summarizer is of
interest, for example, to analysts who want to
automatically construct a dossier about a person
over time.
Rather than determining in advance
what sort of information should go into a
biography, our approach is more data-driven,
relying on discovering how people are actually
described in news reports in a collection. We use
corpus statistics from a background corpus along
with linguistic knowledge to select and merge
descriptions from a document collection,
removing redundant descriptions. The focus here
is on synthesizing succinct descriptions.  The
problem of assembling these descriptions into a
coherent narrative is not a focus of our paper;
the system currently uses canned text methods to
produce output text containing these
descriptions. Obviously, the merging of
descriptions should take temporal information
into account; this very challenging issue is also
not addressed here.
To give a clearer idea of the system?s output,
here are some examples of biographies produced
by our system (the descriptions themselves are
underlined, the rest is canned text). The
biographies contain descriptions of the salient
attributes and activities of people in the corpus,
along with lists of their associates. These short
summaries illustrate the extent of compression
provided. The first two summaries are of a
collection of 1300 wire service news documents
on the Clinton impeachment proceedings
(707,000 words in all, called the ?Clinton?
corpus). In this corpus, there are 607 sentences
mentioning Vernon Jordan by name, from which
the system extracted 82 descriptions expressed
as appositives (78) and relative clauses (4),
along with 65 descriptions consisting of
sentences whose deep subject is Jordan. The 4
relative clauses are duplicates of one another:
?who helped Lewinsky find a job?.  The 78
appositives fall into just 2 groups: ?friend? (or
equivalent descriptions, such as ?confidant?),
?adviser? (or equivalent such as ?lawyer?). The
sentential descriptions are filtered in part based
on the presence of verbs like ?testify, ?plead?, or
?greet? that are strongly associated with the
head noun of the appositive, namely ?friend?.
The target length can be varied to produce
longer summaries.
Vernon Jordan is a presidential friend and a
Clinton adviser. He is 63 years old. He helped
Ms. Lewinsky find a job. He testified  that Ms.
Monica Lewinsky said  that she had
conversations  with the president,  that she
talked  to the president. He has numerous
acquaintances, including Susan Collins, Betty
Currie, Pete Domenici, Bob Graham,  James
Jeffords and Linda Tripp.
1,300 docs, 707,000 words (Clinton corpus) 607
Jordan sentences, 78 extracted appositives, 2
groups: friend, adviser.
Henry Hyde is a Republican chairman of House
Judiciary Committee and a prosecutor in Senate
impeachment trial. He will lead the Judiciary
Committee's impeachment review. Hyde urged
his colleagues  to heed  their consciences ,  ?the
voice  that whispers  in our ear ,  ?duty,  duty,
duty.??
Clinton corpus, 503 Hyde sentences, 108
extracted appositives, 2 groups: chairman,
impeachment prosecutor.
Victor Polay  is the Tupac Amaru rebels' top
leader,  founder and the organization's
commander-and-chief. He was arrested  again
in  1992  and is serving  a life sentence. His
associates include  Alberto Fujimori, Tupac
Amaru Revolutionary, and Nestor Cerpa.
73 docs, 38,000 words, 24 Polay sentences, 10
extracted appositives, 3 groups: leader, founder
and commander-in-chief.   
2 Producing biographical descriptions
2.1 Preprocessing
Each document in the collection to be
summarized is processed by a sentence
tokenizer, the Alembic part-of-speech tagger
(Aberdeen et al 1995), the Nametag named
entity tagger  (Krupka 1995) restricted to people
names, and the CASS parser (Abney 1996).  The
tagged sentences are further analyzed by a
cascade of finite state machines leveraging
patterns with lexical and syntactic information,
to identify constructions such as pre- and post-
modifying appositive phrases, e.g., ?Presidential
candidate George Bush?, ?Bush, the presidential
candidate?, and relative clauses, e.g., ?Senator
..., who is running for re-election this Fall,?.
These appositive phrases and relative clauses
capture descriptive information which can
correspond variously to a person?s age,
occupation, or some role a person played in an
incident. In addition, we also extract sentential
descriptions in the form of sentences whose
(deep) subjects are person names.
2.2 Cross-document coreference
The classes of person names identified within
each document are then merged across
documents in the collection using a cross-
document coreference program from the
Automatic Content Extraction (ACE) research
program (ACE 2000), which compares names
across documents based on similarity of a
window of words surrounding each name, as
well as specific rules having to do with different
ways of abbreviating a person?s name (Mani and
MacMillan 1995). The end result of this process
is that for each distinct person, the set of
descriptions found for that person in the
collection are grouped together.
2.3 Appositives
2.3.1 Introduction
The appositive phrases usually provide
descriptions of attributes of a person. However,
the preprocessing component described in
Section 2.1 does produce errors in appositive
extraction, which are filtered out by syntactic
and semantic tests. The system also filters out
redundant descriptions, both duplicate
descriptions as well as similar ones. These
filtering methods are discussed next.
2.3.2 Pruning Erroneous  and Duplicate
Appositives
The appositive descriptions are first pruned to
record only one instance of an appositive phrase
which has multiple repetitions, and descriptions
whose head does not appear to refer to a person.
The latter test relies on a person typing program
which uses semantic information from WordNet
1.6 (Miller 1995) to test whether the head of the
description is a person. A given string is judged
as a person if a threshold percentage ?1  (set to
35% in our work) of senses of the string are
descended from the synset for Person in
WordNet. For example, this picks out ?counsel?
as a person, but ?accessory? as a non-person.
2.3.3 Merging Similar Appositives
The pruning of erroneous and duplicate
descriptions still leaves a large number of
redundant appositive descriptions across
documents. The system compares each pair of
appositive descriptions of a person, merging
them based on corpus frequencies of the
description head stem, syntactic information,
and semantic information based on the
relationship between the heads in WordNet. The
descriptions are merged if they have the same
head stem, or if both heads have a common
parent below Person in WordNet (in the latter
case the head which is more frequent in the
corpus is chosen as the merged head), or if one
head subsumes the other under Person in
WordNet (in which case the more general head
is chosen).
When the heads of descriptions are
merged, the most frequent modifying phrase that
appears in the corpus with the selected head is
used. When a person ends up with more than
one description, the modifiers are checked for
duplication, with distinct modifiers being
conjoined together, so that ?Wisconsin
lawmaker? and ?Wisconsin democrat? yields
?Wisconsin lawmaker and Democrat?.
Prepositional phrase variants of descriptions are
also merged here, so that ?chairman of the
Budget Committee? and ?Budget Committee
Chairman? are merged. Modifiers are dropped
but their original order is preserved for the sake
of fluency.
2.3.4 Appositive Description Weighting
The system then weights the appositives for
inclusion in a summary. A person?s appositives
are grouped into equivalence classes, with a
single head noun being chosen for each
equivalence class, with a weight for that class
based on the corpus frequency of the head noun.
The system then picks descriptions in decreasing
order of class weight until either the
compression rate is achieved or the head noun is
no longer in the top ?2 % most frequent
descriptions (?2 is set to 90% in our work). Note
that the summarizer refrains from choosing a
subsuming term from WordNet that is not
present in the descriptions, preferring to not risk
inventing new descriptions, instead confining
itself to cutting and pasting of actual words used
in the document.
2.4 Relative Clause Weighting
Once the relative clauses have been pruned for
duplicates, the system weights the appositive
clauses for inclusion in a summary. The
weighting is based on how often the relative
clause?s main verb is strongly associated with a
(deep) subject in a large corpus, compared to its
total number of appearances in the corpus. The
idea here is to weed out ?promiscuous? verbs
that are weakly associated with lots of subjects.
The corpus statistics are derived from the
Reuters portion of the North American News
Text Corpus (called ?Reuters? in this paper) --
nearly three years of wire service news reports
containing 105.5 million words.
Examples of verbs in the Reuters corpus
which show up as promiscuous include ?get?,
?like?, ?give?, ?intend?, ?add?, ?want?, ?be?,
?do?, ?hope?, ?think?, ?make?, ?dream?,
?have?, ?say?, ?see?, ?tell?, ?try?. In a test,
detailed below in Section 4.2, this feature fired
40 times in 184 trials.
To compute strong associations, we
proceed as follows. First, all subject-verb pairs
are extracted from the Reuters corpus with a
specially developed finite state grammar and the
CASS parser. The head nouns and main verbs
are reduced to their base forms by changing
plural endings and tense markers for the verbs.
Also included are ?gapped? subjects, such as the
subject of ?run? in ?the student promised to run
the experiment?; in this example, both pairs
?student-promise? and ?student-run? are
recorded. Passive constructions are also
recognized and the object of the by-PP
following the verb is taken as the deep subject.
Strength of association between subject i and
verb j is measured using mutual information
(Church and Hanks 1990):
)ln(),(
ji
ij
tftf
tfNjiMI
?
?
= .
Here tfij is the maximum frequency of
subject-verb pair ij in the Reuters corpus, tfi is
the frequency of subject head noun i in the
corpus, tfj is the frequency of verb j in the
corpus, and N is the number of terms in the
corpus. The associations are only scored for tf
counts greater than 4, and a threshold ?3  (set to
log score > -21 in our work) is used for a strong
association.
The relative clauses are thus filtered
initially (Filter 1) by excluding those whose
main verbs are highly promiscuous.  Next, they
are filtered (Filter 2) based on various syntactic
features, as well as the number of proper names
and pronouns. Finally, the relative clauses are
scored conventionally (Filter 3) by summing the
within-document relative term frequency of
content terms in the clause (i.e., relative to the
number of terms in the document), with an
adjustment for sentence length (achieved by
dividing by the total number of content terms in
the clause).
3 Sentential Descriptions
These descriptions are the relatively large set of
sentences which have a person name as a (deep)
subject. We filter them based on whether their
main verb is strongly associated with either of
the head nouns of the appositive descriptions
found for that person name (Filter 4). The
intuition here is that particular occupational
roles will be strongly associated with particular
verbs. For example, politicians vote and elect,
executives resign and appoint, police arrest and
shoot; so, a summary of information about a
policeman may include an arresting and
shooting event he was involved with. (The verb-
occupation association isn?t manifest in relative
clauses because the latter are too few in
number).
A portion of the results of doing this is
shown in Table 1.  The results for ?executive?
are somewhat loose, whereas for ?politician?
and ?police?, the associations seem tighter, with
the associated verbs meeting our intuitions.
All sentences which survive Filter 4 are
extracted and then scored, just as relative clauses
are, using Filter 1 and Filter 3. Filter 4 alone
provides a high degree of compression; for
example, it reduces a total of 16,000 words in
the combined sentences that include Vernon
Jordan' s name in the Clinton corpus to 578
words in 12 sentences; sentences up to the target
length can be selected from these based on
scores from Filter 1 and then Filter 3.
However, there are several difficulties with
these sentences. First, we are missing a lot of
them due to the fact that we do not as yet handle
pronominal subjects which are coreferential with
the proper name. Second, these sentences
contain lots of dangling anaphors, which will
need to be resolved. Third, there may be
redundancy between the sentential descriptions,
on one hand, and the appositive and relative
clause descriptions, on the other. Finally, the
entire sentence is extracted, including any
subordinate clauses, although we are working on
refinements involving sentence compaction. As
a result, we believe that more work is required
before the sentential descriptions can be fully
integrated into the biographies.
executive police politician
reprimand
16.36 shoot 17.37 clamor 16.94
conceal 17.46 raid 17.65 jockey 17.53
bank 18.27 arrest 17.96 wrangle 17.59
foresee 18.85 detain 18.04 woo 18.92
conspire 18.91 disperse 18.14 exploit 19.57
convene 19.69 interrogate18.36 brand 19.65
plead 19.83 swoop 18.44 behave 19.72
sue 19.85 evict 18.46 dare 19.73
answer 20.02 bundle 18.50 sway 19.77
commit 20.04 manhandle18.59 criticize 19.78
worry 20.04 search 18.60 flank 19.87
accompany
20.11
confiscate
18.63
proclaim
19.91
own 20.22 apprehend18.71 annul 19.91
witness 20.28 round 18.78 favor 19.92
testify 20.40 corner 18.80 denounce20.09
shift 20.42 pounce 18.81 condemn20.10
target 20.56 hustle 18.83 prefer 20.14
lie 20.58 nab 18.83 wonder 20.18
expand 20.65 storm 18.90 dispute 20.18
learn 20.73 tear 19.00 interfere 20.37
shut 20.80 overpower19.09 voice 20.38
Table 1. Verbs strongly associated with
particular classes of people in the Reuters
corpus (negative log scores).
4 Evaluation
4.1 Overview
Methods for evaluating text summarization can
be broadly classified into two categories
(Sparck-Jones and Galliers 1996). The first, an
extrinsic evaluation, tests the summarization
based on how it affects the completion of some
other task, such as comprehension, e.g., (Morris
et al 1992), or relevance assessment (Brandow
et al 1995) (Jing et al 1998) (Tombros and
Sanderson 1998) (Mani et al 1998). An intrinsic
evaluation, on the other hand, can involve
assessing the coherence of the summary
(Brandow et al 1995) (Saggion and Lapalme
2000).
Another intrinsic approach involves
assessing the informativeness of the summary,
based on to what extent key information from
the source is preserved in the system summary at
different levels of compression (Paice and Jones
1993), (Brandow et al 1995). Informativeness
can also be assessed in terms of how much
information in an ideal (or ?reference?) summary
is preserved in the system summary, where the
summaries being compared are at similar levels
of compression  (Edmundson 1969).
We have carried out a number of intrinsic
evaluations of the accuracy of components
involved in the summarization process, as well
as the succinctness, coherence and
informativeness of the descriptions. As this is a
MDS system, we also evaluate the non-
redundancy of the descriptions, since similar
information may be repeated across documents.
4.2 Person Typing Evaluation
The component evaluation tests how accurately
the tagger can identify whether a head noun in a
description is appropriate as a person description
The evaluation uses the WordNet 1.6 SEMCOR
semantic concordance, which has files from the
Brown corpus whose words have semantic tags
(created by WordNet' s creators) indicating
WordNet sense numbers. Evaluation on 6,000
sentences with almost 42,000 nouns compares
people tags generated by the program with
SEMCOR tags, and provided the following
results: right = 41,555, wrong = 1,298, missing
= 0, yielding Precision, Recall, and F-Measure
of 0.97.
4.3 Relative Clause Extraction Evaluation
This component evaluation tests the well-
formedness of the extracted relative clauses. For
this evaluation, we used the Clinton corpus. The
relative clause is judged correct if it has the right
extent, and the correct coreference index
indicating which person the relative clause
description pertains to. The judgments are based
on 36 instances of relative clauses from 22
documents. The results show 28 correct relative
clauses found, plus 4 spurious finds, yielding
Precision of 0.87, Recall of 0.78, and F-measure
of .82. Although the sample is small, the results
are very promising.
4.4 Appositive Merging Evaluation
This component evaluation tests the system?s
ability to accurately merge appositive
descriptions. The score is based on an automatic
comparison of the system?s merge of system-
generated appositive descriptions against a
human merge of them. We took all the names
that were identified in the Clinton corpus and
ran the system on each document in the corpus.
We took the raw descriptions that the system
produced before merging, and wrote a brief
description by hand for each person who had
two or more raw descriptions. The hand-written
descriptions were not done with any reference to
the automatically merged descriptions nor with
any reference to the underlying source material.
The hand-written descriptions were then
compared with the final output of the system
(i.e., the result after merging). The comparison
was automatic, measuring similarity among
vectors of content words (i.e., stop words such
as articles and prepositions were removed).
Here is an example to further clarify the
strict standard of the automatic evaluation
(words scored correct are underlined):
System: E. Lawrence Barcella is a Washington
lawyer, Washington white-collar defense lawyer,
former federal prosecutor
System Merge: Washington white-collar defense
lawyer
Human Merge: a Washington lawyer and former
federal prosecutor
Automatic Score: Correct=2; Extra-Words=2;
Missed-Words=3
Thus, although ?lawyer? and
?prosecutor? are synonymous in WordNet, the
automatic scorer doesn?t know that, and so
?prosecutor? is penalized as an extra word.
The evaluation was carried out over the
entire Clinton corpus, with descriptions
compared for 226 people who had more than
one description. 65 out of the 226 descriptions
were Correct (28%), with a further 32 cases
being semantically correct ?obviously similar?
substitutions which the automatic scorer missed
(giving an adjusted accuracy of 42%). As a
baseline, a merging program which performed
just a string match scored 21% accuracy. The
major problem areas were errors in coreference
(e.g., Clinton family members being put in the
same coreference class), lack of good
descriptions for famous people (news articles
tend not to introduce such people), and parsing
limitations (e.g., ?Senator Clinton? being parsed
erroneously as an NP in ?The Senator Clinton
disappointed??). Ultimately, of course,
domain-independent systems like ours are
limited semantically in merging by the lack of
world knowledge, e.g., knowing that Starr' s
chief lieutenant can be a prosecutor.
4.5 Description Coherence and
Informativeness Evaluation
To assess the coherence and informativeness of
the relative clause descriptions3, we asked  4
subjects who were unaware of our research to
judge descriptions generated by our system from
the Clinton corpus. For each relative clause
description, the subject was given the
description, a person name to whom that
description pertained, and a capsule description
consisting of merged appositives created by the
system. The subject was asked to assess (a) the
coherence of the relative clause description in
terms of its succinctness (was it a good length?)
and its comprehensibility (was it and
understandable by itself or in conjunction with
the capsule?), and (b) its informativeness in
terms of whether it was an accurate description
(does it conflict with the capsule or with what
you know?) and whether it was non-redundant
(is it distinct or does it repeat what is in the
capsule?).
 The subjects marked 87% of the
descriptions as accurate, 96% as non-redundant,
and 65% as coherent. A separate 3-subject inter-
                                                       
3
 Appositives are not assessed in this way as few errors of
coherence or informativeness were noticed in the
appositive extraction.
annotator agreement study, where all subjects
judged the same 46 decisions, showed that all
three subjects agreed on 82% of the accuracy
decisions, 85% of the non-redundancy decisions
and 82% of the coherence decisions.
5 Learning  to Produce Coherent
Descriptions
5.1 Overview
To learn rules for coherence for extracting
sentential descriptions, we used the examples
and judgments we obtained for coherence in the
evaluation of relative clause descriptions in
Section 4.5. Our focus was on features that
might relate to content and specificity: low verb
promiscuity scores, presence of proper names,
pronouns, definite and indefinite clauses. The
entire list is as follows:
badend:
boolean. is there an impossible
end, indicating a bad extraction (
... Mr.)?
bestverb:
continuous. use the verb
promiscuity threshhold ?3 to
find the score of the most non-
promiscuous verb in the clause
classes
(label):
boolean. accept the clause,
reject the clause
count
pronouns:
continuous. number of personal
pronouns
count
proper:
continuous. number of nouns
tagged as NP
hasobject: continuous. how many np'sfollow the verb?
haspeople: continuous. how many "name"
constituents are found?
has
possessive:
continuous. how many
possessive pronouns are there?
hasquote: boolean. is there a quotation?
hassubc: boolean. is there a subordinate
clause?
isdefinite: continuous. how many definiteNP's are there?
repeater: boolean. is the subject's name
repeated, or is there no subject?
timeref: boolean. is there a time
reference?
withquit: is there a ?quit? or ?resign?
verb?
withsay: boolean. is there a ?say? verb inthe clause?
5.2 Accuracy of Learnt Descriptions
Table 2 provides information on different
learning methods. The results are for a ten-fold
cross-validation on 165 training vectors and 19
test vectors, measured in terms of Predictive
Accuracy (percentage test vectors correctly
classified).
Tool Accuracy
Barry?s Rules .69
MC4 Decision Tree .69
C4.5Rules .67
Ripper .62
Naive Bayes .62
Majority Class (coherent) .60
Table 2.  Accuracy of Different Description
Learners on Clinton corpus
The best learning methods are comparable
with rules created by hand by one of the authors
(Barry?s rules). In the learners, the bestverb
feature is used heavily in tests for the negative
class, whereas in Barry?s Rules it occurs in tests
for the positive class.
6 Related Work
Our work on measuring subject-verb
associations has a different focus from the
previous work. (Lee and Pereira 1999), for
example, examined verb-object pairs. Their
focus was on a method that would improve
techniques for gathering statistics where there
are a multitude of sparse examples. We are
focusing on the use of the verbs for the specific
purpose of finding associations that we have
previously observed to be strong, with a view
towards selecting a clause or sentence, rather
than just to measure similarity. We also try to
strengthen the numbers by dealing with ?gapped?
constructions.
While there has been plenty of work on
extracting named entities and relations between
them, e.g., (MUC-7 1998), the main previous
body of work on biographical summarization is
that of (Radev and McKeown 1998). The
fundamental differences in our work are as
follows: (1) We extract not only appositive
phrases, but also clauses at large based on
corpus statistics; (2) We make heavy use of
coreference, whereas they don?t use coreference
at all; (3) We focus on generating succinct
descriptions by removing redundancy and
merging, whereas they categorize descriptions
using WordNet, without a focus on succinctness.
7 Conclusion
This research has described and evaluated
techniques for producing a novel kind of
summary called biographical summaries. The
techniques use syntactic analysis and semantic
type-checking (from WordNet), in combination
with a variety of corpus statistics. Future
directions could include improved sentential
descriptions as well as further intrinsic and
extrinsic evaluations of the summarizer as a
whole (i.e., including canned text).
References
 J. Aberdeen, J. Burger, D. Day, L. Hirschman,
P. Robinson, and M. Vilain. 1995. ?MITRE:
Description of the Alembic System system as used
for MUC-6?. In Proceedings of the Sixth Message
Understanding Conference (MUC-6), Columbia,
Maryland.
 S. Abney. 1996. ?Partial parsing Via Finite-State
Cascades?. Proceedings of the ESSLLI '96 Robust
Parsing Workshop.
Automatic Context Extraction Program.
http://www.nist.gov/speech/tests/ace/index.htm
R. Brandow, K. Mitze, and L. Rau. 1995. ?Automatic
condensation of electronic publications by
sentence selection.? Information Processing and
Management 31(5): 675-685. Reprinted in
Advances in Automatic Text Summarization, I.
Mani and M.T. Maybury (eds.), 293-303.
Cambridge, Massachusetts: MIT Press.
K. W. Church and P. Hanks. 1990. ?Word association
norms, mutual information, and lexicography?.
Computational Linguistics 16(1): 22-29.
H. P. Edmundson. 1969. ?New methods in automatic
abstracting?.  Journal of the Association for
Computing Machinery 16 (2): 264-285. Reprinted
in Advances in Automatic Text Summarization, I.
Mani and M.T. Maybury (eds.), 21-42.
Cambridge, Massachusetts: MIT Press.
G. Krupka. 1995. ?SRA: Description of the SRA
system as used for MUC-6?. In Proceedings of the
Sixth Message Understanding Conference (MUC-
6), Columbia, Maryland.
L. Lee and F. Pereira. 1999. ?Distributional
Similarity Models: Clustering vs. Nearest
Neighbors?. In Proceedings of the 37th Annual
Meeting of the Association for Computational
Linguistics, 33-40.
I. Mani and T. MacMillan. 1995. ?Identifying
Unknown Proper Names in Newswire Text?. In
Corpus Processing for Lexical Acquisition, B.
Boguraev and J. Pustejovsky (eds.), 41-73.
Cambridge, Massachusetts: MIT Press.
I. Mani and M. T. Maybury. (eds.). 1999. Advances
in Automatic Text Summarization. Cambridge,
Massachusetts: MIT Press.
G. Miller. 1995. ?WordNet: A Lexical Database for
English?. Communications of the Association For
Computing Machinery (CACM) 38(11): 39-41.
A. Morris, G. Kasper, and D. Adams. 1992. ?The
Effects and Limitations of Automatic Text
Condensing on Reading Comprehension
Performance?. Information Systems Research 3(1):
17-35. Reprinted in Advances in Automatic Text
Summarization, I. Mani and M.T. Maybury (eds.),
305-323. Cambridge, Massachusetts: MIT Press.
 MUC-7. 1998. Proceedings of the Seventh Message
Understanding Conference, DARPA.
C. D. Paice and P. A. Jones. 1993. ?The
Identification of Important Concepts in Highly
Structured Technical Papers.? In Proceedings of
the 16th International Conference on Research
and Development in Information Retrieval
(SIGIR'93), 69-78.
D. R. Radev and K. McKeown. 1998. ?Generating
Natural Language Summaries from Multiple On-
Line Sources?. Computational Linguistics 24(3):
469-500.
H. Saggion and G. Lapalme. 2000. ?Concept
Identification and Presentation in the Context of
Technical Text Summarization?. In Proceedings of
the Workshop on Automatic Summarization, 1-10.
K. Sparck-Jones and J. Galliers. 1996. Evaluating
Natural Language Processing Systems: An
Analysis and Review.  Lecture Notes in Artificial
Intelligence 1083. Berlin: Springer.
A. Tombros and M. Sanderson. 1998.?Advantages of
query biased summaries in information retrieval?.
In Proceedings of the 21st International
Conference on Research and Development in
Information Retrieval (SIGIR'98), 2-10.
Using Summarization for Automatic Briefing Generation 
Inderjeet Mani 
. Kristian Concepcion 
Linda Van Guilder 
The MITRE Corporation, W640 
11493 Sunset Hills Road 
Reston, VA 22090, USA 
{imani,kjc9,1cvg}@mitre.org 
Abst rac t  
We describe a system which automatically 
generates multimedia briefings from high- 
level outlines. The system uses 
summarization in content selection and 
creation, and in helping form a coherent 
narrative for the briefing. The approach does 
not require a domain knowledge base. 
1 In t roduct ion  
Document production is an important function in 
many organizations. In addition to instruction 
manuals, reports, courseware, system 
documentation, etc., briefings are a very 
common type of document product, often used 
in slide form as a visual accompaniment to a 
talk. Since so much time is spent by so many 
people in producing briefings, often under 
serious time constraints, any method to reduce 
the amount of time spent on briefing production 
could yield great gains in productivity. 
Briefings involve a high degree of condensation 
of information (e.g., no more than a few points, 
perhaps bul,leted, per slide), and they typically 
contain multimedia information. Many briefings 
have a stereotypical structure, dictated in part by 
the business rules of the organisation. For 
example, a commander may present a daily or 
weekly brief to her superiors, which is more in 
the nature of a routine update of activities ince 
the last briefing; or she may provide an action 
brief, which is triggered by a particular situation, 
and which consists of a situation update 
followed by arguments recommending a 
particular course of action. Further, the process 
of constructing a briefing may involve certain 
stereotypical activities, including culling 
information from particular sources, such as 
messages, news, web pages, previous briefings, 
etc. Thus, while part of the briefing content may 
be created anew by the briefing author 1, other 
parts of the briefing may be constructed from 
existing information sources. However, 
information in those sources need not 
necessarily be in the same form as needed by the 
briefing. 
All these characteristics of briefings make them 
attractive as an application of automatic 
summarization, which is aimed at producing a 
condensed, task-tailored representation f salient 
content in information sources. Often, the 
background information being used in a slide is 
quite considerable; the author needs to identify 
what's salient, presenting it in a succinct manner 
so as to fit on the slide, perhaps creating a 
graphic or other multimedia clip to do so. 
Automatic summarization; by definition, has a 
clear role to play here. A briefing usually 
involves a sequence of slides; as the summary 
becomes longer, it needs to form a coherent 
narrative, built around the prescribed structure. 
Finally, a briefing must strive, to the extent 
possible, to be persuasive and vivid, so that the 
point gets across. This in turn presents a further 
challenge for summarization: the ability to 
generate smoothly narrated, coherent 
summaries. 
I The noun "author" is used throughout the paper to 
designate a human author. 
_ 99  
It is therefore worthwhile investigating whether 
combining automatic summarization with 
intelligent multimedia presentation techniques 
can make the briefing generation amenable to 
full automation. In other words, the author 
should be able to use a computer program to 
generate an initial briefing, which she can then 
edit and revise as needed. The briefing can then 
be presented by the author if desired, or else 
directly by the computer (particularly useful if 
the briefing is being sent to someone lse). The 
starting point for this process would be a high- " 
level outline of the briefing on the part of the 
author. The outline would include references to
particular information sources that had to be, 
summarized in particular ways. If a program 
were able to take such outlines and generate 
briefings which didn't require extensive post- 
editing to massage into a state deemed 
acceptable for the task at hand, the program 
could be regarded as a worthwhile time saving 
tool. 
2 Approach 
Our work forms part of a larger DARPA-funded 
project aimed at improving analysis and 
decision-making in crisis situations by providing 
tools that allow analysts to collaborate to 
develop structured arguments in support of 
particular conclusions and to help predict likely 
future scenarios. These arguments, along with 
background evidence, are packaged together as 
briefing s to high-level decision-makers. In 
leveraging automatic methods along the lines 
suggested above to generate briefings, our 
approach needs to allow the analyst to take on as 
much of the briefing authoring as she wants to 
(e.g., it may take time for her to adapt o or trust 
the machine, or she may want the machine to 
present just part of the briefing). The analyst's 
organisation usually will instantiate one of 
several templates dictating the high-level 
structure of a briefing; for example, a briefing 
may always have to begin with an executive 
summary. The summarization methods also need 
to be relatively domain-independent, given that 
the subject matter of crises are somewhat 
unpredictable; an analyst in a crisis situation is 
likely to be inundated with large numbers of 
crisis-related news and intelligence r ports from 
many different sources. This means that we 
cannot require that a domain knowledge base be 
available to help the briefing generation process. 
Given these task requirements, we have adopted 
an approach that is flexible about 
accommodating different degrees of author 
involvement, that is relatively neutral about the 
rhetorical theory underlying the briefing 
structure (since a template may be provided by 
others), and that is domain-independent. I  our 
approach, the author creates the briefing outline, 
which is then fleshed out further by the system 
based on information i  the outline. The system 
fills out some content by invoking specified 
summarizers; it also makes decisions, when 
needed, about output media type; it introduces 
narrative lements to improve the coherence of 
the briefing; and finally, it assembles the final 
presentation, making decisions about spatial 
layout in the process. 
A briefing is represented asa tree. The structure 
of the tree represents he rhetorical structure of 
the briefing. Each node has a label, which offers 
a brief textual description of the node. Each leaf 
node has an associated goal, which, when 
realized, provides content for that node. There 
are two kinds of goals: content-level goals and 
narrative-level goals. Content-level goals are 
also of two kinds: retrieve goals, which retrieve 
existing media objects of a particular type (text, 
audio, image, audio, video) satisfying some 
description, and create goals, which create new 
media objects of these types using programs 
(called summarization filters). Narrative-level 
goals introduce descriptions of content at other 
nodes: they include captions and running text for 
media objects, and segues, which are rhetorical 
moves describing a transition to a node. 
Ordering relations reflecting temporal and 
spatial ayout are defined on nodes in the tree. 
Two coarse-grained relations, seq for 
precedence, and par for simultaneity, are used to 
specify a temporal ordering on the nodes in the 
tree. As an example, temporal constraints for a 
(tiny) tree of 9 nodes may be expressed as: 
<ordering> <seq> 
<par>7</par> 
<par>8</par> 
<par>3</par> 
<par>4 5</par> 
<par>6</par> 
100 
<par>l 9</par> 
<par>2</par> 
</seq> </ordering> 
The tree representation, along with the temporal 
constraints, can be rendered in text as XML; we 
refer to the XML representation as a script. 
@ 
Player i~ 
User 
Interface 
~ Tem~t~ 
r I Vail dator \[ 
Co 
Cr~ ~ 
C~ound ~t  
ixe utor I 
XMI1 resentati.on \[ 
~k~ Generator / ,  
' Brid"mg 
Generator 
Figure 1: System Architecture 
The overall architecture of our system is shown 
in Figure 1, The user creates the briefing outline 
in the form of a script, by using a GUI. The 
briefing generator takes the script as input. The 
Script Validator applies an XML parser to the 
script, to check for syntactic orrectness. It then 
builds a tree representation for the script, which 
represents the briefing outline, with temporal 
constraints attached to the leaves of the tree. 
Next, a Content Creator takes the input tree and 
expands it by introducing narrative-level goals 
including segues to content nodes, and rtmning 
text and captions describing media objects at 
content nodes. Running text and short captions 
are generated from meta-information associated 
with media objects, by using shallow text 
generation methods (canned text). The end result 
of content selection (which has an XML 
representation callod a ground script) is that the 
complete tree has been fully specified, with all 
the create and retrieve goals fully specified , 
with all the output media types decided. The 
Content Creator is thus responsible for both 
content selection and creation, in terms of tree 
structure and node content. 
Then, a Content Executor executes all the create 
and retrieve goals. This is a very simple step, 
resulting in the generation of all the media 
objects in the presentation, except for the audio 
files for speech to be synthesized. Thus, this step 
results in realization of the content at the leaves 
of the tree. 
Finally, the Presentation Generator takes the 
tree which is output from Content Execution, 
along with its temporal ordering constraints, and 
generates the spatial ayout of the presentation. 
If no spatial ayout constraints are specified (the 
default is to not specify these), the system 
allocates pace using a simple method based on 
the temporal layout for nodes which have spatial 
manifestations. Speech synthesis is also carried 
out here. Once the tree is augmented with spatial 
layout constraints, it is translated by the 
Presentation Generator into SMIL 2 
(Synchronized Multimedia Integration 
Language) (SMIL 99), a W3C-developod 
extension of HTML that can be played by 
standard multimedia players (such as Real 3 and 
Grins 4. This step thus presents the realized 
content, synthesizing it into a multimedia 
presentation laid out spatially and temporally. 
This particular architecture, driven by the above 
project requirements, does not use planning as 
an overall problem-solving strategy, as planning 
requires domain knowledge. It therefore differs 
from traditional intelligent multimedia 
presentation planners, e.g., (Wahlster etal. 93). 
Nevertheless, the system does make a number o f  
intelligent decisions in organizing and 
coordinating presentation decisions. These are 
discussed next, after which we turn to the main 
point of the paper, namely the leveraging of 
summarization in automatic briefing generation. 
2 h. ttp://www.w3.org/AudioVideo/ 
3 www.real.com 
4 www.oratrix.com 
_ J  
101 
3 Intelligent Multimedia Presentation 
Generation 
The author of a briefing may choose to flesh out 
as little of the tree as desired, with the caveat 
that the temporal ordering relations for non- 
narrative nodes need to be provided by her. 
When a media object is generated at a node by a 
create goal, the running text and captions are 
generated by the system. The motivation for this 
is obvious: when a summarization filter (which 
is a program under our control) is generating a
media object, we can often provide sufficient 
recta-information about hat object o generate a
short caption and some running text. By default, 
all segues and spatial layout relations are also 
specified by the system, so the author does not 
have to know about these unless she wants to. 
Finally, the decision as to when to produce 
audio, when not specified by the author, is left to 
the system. 
When summarization filters are used (for create 
goals), the media type of the output is specified 
as a parameter to the filter. This media type may 
be converted to some other type by the system, 
e.g., text to speech conversion using Festival 
(Taylor et al 98). By default, all narrative nodes 
attempt to realize their goals as a speech media 
type, using rules based on text length and 
tnmcatability to less than 250 bytes to decide 
when to use text-to-speech. The truncation 
algorithm is based on dropping syntactic 
constituents, using a method similar to (Mani et 
al. 99). Captions are always realized, in addition, 
as text (i.e., they have a text realization and a. 
possible audio realization). 
Spatial layout is decided in the Presentation 
Generator, after all the individual media objects 
are created along with their temporal constraints 
by the Content Executor. The layout algorithm 
walks through the temporal ordering in 
sequence, allocating a segment o each set of 
objects that is designated to occur 
simultaneously (grouped by par in the temporal 
constraints). Each segment can have up to 4 
frames, in each of which a media object is 
displayed (thus, no more than 4 media objects 
can be displayed at the same time). Since media 
objects declared to be simultaneous (using par) 
in the temporal constraints will go together in a 
separate segment, the temporal constraints 
determine what elements are grouped together in 
a segment. The layout within a segment handles 
two special cases. Captions are placed directly 
undemeath their associated media object. 
Running text, when realized as text, is placed 
beside the media object being described, so that 
they are paired together visually. Thus, 
coherence of a segment is influenced mainly by 
the temporal constraints (which have been 
fleshed out by the Content Creator to include 
narrative nodes), with further handling of special 
cases. Of course, an individual summarization 
filter may choose to coordinate component 
multimedia objects in particular ways in the 
course of generating a composite multimedia 
object. 
Details such as duration and onset of particular 
frames are specified in the translation to SMIL. 
Duration is determined by the number of frames 
present in a segment, unless there is an audio 
media object in the segment (this media object 
may have a spatial representation, e.g., as an 
audio icon, or it may not). If an audio media 
object occurs in a frame, the duration of all 
media objects in that frame is equal to the length 
of all the audio files in the segment. If there is 
no audio present in a segment, he duration is ot 
seconds (or has a default value of 5) times the 
number of frames created. 
4 Summarization Filters 
As mentioned above, create goals are satisfied 
by summarization filters, which create new 
media objects ummarizing information sources. 
These programs are called summarization filters 
because in the course of condensing information, 
they take input information and turn it into some 
more abstract and useful representation, filtering 
out unimportant information. Such filters 
provide a novel way of carrying out content 
selection and creation for automated 
presentation generation. 
Our approach relies on component-based 
software composition, i.e., assembly of software 
units that have contractually specified interfaces 
that can be independently deployed and reused. 
The idea of assembling complex language 
processing programs out of simpler ones is 
102 
hardly new; however, by employing current 
industry standards to specify the interaction 
between the components, we simultaneously 
increase the robustness of the system, ensure the 
reusability of individual components and create 
a more fully plug-and-play capability. Among 
the core technology standards that support his 
plug-and-play component assembly capability 
are (a) Java interfaces, used to specify functions 
that all summarization components must 
implement in order to be used in the system, (b) 
the JavaBeans standard, which allows the 
parameters and methods of individual 
components o be inspected by the system and 
revealed to the users (c) the XML markup 
standard, which we have adopted as an inter- 
component communication language. Using 
these technologies, legacy or third-party 
summarizers are incorporated into the system by 
"wrapping" them so as to meet the interface 
specification of the system. These technologies 
also make possible a graphical environment to 
assemble and configure complex summarization 
filters from individual summarization 
components. 
Among the most important wins over the 
traditional "piping" approach to filter assembly 
is the ability to impose build-time restrictions on 
the component assembly, disallowing "illegal" 
compositions, e.g. component X cannot provide 
input to component Y unless X's output type 
corresponds to Y's input type. Build-time 
restrictions uch as these play a clear role in 
increasing the overall robustness of the run-time 
summarization system. Another build-time win 
lies in the ability of JavaBeans to be serialized, 
i.e., written to disk in such a way as to preserve 
~he state of its parameters settings, ensuring that 
every component in the system can be 
configured and run at different times 
independently of whether the component 
provides aparameter file facility. 
Establishing the standard functions required of a 
summarization filter is challenging on several 
fronts. One class of functions required by the  
interface is necessary to handle the technicalities 
of exchanging information between otherwise 
discrete components. This set includes 
functions for discovering a component's input 
and output types, for handling messages, 
exceptions and events passed between 
components and for interpreting XML based on 
one or more system-wide document type 
definitions (DTDs). The other, more interesting 
set of functions gets to the core of 
summarization functionality. Selecting these 
functions involves identifying parameters likely 
to be broadly applicable across most or all 
summarizers and finding ways to group them 
and/or to generalize them. This is desirable in 
order to reduce the burden on the end user of 
understanding the subtle differences between the 
various settings in the summarizers available to 
her. 
An. example of the difficulty inherent in this 
endeavor is provided by the compression 
(summary length divided by source length) vs. 
reduction (l's complementof compression) vs. 
target length paradigm. Different summarizers 
will implement one or more of these. The 
wrapper maps from the high-level interface 
function, where the application/user can specify 
either compression ortarget length, but not both, 
to the individual summarizer's representation. 
Thus, a user doesn't need to know which 
representation(s) a particular summarizer uses 
for reduction/compression. 
A vanilla summarization Bean includes the 
following functionality, which every summarizer 
must be able to provide methods for: 
source: documents to be summarized 
(this can be a single document, or a 
collection) 
reduction-rate: either summary 
size/source size, or target length 
audience: user-focused or generic 
(user-focused requires the specification 
of a bag of terms, which can be of 
different types) 
output-type: specific data formats 
(specified by DTDs) 
The above are parameters which we expect all 
summarizers to support. More specialized 
summarizer beans can be constructed to reflect 
groupings of summarizers. Among other 
parameters are output-fluency, which specifies 
whether a textual summary is to be made up of 
passages (sentences, paras, blocks), named 
entities, lists of words, phrases, or topics, etc. 
Given that definitions of summarization i more 
103 
theoretical terms have not been entirely 
satisfactory (Mani 2000), it is worth noting that 
the above vanilla Bean provides an operational 
definition of what a summarizer is. 
text, and segues. The captions and running text, 
when not provided by the filters, are provided by 
the script input. In the case of retrieve goals, the 
objects may not have any meta-information, i  
which case a default caption and running-text is
generated. Clearly, a system's explanatory 
narrative will be enhanced by the availability of 
rich meta-information. 
The segues are provided by the system. For 
example, an item with a label "A biography of 
bin Laden" could result in a generated segue 
"Here is a biography of bin Laden". The 
Content Creator, when providing content for 
narrative nodes, uses a variety o f  different 
canned text patterns. For the above example, the 
pattern would be "Here is @6.label", where 6 is 
the number of a non-narrative node, with label 
being its label. 
Figure 2: Summarization Filter 
Composition 
In addition to its practical utility in the ability to 
assimilate, combine and reuse components in 
different combinations, and to do so within a 
GUI, this approach is interesting because it 
allows powerful summarization functions to be 
created by composing together simpler tools. 
(Note that this is different from automatically 
finding the best combination, which our system 
does not address). For example, Figure 2 
illustrates a complex filter created by using a 
GUI to compose together a named entity 
extractor, a date extractor, a component which 
discovers significant associations between the 
two and writes the result to a table, and a 
visualizer which plots the results as a graph. The 
resulting summarizer takes in a large collection 
of documents, and produces as a summary a 
graph (a jpeg) of salient named entity mentions 
over time. Each of its components can be easily 
reused within the filter composition system to 
build other summarizers. 
5 Narrative Summarization 
Peru Action Brief 
1 Preamble 
2 Situation Assessment 
2.1 Chronology of Events 
2.1.2 Late st document summary 
create C'summarize -generic 
-compression. 1 ~peru~p32") 
2.2 Biographies 
2.2.1 Biography of Victor Polay 
2.2.1.1 Picture of @2.2.2.percon 
retrieve("\]) Arawdata~,polay.jpg ") 
2.2.1.2 Biography of @~2.2.2.person 
create("summarize -bio -length 350 
-span multi -person 
@_~2.2.2.person - ut table 
/peru/* ") 
3 Coda 
"This briefing has aszessed aspects of the 
situation in Peru. Overall, the crisis 
appears to be worsening." 
Figure 3: Input Script 
As mentioned above, the system can construct a
narrative to accompany the briefing. Narrative 
nodes are generated to cover captions, running 
104 
Peru Action Brief 
1 Preamble 
audio -- "ln this briefin~ 1 will go over 
the @2.1abel. This ~?ill cover 
@2.1.1abel and @,2. 3.1.1aber" 
2 Situation Assessment 
2. l "An overvie~? of the ~2.2.label" 
(Meta-2.2) 
2.2 C-'hfonology of Events 
2.2.1 audio = "Here is the @2.2.2.laber" 
(1VIeta- 2.2.2) 
2.2.2 text = "Latest document summary" 
audio = text = 
create ("automatize -generic 
-compression .1/reru/p32") 
2.3 Biographies 
2.3.1 audio = 
"A profile of @2. 3.2.person" 
('NIeta-2.3.2) 
2.3.2 Biography of Victor~olay 
2.3.2.1 audio = text = 
"A file photo of 
@,2.3.2.person" 
(Meta-2.3.2.2) 
2.3.2.2 Picture of @,2.&2.person 
image = 
retrie ve("D Arawdata~polay.jpg") 
2.3.2.3 audio = text = 
"ProJile of @2. 3. 2.person" 
(Meta- 2.3.2.3) 
2.3.2.4 Biography of @2. 3.2.person 
audio = text = 
create(%-ummarize-bio length 350 
-span multi -person 
@_r2.Z 2.person -out tab& 
/rend* ") 
3 Coda 
audio = "This briefing has assessed 
a~79ect~r of the situation in Peru. Overall, 
the crisis appears to be ~orr"ening." 
<seq> 
</seq> 
<par> 1 </par> 
<par>2.2.1 2.2.2</par> 
<par>2.3.1 <lpar> 
<par>2.3.2.1 2.3.2.2 
2.3.2.3 2.3.2.4</par> 
<par~3</par> 
Figure 4: Ground Script 
All segue nodes are by default generated 
automatically by the system, based on node 
labels. We always introduce a segue node at the 
beginning of the presentation (called a preamble 
node), which provides a segue covering the 
"crown" of the tree, i.e., all nodes upto a 
particular depth d from the root (d=2) are 
marked with segue nodes. A segue node is also 
produced at the end (called a coda). (Both 
preamble and segue can of course be specified 
by the author if desired). 
For introducing intervening segue nodes, we use 
the following algorithm based on the distance 
between odes and the height in the tree, We 
traverse the non-narrative l aves of the tree in 
their temporal order, evaluating each pair of 
adjacent nodes A and B where A precedes B 
temporally. A segue is introduced between 
nodes A and B if either (a) the maximum of the 
2 distances from A and B to their least common 
ancestor isgreater than 3 nodes or (b) the sum of 
the 2 distances from A and B to the least 
common ancestor isgreater than 4 nodes. This is 
less intrusive than introducing segues at random 
or between every pair of successive nodes, and 
appears to perform better than introducing a
segue at each depth of the tree. 
6 An Example 
We currently have a working version of the 
system with a variety of different single and 
multi-document summarization filters. Figure 3 
shows an input script created by an author (the 
scripts in Figure 3 and 4 are schematic 
representations of the scripts, rather than the raw 
XML). The script includes two create goals, one 
with a single-document generic summarization 
filter, the other with a multi-document user- 
focused summarization filter. Figure 4 shows the 
ground script which was created automatically 
by the Content Creator component. Note the 
addition of media type specifications, the 
introduction of narrative nodes, and the 
extension of the temporal constraints. The final 
presentation generated is shown in Figure 5. 
Here we show screen dumps of the six SMIL 
segments produced, with the audio if any for 
each segment indicated in this paper next to an 
audio icon. 
105 
7 Status 
The summarization filters have incorporated 
several summarizers, including some that have 
been evaluated in the DARPA SUMMAC 
conference (Mani et al 99-1). These carry out 
both single-document and multi-document 
summarization, and include a preliminary 
biographical summarizer we have developed. 
The running text for the biography table in the 
second-last segment of Figure 5 is produced 
from meta-information i the table XML 
generated by the biographical summarizer. The 
production method for running text uses canned 
text which should work for any input table 
conforming to that DTD. 
The summarization filters are. being tested as 
part of a DARPA situated test with end-users. 
The briefing generator itself has been used 
internally to generate numerous briefings, and 
has been demonstrated aspart of the DARPA 
system. We also expect to carry out an 
evaluation to assess the extent to which the 
automation described here provides efficiency 
gains in briefing production. 
8 Related Work 
There is a fair amount of work on automatic 
authoring of multimedia presentations, e.g., 
(Wahlster et al 93), (Dalai et al 96), (Mittal et 
al. 95), (Andre and Rist 97) 5. These efforts 
differ from ours in two ways: first, unlike us, 
they are not open-domain; and, second, they 
don't use summarization components. While 
such efforts are extremely sophisticated 
compared to us in multimedia presentation 
planning and fine-grained coordination and 
synchronization capabilities, many of the 
components used in those efforts are clearly 
applicable to our work. For example, (Andre and 
Rist 96) include methods for leveraging lifelike 
characters in this process; these characters can 
be leveraged in our work as well, to help 
personify the computer narrator. In addition, our 
captions, which are very short, rely on canned 
text based on node labels in the initial script, or 
based on shallow meta-information generated by 
the summarization filter (in XML) along with 
the created media object. (Mittal e t  al. 95) 
describe avariety of strategies for generation of 
longer, more explanatory captions, some of 
which may be exploited in our work by 
deepening the level of recta-information, at least 
for summarization components developed by us. 
In our ability to leverage automatic 
summarization, our work should be clearly 
distinguished from work which attempts to 
format a summary (from an XML 
representation) into something akin to a 
Powerpoint briefing, e.g., (Nagao and Hasida 
98). Our work, by contrast, is focused on using 
summarization i  generating briefings from an 
abstract outline. 
9 Conclusion 
We have described methods for leveraging 
automatic summarization in the automatic 
generation of multimedia briefings. This work 
has taken an open-domain approach, in order to 
meet the requirements of the DARPA 
application we are involved with. We believe 
there is a stronger role that NL generation can 
play in the narrative aspects of our briefings, 
which currently rely for the most part on canned 
text. Our future work on description merging in 
biographical summaries, and on introducing 
referring expressions into the narrative nodes, 
would in effect ake advantage of more powerful 
generation methods, without sacrificing open- 
domain capabilities. This may require much 
richer meta-information specifications than the 
ones we currently use. 
Finally, we have begun the design of the Script 
Creator GUI (the only component in Figure l 
remaining to be built). This will allow the author 
to create scripts for the briefing generator 
(instead of editing templates by hand), by laying 
out icons for media objects in temporal order. A 
user will be able to select a "standard" briefing 
template from a menu, and then view it in a 
briefing/template structure ditor. The user can 
then provide content by adding annotations to 
any node in the briefing template. The user will 
have a choice of saving the edit version in 
template form, or in SMIL or possibly Microsoft 
Powerpoint format. 
106 
I 
I 
I 
I 
I 
i 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
Peru Act ion  Br ie f  ! ;! 
? Exeeadv? Smmmu'y  
o Hypothes is  
? S i tuat ion  Assessmml  :i 
o Ehromdo.e~ o f  \]~','?nls i i  
o B iograph les  :,~ 
? SWuctm-ed A~,mneats  :~ 
? .4 Jtentadve V iews  ' i  
? Der i s ion ,  ~:i 
.<e In this briefing I will go over the situation 
assessment. This will cover an overview of the 
chronology of  events and a profile of Victor 
Polay. 
"e Next, a biography of Victor Polay. 
::. Here is an overview of the chronology of 
events. 
I I III I II Illlll i 
I :  (3qN-  Peruv ian  cebe l~ re leet~e 2 bo , tages  - Dec.  IS~h ~i i  
3; JUOOUC ZOO hOS~flge~ ~.1~ d tn51cle the  h~ 0 ~' Japeme:~e ::~ 
J t~loan=edor Boc lh l=a kok i ,  vhece  Tupec  Jtz~l~u rebe l= were  ~! 
Victor Polay, also known as Comandante 
Rolando, is the Tupac Amaru founder, a
Peruvian guerrilla commander, a former ebel 
leader, and the Tupac Amaru rebels' top leader. 
He studied in both France and Spain. His wife is 
Rosa Polay and his mother is Otilia Campos de 
Polay. His associates include Alan Garcia. 
Here is the latest document summary. 
This briefing has assessed aspects of  the 
situation in Peru. Overall, the crisis appears to 
be worsening. 
Figure 5: Presentation 
107 
References 
Andre, E. and Rist, T. (1997) Towards a New 
Generation of Hypermedia Systems: Extending 
Automated Presentation Design for Hypermedia. 
L. Dybkjaer, ed., Proceedings of the Third Spoken 
Dialogue and Discourse Workshop, Topics in 
Natural Interactive Systems 1. The Maersk Me- 
Kinney Moiler Institute for Production 
Technology, Odense University, Denmark, pp. 10- 
27. 
Dalai, M., Feiner, S., McKeown, K., Pan, S., Zhou, 
M., Hollerer, T., Shaw, J., Feng, Y., and Fromer, J. 
(1996) Negotiation for Automated Generation of 
Temporal MultimediaPresentations. Proceedings 
of ACM Multimedia '96. 
Mani, 1., Gates, B., and Bloedorn, E. (1999) 
Improving Summaries by Revising Them. 
Proceedings of the 37 ~ Annual Meeting of the 
Association for Computational Lihguistics, College 
Park, MD, pp. 558-565. 
Mani, 1., Firmin, T., House, D., Klein, G., Sundheim, 
B., and Hirschman, L. (1999) The TIPSTER 
SUMMA C Text Summarization Evaluation. 
Proceedings of EACL'99, Bergen, Norway, pp. 77- 
85. 
Mani, 1. (2000)Automatic Text Summarization. John 
Benjamins Publishing Company. To appear. 
Mittal, V., Roth, S., Moore, J., Mattis, J., and 
Carenini, G. (1995) Generating Explanatory 
Captions for Information Graphics. Proceedings of 
the International Joint Conference on Artificial 
Intelligence (IJCAr95), pp. 1276-1283. 
Nagao, K. and K. Hasida, K. (1998) Automatic Text 
Summarization Based on the Global Document 
Annotation. Proceedings of COLING'98, Montreal, 
pp. 917-921. 
Power, R. and Scott, D. (1998) Multilingual" 
Authoring using Feedback Texts. Proceedings of 
COLING'98, Montreal, pp. 1053-1059. 
Taylor, P., Black, A., and Caley, R. (1998) The 
architecture of the Festival Speech Synthesis 
System. Proceedings of the Third ESCA Workshop 
on Speech Synthesis, Jenolan Caves, Australia, pp. 
147-151. 
Wahlster, W., Andre, E., Finkler, W., Profitlich, H.- 
J., and Rist, T. (1993) Plan-Based Integration of 
Natural Language and Graphics Generation. AI 
Journal, 63. 
108 
I 
l 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
A Multilingual Approach to Annotating
and Extracting Temporal Information1
George Wilson
Inderjeet Mani2
The MITRE Corporation,
W640
11493 Sunset Hills Road
Reston, VA 20190-5214
USA
gwilson@mitre.org
imani@mitre.org
Beth Sundheim
SPAWAR Systems Center,
D44208, 53140 Gatchell Rd.
San Diego, CA 92152-7420
USA
sundheim@spawar.navy.mil
Lisa Ferro
The MITRE Corporation,
K329, 202 Burlington Road
Bedford, MA 01730-1420
USA
lferro@mitre.org
                                                          
1 This work has been funded by DARPA?s Translingual Information Detection, Extraction, and Summarization (TIDES)
research program, under contract number DAA-B07-99-C-C201 and ARPA Order H049.
2 Also at the Department of Linguistics, Georgetown University, Washington, DC 20037.
Abstract
This paper introduces a set of
guidelines for annotating time
expressions with a canonicalized
representation of the times they refer
to, and describes methods for
extracting such time expressions from
multiple languages.
1 Introduction
The processing of temporal information poses
numerous challenges for NLP. Progress on these
challenges may be accelerated through the use
of corpus-based methods. This paper introduces
a set of guidelines for annotating time
expressions with a canonicalized representation
of the times they refer to, and describes methods
for extracting such time expressions from
multiple languages. Applications that can
benefit include information extraction (e.g.,
normalizing temporal references for database
entry), question answering (answering ?when?
questions), summarization (temporally ordering
information), machine translation (translating
and normalizing temporal references), and
information visualization (viewing event
chronologies).
Our annotation scheme, described in
detail in (Ferro et al 2000), has several novel
features, including the following:
It goes well beyond the one used in the Message
Understanding Conference (MUC7 1998), not
only in terms of the range of expressions that are
flagged, but, also, more importantly, in terms of
representing and normalizing the time values
that are communicated by the expressions.
In addition to handling fully-specified time
expressions (e.g., September 3rd, 1997), it also
handles context-dependent expressions. This is
significant because of the ubiquity of context-
dependent time expressions; a recent corpus
study (Mani and Wilson 2000) revealed that
more than two-thirds of time expressions in print
and broadcast news were context-dependent
ones. The context can be local (within the same
sentence), e.g., In 1995, the months of June and
July were devilishly hot, or global (outside the
sentence), e.g., The hostages were beheaded that
afternoon. A subclass of these context-
dependent expressions are ?indexical?
expressions, which require knowing when the
speaker is speaking to determine the intended
time value, e.g., now, today, yesterday,
tomorrow, next Tuesday, two weeks ago, etc.
The annotation scheme has been
designed to meet the following criteria:
? Simplicity with precision: We have tried to
keep the scheme simple enough to be
executed confidently by humans, and yet
precise enough for use in various natural
language processing tasks.
? Naturalness: We assume that the annotation
scheme should reflect those distinctions that
a human could be expected to reliably
annotate, rather than reflecting an
artificially-defined smaller set of
distinctions that automated systems might
be expected to make. This means that some
aspects of the annotation will be well
beyond the reach of current systems.
? Expressiveness:  The guidelines require that
one specify time values as fully as possible,
within the bounds of what can be
confidently inferred by annotators. The use
of ?parameters? and the representation of
?granularity? (described below) are tools to
help ensure this.
? Reproducibility: In addition to leveraging
the (ISO-8601 1997) format for representing
time values, we have tried to ensure
consistency among annotators by providing
an example-based approach, with each
guideline closely tied to specific examples.
While the representation accommodates
both points and intervals, the guidelines are
aimed at using the point representation to
the extent possible, further helping enforce
consistency.
The annotation process is decomposed into two
steps: flagging a temporal expression in a
document (based on the presence of specific
lexical trigger words), and identifying the time
value that the expression designates, or that the
speaker intends for it to designate. The flagging
of temporal expressions is restricted to those
temporal expressions which contain a reserved
time word used in a temporal sense, called a
?lexical trigger?, which include words like day,
week, weekend, now, Monday, current, future,
etc.
2 Interlingual Representation
2.1 Introduction
Although the guidelines were developed with
detailed examples drawn from English (along
with English-specific tokenization rules and
guidelines for determining tag extent), the
semantic representation we use is intended for
use across languages. This will permit the
development of temporal taggers for different
languages trained using a common annotation
scheme.
It will also allow for new methods for
evaluating machine translation of temporal
expressions at the level of interpretation as well
as at the surface level. As discussed in
(Hirschman et al 2000), time expressions
generally fall into the class of so-called named
entities, which includes proper names and
various kinds of numerical expressions.  The
translation of named entities is less variable
stylistically than the translation of general text,
and once predictable variations due to
differences in transliteration, etc. are accounted
for, the alignment of the machine-translated
expressions with a reference translation
produced by a human can readily be
accomplished. A variant of the word-error
metric used to evaluate the output of automatic
speech transcription can then be applied to
produce an accuracy score. In the case of our
current work on temporal expressions, it will
also be possible to use the normalized time
values to participate in the alignment and
scoring.
2.2 Semantic Distinctions
Three different kinds of time values are
represented: points in time (answering the
question ?when??), durations (answering ?how
long??), and frequencies (answering ?how
often??).
? Points in time are calendar dates and times-
of-day, or a combination of both, e.g.,
Monday 3 pm, Monday next week, a Friday,
early Tuesday morning, the weekend. These
are all represented with values (the tag
attribute VAL) in the ISO format, which
allows for representation of date of the
month, month of the year, day of the week,
week of the year, and time of day, e.g.,
<TIMEX2 VAL=?2000-11-
29T16:30?>4:30 p.m. yesterday afternoon
</TIMEX2>.
? Durations also use the ISO format to
represent a period of time. When only the
period of time is known, the value is
represented as a duration, e.g., <TIMEX2
VAL=?P3D?>a three-day </TIMEX2>
visit.
? Frequencies reference sets of time points
rather than particular points.  SET and
GRANULARITY attributes are used for
such expressions, with the PERIODICITY
attribute being used for regularly recurring
times, e.g., <TIMEX2 VAL=?XXXX-WXX-
2? SET=?YES? PERIODICITY=?F1W?
GRANULARITY=?G1D?>every
Tuesday</TIMEX2>.
Here ?F1W? means frequency of once a week,
and the granularity ?G1D? means the set
members are counted in day-sized units.
The annotation scheme also addresses several
semantic problems characteristic of temporal
expressions:
? Fuzzy boundaries. Expressions like
Saturday morning and Fall are fuzzy in their
intended value with respect to when the time
period starts and ends; the early 60?s is
fuzzy as to which part of the 1960?s is
included. Our format for representing time
values includes parameters such as FA (for
Fall), EARLY (for early, etc.),
PRESENT_REF (for today, current, etc.),
among others. For example, we have
<TIMEX2 VAL=?1990-SU?>Summer of
1990</TIMEX2>. Fuzziness in modifiers is
also represented, e.g., <TIMEX2
VAL=?1990? MOD=?BEFORE?>more
than a decade ago</TIMEX2>. The intent
here is that a given application may choose
to assign specific values to these parameters
if desired; the guidelines themselves don?t
dictate the specific values.
? Non-Specificity. Our scheme directs the
annotator to represent the values, where
possible, of temporal expressions that do not
indicate a specific time.  These non-specific
expressions include generics, which state a
generalization or regularity of some kind,
e.g., <TIMEX2 VAL=?XXXX-04?
NON_SPECIFIC=?YES?>April</TIMEX>
is usually wet, and non-specific indefinites,
like <TIMEX2 VAL="1999-06-XX"
NON_SPECIFIC="YES? GRANULARITY=
"G1D">a sunny day in <TIMEX2
VAL="199906">June</TIMEX2>
</TIMEX2>.
3 Reference Corpus
Based on the guidelines, we have arranged for 6
subjects to annotate an English reference corpus,
consisting of 32,000 words of a telephone dialog
corpus ? English translations of the ?Enthusiast?
corpus of Spanish meeting scheduling dialogs
used at CMU and by (Wiebe et al 1998), 35,000
words of New York Times newspaper text and
120,000 words of broadcast news (TDT2 1999).
This corpus will soon be made available to the
research community.
4 Time Tagger System
4.1 Architecture
The tagging program takes in a document which
has been tokenized into words and sentences and
tagged for part-of-speech. The program passes
each sentence first to a module that flags time
expressions, and then to another module (SC)
that resolves self-contained (i.e., ?absolute?)
time expressions. Absolute expressions are
typically processed through a lookup table that
translates them into a point or period that can be
described by the ISO standard.
The program then takes the entire
document and passes it to a discourse processing
module (DP) which resolves context-dependent
(i.e., ?relative?) time expressions (indexicals as
well as other expressions). The DP module
tracks transitions in temporal focus, using
syntactic clues and various other knowledge
sources.
The module uses a notion of Reference
Time to help resolve context-dependent
expressions. Here, the Reference Time is the
time a context-dependent expression is relative
to. The reference time (italicized here) must
either be described (as in ?a week from
Wednesday?) or implied (as in ?three days ago
[from today]?). In our work, the reference time
is assigned the value of either the Temporal
Focus or the document (creation) date. The
Temporal Focus is the time currently being
talked about in the narrative. The initial
reference time is the document date.
4.2 Assignment of Time Values
We now discuss the assigning of values to
identified time expressions. Times which are
fully specified are tagged with their value, e.g,
?June 1999? as 1999-06 by the SC module. The
DP module uses an ordered sequence of rules to
handle the context-dependent expressions. These
cover the following cases:
? Explicit offsets from reference time:
indexicals like ?yesterday?, ?today?,
?tomorrow?, ?this afternoon?, etc., are
ambiguous between a specific and a non-
specific reading. The specific use
(distinguished from the generic one by
machine learned rules discussed in (Mani
and Wilson 2000)) gets assigned a value
based on an offset from the reference time,
but the generic use does not. For example, if
?fall? is immediately preceded by ?last? or
?next?, then ?fall? is seasonal  (97.3%
accurate rule). If ?fall? is followed 2 words
after by a year expression, then ?fall? is
seasonal (86.3% accurate).
? Positional offsets from reference time:
Expressions like ?next month?, ?last year?
and ?this coming Thursday? use lexical
markers (underlined) to describe the
direction and magnitude of the offset from
the reference time.
? Implicit offsets based on verb tense:
Expressions like ?Thursday? in ?the action
taken Thursday?, or bare month names like
?February? are passed to rules that try to
determine the direction of the offset from
the reference time, and the magnitude of the
offset. The tense of a neighboring verb is
used to decide what direction to look to
resolve the expression.
? Further use of lexical markers:  Other
expressions lacking a value are examined
for the nearby presence of a few additional
markers, such as ?since? and ?until?, that
suggest the direction of the offset.
? Nearby Dates:  If a direction from the
reference time has not been determined,
some dates, like ?Feb. 14?, and other
expressions that indicate a particular date,
like ?Valentine?s Day?, may still be
untagged because the year has not been
determined.  If the year can be chosen in a
way that makes the date in question less
than a month from the reference date, that
year is chosen. Dates more than a month
away are not assigned values by this rule.
4.3 Time Tagging Performance
The system performance on a test set of 221
articles from the print and broadcast news
section of the reference corpus (the test set had a
total of 78,171 words) is shown in Table 13.
Note that if the human said the tag had no value,
and the system decided it had a value,  this is
treated as  an error. A baseline of just tagging
values of absolute, fully specified expressions
(e.g., ?January 31st, 1999?) is shown for
comparison in parentheses.
Type Human
Found
Correct
System
Found
System
Correct
F-
measure
TIMEX2 728 719 696 96.2
VAL 728 719 602
(234)
83.2
(32.3)
Table 1: Performance of Time Tagger
(English)
5 Multilingual Tagging
The development of a tagging program for other
languages closely parallels the process for
English and reuses some of the code. Each
language has its own set of lexical trigger words
that signal a temporal expression. Many of
these, e.g. day, week, etc., are simply
translations of English words.
Often, there will be some additional
triggers with no corresponding word in English.
For example, some languages contain a single
lexical item that would translate in English as
?the day after tomorrow?. For each language,
the triggers and lexical markers must be
identified.
As in the case of English, the SC
module for a new language handles the case of
absolute expressions, with the DP module
                                                          
3 The evaluated version of the system does not adjust the
Reference Time for subsequent sentences.
handling the relative ones. It appears that in
most languages, in the absence of other context,
relative expressions with an implied reference
time are relative to the present. Thus, tools built
for one language that compute offsets from a
base reference time will carry over to other
languages.
As an example, we will briefly describe
the changes that were needed to develop a
Spanish module, given our English one. Most of
the work involved pairing the Spanish surface
forms with the already existing computations,
e.g. we already computed ?yesterday? as
meaning ?one day back from the reference
point?. This had to be attached to the new
surface form ?ayer?. Because not all computers
generate the required character encodings, we
allowed expressions both with and without
diacritical marks, e.g., ma?ana and manana.
Besides the surface forms, there are a
few differences in conventions that had to be
accounted for. Times are mostly stated using a
24-hour clock. Dates are usually written in the
European form day/month/year rather than the
US-English convention of month/day/year.
A difficulty arises because of the use of
multiple calendric systems. While the Gregorian
calendar is widely used for business across the
world, holidays and other social events are often
represented in terms of other calendars. For
example, the month of Ramadan is a regularly
recurring event in the Islamic calendar, but
shifts around in the Gregorian4.
Here are some examples of tagging of
parallel text from Spanish and English with a
common representation.
<TIMEX2 VAL="2001-04-
01">hoy</TIMEX2>
<TIMEX2 VAL="2001-04-
01">today</TIMEX2>
<TIMEX2 VAL="1999-03-13">el trece de
marzo de 1999</TIMEX2>
<TIMEX2 VAL="1999-03-13">the thirteenth of
March, 1999</TIMEX2>
                                                          
4 Our annotation guidelines state that a holiday name is
markable but should receive a value only when that value
can be inferred from the context of the text, rather than
from cultural and world knowledge.
<TIMEX2 VAL="2001-W12">la semana
pasada</TIMEX2>
<TIMEX2 VAL="2001-W12">last
week</TIMEX2>
6 Related Work
Our scheme differs from the recent scheme of
(Setzer and Gaizauskas 2000) in terms of our in-
depth focus on representations for the values of
specific classes of time expressions, and in the
application of our scheme to a variety of
different genres, including print news, broadcast
news, and meeting scheduling dialogs. Others
have used temporal annotation schemes for the
much more constrained domain of meeting
scheduling, e.g., (Wiebe et al 1998),
(Alexandersson et al 1997), (Busemann et al
1997).  Our scheme has been applied to such
domains as well, our annotation of the
Enthusiast corpus being an example.
7 Conclusion
In the future, we hope to extend our English
annotation guidelines into a set of multilingual
annotation guidelines, which would include
language-specific supplements specifying
examples, tokenization rules, and rules for
determining tag extents. To support
development of such guidelines, we expect to
develop large keyword-in-context concordances,
and would like to use the time-tagger system as
a tool in that effort.  Our approach would be (1)
to run the tagger over the desired text corpora;
(2) to run the concordance creation utility over
the annotated version of the same corpora, using
not only TIMEX2 tags but also lexical trigger
words as input criteria; and (3) to partition the
output of the creation utility into entries that are
tagged as temporal expressions and entries that
are not so tagged.  We can then review the
untagged entries to discover classes of cases that
are not yet covered by the tagger (and hence,
possibly not yet covered by the guidelines), and
we can review the tagged entries to discover any
spuriously tagged cases that may correspond to
guidelines that need to be tightened up.
We also expect to create and distribute
multilingual corpora annotated according to
these guidelines. Initial feedback from machine
translation system grammar writers (Levin
2000) indicates that the guidelines were found to
be useful in extending an existing interlingua for
machine translation. For the existing English
annotations, we are currently carrying out inter-
annotator agreement studies of the work of the 6
annotators.
References
J. Alexandersson, N. Reithinger, and E. Maier.
Insights into the Dialogue Processing of
VERBMOBIL. Proceedings of the Fifth
Conference on Applied Natural Language
Processing, 1997, 33-40.
S. Busemann, T. Declerck, A. K. Diagne, L. Dini, J.
Klein, and S. Schmeier. Natural Language
Dialogue Service for Appointment Scheduling
Agents. Proceedings of the Fifth Conference on
Applied Natural Language Processing, 1997, 25-
32.
L. Ferro, I. Mani, B. Sundheim, and G. Wilson.
TIDES Temporal Annotation Guidelines. Draft
Version 1.0. MITRE Technical Report MTR
00W0000094, October 2000.
L. Hirschman, F. Reeder, J. Burger, and K. Miller,
Name Translation as a Machine Translation
Evaluation Task. Proceedings of LREC?2000.
ISO-8601 ftp://ftp.qsl.net/pub/g1smd/8601v03.pdf
1997.
L. Levin. Personal Communication.
I. Mani and G. Wilson. Robust Temporal Processing
of News, Proceedings of the ACL'2000
Conference, 3-6 October 2000, Hong Kong.
MUC-7. Proceedings of the Seventh Message
Understanding Conference, DARPA. 1998.
http://www.itl.nist.gov/iad/894.02/related_projects/muc/
A. Setzer and R. Gaizauskas. Annotating Events and
Temporal Information in Newswire Texts.
Proceedings of the Second International
Conference On Language Resources And
Evaluation (LREC-2000), Athens, Greece, 31
May- 2 June 2000.
TDT2
http://morph.ldc.upenn.edu/Catalog/LDC99T37.ht
ml 1999
J. M. Wiebe, T. P. O?Hara, T. Ohrstrom-Sandgren,
and K. J. McKeever. An Empirical Approach to
Temporal Reference Resolution. Journal of
Artificial Intelligence Research, 9, 1998, pp. 247-
293.
Appendix 1: Annotated
Corpus: Enthusiast Dialog
Example (one utterance)
Transcript of Spanish source:
EL LUNES DIECISIETE IMAGINO QUE
QUIERE DECIR EL DIECISIETE TENGO UN
SEMINARIO DESDE LAS DIEZ HASTA LAS
CINCO
Annotated English translation:
<TIMEX2 VAL=?2000-05-17?>MONDAY
THE SEVENTEENTH</TIMEX2> I IMAGINE
YOU MEAN <TIMEX2 VAL=?2000-05-
17?>THE SEVENTEENTH</TIMEX2> I
HAVE A SEMINAR FROM <TIMEX2
VAL=?2000-05-17T10?>TEN </TIMEX2>
UNTIL <TIMEX2 VAL=?2000-05-
17T17?>FIVE
</TIMEX2>
Note:  Elements of range expressions are tagged
separately.  The VAL includes date as well as
time because of the larger context.  The
annotator has confidently inferred that the
seminar is during the daytime, and has coded the
time portion of the VAL accordingly.
Appendix 2: Annotated
Corpus: New York Times
Article (excerpt)
   Dominique Strauss-Kahn, France's finance
minister, said: "<TIMEX2 VAL=?1999-01-01?>
Today</TIMEX2> is clearly <TIMEX2
NON_SPECIFIC=?YES?>a historic day for the
European enterprise</TIMEX2>. Europe will be
strong, stronger than in <TIMEX2
VAL=?PAST_REF?>the past</TIMEX2>,
because it will speak with a single monetary
voice."
   But even on <TIMEX2 VAL=?1998-12-31?>
Thursday </TIMEX2>, there were signs of
potential battles ahead.
   One hint came from Duisenberg, a former
Dutch central banker who was named president
of the European Central Bank only after a bitter
political fight <TIMEX2 VAL=?1998-05?>last
May</TIMEX2> between France and Germany.
Duisenberg, a conservative on monetary policy,
was favored by Helmut Kohl, who was
<TIMEX2 VAL=?1998-05?>then</TIMEX2>
chancellor of Germany. But President Jacques
Chirac of France insisted on the head of the
Bank of France, Jean-Claude Trichet.
   Germany and France eventually cut a deal
under which Duisenberg would become
president of the new European bank, but
"voluntarily" agree to step down well ahead of
<TIMEX2 VAL=?P8Y? MOD=?END?>the end
of his eight-year term</TIMEX2>.
                                                               Edmonton, May-June 2003
                                                                     Tutorials , pg. 6
                                                              Proceedings of HLT-NAACL
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 363?370, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Disambiguating Toponyms in News 
 
Eric Garbin Inderjeet Mani 
Department of Linguistics Department of Linguistics 
Georgetown University Georgetown University 
Washington, DC 20057, USA Washington, DC 20057, USA 
egarbin@cox.net im5@georgetown.edu 
 
 
Abstract 
This research is aimed at the problem of 
disambiguating toponyms (place names) 
in terms of a classification derived by 
merging information from two publicly 
available gazetteers. To establish the dif-
ficulty of the problem, we measured the 
degree of ambiguity, with respect to a 
gazetteer, for toponyms in news. We 
found that 67.82% of the toponyms found 
in a corpus that were ambiguous in a gaz-
etteer lacked a local discriminator in the 
text. Given the scarcity of human-
annotated data, our method used unsuper-
vised machine learning to develop disam-
biguation rules. Toponyms were 
automatically tagged with information 
about them found in a gazetteer. A 
toponym that was ambiguous in the gazet-
teer was automatically disambiguated 
based on preference heuristics. This 
automatically tagged data was used to 
train a machine learner, which disambigu-
ated toponyms in a human-annotated 
news corpus at 78.5% accuracy.  
1 Introduction 
Place names, or toponyms, are ubiquitous in natu-
ral language texts. In many applications, including 
Geographic Information Systems (GIS), it is nec-
essary to interpret a given toponym mention as a 
particular entity in a geographical database or gaz-
etteer. Thus the mention ?Washington? in ?He vis-
ited Washington last year? will need to be 
interpreted as a reference to either the city Wash-
ington, DC or the U.S. state of Washington, and 
?Berlin? in ?Berlin is cold in the winter? could 
mean Berlin, New Hampshire or Berlin, Germany, 
among other possibilities. While there has been a 
considerable body of work distinguishing between 
a toponym and other kinds of names (e.g., person 
names), there has been relatively little work on 
resolving which place and what kind of place given 
a classification of kinds of places in a gazetteer. 
Disambiguated toponyms can be used in a GIS to 
highlight a position on a map corresponding to the 
coordinates of the place, or to draw a polygon rep-
resenting the boundary. 
In this paper, we describe a corpus-based method 
for disambiguating toponyms. To establish the dif-
ficulty of the problem, we began by quantifying 
the degree of ambiguity of toponyms in a corpus 
with respect to a U.S. gazetteer. We then carried 
out a corpus-based investigation of features that 
could help disambiguate toponyms. Given the 
scarcity of human-annotated data, our method used 
unsupervised machine learning to develop disam-
biguation rules. Toponyms were automatically 
tagged with information about them found in a 
gazetteer. A toponym that was ambiguous in the 
gazetteer was automatically disambiguated based 
on preference heuristics. This automatically tagged 
data was used to train the machine learner. We 
compared this method with a supervised machine 
learning approach trained on a corpus annotated 
and disambiguated by hand. 
Our investigation targeted toponyms that name 
cities, towns, counties, states, countries or national 
capitals.   We sought to classify each toponym as a 
national capital, a civil political/administrative 
region, or a populated place (administration un-
specified). In the vector model of GIS, the type of 
place crucially determines the geometry chosen to 
represent it (e.g., point, line or polygon) as well as 
any reasoning about geographical inclusion. The 
class of the toponym can be useful in ?grounding? 
the toponym to latitude and longitude coordinates,  
363
Entry 
Number 
Topony
m 
U.S. 
County 
U.S. State Lat-Long 
(dddmmss) 
Elevation (ft. 
above sea 
level) 
Class 
110 Acton     
 
Middlesex Massachu-
setts 
422906N-
0712600W 
260 
 
Ppl (popu-
lated place) 
111 Acton Yellow-
stone 
Montana 455550N-
1084048W 
3816 Ppl 
112 Acton Los Ange-
les 
California 342812N-
1181145W 
2720 Ppl 
Table 1. Example GNIS entries for an ambiguous toponym 
 
but it can also go beyond grounding to support spa-
tial reasoning. For example, if the province is 
merely grounded as a point in the data model (e.g., 
if the gazetteer states that the centroid of a prov-
ince is located at a particular latitude-longitude) 
then without the class information, the inclusion of 
a city within a province can?t be established. Also, 
resolving multiple cities or a unique capital to a 
political region mentioned in the text can be a use-
ful adjunct to a map that lacks political boundaries 
or whose boundaries are dated. 
It is worth noting that our classification is more 
fine-grained than efforts like the EDT task in 
Automatic Content Extraction1 program (Mitchell 
and Strassel 2002), which distinguishes between 
toponyms that are a Facility ?Alfredo Kraus Audi-
torium?, a Location ?the Hudson River?, and Geo-
Political Entities that include territories ?U.S. 
heartland?, and metonymic or other derivative 
place references ?Russians?, ?China (offered)?, 
?the U.S. company?, etc. Our classification, being 
gazetteer based, is more suited to GIS-based appli-
cations. 
2 Quantifying Toponym Ambiguity  
2.1 Data 
We used a month?s worth of articles from the New 
York Times (September 2001), part of the English 
Gigaword (LDC 2003).  This corpus consisted of 
7,739 documents and, after SGML stripping, 6.51 
million word tokens with a total size of 36.4MB).  
We tagged the corpus using a list of place names 
from the USGS Concise Gazetteer (GNIS). The 
resulting corpus is called MAC1, for ?Machine 
Annotated Corpus 1?. GNIS covers cities, states, 
                                                          
1 www.ldc.upenn.edu/Projects/ACE/ 
and counties in the U.S., which are classified as 
?civil? and ?populated place? geographical enti-
ties.  A geographical entity is an entity on the 
Earth?s surface that can be represented by some 
geometric specification in a GIS; for example, as a 
point, line or polygon. GNIS also covers 53 other 
types of geo-entities, e.g., ?valley,? ?summit?, 
?water? and ?park.? GNIS has 37,479 entries, with 
27,649 distinct toponyms, of which 13,860 
toponyms had multiple entries in the GNIS (i.e., 
were ambiguous according to GNIS). Table 1 
shows the entries in GNIS for an ambiguous 
toponym. 
2.2 Analysis 
Let E be a set of elements, and let F be a set of fea-
tures. We define a feature g in F to be a disam-
biguator for E iff for all pairs <ex, ey> in E X E, 
g(ex) ? g(ey) and neither g(ex) nor g(ey) are null-
valued.  As an example, consider the GNIS gazet-
teer in Table 1, let F = {U.S. County, U.S. State, 
Lat-Long, and Elevation}. We can see that each 
feature in F is a disambiguator for the set of entries 
E = {110, 111, 112}.  
Let us now characterize the mapping between 
texts and gazetteers. A string s1 in a text is said to 
be a discriminator within a window w for another 
string s2 no more than w words away if s1 matches 
a disambiguator d for s2 in a gazetteer. For exam-
ple, ?MT? is a  discriminator within a window 5 
for the toponym ?Acton? in ?Acton, MT,? since 
?MT? occurs within a ?5-word window of ?Acton? 
and matches, via an abbreviation, ?Montana?, the 
value of a GNIS disambiguator U.S. State (here the 
tokenized words are ?Acton?, ?,?, and ?MT?).  
A trie-based lexical lookup tool (called LexScan) 
was used to match each toponym in GNIS against 
the corpus MAC1. Of the 27,649 distinct toponyms 
364
in GNIS, only 4553 were found in the corpus (note 
that GNIS has only U.S. toponyms). Of the 4553 
toponyms, 2911 (63.94%) were ?bare? toponyms, 
lacking a local discriminator within a ?5-word 
window that could resolve the name.  
Of the 13,860 toponyms that were ambiguous 
according to GNIS, 1827 of them were found in 
MAC1, of which only 588 had discriminators 
within a ?5-word window (i.e., discriminators 
which matched gazetteer features that disambigu-
ated the toponym). Thus, 67.82% of the 1827 
toponyms found in MAC1 that were ambiguous in 
GNIS lacked a discriminator.    
This 67.82% proportion is only an estimate of 
true toponym ambiguity, even for the sample 
MAC1. There are several sources of error in this 
estimate: (i) World cities, capitals and countries 
were not yet considered, since GNIS only covered 
U.S. toponyms. (ii) In general, a single feature 
(e.g., County, or State) may not be sufficient to 
disambiguate a set of entries. It is of course possi-
ble for two different places named by a common 
toponym to be located in the same county in the 
same state. However, there were no toponyms with 
this property in GNIS. (iii) A string in MAC1 
tagged by GNIS lexical lookup as a toponym may 
not have been a place name at all (e.g., ?Lord Ac-
ton lived ??). Of the toponyms that were spurious, 
most were judged by us to be common words and 
person names.  This should not be surprising, as 
5341 toponyms in GNIS are also person names 
according to the U.S. Census Bureau2 (iv) LexScan 
wasn?t perfect, for the following reasons. First, it 
sought only exact matches. Second, the matching 
relied on expansion of standard abbreviations. Due 
to non-standard abbreviations, the number of true 
U.S. toponyms in the corpus likely exceeded 4553.  
Third, the matches were all case-sensitive: while 
case-insensitivity caused numerous spurious 
matches, case-sensitivity missed a more predict-
able set, i.e. all-caps dateline toponyms or lower-
case toponyms in Internet addresses. 
Note that the 67.82% proportion is just an esti-
mate of local ambiguity. Of course, there are often 
non-local discriminators (outside the ?5-word 
windows); for example, an initial place name ref-
erence could have a local discriminator, with sub-
                                                          
                                                          2 www.census.gov/genealogy/www/freqnames.html 
 
sequent references in the article lacking local dis-
criminators while being coreferential with the ini-
tial reference. To estimate this, we selected cases 
where a toponym was discriminated on its first 
mention.  In those cases, we counted the number of 
times the toponym was repeated in the same 
document without the discriminator. We found that 
73% of the repetitions lacked a local discriminator, 
suggesting an important role for coreference (see 
Sections 4 and 5). 
3 Knowledge Sources for Automatic Dis-
ambiguation  
To prepare a toponym disambiguator, we required 
a gazetteer as well as corpora for training and test-
ing it.  
3.1 Gazetteer 
To obtain a gazetteer that covered worldwide 
information, we harvested countries, country capi-
tals, and populous world cities from two websites 
ATLAS3 and GAZ4, to form a consolidated gazet-
teer (WAG) with four features G1,..,G4 based on 
geographical inclusion, and three classes, as shown 
in Table 2. As an example, an entry for Aberdeen 
could be the following feature vector: G1=United 
States, G2=Maryland, G3=Harford County, 
G4=Aberdeen, CLASS=ppl.  
We now briefly discuss the merging of ATLAS 
and GAZ to produce WAG. ATLAS provided a 
simple list of countries and their capitals.  GAZ 
recorded the country as well as the population of 
700 cities of at least 500,000 people.  If a city was 
in both sources, we allowed two entries but or-
dered them in WAG to make the more specific 
type (e.g. ?capital?) the default sense, the one that 
LexScan would use. Accents and diacritics were 
stripped from WAG toponyms by hand, and aliases 
were associated with standard forms. Finally, we 
merged GNIS state names with these, as well as 
abbreviations discovered by our abbreviation ex-
pander.  
3.2 Corpora 
We selected a corpus consisting of 15,587 articles 
from the complete Gigaword Agence France 
3 . www.worldatlas.com 
4 www.worldgazetteer.com 
365
Presse, May 2002.  LexScan was used to tag, in-
sensitive to case, all WAG toponyms found in this 
corpus, with the attributes in Table 2.  If there were 
multiple entries in WAG for a toponym, LexScan 
only tagged the preferred sense, discussed below. 
The resulting tagged corpus, called MAC-DEV, 
 
Tag At-
tribute 
Description 
CLASS Civil (Political Region or Administrative Area, e.g. Country, Province, County), Ppl 
(Populated Place, e.g. City, Town), Cap (Country Capital, Provincial Capital, or County 
Seat) 
G1 Country 
G2 Province (State) or Country-Capital 
G3 County or Independent City 
G4 City, Town (Within County) 
Table 2: WAG Gazetteer Attributes 
 
Corpus Size Use How Annotated 
MAC1 6.51 million words with 
61,720 place names (4553 
distinct) from GNIS 
Ambiguity Study (Gigaword NYT Sept. 
2001) (Section 2) 
LexScan of all 
senses, no attributes 
marked  
MAC-
DEV 
5.47 million words with 
124,175 place names 
(1229 distinct) from 
WAG 
Development Corpus (Gigaword AFP 
May 2002) (Section 4) 
LexScan using at-
tributes from WAG, 
with heuristic pref-
erence 
MAC-
ML 
6.21 million words with 
181,866 place names 
(1322 distinct) from 
WAG 
Machine Learning Corpus (Gigaword AP 
Worldwide January 2002) (Section 5) 
LexScan using at-
tributes from WAG, 
with heuristic pref-
erence 
HAC 83,872 words with 1275 
place names (435 distinct) 
from WAG.   
Human Annotated Corpus (from Time-
Bank 1.2,  and Gigaword NYT Sept. 2001 
and June 2002) (Section 5) 
LexScan using 
WAG, with attrib-
utes and sense being 
manually corrected 
Table 3.  Summary of Corpora 
 
Term found 
with Cap 
T-test 
Civil 
T-
test 
Ppl 
Term found 
with Ppl 
T-test 
Civil 
T-test 
Cap 
Term found 
with Civil 
T-
test 
Ppl 
T-test 
Cap 
?stock? 4 4 ?winter? 3.61 3.61 ?air? 3.16 3.16 
?exchange? 4.24 4.24 ?telephone? 3.16 3.16 ?base? 3.16 3.16 
?embassy? 3.61 3.61 ?port? 3.46 3.46 ?accuses? 3.61 3.61 
?capital? 1.4 2.2 ?midfielder? 3.46 3.46 ?northern? 5.57 5.57 
?airport? 3.32 3.32 ?city? 1.19 1.19 ?airlines? 4.8 4.8 
?summit? 4 4 ?near? 2.77 3.83 ?invaded? 3.32 3.32 
?lower? 3.16 3.16 ?times? 3.16 3.16 ?southern? 3.87 6.71 
?visit? 4.61 4.69 ?southern? 3.87 3.87 ?friendly? 4 4 
?conference? 4.24 4.24 ?yen? 4 0.56 ?state-run? 3.32 3.32 
?agreement? 3.16 3.16 ?attack? 0.18 3.87 ?border? 7.48 7.48 
Table 4. Top 10 terms disambiguating toponym classes
was used as a development corpus for feature 
exploration. To disambiguate the sense for a 
toponym that was ambiguous in WAG, we used 
two preference heuristics. First, we searched 
366
MAC1 for two dozen highly frequent ambiguous 
toponym strings (e.g., ?Washington?, etc.), and 
observed by inspection which sense predomi-
nated in MAC1, preferring the predominant 
sense for each of these frequently mentioned 
toponyms. For example, in MAC1, ?Washing-
ton? was predominantly a Capital. Second, for 
toponyms outside this most frequent set, we 
used the following specificity-based preference: 
Cap. > Ppl > Civil. In other words, we prefer 
the more specific sense; since there are a smaller 
number of Capitals than Populated places, we 
prefer Capitals to Populated Places.  
For machine learning, we used the Gigaword 
Associated Press Worldwide January 2002 
(15,999 articles), tagged in the same way by 
LexScan as MAC-DEV was. This set was called 
MAC-ML. Thus, MAC1, MAC-DEV, and 
MAC-ML were all generated automatically, 
without human supervision. 
For a blind test corpus with human annotation, 
we opportunistically sampled three corpora: 
MAC1, TimeBank 1.25 and the June 2002 New 
York Times from the English Gigaword, with 
the first author tagging a random 28, 88, and 49 
documents respectively from each. Each tag in 
the resulting human annotated corpus (HAC) 
had the WAG attributes from Table 2 with man-
ual correction of all the WAG attributes. A 
summary of the corpora, their source, and anno-
tation status is shown in Table 3.  
4 Feature Exploration 
We used the tagged toponyms in MAC-DEV to 
explore useful features for disambiguating the 
classes of toponyms.  We identified single-word 
terms that co-occurred significantly with classes 
within a k-word window (we tried k= ?3, and 
k=?20). These terms were scored for pointwise 
mutual information (MI) with the classes. Terms 
with average tf.idf of less than 4 in the collection 
were filtered out as these tended to be personal 
pronouns, articles and prepositions.  
To identify which terms helped select for par-
ticular classes of toponyms, the set of 48 terms 
whose MI scores were above a threshold (-11, 
chosen by inspection) were filtered using the 
student?s t-statistic, based on an idea in (Church 
                                                          
5 www.timeml.org 
and Hanks 1991). The t-statistic was used to 
compare the distribution of the term with one 
class of toponym to its distribution with other 
classes to assess whether the underlying distri-
butions were significantly different with at least 
95% confidence. The results are shown in Table 
4, where scores for a term that occurred jointly 
in a window with at least one other class label 
are shown in bold. A t-score > 1.645 is a signifi-
cant difference with 95% confidence. However, 
because joint evidence was scarce, we eventu-
ally chose not to eliminate Table 4 terms such as 
?city? (t =1.19) as features for machine learning.  
Some of the terms were significant disambigua-
tors between only one pair of classes, e.g. ?yen,? 
?attack,? and ?capital,? but we kept them on that 
basis.  
 
Feature 
Name 
Description 
Abbrev Value is true iff the 
toponym  is abbreviated. 
AllCaps Value is true iff the 
toponym is all capital let-
ters.  
Left/Right 
Pos{1,.., k} 
Values are the ordered 
tokens up to k positions to 
the left/right 
WkContext Value is the set of MI 
collocated terms found in 
windows of ? k tokens (to 
the left and right) 
TagDis-
course 
 Value is the set of 
CLASS values represented 
by all toponyms from the 
document:  e.g., the set 
{civil, capital, ppl} 
CorefClass Value is the CLASS if 
any for a prior mention of 
a toponym in the docu-
ment, or none 
 Table 5. Features for Machine Learning 
 
Based on the discovered terms in experiments 
with different window sizes, and an examination 
of MAC1 and MAC-DEV, we identified a final 
set of features that, it seemed, might be useful 
for machine learning experiments. These are 
shown in Table 5.  The features Abbrev and All-
caps describe evidence internal to the toponym: 
367
an abbreviation may indicate a state (Mass.), 
territory (N.S.W.), country (U.K.), or some other 
civil place; an all-caps toponym might be a capi-
tal or ppl in a dateline.  The feature sets LeftPos 
and RightPos target the ?k positions in each 
window as ordered tokens, but note that only 
windows with a MI term are considered.  The 
domain of WkContext is the window of ?k to-
kens around a toponym that contains a MI collo-
cated term.     
   We now turn to the global discourse-level fea-
tures. The domain for TagDiscourse is the whole 
document, which is evaluated for the set of 
toponym classes present: this information may 
reflect the discourse topic, e.g. a discussion of 
U.S. sports teams will favor mentions of cities 
over states or capitals.  The feature CorefClass 
implements a one sense per discourse strategy, 
motivated by our earlier observation (from Sec-
tion 2) that 73% of subsequent mentions of a 
toponym that was discriminated on first mention 
were expressed without a local discriminator. 
5 Machine Learning 
The features in Table 5 were used to code fea-
ture vectors for a statistical classifier. The results 
are shown in Table 6.  As an example, when the 
Ripper classifier (Cohen 1996) was trained on 
MAC-ML with a window of k= ?3 word tokens, 
the predictive accuracy when tested using cross-
validation MAC-ML was 88.39% ?0.24 (where 
0.24 is the standard deviation across 10 folds).
 
Accuracy on Test Set   
Window = ?3 Window = ?20 
Training 
Set 
Test Set Predictive 
Accuracy 
Recall,  Preci-
sion, F-measure 
Predictive 
Accuracy 
Recall, Precision, 
F-measure 
MAC-ML  MAC-ML 
(cross-
validation) 
88.39 ? 
0.24 (Civ. 
65.0)  
Cap r70 p88 f78 
Civ. r94 p90 f92 
Ppl r87 p82 f84 
Avg. r84 p87 f85 
80.97 ? 
0.33 (Civ. 
57.1) 
Cap r61 p77 f68 
Civ. r83 p86 f84 
Ppl r81 p72 f76 
Avg. r75 p78 f76 
MAC-DEV  MAC-DEV 
(cross-
validation) 
87.08 ? 
0.28 (Civ. 
57.8) 
Cap r74 p87 f80 
Civ. r93 p88 f91 
Ppl r82 p80 f81 
Avg. r83 p85 f84 
81.36 ? 
0.59 (Civ. 
59.3) 
Cap r49 p78 f60 
Civ. r92 p81 f86 
Ppl r56 p70 f59 
Avg. r66 p77 f68 
MAC-DEV HAC 68.66 (Civ. 
59.7) 
Cap r50 p71 f59 
Civ. r93 p70 f80 
Ppl r24 p57 f33 
Avg. r56 p66 f57  
65.33 
(Civ. 50.7) 
Cap r100 p100 
f100 
Civ. r84 p62 f71 
Ppl r43 p71 f54 
Avg. r76 p78 f75 
HAC 
  
HAC 
 (cross-
validation) 
77.5 ? 2.94  
(Ppl 72.9) 
Cap r70 p97 f68 
Civ. r34 p94 f49 
Ppl r98 p64 f77 
Avg. r67 p85 f65 
73.12 ? 
3.09 (Ppl 
51.3) 
Cap r17 p90 f20 
Civ. r63 p76 f68 
Ppl r84 p73 f77 
Avg. r54 p79 f55 
MAC-
DEV+MAC-
ML 
MAC-
DEV+MAC-
ML (cross-
validation) 
86.76 ? 
0.18 (Civ. 
60.7) 
Cap r70 p89 f78 
Civ. r94 r88 f91 
Ppl r81 p80 f80 
Avg. r82 p86 f83 
79.70 ? 
0.30 (Civ. 
59.7) 
Cap r56 p73 f63 
Civ. r83 p86 f84 
Ppl r80 p68 f73 
Avg. r73 p76 f73 
MAC-
DEV+MAC-
ML 
HAC 73.07 (Civ. 
51.7) 
Cap r71 p83 f77 
Civ. r91 p69 f79 
Ppl r45 f81 f58 
Avg. r69 p78 f71 
78.30 
(Civ. 50) 
Cap r100 p63 f77 
Civ. r91 p75 f82 
Ppl r63 p88 f73 
Avg. r85 p75 f77 
Table 6. Machine Learning Accuracy 
368
The majority class (Civil) had the predictive accu-
racy shown in parentheses.  (When tested on a dif-
ferent set from the training set, cross-validation 
wasn?t used). Ripper reports a confusion matrix for 
each class; Recall, Precision, and F-measure for 
these classes are shown, along with their average 
across classes.  
In all cases, Ripper is significantly better in pre-
dictive accuracy than the majority class. When 
testing using cross-validation on the same ma-
chine-annotated corpus as the classifier was trained 
on, performance is comparable across corpora, and 
is in the high 80%, e.g., 88.39 on MAC-ML 
(k=?3). Performance drops substantially when we 
train on machine-annotated corpora but test on the 
human-annotated corpus (HAC) (the unsupervised 
approach), or when we both train and test on HAC 
(the supervised approach). The noise in the auto-
generated classes in the machine-annotated corpus 
is a likely cause for the lower accuracy of the un-
supervised approach. The poor performance of the 
supervised approach can be attributed to the lack of 
human-annotated training data: HAC is a small, 
83,872-word corpus.  
 
Rule Description  
(Window = ?3) 
Coverage 
of Examples 
in Testing 
(Accuracy) 
If not AllCaps(P) and  Right-
Pos1(P,?SINGLE_QUOTE?) 
and Civil ? TagDiscourse Then 
Civil(P). 
5/67 
(100%) 
If not AllCaps(P) and  Left-
Pos1(P, southern) and Civil ? 
TagDiscourse Then Civil(P). 
13/67 
(100%) 
Table 7. Sample Rules Learnt by Ripper 
TagDiscourse was a critical feature; ignoring it 
during learning dropped the accuracy nearly 9 per-
centage points. This indicates that prior mention of 
a class increases the likelihood of that class. (Note 
that when inducing a rule involving a set-valued 
feature, Ripper tests whether an element is a mem-
ber of that set-valued feature, selecting the test that 
maximizes information gain for a set of examples.) 
Increasing the window size only lowered accuracy 
when tested on the same corpus (using cross-
validation); for example, an increase from ?3 
words to ?20 words (intervening sizes are not 
shown for reasons of space) lowered the PA by 5.7 
percentage points on MAC-DEV. However, in-
creasing the training set size was effective, and this 
increase was more substantial for larger window 
sizes: combining MAC-ML with MAC-DEV im-
proved accuracy on HAC by about 4.5% for k= ?3, 
but an increase of 13% was seen for k =?20.  In 
addition, F-measure for the classes was steady or 
increased. As Table 6 shows, this was largely due 
to the increase in recall on the non-majority 
classes. The best performance when training Rip-
per on the machine-annotated MAC-DEV+MAC-
ML and testing on the human-annotated corpus 
HAC was 78.30.  
Another learner we tried, the SMO support-
vector machine from WEKA (Witten and Frank 
2005), was marginally better, showing 81.0 predic-
tive accuracy training and testing on MAC-
DEV+MAC-ML (ten-fold cross-validation, k=?20) 
and 78.5 predictive accuracy training on MAC-
DEV+MAC-ML and testing on HAC (k=?20). 
Ripper rules are of course more transparent: exam-
ple rules learned from MAC-DEV are shown in 
Table 7, along with their coverage of feature vec-
tors and accuracy on the test set HAC.  
6 Related Work 
Work related to toponym tagging has included 
harvesting of gazetteers from the Web (Uryupina 
2003), hand-coded rules to place name disam-
biguation, e.g., (Li et al 2003) (Zong et al 2005), 
and machine learning approaches to the problem, 
e.g., (Smith and Mann 2003). There has of course 
been a large amount of work on the more general 
problem of word-sense disambiguation, e.g., 
(Yarowsky 1995) (Kilgarriff and Edmonds 2002). 
We discuss the most relevant work here.  
While (Uryupina 2003) uses machine learning to 
induce gazetteers from the Internet, we merely 
download and merge information from two popular 
Web gazetteers. (Li et al 2003) use a statistical 
approach to tag place names as a LOCation class. 
They then use a heuristic approach to location 
normalization, based on a combination of hand-
coded pattern-matching rules as well as discourse 
features based on co-occurring toponyms (e.g., a 
document with ?Buffalo?, ?Albany? and ?Roches-
ter? will likely have those toponyms disambiguated 
to New York state). Our TagDiscourse feature is 
more coarse-grained. Finally, they assume one 
sense per discourse in their rules, whereas we use it 
369
as a feature CorefClass for use in learning. Overall, 
our approach is based on unsupervised machine 
learning, rather than hand-coded rules for location 
normalization. 
(Smith and Mann 2003) use a ?minimally super-
vised? method that exploits as training data 
toponyms that are found locally disambiguated, 
e.g., ?Nashville, Tenn.?; their disambiguation task 
is to identify the state or country associated with 
the toponym in test data that has those disambigua-
tors stripped off. Although they report 87.38% ac-
curacy on news, they address an easier problem 
than ours, since: (i) our earlier local ambiguity es-
timate suggests that as many as two-thirds of the 
gazetteer-ambiguous toponyms may be excluded 
from their test on news, as they would lack local 
discriminators (ii) the classes our tagger uses (Ta-
ble 3) are more fine-grained.  Finally, they use one 
sense per discourse as a bootstrapping strategy to 
expand the machine-annotated data, whereas in our 
case CorefClass is used as a feature. 
Our approach is distinct from other work in that 
it firstly, attempts to quantify toponym ambiguity, 
and secondly, it uses an unsupervised approach 
based on learning from noisy machine-annotated 
corpora using publicly available gazetteers.  
7 Conclusion 
This research provides a measure of the degree of 
of ambiguity with respect to a gazetteer for 
toponyms in news. It has developed a toponym 
disambiguator that, when trained on entirely ma-
chine annotated corpora that avail of easily avail-
able Internet gazetteers, disambiguates toponyms 
in a human-annotated corpus at 78.5% accuracy.  
Our current project includes integrating our dis-
ambiguator with other gazetteers and with a geo-
visualization system. We will also study the effect 
of other window sizes and the combination of this 
unsupervised approach with minimally-supervised 
approaches such as (Brill 1995) (Smith and Mann 
2003). To help mitigate against data sparseness, we 
will cluster terms based on stemming and semantic 
similarity.  
The resources and tools developed here may be 
obtained freely by contacting the authors.  
References  
Eric Brill. 1995. Unsupervised learning of disambigua-
tion rules for part of speech tagging. ACL Third 
Workshop on Very Large Corpora, Somerset, NJ, p. 
1-13. 
Ken Church, Patrick Hanks, Don Hindle, and William 
Gale. 1991. Using Statistics in Lexical Analysis. In 
U. Zernik (ed), Lexical Acquisition: Using On-line 
Resources to Build a Lexicon, Erlbaum, p. 115-164. 
William Cohen. 1996. Learning Trees and Rules with 
Set-valued Features. Proceedings of AAAI 1996, 
Portland, Oregon, p. 709-716. 
Adam Kilgarriff and Philip Edmonds. 2002. Introduc-
tion to the Special Issue on Evaluating Word Sense 
Disambiguation Systems. Journal of Natural Lan-
guage Engineering 8 (4).  
Huifeng Li, Rohini K. Srihari, Cheng Niu, and Wei Li. 
2003. A hybrid approach to geographical references 
in information extraction. HLT-NAACL 2003 Work-
shop: Analysis of Geographic References, Edmonton, 
Alberta, Canada.   
LDC. 2003. Linguistic Data Consortium: English Giga-
word 
www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalo
gId=LDC2003T05 
Alexis Mitchell and Stephanie Strassel. 2002. Corpus 
Development for the ACE (Automatic Content Ex-
traction) Program. Linguistic Data Consortium  
www.ldc.upenn.edu/Projects/LDC_Institute/ 
Mitchell/ACE_LDC_06272002.ppt 
David Smith and Gideon Mann.  2003.  Bootstrapping 
toponym classifiers. HLT-NAACL 2003 Workshop: 
Analysis of Geographic References, p. 45-49, Ed-
monton, Alberta, Canada.   
Ian Witten and Eibe Frank. 2005. Data Mining: Practi-
cal machine learning tools and techniques, 2nd Edi-
tion. Morgan Kaufmann, San Francisco.  
Olga Uryupina. 2003. Semi-supervised learning of geo-
graphical gazetteers from the internet. HLT-NAACL 
2003 Workshop: Analysis of Geographic References, 
Edmonton, Alberta, Canada.   
 David Yarowsky. 1995. Unsupervised Word Sense 
Disambiguation Rivaling Supervised Methods. Pro-
ceedings of ACL 1995, Cambridge, Massachusetts. 
Wenbo Zong, Dan Wu, Aixin Sun, Ee-Peng Lim, and 
Dion H. Goh. 2005. On Assigning Place Names to 
Geography Related Web Pages. Joint Conference on 
Digital Libraries (JCDL2005), Denver, Colorado. 
370
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 81?84, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Automating Temporal Annotation with TARSQI
Marc Verhagen?, Inderjeet Mani?, Roser Sauri?,
Robert Knippen?, Seok Bae Jang?, Jessica Littman?,
Anna Rumshisky?, John Phillips?, James Pustejovsky?
? Department of Computer Science, Brandeis University, Waltham, MA 02254, USA
{marc,roser,knippen,jlittman,arum,jamesp}@cs.brandeis.edu
? Computational Linguistics, Georgetown University, Washington DC, USA
{im5,sbj3,jbp24}@georgetown.edu
Abstract
We present an overview of TARSQI, a
modular system for automatic temporal
annotation that adds time expressions,
events and temporal relations to news
texts.
1 Introduction
The TARSQI Project (Temporal Awareness and
Reasoning Systems for Question Interpretation)
aims to enhance natural language question an-
swering systems so that temporally-based questions
about the events and entities in news articles can be
addressed appropriately. In order to answer those
questions we need to know the temporal ordering of
events in a text. Ideally, we would have a total order-
ing of all events in a text. That is, we want an event
like marched in ethnic Albanians marched Sunday
in downtown Istanbul to be not only temporally re-
lated to the nearby time expression Sunday but also
ordered with respect to all other events in the text.
We use TimeML (Pustejovsky et al, 2003; Saur?? et
al., 2004) as an annotation language for temporal
markup. TimeML marks time expressions with the
TIMEX3 tag, events with the EVENT tag, and tempo-
ral links with the TLINK tag. In addition, syntactic
subordination of events, which often has temporal
implications, can be annotated with the SLINK tag.
A complete manual TimeML annotation is not
feasible due to the complexity of the task and the
sheer amount of news text that awaits processing.
The TARSQI system can be used stand-alone
or as a means to alleviate the tasks of human
annotators. Parts of it have been intergrated in
Tango, a graphical annotation environment for event
ordering (Verhagen and Knippen, Forthcoming).
The system is set up as a cascade of modules
that successively add more and more TimeML
annotation to a document. The input is assumed to
be part-of-speech tagged and chunked. The overall
system architecture is laid out in the diagram below.
Input Documents
GUTime
Evita
SlinketGUTenLINK
SputLink
TimeML Documents
In the following sections we describe the five
TARSQI modules that add TimeML markup to news
texts.
2 GUTime
The GUTime tagger, developed at Georgetown Uni-
versity, extends the capabilities of the TempEx tag-
ger (Mani and Wilson, 2000). TempEx, developed
81
at MITRE, is aimed at the ACE TIMEX2 standard
(timex2.mitre.org) for recognizing the extents and
normalized values of time expressions. TempEx
handles both absolute times (e.g., June 2, 2003) and
relative times (e.g., Thursday) by means of a num-
ber of tests on the local context. Lexical triggers like
today, yesterday, and tomorrow, when used in a spe-
cific sense, as well as words which indicate a posi-
tional offset, like next month, last year, this coming
Thursday are resolved based on computing direc-
tion and magnitude with respect to a reference time,
which is usually the document publication time.
GUTime extends TempEx to handle time ex-
pressions based on the TimeML TIMEX3 standard
(timeml.org), which allows a functional style of en-
coding offsets in time expressions. For example, last
week could be represented not only by the time value
but also by an expression that could be evaluated to
compute the value, namely, that it is the week pre-
ceding the week of the document date. GUTime also
handles a variety of ACE TIMEX2 expressions not
covered by TempEx, including durations, a variety
of temporal modifiers, and European date formats.
GUTime has been benchmarked on training data
from the Time Expression Recognition and Normal-
ization task (timex2.mitre.org/tern.html) at .85, .78,
and .82 F-measure for timex2, text, and val fields
respectively.
3 EVITA
Evita (Events in Text Analyzer) is an event recogni-
tion tool that performs two main tasks: robust event
identification and analysis of grammatical features,
such as tense and aspect. Event identification is
based on the notion of event as defined in TimeML.
Different strategies are used for identifying events
within the categories of verb, noun, and adjective.
Event identification of verbs is based on a lexi-
cal look-up, accompanied by a minimal contextual
parsing, in order to exclude weak stative predicates
such as be or have. Identifying events expressed by
nouns, on the other hand, involves a disambigua-
tion phase in addition to lexical lookup. Machine
learning techniques are used to determine when an
ambiguous noun is used with an event sense. Fi-
nally, identifying adjectival events takes the conser-
vative approach of tagging as events only those ad-
jectives that have been lexically pre-selected from
TimeBank1, whenever they appear as the head of a
predicative complement. For each element identi-
fied as denoting an event, a set of linguistic rules
is applied in order to obtain its temporally relevant
grammatical features, like tense and aspect. Evita
relies on preprocessed input with part-of-speech tags
and chunks. Current performance of Evita against
TimeBank is .75 precision, .87 recall, and .80 F-
measure. The low precision is mostly due to Evita?s
over-generation of generic events, which were not
annotated in TimeBank.
4 GUTenLINK
Georgetown?s GUTenLINK TLINK tagger uses
hand-developed syntactic and lexical rules. It han-
dles three different cases at present: (i) the event
is anchored without a signal to a time expression
within the same clause, (ii) the event is anchored
without a signal to the document date speech time
frame (as in the case of reporting verbs in news,
which are often at or offset slightly from the speech
time), and (iii) the event in a main clause is anchored
with a signal or tense/aspect cue to the event in the
main clause of the previous sentence. In case (iii), a
finite state transducer is used to infer the likely tem-
poral relation between the events based on TimeML
tense and aspect features of each event. For ex-
ample, a past tense non-stative verb followed by a
past perfect non-stative verb, with grammatical as-
pect maintained, suggests that the second event pre-
cedes the first.
GUTenLINK uses default rules for ordering
events; its handling of successive past tense non-
stative verbs in case (iii) will not correctly or-
der sequences like Max fell. John pushed him.
GUTenLINK is intended as one component in a
larger machine-learning based framework for order-
ing events. Another component which will be de-
veloped will leverage document-level inference, as
in the machine learning approach of (Mani et al,
2003), which required annotation of a reference time
(Reichenbach, 1947; Kamp and Reyle, 1993) for the
event in each finite clause.
1TimeBank is a 200-document news corpus manually anno-
tated with TimeML tags. It contains about 8000 events, 2100
time expressions, 5700 TLINKs and 2600 SLINKs. See (Day
et al, 2003) and www.timeml.org for more details.
82
An early version of GUTenLINK was scored at
.75 precision on 10 documents. More formal Pre-
cision and Recall scoring is underway, but it com-
pares favorably with an earlier approach developed
at Georgetown. That approach converted event-
event TLINKs from TimeBank 1.0 into feature vec-
tors where the TLINK relation type was used as the
class label (some classes were collapsed). A C5.0
decision rule learner trained on that data obtained an
accuracy of .54 F-measure, with the low score being
due mainly to data sparseness.
5 Slinket
Slinket (SLINK Events in Text) is an application
currently being developed. Its purpose is to automat-
ically introduce SLINKs, which in TimeML specify
subordinating relations between pairs of events, and
classify them into factive, counterfactive, evidential,
negative evidential, and modal, based on the modal
force of the subordinating event. Slinket requires
chunked input with events.
SLINKs are introduced by a well-delimited sub-
group of verbal and nominal predicates (such as re-
gret, say, promise and attempt), and in most cases
clearly signaled by the context of subordination.
Slinket thus relies on a combination of lexical and
syntactic knowledge. Lexical information is used to
pre-select events that may introduce SLINKs. Pred-
icate classes are taken from (Kiparsky and Kiparsky,
1970; Karttunen, 1971; Hooper, 1975) and subse-
quent elaborations of that work, as well as induced
from the TimeBank corpus. A syntactic module
is applied in order to properly identify the subor-
dinated event, if any. This module is built as a
cascade of shallow syntactic tasks such as clause
boundary recognition and subject and object tag-
ging. Such tasks are informed from both linguistic-
based knowledge (Papageorgiou, 1997; Leffa, 1998)
and corpora-induced rules (Sang and De?je?an, 2001);
they are currently being implemented as sequences
of finite-state transducers along the lines of (A??t-
Mokhtar and Chanod, 1997). Evaluation results are
not yet available.
6 SputLink
SputLink is a temporal closure component that takes
known temporal relations in a text and derives new
implied relations from them, in effect making ex-
plicit what was implicit. A temporal closure compo-
nent helps to find those global links that are not nec-
essarily derived by other means. SputLink is based
on James Allen?s interval algebra (1983) and was in-
spired by (Setzer, 2001) and (Katz and Arosio, 2001)
who both added a closure component to an annota-
tion environment.
Allen reduces all events and time expressions to
intervals and identifies 13 basic relations between
the intervals. The temporal information in a doc-
ument is represented as a graph where events and
time expressions form the nodes and temporal re-
lations label the edges. The SputLink algorithm,
like Allen?s, is basically a constraint propagation al-
gorithm that uses a transitivity table to model the
compositional behavior of all pairs of relations. For
example, if A precedes B and B precedes C, then
we can compose the two relations and infer that A
precedes C. Allen allowed unlimited disjunctions of
temporal relations on the edges and he acknowl-
edged that inconsistency detection is not tractable
in his algebra. One of SputLink?s aims is to ensure
consistency, therefore it uses a restricted version of
Allen?s algebra proposed by (Vilain et al, 1990). In-
consistency detection is tractable in this restricted al-
gebra.
A SputLink evaluation on TimeBank showed that
SputLink more than quadrupled the amount of tem-
poral links in TimeBank, from 4200 to 17500.
Moreover, closure adds non-local links that were
systematically missed by the human annotators. Ex-
perimentation also showed that temporal closure al-
lows one to structure the annotation task in such
a way that it becomes possible to create a com-
plete annotation from local temporal links only. See
(Verhagen, 2004) for more details.
7 Conclusion and Future Work
The TARSQI system generates temporal informa-
tion in news texts. The five modules presented here
are held together by the TimeML annotation lan-
guage and add time expressions (GUTime), events
(Evita), subordination relations between events
(Slinket), local temporal relations between times and
events (GUTenLINK), and global temporal relations
between times and events (SputLink).
83
In the nearby future, we will experiment with
more strategies to extract temporal relations from
texts. One avenue is to exploit temporal regularities
in SLINKs, in effect using the output of Slinket as
a means to derive even more TLINKs. We are also
compiling more annotated data in order to provide
more training data for machine learning approaches
to TLINK extraction. SputLink currently uses only
qualitative temporal infomation, it will be extended
to use quantitative information, allowing it to reason
over durations.
References
Salah A??t-Mokhtar and Jean-Pierre Chanod. 1997. Sub-
ject and Object Dependency Extraction Using Finite-
State Transducers. In Automatic Information Extrac-
tion and Building of Lexical Semantic Resources for
NLP Applications. ACL/EACL-97 Workshop Proceed-
ings, pages 71?77, Madrid, Spain. Association for
Computational Linguistics.
James Allen. 1983. Maintaining Knowledge about
Temporal Intervals. Communications of the ACM,
26(11):832?843.
David Day, Lisa Ferro, Robert Gaizauskas, Patrick
Hanks, Marcia Lazo, James Pustejovsky, Roser Saur??,
Andrew See, Andrea Setzer, and Beth Sundheim.
2003. The TimeBank Corpus. Corpus Linguistics.
Joan Hooper. 1975. On Assertive Predicates. In John
Kimball, editor, Syntax and Semantics, volume IV,
pages 91?124. Academic Press, New York.
Hans Kamp and Uwe Reyle, 1993. From Discourse to
Logic, chapter 5, Tense and Aspect, pages 483?546.
Kluwer Academic Publishers, Dordrecht, Netherlands.
Lauri Karttunen. 1971. Some Observations on Factivity.
In Papers in Linguistics, volume 4, pages 55?69.
Graham Katz and Fabrizio Arosio. 2001. The Anno-
tation of Temporal Information in Natural Language
Sentences. In Proceedings of ACL-EACL 2001, Work-
shop for Temporal and Spatial Information Process-
ing, pages 104?111, Toulouse, France. Association for
Computational Linguistics.
Paul Kiparsky and Carol Kiparsky. 1970. Fact. In
Manfred Bierwisch and Karl Erich Heidolph, editors,
Progress in Linguistics. A collection of Papers, pages
143?173. Mouton, Paris.
Vilson Leffa. 1998. Clause Processing in Complex Sen-
tences. In Proceedings of the First International Con-
ference on Language Resources and Evaluation, vol-
ume 1, pages 937?943, Granada, Spain. ELRA.
Inderjeet Mani and George Wilson. 2000. Processing
of News. In Proceedings of the 38th Annual Meet-
ing of the Association for Computational Linguistics
(ACL2000), pages 69?76.
Inderjeet Mani, Barry Schiffman, and Jianping Zhang.
2003. Inferring Temporal Ordering of Events in News.
Short Paper. In Proceedings of the Human Language
Technology Conference (HLT-NAACL?03).
Harris Papageorgiou. 1997. Clause Recognition in the
Framework of Allignment. In Ruslan Mitkov and
Nicolas Nicolov, editors, Recent Advances in Natural
Language Recognition. John Benjamins, Amsterdam,
The Netherlands.
James Pustejovsky, Jose? Castan?o, Robert Ingria, Roser
Saur??, Robert Gaizauskas, Andrea Setzer, and Graham
Katz. 2003. TimeML: Robust Specification of Event
and Temporal Expressions in Text. In IWCS-5 Fifth
International Workshop on Computational Semantics.
Hans Reichenbach. 1947. Elements of Symbolic Logic.
MacMillan, London.
Tjong Kim Sang and Erik Herve De?je?an. 2001. Introduc-
tion to the CoNLL-2001 Shared Task: Clause Identifi-
cation. In Proceedings of the Fifth Workshop on Com-
putational Language Learning (CoNLL-2001), pages
53?57, Toulouse, France. ACL.
Roser Saur??, Jessica Littman, Robert Knippen, Robert
Gaizauskas, Andrea Setzer, and James Puste-
jovsky. 2004. TimeML Annotation Guidelines.
http://www.timeml.org.
Andrea Setzer. 2001. Temporal Information in Newswire
Articles: an Annotation Scheme and Corpus Study.
Ph.D. thesis, University of Sheffield, Sheffield, UK.
Marc Verhagen and Robert Knippen. Forthcoming.
TANGO: A Graphical Annotation Environment for
Ordering Relations. In James Pustejovsky and Robert
Gaizauskas, editors, Time and Event Recognition in
Natural Language. John Benjamin Publications.
Marc Verhagen. 2004. Times Between The Lines. Ph.D.
thesis, Brandeis University, Waltham, Massachusetts,
USA.
Marc Vilain, Henry Kautz, and Peter van Beek. 1990.
Constraint propagation algorithms: A revised report.
In D. S. Weld and J. de Kleer, editors, Qualitative Rea-
soning about Physical Systems, pages 373?381. Mor-
gan Kaufman, San Mateo, California.
84
Temporal Discourse Models for Narrative Structure 
Inderjeet MANI 
Department of Linguistics 
Georgetown University 
ICC 452 
Washington, DC 20057 
im5@georgetown.edu 
James PUSTEJOVSKY 
Department of Computer Science 
Brandeis University 
Volen 258 
Waltham, Massachusetts 02254 
jamesp@cs.brandeis.edu 
 
Abstract 
Getting a machine to understand human 
narratives has been a classic challenge for 
NLP and AI. This paper proposes a new 
representation for the temporal structure of 
narratives. The representation is parsimonious, 
using temporal relations as surrogates for 
discourse relations. The narrative models, 
called Temporal Discourse Models, are tree-
structured, where nodes include abstract 
events interpreted as pairs of time points and 
where the dominance relation is expressed by 
temporal inclusion. Annotation examples and 
challenges are discussed, along with a report 
on progress to date in creating annotated 
corpora. 
1 Introduction 
Getting a machine to understand human narratives 
has been a classic challenge for NLP and AI. 
Central to all narratives is the notion of time and 
the unfolding of events. When we understand a 
story, in addition to understanding other aspects 
such as plot, characters, goals, etc., we are able to 
understand the order of happening of events. A 
given text may have multiple stories; when we 
understand such a text, we are able to tease apart 
these distinct stories. Thus, understanding the story 
from a text involves building a global model of the 
sequences of events in the text, as well as the 
structure of nested stories. We refer to such models 
as Temporal Discourse Models (TDMs).  
 Currently, while we have informal 
descriptions of the structure of narratives, e.g., 
(Bell 1999), we lack a precise understanding of 
this aspect of discourse. What sorts of structural 
configurations are observed? What formal 
characteristics do they have? For syntactic 
processing of natural languages, we have, 
arguably, answers to similar questions. However, 
for discourse, we have hardly begun to ask the 
questions. 
 One of the problems here is that most of 
the information about narrative structure is implicit  
in the text. Thus, while linguistic information in 
the form of tense, aspect, temporal adverbials and 
discourse markers is often present, people use 
commonsense knowledge to fill in information. 
Consider a simple discourse: Yesterday Holly was 
running a marathon when she twisted her ankle. 
David had pushed her. Here, aspectual information 
indicates that the twisting occurred during the 
running, while tense suggests that the pushing 
occurs before the twisting. Commonsense 
knowledge also suggests that the pushing caused 
the twisting.  
 We can see that even for interpreting such 
relatively simple discourses, a system might 
require a variety of sources of linguistic 
knowledge, including knowledge of tense, aspect, 
temporal adverbials, discourse relations, as well as 
background knowledge. Of course, other 
inferences are clearly possible, e.g., that the 
running stopped after the twisting, but when 
viewed as defaults, these latter inferences seem to 
be more easily violated. The need for 
commonsense inferences has motivated 
computational approaches that are domain-
specific, using hand-coded knowledge (e.g., Asher 
and Lascarides 2003, Hitzeman et al 1995).  
 A number of theories have postulated the 
existence of various discourse relations that relate 
elements in the text to produce a global model of 
discourse, e.g., (Mann and Thompson 1988), 
(Hobbs 1985), (Hovy 1990) and others. In RST 
(Mann and Thompson 1988), (Marcu 2000), these 
relations are ultimately between semantic elements 
corresponding to discourse units that can be simple 
sentences or clauses as well as entire discourses. In 
SDRT (Asher and Lascarides 2003), these relations 
are between representations of propositional 
content, called Discourse Representation 
Structures (Kamp and Reyle, 1993).  
 Despite a considerable amount of very 
productive research, annotating such discourse 
relations has proved problematic. This is due to the 
fact that discourse markers may be absent (i.e., 
implicit) or ambiguous; but more importantly, 
because in many cases the precise nature of these 
discourse relations is unclear. Although (Marcu et 
 In addition to T1, we also have the 
temporal ordering constraints C1:  {Eb < Ec, Ec < 
Ea, Ea < Ed}. These are represented separately 
from the tree. A TDM is thus a pairing of tree 
structures and temporal constraints. More 
precisely, a Temporal Discourse Model for a text is 
a pair <T, C>, where T is a rooted, unordered, 
directed tree with nodes N = ?E ? A?, where E is 
the set of events mentioned in the text and A is a 
set of abstract events, and a parent-child ordering 
relation, ? (temporal inclusion). A non-leaf node 
can be textually mentioned or abstract. Nodes also 
have a set of atomic-valued features. Note that the 
tree is temporally unordered left to right. C is a set 
of temporal ordering constraints using the ordering 
relation, < (temporal precedence) as well as (for 
states, clarified below) ?minimal restrictions? on 
the above temporal inclusion relation (expressed as 
a ?min).  
al. 1999) (Carlson et al 2001) reported relatively 
high levels of inter-annotator agreement, this was 
based on an annotation procedure where the 
annotators were allowed to iteratively revise the 
instructions based on joint discussion.  
 While we appreciate the importance of 
representing rhetorical relations in order to carry 
out temporal inferences about event ordering, we 
believe that there are substantial advantages in 
isolating the temporal aspects and modeling them 
separately as TDMs. This greatly simplifies the 
representation, which we discuss next.   
2 Temporal Discourse Models 
A TDM is a tree-structured syntactic model of 
global discourse structure, where temporal 
relations are used as surrogates for discourse 
relations, and where abstract events corresponding 
to entire discourses are introduced as nodes in the 
tree.   In (1) the embedding nodes E0 and E1 were abstract, but textually mentioned events can 
also create embeddings, as in (2) (example from 
(Spejewski 1988)): 
 We begin by illustrating the basic 
intuition. Consider discourse (1), from (Webber 
1988): (2) a. Edmond made his own Christmas 
presents this year. b. First he dried a bunch 
of tomatoes in his oven. c. Then he made a 
booklet of recipes that use dried tomatoes. d. 
He scanned in the recipes from his gourmet 
magazines. e. He gave these gifts to his 
family. 
(1) a. John went into the florist shop.  
b. He had promised Mary some flowers.  
c. She said she wouldn?t forgive him if he 
forgot. d. So he picked out three red roses. 
 The discourse structure of (1) can be 
represented by the tree, T1, shown below.  
       T2 =                 E0               E0                    Ea                Ee               Ea         E1                      Ed                  Eb    Ec                                          Ed                        Eb       Ec               Here E0 has children Ea, E1, and Ed, and 
E1 has children Eb and Ec. The nodes with 
alphabetic subscripts are events mentioned in the 
text, whereas nodes with numeric subscripts are 
abstract events, i.e., events that represent abstract 
discourse objects. A node X is a child of node Y iff 
X is temporally included in Y. In our scheme, 
events are represented as pairs of time points. So, 
E0 is an abstract node representing a top-level 
story, and E1 is an abstract node representing an 
embedded story. Note that the mentioned events 
are ordered left to right in text order for notational 
convenience, but no temporal ordering is directly 
represented in the tree. Since the nodes in this 
representation are at a semantic level, the tree 
structure is not necessarily isomorphic to a 
representation at the text level, although T1 
happens to be isomorphic.  
      C2 = {Ea < Ee, Eb < Ec} 
 Note that the partial ordering C can be 
extended using T and temporal closure axioms 
(Setzer and Gaizauskas 2001), (Verhagen 2004), so 
that in the case of <T2, C2>, we can infer, for 
example, that Eb < Ed, Ed < Ee, and so forth.  
 In representing states, we take a 
conservative approach to the problems of 
ramification and change (McCarthy and Hayes 
1969). This is the classic problem of recognizing 
when states (the effects of actions) change as a 
result of actions. Any tensed stative predicate will 
be represented as a node in the tree (progressives 
are here treated as stative). Consider an example 
like John walked home. He was feeling great. 
Here we represent the state of feeling great as 
being minimally a part of the event of walking, 
without committing to whether it extends before or 
after the event. While this is interpreted as an 
overloaded temporal inclusion in the TDM tree, a 
constraint is added to C indicating that this 
inclusion is minimal.  
 This conservative approach results in 
logical incompleteness, however. For example, 
given the discourse Max entered the room. He was 
wearing a black shirt, the system will not know 
whether the shirt was worn after he entered the 
room. States are represented as bounded intervals, 
and participate in ordering relations with events in 
the tree. It is clear that in many cases, a state 
should persist throughout the interval spanning 
subsequent events. This is not captured by the 
current tree representation. Opposition structures 
of predicates and gating operations over properties 
can be expressed as constraints introduced by 
events, however, but at this stage of development, 
we have been interested in capturing a coarser 
temporal ordering representation, very robustly. 
We believe, however, that annotation using the 
minimal inclusion relation will allow us to reason 
about persistence heuristically in the future.  
3 Prerequisites 
Prior work on temporal information extraction has 
been fairly extensive and is covered in (Mani et al 
2004). Recent research has developed the TimeML 
annotation scheme (Pustejovsky et al 2002) 
(Pustejovsky et al 2004), as well as a corpus of 
TimeML-annotated news stories (TimeBank 2004) 
and annotation tools that go along with it, such as 
the TANGO tool (Pustejovsky et al 2003). 
TimeML flags tensed verbs, adjectives, and 
nominals that correspond to events and states, 
tagging instances of them with standard TimeML 
attributes, including the class of event (perception, 
reporting, aspectual, state, etc.), tense (past, 
present, future), grammatical aspect (perfective, 
progressive, or both), whether it is negated, any 
modal operators which govern it, and its 
cardinality if the event occurs more than once. 
Likewise, time expressions are flagged, and their 
values normalized, so that Thursday in He left on 
Thursday would get a resolved ISO time value 
depending on context  (TIMEX2 2004). Finally, 
temporal relations between events and time 
expressions (e.g., that the leaving occurs during 
Thursday) are recorded by means of temporal links 
(TLINKs) that express Allen-style interval 
relations (Allen 1984).  
 Several automatic tools have been 
developed in conjunction with TimeML, including 
event taggers (Pustejovsky et al 2003), time 
expression taggers (Mani and Wilson 2000), and 
an exploratory link extractor (Mani et al 2003). 
Temporal reasoning algorithms have also been 
developed, that apply transitivity axioms to expand 
the links using temporal closure algorithms (Setzer 
and Gaizauskas 2001), (Pustejovsky et al 2003).  
 However, TimeML is inadequate as a 
temporal model of discourse: it constructs no 
global representation of the narrative structure, 
instead annotating a complex graph that links 
primitive events and times. 
4 Related Frameworks 
Since the relations in TDMs involve temporal 
inclusion and temporal ordering, the mentioned 
events can naturally be mapped to other discourse 
representations used in computational linguistics. 
A TDM tree can be converted to a first-order 
temporal logic representation (where temporal 
ordering and inclusion operators are added) by 
expanding the properties of the nodes. These 
properties include any additional predications 
made explicitly about the event, e.g., information 
from thematic arguments and adjuncts. In other 
words, a full predicate argument representation, 
e.g., as might be found in the PropBank 
(Kingsbury and Palmer 2002), can be associated 
with each node.  
 TDMs can also be mapped to Discourse 
Representation Structures (DRS) (which in turn 
can be mapped to a logical form). Since TDMs 
represent events as pairs of time points (which can 
be viewed as intervals), and DRT represents events 
as primitives, we can reintroduce time intervals 
based on the standard DRT approach (e ? t for 
events,  e O t for states, except for present tense 
states, where t ? e).  
 Consider an example from the Discourse 
Representation Theory (DRT) literature (from 
Kamp and Reyle 1993): 
(3) a. A man entered the White Hart. b. He 
was wearing a black jacket. c. Bill served 
him a beer.  
   The TDM is <T3, C3> below, with 
internal properties of the nodes as shown: 
 
T3 =      E0 
 
             Ea          Ec 
                
     
         Eb 
 
C3 = {Ea < Ec} 
node.properties(Ea): enter(Ea, x, y), 
man(x), y= theWhiteHart, Ea < n 
node.properties(Eb): PROG(wear(Eb, x1, 
y1)), black-jacket(y1), x1=x, Eb < n,  
node.properties(Ec): serve(Ec, x2, y2, z), 
beer(z), x2=Bill, y2=x, Ec < n 
From T3: Eb ? Ea 
From C3: Ea < Ec 
 The DRT representation is shown below 
 (here we have created variables for the 
 reference times): 
 
  
 
 
 
 
 
  
 
 
  
 
  
 Note that we are by no means claiming 
that DRSs and TDMs are equivalent. TDMs are 
tree-structured and DRSs are not, and the inclusion 
relations involving our abstract events, i.e., Ea ? 
E0 and Ec ? E0, are not usually represented in 
DRT. Nevertheless, there are many similarities 
between TDMs and DRT which are worth 
examining for semantic and computational 
properties. Furthermore, SDRT (Asher and 
Lascarides 2003) extends DRT to include 
discourse relations. SDRT and RST both differ 
fundamentally from TDMs, since we dispense with 
rhetorical relations.  
 It should be pointed out, nevertheless, that 
TDMs, as modeled so far, do not represent 
modality and intensional contexts in the tree 
structure. (However, information about modality 
and negation is stored in the nodes based on 
TimeML preprocessing).  One way of addressing 
this issue is to handle lexically derived modal 
subordination (such as believe and want) by 
introducing embedded events, linked to the modal 
predicate by subordinating relations. For example, 
in the sentence John believed that Mary graduated 
from Harvard, the complement event is 
represented as a subtree linked by a lexical 
relation. 
 DLTAG (Webber et al 2004) is a model 
of discourse structure where explicit or implicit 
discourse markers relating only primitive discourse 
units. Unlike TDMs, where the nodes in the tree 
can contain embedded structures, DLTAG is a 
local model of discourse structure; it thus provides 
a set of binary relations, rather than a tree Like 
TDMs, however, DLTAG models discourse 
structure without postulating the existence of 
rhetorical relations in the discourse tree. Instead, 
the rhetorical relations appear as predicates in the 
semantic forms for discourse markers. In this 
respect, they differ from TDMs, which do not 
commit to specific rhetorical relations.   
 Spejewski (1994) developed a tree-based 
model of the temporal structure of a sequence of 
sentences. Her approach is based on relations of 
temporal coordination and subordination, and is 
thus a major motivation for our own approach. 
However, her approach mixes both reference times 
and events in the same representation, so that the 
parent-child relation sometimes represents 
temporal anchoring, and at other times 
coordination. In the above example of John walked 
home. He was feeling great, her approach would 
represent the ?reference time? of the state (of 
feeling great) as being part of the event of walking 
as well as part of the state, resulting in a graph 
rather than a strict tree. Note that our approach 
uses minimality. 
Ea, x, y , Eb, x1, y1, Ec, x2, y2, z, 
t1, t2, t3 
enter(Ea, x, y), man(x), y= 
theWhiteHart 
PROG(wear(Eb, x1, y1)), black-
jacket(y1), x1=x 
serve(Ec, x2, y2, z), beer(z), 
x2=Bill, y2=x 
t1 < n, Ea ? t1, t2 < n, Eb ? t2, Eb 
? Ea, t3 < n, Ec ? t3, Ea < Ec 
 (Hitzeman et al 1995) developed a 
computational approach to distinguish various 
temporal threads in discourse. The idea here, based 
on the notion of temporal centering, is that there is 
one ?thread? that the discourse is currently 
following. Thus, in (1) above, each utterance is 
associated with exactly one of two threads: (i) 
going into the florist?s shop and (ii) interacting 
with Mary. Hitzeman et al prefer an utterance to 
continue a current thread which has the same tense 
or is semantically related to it, so that in (1) above, 
utterance d would continue the thread (i) above 
based on tense. In place of world knowledge, 
however, semantic distance between utterances is 
used, presumably based on lexical relationships. 
Whether such semantic similarity is effective is a 
matter for evaluation, which is not discussed in 
their paper. For example, it isn?t clear what would 
rule out (1c) as continuing thread (i). 
 While TDMs do not commit to rhetorical 
relations, our expectation is that they can be used 
as an intermediate representation for rhetorical 
parsing. Thus, when event A in a TDM temporally 
precedes its right sibling B, the rhetorical relation 
of Narration will typically be inferred. When B 
precedes is left sibling A, then Explanation will 
typically be inferred. When A temporally includes 
a child node B, then Elaboration is typically 
inferred, etc. TDMs are thus a useful shallow 
representation that can be a useful first step in 
deriving rhetorical relations; indeed, rhetorical 
relations may be implicit in the human annotation 
of such relations, e.g., when explicit discourse 
markers like ?because? indicate a particular 
temporal order. 
5 Annotation Scheme  
The annotation scheme involves taking each 
document that has been preprocessed with time 
expressions and event tags (complying with 
TimeML) and then representing TDM parse trees 
and temporal ordering constraints (the latter also 
compliant with TimeML TLINKS). 
 Each discourse begins with a root abstract 
node. As an annotation convention, (A1) in the 
absence of any overt or covert discourse markers 
or temporal adverbials, a tense shift will license the 
creation of an abstract node, with the event with 
the shifted tense being the leftmost daughter of the 
abstract node. The abstract node will then be 
inserted as the child of the immediately preceding 
text node. In addition, convention (A2) states that 
in the absence of temporal adverbials and overt or 
covert discourse markers, a stative event will 
always be placed as a child of the immediately 
preceding text event when the latter is non-stative. 
Further, convention (A3) states that when the 
previous event is stative, in the absence of 
temporal adverbials and explicit or implicit 
discourse markers, the stative event is a sibling of 
the previous stative (as in a scene-setting fragment 
of discourse). 
 We expect that inter-annotator reliability 
on TDM trees will be quite high, given the 
transparent nature of the tree structure along with 
clear annotation conventions. The Appendices 
provide examples of annotation, to illustrate the 
simplicity of the scheme as well as potential 
problems. 
6  Corpora  
We have begun annotating three corpora with 
Temporal Discourse Model information. The first 
is the Remedia corpus (remedia.com). There are 
115 documents in total, grouped into four reading 
levels, all of which have been tagged by a human 
for time expressions in a separate project by Lisa 
Ferro at MITRE. Each document is short, about 
237 words on average, and has a small number of 
questions after it for reading comprehension.   
 The Brandeis Reading Corpus is a 
collection of 100 K-8 Reading Comprehension 
articles, mined from the web and categorized by 
level of comprehension difficulty. Articles range 
from 50-350 words in length. Complexity of the 
reading task is defined in terms of five basic 
classes of reading difficulty. 
 The last is the Canadian Broadcasting 
Corporation (cbc4kids.ca). The materials are 
current-event stories aimed at an audience of 8-
year-old to 13-year-old students. The stories are 
short (average length around 450 words). More 
than a thousand articles are available. The 
CBC4Kids corpus is already annotated with POS 
and parse tree markup.  
7 Conclusion  
Our assumption so far has been that the temporal 
structure of narratives is tree-structured and 
context-free. Whether the context-free property is 
violated or not remains to be seen.  
 Once the annotation effort is completed, 
we plan to use the annotated corpora in statistical 
parsing algorithms to construct TDMs. This should 
allow features from the corpus to be leveraged 
together to make inferences about narrative 
structure. While such knowledge source 
combination is not by any means guaranteed to 
substitute for commonsense knowledge, it at least 
allows for the introduction of generic, machine 
learning methods for extracting narrative structure 
from stories in any domain. Earlier work in a non-
corpus based (Hitzeman et al 1995) as well as 
corpus-based setting (Mani et al 2003) attests to 
the usefulness of combining knowledge sources for 
inferring temporal relations. We expect to leverage 
similar methods in TDM parsing. 
 We believe that the temporal aspect of 
discourse provides a handle for investigating 
discourse structure, thereby simplifying the 
problem of discourse structure annotation. It is 
therefore of considerable theoretical interest. 
Further, being able to understand the structure of 
narratives will in turn allow us to summarize them 
and answer temporal questions about narrative 
structure. 
References 
J. F. Allen. 1984. Towards a General Theory of 
Action and Time.  Artificial Intelligence 23: 
123-154. 
N. Asher and A. Lascarides. 2003. Logics of 
Conversation. Cambridge University Press. 
A. Bell. 1999. News Stories as Narratives. In A. 
Jaworski and N. Coupland, The Discourse 
Reader, Routledge, London and New York, 236-
251. 
L. Carlson, D. Marcu and M. E. Okurowski. 2001. 
Building a discourse-tagged corpus in the 
framework of rhetorical structure theory. In 
Proceedings of the 2nd SIGDIAL Workshop on 
Discourse and Dialogue, Eurospeech 2001, 
Aalborg, Denmark. 
B. Grosz, A. Joshi and S. Weinstein. 1995. 
Centering: A Framework for Modeling the 
Local Coherence of Discourse. Computational 
Linguistics 2(21), pp. 203-225  
J. Hitzeman, M. Moens and C. Grover. 1995. 
Algorithms for Analyzing the Temporal 
Structure of Discourse. In Proceedings of the 
Annual Meeting of the European Chapter of the 
Association for Computational Linguistics, 
Utrecht, Netherlands, 1995, 253-260. 
J. Hobbs. 1985. On the Coherence and Structure of 
Discourse. Report No. CSLI-85-37. Stanford, 
California: Center for the Study of Language 
and Information, Stanford University. 
E. Hovy. 1990. Parsimonious and Profligate 
Approaches to the Question of Discourse 
Structure Relations. In Proceedings of the Fifth 
International Workshop on Natural Language 
Generation.  
H. Kamp and U. Reyle. 1993. Tense and Aspect. 
Part 2, Chapter 5 of From Discourse to Logic, 
483-546. 
P. Kingsbury and M. Palmer. 2002. From 
Treebank to PropBank. In Proceedings of the 
3rd International Conference on Language 
Resources and Evaluation (LREC-2002), Las 
Palmas, Spain.  
I.. Mani, B. Schiffman and J. Zhang. 2003. 
Inferring Temporal Ordering of Events in News. 
Proceedings of the Human Language 
Technology Conference, HLT?03. 
I. Mani and G. Wilson. 2000.  Processing of News. 
Proceedings of the 38th Annual Meeting of the 
Association for Computational Linguistics 
(ACL'2000), 69-76.   
I. Mani, J. Pustejovsky and R. Gaizauskas. 2004. 
The Language of Time: A Reader. Oxford 
University Press, to appear. 
W. Mann and S. Thompson. 1988. Rhetorical 
structure theory: Toward a functional theory of 
text organization. Text, 8(3): 243-281.  
D. Marcu. 2000. The Theory and Practice of 
Discourse Parsing and Summarization. The 
MIT Press. 
D. Marcu, E. Amorrortu and M. Romera. 1999. 
Experiments in constructing a corpus of 
discourse trees. In Proceedings of the ACL 
Workshop on Standards and Tools for Discourse 
Tagging, College Park, MD, 48-57.  
J. McCarthy and P. Hayes. 1969. Some 
philosophical problems from the standpoint of 
artificial intelligence. In B.Meltzer and D. 
Michie, Eds. Machine Intelligence 4.  
J. Pustejovsky, B. Ingria, R. Sauri, J. Castano, J. 
Littman, R. Gaizauskas, A. Setzer, G. Katz and 
I. Mani. 2004. The Specification Language 
TimeML. In I. Mani, J. Pustejovsky and R. 
Gaizauskas. The Language of Time: A Reader. 
Oxford University Press, to appear. 
A. Setzer and R. Gaizauskas.  2001. A Pilot Study 
on Annotating Temporal Relations in Text. ACL 
2001, Workshop on Temporal and Spatial 
Information Processing  
B. Spejewski. 1994. Temporal Subordination in 
Discourse. .Ph.D. Thesis, University of 
Rochester. 
J. Pustejovsky, I. Mani, L. Belanger, B. Boguraev, 
B. Knippen, J. Littman, A. Rumshisky, A. See, 
S. Symonenko, J. Van Guilder, L. Van Guilder, 
M. Verhagen, R. Ingria. 2003. TANGO Final 
Report. timeml.org. 
 J. Pustejovsky, L. Belanger, J. Castano, R. 
Gaizauskas, P. Hanks, R. Ingria, G. Katz, D. 
Radev, A. Rumshisky, A. Sanfilippo, R. Sauri, 
B. Sundheim, M. Verhagen. 2002. TERQAS 
Final Report. timeml.org. 
TIMEBANK. 2004. timeml.org.  
TIMEX2. 2004. timex2.mitre.org. 
B. Webber. 1998. Tense as Discourse Anaphor. 
Computational Linguistics 14(2): 61-73. 
B. Webber, M. Stone, A. Joshi and A. Knott. 2003. 
Computational Linguistics, 29:4, 545-588. 
Appendix A: Examples from (Hitzeman et al 
1995) 
 
1. (a) John entered the room. (b) Mary stood 
up. 
 
Ea is inserted as left daughter of root. Eb is 
attached as sister (an analogue of a Narration 
default rhetorical relation).  
  E0 
 
  
 Ea   Eb 
 C: Ea<Eb 
 
2. (a) John entered the room. (b) Mary was 
seated behind the desk. 
 
Ea is anchored as left daughter of root. Eb is a 
tensed stative, and is embedded below Ea.  
 
  E0 
 
  Ea 
 
 Ea E1 E2   Eb 
 C: Eb ?min Ea 
  
                      Eb         Ec 3. (a) John fell. (b) Mary pushed him. 
 C: Eb<Ea, Ec<Eb  
  
7. (a) John got to work late. (b) He had left 
the house at 8. (c) He had eaten a big 
breakfast. 
 
 
  E0 
  
   
 Ea  Eb 
  E0  C: Eb<Ea 
  
 4. (a) John entered the room because (b) Mary 
stood up.  Ea E1 E2 
  
   E0 
                   Eb         Ec  
  
 C: Eb<Ea, Ec<Eb  Ea  Eb 
  C: Eb<Ea 
 This is due to the ?because?-inversion rule.  
 Appendix B: Level 200 Story from the Brandeis 
Reading Corpus 5. (a) Mary was tired. (b) She was exhausted. 
 
a. David wants to buy a Christmas present for a 
very special person, his mother.  
E0 
 
b. David's father gives him $5.00 a week pocket 
money and  
 
  Ea 
c. David puts $2.00 a week into his bank 
account.  
 
 
d. After three months David takes $20.00 out of 
his bank account and  
  Eb 
C: Eb ?min Ea 
e. goes to the shopping mall.   
f. He looks and looks for a perfect gift. This case, unlike (2), would be an analogue of an 
Elaboration relation. Here, other knowledge 
sources, such as a centering (Grosz et al 1995) 
could play a role in inferring such a discourse 
relation. 
g. Suddenly he sees a beautiful brooch in the 
shape of his favorite pet.  
h. He says to himself  
i. "Mother loves jewelry, and  
j. the brooch costs only $l7.00."   
k. He buys the brooch and  6. (a) Sam rang the bell. (b) He had lost the key. 
(c) It had fallen through a hole in his pocket.  l. takes it home.  
m. He wraps the present in Christmas paper and   
n. places it under the tree.  Ea is attached as right branching event. Eb is 
attached as sister with precedence constraint 
relative to Ea coming from the past perfect 
marking. Ec is attached as sister with precedence 
constraint relative to Eb coming from past perfect. 
Losing the key is explained by (or elaborated by) 
the description of the key falling through the hole. 
Hence, it should be an embedding relation on this 
reading. Nevertheless, the current parse is arguably 
correct since the falling caused the loss of the key.  
o. He is very excited and  
p. he is looking forward to Christmas morning to 
see the joy on his mother's face. 
q. But when his mother opens the present  
r. she screams with fright because  
s. she sees a spider. 
 
Ea, Eb, and Ec are all statively interpreted due to 
the presence of modification by frequency 
adverbial TIMEX3 expressions (from TimeML), 
giving rise to habitual event interpretations. They 
are embedded inside an abstract E0 node. 
  E0 
 
 
  E3  
   E0 
  
  
 Eq Er Es  
  Ea Eb Ec 
 C: Eq<Er, Es<Er, Eq<Es  C: Ea ?min E0, Eb ?min E0, Ec ?min E0 
 E1 is created with the recognition of the time 
expression ?after three months?. Ed is attached as 
the left daughter node in E1. Ee is attached as 
sister (default Narrative). Similarly for Ef, Eg, and 
Eh.  
The TDM for the  entire article is 
represented below: 
 
 
  E1 
 
 
 
 
 Ed Ee Ef Eg Eh 
 
 C: Ed<Ee, Ee<Ef, Ef<Eg, Eg<Eh 
 
The syntactically embedded sentences in (i) and 
(j) are recognized as states and are embedded 
within Eh.  
  E1 
 
 
 
 
 Ed Ee  Ef  Eg  Eh 
 
 
         Ei                Ej 
C: Ed<Ee, Ee<Ef, Ef<Eg, Eg<Eh, Ei in Eh, Ej in 
Eh 
 
Attachment and narrative order holds for Ek, 
El, Em, and En. The states in Eo and Ep will 
be embedded under En: 
 
 E1 
 
 
 
  Ed  Ee  Ef  Eg  Eh  Ek    El   Em     En 
 
              
 
                        Ei     Ej          Eo      Ep 
 
The presence of ?when? as a TimeML signal 
creates a new abstract event, E3, and the 
subsequent ordering relation E3>E2.  
 
Finally, narration continues under E3 with Eq, 
Er, and Es, as daughters to E3, with the additional 
constraint of ?because-inversion?, Es<Er.  
INVITED LECTURE 
 
Narrative Summarization 
Inderjeet MANI 
Department of Linguistics, ICC 452 
Georgetown University  
37th and O Street 
Washington, DC 20057  
USA 
im5@georgetown.edu 
 
Abstract 
The understanding and summarization of stories 
remains a challenge, in part because of the 
inability to adequately capture the ?aboutness? of 
information content. This can result in 
inappropriate content selection as well as summary 
incoherence. This talk sketches a general 
framework for narrative summarization that relies 
in part on explo itation of temporal information. I 
will show how this framework can help address 
the above problems. I go on to discuss how 
different levels of narrative structure can provide 
useful information for summarization. 
Automatically Inducing Ontologies from Corpora 
Inderjeet Mani 
 
 
Department of Linguistics 
Georgetown University, ICC 452 
37th and O Sts, NW 
Washington, DC 20057, USA 
im5@georgetown.edu 
Ken Samuel, Kris Concepcion and 
David Vogel 
 
The MITRE Corporation 
7515 Colshire Drive 
McLean, VA 22102, USA 
{samuel, kjc9, dvogel}@mitre.org 
 
Abstract 
The emergence of vast quantities of on-line 
information has raised the importance of methods 
for automatic cataloguing of information in a 
variety of domains, including electronic commerce 
and bioinformatics. Ontologies can play a critical 
role in such cataloguing. In this paper, we describe 
a system that automatically induces an ontology 
from any large on-line text collection in a specific 
domain. The ontology that is induced consists of 
domain concepts, related by kind-of and part-of 
links. To achieve domain-independence, we use a 
combination of relatively shallow methods along 
with any available repositories of applicable 
background knowledge. We describe our 
evaluation experiences using these methods, and 
provide examples of induced structures.  
1 Introduction 
The emergence of vast quantities of on-line 
information has raised the importance of methods 
for automatic cataloguing of information in a 
variety of domains, including electronic commerce 
and bioinformatics. Ontologies1 can play a critical 
role in such cataloguing. In bioinformatics, for 
example, there is growing recognition that 
common ontologies, e.g., the Gene Ontology2, are 
critical to interoperation and integration of 
biological data, including both structured data as 
found in protein databases, as well as unstructured 
data, as found in on-line biomedical literature.  
Constructing an ontology is an extremely 
laborious effort. Even with some reuse of ?core? 
knowledge from an Upper Model (Cohen et al 
1999), the task of creating an ontology for a 
particular domain and task has a high cost, 
incurred for each new domain. Tools that could 
automate, or semi-automate, the construction of 
                                                     
1 This research was supported by the National Science 
Foundation (ITR-0205470). 
2 www.geneontology.org 
ontologies for different domains could 
dramatically reduce the knowledge creation cost.  
One approach to developing such tools is to rely 
on information implicit in collections of on-line 
text in a particular domain. If it were possible to 
automatically extract terms and their semantic 
relations from the text corpus, the ontology 
developer could build on that knowledge, revising 
it, as needed, etc. This would be more cost-
effective than having a human develop the 
ontology from scratch.  
Our approach is inspired by research on topic-
focused multi-document summarization of large 
text collections, where there is a need to 
characterize the collection content succinctly in a 
hierarchy of topic terms and their relationships. 
Current approaches to multi-document 
summarization combine linguistic analysis, corpus 
statistics, and the use of background semantic 
knowledge from generic thesauri such as WordNet 
to infer semantic information about a person. In 
extending such approaches to ontology induction, 
the hypothesis is that similar hybrid approaches 
can be used to identify technical terms in a 
domain-specific corpus and infer semantic 
relationships among them.  
In this paper, we describe a system that 
automatically induces an ontology from any large 
on-line text collection in a specific domain, to 
support cataloguing in information access and data 
integration tasks. The induced ontology consists of 
domain concepts related by kind-of and part-of 
links, but does not include more specialized 
relations or axioms. The structure of the ontology 
is a directed acyclic graph (DAG). To achieve 
domain-independence, we use a combination of 
relatively shallow methods along with existing 
repositories of applicable background knowledge. 
These are described in Section 2. In Section 3, we 
also introduce a new metric Relation Precision for 
evaluating induced ontologies in comparison with 
reference ontologies. We have applied our system 
to produce ontologies in numerous domains: 
CompuTerm 2004  -  3rd International Workshop on Computational Terminology 47
  
 
 
 
 
 
 
 
Figure 1: System Architecture 
 
IRS 
Publication 17 
285 
 
k1 
0 285 
 
n1 
Reuters 
Corpus 
9 
k2 
19,024 19,043 
n2 
Total 294 19,024 19,328 
 
Table 1: Distribution of ?income tax? in domain and background corpora
(i) newswire from the TREC collection (ii) 
taxation information from the IRS (Publication 17, 
from (IRS 2001)), (iii) epidemiological newsgroup 
messages from the Program for Monitoring 
Emerging Diseases (PROMED) from the 
Federation of American Scientists3, (iv) the text of 
a book by the first author called Automatic 
Summarization, and (v) MEDLINE biomedical 
abstracts retrieved from the National Library of 
Medicine?s PubMed system4. In the latter domain, 
we have begun building a large ontology using the 
ontology induction methods along with post-
editing by domain experts in molecular biology at 
Georgetown University 5 . This ontology, called 
PRONTO, involves hundreds of thousands of 
protein names found in MEDLINE abstracts and in 
UNIPROT, the world?s largest protein database6. It 
is therefore infeasible to construct PRONTO by 
hand from scratch. PRONTO is also much larger 
than other ontologies in the biology area; for 
example, the Gene Ontology is rather high-level, 
and contains (as of March 2004) only about 17,000 
terms. 
                                                     
3 www.fas.org/promed/ 
4www4.ncbi.nlm.nih.gov/PubMed/ 
5 complingone.georgetown.edu/~prot/ 
6pir.georgetown.edu 
2 Approach 
2.1 System Architecture 
An overall architecture for domain-independent 
ontology induction is shown in Figure 1. The 
documents are preprocessed to separate out 
headers. Next, terms are extracted using finite-state 
syntactic parsing and scored to discover domain-
relevant terms. The subsequent processing infers 
semantic relations between pairs of terms using the 
?weak? knowledge sources run in the order 
described below. Evidence from multiple 
knowledge sources is then combined to infer the 
resulting relations. The resulting ontologies are 
written out in a standard XML-based format (e.g., 
XOL, RDF, OWL), for use in various information 
access applications.  
While the ontology induction procedure does not 
involve human labor, except for writing the 
preprocessing and term tokenization program for 
specialized technical domains, the human may edit 
the resulting ontology for use in a given 
application. An ontology editor has been 
developed, discussed briefly in Section 3.1. 
2.2 Term Discovery 
The system takes a collection of documents in a 
subject area, and identifies terms characteristic of 
the domain.  In a given domain such as 
CompuTerm 2004  -  3rd International Workshop on Computational Terminology48
bioinformatics, specialized term tokenization (into 
single- and multi-word terms) is required. The 
protein names can be long, e.g., 
?steroid/thyroid/retinoic nuclear hormone receptor 
homolog nhr-35?, and involve specialized patterns. 
In constructing PRONTO, we have used a protein 
name tagger based on an ensemble of statistical 
classifiers to tag protein names in collections of 
MEDLINE abstracts (Anon 2004). Thus, in such a 
domain, a specialized tagger replaces the 
components in the dotted box in Figure 1. 
In other domains, we adopt a generic term-
discovery approach. Here the text is tagged for 
part-of-speech, and single- and multi-word terms 
consisting of minimal NPs are extracted using 
finite-state parsing with CASS (Abney 1996). All 
punctuation except for hyphens are removed from 
the terms, which are then lower-cased. Each word 
in each term is stemmed, with statistics (see below) 
being gathered for each stemmed term. Multi-word 
terms are clustered so that open, closed and 
hyphenated compounds are treated as equivalent, 
with the most frequent term in the collection being 
used as the cluster representative.  
The terms are scored for domain-relevance based 
on the assumption that if a term occurs 
significantly more in a domain corpus than in a 
more diffuse background corpus, then the term is 
clearly domain relevant.  
As an illustration, in Table 1 we compare the 
number of documents containing the term ?income 
tax? (or ?income taxes?) in a long (2.18 Mb) IRS 
publication, Publication 17, from an IRS web site 
(IRS 2001) compared to a larger (27.63 Mb subset 
of the) Reuters 21578 news corpus7. One would 
expect that ?income tax? is much more a 
characteristic of the IRS publication, and this is 
borne out by the document frequencies in the table. 
We use the log likelihood ratio (LLR) (Dunning 
1993) given by 
-2log2(Ho(p;k1,n1,k2,n2)/Ha(p1,p2;n1,k1,n2,k2))  
LLR measures the extent to which a 
hypothesized model of the distribution of cell 
counts, Ha, differs from the null hypothesis, Ho  
(namely, that the percentage of documents 
containing this term is the same in both corpora). 
We used a binomial model for Ho and Ha8.   
2.3 Relationship Discovery 
The main innovation in our approach is to fuse 
together information from multiple knowledge 
                                                     
7 In Publication 17, each ?chapter? is a document. 
8From Table 1, p=294/19238=.015, p1=285/285=1.0, 
p2=9/19043=4.72, k1=285, n1=285, k2=9, n2=19043.  
sources as evidence for particular semantic 
relationships between terms. To infer semantic 
relations such as kind-of and part-of, the system 
uses a bottom-up data-driven approach using a 
combination of evidence from shallow methods. 
2.3.1 Subphrase Relations  
These are based on the presence of common 
syntactic heads, and allow us to infer, for example, 
that ?p68 protein? is a kind-of ?protein?. Likewise, 
in the TREC domain, subphrase analysis tells us 
that ?electric car? is a kind of ?car?, and in the IRS 
domain, that ?federal income tax? is a kind of 
?income tax?.  
2.3.2 Existing Ontology Relations 
These are obtained from a thesaurus. For 
example, the Gene Ontology can be used to infer 
that ?ATP-dependent RNA helicase? is a kind of 
?RNA-helicase?. Likewise, in the TREC domain, 
using WordNet tells us that ?tailpipe? is part of 
?automobile?, and in the IRS domain, that ?spouse? 
is a kind of ?person?.  Synonyms are also merged 
together at this stage. 
2.3.3 Contextual Subsumption Relations 
We also infer hierarchical relations between 
terms, by top-down clustering using a context-
based subsumption (CBS) algorithm. The 
algorithm uses a probabilistic measure of set 
covering to find subsumption relations. For each 
term in the corpus, we note the set of contexts in 
which the term appears. Term1 is said to subsume 
term2 when the conditional probability of term1 
appearing in a context given the presence of term2, 
i.e., P(term1|term2), is greater than some threshold.  
CBS is based on the algorithm of (Lawrie et al 
2001), which used a greedy approximation of the 
Domination Set Problem for graphs to discover 
subsumption relations among terms. Unlike their 
work, we did not seek to minimize the set of 
covering terms; therefore, a subsumed term may 
have multiple parents. The conditional probability 
threshold (0.8) we use to determine subsumption is 
much higher than in their approach. We also 
restrict the height of the hierarchies we build to 
three tiers. Tightening these latter two constraints 
appears to notably improve the quality of our 
subsumption relations.  
The largest corpus against which CBS has run is 
the ProMed corpus where, considering each 
paragraph a distinct context, there were 117,690 
contexts in the 11,198 documents. Here is an 
example from ProMed of a transitive relation that 
spans three tiers: ?mosquito? is a hypernym of 
?mosquito pool?, and ?mosquito? is also a 
hypernym of ?standing water?. 
CompuTerm 2004  -  3rd International Workshop on Computational Terminology 49
2.3.4 Explicit Patterns Relations 
This knowledge source infers specific relations 
between terms based on characteristic cue-phrases 
which relate them. For example, the cue-phrase 
?such as? (Hearst 1992) (Caraballo 1999) suggest a 
kind-of relation, e.g., ?a ligand such as 
triethylphosphine? tells us that ?triethylphosphene? 
is a kind of ?ligand?. Likewise, in the TREC 
domain, ?air toxics such as benzene? can suggest 
that ?benzene? is a kind of ?air toxic?. However, 
since such cue-phrase patterns tend to be sparse in 
occurrence, we do not use them in the evaluations 
described below.  
2.3.5 Domain-Specific Knowledge Sources 
Although our approach is domain-independent, it 
is possible to factor in domain knowledge sources 
for a given domain. For example, in biology, ?ase? 
is usually a suffix indicating an enzyme.  
Postmodifying PPs (found using a CASS grammar) 
can also be useful in some domains, as shown in  
?tax on investment income of child? in Figure 2. 
We have so far, however, not investigated other 
domain-specific knowledge sources. 
 
2.4 Evidence Combination 
The main point about these and other knowledge 
sources is that each may provide only partial 
information. Combining these knowledge sources 
together, we expect, will lead to superior 
performance compared to just any one of them. 
Not only do inferences from different knowledge 
sources support each other, but they are also 
combined to produce new inferences by transitivity 
relations. For example, since phrase analysis tells 
us that ?pyridine metabolism? is a kind-of 
?metabolism?, and Gene Ontology tells us that 
?metabolism? is a kind-of ?biological process?, it 
follows that ?pyridine metabolism? is a kind-of 
?biological process?. The evidence combination, in 
addition to computing transitive closure of these 
relations, also detects inconsistencies, querying the 
user to resolve them when detected. 
3 Evaluation 
3.1 Informal Assessment 
Subphrase Relations is a relatively high-
precision knowledge source compared to the 
others, producing many linked chains. Its 
performance can be improved by flagging and 
excluding proper names and idioms from its input 
(e..g, so that ?palm pilot? doesn?t show up as a 
kind-of ?pilot?). However, a chain of such relations 
can be interrupted by terms that aren?t lexically 
similar, but that are nevertheless in a kind-of 
relation. Some of these gaps are filled by 
transitivity relations involving other knowledge 
sources, especially Existing Ontologies, which is 
especially useful in filling gaps in some of the 
upper levels of the ontology. While Contextual 
Subsumption is good at  discovering associations 
between ?leaves? in the DAG and other concepts, 
the method cannot reliably infer the label of the 
relation. For example, in the IRS domain, we 
obtain ?divorce? as more general than ?decree of 
divorce? and ?separate maintenance?, but we don?t 
know the nature of the relations. Contextual 
Subsumption-inferred links are directed edges with 
label ?unknown?. 
Overall, the ontologies produced are noisy and 
require human correction, and the methods can 
produce many fragments that need to be linked by 
hand. While the system can detect cycles that need 
resolution by the human, these rarely arise
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2: An IRS Ontology viewed in the Ontology Editor 
 
CompuTerm 2004  -  3rd International Workshop on Computational Terminology50
 
Term Target  
DF 
Back-
ground 
DF 
LLR IG MI DF TF TF * 
IDF 
electric 80 61 99.9 99.9 81.3 99.9 99.9 27.8 
car 77 56 99.6 99.3 81.5 99.8 99.9 79.4 
battery 54 16 99.0 98.2 86.9 98.7 99.9 94.9 
emission 15 0 96.5 96.8 99.2 79.1 96.6 64.8 
year 58 505 67.9 67.6 25.0 99.2 99.7 65.7 
informal 10 29 66.2 66.3 0.2 48.6 99.7 99.2 
record 8 138 15.2 15.7 4.4 50.2 99.9 99.9 
osha 1 0 0.0 0.0 0.0 0.0 99.9 0.0 
Table 2: Comparing Topic 230 Term Percentile Rankings 
 
For a flavor of the kind of results we get, see 
Figure 2, which displays an ontology induced 
without any human intervention from IRS 
Publication 17. Here the DAG is displayed as a 
tree. The immediate children of  ?person?, a node 
high in the ontology, is shown in the left part of the 
window. Selecting ?child? brings up its kinds as 
well as some other children linked by ?unknown? 
label via Contextual Subsumption, e.g., ?full-time 
student?. A list of orphaned terms that aren?t 
related to any others are shown on the far right. 
The terms with checkboxes are those that occur in 
the corpus; the others are those that are found 
exclusively by Existing Ontology Relations. 
Checking a term allows it to be inspected in its 
occurrence context in the corpus. The editor comes 
with a variety of tools to help integrate ontology 
fragments. 
3.2 Human Evaluation 
3.2.1 Term Scoring  
To evaluate term scoring, we used a corpus of 
news articles about automobiles that consisted of 
85 documents relevant to the TREC Topic 230 
query: ?Is the automobile industry making an 
honest effort to develop and produce an electric-
powered automobile?? In Table 2, we provide 
some examples of how the LLR term scoring 
statistic performed with respect to five others on 
selected unigrams in the Topic 230 domain: term 
frequency, document frequency, term frequency 
times inverse document frequency (TF*IDF), 
pointwise mutual information (MI), and 
information gain (IG). Terms in bold are ones we 
judged important in the Topic 230 domain, the 
others are deemed unimportant. The numbers are 
percentile rankings. LLR and IG do equally well, 
outperforming the others. 
We carried out other comparisons for two other 
domains. In the income-tax domain, a hand-built 
term list from the IRS contained 82 terms which 
occurred in IRS Publication 17, of which the 
system discovered 77 (94% recall). In the ProMed 
domain, a pre-existing hand-built taxonomy 
produced by a bioterrorism analyst had 1048 terms 
which occurred in the ProMed message corpus, of 
which 607 were discovered by the system (58% 
recall). However, the hand-built taxonomy, which 
was built without consulting a corpus, wasn?t a 
full-fledged ontology, for example, there was no 
label for the parent-child relation.  
3.2.2 Term Relationships  
We also carried out an evaluation experiment to 
determine if the relations being discovered by the 
machine were in keeping with human judgments. 
We focused here on an evaluation of pairs of 
knowledge sources. Our experiment examined the 
case where the system discovered a kind-of 
relation. Here each subject was first asked to read 
four newspaper articles from the TREC topic-230 
sub-collection. The articles were then kept 
accessible to the subject in a browser window for 
the subject to consult if needed in answering 
subsequent questions. The subject was asked to 
judge, based on the documents read, whether term 
X was a kind of term Y, term Y was a kind of term 
X, or neither; e.g., ?Is acid a kind of pollutant, or is 
pollutant a kind of acid, or neither??. The subject 
had one of three mutually exclusive choices; the 
first two choices were presented in randomized 
order. 
The subjects were 16 native speakers of English 
unconnected with the project. Each subject was 
given ten questions to answer in each of the 
experiments. For each set of ten questions, five 
were chosen at random from pairs of terms related 
CompuTerm 2004  -  3rd International Workshop on Computational Terminology 51
by (immediate) kind-of relations. The remaining 
five questions were chosen at random from pairs of 
terms between which the system found no relation 
whatsoever. 
 
 Human 
System kind-
of(A, B) 
not kind-
of(A,B) 
kind-of(A, B) 56 18 
not kind-
of(A,B) 
6 
 
74 
 
Table 3: Is X a kind-of Y? 
 
We first discuss inter-subject agreement. Three 
subjects given the same relation to judge agreed 
75% of the time, leading to a Kappa score of 0.72, 
indicating a good level of agreement. This means 
that subjects were able to reliably make judgments 
as to whether A is a kind of B in some document. 
The results for the 16 subjects are shown in 
Table 3. When the system is compared to the 
human as ground truth, this gives a Precision of 
.90, a Recall of .75, and an F-measure of .82. This 
performance is also significantly better than 
random assignment: with chi-square=74.29, with p 
< 0.0019. The substantial effect sizes of the chi-
square indicates a very solid result. There were 62 
decisions involving Subphrase Relations (with 44 
True Positives and 18 False Negatives), and 10 
decisions involving WordNet (with 12 True 
Positives). This shows that there is solid agreement 
between the human subjects and the system on the 
kind-of relations.  However, these 154 decisions 
involved only four newspaper articles, so clearly 
more data would be helpful.  
3.3 Automatic Evaluation 
While evaluation by humans is valuable, it is 
expensive to carry out, and this expense must be 
incurred each time one wants to do an evaluation. 
Automatic comparison of a machine-generated 
ontology against reference ontologies constructed 
by humans, e.g., (Zhang et al 1996) (Sekine et al 
1999) (Daude et al 2001), is therefore desirable, 
provided suitable reference ontologies are 
available. In this evaluation, the human-generated 
taxonomy for ProMed described in Section 3.2.1 
was used as a reference ontology, with its 
unlabeled parent-child relation treated as a kind-of 
link. However, the human ?ontology? was created 
without looking at a corpus, and was developed for 
use with a different set of goals in mind. Although 
this involves comparing ?apples? and ?oranges?, a 
comparison is nevertheless illustrative, and can in 
addition be useful when comparing mutiple 
ontologies created under similar conditions. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 3:  Automatically Induced Fragment 
from ProMed 
To set aside the problem of differences in 
terminology involved in the comparison, we 
decided to restrict our attention to the set of terms 
TH (of cardinality 3025) in the human ontology 
(H), and have our system induce relations between 
them using the ProMed corpus. Relations were 
induced automatically in the machine ontology (M) 
for just 761 of those terms, yielding a set TH1.  The 
structure of  TH1 is shown in a fragment in Figure 
3.  Here A is a kind-of B if it is printed under B 
without a label; A is a part-of B if it is printed 
under B with a ?p? label. 
We then automatically computed, for each pair 
of terms t1 and t2 in TH1 that were linked distance 1 
apart in M, the distance between those terms in H. 
Likewise, we also computed, for each pair of terms 
t1 and t2 in TH1 distance 1 apart in H, the distance 
between those terms in M. 
The results of this comparison are as follows. 
The number of relations where the two ontologies 
agree exactly is 63 (i.e., the terms are distance 1 
apart in both ontologies). Since, given a set of 
terms, there are many different ways to construct 
an ontology, this is encouraging.   
The number of relations that our system found 
which were ?missed?, i.e., more than distance 1 
away, in H is 1203. Given the previous experiment 
where the human subjects agreed with the system's 
relations, these 1203 relations are likely to contain 
many that the human probably missed. For 
example, the relations in the machine ontology 
between ?eye? and ?farsightedness?, and ?medicine? 
                                                     
9  The chi-square for Subphrase Relations is 61.68, 
and the chi-square for WordNet is 56.73, with p < 0.001 
in all cases. 
CompuTerm 2004  -  3rd International Workshop on Computational Terminology52
4 Related Work and ?chiropractic medicine? are missed by H. This 
highlights a problem with human-generated 
ontologies: substantial errors of omission.  The existing approaches to ontology induction include those that start from structured data, 
merging ontologies or database schemas (Doan et 
al. 2002). Other approaches use natural language 
data, sometimes just by analyzing the corpus 
(Sanderson and Croft 1999), (Caraballo 1999) or 
by learning to expand WordNet with clusters of 
terms from a corpus, e.g., (Girju et al 2003). 
Information extraction approaches that infer 
labeled relations either require substantial hand-
created linguistic or domain knowledge, e.g., 
(Craven and Kumlien 1999) (Hull and Gomez 
1993), or require human-annotated training data 
with relation information for each domain (Craven 
et al 1998).  
The number of relations in H that our system 
missed (relations that were more than distance 1 
away in the system ontology), is 3493. However, 
of these 3493 relations, 2955 involved at least 1 
term that was not included in M, leaving 538 
relations that we could calculate the distance for in 
M. These 538 relations in H include relations 
between ?acid indigestion medicine? and ?maalox?, 
and ?alternative medicine? and ?acupuncture? (a 
majority of the misses involved relations between a 
disease and the name of a specific drug for it, 
which aren?t part-of or kind-of relations).  
 
0 
0.1 
0.2 
0.3 
0.4 
0.5 
0.6 
0.7 
0.8 
0.9 
1 
1 2 3 4 5 6 7 8 9 
D 
Relation-
Prec.(H,M,D)
Relation-
Prec.(M,H,D)
 
Many, though not all, domain-independent 
approaches (Evans et al 1991) (Grefenstette 1997) 
have restricted themselves to discovering term-
associations, rather than labeled relations. A 
notable exception is (Sanderson and Croft 1995), 
which (unlike our approach) assumes the existence 
of a query that was used to originally retrieve the 
documents (so that terms can be extracted from the 
query and then expanded to generate additional 
terms for the ontology). Their approach also is 
restricted to one method to discover relations, 
while we use several.  
Our approach is complementary to approaches 
aimed at automatically enhancing existing 
resources for a particular domain, e.g. (Moldovan 
et al 2000). Finally, the prior methods, while they 
often carry out evaluation, lack standard criteria for 
ontology evaluation. Although ontology evaluation 
remains challenging, we have discussed several 
evaluation methods in this paper. 
Figure 4: Relation Precision 
These observations lead to a metric for 
comparing one ontology with another one serving 
as a reference ontology. Given two ontologies A 
and B, define Relation Precision (A, B, D) as the 
proportion of the distance 1 relations in A that are 
at most a distance D apart in B. This measure can 
be plotted for different values of D. In Figure 4, we 
show the Relation Precision(H, M, D), and 
Relation Precision(M, H, D), for our machine 
ontology M and human ontology H. Both curves 
show Relation Precision(H, M, D) growing faster 
than Relation Precision(M, H, D), with 70% of the 
area being below the former curve and 54% being 
below the latter curve. The graph shows that while 
22% of distance 1 relations in M are at most 3 
apart in H (but keep in mind the errors of omission 
in H), 40% of distance 1 relations in H are at most 
3 apart in M10. 
5 Conclusion 
The evidence combination described above is 
based on transitivity and union. Since the above 
evaluations, we have been experimenting with an 
ad hoc weighted evidence combination scheme, 
based on each knowledge source expressing a 
strength for a posited relation. In future, we will 
also investigate using an initial seed ontology to 
provide a better ?backbone? for induction, and then 
using a spreading activation method to activate 
nodes related by existing knowledge sources to 
seed nodes. Corpus statistics can be used to weight 
the links. For example, based on (Caraballo 1999), 
each parent of a leaf node could be viewed as a 
cluster label for its children, with the weight of a 
parent-child link being determined based on how 
strongly the child is associated with the cluster.  
                                                     
10 The mean distance in H between terms that are 
distance 1 apart in M is 5.17, with a standard deviation 
of 2.12. The mean distance in M between terms which 
are distance 1 apart in H is 3.85, with a standard 
deviation of 1.69. 
The ontology induction methods described here 
can allow for considerable savings in time in 
CompuTerm 2004  -  3rd International Workshop on Computational Terminology 53
constructing ontologies. The evaluations we have 
carried out are suggestive, but many issues remain 
open. There are many unanswered questions about 
human-created reference ontologies, including lack 
of inter-annotator agreement studies. Indeed, 
experience shows that without guidelines for 
ontology construction, humans are prone to come 
up with very different ontologies for a domain. 
Comparing a machine-induced ontology against an 
ideal human reference ontology, were one to be 
available, is also fraught with problems. Our 
experience with using an implementation of the 
(Daude et al 2001) constraint relaxation algorithm 
for ontology comparison suggests that much work 
is needed on distance metrics which are not over-
sensitive to small differences in structure.  
Our interest, therefore, is focused more towards 
an extrinsic evaluation. PRONTO, which is due to 
be released in 2004, offers the opportunity to 
measure costs of ontology induction and post-
editing on a large-scale problem of value to the 
biology community. We also plan to measure the 
effectiveness of PRONTO in query expansion for 
information access to MEDLINE and protein 
databases. Finally, we will investigate more 
sophisticated evidence combination methods, and 
compare against other automatic methods for 
ontology induction. 
The ontology induction tools are available for 
free distribution for research purposes. 
References  
Abney, S. 1996. Partial parsing Via Finite-State 
Cascades. Proceedings of the ESSLLI '96 Robust 
Parsing Workshop. 
Caraballo, S. A. 1999. Automatic Construction of a 
hypernym-labeled noun hierarchy from text. In 
Proceedings of the 37th Annual Meeting of the 
Association for Computational Linguistics 
(ACL'1999), 120-122.  
Cohen, P. R., Chaudhri, V., Pease, A. and Schrag, 
R. 1999. Does Prior Knowledge Facilitate the 
Development of Knowledge-based Systems? The 
Sixteenth National Conference on Artificial 
Intelligence (AAAI-99).  
Craven, M. and Kumlien, J. 1999. Constructing 
biological knowledge bases by extracting 
information from text sources. Proc Int Conf 
Intell Syst Mol Biol., 77-86.  
Craven, M., DiPasquo, D., Freitag, D., McCallum, 
A., Mitchell, T., Nigam, K., and Slattery, S.. 
1998. Learning to Extract Symbolic Knowledge 
from the World Wide Web. Proceedings of 
AAAI-98, 509-516. 
Daude, J., Padro, L. and Rigau, G. 2001 A 
Complete WN1.5 to WN1.6 Mapping. NAACL-
2001 Workshop on WordNet and Other Lexical 
Resources: Applications, Extension, and 
Customization, 83-88.  
Doan, A.,  Madhavan, J. , Domings, P. and Halevy, 
A. 2002. Learning to Map between Ontologies 
on the Semantic Web. WWW?2002. 
Dunning, T. 1993. Accurate Methods for the 
Statistics of Surprise and Coincidence,? 
Computational Linguistics, 19(1):61-74, 1993. 
Girju, R.,  Badulescu, A., and Moldovan, D. 2003. 
Learning Semantic Constraints for the Automatic 
Discovery of Part-Whole Relations. Proceedings 
of HLT?2003, Edmonton. 
Grefenstette, G. 1997. Explorations in Automatic 
Thesaurus Discovery.  Kluwer International 
Series in Engineering and Computer Science, 
Vol 278. 
Hearst, M. 1992. Automatic Acquisition of 
Hyponyms from Large Text Corpora. 
Proceedings of the fourteenth International 
Conference on Computational Linguistics, 
Nantes, France, July 1992.  
Hull, R. and Gomez, F. 1993. Inferring Heuristic 
Classification Hierarchies from Natural 
Language Input. Telematics and Informatics, 
9(3/4), pp. 265-281. 
IRS (Internal Revenue Service). 2001. Tax Guide 
2001. Publication 17. http://www.irs.gov/pub/irs-
pdf/p17.pdf 
Lawrie, D., Croft, W. B., and Rosenberg, A. 2001. 
Finding topic words for hierarchical 
summarization. 24th ACM Intl. Conf. on 
Research and Development in Information 
Retrieval, 349-357, 2001. 
Miller, G. (1995). WordNet: A Lexical Database 
for English. Communications Of the Association 
For Computing Machinery (CACM) 38, 39-41.  
Sanderson, M. and Croft, B. 1995. Deriving 
concept hierarchies from text. Proceedings of the 
22nd Annual Internationaql ACM SIGIR 
Conference on Research and Development in 
Information Retrieval, 160-170. 
Sekine, S., Sudo, K. and Ogino, T. 1999. Statistical 
Matching of Two Ontologies. Proceedings of 
ACL SIGLEX99 Workshop: Standardizing 
Lexical Resources. 
Zhang, K., Wang, J. T. L. and Shasha, D.  1996. 
On the Editing Distance between Undirected 
Acyclic Graphs and Related Problems. 
International Journal of Foundations of 
Computer Science 7, 43-58. 
 
CompuTerm 2004  -  3rd International Workshop on Computational Terminology54
Proceedings of the Workshop on Annotating and Reasoning about Time and Events, pages 23?29,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Pilot Study on Acquiring Metric Temporal Constraints for Events 
Inderjeet Mani and Ben Wellner 
The MITRE Corporation 
202 Burlington Road, Bedford, MA 01730, USA 
and 
Department of Computer Science, Brandeis University 
415 South St., Waltham, MA 02254, USA 
{imani, wellner}@mitre.org 
  
 
Abstract 
Previous research on temporal anchoring 
and ordering has focused on the annota-
tion and learning of temporal relations 
between events. These qualitative rela-
tions can be usefully supplemented with 
information about metric constraints, 
specifically as to how long events last. 
This paper describes the first steps in ac-
quiring metric temporal constraints for 
events. The work is carried out in the 
context of the TimeML framework for 
marking up events and their temporal re-
lations. This pilot study examines the fea-
sibility of acquisition of metric temporal 
constraints from corpora.  
1 Introduction 
The growing interest in practical NLP applica-
tions such as question-answering and text sum-
marization places increasing demands on the 
processing of temporal information. In multi-
document summarization of news articles, it can 
be useful to know the relative order of events so 
as to merge and present information from multi-
ple news sources correctly. In question-
answering, one would like to be able to ask when 
an event occurs, or what events occurred prior to 
a particular event. A wealth of prior research by 
(Passoneau 1988), (Webber 1988), (Hwang and 
Schubert 1992), (Kamp and Reyle 1993), (Las-
carides and Asher 1993), (Hitzeman et al 1995), 
(Kehler 2000) and others, has explored the dif-
ferent knowledge sources used in inferring the 
temporal ordering of events, including temporal 
adverbials, tense, aspect, rhetorical relations, 
pragmatic conventions, and background knowl-
edge. For example, the narrative convention of 
events being described in the order in which they 
occur is followed in (1), but overridden by means 
of a discourse relation, Explanation in (2).  
 
(1) Max stood up. John greeted him.  
(2) Max fell. John pushed him. 
 
While there has been a spurt of recent research 
addressing the event ordering problem, e.g., 
(Mani and Wilson 2000) (Filatova and Hovy 
2001) (Schilder and Habel 2001) (Li et al 2001) 
(Mani et al 2003) (Li et al 2004) (Lapata and 
Lascarides 2004) (Boguraev and Ando 2005) 
(Mani et al 2006), that research relies on qualita-
tive temporal relations. Qualitative relations (e.g., 
event A BEFORE event B, or event A DURING 
time T) are certainly of interest in developing 
timelines of events in news and other genres. 
However, metric constraints can also be poten-
tially useful in this ordering problem. For exam-
ple, in (3), it can be crucial to know whether the 
bomb landed a few minutes to hours or several 
years BEFORE the hospitalization. While hu-
mans have strong intuitions about this from 
commonsense knowledge, machines don?t. 
 
 (3) An elderly Catholic man was 
hospitalized from cuts after a Prot-
estant gasoline bomb landed in his 
back yard.  
 
Fortunately, there are numerous instances such 
as (4), where metric constraints are specified ex-
plicitly: 
 
 (4) The company announced Tuesday 
that third quarter sales had fallen. 
  
In (4), the falling of sales occurred over the 
three-month period of time inferable from the 
speech time. However, while the announcement 
is anchored to a day inferable from the speech 
23
time, the length of the announcement is not 
specified.  
These examples suggest that it may be possi-
ble to mine information from a corpus to fill in 
extents for the time intervals of and between 
events, when these are either unspecified or par-
tially specified. Metric constraints can also po-
tentially lead to better qualitative links, e.g., 
events with long durations are more likely to 
overlap with other events.  
This paper describes some preliminary ex-
periments to acquire metric constraints. The ap-
proach extends the TimeML representation 
(Pustejovsky et al 2005) to include such con-
straints. We first translate a TimeML representa-
tion with qualitative relations into one where 
metric constraints are added. This representation 
may tell us how long certain events last, and the 
length of the gaps between them, given the in-
formation in the text. However, the information 
in the text may be incomplete; some extents may 
be unknown. We therefore need an external 
source of knowledge regarding the typical ex-
tents of events, which we can use when the text 
doesn?t provide it. We accordingly describe an 
initial attempt to bootstrap event durations from 
raw corpora as well as corpora annotated with 
qualitative relations.   
2 Annotation Scheme and Corpora 
TimeML (Pustejovsky et al 2005) 
(www.timeml.org) is an annotation scheme for 
markup of events, times, and their qualitative 
temporal relations in news articles. The TimeML 
scheme flags tensed verbs, adjectives, and nomi-
nals with EVENT tags with various attributes, 
including the class of event, tense, grammatical 
aspect, polarity (negative or positive), any modal 
operators which govern the event being tagged, 
and cardinality of the event if it?s mentioned 
more than once. Likewise, time expressions are 
flagged and their values normalized, based on an 
extension of the ACE (2004) (tern.mitre.org) 
TIMEX2 annotation scheme (called TIMEX3).  
For temporal relations, TimeML defines a 
TLINK tag that links tagged events to other 
events and/or times. For example, given sentence 
(4), a TLINK tag will anchor the event instance 
of announcing to the time expression Tuesday 
(whose normalized value will be inferred from 
context), with the relation IS_INCLUDED. This 
is shown in (5). 
 
(5) The company <EVENT even-
tID=e1>announced</EVENT> <TIMEX3 
tid=t1 value=1998-01-08>Tuesday 
</TIMEX3> that <TIMEX3 tid=t2 
value=P1Q3 beginPoint=t3 end-
Point=t4>third-quarter</TIMEX3> 
sales <EVENT eventID=e2> had 
fallen</EVENT>.  
<TLINK eventID=e1 relatedToEven-
tID=e2 relType=AFTER/> 
<TLINK eventID=e1 relatedToTimeID=t1 
relType=IS_INCLUDED/> 
<TIMEX3 tid=t3 value=1997-07/> 
<TIMEX3 tid=t4 value=1997-09/> 
 
The representation of time expressions in Ti-
meML uses TIMEX2, which is an extension of 
the TIMEX2 scheme (Ferro et al 2005). It repre-
sents three different kinds of time values: points 
in time (answering the question ?when??), dura-
tions (answering ?how long??), and frequencies 
(answering ?how often??)1.  
TimeML uses 14 temporal relations in the 
TLINK relTypes. Among these, the 6 inverse 
relations are redundant. In order to have a non-
hierarchical classification, SIMULTANEOUS 
and IDENTITY are collapsed, since IDENTITY 
is a subtype of SIMULTANEOUS. (An event or 
time is SIMULTANEOUS with another event or 
time if they occupy the same time interval. X and 
Y are IDENTICAL if they are simultaneous and 
coreferential). DURING and IS_INCLUDED are 
collapsed since DURING is a subtype of 
IS_INCLUDED that anchors events to times that 
are durations. (An event or time INCLUDES an-
other event or time if the latter occupies a proper 
subinterval of the former.) IBEFORE (immedi-
ately before) corresponds to MEETS in Allen?s 
interval calculus (Allen 1984). Allen?s OVER-
LAPS relation is not represented in TimeML.  
The above considerations allow us to collapse 
the TLINK relations to a disjunctive classifica-
tion of 6 temporal relations TRels = {SIMUL-
TANEOUS, IBEFORE, BEFORE, BEGINS, 
ENDS, INCLUDES}. These 6 relations and their 
inverses map one-to-one to 12 of Allen?s 13 ba-
sic relations (Allen 1984).  
Formally, each TLINK is a constraint of the 
general form x R y, where x and y are intervals, 
and R is a disjunct ?i=1,..,6(ri), where  ri is a rela-
tion in TRels. In annotating a document for Ti-
                                                 
1 Our representation (using t3 and t4) grounds the fuzzy 
primitive P1Q3 (i.e., a period of one 3rd-quarter) to specific 
months, though this is an application-specific step. In ana-
lyzing our data, we normalize P1Q3 as P3M (i.e., a period 
of 3 months). For conciseness, we omit TimeML EVENT 
and TIMEX3 attributes that aren?t relevant to the discus-
sion. 
24
meML, the annotator adds a TLINK iff she can 
commit to the TLINK relType being unambigu-
ous, i.e., having exactly one relType r. 
Two human-annotated corpora have been re-
leased based on TimeML2: TimeBank 1.2 (Puste-
jovsky et al 2003) with 186 documents and 
64,077 words of text, and the Opinion Corpus 
(www.timeml.org), with 73 documents and 
38,709 words. TimeBank 1.2 (we use 1.2.a) was 
created in the early stages of TimeML develop-
ment, and was partitioned across five annotators 
with different levels of expertise. The Opinion 
Corpus was developed recently, and was parti-
tioned across just two highly trained annotators, 
and could therefore be expected to be less noisy. 
In our experiments, we merged the two datasets 
to produce a single corpus, called OTC. 
3 Translation 
3.1 Introduction 
The first step is to translate a TimeML rep-
resentation with qualitative relations into one 
where metric constraints are added. This transla-
tion needs to produce a consistent metric repre-
sentation. The temporal extents of events, and 
between events, can be read off, when there are 
no unknowns, from the metric representation. 
The problem, however is that the representation 
may have unknowns, and the extents may not be 
minimal. 
3.2 Mapping to Metric Representation 
Let each event or time interval x be repre-
sented as a pair of start and end time points <x1, 
x2>. For example, given sentence (4), and the 
TimeML representation shown in (5), let x be 
fall and y be announce. Then, we have x1 = 
19970701T00, x2 = 19970930T23:59, y1 = 
19980108Tn1, y2 = 19980108Tn2 (here T repre-
sents time of day in hours). 
To add metric constraints, given a pair of 
events or times x and y, where x=<x1, x2> and 
y=<y1, y2>, we need to add, based on the quali-
tative relation between x and y, constraints of the 
general form (xi-yj) ? n, for 1 ? i, j ?2. We fol-
low precisely the method ?Allen-to-metric? of 
(Kautz and Ladkin 1991) which defines metric 
constraints for each relation R in TRels. For ex-
ample, here is a qualitative relation and its metric 
constraints: 
 
(6) x is BEFORE y iff (x2-y1) < 0.  
                                                 
2More details can be found at timeml.org. 
 
In our example, where x is fall and y is the 
announce, we are given the qualitative relation-
ship that x is BEFORE Y, so the metric con-
straint (x2-y1) < 0 can be asserted. 
Consider another qualitative relation and its 
metric constraint:  
 
(7) z INCLUDES y iff (z1-y1) < 0 and 
(y2-z2) < 0. 
 
Let y be announce in (4), as before, and let 
z=<z1, z2> be the time of Tuesday, where z1 = 
19980108T00, and z2 = 19980108T23:59. Since 
we are given the qualitative relation y 
IS_INCLUDED z, the metric constraints (z1-y1) 
< 0 and (y2-z2) < 0 can be asserted. 
3.3 Consistency Checking 
We now turn to the general problem of check-
ing consistency. The set of TLINKs for a docu-
ment constitutes a graph, where the nodes are 
events or times, and the edges are TLINKs. 
Given such a TimeML-derived graph for a 
document, a temporal closure algorithm (Verha-
gen 2005) carries out a transitive closure of the 
graph. The transitive closure algorithm was in-
spired by (Setzer and Gaizauskas 2000) and is 
based on Allen?s interval algebra, taking into 
account the limitations on that algebra that were 
pointed out by (Vilain et al 1990). It is basically 
a constraint propagation algorithm that uses a 
transitivity table to model the compositional be-
havior of all pairs of relations in a document. The 
algorithm?s transitivity table is represented by 
745 axioms. An example axiom is shown in (8):  
 
(8) If relation(A, B) = BEFORE && 
   relation(B, C) = INCLUDES 
then infer relation(A, C) = BEFORE. 
 
In propagating constraints, links added by clo-
sure can have a disjunction of one or more rela-
tions in TRels. When the algorithm terminates, 
any TLINK with more than one disjunct is dis-
carded. Thus, a closed graph is consistent and 
has a single relType r in TRels for each TLINK 
edge. The algorithm runs in O(n3) time, where n 
is the number of intervals.  
The closed graph is augmented so that when-
ever input edges a r1 b and b r2 c are composed to 
yield the output edge a r3 c, where r1, r2, and r3 
are in TRels, the metric constraints for r3 are 
added to the output edge. To continue our exam-
ple, since the fall x is BEFORE the Tuesday z 
25
and z INCLUDES y (announce), we can infer, 
using rule 8, that x is BEFORE y, i.e., that fall 
precedes announce. Using rule (6), we can again 
assert that (x2-y1) < 0. 
3.4 Reading off Temporal Extents 
 
Figure 1. Metric Constraints 
 
We now have the metric constraints added to 
the graph in a consistent manner. It remains to 
compute, given each event or time x=<x1, x2>, 
the values for x1 and x2. In our example, we 
have fall x=<19970701T00, 19970930T23:59>, 
announce y=<19980108Tn1, 19980108Tn2>, 
and Tuesday z=<19980108T00, 
19980108T23:59>, and the added metric con-
straints that (x2-y1), (z1-y1), and (y2-z2) are all 
negative. Graphically, this can be pictured as in 
Figure 1.  
As can be see in Figure 1, there are still un-
knowns (n1 and n2): we aren?t told exactly how 
long announce lasted -- it could be anywhere up 
to a day. We therefore need to acquire informa-
tion about how long events last when the exam-
ple text doesn?t tell us. We now turn to this prob-
lem. 
4 Acquisition 
We started with the 4593 event-time TLINKs 
we found in the unclosed human-annotated OTC. 
From these, we restricted ourselves to those 
where the times involved were of type TIMEX3 
DURATION. We augmented the TimeBank data 
with information from the raw (un-annotated) 
British National Corpus.  We tried a variety of 
search patterns to try and elicit durations, finally 
converging on the single pattern ?lasted?. There 
were 1325 hits for this query in the BNC. (The 
public web interface to the BNC only shows 50 
random results at a time, so we had to iterate.) 
The retrieved hits (sentences and fragments of 
sentences) were then processed with components 
from the TARSQI toolkit (Verhagen et al 2005) 
to provide automatic TimeML annotations. The 
TLINKs between events and times that were 
TIMEX3 DURATIONS were then extracted. 
These links were then corrected and validated by 
hand and then added to the OTC data to form an 
integrated corpus. An example from the BNC is 
shown in (9). 
 
(9) The <EVENT>storm</EVENT> 
<EVENT>lasted</EVENT> <TIMEX3 
VAL="P5D">five days</TIMEX3>.   
 
Next, the resulting data was subject to mor-
phological normalization in a semi-automated 
fashion to generate more counts for each event. 
Hyphens were removed, plurals were converted 
to singular forms, finite verbs to infinitival forms, 
and gerundive nominals to verbs. Derivational 
ending on nominals were stripped and the corre-
sponding infinitival verb form generated. These 
normalizations are rather aggressive and can lead 
to loss of important distinctions. For example, 
sets of events (e.g., storms or bombings) as a 
whole can have much longer durations compared 
to individual events. In addition, no word-sense 
disambiguation was carried out, so different 
senses of a given verb or event nominal may be 
confounded together.  
5 Results 
 
Number of 
durations 
Number of 
Events 
Normalized 
Form of 
Event 
26 1 lose 
16 1 earn 
10 1 fall 
9 1 rise 
8 1 drop 
7 2 decline, in-
crease 
6 4 end, grow, 
say, sell 
5 2 income, 
stop 
4 9 ? 
3 17 ? 
2 40 ? 
1 176 ? 
Table 1. Frequencies of event durations 
 
The resulting dataset had 255 distinct events 
with the number of durations for the events as 
shown in the frequency distribution in Table 1. 
The granularities found in news corpora such as 
OTC and mixed corpora such as BNC are domi-
26
nated by quarterly reports, which reflect the in-
fluence of specific information pinpointing the 
durations of financial events. This explains the 
fact that 12 of the top 13 events in Table 1 are 
financial ones, with the reporting verb say being 
the only non-financial event in the top 13.  
The durations for the most frequent event, rep-
resented by the verb to lose, is shown in Table 2. 
Most losses are during a quarter, or a year, be-
cause financial news tends to quantize losses for 
those periods. 
 
Duration Frequency
1 day (P1D) 1 
2 months (P2M) 1 
unspecified weeks (PXW) 1 
unspecified months (PXM) 1 
3 months (P3M) 9 
9 months (P9M) 3 
1 year (P1Y) 6 
5 years (P5Y) 1 
1 decade (P1Y) 1 
TOTAL 26 
Table 2. Distribution of durations for event 
 to lose 
 
Ideally, we would be able to generalize over 
the duration values, grouping them into classes. 
Table 3 shows some hand-aggregated duration 
classes for the data. These classes are ranges of 
durations. It can be seen that the temporal span 
of events across the data is dominated by 
granularities of weeks and months, extending 
into small numbers of years.  
 
Duration Class Count
<1 min 1 
5-15 min 12 
1-<24 hr 20 
1 day 14 
2-14 days 49 
1-3 months 120 
7-9 months 48 
1-6 years 97 
1 decade - < 1 century 30 
1-2 centuries 2 
vague (unspecified 
mins/days/months, continu-
ous present, indefinite fu-
ture, etc.) 
69 
Table 3. Distribution of aggregated durations 
 
   Interestingly, 67 events in the data correspond 
to ?achievement? verbs, whose main characteris-
tic is that they can have a near-instantaneous du-
ration (though of course they can be iterated or 
extended to have other durations). We obtained a 
list of achievement verbs from the LCS lexicon 
of (Dorr and Olsen 1997)3. Achievements can be 
marked as having durations of PTXS, i.e., an 
unspecified number of seconds. Such values 
don?t reinforce any of the observed values, in-
stead extending the set of durations to include 
much smaller durations. As a result, these hidden 
values are not shown in our data 
6 Estimating Duration Probabilities 
Given a distribution of durations for events 
observed in corpora, one of the challenges is to 
arrive at an appropriate value for a given event 
(or class of events). Based on data such as Table 
2, we could estimate the probability P(lose, P3M) 
? 0.346, while P(lose, P1D) ? 0.038, which is 
nearly ten times less likely. Table 2 reveals peak-
ing at 3 months, 6 months, and 9 months, with 
uniform probabilities for all others. Further, we 
can estimate the probability that losses will be 
during periods of 2 months, 3 months, or 9 
months as ? 0.46. Of course, we would prefer a 
much large sample to get more reliable estimates.  
One could also infer a max-min time range, 
but the maximum or minimum may not always 
be likely, as in the case of lose, which has rela-
tively low probability of extending for ?P1D? or 
?P1E?.  Turning to earnings, we find that P(earn, 
P9M) ? 4/16 = 0.25, P(earn, P1Y) ? 0.31, but 
P(earn, P3M) ? 0.43, since most earnings are 
reported for a quarter. 
 
Figure 2. Distribution of durations 
of event to lose 
 
So far, we have considered durations to be 
discrete, falling into a fixed number of categories. 
These categories could be atomic TimeML DU-
                                                 
3See  www.umiacs.umd.edu/ ~bonnie/ LCS_ Data-
base_Documentation.html. 
27
RATION values, as in the examples of durations 
in Table 2, or they could be aggregated in some 
fashion, as in Table 3. In the discrete view, 
unless we have a category of 4 months, the prob-
ability of a loss extending over 4 months is unde-
fined. Viewed this way, the problem is one of 
classification, namely providing the probability 
that an event has a particular duration category.  
The second view takes duration to be continu-
ous, so the duration of an event can have any 
subinterval as a value. The problem here is one 
of regression. We can re-plot the data in Table 2 
as Figure 2, where we have plotted durations in 
days on the x-axis in a natural log scale, and fre-
quency on the y-axis. Since we have plotted the 
durations as a curve, we can interpolate and ex-
trapolate durations, so that we can obtain the 
probability of a loss for 4 months. Of course, we 
would like to fit the best curve possible, and, as 
always, the more data points we have, the better.  
7 Possible Enhancements 
    One of the basic problems with this approach 
is data sparseness, with few examples for each 
event. This makes it difficult to generalize about 
durations. In this section, we discuss enhance-
ments that can address this problem. 
7.1 Converting points to durations 
More durations can be inferred from the OTC 
by coercing TIMEX3 DATE and TIME expres-
sions to DURATIONS; for example, if someone 
announced something in 1997, the maximum 
duration would be one year. Whether this leads 
to reliable heuristics or not remains to be seen.  
7.2 Event class aggregation 
A more useful approach might be to aggregate 
events into classes, as we have done implicitly 
with financial events. Reporting verbs are al-
ready identified as a TimeML subclass, as are 
aspectual verbs such as begin, continue and fin-
ish. Arriving at an appropriate set of classes, 
based on distributional data or resource-derived 
classes (e.g., TimeML, VerbNet, WordNet, etc.) 
remains to be explored. 
7.3 Expanding the corpus sample 
Last but not least, we could expand substan-
tially the search patterns and size of the corpus 
searched against. In particular, we could emulate 
the approach used in VerbOcean (Chklovski and 
Pantel 2004). This resource consists of lexical 
relations mined from Google searches. The min-
ing uses a set of lexical and syntactic patterns to 
test for pairs of verbs strongly associated on the 
Web in a particular semantic relation. For exam-
ple, the system discovers that marriage happens-
before divorce, and that tie happens-before untie. 
Such results are based on estimating the prob-
ability of the joint occurrence of the two verbs 
and the pattern. One can imagine a similar ap-
proach being used for durations. Bootstrapping 
of patterns may also be possible. 
8 Conclusion 
This paper describes the first steps in acquir-
ing metric temporal constraints for events. The 
work is carried out in the context of the TimeML 
framework for marking up events and their tem-
poral relations. We have identified a method for 
enhancing TimeML annotations with metric con-
straints. Although the temporal reasoning re-
quired to carry that out has been described in the 
prior literature, e.g., (Kautz and Ladkin 1991), 
this is a first attempt at lexical acquisition of 
metrical constraints. As a pilot study, it does 
suggest the feasibility of acquisition of metric 
temporal constraints from corpora. In follow-on 
research, we will explore the enhancements de-
scribed in Section 7. 
However, this work is limited by the lack of 
evaluation, in terms of assessing how valid the 
durations inferred by our method are compared 
with human annotations. In ongoing work, Jerry 
Hobbs and his colleagues (Pan et al 2006) have 
developed an annotation scheme for humans to 
mark up event durations in documents. Once 
such enhancements are carried out, it will cer-
tainly be fruitful to compare the duration prob-
abilities obtained with the ranges of durations 
provided in that corpus.  
In future, we will explore both regression and 
classification models for duration learning. In the 
latter case, we will investigate the use of con-
structive induction e.g., (Bloedorn and Michalski 
1998). In particular, we will avail of operators to 
implement attribute abstraction that will cluster 
durations into coarse-grained classes, based on 
distributions of atomic durations observed in the 
data. We will also investigate the extent to which 
learned durations can be used to constrain 
TLINK ordering. 
References 
James Allen. 1984. Towards a General Theory of Ac-
tion and Time.  Artificial Intelligence, 23, 2, 123-
154. 
28
Eric Bloedorn  and Ryszard S. Michalski. 1998. Data-
Driven Constructive Induction. IEEE Intelligent 
Systems, 13, 2.  
Branimir Boguraev and Rie Kubota Ando. 2005. 
TimeML-Compliant Text Analysis for Temporal 
Reasoning. Proceedings of IJCAI-05, 997-1003. 
Timothy Chklovski and Patrick Pantel. 
2004.VerbOcean: Mining the Web for Fine-
Grained Semantic Verb Relations. Proceedings of 
EMNLP-04. http://semantics.isi.edu/ocean 
B. Dorr and M. B. Olsen. Deriving Verbal and Com-
positional Lexical Aspect for NLP Applications. 
ACL'1997, 151-158.   
Janet Hitzeman, Marc Moens and Clare Grover. 1995. 
Algorithms for Analyzing the Temporal Structure 
of Discourse. Proceedings of  EACL?95, Dublin, 
Ireland, 253-260. 
Feng Pan, Rutu Mulkar, and Jerry Hobbs.  Learning 
Event Durations from Event Descriptions. Proceed-
ings of Workshop on Annotation and Reasoning 
about Time and Events (ARTE?2006),  ACL?2006.  
C.H. Hwang and L. K. Schubert. 1992. Tense Trees as 
the fine structure of discourse. Proceedings of 
ACL?1992, 232-240. 
Hans Kamp and Uwe Ryle. 1993. From Discourse to 
Logic (Part 2). Dordrecht: Kluwer. 
Andrew Kehler. 2000. Resolving Temporal Relations 
using Tense Meaning and Discourse Interpretation, 
in M. Faller, S. Kaufmann, and M. Pauly, (eds.), 
Formalizing the Dynamics of Information, CSLI 
Publications, Stanford. 
Henry A. Kautz and Peter B. Ladkin. 1991. Integrat-
ing Metric and Qualitative Temporal Reasoning. 
AAAI'91. 
Mirella Lapata and Alex Lascarides. 2004. Inferring 
Sentence-internal Temporal Relations. In Proceed-
ings of the North American Chapter of the Assoca-
tion of Computational Linguistics, 153-160.  
Alex Lascarides and Nicholas Asher. 1993. Temporal 
Relations, Discourse Structure, and Commonsense 
Entailment. Linguistics and Philosophy 16, 437-
494. 
Wenjie Li, Kam-Fai Wong, Guihong Cao and Chunfa 
Yuan. 2004. Applying Machine Learning to Chi-
nese Temporal Relation Resolution. Proceedings of 
ACL?2004, 582-588. 
Inderjeet Mani and George Wilson. 2000. Robust 
Temporal Processing of News.  Proceedings of 
ACL?2000. 
Inderjeet Mani, Barry Schiffman, and Jianping Zhang.  
2003.  Inferring Temporal Ordering of Events in 
News. Short Paper. Proceedings of HLT-
NAACL'03, 55-57.  
Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong 
Min Lee, and James Pustejovsky.  2006.  Machine 
Learning of Temporal Relations. Proceedings of 
ACL?2006.  
Rebecca J. Passonneau. A Computational Model of 
the Semantics of Tense and Aspect. Computational 
Linguistics, 14, 2, 1988, 44-60. 
James Pustejovsky, Patrick Hanks, Roser Sauri, An-
drew See, David Day, Lisa Ferro, Robert Gai-
zauskas, Marcia Lazo, Andrea Setzer, and Beth 
Sundheim. 2003. The TimeBank Corpus. Corpus 
Linguistics, 647-656. 
James Pustejovsky, Bob Ingria, Roser Sauri, Jose 
Castano, Jessica Littman, Rob Gaizauskas, Andrea 
Setzer, G. Katz,  and I. Mani. 2005. The Specifica-
tion Language TimeML. In I. Mani, J. Pustejovsky, 
and R. Gaizauskas, (eds.), The Language of Time: 
A Reader. Oxford University Press.  
Roser Saur?, Robert Knippen, Marc Verhagen and 
James Pustejovsky. 2005. Evita: A Robust Event 
Recognizer for QA Systems. Short Paper. Proceed-
ings of HLT/EMNLP 2005: 700-707. 
Frank Schilder and Christof Habel. 2005. From tem-
poral expressions to temporal information: seman-
tic tagging of news messages. In I. Mani, J. Puste-
jovsky, and R. Gaizauskas, (eds.), The Language of 
Time: A Reader. Oxford University Press.  
Andrea Setzer and Robert Gaizauskas. 2000. Annotat-
ing Events and Temporal Information in Newswire 
Texts. Proceedings of LREC-2000, 1287-1294.  
Marc Verhagen. 2004. Times Between The Lines. 
Ph.D. Dissertation, Department of Computer Sci-
ence, Brandeis University. 
Marc Verhagen, Inderjeet Mani, Roser Saur?, Robert 
Knippen, Jess Littman and James Pustejovsky. 
2005. Automating Temporal Annotation with 
TARSQI. Demo Session, ACL 2005. 
Marc Vilain, Henry Kautz, and Peter Van Beek. 1989. 
Constraint propagation algorithms for temporal 
reasoning: A revised report. In D. S. Weld and J. 
de Kleer (eds.), Readings in Qualitative Reasoning 
about Physical Systems, Morgan-Kaufman, 373-
381. 
Bonnie Webber. 1988. Tense as Discourse Anaphor. 
Computational Linguistics, 14, 2, 1988, 61-73. 
29
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 753?760,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Machine Learning of Temporal Relations 
Inderjeet Mani??, Marc Verhagen?, Ben Wellner?? 
Chong Min Lee? and James Pustejovsky? 
?The MITRE Corporation 
202 Burlington Road, Bedford, MA 01730, USA 
?Department of Linguistics, Georgetown University 
37th and O Streets, Washington, DC 20036, USA 
?Department of Computer Science, Brandeis University 
415 South St., Waltham, MA 02254, USA 
{imani, wellner}@mitre.org, {marc, jamesp}@cs.brandeis.edu, cml54@georgetown.edu 
Abstract 
This paper investigates a machine learn-
ing approach for temporally ordering and 
anchoring events in natural language 
texts. To address data sparseness, we 
used temporal reasoning as an over-
sampling method to dramatically expand 
the amount of training data, resulting in 
predictive accuracy on link labeling as 
high as 93% using a Maximum Entropy 
classifier on human annotated data. This 
method compared favorably against a se-
ries of increasingly sophisticated base-
lines involving expansion of rules de-
rived from human intuitions. 
1 Introduction 
The growing interest in practical NLP applica-
tions such as question-answering and text sum-
marization places increasing demands on the 
processing of temporal information. In multi-
document summarization of news articles, it can 
be useful to know the relative order of events so 
as to merge and present information from multi-
ple news sources correctly. In question-
answering, one would like to be able to ask when 
an event occurs, or what events occurred prior to 
a particular event.  
A wealth of prior research by (Passoneau 
1988), (Webber 1988), (Hwang and Schubert 
1992), (Kamp and Reyle 1993), (Lascarides and 
Asher 1993), (Hitzeman et al 1995), (Kehler 
2000) and others, has explored the different 
knowledge sources used in inferring the temporal 
ordering of events, including temporal adver-
bials, tense, aspect, rhetorical relations, prag-
matic conventions, and background knowledge. 
For example, the narrative convention of events 
being described in the order in which they occur 
is followed in (1), but overridden by means of a 
discourse relation, Explanation in (2).  
(1) Max stood up. John greeted him.  
(2) Max fell. John pushed him.  
In addition to discourse relations, which often 
require inferences based on world knowledge, 
the ordering decisions humans carry out appear 
to involve a variety of knowledge sources, in-
cluding tense and grammatical aspect (3a), lexi-
cal aspect (3b), and temporal adverbials (3c): 
(3a) Max entered the room. He had drunk a lot 
of wine.  
(3b) Max entered the room. Mary was seated 
behind the desk.  
(3c) The company announced Tuesday that 
third-quarter sales had fallen.  
Clearly, substantial linguistic processing may 
be required for a system to make these infer-
ences, and world knowledge is hard to make 
available to a domain-independent program. An 
important strategy in this area is of course the 
development of annotated corpora than can fa-
cilitate the machine learning of such ordering 
inferences. 
This paper 1  investigates a machine learning 
approach for temporally ordering events in natu-
ral language texts. In Section 2, we describe the 
annotation scheme and annotated corpora, and 
the challenges posed by them. A basic learning 
approach is described in Section 3. To address 
data sparseness, we used temporal reasoning as 
an over-sampling method to dramatically expand 
the amount of training data.  
As we will discuss in Section 5, there are no 
standard algorithms for making these inferences 
that we can compare against. We believe 
strongly that in such situations, it?s worthwhile 
for computational linguists to devote consider-
                                                 
1Research at Georgetown and Brandeis on this prob-
lem was funded in part by a grant from the ARDA 
AQUAINT Program, Phase II.  
753
able effort to developing insightful baselines. 
Our work is, accordingly, evaluated in compari-
son against four baselines: (i) the usual majority 
class statistical baseline, shown along with each 
result, (ii) a more sophisticated baseline that uses 
hand-coded rules (Section 4.1), (iii) a hybrid 
baseline based on hand-coded rules expanded 
with Google-induced rules (Section 4.2), and (iv) 
a machine learning version that learns from im-
perfect annotation produced by (ii) (Section 4.3).  
2 Annotation Scheme and Corpora 
2.1 TimeML 
TimeML (Pustejovsky et al 2005) 
(www.timeml.org) is an annotation scheme for 
markup of events, times, and their temporal rela-
tions in news articles. The TimeML scheme flags 
tensed verbs, adjectives, and nominals with 
EVENT tags with various attributes, including 
the class of event, tense, grammatical aspect, po-
larity (negative or positive), any modal operators 
which govern the event being tagged, and cardi-
nality of the event if it?s mentioned more than 
once. Likewise, time expressions are flagged and 
their values normalized, based on TIMEX3, an 
extension of the ACE (2004) (tern.mitre.org) 
TIMEX2 annotation scheme.  
For temporal relations, TimeML defines a 
TLINK tag that links tagged events to other 
events and/or times. For example, given (3a), a 
TLINK tag orders an instance of the event of 
entering to an instance of the drinking with the 
relation type AFTER. Likewise, given the sen-
tence (3c), a TLINK tag will anchor the event 
instance of announcing to the time expression 
Tuesday (whose normalized value will be in-
ferred from context), with the relation 
IS_INCLUDED. These inferences are shown (in 
slightly abbreviated form) in the annotations in 
(4) and (5). 
(4) Max <EVENT eventID=?e1? 
class=?occurrence? tense=?past? as-
pect=?none?>entered</EVENT> the room. 
He <EVENT eventID=?e2? 
class=?occurrence? tense=?past? as-
pect=?perfect?>had drunk</EVENT>a 
lot of wine.  
<TLINK eventID=?e1? relatedToEven-
tID=?e2? relType=?AFTER?/> 
 (5) The company <EVENT even-
tID=?e1? class=?reporting? 
tense=?past? as-
pect=?none?>announced</EVENT> 
<TIMEX3 tid=?t2? type=?DATE? tempo-
ralFunction=?false? value=?1998-01-
08?>Tuesday </TIMEX3> that third-
quarter sales <EVENT eventID=?e2? 
class=?occurrence? tense=?past? as-
pect=?perfect?> had fallen</EVENT>.  
<TLINK eventID=?e1? relatedToEven-
tID=?e2? relType=?AFTER?/> 
<TLINK eventID=?e1? relatedTo-
TimeID=?t2? relType=?IS_INCLUDED?/> 
 
The anchor relation is an Event-Time TLINK, 
and the order relation is an Event-Event TLINK. 
TimeML uses 14 temporal relations in the 
TLINK RelTypes, which reduce to a disjunctive 
classification of 6 temporal relations RelTypes = 
{SIMULTANEOUS, IBEFORE, BEFORE, BE-
GINS, ENDS, INCLUDES}. An event or time is 
SIMULTANEOUS with another event or time if 
they occupy the same time interval. An event or 
time INCLUDES another event or time if the 
latter occupies a proper subinterval of the former. 
These 6 relations and their inverses map one-to-
one to 12 of Allen?s 13 basic relations (Allen 
1984)2. There has been a considerable amount of 
activity related to this scheme; we focus here on 
some of the challenges posed by the TLINK an-
notation, the part that is directly relevant to the 
temporal ordering and anchoring problems. 
2.2 Challenges 
The annotation of TimeML information is on a 
par with other challenging semantic annotation 
schemes, like PropBank, RST annotation, etc., 
where high inter-annotator reliability is crucial 
but not always achievable without massive pre-
processing to reduce the user?s workload. In Ti-
meML, inter-annotator agreement for time ex-
pressions and events is 0.83 and 0.78 (average of 
Precision and Recall) respectively, but on 
TLINKs it is 0.55 (P&R average), due to the 
large number of event pairs that can be selected 
for comparison. The time complexity of the hu-
man TLINK annotation task is quadratic in the 
number of events and times in the document. 
Two corpora have been released based on Ti-
meML: the TimeBank (Pustejovsky et al 2003) 
(we use version 1.2.a) with 186 documents and 
                                                 
2Of the 14 TLINK relations, the 6 inverse relations are re-
dundant. In order to have a disjunctive classification, SI-
MULTANEOUS and IDENTITY are collapsed, since 
IDENTITY is a subtype of SIMULTANEOUS. (Specifi-
cally, X and Y are identical if they are simultaneous and 
coreferential.) DURING and IS_INCLUDED are collapsed 
since DURING is a subtype of IS_INCLUDED that anchors 
events to times that are durations. IBEFORE (immediately 
before) corresponds to Allen?s MEETS. Allen?s OVER-
LAPS relation is not represented in TimeML. More details 
can be found at timeml.org. 
754
64,077 words of text, and the Opinion Corpus 
(www.timeml.org), with 73 documents and 
38,709 words. The TimeBank was developed in 
the early stages of TimeML development, and 
was partitioned across five annotators with dif-
ferent levels of expertise. The Opinion Corpus 
was developed very recently, and was partitioned 
across just two highly trained annotators, and 
could therefore be expected to be less noisy. In 
our experiments, we merged the two datasets to 
produce a single corpus, called OTC. 
Table 1 shows the distribution of EVENTs and 
TIMES, and TLINK RelTypes3 in the OTC. The 
majority class percentages are shown in paren-
theses. It can be seen that BEFORE and SI-
MULTANEOUS together form a majority of 
event-ordering (Event-Event) links, whereas 
most of the event anchoring (Event-Time) links 
are INCLUDES.  
 
12750 Events, 2114 Times 
Relation Event-Event Event-Time 
IBEFORE 131 15 
BEGINS 160 112 
ENDS 208 159 
SIMULTANEOUS 1528 77 
INCLUDES 950 3001 (65.3%) 
BEFORE 3170 (51.6%) 1229 
TOTAL 6147 4593 
Table 1. TLINK Class Distributions in OTC 
Corpus 
 
The lack of TLINK coverage in human anno-
tation could be helped by preprocessing, pro-
vided it meets some threshold of accuracy. Given 
the availability of a corpus like OTC, it is natural 
to try a machine learning approach to see if it can 
be used to provide that preprocessing. However, 
the noise in the corpus and the sparseness of 
links present challenges to a learning approach. 
3 Machine Learning Approach 
3.1 Initial Learner 
There are several sub-problems related to in-
ferring event anchoring and event ordering. Once 
a tagger has tagged the events and times, the first 
task (A) is to link events and/or times, and the 
second task (B) is to label the links. Task A is 
hard to evaluate since, in the absence of massive 
preprocessing, many links are ignored by the 
human in creating the annotated corpora. In addi-
                                                 
3The number of TLINKs shown is based on the number of 
TLINK vectors extracted from the OTC. 
tion, a program, as a baseline, can trivially link 
all tagged events and times, getting 100% recall 
on Task A. We focus here on Task B, the label-
ing task. In the case of humans, in fact, when a 
TLINK is posited by both annotators between the 
same pairs of events or times, the inter-annotator 
agreement on the labels is a .77 average of P&R. 
To ensure replicability of results, we assume per-
fect (i.e., OTC-supplied) events, times, and links.  
Thus, we can consider TLINK inference as the 
following classification problem: given an or-
dered pair of elements X and Y, where X and Y 
are events or times which the human has related 
temporally via a TLINK, the classifier has to as-
sign a label in RelTypes. Using RelTypes instead 
of RelTypes ?  {NONE} also avoids the prob-
lem of heavily skewing the data towards the 
NONE class.  
To construct feature vectors for machine 
learning, we took each TLINK in the corpus and 
used the given TimeML features, with the 
TLINK class being the vector?s class feature.  
For replicability by other users of these corpora, 
and to be able to isolate the effect of components, 
we used ?perfect? features; no feature engineer-
ing was attempted. The features were, for each 
event in an event-ordering pair, the event-class, 
aspect, modality, tense and negation (all nominal 
features); event string, and signal (a preposi-
tion/adverb, e.g., reported on Tuesday), which 
are string features, and contextual features indi-
cating whether the same tense and same aspect 
are true of both elements in the event pair. For 
event-time links, we used the above event and 
signal features along with TIMEX3 time features. 
For learning, we used an off-the-shelf Maxi-
mum Entropy (ME) classifier (from Carafe, 
available at sourceforge.net/projects/carafe). As 
shown in the UNCLOSED (ME) column in Ta-
ble 24, accuracy of the unclosed ME classifier 
does not go above 77%, though it?s always better 
than the majority class (in parentheses). We also 
tried a variety of other classifiers, including the 
SMO support-vector machine and the na?ve 
Bayes tools in WEKA (www.weka.net.nz). SMO 
performance (but not na?ve Bayes) was compa-
rable with ME, with SMO trailing it in a few 
cases (to save space, we report just ME perform-
ance). It?s possible that feature engineering could 
improve performance, but since this is ?perfect? 
data, the result is not encouraging.  
                                                 
4All machine learning results, except for ME-C in Table 4, 
use 10-fold cross-validation. ?Accuracy? in tables is Predic-
tive Accuracy. 
755
 
 
 UNCLOSED (ME) CLOSED (ME-C) 
 Event-Event Event-Time Event-Event Event-Time 
Accuracy: 62.5 (51.6) 76.13 (65.3) 93.1 (75.2) 88.25 (62.3) 
Relation Prec Rec F Prec Rec F Prec Rec F Prec Rec F 
IBEFORE 50.00 27.27 35.39 0 0 0 77.78 60.86 68.29 0 0 0 
BEGINS 50.00 41.18 45.16 60.00 50.00 54.54 85.25 82.54 83.87 76.47 74.28 75.36 
ENDS 94.74 66.67 78.26 41.67 27.78 33.33 87.83 94.20 90.90 79.31 77.97 78.62 
SIMULTANEOUS 50.35 50.00 50.17 33.33 20.00 25.00 62.50 38.60 47.72 73.68 56.00 63.63 
INCLUDES 47.88 34.34 40.00 80.92 62.72 84.29 90.41 88.23 89.30 86.07 80.78 83.34 
BEFORE 68.85 79.24 73.68 70.47 62.72 66.37 94.95 97.26 96.09 90.16 93.56 91.83 
 
Table 2. Machine learning results using unclosed and closed data
 
3.2 Expanding Training Data using Tem-
poral Reasoning 
To expand our training set, we use a temporal  
closure component SputLink (Verhagen 2004), 
that takes known temporal relations in a text and  
derives new implied relations from them, in ef-
fect making explicit what was implicit. SputLink 
was inspired by (Setzer and Gaizauskas 2000) 
and is based on Allen?s interval algebra, taking 
into account the limitations on that algebra that 
were pointed out by (Vilain et al 1990). It is ba-
sically a constraint propagation algorithm that 
uses a transitivity table to model the composi-
tional behavior of all pairs of relations in a 
document. SputLink?s transitivity table is repre-
sented by 745 axioms. An example axiom:  
 
If relation(A, B) = BEFORE && 
   relation(B, C) = INCLUDES 
then infer relation(A, C) = BEFORE 
 
Once the TLINKs in each document in the 
corpus are closed using SputLink, the same vec-
tor generation procedure and feature representa-
tion described in Section 3.1 are used. The effect 
of closing the TLINKs on the corpus has a dra-
matic impact on learning. Table 2, in the 
CLOSED (ME-C) column shows that accura-
cies for this method (called ME-C, for Maximum 
Entropy learning with closure) are now in the 
high 80?s and low 90?s, and still outperform the 
closed majority class (shown in parentheses).  
What is the reason for the improvement?5 One 
reason is the dramatic increase in the amount of 
training data. The more connected the initial un-
                                                 
5Interestingly, performance does not improve for SIMUL-
TANEOUS.  The reason for this might be due to the rela-
tively modest increase in SIMULTANEOUS relations from 
applying closure (roughly factor of 2). 
closed graph for a document is in TLINKs, the 
greater the impact in terms of closure. When the 
OTC is closed, the number of TLINKs goes up 
by more than 11 times, from 6147 Event-Event 
and 4593 Event-Time TLINKs to 91,157 Event-
Event and 29,963 Event-Time TLINKs. The 
number of BEFORE links goes up from 3170 
(51.6%) Event-Event and 1229 Event-Time 
TLINKs (26.75%) to 68585 (75.2%) Event-
Event and 18665 (62.3%) Event-Time TLINKs, 
making BEFORE the majority class in the closed 
data for both Event-Event and Event-Time 
TLINKs. There are only an average of 0.84 
TLINKs per event before closure, but after clo-
sure it shoots up to 9.49 TLINKs per event. 
(Note that as a result, the majority class percent-
ages for the closed data have changed from the 
unclosed data.) 
Being able to bootstrap more training data is 
of course very useful. However, we need to dig 
deeper to investigate how the increase in data 
affected the machine learning. The improvement 
provided by temporal closure can be explained 
by three factors:  (1) closure effectively creates a 
new classification problem with many more in-
stances, providing more data to train on; (2) the 
class distribution is further skewed which results 
in a higher majority class baseline (3) closure 
produces additional data in such a way as to in-
crease the frequencies and statistical power of 
existing features in the unclosed data, as opposed 
to adding new features.  For example, with un-
closed data, given A BEFORE B and B BE-
FORE C, closure generates A BEFORE C which 
provides more significance for the features re-
lated to A and C appearing as first and second 
arguments, respectively, in a BEFORE relation.  
In order to help determine the effects of the 
above factors, we carried out two experiments in 
which we sampled 6145 vectors from the closed 
756
data ? i.e. approximately the number of Event-
Event vectors in the unclosed data.  This effec-
tively removed the contribution of factor (1) 
above. The first experiment (Closed Class Dis-
tribution) simply sampled 6145 instances uni-
formly from the closed instances, while the sec-
ond experiment (Unclosed Class Distribution) 
sampled instances according to the same distri-
bution as the unclosed data. Table 3 shows these 
results.  The greater class distribution skew in the 
closed data clearly contributes to improved accu-
racy. However, when using the same class distri-
bution as the unclosed data (removing factor (2) 
from above), the accuracy, 76%, is higher than 
using the full unclosed data.  This indicates that 
closure does indeed help according to factor (3). 
4 Comparison against Baselines 
4.1 Hand-Coded Rules 
Humans have strong intuitions about rules for 
temporal ordering, as we indicated in discussing 
sentences (1) to (3). Such intuitions led to the 
development of pattern matching rules incorpo-
rated in a TLINK tagger called GTag. GTag 
takes a document with TimeML tags, along with 
syntactic information from part-of-speech tag-
ging and chunking from Carafe, and then uses 
187 syntactic and lexical rules to infer and label 
TLINKs between tagged events and other tagged 
events or times. The tagger takes pairs of 
TLINKable items (event and/or time) and 
searches for the single most-confident rule to 
apply to it, if any, to produce a labeled TLINK 
between those items. Each (if-then) rule has a 
left-hand side which consists of a conjunction of 
tests based on TimeML-related feature combina-
tions (TimeML features along with part-of-
speech and chunk-related features), and a right-
hand side which is an assignment to one of the 
TimeML TLINK classes.  
The rule patterns are grouped into several dif-
ferent classes: (i) the event is anchored with or 
without a signal to a time expression within the 
same clause, e.g., (3c), (ii) the event is anchored 
without a signal to the document date (as is often 
the case for reporting verbs in news), (iii) an 
event is linked to another event in the same sen-
tence, e.g., (3c), and (iv) the event in a main 
clause of one sentence is anchored with a signal 
or tense/aspect cue to an event in the main clause 
of the previous sentence, e.g., (1-2), (3a-b). 
The performance of this baseline is shown in 
Table 4 (line GTag). The top most accurate rule 
(87% accuracy) was GTag Rule 6.6, which links 
a past-tense event verb joined by a conjunction to 
another past-tense event verb as being BEFORE 
the latter (e.g., they traveled and slept the 
night ..): 
 
If sameSentence=YES && 
 sentenceType=ANY && 
 conjBetweenEvents=YES && 
 arg1.class=EVENT && 
 arg2.class=EVENT && 
 arg1.tense=PAST && 
 arg2.tense=PAST && 
 arg1.aspect=NONE && 
 arg2.aspect=NONE && 
 arg1.pos=VB && 
 arg2.pos=VB && 
 arg1.firstVbEvent=ANY && 
 arg2.firstVbEvent=ANY  
then infer relation=BEFORE 
 
The vast majority of the intuition-bred rules 
have very low accuracy compared to ME-C, with 
intuitions failing for various feature combina-
tions and relations (for relations, for example, 
GTag lacks rules for IBEFORE, STARTS, and 
ENDS). The bottom-line here is that even when 
heuristic preferences are intuited, those prefer-
ences need to be guided by empirical data, 
whereas hand-coded rules are relatively ignorant 
of the distributions that are found in data. 
4.2 Adding Google-Induced Lexical Rules 
One might argue that the above baseline is too 
weak, since it doesn?t allow for a rich set of lexi-
cal relations. For example, pushing can result in 
falling, killing always results in death, and so 
forth. These kinds of defeasible rules have been 
investigated in the semantics literature, including 
the work of Lascarides and Asher cited in Sec-
tion 1.  
However, rather than hand-creating lexical 
rules and running into the same limitations as 
with GTag?s rules, we used an empirically-
derived resource called VerbOcean (Chklovski 
and Pantel 2004), available at 
http://semantics.isi.edu/ocean. This resource con-
sists of lexical relations mined from Google 
searches. The mining uses a set of lexical and 
syntactic patterns to test for pairs of verb 
strongly associated on the Web in an asymmetric 
?happens-before? relation. For example, the sys-
tem discovers that marriage happens-before di-
vorce, and that tie happens-before untie.  
We automatically extracted all the ?happens-
before? relations from the VerbOcean resource at 
the above web site, and then automatically con-
verted those relations to GTag format, producing 
4,199 rules. Here is one such converted rule: 
757
 
If arg1.class=EVENT && 
   arg2.class=EVENT && 
   arg1.word=learn && 
   arg2.word=forget && 
then infer relation=BEFORE 
 
Adding these lexical rules to GTag (with mor-
phological normalization being added for rule 
matching on word features) amounts to a consid-
erable augmentation of the rule-set, by a factor of 
22. GTag with this augmented rule-set might be 
a useful baseline to consider, since one would 
expect the gigantic size of the Google ?corpus? to 
yield fairly robust, broad-coverage rules.  
What if both a core GTag rule and a VerbO-
cean-derived rule could both apply? We assume 
the one with the higher confidence is chosen. 
However, we don?t have enough data to reliably 
estimate rule confidences for the original GTag 
rules; so, for the purposes of VerbOcean rule 
integration, we assigned either the original Ver-
bOcean rules as having greater confidence than 
the original GTag rules in case of a conflict (i.e., 
a preference for the more specific rule), or vice-
versa.  
 The results are shown in Table 4 (lines 
GTag+VerbOcean). The combined rule set, un-
der both voting schemes, had no statistically sig-
nificant difference in accuracy from the original 
GTag rule set. So, ME-C beat this baseline as 
well.  
The reason VerbOcean didn?t help is again 
one of data sparseness, due to most verbs occur-
ring rarely in the OTC. There were only 19 occa-
sions when a happens-before pair from VerbO-
cean correctly matched a human BEFORE 
TLINK, of which 6 involved the same rule being 
right twice (including learn happens-before for-
get, a rule which students are especially familiar 
with!), with the rest being right just once. There 
were only 5 occasions when a VerbOcean rule 
incorrectly matched a human BEFORE TLINK, 
involving just three rules. 
 
 
 Closed Class Distribution UnClosed Class Distribution 
Relation Prec Rec F Accuracy Prec Rec F Accuracy 
IBEFORE 100.0 100.0 100.0 83.33 58.82 68.96 
BEGINS 0 0 0 72.72 50.0 59.25 
ENDS 66.66 57.14 61.53 62.50 50.0 55.55 
SIMULTANEOUS 14.28 6.66 9.09 60.54 66.41 63.34 
INCLUDES 73.91 77.98 75.89 75.75 77.31 76.53 
BEFORE 90.68 92.60 91.63 
87.20  
(72.03) 
84.09 84.61 84.35 
76.0 
(40.95)  
Table 3. Machine Learning from subsamples of the closed data 
 
Accuracy Baseline 
Event-Event Event-Time 
GTag 63.43 72.46 
GTag+VerbOcean - GTag overriding VerbOcean 64.80 74.02 
GTag+VerbOcean - VerbOcean overriding GTag 64.22 73.37 
GTag+closure+ME-C 53.84 (57.00) 67.37 (67.59) 
Table 4. Accuracy of ?Intuition? Derived Baselines 
 
4.3 Learning from Hand-Coded Rules 
Baseline 
The previous baseline was a hybrid confi-
dence-based combination of corpus-induced 
lexical relations with hand-created rules for tem-
poral ordering. One could consider another obvi-
ous hybrid, namely learning from annotations 
created by GTag-annotated corpora. Since the 
intuitive baseline fares badly, this may not be 
that attractive. However, the dramatic impact of 
closure could help offset the limited coverage 
provided by human intuitions.   
Table 4 (line GTag+closure+ME-C) shows the 
results of closing the TLINKs produced by 
GTag?s annotation and then training ME from 
the resulting data. The results here are evaluated 
against a held-out test set. We can see that even 
after closure, the baseline of learning from un-
closed human annotations is much poorer than 
ME-C, and is in fact substantially worse than the  
majority class on event ordering.  
This means that for preprocessing new data 
sets to produce noisily annotated data for this 
classification task, it is far better to use machine-
learning from closed human annotations rather 
758
than machine-learning from closed annotations 
produced by an intuitive baseline. 
5 Related Work 
Our approach of classifying pairs independ-
ently during learning does not take into account 
dependencies between pairs.  For example, a 
classifier may label <X, Y> as BEFORE. Given 
the pair <X, Z>,  such a classifier has no idea if 
<Y, Z> has been classified as BEFORE, in 
which case, through closure, <X, Z> should be 
classified as BEFORE. This can result in the 
classifier producing an inconsistently annotated 
text. The machine learning approach of (Cohen 
et al 1999) addresses this, but their approach is 
limited to total orderings involving BEFORE, 
whereas TLINKs introduce partial orderings in-
volving BEFORE and five other relations. Future 
research will investigate methods for tighter in-
tegration of temporal reasoning and statistical 
classification. 
The only closely comparable machine-
learning approach to the problem of TLINK ex-
traction was that of (Boguraev and Ando 2005), 
who trained a classifier on Timebank 1.1 for 
event anchoring for events and times within the 
same sentence, obtaining an F-measure (for tasks 
A and B together) of 53.1. Other work in ma-
chine-learning and hand-coded approaches, 
while interesting, is harder to compare in terms 
of accuracy since they do not use common task 
definitions, annotation standards, and evaluation 
measures. (Li et al 2004) obtained 78-88% accu-
racy on ordering within-sentence temporal rela-
tions in Chinese texts. (Mani et al 2003) ob-
tained 80.2 F-measure training a decision tree on 
2069 clauses in anchoring events to reference 
times that were inferred for each clause. (Ber-
glund et al 2006) use a document-level evalua-
tion approach pioneered by (Setzer and Gai-
zauskas 2000), which uses a distinct evaluation 
metric. Finally, (Lapata and Lascarides 2004) use 
found data to successfully learn which (possibly 
ambiguous) temporal markers connect a main 
and subordinate clause, without inferring under-
lying temporal relations. 
In terms of hand-coded approaches, (Mani and 
Wilson 2000) used a baseline method of blindly 
propagating TempEx time values to events based 
on proximity, obtaining 59.4% on a small sample 
of 8,505 words of text. (Filatova and Hovy 2001) 
obtained 82% accuracy on ?timestamping? 
clauses for a single type of event/topic on a data 
set of 172 clauses. (Schilder and Habel 2001) 
report 84% accuracy inferring temporal relations 
in German data, and (Li et al 2001) report 93% 
accuracy on extracting temporal relations in Chi-
nese. Because these accuracies are on different 
data sets and metrics, they cannot be compared 
directly with our methods. 
Recently, researchers have developed other 
tools for automatically tagging aspects of Ti-
meML, including EVENT (Sauri et al 2005) at 
0.80 F-measure and TIMEX36 tags at 0.82-0.85 
F-measure. In addition, the TERN competition 
(tern.mitre.org) has shown very high (close to .95  
F-measures) for TIMEX2 tagging, which is fairly 
similar to TIMEX3. These results suggest the 
time is ripe for exploiting ?imperfect? features in 
our machine learning approach. 
6 Conclusion 
Our research has uncovered one new finding: 
semantic reasoning (in this case, logical axioms 
for temporal closure), can be extremely valuable 
in addressing data sparseness. Without it, per-
formance on this task of learning temporal rela-
tions is poor; with it, it is excellent. We showed 
that temporal reasoning can be used as an over-
sampling method to dramatically expand the 
amount of training data for TLINK labeling, re-
sulting in labeling predictive accuracy as high as 
93% using an off-the-shelf Maximum Entropy 
classifier. Future research will investigate this 
effect further, as well as examine factors that 
enhance or mitigate this effect in different cor-
pora. 
The paper showed that ME-C performed sig-
nificantly better than a series of increasingly so-
phisticated baselines involving expansion of 
rules derived from human intuitions. Our results 
in these comparisons confirm the lessons learned 
from the corpus-based revolution, namely that 
rules based on intuition alone are prone to in-
completeness and are hard to tune without access 
to the distributions found in empirical data.  
Clearly, lexical rules have a role to play in se-
mantic and pragmatic reasoning from language, 
as in the discussion of example (2) in Section 1. 
Such rules, when mined by robust, large corpus-
based methods, as in the Google-derived VerbO-
cean, are clearly relevant, but too specific to ap-
ply more than a few times in the OTC corpus.  
It may be possible to acquire confidence 
weights for at least some of the intuitive rules in 
GTag from Google searches, so that we have a 
                                                 
6http://complingone.georgetown.edu/~linguist/GU_TIME_
DOWNLOAD.HTML 
759
level field for integrating confidence weights 
from the fairly general GTag rules and the fairly 
specific VerbOcean-like lexical rules. Further, 
the GTag and VerbOcean rules could be incorpo-
rated as features for machine learning, along with 
features from automatic preprocessing.  
We have taken pains to use freely download-
able resources like Carafe, VerbOcean, and 
WEKA to help others easily replicate and 
quickly ramp up a system. To further facilitate 
further research, our tools as well as labeled vec-
tors (unclosed as well as closed) are available for 
others to experiment with. 
References 
James Allen. 1984. Towards a General Theory of Ac-
tion and Time.  Artificial Intelligence, 23, 2, 123-
154. 
Anders Berglund, Richard Johansson and Pierre 
Nugues. 2006. A Machine Learning Approach to 
Extract Temporal Information from Texts in Swed-
ish and Generate Animated 3D Scenes.  Proceed-
ings of EACL-2006. 
Branimir Boguraev and Rie Kubota Ando. 2005. Ti-
meML-Compliant Text Analysis for Temporal 
Reasoning. Proceedings of IJCAI-05, 997-1003. 
Timothy Chklovski and Patrick Pantel. 
2004.VerbOcean: Mining the Web for Fine-
Grained Semantic Verb Relations. Proceedings of 
EMNLP-04. http://semantics.isi.edu/ocean 
W. Cohen, R. Schapire, and Y. Singer. 1999. Learn-
ing to order things. Journal of Artificial Intelli-
gence Research, 10:243?270, 1999. 
Janet Hitzeman, Marc Moens and Clare Grover. 1995. 
Algorithms for Analyzing the Temporal Structure 
of Discourse. Proceedings of  EACL?95, Dublin, 
Ireland, 253-260. 
C.H. Hwang and L. K. Schubert. 1992. Tense Trees as 
the fine structure of discourse. Proceedings of 
ACL?1992, 232-240. 
Hans Kamp and Uwe Ryle. 1993. From Discourse to 
Logic (Part 2). Dordrecht: Kluwer. 
Andrew Kehler. 2000. Resolving Temporal Relations 
using Tense Meaning and Discourse Interpretation, 
in M. Faller, S. Kaufmann, and M. Pauly, (eds.), 
Formalizing the Dynamics of Information, CSLI 
Publications, Stanford. 
Mirella Lapata and Alex Lascarides. 2004. Inferring 
Sentence-internal Temporal Relations. In Proceed-
ings of the North American Chapter of the Assoca-
tion of Computational Linguistics, 153-160.  
Alex Lascarides and Nicholas Asher. 1993. Temporal 
Relations, Discourse Structure, and Commonsense 
Entailment. Linguistics and Philosophy 16, 437-
494. 
Wenjie Li, Kam-Fai Wong, Guihong Cao and Chunfa 
Yuan. 2004. Applying Machine Learning to Chi-
nese Temporal Relation Resolution. Proceedings of 
ACL?2004, 582-588. 
Inderjeet Mani, Barry Schiffman, and Jianping Zhang.  
2003.  Inferring Temporal Ordering of Events in 
News. Short Paper. Proceedings of HLT-
NAACL'03, 55-57.  
Inderjeet Mani and George Wilson. 2000. Robust 
Temporal Processing of News.  Proceedings of 
ACL?2000. 
Rebecca J. Passonneau. A Computational Model of 
the Semantics of Tense and Aspect. Computational 
Linguistics, 14, 2, 1988, 44-60. 
James Pustejovsky, Patrick Hanks, Roser Sauri, An-
drew See, David Day, Lisa Ferro, Robert Gai-
zauskas, Marcia Lazo, Andrea Setzer, and Beth 
Sundheim. 2003. The TimeBank Corpus. Corpus 
Linguistics, 647-656. 
James Pustejovsky, Bob Ingria, Roser Sauri, Jose 
Castano, Jessica Littman, Rob Gaizauskas, Andrea 
Setzer, G. Katz,  and I. Mani. 2005. The Specifica-
tion Language TimeML. In I. Mani, J. Pustejovsky, 
and R. Gaizauskas, (eds.), The Language of Time: 
A Reader. Oxford University Press.  
Roser Saur?, Robert Knippen, Marc Verhagen and 
James Pustejovsky. 2005. Evita: A Robust Event 
Recognizer for QA Systems. Short Paper. Proceed-
ings of HLT/EMNLP 2005: 700-707. 
Frank Schilder and Christof Habel. 2005. From tem-
poral expressions to temporal information: seman-
tic tagging of news messages. In I. Mani, J. Puste-
jovsky, and R. Gaizauskas, (eds.), The Language of 
Time: A Reader. Oxford University Press.  
Andrea Setzer and Robert Gaizauskas. 2000. Annotat-
ing Events and Temporal Information in Newswire 
Texts. Proceedings of LREC-2000, 1287-1294.  
Marc Verhagen. 2004. Times Between The Lines. 
Ph.D. Dissertation, Department of Computer Sci-
ence, Brandeis University. 
Marc Vilain, Henry Kautz, and Peter Van Beek. 1989. 
Constraint propagation algorithms for temporal 
reasoning: A revised report. In D. S. Weld and J. 
de Kleer (eds.), Readings in Qualitative Reasoning 
about Physical Systems, Morgan-Kaufman, 373-
381. 
Bonnie Webber. 1988. Tense as Discourse Anaphor. 
Computational Linguistics, 14, 2, 1988, 61-73. 
760
Last Words
Improving Our Reviewing Processes
Inderjeet Mani?
The MITRE Corporation
1. Introduction
Our reviewing practices today are failing. With the number of ACL submissions steadily
growing over the last several years (for example, ACL 2009 had a 24% increase in sub-
missions over ACL 2008), the need for more reviewers has become more pronounced.
However, qualified reviewers are becoming hard to find, and when they are found,
they are often hard-pressed for time. As a result, slipshod reviews are becoming com-
monplace. Allowing this situation to continue as before will result in the deterioration
of our ability to recognize excellence in our research. An ?intervention? is therefore
needed.
As I see it, there are two distinct problems to tackle: first, a lack of qualified
reviewers, and second, a lack of quality control in reviews. After discussing these, I
will suggest some solutions that I believe are worth implementing.
2. Problems with Reviewing
2.1 The Lack of Qualified Reviewers
In an earlier Last Words piece, Ken Church (Church 2006) pointed out how the ACL
conference reviewing process can be derailed by the lack of positive endorsement by
reviewers who are not well qualified to review a given paper. He went on to suggest
that papers rejected by NAACL are ?often strong contenders for the best-paper award
at ACL.? An instance of this phenomenon was observed in 2009, when a paper rejected
from NAACL 2009 with an average acceptance score of 2.3 out of 5 was given a best
paper award at ACL 2009 (Branavan et al 2009).1
It is especially hard to find qualified reviewers these days partly because compu-
tational linguistics has become increasingly specialized. Papers in fields like parsing
and machine translation involve very technical modifications to a few current models.
Reviewers for such areas need to be ?insiders?, well-versed in the latest developments in
the sub-area. This need is likely to become more pronounced as the specialization trend
continues.
Reviewers are currently selected based on informal social networks. Unfortunately,
most researchers do not have an extensive set of names of reviewers at hand, and relying
? The MITRE Corporation, 202 Burlington Road, Bedford, MA 01730, USA. E-mail: imani@mitre.org.
1 Such discrepancies in judgments across conferences are not confined to computational linguistics; for
example, the classic Page Rank paper from WWW 1998 (Brin and Page 1998) had previously been
rejected by SIGIR 1998.
? 2011 Association for Computational Linguistics
Computational Linguistics Volume 37, Number 1
on personal connections (not to mention memories of reviewers? prior performance)
is limiting and could bias the selection of reviewers to those who share a particular
point-of-view. This lack of information as to whom to contact can result in woefully
inappropriate selections of reviewers.
2.2 The Lack of Quality Control
Even when qualified reviewers can be found, reviews are often hurried. At the 2009 ACL
Business Meeting, Ido Dagan pointed to the growing dissatisfaction with the quality of
conference reviewing, adding that the problems seemed to be exacerbated by increasing
the number of reviewers. The lack of quality is in part due to the large number of
conferences that compete for the reviewer?s time. As Fortnow (2009) observes, in the
case of computer science conferences the intensive time commitment required for re-
viewing makes it less likely that more experienced researchers will sign on as reviewers.
Such hurried reviewing and decision-making can result in a preference for safe, more
incremental papers rather than those that develop new models and research directions
(Fortnow 2009).
3. Finding Qualified Reviewers: An ACL Reviewer Database
To encourage better selection of reviewers, improved information management is
needed in terms of keeping track of reviewer background and areas of expertise. This
goal can be achieved by maintaining an ACL database of reviewer profiles that is accessible
to the public, as a resource for use in selecting reviewers. It would work as follows:
Whenever reviews are produced in an ACL-related forum, the Program Chair or Editor
would be responsible for updating the database (assisted in part by the conference or
journal management software). S/he would record the reviewer?s name and affiliation,
the name and type of forum (conference, workshop, journal, etc.), its time and place, the
number of papers reviewed, and the reviewer?s areas of interest. To protect a reviewer?s
privacy, information as to which papers were assigned to the reviewer should not be
revealed. Note that most conferences already publish a list of reviewers involved, so
this is not a huge step beyond what is there today.
Once created, the ACL Reviewer Database would provide for easier selection of
reviewers, and would allow a forum organizer or editor to determine, at least semi-
automatically, categories of reviewers such as specialist/generalist, prolific/occasional,
and skill level (e.g., journal paper vs. workshop reviewing, senior vs. extended program
committee experience).
Many reviewers work extremely hard at their reviews. Being recognized as a
well-respected reviewer is well worth striving for, and ideally, this reputation would
be reflected in part by one?s reviewing profile. Just as the impacts of journals and
authors are measured based on citations, there is no reason why reviewers should
not be assessed in terms of their impacts in guiding and encouraging computational
linguistics. Specifically, the database can be used to track how many journal and
conference articles a person has reviewed, and how many articles were submitted
to those venues. A reviewer?s impact factor can be computed by dividing the for-
mer by the latter, and including some normalization parameters (such as taking into
account the level of publishing activity in the particular computational linguistics
subfield).
262
Mani Improving our Reviewing Processes
4. Encouraging High-Quality Reviews
4.1 Open Peer Review: Signing and Publication History
Reviewers often get away with slipshod, low-quality reviews because they can hide
under the cloak of anonymity. Other research communities have reduced the level of
anonymity by using open peer review. In an open peer-review system, reviewers may
sometimes become known to the paper authors, and in some instances, reviewers and
the content of their reviews may become known to all the readers as well. Journals that
have successfully deployed open peer-review include Atmospheric Chemistry and Physics
(ACP),2 the British Medical Journal (BMJ),3 and all forty-one of the Biomed Central (BMC)
medical journals.4
Open peer-review systems can differ in terms of reviewer transparency: Some
venues (e.g., the BMJ, and BMC medical journals such as BMC Cancer) always require
that reviews be signed, that is, visible to the author, whereas others do not (e.g., the
ACP journal), or else they leave it up to the reviewer. Signed reviews can considerably
raise the stakes on review quality: It is one thing to palm off a hurried review on a
hapless author, quite another to be accountable in front of all one?s colleagues for the
poor quality of one?s review. Although it is theoretically possible that signed reviews
might be less frank, in order to avoid alienating particular authors, such a problem
tends not to occur in practice. In the case of a randomized trial with medical articles
in the BMJ, signing did not lower the review quality or recommendations (van Rooyen
et al 1999). Nor has the presence of signed reviews in journals that have used them led
to a rise in litigation against those journals. Finally, many journals using signed reviews,
such as BMC Cancer, continue to thrive and flourish.
Whereas the BMJ publishes only the final version of the paper, both the BMC med-
ical journals and the ACP journal provide public access to the publication history, namely,
all previous versions of the paper along with their reviews and author responses.5 A
view of such a publication history can be extremely instructive to both prospective
authors and reviewers. A further advantage of publication history is that prior submis-
sions within the community can be tracked, providing a collective memory of reviewers?
comments. That is far better than the situation today, where regular reviewers can often
recall reviewing some version of a paper earlier for some other forum, but may not be
able to recall the individual recommendations.
4.2 Reviewer Training
Reviewing is one of the most important activities a researcher carries out, and yet no
formal training is provided to reviewers. If we require high-quality reviews, we need
to train reviewers as to the best practices in reviewing as carried out by a particular
ACL-related forum. This could be organized as an on-line course specific to the journal
or conference, which every reviewer for that forum should take. The course, even if
2 http://www.atmos-chem-phys.net/volumes and issues.html.
3 http://www.bmj.com/.
4 http://www.biomedcentral.com/.
5 For an example of an accepted paper with signed reviews and author comments, see
www.biomedcentral.com/1471-2407/9/348/prepub. For an interesting discussion around a rejected
paper, see the following ACP paper: http://preview.tinyurl.com/ycxq65e.
263
Computational Linguistics Volume 37, Number 1
involving only self-study, can be viewed as a more stringent requirement than simply
being encouraged to read, as is customary today, the reviewing guidelines. Here, too,
the conference or journal management software can check that the course has been
completed within the last couple of years before allowing the reviewer to proceed. The
course can be updated from time to time.
As an example, journals like the BMJ offer materials for training reviewers.6 These
materials include PowerPoint presentations on reviewing best practices (?What we
know about peer review,? ?What editors want?) and written exercises (reading and
assessing three referee reports for a paper, and comparing the assessments with the
editor?s critique of those reports, as well as doing a practice review of a paper and
comparing it with the published reviews). As shown in a BMJ study of the effectiveness
of these materials (Schroter et al 2004), trained reviewers detected significantly more
major errors in papers than those who weren?t trained.
For computational linguistics journals and conferences, it would be straightfor-
ward, under either an open peer review system and/or with permission of authors and
reviewers, to collect published reviews of several articles and have potential reviewers
go through a similar exercise to that provided by the BMJ. In particular, one could focus
on practice reviews for a paper. In addition to such on-line courses, improved review
quality can be engendered by discussion of reviewing methods in classroom settings,
particularly in seminar courses where papers have to be read and jointly discussed.
5. Conclusions
To discover qualified reviewers, I have suggested creating an ACL Reviewer Database.
To improve review quality, I have advocated more use of open peer review, with review
signing and/or maintaining a history of the publication of each article, as well as specific
measures for improved reviewer training. These methods for improving review quality
have become common practice in some other fields, and it is high time computational
linguists started to explore them.
These improvements will require a higher degree of transparency than has been
customary in computational linguistics reviewing, but this transparency will reap con-
siderable benefits in further recognizing and promoting excellence in our research. It
would therefore be worthwhile to initiate a discussion group or a workshop to plan a
pilot that will try out some of these methods. Over time, it will also be useful to conduct
studies of how effective these methods are.
Acknowledgments
This research has been funded by the MITRE
Innovation Program (Public Release Case
Number 07-0862). I am grateful to Robert
Dale for his in-depth comments.
References
Branavan, S. R. K., Harr Chen, Luke
Zettlemoyer, and Regina Barzilay. 2009.
Reinforcement learning for mapping
instructions to actions. Proceedings of the
Joint Conference of the 47th Annual Meeting
of the ACL and the 4th International Joint
Conference on Natural Language Processing
of the AFNLP, pages 82?90, Singapore,
August 2?7.
Brin, Sergey and Lawrence Page. 1998. The
anatomy of a large-scale hypertextual
Web search engine. Proceedings of the
Seventh International Conference on the
6 http://resources.bmj.com/bmj/reviewers/training-materials.
264
Mani Improving our Reviewing Processes
World Wide Web, pages 107?117, Brisbane,
Australia, April 14?18.
Church, Kenneth. 2006. Reviewing the
reviewers. Computational Linguistics,
31(4):575?578.
Fortnow, Lance. 2009. Viewpoint: Time
for computer science to grow up.
Communications of the ACM, 52(8):33?35.
Schroter, Sara, Nick Black, Stephen Evans,
James Carpenter, Fiona Godlee, and
Richard Smith. 2004. Effects of training
on quality of peer review: Randomised
controlled trial. British Medical Journal,
328:673.
van Rooyen, Susan, Fiona Godlee, Stephen
Evans, Nick Black, and Richard Smith.
1999. Effect of open peer review on
quality of reviews and on reviewers?
recommendations: A randomised trial.
British Medical Journal, 318:23?27.
265
Tutorial Abstracts of ACL 2012, page 1,
Jeju, Republic of Korea, 8 July 2012. c?2012 Association for Computational Linguistics
Qualitative Modeling of Spatial Prepositions and Motion Expressions
Inderjeet Mani
Children?s Organization of
Southeast Asia
Thailand
inderjeet.mani@gmail.com
James Pustejovsky
Computer Science Department
Brandeis University
Waltham, MA USA
jamesp@cs.brandeis.edu
The ability to understand spatial prepositions and
motion in natural language will enable a variety of
new applications involving systems that can respond
to verbal directions, map travel guides, display in-
cident reports, etc., providing for enhanced infor-
mation extraction, question-answering, information
retrieval, and more principled text to scene render-
ing. Until now, however, the semantics of spatial re-
lations and motion verbs has been highly problem-
atic. This tutorial presents a new approach to the
semantics of spatial descriptions and motion expres-
sions based on linguistically interpreted qualitative
reasoning. Our approach allows for formal inference
from spatial descriptions in natural language, while
leveraging annotation schemes for time, space, and
motion, along with machine learning from annotated
corpora. We introduce a compositional semantics
for motion expressions that integrates spatial primi-
tives drawn from qualitative calculi.
No previous exposure to the semantics of spatial
prepositions or motion verbs is assumed. The tu-
torial will sharpen cross-linguistic intuitions about
the interpretation of spatial prepositions and mo-
tion constructions. The attendees will also learn
about qualitative reasoning schemes for static and
dynamic spatial information, as well as three annota-
tion schemes: TimeML, SpatialML, and ISO-Space,
for time, space, and motion, respectively.
While both cognitive and formal linguistics have
examined the meaning of motion verbs and spatial
prepositions, these earlier approaches do not yield
precise computable representations that are expres-
sive enough for natural languages. However, the
previous literature makes it clear that communica-
tion of motion relies on imprecise and highly ab-
stract geometric descriptions, rather than Euclidean
ones that specify the coordinates and shapes of ev-
ery object. This property makes these expressions
a fit target for the field of qualitative spatial reason-
ing in AI, which has developed a rich set of geomet-
ric primitives for representing time, space (including
distance, orientation, and topological relations), and
motion. The results of such research have yielded a
wide variety of spatial and temporal reasoning logics
and tools. By reviewing these calculi and resources,
this tutorial aims to systematically connect qualita-
tive reasoning to natural language.
Tutorial Schedule:
I. Introduction. i. Overview of geometric idealiza-
tions underlying spatial PPs; ii. Linguistic patterns
of motion verbs across languages; iii. A qualita-
tive model for static spatial descriptions and for path
verbs; iv. Overview of relevant annotation schemes.
II. Calculi for Qualitative Spatial Reasoning. i.
Semantics of spatial PPs mapped to qualitative spa-
tial reasoning; ii. Qualitative calculi for representing
topological and orientation relations; iii. Qualitative
calculi to represent motion.
III. Semantics of Motion Expressions. i. Introduc-
tion to Dynamic Interval Temporal Logic (DITL); ii.
DITL representations for manner-of-motion verbs
and path verbs; iii. Compositional semantics for mo-
tion expressions in DITL, with the spatial primitives
drawn from qualitative calculi.
IV. Applications and Research Topics. i. Route
navigation, mapping travel narratives, QA, scene
rendering from text, and generating event descrip-
tions; ii. Open issues and further research topics.
1
Appendix I I :  Discussion Panel on Evaluation 
Research 
in Generation 
Moderator: Inderjeet Mani 
Evaluation is critical in offering feedback on progress_toboth developers andpotential consumers 
of NLG technology. However, evaluation has thus far not been as well-established in NLG as it 
has become in NLU. This panel will discuss evaluation methods and resources. It is aimed at 
building a better understanding ofNLG evaluation methods, and hopefully arriving at steps to 
facilitate future evaluations. 
Applicable evaluation methods can be derived from work in NLG as well as Text Summarization 
and Machine Translation. The evaluation methods include intrinsic methods which test the 
generation system in itself, and extrinsic methods which test the generation system in relation to 
some other task. 
Intrinsic methods can include assessing coverage of different varieties of generation input, the 
quality of the generated output, and comparison of generated output against reference output at 
some level (e.g., by using subjective grading, comparison against emplates, or comparing human 
correctness in answering questions based on each type of output, etc.) Of course, a fundamental 
problem in evaluating NLG is that there may be many acceptable outputs. 
Extrinsic methods can include measuring efficiency in executing enerated instructions (e.g., how 
easy was it to install the component by following the generated manual?), assessing the relevance 
of generated output o some information eed or goal (e.g., are the generated business letters 
effective?), its impact on a system in which it is embedded (e.g., how much does the generation 
help the question answering system?), measuring the amount of effort required to post-edit the 
output (e.g., how much do the generated briefings need to be fixed up?), etc. 
As the generation technology becomes more mature, it is useful to assess end-user acceptability 
of generated output, extensibility and portability, throughput, cost-benefit measures, etc. it is also 
interesting for evaluations address both features important to the overall task, as well as features 
unique to NL generation. 
Participants will address the following issues: 
1. What evaluation methods are applicable to NLG? 
2. What are the pros and cons of NLG evaluations you have carried out? 
3. Can we construct corpora to help evaluate NLG systems? 
4. What steps can we collectively take to improve the role of evaluation in NLG? 
273 
Coling 2008: Proceedings of the workshop on Multi-source Multilingual Information Extraction and Summarization, pages 2?9
Manchester, August 2008
Learning to Match Names Across Languages 
Inderjeet Mani 
The MITRE Corporation 
202 Burlington Road 
Bedford, MA 01730, USA 
imani@mitre.org 
Alex Yeh 
The MITRE Corporation 
202 Burlington Road 
Bedford, MA 01730, USA 
asy@mitre.org 
Sherri Condon 
The MITRE Corporation 
7515 Colshire Drive 
McLean, VA 22102, USA 
scondon@mitre.org 
 
Abstract 
We report on research on matching 
names in different scripts across languag-
es. We explore two trainable approaches 
based on comparing pronunciations. The 
first, a cross-lingual approach, uses an 
automatic name-matching program that 
exploits rules based on phonological 
comparisons of the two languages carried 
out by humans. The second, monolingual 
approach, relies only on automatic com-
parison of the phonological representa-
tions of each pair. Alignments produced 
by each approach are fed to a machine 
learning algorithm. Results show that the 
monolingual approach results in ma-
chine-learning based comparison of per-
son-names in English and Chinese at an 
accuracy of over 97.0 F-measure. 
1 Introduction 
The problem of matching pairs of names which 
may have different spellings or segmentation 
arises in a variety of common settings, including 
integration or linking database records, mapping 
from text to structured data (e.g., phonebooks, 
gazetteers, and biological databases), and text to 
text comparison (for information retrieval, 
clustering, summarization, coreference, etc.).  
For named entity recognition, a name from a 
gazetteer or dictionary may be matched against 
text input; even within monolingual applications, 
the forms of these names might differ. In multi-
document summarization, a name may have 
different forms across different sources. Systems 
                                                 
? 2008 The MITRE Corporation.  All rights reserved. Licensed for 
use in the proceedings of the Workshop on Multi-source, Multilin-
gual Information Extraction and Summarization (MIMIES2) at 
COLING?2008. 
that address this problem must be able to handle 
variant spellings, as well as abbreviations, 
missing or additional name parts, and different 
orderings of name parts.  
In multilingual settings, where the names 
being compared can occur in different scripts in 
different languages, the problem becomes 
relevant to additional practical applications, 
including both multilingual information retrieval 
and machine translation. Here special challenges 
are posed by the fact that there usually aren?t 
one-to-one correspondences between sounds 
across languages. Thus the name Stewart, 
pronounced   / s t u w ? r t / in IPA, can be 
mapped to Mandarin ????? ?, which is 
Pinyin ?si tu er te?, pronounced /s i t? u a ? t? e/, 
and the name Elizabeth / I l I z ? b ? ?/ can map 
to ??????, which is Pinyin ?yi li sha bai?, 
pronounced /I l I ? ? p aI/. Further, in a given 
writing system, there may not be a one-to-one 
correspondence between orthography and sound, 
a well-known case in point being English. In 
addition, there may be a variety of variant forms, 
including dialectical variants, (e.g., Bourguiba 
can map to Abu Ruqayba), orthographic 
conventions (e.g., Anglophone Wasim can map 
to Francophone Ouassime), and differences in 
name segmentation (Abd Al Rahman can map to 
Abdurrahman).  Given the high degree of 
variation and noise in the data, approaches based 
on machine learning are needed. 
The considerable differences in possible 
spellings of a name also call for approaches 
which can compare names based on 
pronunciation. Recent work has developed 
pronunciation-based models for name 
comparison, e.g., (Sproat, Tao and Zhai 2006) 
(Tao et al 2006). This paper explores trainable 
pronunciation-based models further.  
2
Table 1: Matching ?Ashburton? and ?????? 
Consider the problem of matching Chinese 
script names against their English (Pinyin) Ro-
manizations. Chinese script has nearly 50,000 
characters in all, with around 5,000 characters in 
use by the well-educated. However, there are 
only about 1,600 Pinyin syllables when tones are 
counted, and as few as 400 when they aren?t. 
This results in multiple Chinese script represen-
tations for a given Roman form name and many 
Chinese characters that map to the same Pinyin 
forms. In addition, one can find multiple Roman 
forms for many names in Chinese script, and 
multiple Pinyin representations for a Chinese 
script representation.  
In developing a multilingual approach that can 
match names from any pair of languages, we 
compare an approach that relies strictly on mo-
nolingual knowledge for each language, specifi-
cally, grapheme-to-phoneme rules for each lan-
guage, with a method that relies on cross-lingual 
rules which in effect map between graphemic 
and/or phonemic representations for the specific 
pair of languages.  
The monolingual approach requires finding 
data on the phonemic representations of a name 
in a given language, which (as we describe in 
Section 4) may be harder than finding more 
graphemic representations. But once the 
phonemic representation is found for names in a 
given language, then as one adds more languages 
to a system, no more work needs to be done in 
that given language. In contrast, with the cross-
lingual approach, whenever a new language is 
added, one needs to  go over all the existing 
languages already in the system and compare 
each of them with the new language to develop 
cross-lingual rules for each such language pair. 
The engineering of such rules requires bilingual 
expertise, and knowledge of differences between 
language pairs. The cross-lingual approach is 
thus more expensive to develop, especially for 
applications which require coverage of a large 
number of languages. 
Our paper investigates whether we can address 
the name-matching problem without requiring 
such a knowledge-rich approach, by carrying out 
a comparison of the performance of the two 
approaches. We present results of large-scale 
machine-learning for matching personal names 
in Chinese and English, along with some 
preliminary results for English and Urdu. 
2 Basic Approaches 
2.1 Cross-Lingual Approach 
Our cross-lingual approach (called MLEV) is 
based on (Freeman et al 2006), who used a 
modified Levenshtein string edit-distance 
algorithm to match Arabic script person names 
against their corresponding English versions. The 
Levenshtein edit-distance algorithm counts the 
minimum number of insertions, deletions or 
substitutions required to make a pair of strings  
match. Freeman et al (2006) used (1) insights 
about phonological differences between the two  
languages to create rules for equivalence classes 
of characters that are treated as identical in the 
computation of edit-distance and (2) the use of 
normalization rules applied to the English and 
transliterated Arabic names based on mappings 
between characters in the respective writing 
systems. For example, characters corresponding 
to low diphthongs in English are normalized as 
?w?, the transliteration for the Arabic 
???character, while high diphthongs are mapped 
to ?y?, the transliteration for the Arabic ??? 
character.   
Table 1 shows the representation and 
comparison of a Roman-Chinese name pair 
(shown in the title) obtained from the Linguistic 
Data Consortium?s LDC Chinese-English name 
pairs corpus (LDC 2005T34). This corpus 
provides name part pairs, the first element in 
English (Roman characters) and the second in 
Chinese characters, created by the LDC from 
Xinhua Newswire's proper name and who's who 
databases. The name part can be a first, middle 
or last name. We compare the English form of 
the name with a Pinyin Romanization of the 
Chinese. (Since the Chinese is being compared 
with English, which is toneless, the tone part of 
Pinyin is being ignored throughout this paper.) 
For this study, the Levenshtein edit-distance 
score (where a perfect match scores zero) is 
 Roman Chinese (Pinyin) Alignment Score 
LEV ashburton ashenbodu |   a   s   h   b   u   r   t   o   n   | 
|   a   s   h   e   n   b  o  d    u   | 
0.67 
MLEV ashburton ashenbodu |  a   s   h   -   -   b   u   r    t   o   n  | 
|  a   s   h   e   n   b   o   -   d   u   -  | 
0.72 
MALINE asVburton aseCnpotu |   a   sV  -   b   <   u   r   t   o   |   n 
|   a   s   eC  n   p   o   -   t   u   |   - 
0.48 
3
normalized to a similarity score as in (Freeman et 
al. 2006), where the score ranges from 0 to 1, 
with 1 being a perfect match. This edit-distance 
score is shown in the LEV row. 
The MLEV row, under the Chinese Name 
column, shows an ?Englishized? normalization 
of the Pinyin for Ashburton. Certain characters or 
character sequences in Pinyin are pronounced 
differently than in English. We therefore apply 
certain transforms to the Pinyin; for example, the 
following substitutions are applied at the start of 
a Pinyin syllable, which makes it easier for an 
English speaker to see how to pronounce it and 
renders the Pinyin more similar to English 
orthography: ?u:? (umlaut ?u?) => ?u?, ?zh? => 
?j?, ?c? => ?ts?, and ?q? => ?ch? (so the Pinyin 
?Qian? is more or less pronounced as if it were 
spelled as ?Chian?, etc.). The MLEV algorithm 
uses equivalence classes that allow ?o? and ?u? 
to match, which results in a higher score than the 
generic score using the LEV method.  
2.2 Monolingual Approach 
Instead of relying on rules that require extensive 
knowledge of differences between a language 
pair2, the monolingual approach first builds pho-
nemic representations for each name, and then 
aligns them. Earlier research by (Kondrak 2000) 
used dynamic programming to align strings of 
phonemes, representing the phonemes as vectors 
of phonological features, which are associated 
with scores to produce similarity values. His 
program ALINE includes a ?skip? function in the 
alignment operations that can be exploited for 
handling epenthetic segments, and in addition to 
1:1 alignments, it also handles 1:2 and 2:1 
alignments. In this research, we made extensive 
modifications to ALINE to add the phonological 
features for languages like Chinese and Arabic 
and to normalize the similarity scores, producing 
a system called MALINE. 
In Table 1, the MALINE row3 shows that the 
English name has a palato-alveolar modification 
                                                 
                                                                         
2As (Freeman et al, 2006) point out, these insights are 
not easy to come by: ?These rules are based on first 
author Dr. Andrew Freeman?s experience with read-
ing and translating Arabic language texts for more 
than 16 years? (Freeman et al, 2006, p. 474). 
3For the MALINE row in Table 1, the ALINE docu-
mentation explains the notation as follows: ?every 
phonetic symbol is represented by a single lowercase 
letter followed by zero or more uppercase letters. The 
initial lowercase letter is the base letter most similar 
to the sound represented by the phonetic symbol. The 
remaining uppercase letters stand for the feature mod-
on the ?s? (expressed as ?sV?), so that we get the 
sound corresponding to ?sh?; the Pinyin name 
inserts a centered ?e? vowel, and devoices the 
bilabial plosive /b/ to /p/. There are actually 
sixteen different Chinese ?pinyinizations? of 
Ashburton, according to our data prepared from 
the LDC corpus.  
3 Experimental Setup 
3.1 Machine Learning Framework 
Neither of the two basic approaches described so 
far use machine learning. Our machine learning 
framework is based on learning from alignments 
produced by either approach. To view the learn-
ing problem as one amenable to a statistical clas-
sifier, we need to generate labeled feature vectors 
so that each feature vector includes an additional 
class feature that can have the value ?true? or 
?false.? Given a set of such labeled feature vec-
tors as training data, the classifier builds a model 
which is then used to classify unlabeled feature 
vectors with the right labels. 
A given set of attested name pairs constitutes a 
set of positive examples. To create negative 
pairs, we have found that randomly selecting 
elements that haven?t been paired will create 
negative examples in which the pairs of elements 
being compared are so different that they can be 
trivially separated from the positive examples. 
The experiments reported here used the MLEV 
score as a threshold to select negatives, so that 
examples below the threshold are excluded. As 
the threshold is raised, the negative examples 
should become harder to discriminate from 
positives (with the harder problems mirroring 
some of the ?confusable name? characteristics of 
the real-world name-matching problems this 
technology is aimed at). Positive examples below 
the threshold are also eliminated. Other criteria, 
including a MALINE score, could be used, but 
the MLEV scores seemed adequate for these 
preliminary experiments.  
Raising the threshold reduces the number of 
negative examples. It is highly desirable to 
balance the number of positive and negative 
examples in training, to avoid the learning being 
 
ifiers which alter the sound defined by the base letter. 
By default, the output contains the alignments togeth-
er with the overall similarity scores. The aligned sub-
sequences are delimited by '|' signs. The '<' sign signi-
fies that the previous phonetic segment has been 
aligned with two segments in the other sequence, a 
case of compression/expansion. The '-' sign denotes a 
?skip?, a case of insertion/deletion.?  
4
biased by a skewed distribution. However, when 
one starts with a balanced distribution of positive 
and negatives, and then excludes a number of 
negative examples below the threshold, a 
corresponding number of positive examples must 
also be removed to preserve the balance. Thus, 
raising the threshold reduces the size of the 
training data. Machine learning algorithms, 
however, can benefit from more training data.  
Therefore, in the experiments below, thresholds 
which provided woefully inadequate training set 
sizes were eliminated.  
One can think of both the machine learning 
method and the basic name comparison methods 
(MLEV and MALINE) as taking each pair of 
names with a known label and returning a 
system-assigned class for that pair. Precision, 
Recall, and F-Measure can be defined in an 
identical manner for both machine learning and 
basic name comparison methods. In such a 
scheme, a threshold on the similarity score is 
used to determine whether the basic comparison 
match is a positive match or not. Learning the 
best threshold for a dataset can be determined by 
searching over different values for the threshold.  
In short, the methodology employed for this 
study involves two types of thresholds: the 
MLEV threshold used to identify negative 
examples and the threshold that is applied to the 
basic comparison methods, MLEV and 
MALINE, to identify matches. To avoid 
confusion, the term negative threshold refers to 
the former, while the term positive threshold is 
used for the latter. 
The basic comparison methods were used as 
baselines in this research. To be able to provide a 
fair basic comparison score at each negative 
threshold, we ?trained? each basic comparison 
matcher at twenty different positive thresholds 
on the same training set used by the learner.  For 
each negative threshold, we picked the positive 
threshold that gave the best performance on the 
training data, and used that to score the matcher 
on the same test data as used by the learner.  
3.2 Feature Extraction 
Consider the MLEV alignment in Table 1. It can 
be seen that the first three characters are matched 
identically across both strings; after that, we get 
an ?e? inserted, an ?n? inserted, a ?b? matched 
identically, a ?u? matched to an ?o?, a ?r? de-
leted, a ?t? matched to a ?d?, an ?o? matched to a 
?u?, and an ?n? deleted. The match unigrams are 
thus ?a:a?, ?s:s?, ?h:h?, ?-:e?, ?-:n?, ?b:b?, ?u:o?, 
?r:-?, ?t:d?, ?o:u?, and ?n:-?. Match bigrams 
were generated by considering any insertion, de-
letion, and (non-identical) substitution unigram, 
and noting the unigram, if any, to its left, pre-
pending that left unigram to it (delimited by a 
comma). Thus, the match bigrams in the above 
example include ?h:h,-:e?, ?-:e,-:n?, ?b:b,u:o?, 
?u:o,r:-?, ?r:-,t:d?, ?t:d,o:u?, ?o:u,n:-?.  
These match unigram and match bigram 
features are generated from just a single MLEV 
match. The composite feature set is the union of 
the complete match unigram and bigram feature 
sets. Given the composite feature set, each match 
pair is turned into a feature vector consisting of 
the following features: string1, string2, the match 
score according to each of the basic comparison 
matchers (MLEV and MALINE), and the 
Boolean value of each feature in the composite 
feature set. 
3.3 Data Set 
Our data is a (roughly 470,000 pair) subset of the 
Chinese-English personal name pairs in LDC 
2005T34. About 150,000 of the pairs had more 
than 1 way to pronounce the English and/or Chi-
nese. For these, to keep the size of the experi-
ments manageable from the point of view of 
training the learners, one pronunciation was ran-
domly chosen as the one to use. (Even with this 
restriction, a minimum negative threshold results 
in over half a million examples). Chinese charac-
ters were mapped into Hanyu Pinyin representa-
tions, which are used for MLEV alignment and 
string comparisons.   Since the input to MALINE 
uses a phonemic representation that encodes 
phonemic features in one or more letters, both 
Pinyin and English forms were mapped into the 
MALINE notation.   
There are a number of slightly varying ways to 
map Pinyin into an international pronunciation 
system like IPA. For example, (Wikipedia 2006) 
and (Salafra 2006) have mappings that differ 
from each other and also each of these two 
sources have changed its mapping over time. We 
used a version of Salafra from 2006 (but we 
ignored the ejectives). For English, the CMU 
pronouncing dictionary (CMU 2008) provided 
phonemic representations that were then mapped 
into the MALINE notation. The dictionary had 
entries for 12% of our data set. For the names not 
in the CMU dictionary, a simple grapheme to 
phoneme script provided an approximate 
phonemic form. We did not use a monolingual 
mapping of Chinese characters (Mandarin 
pronunciation) into IPA because we did not find 
any. 
5
60
65
70
75
80
85
90
95
100
105
0 0.2 0.4 0.6 0.8
M
X
C
MB
XB
CB
Note that we could insist that all pairs in our 
dataset be distinct, requiring that there be exactly 
one match for each Roman name and exactly one 
match for each Pinyin name. This in our view is 
unrealistic, since large corpora will be skewed 
towards names which tend to occur frequently 
(e.g., international figures in news) and occur 
with multiple translations.  We included attested 
match pairs in our test corpora, regardless of the 
number of matches that were associated with a 
member of the pair. 
4 Results 
A variety of machine learning algorithms were 
tested. Results are reported, unless otherwise in-
dicated, using SVM Lite, a Support Vector Ma-
chine (SVM4) classifier5 that scales well to large 
data sets.  
Testing with SVM Lite was done with a 90/10 
train-test split. Further testing was carried out 
with the weka SMO SVM classifier, which used 
built-in cross-validation. Although the latter clas-
sifier didn?t scale to the larger data sets we used, 
it did show that cross-validation didn?t change 
the basic results for the data sets it was tried on.  
4.1 Machine Learning with Different Fea-
ture Sets 
Figure 1:  F-measure with Different Fea-
ture Sets 
Figure 1 shows the F-measure of learning for 
monolingual features (M, based on MALINE), 
cross-lingual features (X, based on MLEV), and 
a combined feature set (C) of both types of fea-
tures6 at different negative thresholds (shown on 
the horizontal axis). Baselines are shown with 
the suffix B, e.g., the basic MALINE without 
learning is MB. When using both monolingual 
and cross-lingual features (C), the baseline (CB) 
                                                 
                                                
4We used a linear kernel function in our SVM expe-
riments; using polynomial or radial basis kernels did 
not improve performance. 
5 From svmlight.joachims.org. 
6In Figure 1, the X curve is more or less under the C 
curve. 
is set to a system response of ?true? only when 
both the MALINE and MLEV baseline systems 
by themselves respond ?true?. Table 2 shows the 
number of examples at each negative threshold 
and the Precision and Recall for these methods, 
along with baselines using the basic methods 
shown in square brackets. 
The results show that the learning method (i) 
outperforms the baselines (basic methods), and 
(ii) the gap between learning and basic compari-
son widens as the problem becomes harder (i.e., 
as the threshold is raised). 
For separate monolingual and cross-lingual 
learning, the increase in accuracy of the learning 
over the baseline (non-learning) results7 was sta-
tistically significant at all negative thresholds 
except 0.6 and 0.7. For learning with combined 
monolingual and cross-lingual features (C), the 
increase over the baseline (non-learning) com-
bined results was statistically significant at all 
negative thresholds except for 0.7. 
In comparing the mono-lingual and cross-
lingual learning approaches, however, the only 
statistically significant differences were that the 
cross-lingual features were more accurate than 
the monolingual features at the 0 to 0.4 negative 
thresholds. This suggests that (iii) the mono-
lingual learning approach is as viable as the 
cross-lingual one as the problem of confusable 
names becomes harder.  
However, using the combined learning ap-
proach (C) is better than using either one. Learn-
ing accuracy with both monolingual and cross-
lingual features is statistically significantly better 
than learning with monolingual features at the 
0.0 to 0.4 negative thresholds, and better than 
learning with cross-lingual features at the 0.0 to 
0.2, and 0.4 negative thresholds. 
 
7Statistical significance between F-measures is not 
directly computable since the overall F-measure is not 
an average of the F-measures of the data samples. 
Instead, we checked the statistical significance of the 
increase in accuracy (accuracy is not shown for rea-
sons of space) due to learning over the baseline. The 
statistical significance test was done by assuming that 
the accuracy scores were binomials that were approx-
imately Gaussian. When the Gaussian approximation 
assumption failed (due to the binomial being too 
skewed), a looser, more general bound was used 
(Chebyshev?s inequality, which applies to all proba-
bility distributions). All statistically significant differ-
ences are at the 1% level (2-sided). 
6
4.2 Feature Set Analyses 
The unigram features reflect common correspon-
dences between Chinese and English pronuncia-
tion.  For example, (Sproat, Tao and Zhai 2006) 
note that Chinese /l/ is often associated with Eng-
lish /r/, and the feature l:r is among the most fre-
quent unigram mappings in both the MLEV and 
MALINE alignments. At a frequency of 103,361, 
it is the most frequent unigram feature in the 
MLEV mappings, and it is the third most fre-
quent unigram feature in the MALINE align-
ments (56,780). 
Systematic correspondences among plosives 
are also captured in the MALINE unigram map-
pings.  The unaspirated voiceless Chinese plo-
sives /p,t,k/ contrast with aspirated plosives 
/p?,t?,k?/, whereas the English voiceless plosives 
(which are aspirated in predictable environments) 
contrast with voiced plosives /b,d,g/.  As a result, 
English /b,d,g/ phonemes are usually translite-
rated using Chinese characters that are pro-
nounced /p,t,k/, while English /p,t,k/ phonemes 
usually correspond to Chinese /p?,t?,k?/. The ex-
amples of Stewart and Elizabeth in Section 1 
illustrate the correspondence of English /t/ and 
Chinese / t?/ and of English /b/ with Chinese /p/ 
respectively. All six of the unigram features that  
result from these correspondences occur among 
the 20 most frequent in the MALINE alignments, 
ranging in frequency from 23,602 to 53,535. 
 
 
Neg-
ative 
Thre-
shold 
Exam-
ples 
Monolingual  (M) Cross-Lingual (X) Combined (C) 
  P R P R P R 
0 538,621 94.69 
[90.6] 
95.73 
[91.0] 
96.5 
[90.0] 
97.15 
[93.4] 
97.13 
[90.8] 
97.65 
[91.0] 
0.1 307,066 95.28 
[87.1] 
96.23 
[83.4] 
98.06 
[89.2] 
98.25 
[89.9] 
98.4 
[87.6] 
98.64 
[84.1] 
0.2 282,214 95.82 
[86.2] 
96.63 
[84.4] 
97.91 
[88.4] 
98.41 
[90.3] 
98.26 
[86.7] 
98.82 
[84.7] 
0.3 183,188 95.79 
[80.6] 
96.92 
[85.3] 
98.18 
[86.3] 
98.8 
[90.7] 
98.24 
[80.6] 
99.27 
[84.8] 
0.4 72,176 96.31 
[77.1] 
98.69 
[82.3] 
97.89 
[91.8] 
99.61 
[86.2] 
98.91 
[77.1] 
99.64 
[80.9] 
0.5 17,914 94.62 
[64.6] 
98.63 
[84.3] 
99.44 
[89.4] 
100.0 
[91.9] 
99.46 
[63.8] 
99.89 
[84.7] 
0.6 2,954 94.94 
[66.1] 
100 
[77.0] 
98.0 
[85.2] 
98.66 
[92.8] 
99.37 
[61.3] 
100.0 
[73.1] 
0.7 362 95.24 
[52.8] 
100 
[100.0] 
94.74 
[78.9] 
100.0 
[78.9] 
100.0 
[47.2] 
94.74 
[100.0] 
Table 2:  Precision and Recall with Different Feature Sets 
(Baseline scores in square brackets) 
 
4.3 Comparison with other Learners 
To compare with other machine learning tools, 
we used the WEKA toolkit (from 
www.weka.net.nz). Table 3 shows the compar-
isons on the MLEV data for a fixed size at one 
threshold. Except for SVM Light, the results 
are based on 10-fold cross validation.  The 
other classifiers appear to perform relatively 
worse at that setting for the MLEV data, but 
the differences in accuracy are not statistically 
significant even at the 5% level. A large con-
tributor to the lack of significance is the small 
test set size of 66 pairs (10% of 660 examples) 
used in the SVM Light test. 
4.4 Other Language Pairs 
Some earlier experiments for Arabic-Roman 
comparisons were carried out using a Condi-
tional Random Field learner (CRF), using the 
Carafe toolkit (from source-
forge.net/projects/carafe). The method com-
putes its own Levenshtein edit-distance scores, 
and learns edit-distance costs from that. The 
scores obtained, on average, had only a .6 cor-
relation with the basic comparison Levenshtein 
scores. However, these experiments did not 
return accuracy results, as ground-truth data 
was not specified for this task. 
7
Several preliminary machine learning expe-
riments were also carried out on Urdu-Roman 
comparisons. The data used were Urdu data 
extracted from a parallel corpus recently pro-
duced by the LDC (LCTL_Urdu.20060408).  
The results are shown in Table 4. Here a .55 
MALINE score and a .85 MLEV score were 
used for selecting positive examples by basic 
comparison, and negative examples were se-
lected at random. Here the MALINE method 
(row 1) using the weka SMO SVM made use 
of a threshold based on a MALINE score. In 
these earlier experiments, machine learning 
does not really improve the system perfor-
mance (F-measure decreases with learning on 
one test and only increases by 0.1% on the 
other test). However, since these earlier expe-
riments did not benefit from the use of differ-
ent negative thresholds, there was no control 
over problem difficulty.  
5 Related Work 
While there is a substantial literature employ-
ing learning techniques for record linkage 
based on the theory developed by Fellegi and 
Sunter (1969), researchers have only recently 
developed applications that focus on name 
strings and that employ methods which do not 
require features to be independent (Cohen and 
Richman 2002). Ristad and Yianilos (1997) 
have developed a generative model for learn-
ing string-edit distance that learns the cost of 
different edit operations during string align-
ment. Bilenko and Mooney (2003) extend Ris-
tad?s approach to include gap penalties (where 
the gaps are contiguous sequences of mis-
matched characters) and compare this genera-
tive approach with a vector similarity approach 
that doesn?t carry out alignment. McCallum et 
al. (2005) use Conditional Random Fields 
(CRFs) to learn edit costs, arguing in favor of 
discriminative training approaches and against 
generative approaches, based in part on the 
fact that the latter approaches ?cannot benefit 
from negative evidence from pairs of strings 
that (while partially overlapping) should be 
considered dissimilar?. Such CRFs model the 
conditional probability of a label sequence (an 
alignment of two strings) given a sequence of 
observations (the strings).  
A related thread of research is work on au-
tomatic transliteration, where training sets are 
typically used to compute probabilities for 
mappings in weighted finite state transducers 
(Al-Onaizan and Knight 2002; Gao et al 2004) 
or source-channel models (Knight and Graehl 
1997; Li et al 2004). (Sproat et al 2006) have 
compared names from comparable and con-
temporaneous English and Chinese texts, scor-
ing matches by training a learning algorithm to 
compare the phonemic representations of the 
names in the pair, in addition to taking into 
account the frequency distribution of the pair 
over time.  (Tao et al 2006) obtain similar re-
sults using frequency and a similarity score 
based on a phonetic cost matrix 
The above approaches have all developed 
special-purpose machine-learning architectures 
to address the matching of string sequences. 
They take pairs of strings that haven?t been 
aligned, and learn costs or mappings from 
them, and once trained, search for the best 
match given the learned representation  
 
Positive  
Threshold
Examples Method P R F Accuracy 
.65 660 SVM Light 90.62 87.88 89.22 89.39   
.65 660 WEKA SMO 80.6 83.3 81.92 81.66 
.65 660 AdaBoost M1 84.9 78.5 81.57 82.27 
Table 3: Comparison of Different Classifiers 
 
Method Positive 
Threshold 
Examples P R F 
WEKA SMO .55 (MALINE) 206 (MALINE) 84.8 [81.5] 86.4 [93.3] 85.6 [87.0] 
WEKA SMO .85 (MLEV) 584 (MLEV) 89.9 [93.2] 94.7 [91.2] 92.3 [92.2] 
Table 4: Urdu-Roman Name Matching Results with Random Negatives 
(Baseline scores in square brackets) 
 
8
Our approach, by contrast, takes pairs of 
strings along with an alignment, and using fea-
tures derived from the alignments, trains a learn-
er to derive the best match given the features. 
This offers the advantage of modularity, in that 
any type of alignment model can be combined 
with SVMs or other classifiers (we have pre-
ferred SVMs since they offer discriminative 
training). Our approach allows leveraging of any 
existing alignments, which can lead to starting 
the learning from a higher baseline and less train-
ing data to get to the same level of performance. 
Since the learner itself doesn?t compute the 
alignments, the disadvantage of our approach is 
the need to engineer features that communicate 
important aspects of the alignment to the learner.  
In addition, our approach, as with McCallum 
et al (2005), allows one to take advantage of 
both positive and negative training examples, 
rather than positive ones alone. Our data genera-
tion strategy has the advantage of generating 
negative examples so as to vary the difficulty of 
the problem, allowing for more fine-grained per-
formance measures. Metrics based on such a 
control are likely to be useful in understanding 
how well a name-matching system will work in 
particular applications, especially those involving 
confusable names. 
6 Conclusion 
The work presented here has established a 
framework for application of machine learning 
techniques to multilingual name matching.  The 
results show that machine learning dramatically 
outperforms basic comparison methods, with F-
measures as high as 97.0 on the most difficult 
problems. This approach is being embedded in a 
larger system that matches full names using a 
vetted database of full-name matches for evalua-
tion.  
So far, we have confined ourselves to minimal 
feature engineering. Future work will investigate 
a more abstract set of phonemic features. We 
also hope to leverage ongoing work on harvest-
ing name pairs from web resources, in addition 
applying them to less commonly taught languag-
es, as and when appropriate resources for them 
become available. 
References 
Al-Onaizan, Y. and K. Knight, K. 2002. Machine 
Transliteration of Names in Arabic Text. Proceed-
ings of the ACL Workshop on Computational Ap-
proaches to Semitic Languages. 
Bilenko, M. and Mooney, R.J. 2003. Adaptive dupli-
cate detection using learnable string similarity 
measures. In Proc. of SIGKDD-2003. 
CMU. 2008. The CMU Pronouncing 
nary. ftp://ftp.cs.cmu.edu/project/speech/dict/ 
Cohen, W. W., and Richman, J. 2002. Learning to 
match and cluster large high-dimensional data sets 
for data integration. In Proceedings of The Eighth 
ACM SIGKDD International Conference on Know-
ledge Discovery and Data Mining (KDD-2002). 
Fellegi, I. and Sunter, A.  1969. A theory for record 
linkage.  Journal of the American Statistical Socie-
ty, 64:1183-1210, 1969. 
Freeman, A., Condon, S. and Ackermann, C. 2006. 
Cross Linguistic Name Matching in English and 
Arabic. Proceedings of HLT. 
Gao, W., Wong, K., and Lam, W. 2004. Phoneme-
based transliteration of foreign names for OOV 
problem.  In Proceedings of First International 
Joint Conference on Natural Language Processing. 
Kondrak, G. 2000. A New Algorithm for the Align-
ment of Phonetic Sequences. Proceedings of the 
First Meeting of the North American Chapter of 
the Association for Computational Linguistics 
(ANLP-NAACL 2000),  288-295.  
Knight, K. and Graehl, J., 1997. Machine Translitera-
tion, In Proceedings of the Conference of the Asso-
ciation for Computation Linguistics (ACL). 
Li, H., Zhang, M., & Su, J. 2004. A joint source-
channel model for machine transliteration.  In Pro-
ceedings of Conference of the Association for 
Computation Linguistics (ACL). 
McCallum, A., Bellare, K. and Pereira, F. 2005. A 
Conditional Random Field for Discriminatively-
trained Finite-state String Edit Distance. Confe-
rence on Uncertainty in AI (UAI). 
Ristad, E. S. and Yianilos, P. N. 1998.  Learning 
string edit distance.  IEEE Transactions on Pattern 
Recognition and Machine Intelligence. 
Salafra. 2006. http://www.safalra.com /science 
/linguistics/pinyin-pronunciation/ 
Sproat, R., Tao, T. and Zhai, C. 2006.  Named Entity 
Transliteration with Comparable Corpora.  In Pro-
ceedings of the Conference of the Association for 
Computational Linguistics.  New York. 
Tao, T., Yoon, S. Fister, A., Sproat, R. and Zhai, C. 
2006.  Unsupervised Named Entity Transliteration 
Using Temporal and Phonetic Correlation.  In Pro-
ceedings of the ACL Empirical Methods in Natural 
Language Processing Workshop. 
Wikipedia. 2006. http://en.wikipedia.org/wiki/Pinyin 
9
