Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 55?60,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Sprinkling Topics for Weakly Supervised Text Classification
Swapnil Hingmire
1,2
swapnil.hingmire@tcs.com
Sutanu Chakraborti
2
sutanuc@cse.iitm.ac.in
1
Systems Research Lab, Tata Research Development and Design Center, Pune, India
2
Department of Computer Science and Engineering,
Indian Institute of Technology Madras, Chennai, India
Abstract
Supervised text classification algorithms
require a large number of documents la-
beled by humans, that involve a labor-
intensive and time consuming process.
In this paper, we propose a weakly su-
pervised algorithm in which supervision
comes in the form of labeling of Latent
Dirichlet Allocation (LDA) topics. We
then use this weak supervision to ?sprin-
kle? artificial words to the training docu-
ments to identify topics in accordance with
the underlying class structure of the cor-
pus based on the higher order word asso-
ciations. We evaluate this approach to im-
prove performance of text classification on
three real world datasets.
1 Introduction
In supervised text classification learning algo-
rithms, the learner (a program) takes human la-
beled documents as input and learns a decision
function that can classify a previously unseen doc-
ument to one of the predefined classes. Usually a
large number of documents labeled by humans are
used by the learner to classify unseen documents
with adequate accuracy. Unfortunately, labeling
a large number of documents is a labor-intensive
and time consuming process.
In this paper, we propose a text classification
algorithm based on Latent Dirichlet Allocation
(LDA) (Blei et al, 2003) which does not need la-
beled documents. LDA is an unsupervised prob-
abilistic topic model and it is widely used to dis-
cover latent semantic structure of a document col-
lection by modeling words in the documents. Blei
et al (Blei et al, 2003) used LDA topics as fea-
tures in text classification, but they use labeled
documents while learning a classifier. sLDA (Blei
and McAuliffe, 2007), DiscLDA (Lacoste-Julien
et al, 2008) and MedLDA (Zhu et al, 2009) are
few extensions of LDA which model both class
labels and words in the documents. These models
can be used for text classification, but they need
expensive labeled documents.
An approach that is less demanding in terms
of knowledge engineering is ClassifyLDA (Hing-
mire et al, 2013). In this approach, a topic model
on a given set of unlabeled training documents is
constructed using LDA, then an annotator assigns
a class label to some topics based on their most
probable words. These labeled topics are used
to create a new topic model such that in the new
model topics are better aligned to class labels. A
class label is assigned to a test document on the ba-
sis of its most prominent topics. We extend Clas-
sifyLDA algorithm by ?sprinkling? topics to unla-
beled documents.
Sprinkling (Chakraborti et al, 2007) integrates
class labels of documents into Latent Semantic In-
dexing (LSI)(Deerwester et al, 1990). The ba-
sic idea involves encoding of class labels as ar-
tificial words which are ?sprinkled? (appended)
to training documents. As LSI uses higher or-
der word associations (Kontostathis and Pottenger,
2006), sprinkling of artificial words gives better
and class-enriched latent semantic structure. How-
ever, Sprinkled LSI is a supervised technique and
hence it requires expensive labeled documents.
The paper revolves around the idea of labeling top-
ics (which are far fewer in number compared to
documents) as in ClassifyLDA, and using these la-
beled topic for sprinkling.
As in ClassifyLDA, we ask an annotator to as-
sign class labels to a set of topics inferred on the
unlabeled training documents. We use the labeled
topics to find probability distribution of each train-
ing document over the class labels. We create a
set of artificial words corresponding to a class la-
bel and add (or sprinkle) them to the document.
The number of such artificial terms is propor-
55
tional to the probability of generating the docu-
ment by the class label. We then infer a set of
topics on the sprinkled training documents. As
LDA uses higher order word associations (Lee et
al., 2010) while discovering topics, we hypothe-
size that sprinkling will improve text classification
performance of ClassifyLDA. We experimentally
verify this hypothesis on three real world datasets.
2 Related Work
Several researchers have proposed semi-
supervised text classification algorithms with
the aim of reducing the time, effort and cost
involved in labeling documents. These algorithms
can be broadly categorized into three categories
depending on how supervision is provided. In the
first category, a small set of labeled documents
and a large set of unlabeled documents is used
while learning a classifier. Semi-supervised text
classification algorithms proposed in (Nigam et
al., 2000), (Joachims, 1999), (Zhu and Ghahra-
mani, 2002) and (Blum and Mitchell, 1998) are a
few examples of this type. However, these algo-
rithms are sensitive to initial labeled documents
and hyper-parameters of the algorithm.
In the second category, supervision comes in the
form of labeled words (features). (Liu et al, 2004)
and (Druck et al, 2008) are a few examples of this
type. An important limitation of these algorithms
is coming up with a small set of words that should
be presented to the annotators for labeling. Also
a human annotator may discard or mislabel a pol-
ysemous word, which may affect the performance
of a text classifier.
The third type of semi-supervised text classifi-
cation algorithms is based on active learning. In
active learning, particular unlabeled documents or
features are selected and queried to an oracle (e.g.
human annotator).(Godbole et al, 2004), (Ragha-
van et al, 2006), (Druck et al, 2009) are a few ex-
amples of active learning based text classification
algorithms. However, these algorithms are sensi-
tive to the sampling strategy used to query docu-
ments or features.
In our approach, an annotator does not label
documents or words, rather she labels a small set
of interpretable topics which are inferred in an un-
supervised manner. These topics are very few,
when compared to the number of documents. As
the most probable words of topics are representa-
tive of the dataset, there is no need for the annota-
tor to search for the right set of features for each
class. As LDA topics are semantically more mean-
ingful than individual words and can be acquired
easily, our approach overcomes limitations of the
semi-supervised methods discussed above.
3 Background
3.1 LDA
LDA is an unsupervised probabilistic generative
model for collections of discrete data such as text
documents. The generative process of LDA can be
described as follows:
1. for each topic t, draw a distribution over
words: ?
t
? Dirichlet(?
w
)
2. for each document d ? D
a. Draw a vector of topic proportions:
?
d
? Dirichlet(?
t
)
b. for each word w at position n in d
i. Draw a topic assignment:
z
d,n
? Multinomial(?
d
)
ii. Draw a word:
w
d,n
? Multinomial(z
d,n
)
Where, T is the number of topics, ?
t
is the word
probabilities for topic t, ?
d
is the topic probabil-
ity distribution, z
d,n
is topic assignment and w
d,n
is word assignment for nth word position in docu-
ment d respectively. ?
t
and ?
w
are topic and word
Dirichlet priors.
The key problem in LDA is posterior inference.
The posterior inference involves the inference of
the hidden topic structure given the observed doc-
uments. However, computing the exact posterior
inference is intractable. In this paper we estimate
approximate posterior inference using collapsed
Gibbs sampling (Griffiths and Steyvers, 2004).
The Gibbs sampling equation used to update the
assignment of a topic t to the word w ? W at the
position n in document d, conditioned on ?
t
, ?
w
is:
P (z
d,n
= t|z
d,?n
, w
d,n
= w,?
t
, ?
w
) ?
?
w,t
+ ?
w
? 1
?
v?W
?
v,t
+ ?
v
? 1
? (?
t,d
+ ?
t
? 1) (1)
where ?
w,c
is the count of the word w assigned
to the topic c, ?
c,d
is the count of the topic c
assigned to words in the document d and W is
the vocabulary of the corpus. We use a subscript
d,?n to denote the current token, z
d,n
is ignored
in the Gibbs sampling update. After performing
collapsed Gibbs sampling using equation 1, we
use word topic assignments to compute a point
56
estimate of the distribution over words ?
w,c
and
a point estimate of the posterior distribution over
topics for each document d (?
d
) is:
?
w,t
=
?
w,t
+ ?
w
[
?
v?W
?
v,t
+ ?
v
]
(2)
?
t,d
=
?
t,d
+ ?
t
[
T?
i=1
?
i,d
+ ?
i
]
(3)
Let M
D
=< Z,?,? > be the hidden topic
structure, where Z is per word per document topic
assignment, ? = {?
t
} and ? = {?
d
}.
3.2 Sprinkling
(Chakraborti et al, 2007) propose a simple ap-
proach called ?sprinkling? to incorporate class la-
bels of documents into LSI. In sprinkling, a set of
artificial words are appended to a training docu-
ment which are specific to the class label of the
document. Consider a case of binary classification
with classes c
1
and c
2
. If a document d belongs
to the class c
1
then a set of artificial words which
represent the class c
1
are appended into the doc-
ument d, otherwise a set of artificial words which
represent the class c
2
are appended.
Singular Value Decomposition (SVD) is then
performed on the sprinkled training documents
and a lower rank approximation is constructed
by ignoring dimensions corresponding to lower
singular values. Then, the sprinkled terms are
removed from the lower rank approximation.
(Chakraborti et al, 2007) empirically show that
sprinkled words boost higher order word associ-
ations and projects documents with same class la-
bels close to each other in latent semantic space.
4 Topic Sprinkling in LDA
In our text classification algorithm, we first infer a
set of topics on the given unlabeled document cor-
pus. We then ask a human annotator to assign one
or more class labels to the topics based on their
most probable words. We use these labeled topics
to create a new LDA model as follows. If the topic
assigned to the word w at the position n in docu-
ment d is t, then we replace it by the class label
assigned to the topic t. If more than one class la-
bels are assigned to the topic t, then we randomly
select one of the class labels assigned to the topic
t. If the annotator is unable to label a topic then
we randomly select a class label from the set of all
class labels. We then update the new LDA model
using collapsed Gibbs sampling.
We use this new model to infer the probability
distribution of each unlabeled training document
over the class labels. Let, ?
c,d
be the probability of
generating document d by class c. We then sprin-
kle s artificial words of class label c to document
d, such that s = K ? ?
c,d
for some constant K.
We then infer a set of |C| number of topics on
the sprinkled dataset using collapsed Gibbs sam-
pling, where C is the set of class labels of the
training documents. We modify collapsed Gibbs
sampling update in Equation 1 to carry class label
information while inferring topics. If a word in a
document is a sprinkled word then while sampling
a class label for it, we sample the class label asso-
ciated with the sprinkled word, otherwise we sam-
ple a class label for the word using Gibbs update
in Equation 1.
We name this model as Topic Sprinkled LDA
(TS-LDA). While classifying a test document, its
probability distribution over class labels is inferred
using TS-LDA model and it is classified to its most
probable class label. Algorithm for TS-LDA is
summarized in Table 1.
5 Experimental Evaluation
We determine the effectiveness of our algorithm
in relation to ClassifyLDA algorithm proposed in
(Hingmire et al, 2013). We evaluate and com-
pare our text classification algorithm by comput-
ing Macro averaged F1. As the inference of LDA
is approximate, we repeat all the experiments for
each dataset ten times and report average Macro-
F1. Similar to (Blei et al, 2003) we also learn
supervised SVM classifier (LDA-SVM) for each
dataset using topics as features and report average
Macro-F1.
5.1 Datasets
We use the following datasets in our experiments.
1. 20 Newsgroups: This dataset contains
messages across twenty newsgroups. In our
experiments, we use bydate version of the
20Newsgroup dataset
1
. This version of the dataset
is divided into training (60%) and test (40%)
datasets. We construct classifiers on training
datasets and evaluate them on test datasets.
2. SRAA: Simulated/Real/Aviation/Auto
UseNet data
2
: This dataset contains 73,218
1
http://qwone.com/
?
jason/20Newsgroups/
2
http://people.cs.umass.edu/
?
mccallum/
data.html
57
? Input: unlabeled document corpus-D, number of
topics-T and number of sprinkled terms-K
1. Infer T number of topics on D for LDA using col-
lapsed Gibbs sampling. Let M
D
be the hidden
topic structure of this model.
2. Ask an annotator to assign one or more class labels
c
i
? C to a topic based on its 30 most probable
words.
3. Initialization: For nth word in document d ? D
if z
d,n
= t and the annotator has labeled topic t
with c
i
then, z
d,n
= c
i
4. Update M
D
using collapsed Gibbs sampling up-
date in Equation 1.
5. Sprinkling: For each document d ? D:
(a) Infer a probability distribution ?
d
over class
labels using M
D
using Equation 3.
(b) Let, ?
c,d
be probability of generating docu-
ment d by class c.
(c) InsertK ??
c,d
distinct words associated with
the class c to the document d.
6. Infer |C| number of topics on the sprinkled docu-
ment corpus D using collapsed Gibbs sampling up-
date.
7. Let M
?
D
be the new hidden topic structure. Let us
call this hidden structure as TS-LDA.
8. Classification of an unlabled document d
(a) Infer ?
?
d
for document d using M
?
D
.
(b) k = argmax
i
?
?
i,d
(c) y
d
= c
k
Table 1: Algorithm for sprinkling LDA topics for
text classification
UseNet articles from four discussion groups,
for simulated auto racing (sim auto), simulated
aviation (sim aviation), real autos (real auto), real
aviation (real aviation). Following are the three
classification tasks associated with this dataset.
1. sim auto vs sim aviation vs real auto vs
real aviation
2. auto (sim auto + real auto) vs aviation
(sim aviation + real aviation)
3. simulated (sim auto + sim aviation) vs real
(real auto + real aviation)
We randomly split SRAA dataset such that 80%
is used as training data and remaining is used as
test data.
3. WebKB: The WebKB dataset
3
contains 8145
web pages gathered from university computer
3
http://www.cs.cmu.edu/
?
webkb/
science departments. The task is to classify the
webpages as student, course, faculty or project.
We randomly split this dataset such that 80% is
used as training and 20% is used as test data.
We preprocess these datasets by removing
HTML tags and stop-words.
For various subsets of the 20Newsgroups and
WebKB datasets discussed above, we choose
number of topics as twice the number of classes.
For SRAA dataset we infer 8 topics on the train-
ing dataset and label these 8 topics for all the three
classification tasks. While labeling a topic, we
show its 30 most probable words to the human an-
notator.
Similar to (Griffiths and Steyvers, 2004), we set
symmetric Dirichlet word prior (?
w
) for each topic
to 0.01 and symmetric Dirichlet topic prior (?
t
)
for each document to 50/T, where T is number of
topics. We set K i.e. maximum number of words
sprinkled per class to 10.
5.2 Results
Table 2 shows experimental results. We can ob-
serve that, TS-LDA performs better than Classi-
fyLDA in 5 of the total 9 subsets. For the comp-
religion-sci dataset TS-LDA and ClassifyLDA
have the same performance. However, Classi-
fyLDA performs better than TS-LDA for the three
classification tasks of SRAA dataset. We can also
observe that, performance of TS-LDA is close to
supervised LDA-SVM. We should note here that
in TS-LDA, the annotator only labels a few topics
and not a single document. Hence, our approach
exerts a low cognitive load on the annotator, at
the same time achieves text classification perfor-
mance close to LDA-SVM which needs labeled
documents.
5.3 Example
Table 3 shows most prominent words of four
topics inferred on the med-space subset of the
20Newsgroup dataset. We can observe here that
most prominent words of the first topic do not rep-
resent a single class, while other topics represent
either med (medical) or space class. We can say
here that, these topics are not ?coherent?.
We use these labeled topics and create a TS-
LDA model using the algorithm described in Table
1. Table 4 shows words corresponding to the top
two topics of the TS-LDA model. We can observe
here that these two topics are more coherent than
the topics in Table 3.
58
Text Classification (Macro-F1)
Dataset # Topics ClassifyLDA TS-LDA LDA-SVM
20Newsgroups
med-space 4 0.892 0.938 0.933
politics-religion 4 0.836 0.897 0.901
politics-sci 4 0.887 0.901 0.910
comp-religion-sci 6 0.853 0.853 0.872
politics-rec-religion-sci 8 0.842 0.858 0.862
SRAA
real auto-real aviation-sim auto-
sim aviation
8 0.766 0.741 0.820
auto-aviation 8 0.926 0.910 0.934
real-sim 8 0.918 0.902 0.923
WebKB
WebKB 8 0.627 0.672 0.730
Table 2: Experimental results of text classification on various datasets.
ID Most prominent words in the
topic
Class (med
/ space)
0 science scientific idea large theory
bit pat thought problem isn
med +
space
1 information health research medi-
cal water cancer hiv aids children
institute newsletter
med
2 msg food doctor disease pain
day treatment blood steve dyer
medicine symptoms
med
3 space nasa launch earth orbit
moon shuttle data lunar satellite
space
Table 3: Topic labeling on the med-space subset of the
20Newsgroup dataset
ID Most prominent words in the
topic
Class (med
/ space)
0 msg medical health food disease
years problem information doctor
pain cancer
med
1 space launch earth data orbit
moon program shuttle lunar satel-
lite
space
Table 4: Topics inferred on the med-space subset of the
20Newsgroup dataset after sprinkling labeled topics from Ta-
ble 3.
Hence, we can say here that, in addition to text
classification, sprinkling improves coherence of
topics.
We should note here that, in ClassifyLDA, the
annotator is able to assign a single class label to
a topic. If the annotator assigns a wrong class la-
bel to a topic representing multiple classes (e.g.
first topic in Table 3), then it may affect the perfor-
mance of the resulting classifier. However, in our
approach the annotator can assign multiple class
labels to a topic, hence our approach is more flexi-
ble for the annotator to encode her domain knowl-
edge efficiently.
6 Conclusions and Future Work
In this paper we propose a novel algorithm that
classifies documents based on class labels over
few topics. This reduces the need to label a large
collection of documents. We have used the idea
of sprinkling originally proposed in the context
of supervised Latent Semantic Analysis, but the
setting here is quite different. Unlike the work
in (Chakraborti et al, 2007), we do not assume
that we have class labels over the set of training
documents. Instead, to realize our goal of reduc-
ing knowledge acquisition overhead, we propose a
way of propagating knowledge of few topic labels
to the words and inducing a new topic distribu-
tion that has its topics more closely aligned to the
class labels. The results show that the approach
can yield performance comparable to entirely su-
pervised settings. In future work, we also envi-
sion the possibility of sprinkling knowledge from
background knowledge sources like Wikipedia
(Gabrilovich and Markovitch, 2007) to realize an
alignment of topics to Wikipedia concepts. We
would like to study effect of change in number of
topics on the text classification performance. We
will also explore techniques which will help an-
notators to encode their domain knowledge effi-
ciently when the topics are not well aligned to the
class labels.
References
David M. Blei and Jon D. McAuliffe. 2007. Super-
vised Topic Models. In NIPS.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. The Journal of
Machine Learning Research, 3:993?1022, March.
59
Avrim Blum and Tom Mitchell. 1998. Combining La-
beled and Unlabeled Data with Co-Training. In Pro-
ceedings of the eleventh annual conference on Com-
putational learning theory, pages 92?100.
Sutanu Chakraborti, Rahman Mukras, Robert Lothian,
Nirmalie Wiratunga, Stuart N. K. Watt, David J.
Harper. 2007. Supervised Latent Semantic Indexing
Using Adaptive Sprinkling. In IJCAI, pages 1582-
1587.
Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by Latent Semantic Analysis. JA-
SIS, 41(6):391?407.
Gregory Druck, Gideon Mann, and Andrew McCal-
lum. 2008. Learning from Labeled Features using
Generalized Expectation criteria. In SIGIR, pages
595?602.
Gregory Druck, Burr Settles, and Andrew McCallum.
2009. Active Learning by Labeling Features. In
EMNLP, pages 81?90.
Shantanu Godbole, Abhay Harpale, Sunita Sarawagi,
and Soumen Chakrabarti. 2004. Document Classifi-
cation through Interactive Supervision of Document
and Term Labels. In PKDD, pages 185?196.
Thomas L. Griffiths and Mark Steyvers. 2004. Finding
Scientific Topics. PNAS, 101(suppl. 1):5228?5235,
April.
Swapnil Hingmire, Sandeep Chougule, Girish K. Pal-
shikar, and Sutanu Chakraborti. 2013. Document
Classification by Topic Labeling. In SIGIR, pages
877?880.
Thorsten Joachims. 1999. Transductive Inference for
Text Classification using Support Vector Machines.
In ICML, pages 200?209.
April Kontostathis and William M. Pottenger. 2006. A
Framework for Understanding Latent Semantic In-
dexing (LSI) Performance. Inf. Process. Manage.,
42(1):56?73, January.
Simon Lacoste-Julien, Fei Sha, and Michael I. Jordan.
2008. DiscLDA: Discriminative Learning for Di-
mensionality Reduction and Classification. In NIPS.
Sangno Lee, Jeff Baker, Jaeki Song, and James C.
Wetherbe. 2010. An Empirical Comparison of
Four Text Mining Methods. In Proceedings of the
2010 43rd Hawaii International Conference on Sys-
tem Sciences, pages 1?10.
Bing Liu, Xiaoli Li, Wee Sun Lee, and Philip S. Yu.
2004. Text Classification by Labeling Words. In
Proceedings of the 19th national conference on Ar-
tifical intelligence, pages 425?430.
Kamal Nigam, Andrew Kachites McCallum, Sebastian
Thrun, and Tom Mitchell. 2000. Text Classification
from Labeled and Unlabeled Documents using EM.
Machine Learning - Special issue on information re-
trieval, 39(2-3), May-June.
Hema Raghavan, Omid Madani, and Rosie Jones.
2006. Active Learning with Feedback on Features
and Instances. JMLR, 7:1655?1686, December.
Xiaojin Zhu and Zoubin Ghahramani. 2002. Learning
from Labeled and Unlabeled Data with Label Prop-
agation. Technical report, Carnegie Mellon Univer-
sity.
Jun Zhu, Amr Ahmed, and Eric P. Xing. 2009.
MedLDA: Maximum Margin Supervised Topic
Models for Regression and Classification. In ICML,
pages 1257?1264.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing Semantic Relatedness Using Wikipedia-
based Explicit Semantic Analysis. In IJCAI, pages
1606?1611.
60
Proceedings of the 2013 Workshop on Biomedical Natural Language Processing (BioNLP 2013), pages 120?123,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Parallels between Linguistics and Biology
Ashish Vijay Tendulkar
IIT Madras
Chennai-600 036. India.
ashishvt@gmail.com
Sutanu Chakraborti
IIT Madras
Chennai-600 036. India.
sutanu@cse.iitm.ac.in
Abstract
In this paper we take a fresh look at par-
allels between linguistics and biology. We
expect that this new line of thinking will
propel cross fertilization of two disciplines
and open up new research avenues.
1 Introduction
Protein structure prediction problem is a long
standing open problem in Biology. The compu-
tational methods for structure prediction can be
broadly classified into the following two types:
(i) Ab-initio or de-novo methods seek to model
physics and chemistry of protein folding from first
principles. (ii) Knowledge based methods make
use of existing protein structure and sequence in-
formation to predict the structure of the new pro-
tein. While protein folding takes place at a scale
of millisecond in nature, the computer programs
for the task take a large amount of time. Ab-initio
methods take several hours to days and knowledge
based methods takes several minutes to hours de-
pending upon the complexity. We feel that the
protein structure prediction methods struggle due
to lack of understanding of the folding code from
protein sequence. In larger context, we are in-
terested in the following question: Can we treat
biological sequences as strings generated from a
specific but unknown language and find the rules
of these languages? This is a deep question and
hence we start with baby-steps by drawing par-
allels between Natural Language and Biological
systems. David Searls has done interesting work
in this direction and have written a number of
articles about role of language in understanding
Biological sequences(Searls, 2002). We intend
to build on top of that work and explore further
analogies between the two fields.
This is intended to be an idea paper that ex-
plores parallels between linguistics and biology
that have the potential to cross fertilization two
disciplines and open up new research avenues.
The paper is intentionally made speculative at
places to inspire out-of-the-box deliberations from
researchers in both areas.
2 Analogies
In this section, we explore some pivotal ideas in
linguistics (with a specific focus on Computational
Linguistics) and systematically uncover analogous
ideas in Biology.
2.1 Letters
The alphabet in a natural language is well speci-
fied. English language has 26 letters. The genes
are made up of 4 basic elements called as nu-
cleotide: adenine (A), thymine (T), cytosine (C)
and guanine (G). During protein synthesis, genes
are transcribed into messenger RNA (mRNA),
which is made up of 4 basic elements: adenine
(A), uracil (U), cytosine (C) and guanine (G).
mRNA is translated to proteins that are made up
of 20 amino acids denotes by the following letters:
{A, C, D, E, F, G, H, I, K, L, M, N, P, Q, R, S, T,
V, W, Y}.
2.2 Words
A word is an atomic unit of meaning in a language.
When it comes to biological sequences, a funda-
mental problem is to identify words. Like English,
the biological language seems to have a fixed al-
phabet when it comes to letters. However, unless
we have a mechanism to identify atomic ?func-
tional? units, we cannot construct a vocabulary of
biological words.
The first property of a word in NL is that it has
a meaning; a word is a surrogate for something
in the material or the abstract world. One cen-
tral question is: how do we make machines un-
derstand meanings of words? Humans use dictio-
naries which explain meanings of complex words
120
in terms of simple ones. For machines to use dic-
tionaries, we have two problems. The first is, how
do we communicate the meaning of simple words
(like ?red? or ?sad?)? The second is, to under-
stand meanings of complex words out of simple
ones, we would need the machine to understand
English in the first place. The first problem has
no easy solution; there are words whose meanings
are expressed better in the form of images or when
contrasted with other words (?orange? versus ?yel-
low?). The second problem of defining words in
terms of others can be addressed using a knowl-
edge representation formalism like a semantic net-
work. Some biological words have functions that
cannot be easily expressed in terms of functions
of other words. For the other words, we can define
the function (semantics) of a biological word in
terms of other biological words, leading to a dic-
tionary or ontology of such words.
The second property of a word is its Part of
Speech which dictates the suitability of words to
tie up with each other to give rise to grammatical
sentences. An analogy can be drawn to valency
of atoms, which is primarily responsible in dictat-
ing which molecules are possible and which are
not. Biological words may have Parts of speech
that dictate their ability to group together to form
higher level units like sentences, using the compo-
sition of functions which has its analog in compo-
sitional semantics. The third property of a word
is its morphology, which is its structure or form.
This refers to the sequence of letters in the words.
There are systematic ways in which the form of a
root word (like sing) can be changed to give birth
to new words (like singing). Two primary pro-
cesses are inflection and derivation. This can be
related to mutations in Biology, where we obtain a
new sequence or structure by mutating the existing
sequences/structures.
3 Concepts
Effective Dimensionality: The Vector Space
Model (VSM) is used frequently as a formalism
in Information Retrieval. When used over a large
collection of documents as in the web, VSM pic-
tures the webpages as vectors in a high dimen-
sional vector space, where each dimension corre-
sponds to a word. Interestingly, thanks to strong
clustering properties exhibited by documents, this
high dimensional space is only sparsely populated
by real world documents. As an example to il-
lustrate this, we would not expect a webpage to si-
multaneously talk about Margaret Thatcher, Diego
Maradona and Machine Learning. Thus, more of-
ten than not, the space defined by intersection of
two or more words is empty. The webspace is like
the night sky: mostly dark and few clusters sprin-
kled in between. In IR parlance, we say that the
effective dimensionality of the space is much less
than the true dimensionality, and this fact can be
exploited cleverly to overcome ?curse of dimen-
sionality? and to speed up retrieval. It is worth
noting that the world of biological sequences is
not very different. Of all the sequences that can
be potentially generated, only a few correspond to
stable configurations.
Ramachandran plot is used to understand con-
straints in protein conformations (Ramachandran,
1963). It plots possible ? ? ? angle pairs in pro-
tein structures based on the van der Waal radii of
amino acids. It demonstrates that the protein con-
formational space is sparse and is concentrated in
clusters of a few ?? ? regions.
3.1 Machine Translation
Genes and mRNAs can be viewed as strings gen-
erated from four letters (A,T,C,G for genes and
A,U,C,G for mRNAs). Proteins can be viewed
as strings generated from twenty amino acids. In
addition proteins and mRNAs have correspond-
ing structures for which we do not even know
the alphabets. The genes are storing a blue-print
for synthesizing proteins. Whenever the cell re-
quires a specific protein, the protein synthesis
takes place, in which first the genes encoding that
protein are read and are transcribed into mRNA
which are then translated to make proteins with
relevant amino acids. This is similar to writing the
same document in multiple languages so that it can
be consumed by the people familiar to different
languages. Here the protein sequence is encoded
in genes and is communicated in form of mRNA
during the synthesis process. Another example is
sequence and structure representations of protein:
Both of them carry the same information specified
in different forms.
3.2 Evolution of Languages
Language evolves over time to cater to evolution
in our communication goals. New concepts orig-
inate which warrant revisions to our vocabulary.
The language of mathematics has evolved to make
communication more precise. Sentence structures
121
evolve, often to address the bottlenecks faced by
native speakers and second language learners. En-
glish, for example, has gone out of fashion. Thus
there is a survival goal very closely coupled to the
environment in which a language thrives that dic-
tates its evolution. The situation is not very differ-
ent in biology.
Scientific community believes that the life on
the Earth started with prokaryotes1 and evolved
into eukaryotes. Prokaryotes inhibited earth from
approximately 3-4 Billion years ago. About 500
million years ago, plant and fungi colonized the
Earth. The modern human came into existence
since 250,000 years. At a genetic level, new
genes were formed by means of insertion, dele-
tion and mutation of certain nucleotide with other
nucleotides.
3.3 Garden Path Sentences
English is replete with examples where a small
change in a sentence leads to a significant change
in its meaning. A case in point is the sen-
tence ?He eats shoots and leaves?, whose meaning
changes drastically when a comma is inserted be-
tween ?eats? and ?shoots?. This leads to situations
where the meaning of a sentence cannot be com-
posed by a linear composition of the meanings of
words. The situation is not very different in biol-
ogy, where the function of a sequence can change
when any one element in the sequence changed.
3.4 Text and background knowledge needed
to understand it
Interaction between the ?book? and the reader is
essential to comprehension; so language under-
standing is not just sophisticated modeling of in-
teraction between words, sentences and discourse.
Similarly the book of life (the gene sequence) does
not have everything that is needed to determine
function; it needs to be read by the reader (played
by the CD player). This phenomenon is similar
to protein/ gene interaction. Proteins/genes pos-
sess binding sites, that is used to bind other pro-
teins/genes to form a complex, which carry out the
desired function in the biological process.
3.5 Complexity of Dataset
Several measures have been proposed in the con-
text of Information Retrieval and Text Classifica-
tion which aim at capturing the complexity of a
1http://www.wikipedia.org
dataset. In unsupervised domains, a high clus-
tering tendency indicates a low complexity and
a low clustering tendency corresponds to a situ-
ation where the objects are spread out more or
less uniformly in space. The latter situation cor-
responds to high complexity. In supervised do-
mains, a dataset is said to be complex if objects
that are similar to each other have same category
labels. Interestingly, these ideas may apply in ar-
riving at estimates of structure complexity. In par-
ticular, weak structure function correspondences
would correspond to high complexity.
3.6 Stop words (function words) and their
role in syntax
Function words such as articles, prepositions play
an important role in understanding natural lan-
guages. On the same note, function words exist
in Biology and they play various important roles
depending on the context. For example, Protein
structures are made up of secondary structures.
Around 70% of these structures are ?-helix and
?-strands which repeat in functionally unrelated
proteins. Based on this criterion, ?-helix and ?-
strands can be categorized as functional words.
These secondary structures are important in form-
ing protein structural frame on which functional
sites can be mounted. At genomic level, as much
as 97% of human genome does not code for pro-
teins and hence termed as junk DNA. This is an-
other instance of function word in Biology. Scien-
tists are realizing off late some important functions
of these junk DNA such as their role in alternative
splicing.
3.7 Natural Language Generation
Natural Language Generation (NLG) is com-
plementary to Natural Language Understanding
(NLU), in that it aims at constructing natural lan-
guage text from a variety of non-textual repre-
sentations like maps, graphs, tables and tempo-
ral data. NLG can be used to automate routine
tasks like generation of memos, letters or simula-
tion reports. At the creative end of the spectrum,
an ambitious goal of NLG would be to compose
jokes, advertisements, stories and poetry. NLG
is carried out in four steps: (i) macroplanning;
(ii)microplanning; (iii) surface realization and (iv)
presentation. Macroplanning step uses Rhetorical
Structure Theory (RST), which defines relations
between units of text. For example, the relation
cause connects the two sentences: ?The hotel was
122
costly.? and ?We started looking for a cheaper op-
tion?. Other such relations are purpose, motivation
and enablement. The text is organized into two
segments; the first is called nucleus, which car-
ries the most important information, and the sec-
ond satellites, which provide a flesh around the nu-
cleus. It seems interesting to look for a parallel of
RST in the biological context.
Analogously protein design or artificial life de-
sign is a form of NLG in Biology. Such artifi-
cial organisms and genes/proteins can carry out
specific tasks such as fuel production, making
medicines and combating global warming. For ex-
ample, Craig Venter and colleagues created syn-
thetic genome in the lab and has filed a patent for
the first life form created by humanity. These tasks
are very similar to NLG in terms of scale and com-
plexity.
3.8 Hyperlinks
Hyperlinks connect two or more documents
through links. There is an analogy in Biology
for hyperlinks. Proteins contain sites to bind with
other molecules such as proteins, DNA, metals
or any other chemical compound. The binding
sites are similar to hyperlinks and enable protein-
protein interaction and protein-DNA interaction.
3.9 Ambiguity and Context
An NLP system must be able to effectively handle
ambiguities. The news headline ?Stolen Painting
Found by Tree? has two possible interpretations,
though an average reader has no trouble favoring
one over the other. In many situations, the con-
text is useful in disambiguation. For example, pro-
tein function can be specified unambiguously with
the help of biological process and cellular loca-
tion. In other words, protein functions in the con-
text of biological process and within a particular
cellular location. In the context of protein struc-
ture, highly similar subsequences take different
substructures such as ?-helix or ?-strand depend-
ing on their spatial neighborhood. Moonlighting
proteins carry out multiple functions and their ex-
act function can be determined only based on the
context.
Let us consider the following example: ?Mary
ordered a pizza. She left a tip before leaving the
restaurant.? To understand the above sentences,
the reader must have knowledge of what people
typically do when they visit restaurants. Statisti-
cally mined associations and linguistic knowledge
are both inadequate in capturing meaning when
the background knowledge is absent. Background
knowledge about function and interacting partners
about a protein help in determining its structures.
4 Conclusion
In this paper, we presented a number of parallels
between Linguistics and Biology. We believe that
this line of thought process will lead to previously
unexplored research directions and bring in new
insights in our understanding of biological sys-
tems. Linguistics on other hand can also benefit
from a deeper understanding of analogies with bi-
ological systems.
Acknowledgments
AVT is supported by Innovative Young Biotech-
nologist Award (IYBA) by Department of
Biotechnology, Government of India.
References
David B. Searls. 2002. The language of genes Nature,
420:211?217.
G. N. Ramachandran, C. Ramakrishnan, and
V. Sasisekharan. 1963. Stereochemistry of
polypeptide chain configurations Journal of
Molecular Biology, 7:95?99.
123
