Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 202?206,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Fitting Sentence Level Translation Evaluation
with Many Dense Features
Milo
?
s Stanojevi
?
c and Khalil Sima?an
Institute for Logic, Language and Computation
University of Amsterdam
Science Park 107, 1098 XG Amsterdam, The Netherlands
{m.stanojevic,k.simaan}@uva.nl
Abstract
Sentence level evaluation in MT has turned out
far more difficult than corpus level evaluation.
Existing sentence level metrics employ a lim-
ited set of features, most of which are rather
sparse at the sentence level, and their intricate
models are rarely trained for ranking. This pa-
per presents a simple linear model exploiting
33 relatively dense features, some of which are
novel while others are known but seldom used,
and train it under the learning-to-rank frame-
work. We evaluate our metric on the stan-
dard WMT12 data showing that it outperforms
the strong baseline METEOR. We also ana-
lyze the contribution of individual features and
the choice of training data, language-pair vs.
target-language data, providing new insights
into this task.
1 Introduction
Evaluating machine translation (MT) output at the sen-
tence/ segment level has turned out far more challeng-
ing than corpus/ system level. Yet, sentence level
evaluation can be useful because it allows fast, fine-
grained analysis of system performance on individual
sentences.
It is instructive to contrast two widely used metrics,
METEOR (Michael Denkowski and Alon Lavie, 2014)
and BLEU (Papineni et al., 2002), on sentence level
evaluation. METEOR constantly shows better corre-
lation with human ranking than BLEU (Papineni et
al., 2002). Arguably, this shows that sentence level
evaluation demands finer grained and trainable models
over less sparse features. Ngrams, the core of BLEU,
are sparse at the sentence level, and a mismatch for
longer ngrams implies that BLEU falls back on shorter
ngrams. In contrast, METEOR has a trainable model
and incorporates a small, yet wider set of features that
are less sparse than ngrams. We think that METEOR?s
features and its training approach only suggest that sen-
tence level evaluation should be treated as a modelling
challenge. This calls for questions such as what model,
what features and what training objective are better
suited for modelling sentence level evaluation.
We start out by explicitly formulating sentence level
evaluation as the problem of ranking a set of compet-
ing hypothesis. Given data consisting of human ranked
system outputs, the problem then is to formulate an
easy to train model for ranking. One particular exist-
ing approach (Ye et al., 2007) looks especially attrac-
tive because we think it meshes well with a range of
effective techniques for learning-to-rank (Li, 2011).
We deliberately select a linear modelling approach
inspired by RankSVM (Herbrich et al., 1999), which is
easily trainable for ranking and allows analysis of the
individual contributions of features. Besides presenting
a new metric and a set of known, but also a set of novel
features, we target three questions of interest to the MT
community:
? What kind of features are more helpful for sen-
tence level evaluation?
? How does a simple linear model trained for rank-
ing compare to the well-developed metric ME-
TEOR on sentence level evaluation?
? Should we train the model for each language pair
separately or for a target language?
Our new metric dubbed BEER
1
outperforms ME-
TEOR on WMT12 data showing the effectiveness of
dense features in a learning-to-rank framework. The
metric and the code are available as free software
2
.
2 Model
Our model is a linear combination of features trained
for ranking similar to RankSVM (Herbrich et al., 1999)
or, to readers familiar with SMT system tuning, to PRO
tuning (Hopkins and May, 2011):
score(sys) = ~w ? ~x
sys
where ~w represents a weight vector and ~x
sys
a vec-
tor of feature values for system output sys. Look-
ing at evaluation as a ranking problem, we con-
trast (at least) two system translations good and
bad for the same source sentence. Assuming that
humanRank(good) > humanRank(bad) as ranked
1
BEER participated on WMT14 evaluation metrics task
where it was the highest scoring sentence level evaluation
metric on average over all language pairs (Stanojevi?c and
Sima?an, 2014)
2
https://github.com/stanojevic/beer
202
by human judgement, we expect metric score(?) to ful-
fill score(good) > score(bad):
~w ? ~x
good
> ~w ? ~x
bad
?
~w ? ~x
good
? ~w ? ~x
bad
> 0 ?
~w ? (~x
good
? ~x
bad
) > 0 ?
~w ? (~x
bad
? ~x
good
) < 0
The two feature vectors (~x
good
? ~x
bad
) and (~x
bad
?
~x
good
) can be considered as positive and negative in-
stances for training our linear classifier. For training
this model, we use Logistic Regression from the Weka
toolkit (Hall et al., 2009).
3 Features
Generally speaking we identify adequacy and fluency
features. For both types we devise far less sparse fea-
tures than word ngrams.
Adequacy features We use precision P , recallR and
F1-score F as follows:
P
func
, R
func
, F
func
on matched function words
P
cont
, R
cont
, F
cont
on matched content words
P
all
, R
all
, F
all
on matched words of any type
P
char
, R
char
, F
char
matching of the char ngrams
By differentiating between function and non-function
words, our metric weighs each kind of words accord-
ing to importance for evaluation. Matching character
ngrams, originally proposed in (Yang et al., 2013), re-
wards certain translations even if they did not get the
morphology completely right. Existing metrics use
stemmers for this, but using character ngrams is inde-
pendent of the availability of a good quality stemmer.
Higher-order character ngrams have less risk of sparse
counts than word ngrams. In our experiments we used
char ngrams for n up to 6, which makes the total num-
ber of adequacy features 27.
Fluency features To evaluate word order we follow
(Isozaki et al., 2010; Birch and Osborne, 2010) in rep-
resenting reordering as a permutation pi over [1..n] and
then measuring the distance to the ideal monotone per-
mutation ?1, 2, ? ? ? , n?. We present a novel approach
based on factorization into permutation trees (PETs)
(Zhang and Gildea, 2007), and contrast it with Kendall
? (Birch and Osborne, 2010; Isozaki et al., 2010). PETs
are factorizations of permutations, which allows for an
abstract and less sparse view of word order as exempli-
fied next. Kendall score was regularly shown to have
high correlation with human judgment on distant lan-
guage pairs (Isozaki et al., 2010; Birch and Osborne,
2010).
Features based on PETs We informally review
PETs in order to exploit them for novel ordering fea-
tures. We refer the reader to (Zhang and Gildea, 2007)
and (Maillette de Buy Wenniger and Sima?an, 2011)
for a formal treatment of PETs and efficient factoriza-
tion algorithms.
A PET of permutation pi is a tree organization of pi?s
unique, atomic building blocks, called operators. Ev-
ery operator on a PET node is an atomic permutation
(not factorizing any further),
3
and it stands for the per-
mutation of the direct children of that node. Figure 1a
shows an example PET that has one 4-branching node
with operator ?2, 4, 1, 3?, two binary branching nodes
of which one decorated with the inverted operator ?2, 1?
and another with the monotone ?1, 2?.
PETs have two important properties making them at-
tractive for measuring order difference: firstly, order
difference is measured on the operators ? the atomic
reordering building blocks of the permutation, and sec-
ondly, the operators on higher level nodes capture hid-
den ordering patterns that cannot be observed without
factorization. Statistics over ordering patterns in PETs
are far less sparse than word or character ngram statis-
tics.
Intuitively, among the atomic permutations, the bi-
nary monotone operator ?1, 2? signifies no ordering dif-
ference at all, whereas the binary inverted ?2, 1? signi-
fies the shortest unit of order difference. Operators of
length four like ?2, 4, 1, 3? (Wu, 1997) are presumably
more complex than ?2, 1?, whereas operators longer
than four signify even more complex order difference.
Therefore, we devise possible branching feature func-
tions over the operator length for the nodes in PETs:
? factor 2 - with two features: ?
[ ]
and ?
<>
(there
are no nodes with factor 3 (Wu, 1997))
? factor 4 - feature ?
=4
? factor bigger than 4 - feature ?
>4
Consider permutations ?2, 1, 4, 3? and ?4, 3, 2, 1?, none
of which has exactly matching ngrams beyond uni-
grams. Their PETs are in Figures 1b and 1c. Intuitively,
?2, 1, 4, 3? is somewhat less scrambled than ?4, 3, 2, 1?
because it has at least some position in correct order.
These ?abstract ngrams? pertaining to correct order-
ing of full phrases could be counted using ?
[ ]
which
would recognize that on top of the PET in 1b there is
a binary monotone node, unlike the PET in Figure 1c
which has no monotone nodes at all.
Even though the set of operators that describe a per-
mutation is unique for the given permutation, the ways
in which operators are combined (the derivation tree)
is not unique. For example, for the fully monotone
3
For example ?2, 4, 1, 3? is atomic whereas ?4, 3, 2, 1? is
not. The former does not contain any contiguous sub-ranges
of integers whereas the latter contains sub-range {2, 3, 4} in
reverse order ?4, 3, 2?, which factorizes into two binary in-
verting nodes cf. Fig. 1c.
203
?2, 4, 1, 3?
2 ?2, 1?
?1, 2?
5 6
4
1 3
(a) Complex PET
?1, 2?
?2, 1?
2 1
?2, 1?
4 3
(b) PET with inversions
?2, 1?
?2, 1?
?2, 1?
4 3
2
1
(c) Canonical fully
inverted PET
?2, 1?
?2, 1?
4 ?2, 1?
3 2
1
(d) Alternative fully
inverted PET
?2, 1?
?2, 1?
4 3
?2, 1?
2 1
(e) Alternative fully
inverted PET
?2, 1?
4 ?2, 1?
?2, 1?
3 2
1
(f) Alternative fully
inverted PET
?2, 1?
4 ?2, 1?
3 ?2, 1?
2 1
(g) Alternative fully
inverted PET
Figure 1: Examples of PETs
permutation ?4, 3, 2, 1? there are 5 possible derivations
(PETs) presented in Figures 1c, 1d, 1e, 1f and 1g. The
features on PETs that we described so far look at the
operators independently (they treat a derivation as a
set of operators) so differenct derivations do not influ-
ence the score?whichever derviation we use we will
get the same feature score. However, the number of
derivations might say something about the goodness of
the permutation. Similar property of permutations was
found to be helpful earlier in (Mylonakis and Sima?an,
2008) as an ITG prior for learning translation rule prob-
abilities.
Permutations like ?3, 2, 1, 4? and ?2, 4, 3, 1? have the
same set of operators, but the former factorizes into
more PETs than the latter because ?4, 3? must group
first before grouping it with 2 and then 1 in ?2, 4, 3, 1?.
The ?freedom to bracket? in different ways could be a
signal of better grouping of words (even if they have
inverted word order). Hence we exploit one more fea-
ture:
?
count
the ratio between the number of alternative
PETs for the given permutation, to the number of
PETs that could be built if permutation was per-
fectly grouped (fully monotone or fully inverted).
Finding the number of PETs that could be built does
not require building all PETs or encoding them in the
chart. The number can be computed directly from the
canonical left-branching PET. Since multiple different
PETs appear only in cases when there is a sequence of
more than one node that is either ?1, 2? or ?2, 1? (Zhang
et al., 2008), we can use these sequences to predict the
number of PETs that could be built. Let X represent a
set of sequences of the canonical derivation. The num-
ber of PETs is computed in the following way:
#PETs =
?
x?X
Cat(|x|) (1)
Cat(n) =
1
n+ 1
(
2n
n
)
(2)
whereCat(?) is a Catalan number. The proof for this
formula is beyond the scope of this paper. The reader
can consider the example of the PET in Figure 1c. That
derivation has one sequence of monotone operators of
length 3. So the number of PETs that could be built is
Cat(3) = 5.
4 Experiments
We use human judgments from the WMT tasks:
WMT13 is used for training whereas WMT12 for test-
ing. The baseline is METEOR?s latest version (Michael
Denkowski and Alon Lavie, 2014), one of the best met-
rics on sentence level. To avoid contaminating the re-
sults with differences with METEOR due to resources,
we use the same alignment, tokenization and lower-
casing (-norm in METEOR) algorithms, and the same
tables of function words, synonyms, paraphrases and
stemmers.
Kendall ? correlation is borrowed from WMT12
(Callison-Burch et al., 2012):
? =
#concordant?#discordant?#ties
#concordant+ #discordant+ #ties
#concordant represents the number of pairs or-
dered in the same way by metric and by human,
#discordant the number of opposite orderings and
#ties the number of tied rankings by metric.
Beside testing our full metric BEER, we perform ex-
periments where we remove one kind of the following
features at a time:
1. char n-gram features (P, R and F-score)
2. all word features (P, R and F-score for all, function
and content words),
3. all function and content words features
4. all F-scores (all words, function words, content
words, char ngrams)
204
metric en-cs en-fr en-de en-es cs-en fr-en de-en es-en avg ?
BEER without char features 0.124 0.178 0.168 0.149 0.121 0.17 0.179 0.078 0.146
BEER without all word features 0.184 0.237 0.223 0.217 0.192 0.209 0.243 0.199 0.213
BEER without all F-scores 0.197 0.243 0.219 0.22 0.177 0.227 0.254 0.211 0.219
METEOR 0.156 0.252 0.173 0.202 0.208 0.249 0.273 0.246 0.22
BEER without PET features 0.202 0.248 0.243 0.225 0.198 0.249 0.268 0.234 0.233
BEER without function words 0.2 0.245 0.231 0.227 0.189 0.268 0.267 0.253 0.235
BEER without fluency features 0.201 0.248 0.236 0.223 0.202 0.257 0.283 0.243 0.237
BEER without Kendall ? 0.205 0.246 0.244 0.227 0.202 0.257 0.282 0.248 0.239
BEER full 0.206 0.245 0.244 0.23 0.198 0.263 0.283 0.245 0.239
Table 1: Kendall ? scores on WMT12 data
5. PET features
6. Kendall ? features
7. all fluency features (PET and Kendall ? )
Table 1 shows the results sorted by their average
Kendall ? correlation with human judgment.
5 Analysis
Given these experimental results, we are coming back
to the questions we asked in the introduction.
5.1 What kind of features are more helpful for
sentence level evaluation?
Fluency vs. Adequacy The fluency features play a
smaller role than adequacy features. Apparently, many
SMT systems participating in this task have rather sim-
ilar reordering models, trained on similar data, which
makes the fluency features not that discriminative rel-
ative to adequacy features. Perhaps in a different ap-
plication, for example MT system tuning, the reorder-
ing features would be far more relevant because ignor-
ing them would basically imply disregarding the im-
portance of the reordering model in MT.
Character vs. Word features We observe that, pre-
cision, recall and F-score on character ngrams are cru-
cial. We think that this shows that less sparse features
are important for sentence level evaluation. The sec-
ond best features are word features. Without word
features, BEER scores just below METEOR, which
suggests that word boundaries play a role as well. In
contrast, differentiating between function and content
words does not seem to be important.
PETs vs. Kendall ? Despite the smaller role for
reordering features we can make a few observations.
Firstly, while PETs and Kendall seem to have simi-
lar effect on English-Foreign cases, in all four cases of
Foreign-English PETs give better scores. We hypoth-
esize that the quality of the permutations (induced be-
tween system output and reference) is better for English
than for the other target languages. Discarding PET
features has far larger impact than discarding Kendall.
Most interestingly, for de-en it makes the difference
in outperforming METEOR. In many cases discarding
Kendall ? improves the BEER score, likely because it
conflicts with the PET features that are found more ef-
fective.
5.2 Is a linear model sufficient?
A further insight, from our perspective, is that F-score
features constitute a crucial set of features, even when
the corresponding precision and recall features are in-
cluded. Because our model merely allows for linear in-
terpolation, whereas F-score is a non-linear function of
precision and recall, we think this suggests that a non-
linear interpolation of precision and recall is useful.
4
By formulating the evaluation as a ranking problem it is
relatively easy to ?upgrade? for using non-linear mod-
els while using the same (or larger) set of features.
5.3 Train for the language pair or only for the
target language?
All our models were trained for each language pair.
This is not the case with many other metrics which
train their models for each target language instead of
language pair. We contrast these two settings in Table
2. Training for each language pair separately does not
give significant improvement over training for the tar-
get language only. A possible reason could be that by
training for the target language we have more training
data (in this case four times more).
Train for cs-en fr-en de-en es-en avg ?
target lang 0.199 0.257 0.273 0.248 0.244
lang pair 0.198 0.263 0.283 0.245 0.247
Table 2: Kendall ? scores on WMT12 for different
training data
5.4 BEER vs. METEOR
The results across individual language pairs are mostly
consistent with the averages with a few exceptions.
BEER outperforms METEOR in five out of eight lan-
guage pairs, ties at one (the difference is only 0.001 on
es-en) and loses in two (en-fr and cs-en). In some cases
BEER is better than METEOR by a large margin (see,
e.g., en-cs, en-de).
4
Interestingly, METEOR tunes ? in F
?
.
205
6 Conclusion
In this work we show that combining less sparse fea-
tures at the sentence level into a linear model that is
trained on ranking we can obtain state-of-the-art re-
sults. The analysis of the results shows that features on
character ngrams are crucial, besides the standard word
level features. The reordering features, while rather
important, are less effective within this WMT task, al-
beit the more abstract PET features have larger impact
than the often used Kendall. Good performance of F-
score features leads to the conclusion that linear models
might not be sufficient for modeling human sentence
level ranking and to learn the right relation between
precision and recall it could be worthwhile exploring
non-linear models.
Acknowledgments
This work is supported by STW grant nr. 12271 and
NWO VICI grant nr. 277-89-002. We also thank TAUS
and the other DatAptor project User Board members.
References
Alexandra Birch and Miles Osborne. 2010. LRscore
for Evaluating Lexical and Reordering Quality in
MT. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR,
pages 327?332, Uppsala, Sweden, July. Association
for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Ma-
chine Translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montr?eal, Canada, June. Association for
Computational Linguistics.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An
Update. SIGKDD Explor. Newsl., 11(1):10?18,
November.
Ralf Herbrich, Thore Graepel, and Klaus Obermayer.
1999. Support Vector Learning for Ordinal Regres-
sion. In In International Conference on Artificial
Neural Networks, pages 97?102.
Mark Hopkins and Jonathan May. 2011. Tuning as
Ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing,
pages 1352?1362, Edinburgh, Scotland, UK., July.
Association for Computational Linguistics.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010. Automatic
Evaluation of Translation Quality for Distant Lan-
guage Pairs. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP ?10, pages 944?952, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Hang Li. 2011. Learning to Rank for Information Re-
trieval and Natural Language Processing. Synthesis
Lectures on Human Language Technologies. Mor-
gan & Claypool Publishers.
Gideon Maillette de Buy Wenniger and Khalil Sima?an.
2011. Hierarchical Translation Equivalence over
Word Alignments. In ILLC Prepublication Series,
PP-2011-38. University of Amsterdam.
Michael Denkowski and Alon Lavie. 2014. Meteor
Universal: Language Specific Translation Evalua-
tion for Any Target Language. In Proceedings of the
ACL 2014 Workshop on Statistical Machine Transla-
tion.
Markos Mylonakis and Khalil Sima?an. 2008.
Phrase Translation Probabilities with ITG Priors and
Smoothing as Learning Objective. In Proceedings of
the 2008 Conference on Empirical Methods in Natu-
ral Language Processing, pages 630?639, Honolulu,
USA, October. Association for Computational Lin-
guistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method for Automatic
Evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ?02, pages 311?318,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Milo?s Stanojevi?c and Khalil Sima?an. 2014. BEER:
BEtter Evaluation as Ranking. In Proceedings of the
Ninth Workshop on Statistical Machine Translation,
pages 414?419, Baltimore, Maryland, USA, June.
Association for Computational Linguistics.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational linguistics, 23(3):377?403.
Muyun Yang, Junguo Zhu, Sheng Li, and Tiejun Zhao.
2013. Fusion of Word and Letter Based Metrics
for Automatic MT Evaluation. In Proceedings of
the Twenty-Third International Joint Conference on
Artificial Intelligence, IJCAI?13, pages 2204?2210.
AAAI Press.
Yang Ye, Ming Zhou, and Chin-Yew Lin. 2007. Sen-
tence Level Machine Translation Evaluation As a
Ranking Problem: One Step Aside from BLEU. In
Proceedings of the Second Workshop on Statistical
Machine Translation, StatMT ?07, pages 240?247,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Hao Zhang and Daniel Gildea. 2007. Factorization of
synchronous context-free grammars in linear time.
In In NAACL Workshop on Syntax and Structure in
Statistical Translation (SSST.
Hao Zhang, Daniel Gildea, and David Chiang.
2008. Extracting Synchronous Grammar Rules
From Word-Level Alignments in Linear Time. In
Proceedings of the 22nd International Conference
on Computational Linguistics (COLING-08), pages
1081?1088, Manchester, UK.
206
Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 138?147,
October 25, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Evaluating Word Order Recursively over Permutation-Forests
Milo
?
s Stanojevi
?
c and Khalil Sima?an
Institute for Logic, Language and Computation
University of Amsterdam
Science Park 107, 1098 XG Amsterdam, The Netherlands
{m.stanojevic,k.simaan}@uva.nl
Abstract
Automatically evaluating word order of
MT system output at the sentence-level is
challenging. At the sentence-level, ngram
counts are rather sparse which makes it
difficult to measure word order quality ef-
fectively using lexicalized units. Recent
approaches abstract away from lexicaliza-
tion by assigning a score to the permuta-
tion representing how word positions in
system output move around relative to a
reference translation. Metrics over per-
mutations exist (e.g., Kendal tau or Spear-
man Rho) and have been shown to be
useful in earlier work. However, none
of the existing metrics over permutations
groups word positions recursively into
larger phrase-like blocks, which makes it
difficult to account for long-distance re-
ordering phenomena. In this paper we ex-
plore novel metrics computed over Per-
mutation Forests (PEFs), packed charts
of Permutation Trees (PETs), which are
tree decompositions of a permutation into
primitive ordering units. We empirically
compare PEFs metric against five known
reordering metrics on WMT13 data for ten
language pairs. The PEFs metric shows
better correlation with human ranking than
the other metrics almost on all language
pairs. None of the other metrics exhibits
as stable behavior across language pairs.
1 Introduction
Evaluating word order (also reordering) in MT is
one of the main ingredients in automatic MT eval-
uation, e.g., (Papineni et al., 2002; Denkowski
and Lavie, 2011). To monitor progress on eval-
uating reordering, recent work explores dedicated
reordering evaluation metrics, cf. (Birch and Os-
borne, 2011; Isozaki et al., 2010; Talbot et al.,
2011). Existing work computes the correlation be-
tween the ranking of the outputs of different sys-
tems by an evaluation metric to human ranking, on
e.g., the WMT evaluation data.
For evaluating reordering, it is necessary to
word align system output with the correspond-
ing reference translation. For convenience, a 1:1
alignment (a permutation) is induced between the
words on both sides (Birch and Osborne, 2011),
possibly leaving words unaligned on either side.
Existing work then concentrates on defining mea-
sures of reordering over permutations, cf. (Lap-
ata, 2006; Birch and Osborne, 2011; Isozaki et al.,
2010; Talbot et al., 2011). Popular metrics over
permutations are: Kendall?s tau, Spearman, Ham-
ming distance, Ulam and Fuzzy score. These met-
rics treat a permutation as a flat sequence of inte-
gers or blocks, disregarding the possibility of hier-
archical grouping into phrase-like units, making it
difficult to measure long-range order divergence.
Next we will show by example that permutations
also contain latent atomic units that govern the re-
cursive reordering of phrase-like units. Account-
ing for these latent reorderings could actually be
far simpler than the flat view of a permutation.
Isozaki et al. (2010) argue that the conventional
metrics cannot measure well the long distance
reordering between an English reference sentence
?A because B? and a Japanese-English hypothesis
translation ?B because A?, where A and B are
blocks of any length with internal monotonic
alignments. In this paper we explore the idea of
factorizing permutations into permutation-trees
(PETs) (Gildea et al., 2006) and defining new
138
?2,4,1,3?
2 ?1,2?
4 ?1,2?
5 6
1 3
Figure 1: A permutation tree for ?2, 4, 5, 6, 1, 3?
tree-based reordering metrics which aims at
dealing with this type of long range reorderings.
For the Isozaki et al. (2010) Japanese-English
example, there are two PETs (when leaving A and
B as encapsulated blocks):
?2,1?
A ?2,1?
because B
?2,1?
?2,1?
A because
B
Our PET-based metrics interpolate the scores over
the two inversion operators ?2, 1? with the internal
scores for A and B, incorporating a weight
for subtree height. If both A and B are large
blocks, internally monotonically (also known as
straight) aligned, then our measure will not count
every single reordering of a word in A or B,
but will consider this case as block reordering.
From a PET perspective, the distance of the
reordering is far smaller than when looking at a
flat permutation. But does this hierarchical view
of reordering cohere better with human judgement
than string-based metrics?
The example above also shows that a permuta-
tion may factorize into different PETs, each corre-
sponding to a different segmentation of a sentence
pair into phrase-pairs. In this paper we introduce
permutation forests (PEFs); a PEF is a hypergraph
that compactly packs the set of PETs that factorize
a permutation.
There is yet a more profoud reasoning behind
PETs than only accounting for long-range reorder-
ings. The example in Figure 1 gives the flavor of
PETs. Observe how every internal node in this
PET dominates a subtree whose fringe
1
is itself a
permutation over an integer sub-range of the orig-
inal permutation. Every node is decorated with a
permutation over the child positions (called oper-
ator). For example ?4, 5, 6? constitutes a contigu-
ous range of integers (corresponding to a phrase
pair), and hence will be grouped into a subtree;
1
Ordered sequence of leaf nodes.
which in turn can be internally re-grouped into a
binary branching subtree. Every node in a PET is
minimum branching, i.e., the permutation factor-
izes into a minimum number of adjacent permuta-
tions over integer sub-ranges (Albert and Atkin-
son, 2005). The node operators in a PET are
known to be the atomic building blocks of all per-
mutations (called primal permutations). Because
these are building atomic units of reordering, it
makes sense to want to measure reordering as a
function of the individual cost of these operators.
In this work we propose to compute new reorder-
ing measures that aggregate over the individual
node-permutations in these PETs.
While PETs where exploited rather recently for
extracting features used in the BEER metric sys-
tem description (Stanojevi?c and Sima?an, 2014) in
the official WMT 2014 competition, this work is
the first to propose integral recursive metrics over
PETs and PEFs solely for measuring reordering
(as opposed to individual non-recursive features in
a full metric that measures at the same time both
fluency and adequacy). We empirically show that
a PEF-based evaluation measure correlates better
with human rankings than the string-based mea-
sures on eight of the ten language pairs in WMT13
data. For the 9
th
language pair it is close to best,
and for the 10
th
(English-Czech) we find a likely
explanation in the Findings of the 2013 WMT (Bo-
jar et al., 2013). Crucially, the PEF-based mea-
sure shows more stable ranking across language
pairs than any of the other measures. The metric
is available online as free software
2
.
2 Measures on permutations: Baselines
In (Birch and Osborne, 2010; Birch and Osborne,
2011) Kendall?s tau and Hamming distance are
combined with unigram BLEU (BLEU-1) leading
to LRscore showing better correlation with human
judgment than BLEU-4. Birch et al. (2010) ad-
ditionally tests Ulam distance (longest common
subsequence ? LCS ? normalized by the permu-
tation length) and the square root of Kendall?s tau.
Isozaki et al. (2010) presents a similar approach
to (Birch and Osborne, 2011) additionally test-
ing Spearman rho as a distance measure. Talbot
et al. (2011) extracts a reordering measure from
METEOR (Denkowski and Lavie, 2011) dubbed
Fuzzy Reordering Score and evaluates it on MT
reordering quality.
2
https://github.com/stanojevic/beer
139
For an evaluation metric we need a function
which would have the standard behaviour of evalu-
ation metrics - the higher the score the better. Bel-
low we define the baseline metrics that were used
in our experiments.
Baselines A permutation over [1..n] (subrange
of the positive integers where n > 1) is a bijective
function from [1..n] to itself. To represent permu-
tations we will use angle brackets as in ?2, 4, 3, 1?.
Given a permutation pi over [1..n], the notation pi
i
(1 ? i ? n) stands for the integer in the i
th
posi-
tion in pi; pi(i) stands for the index of the position
in pi where integer i appears; and pi
j
i
stands for the
(contiguous) sub-sequence of integers pi
i
, . . . pi
j
.
The definitions of five commonly used met-
rics over permutations are shown in Figure 2.
In these definitions, we use LCS to stand for
Longest Common Subsequence, and Kronecker
?[a] which is 1 if (a == true) else zero, and
A
n
1
= ?1, ? ? ? , n? which is the identity permuta-
tion over [1..n]. We note that all existing metrics
kendall(pi) =
?
n?1
i=1
?
n
j=i+1
?[pi(i) < pi(j)]
(n
2
? n)/2
hamming(pi) =
?
n
i=1
?[pi
i
== i]
n
spearman(pi) = 1?
3
?
n
i=1
(pi
i
? i)
2
n(n
2
? 1)
ulam(pi) =
LCS(pi,A
n
1
)? 1
n? 1
fuzzy(pi) = 1?
c? 1
n? 1
where c is # of monotone sub-permutations
Figure 2: Five commonly used metrics over per-
mutations
are defined directly over flat string-level permuta-
tions. In the next section we present an alternative
view of permutations are compositional, recursive
tree structures.
3 Measures on Permutation Forests
Existing work, e.g., (Gildea et al., 2006), shows
how to factorize any permutation pi over [1..n]
into a canonical permutation tree (PET). Here we
will summarize the relevant aspects and extend
PETs to permutation forests (PEFs).
A non-empty sub-sequence pi
j
i
of a permutation
pi is isomorphic with a permutation over [1..(j ?
i + 1)] iff the set {pi
i
, . . . , pi
j
} is a contiguous
range of positive integers. We will use the term
a sub-permutation of pi to refer to a subsequence
of pi that is isomorphic with a permutation. Note
that not every subsequence of a permutation pi is
necessarily isomorphic with a permutation, e.g.,
the subsequence ?3, 5? of ?1, 2, 3, 5, 4? is not a
sub-permutation. One sub-permutation pi
1
of pi is
smaller than another sub-permutation pi
2
of pi iff
every integer in pi
1
is smaller than all integers in
pi
2
. In this sense we can put a full order on non-
overlapping sub-permutations of pi and rank them
from the smallest to the largest.
For every permutation pi there is a minimum
number of adjacent sub-permutations it can be fac-
torized into (see e.g., (Gildea et al., 2006)). We
will call this minimum number the arity of pi and
denote it with a(pi) (or simply a when pi is un-
derstood from the context). For example, the arity
of pi = ?5, 7, 4, 6, 3, 1, 2? is a = 2 because it can
be split into a minimum of two sub-permutations
(Figure 3), e.g. ?5, 7, 4, 6, 3? and ?1, 2? (but alter-
natively also ?5, 7, 4, 6? and ?3, 1, 2?). In contrast,
pi = ?2, 4, 1, 3? (also known as the Wu (1997) per-
mutation) cannot be split into less than four sub-
permutations, i.e., a = 4. Factorization can be
applied recursively to the sub-permutations of pi,
resulting in a tree structure (see Figure 3) called a
permutation tree (PET) (Gildea et al., 2006; Zhang
and Gildea, 2007; Maillette de Buy Wenniger and
Sima?an, 2011).
Some permutations factorize into multiple alter-
native PETs. For pi = ?4, 3, 2, 1? there are five
PETs shown in Figure 3. The alternative PETs
can be packed into an O(n
2
) permutation forest
(PEF). For many computational purposes, a sin-
gle canonical PET is sufficient, cf. (Gildea et al.,
2006). However, while different PETs of pi exhibit
the same reordering pattern, their different binary
branching structures might indicate important dif-
ferences as we show in our experiments.
A permutation forest (akin to a parse forest)
F for pi (over [1..n]) is a data structure consisting
of a subset of {[[i, j, I
j
i
, O
j
i
]] | 0 ? i ? j ? n},
where I
j
i
is a (possibly empty) set of inferences
(sets of split points) for pi
j
i+1
and O
j
i
is an oper-
ator shared by all inferences of pi
j
i+1
. If pi
j
i+1
is
a sub-permutation and it has arity a ? (j ? (i +
140
?2,1?
?2,1?
?2,4,1,3?
5 7 4 6
3
?1,2?
1 2
?2,1?
4 ?2,1?
3 ?2,1?
2 1
?2,1?
4 ?2,1?
?2,1?
3 2
1
?2,1?
?2,1?
4 3
?2,1?
2 1
?2,1?
?2,1?
?2,1?
4 3
2
1
?2,1?
?2,1?
4 ?2,1?
3 2
1
Figure 3: A PET for pi = ?5, 7, 4, 6, 3, 1, 2?. And five different PETs for pi = ?4, 3, 2, 1?.
1)), then each inference consists of a a ? 1-tuple
[l
1
, . . . , l
a?1
], where for each 1 ? x ? (a? 1), l
x
is a ?split point? which is given by the index of the
last integer in the x
th
sub-permutation in pi. The
permutation of the a sub-permutations (?children?
of pi
j
i+1
) is stored in O
j
i
and it is the same for all
inferences of that span (Zhang et al., 2008).
?2,1?
4
3 2 1
?2,1?
4 3 2 1
?2,1?
4 3 2
1
Figure 4: The factorizations of pi = ?4, 3, 2, 1?.
Let us exemplify the inferences on pi =
?4, 3, 2, 1? (see Figure 4) which factorizes into
pairs of sub-permutations (a = 2): a split point
can be at positions with index l
1
? {1, 2, 3}.
Each of these split points (factorizations) of pi will
be represented as an inference for the same root
node which covers the whole of pi (placed in entry
[0, 4]); the operator of the inference here consists
of the permutation ?2, 1? (swapping the two ranges
covered by the children sub-permutations) and in-
ference consists of a? 1 indexes l
1
, . . . , l
a?1
sig-
nifying the split points of pi into sub-permutations:
since a = 2 for pi, then a single index l
1
?
{1, 2, 3} is stored with every inference. For the
factorization ((4, 3), (2, 1)) the index l
1
= 2 sig-
nifying that the second position is a split point into
?4, 3? (stored in entry [0, 2]) and ?2, 1? (stored in
entry [2, 4]). For the other factorizations of pi sim-
ilar inferences are stored in the permutation forest.
Figure 5 shows a simple top-down factorization
algorithm which starts out by computing the ar-
ity a using function a(pi). If a = 1, a single leaf
node is stored with an empty set of inferences. If
a > 1 then the algorithm computes all possible
factorizations of pi into a sub-permutations (a se-
quence of a? 1 split points) and stores their infer-
ences together as I
j
i
and their operator O
j
i
asso-
ciated with a node in entry [[i, j, I
j
i
, O
j
i
]]. Subse-
quently, the algorithm applies recursively to each
sub-permutation. Efficiency is a topic beyond
the scope of this paper, but this naive algorithm
has worst case time complexity O(n
3
), and when
computing only a single canonical PET this can be
O(n) (see e.g., (Zhang and Gildea, 2007)).
Function PEF (i, j, pi,F);
# Args: sub-perm. pi over [i..j] and forest F
Output: Parse-Forest F(pi) for pi;
begin
if ([[i, j, ?]] ? F) then return F ; #memoization
a := a(pi);
if a = 1 return F := F ? {[[i, j, ?]]};
For each set of split points {l
1
, . . . , l
a?1
} do
O
j
i
:= RankListOf(pi
l
1
(l
0
+1)
, pi
l
2
(l
1
+1)
, . . . , pi
l
a
(l
a?1
+1)
);
I
j
i
:= I
j
i
? [l
1
, . . . , l
a?1
];
For each pi
v
? {pi
l
1
l
0
+1
, pi
l
2
(l
1
+1)
, . . . , pi
l
a
(l
a?1
+1)
} do
F := F ? PermForest(pi
v
);
F := F ? {[[i, j, I
j
i
, O
j
i
]]};
Return F ;
end;
Figure 5: Pseudo-code of permutation-forest fac-
torization algorithm. Function a(pi) returns the ar-
ity of pi. Function RankListOf(r
1
, . . . , r
m
) re-
turns the list of rank positions (i.e., a permutation)
of sub-permutations r
1
, . . . , r
m
after sorting them
smallest first. The top-level call to this algorithm
uses pi, i = 0, j = n and F = ?.
Our measure (PEFscore) uses a function
opScore(p) which assigns a score to a given oper-
ator, which can be instantiated to any of the exist-
ing scoring measures listed in Section 2, but in this
case we opted for a very simple function which
gives score 1 to monotone permutation and score
0 to any other permutation.
Given an inference l ? I
j
i
where l =
[l
1
, . . . , l
a?1
], we will use the notation l
x
to refer
to split point l
x
in l where 1 ? x ? (a ? 1), with
the convenient boundary assumption that l
0
= i
and l
a
= j.
141
PEFscore(pi) = ?
node
(0, n, PEF (pi))
?
node
(i, j,F) =
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
if (I
j
i
== ?) then 1
else if (a(pi
j
i+1
) = j ? i) then opScore(O
j
i
)
else ? ? opScore(O
j
i
) + (1? ?)?
?
l?I
j
i
?
inf
(l,F ,a(pi
j
i+1
))
|I
j
i
|
? ?? ?
Avg. inference score over I
j
i
?
inf
(l,F , a) =
?
a
x=1
?[l
x
?l
x?1
>1]??
node
(l
(x?1)
,l
x
,F)
?
a
x=1
?[l
x
?l
(x?1)
>1]
? ?? ?
Avg. score for non-terminal children
opScore(p) =
{
if (p == ?1, 2?) then 1
else 0
Figure 6: The PEF Score
The PEF-score, PEFscore(pi) in Figure 6,
computes a score for the single root node
[[0, n, I
n
0
, O
n
0
]]) in the permutation forest. This
score is the average inference score ?
inf
over all
inferences of this node. The score of an inference
?
inf
interpolates (?) between the opScore of the
operator in the current span and (1? ?) the scores
of each child node. The interpolation parameter ?
can be tuned on a development set.
The PET-score (single PET) is a simplification
of the PEF-score where the summation over all in-
ferences of a node
?
l?I
j
i
in ?
node
is replaced by
?Select a canonical l ? I
j
i
?.
4 Experimental setting
Data The data that was used for experiments are
human rankings of translations from WMT13 (Bo-
jar et al., 2013). The data covers 10 language pairs
with a diverse set of systems used for translation.
Each human evaluator was presented with 5 differ-
ent translations, source sentence and a reference
translation and asked to rank system translations
by their quality (ties were allowed).
3
Meta-evaluation The standard way for doing
meta-evaluation on the sentence level is with
Kendall?s tau correlation coefficient (Callison-
Burch et al., 2012) computed on the number of
times an evaluation metric and a human evaluator
agree (and disagree) on the rankings of pairs of
3
We would like to extend our work also to English-
Japanese but we do not have access to such data at the mo-
ment. In any case, the WMT13 data is the largest publicly
available data of this kind.
translations. We extract pairs of translations from
human evaluated data and compute their scores
with all metrics. If the ranking assigned by a met-
ric is the same as the ranking assigned by a hu-
man evaluator then that pair is considered concor-
dant, otherwise it is a discordant pair. All pairs
which have the same score by the metric or are
judged as ties by human evaluators are not used
in meta-evaluation. The formula that was used for
computing Kendall?s tau correlation coefficient is
shown in Equation 1. Note that the formula for
Kendall tau rank correlation coefficient that is used
in meta-evaluation is different from the Kendall
tau similarity function used for evaluating permu-
tations. The values that it returns are in the range
[?1, 1], where ?1 means that order is always op-
posite from the human judgment while the value 1
means that metric ranks the system translations in
the same way as humans do.
? =
#concordant pairs?#discordant pairs
#concordant pairs+#discordant pairs
(1)
Evaluating reordering Since system transla-
tions do not differ only in the word order but also
in lexical choice, we follow Birch and Osborne
(2010) and interpolate the score given by each re-
ordering metric with the same lexical score. For
lexical scoring we use unigram BLEU. The param-
eter that balances the weights for these two metrics
? is chosen to be 0.5 so it would not underesti-
mate the lexical differences between translations
(?  0.5) but also would not turn the whole met-
ric into unigram BLEU (?  0.5). The equation
142
for this interpolation is shown in Equation 2.
4
FullMetric(ref, sys) = ? lexical(ref, sys) +
(1? ?)? bp(|ref |, |pi|)? ordering(pi) (2)
Where pi(ref, sys) is the permutation represent-
ing the word alignment from sys to ref . The ef-
fect of ? on the German-English evaluation is vis-
ible on Figure 7. The PET and PEF measures have
an extra parameter ? that gives importance to the
long distance errors that also needs to be tuned. On
Figure 8 we can see the effect of ? on German-
English for ? = 0.5. For all language pairs for
? = 0.6 both PETs and PEFs get good results so
we picked that as value for ? in our experiments.
Figure 7: Effect of ? on German-English evalua-
tion for ? = 0.6
Choice of word alignments The issue we did
not discuss so far is how to find a permutation
from system and reference translations. One way
is to first get alignments between the source sen-
tence and the system translation (from a decoder
or by automatically aligning sentences), and also
alignments between the source sentence and the
reference translation (manually or automatically
aligned). Subsequently we must make those align-
ments 1-to-1 and merge them into a permutation.
That is the approach that was followed in previ-
ous work (Birch and Osborne, 2011; Talbot et al.,
4
Note that for reordering evaluation it does not make
sense to tune ? because that would blur the individual contri-
butions of reordering and adequacy during meta evaluation,
which is confirmed by Figure 7 showing that ?  0.5 leads
to similar performance for all metrics.
Figure 8: Effect of ? on German-English evalua-
tion for ? = 0.5
2011). Alternatively, we may align system and ref-
erence translations directly. One of the simplest
ways to do that is by finding exact matches be-
tween words and bigrams between system and ref-
erence translation as done in (Isozaki et al., 2010).
The way we align system and reference transla-
tions is by using the aligner supplied with ME-
TEOR (Denkowski and Lavie, 2011) for finding
1-to-1 alignments which are later converted to a
permutation. The advantage of this method is that
it can do non-exact matching by stemming or us-
ing additional sources for semantic similarity such
as WordNets and paraphrase tables. Since we will
not have a perfect permutation as input, because
many words in the reference or system transla-
tions might not be aligned, we introduce a brevity
penalty (bp(?, ?) in Equation 2) for the ordering
component as in (Isozaki et al., 2010). The brevity
penalty is the same as in BLEU with the small
difference that instead of taking the length of sys-
tem and reference translation as its parameters, it
takes the length of the system permutation and the
length of the reference.
5 Empirical results
The results are shown in Table 1 and Table 2.
These scores could be much higher if we used
some more sophisticated measure than unigram
BLEU for the lexical part (for example recall is
very useful in evaluation of the system translations
(Lavie et al., 2004)). However, this is not the issue
here since our goal is merely to compare different
ways to evaluate word order. All metrics that we
tested have the same lexical component, get the
same permutation as their input and have the same
value for ?.
143
E
n
g
l
i
s
h
-
C
z
e
c
h
E
n
g
l
i
s
h
-
S
p
a
n
i
s
h
E
n
g
l
i
s
h
-
G
e
r
m
a
n
E
n
g
l
i
s
h
-
R
u
s
s
i
a
n
E
n
g
l
i
s
h
-
F
r
e
n
c
h
Kendall 0.16 0.170 0.183 0.193 0.218
Spearman 0.157 0.170 0.181 0.192 0.215
Hamming 0.150 0.163 0.168 0.187 0.196
FuzzyScore 0.155 0.166 0.178 0.189 0.215
Ulam 0.159 0.170 0.181 0.189 0.221
PEFs 0.156 0.173 0.185 0.196 0.219
PETs 0.157 0.165 0.182 0.195 0.216
Table 1: Sentence level Kendall tau scores for
translation out of English with ? = 0.5 and ? =
0.6
C
z
e
c
h
-
E
n
g
l
i
s
h
S
p
a
n
i
s
h
-
E
n
g
l
i
s
h
G
e
r
m
a
n
-
E
n
g
l
i
s
h
R
u
s
s
i
a
n
-
E
n
g
l
i
s
h
F
r
e
n
c
h
-
E
n
g
l
i
s
h
Kendall 0.196 0.265 0.235 0.173 0.223
Spearman 0.199 0.265 0.236 0.173 0.222
Hamming 0.172 0.239 0.215 0.157 0.206
FuzzyScore 0.184 0.263 0.228 0.169 0.216
Ulam 0.188 0.264 0.232 0.171 0.221
PEFs 0.201 0.265 0.237 0.181 0.228
PETs 0.200 0.264 0.234 0.174 0.221
Table 2: Sentence level Kendall tau scores for
translation into English with ? = 0.5 and ? = 0.6
5.1 Does hierarchical structure improve
evaluation?
The results in Tables 1, 2 and 3 suggest that the
PEFscore which uses hierarchy over permutations
outperforms the string based permutation metrics
in the majority of the language pairs. The main
exception is the English-Czech language pair in
which both PETs and PEFs based metric do not
give good results compared to some other met-
rics. For discussion about English-Czech look at
the section 6.1.
5.2 Do PEFs help over one canonical PET?
From Figures 9 and 10 it is clear that using all
permutation trees instead of only canonical ones
makes the metric more stable in all language pairs.
Not only that it makes results more stable but it
metric avg rank avg Kendall
PEFs 1.6 0.2041
Kendall 2.65 0.2016
Spearman 3.4 0.201
PETs 3.55 0.2008
Ulam 4 0.1996
FuzzyScore 5.8 0.1963
Hamming 7 0.1853
Table 3: Average ranks and average Kendall
scores for each tested metrics over all language
pairs
Figure 9: Plot of scaled Kendall tau correlation for
translation from English
also improves them in all cases except in English-
Czech where both PETs and PEFs perform badly.
The main reason why PEFs outperform PETs is
that they encode all possible phrase segmentations
of monotone and inverted sub-permutations. By
giving the score that considers all segmentations,
PEFs also include the right segmentation (the one
perceived by human evaluators as the right seg-
mentation), while PETs get the right segmentation
only if the right segmentation is the canonical one.
5.3 Is improvement consistent over language
pairs?
Table 3 shows average rank (metric?s position af-
ter sorting all metrics by their correlation for each
language pair) and average Kendall tau correlation
coefficient over the ten language pairs. The table
shows clearly that the PEFs metric outperforms all
other metrics. To make it more visible how met-
rics perform on the different language pairs, Fig-
ures 9 and 10 show Kendall tau correlation co-
efficient scaled between the best scoring metric
for the given language (in most cases PEFs) and
144
Figure 10: Plot of scaled Kendall tau correlation
for translation into English
the worst scoring metric (in all cases Hamming
score). We can see that, except in English-Czech,
PEFs are consistently the best or second best (only
in English-French) metric in all language pairs.
PETs are not stable and do not give equally good
results in all language pairs. Hamming distance
is without exception the worst metric for evalua-
tion since it is very strict about positioning of the
words (it does not take relative ordering between
words into account). Kendall tau is the only string
based metric that gives relatively good scores in
all language pairs and in one (English-Czech) it is
the best scoring one.
6 Further experiments and analysis
So far we have shown that PEFs outperform the
existing metrics over the majority of language
pairs. There are two pending issues to discuss.
Why is English-Czech seemingly so difficult?
And does preferring inversion over non-binary
branching correlate better with human judgement.
6.1 The results on English-Czech
The English-Czech language pair turned out to
be the hardest one to evaluate for all metrics.
All metrics that were used in the meta-evaluation
that we conducted give much lower Kendall tau
correlation coefficient compared to the other lan-
guage pairs. The experiments conducted by other
researchers on the same dataset (Mach?a?cek and
Bojar, 2013), using full evaluation metrics, also
get far lower Kendall tau correlation coefficient
for English-Czech than for other language pairs.
In the description of WMT13 data that we used
(Bojar et al., 2013), it is shown that annotator-
agreement for English-Czech is a few times lower
than for other languages. English-Russian, which
is linguistically similar to English-Czech, does
not show low numbers in these categories, and is
one of the language pairs where our metrics per-
form the best. The alignment ratio is equally high
between English-Czech and English-Russian (but
that does not rule out the possibility that the align-
ments are of different quality). One seemingly
unlikely explanation is that English-Czech might
be a harder task in general, and might require a
more sophisticated measure. However, the more
plausible explanation is that the WMT13 data for
English-Czech is not of the same quality as other
language pairs. It could be that data filtering, for
example by taking only judgments for which many
evaluators agree, could give more trustworthy re-
sults.
6.2 Is inversion preferred over non-binary
branching?
Since our original version of the scoring function
for PETs and PEFs on the operator level does not
discriminate between kinds of non-monotone op-
erators (all non-monotone get zero as a score) we
also tested whether discriminating between inver-
sion (binary) and non-binary operators make any
difference.
E
n
g
l
i
s
h
-
C
z
e
c
h
E
n
g
l
i
s
h
-
S
p
a
n
i
s
h
E
n
g
l
i
s
h
-
G
e
r
m
a
n
E
n
g
l
i
s
h
-
R
u
s
s
i
a
n
E
n
g
l
i
s
h
-
F
r
e
n
c
h
PEFs ? = 0.0 0.156 0.173 0.185 0.196 0.219
PEFs ? = 0.5 0.157 0.175 0.183 0.195 0.219
PETs ? = 0.0 0.157 0.165 0.182 0.195 0.216
PETs ? = 0.5 0.158 0.165 0.183 0.195 0.217
Table 4: Sentence level Kendall tau score for
translation out of English different ? with ? = 0.5
and ? = 0.6
Intuitively, we might expect that inverted binary
operators are preferred by human evaluators over
non-binary ones. So instead of assigning zero as a
score to inverted nodes we give them 0.5, while for
non-binary nodes we remain with zero. The ex-
periments with the inverted operator scored with
0.5 (i.e., ? = 0.5) are shown in Tables 4 and 5.
The results show that there is no clear improve-
ment by distinguishing between the two kinds of
145
C
z
e
c
h
-
E
n
g
l
i
s
h
S
p
a
n
i
s
h
-
E
n
g
l
i
s
h
G
e
r
m
a
n
-
E
n
g
l
i
s
h
R
u
s
s
i
a
n
-
E
n
g
l
i
s
h
F
r
e
n
c
h
-
E
n
g
l
i
s
h
PEFs ? = 0.0 0.201 0.265 0.237 0.181 0.228
PEFs ? = 0.5 0.201 0.264 0.235 0.179 0.227
PETs ? = 0.0 0.200 0.264 0.234 0.174 0.221
PETs ? = 0.5 0.202 0.263 0.235 0.176 0.224
Table 5: Sentence level Kendall tau score for
translation into English for different ? with ? =
0.5 and ? = 0.6
non-monotone operators on the nodes.
7 Conclusions
Representing order differences as compact permu-
tation forests provides a good basis for develop-
ing evaluation measures of word order differences.
These hierarchical representations of permutations
bring together two crucial elements (1) grouping
words into blocks, and (2) factorizing reorder-
ing phenomena recursively over these groupings.
Earlier work on MT evaluation metrics has of-
ten stressed the importance of the first ingredient
(grouping into blocks) but employed it merely in a
flat (non-recursive) fashion. In this work we pre-
sented novel metrics based on permutation trees
and forests (the PETscore and PEFscore) where
the second ingredient (factorizing reordering phe-
nomena recursively) plays a major role. Permuta-
tion forests compactly represent all possible block
groupings for a given permutation, whereas per-
mutation trees select a single canonical grouping.
Our experiments with WMT13 data show that our
PEFscore metric outperforms the existing string-
based metrics on the large majority of language
pairs, and in the minority of cases where it is not
ranked first, it ranks high. Crucially, the PEFs-
core is by far the most stable reordering score over
ten language pairs, and works well also for lan-
guage pairs with long range reordering phenom-
ena (English-German, German-English, English-
Russian and Russian-English).
Acknowledgments
This work is supported by STW grant nr. 12271
and NWO VICI grant nr. 277-89-002. We thank
TAUS and the other DatAptor project User Board
members. We also thank Ivan Titov for helpful
comments on the ideas presented in this paper.
References
Michael H. Albert and Mike D. Atkinson. 2005. Sim-
ple permutations and pattern restricted permutations.
Discrete Mathematics, 300(1-3):1?15.
Alexandra Birch and Miles Osborne. 2010. LRscore
for Evaluating Lexical and Reordering Quality in
MT. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR,
pages 327?332, Uppsala, Sweden, July. Association
for Computational Linguistics.
Alexandra Birch and Miles Osborne. 2011. Reorder-
ing Metrics for MT. In Proceedings of the Associ-
ation for Computational Linguistics, Portland, Ore-
gon, USA. Association for Computational Linguis-
tics.
Alexandra Birch, Miles Osborne, and Phil Blunsom.
2010. Metrics for MT evaluation: evaluating re-
ordering. Machine Translation, pages 1?12.
Ond?rej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 1?44, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Ma-
chine Translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montr?eal, Canada, June. Association for
Computational Linguistics.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic Metric for Reliable Optimization
and Evaluation of Machine Translation Systems. In
Proceedings of the EMNLP 2011 Workshop on Sta-
tistical Machine Translation.
Daniel Gildea, Giorgio Satta, and Hao Zhang. 2006.
Factoring Synchronous Grammars by Sorting. In
ACL.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010. Automatic
evaluation of translation quality for distant language
pairs. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ?10, pages 944?952, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Mirella Lapata. 2006. Automatic Evaluation of In-
formation Ordering: Kendall?s Tau. Computational
Linguistics, 32(4):471?484.
146
Alon Lavie, Kenji Sagae, and Shyamsundar Jayara-
man. 2004. The significance of recall in auto-
matic metrics for MT evaluation. In Proceedings of
the Sixth Conference of the Association for Machine
Translation in the Americas.
Matou?s Mach?a?cek and Ond?rej Bojar. 2013. Results
of the WMT13 Metrics Shared Task. In Proceed-
ings of the Eighth Workshop on Statistical Machine
Translation, pages 45?51, Sofia, Bulgaria, August.
Association for Computational Linguistics.
Gideon Maillette de Buy Wenniger and Khalil Sima?an.
2011. Hierarchical Translation Equivalence over
Word Alignments. In ILLC Prepublication Series,
PP-2011-38. University of Amsterdam.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of ACL?02, pages 311?318, Philadelphia, PA, USA.
Milo?s Stanojevi?c and Khalil Sima?an. 2014. BEER:
BEtter Evaluation as Ranking. In Proceedings of the
Ninth Workshop on Statistical Machine Translation,
pages 414?419, Baltimore, Maryland, USA, June.
Association for Computational Linguistics.
David Talbot, Hideto Kazawa, Hiroshi Ichikawa, Jason
Katz-Brown, Masakazu Seno, and Franz Och. 2011.
A Lightweight Evaluation Framework for Machine
Translation Reordering. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
12?21, Edinburgh, Scotland, July. Association for
Computational Linguistics.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 3(23):377?403.
Hao Zhang and Daniel Gildea. 2007. Factorization
of Synchronous Context-Free Grammars in Linear
Time. In NAACL Workshop on Syntax and Structure
in Statistical Translation (SSST), pages 25?32.
Hao Zhang, Daniel Gildea, and David Chiang. 2008.
Extracting synchronous grammar rules from word-
level alignments in linear time. In Proceedings
of the 22nd International Conference on Computa-
tional Linguistics-Volume 1, pages 1081?1088. As-
sociation for Computational Linguistics.
147
