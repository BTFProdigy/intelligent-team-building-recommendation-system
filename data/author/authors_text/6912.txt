227
228
229
230
231
232
233
234
199
200
201
202
Proceedings of the Linguistic Annotation Workshop, pages 148?155,
Prague, June 2007. c?2007 Association for Computational Linguistics
Standoff Coordination for Multi-Tool Annotation in a Dialogue Corpus
Kepa Joseba Rodr??guez?, Stefanie Dipper?, Michael Go?tze?, Massimo Poesio?,
Giuseppe Riccardi?, Christian Raymond?, Joanna Wisniewska?
?Piedmont Consortium for Information Systems (CSI-Piemonte)
KepaJoseba.Rodriguez@csi.it
?Department of Linguistics. University of Potsdam.
{dipper|goetze}@ling.uni-potsdam.de
?Center for Mind/Brain Sciences. University of Trento.
massimo.poesio@unitn.it
?Department of Information and Communication Technology. University of Trento.
{christian.raymond|riccardi}@dit.unitn.it
?Institute of Computer Science. Polish Academy of Science.
jwisniewska@poczta.uw.edu.pl
Abstract
The LUNA corpus is a multi-lingual, multi-
domain spoken dialogue corpus currently
under development that will be used to de-
velop a robust natural spoken language un-
derstanding toolkit for multilingual dialogue
services. The LUNA corpus will be an-
notated at multiple levels to include an-
notations of syntactic, semantic, and dis-
course information; specialized annotation
tools will be used for the annotation at each
of these levels. In order to synchronize these
multiple layers of annotation, the PAULA
standoff exchange format will be used. In
this paper, we present the corpus and its
PAULA-based architecture.1
1 Introduction
XML standoff markup (Thompson and McKelvie,
1997; Dybkj?r et al, 1998) is emerging as the clean-
est way to organize multi-level annotations of cor-
pora. In many of the current annotation efforts based
on standoff a single multi-purpose tool such as the
NITE XML Toolkit (Carletta et al, 2003) or Word-
Freak (Morton and LaCivita, 2003) is used to anno-
1The members of the LUNA project consortium are: Pied-
mont Consortium for Information Systems (IT), University of
Trento (IT), Loquendo SpA (IT), RWTH-Aachen (DE), Uni-
versity of Avignon (FR), France Telecom R&D Division S.A.
(FR), Polish-Japanese Institute of Information Technology (PL)
and the Institute for Computer Science of the Polish Academy
of Sciences (PL), http://www.ist-luna.eu.
This research was performed in the LUNA project funded by the
EC, DG Infso, Unit E1 and in the Collaborative Research Cen-
ter 632 ?Information Structure?, funded by the German Science
Foundation, http://www.sfb632.uni-potsdam.de.
tate as well as maintain all annotation levels (cf. the
SAMMIE annotation effort (Kruijff-Korbayova? et al,
2006b)).
However, it is often the case that specialized tools
are developed to facilitate the annotation of particu-
lar levels: examples include tools for segmentation
and transcription of the speech signal like PRAAT
(Boersma and Weenink, 2005) and TRANSCRIBER
(Barras et al, 1998), the SALSA tools for FrameNet-
style annotation (Burchardt et al, 2006), and MMAX
(Mu?ller and Strube, 2003) for coreference annota-
tion. Even in these cases, however, it may still be
useful, or even necessary, to be able to visualize
more than one level at once, or to ?knit? together2
multiple levels to create a file that can be used to
train a model for a particular type of annotation.
The Linguistic Annotation Framework by (Ide et al,
2003) was proposed as a unifying markup format to
be used to synchronize heterogeneous markup for-
mats for such purposes.
In this paper, we discuss how the PAULA represen-
tation format, a standoff format inspired by the Lin-
guistic Annotation Framework, is being used to syn-
chronize multiple levels of annotation in the LUNA
corpus, a corpus of spoken dialogues in multiple lan-
guages and multiple domains that is being created to
support the development of robust spoken language
understanding models for multilingual dialogue ser-
vices. The corpus is richly annotated with linguistic
information that is considered relevant for research
on dialogue, including chunks, named entities, argu-
ment structure, coreference, and dialogue acts. We
chose to adopt specialized tools for each level: e.g.,
2In the sense of the knit tool of the LT-XML suite.
148
transcription using TRANSCRIBER, coreference us-
ing MMAX, attributes using SEMANTIZER, etc. To
synchronize the annotation and allow cross-layer op-
erations, the annotations are mapped to a common
representation format, PAULA.
The structure of the paper is as follows. In Sec-
tion 2, we present the LUNA project and the LUNA
corpus with its main annotation levels. In Section 3,
we introduce the PAULA exchange format, focusing
on the representation of time alignment and dialogue
phenomena. Finally we show how PAULA is used in
the LUNA corpus and discuss alternative formats.
2 The LUNA project
The aim of the LUNA project is to advance the state
of the art in understanding conversational speech
in Spoken Dialogue Systems (Gupta et al, 2005),
(Bimbot et al, 2006).
Three aspects of Spoken Language Understand-
ing (SLU) are of particular concern in LUNA: gen-
eration of semantic concept tags, semantic compo-
sition into conceptual structures and context sensi-
tive validation using information provided by the di-
alogue manager. In order to train and evaluate SLU
models, we will create an annotated corpus of spo-
ken dialogues in multiple domains and multiple lan-
guages: French, Italian, and Polish.
2.1 The LUNA corpus
The LUNA corpus is currently being collected, with
a target to collect 8100 human-machine dialogues
and 1000 human-human dialogues in Polish, Italian
and French. The dialogues are collected in the fol-
lowing application domains: stock exchange, hotel
reservation and tourism inquiries, customer support
service/help-desk and public transportation.
2.2 Multilevel annotation
Semantic interpretation involves a number of sub-
tasks, ranging from identifying the meaning of indi-
vidual words to understanding which objects are be-
ing referred to up to recovering the relation between
different semantic objects in the utterance and dis-
course level to, finally, understanding the commu-
nicative force of an utterance.
In some annotation efforts?e.g., in the annotation
of the French MEDIA Corpus (Bonneau-Maynard
and Rosset, 2003)? information about the meaning
of semantic chunks, contextual information about
coreference, and information about dialogue acts are
all kept in a single file. This approach however suf-
fers from a number of problems, including the fact
that errors introduced during the annotation at one
level may make other levels of annotation unusable
as well, and that it is not possible for two anno-
tators to work on different types of annotation for
the same file at the same time. Most current an-
notation efforts, therefore, tend to adopt the ?multi-
level? approach pioneered during the development
of the MAPTASK corpus and then developed as part
of work on the EU-funded MATE project (McKelvie
et al, 2001), in which each aspect of interpreta-
tion is annotated in a separate level, independently
maintained. This approach is being followed, for
instance, in the ONTONOTES project (Hovy et al,
2006) and the SAMMIE project (Kruijff-Korbayova
et al, 2006a).
For the annotation of the LUNA corpus, we de-
cided to follow the multilevel approach as well. That
allows us to achieve more granularity in the anno-
tation of each of the levels and to investigate more
easily dependencies between features that belong to
different levels. Furthermore, we can use different
specialized off-the-shelf annotation tools, splitting
up the annotation task and thus facilitating consis-
tent annotation.
2.3 Annotation levels
The LUNA corpus will contain different types of in-
formation. The first levels are necessary to prepare
the corpus for subsequent semantic annotation, and
include segmentation of the corpus in dialogue turns,
transcription of the speech signal, and syntactic pre-
processing with POS-tagging and shallow parsing.
The next level consists of the annotation of do-
main information using attribute-value pairs. This
annotation will be performed on all dialogues in the
corpus.
The other levels of the annotation scheme are not
mandatory, but at least a part of the dialogues will
be annotated in order to investigate contextual as-
pects of the semantic interpretation. These levels in-
clude the predicate structure, the relations between
referring expressions, and the annotation of dialogue
acts.
149
2.3.1 Segmentation and transcription of the
speech signal
Before transcription and annotation can begin, it
is necessary to segment the speech signal into dia-
logue turns and annotate them with speaker identity
and mark where speaker overlap occurs. The goal
of this segmentation is to be able to perform a tran-
scription and annotation of the dialogue turns with
or without dialogue context. While dialogue context
is preferable for semantic annotation, it slows down
the annotation process.
The tool we will use for the segmentation and
transcription of the speech signal is the open source
tool TRANSCRIBER3 (Barras et al, 1998).
The next step is the transcription of the speech
signal, using conventions for the orthographic tran-
scription and for the annotation of non-linguistic
acoustic events.
2.3.2 Part Of Speech Tagging and Chunking
The transcribed material will be annotated with
POS-tags, morphosyntactic information like agree-
ment features, and segmented based on syntactic
constituency.
For the POS-tags and morphosyntactic features,
we will follow the recommendations made in EA-
GLES (EAGLES, 1996), which allows us to have a
unified representation format for the corpus, inde-
pendently of the tools used for each language.
2.3.3 Domain Attribute Annotation
At this level, semantic segments will be anno-
tated following an approach used for the annotation
for the French MEDIA dialogue corpus (Bonneau-
Maynard and Rosset, 2003).
We specify the domain knowledge in domain on-
tologies. These are used to build domain-specific
dictionaries. Each dictionary contains:
? Concepts corresponding to classes of the ontol-
ogy and attributes of the annotation.
? Values corresponding to the individuals of the
domain.
? Constraints on the admissible values for each
concept.
3http://trans.sourceforge.net
The concept dictionaries are used to annotate se-
mantic segments with attribute-value pairs. The se-
mantic segments are produced by concatenation of
the chunks produced by the shallow parser. A se-
mantic segment is a unit that corresponds unambigu-
ously to a concept of the dictionary.
(1) buongiorno lei [puo` iscriversi]concept1 [agli
esami]concept2 [oppure]concept3 [ottenere
delle informazioni]concept4 come la posso
aiutare4
<concept1 action:inscription>
<concept2 objectDB:examen>
<concept3 conjunctor:alternative>
<concept4 action:obtain info>
2.3.4 Predicate structure
The annotation of predicate structure facilitates
the interpretation of the relation between entities and
events occurring in the dialogue.
There are different approaches to annotate predi-
cate structure. Some of them are based upon syntac-
tic structure, with PropBank (Kingsbury and Palmer,
2003) being one of the most relevant, building the
annotation upon the syntactic representation of the
TreeBank corpus (Marcus et al, 1993). An alter-
native to syntax-driven approaches is the annotation
using semantic roles as in FrameNet (Baker et al,
1998).
For the annotation of predicate structure in the
LUNA corpus, we decided to use a FrameNet-like
approach, rather than a syntax-based approach:
1. Annotation of dialogue interaction has to deal
with disfluencies, non-complete sentences, un-
grammaticality, etc., which complicates the use
of deep syntactic representations.
2. If we start from a syntactic representation, we
have to follow a long way to achieve the seman-
tic interpretation. Syntactic constituents must
be mapped to ?-roles, and then to semantic
roles. FrameNet offers the possibility of anno-
tating using directly semantic criteria.
4Good morning, you can register for the exam or obtain in-
formation. How can I help you?
150
For each domain, we define a set of frames. These
frames are defined based on the domain ontology,
with the named entities providing the frame ele-
ments. For all the frames we introduce the negation
as a default frame element.
For the annotation, first of all we annotate the en-
tities with a frame and a frame element.
Then if the target is overtly realized we make a
pointer from the frame elements to the target. The
next step is putting the frame elements and the target
(if overtly realized) in a set.
(2) buongiorno [lei]fe1 [puo` iscriversi]fe2
[agli esami]fe3 oppure [ottenere delle
informazioni]fe4 come la posso aiutare
set1 = {id1, id2, id3}
frame: inscription
frame-elements:{student, examen, date}
set2 = {id4}
frame = info-request
frame-elements:{student, addressee, topic}
<fe1 frame="inscription"
FE="student" member="set1"
pointer="fe2">
<fe2 frame="inscription"
FE="target" member="set1">
<fe3 frame="inscription"
FE="examen" member="set1"
pointer="fe2">
<fe4 frame="information"
FE="target" member="set2">
2.3.5 Coreference / Anaphoric relations
To annotate anaphoric relations we will use an an-
notation scheme close to the one used in the ARRAU
project (Artstein and Poesio, 2006). This scheme
has been extensively tested with dialogue corpora
and includes instructions for annotating a variety of
anaphoric relations, including bridging relations. A
further reason is the robustness of the scheme that
doesn?t require one single interpretation in the an-
notation.
The first step is the annotation of the information
status of the markables with the tags given and
new. If the markables are annotated with given,
the annotator will select the most recent occurrence
of the object and add a pointer to it. If the mark-
able is annotated with new, we distinguish between
markables that are related to a previously mentioned
object (associative reference) or don?t have such a
relation.
If there are alternative interpretations, which of a
list of candidates can be the antecedent, the annota-
tor can annotate the markable as ambiguous and
add a pointer to each of the possible antecedents.
(3) Wizard: buongiorno [lei]cr1 [puo`
iscriversi]cr2 [agli esami]cr3 oppure ot-
tenere [delle informazioni]cr4 come la posso
aiutare
<cr1 inf status="new" related="no">
<cr2 inf status="new" related="no">
<cr3 inf status="new" related="no">
<cr4 inf status="new" related="no">
Caller: [iscrizione]cr5 [esami]cr65
<cr5 inf status="given"
single phrase antecedent="cr2"
ambiguity="unambiguous">
<cr6 inf status="given"
single phrase antecedent="cr3"
ambiguity="unambiguous">
2.3.6 Dialogue acts
In order to associate the intentions of the speaker
with the propositional content of the utterances, the
segmentation of the dialogue turns in utterances is
based on the annotation of predicate structure. Each
set of frame elements will correspond to an utter-
ance.
Each utterance will be annotated using a multi-
dimensional annotation scheme partially based on
the DAMSL scheme (Allen and Core, 1997) and on
the proposals of ICSI-MRDA (Dhillon et al, 2004).
We have selected nine dialogue acts from the
DAMSL scheme as initial tagset, that can be extended
for the different application domains. Each utter-
ance will be annotated with as many tags as applica-
ble.
(4) Wizard: [buongiorno]utt1 [lei puo` iscriversi
agli esami]utt2 oppure [ottenere delle
5Register for the exam.
151
informzaioni]utt3 [come la posso aiutare]utt4
<utt1 d-act="opening/closing">
<utt2 d-act="statement"
link-frame="set1">
<utt3 d-act="statement"
link-frame="set2">
<utt4 d-act="info-request">
Caller: [iscrizione esami]utt5
<utt5 d-act="answer;statement"
link-frame="set3">
3 PAULA - a Linguistic Standoff Exchange
Format
PAULA stands for Potsdamer Austauschformat fu?r
linguistische Annotation (?Potsdam Interchange
Format for Linguistic Annotation?) and has been de-
veloped for the representation of data annotated at
multiple layers. The application scenario is sketched
in Fig 1: researchers use multiple, specialized off-
the-shelf annotation tools, such as EXMARALDA or
MMAX, to enrich data with linguistic information.
The tools store the data in tool-specific formats and,
hence, it is not straightforward to combine informa-
tion from different sources and, e.g., to search for
correlations across multiple annotation layers.
This is where PAULA comes in: PAULA maps
the tool-specific formats to a common format and
serves as an interchange format between these
tools.6 Moreover, the annotations from the different
sources are merged into one single representation.
PAULA makes this data available for further appli-
cations, such as searching the data by means of the
tool ANNIS7, or to feed statistical applications like
WEKA8.
PAULA is an XML-based standoff format for lin-
guistic annotations, inspired by the ?dump format?
6Currently, we provide PAULA import filters for the follow-
ing tools and formats: Exmaralda, MMAX, RST Tool/URML,
annotate/TIGER XML. Export from PAULA to the tool formats
is at present supported for the original source format only. We
plan to support the export of selected annotations to other tools.
This is, however, not a trivial task since it may involve loss of
information.
7ANNIS: http://www.sfb632.uni-potsdam.de/
annis
8WEKA: http://www.cs.waikato.ac.nz/ml/
weka
Figure 1: PAULA annotation scenario
of the Linguistic Annotation Framework (Ide et al,
2003).9 With PAULA, not only is the primary data
separated from its annotations, but individual anno-
tation layers (such as parts of speech and dialogue
acts) are separated from each other as well. The
standoff approach allows us to mark overlapping
segments in a straightforward way: by distributing
annotations over different files (XML as such does
not easily account for overlapping segments, since
its object model is a hierarchical, tree-like structure).
Moreover, new annotation layers can be added eas-
ily.
PAULA assumes that a representation of the pri-
mary data is stored in a file that optionally spec-
ifies a header with meta information, followed by
a tag <body>, which contains a representation of
the primary data. In Fig. 2, the first box displays
the transcription, with all contributions from the first
speaker coming first, and the contributions from the
other speaker(s) following (put in italics in the Fig-
ure).
The basic type of ?annotation? are markables, en-
coded by the XML element <mark>. Markables
specify ?anchors?, i.e., locations or ranges that can
be annotated by linguistic information. The loca-
tions and ranges are positions or spans in the source
text or timeline, which are referenced by means of
XLinks and XPointer expressions. For instance, the
?Token? markables in Fig. 2 define spans that cor-
9The term ?standoff? describes the situation where primary
data (e.g., the transcription) and annotations of this data are
stored in separate files (Thompson and McKelvie, 1997).
152
 

	
	

Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 104?107,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
BART: A Multilingual Anaphora Resolution System
Samuel Broscheit?, Massimo Poesio?, Simone Paolo Ponzetto?, Kepa Joseba Rodriguez?,
Lorenza Romano?, Olga Uryupina?, Yannick Versley?, Roberto Zanoli?
?Seminar fu?r Computerlinguistik, University of Heidelberg
?CiMeC, University of Trento
?Fondazione Bruno Kessler
?SFB 833, University of Tu?bingen
broscheit@cl.uni-heidelberg.de, massimo.poesio@unitn.it,
ponzetto@cl.uni-heidelberg.de, kepa.rodriguez@unitn.it,
romano@fbk.eu, uryupina@gmail.com,
versley@sfs.uni-tuebingen.de, zanoli@fbk.eu
Abstract
BART (Versley et al, 2008) is a highly mod-
ular toolkit for coreference resolution that
supports state-of-the-art statistical approaches
and enables efficient feature engineering. For
the SemEval task 1 on Coreference Resolu-
tion, BART runs have been submitted for Ger-
man, English, and Italian.
BART relies on a maximum entropy-based
classifier for pairs of mentions. A novel entity-
mention approach based on Semantic Trees is
at the moment only supported for English.
1 Introduction
This paper presents a multilingual coreference reso-
lution system based on BART (Versley et al, 2008).
BART is a modular toolkit for coreference resolution
that supports state-of-the-art statistical approaches
to the task and enables efficient feature engineer-
ing. BART has originally been created and tested
for English, but its flexible modular architecture en-
sures its portability to other languages and domains.
In SemEval-2010 task 1 on Coreference Resolution,
BART has shown reliable performance for English,
German and Italian.
In our SemEval experiments, we mainly focus on
extending BART to cover multiple languages. Given
a corpus in a new language, one can re-train BART
to obtain baseline results. Such a language-agnostic
system, however, is only used as a starting point:
substantial improvements can be achieved by incor-
porating language-specific information with the help
of the Language Plugin. This design provides ef-
fective separation between linguistic and machine
learning aspects of the problem.
2 BART Architecture
The BART toolkit has five main components: pre-
processing pipeline, mention factory, feature extrac-
tion module, decoder and encoder. In addition, an
independent LanguagePlugin module handles all the
language specific information and is accessible from
any component. The architecture is shown on Figure
1. Each module can be accessed independently and
thus adjusted to leverage the system?s performance
on a particular language or domain.
The preprocessing pipeline converts an input doc-
ument into a set of lingustic layers, represented
as separate XML files. The mention factory uses
these layers to extract mentions and assign their
basic properties (number, gender etc). The fea-
ture extraction module describes pairs of mentions
{M
i
,M
j
}, i < j as a set of features.
The decoder generates training examples through
a process of sample selection and learns a pairwise
classifier. Finally, the encoder generates testing ex-
amples through a (possibly distinct) process of sam-
ple selection, runs the classifier and partitions the
mentions into coreference chains.
3 Language-specific issues
Below we briefly describe our language-specific ex-
tensions to BART. These issues are addressed in
more details in our recent papers (Broscheit et al,
2010; Poesio et al, 2010).
3.1 Mention Detection
Robust mention detection is an essential component
of any coreference resolution system. BART sup-
ports different pipelines for mention detection. The
104
Parser
Dep-to-Const
Converter
Morphology
Preprocessing
Mention
Factory
Decoder
Basic features
Syntactic features
Knowledge-based
features
MaxEnt
Classifier
Mention
(with basic
 properties):
- Number
- Gender
- Mention Type
- Modifiers
Unannotated
Text
Coreference
Chains
LanguagePlugin
Figure 1: BART architecture
choice of a pipeline depends crucially on the avail-
ability of linguistic resources for a given language.
For English and German, we use the Parsing
Pipeline and Mention Factory to extract mentions.
The parse trees are used to identify minimal and
maximal noun projections, as well as additional fea-
tures such as number, gender, and semantic class.
For English, we use parses from a state-of-the-art
constituent parser (Petrov et al, 2006) and extract
all base noun phrases as mentions. For German,
the SemEval dependency tree is transformed to a
constituent representation and minimal and maxi-
mal phrases are extracted for all nominal elements
(pronouns, common nouns, names), except when the
noun phrase is in a non-referring syntactic position
(for example, expletive ?es?, predicates in copula
constructions).
For Italian, we use the EMD Pipeline and Men-
tion Factory. The Typhoon (Zanoli et al, 2009)
and DEMention (Biggio et al, 2009) systems were
used to recognize mentions in the test set. For each
mention, its head and extension were considered.
The extension was learned by using the mention an-
notation provided in the training set (13th column)
whereas the head annotation was learned by exploit-
ing the information produced by MaltParser (Nivre
et al, 2007). In addition to the features extracted
from the training set, such as prefixes and suffixes
(1-4 characters) and orthographic information (capi-
talization and hyphenation), a number of features ex-
tracted by using external resources were used: men-
tions recognized by TextPro (http://textpro.fbk.eu),
gazetteers of generic proper nouns extracted from
the Italian phone-book and Wikipedia, and other fea-
tures derived from WordNet. Each of these features
was extracted in a local context of ?2 words.
3.2 Features
We view coreference resolution as a binary classifi-
cation problem. Each classification instance consists
of two markables, i.e. an anaphor and potential an-
tecedent. Instances are modeled as feature vectors
(cf. Table 1) and are handed over to a binary clas-
sifier that decides, given the features, whether the
anaphor and the candidate are coreferent or not. All
the feature values are computed automatically, with-
out any manual intervention.
Basic feature set. We use the same set of rela-
tively language-independent features as a backbone
of our system, extending it with a few language-
specific features for each subtask. Most of them are
used by virtually all the state-of-the-art coreference
resolution systems. A detailed description can be
found, for example, in (Soon et al, 2001).
English. Our English system is based on a novel
model of coreference. The key concept of our model
is a Semantic Tree ? a filecard associated with each
discourse entity containing the following fields:
? Types: the list of types for mentions of a given
entity. For example, if an entity contains the
mention ?software from India?, the shallow
predicate ?software? is added to the types.
? Attributes: this field collects the premodifiers.
For instance, if one of the mentions is ?the ex-
pensive software? the shallow attribute ?expen-
sive? is added to the list of attributes.
? Relations: this field collects the prepositional
postmodifiers. If an entity contains the men-
tion ?software from India?, the shallow relation
?from(India)? is added to the list of relations.
105
For each mention BART creates such a filecard
using syntactic information. If the classifier decides
that both mentions are corefering, the filecard of
the anaphora is merged into the filecard of the an-
tecedent (cf. Section 3.3 below).
The SemanticTreeCompatibility feature
extractor checks whether individual slots of the
anaphor?s filecard are compatible with those of the
antecedent?s.
The StrudelRelatedness feature relies on
Strudel ? a distributional semantic model (Baroni et
al., 2010). We compute Strudel vectors for the sets
of types of the anaphor and the antecedent. The re-
latedness value is determined as the cosine between
the two.
German. We have tested extra features for Ger-
man in our previous study (Broscheit et al, 2010).
The NodeDistance feature measures the num-
ber of clause nodes (SIMPX, R-SIMPX) and preposi-
tional phrase nodes (PX) along the path between M
j
and M
i
in the parse tree.
The PartialMorphMatch feature is a sub-
string match with a morphological extension for
common nouns. In German the frequent use of
noun composition makes a simple string match for
common nouns unfeasible. The feature checks for
a match between the noun stems of M
i
and M
j
.
We extract the morphology with SMOR/Morphisto
(Schmid et al, 2004).
The GermanetRelatedness feature uses the
Pathfinder library for GermaNet (Finthammer and
Cramer, 2008) that computes and discretizes raw
scores into three categories of semantic relatedness.
In our experiments we use the measure from Wu and
Palmer (1994), which has been found to be the best
performing on our development data.
Italian. We have designed a feature to cover Ital-
ian aliasing patterns. A list of company/person des-
ignators (e.g., ?S.p.a? or ?D.ssa?) has been manually
crafted. We have collected patterns of name variants
for locations. Finally, we have relaxed abbreviation
constraints, allowing for lower-case characters in the
abbreviations. Our pilot experiments suggest that,
although a universal aliasing algorithm is able to re-
solve some coreference links between NEs, creating
a language-specific module boosts the system?s per-
formance for Italian substantially.
Basic feature set
MentionType(M
i
),MentionType(M
j
)
SemanticClass(M
i
), SemanticClass(M
j
)
GenderAgreement(M
i
,M
j
)
NumberAgreement(M
i
,M
j
)
AnimacyAgreement(M
i
,M
j
)
StringMatch(M
i
,M
j
)
Distance(M
i
,M
j
)
Basic features used for English and Italian
Alias(M
i
,M
j
)
Apposition(M
i
,M
j
)
FirstMention(M
i
)
English
IsSubject(M
i
)
SemanticTreeCompatibility(M
i
,M
j
)
StrudelRelatedness(M
i
,M
j
)
German
InQuotedSpeech(M
i
), InQuotedSpeech(M
j
)
NodeDistance(M
i
,M
j
)
PartialMorphMatch(M
i
,M
j
)
GermanetRelatedness(M
i
,M
j
)
Italian
AliasItalian(M
i
,M
j
)
Table 1: Features used by BART: each feature describes
a pair of mentions {M
i
,M
j
}, i < j, where M
i
is a can-
didate antecedent and M
j
is a candidate anaphor
3.3 Resolution Algorithm
The BART toolkit supports several models of coref-
erence (pairwise modeling, rankers, semantic trees),
as well as different machine learning algorithms.
Our final setting relies on a pairwise maximum en-
tropy classifier for Italian and German.
Our English system is based on an entity-mention
model of coreference. The key concept of our model
is a Semantic Tree - a filecard associated to each dis-
course entity (cf. Section 3.2). Semantic trees are
used for both computing feature values and guiding
the resolution process.
We start by creating a Semantic Tree for each
mention. We process the document from left to
right, trying to find an antecedent for each men-
tion (candidate anaphor). When the antecedent is
found, we extend its Semantic Tree with the types,
attributes and relations of the anaphor, provided
they are mutually compatible. Consider, for ex-
106
ample, a list of mentions, containing, among oth-
ers, ?software from India?, ?the software? and ?soft-
ware from China?. Initially, BART creates the fol-
lowing semantic trees: ?(type: software) (relation:
from(India))?, ?(type: software)? and ?(type: soft-
ware) (relation: from(China))?. When the second
mention gets resolved to the first one, their seman-
tic trees are merged to ?(type: software) (relation:
from(India)?. Therefore, when we attempt to resolve
the third mention, both candidate antecedents are re-
jected, as their relation attributes are incompatible
with ?from(China)?. This approach helps us avoid
erroneous links (such as the link between the second
and the third mentions in our example) by leveraging
entity-level information.
4 Evaluation
The system was evaluated on the SemEval task 1
corpus by using the SemEval scorer.
First, we have evaluated our mention detection
modules: the system?s ability to recognize both the
mention extensions and the heads in the regular set-
ting. BART has achieved the best score for men-
tion detection in German and has shown reliable
figures for English. For Italian, the moderate per-
formance level is due to the different algorithms
for identifying the heads: the MaltParser (trained
on TUT: http://www.di.unito.it/?tutreeb) produces a
more semantic representation, while the SemEval
scorer seems to adopt a more syntactic approach.
Second, we have evaluated the quality of our
coreference resolution modules. For German, BART
has shown better performance than all the other sys-
tems on the regular track.
For English, the only language targeted by all sys-
tems, BART shows good performance over all met-
rics in the regular setting, usually only outperformed
by systems that were tuned to a particular metric.
Finally, the Italian version of BART shows re-
liable figures for coreference resolution, given the
mention alignment problem discussed above.
5 Conclusion
We have presented BART ? a multilingual toolkit
for coreference resolution. Due to its highly modu-
lar architecture, BART allows for efficient language-
specific feature engineering. Our effort represents
the first steps towards building a freely available
coreference resolution system for many languages.
References
Marco Baroni, Brian Murphy, Eduard Barbu, and Mas-
simo Poesio. 2010. Strudel: A corpus-based semantic
model based on properties and types. Cognitive Sci-
ence, 34(2):222?254.
Silvana Marianela Bernaola Biggio, Claudio Giuliano,
Massimo Poesio, Yannick Versley, Olga Uryupina, and
Roberto Zanoli. 2009. Local entity detection and
recognition task. In Proc. of Evalita-09.
Samuel Broscheit, Simone Paolo Ponzetto, Yannick Ver-
sley, and Massimo Poesio. 2010. Extending BART to
provide a coreference resolution system for German.
In Proc. of LREC ?10.
Marc Finthammer and Irene Cramer. 2008. Explor-
ing and navigating: Tools for GermaNet. In Proc. of
LREC ?08.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gulsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. Maltparser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Slav Petrov, Leon Barett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. of COLING-ACL-06.
Massimo Poesio, Olga Uryupina, and Yannick Versley.
2010. Creating a coreference resolution system for
Italian. In Proc. of LREC ?10.
Helmut Schmid, Arne Fitschen, and Ulrich Heid. 2004.
SMOR: A German computational morphology cover-
ing derivation, composition and inflection. In Proc. of
LREC ?04.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational Lin-
guistics (Special Issue on Computational Anaphora
Resolution), 27(4):521?544.
Yannick Versley, Simone Paolo Ponzetto, Massimo Poe-
sio, Vladimir Eidelman, Alan Jern, Jason Smith,
Xiaofeng Yang, and Alessandro Moschitti. 2008.
BART: A modular toolkit for coreference resolution.
In Proceedings of the Linguistic Coreference Work-
shop at the International Conference on Language Re-
sources and Evaluation (LREC-2008).
Zhibiao Wu and Martha Palmer. 1994. Verb semantics
and lexical selection. In Proc. of ACL-94, pages 133?
138.
Roberto Zanoli, Emiliano Pianta, and Claudio Giuliano.
2009. Named entity recognition through redundancy
driven classifier. In Proc. of Evalita-09.
107
