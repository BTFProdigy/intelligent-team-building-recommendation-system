Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 617?624,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Incorporating speech recognition confidence into
discriminative named entity recognition of speech data
Katsuhito Sudoh Hajime Tsukada Hideki Isozaki
NTT Communication Science Laboratories
Nippon Telegraph and Telephone Corporation
2-4 Hikaridai, Seika-cho, Keihanna Science City, Kyoto 619-0237, Japan
{sudoh,tsukada,isozaki}@cslab.kecl.ntt.co.jp
Abstract
This paper proposes a named entity recog-
nition (NER) method for speech recogni-
tion results that uses confidence on auto-
matic speech recognition (ASR) as a fea-
ture. The ASR confidence feature indi-
cates whether each word has been cor-
rectly recognized. The NER model is
trained using ASR results with named en-
tity (NE) labels as well as the correspond-
ing transcriptions with NE labels. In ex-
periments using support vector machines
(SVMs) and speech data from Japanese
newspaper articles, the proposed method
outperformed a simple application of text-
based NER to ASR results in NER F-
measure by improving precision. These
results show that the proposed method is
effective in NER for noisy inputs.
1 Introduction
As network bandwidths and storage capacities
continue to grow, a large volume of speech data
including broadcast news and PodCasts is becom-
ing available. These data are important informa-
tion sources as well as such text data as newspaper
articles and WWW pages. Speech data as infor-
mation sources are attracting a great deal of inter-
est, such as DARPA?s global autonomous language
exploitation (GALE) program. We also aim to use
them for information extraction (IE), question an-
swering, and indexing.
Named entity recognition (NER) is a key tech-
nique for IE and other natural language process-
ing tasks. Named entities (NEs) are the proper ex-
pressions for things such as peoples? names, loca-
tions? names, and dates, and NER identifies those
expressions and their categories. Unlike text data,
speech data introduce automatic speech recogni-
tion (ASR) error problems to NER. Although im-
provements to ASR are needed, developing a ro-
bust NER for noisy word sequences is also impor-
tant. In this paper, we focus on the NER of ASR
results and discuss the suppression of ASR error
problems in NER.
Most previous studies of the NER of speech
data used generative models such as hidden
Markov models (HMMs) (Miller et al, 1999;
Palmer and Ostendorf, 2001; Horlock and King,
2003b; Be?chet et al, 2004; Favre et al, 2005).
On the other hand, in text-based NER, better re-
sults are obtained using discriminative schemes
such as maximum entropy (ME) models (Borth-
wick, 1999; Chieu and Ng, 2003), support vec-
tor machines (SVMs) (Isozaki and Kazawa, 2002),
and conditional random fields (CRFs) (McCal-
lum and Li, 2003). Zhai et al (2004) applied a
text-level ME-based NER to ASR results. These
models have an advantage in utilizing various fea-
tures, such as part-of-speech information, charac-
ter types, and surrounding words, which may be
overlapped, while overlapping features are hard to
use in HMM-based models.
To deal with ASR error problems in NER,
Palmer and Ostendorf (2001) proposed an HMM-
based NER method that explicitly models ASR er-
rors using ASR confidence and rejects erroneous
word hypotheses in the ASR results. Such rejec-
tion is especially effective when ASR accuracy is
relatively low because many misrecognized words
may be extracted as NEs, which would decrease
NER precision.
Motivated by these issues, we extended their ap-
proach to discriminative models and propose an
NER method that deals with ASR errors as fea-
617
tures. We use NE-labeled ASR results for training
to incorporate the features into the NER model as
well as the corresponding transcriptions with NE
labels. In testing, ASR errors are identified by
ASR confidence scores and are used for the NER.
In experiments using SVM-based NER and speech
data from Japanese newspaper articles, the pro-
posed method increased the NER F-measure, es-
pecially in precision, compared to simply applying
text-based NER to the ASR results.
2 SVM-based NER
NER is a kind of chunking problem that can
be solved by classifying words into NE classes
that consist of name categories and such chunk-
ing states as PERSON-BEGIN (the beginning of
a person?s name) and LOCATION-MIDDLE (the
middle of a location?s name). Many discrimi-
native methods have been applied to NER, such
as decision trees (Sekine et al, 1998), ME mod-
els (Borthwick, 1999; Chieu and Ng, 2003), and
CRFs (McCallum and Li, 2003). In this paper, we
employ an SVM-based NER method in the follow-
ing way that showed good NER performance in
Japanese (Isozaki and Kazawa, 2002).
We define three features for each word: the
word itself, its part-of-speech tag, and its charac-
ter type. We also use those features for the two
preceding and succeeding words for context de-
pendence and use 15 features when classifying a
word. Each feature is represented by a binary
value (1 or 0), for example, ?whether the previous
word is Japan,? and each word is classified based
on a long binary vector where only 15 elements
are 1.
We have two problems when solving NER
using SVMs. One, SVMs can solve only a
two-class problem. We reduce multi-class prob-
lems of NER to a group of two-class problems
using the one-against-all approach, where each
SVM is trained to distinguish members of a
class (e.g., PERSON-BEGIN) from non-members
(PERSON-MIDDLE, MONEY-BEGIN, ... ). In this
approach, two or more classes may be assigned to
a word or no class may be assigned to a word. To
avoid these situations, we choose class c that has
the largest SVM output score gc(x) among all oth-
ers.
The other is that the NE label sequence must be
consistent; for example, ARTIFACT-END
must follow ARTIFACT-BEGIN or
Speech data
NE-labeled
transcriptions
Transcriptions ASR results
ASR-based
training data
Text-based
training data
           Manual
transcription ASR
NE labeling
Setting ASR
confidence
feature to 1
Alignment
&
identifying
ASR errors
and NEs
Figure 1: Procedure for preparing training data.
ARTIFACT-MIDDLE. We use a Viterbi search to
obtain the best and consistent NE label sequence
after classifying all words in a sentence, based
on probability-like values obtained by applying
sigmoid function sn(x) = 1/(1 + exp(??nx)) to
SVM output score gc(x).
3 Proposed method
3.1 Incorporating ASR confidence into NER
In the NER of ASR results, ASR errors cause NEs
to be missed and erroneous NEs to be recognized.
If one or more words constituting an NE are mis-
recognized, we cannot recognize the correct NE.
Even if all words constituting an NE are correctly
recognized, we may not recognize the correct NE
due to ASR errors on context words. To avoid
this problem, we model ASR errors using addi-
tional features that indicate whether each word is
correctly recognized. Our NER model is trained
using ASR results with a feature, where feature
values are obtained through alignment to the cor-
responding transcriptions. In testing, we estimate
feature values using ASR confidence scores. In
this paper, this feature is called the ASR confidence
feature.
Note that we only aim to identify NEs that are
correctly recognized by ASR, and NEs containing
ASR errors are not regarded as NEs. Utilizing er-
roneous NEs is a more difficult problem that is be-
yond the scope of this paper.
3.2 Training NER model
Figure 1 illustrates the procedure for preparing
training data from speech data. First, the speech
618
data are manually transcribed and automatically
recognized by the ASR. Second, we label NEs
in the transcriptions and then set the ASR con-
fidence feature values to 1 because the words in
the transcriptions are regarded as correctly recog-
nized words. Finally, we align the ASR results to
the transcriptions to identify ASR errors for the
ASR confidence feature values and to label cor-
rectly recognized NEs in the ASR results. Note
that we label the NEs in the ASR results that exist
in the same positions as the transcriptions. If a part
of an NE is misrecognized, the NE is ignored, and
all words for the NE are labeled as non-NE words
(OTHER). Examples of text-based and ASR-based
training data are shown in Tables 1 and 2. Since
the name Murayama Tomiichi in Table 1 is mis-
recognized in ASR, the correctly recognized word
Murayama is also labeled OTHER in Table 2. An-
other approach can be considered, where misrec-
ognized words are replaced by word error symbols
such as those shown in Table 3. In this case, those
words are rejected, and those part-of-speech and
character type features are not used in NER.
3.3 ASR confidence scoring for using the
proposed NER model
ASR confidence scoring is an important technique
in many ASR applications, and many methods
have been proposed including using word poste-
rior probabilities on word graphs (Wessel et al,
2001), integrating several confidence measures us-
ing neural networks (Schaaf and Kemp, 1997),
using linear discriminant analysis (Kamppari and
Hazen, 2000), and using SVMs (Zhang and Rud-
nicky, 2001).
Word posterior probability is a commonly used
and effective ASR confidence measure. Word pos-
terior probability p([w; ?, t]|X) of word w at time
interval [?, t] for speech signal X is calculated as
follows (Wessel et al, 2001):
p([w; ?, t]|X)
=
?
W?W [w;?,t]
{
p(X|W ) (p(W ))?
}?
p(X) , (1)
where W is a sentence hypothesis, W [w; ?, t] is
the set of sentence hypotheses that include w in
[?, t], p(X|W ) is a acoustic model score, p(W )
is a language model score, ? is a scaling param-
eter (?<1), and ? is a language model weight.
? is used for scaling the large dynamic range of
Word Confidence NE label
Murayama 1 PERSON-BEGIN
Tomiichi 1 PERSON-END
shusho 1 OTHER
wa 1 OTHER
nento 1 DATE-SINGLE
Table 1: An example of text-based training data.
Word Confidence NE label
Murayama 1 OTHER
shi 0 OTHER
ni 0 OTHER
ichi 0 OTHER
shiyo 0 OTHER
wa 1 OTHER
nento 1 DATE-SINGLE
Table 2: An example of ASR-based training data.
Word Confidence NE label
Murayama 1 OTHER
(error) 0 OTHER
(error) 0 OTHER
(error) 0 OTHER
(error) 0 OTHER
wa 1 OTHER
nento 1 DATE-SINGLE
Table 3: An example of ASR-based training data
with word error symbols.
p(X|W )(p(W ))? to avoid a few of the top hy-
potheses dominating posterior probabilities. p(X)
is approximated by the sum over all sentence hy-
potheses and is denoted as
p(X) =
?
W
{
p(X|W ) (p(W ))?
}? . (2)
p([w; ?, t]|X) can be efficiently calculated using a
forward-backward algorithm.
In this paper, we use SVMs for ASR confidence
scoring to achieve a better performance than when
using word posterior probabilities as ASR confi-
dence scores. SVMs are trained using ASR re-
sults, whose errors are known through their align-
ment to their reference transcriptions. The follow-
ing features are used for confidence scoring: the
word itself, its part-of-speech tag, and its word
posterior probability; those of the two preceding
and succeeding words are also used. The word
itself and its part-of-speech are also represented
619
by a set of binary values, the same as with an
SVM-based NER. Since all other features are bi-
nary, we reduce real-valued word posterior prob-
ability p to ten binary features for simplicity: (if
0 < p ? 0.1, if 0.1 < p ? 0.2, ... , and if
0.9 < p ? 1.0). To normalize SVMs? output
scores for ASR confidence, we use a sigmoid func-
tion sw(x) = 1/(1 + exp(??wx)). We use these
normalized scores as ASR confidence scores. Al-
though a large variety of features have been pro-
posed in previous studies, we use only these sim-
ple features and reserve the other features for fur-
ther studies.
Using the ASR confidence scores, we estimate
whether each word is correctly recognized. If the
ASR confidence score of a word is greater than
threshold tw, the word is estimated as correct, and
we set the ASR confidence feature value to 1; oth-
erwise we set it to 0.
3.4 Rejection at the NER level
We use the ASR confidence feature to suppress
ASR error problems; however, even text-based
NERs sometimes make errors. NER performance
is a trade-off between missing correct NEs and
accepting erroneous NEs, and requirements dif-
fer by task. Although we can tune the parame-
ters in training SVMs to control the trade-off, it
seems very hard to find appropriate values for all
the SVMs. We use a simple NER-level rejection
by modifying the SVM output scores for the non-
NE class (OTHER). We add constant offset value to
to each SVM output score for OTHER. With a large
to, OTHER becomes more desirable than the other
NE classes, and many words are classified as non-
NE words and vice versa. Therefore, to works as a
parameter for NER-level rejection. This approach
can also be applied to text-based NER.
4 Experiments
We conducted the following experiments related
to the NER of speech data to investigate the per-
formance of the proposed method.
4.1 Setup
In the experiment, we simulated the procedure
shown in Figure 1 using speech data from the
NE-labeled text corpus. We used the training
data of the Information Retrieval and Extraction
Exercise (IREX) workshop (Sekine and Eriguchi,
2000) as the text corpus, which consisted of 1,174
Japanese newspaper articles (10,718 sentences)
and 18,200 NEs in eight categories (artifact, or-
ganization, location, person, date, time, money,
and percent). The sentences were read by 106
speakers (about 100 sentences per speaker), and
the recorded speech data were used for the exper-
iments. The experiments were conducted with 5-
fold cross validation, using 80% of the 1,174 ar-
ticles and the ASR results of the corresponding
speech data for training SVMs (both for ASR con-
fidence scoring and for NER) and the rest for the
test.
We tokenized the sentences into words and
tagged the part-of-speech information using the
Japanese morphological analyzer ChaSen 1 2.3.3
and then labeled the NEs. Unreadable to-
kens such as parentheses were removed in to-
kenization. After tokenization, the text cor-
pus had 264,388 words of 60 part-of-speech
types. Since three different kinds of charac-
ters are used in Japanese, the character types
used as features included: single-kanji
(words written in a single Chinese charac-
ter), all-kanji (longer words written in Chi-
nese characters), hiragana (words written
in hiragana Japanese phonograms), katakana
(words written in katakana Japanese phono-
grams), number, single-capital (words
with a single capitalized letter), all-capital,
capitalized (only the first letter is capital-
ized), roman (other roman character words), and
others (all other words). We used all the fea-
tures that appeared in each training set (no feature
selection was performed). The chunking states in-
cluded in the NE classes were: BEGIN (beginning
of a NE), MIDDLE (middle of a NE), END (ending
of a NE), and SINGLE (a single-word NE). There
were 33 NE classes (eight categories * four chunk-
ing states + OTHER), and therefore we trained 33
SVMs to distinguish words of a class from words
of other classes. For NER, we used an SVM-based
chunk annotator YamCha 2 0.33 with a quadratic
kernel (1 + ~x ? ~y)2 and a soft margin parameter
of SVMs C=0.1 for training and applied sigmoid
function sn(x) with ?n=1.0 and Viterbi search to
the SVMs? outputs. These parameters were exper-
imentally chosen using the test set.
We used an ASR engine (Hori et al, 2004) with
a speaker-independent acoustic model. The lan-
1http://chasen.naist.jp/hiki/ChaSen/ (in Japanese)
2http://www.chasen.org/?taku/software/yamcha/
620
guage model was a word 3-gram model, trained
using other Japanese newspaper articles (about
340 M words) that were also tokenized using
ChaSen. The vocabulary size of the word 3-gram
model was 426,023. The test-set perplexity over
the text corpus was 76.928. The number of out-
of-vocabulary words was 1,551 (0.587%). 223
(1.23%) NEs in the text corpus contained such out-
of-vocabulary words, so those NEs could not be
correctly recognized by ASR. The scaling param-
eter ? was set to 0.01, which showed the best ASR
error estimation results using word posterior prob-
abilities in the test set in terms of receiver operator
characteristic (ROC) curves. The language model
weight ? was set to 15, which is a commonly used
value in our ASR system. The word accuracy ob-
tained using our ASR engine for the overall dataset
was 79.45%. In the ASR results, 82.00% of the
NEs in the text corpus remained. Figure 2 shows
the ROC curves of ASR error estimation for the
overall five cross-validation test sets, using SVM-
based ASR confidence scoring and word posterior
probabilities as ASR confidence scores, where
True positive rate
= # correctly recognized words estimated as correct
# correctly recognized words
False positive rate
= # misrecognized words estimated as correct
# misrecognized words .
In SVM-based ASR confidence scoring, we used
the quadratic kernel and C=0.01. Parameter ?w of
sigmoid function sw(x) was set to 1.0. These pa-
rameters were also experimentally chosen. SVM-
based ASR confidence scoring showed better per-
formance in ASR error estimation than simple
word posterior probabilities by integrating mul-
tiple features. Five values of ASR confidence
threshold tw were tested in the following experi-
ments: 0.2, 0.3, 0.4, 0.5, and 0.6 (shown by black
dots in Figure 2).
4.2 Evaluation metrics
Evaluation was based on an averaged NER F-
measure, which is the harmonic mean of NER pre-
cision and recall:
NER precision = # correctly recognized NEs
# recognized NEs
NER recall = # correctly recognized NEs
# NEs in original text
.
 0
 20
 40
 60
 80
 100
 0  20  40  60  80  100
Tr
ue
 p
os
itv
e 
ra
te
 (%
)
False positive rate (%)
=0.3
=0.4
SVM-based
confidence
scoring
Word posterior probability
tw
t
t
t
w
=0.2tw
=0.6w
=0.5w
Figure 2: SVM-based confidence scoring outper-
forms word posterior probability for ASR error es-
timation.
A recognized NE was accepted as correct if and
only if it appeared in the same position as its refer-
ence NE through alignment, in addition to having
the correct NE surface and category, because the
same NEs might appear more than once. Compar-
isons of NE surfaces did not include differences
in word segmentation because of the segmentation
ambiguity in Japanese. Note that NER recall with
ASR results could not exceed the rate of the re-
maining NEs after ASR (about 82%) because NEs
containing ASR errors were always lost.
In addition, we also evaluated the NER perfor-
mance in NER precision and recall with NER-
level rejection using the procedure in Section 3.4,
by modifying the non-NE class scores using offset
value to.
4.3 Compared methods
We compared several combinations of features
and training conditions for evaluating the effect of
incorporating the ASR confidence feature and in-
vestigating differences among training data: text-
based, ASR-based, and both.
Baseline does not use the ASR confidence fea-
ture and is trained using text-based training data
only.
NoConf-A does not use the ASR confidence
feature and is trained using ASR-based training
data only.
621
Method Confidence Training Test F-measure (%) Precision (%) Recall (%)
Baseline Text ASR 67.00 70.67 63.70
NoConf-A Not used ASR ASR 65.52 78.86 56.05
NoConf-TA Text+ASR ASR 66.95 77.55 58.91
Conf-A ASR ASR? 67.69 76.69 60.59
Proposed Used Text+ASR ASR? 69.02 78.13 61.81
Conf-Reject Used? Text+ASR ASR? 68.77 77.57 61.78
Conf-UB Used Text+ASR ASR?? 73.14 87.51 62.83
Transcription Not used Text Text 84.04 86.27 81.93
Table 4: NER results in averaged NER F-measure, precision, and recall without considering NER-level
rejection (to = 0). ASR word accuracy was 79.45%, and 82.00% of NEs remained in ASR results.
(?Unconfident words were rejected and replaced by word error symbols, ?tw = 0.4, ??ASR errors were
known.)
NoConf-TA does not use the ASR confidence
feature and is trained using both text-based and
ASR-based training data.
Conf-A uses the ASR confidence feature and is
trained using ASR-based training data only.
Proposed uses the ASR confidence feature and
is trained using both text-based and ASR-based
training data.
Conf-Reject is almost the same as Proposed,
but misrecognized words are rejected and replaced
with word error symbols, as described at the end
of Section 3.2.
The following two methods are for reference.
Conf-UB assumes perfect ASR confidence scor-
ing, so the ASR errors in the test set are known.
The NER model, which is identical to Proposed,
is regarded as the upper-boundary of Proposed.
Transcription applies the same model as Base-
line to reference transcriptions, assuming word ac-
curacy is 100%.
4.4 NER Results
In the NER experiments, Proposed achieved the
best results among the above methods. Table
4 shows the NER results obtained by the meth-
ods without considering NER-level rejection (i.e.,
to = 0), using threshold tw = 0.4 for Conf-A,
Proposed, and Conf-Reject, which resulted in the
best NER F-measures (see Table 5). Proposed
showed the best F-measure, 69.02%. It outper-
formed Baseline by 2.0%, with a 7.5% improve-
ment in precision, instead of a recall decrease of
1.9%. Conf-Reject showed slightly worse results
Method tw F (%) P (%) R (%)
0.2 66.72 71.28 62.71
0.3 67.32 73.68 61.98
Conf-A 0.4 67.69 76.69 60.59
0.5 67.04 79.64 57.89
0.6 64.48 81.90 53.14
0.2 68.08 72.54 64.14
0.3 68.70 75.11 63.31
Proposed 0.4 69.02 78.13 61.81
0.5 68.17 80.88 58.93
0.6 65.39 83.00 53.96
0.2 68.06 72.49 64.14
0.3 68.61 74.88 63.31
Conf-Reject 0.4 68.77 77.57 61.78
0.5 67.93 80.23 58.91
0.6 64.93 82.05 53.73
Table 5: NER results with varying ASR confi-
dence score threshold tw for Conf-A, Proposed,
and Conf-Reject. (F: F-measure, P: precision, R:
recall)
than Proposed. Conf-A resulted in 1.3% worse F-
measure than Proposed. NoConf-A and NoConf-
TA achieved 7-8% higher precision than Base-
line; however, their F-measure results were worse
than Baseline because of the large drop of recall.
The upper-bound results of the proposed method
(Conf-UB) in F-measure was 73.14%, which was
4% higher than Proposed.
Figure 3 shows NER precision and recall with
NER-level rejection by to for Baseline, NoConf-
TA, Proposed, Conf-UB, and Transcription. In the
figure, black dots represent results with to = 0,
as shown in Table 4. By all five methods, we
622
 0
 20
 40
 60
 80
 100
 50  60  70  80  90  100
Reca
ll (%)
Precision (%)
Baseline
NoConf-TA
ProposedConf-UB
Transcription
Figure 3: NER precision and recall with NER-
level rejection by to
obtained higher precision with to > 0. Pro-
posed achieved more than 5% higher precision
than Baseline on most recall ranges and showed
higher precision than NoConf-TA on recall ranges
higher than about 35%.
5 Discussion
The proposed method effectively improves NER
performance, as shown by the difference between
Proposed and Baseline in Tables 4 and 5. Improve-
ment comes from two factors: using both text-
based and ASR-based training data and incorpo-
rating ASR confidence feature. As shown by the
difference between Baseline and the methods us-
ing ASR-based training data (NoConf-A, NoConf-
TA, Conf-A, Proposed, Conf-Reject), ASR-based
training data increases precision and decreases
recall. In ASR-based training data, all words
constituting NEs that contain ASR errors are re-
garded as non-NE words, and those NE exam-
ples are lost in training, which emphasizes NER
precision. When text-based training data are also
available, they compensate for the loss of NE
examples and recover NER recall, as shown by
the difference between the methods without text-
based training data (NoConf-A, Conf-A) and those
with (NoConf-TA, Proposed). The ASR confi-
dence feature also increases NER recall, as shown
by the difference between the methods without
it (NoConf-A, NoConf-TA) and with it (Conf-A,
Proposed). This suggests that the ASR confidence
feature helps distinguish whether ASR error influ-
ences NER and suppresses excessive rejection of
NEs around ASR errors.
With respect to the ASR confidence feature, the
small difference between Conf-Reject and Pro-
posed suggests that ASR confidence is a more
dominant feature in misrecognized words than the
other features: the word itself, its part-of-speech
tag, and its character type. In addition, the dif-
ference between Conf-UB and Proposed indicated
that there is room to improve NER performance
with better ASR confidence scoring.
NER-level rejection also increased precision, as
shown in Figure 3. We can control the trade-
off between precision and recall with to accord-
ing to the task requirements, even in text-based
NER. In the NER of speech data, we can ob-
tain much higher precision using both ASR-based
training data and NER-level rejection than using
either one.
6 Related work
Recent studies on the NER of speech data consider
more than 1-best ASR results in the form of N-best
lists and word lattices. Using many ASR hypothe-
ses helps recover the ASR errors of NE words in
1-best ASR results and improves NER accuracy.
Our method can be extended to multiple ASR hy-
potheses.
Generative NER models were used for multi-
pass ASR and NER searches using word lattices
(Horlock and King, 2003b; Be?chet et al, 2004;
Favre et al, 2005). Horlock and King (2003a)
also proposed discriminative training of their NER
models. These studies showed the advantage of
using multiple ASR hypotheses, but they do not
use overlapping features.
Discriminative NER models were also applied
to multiple ASR hypotheses. Zhai et al (2004) ap-
plied text-based NER to N-best ASR results, and
merged the N-best NER results by weighted vot-
ing based on several sentence-level results such as
ASR and NER scores. Using the ASR confidence
feature does not depend on SVMs and can be used
with their method and other discriminative mod-
els.
7 Conclusion
We proposed a method for NER of speech data
that incorporates ASR confidence as a feature
of discriminative NER, where the NER model
623
is trained using both text-based and ASR-based
training data. In experiments using SVMs,
the proposed method showed a higher NER F-
measure, especially in terms of improving pre-
cision, than simply applying text-based NER to
ASR results. The method effectively rejected erro-
neous NEs due to ASR errors with a small drop of
recall, thanks to both the ASR confidence feature
and ASR-based training data. NER-level rejection
also effectively increased precision.
Our approach can also be used in other tasks
in spoken language processing, and we expect it
to be effective. Since confidence itself is not lim-
ited to speech, our approach can also be applied to
other noisy inputs, such as optical character recog-
nition (OCR). For further improvement, we will
consider N-best ASR results or word lattices as in-
puts and introduce more speech-specific features
such as word durations and prosodic features.
Acknowledgments We would like to thank
anonymous reviewers for their helpful comments.
References
Fre?de?ric Be?chet, Allen L. Gorin, Jeremy H. Wright,
and Dilek Hakkani-Tu?r. 2004. Detecting and ex-
tracting named entities from spontaneous speech in a
mixed-initiative spoken dialogue context: How May
I Help You? Speech Communication, 42(2):207?
225.
Andrew Borthwick. 1999. A Maximum Entropy Ap-
proach to Named Entity Recognition. Ph.D. thesis,
New York University.
Hai Leong Chieu and Hwee Tou Ng. 2003. Named en-
tity recognition with a maximum entropy approach.
In Proc. CoNLL, pages 160?163.
Beno??t Favre, Fre?de?ric Be?chet, and Pascal Noce?ra.
2005. Robust named entity extraction from large
spoken archives. In Proc. HLT-EMNLP, pages 491?
498.
Takaaki Hori, Chiori Hori, and Yasuhiro Minami.
2004. Fast on-the-fly composition for weighted
finite-state transducers in 1.8 million-word vocab-
ulary continuous-speech recognition. In Proc. IC-
SLP, volume 1, pages 289?292.
James Horlock and Simon King. 2003a. Discrimi-
native methods for improving named entity extrac-
tion on speech data. In Proc. EUROSPEECH, pages
2765?2768.
James Horlock and Simon King. 2003b. Named en-
tity extraction from word lattices. In Proc. EU-
ROSPEECH, pages 1265?1268.
Hideki Isozaki and Hideto Kazawa. 2002. Efficient
support vector classifiers for named entity recogni-
tion. In Proc. COLING, pages 390?396.
Simo O. Kamppari and Timothy J. Hazen. 2000. Word
and phone level acoustic confidence scoring. In
Proc. ICASSP, volume 3, pages 1799?1802.
Andrew McCallum and Wei Li. 2003. Early results for
named entity recognition with conditional random
fields, feature induction and web-enhanced lexicons.
In Proc. CoNLL, pages 188?191.
David Miller, Richard Schwartz, Ralph Weischedel,
and Rebecca Stone. 1999. Named entity extraction
from broadcast news. In Proceedings of the DARPA
Broadcast News Workshop, pages 37?40.
David D. Palmer and Mari Ostendorf. 2001. Im-
proving information extraction by modeling errors
in speech recognizer output. In Proc. HLT, pages
156?160.
Thomas Schaaf and Thomas Kemp. 1997. Confidence
measures for spontaneous speech recognition. In
Proc. ICASSP, volume II, pages 875?878.
Satoshi Sekine and Yoshio Eriguchi. 2000. Japanese
named entity extraction evaluation - analysis of re-
sults. In Proc. COLING, pages 25?30.
Satoshi Sekine, Ralph Grishman, and Hiroyuki Shin-
nou. 1998. A decision tree method for finding and
classifying names in Japanese texts. In Proc. the
Sixth Workshop on Very Large Corpora, pages 171?
178.
Frank Wessel, Ralf Schlu?ter, Klaus Macherey, and
Hermann Ney. 2001. Confidence measures for
large vocabulary continuous speech recognition.
IEEE Transactions on Speech and Audio Process-
ing, 9(3):288?298.
Lufeng Zhai, Pascale Fung, Richard Schwartz, Marine
Carpuat, and Dekai Wu. 2004. Using N-best lists
for named entity recognition from chinese speech.
In Proc. HLT-NAACL, pages 37?40.
Rong Zhang and Alexander I. Rudnicky. 2001. Word
level confidence annotation using combinations of
features. In Proc. EUROSPEECH, pages 2105?
2108.
624
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 439?446,
Beijing, August 2010
Hierarchical Phrase-based Machine Translation with Word-based
Reordering Model
Katsuhiko Hayashi*, Hajime Tsukada**
Katsuhito Sudoh**, Kevin Duh**, Seiichi Yamamoto*
*Doshisha University
katsuhiko-h@is.naist.jp, seyamamo@mail.doshisha.ac.jp
**NTT Communication Science Laboratories
tsukada, sudoh, kevinduh@cslab.kecl.ntt.co.jp
Abstract
Hierarchical phrase-based machine trans-
lation can capture global reordering with
synchronous context-free grammar, but
has little ability to evaluate the correctness
of word orderings during decoding. We
propose a method to integrate word-based
reordering model into hierarchical phrase-
based machine translation to overcome
this weakness. Our approach extends the
synchronous context-free grammar rules
of hierarchical phrase-based model to in-
clude reordered source strings, allowing
efficient calculation of reordering model
scores during decoding. Our experimen-
tal results on Japanese-to-English basic
travel expression corpus showed that the
BLEU scores obtained by our proposed
system were better than those obtained by
a standard hierarchical phrase-based ma-
chine translation system.
1 Introduction
Hierarchical phrase-based machine translation
(Chiang, 2007; Watanabe et al, 2006) is one of
the promising statistical machine translation ap-
proaches (Brown et al, 1993). Its model is for-
mulated by a synchronous context-free grammar
(SCFG) which captures the syntactic information
between source and target languages. Although
the model captures global reordering by SCFG,
it does not explicitly introduce reordering model
to constrain word order. In contrast, lexicalized
reordering models (Tillman, 2004; Koehn et al,
2005; Nagata et al, 2006) are extensively used
for phrase-based translation. These lexicalized re-
ordering models cannot be directly applied to hi-
erarchical phrased-based translation since the hi-
erarchical phrase representation uses nonterminal
symbols.
To handle global reordering in phrase-based
translation, various preprocessing approaches
have been proposed, where the source sentence
is reordered to target language order beforehand
(Xia and McCord, 2004; Collins et al, 2005; Li et
al., 2007; Tromble and Eisner, 2009). However,
preprocessing approaches cannot utilize other in-
formation in the translation model and target lan-
guage model, which has been proven helpful in
decoding.
This paper proposes a method that incorpo-
rates word-based reordering model into hierarchi-
cal phrase-based translation to constrain word or-
der. In this paper, we adopt the reordering model
originally proposed by Tromble and Eisner (2009)
for the preprocessing approach in phrase-based
translation. To integrate the word-based reorder-
ing model, we added a reordered source string
into the right-hand-side of SCFG?s rules. By this
extension, our system can generate the reordered
source sentence as well as target sentence and is
able to efficiently calculate the score of the re-
ordering model. Our method utilizes the transla-
tion model and target language model as well as
the reordering model during decoding. This is an
advantage of our method over the preprocessing
approach.
The remainder of this paper is organized as
follows. Section 2 describes the concept of our
approach. Section 3 briefly reviews our pro-
posed method on hierarchical phrase-based ma-
439
Standard SCFG X ?< X1 wa jinsei no X2 da , X1 is X2 of life>
SCFG (move-to-front) X ?< X1 wa jinsei no X2 da , wa X1 da X2 no jinsei , X1 is X2 of life>
SCFG (attach) X ?< X1 wa jinsei no X2 da , X1 wa da X2 no jinsei , X1 is X2 of life>
Table 1: A Japanese-to-English example of various SCFG?s rule representations. Japanese words are
romanized. Our proposed representation of rules has reordered source string to generate reordered
source sentence S? as well as target sentence T . The ?move-to-front? means Tromble and Eisner (2009)
?s algorithm and the ?attach? means Al-Onaizan and Papineni (2006) ?s algorithm.
chine translation model. We experimentally com-
pare our proposed system to a standard hierarchi-
cal phrase-based system on Japanese-to-English
translation task in Section 4. Then we discuss on
related work in Section 5 and conclude this paper
in Section 6.
2 The Concept of Our Approach
The preprocessing approach (Xia and McCord,
2004; Collins et al, 2005; Li et al, 2007; Tromble
and Eisner, 2009) splits translation procedure into
two stages:
S ? S? ? T (1)
where S is a source sentence, S? is a reordered
source sentence with respect to the word order of
target sentence T . Preprocessing approach has the
very deterministic and hard decision in reorder-
ing. To overcome the problem, Li et al (2007)
proposed k-best appoach. However, even with a
k-best approach, it is difficult to generate good hy-
potheses S? by using only a reordering model.
In this paper, we directly integrated the reorder-
ing model into the decoder in order to use the
reordering model together with other information
in the hierarchical phrase-based translation model
and target language model. Our approach is ex-
pressed as the following equation.
S ? (S? , T ). (2)
Our proposed method generates the reordered
source sentence S? by SCFG and evaluates the
correctness of the reorderings using a word-based
reordering model of S? which will be introduced
in section 3.4.
Figure 1: A derivation tree for Japanse-to-English
translation.
3 Hierarchical Phrase-based Model
Extension
3.1 Hierarchical Phrase-based Model
Hierarchical phrase-based model (Chiang, 2007)
induces rules of the form
X ?< ?, ?,?, w > (3)
where X is a non-terminal symbol, ? is a se-
quence string of non-terminals and source termi-
nals, ? is a sequence string of non-terminals and
target terminals. ? is a one-to-one correspon-
dence for the non-terminals appeared in ? and ?.
Given a source sentence S, the translation task
under this model can be expressed as
T? = T
(
argmax
D:S(D)=S
w(D)
)
(4)
where D is a derivation and w(D) is a score of
the derivation. Decoder seeks a target sentence
440
Figure 2: Reordered source sentence generated by
our proposed system.
T (D) which has the highest score w(D). S(D)
is a source sentence under a derivation D. Fig-
ure 1 shows the example of Japanese-to-English
translation by hierarchical phrase-based machine
translation model.
3.2 Rule Extension
To generate reordered source sentence S? as well
as target sentence T , we extend hierarchical
phrase rule expressed in Equation 3 to
X ?< ?, ?? , ?,?, w > (5)
where ?? is a sequence string of non-terminals and
source terminals, which is reordered ? with re-
spect to the word order of target string ?. The
reason why we add ?? to rules is to efficiently cal-
culate the reordering model scores. If each rule
does not have ?? , the decoder need to keep word
alignments because we cannot know word order
of S? without them. The calculation of reorder-
ing model scores using word alignments is very
wasteful when decoding.
The translation task under our model extends
Equation 4 to the following equation:
T? = (S?? , T? ) = (S? , T )
(
argmax
D:S(D)=S
w(D)
)
. (6)
Our system generates the reordered source sen-
tence S? as well as target sentence T . Figure 2
shows the generated reordered source sentence S?
Uni-gram Features
sr, s-posr
sr
s-posr
sl, s-posl
sl
s-posl
Bi-gram Features
sr, s-posr, sl, s-posl
s-posr, sl, s-posl
sr, sl, s-posl
sr, s-posr, s-posl
sr, s-posr, sl
sr, sl
s-posr, s-posl
Table 2: Features used by Word-based Reordering
Model. pos means part-of-speech tag.
when translating the example of Figure 1. Note
that the structure of S? is the same as that of target
sentence T . The decoder generates both Figure 2
and the right hand side of Figure 1, allowing us to
score both global and local word reorderings.
To add ?? to rules, we permuted ? into ?? after
rule extraction based on Grow-diag-final (Koehn
et al, 2005) alignment by GIZA++ (Och and Ney,
2003). To do this permutation on rules, we ap-
plied two methods. One is the same algorithm
as Tromble and Eisner (2009), which reorders
aligned source terminals and nonterminals in the
same order as that of target side and moves un-
aligned source terminals to the front of aligned
terminals or nonterminals (move-to-front). The
other is the same algorithm as AI-Onaizan and
Papineni (2006), which differs from Tromble and
Eisner?s approach in attaching unaligned source
terminals to the closest prealigned source termi-
nals or nonterminals (attach). This extension of
adding ?? does not increase the number of rules.
Table 1 shows a Japanese-to-English example
of the representation of rules for our proposed sys-
tem. Japanese words are romanized. Suppose that
source-side string is (X1 wa jinsei no X2 da) and
target-side string is (X1 is X2 of life) and their
word alignments are a=((jinsei , life) , (no , of)
, (da , is)). Source-side aligned words and non-
terminal symbols are sorted into the same order of
target string. Source-side unaligned word (wa) is
moved to the front or right of the prealigned sym-
bol (X1).
441
Surrounding Word Pos Features
s-posr, s-posr + 1, s-posl ? 1, s-posl
s-posr ? 1, s-posr, s-posl ? 1, s-posl
s-posr, s-posr + 1, s-posl, s-posl + 1
s-posr ? 1, s-posr, s-posl, s-posl + 1
Table 3: The Example of Context Features
3.3 Word-based Reordering Model
We utilize the following score(S?) as a feature for
the word-based reordering model. This is incor-
polated into the log-linear model (Och and Ney,
2002) of statistical machine translation.
score(S?) =
?
i,j:1?i<j?n
B[s?i, s
?
j ] (7)
B[s?l, s
?
r] = ? ? ?(s
?
l, s
?
r) (8)
where n is the length of reordered source sen-
tence S? (= (s?1 . . . s
?
n)), ? is a weight vector and
? is a vector of features. This reordering model,
which is originally proposed by Tromble and Eis-
ner (2009), can assign a score to any possible per-
mutation of source sentences. Intuitively B[s?l, s
?
r]
represents the score of ordering s?l before s
?
r; the
higher the value, the more we prefer word s?l oc-
curs before s?r. Whether S
?
l should occur before S
?
r
depends on how often this reordering occurs when
we reorder the source to target sentence order.
To train B, we used binary feature functions
? as used in (Tromble and Eisner, 2009), which
were introduced for dependency parsing by Mc-
Donald et al (2005). Table 2 shows the kind
of features we used in our experiments. We did
not use context features like surrounding word pos
features in Table 3 because they were not useful in
our preliminary experiments and propose an effi-
cient implementation described in the next section
in order to calculate this reordering model when
decoding. To train the parameter ?, we used the
perceptron algorithm following Tromble and Eis-
ner (2009).
3.4 Integration to Cube Pruning
CKY parsing and cube-pruning are used for de-
coding of hierarchical phrase-based model (Chi-
ang, 2007). Figure 3 displays that hierarchical
phrase-based decoder seeks new span [1,7] items
Figure 3: Creating new items from subitems and
rules, that have a span [1,7] in source sentence.
with rules, utilizing subspan [1,3] items and sub-
span [4,7] items. In this example, we use 2-gram
language model and +LM decoding. uni(?) means
1-gram language model cost for heuristics and in-
teraction usually means language model cost that
cannot be calculated offline. Here, we introduce
our two implementations to calculate word-based
reordering model scores in this decoding algo-
rithm.
First, we explain a naive implementation shown
in the left side of Figure 4. This algorithm per-
forms the same calculation of reordering model as
that of language model. Each item keeps a part of
reordered source sentence. The reordering score
of new item can be calculated as interaction cost
when combining subitems with the rule.
The right side of Figure 4 shows our pro-
posed implementation. This implementation can
be adopted to decoding only when we do not use
context features like surrounding word pos fea-
tures in Table 3 (and consider a distance between
words in features). If a span is given, the reorder-
ing scores of new item can be calculated for each
rule, being independent from the word order of
reordered source segment of a subitem. So, the
reordering model scores can be calculated for all
rules with spans by using a part of the input source
sentence before sorting them for cube pruning.
We expect this sorting of rules with reordering
442
Figure 4: The ?naive? and ?proposed? implementation to calculate the reordering cost of new items.
model scores will have good influence on cube
pruning. The right hand side of Figure 4 shows
the diffrence between naive and proposed imple-
mentation (S? is not shown to allow for a clear pre-
sentation). Note the difference is in where/when
the reordering scores are inserted: together with
the N -gram scores in the case of naive implemen-
tation; incorpolated into sorted rules for the pro-
posed implementation.
4 Experiment
4.1 Purpose
To reveal the effectiveness of integrating the re-
ordering model into decoder, we compared the
following setups:
? baseline: a standard hierarchical phrase-
based machine translation (Hiero) system.
? preprocessing: applied Tromble and Eisner?s
approach, then translate by Hiero system.
? Hiero system + reordering model: integrated
reordering model into Hiero system.
We used the Joshua Decoder (Li and Khudanpur,
2008) as the baseline Hiero system. This decoder
uses a log-linear model with seven features, which
consist of N -gram language model PLM (T ), lex-
ical translation model Pw(?|?), Pw(?|?), rule
translation model P (?|?), P (?|?), word penalty
and arity penalty.
The ?Hiero + Reordering model? system has
word-based reordering model as an additional fea-
ture to baseline features. For this approach, we
use two systems. One has ?move-to-front? sys-
tem and the other is ?attach? system explained in
Section 3.2. We implemented our proposed algo-
rithm in Section 3.4 to both ?Hiero + Reordering
model? systems. As for beam width, we use the
same setups for each system.
4.2 Data Set
Data Sent. Word. Avg. leng
Training ja 200.8K 2.4M 12.0
en 200.8K 2.3M 11.5
Development ja 1.0K 10.3K 10.3
en 1.0K 9.8K 9.8
Test ja 1.0K 14.2K 14.2
en 1.0K 13.5K 13.5
Table 4: The Data statistics
For experiments we used a Japanese-English
basic travel expression corpus (BTEC). Japanese
word order is linguistically very different from
English and we think Japanese-English pair is
a very good test bed for evaluating reordering
model.
443
XXXXXXXXXXXSystem
Metrics BLEU PER
Baseline (Hiero) 28.09 39.68
Preprocessing 17.32 45.27
Hiero + move-to-front 28.85 39.89
Hiero + attach 29.25 39.43
Table 5: BLEU and PER scores on the test set.
Our training corpus contains about 200.8k sen-
tences. Using the training corpus, we extracted
hierarchical phrase rules and trained 4-gram lan-
guage model and word-based reordering model.
Parameters were tuned over 1.0k sentences (devel-
opment data) with single reference by minimum
error rate training (MERT) (Och, 2003). Test data
consisted of 1.0k sentences with single reference.
Table 4 shows the condition of corpus in detail.
4.3 Results
Table 5 shows the BLEU (Papineni et al, 2001)
and PER (Niesen et al, 2000) scores obtained by
each system. The results clearly indicated that
our proposed system with word-based reorder-
ing model (move-to-front or attach) outperformed
baseline system on BLEU scores. In contrast,
there is no significant improvement from baseline
on PER. This suggests that the improvement of
BLEU mainly comes from reordering. In our ex-
periment, preprocessing approach resulted in very
poor scores.
4.4 Discussion
Table 6 displays examples showing the cause of
the improvements of our system with reordering
model (attach) comparing to baseline system. We
can see that the outputs of our system are more
fluent than those of baseline system because of re-
ordering model.
As a further analysis, we calculated the BLEU
scores of Japanese S? predicted from reorder-
ing model against true Japanese S? made from
GIZA++ alignments, were only 26.2 points on de-
velopment data. We think the poorness mainly
comes from unaligned words since they are un-
tractable for the word-based reordering model.
Actually, Japanese sentences in our training data
include 34.7% unaligned words. In spite of the
poorness, our proposed method effectively utilize
this reordering model in contrast to preprocessing
approach.
5 Related Work
Our approach is similar to preprocessing approach
(Xia and McCord, 2004; Collins et al, 2005; Li
et al, 2007; Tromble and Eisner, 2009) in that it
reorders source sentence in target order. The dif-
ference is this sentence reordering is done in de-
coding rather than in preprocessing.
A lot of studies on lexicalized reordering (Till-
man, 2004; Koehn et al, 2005; Nagata et al,
2006) focus on the phrase-based model. These
works cannnot be directly applied to hierarchi-
cal phrase-based model because of the difference
between normal phrases and hierarchical phrases
that includes nonterminal symbols.
Shen et al (2008,2009) proposed a way to inte-
grate dependency structure into target and source
side string on hierarchical phrase rules. This ap-
proach is similar to our approach in extending the
formalism of rules on hierarchical phrase-based
model in order to consider the constraint of word
order. But, our approach differs from (Shen et al,
2008; Shen et al, 2009) in that syntax annotation
is not necessary.
6 Conclusion and Future Work
We proposed a method to integrate word-based
reordering model into hierarchical phrase-based
machine translation system. We add ?? into the
hiero rules, but this does not increase the num-
ber of rules. So, this extension itself does not af-
fect the search space of decoding. In this paper
we used Tromble and Eisner?s reordering model
for our method, but various reordering model can
be incorporated to our method, for example S?
N -gram language model. Our experimental re-
sults on Japanese-to-English task showed that our
system outperformed baseline system and prepro-
cessing approach.
In this paper we utilize ?? only for reorder-
ing model. However, it is possible to use ?? for
other modeling, for example we can use it for
rule translation probabilities P (?? |?), P (?|??) for
additional feature functions. Of course, we can
444
S america de seihin no hanbai wo hajimeru keikaku ga ari masu ka . kono tegami wa koukuubin de nihon made ikura kakari masu ka .
TB sales of product in america are you planning to start ? this letter by airmail to japan . how much is it ?
TP are you planning to start products in the u.s. ? how much does it cost to this letter by airmail to japan ?
R do you plan to begin selling your products in the u.s. ? how much will it cost to send this letter by air mail to japan ?
Table 6: Examples of outputs for input sentence S from baseline system TB and our proposed sys-
tem (attach) TP . R is a reference. The underlined portions have equivalent meanings and show the
reordering differences.
also utilize reordered target sentence T ? for vari-
ous modeling as well. Addtionally we plan to use
S? for MERT because we hypothesize the fluent
S? leads to fluent T .
References
AI-Onaizan, Y. and K. Papineni. 2006. Distortion
models for statistical machine translation. In Proc.
the 44th ACL, pages 529?536.
Brown, P. F., S. A. D. Pietra, V. D. J. Pietra, and R. L.
Mercer. 1993. The mathematics of statistical ma-
chine translation: Parameter estimation. Computa-
tional Linguitics, 19:263?312.
Chiang, D., K. Knight, and W. Wang. 2009. 11,001
new features for statistical machine translation. In
Proc. NAACL, pages 216?226.
Chiang, D. 2007. Hierachical phrase-based transla-
tion. Computational Linguitics, 33:201?228.
Collins, M., P. Koehn, and I. Kucerova. 2005. Clause
restructuring for statistical machine translation. In
Proc. the 43th ACL, pages 531?540.
Collins, M. 2002. Discriminative training methods for
hidden markov models. In Proc. of EMNLP.
Freund, Y. and R. E. Schapire. 1996. Experiments
with a new boosting algorithm. In Proc. of the 13th
ICML, pages 148?156.
Koehn, P., A. Axelrod, A-B. Mayne, C. Callison-
Burch, M. Osborne, and D. Talbot. 2005. Ed-
inburgh system description for 2005 iwslt speech
translation evaluation. In Proc. the 2nd IWSLT.
Li, Z. and S. Khudanpur. 2008. A scalable decoder
for parsing-based machine translation with equiv-
alent language model state maintenance. In Proc.
ACL SSST.
Li, C-H., D. Zhang, M. Li, M. Zhou, K. Li, and
Y. Guan. 2007. A probabilistic approach to syntax-
based reordering for statistical machine translation.
In Proc. the 45th ACL, pages 720?727.
McDonald, R., K. Crammer, and F. Pereira. 2005.
Spanning tree methods for discriminative training of
dependency parsers. In Thechnical Report MS-CIS-
05-11, UPenn CIS.
Nagata, M., K. Saito, K. Yamamoto, and K. Ohashi.
2006. A clustered global phrase reordering model
for statistical machine translation. In COLING-
ACL, pages 713?720.
Niesen, S., F.J. Och, G. Leusch, and H. Ney. 2000.
An evaluation tool for machine translation: Fast
evaluation for mt research. In Proc. the 2nd In-
ternational Conference on Language Resources and
Evaluation.
Och, F. J. and H. Ney. 2002. Discriminative train-
ing and maximum entropy models for statistical ma-
chine translation. In Proc. the 40th ACL, pages 295?
302.
Och, F. and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29:19?51.
Och, F. J. 2003. Minimum error rate training in sta-
tistical machine translation. In Proc. the 41th ACL,
pages 160?167.
Papineni, K. A., S. Roukos, T. Ward, and W-J. Zhu.
2001. Bleu: a method for automatic evaluation of
machine translation. In Proc. the 39th ACL, pages
311?318.
Shen, L., J. Xu, and R. Weischedel. 2008. A new
string-to-dependency machine translation algorithm
with a target dependency language model. In Proc.
ACL, pages 577?585.
Shen, L., J. Xu, B. Zhang, S. Matsoukas, and
R. Weischedel. 2009. Effective use of linguistic and
contextual information for statistical machine trans-
lation. In Proc. EMNLP, pages 72?80.
Tillman, C. 2004. A unigram orientation model
for statistical machine translation. In Proc. HLT-
NAACL, pages 101?104.
Tromble, R. and J. Eisner. 2009. Learning linear
ordering problems for better translation. In Proc.
EMNLP, pages 1007?1016.
445
Watanabe, T., H. Tsukada, and H. Isozaki. 2006. Left-
to-right target generation for hierarchical phrase-
based translation. In Proc. COLING-ACL, pages
777?784.
Xia, F. and M. McCord. 2004. Improving a statis-
tical mt system with automatically learned rewrite
patterns. In Proc. the 18th ICON, pages 508?514.
446
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 944?952,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Automatic Evaluation of Translation Quality for Distant Language Pairs
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito Sudoh, Hajime Tsukada
NTT Communication Science Laboratories, NTT Corporation
2-4 Hikaridai, Seikacho, Sorakugun, Kyoto, 619-0237, Japan
{isozaki,hirao,kevinduh,sudoh,tsukada}@cslab.kecl.ntt.co.jp
Abstract
Automatic evaluation of Machine Translation
(MT) quality is essential to developing high-
quality MT systems. Various evaluation met-
rics have been proposed, and BLEU is now
used as the de facto standard metric. How-
ever, when we consider translation between
distant language pairs such as Japanese and
English, most popular metrics (e.g., BLEU,
NIST, PER, and TER) do not work well. It
is well known that Japanese and English have
completely different word orders, and special
care must be paid to word order in transla-
tion. Otherwise, translations with wrong word
order often lead to misunderstanding and in-
comprehensibility. For instance, SMT-based
Japanese-to-English translators tend to trans-
late ?A because B? as ?B because A.? Thus,
word order is the most important problem
for distant language translation. However,
conventional evaluation metrics do not sig-
nificantly penalize such word order mistakes.
Therefore, locally optimizing these metrics
leads to inadequate translations. In this pa-
per, we propose an automatic evaluation met-
ric based on rank correlation coefficients mod-
ified with precision. Our meta-evaluation of
the NTCIR-7 PATMT JE task data shows that
this metric outperforms conventional metrics.
1 Introduction
Automatic evaluation of machine translation (MT)
quality is essential to developing high-quality ma-
chine translation systems because human evaluation
is time consuming, expensive, and irreproducible. If
we have a perfect automatic evaluation metric, we
can tune our translation system for the metric.
BLEU (Papineni et al, 2002b; Papineni et al,
2002a) showed high correlation with human judg-
ments and is still used as the de facto standard au-
tomatic evaluation metric. However, Callison-Burch
et al (2006) argued that the MT community is overly
reliant on BLEU by showing examples of poor per-
formance. For Japanese-to-English (JE) translation,
Echizen-ya et al (2009) showed that the popular
BLEU and NIST do not work well by using the sys-
tem outputs of the NTCIR-7 PATMT (patent transla-
tion) JE task (Fujii et al, 2008). On the other hand,
ROUGE-L (Lin and Hovy, 2003), Word Error Rate
(WER), and IMPACT (Echizen-ya and Araki, 2007)
worked better.
In these studies, Pearson?s correlation coefficient
and Spearman?s rank correlation ? with human eval-
uation scores are used to measure how closely an
automatic evaluation method correlates with human
evaluation. This evaluation of automatic evaluation
methods is called meta-evaluation. In human eval-
uation, people judge the adequacy and the fluency of
each translation.
Denoual and Lepage (2005) pointed out that
BLEU assumes word boundaries, which is ambigu-
ous in Japanese and Chinese. Here, we assume
the word boundaries given by ChaSen, one of the
standard morphological analyzers (http://chasen-
legacy.sourceforge.jp/) following Fujii et al
(2008)
In JE translation, most Statistical Machine Trans-
lation (SMT) systems translate the Japanese sen-
tence
(J0) kare wa sono hon wo yonda node
sekaishi ni kyoumi ga atta
which means
944
(R0) he was interested in world
history because he read the book
into an English sentence such as
(H0) he read the book because he was
interested in world history
in which the cause and the effect are swapped. Why
does this happen? The former half of (J0) means ?He
read the book,? and the latter half means ?(he) was
interested in world history.? The middle word
?node? between them corresponds to ?because.?
Therefore, SMT systems output sentences like (H0).
On the other hand, Rule-based Machine Translation
(RBMT) systems correctly give (R0).
In order to find (R0), SMT systems have to search
a very large space because we cannot restrict its
search space with a small distortion limit. Most
SMT systems thus fail to find (R0).
Consequently, the global word order is essential
for translation between distant language pairs, and
wrong word order can easily lead to misunderstand-
ing or incomprehensibility. Perhaps, some readers
do not understand why we emphasize word order
from this example alone. A few more examples
will clarify what happens when SMT is applied to
Japanese-to-English translation. Even the most fa-
mous SMT service available on the web failed to
translate the following very simple sentence at the
time of writing this paper.
Japanese: meari wa jon wo koroshita.
Reference: Mary killed John.
SMT output: John killed Mary.
Since it cannot translate such a simple sentence, it
obviously cannot translate more complex sentences
correctly.
Japanese: bobu ga katta hon wo jon wa yonda.
Reference: John read a book that Bob bought.
SMT output: Bob read the book John bought.
Another example is:
Japanese: bobu wa meari ni yubiwa wo kau
tameni, jon no mise ni itta.
Reference: Bob went to John?s store to buy a
ring for Mary.
SMT output: Bob Mary to buy the ring, John
went to the store.
In this way, this SMT service usually gives incom-
prehensible or misleading translations, and thus peo-
ple prefer RBMT services. Other SMT systems also
tend to make similar word order mistakes, and spe-
cial care should be paid to the translation between
distant language pairs such as Japanese and English.
Even Japanese people cannot solve this word or-
der problem easily: It is well known that Japanese
people are not good at speaking English.
From this point of view, conventional automatic
evaluation metrics of translation quality disregard
word order mistakes too much. Single-reference
BLEU is defined by a geometrical mean of n-gram
precisions pn and is modified by Brevity Penalty
(BP) min(1, exp(1? r/h)), where r is the length of
the reference and h is the length of the hypothesis.
BLEU = BP? (p1p2p3p4)
1/4.
Its range is [0, 1]. The BLEU score of (H0) with ref-
erence (R0) is 1.0?(11/11?9/10?6/9?4/8)1/4 =
0.740. Therefore, BLEU gives a very good score to
this inadequate translation because it checks only n-
grams and does not regard global word order.
Since (R0) and (H0) look similar in terms of flu-
ency, adequacy is more important than fluency in
the translation between distant language pairs.
Similarly, other popular scores such as NIST,
PER, and TER (Snover et al, 2006) also give
relatively good scores to this translation. NIST
also considers only local word orders (n-grams).
PER (Position-Independent Word Error Rate) was
designed to disregard word order completely.
TER (Snover et al, 2006) was designed to allow
phrase movements without large penalties. There-
fore, these standard metrics are not optimal for eval-
uating translation between distant language pairs.
In this paper, we propose an alternative automatic
evaluation metric appropriate for distant language
pairs. Our method is based on rank correlation co-
efficients. We use them to compare the word ranks
in the reference with those in the hypothesis.
There are two popular rank correlation coeffi-
cients: Spearman?s ? and Kendall?s ? (Kendall,
1975). In Isozaki et al (2010), we used Kendall?s ?
to measure the effectiveness of our Head Finaliza-
tion rule as a preprocessor for English-to-Japanese
translation, but we measured the quality of transla-
tion by using conventional metrics.
945
It is not clear how well ? works as an automatic
evaluation metric of translation quality. Moreover,
Spearman?s ? might work better than Kendall?s ? .
As we discuss later, ? considers only the direction
of the rank change, whereas ? considers the distance
of the change.
The first objective of this paper is to examine
which is the better metric for distant language pairs.
The second objective is to find improvements of
these rank correlation-metrics.
Spearman?s ? is based on Pearson?s correlation
coefficients. Suppose we have two lists of numbers
x = [0.1, 0.4, 0.2, 0.6],
y = [0.9, 0.6, 0.2, 0.7].
To obtain Pearson?s coefficients between x and y,
we use the raw values in these lists. If we substitute
their ranks for their raw values, we get
x? = [1, 3, 2, 4] and y? = [4, 2, 1, 3].
Then, Spearman?s ? between x and y is given by
Pearson?s coefficients between x? and y?. This ?
can be rewritten as follows when there is no tie:
? = 1?
?
i d
2
i
n+1C3
.
Here, di indicates the difference in the ranks of the
i-th element. Rank distances are squared in this
formula. Because of this square, we expect that ?
decreases drastically when there is an element that
significantly changes in rank. But we are also afraid
that ? may be too severe for alternative good trans-
lations.
Since Pearson?s correlation metric assumes lin-
earity, nonlinear monotonic functions can change
its score. On the other hand, Spearman?s ? and
Kendall?s ? uses ranks instead of raw evaluation
scores, and simple application of monotonic func-
tions cannot change them (use of other operations
such as averaging sentence scores can change them).
2 Methodology
2.1 Word alignment for rank correlations
We have to determine word ranks to obtain rank cor-
relation coefficients. Suppose we have:
(R1) John hit Bob yesterday
(H1) Bob hit John yesterday
The 1st word ?John? in R1 becomes the 3rd word
in H1. The 2nd word ?hit? in R1 becomes the 2nd
word in H1. The 3rd word ?Bob? in R1 becomes the
1st word in H1. The 4th word ?yesterday? in R1 be-
comes the 4th word in H1. Thus, we get H1?s word
order list [3, 2, 1, 4]. The number of all pairs of in-
tegers in this list is 4C2 = 6. It has three increasing
pairs: (3,4), (2,4), and (1,4). Since Kendall?s ? is
given by:
? = 2?
the number of increasing pairs
the number of all pairs
? 1,
H1?s ? is 2? 3/6? 1 = 0.0.
In this case, we can obtain Spearman?s ? as fol-
lows: ?John? moved by d1 = 2 words, ?hit? moved
by d2 = 0 words, ?Bob? moved by d3 = 2 words,
and ?yesterday? moved by d4 = 0 words. Therefore,
H1?s ? is 1? (22 + 02 + 22 + 02)/5C3 = 0.2.
Thus, ? considers only the direction of the move-
ment, whereas ? considers the distance of the move-
ment. Both ? and ? have the same range [?1, 1]. The
main objective of this paper is to clarify which rank
correlation is closer to human evaluation scores.
We have to consider the limitation of the rank cor-
relation metrics. They are defined only when there
is one-to-one correspondence. However, a refer-
ence sentence and a hypothesis sentence may have
different numbers of words. They may have two or
more occurrences of the same word in one sentence.
Sometimes, a word in the reference does not appear
in the hypothesis, or a word in the hypothesis does
not appear in the reference. Therefore, we cannot
calculate ? and ? following the above definitions in
general.
Here, we determine the correspondence of words
between hypotheses and references as follows. First,
we find one-to-one corresponding words. That is,
we find words that appear in both sentences and only
once in each sentence. Suppose we have:
(R2) the boy read the book
(H2) the book was read by the boy
By removing non-aligned words by one-to-one cor-
respondence, we get:
946
(R3) boy read book
(H3) book read boy
Thus, we lost ?the.? We relax this one-to-one cor-
respondence constraint by using one-to-one corre-
sponding bigrams. (R2) and (H2) share ?the boy?
and ?the book,? and we can align these instances of
?the? correctly.
(R4) the1 boy2 read3 the4 book5
(H4) the4 book5 read3 the1 boy2
Now, we have five aligned words, and H4?s word
order is represented by [4, 5, 3, 1, 2].
In returning to H0 and R0, we find that each of
these sentences has eleven words. Almost all words
are aligned by one-to-one correspondence but ?he?
is not aligned because it appears twice in each sen-
tence. By considering one-to-one corresponding bi-
grams (?he was? and ?he read?), ?he? is aligned as
follows.
(R5) he1 was2 interested3 in4 world5
history6 because7 he8 read9 the10
book11
(H5) he8 read9 the10 book11 because7
he1 was2 interested3 in4 world5
history6
H5?s word order is [8, 9, 10, 11, 7, 1, 2, 3, 4, 5, 6].
The number of increasing pairs is: 4C2 = 6 pairs in
[8, 9, 10, 11] and 6C2 = 15 pairs in [1, 2, 3, 4, 5,
6]. Then we obtain ? = 2 ? (6 + 15)/11C2 ? 1 =
?0.236. On the other hand,
?
i d
2
i = 5
2 ? 6 + 22 +
72 ? 4 = 350, and we obtain ? = 1 ? 350/12C3 =
?0.591.
Therefore, both Spearman?s ? and Kendall?s ?
give very bad scores to the misleading translation
H0. This fact implies they are much better metrics
than BLEU, which gave a good score to it. ? is much
lower than ? as we expected.
In general, we can use higher-order n-grams for
this alignment, but here we use only unigrams and
bigrams for simplicity. This algnment algorithm is
given in Figure 1. Since some hypothesis words do
not have corresponding reference words, the output
integer list worder is sometimes shorter than the
evaluated sentence. Therefore, we should not use
worder[i] ? i as di directly. We have to renumber
the list by rank as we did in Section 1.
Read a hypothesis sentence h = h1h2 . . . hm
and its reference sentence r = r1r2 . . . rn.
Initialize worder with an empty list.
For each word hi in h:
? If hi appears only once each in h and r, append j
s.t. rj = hi to worder.
? Otherwise, if the bigram hihi+1 appears only once
each in h and r, append j s.t. rjrj+1 = hihi+1 to
worder.
? Otherwise, if the bigram hi?1hi appears only once
each in h and r, append j s.t. rj?1rj = hi?1hi to
worder.
Return worder.
Figure 1: Word alignment algorithm for rank correlation
2.2 Word order metrics and meta-evaluation
metrics
These rank correlation metrics sometimes have neg-
ative values. In order to make them just like other
automatic evaluation metrics, we normalize them as
follows.
? Normalized Kendall?s ? : NKT = (? + 1)/2.
? Normalized Spearman?s ?: NSR = (?+ 1)/2.
Accordingly, NKT is 0.382 and NSR is 0.205.
These metrics are defined only when the number
of aligned words is two or more. We define both
NKT and NSR as zero when the number is one or
less. Consequently, these normalized metrics have
the same range [0, 1].
In order to avoid confusion, we use these abbre-
viations (NKT and NSR) when we use rank corre-
lations as word order metrics, because these cor-
relation metrics are also used in the machine trans-
lation community for meta-evaluation. For meta-
evaluation, we use Spearman?s ? and Pearson?s cor-
relation coefficient and call them ?Spearman? and
?Pearson,? respectively.
2.3 Overestimation problem
Since we measure the rank correlation of only cor-
responding words, these metrics will overestimate
the correlation. For instance, a hypothesis sentence
might have only two corresponding words among
947
0 0.2 0.4 0.6 0.8 1.0
0
0.2
0.4
0.6
0.8
1.0
BP (brevity penalty)
no
rm
al
iz
ed
av
er
ag
e
ad
eq
ua
cy
? ?
?
?
? ?
?
?
?
?
?
?
?
??
?
?
?
?? ??
?
?
?
??
?
?
?
?
?
?
?
?
?
??
? ?
?
?
?
??
?
?
?
??
?
? ?
?
??
?
?
?
?
? ? ?
? ? ? ??
?
?
?
? ?
?
?
?
?? ??
?
? ?
?
?
?
?
?
?
?
?
?
?
?
?
? ?
?
?
?
?
?
?
?
??
?
??
?
?
?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
?
? ?
?
?
?
?
?
? ?
?
?
?
?
?
?
?
?
?
?
? ?
?
? ?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
? ?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
0 0.2 0.4 0.6 0.8 1.0
0
0.2
0.4
0.6
0.8
1.0
P (precision)
no
rm
al
iz
ed
av
er
ag
e
ad
eq
ua
cy
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
?
?
?
?
?? ?
?
?
?
?
?
?
?
??
?
? ?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
? ?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
??
?
???
?
??? ?
?
?
? ?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
??
?
?
?
? ?
?
?
?
?
?
?
?
?
?
?
?
?
? ?
??
?
?
?
?
??
?
?
?
?
?
?
?
? ?
? ?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
?
??
?
?
?
?? ?
?
???
?
?
??
?
??
?
?
?
?
? ?
?
?
?
??
?
?
?
?
?
?
?
?
?
?? ?
?
?
?
?
?
??
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
?
?
? ?
?
?
?
?
? ??
?
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
??
?
?
? ?
??
?
? ?
??
?
?
?
?
?
Figure 2: Scatter plots of normalized average adequacy with brevity penalty (left) and precision (right).
(Each ? corresponds to one sentence generated by one MT system)
dozens of words. In this case, these two words
determine the score of the whole sentence. If the
two words appear in their order in the reference,
the whole sentence obtains the best score, NSR =
NKT = 1.0, in spite of the fact that only two words
matched.
Solving this overestimation problem is the second
objective of this paper. BLEU uses ?Brevity Penalty
(BP)? (Section 1) to reduce the scores of too-short
sentences. We can combine the above word order
metrics with BP, e.g., NKT? BP and NSR? BP.
However, we cannot very much expect from this
solution because BP scores do not correlate with
human judgments well. The left graph of Figure
2 shows a scatter plot of BP and ?normalized av-
erage adequacy.? This graph has 15 (systems) ?
100 (sentences) dots. Each dot (?) corresponds to
one sentence from one translation system.
In the NTCIR-7 data, three human judges gave
five-point scores (1, 2, 3, 4, 5) for ?adequacy? and
?fluency? of each translated sentence. Although
each system translated 1,381 sentences, only 100
sentences were evaluated by the judges.
For each translated sentence, we averaged three
judges? adequacy scores and normalized this aver-
age x by (x?1)/4. This is our ?normalized average
adequacy,? and the dots appears only at multiples of
1/3? 1/4.
This graph shows that BP has very little correla-
tion with adequacy, and we cannot expect BP to im-
prove the meta-evaluation performance very much.
Perhaps, BP?s poor performance was caused by the
fact that most MT systems output almost the same
number of words, and if the number exceeds the
length of the reference, BP=1.0 holds.
Therefore, we have to consider other modifiers
for this overestimation problem. We can use other
common metrics such as precision, recall, and F-
measure to reduce the overestimation of NSR and
NKT.
? Precision: P = c/|h|, where c is the number of
corresponding words and |h| is the number of
words in the hypothesis sentence h.
? Recall: R = c/r, where |r| is the number of
words in the reference sentence r.
? F-measure: F? = (1 + ?2)PR/(?2P + R),
where ? is a parameter.
In (R2)&(H2)?s case, precision is 5/7 = 0.714 and
recall is 5/5 = 1.000.
Which metric should we use? Our preliminary
experiments with NTCIR-7 data showed that preci-
sion correlated best with adequacy among these
three metrics (P , R, and F?=1). In addition, BLEU
is essentially made for precision. Therefore, preci-
sion seems the most promising modifier.
The right graph of Figure 2 shows a scatter plot
of precision and normalized average adequacy. The
graph shows that precision has more correlation with
adequacy than BP. We can observe that sentences
with very small P values usually obtain very low
adequacy scores but those with mediocre P values
often obtain good adequacy scores.
948
If we multiply P directly by NSR or NKT, those
sentences with mediocre P values will lose too
much of their scores. The use of
?
x will miti-
gate this problem. Since
?
P is closer to 1.0 than
P itself, multiplication of
?
P instead of P itself
will save these sentences. If we apply
?
x twice
(
??
P = 4
?
P ), it will further save them. There-
fore, we expect?
?
P and? 4
?
P to work better than
?P . Now, we propose two new metrics:
NSRP? and NKTP?,
where ? is a parameter (0 ? ? ? 1).
3 Experiments
3.1 Meta-evaluation with NTCIR-7 data
In order to compare automatic translation evalua-
tion methods, we use submissions to the NTCIR-7
Patent Translation (PATMT) task (Fujii et al, 2008).
Fourteen MT systems participated in the Japanese-
English intrinsic evaluation. There were two Rule-
Based MT (RMBT) systems and one Example-
based MT (EBMT) system. All other systems were
Statistical MT (SMT) systems. The task organiz-
ers provided a baseline SMT system. These 15 sys-
tems translated 1,381 Japanese sentences into En-
glish. The organizers evaluated these translations by
using BLEU and human judgments. In the human
judgements, three experts independently evaluated
100 selected sentences in terms of ?adequacy? and
?fluency.?
For automatic evaluation, we used a single refer-
ence sentence for each of these 100 manually evalu-
ated sentences. Echizen-ya et al (2009) used multi-
reference data, but their data is not publicly available
yet.
For this meta-evaluation, we measured the
corpus-level correlation between the human evalua-
tion scores and the automatic evaluation scores. We
simply averaged scores of 100 sentences for the pro-
posed metrics. For existing metrics such as BLEU,
we followed their definitions for corpus-level eval-
uation instead of simple averages of sentence-level
scores. We used default settings for conventional
metrics, but we tuned GTM (Melamed et al, 2007)
with -e option. This option controls preferences
on longer word runs. We also used the para-
phrase database TERp (http://www.umiacs.umd.
edu/?snover/terp) for METEOR (Banerjee and
Lavie, 2005).
3.2 Meta-evaluation with WMT-07 data
We developed our metric mainly for automatic eval-
uation of translation quality for distant language
pairs such as Japanese-English, but we also want
to know how well the metric works for similar lan-
guage pairs. Therefore, we also use the WMT-
07 data (Callison-Burch et al, 2007) that covers
only European language pairs. Callison-Burch et al
(2007) tried different human evaluation methods and
showed detailed evaluation scores. The Europarl test
set has 2,000 sentences, and The News Commentary
test set has 2,007 sentences.
This data has different language pairs: Spanish,
French, German ? English. We exclude Czech-
English because there were so few systems (See the
footnote of p. 146 in their paper).
4 Results
4.1 Meta-evaluation with NTCIR-7 data
Table 1 shows the main results of this paper. The
left part has corpus-level meta-evaluation with ade-
quacy. Error metrics, WER, PER, and TER, have
negative correlation coefficients, but we did not
show their minus signs here.
Both NSR-based metrics and NKT-based metrics
perform better than conventional metrics for this NT-
CIR PATMT JE translation data. As we expected,
?BP and ?P (1/1) performed badly. Spearman of
BP itself is zero.
NKT performed slightly better than NSR. Per-
haps, NSR penalized alternative good translations
too much. However, one of the NSR-based metrics,
NSRP 1/4, gave the best Spearman score of 0.947,
and the difference between NSRP? and NKTP?
was small. Modification with P led to this improve-
ment.
NKT gave the best Pearson score of 0.922. How-
ever, Pearson measures linearity and we can change
its score through a nonlinear monotonic function
without changing Spearman very much. For in-
stance, (NSRP 1/4)1.5 also has Spearman of 0.947
but its Pearson is 0.931, which is better than NKT?s
0.922. Thus, we think Spearman is a better meta-
evaluation metric than Pearson.
949
Table 1: NTCIR-7 Meta-evaluation: correlation with hu-
man judgments (Spm = Spearman, Prs = Pearson)
human judge Adequacy Fluency
eval\ meta-eval Spm Prs Spm Prs
P 0.615 0.704 0.672 0.876
R 0.436 0.669 0.461 0.854
F?=1 0.525 0.692 0.543 0.871
BP 0.000 0.515 -0.007 0.742
NSR 0.904 0.906 0.869 0.910
NSRP 1/8 0.937 0.905 0.890 0.934
NSRP 1/4 0.947 0.900 0.901 0.944
NSRP 1/2 0.937 0.890 0.926 0.949
NSRP 1/1 0.883 0.872 0.883 0.939
NSR ? BP 0.851 0.874 0.769 0.910
NKT 0.940 0.922 0.887 0.931
NKTP 1/8 0.940 0.913 0.908 0.944
NKTP 1/4 0.940 0.904 0.908 0.949
NKTP 1/2 0.929 0.890 0.897 0.949
NKTP 1/1 0.897 0.869 0.879 0.936
NKT ? BP 0.829 0.878 0.726 0.918
ROUGE-L 0.903 0.874 0.889 0.932
ROUGE-S(4) 0.593 0.757 0.640 0.869
IMPACT 0.797 0.813 0.751 0.932
WER 0.894 0.822 0.836 0.926
TER 0.854 0.806 0.372 0.856
PER 0.375 0.642 0.393 0.842
METEOR(TERp) 0.490 0.708 0.508 0.878
GTM(-e 12) 0.618 0.723 0.601 0.850
NIST 0.343 0.661 0.372 0.856
BLEU 0.515 0.653 0.500 0.795
The right part of Table 1 shows correlation with
fluency, but adequacy is more important, because
our motivation is to provide a metric that is useful to
reduce incomprehensible or misunderstanding out-
puts of MT systems. Again, the correlation-based
metrics gave better scores than conventional metrics,
and BP performed badly. NSR-based metrics proved
to be as good as NKT-based metrics.
Meta-evaluation scores of the de facto standard
BLEU is much lower than those of other metrics.
Echizen-ya et al (2009) reported that IMPACT per-
formed very well for sentence-level evaluation of
NTCIR-7 PATMT JE data. This corpus-level result
also shows that IMPACT works better than BLEU,
but ROUGE-L, WER, and our methods give better
scores than IMPACT.
Table 2: WMT-07 meta-evaluation: Each source lan-
guage has two columns: the left one is News Corpus and
the right one is Europarl.
Spearman?s ? with human ?rank?
source French Spanish German
NSR 0.775 0.837 0.523 0.766 0.700 0.593
NSRP 1/8 0.821 0.857 0.786 0.595 0.400 0.685
NSRP 1/4 0.821 0.857 0.786 0.455 0.400 0.714
NSRP 1/2 0.821 0.857 0.786 0.347 0.400 0.714
NKT 0.845 0.857 0.607 0.838 0.700 0.630
NKTP 1/8 0.793 0.857 0.786 0.595 0.400 0.714
NKTP 1/4 0.793 0.857 0.786 0.524 0.400 0.714
NKTP 1/2 0.793 0.857 0.786 0.347 0.400 0.714
BLEU 0.786 0.679 0.750 0.595 0.400 0.821
WER 0.607 0.857 0.750 0.429 0.000 0.500
ROUGEL 0.893 0.739 0.786 0.707 0.700 0.857
ROUGES 0.883 0.679 0.786 0.690 0.400 0.929
4.2 Meta-evaluation with WMT-07 data
Callison-Burch et al (2007) have performed differ-
ent human evaluation methods for different language
pairs and different corpora. Their Table 5 shows
inter-annotator agreements for the human evaluation
methods. According to their table, the ?sentence
ranking? (or ?rank?) method obtained better agree-
ment than ?adequacy.? Therefore, we show Spear-
man?s ? for ?rank.? We used the scores given in
their Tables 9, 10, and 11. (The ?constituent? meth-
ods obtained the best inter-annotator agreement, but
these methods focus on local translation quality and
have nothing to do with global word order, which we
are discussing here.)
Table 2 shows that our metrics designed for
distant language pairs are comparable to conven-
tional methods even for similar language pairs, but
ROUGE-L and ROUGE-S performed better than
ours for French News Corpus and German Europarl.
BLEU scores in this table agree with those in Table
17 of Callison-Burch et al (2007) within rounding
errors.
After some experiments, we noticed that the use
ofR instead of P often gives better scores for WMT-
07, but it degrades NTCIR-7 scores. We can extend
our metric by F? , weighted harmonic mean of P and
R, or any other interpolation, but the introduction
of new parameters into our metric makes it difficult
950
to control. Improvement without new parameters is
beyond the scope of this paper.
5 Discussion
It has come to our attention that Birch et al (2010)
has independently proposed an automatic evaluation
method based on Kendall?s ? . First, they started
with Kendall?s ? distance, which can be written as
?1?NKT? in our terminology, and then subtracted
it from one. Thus, their metric is nothing but NKT.
Then, they proposed application of the square root
to get better Pearson by improving ?the sensitivity
to small reorderings.? Since they used ?Kendall?s ??
and ?Kendall?s ? distance? interchangeably, it is not
clear what they mean by ?
?
Kendall?s ? ,? but per-
haps they mean 1 ?
?
1?NKT because
?
NKT is
more insensitive to small reorderings. Table 3 shows
the performance of these metrics for NTCIR-7 data.
Pearson?s correlation coefficient with adequacy was
improved by 1 ?
?
1? NKT, but other scores were
degraded in this experiment.
The difference between our method and Birch et
al. (2010)?s method comes from the fact that we
used Japanese-English translation data and Spear-
man?s correlation for meta-evaluation, whereas they
used Chinese-English translation data and only Pear-
son?s correlation for meta-evaluation. Chinese word
order is different from English, but Chinese is a
Subject-Verb-Object (SVO) language and thus is
much closer to English word order than Japanese,
a typical SOV language.
We preferred NSR because it penalizes global
word order mistakes much more than does NKT, and
as discussed above, global word order mistakes of-
ten lead to incomprehensibility and misunderstand-
ing.
On the other hand, they also tried Hamming dis-
tance, and summarized their experiments as follows:
However, the Hamming distance seems to
be more informative than Kendall?s tau for
small amounts of reordering.
This sentence and the introduction of the square root
to NKT imply that Chinese word order is close to
that of English, and they have to measure subtle
word order mistakes.
Table 3: NTCIR-7 meta-evaluation: Effects of square
root (b(x) = 1?
?
1? x)
NKT
?
NKT b(NKT)
Spearman w/ adequacy 0.940 0.940 0.922
Pearson w/ adequacy 0.922 0.817 0.941
Spearman w/ fluency 0.887 0.865 0.858
Pearson w/ fluency 0.931 0.917 0.833
In spite of these differences, the two groups inde-
pendently recognized the usefulness of rank correla-
tions for automatic evaluation of translation quality
for distant language pairs.
In their WMT-2010 paper (Birch and Osborne,
2010), they multiplied NKT with the brevity penalty
and interpolated it with BLEU for the WMT-2010
shared task. This fact implies that incomprehensible
or misleading word order mistakes are rare in trans-
lation among European languages.
6 Conclusions
When Statistical Machine Translation is applied to
distant language pairs such as Japanese and English,
word order becomes an important problem. SMT
systems often fail to find an appropriate translation
because of a large search space. Therefore, they
often output misleading or incomprehensible sen-
tences such as ?A because B? vs. ?B because A.? To
penalize such inadequate translations, we presented
an automatic evaluation method based on rank corre-
lation. There were two questions for this approach.
First, which correlation coefficient should we use:
Spearman?s ? or Kendall?s ?? Second, how should
we solve the overestimation problem caused by the
nature of one-to-one correspondence?
We answered these questions through our exper-
iments using the NTCIR-7 PATMT JE translation
data. For the first question, ? was slightly better
than ?, but ? was improved by precision. For the
second question, it turned out that BLEU?s Brevity
Penalty was counter-productive. A precision-based
penalty gave a better solution. With this precision-
based penalty, both ? and ? worked well and they
outperformed conventional methods for NTCIR-7
data. For similar language pairs, our method was
comparable to conventional evaluation methods. Fu-
951
ture work includes extension of the method so that it
can outperform conventional methods even for sim-
ilar language pairs.
References
Satanjeev Banerjee and Alon Lavie. 2005. Meteor:
An automatic metric for MT evaluation with improved
correlation with human judgements. In Proc. of ACL
Workshop on Intrinsic and Extrinsic Evaluation Mea-
sures for MT and Summarization, pages 65?72.
Alexandra Birch and Miles Osborne. 2010. LRscore for
evaluating lexical and reordering quality in MT. In
Proceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 327?
332.
Alexandra Birch, Miles Osborne, and Phil Blunsom.
2010. Metrics for MT evaluation: evaluating reorder-
ing. Machine Translation, 24(1):15?26.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluatiing the role of Bleu in ma-
chine translation research. In Proc. of the Conference
of the European Chapter of the Association for Com-
putational Linguistics, pages 249?256.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Chrstof Monz, and Josh Schroeder. 2007.
(Meta-)Evaluation of machine translation. In Proc. of
the Workshop on Machine Translation (WMT), pages
136?158.
Etienne Denoual and Yves Lepage. 2005. BLEU in char-
acters: towards automatic MT evaluation in languages
without word delimiters. In Companion Volume to the
Proceedings of the Second International Joint Confer-
ence on Natural Language Processing, pages 81?86.
Hiroshi Echizen-ya and Kenji Araki. 2007. Automatic
evaluation of machine translation based on recursive
acquisition of an intuitive common parts continuum.
In Proceedings of MT Summit XII Workshop on Patent
Translation, pages 151?158.
Hiroshi Echizen-ya, Terumasa Ehara, Sayori Shimohata,
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto,
Takehito Utsuro, and Noriko Kando. 2009. Meta-
evaluation of automatic evaluation methods for ma-
chine translation using patent translation data in ntcir-
7. In Proceedings of the 3rd Workshop on Patent
Translation, pages 9?16.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and
Takehito Utsuro. 2008. Overview of the patent
translation task at the NTCIR-7 workshop. In Work-
ing Notes of the NTCIR Workshop Meeting (NTCIR),
pages 389?400.
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010. Head Finalization: A simple re-
ordering rule for SOV languages. In Proceedings of
the Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 250?257.
Maurice G. Kendall. 1975. Rank Correlation Methods.
Charles Griffin.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic evalu-
ation of summaries using n-gram co-occurrence statis-
tics. In Proc. of the North American Chapter of the
Association of Computational Linguistics (NAACL),
pages 71?78.
Dan Melamed, Ryan Green, and Joseph P. Turian. 2007.
Precision and recall of machine translation. In Proc.
of NAACL-HLT, pages 61?63.
Kishore Papineni, Salim Roukos, Todd Ward, John Hen-
derson, and Florence Reeder. 2002a. Corpus-based
comprehensive and diagnostic MT evaluation: Initial
Arabic, Chinese, French, and Spanish Results. In
Proc. of the International Conference on Human Lan-
guage Technology Research (HLT), pages 132?136.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002b. BLEU: a method for automatic eval-
uation of machine translation. In Proc. of the Annual
Meeting of the Association of Computational Linguis-
tics (ACL), pages 311?318.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Translation
in the Americas.
952
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 204?209,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Noise-aware Character Alignment for Bootstrapping Statistical Machine
Transliteration from Bilingual Corpora
Katsuhito Sudoh?? Shinsuke Mori? Masaaki Nagata?
?NTT Communication Science Laboratories
?Graduate School of Informatics, Kyoto University
?Academic Center for Computing and Media Studies, Kyoto University
sudoh.katsuhito@lab.ntt.co.jp
Abstract
This paper proposes a novel noise-aware char-
acter alignment method for bootstrapping sta-
tistical machine transliteration from automat-
ically extracted phrase pairs. The model is
an extension of a Bayesian many-to-many
alignment method for distinguishing non-
transliteration (noise) parts in phrase pairs. It
worked effectively in the experiments of boot-
strapping Japanese-to-English statistical ma-
chine transliteration in patent domain using
patent bilingual corpora.
1 Introduction
Transliteration is used for providing translations for
source language words that have no appropriate
counterparts in target language, such as some tech-
nical terms and named entities. Statistical machine
transliteration (Knight and Graehl, 1998) is a tech-
nology to solve it in a statistical manner. Bilin-
gual dictionaries can be used to train its model, but
many of their entries are actually translation but not
transliteration. Such non-transliteration pairs hurt
the transliteration model and should be eliminated
beforehand.
Sajjad et al (2012) proposed a method to iden-
tify such non-transliteration pairs, and applied it
successfully to noisy word pairs obtained from au-
tomatic word alignment on bilingual corpora. It
enables the statistical machine transliteration to be
bootstrapped from bilingual corpora. This approach
is beneficial because it does not require carefully-
developed bilingual transliteration dictionaries and
it can learn domain-specific transliteration patterns
from bilingual corpora in the target domain. How-
ever, their transliteration mining approach is sample-
wise; that is, it makes a decision whether a bilingual
phrase pair is transliteration or not. Suppose that
a compound word in a language A is transliterated
into two words in another language B. Their corre-
spondence may not be fully identified by automatic
word alignment and a wrong alignment between the
compound word in A and only one component word
in B is found. The sample-wise mining cannot make
a correct decision of partial transliteration on the
aligned candidate, and may introduces noise to the
statistical transliteration model.
This paper proposes a novel transliteration mining
method for such partial transliterations. The method
uses a noise-aware character alignment model that
distinguish non-transliteration (noise) parts from
transliteration (signal) parts. The model is an ex-
tension of a Bayesian alignment model (Finch and
Sumita, 2010) and can be trained by a sampling al-
gorithm extended for a constraint on noise. Our
experiments of Japanese-to-English transliteration
achieved 16% relative error reduction in transliter-
ation accuracy from the sample-wise method. The
main contribution of this paper is two-fold:
? we formulate alignment over string pairs with
partial noise and present a solution with a
noise-aware alignment model;
? we proved its effectiveness by experiments
with frequent unknown words in actual
Japanese-to-English patent translation data.
204
2 Bayesian many-to-many alignment
We briefly review a Bayesian many-to-many charac-
ter alignment proposed by Finch and Sumita (2010)
on which our model is based. The model is based
on a generative process of bilingual substring pairs
?s?, t?? by the following Dirichlet process (DP):
G|?,G0 ? DP(?,G0)
?s?, t??|G ? G,
where G is a probability distribution over substring
pairs according to a DP prior with base measure G0
and hyperparameter ?. G0 is modeled as a joint
spelling model as follows:
G0 (?s?, t??) =
?|s?|s
|s?|! e
??sv?|s?|s ?
?|t?|t
|?t|! e
??tv?|t?|t . (1)
This is a simple joint probability of the spelling
models, in which each alphabet appears based on
a uniform distribution over the vocabulary (of size
vs and vt) and each string length follows a Poisson
distribution (with the average length ?s and ?t).
The model handles infinite number of substring
pairs according to the Chinese Restaurant Process
(CRP). The probability of a substring pair ?s?k, t?k?
is based on the counts of all other substring pairs as
follows:
p
(
?s?k, t?k?| {?s?, t??}?k
)
= N (?s?k, t?k?) + ?G0 (?s?k, t?k?)?
i N (?s?i, t?i?) + ?
. (2)
Here {?s?, t??}?k means a set of substring pairs ex-
cluding ?s?k, t?k?, and N (?s?k, t?k?) is the number of
?s?k, t?k? in the current sample space. This align-
ment model is suitable for representing very sparse
distribution over arbitrary substring pairs, thanks to
reasonable CRP-based smoothing for unseen pairs
based on the spelling model.
3 Proposed method
We propose an extended many-to-many alignment
model that can handle partial noise. We extend the
model in the previous section by introducing a noise
symbol and state-based probability calculation.
? ?
k e y 
(a) no noise
? ?
f l y 
noise
noise
(b) noise
? ?
g i v 
? ? ?
noisee 
(c) partial noise: English
side should be ?give up?
? ?
k e y 
? ?
g i v 
? ? ?
noisee 
r e c 
? ? ?noise
o v e r 
(d) partial noise: Japanese side
should be ??????
Figure 1: Three types of noise in transliteration data.
Solid lines are correct many-to-many alignment links.
3.1 Partial noise in transliteration data
Figure 1 shows transliteration examples with ?no
noise,? ?noise,? and ?partial noise.? Solid lines in the
figure show correct many-to-many alignment links.
The examples (a) and (b) can be distinguished ef-
fectively by Sajjad et al (2012). We aim to do align-
ment as in the examples (c) and (d) by distinguishing
its non-transliteration (noise) part, which cannot be
handled by the existing methods.
3.2 Noise-aware alignment model
We introduce a noise symbol to handle partial noise
in the many-to-many alignment model. Htun et al
(2012) extended the many-to-many alignment for
the sample-wise transliteration mining, but its noise
model only handles the sample-wise noise and can-
not distinguish partial noise. We model partial noise
in the CRP-based joint substring model.
Partial noise in transliteration data typically ap-
pears in compound words as mentioned earlier, be-
cause their counterparts consisting of two or more
words may not be fully covered in automatically ex-
tracted words and phrases as shown in Figure 1(c).
Another type of partial noise is derived from mor-
phological differences due to inflection, which usu-
ally appear in the sub-word level as prefixes and suf-
fixes as shown in Figure 1(d). According to this
intuition, we assume that partial noise appears in
the beginning and/or end of transliteration data (in
case of sample-wise noise, we assume the noise is in
the beginning). This assumption derives a constraint
between signal and noise parts that helps to avoid
a welter of transliteration and non-transliteration
parts. It also has a shortcoming that it is generally
205
? ?
t h e 
? ? ?
sp 
? ? ?sp 
e t c h i n g sp m a s k s 
noise noise noise
Figure 2: Example of many-to-many alignment with par-
tial noise in the beginning and end. ?noise? stands for the
noise symbol and ?sp? stands for a white space.
not appropriate for noise in the middle, but handling
arbitrary number of noise parts increases computa-
tional complexity and sparseness. We rely on this
simple assumption in this paper and consider a more
complex mid-noise problem as future work.
Figure 2 shows a partial noise example in both
the beginning and end. This example is actually
correct translation but includes noise in a sense of
transliteration; an article ?the? is wrongly included
in the phrase pair (no articles are used in Japanese)
and a plural noun ?masks? is transliterated into
?????(mask). These non-transliteration parts are
aligned to noise symbols in the proposed model. The
noise symbols are treated as zero-length substrings
in the model, same as other substrings.
3.3 Constrained Gibbs sampling
Finch and Sumita (2010) used a blocked Gibbs sam-
pling algorithm with forward-filtering backward-
sampling (FFBS) (Mochihashi et al, 2009). We ex-
tend their algorithm for our noise-aware model us-
ing a state-based calculation over the three states:
non-transliteration part in the beginning (noiseB),
transliteration part (signal), non-transliteration part
in the end (noiseE).
Figure 3 illustrates our FFBS steps. At first in
the forward filtering, we begin with transition to
noiseB and signal. The calculation of forward
probabilities itself is almost the same as Finch and
Sumita (2010) except for state transition constraints:
from noiseB to signal, from signal to noiseE. The
backward-sampling traverses a path by probability-
based sampling with true posteriors, starting from
the choice of the ending state among noiseB (means
full noise), signal, and noiseE. This algorithm in-
creases the computational cost by three times to con-
sider three different states, compared to that of Finch
and Sumita (2010).
noiseB
signal
noiseE
noiseB
signal
noiseE
s
s
s
s
s
s
t
t
t
t
t
t
(a) Forward filtering
noiseB
signal
noiseE
noiseB
signal
noiseE
s
s
s
s
s
s
t
t
t
t
t
t
(b) Backward sampling
Figure 3: State-based FFBS for the proposed model.
4 Experiments
We conducted experiments comparing the pro-
posed method with the conventional sample-wise
method for the use in bootstrapping statistical
machine transliteration using Japanese-to-English
patent translation dataset (Goto et al, 2013).
4.1 Training data setup
First, we trained a phrase table on the 3.2M paral-
lel sentences by a standard training procedure using
Moses, with Japanese tokenization using MeCab1.
We obtained 591,840 phrase table entries whose
Japanese side was written in katakana (Japanese
phonogram) only2. Then, we iteratively ran the
method of Sajjad et al (2012) on these entries and
eliminate non-transliteration pairs, until the num-
ber of pairs converged. Finally we obtain 104,563
katakana-English pairs after 10 iterations; they were
our baseline training set mined by sample-wise
method. We used Sajjad et al?s method as pre-
processing for filtering sample-wise noise while the
proposed method could also do that, because the
proposed method took much more training time for
all phrase table entries.
4.2 Transliteration experiments
The transliteration experiment used a translation-
based implementation with Moses, using a
1http://code.google.com/p/mecab/
2This katakana-based filtering is a language dependent
heuristic for choosing potential transliteration candidate, be-
cause transliterations in Japanese are usually written in
katakana.
206
character-based 7-gram language model trained on
300M English patent sentences. We compared three
transliteration models below.
The test set was top-1000 unknown (in the
Japanese-to-English translation model) katakana
words appeared in 400M Japanese patent sentences.
They covered 15.5% of all unknown katakanawords
and 8.8% of all unknown words (excluding num-
bers); that is, more than a half of unknown words
were katakana words.
4.2.1 Sample-wise method (BASELINE)
We used the baseline training set to train sta-
tistical machine transliteration model for our base-
line. The training procedure was based on Moses:
MGIZA++ word alignment, grow-diag-final-and
alignment symmetrization and phrase extraction
with the maximum phrase length of 7.
4.2.2 Proposed method (PROPOSED)
We applied the proposed method to the baseline
training set with 30 sampling iterations and elimi-
nated partial noise. The transliteration model was
trained in the same manner as BASELINE after elim-
inating noise.
The hyperparameters, ?, ?s, and ?t, were op-
timized using a held-out set of 2,000 katakana-
English pairs that were randomly chosen from a
general-domain bilingual dictionary. The hyperpa-
rameter optimization was based on F-score values
on the held-out set with varying ? among 0.01, 0.02,
0.05, 0.1, 1.0, and ?s among 1, 2, 3, 5.
Table 1 compares the statistics on the training sets
of BASELINE and PROPOSED. Note that we ap-
plied the proposed method to BASELINE data (the
sample-wise method was already applied until con-
vergence). The proposed method eliminated only
two transliteration candidates in sample-wise but
also eliminated 5,714 (0.64%) katakana and 55,737
(4.1%) English characters3.
4.2.3 Proposed method using aligned joint
substrings as phrases (PROPOSED-JOINT)
The many-to-many character alignment actually
induces substring pairs, which can be used as
3The reason of larger number of partial noise in English side
would be a syntactic difference as shown in Figure 2 and the
katakana-based filtering heuristics.
Table 1: Statistics of the training sets.
Method #pairs #Ja chars. #En chars.
BASELINE 104,563 899,080 1,372,993
PROPOSED 104,561 893,366 1,317,256
phrases in statistical machine transliteration and
improved transliteration performance (Finch and
Sumita, 2010). We extracted them by: 1) generate
many-to-many word alignment, in which all possi-
ble word alignment links in many-to-many corre-
spondences (e.g., 0-0 0-1 0-2 1-0 1-1 1-2 for ?? ?,
c o m?), 2) run phrase extraction and scoring same as
a standard Moses training. This procedure extracts
longer phrases satisfying the many-to-many align-
ment constraints than the simple use of extracted
joint substring pairs as phrases.
4.3 Results
Table 2 shows the results. We used three evalua-
tion metrics: ACC, F-score, and BLEUc. ACC is
a sample-wise accuracy and F-score is a character-
wise F-measure-like score (Li et al, 2010). BLEUc
is BLEU (Papineni et al, 2002) in the character level
with n=4.
PROPOSED achieved 63% in ACC (16% rela-
tive error reduction from BASELINE), and 94.6% in
F-score (25% relative error reduction from BASE-
LINE). These improvements clearly showed an ad-
vantage of the proposed method over the sample-
wise mining. BLEUc showed a similar improve-
ments. Recall that BASELINE and PROPOSED had
a small difference in their training data, actually
0.64% (katakana) and 4.1% (English) in the num-
ber of characters. The results suggest that the partial
noise can hurt transliteration models.
PROPOSED-JOINT showed similar performance
as PROPOSED with a slight drop in BLEUc, al-
though many-to-many substring alignment was ex-
pected to improve transliteration as reported by
Finch and Sumita (2010). The difference may be
due to the difference in coverage of the phrase
tables; PROPOSED-JOINT retained relatively long
substrings by the many-to-many alignment con-
straints in contrast to the less-constrained grow-
diag-final-and alignments in PROPOSED. Since the
training data in our bootstrapping experiments con-
207
Table 2: Japanese-to-English transliteration results for
top-1000 unknown katakana words. ACC and F-score
stand for the ones used in NEWS workshop, BLEUc is
character-wise BLEU.
Method ACC F-score BLEUc
BASELINE 0.56 0.929 0.864
PROPOSED 0.63 0.946 0.897
PROPOSED-JOINT 0.63 0.943 0.888
tained many similar phrases unlike dictionary-based
data in Finch and Sumita (2010), the phrase table of
PROPOSED-JOINT may have a small coverage due
to long and sparse substring pairs with large prob-
abilities even if the many-to-many alignment was
good. This sparseness problem is beyond the scope
of this paper and worth further study.
4.4 Alignment Examples
Figure 4 shows examples of the alignment results in
the training data. As expected, partial noise both in
Japanese and English was identified correctly in (a),
(b), and (c). There were some alignment errors in the
signal part in (b), in which characters in boundary
positions were aligned incorrectly to adjacent sub-
strings. These alignment errors did not directly de-
grade the partial noise identification but may cause
a negative effect on overall alignment performance
in the sampling-based optimization. (d) is a nega-
tive example in which partial noise was incorrectly
aligned. (c) and (d) have similar partial noise in their
English word endings, but it could not be identified
in (d). One possible reason for that is the sparse-
ness problem mentioned above, as shown in erro-
neous long character alignments in (d).
5 Conclusion
This paper proposed a noise-aware many-to-many
alignment model that can distinguish partial noise in
transliteration pairs for bootstrapping statistical ma-
chine transliteration model from automatically ex-
tracted phrase pairs. The model and training al-
gorithm are straightforward extension of those by
Finch and Sumita (2010). The proposed method
was proved to be effective in Japanese-to-English
transliteration experiments in patent domain.
Future work will investigate the proposed method
? ?
a n sp 
? sp ?
a r c sp t a n g e n t 
? ? ? ? ?noise noise
(a) Correctly aligned
? ? 
d o p 
? ? ? 
i n g sp e n e r g y 
? ? ? ? ? ? ? ? ? 
noise noise
(b) Some alignment errors in transliteration part
? ? 
f o r 
? ? 
m e d 
noise
(c) Correctly aligned
? ? 
c u s 
? ? ? 
t o m i z e d 
? noise
(d) Errors in partial noise
Figure 4: Examples of noise-aware many-to-many align-
ment in the training data. ? stands for a zero-length sub-
string. Dashed lines show incorrect alignments, and bold
grey lines mean their corrections.
in other domains and language pairs. The partial
noise would appear in other language pairs, typ-
ically between agglutinative and non-agglutinative
languages. It is also worth extending the approach
into word alignment in statistical machine transla-
tion.
Acknowledgments
We would like to thank anonymous reviewers for
their valuable comments and suggestions.
References
Andrew Finch and Eiichiro Sumita. 2010. A Bayesian
Model of Bilingual Segmentation for Transliteration.
In Proceedings of the seventh International Workshop
on Spoken Language Translation (IWSLT), pages 259?
266.
Isao Goto, Ka Po Chow, Bin Lu, Eiichiro Sumita, and
Benjamin K. Tsou. 2013. Overview of the Patent Ma-
chine Translation Task at the NTCIR-10 Workshop. In
The 10th NTCIR Conference, June.
Ohnmar Htun, Andrew Finch, Eiichiro Sumita, and
Yoshiki Mikami. 2012. Improving Transliteration
Mining by Integrating Expert Knowledge with Statis-
tical Approaches. International Journal of Computer
Applications, 58(17):12?22, November.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Computational Linguistics, 24(4):599?
612.
208
Haizhou Li, A Kumaran, Min Zhang, and Vladimir Per-
vouchine. 2010. Whitepaper of NEWS 2010 Shared
Task on Transliteration Generation. In Proceedings
of the 2010 Named Entities Workshop, pages 12?20,
Uppsala, Sweden, July. Association for Computational
Linguistics.
Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda.
2009. Bayesian Unsupervised Word Segmentation
with Nested Pitman-Yor Language Modeling. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 100?108, Suntec, Singapore, August.
Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.
Hassan Sajjad, Alexander Fraser, and Helmut Schmid.
2012. A statistical model for unsupervised and semi-
supervised transliteration mining. In Proceedings of
the 50th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers), pages
469?477, Jeju Island, Korea, July. Association for
Computational Linguistics.
209
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1382?1386,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Shift-Reduce Word Reordering for Machine Translation
Katsuhiko Hayashi?, Katsuhito Sudoh, Hajime Tsukada, Jun Suzuki, Masaaki Nagata
NTT Communication Science Laboratories, NTT Corporation
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237 Japan
?hayashi.katsuhiko@lab.ntt.co.jp
Abstract
This paper presents a novel word reordering
model that employs a shift-reduce parser for
inversion transduction grammars. Our model
uses rich syntax parsing features for word re-
ordering and runs in linear time. We apply it to
postordering of phrase-based machine trans-
lation (PBMT) for Japanese-to-English patent
tasks. Our experimental results show that our
method achieves a significant improvement
of +3.1 BLEU scores against 30.15 BLEU
scores of the baseline PBMT system.
1 Introduction
Even though phrase-based machine translation
(PBMT) (Koehn et al, 2007) and tree-based MT
(Graehl and Knight, 2004; Chiang, 2005; Galley
et al, 2006) systems have achieved great success,
many problems remain for distinct language pairs,
including long-distant word reordering.
To improve such word reordering, one promis-
ing way is to separate it from the translation pro-
cess as preordering (Collins et al, 2005; DeNero
and Uszkoreit, 2011) or postordering (Sudoh et al,
2011; Goto et al, 2012). Many studies utilize a rule-
based or a probabilistic model to perform a reorder-
ing decision at each node of a syntactic parse tree.
This paper presents a parser-based word reorder-
ing model that employs a shift-reduce parser for in-
version transduction grammars (ITG) (Wu, 1997).
To the best of our knowledge, this is the first study
on a shift-reduce parser for word reordering.
The parser-based reordering approach uses rich
syntax parsing features for reordering decisions.
Our propoesd method can also easily define such
.
Source-
ordered Target
Sentence (HFE)
Source Sen-
tence (J)
Target Sen-
tence (E)
reordering
Figure 1: A description of the postordering MT system.
non-local features as theN -gram words of reordered
strings. Even when using these non-local features,
the complexity of the shift-reduce parser does not
increase at all due to give up achieving an optimal
solution. Therefore, it works much more efficient.
In our experiments, we apply our proposed
method to postordering for J-to-E patent tasks be-
cause their training data for reordering have little
noise and they are ideal for evaluating reordering
methods. Although our used J-to-E setups need
a language-dependent scheme and we describe our
proposed method as a J-to-E postordering method,
the key algorithm is language-independent and it can
be applicable to preordering as well as postordering
if the training data for reordering are available.
2 Postordering by Parsing
As shown in Fig.1, postordering (Sudoh et al, 2011)
has two steps; the first is a translation step that trans-
lates an input sentence into source-ordered transla-
tions. The second is a reordering step in which the
translations are reordered in the target language or-
der. The key to postordering is the second step.
Goto et al (2012) modeled the second step by
parsing and created training data for a postordering
parser using a language-dependent rule called head-
finalization. The rule moves syntactic heads of a
lexicalized parse tree of an English sentence to the
1382
.S(saw)
. .VP(saw)
. .PP(with)
. .NP(telescope)
. .N(telescope)
.telescope.
D(a)
.a
.
PR(with)
.with
.
VP(saw)
. .NP(girl)
. .N(girl)
.girl.
D(a)
.a
.
V(saw)
.saw
.
NP(I)
.N(I)
.I
.
. .mita. .wo.shoujyo.de.bouenkyo. .wawatashi
.S(saw)
. .VP#(saw)
. .VP#(saw)
. .V(saw)
.saw
.
NP(wo)?a/an?
. .WO(wo)
.wo
N(girl)
.girl.
PP#(with)
. .PR(with)
.with
NP(telescope)?a/an?
.N(telescope)
.telescope
.
NP(wa)?no articles?
. .WA(wa)
.wa.
N(I)
.I
.
. .mita. .wo.shoujyo.de.bouenkyo. .wawatashi
Figure 2: An example of the head-finzalizaton process for an English-Japanese sentence pair: the left-hand side tree
is the original English tree, and the right-hand side tree is its head-final English tree.
end of the corresponding syntactic constituents. As
a result, the terminal symbols of the English tree are
sorted in a Japanese-like order. In Fig.2, we show an
example of head-finalization and a tree on the right-
hand side is a head-finalized English (HFE) tree of
an English tree on the left-hand side. We annotate
each parent node of the swapped edge with # sym-
bol. For example, a nonterminal symbol PP#(with)
shows that a noun phrase ?a/an telescope? and a
word ?with? are inverted.
For better word alignments, Isozaki et al (2012)
also deleted articles ?the? ?a? ?an? from English be-
cause Japanese has no articles, and inserted Japanese
particles ?ga? ?wo? ?wa? into English sentences.
We privilege the nonterminals of a phrase modified
by a deleted article to determine which ?the? ?a/an?
or ?no articles? should be inserted at the front of the
phrase. Note that an original English sentence can
be recovered from its HFE tree by using # symbols
and annotated articles and deleting Japanese parti-
cles.
As well as Goto et al (2012), we solve postorder-
ing by a parser whose model is trained with a set
of HFE trees. The main difference between Goto et
al. (2012)?s model and ours is that while the former
simply used the Berkeley parser (Petrov and Klein,
2007), our shift-reduce parsing model can use such
non-local task specific features as theN -gram words
of reordered strings without sacrificing efficiency.
Our method integrates postediting (Knight and
Chander, 1994) with reordering and inserts articles
into English translations by learning an additional
?insert? action of the parser. Goto et al (2012)
solved the article generation problem by using an
N -gram language model, but this somewhat compli-
cates their approach. Compared with other parsers,
one advantage of the shift-reduce parser is to easily
define such additional operations as ?insert?.
HFE trees can be defined as monolingual ITG
trees (DeNero and Uszkoreit, 2011). Our monolin-
gual ITG G is a tuple G = (V, T, P, I, S) where V
is a set of nonterminals, T is a set of terminals, P
is a set of production rules, I is a set of nontermi-
nals on which ?the? ?a/an? or ?no articles? must be
determined, and S is the start symbol.
Set P consists of terminal production rules that
are responsible for generating word w(? T ):
X ? w
and binary production rules in two forms:
X ? YZ
X# ? YZ
where X, X#, Y and Z are nonterminals. On
the right-hand side, the second rule generates two
phrases Y and Z in the reverse order. In our experi-
ments, we removed all unary production rules.
3 Shift-Reduce Parsing
Given an input sentence w1 . . . wn, the shift-reduce
parser uses a stack of partial derivations, a buffer of
input words, and a set of actions to build a parse tree.
The following is the parser?s configuration:
? : ?i, j, S? : pi
where ? is the step size, S is a stack of elements
s0, s1, . . . , i is the leftmost span index of the stack
1383
top element s0, j is an index of the next input word
of the buffer, and pi is a set of predictor states1.
Each stack element has at least the following com-
ponents of its partial derivation tree:
s = {H, h, wleft, wright, a}
where H is a root nonterminal or a part-of-speech tag
of the subtree, h is a head index of H, a is a variable
to which ?the? ?a/an? ?no articles? or null are as-
signed, and wleft, wright are the leftmost and right-
most words of phrase H. When referring to compo-
nent ?, we use a s.? notation.
Our proposed system has 4 actions shift-X, insert-
x, reduce-MR-X and reduce-SR-X.
The shift-X action pushes the next input word
onto the stack and assigns a part-of-speech tag X to
the word. The deduction step is as follows:
X ? wj ? P
p
? ?? ?
? : ?i, j, S|s?0? : pi
? + 1 : ?j, j + 1, S|s?0|s0)? : {p}
where s0 is {X, j, wj , wj , null}.
The insert-x action determines whether to gener-
ate ?the? ?a/an? or ?no articles? (= x):
s?0.X ? I ? (s?0.a ?= ?the? ? s?0.a ?= ?a/an?)
? : ?i, j, S|s?0)? : pi
? + 1 : ?i, j, S|s0? : pi
where s?0 is {X, h, wleft, wright, a} and s0 is
{X, h, wleft, wright, x} (i ? h, left, right < j).
The side condition prevents the parser from inserting
articles into phrase X more than twice. During pars-
ing, articles are not explicitly inserted into the input
string: they are inserted into it when backtracking to
generate a reordered string after parsing.
The reduce-MR-X action has a deduction rule:
X ? Y Z ? P ? q ? pi
q
? ?? ?
: ?k, i, S|s?2|s?1? : pi? ? : ?i, j, S|s?1|s?0? : pi
? + 1 : ?k, j, S|s?2|s0? : pi?
1Since our notion of predictor states is identical to that in
(Huang and Sagae, 2010), we omit the details here.
s0.wh ? s0.th s0.H s0.H ? s0.th s0.wh ? s0.H
s1.wh ? s1.th s1.H s1.H ? s1.th s1.wh ? s1.H
s2.th ? s2.H s2.wh ? s2.H q0.w q1.w q2.w
s0.tl ? s0.L s0.wl ? s0.L s1.tl ? s1.L s1.wl ? s1.L
s0.wh ? s0.H ? s1.wh ? s1.H s0.H ? s1.wh s0.wh ? s1.H
s0.H ? s1.H s0.wh ? s0.H ? q0.w s0.H ? q0.w
s1.wh ? s1.H ? q0.w s1.H ? q0.w s1.th ? q0.w ? q1.w
s0.wh ? s0.H ? s1.H ? q0.w s0.H ? s1.wh ? s1.H ? q0.w
s0.H ? s1.H ? q0.w s0.th ? s1.th ? q0.w
s0.wh ? s1.H ? q0.w ? q1.w s0.H ? q0.w ? q1.w
s0.th ? q0.w ? q1.w s0.wh ? s0.H ? s1.H ? s2.H
s0.H ? s1.wh ? s1.H ? s2.H s0.H ? s1.H ? s2.wh ? s2.H
s0.H ? s1.H ? s2.H s0.th ? s1.th ? s2.th
s0.H ? s0.R ? s0.L s1.H ? s1.R ? s1.L s0.H ? s0.R ? q0.w
s0.H ? s0.L ? s1.H s0.H ? s0.L ? s1.wh s0.H ? s1.H ? s1.L
s0.wh ? s1.H ? s1.R
s0.wleft ? s1.wright s0.tleft ? s1.tright
s0.wright ? s1.wleft s0.tright ? s1.tleft
s0.a ? s0.wleft s0.a ? s0.tleft s0.a ? s0.wleft ? s1.wright
s0.a ? s0.tleft ? s1.tright s0.a ? s0.wh s0.a ? s0.th
Table 1: Feature templates: s.L and s.R denote the left
and right subnodes of s. l and r are head indices of L and
R. q denotes a buffer element. t is a part-of-speech tag.
where s?0 is {Z, h0, wleft0, wright0, a0} and s?1 is
{Y, h1, wleft1, wright1, a1}. The action generates s0
by combining s?0 and s?1 with binary rule X?Y Z:
s0 = {X, h0, wleft1, wright0, a1}.
New nonterminal X is lexicalized with head word
wh0 of right nonterminal Z. This action expands Y
and Z in a straight order. The leftmost word of
phrase X is set to leftmost word wleft1 of Y, and the
rightmost word of phrase X is set to rightmost word
wright0 of Z. Variable a is set to a1 of Y.
The difference between reduce-MR-X and
reduce-SR-X actions is new stack element s0. The
reduce-SR-X action generates s0 by combining s?0
and s?1 with binary rule X# ?Y Z:
s0 = {X#, h0, wleft0, wright1, a0}.
This action expands Y and Z in a reverse order, and
the leftmost word of X# is set to wleft0 of Z, and the
rightmost word of X# is set to wright1 of Y. Variable
a is set to a0 of Z.
We use a linear model that is discriminatively
trained with the averaged perceptron (Collins and
Roark, 2004). Table 1 shows the feature templates
used in our experiments and we call the features in
the bottom two rows ?non-local? features.
1384
train dev test9 test10
# of sent. 3,191,228 2,000 2,000 2,300
ave. leng. (J) 36.4 36.6 37.0 43.1
ave. leng. (E) 33.3 33.3 33.7 39.6
Table 2: NTCIR-9 and 10 data statistics.
4 Experiments
4.1 Experimental Setups
We conducted experiments for NTCIR-9 and 10
patent data using a Japanese-English language pair.
Mecab2 was used for the Japanese morphological
analysis. The data are summarized in Table 2.
We used Enju (Miyao and Tsujii, 2008) for pars-
ing the English training data and converted parse
trees into HFE trees by a head-finalization scheme.
We extracted grammar rules from all the HFE trees
and randomly selected 500,000 HFE trees to train
the shift-reduce parser.
We used Moses (Koehn et al, 2007) with lexical-
ized reordering and a 6-gram language model (LM)
trained using SRILM (Stolcke et al, 2011) to trans-
late the Japanese sentences into HFE sentences.
To recover the English sentences, our shift-reduce
parser reordered only the 1-best HFE sentence. Our
strategy is much simpler than Goto et al (2012)?s
because they used a linear inteporation of MT cost,
parser cost and N -gram LM cost to generate the best
English sentence from the n-best HFE sentences.
4.2 Main Results
The main results in Table 3 indicate our method was
significantly better and faster than the conventional
PBMT system. Our method also ourperformed Goto
et al (2012)?s reported systems as well as a tree-
based (moses-chart) system3. Our proposed model
with ?non-local? features (w/ nf.) achieved gains
against that without the features (w/o nf.). Further
feature engineering may improve the accuracy more.
4.3 Analysis
We show N -gram precisions of PBMT (dist=6,
dist=20) and proposed systems in Table 5. The re-
sults clearly show that improvements of 1-gram pre-
2https://code.google.com/p/mecab/
3All the data and the MT toolkits used in our experiments
are the same as theirs.
test9 test10
BLEU RIBES BLEU RIBES
HFE w/ art. 28.86 73.45 29.9 73.52
proposed 32.93 76.68 33.25 76.74
w/o art. 19.86 75.62 20.17 75.63
N -gram 32.15 76.52 32.28 76.46
Table 4: The effects of article generation: ?w/o art.? de-
notes evaluation scores for translations of the best system
(?proposed?) in Table 3 from which articles are removed.
?HFE w/ art.? system used HFE data with articles and
generated them by MT system and the shift-reduce parser
performed only reordering. ?N -gram? system inserted
articles into the translations of ?w/o art.? by Goto et al
(2012)?s article generation method.
(1?4)-gram precision
moses (dist=6) 67.1 / 36.9 / 20.7 / 11.5
moses (dist=20) 67.7 / 38.9 / 23.0 / 13.7
proposed 68.9 / 40.6 / 25.7 / 16.7
Table 5: N -gram precisions of moses (dist=6, dist=20)
and proposed systems for test9 data.
cisions are the main factors that contribute to bet-
ter performance of our proposed system than PBMT
systems. It seems that the gains of 1-gram presicions
come from postediting (article generation).
In table 4, we show the effectiveness of our joint
reordering and postediting approach (?proposed?).
The ?w/o art.? results clearly show that generating
articles has great effects on MT evaluations espe-
cially for BLEU metric. Comparing ?proposed? and
?HFEw/ art.? systems, these results show that poste-
diting is much more effective than generating arti-
cles by MT. Our joint approach also outperformed
?N -gram? postediting system.
5 Conclusion
We proposed a shift-reduce word ordering model
and applied it to J-to-E postordering. Our experi-
mental results indicate our method can significantly
improve the performance of a PBMT system.
Future work will investigate our method?s use-
fulness on various language datasets. We plan to
study more general methods that use word align-
ments to embed swap information in trees (Galley
et al, 2006).
1385
test9 test10
BLEU RIBES time (sec.) BLEU RIBES time (sec.)
PBMT (dist=6) 27.1 67.76 2.66 27.92 68.13 3.18
PBMT (dist=12) 29.55 69.84 4.15 30.03 69.88 4.93
PBMT (dist=20) 29.98 69.87 6.22 30.15 69.43 7.19
Tree-based MT** (Goto et al, 2012) 29.53 69.22 ? ? ? ?
PBMT (dist=20)** (Goto et al, 2012) 30.13 68.86 ? ? ? ?
Goto et al (2012)** 31.75 72.57 ? ? ? ?
PBMT (dist=0) + proposed w/o nf. (beam=12) 32.59 76.35 1.46 + 0.01 32.83 76.44 1.7 + 0.01
PBMT (dist=0) + proposed w/o nf. (beam=48) 32.61 76.58 1.46 + 0.06 32.86 76.6 1.7 + 0.06
PBMT (dist=0) + proposed w/ nf. (beam=12) 32.91 76.38 1.46 + 0.01 33.15 76.53 1.7 + 0.02
PBMT (dist=0) + proposed w/ nf. (beam=48) 32.93 76.68 1.46 + 0.07 33.25 76.74 1.7 + 0.07
Table 3: System comparison: time represents the average second per sentence. ** denotes ?not our experiments?.
References
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 263?270.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceedings
of the 42nd Annual Meeting on Association for Com-
putational Linguistics, page 111.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 531?540.
John DeNero and Jakob Uszkoreit. 2011. Inducing sen-
tence structure from parallel corpora for reordering. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 193?203.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and the 44th annual meeting of
the Association for Computational Linguistics, pages
961?968.
Isao Goto, Masao Utiyama, and Eiichiro Sumita. 2012.
Post-ordering by parsing for japanese-english statisti-
cal machine translation. In Proceedings of the 50th
Annual Meeting of the Association for Computational
Linguistics, pages 311?316.
Jonathan Graehl and Kevin Knight. 2004. Training tree
transducers. In Proc. HLT-NAACL, pages 105?112.
Liang Huang and Kenji Sagae. 2010. Dynamic program-
ming for linear-time incremental parsing. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 1077?1086.
Hideki Isozaki, Jun Suzuki, Hajime Tsukada, Masaaki
Nagata, Sho Hoshino, and Yusuke Miyao. 2012.
HPSG-based preprocessing for English-to-Japanese
translation. ACM Transactions on Asian Language In-
formation Processing (TALIP), 11(3).
Kevin Knight and Ishwar Chander. 1994. Automated
postediting of documents. In Proceedings of the
National Conference on Artificial Intelligence, pages
779?779.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, et al 2007. Moses: Open source toolkit for sta-
tistical machine translation. In Proceedings of the 45th
Annual Meeting of the ACL on Interactive Poster and
Demonstration Sessions, pages 177?180.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature forest
models for probabilistic hpsg parsing. Computational
Linguistics, 34(1):35?80.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Human language tech-
nologies 2007: the conference of the North American
chapter of the Association for Computational Linguis-
tics, pages 404?411.
Andreas Stolcke, Jing Zheng, Wen Wang, and Victor
Abrash. 2011. Srilm at sixteen: Update and outlook.
In Proceedings of IEEE Automatic Speech Recognition
and Understanding Workshop.
Katsuhito Sudoh, Xianchao Wu, Kevin Duh, Hajime
Tsukada, and Masaaki Nagata. 2011. Post-ordering in
statistical machine translation. In Proc. MT Summit.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
1386
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1?10,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Learning to Translate with Multiple Objectives
Kevin Duh? Katsuhito Sudoh Xianchao Wu Hajime Tsukada Masaaki Nagata
NTT Communication Science Laboratories
2-4 Hikari-dai, Seika-cho, Kyoto 619-0237, JAPAN
kevinduh@is.naist.jp, lastname.firstname@lab.ntt.co.jp
Abstract
We introduce an approach to optimize a ma-
chine translation (MT) system on multiple
metrics simultaneously. Different metrics
(e.g. BLEU, TER) focus on different aspects
of translation quality; our multi-objective ap-
proach leverages these diverse aspects to im-
prove overall quality.
Our approach is based on the theory of Pareto
Optimality. It is simple to implement on top of
existing single-objective optimization meth-
ods (e.g. MERT, PRO) and outperforms ad
hoc alternatives based on linear-combination
of metrics. We also discuss the issue of metric
tunability and show that our Pareto approach
is more effective in incorporating new metrics
from MT evaluation for MT optimization.
1 Introduction
Weight optimization is an important step in build-
ing machine translation (MT) systems. Discrimi-
native optimization methods such as MERT (Och,
2003), MIRA (Crammer et al, 2006), PRO (Hop-
kins and May, 2011), and Downhill-Simplex (Nelder
and Mead, 1965) have been influential in improving
MT systems in recent years. These methods are ef-
fective because they tune the system to maximize an
automatic evaluation metric such as BLEU, which
serve as surrogate objective for translation quality.
However, we know that a single metric such as
BLEU is not enough. Ideally, we want to tune to-
wards an automatic metric that has perfect corre-
lation with human judgments of translation quality.
?*Now at Nara Institute of Science & Technology (NAIST)
While many alternatives have been proposed, such a
perfect evaluation metric remains elusive.
As a result, many MT evaluation campaigns now
report multiple evaluation metrics (Callison-Burch
et al, 2011; Paul, 2010). Different evaluation met-
rics focus on different aspects of translation quality.
For example, while BLEU (Papineni et al, 2002)
focuses on word-based n-gram precision, METEOR
(Lavie and Agarwal, 2007) allows for stem/synonym
matching and incorporates recall. TER (Snover
et al, 2006) allows arbitrary chunk movements,
while permutation metrics like RIBES (Isozaki et
al., 2010; Birch et al, 2010) measure deviation in
word order. Syntax (Owczarzak et al, 2007) and se-
mantics (Pado et al, 2009) also help. Arguably, all
these metrics correspond to our intuitions on what is
a good translation.
The current approach of optimizing MT towards
a single metric runs the risk of sacrificing other met-
rics. Can we really claim that a system is good if
it has high BLEU, but very low METEOR? Simi-
larly, is a high-METEOR low-BLEU system desir-
able? Our goal is to propose a multi-objective op-
timization method that avoids ?overfitting to a sin-
gle metric?. We want to build a MT system that
does well with respect to many aspects of transla-
tion quality.
In general, we cannot expect to improve multi-
ple metrics jointly if there are some inherent trade-
offs. We therefore need to define the notion of Pareto
Optimality (Pareto, 1906), which characterizes this
tradeoff in a rigorous way and distinguishes the set
of equally good solutions. We will describe Pareto
Optimality in detail later, but roughly speaking, a
1
hypothesis is pareto-optimal if there exist no other
hypothesis better in all metrics. The contribution of
this paper is two-fold:
? We introduce PMO (Pareto-based Multi-
objective Optimization), a general approach for
learning with multiple metrics. Existing single-
objective methods can be easily extended to
multi-objective using PMO.
? We show that PMO outperforms the alterna-
tive (single-objective optimization of linearly-
combined metrics) in multi-objective space,
and especially obtains stronger results for met-
rics that may be difficult to tune individually.
In the following, we first explain the theory of
Pareto Optimality (Section 2), and then use it to
build up our proposed PMO approach (Section 3).
Experiments on NIST Chinese-English and PubMed
English-Japanese translation using BLEU, TER, and
RIBES are presented in Section 4. We conclude by
discussing related work (Section 5) and opportuni-
ties/limitations (Section 6).
2 Theory of Pareto Optimality
2.1 Definitions and Concepts
The idea of Pareto optimality comes originally from
economics (Pareto, 1906), where the goal is to char-
acterize situations when a change in allocation of
goods does not make anybody worse off. Here, we
will explain it in terms of MT:
Let h ? L be a hypothesis from an N-best list L.
We have a total of K different metrics Mk(h) for
evaluating the quality of h. Without loss of gen-
erality, we assume metric scores are bounded be-
tween 0 and 1, with 1 being perfect. Each hypoth-
esis h can be mapped to a K-dimensional vector
M(h) = [M1(h);M2(h); ...;MK(h)]. For exam-
ple, suppose K = 2, M1(h) computes the BLEU
score, and M2(h) gives the METEOR score of h.
Figure 1 illustrates the set of vectors {M(h)} in a
10-best list.
For two hypotheses h1, h2, we write M(h1) >
M(h2) if h1 is better than h2 in all metrics, and
M(h1) ? M(h2) if h1 is better than or equal
to h2 in all metrics. When M(h1) ? M(h2) and
Mk(h1) > Mk(h2) for at least one metric k, we say
that h1 dominates h2 and write M(h1) . M(h2).
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
metric1
me
tric2
Figure 1: Illustration of Pareto Frontier. Ten hypotheses
are plotted by their scores in two metrics. Hypotheses
indicated by a circle (o) are pareto-optimal, while those
indicated by a plus (+) are not. The line shows the convex
hull, which attains only a subset of pareto-optimal points.
The triangle (4) is a point that is weakly pareto-optimal
but not pareto-optimal.
Definition 1. Pareto Optimal: A hypothesis h? ?
L is pareto-optimal iff there does not exist another
hypothesis h ? L such that M(h) . M(h?).
In Figure 1, the hypotheses indicated by circle
(o) are pareto-optimal, while those with plus (+) are
not. To visualize this, take for instance the pareto-
optimal point (0.4,0.7). There is no other point with
either (metric1 > 0.4 and metric2 ? 0.7), or (met-
ric1 ? 0.4 and metric2 > 0.7). On the other hand,
the non-pareto point (0.6,0.4) is ?dominated? by an-
other point (0.7,0.6), because for metric1: 0.7 > 0.6
and for metric2: 0.6 > 0.4.
There is another definition of optimality, which
disregards ties and may be easier to visualize:
Definition 2. Weakly Pareto Optimal: A hypothesis
h? ? L is weakly pareto-optimal iff there is no other
hypothesis h ? L such that M(h) > M(h?).
Weakly pareto-optimal points are a superset of
pareto-optimal points. A hypothesis is weakly
pareto-optimal if there is no other hypothesis that
improves all the metrics; a hypothesis is pareto-
optimal if there is no other hypothesis that improves
at least one metric without detriment to other met-
rics. In Figure 1, point (0.1,0.8) is weakly pareto-
optimal but not pareto-optimal, because of the com-
peting point (0.3,0.8). Here we focus on pareto-
optimality, but note our algorithms can be easily
2
modified for weakly pareto-optimality. Finally, we
can introduce the key concept used in our proposed
PMO approach:
Definition 3. Pareto Frontier: Given an N-best list
L, the set of all pareto-optimal hypotheses h ? L is
called the Pareto Frontier.
The Pareto Frontier has two desirable properties
from the multi-objective optimization perspective:
1. Hypotheses on the Frontier are equivalently
good in the Pareto sense.
2. For each hypothesis not on the Frontier, there
is always a better (pareto-optimal) hypothesis.
This provides a principled approach to optimiza-
tion: i.e. optimizing towards points on the Frontier
and away from those that are not, and giving no pref-
erence to different pareto-optimal hypotheses.
2.2 Reduction to Linear Combination
Multi-objective problems can be formulated as:
arg max
w
[M1(h);M2(h); . . . ;Mk(h)] (1)
where h = Decode(w, f)
Here, the MT system?s Decode function, parame-
terized by weight vector w, takes in a foreign sen-
tence f and returns a translated hypothesis h. The
argmax operates in vector space and our goal is to
find w leading to hypotheses on the Pareto Frontier.
In the study of Pareto Optimality, one central
question is: To what extent can multi-objective prob-
lems be solved by single-objective methods? Equa-
tion 1 can be reduced to a single-objective problem
by scalarizing the vector [M1(h); . . . ;Mk(h)] with
a linear combination:
arg max
w
K?
k=1
pkMk(h) (2)
where h = Decode(w, f)
Here, pk are positive real numbers indicating the rel-
ative importance of each metric (without loss of gen-
erality, assume
?
k pk = 1). Are the solutions to
Eq. 2 also solutions to Eq. 1 (i.e. pareto-optimal)
and vice-versa? The theory says:
Theorem 1. Sufficient Condition: If w? is solution
to Eq. 2, then it is weakly pareto-optimal. Further,
if w? is unique, then it is pareto-optimal.
Theorem 2. No Necessary Condition: There may
exist solutions to Eq. 1 that cannot be achieved by
Eq. 2, irregardless of any setting of {pk}.
Theorem 1 is a positive result asserting that lin-
ear combination can give pareto-optimal solutions.
However, Theorem 2 states the limits: in partic-
ular, Eq. 2 attains only pareto-optimal points that
are on the convex hull. This is illustrated in Fig-
ure 1: imagine sweeping all values of p1 = [0, 1]
and p2 = 1? p1 and recording the set of hypotheses
that maximizes
?
k pkMk(h). For 0.6 < p1 ? 1 we
get h = (0.9, 0.1), for p1 = 0.6 we get (0.7, 0.6),
and for 0 < p1 < 0.6 we get (0.4, 0.8). At no
setting of p1 do we attain h = (0.4, 0.7) which
is also pareto-optimal but not on the convex hull.1
This may have ramifications for issues like metric
tunability and local optima. To summarize, linear-
combination is reasonable but has limitations. Our
proposed approach will instead directly solve Eq. 1.
Pareto Optimality and multi-objective optimiza-
tion is a deep field with active inquiry in engineer-
ing, operations research, economics, etc. For the in-
terested reader, we recommend the survey by Mar-
ler and Arora (2004) and books by (Sawaragi et al,
1985; Miettinen, 1998).
3 Multi-objective Algorithms
3.1 Computing the Pareto Frontier
Our PMO approach will need to compute the Pareto
Frontier for potentially large sets of points, so we
first describe how this can be done efficiently. Given
a set of N vectors {M(h)} from an N-best list L,
our goal is extract the subset that are pareto-optimal.
Here we present an algorithm based on iterative
filtering, in our opinion the simplest algorithm to
understand and implement. The strategy is to loop
through the list L, keeping track of any dominant
points. Given a dominant point, it is easy to filter
out many points that are dominated by it. After suc-
cessive rounds, any remaining points that are not fil-
1We note that scalarization by exponentiated-combination
?
k pkMk(h)
q , for a suitable q > 0, does satisfy necessary
conditions for pareto optimality. However the proper tuning of q
is not known a priori. See (Miettinen, 1998) for theorem proofs.
3
Algorithm 1 FindParetoFrontier
Input: {M(h)}, h ? L
Output: All pareto-optimal points of {M(h)}
1: F = ?
2: while L is not empty do
3: h? = shift(L)
4: for each h in L do
5: if (M(h?) . M(h)): remove h from L
6: else if (M(h) . M(h?)): remove h from L; set
h? = h
7: end for
8: Add h? to Frontier Set F
9: for each h in L do
10: if (M(h?) . M(h)): remove h from L
11: end for
12: end while
13: Return F
tered are necessarily pareto-optimal. Algorithm 1
shows the pseudocode. In line 3, we take a point h?
and check if it is dominating or dominated in the for-
loop (lines 4-8). At least one pareto-optimal point
will be found by line 8. The second loop (lines 9-11)
further filters the list for points that are dominated by
h? but iterated before h? in the first for-loop.
The outer while-loop stops exactly after P iter-
ations, where P is the actual number of pareto-
optimal points in L. Each inner loop costs O(KN)
so the total complexity is O(PKN). Since P ? N
with the actual value depending on the probability
distribution of {M(h)}, the worst-case run-time is
O(KN2). For a survey of various Pareto algorithms,
refer to (Godfrey et al, 2007). The algorithm we de-
scribed here is borrowed from the database literature
in what is known as skyline operators.2
3.2 PMO-PRO Algorithm
We are now ready to present an algorithm for multi-
objective optimization. As we will see, it can be seen
as a generalization of the pairwise ranking optimiza-
tion (PRO) of (Hopkins and May, 2011), so we call
it PMO-PRO. PMO-PRO approach works by itera-
tively decoding-and-optimizing on the devset, sim-
2The inquisitive reader may wonder how is Pareto related
to databases. The motivation is to incorporate preferences into
relational queries(Bo?rzso?nyi et al, 2001). For K = 2 metrics,
they also present an alternative faster O(N logN) algorithm by
first topologically sorting along the 2 dimensions. All domi-
nated points can be filtered by one-pass by comparing with the
most-recent dominating point.
ilar to many MT optimization methods. The main
difference is that rather than trying to maximize a
single metric, we maximize the number of pareto
points, in order to expand the Pareto Frontier
We will explain PMO-PRO in terms of the
pseudo-code shown in Algorithm 2. For each sen-
tence pair (f, e) in the devset, we first generate an
N-best list L ? {h} using the current weight vector
w (line 5). In line 6, we evaluate each hypothesis
h with respect to the K metrics, giving a set of K-
dimensional vectors {M(h)}.
Lines 7-8 is the critical part: it gives a ?la-
bel? to each hypothesis, based on whether it is
in the Pareto Frontier. In particular, first we call
FindParetoFrontier (Algorithm 1), which re-
turns a set of pareto hypotheses; pareto-optimal hy-
potheses will get label 1 while non-optimal hypothe-
ses will get label 0. This information is added to
the training set T (line 8), which is then optimized
by any conventional subroutine in line 10. We will
follow PRO in using a pairwise classifier in line 10,
which finds w? that separates hypotheses with labels
1 vs. 0. In essence, this is the trick we employ to
directly optimize on the Pareto Frontier. If we had
used BLEU scores rather than the {0, 1} labels in
line 8, the entire PMO-PRO algorithm would revert
to single-objective PRO.
By definition, there is no single ?best? result
for multi-objective optimization, so we collect all
weights and return the Pareto-optimal set. In line 13
we evaluate each weight w on K metrics across the
entire corpus and call FindParetoFrontier
in line 14.3 This choice highlights an interesting
change of philosophy: While setting {pk} in linear-
combination forces the designer to make an a priori
preference among metrics prior to optimization, the
PMO strategy is to optimize first agnostically and
a posteriori let the designer choose among a set of
weights. Arguably it is easier to choose among so-
lutions based on their evaluation scores rather than
devising exact values for {pk}.
3.3 Discussion
Variants: In practice we find that a slight modifi-
cation of line 8 in Algorithm 2 leads to more sta-
3Note this is the same FindParetoFrontier algorithm as used
in line 7. Both operate on sets of points in K-dimensional
space, induced from either weights {w} or hypotheses {h}.
4
Algorithm 2 Proposed PMO-PRO algorithm
Input: Devset, max number of iterations I
Output: A set of (pareto-optimal) weight vectors
1: Initialize w. LetW = ?.
2: for i = 1 to I do
3: Let T = ?.
4: for each (f, e) in devset do
5: {h} =DecodeNbest(w,f )
6: {M(h)}=EvalMetricsOnSentence({h}, e)
7: {f} =FindParetoFrontier({M(h)})
8: foreach h ? {h}:
if h ? {f}, set l=1, else l=0; Add (l, h) to T
9: end for
10: w?=OptimizationSubroutine(T , w)
11: Add w? toW; Set w = w?.
12: end for
13: M(w) =EvalMetricsOnCorpus(w,devset) ?w ? W
14: Return FindParetoFrontier({M(w)})
ble results for PMO-PRO: for non-pareto hypothe-
ses h /? {f}, we set label l =
?
kMk(h)/K in-
stead of l= 0, so the method not only learns to dis-
criminate pareto vs. non-pareto but also also learns
to discriminate among competing non-pareto points.
Also, like other MT works, in line 5 the N-best list is
concatenated to N-best lists from previous iterations,
so {h} is a set with i ?N elements.
General PMO Approach: The strategy we out-
lined in Section 3.2 can be easily applied to other
MT optimization techniques. For example, by re-
placing the optimization subroutine (line 10, Algo-
rithm 2) with a Powell search (Och, 2003), one can
get PMO-MERT4. Alternatively, by using the large-
margin optimizer in (Chiang et al, 2009) and mov-
ing it into the for-each loop (lines 4-9), one can
get an online algorithm such PMO-MIRA. Virtually
all MT optimization algorithms have a place where
metric scores feedback into the optimization proce-
dure; the idea of PMO is to replace these raw scores
with labels derived from Pareto optimality.
4 Experiments
4.1 Evaluation Methodology
We experiment with two datasets: (1) The PubMed
task is English-to-Japanese translation of scientific
4A difference with traditional MERT is the necessity of
sentence-BLEU (Liang et al, 2006) in line 6. We use sentence-
BLEU for optimization but corpus-BLEU for evaluation here.
abstracts. As metrics we use BLEU and RIBES
(which demonstrated good human correlation in
this language pair (Goto et al, 2011)). (2) The
NIST task is Chinese-to-English translation with
OpenMT08 training data and MT06 as devset. As
metrics we use BLEU and NTER.
? BLEU = BP ? (?precn)1/4. BP is brevity
penality. precn is precision of n-gram matches.
? RIBES = (? + 1)/2 ? prec1/41 , with Kendall?s
? computed by measuring permutation between
matching words in reference and hypothesis5.
? NTER=max(1?TER, 0), which normalizes
Translation Edit Rate6 so that NTER=1 is best.
We compare two multi-objective approaches:
1. Linear-Combination of metrics (Eq. 2),
optimized with PRO. We search a range
of combination settings: (p1, p2) =
{(0, 1), (0.3, 0.7), (0.5, 0.5), (0.7, 0.3), (1, 0)}.
Note (1, 0) reduces to standard single-metric
optimization of e.g. BLEU.
2. Proposed Pareto approach (PMO-PRO).
Evaluation of multi-objective problems can be
tricky because there is no single figure-of-merit.
We thus adopted the following methodology: We
run both methods 5 times (i.e. using the 5 differ-
ent (p1, p2) setting each time) and I = 20 iterations
each. For each method, this generates 5x20=100 re-
sults, and we plot the Pareto Frontier of these points
in a 2-dimensional metric space (e.g. see Figure 2).
A method is deemed better if its final Pareto Fron-
tier curve is strictly dominating the other. We report
devset results here; testset trends are similar but not
included due to space constraints.7
5from www.kecl.ntt.co.jp/icl/lirg/ribes
6from www.umd.edu/?snover/tercom
7An aside: For comparing optimization methods, we believe
devset comparison is preferable to testset since data mismatch
may confound results. If one worries about generalization, we
advocate to re-decode the devset with final weights and evaluate
its 1-best output (which is done here). This is preferable to sim-
ply reporting the achieved scores on devset N-best (as done in
some open-source scripts) since the learned weight may pick
out good hypotheses in the N-best but perform poorly when
re-decoding the same devset. The re-decode devset approach
avoids being overly optimistic while accurately measuring op-
timization performance.
5
Train Devset #Feat Metrics
PubMed 0.2M 2k 14 BLEU, RIBES
NIST 7M 1.6k 8 BLEU, NTER
Table 1: Task characteristics: #sentences in Train/Dev, #
of features, and metrics used. Our MT models are trained
with standard phrase-based Moses software (Koehn and
others, 2007), with IBM M4 alignments, 4gram SRILM,
lexical ordering for PubMed and distance ordering for the
NIST system. The decoder generates 50-best lists each
iteration. We use SVMRank (Joachims, 2006) as opti-
mization subroutine for PRO, which efficiently handle all
pairwise samples without the need for sampling.
4.2 Results
Figures 2 and 3 show the results for PubMed and
NIST, respectively. A method is better if its Pareto
Frontier lies more towards the upper-right hand cor-
ner of the graph. Our observations are:
1. PMO-PRO generally outperforms Linear-
Combination with any setting of (p1, p2).
The Pareto Frontier of PMO-PRO dominates
that of Linear-Combination. This implies
PMO is effective in optimizing towards Pareto
hypotheses.
2. For both methods, trading-off between met-
rics is necessary. For example in PubMed,
the designer would need to make a choice be-
tween picking the best weight according to
BLEU (BLEU=.265,RIBES=.665) vs. another
weight with higher RIBES but poorer BLEU,
e.g. (.255,.675). Nevertheless, both the PMO
and Linear-Combination with various (p1, p2)
samples this joint-objective space broadly.
3. Interestingly, a multi-objective approach can
sometimes outperform a single-objective opti-
mizer in its own metric. In Figure 2, single-
objective PRO focusing on optimizing RIBES
only achieves 0.68, but PMO-PRO using both
BLEU and RIBES outperforms with 0.685.
The third observation relates to the issue of metric
tunability (Liu et al, 2011). We found that RIBES
can be difficult to tune directly. It is an extremely
non-smooth objective with many local optima?slight
changes in word ordering causes large changes in
RIBES. So the best way to improve RIBES is to
0.2 0.21 0.22 0.23 0.24 0.25 0.26 0.270.665
0.67
0.675
0.68
0.685
0.69
0.695
bleu
ribe
s
 
 Linear CombinationPareto (PMO?PRO)
Figure 2: PubMed Results. The curve represents the
Pareto Frontier of all results collected after multiple runs.
0.146 0.148 0.15 0.152 0.154 0.156 0.158 0.16 0.162 0.1640.694
0.695
0.696
0.697
0.698
0.699
0.7
0.701
0.702
0.703
0.704
bleu
nte
r
 
 Linear CombinationPareto (PMO?PRO)
Figure 3: NIST Results
not to optimize it directly, but jointly with a more
tunable metric BLEU. The learning curve in Fig-
ure 4 show that single-objective optimization of
RIBES quickly falls into local optimum (at iteration
3) whereas PMO can zigzag and sacrifice RIBES in
intermediate iterations (e.g. iteration 2, 15) leading
to a stronger result ultimately. The reason is the
diversity of solutions provided by the Pareto Fron-
tier. This finding suggests that multi-objective ap-
proaches may be preferred, especially when dealing
with new metrics that may be difficult to tune.
4.3 Additional Analysis and Discussions
What is the training time? The Pareto approach
does not add much overhead to PMO-PRO. While
FindParetoFrontier scales quadratically by size of
N-best list, Figure 5 shows that the runtime is triv-
6
0 2 4 6 8 10 12 14 16 18 200.63
0.64
0.65
0.66
0.67
0.68
0.69
iteration
ribe
s
 
 
Single?Objective RIBES
Pareto (PMO?PRO)
Figure 4: Learning Curve on RIBES: comparing single-
objective optimization and PMO.
0 100 200 300 400 500 600 700 800 900 10000
0.05
0.1
0.15
0.2
0.25
0.3
0.35
Set size |L|
Run
time
 (sec
onds
)
 
 
Algorithm 1
TopologicalSort (footnote 2)
Figure 5: Avg. runtime per sentence of FindPareto
ial (0.3 seconds for 1000-best). Table 2 shows
the time usage breakdown in different iterations for
PubMed. We see it is mostly dominated by decod-
ing time (constant per iteration at 40 minutes on
single 3.33GHz processor). At later iterations, Opt
takes more time due to larger file I/O in SVMRank.
Note Decode and Pareto can be ?embarrasingly par-
allelized.?
Iter Time Decode Pareto Opt Misc.
(line 5) (line 7) (line 10) (line 6,8)
1 47m 85% 1% 1% 13%
10 62m 67% 6% 8% 19%
20 91m 47% 15% 22% 16%
Table 2: Training time usage in PMO-PRO (Algo 2).
How many Pareto points? The number of pareto
0 2 4 6 8 10 12 14 16 185
10
15
20
25
30
35
Iterations
Num
ber 
of P
aret
o P
oint
s
 
 
NIST
PubMed
Figure 6: Average number of Pareto points
hypotheses gives a rough indication of the diversity
of hypotheses that can be exploited by PMO. Fig-
ure 6 shows that this number increases gradually per
iteration. This perhaps gives PMO-PRO more direc-
tions for optimizing around potential local optimal.
Nevertheless, we note that tens of Pareto points is far
few compared to the large size of N-best lists used
at later iterations of PMO-PRO. This may explain
why the differences between methods in Figure 3
are not more substantial. Theoretically, the num-
ber will eventually level off as it gets increasingly
harder to generate new Pareto points in a crowded
space (Bentley et al, 1978).
Practical recommendation: We present the
Pareto approach as a way to agnostically optimize
multiple metrics jointly. However, in practice, one
may have intuitions about metric tradeoffs even if
one cannot specify {pk}. For example, we might
believe that approximately 1-point BLEU degra-
dation is acceptable only if RIBES improves by
at least 3-points. In this case, we recommend
the following trick: Set up a multi-objective prob-
lem where one metric is BLEU and the other is
3/4BLEU+1/4RIBES. This encourages PMO to ex-
plore the joint metric space but avoid solutions that
sacrifice too much BLEU, and should also outper-
form Linear Combination that searches only on the
(3/4,1/4) direction.
5 Related Work
Multi-objective optimization for MT is a relatively
new area. Linear-combination of BLEU/TER is
7
the most common technique (Zaidan, 2009), some-
times achieving good results in evaluation cam-
paigns (Dyer et al, 2009). As far as we known, the
only work that directly proposes a multi-objective
technique is (He and Way, 2009), which modifies
MERT to optimize a single metric subject to the
constraint that it does not degrade others. These
approaches all require some setting of constraint
strength or combination weights {pk}. Recent work
in MT evaluation has examined combining metrics
using machine learning for better correlation with
human judgments (Liu and Gildea, 2007; Albrecht
and Hwa, 2007; Gimnez and Ma`rquez, 2008) and
may give insights for setting {pk}. We view our
Pareto-based approach as orthogonal to these efforts.
The tunability of metrics is a problem that is gain-
ing recognition (Liu et al, 2011). If a good evalu-
ation metric could not be used for tuning, it would
be a pity. The Tunable Metrics task at WMT2011
concluded that BLEU is still the easiest to tune
(Callison-Burch et al, 2011). (Mauser et al, 2008;
Cer et al, 2010) report similar observations, in ad-
dition citing WER being difficult and BLEU-TER
being amenable. One unsolved question is whether
metric tunability is a problem inherent to the metric
only, or depends also on the underlying optimization
algorithm. Our positive results with PMO suggest
that the choice of optimization algorithm can help.
Multi-objective ideas are being explored in other
NLP areas. (Spitkovsky et al, 2011) describe a tech-
nique that alternates between hard and soft EM ob-
jectives in order to achieve better local optimum in
grammar induction. (Hall et al, 2011) investigates
joint optimization of a supervised parsing objective
and some extrinsic objectives based on downstream
applications. (Agarwal et al, 2011) considers us-
ing multiple signals (of varying quality) from online
users to train recommendation models. (Eisner and
Daume? III, 2011) trades off speed and accuracy of
a parser with reinforcement learning. None of the
techniques in NLP use Pareto concepts, however.
6 Opportunities and Limitations
We introduce a new approach (PMO) for training
MT systems on multiple metrics. Leveraging the
diverse perspectives of different evaluation metrics
has the potential to improve overall quality. Based
on Pareto Optimality, PMO is easy to implement
and achieves better solutions compared to linear-
combination baselines, for any setting of combi-
nation weights. Further we observe that multi-
objective approaches can be helpful for optimiz-
ing difficult-to-tune metrics; this is beneficial for
quickly introducing new metrics developed in MT
evaluation into MT optimization, especially when
good {pk} are not yet known. We conclude by draw-
ing attention to some limitations and opportunities
raised by this work:
Limitations: (1) The performance of PMO is
limited by the size of the Pareto set. Small N-best
lists lead to sparsely-sampled Pareto Frontiers, and
a much better approach would be to enlarge the hy-
pothesis space using lattices (Macherey et al, 2008).
How to compute Pareto points directly from lattices
is an interesting open research question. (2) The
binary distinction between pareto vs. non-pareto
points ignores the fact that 2nd-place non-pareto
points may also lead to good practical solutions. A
better approach may be to adopt a graded definition
of Pareto optimality as done in some multi-objective
works (Deb et al, 2002). (3) A robust evaluation
methodology that enables significance testing for
multi-objective problems is sorely needed. This will
make it possible to compare multi-objective meth-
ods on more than 2 metrics. We also need to follow
up with human evaluation.
Opportunities: (1) There is still much we do
not understand about metric tunability; we can learn
much by looking at joint metric-spaces and exam-
ining how new metrics correlate with established
ones. (2) Pareto is just one approach among many
in multi-objective optimization. A wealth of meth-
ods are available (Marler and Arora, 2004) and more
experimentation in this space will definitely lead to
new insights. (3) Finally, it would be interesting to
explore other creative uses of multiple-objectives in
MT beyond multiple metrics. For example: Can we
learn to translate faster while sacrificing little on ac-
curacy? Can we learn to jointly optimize cascaded
systems, such as as speech translation or pivot trans-
lation? Life is full of multiple competing objectives.
Acknowledgments
We thank the reviewers for insightful feedback.
8
References
Deepak Agarwal, Bee-Chung Chen, Pradheep Elango,
and Xuanhui Wang. 2011. Click shaping to optimize
multiple objectives. In Proceedings of the 17th ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, KDD ?11, pages 132?140,
New York, NY, USA. ACM.
J. Albrecht and R. Hwa. 2007. A re-examination of ma-
chine learning approaches for sentence-level mt evalu-
ation. In ACL.
J. L. Bentley, H. T. Kung, M. Schkolnick, and C. D.
Thompson. 1978. On the average number of max-
ima in a set of vectors and applications. Journal of the
Association for Computing Machinery (JACM), 25(4).
Alexandra Birch, Phil Blunsom, and Miles Osborne.
2010. Metrics for MT evaluation: Evaluating reorder-
ing. Machine Translation, 24(1).
S. Bo?rzso?nyi, D. Kossmann, and K. Stocker. 2001. The
skyline operator. In Proceedings of the 17th Interna-
tional Conference on Data Engineering (ICDE).
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 work-
shop on statistical machine translation. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion, pages 22?64, Edinburgh, Scotland, July. Associ-
ation for Computational Linguistics.
Daniel Cer, Christopher Manning, and Daniel Jurafsky.
2010. The best lexical metric for phrase-based statis-
tical MT system optimization. In NAACL HLT.
David Chiang, Wei Wang, and Kevin Knight. 2009.
11,001 new features for statistical machine translation.
In NAACL.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passiveag-
gressive algorithms. Journal of Machine Learning Re-
search, 7.
Kalyanmoy Deb, Amrit Pratap, Sammer Agarwal, and
T. Meyarivan. 2002. A fast and elitist multiobjective
genetic algorithm: NSGA-II. IEEE Transactions on
Evolutionary Computation, 6(2).
Chris Dyer, Hendra Setiawan, Yuval Marton, and Philip
Resnik. 2009. The university of maryland statistical
machine translation system for the fourth workshop on
machine translation. In Proc. of the Fourth Workshop
on Machine Translation.
Jason Eisner and Hal Daume? III. 2011. Learning speed-
accuracy tradeoffs in nondeterministic inference algo-
rithms. In COST: NIPS 2011 Workshop on Computa-
tional Trade-offs in Statistical Learning.
Jesu?s Gimnez and Llu??s Ma`rquez. 2008. Heterogeneous
automatic mt evaluation through non-parametric met-
ric combinations. In ICJNLP.
Parke Godfrey, Ryan Shipley, and Jarek Gyrz. 2007. Al-
gorithms and analyses for maximal vector computa-
tion. VLDB Journal, 16.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K. Tsou. 2011. Overview of the patent ma-
chine translation task at the ntcir-9 workshop. In Pro-
ceedings of the NTCIR-9 Workshop Meeting.
Keith Hall, Ryan McDonald, Jason Katz-Brown, and
Michael Ringgaard. 2011. Training dependency
parsers by jointly optimizing multiple objectives.
In Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1489?1499, Edinburgh, Scotland, UK., July. Associa-
tion for Computational Linguistics.
Yifan He and Andy Way. 2009. Improving the objec-
tive function in minimum error rate training. In MT
Summit.
Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In Proceedings of the 2011 Conference on Empir-
ical Methods in Natural Language Processing, pages
1352?1362, Edinburgh, Scotland, UK., July. Associa-
tion for Computational Linguistics.
H. Isozaki, T. Hirao, K. Duh, K. Sudoh, and H. Tsukada.
2010. Automatic evaluation of translation quality for
distant language pairs. In EMNLP.
T. Joachims. 2006. Training linear SVMs in linear time.
In KDD.
P. Koehn et al 2007. Moses: open source toolkit for
statistical machine translation. In ACL.
A. Lavie and A. Agarwal. 2007. METEOR: An auto-
matic metric for mt evaluation with high levels of cor-
relation with human judgments. In Workshop on Sta-
tistical Machine Translation.
P. Liang, A. Bouchard-Cote, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In ACL.
Ding Liu and Daniel Gildea. 2007. Source-language fea-
tures and maximum correlation training for machine
translation evaluation. In NAACL.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng. 2011.
Better evaluation metrics lead to better machine trans-
lation. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Wolfgang Macherey, Franz Och, Ignacio Thayer, and
Jakob Uszkoreit. 2008. Lattice-based minimum er-
ror rate training for statistical machine translation. In
EMNLP.
R. T. Marler and J. S. Arora. 2004. Survey of
multi-objective optimization methods for engineering.
Structural and Multidisciplinary Optimization, 26.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2008.
Automatic evaluation measures for statistical machine
9
translation system optimization. In International Con-
ference on Language Resources and Evaluation, Mar-
rakech, Morocco, May.
Kaisa Miettinen. 1998. Nonlinear Multiobjective Opti-
mization. Springer.
J.A. Nelder and R. Mead. 1965. The downhill simplex
method. Computer Journal, 7(308).
Franz Och. 2003. Minimum error rate training in statis-
tical machine translation. In ACL.
Karolina Owczarzak, Josef van Genabith, and Andy Way.
2007. Labelled dependencies in machine translation
evaluation. In Proceedings of the Second Workshop
on Statistical Machine Translation.
Sebastian Pado, Daniel Cer, Michel Galley, Dan Jurafsky,
and Christopher D. Manning. 2009. Measuring ma-
chine translation quality as semantic equivalence: A
metric based on entailment features. Machine Trans-
lation, 23(2-3).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eval-
uation of machine translation. In ACL.
Vilfredo Pareto. 1906. Manuale di Economica Politica,
(Translated into English by A.S. Schwier as Manual of
Political Economy, 1971). Societa Editrice Libraria,
Milan.
Michael Paul. 2010. Overview of the iwslt 2010 evalua-
tion campaign. In IWSLT.
Yoshikazu Sawaragi, Hirotaka Nakayama, and Tetsuzo
Tanino, editors. 1985. Theory of Multiobjective Opti-
mization. Academic Press.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In AMTA.
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Juraf-
sky. 2011. Lateen em: Unsupervised training with
multiple objectives, applied to dependency grammar
induction. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing,
pages 1269?1280, Edinburgh, Scotland, UK., July. As-
sociation for Computational Linguistics.
Omar Zaidan. 2009. Z-MERT: A fully configurable open
source tool for minimum error rate training of machine
translation systems. In The Prague Bulletin of Mathe-
matical Linguistics.
10
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 100?104,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Comparative Study of Target Dependency Structures
for Statistical Machine Translation
Xianchao Wu?, Katsuhito Sudoh, Kevin Duh?, Hajime Tsukada, Masaaki Nagata
NTT Communication Science Laboratories, NTT Corporation
2-4 Hikaridai Seika-cho, Soraku-gun Kyoto 619-0237 Japan
wuxianchao@gmail.com,sudoh.katsuhito@lab.ntt.co.jp,
kevinduh@is.naist.jp,{tsukada.hajime,nagata.masaaki}@lab.ntt.co.jp
Abstract
This paper presents a comparative study of
target dependency structures yielded by sev-
eral state-of-the-art linguistic parsers. Our ap-
proach is to measure the impact of these non-
isomorphic dependency structures to be used
for string-to-dependency translation. Besides
using traditional dependency parsers, we also
use the dependency structures transformed
from PCFG trees and predicate-argument
structures (PASs) which are generated by an
HPSG parser and a CCG parser. The experi-
ments on Chinese-to-English translation show
that the HPSG parser?s PASs achieved the best
dependency and translation accuracies.
1 Introduction
Target language side dependency structures have
been successfully used in statistical machine trans-
lation (SMT) by Shen et al (2008) and achieved
state-of-the-art results as reported in the NIST 2008
Open MT Evaluation workshop and the NTCIR-9
Chinese-to-English patent translation task (Goto et
al., 2011; Ma and Matsoukas, 2011). A primary ad-
vantage of dependency representations is that they
have a natural mechanism for representing discon-
tinuous constructions, which arise due to long-
distance dependencies or in languages where gram-
matical relations are often signaled by morphology
instead of word order (McDonald and Nivre, 2011).
It is known that dependency-style structures can
be transformed from a number of linguistic struc-
?Now at Baidu Inc.
?Now at Nara Institute of Science & Technology (NAIST)
tures. For example, using the constituent-to-
dependency conversion approach proposed by Jo-
hansson and Nugues (2007), we can easily yield de-
pendency trees from PCFG style trees. A seman-
tic dependency representation of a whole sentence,
predicate-argument structures (PASs), are also in-
cluded in the output trees of (1) a state-of-the-art
head-driven phrase structure grammar (HPSG) (Pol-
lard and Sag, 1994; Sag et al, 2003) parser, Enju1
(Miyao and Tsujii, 2008) and (2) a state-of-the-art
CCG parser2 (Clark and Curran, 2007). The moti-
vation of this paper is to investigate the impact of
these non-isomorphic dependency structures to be
used for SMT. That is, we would like to provide a
comparative evaluation of these dependencies in a
string-to-dependency decoder (Shen et al, 2008).
2 Gaining Dependency Structures
2.1 Dependency tree
We follow the definition of dependency graph and
dependency tree as given in (McDonald and Nivre,
2011). A dependency graph G for sentence s is
called a dependency tree when it satisfies, (1) the
nodes cover all the words in s besides the ROOT;
(2) one node can have one and only one head (word)
with a determined syntactic role; and (3) the ROOT
of the graph is reachable from all other nodes.
For extracting string-to-dependency transfer
rules, we use well-formed dependency structures,
either fixed or floating, as defined in (Shen et al,
2008). Similarly, we ignore the syntactic roles
1http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html
2http://groups.inf.ed.ac.uk/ccg/software.html
100
 when the fluid pressure cylinder 31 is used , fluid is gradually applied . 
t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 
c2 c5 c7 c9 c11 c12 c14 c15 c17 c20 c22 c24 c25 
c3 
c4 
c6 
c8 
c10 c13 
c18 
c19 
c21 
c23 
c16 
c1 
c0 
conj_ 
arg12 
det_ 
arg1 
adj_ 
arg1 
noun_ 
arg1 
noun_ 
arg0 
adj_ 
arg1 
aux_ 
arg12 
verb_ 
arg12 
punct_ 
arg1 
noun_ 
arg0 
aux_ 
arg12 
adj_ 
arg1 
verb_ 
arg12 
* + 
* + 
* + 
* 
+ 
* + 
* + 
* + 
*  
* + 
* + 
* + 
* + 
* + 
+ 
Figure 1: HPSG tree of an example sentence. ?*?/
?+?=syntactic/semantic heads. Arrows in red (upper)=
PASs, orange (bottom)=word-level dependencies gener-
ated from PASs, blue=newly appended dependencies.
both during rule extracting and target dependency
language model (LM) training.
2.2 Dependency parsing
Graph-based and transition-based are two predom-
inant paradigms for data-driven dependency pars-
ing. The MST parser (McDonald et al, 2005) and
the Malt parser (Nivre, 2003) stand for two typical
parsers, respectively. Parsing accuracy comparison
and error analysis under the CoNLL-X dependency
shared task data (Buchholz and Marsi, 2006) have
been performed by McDonald and Nivre (2011).
Here, we compare them on the SMT tasks through
parsing the real-world SMT data.
2.3 PCFG parsing
For PCFG parsing, we select the Berkeley parser
(Petrov and Klein, 2007). In order to generate word-
level dependency trees from the PCFG tree, we use
the LTH constituent-to-dependency conversion tool3
written by Johansson and Nugues (2007). The head
finding rules4 are according to Magerman (1995)
and Collins (1997). Similar approach has been orig-
inally used by Shen et al (2008).
2.4 HPSG parsing
In the Enju English HPSG grammar (Miyao et al,
2003) used in this paper, the semantic content of
3http://nlp.cs.lth.se/software/treebank converter/
4http://www.cs.columbia.edu/ mcollins/papers/heads
a sentence/phrase is represented by a PAS. In an
HPSG tree, each leaf node generally introduces a
predicate, which is represented by the pair made up
of the lexical entry feature and predicate type fea-
ture. The arguments of a predicate are designated by
the arrows from the argument features in a leaf node
to non-terminal nodes (e.g., t0?c3, t0?c16).
Since the PASs use the non-terminal nodes in the
HPSG tree (Figure 1), this prevents their direct us-
age in a string-to-dependency decoder. We thus need
an algorithm to transform these phrasal predicate-
argument dependencies into a word-to-word depen-
dency tree. Our algorithm (refer to Figure 1 for an
example) for changing PASs into word-based depen-
dency trees is as follows:
1. finding, i.e., find the syntactic/semantic head
word of each argument node through a bottom-
up traversal of the tree;
2. mapping, i.e., determine the arc directions
(among a predicate word and the syntac-
tic/semantic head words of the argument nodes)
for each predicate type according to Table 1.
Then, a dependency graph will be generated;
3. checking, i.e., post modifying the dependency
graph according to the definition of dependency
tree (Section 2.1).
Table 1 lists the mapping from HPSG?s PAS types
to word-level dependency arcs. Since a non-terminal
node in an HPSG tree has two kinds of heads, syn-
tactic or semantic, we will generate two dependency
graphs after mapping. We use ?PAS+syn? to repre-
sent the dependency trees generated from the HPSG
PASs guided by the syntactic heads. For semantic
heads, we use ?PAS+sem?.
For example, refer to t0 = when in Figure 1.
Its arg1 = c16 (with syntactic head t10), arg2
= c3 (with syntactic head t6), and PAS type =
conj arg12. In Table 1, this PAS type corresponds
to arg2?pred?arg1, then the result word-level de-
pendency is t6(is)?t0(when)?t10(is).
We need to post modify the dependency graph af-
ter applying the mapping, since it is not guaranteed
to be a dependency tree. Referring to the definition
of dependency tree (Section 2.1), we need the strat-
egy for (1) selecting only one head from multiple
101
PAS Type Dependency Relation
adj arg1[2] [arg2 ?] pred ? arg1
adj mod arg1[2] [arg2 ?] pred ? arg1 ? mod
aux[ mod] arg12 arg1/pred ? arg2 [? mod]
conj arg1[2[3]] [arg2[/arg3]] ? pred ? arg1
comp arg1[2] pred ? arg1 [? arg2]
comp mod arg1 arg1 ? pred ? mod
noun arg1 pred ? arg1
noun arg[1]2 arg2 ? pred [? arg1]
poss arg[1]2 pred ? arg2 [? arg1]
prep arg12[3] arg2[/arg3] ? pred ? arg1
prep mod arg12[3] arg2[/arg3] ? pred ? arg1 ? mod
quote arg[1]2 [arg1 ?] pred ? arg2
quote arg[1]23 [arg1/]arg3 ? pred ? arg2
lparen arg123 pred/arg2 ? arg3 ? arg1
relative arg1[2] [arg2 ?] pred ? arg1
verb arg1[2[3[4]]] arg1[/arg2[/arg3[/arg4]]] ? pred
verb mod arg1[2[3[4]]] arg1[/arg2[/arg3[/arg4]]]?pred?mod
app arg12,coord arg12 arg2/pred ? arg1
det arg1,it arg1,punct arg1 pred ? arg1
dtv arg2 pred ? arg2
lgs arg2 arg2 ? pred
Table 1: Mapping fromHPSG?s PAS types to dependency
relations. Dependent(s)? head(s), / = and, [] = optional.
heads and (2) appending dependency relations for
those words/punctuation that do not have any head.
When one word has multiple heads, we only keep
one. The selection strategy is that, if this arc was
deleted, it will cause the biggest number of words
that can not reach to the root word anymore. In case
of a tie, we greedily pack the arc that connect two
words wi and wj where |i? j| is the biggest. For all
the words and punctuation that do not have a head,
we greedily take the root word of the sentence as
their heads. In order to fully use the training data,
if there are directed cycles in the result dependency
graph, we still use the graph in our experiments,
where only partial dependency arcs, i.e., those target
flat/hierarchical phrases attached with well-formed
dependency structures, can be used during transla-
tion rule extraction.
2.5 CCG parsing
We also use the predicate-argument dependencies
generated by the CCG parser developed by Clark
and Curran (2007). The algorithm for generating
word-level dependency tree is easier than processing
the PASs included in the HPSG trees, since the word
level predicate-argument relations have already been
included in the output of CCG parser. The mapping
from predicate types to the gold-standard grammat-
ical relations can be found in Table 13 in (Clark and
Curran, 2007). The post-processing is like that de-
scribed for HPSG parsing, except we greedily use
the MST?s sentence root when we can not determine
it based on the CCG parser?s PASs.
3 Experiments
3.1 Setup
We re-implemented the string-to-dependency de-
coder described in (Shen et al, 2008). Dependency
structures from non-isomorphic syntactic/semantic
parsers are separately used to train the transfer
rules as well as target dependency LMs. For intu-
itive comparison, an outside SMT system is Moses
(Koehn et al, 2007).
For Chinese-to-English translation, we use the
parallel data from NIST Open Machine Translation
Evaluation tasks. The training data contains 353,796
sentence pairs, 8.7M Chinese words and 10.4M En-
glish words. The NIST 2003 and 2005 test data
are respectively taken as the development and test
set. We performed GIZA++ (Och and Ney, 2003)
and the grow-diag-final-and symmetrizing strategy
(Koehn et al, 2007) to obtain word alignments. The
Berkeley Language Modeling Toolkit, berkeleylm-
1.0b35 (Pauls and Klein, 2011), was employed to
train (1) a five-gram LM on the Xinhua portion of
LDC English Gigaword corpus v3 (LDC2007T07)
and (2) a tri-gram dependency LM on the English
dependency structures of the training data. We re-
port the translation quality using the case-insensitive
BLEU-4 metric (Papineni et al, 2002).
3.2 Statistics of dependencies
We compare the similarity of the dependencies with
each other, as shown in Table 2. Basically, we in-
vestigate (1) if two dependency graphs of one sen-
tence share the same root word and (2) if the head of
one word in one sentence are identical in two depen-
dency graphs. In terms of root word comparison, we
observe that MST and CCG share 87.3% of iden-
tical root words, caused by borrowing roots from
MST to CCG. Then, it is interesting that Berkeley
and PAS+syn share 74.8% of identical root words.
Note that the Berkeley parser is trained on the Penn
treebank (Marcus et al, 1994) yet the HPSG parser
is trained on the HPSG treebank (Miyao and Tsujii,
5http://code.google.com/p/berkeleylm/
102
Dependency Precision Recall BLEU-Dev BLEU-Test # phrases # hier rules # illegal dep trees # directed cycles
Moses-1 - - 0.3349 0.3207 5.4M - - -
Moses-2 - - 0.3445 0.3262 0.7M 4.5M - -
MST 0.744 0.750 0.3520 0.3291 2.4M 2.1M 251 0
Malt 0.732 0.738 0.3423 0.3203 1.5M 1.3M 130,960 0
Berkeley 0.800 0.806 0.3475 0.3312 2.4M 2.2M 282 0
PAS+syn 0.818 0.824 0.3499 0.3376 2.2M 1.9M 10,411 5,853
PAS+sem 0.777 0.782 0.3484 0.3343 2.1M 1.6M 14,271 9,747
CCG 0.701 0.705 0.3442 0.3283 1.7M 1.3M 61,015 49,955
Table 3: Comparison of dependency and translation accuracies. Moses-1 = phrasal, Moses-2 = hierarchical.
Malt Berkeley PAS PAS CCG
+syn +sem
MST 70.5 62.5 69.2 53.3 87.3
(77.3) (64.6) (58.5) (58.1) (61.7)
Malt 66.2 73.0 46.8 62.9
(63.2) (57.7) (56.6) (58.1)
Berkeley 74.8 44.2 56.5
(64.3) (56.0) (59.2)
PAS+ 59.3 62.9
syn (79.1) (61.0)
PAS+ 60.0
sem (58.8)
Table 2: Comparison of the dependencies of the English
sentences in the training data. Without () = % of similar
root words; with () = % of similar head words.
2008). In terms of head word comparison, PAS+syn
and PAS+sem share 79.1% of identical head words.
This is basically due to that we used the similar
PASs of the HPSG trees. Interestingly, there are only
59.3% identical root words shared by PAS+syn and
PAS+sem. This reflects the significant difference be-
tween syntactic and semantic heads.
We also manually created the golden dependency
trees for the first 200 English sentences in the train-
ing data. The precision/recall (P/R) are shown in
Table 3. We observe that (1) the translation accura-
cies approximately follow the P/R scores yet are not
that sensitive to their large variances, and (2) it is
still tough for domain-adapting from the treebank-
trained parsers to parse the real-world SMT data.
PAS+syn performed the best by avoiding the errors
of missing of arguments for a predicate, wrongly
identified head words for a linguistic phrase, and in-
consistency dependencies inside relatively long co-
ordinate structures. These errors significantly influ-
ence the number of extractable translation rules and
the final translation accuracies.
Note that, these P/R scores on the first 200 sen-
tences (all from less than 20 newswire documents)
shall only be taken as an approximation of the total
training data and not necessarily exactly follow the
tendency of the final BLEU scores. For example,
CCG is worse than Malt in terms of P/R yet with a
higher BLEU score. We argue this is mainly due to
that the number of illegal dependency trees gener-
ated by Malt is the highest. Consequently, the num-
ber of flat/hierarchical rules generated by using Malt
trees is the lowest. Also, PAS+sem has a lower P/R
than Berkeley, yet their final BLEU scores are not
statistically different.
3.3 Results
Table 3 also shows the BLEU scores, the number of
flat phrases and hierarchical rules (both integrated
with target dependency structures), and the num-
ber of illegal dependency trees generated by each
parser. From the table, we have the following ob-
servations: (1) all the dependency structures (except
Malt) achieved a significant better BLEU score than
the phrasal Moses; (2) PAS+syn performed the best
in the test set (0.3376), and it is significantly better
than phrasal/hierarchical Moses (p < 0.01), MST
(p < 0.05), Malt (p < 0.01), Berkeley (p < 0.05),
and CCG (p < 0.05); and (3) CCG performed as
well as MST and Berkeley. These results lead us to
argue that the robustness of deep syntactic parsers
can be advantageous in SMT compared with tradi-
tional dependency parsers.
4 Conclusion
We have constructed a string-to-dependency trans-
lation platform for comparing non-isomorphic tar-
get dependency structures. Specially, we proposed
an algorithm for generating word-based dependency
trees from PASs which are generated by a state-of-
the-art HPSG parser. We found that dependency
trees transformed from these HPSG PASs achieved
the best dependency/translation accuracies.
103
Acknowledgments
We thank the anonymous reviewers for their con-
structive comments and suggestions.
References
Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared
task on multilingual dependency parsing. In Proceed-
ings of the Tenth Conference on Computational Nat-
ural Language Learning (CoNLL-X), pages 149?164,
New York City, June. Association for Computational
Linguistics.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with ccg and log-
linear models. Computational Linguistics, 33(4):493?
552.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Annual Meeting of the Association for Computa-
tional Linguistics, pages 16?23, Madrid, Spain, July.
Association for Computational Linguistics.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K. Tsou. 2011. Overview of the patent ma-
chine translation task at the ntcir-9 workshop. In Pro-
ceedings of NTCIR-9, pages 559?578.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for english. In
In Proceedings of NODALIDA, Tartu, Estonia, April.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the ACL 2007 Demo and Poster Sessions, pages
177?180.
Jeff Ma and Spyros Matsoukas. 2011. Bbn?s systems
for the chinese-english sub-task of the ntcir-9 patentmt
evaluation. In Proceedings of NTCIR-9, pages 579?
584.
David Magerman. 1995. Statistical decision-tree models
for parsing. In In Proceedings of of the 33rd Annual
Meeting of the Association for Computational Linguis-
tics, pages 276?283.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The penn tree-
bank: Annotating predicate argument structure. In
Proceedings of the Workshop on HLT, pages 114?119,
Plainsboro.
Ryan McDonald and Joakim Nivre. 2011. Analyzing
and integrating dependency parsers. Computational
Linguistics, 37(1):197?230.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 91?98, Ann Arbor, Michigan, June.
Association for Computational Linguistics.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature forest
models for probabilistic hpsg parsing. Computational
Lingustics, 34(1):35?80.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsu-
jii. 2003. Probabilistic modeling of argument struc-
tures including non-local dependencies. In Proceed-
ings of the International Conference on Recent Ad-
vances in Natural Language Processing, pages 285?
291, Borovets.
Joakim Nivre. 2003. An efficient algorithm for projec-
tive dependency parsing. In Proceedings of the 8th In-
ternational Workshop on Parsing Technologies (IWPT,
pages 149?160.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of ACL,
pages 311?318.
Adam Pauls and Dan Klein. 2011. Faster and smaller
n-gram language models. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
258?267, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Human Language Tech-
nologies 2007: The Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics; Proceedings of the Main Conference, pages
404?411, Rochester, New York, April. Association for
Computational Linguistics.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press.
Ivan A. Sag, Thomas Wasow, and Emily M. Bender.
2003. Syntactic Theory: A Formal Introduction.
Number 152 in CSLI Lecture Notes. CSLI Publica-
tions.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL-08:HLT, pages 577?585, Colum-
bus, Ohio.
104
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 678?683,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Adaptation Data Selection using Neural Language Models:
Experiments in Machine Translation
Kevin Duh, Graham Neubig
Graduate School of Information Science
Nara Institute of Science and Technology
8916-5 Takayama, Ikoma, Japan
kevinduh@is.naist.jp
neubig@is.naist.jp
Katsuhito Sudoh, Hajime Tsukada
NTT Communication Science Labs.
NTT Corporation
2-4 Hikaridai, Seika, Kyoto, Japan
sudoh.katsuhito@lab.ntt.co.jp
tsukada.hajime@lab.ntt.co.jp
Abstract
Data selection is an effective approach
to domain adaptation in statistical ma-
chine translation. The idea is to use lan-
guage models trained on small in-domain
text to select similar sentences from large
general-domain corpora, which are then
incorporated into the training data. Sub-
stantial gains have been demonstrated in
previous works, which employ standard n-
gram language models. Here, we explore
the use of neural language models for data
selection. We hypothesize that the con-
tinuous vector representation of words in
neural language models makes them more
effective than n-grams for modeling un-
known word contexts, which are prevalent
in general-domain text. In a comprehen-
sive evaluation of 4 language pairs (En-
glish to German, French, Russian, Span-
ish), we found that neural language mod-
els are indeed viable tools for data se-
lection: while the improvements are var-
ied (i.e. 0.1 to 1.7 gains in BLEU), they
are fast to train on small in-domain data
and can sometimes substantially outper-
form conventional n-grams.
1 Introduction
A perennial challenge in building Statistical Ma-
chine Translation (SMT) systems is the dearth
of high-quality bitext in the domain of interest.
An effective and practical solution is adaptation
data selection: the idea is to use language models
(LMs) trained on in-domain text to select similar
sentences from large general-domain corpora. The
selected sentences are then incorporated into the
SMT training data. Analyses have shown that this
augmented data can lead to better statistical esti-
mation or word coverage (Duh et al, 2010; Had-
dow and Koehn, 2012).
Although previous works in data selection (Ax-
elrod et al, 2011; Koehn and Haddow, 2012; Ya-
suda et al, 2008) have shown substantial gains, we
suspect that the commonly-used n-gram LMs may
be sub-optimal. The small size of the in-domain
text implies that a large percentage of general-
domain sentences will contain words not observed
in the LM training data. In fact, as many as 60% of
general-domain sentences contain at least one un-
known word in our experiments. Although the LM
probabilities of these sentences could still be com-
puted by resorting to back-off and other smoothing
techniques, a natural question remains: will alter-
native, more robust LMs do better?
We hypothesize that the neural language model
(Bengio et al, 2003) is a viable alternative, since
its continuous vector representation of words is
well-suited for modeling sentences with frequent
unknown words, providing smooth probability es-
timates of unseen but similar contexts. Neu-
ral LMs have achieved positive results in speech
recognition and SMT reranking (Schwenk et al,
2012; Mikolov et al, 2011a). To the best of our
knowledge, this paper is the first work that exam-
ines neural LMs for adaptation data selection.
2 Data Selection Method
We employ the data selection method of (Ax-
elrod et al, 2011), which builds upon (Moore
and Lewis, 2010). The intuition is to select
general-domain sentences that are similar to in-
domain text, while being dis-similar to the average
general-domain text.
To do so, one defines the score of an general-
domain sentence pair (e, f) as:
[INE(e)?GENE(e)] + [INF (f)?GENF (f)]
(1)
where INE(e) is the length-normalized cross-
entropy of e on the English in-domain LM.
GENE(e) is the length-normalized cross-entropy
678
Figure 1: Recurrent neural LM.
of e on the English general-domain LM, which
is built from a sub-sample of the general-domain
text. Similarly, INF (f) and GENF (f) are the
cross-entropies of f on Foreign-side LM. Finally,
sentence pairs are ranked according to Eq. 1 and
those with scores lower than some empirically-
chosen threshold are added to the bitext for trans-
lation model training.
2.1 Neural Language Models
The four LMs used to compute Eq. 1 have con-
ventionally been n-grams. N-grams of the form
p(w(t)|w(t ? 1), w(t ? 2), . . .) predict words by
using multinomial distributions conditioned on the
context (w(t?1), w(t?2), . . .). But when the con-
text is rare or contains unknown words, n-grams
are forced to back-off to lower-order models, e.g.
p(w(t)|w(t ? 1)). These backoffs are unfortu-
nately very frequent in adaptation data selection.
Neural LMs, in contrast, model word probabili-
ties using continuous vector representations. Fig-
ure 1 shows a type of neural LMs called recurrent
neural networks (Mikolov et al, 2011b).1 Rather
than representing context as an identity (n-gram
hit-or-miss) function on [w(t ? 1), w(t ? 2), . . .],
neural LMs summarize the context by a hidden
state vector s(t). This is a continuous vector of
dimension |S| whose elements are predicted by
the previous word w(t ? 1) and previous state
s(t ? 1). This is robust to rare contexts because
continuous representations enable sharing of sta-
tistical strength between similar contexts. Bengio
(2009) shows that such representations are better
than multinomials in alleviating sparsity issues.
1Another major type of neural LMs are the so-called
feed-forward networks (Bengio et al, 2003; Schwenk, 2007;
Nakamura et al, 1990). Both types of neural LMs have seen
many improvements recently, in terms of computational scal-
ability (Le et al, 2011) and modeling power (Arisoy et al,
2012; Wu et al, 2012; Alexandrescu and Kirchhoff, 2006).
We focus on recurrent networks here since there are fewer
hyper-parameters and its ability to model infinite context us-
ing recursion is theoretically attractive. But we note that feed-
forward networks are just as viable.
Now, given state vector s(t), we can predict the
probability of the current word. Figure 1 is ex-
pressed formally in the following equations:
w(t) = [w0(t), . . . , wk(t), . . . w|W |(t)] (2)
wk(t) = g
?
?
|S|?
j=0
sj(t)Vkj
?
? (3)
sj(t)=f
?
?
|W |?
i=0
wi(t? 1)Uji +
|S|?
i?=0
si?(t? 1)Aji?
?
?
(4)
Here, w(t) is viewed as a vector of dimension
|W | (vocabulary size) where each element wk(t)
represents the probability of the k-th vocabulary
item at sentence position t. The function g(zk) =
ezk/?k ezk is a softmax function that ensures the
neural LM outputs are proper probabilities, and
f(z) = 1/(1 + e?z) is a sigmoid activation that
induces the non-linearity critical to the neural net-
work?s expressive power. The matrices V , U , and
A are trained by maximizing likelihood on train-
ing data using a ?backpropagation-through-time?
method.2 Intuitively, U and A compress the con-
text (|S| < |W |) such that contexts predictive of
the same word w(t) are close together.
Since proper modeling of unknown contexts is
important in our problem, training text for both n-
gram and neural LM is pre-processed by convert-
ing all low-frequency words in the training data
(frequency=1 in our case) to a special ?unknown?
token. This is used only in Eq. 1 for selecting
general-domain sentences; these words retain their
surface forms in the SMT train pipeline.
3 Experiment Setup
We experimented with four language pairs in the
WIT3 corpus (Cettolo et al, 2012), with English
(en) as source and German (de), Spanish (es),
French (fr), Russian (ru) as target. This is the
in-domain corpus, and consists of TED Talk tran-
scripts covering topics in technology, entertain-
ment, and design. As general-domain corpora,
we collected bitext from the WMT2013 campaign,
including CommonCrawl and NewsCommentary
for all 4 languages, Europarl for de/es/fr, UN for
es/fr, Gigaword for fr, and Yandex for ru. The in-
domain data is divided into a training set (for SMT
2The recurrent states are unrolled for several time-steps,
then stochastic gradient descent is applied.
679
en-de en-es en-fr en-ru
In-domain Training Set
#sentence 129k 140k 139k 117k
#token (en) 2.5M 2.7M 2.7M 2.3M
#vocab (en) 26k 27k 27k 25k
#vocab (f) 42k 39k 34k 58k
General-domain Bitext
#sentence 4.4M 14.7M 38.9M 2.0M
#token (en) 113M 385M 1012M 51M
%unknown 60% 58% 64% 65%
Table 1: Data statistics. ?%unknown?=fraction of
general-domain sentences with unknown words.
pipeline and neural LM training), a tuning set (for
MERT), a validation set (for choosing the optimal
threshold in data selection), and finally a testset of
1616 sentences.3 Table 1 lists data statistics.
For each language pair, we built a baseline in-
data SMT system trained only on in-domain data,
and an alldata system using combined in-domain
and general-domain data.4 We then built 3 systems
from augmented data selected by different LMs:
? ngram: Data selection by 4-gram LMs with
Kneser-Ney smoothing (Axelrod et al, 2011)
? neuralnet: Data selection by Recurrent neu-
ral LM, with the RNNLM Toolkit.5
? combine: Data selection by interpolated LM
using n-gram & neuralnet (equal weight).
All systems are built using standard settings in
the Moses toolkit (GIZA++ alignment, grow-diag-
final-and, lexical reordering models, and SRILM).
Note that standard n-grams are used as LMs for
SMT; neural LMs are only used for data selection.
Multiple SMT systems are trained by thresholding
on {10k,50k,100k,500k,1M} general-domain sen-
tence subsets, and we empirically determine the
single system for testing based on results on a sep-
arate validation set (in practice, 500k was chosen
for fr and 1M for es, de, ru.).
3The original data are provided by http://wit3.fbk.eu and
http://www.statmt.org/wmt13/. Our domain adaptation sce-
nario is similar to the IWSLT2012 campaign but we used our
own random train/test splits, since we wanted to ensure the
testset for all languages had identical source sentences for
comparison purposes. For replicability, our software is avail-
able at http://cl.naist.jp/?kevinduh/a/acl2013.
4More advanced phrase table adaptation methods are pos-
sible. but our interest is in comparing data selection methods.
The conclusions should transfer to advanced methods such as
(Foster et al, 2010; Niehues and Waibel, 2012).
5http://www.fit.vutbr.cz/?imikolov/rnnlm/
4 Results
4.1 LM Perplexity and Training Time
First, we measured perplexity to check the gen-
eralization ability of our neural LMs as language
models. Recall that we train four LMs to com-
pute each of the components of Eq. 1. In Table 2,
we compared each of the four versions of ngram,
neuralnet, and combine LMs on in-domain test
sets or general-domain held-out sets. It re-affirms
previous positive results (Mikolov et al, 2011a),
with neuralnet outperforming ngram by 20-30%
perplexity across all tasks. Also, combine slightly
improves the perplexity of neuralnet.
Task ngram neuralnet combine
In-Domain Test Set
en-de de 157 110 (29%) 110 (29%)
en-de en 102 81 (20%) 78 (24%)
en-es es 129 102 (20%) 98 (24%)
en-es en 101 80 (21%) 77 (24%)
en-fr fr 90 67 (25%) 65 (27%)
en-fr en 102 80 (21%) 77 (24%)
en-ru ru 208 167 (19%) 155 (26%)
en-ru en 103 83 (19%) 79 (23%)
General-Domain Held-out Set
en-de de 234 174 (25%) 161 (31%)
en-de en 218 168 (23%) 155 (29%)
en-es es 62 43 (31%) 43 (31%)
en-es en 84 61 (27%) 59 (30%)
en-fr fr 64 43 (33%) 43 (33%)
en-fr en 95 67 (30%) 65 (32%)
en-ru ru 242 199 (18%) 176 (27%)
en-ru en 191 153 (20%) 142 (26%)
Table 2: Perplexity of various LMs. Number in
parenthesis is percentage improvement vs. ngram.
Second, we show that the usual concern of neu-
ral LM training time is not so critical for the in-
domain data sizes used domain adaptation. The
complexity of training Figure 1 is dominated by
computing Eq. 3 and scales as O(|W | ? |S|) in
the number of tokens. Since |W | can be large, one
practical trick is to cluster the vocabulary so that
the output dimension is reduced. Table 3 shows
the training times on a 3.3GHz XeonE5 CPU by
varying these two main hyper-parameters (|S| and
cluster size). Note that the setting |S| = 200 and
cluster size of 100 already gives good perplexity
in reasonable training time. All neural LMs in this
paper use this setting, without additional tuning.
680
|S| Cluster Time Perplexity
200 100 198m 110
100 |W | 12915m 110
200 400 208m 113
100 100 52m 118
100 400 71m 120
Table 3: Training time (in minutes) for various
neural LM architectures (Task: en-de de).
4.2 End-to-end SMT Evaluation
Table 4 shows translation results in terms of BLEU
(Papineni et al, 2002), RIBES (Isozaki et al,
2010), and TER (Snover et al, 2006). We observe
that all three data selection methods essentially
outperform alldata and indata for all language
pairs, and neuralnet tend to be the best in all met-
rics. E.g., BLEU improvements over ngram are
in the range of 0.4 for en-de, 0.5 for en-es, 0.1
for en-fr, and 1.7 for en-ru. Although not all im-
provements are large in absolute terms, many are
statistically significant (95% confidence).
We therefore believe that neural LMs are gen-
erally worthwhile to try for data selection, as it
rarely underperform n-grams. The open question
is: what can explain the significant improvements
in, for example Russian, Spanish, German, but the
lack thereof in French? One conjecture is that
neural LMs succeeded in lowering testset out-of-
vocabulary (OOV) rate, but we found that OOV
reduction is similar across all selection methods.
The improvements appear to be due to better
probability estimates of the translation/reordering
models. We performed a diagnostic by decoding
the testset using LMs trained on the same test-
set, while varying the translation/reordering ta-
bles with those of ngram and neuralnet; this is a
kind of pseudo forced-decoding that can inform us
about which table has better coverage. We found
that across all language pairs, BLEU differences of
translations under this diagnostic become insignif-
icant, implying that the raw probability value is
the differentiating factor between ngram and neu-
ralnet. Manual inspection of en-de revealed that
many improvements come from lexical choice in
morphological variants (?meinen Sohn? vs. ?mein
Sohn?), segmentation changes (?baking soda? ?
?Backpulver? vs. ?baken Soda?), and handling of
unaligned words at phrase boundaries.
Finally, we measured the intersection between
the sentence set selected by ngram vs neural-
Task System BLEU RIBES TER
en-de indata 20.8 80.1 59.0
alldata 21.5 80.1 59.1
ngram 21.5 80.3 58.9
neuralnet 21.9+ 80.5+ 58.4
combine 21.5 80.2 58.8
en-es indata 30.4 83.5 48.7
alldata 31.2 83.2 49.9
ngram 32.0 83.7 48.4
neuralnet 32.5+ 83.7 48.3+
combine 32.5+ 83.8 48.3+
en-fr indata 31.4 83.9 51.2
alldata 31.5 83.5 51.4
ngram 32.7 83.7 50.4
neuralnet 32.8 84.2+ 50.3
combine 32.5 84.0 50.5
en-ru indata 14.8 72.5 69.5
alldata 23.4 75.0 62.3
ngram 24.0 75.7 61.4
neuralnet 25.7+ 76.1 60.0+
combine 23.7 75.9 61.9?
Table 4: End-to-end Translation Results. The best
results are bold-faced. We also compare neural
LMs to ngram using pairwise bootstrap (Koehn,
2004): ?+? means statistically significant im-
provement and ??? means significant degradation.
net. They share 60-75% of the augmented train-
ing data. This high overlap means that ngram
and neuralnet are actually not drastically different
systems, and neuralnet with its slightly better se-
lections represent an incremental improvement.6
5 Conclusions
We perform an evaluation of neural LMs for
adaptation data selection, based on the hypothe-
sis that their continuous vector representations are
effective at comparing general-domain sentences,
which contain frequent unknown words. Com-
pared to conventional n-grams, we observed end-
to-end translation improvements from 0.1 to 1.7
BLEU. Since neural LMs are fast to train in the
small in-domain data setting and achieve equal or
incrementally better results, we conclude that they
are an worthwhile option to include in the arsenal
of adaptation data selection techniques.
6This is corroborated by another analysis: taking the
union of sentences found by ngram and neuralnet gives sim-
ilar BLEU scores as neuralnet.
681
Acknowledgments
We thank Amittai Axelrod for discussions about
data selection implementation details, and an
anonymous reviewer for suggesting the union idea
for results analysis. K. D. would like to credit Spy-
ros Matsoukas (personal communication, 2010)
for the trick of using LM-based pseudo forced-
decoding for error analysis.
References
Andrei Alexandrescu and Katrin Kirchhoff. 2006.
Factored neural language models. In Proceed-
ings of the Human Language Technology Confer-
ence of the NAACL, Companion Volume: Short Pa-
pers, NAACL-Short ?06, pages 1?4, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Ebru Arisoy, Tara N. Sainath, Brian Kingsbury, and
Bhuvana Ramabhadran. 2012. Deep neural network
language models. In Proceedings of the NAACL-
HLT 2012 Workshop: Will We Ever Really Replace
the N-gram Model? On the Future of Language
Modeling for HLT, pages 20?28, Montre?al, Canada,
June. Association for Computational Linguistics.
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 355?362, Edinburgh, Scotland, UK., July.
Association for Computational Linguistics.
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage models. JMLR.
Yoshua Bengio. 2009. Learning Deep Architectures
for AI, volume Foundations and Trends in Machine
Learning. NOW Publishers.
Mauro Cettolo, Christian Girardi, and Marcello Fed-
erico. 2012. Wit3: Web inventory of transcribed
and translated talks. In Proceedings of the 16th Con-
ference of the European Association for Machine
Translation (EAMT), pages 261?268, Trento, Italy,
May.
Kevin Duh, Katsuhito Sudoh, and Hajime Tsukada.
2010. Analysis of translation model adaptation for
statistical machine translation. In Proceedings of the
International Workshop on Spoken Language Trans-
lation (IWSLT) - Technical Papers Track.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adap-
tation in statistical machine translation. In EMNLP.
Barry Haddow and Philipp Koehn. 2012. Analysing
the effect of out-of-domain data on smt systems. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 422?432, Montre?al,
Canada, June. Association for Computational Lin-
guistics.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010. Automatic
evaluation of translation quality for distant language
pairs. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Process-
ing, pages 944?952, Cambridge, MA, October. As-
sociation for Computational Linguistics.
Philipp Koehn and Barry Haddow. 2012. Towards
effective use of training data in statistical machine
translation. In WMT.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In EMNLP.
Hai-Son Le, I. Oparin, A. Allauzen, J. Gauvain, and
F. Yvon. 2011. Structured output layer neural net-
work language model. In Acoustics, Speech and Sig-
nal Processing (ICASSP), 2011 IEEE International
Conference on, pages 5524?5527.
Toma?s? Mikolov, Anoop Deoras, Daniel Povey, Luka?s?
Burget, and Jan C?ernocky?. 2011a. Strategies for
training large scale neural network language model.
In ASRU.
Toma?s? Mikolov, Stefan Kombrink, Luka?s? Burget, Jan
C?ernocky?, and Sanjeev Khudanpur. 2011b. Exten-
sions of recurrent neural network language model.
In Proceedings of the 2011 IEEE International Con-
ference on Acoustics, Speech, and Signal Processing
(ICASSP).
Robert C. Moore and William Lewis. 2010. Intelligent
selection of language model training data. In Pro-
ceedings of the ACL 2010 Conference Short Papers,
pages 220?224, Uppsala, Sweden, July. Association
for Computational Linguistics.
Masami Nakamura, Katsuteru Maruyama, Takeshi
Kawabata, and Kiyohiro Shikano. 1990. Neural
network approach to word category prediction for
english texts. In Proceedings of the 13th conference
on Computational linguistics - Volume 3, COLING
?90, pages 213?218, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Jan Niehues and Alex Waibel. 2012. Detailed analysis
of different strategies for phrase table adaptation in
SMT. In AMTA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In ACL.
Holger Schwenk, Anthony Rousseau, and Mohammed
Attik. 2012. Large, pruned or continuous space
language models on a gpu for statistical machine
translation. In Proceedings of the NAACL-HLT 2012
Workshop: Will We Ever Really Replace the N-gram
Model? On the Future of Language Modeling for
682
HLT, pages 11?19, Montre?al, Canada, June. Associ-
ation for Computational Linguistics.
Holger Schwenk. 2007. Continuous space language
models. Comput. Speech Lang., 21(3):492?518,
July.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In AMTA.
Youzheng Wu, Xugang Lu, Hitoshi Yamamoto,
Shigeki Matsuda, Chiori Hori, and Hideki Kashioka.
2012. Factored language model based on recurrent
neural network. In Proceedings of COLING 2012,
pages 2835?2850, Mumbai, India, December. The
COLING 2012 Organizing Committee.
Keiji Yasuda, Ruiqiang Zhang, Hirofumi Yamamoto,
and Eiichiro Sumita. 2008. Method of selecting
training data to build a compact and efficient trans-
lation model. In ICJNLP.
683
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 244?251,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Head Finalization: A Simple Reordering Rule for SOV Languages
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, Kevin Duh
NTT Communication Science Laboratories, NTT Corporation
2-4 Hikaridai, Seikacho, Sorakugun, Kyoto, 619-0237, Japan
{isozaki,sudoh,tsukada,kevinduh}@cslab.kecl.ntt.co.jp
Abstract
English is a typical SVO (Subject-Verb-
Object) language, while Japanese is a typ-
ical SOV language. Conventional Statis-
tical Machine Translation (SMT) systems
work well within each of these language
families. However, SMT-based translation
from an SVO language to an SOV lan-
guage does not work well because their
word orders are completely different. Re-
cently, a few groups have proposed rule-
based preprocessing methods to mitigate
this problem (Xu et al, 2009; Hong et al,
2009). These methods rewrite SVO sen-
tences to derive more SOV-like sentences
by using a set of handcrafted rules. In this
paper, we propose an alternative single re-
ordering rule: Head Finalization. This
is a syntax-based preprocessing approach
that offers the advantage of simplicity. We
do not have to be concerned about part-
of-speech tags or rule weights because the
powerful Enju parser allows us to imple-
ment the rule at a general level. Our ex-
periments show that its result, Head Final
English (HFE), follows almost the same
order as Japanese. We also show that this
rule improves automatic evaluation scores.
1 Introduction
Statistical Machine Translation (SMT) is useful
for building a machine translator between a pair of
languages that follow similar word orders. How-
ever, SMT does not work well for distant language
pairs such as English and Japanese, since English
is an SVO language and Japanese is an SOV lan-
guage.
Some existing methods try to solve this word-
order problem in language-independent ways.
They usually parse input sentences and learn a re-
ordering decision at each node of the parse trees.
For example, Yamada and Knight (2001), Quirk et
al. (2005), Xia and McCord (2004), and Li et al
(2007) proposed such methods.
Other methods tackle this problem in language-
dependent ways (Katz-Brown and Collins, 2008;
Collins et al, 2005; Nguyen and Shimazu, 2006).
Recently, Xu et al (2009) and Hong et al (2009)
proposed rule-based preprocessing methods for
SOV languages. These methods parse input sen-
tences and reorder the words using a set of hand-
crafted rules to get SOV-like sentences.
If we could completely reorder the words in in-
put sentences by preprocessing to match the word
order of the target language, we would be able to
greatly reduce the computational cost of SMT sys-
tems.
In this paper, we introduce a single reordering
rule: Head Finalization. We simply move syntac-
tic heads to the end of the corresponding syntactic
constituents (e.g., phrases and clauses). We use
only this reordering rule, and we do not have to
consider part-of-speech tags or rule weights be-
cause the powerful Enju parser allows us to im-
plement the rule at a general level.
Why do we think this works? The reason is
simple: Japanese is a typical head-final language.
That is, a syntactic head word comes after non-
head (dependent) words. SOV is just one as-
pect of head-final languages. In order to imple-
ment this idea, we need a parser that outputs syn-
tactic heads. Enju is such a parser from the
University of Tokyo (http://www-tsujii.is.s.
u-tokyo.ac.jp/enju). We discuss other parsers
in section 5.
There is another kind of head: semantic heads.
Hong et al (2009) used Stanford parser (de Marn-
effe et al, 2006), which outputs semantic head-
based dependencies; Xu et al (2009) also used the
same representation.
The use of syntactic heads and the number
of dependents are essential for the simplicity of
244
Head Finalization (See Discussion). Our method
simply checks whether a tree node is a syntactic
head. We do not have to consider what we are
moving and how to move it. On the other hand, Xu
et al had to introduce dozens of weighted rules,
probably because they used the semantic head-
based dependency representation without restric-
tion on the number of dependents.
The major difference between our method and
the above conventional methods, other than its
simplicity, is that our method moves not only verbs
and adjectives but also functional words such as
prepositions.
2 Head Finalization
Figure 1 shows Enju?s XML output for the simple
sentence: ?John hit a ball.? The tag <cons>
indicates a nonterminal node and <tok> indicates
a terminal node or a word (token). Each node has
a unique id. Head information is given by the
node?s head attribute. For instance, node c0?s head
is node c3, and c3 is a VP, or verb phrase. Thus,
Enju treats not only words but also non-terminal
nodes as heads.
Enju outputs at most two child nodes for each
node. One child is a head and the other is a depen-
dent. c3?s head is c4, which is VX, or a fragment of
a verb phrase. c4?s head is t1 or hit, which is VBD
or a past-tense verb. The upper picture of Figure 2
shows the parse tree graphically. Here, ? indicates
an edge that is linked from a ?head.?
Our Head Finalization rule simply swaps two
children when the head child appears before the
dependent child. In the upper picture of Fig. 2, c3
has two children c4 and c5. Here, c3?s head c4
appears before c5, so c4 and c5 are swapped.
The lower picture shows the swapped result.
Then we get John a ball hit, which has the
same word order as its Japanese translation jon wa
bohru wo utta except for the functional words a,
wa, and wo.
We have to add Japanese particles wa (topic
marker) or ga (nominative case marker) for John
and wo (objective case marker) for ball to get an
acceptable Japanese sentence.
It is well known that SMT is not good at gen-
erating appropriate particles from English, whitch
does not have particles. Particle generation was
tackled by a few research groups (Toutanova and
Suzuki, 2007; Hong et al, 2009).
Here, we use Enju?s output to generate seeds
?sentence id=?s0? parse status=?success??
?cons id=?c0? cat=?S? xcat=?? head=?c3??
?cons id=?c1? cat=?NP? xcat=?? head=?c2??
?cons id=?c2? cat=?NX? xcat=?? head=?t0??
?tok id=?t0? cat=?N? pos=?NNP?
base=?john??John?/tok?
?/cons?
?/cons?
?cons id=?c3? cat=?VP? xcat=?? head=?c4??
?cons id=?c4? cat=?VX? xcat=?? head=?t1??
?tok id=?t1? cat=?V? pos=?VBD? base=?hit?
arg1=?c1? arg2=?c5??hit?/tok?
?/cons?
?cons id=?c5? cat=?NP? xcat=?? head=?c7??
?cons id=?c6? cat=?DP? xcat=?? head=?t2?
?tok id=?t2? cat=?D? pos=?DT? base=?a?
arg1=?c7??a?/tok?
?/cons?
?cons id=?c7? cat=?NX? xcat=?? head=?t3??
?tok id=?t3? cat=?N? pos=?NN?
base=?ball??ball?/tok?
?/cons?
?/cons?
?/cons?
?/cons?
.?/sentence?
Figure 1: Enju?s XML output (some attributes are
removed for readability).
t0
John
t1
hit
t2
a
t3
ball
c7?c6?
c5?
c4?
c3?
c2?
c1?
c0 Original English?
t0
John
jon (wa)
t1
hit
utta
t2
a
?
t3
ball
bohru (wo)
c7?c6?
c5?
c4?
c3?
c2?
c1?
c0 Head Final English?
Figure 2: Head Finalization of a simple sentence
(? indicates a head).
245
2
John
5
went
7
to
9
the
10
police
12
because
15
Mary
17
lost
19
his
20
wallet
1? 14?8? 18?
6?
4? 16?
13?
11?
3?
0 Original English?
2
John
jon (wa)
5
Mary
meari (ga)
19
his
kare no
20
wallet
saifu (wo)
17
lost
nakushita
12
because
node
9
the
?
10
police
keisatsu
7
to
ni
5
went
itta
1? 14? 8?18?
6?
4?16?
13?
11?
3 ?
0 Head Final English?
Figure 3: Head-Finalizing a complex sentence.
for particles. As Fig. 1 shows, the verb hit has
arg1="c1" and arg2="c5". This indicates that c1
(John) is the subject of hit and c5 (a ball) is
the object of hit. We add seed words va1 after
arg1 and va2 after arg2. Then, we obtain John
va1 a ball va2 hit. We do not have to add
arg2 for be because be?s arg2 is not an object but
a complement. We introduced the idea of particle
seed words independently but found that it is very
similar to Hong et al (2009)?s method for Korean.
Figure 3 shows Enju?s parse tree for a
more complicated sentence ?John went to the
police because Mary lost his wallet.? For
brevity, we hide the terminal nodes, and we re-
moved the nonterminal nodes? prefix c.
Conventional Rule-Based Machine Translation
(RBMT) systems swap X and Y of ?X because Y?
and move verbs to the end of each clause. Then we
get ?Mary his wallet lost because John the police
to went.? Its word-to-word translation is a fluent
Japanese sentence: meari (ga) kare no saifu (wo)
nakushita node jon (wa) keisatsu ni itta.
On the other hand, our Head Finalization with
particle seed words yields a slightly different word
order ?John va1 Mary va1 his wallet va2 lost
because the police to went.? Its word-to-word
translation is jon wa meari ga kare no saifu wo
nakushita node keisatsu ni itta. This is also an ac-
ceptable Japanese sentence.
This difference comes from the syntactic role
of ?because.? In our method, Enju states that
because is a dependent of went, whereas RBMT
systems treat because as a clause conjunction.
When we use Xu et al?s preprocessing method,
?because? moves to the beginning of the sentence.
We do not know a good monotonic translation of
the result.
Preliminary experiments show that HFE looks
good as a first approximiation of Japanese word
order. However, we can make it better by intro-
ducing some heuristic rules. (We did not see the
test set to develop these heuristic rules.)
From a preliminary experiment, we found that
coordination expressions such as A and B and A
or B are reordered as B and A and B or A. Al-
though A and B have syntactically equal positions,
the order of these elements sometimes matters.
Therefore, we decided to stop swapping them at
coordination nodes, which are indicated cat and
xcat attributes of the Enju output. We call this
the coordination exception rule. In addition,
we avoid Enju?s splitting of numerical expressions
such as ?12,345? and ?(1)? because this splitting
leads to inappropriate word orders.
246
3 Experiments
In order to show how closely our Head Finaliza-
tion makes English follow Japanese word order,
we measured Kendall?s ? , a rank correlation co-
efficient. We also measured BLEU (Papineni et
al., 2002) and other automatic evaluation scores to
show that Head Finalization can actually improve
the translation quality.
We used NTCIR7 PAT-MT?s Patent corpus (Fu-
jii et al, 2008). Its training corpus has 1.8 mil-
lion sentence pairs. We used MeCab (http://
mecab.sourceforge.net/) to segment Japanese
sentences.
3.1 Rough evaluation of reordering
First, we examined rank correlation between Head
Final English sentences produced by the Head Fi-
nalization rule and Japanese reference sentences.
Since we do not have handcrafted word alignment
data for an English-to-Japanese bilingual corpus,
we used GIZA++ (Och and Ney, 2003) to get au-
tomatic word alignment.
Based on this automatic word alignment, we
measured Kendall?s ? for the word order between
HFE sentences and Japanese sentences. Kendall?s
? is a kind of rank correlation measure defined as
follows. Suppose a list of integers such as L = [2,
1, 3, 4]. The number of all integer pairs in this list
is 4C2 = 4 ? 3/(2 ? 1) = 6. The number of in-
creasing pairs is five: (2, 3), (2, 4), (1, 3), (1, 4),
and (3, 4). Kendall?s ? is defined by
? = #increasing pairs
#all pairs
? 2? 1.
In this case, we get ? = 5/6? 2? 1 = 0.667.
For each sentence in the training data,
we calculate ? based on a GIZA++ align-
ment file, en-ja.A3.final. (We also tried
ja-en.A3.final, but we got similar results.) It
looks something like this:
John hit a ball .
NULL ({3}) jon ({1}) wa ({}) bohru ({4})
wo ({}) utta ({2}) . ({5})
Numbers in ({ }) indicate corresponding En-
glish words. The article ?a? has no correspond-
ing word in Japanese, and such words are listed
in NULL ({ }). From this alignment information,
we get an integer list [1, 4, 2, 5]. Then, we get
? = 5/4C2 ? 2? 1 = 0.667.
For HFE in Figure 2, we will get the following
alignment.
John va1 a ball va2 hit .
NULL ({3}) jon ({1}) wa ({2}) bohru ({4})
wo ({5}) utta ({6}) . ({7})
Then, we get [1, 2, 4, 5, 6, 7] and ? = 1.0. We
use ? or the average of ? over all training sentences
to observe the tendency.
Sometimes, one Japanese word corresponds to
an English phrase:
John went to Costa Rica .
NULL ({}) jon ({1}) wa ({}) kosutarika ({4 5})
ni ({3}) itta ({2}) . ({6})
We get [1, 4, 5, 3, 2, 6] from this alignment.
When the same word (or derivative words) ap-
pears twice or more in a single English sentence,
two or more non-consecutive words in the English
sentence are aligned to a single Japanese word:
rate of change of speed
NULL ({}) sokudo ({5}) henka ({3})
no ({2 4}) wariai ({1})
We excluded the ambiguously aligned words (2
4) from the calculation of ? . We use only [5, 3,
1] and get ? = ?1.0. The exclusion of these
words will be criticized by statisticians, but even
this rough calculation of ? sheds light on the weak
points of Head Finalization.
Because of this exclusion, the best value ? =
1.0 does not mean that we obtained the perfect
word ordering, but low ? values imply failures. In
section 4, we use ? to analyze failures.
By examining low ? sentences, we found that
patent documents have a lot of expressions such
as ?motor 2.? These are reordered (2 motor) and
slightly degrade ? . We did not notice this problem
until we handled the patent corpus because these
expressions are rare in other documents such as
news articles. Here, we added a rule to keep these
expressions.
We did not use any dictionary in our experi-
ment, but if we add dictionary entries to the train-
ing data, it raises ? because most entries are short.
One-word entries do not affect ? because we can-
not calculate ? . Most multi-word entries are short
noun phrases that are not reordered (? = 1.0).
Therefore, we should exclude dictionary entries
from the calculation of ? .
3.2 Quality of translation
It must be noted that the rank correlation does not
directly measure the quality of translation. There-
fore, we also measured BLEU and other automatic
evaluation scores of the translated sentences. We
used Moses (Koehn, 2010) for Minimum Error
Rate Training and decoding.
247
0%
5%
10%
15%
20%
-1.0 -0.8 -0.6 -0.4 -0.2 0.0 0.2 0.4 0.6 0.8 1.0
? of English sentences
0%
5%
10%
15%
20%
-1.0 -0.8 -0.6 -0.4 -0.2 0.0 0.2 0.4 0.6 0.8 1.0
? of Head Finalized English sentences
Figure 4: Distribution of ?
We used the development set (915 sentences) in
the NTCIR7 PAT-MT PSD data as well as the for-
mal run test set (1,381 sentences).
In the NTCIR7 PAT-MT workshop held in 2008,
its participants used different methods such as hi-
erarchical phrase-based SMT, RBMT, and EBMT
(Example-Based Machine Translation). However,
the organizers? Moses-based baseline system ob-
tained the best BLEU score.
4 Results
First, we show ? values to evaluate word order,
and then we show BLEU and other automatic eval-
uation scores.
4.1 Rank correlation
The original English sentences have ? = 0.451.
Head Finalization improved it to 0.722. Figure
4 shows the distribution of ? for all training sen-
tences. HFE reduces the percentage of low ? sen-
tences: 49.6% of the 1.8 million HFE sentences
have ? ? 0.8 and 15.1% have ? = 1.0.
We also implemented Xu et al?s method with
the Stanford parser 1.6.2. Its ? was 0.624. The
rate of the sentences with ? ? 0.8 was 30.6% and
the rate of ? = 1.0 was 4.3%.
We examined low ? sentences of our method
and found the following reasons for low ? values.
? The sentence pair is not an exact one-to-one
translation. A Japanese reference sentence
for ?I bought the cake.? can be some-
thing like ?The cake I bought.? or ?The
person who bought the cake is me.?
? Mistakes in Enju?s tagging or parsing. We
encountered certain POS tag mistakes:
? VBZ/NNS mistake: ?advances? of ?. . .
device advances along . . .? is VBZ,
main cause count
tagging/parsing mistakes 12
VBN/VBD mistake (4)
VBZ/NNS mistake (2)
comma or and (2)
inexact translation 7
wrong alignment 1
Table 1: Main causes of 20 worst sentences
but NNS is assigned.
? VBN/VBD mistake: ?encoded? of
?. . . the error correction encoded
data is supplied . . .? is VBN, but
VBD is assigned.
These tagging mistakes lead to global parsing
mistakes. In addition, just like other parsers,
Enju tends to make mistakes when a sentence
has a comma or ?and.?
? Mistakes/Ambiguity of GIZA++ automatic
word alignment. Ambiguity happens when
a single sentence has two or more occur-
rences of a word or derivatives of a word
(e.g., difference/different/differential). As we
described above, ambiguously aligned words
are removed from calculation of ? , and small
reordering mistakes in other words are em-
phasized.
We analyzed the 20 worst sentences with ? <
?0.5 when we used only 400,000 sentences for
GIZA++. Their causes are summarized in Table
1. In general, low ? sentences have two or more
causes, but here we show only the most influen-
tial cause for each sentence. This table shows that
mistakes in tagging and parsing are major causes
of low ? values. When we used all of 1.8 million
248
Method BLEU WER TER
proposed (0) 30.79 0.663 0.554
proposed (3) 30.97 0.665 0.554
proposed (6) 31.21 0.660 0.549
proposed (9) 31.11 0.661 0.549
proposed (12) 30.98 0.662 0.551
proposed (15) 31.00 0.662 0.552
no va (6) 30.99 0.669 0.559
Organizer 30.58 0.755 0.592
Table 2: Automatic Evaluation of Translation
Quality (Numbers in parentheses indicate distor-
tion limits).
sentence pairs, only 11 sentences had ? < ?0.5
among the 1.8 million sentences.
4.2 Automatic Evaluation of Translation
Quality
In general, it is believed that translation between
English and Japanese requires a large distortion
limit (dl), which restricts how far a phrase can
move. SMT reasearchers working on E-J or J-
E translation often use dl=?1 (unlimited) as a
default value, and this takes a long translation
time.
For PATMT J-E translation, Katz-Brown and
Collins (2008) showed that dl=unlimited is the
best and it requires a very long translation time.
For PATMT E-J translation, Kumai et al (2008)
claimed that they achieved the best result ?when
the distortion limit was 20 instead of ?1.?
Table 2 compares the single-reference BLEU
score of the proposed method and that of the
Moses-based system by the NTCIR-7 PATMT
organizers. This organizers? system was better
than all participants (Fujii et al, 2008) in terms
of BLEU. Here, we used Bleu Kit (http://
www.mibel.cs.tsukuba.ac.jp/norimatsu/
bleu kit/) following the PATMT?s overview
paper (Fujii et al, 2008). The table shows that
dl=6 gives the best result, and even dl=0 (no
reordering in Moses) gives better scores than the
organizers? Moses.
Table 2 also shows Word Error Rates (WER)
and Translation Error Rates (TER) (Snover et al,
2006). Since they are error rates, smaller is better.
Although the improvement of BLEU is not very
impressive, the score of WER is greatly reduced.
This difference comes from the fact that BLEU
measures only local word order, while WER mea-
Method ROUGE-L IMPACT PER
proposed (6) 0.480 0.369 0.390
no va (6) 0.475 0.368 0.398
Organizer 0.403 0.339 0.384
Table 3: Improvement in word order
sures global word order. Another line ?no va?
stands for our method without vas or particle
seeds. Without particle seeds, all scores slightly
drop.
Echizen-ya et al (2009) showed that IMPACT
and ROUGE-L are highly correlated to human
evaluation in evaluating J-E patent translation.
Therefore, we also used these evaluation methods
here for E-J translation. Table 3 shows that the
proposed method is also much better than the or-
ganizers? Moses in terms of these measures. With-
out particle seeds, these scores also drop slightly.
On the other hand, Position-independent Word
Error Rate (PER), which completely disregards
word order, does not change very much. These
facts indicate that our method improves word or-
der, which is the most important problem in E-J
translation.
The organizers? Moses uses dl=unlimited, and
it has been reported that its MERT training took
two weeks. On the other hand, our MERT training
with dl=6 took only eight hours on a PC: Xeon
X5570 2.93 GHz. Our method takes extra time to
parse sentences by Enju, but it is easy to run the
parser in parallel.
5 Discussion
Our method used an HPSG parser, which gives
rich information, but it is not easy to build such a
parser. It is much easier to build word dependency
parsers and Penn Treebank-style parsers. In order
use these parsers, we have to add some heuristic
rules.
5.1 Word Dependency Parsers
At first, we thought that we could substitute a word
dependency parser for Enju by simply rephrasing
a head with a modified word. Xu et al (2009)
used a semantic head-based dependency parser for
a similar purpose. Even when we use a syntac-
tic head-based dependency parser instead, we en-
countered their ?excessive movement? problem.
A straightforward application of their rules
changes
249
3
John
5
hit
7
the
8
ball
10
but
13
Sam
15
threw
17
the
18
ball
16?
14?
12?
11?
9?
6?
4?
2?
1?
0?
xcat="COOD"
cat="COOD"
Figure 5: Head Finilization does not mix up
clauses
(0) John hit the ball but Sam threw the ball.
to
(1) John the ball but Sam the ball threw hit.
Here, the two clauses are mixed up. To prevent
this, they disallow any movement across punctua-
tion and conjunctions. Then they get a better re-
sult:
(2) John the ball hit but Sam the ball threw.
When we used Enju, these clauses were not
mixed up. Enju-based Head Finalization gave the
same word order as (2):
(3) John va1 ball va2 hit but Sam va1 ball va2
throw.
Figure 5 shows Enju?s parse tree. When Head Fi-
nalization swaps the children of a mother node,
the children do not move beyond the range of
the mother node. Therefore, Head Finalization
based on Enju does not mix up the first clause
John hit the ball covered by Node 1 with the
second clause Sam threw the ball covered by
Node 11. Moreover, our coordination exception
rule keeps the order of these clauses. Thus, non-
terminal nodes in Enju?s output are useful to pro-
tect clauses.
When we use a word-dependency parser, we as-
sume that the modified words are heads. Further-
more, the Head Finalization rule is rephrased as
?move modified words after modifiers.? There-
fore, hit is moved after threw just like (2), and
the two clauses become mixed up. Consequently,
we need a heuristic rule like Xu?s.
5.2 Penn Treebank-style parsers
We also tried Charniak-Johnson?s parser (Char-
niak and Johnson, 2005). PyInputTree
(http://www.cs.brown.edu/?dmcc/software/
PyInputTree/) gives heads. Enju outputs at
most two children for a mother node, but Penn
Treebank-style parsers do not have such a limita-
tion on the number of children. This fact causes a
problem.
When we use Enju, ?This toy is popular in
Japan? is reordered as ?This toy va1 Japan in
popular is.? Its monotonic translation is fluent:
kono omocha wa nihon de ninki ga aru.
On the other hand, Charniak-Johnson?s parser
outputs the following S-expression for this sen-
tence (we added asterisks (*) to indicate heads).
(S (NP (DT This) (NN* toy))
(VP* (AUX* is)
(ADJP (JJ* popular))
(PP (IN* in) (NP (NNP* Japan)))))
Simply moving heads to the end introduces
?Japan in? between ?is? and ?popular?: this toy
va1 popular Japan in is. It is difficult to translate
this monotonically because of this interruption.
Reversing the children order (Xu et al, 2009)
reconnects is and popular. We get ?This toy
(va1) Japan in popular is? from the follow-
ing reversed S-expression.
(S (NP (DT This) (NN* toy))
(VP* (PP (IN* in) (NP (NNP* Japan)))
(ADJP (JJ* popular))
(AUX* is)))
5.3 Limitation of Head Finalization
Head Finalization gives a good first approximation
of Japanese word order in spite of its simplicity.
However, it is not perfect. In fact, a small distor-
tion limit improved the performance.
Sometimes, the Japanese language does not
have an appropriate word for monotonic transla-
tion. For instance, ?I have no time? becomes
?I va1 no time va2 have.? Its monotonic trans-
lation is ?watashi wa nai jikan wo motteiru,?
but this sentence is not acceptable. An acceptable
literal translation is ?watashi wa jikan ga nai.?
Here, ?no? corresponds to ?nai? at the end of the
sentence.
6 Conclusion
To solve the word-order problem between SVO
languages and SOV langugages, we introduced
a new reordering rule called Head Finalization.
This rule is simple, and we do not have to consider
POS tags or rule weights. We also showed that this
reordering improved automatic evaluation scores
of English-to-Japanese translation. Improvement
of the BLEU score is not very impressive, but
other evaluation scores (WER, TER, LOUGE-L,
and IMPACT) are greatly improved.
250
However, Head Finalization requires a sophis-
ticated HPSG tagger such as Enju. We showed
that severe failures are caused by Enju?s POS tag-
ging mistakes. We discussed the problems of other
parsers and how to solve them.
Our future work is to build our own parser that
makes fewer errors and to apply Head Finalization
to other SOV languages such as Korean.
Acknowledgements
We would like to thank Dr. Yusuke Miyao for
his useful advice on the usage of Enju. We also
thank anonymous reviewers for their valuable sug-
gestions.
References
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proc. of the Annual Meeting of the As-
sociation of Computational Linguistics (ACL), pages
173?180.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proc. of the Annual Meeting of the
Association of Computational Linguistics (ACL).
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proc. of the Language Resources and Evaluation
Conference (LREC), pages 449?454.
Hiroshi Echizen-ya, Terumasa Ehara, Sayori Shimo-
hata, Atsushi Fujii, Masao Utiyama, Mikio Ya-
mamoto, Takehito Utsuro, and Noriko Kando. 2009.
Meta-evaluation of automatic evaluation methods
for machine translation using patent translation data
in NTCIR-7. In Proceedings of the 3rd Workshop on
Patent Translation, pages 9?16.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and
Takehito Utsuro. 2008. Overview of the patent
translation task at the NTCIR-7 workshop. In Work-
ing Notes of the NTCIR Workshop Meeting (NTCIR),
pages 389?400.
Gumwon Hong, Seung-Wook Lee, and Hae-Chang
Rim. 2009. Bridging morpho-syntactic gap be-
tween source and target sentences for English-
Korean statistical machine translation. In Proc. of
ACL-IJCNLP, pages 233?236.
Jason Katz-Brown and Michael Collins. 2008. Syn-
tactic reordering in preprocessing for Japanese ?
English translation: MIT system description for
NTCIR-7 patent translation task. In Working Notes
of the NTCIR Workshop Meeting (NTCIR).
Philipp Koehn, 2010. MOSES, Statistical Machine
Translation System, User Manual and Code Guide.
Hiroyuki Kumai, Hirohiko Segawa, and Yasutsugu
Morimoto. 2008. NTCIR-7 patent translation ex-
periments at Hitachi. In Working Notes of the NT-
CIR Workshop Meeting (NTCIR), pages 441?444.
Chi-Ho Li, Dongdong Zhang, Mu Li, Ming Zhou,
Minghui Li, and Yi Guan. 2007. A probabilistic
approach to syntax-based reordering for statistical
machine translation. In Proc. of the Annual Meet-
ing of the Association of Computational Linguistics
(ACL), pages 720?727.
Thai Phuong Nguyen and Akira Shimazu. 2006.
Improving phrase-based statistical machine transla-
tion with morphosyntactic transformation. Machine
Translation, 20(3):147?166.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proc. of the
Annual Meeting of the Association of Computational
Linguistics (ACL), pages 311?318.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal SMT. In Proc. of the Annual Meet-
ing of the Association of Computational Linguistics
(ACL), pages 271?279.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas.
Kristina Toutanova and Hisami Suzuki. 2007. Gener-
ating case markers in machine translation. In Proc.
of NAACL-HLT, pages 49?56.
Fei Xia and Michael McCord. 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. In Proc. of the International Con-
ference on Computational Linguistics (COLING),
pages 508?514.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
SMT for Subject-Object-Verb languages. In Proc.
of NAACL-HLT, pages 245?253.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proc. of the
Annual Meeting of the Association of Computational
Linguistics (ACL), pages 523?530.
251
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 375?383,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
N-best Reranking by Multitask Learning
Kevin Duh Katsuhito Sudoh Hajime Tsukada Hideki Isozaki Masaaki Nagata
NTT Communication Science Laboratories
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237, Japan
{kevinduh,sudoh,tsukada,isozaki}@cslab.kecl.ntt.co.jp
nagata.masaaki@lab.ntt.co.jp
Abstract
We propose a new framework for N-best
reranking on sparse feature sets. The idea
is to reformulate the reranking problem as
a Multitask Learning problem, where each
N-best list corresponds to a distinct task.
This is motivated by the observation that
N-best lists often show significant differ-
ences in feature distributions. Training a
single reranker directly on this heteroge-
nous data can be difficult.
Our proposed meta-algorithm solves this
challenge by using multitask learning
(such as ?1/?2 regularization) to discover
common feature representations across N-
best lists. This meta-algorithm is simple to
implement, and its modular approach al-
lows one to plug-in different learning algo-
rithms from existing literature. As a proof
of concept, we show statistically signifi-
cant improvements on a machine transla-
tion system involving millions of features.
1 Introduction
Many natural language processing applications,
such as machine translation (MT), parsing, and
language modeling, benefit from the N-best
reranking framework (Shen et al, 2004; Collins
and Koo, 2005; Roark et al, 2007). The advan-
tage of N-best reranking is that it abstracts away
the complexities of first-pass decoding, allowing
the researcher to try new features and learning al-
gorithms with fast experimental turnover.
In the N-best reranking scenario, the training
data consists of sets of hypotheses (i.e. N-best
lists) generated by a first-pass system, along with
their labels. Given a new N-best list, the goal is
to rerank it such that the best hypothesis appears
near the top of the list. Existing research have fo-
cused on training a single reranker directly on the
entire data. This approach is reasonable if the data
is homogenous, but it fails when features vary sig-
nificantly across different N-best lists. In partic-
ular, when one employs sparse feature sets, one
seldom finds features that are simultaneously ac-
tive on multiple N-best lists.
In this case, we believe it is more advantageous
to view the N-best reranking problem as a multi-
task learning problem, where each N-best list cor-
responds to a distinct task. Multitask learning, a
subfield of machine learning, focuses on how to
effectively train on a set of different but related
datasets (tasks). Our heterogenous N-best list data
fits nicely with this assumption.
The contribution of this work is three-fold:
1. We introduce the idea of viewing N-best
reranking as a multitask learning problem.
This view is particularly apt to any general
reranking problem with sparse feature sets.
2. We propose a simple meta-algorithm that
first discovers common feature representa-
tions across N-bests (via multitask learning)
before training a conventional reranker. Thus
it is easily applicable to existing systems.
3. We demonstrate that our proposed method
outperforms the conventional reranking ap-
proach on a English-Japanese biomedical
machine translation task involving millions
of features.
The paper is organized as follows: Section 2 de-
scribes the feature sparsity problem and Section 3
presents our multitask solution. The effectiveness
of our proposed approach is validated by experi-
ments demonstrated in Section 4. Finally, Sections
5 and 6 discuss related work and conclusions.
2 The Problem of Sparse Feature Sets
For concreteness, we will describe N-best rerank-
ing in terms of machine translation (MT), though
375
our approach is agnostic to the application. In MT
reranking, the goal is to translate a foreign lan-
guage sentence f into an English sentence e by
picking from a set of likely translations. A stan-
dard approach is to use a linear model:
e? = argmax
e?N(f)
wT ? h(e, f) (1)
where h(e, f) is a D-dimensional feature vector,
w is the weight vector to be trained, and N(f) is
the set of likely translations of f , i.e. the N-best
list. The feature h(e, f) can be any quantity de-
fined in terms of the sentence pair, such as transla-
tion model and language model probabilities.
Here we are interested in situations where the
feature definitions can be quite sparse. A com-
mon methodology in reranking is to first design
feature templates based on linguistic intuition and
domain knowledge. Then, numerous features are
instantiated based on the training data seen. For
example, the work of (Watanabe et al, 2007) de-
fines feature templates based on bilingual word
alignments, which lead to extraction of heavily-
lexicalized features of the form:
h(e, f) =
?
?
?
?
?
?
?
1 if foreign word ?Monsieur?
and English word ?Mr.?
co-occur in e,f
0 otherwise
(2)
One can imagine that such features are sparse
because it may only fire for input sentences that
contain the word ?Monsieur?. For all other input
sentences, it is an useless, inactive feature.
Another common feature involves word ngram
templates, for example:
h(e, f) =
?
?
?
1 if English trigram
?Mr. Smith said? occurs in e
0 otherwise
(3)
In this case, all possible trigrams seen in the N-
best list are extracted as features. One can see
that this kind of feature can be very sensitive to
the first-pass decoder: if the decoder has loose re-
ordering constraints, then we may extract expo-
nentially many nonsense ngram features such as
?Smith said Mr.? and ?said Smith Mr.?. Granted,
the reranker training algorithm may learn that
these nonsense ngrams are indicative of poor hy-
potheses, but it is unlikely that the exact same non-
sense ngrams will appear given a different test sen-
tence.
In summary, the following issues compound to
create extremely sparse feature sets:
1. Feature templates are heavily-lexicalized,
which causes the number of features to grow
unbounded as the the amount of data in-
creases.
2. The input (f ) has high variability (e.g. large
vocabulary size), so that features for different
inputs are rarely shared.
3. The N-best list output also exhibits high vari-
ability (e.g. many different word reorder-
ings). Larger N may improve reranking per-
formance, but may also increase feature spar-
sity.
When the number of features is too large, even
popular reranking algorithms such as SVM (Shen
et al, 2004) and MIRA (Watanabe et al, 2007;
Chiang et al, 2009) may fail. Our goal here is to
address this situation.
3 Proposed Reranking Framework
In the following, we first give an intuitive com-
parison between single vs. multiple task learning
(Section 3.1), before presenting the general meta-
algorithm (Section 3.2) and particular instantia-
tions (Section 3.3).
3.1 Single vs. Multiple Tasks
Given a set of I input sentences {f i}, the training
data for reranking consists of a set of I N-best lists
{(Hi,yi)}i=1,...,I , where Hi are features and yi
are labels.
To clarify the notation:1 for an input sentence
f i, there is a N-best list N(f i). For a N-best list
N(f i), there are N feature vectors corresponding
to the N hypotheses, each with dimension D. The
collection of feature vectors for N(f i) is repre-
sented by Hi, which can be seen as a D ? N
matrix. Finally, the N -dimensional vector of la-
bels yi indicates the translation quality of each hy-
pothesis in N(f i). The purpose of the reranker
training algorithm is to find good parameters from
{(Hi,yi)}.
1Generally we use bold font h to represent a vector, bold-
capital font H to represent a matrix. Script h and h(?) may
be scalar, function, or sentence (depends on context).
376
The conventional method of training a single
reranker (single task formulation) involves opti-
mizing a generic objective such as:
argmin
w
I
?
i=1
L(w,Hi,yi) + ??(w) (4)
where w ? RD is the reranker trained on all lists,
and L(?) is some loss function. ?(w) is an op-
tional regularizer, whose effect is traded-off by the
constant ?. For example, the SVM reranker for
MT (Shen et al, 2004) defines L(?) to be some
function of sentence-level BLEU score, and ?(w)
to be the large margin regularizer.2
On the other hand, multitask learning involves
solving for multiple weights, w1,w2, . . . ,wI ,
one for each N-best list. One class of multitask
learning algorithms, Joint Regularization, solves
the following objective:
arg min
w1,..,wI
I
?
i=1
L(wi,Hi,yi) + ??(w1, ..,wI )
(5)
The loss decomposes by task but the joint regu-
larizer ?(w1, ..,wI) couples together the different
weight parameters. The key is to note that multi-
ple weights allow the algorithm to fit the heteroge-
nous data better, compared to a single weight vec-
tor. Yet these weights are still tied together so that
some information can be shared across N-best lists
(tasks).
One instantiation of Eq. 5 is ?1/?2 regular-
ization: ?(w1, ..,wI) , ||W||1,2, where W =
[w1|w2| . . . |wI ]T is a I-by-D matrix of stacked
weight vectors. The norm is computed by first tak-
ing the 2-norm on columns of W, then taking a
1-norm on the resulting D-length vector. This en-
courages the optimizer to choose a small subset of
features that are useful across all tasks.
For example, suppose two different sets of
weight vectors Wa and Wb for a 2 lists, 4 fea-
tures reranking problem. The ?1/?2 norm for Wa
is 14; the ?1/?2 norm for Wb is 12. If both have
the same loss L(?) in Eq. 5, the multitask opti-
mizer would prefer Wb since more features are
shared:
Wa :
?
4 0 0 3
0 4 3 0
?
Wb :
?
4 3 0 0
0 4 3 0
?
4 4 3 3 ? 14 4 5 3 0 ? 12
2In MT, evaluation metrics like BLEU do not exactly de-
compose across sentences, so for some training algorithms
this loss is an approximation.
3.2 Proposed Meta-algorithm
We are now ready to present our general reranking
meta-algorithm (see Algorithm 1), termed Rerank-
ing by Multitask Learning (RML).
Algorithm 1 Reranking by Multitask Learning
Input: N-best data {(Hi,yi)}i=1,...,I
Output: Common feature representation hc(e, f)
and weight vector wc
1: [optional] RandomHashing({Hi})
2: W = MultitaskLearn({(Hi ,yi)})
3: hc = ExtractCommonFeature(W)
4: {Hic} = RemapFeature({Hi}, hc)
5: wc = ConventionalReranker({(Hic ,yi)})
The first step, random hashing, is optional. Ran-
dom hashing is an effective trick for reducing the
dimension of sparse feature sets without suffer-
ing losses in fidelity (Weinberger et al, 2009;
Ganchev and Dredze, 2008). It works by collaps-
ing random subsets of features. This step can be
performed to speed-up multitask learning later. In
some cases, the original feature dimension may be
so large that hashed representations may be neces-
sary.
The next two steps are key. A multitask learn-
ing algorithm is run on the N-best lists, and a com-
mon feature space shared by all lists is extracted.
For example, if one uses the multitask objective
of Eq. 5, the result of step 2 is a set of weights
W. ExtractCommonFeature(W) then returns the
feature id?s (either from original or hashed repre-
sentation) that receive nonzero weight in any of
W.3 The new features hc(e, f) are expected to
have lower dimension than the original features
h(e, f). Section 3.3 describes in detail different
multitask methods that can be plugged-in to this
step.
The final two steps involve a conventional
reranker. In step 4, we remap the N-best list
data according to the new feature representations
hc(e, f). In step 5, we train a conventional
reranker on this common representation, which by
now should have overcome sparsity issues. Us-
ing a conventional reranker at the end allows us
to exploit existing rerankers designed for specific
NLP applications. In a sense, our meta-algorithm
simply involves a change of representation for
the conventional reranking scenario, where the
3For example in Wb, features 1-3 have nonzero weights
and are extracted. Feature 4 is discarded.
377
new representation is found by multitask methods
which are well-suited to heterogenous data.
3.3 Multitask Objective Functions
Here, we describe various multitask methods that
can be plugged in Step 2 of Algorithm 1. Our
goal is to demonstrate that a wide range of existing
methods from the multitask learning literature can
be brought to our problem. We categorize multi-
task methods into two major approaches:
1. Joint Regularization: Eq. 5 is an exam-
ple of joint regularization, with ?1/?2 norm being
a particular regularizer. The idea is to use the reg-
ularizer to ensure that the learned functions of re-
lated tasks are close to each other. The popular
?1/?2 objective can be optimized by various meth-
ods, such as boosting (Obozinski et al, 2009) and
convex programming (Argyriou et al, 2008). Yet
another regularizer is the ?1/?? norm (Quattoni et
al., 2009), which replaces the 2-norm with a max.
One could also define a regularizer to ensure
that each task-specific wi is close to some average
parameter, e.g.
?
i ||wi ? wavg||2. If we inter-
pret wavg as a prior, we begin to see links to Hier-
archical Bayesian methods for multitask learning
(Finkel and Manning, 2009; Daume, 2009).
2. Shared Subspace: This approach assumes
that there is an underlying feature subspace that
is common to all tasks. Early works on multi-
task learning implement this by neural networks,
where different tasks have different output layers
but share the same hidden layer (Caruana, 1997).
Another method is to write the weight vector
as two parts w = [u;v] and let the task-specific
function be uT ? h(e, f) + vT ? ? ? h(e, f) (Ando
and Zhang, 2005). ? is a D??D matrix that maps
the original features to a subspace common to all
tasks. The new feature representation is computed
by the projection hc(e, f) , ? ? h(e, f).
Multitask learning is a vast field and relates to
areas like collaborative filtering (Yu and Tresp,
2005) and domain adaptation. Most methods as-
sume some common representation and is thus ap-
plicable to our framework. The reader is urged to
refer to citations in, e.g. (Argyriou et al, 2008) for
a survey.
4 Experiments and Results
As a proof of concept, we perform experiments
on a MT system with millions of features. We
use a hierarchical phrase-based system (Chiang,
100 101 102 103 104
10?7
10?6
10?5
10?4
10?3
10?2
10?1
100
P(
fea
tur
e o
cc
urs
 in
 x 
lis
ts)
x
Figure 1: This log-log plot shows that there are
many rare features and few common features. The
probability that a feature occurs in x number of N-
best lists behaves according to the power-law x??,
where ? = 2.28.
2007) to generate N-best lists (N=100). Sparse
features used in reranking are extracted according
to (Watanabe et al, 2007). Specifically, the major-
ity are lexical features involving joint occurrences
of words within the N-best lists and source sen-
tences.
It is worth noting that the fact that the first pass
system is a hierarchical system is not essential to
the feature extraction step; similar features can be
extracted with other systems as first-pass, e.g. a
phrase-based system. That said, the extent of the
feature sparsity problem may depend on the per-
formance of the first-pass system.
We experiment with medical domain MT, where
large numbers of technical vocabulary cause spar-
sity challenges. Our corpora consists of English
abstracts from PubMed4 with their Japanese trans-
lations. The first-pass system is built on hierarchi-
cal phrases extracted from 17k sentence pairs and
target (Japanese) language models trained on 800k
medical-domain sentences. For our reranking ex-
periments, we used 500 lists as the training set5,
500 lists as held-out, and another 500 for test.
4.1 Data Characteristics
We present some statistics to illustrate the feature
sparsity problem: From 500 N-best lists, we ex-
tracted a total of 2.4 million distinct features. By
type, 75% of these features occur in only one N-
best list in the dataset. Less than 3% of features
4A database of the U.S. National Library of Medicine.
5In MT, training data for reranking is sometimes referred
to as ?dev set? to distinguish from the data used in first-pass.
Also, while the 17k bitext may seem small compared to other
MT work, we note that 1st pass translation quality (around 28
BLEU) is high enough to evaluate reranking methods.
378
occur in ten or more lists. The distribution of fea-
ture occurrence is clearly Zipfian, as seen in the
power-law plot in Figure 1.
We can also observe the feature growth rate (Ta-
ble 1). This is the number of new features intro-
duced when an additional N-best list is seen. It is
important to note that on average, 2599 new fea-
tures are added everytime a new N-best list is seen.
This is as much as 2599/4188 = 62% of the ac-
tive features. Imagine an online training algorithm
(e.g. MIRA or perceptron) on this kind of data:
whenever a loss occurs and we update the weight
vector, less than half of the weight vector update
applies to data we have seen thus far. Herein lies
the potential for overfitting.
From observing the feature grow rate, one may
hypothesize that adding large numbers of N-best
lists to the training set (500 in the experiments
here) may not necessarily improve results. While
adding data potentially improves the estimation
process, it also increases the feature space dramat-
ically. Thus we see the need for a feature extrac-
tion procedure.
(Watanabe et al, 2007) also reports the possibil-
ity of overfitting in their dataset (Arabic-English
newswire translation), especially when domain
differences are present. Here we observe this ten-
dency already on the same domain, which is likely
due to the highly-specialized vocabulary and the
complex sentence structures common in research
paper abstracts.
4.2 MT Results
Our goal is to compare different feature represen-
tations in reranking: The baseline reranker uses
the original sparse feature representation. This is
compared to feature representations discovered by
three different multitask learning methods:
? Joint Regularization (Obozinski et al, 2009)
? Shared Subspace (Ando and Zhang, 2005)
? Unsupervised Multitask Feature Selection
(Abernethy et al, 2007).6
We use existing implementations of the above
methods.7 The conventional reranker (Step 5, Al-
6This is not a standard multitask algorithm since most
multitask algorithms are supervised. We include it to see
if unsupervised or semi-supervised multitask algorithms is
promising. Intuitively, the method tries to select subsets of
features that are correlated across multiple tasks using ran-
dom sampling (MCMC). Features that co-occur in different
tasks form a high probability path.
7Available at http://multitask.cs.berkeley.edu
Nbest id #NewFt #SoFar #Active
1 3900 3900 3900
2 7535 11435 7913
3 6078 17513 7087
4 3868 21381 4747
5 1896 23277 2645
6 3542 26819 4747
....
100 2440 289118 4299
101 1639 290757 2390
102 3468 294225 4755
103 2350 296575 3824
Average 2599 ? 4188
Table 1: Feature growth rate: For N-best list i in
the table, we have (#NewFt = number of new fea-
tures introduced since N-best i ? 1) ; (#SoFar =
Total number of features defined so far); and (#Ac-
tive = number of active features for N-best i). E.g.,
we extracted 7535 new features from N-best 2;
combined with the 3900 from N-best 1, the total
features so far is 11435.
gorithm 1) used in all cases is SVMrank.8 Our
initial experiments show that the SVM baseline
performance is comparable to MIRA training, so
we use SVM throughout. The labels for the SVM
are derived as in (Shen et al, 2004), where top
10% of hypotheses by smoothed sentence-BLEU
is ranked before the bottom 90%. All multitask
learning methods work on hashed features of di-
mension 4000 (Step 1, Algorithm 1). This speeds
up the training process.
All hyperparameters of the multitask method
are tuned on the held-out set. In particular, the
most important is the number of common features
to extract, which we pick from {250, 500, 1000}.
Table 2 shows the results by BLEU (Papineni
et al, 2002) and PER. The Oracle results are ob-
tained by choosing the best hypothesis per N-best
list by sentence-level BLEU, which achieved 36.9
BLEU in both Train and Test. A summary of our
observations is:
1. The baseline (All sparse features) overfits. It
achieves the oracle BLEU score on the train
set (36.9) but performs poorly on the test
(28.6).
2. Similar overfitting occurs when traditional ?1
regularization is used to select features on
8Available at http://svmlight.joachims.org
379
the sparse feature representation9 . ?1 reg-
ularization is a good method of handling
sparse features for classification problems,
but in reranking the lack of tying between
lists makes this regularizer inappropriate. A
small set of around 1200 features are chosen:
they perform well independently on each task
in the training data, but there is little sharing
with the test data.
3. All three multitask methods obtained features
that outperformed the baseline. The BLEU
scores are 28.8, 28.9, 29.1 for Unsupervised
Feature Selection, Joint Regularization, and
Shared Subspace, respectively, which all out-
perform the 28.6 baseline. All improvements
are statistically significant by bootstrap sam-
pling test (1000 samples, p < 0.05) (Zhang
et al, 2004).
4. Shared Subspace performed the best. We
conjecture this is because its feature projec-
tion can create new feature combinations that
is more expressive than the feature selection
used by the two other methods.
5. PER results are qualitatively similar to BLEU
results.
6. As a further analysis, we are interested in see-
ing whether multitask learning extracts novel
features, especially those that have low fre-
quency. Thus, we tried an additional feature
representation (feature threshold) which only
keeps features that occur in more than x N-
bests, and concatenate these high-frequency
features to the multitask features. The fea-
ture threshold alone achieves nice BLEU re-
sults (29.0 for x > 10), but the combination
outperforms it by statistically significant mar-
gins (29.3-29.6). This implies that multitask
learning is extracting features that comple-
ment well with high frequency features.
For the multitask features, improvements of 0.2
to 1.0 BLEU are modest but consistent. Figure
2 shows the BLEU of bootstrap samples obtained
as part of the statistical significance test. We see
that multitask almost never underperform base-
line in any random sampling of the data. This im-
plies that the proposed meta-algorithm is very sta-
9Optimized by the Vowpal Wabbit toolkit:
http://hunch.net/vw/
ble, i.e. it is not a method that sometimes improves
and sometimes degrades.
Finally, a potential question to ask is: what
kinds of features are being selected by the
multitask learning algorithms? We found that
that two kinds of features are usually selected:
one is general features that are not lexicalized,
such as ?count of phrases?, ?count of dele-
tions/insertions?, ?number of punctuation marks?.
The other kind is lexicalized features, such as
those in Equations 2 and 3, but involving functions
words (like the Japanese characters ?wa?, ?ga?,
?ni?, ?de?) or special characters (such as numeral
symbol and punctuation). These are features that
can be expected to be widely applicable, and it is
promising that multitask learning is able to recover
these from the millions of potential features. 10
?0.2 0 0.2 0.4 0.6 0.8 1 1.2
0
50
100
150
200
250
300
BLEU(shared subspace)?BLEU(baseline sparse feature)
Bo
ot
st
ra
p 
sa
m
pl
es
Figure 2: BLEU difference of 1000 bootstrap sam-
ples. 95% confidence interval is [.15, .90] The
proposed approach therefore seems to be a stable
method.
5 Related Work in NLP
Previous reranking work in NLP can be classified
into two different research focuses:
1. Engineering better features: In MT, (Och
and others, 2004) investigates features extracted
from a wide variety of syntactic representations,
such as parse tree probability on the outputs. Al-
though their results show that the proposed syntac-
tic features gave little improvements, they point to
some potential reasons, such as domain mismatch
for the parser and overfitting by the reranking
10Note: In order to do this analysis, we needed to run Joint
Regularization on the original feature representation, since
the hashed representations are less interpretable. This turns
out to be computationally prohibitive in the time being so we
only ran on a smaller data set of 50 lists. Recently new op-
timization methods that are orders of magnitude faster have
been developed (Liu et al, 2009), which makes larger-scale
experiments possible.
380
Train Test Test
Feature Representation #Feature BLEU BLEU PER
(baselines)
First pass 20 29.5 28.5 38.3
All sparse features (Main baseline) 2.4M 36.9 28.6 38.2
All sparse features w/ ?1 regularization 1200 36.5 28.5 38.6
Random hash representation 4000 33.0 28.5 38.2
(multitask learning)
Unsupervised FeatureSelect 500 32.0 28.8 37.7
Joint Regularization 250 31.8 28.9 37.5
Shared Subspace 1000 32.9 29.1 37.3
(combination w/ high-frequency features)
(a) Feature threshold x > 100 3k 31.7 27.9 38.2
(b) Feature threshold x > 10 60k 35.8 29.0 37.9
Unsupervised FeatureSelect + (b) 60.5k 36.2 29.3 37.6
Joint Regularization + (b) 60.25k 36.1 29.4 37.5
Shared Subspace + (b) 61k 36.2 29.6 37.3
Oracle (best possible) ? 36.9 36.9 33.1
Table 2: Results for different feature sets, with corresponding feature size and train/test BLEU/PER. All
multitask features give statistically significant improvements over the baselines (boldfaced), e.g. Shared
Subspace: 29.1 BLEU vs Baseline: 28.6 BLEU. Combinations of multitask features with high frequency
features also give significant improvements over the high frequency features alone.
method. Recent work by (Chiang et al, 2009) de-
scribes new features for hierarchical phrase-based
MT, while (Collins and Koo, 2005) describes
features for parsing. Evaluation campaigns like
WMT (Callison-Burch et al, 2009) and IWSLT
(Paul, 2009) also contains a wealth of information
for feature engineering in various MT tasks.
2. Designing better training algorithms: N-
best reranking can be seen as a subproblem of
structured prediction, so many general structured
prediction algorithms (c.f. (Bakir et al, 2007))
can be applied. In fact, some structured predic-
tion algorithms, such as the MIRA algorithm used
in dependency parsing (McDonald et al, 2005)
and MT (Watanabe et al, 2007) uses iterative
sets of N-best lists in its training process. Other
training algorithms include perceptron-style algo-
rithms (Liang et al, 2006), MaxEnt (Charniak and
Johnson, 2005), and boosting variants (Kudo et al,
2005).
The division into two research focuses is conve-
nient, but may be suboptimal if the training algo-
rithm and features do not match well together. Our
work can be seen as re-connecting the two focuses,
where the training algorithm is explicitly used to
help discover better features.
Multitask learning is currently an active subfield
within machine learning. There has already been
some applications in NLP: For example, (Col-
lobert and Weston, 2008) uses a deep neural net-
work architecture for multitask learning on part-
of-speech tagging, chunking, semantic role label-
ing, etc. They showed that jointly learning these
related tasks lead to overall improvements. (De-
selaers et al, 2009) applies similar methods for
machine transliteration. In information extraction,
learning different relation types can be naturally
cast as a multitask problem (Jiang, 2009; Carlson
et al, 2009). Our work can be seen as following
the same philosophy, but applied to N-best lists.
In other areas, (Reichart et al, 2008) introduced
an active learning strategy for annotating multitask
linguistic data. (Blitzer et al, 2006) applies the
multitask algorithm of (Ando and Zhang, 2005)
to domain adaptation problems in NLP. We expect
that more novel applications of multitask learning
will appear in NLP as the techniques become scal-
able and standard.
6 Discussion and Conclusion
N-best reranking is a beneficial framework for ex-
perimenting with large feature sets, but unfortu-
nately feature sparsity leads to overfitting. We ad-
dressed this by re-casting N-best lists as multitask
381
learning data. Our MT experiments show consis-
tent statistically significant improvements.
From the Bayesian view, multitask formulation
of N-best lists is actually very natural: Each N-
best is generated by a different data-generating
distribution since the input sentences are different,
i.e. p(e|f1) 6= p(e|f2). Yet these N-bests are re-
lated since the general p(e|f) distribution depends
on the same first-pass models.
The multitask learning perspective opens up
interesting new possibilities for future work, e.g.:
? Different ways to partition data into tasks,
e.g. clustering lists by document structure, or
hierarchical clustering of data
? Multitask learning on lattices or N-best lists
with larger N. It is possible that a larger hy-
pothesis space may improve the estimation of
task-specific weights.
? Comparing multitask learning to sparse on-
line learning of batch data, e.g. (Tsuruoka et
al., 2009).
? Modifying the multitask objective to incorpo-
rate application-specific loss/decoding, such
as Minimum Bayes Risk (Kumar and Byrne,
2004)
? Using multitask learning to aid large-scale
feature engineering and visualization.
Acknowledgments
We have received numerous helpful comments
throughout the course of this work. In partic-
ular, we would like to thank Albert Au Yeung,
Jun Suzuki, Shinji Watanabe, and the three anony-
mous reviewers for their valuable suggestions.
References
Jacob Abernethy, Peter Bartlett, and Alexander
Rakhlin. 2007. Multitask learning with expert ad-
vice. In COLT.
Rie Ando and Tong Zhang. 2005. A framework for
learning predictive structures from multiple tasks
and unlabeled data. JMLR.
Andreas Argyriou, Theodoros Evgeniou, and Massim-
iliano Pontil. 2008. Convex multitask feature learn-
ing. Machine Learning, 73(3).
G. Bakir, T. Hofmann, B. Scholkopf, A. Smola,
B. Taskar, and S. V. N. Vishwanathan, editors. 2007.
Predicting structured data. MIT Press.
J. Blitzer, R. McDonald, and F. Pereira. 2006. Domain
adaptation with structural correspondence learning.
In EMNLP.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
workshop on statistical machine translation. In
WMT.
Andrew Carlson, Justin Betteridge, Estevam Hruschka,
and Tom Mitchell. 2009. Coupling semi-supervised
learning of categories and relations. In NAACL
Workshop on Semi-supervised learning for NLP
(SSLNLP).
Rich Caruana. 1997. Multitask learning. Machine
Learning, 28.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In ACL.
David Chiang, Wei Wang, and Kevin Knight. 2009.
11,001 new features for statistical machine transla-
tion. In NAACL.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2).
Michael Collins and Terry Koo. 2005. Discriminative
reranking for natural langauge parsing. Computa-
tional Linguistics, 31(1).
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: deep
neural networks with multitask learning. In ICML.
Hal Daume. 2009. Bayesian multitask learning with
latent hierarchies. In UAI.
Thomas Deselaers, Sasa Hasan, Oliver Bender, and
Hermann Ney. 2009. A deep learning approach to
machine transliteration. In WMT.
Jenny Rose Finkel and Chris Manning. 2009. Hier-
archical Bayesian domain adaptation. In NAACL-
HLT.
Kuzman Ganchev and Mark Dredze. 2008. Small sta-
tistical models by random feature mixing. In ACL-
2008 Workshop on Mobile Language Processing.
Jing Jiang. 2009. Multitask transfer learning for
weakly-supervised relation extraction. In ACL.
Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005.
Boosting-based parse reranking with subtree fea-
tures. In ACL.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In HLT-NAACL.
P. Liang, A. Bouchard-Cote, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In ACL.
382
J. Liu, S. Ji, and J. Ye. 2009. Multi-task feature learn-
ing via efficient l2,1-norm minimization. In UAI.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large margin training of de-
pendency parsers. In ACL.
Guillaume Obozinski, Ben Taskar, and Michael Jor-
dan. 2009. Joint covariate selection and joint sub-
space selection for multiple classification problems.
Statistics and Computing.
F.J. Och et al 2004. A smorgasbord of features for
statistical machine translation. In HLT/NAACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In ACL.
Michael Paul. 2009. Overview of the iwslt 2009 eval-
uation campaign. In IWSLT.
Ariadna Quattoni, Xavier Carreras, Michael Collins,
and Trevor Darrell. 2009. An efficient projection
for L1-Linfinity regularization. In ICML.
Roi Reichart, Katrin Tomanek, Udo Hahn, and Ari
Rappoport. 2008. Multi-task active learning for lin-
guistic annotations. In ACL.
Brian Roark, Murat Saraclar, and Michael Collins.
2007. Discriminative n-gram language modeling.
Computer Speech and Language, 21(2).
Libin Shen, Anoop Sarkar, and Franz Och. 2004. Dis-
criminative reranking for machine translation. In
HLT-NAACL.
Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ana-
niadou. 2009. Stochastic gradient descent training
for l1-regularized log-linear models with cumulative
penalty. In ACL-IJCNLP.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2007. Online large-margin train-
ing for statistical machine translation. In EMNLP-
CoNLL.
Kilian Weinberger, Anirban Dasgupta, John Langford,
Alex Smola, and Josh Attenberg. 2009. Feature
hashing for large scale multitask learning. In ICML.
Kai Yu and Volker Tresp. 2005. Learning to learn and
collaborative filtering. In NIPS-2005 Workshop on
Inductive Transfer.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004.
Interpreting BLEU/NIST scores: How much im-
provement do we need to have a better system? In
LREC.
383
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 418?427,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Divide and Translate: Improving Long Distance Reordering in Statistical
Machine Translation
Katsuhito Sudoh, Kevin Duh, Hajime Tsukada, Tsutomu Hirao, Masaaki Nagata
NTT Communication Science Laboratories
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0237, Japan
sudoh@cslab.kecl.ntt.co.jp
Abstract
This paper proposes a novel method
for long distance, clause-level reordering
in statistical machine translation (SMT).
The proposed method separately translates
clauses in the source sentence and recon-
structs the target sentence using the clause
translations with non-terminals. The non-
terminals are placeholders of embedded
clauses, by which we reduce complicated
clause-level reordering into simple word-
level reordering. Its translation model
is trained using a bilingual corpus with
clause-level alignment, which can be au-
tomatically annotated by our alignment
algorithm with a syntactic parser in the
source language. We achieved signifi-
cant improvements of 1.4% in BLEU and
1.3% in TER by using Moses, and 2.2%
in BLEU and 3.5% in TER by using
our hierarchical phrase-based SMT, for
the English-to-Japanese translation of re-
search paper abstracts in the medical do-
main.
1 Introduction
One of the common problems of statistical ma-
chine translation (SMT) is to overcome the differ-
ences in word order between the source and target
languages. This reordering problem is especially
serious for language pairs with very different word
orders, such as English-Japanese. Many previous
studies on SMT have addressed the problem by
incorporating probabilistic models into SMT re-
ordering. This approach faces the very large com-
putational cost of searching over many possibili-
ties, especially for long sentences. In practice the
search can be made tractable by limiting its re-
ordering distance, but this also renders long dis-
tance movements impossible. Some recent stud-
ies avoid the problem by reordering source words
prior to decoding. This approach faces difficul-
ties when the input phrases are long and require
significant word reordering, mainly because their
reordering model is not very accurate.
In this paper, we propose a novel method for
translating long sentences that is different from
the above approaches. Problematic long sentences
often include embedded clauses1 such as rela-
tive clauses. Such an embedded (subordinate)
clause can usually be translated almost indepen-
dently of words outside the clause. From this
viewpoint, we propose a divide-and-conquer ap-
proach: we aim to translate the clauses sepa-
rately and reconstruct the target sentence using the
clause translations. We first segment a source sen-
tence into clauses using a syntactic parser. The
clauses can include non-terminals as placeholders
for nested clauses. Then we translate the clauses
with a standard SMT method, in which the non-
terminals are reordered as words. Finally we re-
construct the target sentence by replacing the non-
terminals with their corresponding clause transla-
tions. With this method, clause-level reordering is
reduced to word-level reordering and can be dealt
with efficiently. The models for clause translation
are trained using a bilingual corpus with clause-
level alignment. We also present an automatic
clause alignment algorithm that can be applied to
sentence-aligned bilingual corpora.
In our experiment on the English-to-Japanese
translation of multi-clause sentences, the proposed
method improved the translation performance by
1.4% in BLEU and 1.3% in TER by using Moses,
and by 2.2% in BLEU and 3.5% in TER by using
our hierarchical phrase-based SMT.
The main contribution of this paper is two-fold:
1Although various definitions of a clause can be
considered, this paper follows the definition of ?S?
(sentence) in Enju. It basically follows the Penn Tree-
bank II scheme but also includes SINV, SQ, SBAR. See
http://www-tsujii.is.s.u-tokyo.ac.jp/enju/enju-manual/enju-
output-spec.html#correspondence for details.
418
1. We introduce the idea of explicit separa-
tion of in-clause and outside-clause reorder-
ing and reduction of outside-clause reorder-
ing into common word-level reordering.
2. We propose an automatic clause alignment
algorithm, by which our approach can be
used without manual clause-level alignment.
This paper is organized as follows. The next
section reviews related studies on reordering. Sec-
tion 3 describes the proposed method in detail.
Section 4 presents and discusses our experimen-
tal results. Finally, we conclude this paper with
our thoughts on future studies.
2 Related Work
Reordering in SMT can be roughly classified into
two approaches, namely a search in SMT decod-
ing and preprocessing.
The former approach is a straightforward way
that models reordering in noisy channel transla-
tion, and has been studied from the early period
of SMT research. Distance-based reordering is a
typical approach used in many previous studies re-
lated to word-based SMT (Brown et al, 1993) and
phrase-based SMT (Koehn et al, 2003). Along
with the advances in phrase-based SMT, lexical-
ized reordering with a block orientation model was
proposed (Tillmann, 2004; Koehn et al, 2005).
This kind of reordering is suitable and commonly
used in phrase-based SMT. On the other hand,
a syntax-based SMT naturally includes reorder-
ing in its translation model. A lot of research
work undertaken in this decade has used syntac-
tic parsing for linguistically-motivated translation.
(Yamada and Knight, 2001; Graehl and Knight,
2004; Galley et al, 2004; Liu et al, 2006). Wu
(1997) and Chiang (2007) focus on formal struc-
tures that can be extracted from parallel corpora,
instead of a syntactic parser trained using tree-
banks. These syntactic approaches can theoret-
ically model reordering over an arbitrary length,
however, long distance reordering still faces the
difficulty of searching over an extremely large
search space.
The preprocessing approach employs deter-
ministic reordering so that the following trans-
lation process requires only short distance re-
ordering (or even a monotone). Several previ-
ous studies have proposed syntax-driven reorder-
ing based on source-side parse trees. Xia and
McCord (2004) extracted reordering rules auto-
matically from bilingual corpora for English-to-
French translation; Collins et al (2005) used
linguistically-motivated clause restructuring rules
for German-to-English translation; Li et al (2007)
modeled reordering on parse tree nodes by us-
ing a maximum entropy model with surface and
syntactic features for Chinese-to-English trans-
lation; Katz-Brown and Collins (2008) applied
a very simple reverse ordering to Japanese-to-
English translation, which reversed the word order
in Japanese segments separated by a few simple
cues; Xu et al (2009) utilized a dependency parser
with several hand-labeled precedence rules for re-
ordering English to subject-object-verb order like
Korean and Japanese. Tromble and Eisner (2009)
proposed another reordering approach based on a
linear ordering problem over source words with-
out a linguistically syntactic structure. These pre-
processing methods reorder source words close
to the target-side order by employing language-
dependent rules or statistical reordering models
based on automatic word alignment. Although
the use of language-dependent rules is a natural
and promising way of bridging gaps between lan-
guages with large syntactic differences, the rules
are usually unsuitable for other language groups.
On the other hand, statistical methods can be ap-
plied to any language pairs. However, it is very
difficult to reorder all source words so that they are
monotonic with the target words. This is because
automatic word alignment is not usually reliable
owing to data sparseness and the weak modeling
of many-to-many word alignments. Since such
a reordering is not complete or may even harm
word ordering consistency in the source language,
these previous methods further applied reordering
in their decoding. Li et al (2007) used N-best
reordering hypotheses to overcome the reordering
ambiguity.
Our approach is different from those of previous
studies that aim to perform both short and long dis-
tance reordering at the same time. The proposed
method distinguishes the reordering of embedded
clauses from others and efficiently accomplishes it
by using a divide-and-conquer framework. The re-
maining (relatively short distance) reordering can
be realized in decoding and preprocessing by the
methods described above. The proposed frame-
work itself does not depend on a certain language
pair. It is based on the assumption that a source
419
language clause is translated to the corresponding
target language clause as a continuous segment.
The only language-dependent resource we need is
a syntactic parser of the source language. Note
that clause translation in the proposed method is a
standardMT problem and therefore any reordering
method can be employed for further improvement.
This work is inspired by syntax-based meth-
ods with respect to the use of non-terminals. Our
method can be seen as a variant of tree-to-string
translation that focuses only on the clause struc-
ture in parse trees and independently translates the
clauses. Although previous syntax-based methods
can theoretically model this kind of derivation, it
is practically difficult to decode long multi-clause
sentences as described above.
Our approach is also related to sentence sim-
plification and is intended to obtain simple and
short source sentences for better translation. Kim
and Ehara (1994) proposed a rule-based method
for splitting long Japanese sentences for Japanese-
to-English translation; Furuse et al (1998) used
a syntactic structure to split ill-formed inputs in
speech translation. Their splitting approach splits
a sentence sequentially to obtain short segments,
and does not undertake their reordering.
Another related field is clause identification
(Tjong et al, 2001). The proposed method is not
limited to a specific clause identification method
and any method can be employed, if their clause
definition matches the proposed method where
clauses are independently translated.
3 Proposed Method
The proposed method consists of the following
steps illustrated in Figure 1.
During training:
1) clause segmentation of source sentences with
a syntactic parser (section 3.1)
2) alignment of target words with source clauses
to develop a clause-level aligned corpus (section
3.2)
3) training the clause translation models using
the corpus (section 3.3)
During testing:
1) clause translation with the clause translation
models (section 3.4)
2) sentence reconstruction based on non-
terminals (section 3.5)
Bilingual
Corpus
(Training)
source
target
parse & clause
segmentation
parse &
clause
segmen-
tation
Source Sentences
(clause-segmented)
Word Alignment
Model
Target Word Bigram
Language Model
LM training
word
alignment
Bilingual Corpus
(clause-aligned)
automatic clause alignment
Clause
Translation Models
(Phrase Table, N-gram LMs, ...)
training from scratch
Bilingual
Corpus
(Development)
(clause-segmented)
MERT
Test Sentence
Sentence
Translation
clause
clause
clause
clause
translation
clause
translation
clause
translation
sentence reconstruction
based on non-terminals
translation
Original (sentence-aligned)
corpus can also be used
Figure 1: Overview of proposed method.
3.1 Clause Segmentation of Source Sentences
Clauses in source sentences are identified by a
syntactic parser. Figure 2 shows a parse tree for
the example sentence below. The example sen-
tence has a relative clause modifying the noun
book. Figure 3 shows the word alignment of this
example.
English: John lost the book that was borrowed
last week from Mary.
Japanese: john wa (topic marker) senshu (last
week) mary kara (from) kari (borrow) ta
(past tense marker) hon (book) o (direct ob-
ject marker) nakushi (lose) ta (past tense
marker) .
We segment the source sentence at the clause level
and the example is rewritten with two clauses as
follows.
? John lost the book s0 .
? that was borrowed last week from Mary
s0 is a non-terminal symbol the serves as a place-
holder of the relative clause. We allow an arbitrary
420
SS
John
lost
the
book
that
was
borrowed
from Mary
last week
Figure 2: Parse tree for example English sentence.
Node labels are omitted except S.
John
lo
st
the
book
that
w
as
borrow
ed
from
M
ary
last
w
eek
john
wa
ta
nakushi
o
hon
ta
kari
kara
mary
senshu
Figure 3: Word alignment for example bilingual
sentence.
number of non-terminals in each clause2. A nested
clause structure can be represented in the same
manner using such non-terminals recursively.
3.2 Alignment of Target Words with Source
Clauses
To translate source clauses with non-terminal sym-
bols, we need models trained using a clause-level
aligned bilingual corpus. A clause-level aligned
corpus is defined as a set of parallel, bilingual
clause pairs including non-terminals that represent
embedded clauses.
We assume that a sentence-aligned bilingual
corpus is available and consider the alignment of
target words with source clauses. We can manu-
ally align these Japanese words with the English
clauses as follows.
? john wa s0 hon o nakushi ta .
2In practice not so many clauses are embedded in a single
sentence but we found some examples with nine embedded
clauses for coordination in our corpora.
John lost the book s0 .
? senshu mary kara kari ta
that was borrowed last week from Mary
Since the cost of manual clause alignment is
high especially for a large-scale corpus, a natu-
ral question to ask is whether this resource can be
obtained from a sentence-aligned bilingual corpus
automatically with no human input. To answer
this, we now describe a simple method for deal-
ing with clause alignment data from scratch, us-
ing only the word alignment and language model
probabilities inferred from bilingual and monolin-
gual corpora.
Our method is based on the idea that automatic
clause alignment can be viewed as a classification
problem: for an English sentence with N words (e
= (e1, e2, . . . , eN )) andK clauses (e?1,e?2,. . . ,e?K),
and its Japanese translation with M words (f
= (f1, f2, . . . , fM )), the goal is to classify each
Japanese word into one of {1, . . . ,K} classes. In-
tuitively, the probability that a Japanese word fm
is assigned to class k ? {1, . . . ,K} depends on
two factors:
1. The probability of translating fm into the En-
glish words of clause k (i.e.
?
e?e?k p(e|fm)).
We expect fm to be assigned to a clause
where this value is high.
2. The language model probability
(i.e. p(fm|fm?1)). If this value is high,
we expect fm and fm?1 to be assigned to the
same clause.
We implement this intuition using a graph-
based method. For each English-Japanese sen-
tence pair, we construct a graph with K clause
nodes (representing English clauses) and M word
nodes (representing Japanese words). The edge
weights between word and clause nodes are de-
fined as the sum of lexical translation probabilities
?
e?e?k p(e|fm). The edge weights between words
are defined as the bigram probability p(fm|fm?1).
Each clause node is labeled with a class ID k ?
{1, . . . ,K}. We then propagate these K labels
along the graph to label the M word nodes. Fig-
ure 4 shows the graph for the example sentence.
Many label propagation algorithms are avail-
able. The important thing is to use an algo-
rithm that encourages node pairs with strong edge
weights to receive the same label. We use the label
propagation algorithm of (Zhu et al, 2003). If we
421
John  lost  the  book  that  was  borrowed ...
clause(1) clause(2)
John Mary fromlast weektopicmarker
p(John |           )
+ p(lost |           )
+ ...
p(that |        )
+ p(was |        )
+ ...
p(     |         ) p(         |            ) p(        |         )p(            |     )
john kara
karajohn
john wa senshu mary kara
wa  john senshu  wa mary  senshu kara mary
Figure 4: Graph-based representation of the ex-
ample sentence. We propagate the clause labels to
the Japanese word nodes on this graph to form the
clause alignments.
assume the labels are binary, the following objec-
tive is minimized:
argmin
l?RK+M
?
i,j
wij(li ? lj)2 (1)
where wij is the edge weight between nodes i
and j (1 ? i ? K + M , 1 ? j ? K +
M ), and l (li ? {0, 1}) is a vector of labels
on the nodes. The first K elements of l, lc =
(l1, l2, ..., lK)T , are constant because the clause
nodes are pre-labeled. The remaining M ele-
ments, lf = (lK+1, lK+2, ..., lK+M )T , are un-
known and to be determined. Here, we consider
the decomposition of the weight matrixW = [wij ]
into four blocks after the K-th row and column as
follows:
W =
[
W cc W cf
W fc W ff
]
(2)
The solution of eqn. (1), namely lf , is given by the
following equation:
lf = (Dff ?W ff )?1W fc lc (3)
where D is the diagonal matrix with di =
?
j wij
and is decomposed similarly to W . Each element
of lf is in the interval (0, 1) and can be regarded
as the label propagation probability. A detailed ex-
planation of this solution can be found in Section 2
of (Zhu et al, 2003). For our multi-label problem
with K labels, we slightly modified the algorithm
by expanding the vector l to an (M + K) ? K
binary matrix L = [ l1 l2 ... lK ].
After the optimization, we can normalize Lf
to obtain the clause alignment scores t(lm =
k|fm) between each Japanese word fm and En-
glish clause k. Theoretically, we can simply out-
put the clause id k? for each fm by finding k? =
argmaxk t(lm = k|fm). In practice, this may
sometimes lead to Japanese clauses that have too
many gaps, so we employ a two-stage procedure
to extract clauses that are more contiguous.
First, we segment the Japanese sentence into K
clauses based on a dynamic programming algo-
rithm proposed by Malioutov and Barzilay (2006).
We define an M ? M similarity matrix S = [sij ]
with sij = exp(?||li?lj ||) where li is (K + i)-th
row vector in the label matrix L. sij represents
the similarity between the i-th and j-th Japanese
words with respect to their clause alignment score
distributions; if the score distributions are sim-
ilar then sij is large. The details of this algo-
rithm can be found in (Malioutov and Barzilay,
2006). The clause segmentation gives us contigu-
ous Japanese clauses f?1, f?2, ..., f?K , thus min-
imizing inter-segment similarity and maximizing
intra-segment similarity. Second, we determine
the clause labels of the segmented clauses, based
on clause alignment scores T = [Tkk? ] for English
and automatically-segmented Japanese clauses:
Tkk? =
?
fm?f? k?
t(lm = k|fm) (4)
where f?k? is the j?-th Japanese clause. In descend-
ing order of the clause alignment score, we greed-
ily determine the clause label 3.
3.3 Training Clause Translation Models
We train clause translation models using the
clause-level aligned corpus. In addition we can
also include the original sentence-aligned corpus.
We emphasize that we can use standard techniques
for heuristically extracted phrase tables, word n-
gram language models, and so on.
3.4 Clause Translation
By using the source language parser, a multi-
clause source sentence is reduced to a set of
clauses. We translate these clauses with a common
SMT method using the clause translation models.
Here we present another English example I
bought the magazine which Tom recommended
yesterday. This sentence is segmented into clauses
as follows.
3Although a full search is available when the number of
clauses is small, we employ a greedy search in this paper.
422
? I bought the magazine s0 .
? which Tom recommended yersterday
These clauses are translated into Japanese:
? watashi (I) wa (topic marker) s0
zasshi (magazine) o (direct object marker)
kat (buy) ta (past tense marker).
? tom ga (subject marker) kino (yesterday)
susume (recommend) ta (past tense marker)
3.5 Sentence Reconstruction
We reconstruct the target sentence from the clause
translations, based on non-terminals. Starting
from the clause translation of the top clause, we re-
cursively replace non-terminal symbols with their
corresponding clause translations. Here, if a non-
terminal is eventually deleted in SMT decoding,
we simply concatenate the translation behind its
parent clause.
Using the example above, we replace the non-
terminal symbol s0 with the second clause and
obtain the Japanese sentence:
watashi wa tom ga kino susume ta zasshi o kat ta .
4 Experiment
We conducted the following experiments on the
English-to-Japanese translation of research paper
abstracts in the medical domain. Such techni-
cal documents are logically and formally writ-
ten, and sentences are often so long and syntac-
tically complex that their translation needs long
distance reordering. We believe that the medical
domain is suitable as regards evaluating the pro-
posed method.
4.1 Resources
Our bilingual resources were taken from the med-
ical domain. The parallel corpus consisted of
research paper abstracts in English taken from
PubMed4 and the corresponding Japanese transla-
tions.
The training portion consisted of 25,500 sen-
tences (no-clause-seg.; original sentences with-
out clause segmentation). 4,132 English sen-
tences in the corpus were composed of multi-
ple clauses and were separated at the clause level
4http://www.ncbi.nlm.nih.gov/pubmed/
by the procedure in section 3.1. As the syntac-
tic parser, we used the Enju5 (Miyao and Tsu-
jii, 2008) English HPSG parser. For these train-
ing sentences, we automatically aligned Japanese
words with each English clause as described in
section 3.2 and developed a clause-level aligned
corpus, called auto-aligned corpus. We prepared
manually-aligned (oracle) clauses for reference,
called oracle-aligned clauses. The clause align-
ment error rate of the auto-aligned corpus was
14% (number of wrong clause assignments di-
vided by total number of words). The develop-
ment and test portions each consisted of 1,032
multi-clause sentences. because this paper focuses
only on multi-clause sentences. Their English-
side was segmented into clauses in the same man-
ner as the training sentences, and the development
sentences had oracle clause alignment for MERT.
We also used the Life Science Dictionary6 for
training. We extracted 100,606 unique English
entries from the dictionary including entries with
multiple translation options, which we expanded
to one-to-one entries, and finally we obtained
155,692 entries.
English-side tokenization was obtained using
Enju, and we applied a simple preprocessing that
removed articles (a, an, the) and normalized plu-
ral forms to singular ones. Japanese-side tokeniza-
tion was obtained using MeCab7 with ComeJisyo8
(dictionary for Japanese medical document tok-
enization). Our resource statistics are summarized
in Table 1.
4.2 Model and Decoder
We used two decoders in the experiments,
Moses9 (Koehn et al, 2007) and our in-
house hierarchical phrase-based SMT (almost
equivalent to Hiero (Chiang, 2007)). Moses
used a phrase table with a maximum phrase
length of 7, a lexicalized reordering model with
msd-bidirectional-fe, and a distortion
limit of 1210. Our hierarchical phrase-based SMT
used a phrase table with a maximum rule length of
7 and a window size (Hiero?s ?) of 12 11. Both
5http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html
6http://lsd.pharm.kyoto-u.ac.jp/en/index.html
7http://mecab.sourceforge.net/
8http://sourceforge.jp/projects/comedic/ (in Japanese)
9http://www.statmt.org/moses/
10Unlimited distortion was also tested but the results were
worse.
11A larger window size could not be used due to its mem-
ory requirements.
423
Table 1: Data statistics on training, development,
and test sets. All development and test sentences
are multi-clause sentences.
Training
Corpus Type #words #sentences
Parallel E 690,536
(no-clause-seg.) J 942,913
25,550
Parallel E 135,698
(auto-aligned) J 183,043
4,132
(oracle-aligned) J 183,147
(10,766 clauses)
E 263,175 155.692Dictionary
J 291,455 (entries)
Development
Corpus Type #words #sentences
Parallel E 34,417 1,032
(oracle-aligned) J 46,480 (2,683 clauses)
Test
Corpus Type #words #sentences
Parallel E 34,433 1,032
(clause-seg.) J 45,975 (2,737 clauses)
decoders employed two language models: a word
5-gram language model from the Japanese sen-
tences in the parallel corpus and a word 4-gram
language model from the Japanese entries in the
dictionary. The feature weights were optimized
for BLEU (Papineni et al, 2002) by MERT, using
the development sentences.
4.3 Compared Methods
We compared four different training and test con-
ditions with respect to the use of clauses in training
and testing. The development (i.e., MERT) condi-
tions followed the test conditions. Two additional
conditions with oracle clause alignment were also
tested for reference.
Table 2 lists the compared methods. First,
the proposed method (proposed) used the auto-
aligned corpus in training and clause segmen-
tation in testing. Second, the baseline method
(baseline) did not use clause segmentation in ei-
ther training or testing. Using this standard base-
line method, we focused on the advantages of the
divide-and-conquer translation itself. Third, we
tested the same translation models as used with
the proposed method for test sentences without
clause segmentation, (comp.(1)). Although this
comparison method cannot employ the proposed
clause-level reordering, it was expected to be bet-
ter than the baseline method because its transla-
tion model can be trained more precisely using the
finely aligned clause-level corpus. Finally, the sec-
ond comparison method (comp.(2)) translated seg-
mented clauses with the baseline (without clause
segmentation) model, as if each of them was a sin-
gle sentence. Its translation of each clause was
expected to be better than that of the baseline be-
cause of the efficient search over shortened inputs,
while its reordering of clauses (non-terminals) was
unreliable due to the lack of clause information
in training. Its sentence reconstruction based on
non-terminals was the same as with the proposed
method. Although non-terminals in the second
comparison method were out-of-vocabulary words
and may be deleted in decoding, all of them sur-
vived and we could reconstruct sentences from
translated clauses throughout the experiments. In
addition, two other conditions were tested: us-
ing oracle-aligned clauses in training: the pro-
posed method trained using oracle-aligned (ora-
cle) clauses and the first comparison method using
oracle-aligned (oracle-comp.) clauses.
4.4 Results
Table 3 shows the results in BLEU, Transla-
tion Edit Rate (TER) (Snover et al, 2006),
and Position-independent Word-error Rate (PER)
(Och et al, 2001), obtained with Moses and our
hierarchical phrase-based SMT, respectively. Bold
face results indicate the best scores obtained with
the compared methods (excluding oracles).
The proposed method consistently outper-
formed the baseline. The BLEU improve-
ments with the proposed method over the base-
line and comparison methods were statistically
significant according to the bootstrap sampling
test (p < 0.05, 1,000 samples) (Zhang et al,
2004). With Moses, the improvement when us-
ing the proposed method was 1.4% (33.19% to
34.60%) in BLEU and 1.3% (57.83% to 56.50%)
in TER, with a slight improvement in PER
(35.84% to 35.61%). We observed: oracle ?
proposed ? comp.(1) ? baseline ? comp.(2)
by the Bonferroni method, where the symbol
A ? B means ?A?s improvement over B is
statistically significant.? With the hierarchical
phrase-based SMT, the improvement was 2.2%
(32.39% to 34.55%) in BLEU, 3.5% (58.36% to
54.87%) in TER, and 1.5% in PER (36.42% to
34.79%). We observed: oracle ? proposed ?
424
Table 2: Compared methods.
P
P
P
P
P
P
P
P
Test
Training w/ auto-aligned w/o aligned w/ oracle-aligned
clause-seg. proposed comp.(2) oracle
no-clause-seg. comp.(1) baseline oracle-comp.
{comp.(1), comp.(2)} ? baseline by the Bon-
ferroni method. The oracle results were better than
these obtained with the proposed method but the
differences were not very large.
4.5 Discussion
We think the advantage of the proposed method
arises from three possibilities: 1) better translation
model training using the fine-aligned corpus, 2) an
efficient decoder search over shortened inputs, and
3) an effective clause-level reordering model real-
ized by using non-terminals.
First, the results of the first comparison method
(comp.(1)) indicate an advantage of the transla-
tion models trained using the auto-aligned corpus.
The training of the translation models, namely
word alignment and phrase extraction, is difficult
for long sentences due to their large ambiguity.
This result suggests that the use of clause-level
alignment provides fine-grained word alignments
and precise translation models. We can also ex-
pect that the model of the proposed method will
work better for the translation of single-clause sen-
tences.
Second, the average and median lengths (in-
cluding non-terminals) of the clause-seg. test set
were 13.2 and 10 words, respectively. They were
much smaller than those of no-clause-seg. at 33.4
and 30 words and are expected to help realize
an efficient SMT search. Another observation is
the relationship between the number of clauses
and translation performance, as shown in Fig-
ure 5. The proposed method achieved a greater im-
provement in sentences with a greater number of
clauses. This suggests that our divide-and-conquer
approach works effectively for multi-clause sen-
tences. Here, the results of the second comparison
method (comp.(2)) with Moses were worse than
the baseline results, while there was an improve-
ment with our hierarchical phrase-based SMT.
This probably arose from the difference between
the decoders when translating out-of-vocabulary
words. The non-terminals were handled as out-of-
vocabulary words under the comp.(2) condition.
52
54
56
58
60
62
64
66
2 4 53
TE
R
 (%
)
The number of clauses
baseline
proposed
comp.(2)
Figure 5: Relationship between TER and number
of clauses for proposed, baseline, and comp.(2)
when using our hierarchical phrase-based SMT.
Moses generated erroneous translations around
such non-terminals that can be identified at a
glance, while our hierarchical phrase-based SMT
generated relatively good translations. This may
be a decoder-dependent issue and is not an essen-
tial problem.
Third, the results obtained with the proposed
method reveal an advantage in reordering in ad-
dition to the previous two advantages. The differ-
ence between the PERs with the proposed method
and the baseline with Moses was small (0.2%)
in spite of the large differences in BLEU and
TER (about 1.5%). This suggests that the pro-
posed method is better in word ordering and im-
plies our method is also effective in reordering.
With the hierarchical phrase-based SMT, the pro-
posed method showed a large improvement from
the baseline and comparison methods, especially
in TER which was better than the best Moses
configuration (proposed). This suggests that the
decoding of long sentences with long-distance
reordering is not easy even for the hierarchical
phrase-based SMT due to its limited window size,
while the hierarchical framework itself can natu-
rally model a long-distance reordering. If we try to
find a derivation with such long-distance reorder-
ing, we will probably be faced with an intractable
search space and computation time. Therefore,
we can conclude that the proposed divide-and-
425
Table 3: Experimental results obtained with Moses and our hierarchical phrase-based SMT, in BLEU,
TER, and PER.
Moses : BLEU (%) / TER (%) / PER (%)
P
P
P
P
P
P
P
P
Test
Training w/ auto-aligned w/o aligned w/ oracle-aligned
clause-seg. 34.60 / 56.50 / 35.61 32.14 / 58.78 / 36.08 35.31 / 55.12 / 34.42
no-clause-seg. 34.22 / 56.90 / 35.20 33.19 / 57.83 / 35.84 34.24 / 56.67 / 35.03
Hierarchical : BLEU (%) / TER (%) / PER (%)
P
P
P
P
P
P
P
P
Test
Training w/ auto-aligned w/o aligned w/ oracle-aligned
clause-seg. 34.55 / 54.87 / 34.79 33.03 / 56.70 / 36.03 35.08 / 54.22 / 34.77
no-clause-seg. 33.41 / 57.02 / 35.86 32.39 / 58.36 / 36.42 33.83 / 56.26 / 34.96
conquer approach provides more practical long-
distance reordering at the clause level.
We also analyzed the difference between auto-
matic and manual clause alignment. Since auto-
aligned corpus had many obvious alignment er-
rors, we suspected these noisy clauses hurt the
clause translation model. However, they were not
serious in terms of final translation performance.
So we can conclude that our proposed divide-and-
conquer approach is promising for long sentence
translation. Although we aimed to see whether we
could bootstrap using existing bilingual corpora in
this paper, we imagine better clause alignment can
be obtained with some supervised classifiers.
One problem with the divide-and-conquer ap-
proach is that its independently-translated clauses
potentially cause disfluencies in final sentence
translations, mainly due to wrong inflections. A
promising solution is to optimize a whole sentence
translation by integrating search of each clause
translation but this may require a much larger
search space for decoding. More simply, we may
be able to approximate it using n-best clause trans-
lations. This problem should be addressed for fur-
ther improvement in future studies.
5 Conclusion
In this paper we proposed a clause-based divide-
and-conquer approach for SMT that can re-
duce complicated clause-level reordering to sim-
ple word-level reordering. The proposed method
separately translates clauses with non-terminals by
using a well-known SMT method and reconstructs
a sentence based on the non-terminals, to reorder
long clauses. The clause translation models are
trained using a bilingual corpus with clause-level
alignment, which can be obtained with an un-
supervised graph-based method using sentence-
aligned corpora. The proposed method improves
the translation of long, multi-clause sentences and
is especially effective for language pairs with
large word order differences, such as English-to-
Japanese.
This paper focused only on clauses as segments
for division. However, other long segments such
as prepositional phrases are similarly difficult to
reorder correctly. The divide-and-conquer ap-
proach itself can be applied to long phrases, and
it is worth pursuing such an extension. As another
future direction, we must develop a more sophis-
ticated method for automatic clause alignment if
we are to use the proposed method for various lan-
guage pairs and domains.
Acknowledgments
We thank the U. S. National Library of Medicine
for the use of PubMed abstracts and Prof. Shuji
Kaneko of Kyoto University for the use of Life
Science Dictionary. We also thank the anonymous
reviewers for their valuable comments.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263?311.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proc. ACL, pages 531?540.
426
Osamu Furuse, Setsuo Yamada, and Kazuhide Ya-
mamoto. 1998. Splitting long or ill-formed in-
put for robust spoken-language translation. In Proc.
COLING-ACL, pages 421?427.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proc. NAACL, pages 273?280.
Jonathan Graehl and Kevin Knight. 2004. Training
tree transducers. In Proc. HLT-NAACL, pages 105?
112.
Jason Katz-Brown and Michael Collins. 2008. Syntac-
tic reordering in preprocessing for Japanese-English
translation: MIT system description for NTCIR-7
patent translation task. In Proc. NTCIR-7, pages
409?414.
Yeun-Bae Kim and Terumasa Ehara. 1994. A method
for partitioning of long Japanese sentences with sub-
ject resolution in J/E machine translation. In Proc.
International Conference on Computer Processing
of Oriental Languages, pages 467?473.
Phillip Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
HLT-NAACL, pages 263?270.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 IWSLT speech translation evaluation.
In Proc. IWSLT.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. ACL Companion Volume Proceedings of the
Demo and Poster Sessions, pages 177?180.
Chi-Ho Li, Dongdong Zhang, Mu Li, Ming Zhou,
Minghui Li, and Yi Guan. 2007. A probabilistic ap-
proach to syntax-based reordering for statistical ma-
chine translation. In Proc. ACL, pages 720?727.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-String alignment template for statistical machine
translation. In Proc. Coling-ACL, pages 609?616.
Igor Malioutov and Regina Barzilay. 2006. Minimum
cut model for spoken lecture segmentation. In Proc.
Coling-ACL, pages 25?32.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. Compu-
tational Linguistics, 34(1):35?80.
Franz Josef Och, Nicola Ueffing, and Hermann Ney.
2001. An efficient A* search algorithm for statis-
tical machine translation. In Proc. the ACL Work-
shop on Data-Driven Methods in Machine Transla-
tion, pages 55?62.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proc. ACL,
pages 311?318.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proc. AMTA, pages 223?231.
Christoph Tillmann. 2004. A unigram orientation
model for statistical machine translation. In Proc.
HLT-NAACL, pages 101?104.
Erik F. Tjong, Kim Sang, and Herve? De?jean. 2001. In-
troduction to the CoNLL-2001 shared task: Clause
identification. In Proc. CoNLL, pages 53?57.
Roy Tromble and Jason Eisner. 2009. Learning linear
ordering problems for better translation. In Proc.
EMNLP, pages 1007?1016.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?404.
Fei Xia and Michael McCord. 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. In Proc. COLING, pages 508?514.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
SMT for Subject-Object-Verb languages. In Proc.
HLT-NAACL, pages 245?253.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proc. ACL,
pages 523?530.
Ying Zhang, Stephan Vogel, and Alex Weibel. 2004.
Interpreting BLEU/NIST scores: How much im-
provement do we need to have a better system? In
Proc. LREC, pages 2051?2054.
Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty.
2003. Semi-supervised learning using gaussian
fields and harmonic functions. In Proc. ICML, pages
912?919.
427
Proceedings of SSST-6, Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 57?66,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Head Finalization Reordering for Chinese-to-Japanese
Machine Translation
Han Dan+ Katsuhito Sudoh? Xianchao Wu??
Kevin Duh?? Hajime Tsukada? Masaaki Nagata?
+The Graduate University For Advanced Studies, Tokyo, Japan
?NTT Communication Science Laboratories, NTT Corporation
+handan@nii.ac.jp, ?wuxianchao@baidu.com, ?kevinduh@is.naist.jp
?{sudoh.katsuhito, tsukada.hajime, nagata.masaaki}@lab.ntt.co.jp
Abstract
In Statistical Machine Translation, reorder-
ing rules have proved useful in extracting
bilingual phrases and in decoding during
translation between languages that are struc-
turally different. Linguistically motivated
rules have been incorporated into Chinese-
to-English (Wang et al, 2007) and English-
to-Japanese (Isozaki et al, 2010b) transla-
tion with significant gains to the statistical
translation system. Here, we carry out a lin-
guistic analysis of the Chinese-to-Japanese
translation problem and propose one of the
first reordering rules for this language pair.
Experimental results show substantially im-
provements (from 20.70 to 23.17 BLEU)
when head-finalization rules based on HPSG
parses are used, and further gains (to 24.14
BLEU) were obtained using more refined
rules.
1 Introduction
In state-of-the-art Statistical Machine Translation
(SMT) systems, bilingual phrases are the main
building blocks for constructing a translation given
a sentence from a source language. To extract
those bilingual phrases from a parallel corpus,
the first step is to discover the implicit word-
to-word correspondences between bilingual sen-
tences (Brown et al, 1993). Then, a symmetriza-
tion matrix is built (Och and Ney, 2004) by us-
ing word-to-word alignments, and a wide variety
?Now at Baidu Japan Inc.
? Now at Nara Institute of Science and Technology
(NAIST)
of heuristics can be used to extract the bilingual
phrases (Zens et al, 2002; Koehn et al, 2003).
This method performs relatively well when the
source and the target languages have similar word
order, as in the case of French, Spanish, and En-
glish. However, when translating between lan-
guages with very different structures, as in the case
of English and Japanese, or Japanese and Chinese,
the quality of extracted bilingual phrases and the
overall translation quality diminishes.
In the latter scenario, a simple but effective strat-
egy to cope with this problem is to reorder the
words of sentences in one language so that it re-
sembles the word order of another language (Wu
et al, 2011; Isozaki et al, 2010b). The advan-
tages of this strategy are two fold. The first ad-
vantage is at the decoding stage, since it enables
the translation to be constructed almost monoton-
ically. The second advantage is at the training
stage, since automatically estimated word-to-word
alignments are likely to be more accurate and sym-
metrization matrices reveal more evident bilingual
phrases, leading to the extraction of better quality
bilingual phrases and cleaner phrase tables.
In this work, we focus on Chinese-to-Japanese
translation, motivated by the increasing interaction
between these two countries and the need to im-
prove direct machine translation without using a
pivot language. Despite the countries? close cul-
tural relationship, their languages significantly dif-
fer in terms of syntax, which poses a severe diffi-
culty in statistical machine translation. The syntac-
tic relationship of this language pair has not been
carefully studied before in the machine translation
57
field, and our work aims to contribute in this direc-
tion as follows:
? We present a detailed syntactic analysis of
several reordering issues in Chinese-Japanese
translation using the information provided by
an HPSG-based deep parser.
? We introduce novel reordering rules based on
head-finalization and linguistically inspired
refinements to make words in Chinese sen-
tences resemble Japanese word order. We em-
pirically show its effectiveness (e.g. 20.70 to
24.23 BLEU improvement).
The paper is structured as follows. Section 2 in-
troduces the background and gives an overview of
similar techniques related to this work. Section 3
describes the proposed method in detail. Exper-
imental evaluation of the performance of the pro-
posed method is described in section 4. There is an
error analysis on the obtained results in section 5.
Conclusions and a short description on future work
derived from this research are given in the final
section.
2 Background
2.1 Head Finalization
The structure of languages can be characterized
by phrase structures. The head of a phrase is the
word that determines the syntactic category of the
phrase, and its modifiers (also called dependents)
are the rest of the words within the phrase. In En-
glish, the head of a phrase can be usually found
before its modifiers. For that reason, English is
called a head-initial language (Cook and Newson,
1988). Japanese, on the other hand, is head-final
language (Fukui, 1992), since the head of a phrase
always appears after its modifiers.
In certain applications, as in the case of ma-
chine translation, word reordering can be a promis-
ing strategy to ease the task when working with
languages with different phrase structures like En-
glish and Japanese. Head Finalization is a success-
ful syntax-based reordering method designed to re-
order sentences from a head-initial language to re-
semble the word order in sentences from a head-
final language (Isozaki et al, 2010b). The essence
of this rule is to move the syntactic heads to the
end of its dependency by swapping child nodes in
a phrase structure tree when the head child appears
before the dependent child.
Isozaki et al (2010b) proposed a simple method
of Head Finalization, by using an HPSG-based
deep parser for English (Miyao and Tsujii, 2008)
to obtain phrase structures and head information.
The score results from several mainstream evalua-
tion methods indicated that the translation quality
had been improved; the scores of Word Error Rate
(WER) and Translation Edit Rate (TER) (Snover
et al, 2006) had especially been greatly reduced.
2.2 Chinese Deep Parsing
Syntax-based reordering methods need parsed sen-
tences as input. Isozaki et al (2010b) used Enju,
an HPSG-based deep parser for English, but they
also discussed using other types of parsers, such
as word dependency parsers and Penn Treebank-
style parsers. However, to use word dependency
parsers, they needed an additional heuristic rule to
recover phrase structures, and Penn Treebank-style
parsers are problematic because they output flat
phrase structures (i.e. a phrase may have multiple
dependents, which causes a problem of reorder-
ing within a phrase). Consequently, compared to
different types of parsers, Head-Final English per-
forms the best on the basis of English Enju?s pars-
ing result.
In this paper, we follow their observation, and
use the HPSG-based parser for Chinese (Chinese
Enju) (Yu et al, 2011) for Chinese syntactic pars-
ing. Since Chinese Enju is based on the same pars-
ing model as English Enju, it provides rich syn-
tactic information including phrase structures and
syntactic/semantic heads.
Figure 1 shows an example of an XML output
from Chinese Enju for the sentence ?wo (I) qu (go
to) dongjing (Tokyo) he (and) jingdu (Ky-
oto).? The label <cons> and <tok> represent
the non-terminal nodes and terminal nodes, respec-
tively. Each node is identified by a unique ?id?
and has several attributes. The attribute ?head?
indicates which child node is the syntactic head.
In this figure, <head=?c4? id=?c3?> means that
the node that has id=?c4? is the syntactic head of
the node that has id=?c3?.
58
Figure 1: An XML output for a Chinese sentence from
Chinese Enju. For clarity, we only draw information
related to the phrase structure and the heads.
2.3 Related Work
Reordering is a popular strategy for improving
machine translation quality when source and tar-
get languages are structurally very different. Re-
searchers have approached the reordering problem
in multiple ways. The most basic idea is pre-
ordering (Xia and McCord, 2004; Collins et al,
2005), that is, to do reordering during preprocess-
ing time, where the source side of the training and
development data and sentences from a source lan-
guage that have to be translated are first reordered
to ease the training and the translation, respec-
tively. In (Xu et al, 2009), authors used a depen-
dency parser to introduce manually created pre-
ordering rules to reorder English sentences when
translating into five different SOV(Subject-Object-
Verb) languages. Other authors (Genzel, 2010; Wu
et al, 2011) use automatically generated rules in-
duced from parallel data. Tillmann (2004) used a
lexical reordering model, and Galley et al (2004)
followed a syntactic-based model.
In this work, however, we are centered in the
design of manual rules inspired by the Head Final-
ization (HF) reordering (Isozaki et al, 2010b). HF
reordering is one of the simplest methods for pre-
ordering that significantly improves word align-
ments and leads to a better translation quality. Al-
though the method is limited to translation where
the target language is head-final, it requires neither
training data nor fine-tuning. To our knowledge,
HF is the best method to reorder languages when
translating into head-final languages like Japanese.
The implementation of HF method for English-
to-Japanese translation appears to work well. A
reasonable explanation for this is the close match
between the concept of ?head? in this language
pair. However, for Chinese-to-Japanese, there are
differences in the definitions of numbers of impor-
tant syntactic concepts, including the definition of
the syntactic head. We concluded that the diffi-
culties we encountered in using HF to Chinese-to-
Japanese translation were the result of these differ-
ences in the definition of ?head?. As we believe
that such differences are also likely to be observed
in other language pairs, the present work is gener-
ally important for head-initial to head-final trans-
lation as it shows a systematic linguistic analysis
that consistently improves the effectivity of the HF
method.
3 Syntax-based Reordering Rules
This section describes our method for syntax-
based reordering for Chinese-to-Japanese transla-
tion. We start by introducing Head Finalization
for Chinese (HFC), which is a simple adaptation
of Isozaki et al (2010b)?s method for English-to-
Japanese translation. However, we found that this
simple method has problems when applied to Chi-
nese, due to peculiarities in Chinese syntax. In
Section 3.2, we analyze several distinctive cases of
the problem in detail. And following this analysis,
Section 3.3 proposes a refinement of the original
HFC, with a couple of exception rules for reorder-
ing.
3.1 Head Finalization for Chinese (HFC)
Since Chinese and English are both known to be
head-initial languages1, the reordering rule intro-
duced in (Isozaki et al, 2010b) ideally would re-
order Chinese sentences to follow the word order
1As Gao (2008) summarized, whether Chinese is a head-
initial or a head-final language is open for debate. Neverthe-
less, we take the view that most Chinese sentence structures
are head-initial since the written form of Chinese mainly be-
haves as an head-initial language.
59
Figure 2: Simple example for Head-Final Chinese. The left figure shows the parsing tree of the original sentence
and its English translation. The right figure shows the reordered sentence along with its Japanese translation.
( ?*? indicate the syntactic head).
of their Japanese counterparts.
Figure 2 shows an example of a head finalized
Chinese sentence based on the output from Chi-
nese Enju shown in Figure 1. Notice that the
coordination exception rule described in (Isozaki
et al, 2010b) also applies to Chinese reordering.
This exception rule says that child nodes are not
swapped if the node is a coordination2. Another
exception rule is for punctuation symbols, which
are also preserved in their original order. In this
case, as can be seen in the example in Figure 2, the
nodes of c3, c6, and c8 had not been swapped with
their dependency. In this account, only the verb
?qu? had been moved to the end of the sentence,
following the same word order as its Japanese
translation.
3.2 Discrepancies in Head Definition
Head Finalization relies on the idea that head-
dependent relations are largely consistent among
different languages while word orders are differ-
ent. However, in Chinese, there has been much
debate on the definition of head3, possibly because
Chinese has fewer surface syntactic features than
other languages like English and Japanese. This
causes some discrepancies between the definitions
2Coordination is easily detected in the output of
Enju; it is marked by the attributes xcat="COOD" or
schema="coord-left/right" as shown in Figure 1.
3In this paper, we only consider the syntactic head.
of the head in Chinese and Japanese, which leads
to undesirable reordering of Chinese sentences.
Specifically, in preliminary experiments we ob-
served unexpected reorderings that are caused by
the differences in the head definitions, which we
describe below.
3.2.1 Aspect Particle
Although Chinese has no syntactic tense marker,
three aspect particles following verbs can be used
to identify the tense semantically. They are ?le0?
(did), ?zhe0? (doing), and ?guo4? (done), and
their counterparts in Japanese are ?ta?, ?teiru?,
and ?ta?, respectively. Both the first word and
third word can represent the past tense, but the
third one is more often used in the past perfect.
The Chinese parser4 treated aspect particles as
dependents of verbs, whereas their Japanese coun-
terparts are identified as the head. For exam-
ple in Table 15, ?qu? (go) and ?guo? (done)
aligned with ?i? and ?tta?, respectively. How-
ever, since ?guo? is treated as a dependent of
?qu?, by directly implementing the Head Final
Chinese (HFC), the sentence will be reordered like
4The discussions in this section presuppose the syntactic
analysis done by Chinese Enju, but most of the analysis is
consistent with the common explanation for Chinese syntax.
5English translation (En); Chinese original sentence
(Ch); reordered Chinese by Head-Final Chinese (HFC); re-
ordered Chinese by Refined Head-Final Chinese (R-HFC)
and Japanese translation (Ja).
60
HFC in Table 1, which does not follow the word
order of the Japanese (Ja) translation. In contrast,
the reordered sentence from refined-HFC (R-HFC)
can be translated monotonically.
En I have been to Tokyo.
Ch wo qu guo dongjing.
HFC wo dongjing guo qu.
R-HFC wo dongjing qu guo.
Ja watashi (wa) Tokyo (ni) i tta.
Table 1: An example for Aspect Particle. Best word
alignment Ja-Ch (En): ?watashi? ? ?wo?(I); ?Tokyo? ?
?dongjing? (Tokyo); ?i? ? ?qu? (been); ?tta? ? ?guo?
(have).
3.2.2 Adverbial Modifier ?bu4?
Both in Chinese and Japanese, verb phrase mod-
ifiers typically occur in pre-verbal positions, espe-
cially when the modifiers are adverbs. Since ad-
verbial modifiers are dependents in both Chinese
and Japanese, head finalization works perfectly for
them. However, there is an exceptional adverb,
?bu4?, which means negation and is usually trans-
lated into ?nai?, which is always at the end of the
sentence in Japanese and thus is the head. For ex-
ample in Table 2, the word ?kan? (watch) will be
identified as the head and the word ?bu? is its de-
pendent; on the contrary, in the Japanese transla-
tion (Ja), the word ?nai?, which is aligned with
?bu?, will be identified as the head. Therefore,
the Head Final Chinese is not in the same order,
but the reordered sentence by R-HFC obtained the
same order with the Japanese translation.
En I do not watch TV.
Ch wo bu kan dianshi.
HFC wo dianshi bu kan.
R-HFC wo dianshi kan bu.
Ja watashi (wa) terebi (wo) mi nai.
Table 2: An example for Adverbial Modifier bu4.
Best word alignment Ja-Ch (En): ?watashi? ? ?wo? (I);
?terebi? ? ?dianshi? (TV); ?mi? ? ?kan? (watch); ?nai?
? ?bu? (do not).
3.2.3 Sentence-final Particle
Sentence-final particles often appear at the end
of a sentence to express a speaker?s attitude:
e.g. ?ba0, a0? in Chinese, and ?naa, nee? in
Japanese. Although they appear in the same posi-
tion in both Chinese and Japanese, in accordance
with the differences of head definition, they are
identified as the dependent in Chinese while they
are the head in Japanese. For example in Table 3,
since ?a0? was identified as the dependent, it had
been reordered to the beginning of the sentence
while its Japanese translation ?nee? is at the end
of the sentence as the head. Likewise, by refining
the HFC, we can improve the word alignment.
En It is good weather.
Ch tianqi zhenhao a.
HFC a tianqi zhenhao.
R-HFC tianqi zhenhao a.
Ja ii tennki desu nee.
Table 3: An example for Sentence-final Particle.
Best word alignment Ja-Ch (En): ?tennki? ? ?tianqi?
(weather); ?ii? ? ?zhenhao? (good); ?nee? ? ?a? (None).
3.2.4 Et cetera
In Chinese, there are two expressions for rep-
resenting the meaning of ?and other things? with
one Chinese character: ?deng3? and ?deng3
deng3?, which are both identified as dependent
of a noun. In contrast, in Japanese, ?nado? is al-
ways the head because it appears as the right-most
word in a noun phrase. Table 4 shows an example.
En Fruits include apples, etc.
Ch shuiguo baokuo pingguo deng.
HFC shuiguo deng pingguo baokuo.
R-HFC shuiguo pingguo deng baokuo.
Ja kudamono (wa) ringo nado (wo)
fukunde iru.
Table 4: An example for Et cetera. Best word alignment
Ja-Ch (En): ?kudamono? ? ?shuiguo? (Fruits); ?ringo?
? ?pingguo? (apples); ?nado? ? ?deng? (etc.); ?fukunde
iru? ? ?baokuo? (include).
61
AS Aspect particle
SP Sentence-final particle
ETC et cetera (i.e. deng3 and deng3 deng3)
IJ Interjection
PU Punctuation
CC Coordinating conjunction
Table 5: The list of POSs for exception reordering rules
3.3 Refinement of HFC
In the preceding sections, we have discussed syn-
tactic constructions that cause wrong application
of Head Finalization to Chinese sentences. Fol-
lowing the observations, we propose a method to
improve the original Head Finalization reordering
rule to obtain better alignment with Japanese.
The idea is simple: we define a list of POSs,
and when we find one of them as a dependent
child of the node, we do not apply reordering. Ta-
ble 5 shows the list of POSs we define in the cur-
rent implementation6. While interjections are not
discussed in detail, we should obviously not re-
order to interjections because they are position-
independent. The rules for PU and CC are ba-
sically equivalent to the exception rules proposed
by (Isozaki et al, 2010b).
4 Experiments
The corpus we used as training data comes
from the China Workshop on Machine Transla-
tion (CWMT) (Zhao et al, 2011). This is a
Japanese-Chinese parallel corpus in the news do-
main, containing 281, 322 sentence pairs. We also
collected another Japanese-Chinese parallel cor-
pus from news containing 529, 769 sentences and
merged it with the CWMT corpus to create an ex-
tended version of the CWMT corpus. We will re-
fer to this corpus as ?CWMT ext.? We split an in-
verted multi-reference set into a development and a
test set containing 1, 000 sentences each. In these
two sets, the Chinese input was different, but the
Japanese reference was identical. We think that
this split does not pose any severe problem to the
comparison fairness of the experiment, since no
new phrases are added during tuning and the ex-
perimental conditions remain equal for all tested
6The POSs are from Penn Chinese Treebank.
Ch Ja
CWMT
Sentences 282K
Run. words 2.5M 3.2M
Avg. sent. leng. 8.8 11.5
Vocabulary 102K 42K
CWMT ext.
Sentences 811K
Run. words 14.7M 17M
Avg. sent. leng. 18.1 20.9
Vocabulary 249K 95K
Dev.
Sentences 1000
Run. words 29.9K 35.7K
Avg. sent. leng. 29.9 35.7
OoV w.r.t. CWMT 485 106
OoV w.r.t. CWMT ext. 244 53
Test
Sentences 1000
Run. words 25.8K 35.7K
Avg. sent. leng. 25.8 35.7
OoV w.r.t. CWMT 456 106
OoV w.r.t. CWMT ext. 228 53
Table 6: Characteristics of CWMT and extended
CWMT Chinese-Japanese corpus. Dev. stands for De-
velopment, OoV for ?Out of Vocabulary? words, K for
thousands of elements, and M for millions of elements.
Data statistics were collected after tokenizing.
methods. Detailed Corpus statistics can be found
in Table 6.
To parse Chinese sentences, we used Chinese
Enju (Yu et al, 2010), an HPSG-based parser
trained with the Chinese HPSG treebank converted
from Penn Chinese Treebank. Chinese Enju re-
quires segmented and POS-tagged sentences to
do parsing. We used the Stanford Chinese seg-
menter (Chang et al, 2008) and Stanford POS-
tagger (Toutanova et al, 2003) to obtain the seg-
mentation and POS-tagging of the Chinese side of
the training, development, and test sets.
The baseline system was trained following
the instructions of recent SMT evaluation cam-
paigns (Callison-Burch et al, 2010) by using the
MT toolkit Moses (Koehn et al, 2007) in its de-
fault configuration. Phrase pairs were extracted
from symmetrized word alignments and distor-
tions generated by GIZA++ (Och and Ney, 2003)
using the combination of heuristics ?grow-diag-
final-and? and ?msd-bidirectional-fe?. The lan-
guage model was a 5-gram language model es-
timated on the target side of the parallel cor-
pora by using the modified Kneser-Ney smooth-
ing (Chen and Goodman, 1999) implemented in
62
the SRILM (Stolcke, 2002) toolkit. The weights
of the log-linear combination of feature functions
were estimated by using MERT (Och, 2003) on the
development set described in Table 6.
The effectiveness of the reorderings proposed
in Section 3.3 was assessed by using two preci-
sion metrics and two error metrics on translation
quality. The first evaluation metric is BLEU (Pap-
ineni et al, 2002), a very common accuracy metric
in SMT that measures N -gram precision, with a
penalty for too short sentences. The second eval-
uation metric was RIBES (Isozaki et al, 2010a), a
recent precision metric used to evaluate translation
quality between structurally different languages. It
uses notions on rank correlation coefficients and
precision measures. The third evaluation metric is
TER (Snover et al, 2006), another error metric that
computes the minimum number of edits required
to convert translated sentences into its correspond-
ing references. Possible edits include insertion,
deletion, substitution of single words, and shifts of
word sequences. The fourth evaluation metric is
WER, an error metric inspired in the Levenshtein
distance at word level. BLEU, WER, and TER
were used to provide a sense of comparison but
they do not significantly penalize long-range word
order errors. For this reason, RIBES was used to
account for this aspect of translation quality.
The baseline system was trained and tuned us-
ing the same configuration setup described in this
section, but no reordering rule was implemented at
the preprocessing stage.
Three systems have been run to translate the test
set for comparison when the systems were trained
using the two training data sets. They are the
baseline system, the system consisting in the na??ve
implementation of HF reordering, and the system
with refined HFC reordering rules. Assessment of
translation quality can be found in Table 7.
As can be observed in Table 7, the translation
quality, as measured by precision and error met-
rics, was consistently and significantly increased
when the HFC reordering rule was used and was
significantly improved further when the refinement
proposed in this work was used. Specifically, the
BLEU score increased from 19.94 to 20.79 when
the CWMT corpus was used, and from 23.17 to
24.14 when the extended CWMT corpus was used.
AS SP ETC IJ PU COOD
3.8% 0.8% 1.3% 0.0%* 21.0% 38.3%
Table 8: Weighted recall of each exception rule during
reordering on CWMT ext. training data, dev data, and
test data. (* actual value 0.0016%.)
Table 8 shows the recall of each exception rule
listed in Section 3, and was computed by counting
the times an exception rule was triggered divided
by the number of times the head finalization rule
applied. Data was collected for CWMT ext. train-
ing, dev and test sets. Although the exception rules
related to aspect particles, Et cetera, sentence-final
particles and interjections have a comparatively
lower frequency of application than punctuation
or coordination exception rules, the improvements
they led to are significant.
5 Error Analysis
In Section 3 we have analyzed syntactic differ-
ences between Chinese and Japanese that led to
the design of an effective refinement. A manual
error analysis of the results of our refined reorder-
ing rules showed that some more reordering issues
remain and, although they are not side effects of
our proposed rule, they are worth mentioning in
this separate section.
5.1 Serial Verb Construction
Serial verb construction is a phenomenon occur-
ring in Chinese, where several verbs are put to-
gether as one unit without any conjunction be-
tween them. The relationship between these
verbs can be progressive or parallel. Apparently,
Japanese has a largely corresponding construc-
tion, which indicates that no reordering should
be applied. An example to illustrate this fact in
Chinese is ?weishi (maintain) shenhua (deepen)
zhongriguanxi (Japan-China relations) de
(of) gaishan (improvement) jidiao (basic
tone).?7 The two verbs ?weishi? (in Japanese,
iji) and ?shenhua? (in Japanese, shinka) are
used together, and they follow the same order as
in Japanese: ?nicchukankei (Japan-China re-
7English translation: Maintain and deepen the improved
basic tone of Japan-China relations.
63
CWMT CWMT ext.
BLEU RIBES TER WER BLEU RIBES TER WER
baseline 16.74 71.24 70.86 77.45 20.70 74.21 66.10 72.36
HFC 19.94 73.49 65.19 71.39 23.17 75.35 61.38 67.74
refined HFC 20.79 75.09 64.91 70.39 24.14 77.17 59.67 65.31
Table 7: Evaluation of translation quality of a test set when CWMT and CWMT extended corpus were used for
training. Results are given in terms of BLEU, RIBES, TER, and WER for baseline, head finalization, and proposed
refinement of head finalization reordering rules.
lations) no (of) kaizan (improvement) kityo
(basic tone) wo iji (maintain) shinka (deepen)
suru (do).?
5.2 Complementizer
A ?complementizer? is a particle used to intro-
duce a complement. In English, a very common
complementizer is the word ?that? when making a
clausal complement, while in Chinese it can de-
note other types of word, such as verbs, adjec-
tives or quantifiers. The complementizer is iden-
tified as the dependent of the verb that it modi-
fies. For instance, a Chinese sentence: ?wo (I)
mang wan le (have finished the work).? This
can be translated into Japanese: ?watashi (I) wa
shigoto (work) wo owa tta (have finished).? In
Chinese, the verb ?mang? is the head while ?wan?
is the complementizer, and its Japanese counter-
part ?owa tta? has the same word order.
However, during the reordering, ?mang? will be
placed at the end of the sentence and ?wan? in the
beginning, leading to an inconsistency with respect
to the Japanese translation where the complemen-
tizer ?tta? is the head.
5.3 Verbal Nominalization and Nounal
Verbalization
As discussed by Guo (2009), compared to English
and Japanese, Chinese has little inflectional mor-
phology, that is, no inflection to denote tense, case,
etc. Thus, words are extremely flexible, making
verb nominalization and noun verbalization appear
frequently and commonly without any conjugation
or declension. As a result, it is difficult to do dis-
ambiguation during POS tagging and parsing. For
example, the Chinese word ?kaifa? may have
two syntactic functions: verb (develop) and noun
(development). Thus, it is difficult to reliably tag
without considering the context. In contrast, in
Japanese, ?suru? can be used to identify verbs.
For example, ?kaihatu suru? (develop) is a
verb and ?kaihatu? (development) is a noun.
This ambiguity is prone to not only POS tagging
error but also parsing error, and thus affects the
identification of heads, which may lead to incor-
rect reordering.
5.4 Adverbial Modifier
Unlike the adverb ?bu4? we discussed in Sec-
tion 3.2, the ordinary adverbial modifier comes
directly before the verb it modifies both in Chi-
nese and Japanese, but not in English. Nev-
ertheless, in accordance with the principle of
identifying the head for Chinese, the adverb
will be treated as the dependent and it will
not be reordered following the verb it modi-
fied. As a result, the alignment between adverbs
and verbs is non-monotonic. This can be ob-
served in the Chinese sentence ?guojia (coun-
try) yanli (severely) chufa (penalize) jiage
(price) weifa (violation) xingwei (behavior)?8,
and its Japanese translation: ?kuni (country) wa
kakaku (price) no ihou (violation) koui (be-
havior) wo kibisiku (severely) syobatu (penal-
ize).? Both in Chinese and Japanese, the adverbial
modifier ?yanli? and ?kibisiku? are directly
in front of the verb ?chufa? and ?syobatu?, re-
spectively. However, the verb in Chinese is identi-
fied as the head and will be reordered to the end of
the sentence without the adverb.
8English translation: The country severely penalizes vio-
lations of price restrictions.
64
5.5 POS tagging and Parsing Errors
There were word reordering issues not caused
solely by differences in syntactic structures. Here
we summarize two that are difficult to remedy dur-
ing reordering and that are hard to avoid since re-
ordering rules are highly dependent on the tagger
and parser.
? POS tagging errors
In Chinese, for example, the word ?Iran?
was tagged as ?VV? or ?JJ? instead of ?NR?.
This led to identifying ?Iran? as a head in
accordance with the head definition in Chi-
nese, and it was reordered undesirably.
? Parsing errors
For example, in the Chinese verb phrase
?touzi (invest) 20 yi (200 million)
meiyuan (dollars)?, ?20? and ?yi? were
identified as dependent of ?touzi? and
?meiyuan?, respectively, which led to an
unsuitable reordering for posterior word
alignment.
6 Conclusion and Future Work
In the present work, we have proposed novel
Chinese-to-Japanese reordering rules inspired
in (Isozaki et al, 2010b) based on linguistic analy-
sis on Chinese HPSG and differences among Chi-
nese and Japanese. Although a simple implemen-
tation of HF to reorder Chinese sentences per-
forms well, translation quality was substantially
improved further by including linguistic knowl-
edge into the refinement of the reordering rules.
In Section 5, we found more patterns on reorder-
ing issues when reordering Chinese sentences to
resemble Japanese word order. The extraction of
those patterns and their effective implementation
may lead to further improvements in translation
quality, so we are planning to explore this possi-
bility.
In this work, syntactic information from a deep
parser has been used to reorder words better. We
believe that using semantic information can fur-
ther increase the expressive power of reordering
rules. With that objective, Chinese Enju can be
used since it provides the semantic head of nodes
and can interpret sentences by using their semantic
dependency.
Acknowledgments
This work was mainly developed during an intern-
ship at NTT Communication Science Laborato-
ries. We would like to thank Prof. Yusuke Miyao
for his invaluable support on this work.
References
P.F. Brown, S.A. Della Pietra, V.J. Della Pietra, and
R.L. Mercer. 1993. The mathematics of ma-
chine translation. In Computational Linguistics, vol-
ume 19, pages 263?311, June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, and Omar Zaidan, editors. 2010. Pro-
ceedings of the joint 5th workshop on Statistical Ma-
chine Translation and MetricsMATR. Association
for Computational Linguistics, July.
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing Chinese word seg-
mentation for machine translation performance. In
Proceedings of the 3rd Workshop on SMT, pages
224?232, Columbus, Ohio. Association for Compu-
tational Linguistics.
Stanley F. Chen and Joshua Goodman. 1999. An
empirical study of smoothing techniques for lan-
guage modeling. Computer Speech and Language,
4(13):359?393.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
ACL ?05, pages 531?540, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Vivian James Cook and Mark Newson. 1988. Chom-
sky?s Universal Grammar: An introduction. Oxford:
Basil Blackwell.
Naoki Fukui. 1992. Theory of Projection in Syntax.
CSLI Publisher and Kuroshio Publisher.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. Whats in a translation rule?
In Proceedings of HLT-NAACL.
Qian Gao. 2008. Word order in mandarin: Reading and
speaking. In Proceedings of the 20th North Ameri-
can Conference on Chinese Linguistics (NACCL-20),
volume 2, pages 611?626.
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine transla-
tion. In Proceedings of the 23rd International Con-
ference on Computational Linguistics, COLING ?10,
65
pages 376?384, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Yuqing Guo. 2009. Treebank-based acquisition of
Chinese LFG resources for parsing and generation.
Ph.D. thesis, Dublin City University.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010a. Automatic
evaluation of translation quality for distant language
pairs. In Proceedings of Empirical Methods on Nat-
ural Language Processing (EMNLP).
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010b. Head finalization: A simple re-
ordering rule for sov languages. In Proceedings of
WMTMetricsMATR, pages 244?251.
P. Koehn, F. J. Och, and D. Marcu. 2003. Sta-
tistical phrase-based translation. In Proceedings
HLT/NAACL?03, pages 48?54.
Philipp Koehn et al 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings
of the ACL Demo and Poster Sessions, 2007, pages
177?180, June 25?27.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic hpsg parsing. Computa-
tional Linguistics, 34:35?80, March.
F. J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics.
Franz J. Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of the
41st annual conference of the Association for Com-
putational Linguistics, 2003, pages 160?167, July 7?
12.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic eval-
uation of machine translation. In Proceedings of the
40th annual conference of the Association for Com-
putational Linguistics, 2002, pages 311?318, July 6?
12.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas, pages 223?231.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of the 7th
international conference on Spoken Language Pro-
cessing, 2002, pages 901?904, September 16?20.
Christoph Tillmann. 2004. A unigram orientation
model for statistical machine translation. In Pro-
ceedings of HLT-NAACL 2004: Short Papers, HLT-
NAACL-Short ?04, pages 101?104, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings OF HLT-NAACL, pages 252?259.
Chao Wang, Michael Collins, and Philipp Koehn.
2007. Chinese syntactic reordering for statistical
machine translation. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 737?
745, Prague, Czech Republic, June. Association for
Computational Linguistics.
Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Hajime
Tsukada, and Masaaki Nagata. 2011. Extracting
pre-ordering rules from predicate-argument struc-
tures. In Proceedings of 5th International Joint Con-
ference on Natural Language Processing, pages 29?
37, Chiang Mai, Thailand, November. Asian Feder-
ation of Natural Language Processing.
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical mt system with automatically learned rewrite
patterns. In Proceedings of the 20th international
conference on Computational Linguistics, COLING
?04, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
smt for subject-object-verb languages. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
NAACL ?09, pages 245?253, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Kun Yu, Yusuke Miyao, Xiangli Wang, Takuya Mat-
suzaki, and Jun ichi Tsujii. 2010. Semi-
automatically developing chinese hpsg grammar
from the penn chinese treebank for deep parsing. In
COLING (Posters)?10, pages 1417?1425.
Kun Yu, Yusuke Miyao, Takuya Matsuzaki, Xiangli
Wang, and Junichi Tsujii. 2011. Analysis of the dif-
ficulties in chinese deep parsing. In Proceedings of
the 12th International Conference on Parsing Tech-
nologies, pages 48?57.
R. Zens, F.J. Och, and H. Ney. 2002. Phrase-based
statistical machine translation. In Proceedings of
KI?02, pages 18?32.
Hong-Mei Zhao, Ya-Juan Lv, Guo-Sheng Ben, Yun
Huang, and Qun Liu. 2011. Evaluation report
for the 7th china workshop on machine translation
(cwmt2011). The 7th China Workshop on Machine
Translation (CWMT2011).
66
Proceedings of SSST-6, Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 111?118,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Zero Pronoun Resolution can Improve the Quality of J-E Translation
Hirotoshi Taira, Katsuhito Sudoh, Masaaki Nagata
NTT Communication Science Laboratories
2-4, Hikaridai, Seika-cho, Keihanna Science City
Kyoto 619-0237, Japan
{taira.hirotoshi,sudoh.katsuhito,nagata.masaaki}@lab.ntt.co.jp
Abstract
In Japanese, particularly, spoken Japanese,
subjective, objective and possessive cases are
very often omitted. Such Japanese sentences
are often translated by Japanese-English sta-
tistical machine translation to the English sen-
tence whose subjective, objective and posses-
sive cases are omitted, and it causes to de-
crease the quality of translation. We per-
formed experiments of J-E phrase based trans-
lation using Japanese sentence, whose omitted
pronouns are complemented by human. We
introduced ?antecedent F-measure? as a score
for measuring quality of the translated En-
glish. As a result, we found that it improves
the scores of antecedent F-measure while the
BLEU scores were almost unchanged. Every
effectiveness of the zero pronoun resolution
differs depending on the type and case of each
zero pronoun.
1 Introduction
Today, statistical translation systems have been able
to translate between languages at high accuracy us-
ing a lot of corpora . However, the quality of trans-
lation of Japanese to English is not high compar-
ing with the other language pairs that have the sim-
ilar syntactic structure such as the French-English
pair. Particularly, the quality of translation from
spoken Japanese to English is in low. There are
many reasons for the low quality. One is the dif-
ferent syntactic structures, that is, Japanese sentence
structure is SOV while English one is SVO. This
problem has been partly solved by head finalization
techniques (Isozaki et al, 2010). Another big prob-
lem is that subject, object and possessive cases are
often eliminated in Japanese, particularly, spoken
Japanese (Nariyama, 2003). In the case of Japanese
to English translation, the source language has lesser
information in surface than the target language, and
the quality of the translation tends to be low. We
show the example of the omissions in Fig 1. In this
example, the Japanese subject watashi wa (?I?) and
the object anata ni (?to you?) are eliminated in the
sentence. These omissions are not problems for hu-
man speakers and hearers because people easily rec-
ognize who is the questioner or responder (that is,
?I? and ?you?) from the context. However, gener-
ally speaking, the recognition is difficult for statisti-
cal translation systems.
Some European languages allow the elimination
of subject. We show an example in Spanish in Fig 2.
In this case, the subject is eliminated, and it leaves
traces including the case and the sex, on the related
verb. The Spanish word, tengo is the first person
singular form of the verb, tener (it means ?have?).
So it is easier to resolve elimination comparing with
Japanese one for SMT.
Otherwise, Japanese verbs usually have no inflec-
tional form depending on the case and sex. So,
we need take another way for elimination resolu-
tion. For example, if the eliminated Japanese sub-
ject is always ?I? when the sentence is declara-
tive, and the subject is always ?you? when the sen-
tence is a question sentence, phrase based transla-
tion systems are probably able to translate subject-
eliminated Japanese sentences to correct English
sentences. However, the hypothesis is not always
111
Jpn: (watashi wa)  (anata ni) shoushou ukagai tai  koto ga ari masu .
Eng:   I have     some  questions    to        ask     to   you    .
Omission of
subject
Omission of
object
Figure 1: Example of Japanese Ellipsis (Zero Pronoun)
Spa: (yo) Tengo   algunas preguntas  para  hacerle a  usted  .
Eng:   I have     some  questions    to        ask     to   you    .
Omission of subject
Figure 2: Spanish Ellipsis
true.
In this paper, we show that the quality of spoken
Japanese to English translation can improve using
a phrase-based translation system if we can use an
ideal elimination resolution system. However, we
also show that a simple elimination resolution sys-
tem is not effective to the improvement and it is nec-
essary to recognize correctly the modality of the sen-
tence.
2 Previous Work
There are a few researches for adaptation of ellip-
sis resolution to statistical translation systems while
there are a lot of researches for one to rule-based
translation systems in Japanese (Yoshimoto, 1988;
Dohsaka, 1990; Nakaiwa and Yamada, 1997; Ya-
mamoto et al, 1997).
As a research of SMT using elimination resolu-
tion, we have (Furuichi et al, 2011). However, the
target of the research is illustrative sentences in En-
glish to Japanese dictionary. Our research aims spo-
ken language translation and it is different from the
paper.
3 Setup of the Data of Subjects and
Objects Ellipsis in Spoken Japanese
3.1 Ellipsis Resolved Data by Human
In this section, we describe the data used in our ex-
periments. We used BTEC (Basic Travel Expres-
sion Corpus) corpus (Kikui et al, 2003) distributed
in IWSLT07 (Fordyce, 2007). The corpus consists
of tourism-related sentences similar to those that
are usually found in phrasebooks for tourists going
abroad. The characteristics of the dataset are shown
in Table 1. We used ?train? for training, ?devset1-
3? for tuning, and ?test? for evaluation. We did not
use the ?devset4? and ?devset5? sets because of the
different number of English references.
We annotated zero pronouns and the antecedents
to the sentences by hand. Here, zero pronoun is de-
fined as an obligatory case noun phrase that is not
expressed in the utterance but can be understood
through other utterances in the discourse, context, or
out-of-context knowledge (Yoshimoto, 1988). We
annotated the zero pronouns based on pronouns in
the translated English sentences. The BTEC corpus
has multi-references in English. We first chose the
most syntactically and lexically similar translation
in the references and annotated zero pronouns in it.
Our target pronouns are I, my, me, mine, myself, we,
our, us, ours, ourselves, you, your, yourself, your-
selves, he, his, him, himself, she, her, herself, it, its,
itself, they, their, them, theirs and themselves in En-
glish. We show the distribution of the annotation
types in the test set in Table 2.
3.2 Baseline System
We also examined a simple baseline zero pronoun
resolution system for the same data. We defined
112
Table 1: Data distribution
train devset1-3 devset4 devset5 test
# of References 1 16 7 7 16
# of Source Segments 39,953 1,512 489 500 489
Japanese predicate as verb, adjective, and copula (da
form) in the experiments. If the inputted Japanese
sentence contains predicates and it does not contain
?wa? (a binding particle and a topic marker), ?mo? (a
binding particle, which means ?also? and can often
replace ?wa? and ?ga?), and ?ga? (a case particle and
subjective marker), the system regards the sentence
as a candidate sentence to solve the zero pronouns.
Then, if the candidate sentence is declarative, the
system inserts ?watashi wa (I)? when the predicate
is a verb, and ?sore wa (it)? when the predicate is a
adjective or a copula. In the same way, if the candi-
date sentence is a question, the system inserts ?anata
wa (you)? when the predicate is a verb, and ?sore wa
(it)? when the predicate is a adjective or a copula.
These inserted position is the beginning of the sen-
tence. In the case that the sentence is imperative, the
system does not solve the zero pronouns (Fig. 3).
4 Experiments
4.1 Experimental Setting
Fig. 4 shows the outline of the procedure of our ex-
periment. We used Moses (Koehn et al, 2007) for
the training of the translation and language models,
tuning with MERT (Och, 2003) and the decoding.
First, we prepared the data for learning which con-
sists of parallel English and Japanese sentences. We
used MeCab 1 as Japanese tokenizer and the tok-
enizer in Moses Tool kit as English tokenizer. We
used default settings for the parameters of Moses.
Next, Moses learns language model and translation
model from the Japanese and English sentence pairs.
Then, the learned model was tuned by completed
sentences with MERT. and Moses decoded the com-
pleted Japanese sentences to English sentences.
4.2 Evaluation Method
We used BLEU (Papineni et al, 2002) and an-
tecedent Precision, Recall and F-measure for the
1http://mecab.sourceforge.net/
evaluation of the performances, comparing the sys-
tem outputs with the English references of test data.
Using only BLEU score is not adequate for evalua-
tion of pronoun translation (Hardmeier et al, 2010).
We were inspired empty node recovery evaluation
by (Johnson, 2002) and defined antecedent Preci-
sion (P), Recall (R) and F-measure (F) as follows,
P =
|G ? S|
|S|
R =
|G ? S|
|G|
F =
2PR
P +R
Here, S is the set of each pronoun in English
translated by decoder, G is the set of the gold stan-
dard zero pronoun.
We evaluated the effect of performance of every
case among completed sentences by human, ones by
the baseline system, and the original sentences.
4.3 Experimental Result
We show the BLEU scores in Table 3. and the an-
tecedent precision, recall and F-measure in Table 4.
The BLEU scores for experiments using our base-
line system and human annotation, are slightly bet-
ter than for one without ellipsis resolution, 45.4%
and 45.6%, respectively. However, the scores of an-
tecedent F-measure have major difference between
?original? and ?human?. Particularly, the recall is im-
proved. Each 1st, 2nd and 3rd person score is better
than original one.
5 Discussion and Conclusion
We performed experiments of J-E phrase based
translation using Japanese sentences, whose omit-
ted pronouns are complemented by human and a
baseline system. Using ?antecedent F-measure? as a
score for measuring the quality of the translated En-
glish, it improves the score of antecedent F-measure.
Every effectiveness of the zero pronoun resolution
113
ano eiga-wo mimashita.
the movie-OBJ     watched
Declarative sentence
Watashi-wa ano eiga-wo mimashita.
I-TOP the     movie-OBJ     watched
(=  ?I watched the movie.? )   
Question sentence
ano eiga-wo mimashita ka ?
the    movie-OBJ     watched       QUES  ?
Anata-wa ano eiga-wo mimashita ka ?
You-TOP the     movie-OBJ     watched      QUES  ?
(=  ?Did you watch the movie?? )   
Imperative sentence
ano eiga-wo minasai.
the    movie-OBJ     watch-IMP
ano eiga-wo minasai.
the    movie-OBJ     watch-IMP
(=  ?Watch the movie.? )   
Figure 3: Our baseline system of zero pronoun resolution
differed, depending on the type and case of each zero
pronoun. The F-measures for the first person pro-
noun were smaller than expected ones, Rather, the
scores for and possessive pronouns second person
were greater (Table. 3).
We show a better, a worse, and an unchanged
cases of translation using the baseline system of
the elimination resolution in Fig. 5. The left-hand
is the result of the alignment between the origi-
nal Japanese sentence and the decoded English sen-
tence. The right-hand is the result of one using
the Japanese the baseline system solved zero pro-
nouns. In the ?better? case, the alignment of todoke-
te (send) is better than one of the original sen-
tence, and ?Can you? is compensated by the solved
zero pronoun anata-wa (you-TOP). Otherwise, in
the ?worse? case, our baseline system could not rec-
ognize that the sentence is imperative, and inserted
watashi-wa (I-TOP) incorrectly into the sentence. It
indicates that we need a highly accurate recogni-
tion of the modalities of sentences for more correct
completion of the antecedent of zero pronouns. In
the ?unchanged? case, the translation results are the
same. However, the alignment of the right-hand is
more correct than one of the left-hand.
References
Kohji Dohsaka. 1990. Identifying the referents of zero-
pronouns in japanese based on pragmatic constraint in-
terpretation. In Proceedings of ECAI, pages 240?245.
C.S. Fordyce. 2007. Overview of the iwslt 2007 eval-
uation campaign. In Proceedings of the International
Workshop on Spoken Language Translation, pages 1?
12.
M. Furuichi, J. Murakami, M. Tokuhisa, and M. Murata.
2011. The effect of complement subject in japanese
to english statistical machine translation (in Japanese).
In Proceedings of the 17th Annual Meeting of The
114
English
Parallel Corpus for Training
Japanese
Shoushou ukagai tai koto ga ari masu ga.?
I have some questions to ask .
Decoder ?Moses?
Parallel Corpus for Test
Completed Sentences
honkon  ryokou ni tsuite 
siri  tain  desu  ga.
exo1 wa  honkon  ryokou ni tsuite 
siri  tain  desu  ga.
System Output
Training
Translation Model
Language Model
Decoding
I?d like to know about
the Hong Kong trip.
English
I would like to know about
the Hong Kong trip.
Evaluation
Japanese
- - - -
- - - -
- - - -
- - - -
Zero pronoun annotation by hand
or baseline system 
Tuning
Japanese English
Parallel Corpus for Tuning
- - - - - - - -
Zero pronoun annotation 
by hand or baseline system 
Completed Sentences
- - - - - - - -
Figure 4: Outline of the experiment
Association for Natural Language Processing (NLP-
2012).
C. Hardmeier, M. Federico, and F.B. Kessler. 2010.
Modelling pronominal anaphora in statistical machine
translation. In Proceedings of the seventh Inter-
national Workshop on Spoken Language Translation
(IWSLT), pages 283?289.
H. Isozaki, K. Sudoh, H. Tsukada, and K. Duh. 2010.
Head finalization: A simple reordering rule for sov
languages. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metrics-
MATR, pages 244?251. Association for Computational
Linguistics.
Mark Johnson. 2002. A simple pattern-matching al-
gorithm for recovering empty nodes and their an-
tecedents. In Proceedings of 40th Annual Meeting of
the Association for Computational Linguistics, pages
136?143, Philadelphia, Pennsylvania, USA, July. As-
sociation for Computational Linguistics.
G. Kikui, E. Sumita, T. Takezawa, and S. Yamamoto.
2003. Creating corpora for speech-to-speech transla-
tion. In Proceedings of EUROSPEECH, pages 381?
384.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: open source
toolkit for statistical machine translation. In Proc.
of the 45th Annual Conference of the Association for
115
Better
worse
Unchanged 
map-BY   point_out would       QUES
chizu-de   sashi-te morae-masu ka.
Would you point them out on this map ?
You-TOP map-BY   point_out would     QUES
anata-wa chizu-de   sashi-te morae-masu ka.
Would you point them out on this map ?
Hurry up 
Isoi-de  .
Hurry up .
(Ref)  Hurry up.
I-TOP    hurry up 
watashi-wa Isoi-de  .
I  ?m  in a hurry .
(Ref)  Would you point one out on this map?
Today?s    evening       by      send          would           QUES 
Kyou-no  yuugata made-ni todoke-te morae-masu ka .
It   by  this  evening  ?
(Ref) Can you deliver them by this evening?
you-TOP Today?s    evening       by      send          would           QUES 
anata-wa kyou-no  yuugata made-ni todoke-te morae-masu ka .
Can you   send it   by   this evening  ?
Figure 5: Effectiveness of zero pronoun resolution for decoding
Computational Linguistics (ACL-07), Demonstration
Session, pages 177?180.
H. Nakaiwa and S. Yamada. 1997. Automatic identifi-
cation of zero pronouns and their antecedents within
aligned sentence pairs. In Proc. of the 3rd Annual
Meeting of the Association for Natural Language Pro-
cessing.
S. Nariyama. 2003. Ellipsis and reference tracking
in Japanese, volume 66. John Benjamins Publishing
Company.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proc. of the ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eval-
uation of machine translation. In Proc. of the 40th An-
nual Conference of the Association for Computational
Linguistics (ACL-02).
K. Yamamoto, E. Sumita, O. Furuse, and H. Iida. 1997.
Ellipsis resolution in dialogues via decision-tree learn-
ing. In Proc. of NLPRS, volume 97. Citeseer.
K. Yoshimoto. 1988. Identifying zero pronouns in
japanese dialogue. In Proceedings of the 12th con-
ference on Computational linguistics-Volume 2, pages
779?784. Association for Computational Linguistics.
116
Table 2: The Type Distributions of Zero Pronouns in Test Set
Type Pronoun #
First personal pronoun i 121
my 39
me 32
mine 1
myself 0
we 7
our 2
us 2
ours 0
ourselves 0
total 204
Second personal pronoun you 95
your 23
yours 0
yourself 0
yourselves 0
total 118
Third personal pronoun he 1
his 0
him 0
himself 0
she 0
her 2
hers 0
herself 0
it 51
its 0
itself 0
they 2
their 0
them 5
theirs 0
themselves 0
total 61
all total 383
Table 3: BLEU score
BLEU F(Avg.) P R F (1st person) F (2nd person) F (3rd person)
original 45.1 59.7 63.8 56.1 61.6 59.9 52.3
baseline 45.4 58.5 64.1 53.7 61.2 59.2 47.7
human 45.6 71.8 67.5 76.7 70.6 77.6 63.7
117
Table 4: Antecedent precision, recall and F-measure for every pronoun
i (ref:121) my (ref:39) me (ref:32)
BLEU P R F P R F P R F
original 45.1 56.8 51.2 53.9 55.5 51.2 53.3 58.0 56.2 57.1
baseline 45.4 51.8 46.2 48.9 67.8 48.7 56.7 66.6 50.0 57.1
human 45.6 50.9 68.6 58.4 65.2 76.9 70.5 61.2 59.3 60.3
we (ref:7) our (ref:2) us (ref:2)
P R F P R F P R F
original 20.0 14.2 16.6 100.0 50.0 66.6 0.00 0.00 0.00
baseline 25.0 14.2 18.1 100.0 50.0 66.6 0.00 0.00 0.00
human 40.0 28.5 33.3 100.0 50.0 66.6 0.00 0.00 0.00
you (ref:95) your (ref:23)
P R F P R F
original 55.3 54.7 55.0 80.0 52.1 63.1
baseline 57.1 54.7 55.9 58.8 43.4 50.0
human 68.4 80.0 73.7 73.0 82.6 77.5
it (ref:51) its (ref:0)
P R F P R F
original 56.1 45.1 50.0 0.00 0.00 0.00
baseline 51.2 41.1 45.6 0.00 0.00 0.00
human 58.3 54.9 56.5 0.00 0.00 0.00
they (ref:2) their (ref:0) them (ref:5)
P R F P R F P R F
original 100.0 50.0 66.6 0.00 0.00 0.00 0.00 0.00 0.00
baseline 100.0 50.0 66.6 0.00 0.00 0.00 0.00 0.00 0.00
human 58.3 54.9 56.5 0.00 0.00 0.00 0.00 0.00 0.00
118
Proceedings of the Second Workshop on Hybrid Approaches to Translation, pages 25?33,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Using Unlabeled Dependency Parsing for Pre-reordering
for Chinese-to-Japanese Statistical Machine Translation
Dan Han1,2 Pascual Mart??nez-Go?mez2,3 Yusuke Miyao1,2
Katsuhito Sudoh4 Masaaki Nagata4
1The Graduate University For Advanced Studies
2National Institute of Informatics, 3The University of Tokyo
4NTT Communication Science Laboratories, NTT Corporation
{handan,pascual,yusuke}@nii.ac.jp
{sudoh.katsuhito,nagata.masaaki}@lab.ntt.co.jp
Abstract
Chinese and Japanese have a different sen-
tence structure. Reordering methods are
effective, but need reliable parsers to ex-
tract the syntactic structure of the source
sentences. However, Chinese has a loose
word order, and Chinese parsers that ex-
tract the phrase structure do not perform
well. We propose a framework where only
POS tags and unlabeled dependency parse
trees are necessary, and linguistic knowl-
edge on structural difference can be en-
coded in the form of reordering rules. We
show significant improvements in transla-
tion quality of sentences from news do-
main, when compared to state-of-the-art
reordering methods.
1 Introduction
Translation between Chinese and Japanese lan-
guages gains interest as their economic and polit-
ical relationship intensifies. Despite their linguis-
tic influences, these languages have different syn-
tactic structures and phrase-based statistical ma-
chine translation (SMT) systems do not perform
well. Current word alignment models (Och and
Ney, 2003) account for local differences in word
order between bilingual sentences, but fail at cap-
turing long distance word alignments. One of
the main problems in the search of the best word
alignment is the combinatorial explosion of word
orders, but linguistically-motivated heuristics can
help to guide the search.
This work explores syntax-informed pre-
reordering for Chinese; that is, we obtain syntactic
structures of Chinese sentences, reorder the words
to resemble the Japanese word order, and then
translate the reordered sentences using a phrase-
based SMT system. However, Chinese parsers
have difficulties in extracting reliable syntactic in-
formation, mainly because Chinese has a loose
word order and few syntactic clues such as inflec-
tion and function words.
On one hand, parsers implementing head-driven
phrase structure grammars infer a detailed con-
stituent structure, and such a rich syntactic struc-
ture can be exploited to design well informed re-
ordering methods. However, inferring abundant
syntactic information often implies introducing er-
rors, and reordering methods that heavily rely on
detailed information are sensitive to those parsing
errors (Han et al, 2012).
On the other hand, dependency parsers are com-
mitted to the simpler task of finding dependency
relations and dependency labels, which can also be
useful to guide reordering (Xu et al, 2009). How-
ever, reordering methods that rely on those depen-
dency labels will also be prone to errors, specially
in the case of Chinese since it has a richer set of
dependency labels when compared to other lan-
guages. Since improving parsers for Chinese is
challenging, we thus aim at reducing the influence
of parsing errors in the reordering procedure.
We present a hybrid approach that boosts the
performance of phrase-based SMT systems by
pre-reordering the source language using unla-
beled parse trees augmented with constituent
information derived from Part-of-Speech tags.
Specifically, we propose a framework to pre-
reorder a Subject-Verb-Object (SVO) language,
in order to improve its translation to a Subject-
Object-Verb (SOV) language, where the only re-
quired syntactic information are POS tags and un-
labeled dependency parse trees. We test the per-
formance of our pre-reordering method and com-
pare it to state-of-the-art reordering methods in the
news domain for Chinese.
In the next section, we describe similar work on
pre-reordering methods for language pairs that in-
25
volve either Chinese or Japanese, and explain how
our method builds upon them. From a linguis-
tic perspective, we describe in section 3 our ob-
servations of reordering issues between Chinese
and Japanese and detail how our framework solves
those issues. In section 4 we assess to what extent
our pre-reordering method succeeds in reordering
words in Chinese sentences to resemble the order
of Japanese sentences, and measure its impact on
translation quality. The last section is dedicated to
discuss our findings and point to future directions.
2 Related Work
Although there are many works on pre-reordering
methods for other languages to English translation
or inverse (Xia and McCord, 2004; Xu et al, 2009;
Habash, 2007; Wang et al, 2007; Li et al, 2007;
Wu et al, 2011), reordering method for Chinese-
to-Japanese translation, which is a representative
of long distance language pairs, has received little
attention.
The most related work to ours is in (Han et al,
2012), in which the authors introduced a refined
reordering approach by importing an existing re-
ordering method for English proposed in (Isozaki
et al, 2010b). These reordering strategies are
based on Head-driven phrase structure grammars
(HPSG) (Pollard and Sag, 1994), in that the re-
ordering decisions are made based on the head of
phrases. Specifically, HPSG parsers (Miyao and
Tsujii, 2008; Yu et al, 2011) are used to extract the
structure of sentences in the form of binary trees,
and head branches are swapped with their depen-
dents according to certain heuristics to resemble
the word order of the target language. However,
those strategies are sensitive to parsing errors, and
the binary structure of their parse trees impose
hard constraints in sentences with loose word or-
der. Moreover, as Han et al (2012) noted, reorder-
ing strategies that are derived from the HPSG the-
ory may not perform well when the head definition
is inconsistent in the language pair under study. A
typical example for the language pair of Chinese
and Japanese that illustrates this phenomenon is
the adverb ?bu4?, which is the dependent of its
verb in Chinese but the head in Japanese.
The work in (Xu et al, 2009) used an English
dependency parser and formulated handcrafted re-
ordering rules with dependency labels, POS tags
and weights as triplets and implemented them re-
cursively into sentences. This design, however,
limited the extensibility of their method. Our ap-
proach follows the idea of using dependency tree
structures and POS tags, but we discard the infor-
mation on dependency labels since we did not find
them informative to guide our reordering strate-
gies in our preliminary experiments, partly due to
Chinese showing less dependencies and a larger
label variability (Chang et al, 2009).
3 Methodology
In Subject-Verb-Object (SVO) languages, objects
usually follow their verbs, while in Subject-
Object-Verb (SOV) languages, objects precede
them. Our objective is to reorder words in Chinese
sentences (SVO) to resemble the word order of
Japanese sentences (SOV). For that purpose, our
method consists in moving verbs to the right-hand
side of their objects. However, it is challenging
to correctly identify the appropriate verbs and ob-
jects that trigger a reordering, and this section will
be dedicated to that end.
More specifically, the first step of our method
consists in identifying the appropriate verb (and
certain words close to it) that need to be moved to
the right-hand side of its object argument. Verbs
(and those accompanying words) will move as a
block, preserving the relative order among them.
We will refer to them as verbal blocks (Vbs). The
second step will consist in identifying the right-
most argument object of the verb under considera-
tion, and moving the verbal block to the right-hand
side of it. Finally, certain invariable grammatical
particles in the original vicinity of the verb will
also be reordered, but their positions will be de-
cided relative to their verb.
In what follows, we describe in detail how to
identify verbal blocks, their objects and the invari-
able grammatical particles that will play a role in
our reordering method. As mentioned earlier, the
only information that will be used to perform this
task will be the POS tags of the words and their
unlabeled dependency structures.
3.1 Identifying verbal blocks (Vbs)
Verbal blocks are composed of a head (Vb-H)
and possibly accompanying dependents (Vb-D).
In the Chinese sentence ?wo3 (I) chi1 le5 (ate) li2
(pear).?1, ?chi1? refers to the English verb ?eat?
1In this paper, we represent a Chinese character by using
Pinyin plus a tone number (there are 5 tones in Chinese). In
the example, ?chi1(eat)? is a verb and ?le5(-ed)? is an aspect
particle that adds preterit tense to the verb.
26
Vb-H VV VE VC VA P
Vb-D AD AS SP MSP CC VV VE VC VA
BEI LB SB
RM-D NN NR NT PN OD CD M FW CC
ETC LC DEV DT JJ SP IJ ON
Oth-DEP LB SB CS
Table 1: Lists of POS tags in Chinese used to iden-
tify blocks of words to reorder (Vb-H, Vb-D, BEI
lists), the POS tags of their dependents (RM-D
lists) which indicate the reordering position, and
invariable grammatical particles (Oth-DEP) that
need to be reordered.
and the aspect particle ?le5? adds a preterit tense
to the verb. The words ?chi1 le5? are an example
of verbal block that should be reordered as a block
without altering its inner word order, i.e. ?wo3
(I) li2 (pear) chi1 le5 (ate).?, which matches the
Japanese SOV order.
Possible heads of verbal blocks (Vb-H) are
verbs (words with POS tags VV, VE, VC and VA),
or prepositions (words with POS tag P). The Vb-H
entry of Table 1 contains the list of POS tags for
heads of verbal blocks. We use prepositions for
Vb-H identification since they behave similarly to
verbs in Chinese and should be moved to the right-
most position in a prepositional phrase to resemble
the Japanese word order. There are three condi-
tions that a word should meet to be considered as
a Vb-H:
i) Its POS tag is in the set of Vb-H in Table 1.
ii) It is a dependency head, which indicates that
it may have an object as a dependent.
iii) It has no dependent whose POS tag is in the
set of BEI in Table 1. BEI particles indicate
that the verb is in passive voice and should
not be reordered since it already resembles
the Japanese order.
Chinese language does not have inflection, con-
jugation, or case markers (Li and Thompson,
1989). For that reason, some adverbs (AD), as-
pect particles (AS) or sentence-final particles (SP)
are used to signal modality, indicate grammati-
cal tense or add aspectual value to verbs. Words
in this category preserve the order when translat-
ing to Japanese, and they will be candidates to be
part of the verbal block (Vb-D) and accompany
the verb when it is reordered. Other words in this
category are coordinating conjunctions (CC) that
connect multiple verbs, and both resultative ?de5?
(DER) and manner ?de5? (DEV). The full list of
POS tags used to identify Vb-Ds can be found in
Table 1. To be a Vb-D, there are three necessary
conditions as well:
i) Its POS tag is in the Vb-D entry in Table 1.
ii) It is a dependent of a word that is already in
the Vb.
iii) It is next to its dependency head or only a
coordination conjunction is in between.
To summarize, to build verbal blocks (Vbs) we
first find the words that meet the three Vb-H con-
ditions. Then, we test the Vb-D conditions on the
words adjacent to the Vb-Hs and extend the verbal
blocks to them if they meet the conditions. This
process is iteratively applied to the adjacent words
of a block until no more words can be added to the
verbal block, possibly nesting other verbal blocks
if necessary.
Figure 1a 2 shows an example of a dependency
tree of a Chinese sentence that will be used to il-
lustrate Vb identification. By observing the POS
tags of the words in the sentence, only the words
?bian1 ji4 (edit)? and ?chu1 ban3 (publish)? have
a POS tag (i.e. VV) in the Vb-H entry of Table 1.
Moreover, both words are dependency heads and
do not have any dependent whose POS tag is in
the BEI entry of Table 1. Thus, ?bian1 ji4 (edit)?
and ?chu1 ban3 (publish)? will be selected as Vb-
Hs and form, by themselves, two separate incipi-
ent Vbs. We arbitrarily start building the Vb from
the word ?chu1 ban3 (publish)?, by analyzing its
adjacent words that are its dependents.
We observe that only ?le5 (-ed)? is adjacent to
?chu1 ban3 (publish)?, it is its dependent, and its
POS tag is in the Vb-D list. Since ?le5 (-ed)?
meets all three conditions stated above, ?le5 (-ed)?
will be included in the Vb originated by ?chu1
ban3 (publish)?. The current Vb thus consists of
the sequence of tokens ?chu1 ban3 (publish)? and
?le5 (-ed)?, and the three conditions for Vb-D are
tested on the adjacent words of this block. Since
the adjacent words (or words separated by a coor-
dinating conjunction) do not meet the conditions,
the block is not further extended. Figure 1b shows
the dependency tree where the Vb block that con-
sists of the words ?chu1 ban3 (publish)? and ?le5
(-ed)? is represented by a rectangular box.
By checking in the same way, there are three
dependents that meet the requirements of being
2For all the dependency parsing trees in this paper, arrows
are pointing from heads to their dependents.
27
..xue2 xiao4 .yi3 jing1 .bian1 ji4 .he2 .chu1 ban3 .le5 .yi1 .ben3 .shu1 ..?? .?? .?? .? .?? .? .? .? .? .?.School .has already .edit (-ed) .and .publish .-ed .a . .book.NN .AD .VV .CC .VV .AS .CD .M .NN .PU
.ROOT.o .o
.o.o .o .o .o .o .o
(a) Original dependency tree
..xue2 xiao4 .yi3 jing1 .bian1 ji4 .he2 .chu1 ban3 .le5 .yi1 .ben3 .shu1 ..?? .?? .?? .? .?? .? .? .? .? .?.School .has already .edit (-ed) .and .publish .-ed .a . .book
.NN .AD .VV .CC .VV .AS .CD .M .NN .PU
.ROOT .o.o .o .o.o
(b) Vbs in rectangular boxes
..xue2 xiao4 .yi3 jing1 .bian1 ji4 .he2 .chu1 ban3 .le5 .yi1 .ben3 .shu1 ..?? .? .? .? .?? .?? .? .?? .? .?.School .a . .book .has already .edit (-ed) .and .publish .-ed
(c) Merged and reordered Vb
Figure 1: An example that shows how to de-
tect and reorder a Verbal block (Vb) in a sen-
tence. In the first two figures 1a and 1b, Chi-
nese Pinyin, Chinese tokens, word-to-word En-
glish translations, and POS tags of each Chinese
token are listed in four lines. In Figure 1c, there
are Chinese Pinyin, reordered Chinese sentence
and its word-to-word English counterpart.
Vb-Ds for ?bian1 ji4 (edit)?: ?yi3 jing1 (has al-
ready)?, ?he2 (and)? and ?chu1 ban4 (publish)?
and hence this Vb consists of three tokens and one
Vb. The outer rectangular box in Figure 1b shows
that the Vb ?bian1 ji4 (edit)? as the Vb-H. Fig-
ure 1c shows an image of how this Vb will be
reordered while the inner orders are kept. Note
that the order of building Vbs from which Vb-Hs,
?chu1 ban3 (publish)? or ?bian1 ji4 (edit)? will not
affect any change of the final result.
3.2 Identifying objects
In the most general form, objects are dependents
of verbal blocks3 that act as their arguments.
While the simplest objects are nouns (N) or pro-
nouns (PN), they can also be comprised of noun
phrases or clauses (Downing and Locke, 2006)
such as nominal groups, finite clauses (e.g. that
clauses, wh-clauses) or non-finite clauses (e.g. -
ing clauses), among others.
For every Vb in a verb phrase, clause, or sen-
tence, we define the right-most object dependent
(RM-D) as the word that:
3Dependents of verbal blocks are dependents of any word
within the verbal block.
..ta1 .chi1 .le5 .wu3 fan4 . .qu4 .xue2 xiao4 ..? .? .? .?? .? .? .?? .?.he .eat .-ed .lunch .(and) .go(to) .school ..PN .VV .AS .NN .PU .VV .NN .PU
.ROOT.o .o .o.o
.o.o .o
? ?? ? ? ? ?? ? ?he lunch eat -ed school go(to)
V ?????? O V ???? OS
O ????? V O ???? VS
English Translation: He ate lunch, and went to school.
Figure 2: An example of a Chinese sentence with
a coordination of verb phrases as predicate. Sub-
ject(S), verbs(V), and objects(O) are displayed for
both verb phrases. Lines between the original Chi-
nese sentence and the reordered Chinese sentence
indicate the reordering trace of Verbal blocks(Vb).
i) its POS tag is in the RM-D entry of Table 1,
ii) its dependency head is inside of the verbal
block, and
iii) is the right-most object among all objects of
the verbal block.
All verbal blocks in the phrase, clause, or sen-
tence will move to the right-hand side of their cor-
respondent RM-Ds recursively. Figure 1b and Fig-
ure 1c show a basic example of object identifica-
tion. The Chinese word corresponding to ?shu1
(book)? is a dependent of a word within the verbal
block and its POS tag is within the RM-D entry
list of Table 1 (i.e. NN). For this reason, ?shu1
(book)? is identified as the right-most dependent
of the verbal block (Vb), and the Vb will move to
the right-hand side of it to resemble the Japanese
word order.
A slightly more complex example can be found
in Figure 2. In this example, there is a coordina-
tion structure of verb phrases, and the dependency
tree shows that the first verb, ?chi1 (eat)?, ap-
pears as the dependency head of the second verb,
?qu4 (go)?. The direct right-most object depen-
dent (RM-D) of the first verb, ?chi1 (eat)?, is the
word ?wu3 fan4 (lunch)?, and the verb ?chi1 (eat)?
will be moved to the right-hand side of its object
dependent.
There are cases, however, where there is no co-
ordination structure of verb phrases but a simi-
lar dependency relation occurs between two verbs.
Figure 3 illustrates one of these cases, where the
main verb ?gu3 li4 (encourage)? has no direct de-
28
..xue2 xiao4 .gu3 li4 .xue2 sheng1 .can1 yu3 .she4 hui4 .shi2 jian4 ..?? .?? .?? .?? .?? .?? .?.school .encourage .student .participate .social .practice.NN .VV .NN .VV .NN .NN .PU
.o .ROOT
.o
.o.o .o .o
?? ?? ?? ?? ?? ?? ?school student social practice participate encourage
S ???? V ?????? O
S ???? V ?????????? O
S ?????? O ??????? V
S ???????????? O ??????????? VEnglish Translation: School encourages student to participate in social practice.
Figure 3: An example of a Chinese sentence in
which an embedded clause appears as the object
of the main verb. Subjects (S), verbs (V), and ob-
jects (O) are displayed for both the sentence and
the clause. Lines between the original Chinese
sentence and the reordered Chinese sentence in-
dicate the reordering trace of Verbal blocks (Vb).
pendent that can be considered as an object since
no direct dependent has a POS tag in the RM-D en-
try of Table 1. Instead, an embedded clause (SVO)
appears as the object argument of the main verb,
and the main verb ?gu3 li4 (encourage)? appears
as the dependency head of the verb ?can1 yu2 (par-
ticipate)?.
In the news domain, reported speech is a fre-
quent example that follows this pattern. In our
method, if the main verb of the sentence (labeled
as ROOT) has dependents but none of them is a
direct object, we move the main verb to the end of
the sentence. As for the embedded clause ?xue2
sheng1 (student) can1 yu2 (participate) she4 hui4
(social) shi2 jian4 (practice)?, the verbal block of
the clause is the word ?can1 yu2 (participate)?
and its object is ?shi2 jian4 (practice)?. Apply-
ing our reordering method, the clause order results
in ?xue2 sheng1 (student) she4 hui4 (social) shi2
jian4 (practice) can1 yu2 (participate)?. The result
is an SOV sentence with an SOV clause, which
resembles the Japanese word order.
3.3 Identifying invariable grammatical
particles
In Chinese, certain invariable grammatical parti-
cles that accompany verbal heads have a different
word order relative to their heads, when compared
to Japanese. Those particles are typically ?bei4?
particle (POS tags LB and SB) and subordinating
conjunctions (POS tag CS). Those particles appear
on the left-hand side of their dependency heads in
Chinese, and they should be moved to the right-
hand side of their dependency heads for them to
resemble the Japanese word order. Reordering in-
variable grammatical particles in our framework
can be summarized as:
i) Find dependents of a verbal head (Vb-H)
whose POS tags are in the Oth-DEP entry of
Table 1.
ii) Move those particles to the right-hand side of
their (possibly reordered) heads.
iii) If there is more than one such particle, move
them keeping the relative order among them.
3.4 Summary of the reordering framework
Based on the definitions above, our dependency
parsing based pre-reordering framework can be
summarized in the following steps:
1. Obtain POS tags and an unlabeled depen-
dency tree of a Chinese sentence.
2. Obtain reordering candidates: Vbs.
3. Obtain the object (RM-D) of each Vb.
4. Reorder each Vb in two exclusive cases by
following the order:
(a) If RM-D exists, reorder Vb to be the
right-hand side of RM-D.
(b) If Vb-H is ROOT and its RM-D does not
exist, reorder Vb to the end of the sen-
tence.
(c) If none of above two conditions is met,
no reordering happens.
5. Reorder grammatical particles (Oth-DEPs) to
the right-hand side of their corresponding
Vbs.
Note that, unlike other works in reordering dis-
tant languages (Isozaki et al, 2010b; Han et al,
2012; Xu et al, 2009), we do not prevent chunks
from crossing punctuations or coordination struc-
tures. Thus, our method allows to achieve an
authentic global reordering in reported speech,
which is an important reordering issue in news do-
mains.
In order to illustrate our method, a more compli-
cated Chinese sentence example is given in Fig-
ure 4, which includes the unlabeled dependency
29
..xin1wen2 .bao2dao3 . .sui2zhe5 .jing1ji4 .de5 .fa1zhan3 . .sheng4dan4jie2 .zhu2jian4 .jin4ru4 .le5 .zhong1guo2 . .cheng2wei2 .shang1jia1 .jia1qiang2 .li4cu4 .mai3qi4 .de5 .yi1 .ge4 .ji2ri4 ..?? .?? .? .?? .?? .? .?? .? .??? .?? .?? .? .?? .? .?? .?? .?? .?? .?? .? .? .? .?? .?.news .report . .with .economic .?s .development . .Christmas .gradually .enter .-ed .China . .become .businesses .strengthen .urge .purchase .?s .one .kind .festival ..NN .VV .PU .P .NN .DEG .NN .PU .NN .AD .VV .AS .NR .PU .VV .NN .VV .VV .NN .DEC .CD .M .NN .PU
.ROOT
.o.o .o .o .o.o.o
.o .o .o .o.o .o
.o
.o
.o .o.o .o .o .o.o.o
?? ? ?? ? ?? ?? ? ??? ?? ?? ?? ? ? ?? ?? ?? ?? ? ? ? ???? ?? ?
???? ?? ?? ?? ??? ?????? ? ??? ?? ? ??? ? ?? ? ? ?? ?? ? ?? ? ?? ? ?????? ???Entire English translation: News reports, with the economic development, Christmas has gradually entered into China, and becomes one of the festivals that businesses use to promote commerce.
Figure 4: Dependency parse tree of a complex Chinese sentence example, and word alignments for
reordered sentence with its Japanese counterpart. The first four lines are Chinese Pinyin, tokens, word-
to-word English translations, and the POS tags of each Chinese token. The fifth line shows the reordered
Chinese sentence while the sixth line is the segmented Japanese translation. The entire English transla-
tion for the sentence is showed in the last line.
parsing tree of the original Chinese sentence, and
the word alignment between reordered Chinese
sentence and its Japanese counterpart, etc.
Based on both POS tags and the unlabeled de-
pendency tree, first step of our method is to obtain
all Vbs. For all heads in the tree, according to the
definition of Vb introduced in Section 3.1, there
are six tokens which will be recognized as the can-
didates of Vb-Hs, that is ?bao4 dao3 (report)?,
?sui2 zhe5 (with)?, ?jin4 ru4 (enter)?, ?cheng2
wei2 (become)?, ?jia1 qiang2 (strengthen)?, and
?li4 cu4 (urge)?. Then, for each of the candidate,
its direct dependents will be checked if they are
Vb-Ds. For instance, for the verb of ?jin4 ru4 (en-
ter)?, its dependents of ?zhu2 jian4 (gradually)?
and ?le5 (-ed)? will be considered as the Vb-Ds.
For the case of ?jia1 qiang2 (strengthen)?, instead
of being a Vb-H, it will be recognized as Vb-D
of the Vb ?li4 cu4 (urge)? since it is one of the
direct dependents of ?li4 cu4 (urge)? with a qual-
ified POS tag for Vb-D. Therefore, there are five
Vbs in total, which are ?bao4 dao3 (report)?, ?sui2
zhe5 (with)?, ?zhu2 jian4 (gradually) jin4 ru4 (en-
ter) le5 (-ed)?, ?cheng2 wei2 (become)?, and ?jia1
qiang2 (strengthen) li4 cu4 (urge)?.
The next step is to identify RM-D for each
Vb, if there is one. By checking all conditions,
four Vbs have their RM-Ds: ?fa1 zhan3 (develop-
ment)? is the RM-D of the Vb ?sui2 zhe5 (with)?;
?zhong1 guo2 (China)? is the RM-D of the Vb
?zhu2 jian4 (gradually) jin4 ru4 (enter) le5 (-ed)?;
?jie2 ri4 (festival)? is the RM-D of the Vb ?cheng2
wei2 (become)?; ?mai3 qi4 (purchase)? is the RM-
D of the Vb ?jia1 qiang2 (strengthen) li4 cu4
(urge)?.
After obtaining all RM-Ds, we find those Vbs
that have RM-Ds and move them to right of their
RM-Ds. As for the case of ?bao4 dao3 (report)?,
since it is the root and does not have any matched
RM-D, it will be moved to the end of the sen-
tence, before any final punctuation. Finally, since
there is no any invariable grammatical particle in
the sentence that need to be reordered, reordering
has been finished. From the alignments between
the reordered Chinese and its Japanese translation
showed in the figure, an almost monotonic word
alignment has been achieved.
For comparison purposes, particle seed words
had been inserted into the reordered sentences in
the same way as the Refined-HFC method, which
is using the information of predicate argument
structure output by Chinese Enju (Yu et al, 2011).
We therefore can not entirely disclaim the use
of the HPSG parser at the present stage in our
method. However, we believe that dependency
parser can provide enough information for insert-
ing particles.
4 Experiments
We conducted experiments to assess how our pro-
posed dependency-based pre-reordering for Chi-
nese (DPC) impacts on translation quality, and
compared it to a baseline phrase-based system
and a Refined-HFC pre-reordering for Chinese to
Japanese translation.
We used two Chinese-Japanese training data
30
News CWMT+News
BLEU RIBES BLEU RIBES
Baseline 39.26 84.83 38.96 85.01
Ref-HFC 39.22 84.88 39.26 84.68
DPC 39.93 85.23 39.94 85.22
Table 3: Evaluation of translation quality of two
test sets when CWMT, News and the combination
of both corpora were used for training.
sets of parallel sentences, namely an in-house-
collected Chinese-Japanese news corpus (News),
and the News corpus augmented with the
CWMT (Zhao et al, 2011) corpus. We extracted
disjoint development and test sets from News cor-
pus, containing 1, 000 and 2, 000 sentences re-
spectively. Table 2 shows the corpora statistics.
We used MeCab 4 (Kudo and Matsumoto, 2000)
and the Stanford Chinese segmenter 5 (Chang et
al., 2008) to segment Japanese and Chinese sen-
tences. POS tags of Chinese sentences were ob-
tained using the Berkeley parser 6 (Petrov et al,
2006), while dependency trees were extracted us-
ing Corbit 7 (Hatori et al, 2011). Following the
work in (Han et al, 2012), we re-implemented
the Refined-HFC using the Chinese Enju to ob-
tain HPSG parsing trees. For comparison purposes
with the work in (Isozaki et al, 2010b), particle
seed words were inserted at a preprocessing stage
for Refined-HFC and our DPC method.
DPC and Refined-HFC pre-reordering strate-
gies were followed in the pipeline by a standard
Moses-based baseline system (Koehn et al, 2007),
using a default distance reordering model and a
lexicalized reordering model ?msd-bidirectional-
fe?. A 5-gram language model was built using
SRILM (Stolcke, 2002) on the target side of the
corresponding training corpus. Word alignments
were extracted using MGIZA++ (Gao and Vogel,
2008) and the parameters of the log-linear combi-
nation were tuned using MERT (Och, 2003).
Table 3 summarizes the results of the Baseline
system (no pre-reordering nor particle word inser-
tion), the Refined-HFC (Ref-HFC) and our DPC
method, using the well-known BLEU score (Pap-
ineni et al, 2002) and a word order sensitive met-
ric named RIBES (Isozaki et al, 2010a).
4http://mecab.googlecode.com/svn/trunk/mecab/doc/index.html
5http://nlp.stanford.edu/software/segmenter.shtml
6http://nlp.cs.berkeley.edu/Software.shtml
7http://triplet.cc/software/corbit
As it can be observed, our DPC method obtains
around 0.7 BLEU points of improvement when
compared to the second best system in both cor-
pora. When measuring the translation quality in
terms of RIBES, our method obtains an improve-
ment of 0.3 and 0.2 points when compared to the
second best system in News and CWMT + News
corpora, respectively. We suspect that corpus di-
versity might be one of the reasons for Refined-
HFC not to show any advantage in this setting.
We tested the significance of BLEU improve-
ment for Refined-HFC and DPC when compared
to the baseline phrase-based system. Refined-HFC
tests obtained p-values 0.355 and 0.135 on News
and CWMT + News corpora, while our proposed
DPC method obtained p-values 0.002 and 0.0,
which indicates significant improvements over the
phrase-based system.
5 Conclusions
In the present paper, we have analyzed the dif-
ferences in word order between Chinese and
Japanese sentences. We captured the regulari-
ties of ordering differences between Chinese and
Japanese sentences, and proposed a framework to
reorder Chinese sentences to resemble the word
order of Japanese.
Our framework consists in three steps. First,
we identify verbal blocks, which consist of Chi-
nese words that will move all together as a block
without altering their relative inner order. Sec-
ond, we identify the right-most object of the verbal
block, and move the verbal block to the right of it.
Finally, we identify invariable grammatical parti-
cles in the original vicinity of the verbal block and
move them relative to their dependency heads.
Our framework only uses the unlabeled depen-
dency structure of sentences and POS tag informa-
tion of words. We compared our system to a base-
line phrase-based SMT system and a refined head-
finalization system. Our method obtained a Chi-
nese word order that is more similar to Japanese
word order, and we showed its positive impact on
translation quality.
6 Discussion and future work
In the literature, there are mainly two types of
parsers that have been used to extract sentence
structure and guide reordering. The first type cor-
responds to parsers that extract phrase structures
(i.e. HPSG parsers). These parsers infer a rich
31
News CWMT+News
Chinese Japanese Chinese Japanese
Training
Sentences 342, 050 621, 610
Running words 7,414,749 9,361,867 9,822,535 12,499,112
Vocabulary 145,133 73,909 214,085 98,333
News Devel.
Sentences 1, 000 ?
Running words 46,042 56,748 ? ?
Out of Vocab. 255 54 ? ?
News Test
Sentences 2, 000 ?
Running words 51,534 65,721 ? ?
Out of Vocab. 529 286 ? ?
Table 2: Basic statistics of our corpora. News Devel. and News Test were used to tune and test the
systems trained with both training corpora. Data statistics were collected after tokenizing and filtering
out sentences longer than 64 tokens.
annotation of the sentence in terms of semantic
structure or phrase heads. Other reordering strate-
gies use a different type of parsers, namely depen-
dency parsers. These parsers extract dependency
information among words in the sentence, often
consisting in the dependency relation between two
words and the type of relation (dependency label).
Reordering strategies that use syntactic infor-
mation have proved successful, but they are likely
to magnify parsing errors if their reordering rules
heavily rely on abundant parse information. This
is aggravated when reordering Chinese sentences,
due to its loose word order and large variety of
possible dependency labels.
In this work, we based our study of ordering
differences between Chinese and Japanese solely
on dependency relations and POS tags. This con-
trasts with the work in (Han et al, 2012) that re-
quires phrase structures, phrase-head information
and POS tags, and the work in (Xu et al, 2009)
that requires dependency relations, dependency la-
bels and POS tags.
In spite of the fact that our method uses less syn-
tactic information, it succeeds at reordering sen-
tences with reported speech even in presence of
punctuation symbols. It is worth saying that re-
ported speech is very common in the news domain,
which might be one of the reasons of the supe-
rior translation quality achieved by our reordering
method. Our method also accounted for ordering
differences in serial verb constructions, comple-
mentizers and adverbial modifiers, which would
have required an increase in the complexity of the
reordering logic in other methods.
To the best of our knowledge, dependency
parsers are more common than HPSG parsers
across languages, and our method can potentially
be applied to translate under-resourced languages
into other languages with a very different sentence
structure, as long as they count with dependency
parsers and reliable POS taggers.
Implementing our method for other languages
would first require a linguistic study on the re-
ordering differences between the two distant lan-
guage pairs. However, some word ordering differ-
ences might be consistent across SVO and SOV
language pairs (such as verbs going before or after
their objects), but other ordering differences may
need special treatment for the language pair under
consideration (i.e. Chinese ?bei? particles).
There are two possible directions to extend the
present work. The first one would be to refine the
current method to reduce its sensitivity to POS tag-
ging or dependency parse errors, and to extend our
linguistic study on ordering differences between
Chinese and Japanese languages. The second di-
rection would be to manually or automatically find
common patterns of ordering differences between
SVO and SOV languages. The objective would be
then to create a one-for-all reordering method that
induces monotonic word alignments between sen-
tences from distant language pairs, and that could
also be easily extended to account for the unique
characteristics of the source language of interest.
Acknowledgments
We would like to thank Dr. Takuya Matsuzaki for
his precious advice on this work and Dr. Jun Ha-
tori for his support on using Corbit.
32
References
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing Chinese word segmen-
tation for machine translation performance. In Proc.
of the 3rd Workshop on SMT, pages 224?232.
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and
Christopher D Manning. 2009. Discriminative re-
ordering with Chinese grammatical relations fea-
tures. In Proc. of the Third Workshop on Syntax and
Structure in Statistical Translation, pages 51?59.
Angela Downing and Philip Locke. 2006. English
grammar: a university course. Routledge.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, pages 49?57.
Nizar Habash. 2007. Syntactic preprocessing for sta-
tistical machine translation. In Proc. of Machine
Translation Summit XI, pages 215?222.
Dan Han, Katsuhito Sudoh, Xianchao Wu, Kevin Duh,
Hajime Tsukada, and Masaaki Nagata. 2012. Head
finalization reordering for Chinese-to-Japanese ma-
chine translation. In Proc. of the Sixth Workshop on
Syntax, Semantics and Structure in Statistical Trans-
lation, pages 57?66.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and Ju-
nichi Tsujii. 2011. Incremental joint POS tagging
and dependency parsing in Chinese. In Proc. of
5th International Joint Conference on Natural Lan-
guage Processing, pages 1216?1224.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010a. Automatic
evaluation of translation quality for distant language
pairs. In Proc. of EMNNLP.
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010b. Head finalization: A simple re-
ordering rule for SOV languages. In Proc. of WMT-
MetricsMATR, pages 244?251.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proc. of ACL ?07, Demonstration Sessions, pages
177?180.
Taku Kudo and Yuji Matsumoto. 2000. Japanese de-
pendency structure analysis based on support vector
machines. In Proc. of the EMNLP/VLC-2000, pages
18?25.
Charles N Li and Sandra Annear Thompson. 1989.
Mandarin Chinese: A functional reference gram-
mar. Univ of California Press.
Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li,
Ming Zhou, and Yi Guan. 2007. A probabilistic ap-
proach to syntax-based reordering for statistical ma-
chine translation. In Proc. of ACL, page 720.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. Compu-
tational Linguistics, 34:35?80.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29:19?51.
Franz J. Och. 2003. Minimum error rate training
for statistical machine translation. In Proc. of ACL,
pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic
evaluation of machine translation. In Proc. of ACL,
pages 311?318.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. of the 21st COL-
ING and the 44th ACL, pages 433?440.
Carl Jesse Pollard and Ivan A. Sag. 1994. Head-
driven phrase structure grammar. The University
of Chicago Press and CSLI Publications.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proc. of the 7th interna-
tional conference on Spoken Language Processing,
2002, pages 901?904.
Chao Wang, Michael Collins, and Philipp Koehn.
2007. Chinese syntactic reordering for statistical
machine translation. In Proc. of the 2007 Joint Con-
ference on EMNLP-CoNLL, pages 737?745.
Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Hajime
Tsukada, and Masaaki Nagata. 2011. Extracting
pre-ordering rules from predicate-argument struc-
tures. In Proc. of 5th International Joint Conference
on Natural Language Processing, pages 29?37.
Fei Xia and Michael McCord. 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. In Proc. of the 20th international
conference on Computational Linguistics.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
SMT for subject-object-verb languages. In Proc. of
HLT: NA-ACL 2009, pages 245?253.
Kun Yu, Yusuke Miyao, Takuya Matsuzaki, Xiangli
Wang, and Junichi Tsujii. 2011. Analysis of the
difficulties in Chinese deep parsing. In Proc. of the
12th International Conference on Parsing Technolo-
gies, pages 48?57.
Hong-Mei Zhao, Ya-Juan Lv, Guo-Sheng Ben, Yun
Huang, and Qun Liu. 2011. Evaluation report
for the 7th China workshop on machine translation
(CWMT2011). The 7th China Workshop on Ma-
chine Translation (CWMT2011).
33
