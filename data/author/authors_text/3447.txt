Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 70?75,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Construction of Chinese Segmented and POS-tagged  Conversational Corpora 
and Their Evaluations on Spontaneous Speech Recognitions 
 
Xinhui Hu, Ryosuke Isotani, Satoshi Nakamura 
National Institute of Information and Communications Technology, Japan 
{xinhui.hu, ryosuke.isotani, satoshi.nakamura}@nict.go.jp 
 
Abstract 
The performance of a corpus-based language and 
speech processing system depends heavily on the 
quantity and quality of the training corpora. Although 
several famous Chinese corpora have been developed, 
most of them are mainly written text. Even for some 
existing corpora that contain spoken data, the quantity 
is insufficient and the domain is limited. In this paper, 
we describe the development of Chinese conversational 
annotated textual corpora currently being used in the 
NICT/ATR speech-to-speech translation system. A total 
of 510K manually checked utterances provide 3.5M 
words of Chinese corpora. As far as we know, this is 
the largest conversational textual corpora in the 
domain of travel. A set of three parallel corpora is 
obtained with the corresponding pairs of Japanese and 
English words from which the Chinese words are 
translated. Evaluation experiments on these corpora 
were conducted by comparing the parameters of the 
language models, perplexities of test sets, and speech 
recognition performance with Japanese and English. 
The characteristics of the Chinese corpora, their 
limitations, and solutions to these limitations are 
analyzed and discussed.  
 
1. Introduction 
 
In corpus-based machine translation and speech 
recognition, the performance of the language model 
depends heavily on the size and quality of the corpora. 
Therefore, the corpora are indispensable for these 
studies and applications. In recent decades, corpus 
development has seen rapid growth for many 
languages such as English, Japanese, and Chinese. For 
Chinese, since there are no plain delimiters among the 
words, the creation of a segmented and part-of-speech 
(POS)-tagged corpus is the initial step for most 
statistical language processes. Several such Chinese 
corpora have been developed since the 1990s. The two 
most typical are People?s Daily corpus (referred to as 
PKU), jointly developed by the Institute of 
Computational Linguistics of Peking University and 
the Fujitsu Research & Development Center [1], and 
the Sinica Corpus (referred to as Sinica) developed by 
the Institute of Information Science and the CKIP 
Group in Academia Sinica of Taiwan [2]. The former 
is based on the People?s Daily newspaper in 1998. It!
uses standard articles of news reports. The latter is a 
balanced corpus collected from different areas and 
classified according to five criteria: genre, style, mode, 
topic, and source. Although conversational text is also 
contained in this corpus, it has only 75K of utterances 
and the domains are limited to a few fields, such as 
academia and economics, and the style is mostly in 
address and seldom in conversation. 
Since the features of conversation differ from written 
text, especially in news articles, the development of a 
segmented and POS-tagged corpus of conversational 
language is promising work for spontaneous speech 
recognition and speech-to-speech translation.  
In the Spoken Communication Group of NICT, in 
order to study corpus-based speech translation 
technologies for the real world, a set of corpora on 
travel conversation has been built for Japanese, 
English, and Chinese [3]. These corpora are elaborately 
designed and constructed on the basis of the concept of 
variety in samples, situations, and expressions. Now 
these corpora have been used in the NICT speech-to-
speech translation (S2ST) system [8] and other 
services. 
In this paper, we introduce our work on this Chinese 
corpora development and applications in S2ST speech 
recognition using these corpora. In Section 2, we 
provide a brief description of the contents of the NICT 
corpora, then describe how the Chinese data were 
obtained. In Section 3, we illustrate the specifications 
for the segmentation and POS tagging designed for 
these corpora. Here, we explain the guidelines of 
segmentation and POS tagging, placing particular 
emphasis on the features of conversation and speech 
recognition application. In Section 4, we outline the 
development procedures and explain our methods of 
how to get the segmented and POS-tagged data. Some 
statistical characteristics of the corpora will be shown 
here. In Section 5, evaluation experiments of speech 
recognition utilizing these corpora are reported by 
comparing the results using the same data sets of 
70
Japanese and English. Finally, in Section 6, we discuss 
the performance of the corpora, the problems that 
remain in the corpora, and give our ideas concerning 
future work. 
 
2. Current NICT Chinese Corpora on 
Travel Dialog Domain 
 
At NICT, in order to deal with various conversational 
cases of S2ST research, several kinds of corpora were 
elaborately designed and constructed [3]. Table 1 gives 
a brief description of the data sets related to the 
development of the Chinese corpora. Each corpus 
shown in this table was collected using different 
methods, for different application purposes, and was 
categorized into different domains. 
Table 1. NICT Corpora Used for Chinese Processing 
Name Collecting Method Uttr. Domain 
SLDB 
Bilingual conversation 
evolved by 
interpreters. 
16K 
Dialogues 
with the 
front desk 
clerk at a 
hotel 
MAD 
Bilingual conversation 
evolved by a machine 
translation system. 
19K 
General 
dialogues  
on travel 
BTEC Text in guidebooks for overseas travelers 475K 
General 
dialogues 
on travel 
 
The SLDB (Spoken Language Database) is a 
collection of transcriptions of spoken language 
between two people speaking different languages and 
mediated by a professional interpreter. 
In comparison, the MAD (Machine Translation Aid 
Dialogue) is a similar collection, but it uses our S2ST 
system instead of an interpreter. 
The BTEC (Basic Travel Expression Corpus) is a 
collection of Japanese sentences and their English 
translations written by bilingual travel experts. This 
corpus covers such topics related to travel as shopping, 
hotel or restaurant reservations, airports, lost and 
found, and so on. 
The original data of the above corpora were 
developed in the form of English-to-Japanese 
translation pairs. The Chinese versions are mainly 
translated from the Japanese, but a small portion of 
BTEC (namely, BTEC4, about 70K of utterances) was 
translated from English. Every sentence in these 
corpora has an equivalent in the other two languages, 
and they share a common header (ID), except for the 
language mark. All the data in these three languages 
constitute a set of parallel corpora. The following 
shows examples of sentences in the three languages: 
Chn.: BTEC1\jpn067\03870\zh\\\\??????? 
Eng.: BTEC1\jpn067\03870\en\\\\I'd like to have some strong 
coffee. 
Jap.:BTEC1\jpn067\03870\ja\\\\???????????? 
 
3. Specifications of Segmentation and Part-
of-Speech Tagging 
 
By mainly referring to the PKU and taking into 
account the characteristics of conversational data, we 
made our definitions for segmentation units and POS 
tags. Here, we explain the outlines of these definitions, 
then illustrate the segmentation and POS-tagging items 
relating to those considerations on conversations. 
 
3.1. Guidelines of the Definitions 
 
(1) Compatibility with the PKU and Taking into 
account the Demand of Speech Recognition of 
S2ST 
Since the specification of segmentations and POS-
tagging proposed by the PKU [4] has its palpability 
and maneuverability and is close to China?s national 
standard [5] on segmentation and close to the 
specification on POS tags recommended by the 
National Program of China, namely, the 973-project 
[6], we mainly followed PKU?s specification. We 
adopted the concept of ?segmentation unit,? i.e., words 
with disyllable, trisyllable, some compound words, and 
some phrases were regarded as segmentation units. The 
morpheme character (word) was also regarded as an 
independent unit. 
However, we made some adjustments to these 
specifications. In the speech recognition phase of S2ST 
to deal with data sparseness, the word for ?training? 
needed to be shortened. So a numeral  was divided into 
syllabic units, while both the PKU and the Sinica took 
the whole number as a unit. For the same reason, the 
directional verbs (????), such as ??, ????
????? and ?? ,? which generally follow 
another verb and express action directions, were 
divided from the preceding verb. The modal auxiliary 
verbs (????), such as ?????and ?,? which 
often precede another verb were separated and tagged 
with an individual tag set. Because the numeral can be 
easily reunited as an integrated unit, such a processing 
method for numerals does not harm the translation 
phase of S2ST. Moreover, if the directional verb and 
the modal auxiliary verb can be identified, they will 
help the syntactic analysis and improve the translation 
phrase. These two kinds of verbs, together with ?? 
(be)? and ??  (have)? are more frequently used in 
71
colloquial conversations than in written text, so we 
took them as an individual segmentation unit and 
assigned a POS tag to each. The special processes for 
these kinds of words aim at reflecting the features of 
spoken language and improve the performance of the 
S2ST system. 
 
(2) Ability for Future Expansion 
Although the corpora were developed for speech 
recognition in S2ST system, it is desirable that they 
can be used in other fields when necessary. This 
reflects in both segmentation and POS-tagging. In 
segmentation, the compound words with definitive 
suffix or prefix are divided, so they can be combined 
easily when necessary. In POS-tagging, the nouns and 
verbs are mainly further categorized into several sub-
tags. We selected about 40 POS tags for our corpora, 
as shown in Table 1 in the Appendix. With such scale 
of tag sets, it is regarded to be suitable for 
language model of ASR. When necessary, it is 
also easy to choice an adequate tag set from it to 
meet the needs of other tasks. 
 
(3) Relation with the Corpora of Other Languages 
in NICT 
The original data of the corpora are in Japanese or 
English. It is meaningful to build connections at the 
morphological level among these trilingual parallel 
corpora at least for ?named entities.? For example, we 
adopted the same segmentation units as in Japanese, 
and we subcategorized these words into personal 
names, organization names, location names, and drink 
and food names and assigned them each an individual 
tag. Personal names were further divided into family 
names and first names for Chinese, Japanese, and 
Western names. These subclasses are useful in 
language modeling, especially in the travel domain.  
 
3.2. Some Explanations on Segmentation and 
POS-tagging 
 
(1) About Segmentation 
In our definition of a segmentation unit, words longer 
than 4 Hanzis (Chinese characters) were generally 
divided into their syntactic units. Idioms and some 
greeting phrases were also regarded as segmentation 
units. For example: ???/?????/???/??
? /.? Semantic information was also used to judge 
segmentation unit. For example:  
 
? ?/ ?/ ?/ ?/ ?/ ?/ ??/ ?/ (Tell me the best 
restaurant around here.) 
? ??/ ?/ ??/ ??/ ?/ ?/ ??/ ?/ (I'd like a 
hotel that is not too expensive.) 
 For segmenting compound words with different 
structures, we constituted detailed items to deal with 
them. These structures include ?coordinated (??)?
modifying (?? ), verb-object (?? ), subject-
predicate (??), and verb-complement (??).? The 
main consideration for these was to divide them 
without changing the original meaning. For those 
words that have a strong ability to combine with 
others, we generally separated them from the others. 
This was due to the consideration that if it were done in 
another way, it would result in too many words. For 
example, in the verb-object (?? ) structure, ?? 
(buy)? can combine with many nouns to get 
meaningful words or phrases, such as ???  (buy 
book), ?? (buy meat)??? (buy ticket)?and ??
?  (buy clothes).? We prescribed separating such 
active characters or words, no matter how frequently 
they are used in the real world, to ensure that the 
meaning did not change and ambiguity did not arise. 
So the above phrases should be separated in following 
forms: ??/ ?/ (buy book), ?/ ?/ (buy meat)??/ ?/ 
(buy ticket)?and ?/ ?? /buy clothes).?  
 For the directional verbs, we generally separated 
them from their preceding verbs. For example: 
?/ ??/ ?/ ?/ ??/ ??/ ?/ ?/ (Is it all right to move 
to another seat?) 
?/ ?/ ?/ ?/ ???/ ??/ ?/ ???/ ?/ (Please keep 
this suitcase until one o'clock.) 
Prefix and appendix were commonly separated from 
the root words. For example: 
??/ ?/ ?/ ?/ ??/ ?/ ?/ (Are all students going to 
Kyoto?) 
?/ ?/ ??/ ??/ ?/. (I do free-lance work.) 
 
(2) About POS-Tagging 
The POS tag sets are shown in Table 1 in the Appendix. 
The POS tagging was conducted by the grammar 
function based on how the segmentation unit behaves 
in a sentence. 
 
4. Procedure of Developing the Chinese 
Corpora 
 
The segmented and POS-tagged data were obtained in 
two steps. The first step was to get the raw segmented 
and POS-tagged data automatically by computer. The 
second was to check the raw segmented and POS-
tagged data manually. 
 
(1) Getting Raw Segmented and POS-Tagged Data 
The text data were segmented and POS tagged by 
using the language model shown in formula (1). 
)|()|()1()ww|(w)( 2-i1-iiii2-i1-ii cccPcwPPLP ?? ?+=    (1) 
72
Here iw  denotes the word at the ith position of a 
sentence, and ic  stands for the class to which the word 
iw  belongs. The class we used here is a POS-tag set, 
and  ?  is set 0.9. 
The initial data for training the model were from the 
Sinica due to their balanced characteristics. The 
annotated data were added to the training data when 
producing new data. When the annotated data reached 
a given quantity (here, the BTEC1 was finished, and 
the total words in the corpora exceeded 1M), the Sinica 
data were not used for training. We have conducted an 
experiment with this model for an open test text of 510 
utterances from BTEC, and the segmentation and POS-
tagging accuracy was more than 95%. Furthermore, 
proper noun information was extracted from Japanese 
corpora and marked in the corresponding lines of the 
Chinese segmented and POS-tagged data. 
 
(2) Manual Annotation 
The manual annotations were divided into two phases. 
The first was a line-by-line check of the raw segmented 
and POS-tagged data. The second was to check the 
consistency. The consistency check was conducted in 
the following manner: 
? Find the candidates having differences between the 
manually checked data and the automatically 
segmented and POS-tagged data.  
? Pick up the candidates having a high frequency of 
updating in the above step, and build an 
inconsistency table. The candidates in this table are 
the main objects of the later checks.  
? Check the same sentences with different 
segmentations and POS tags. 
? List all words having multiple POS tags and their 
frequencies. Determine the infrequent ones as 
distrustful candidates and add them into the 
inconsistency tables. 
The released annotated data were appended with a 
header ID for each token (pair of word entry and POS 
tag) in an utterance including a start marker and end 
marker, shown as follows: 
BTEC1\jpn067\03870\zh\\\00010\||||UTT-START|||| 
BTEC1\jpn067\03870\zh\\\00020\?|?||?|r|||| 
BTEC1\jpn067\03870\zh\\\00030\?|?||?|vw|||| 
BTEC1\jpn067\03870\zh\\\00040\?|?||?|v|||| 
BTEC1\jpn067\03870\zh\\\00050\?|?||?|a|||| 
BTEC1\jpn067\03870\zh\\\00060\??|??||??|n|||| 
BTEC1\jpn067\03870\zh\\\00070\?||||UTT-END|||| 
Table 2 shows some of the statistics for the 510K 
utterances in Table 1 for different languages.  
 
Table 2. Some Statistics of Each Corpora in NICT 
 Utter.
Ave. 
words 
/Uttr. 
Words Vocab.
Chinese 510K 6.95 3.50M 47.3K 
Japanese 510K 8.60 4.30M 45.5K 
English 510K 7.74 3.80M 32.9K 
 
 Figure 1 shows the distributions of utterance length 
(words in an utterance) for 3 languages among the 
510K annotated data. From Figure 1, we know that the 
Chinese has the fewest words in an utterance, followed 
by English, with the Japanese having the most.  
 
 
Figure 1. Distribution of utterance length  
 
5. Evaluation Experiments 
 
To verify the effectiveness of the developed Chinese 
textual corpora, we built a language model for speech 
recognition using these corpora. For comparisons with 
other languages, including Japanese and English, we 
also built language models for these two languages 
using the same training sets. Meanwhile, the same test 
set of each language was selected for speech 
recognition.  
 
5.1. Data Sets for Language Models and 
Speech Recognitions  
 
For simplicity, we adopted word 2-gram and word 3-
gram for evaluating perplexities and speech 
recognition performance. The training data were 
selected from the 510K utterances in Table 1, while the 
test sets were also extracted from them, but they are 
guaranteed not to exist in the training sets. In 
evaluations of perplexity, 1524 utterances (a total of 
three sets) were chosen as the test set. In evaluation of 
recognition, 510 utterances were chosen as test set. For 
Japanese and English, the same data sets were also 
chosen for comparisons. 
Distribution of Utterance Length
0%
2%
4%
6%
8%
10%
12%
14%
16%
18%
2 4 6 8 10 12 14
   
   
  >
15
length(words)
Pe
rc
en
ta
ge
Japanese
English
Chinese
73
 
Figure 2. Ratio of 2-gram items with low occurrence 
 
5.2. Comparisons of Language Models 
 
Using the above utterances in the training sets, a word 
2-gram and a 3-gram were built respectively for each 
language. The distributions of items inside these 
models were investigated. Figure 2 shows the ratios of 
2-gram?s items which have low occurrences (from 1 to 
6) in the 2-gram model.  
Compared with the other two languages, the Chinese 
has the biggest vocabulary. Moreover, it also has a 
large amount of low-frequency 1-gram, 2-gram, and 3-
gram items. For example, more than 60% of its 2-gram 
entries appear only once. This can be regarded that the 
Chinese has more varieties when expressing a same 
meaning than the other two languages. It is also partly 
due to bias occurred in the translation process, 
compared to the original languages. So the probability 
computations in 2 or 3-gram related to these entries 
were estimated by using a smoothing approach, so the 
accuracy is not high.  
Table 3 shows average sentence entropies (ASE) of 
the test sets to the 3-gram models. The ASE is obtained 
as follows: (1) first to get the  product of average word 
entropy and the total word count in test set. (2) then 
divide the product by the total sentences in the test set.  
From the table, we know the Chinese has the maximal 
sentence entropy (or maximal perplexity) among the 
three languages. This means that when predicate a 
sentence in the recognition process, Chinese requires a 
much bigger search space than the other two languages.   
 
Table 3. Average Sentence Entropy of the Test Sets to 3-
gram Models 
 Chinese Japanese English
Vocab. of Test Set 10,030 12,344 10,840
Ave. Sen. Entropy  294.58 165.80 202.92
Word Perplexity 45.0 20.1 28.5
 
5.3. Comparison of Speech Recognition 
Performances 
 
 
Figure 3. Word recognition accuracies of 3 languages 
 
The 2-gram language model was used for decoding 
recognition lattice, while the 3-gram model was used  
for rescoring process. The recognition results are 
shown in  Figure 3. Here, WordID  refers to the word?s 
outer layer (entry) together with its POS tag, other 
information like conjugation of verbs, declension of 
nouns, etc., while the surface word contains only its 
outer layer, no POS tag is contained  in this case . 
The difference in word accuracy of speech 
recognition between these two forms is about 2% for 
Chinese, and 1% for English and Japanese. 
 
6. Summary 
 
This paper described the development of Chinese 
conversational segmented and POS-tagged corpora that 
are used for spontaneous speech recognition in S2ST 
system. While referring mainly to the PKU?s 
specifications, we defined ours by taking into account 
the needs of S2ST. About 510K utterances, or about 
3.5M words of conversational Chinese data, are 
contained in these corpora. As far as we know, they are 
presently the biggest ones in the domain of travel, with 
a style of conversations. Moreover, a parallel corpus 
was obtained using these 510K pairs of utterances of 
Chinese, Japanese, and English. These corpora now 
play a big role in spontaneous language and speech 
processing, and are used in the NICT/ATR Chinese-
Japanese-English Speech-to-Speech Translation 
System [8] and other communication services. 
However, according to our evaluations in this paper, 
there are still some difference in performance among 
Chinese and other languages, especially Japanese. 
There is still some room to improve the quality of these 
corpora mainly because the Chinese text data were 
translated from other languages, mainly Japanese, with 
a few words from English. There is some bias in 
expression, especially for the transliterations of proper 
nouns. For examples, ?Los Angles? is translated as ??
??,???,???, and ???.?also, some 
utterances are not like those spoken by native speakers, 
0
10
20
30
40
50
60
70
1 2 3 4 5 6
Chinese
Japanese
English
Ratio of 2-gram Item [%]
occurrence
81
87.96
93.87
82.89
89.29
94.67
70
75
80
85
90
95
100
Chinese English Japanese
W
or
d 
A
cc
ur
ac
y
Word Accuracy for Each Language
WordID
Surface Word
74
like sentence of ????????? ? which 
corresponds to the original sentence of  ??????
????(I appreciate your kindness).?    
For future work, while continuing to improve the 
consistency of the corpora, we will expand the Chinese 
corpora from external data resource, such as Web sites 
and LDC databases, to extract original Chinese 
spontaneous text data.  
 
7. References 
[1] H.M. Duan, J. Song, G.W. Xu, G.X. Hu and S.W. Yu, 
?The Development of a Large-scale Tagged Chinese 
Corpus and Its Applications.? http://icl.pku.edu.cn/icl_tr 
[2] C.R. Huang, and K.J. Chen, ?Introduction to Sinica 
Corpus,? CKIP Technical Report 95-02/98-04, 
http://www.sinica.edu.tw/SinicaCorpus 
[3] G. Kikui, E. Sumita, T. Takezawa, S. Yamamoto, 
?Creating Corpora for Speech-to-Speech Translation.? 8th 
European Conference on Speech Communication and 
Technology, Vol.1, pp.381-384, Sep., 2003 
[4] S.W. Yu, X.F. Zhu, and H.M. Duan, ?The Guideline for 
Segmentation and Part-Of-Speech Tagging on Very Large 
Scale Corpus of Contemporary Chinese.? 
http://icl.pku.edu.cn/icl_tr 
[5] The National Standard of PRC, ?Standardization of 
Segmentation for Contemporary Chinese.? GB13715, 
1992. 
[6] Institute of Applied Linguistics of the Ministry of 
Education, China, ?Specification on Part-of-Speech 
Tagging of Contemporary Chinese for Information 
Processing (Draft).? 2002. 
[7] H. Yamamoto, S. Isogai, and Y. Sagisaka, ?Multi-class 
Composite N-gram Language Model,? Speech 
Communication, 2003, Vol.41, pp369-379. 
[8] T. Shimizu, Y. Ashikari, E. Sumita, J.S. Zhang, S. 
Nakamura, ?NICT/ATR Chinese-Japanese-English 
Speech-to-Speech Translation System.? Tsinghua Science 
and Technology, Vol.13, No.4, pp540-544, Aug. 2008. 
 
Appendix Table 1. Chinese POS Tag Table 
 
POS Tag 
Description 
POS Tag 
Description 
Chinese English Chinese English 
a ??? Adjective 
n
nppx ????? Chinese  family name 
b ??? Non-predicate  adjective nppm ????? 
Chinese  
first name 
c ?? Conjunction nppxj ????? Japanese  family name 
d ?? Adverb nppmj ????? Japanese  first name 
de ???? Attributive nppxw ??????? Western  family name 
e ?? Interjection nppmw ??????? Western  first name 
g ??? Morpheme Word npl ?? Place 
h ??? Prefix npo ??? Organization 
i 
??, 
??? Idiom npfd ???? Drink and food 
j ??? Abbreviation o ??? Onomatopoeia 
k ??? Suffix p ?? Preposition 
m 
m ?? Numeral q ?? Quantifier 
ma ???? Numeral Classifier r ?? Pronoun 
mb ??? Approximate  numeral u ?? Auxiliary 
n 
n ???? Noun 
v
v ???? Verb 
nd ??? Directional locality v1 ??????? 
Auxiliary  
verb 
ns ??? Space word v2 ????? Verb ?Have? 
nt ??? Time word vt ???? Directional verb 
nx 
???, 
?? 
Numeric, 
character string vw ???? Modal verb 
np ???? Proper noun w ???? Punctuation 
npp ?? Personal name y ???? Modal particle 
75
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 33?41, Prague, June 2007. c?2007 Association for Computational Linguistics
Learning Unsupervised SVM Classifier for Answer Selection in Web
Question Answering
Youzheng Wu, Ruiqiang Zhang, Xinhui Hu, and Hideki Kashioka
National Institute of Information and Communications Technology (NICT),
ATR Spoken Language Communication Research Labs.
2-2-2 Hikaridai ?Keihanna Science City? Kyoto 619-0288 Japan
{Youzheng.wu,Ruiqiang.zhang,Xinhui.hu,Hideki.kashioka}@atr.jp
Abstract
Previous machine learning techniques for
answer selection in question answering
(QA) have required question-answer train-
ing pairs. It has been too expensive and
labor-intensive, however, to collect these
training pairs. This paper presents a novel
unsupervised support vector machine (U-
SVM) classifier for answer selection, which
is independent of language and does not re-
quire hand-tagged training pairs. The key
ideas are the following: 1. unsupervised
learning of training data for the classifier by
clustering web search results; and 2. select-
ing the correct answer from the candidates
by classifying the question. The compara-
tive experiments demonstrate that the pro-
posed approach significantly outperforms
the retrieval-based model (Retrieval-M), the
supervised SVM classifier (S-SVM), and the
pattern-based model (Pattern-M) for answer
selection. Moreover, the cross-model com-
parison showed that the performance rank-
ing of these models was: U-SVM > Pattern-
M > S-SVM > Retrieval-M.
1 Introduction
The purpose of answer selection in QA is to se-
lect the exact answer to the question from the ex-
tracted candidate answers. In recent years, many
supervised machine learning techniques for answer
selection in open-domain question answering have
been investigated in some pioneering studies [Itty-
cheriah et al 2001; Ng et al 2001; Suzuki et al
2002; Sasaki, et al 2005; and Echihabi et al 2003].
Compared with retrieval-based [Yang et al 2003],
pattern-based [Ravichandran et al 2002 and Soub-
botin et al 2002], and deep NLP-based [Moldovan
et al 2002, Hovy et al 2001; and Pasca et al 2001]
answer selection, machine learning techniques are
more effective in constructing QA components from
scratch. These techniques suffer, however, from the
problem of requiring an adequate number of hand-
tagged question-answer training pairs. It is too ex-
pensive and labor intensive to collect such training
pairs for supervised machine learning techniques.
To tackle this knowledge acquisition bottleneck,
this paper presents an unsupervised SVM classifier
for answer selection, which is independent of lan-
guage and question type, and avoids the need for
hand-tagged question-answer pairs. The key ideas
are as follows:
1. Regarding answer selection as a kind of classi-
fication task and adopting an SVM classifier;
2. Applying unsupervised learning of pseudo-
training data for the SVM classifier by cluster-
ing web search results;
3. Training the SVM classifier by using three
types of features extracted from the pseudo-
training data; and
4. Selecting the correct answer from the candidate
answers by classifying the question. Note that
this means classifying a question into one of
the clusters learned by clustering web search
results. Therefore, our classifying the question
33
Figure 1: Web Question Answering Architecture
is different from conventional question classifi-
cation (QC) [Li et al 2002] that determines the
answer type of the question.
The proposed approach is fully unsupervised and
starts only from a user question. It does not require
richly annotated corpora or any deep linguistic tools.
To the best of our knowledge, no research on this
kind of study we discuss here has been reported.
Figure 1 illustrates the architecture of our web QA
approach. The S-SVM and Pattern-M models are
included for comparison.
Because the focus of this paper just evaluates the
answer selection part, our approach requires knowl-
edge of the answer type to the question in order to
find candidate answers, and that the answer must be
a NE for convenience in candidate extraction. Ex-
periments using Chinese versions of the TREC 2004
and 2005 test data sets show that our approach sig-
nificantly outperforms the S-SVM for answer selec-
tion, with a top 1 score improvement of more than
20%. Results obtained with the test data set in [Wu
et al 2004] show that the U-SVM increases the
top 1/mrr 5/top 5 scores by 5.95%/6.06%/8.68%
as compared with the Pattern-M. Moreover, our
cross-model comparison demonstrates that the per-
formance ranking of all models considered is: U-
SVM > Pattern-M > S-SVM > Retrieval-M.
2 Comparison among Models
Related researches on answer selection in QA can be
classified into four categories. The retrieval-based
model [Yang et al 2003] selects a correct answer
from the candidates according to the distance be-
tween a candidate and all question keywords. This
model does not work, however, if the question and
the answer-bearing sentences do not match on the
surface. The pattern-based model [Ravichandran
et al 2002 and Soubbotin et al 2002] first clas-
sifies the question into predefined categories, and
then extracts the exact answer by using answer pat-
terns learned off-line. Although the pattern-based
model can obtain high precision for some prede-
fined types of questions, it is difficult to define ques-
tion types in advance for open-domain question an-
swering. Furthermore, this model is not suitable for
all types of questions. The deep NLP-based model
[Moldovan et al 2002; Hovy et al 2001; and Pasca
et al 2001] usually parses the user question and an
answer-bearing sentence into a semantic represen-
tation, and then semantically matches them to find
the answer. This model has performed very well at
TREC workshops, but it heavily depends on high-
performance NLP tools, which are time consuming
and labor intensive for many languages. Finally, the
machine learning-based model has also been inves-
tigated. current models of this type are based on su-
pervised approaches [Ittycheriah et al 2001; Ng et
al. 2001; Suzuki et al 2002; and Sasaki et al 2005]
that are heavily dependent on hand-tagged question-
answer training pairs, which not readily available.
In response to this situation, this paper presents
the U-SVM for answer selection in open-domain
web question answering system. Our U-SVM has
the following advantages over supervised machine
learning techniques. First, the U-SVM classifies
questions into a question-dependent set of clusters,
and the answer is the name of a question cluster.
In contrast, most previous models have classified
candidates into positive and negative. Second, the
U-SVM automatically learns the unique question-
dependent clusters and the pseudo-training for each
34
Table 1: Comparison of Various Machine Learning Techniques
System Model Key Idea Training Data
[Ittycheriah et al 2001] ME Classifier Classifying candidates into positive
and negative
5,000 English
Q-A pairs
[Suzuki et al 2002] SVM Classifier Classifying candidates into positive
and negative
1358 Japanese
Q-A pairs
[Echihabi et al 2003] N-C Model Selecting correct answer by aligning
question with sentences
90,000 English
Q-A pairs
[Sasaki et al 2005] ME Classifier Classifying words in sentences into an-
swer and non-answer words
2,000 Japanese
Q-A pairs
Our U-SVM Model SVM Classifier Classifying question into a set of
question-dependent clusters
No Q-A pairs
question. This differs from the supervised tech-
niques, in which a large number of hand-tagged
training pairs are shared by all of the test ques-
tions. In addition, supervised techniques indepen-
dently process the answer-bearing sentences, so the
answers to the questions may not always be ex-
tractable because of algorithmic limitations. On the
other hand, the U-SVM can use the interdependence
between answer-bearing sentences to select the an-
swer to a question.
Table 1 compares the key idea and training data
used in the U-SVM with those used in the supervised
machine learning techniques. Here, ME means the
maximum entropy model, and N-C means the noisy-
channel model.
3 The U-SVM
The essence of the U-SVM is to regard answer selec-
tion as a kind of text categorization-like classifica-
tion task, but with no training data available. In the
U-SVM, the steps of ?clustering web search results?,
?classifying the question?, and ?training SVM clas-
sifier? play very important roles.
3.1 Clustering Web Search Results
Web search results, such as snippets returned by
Google, usually include a mixture of multiple
subtopics (called clusters in this paper) related to
the user question. To group the web search results
into clusters, we assume that the candidate answer in
each Google snippet can represent the ?signature? of
its cluster. In other words, the Google snippets con-
taining the same candidate are regarded as aligned
snippets, and thus belong to the same cluster. Web
search results are clustered in two phases.
? A first-stage Google search (FGS) is ap-
plied to extract n candidate answers
{c1, c2, . . . , cn} from the top m Google
snippets {s1, s2, . . . , sm} by a NER tool
[Wu et al 2005]. Those snippets containing
the candidates {ci} and at least one ques-
tion keyword {qi} are retained. Finally,
the retained snippets {s1, s2, . . . , sm} are
clustered into n clusters {C1, C2, . . . , Cn}
by clustering web search results, that is,
If a snippet includes L different candidates,
the snippet belongs to L different clusters.
If the candidates of different snippets are
the same, these snippets belong to the same
clusters.
Consequently, the number of clusters {Ci} is
fully determined by the number of candidates
{ci}, and the cluster name of a cluster Ci is the
candidate answer ci. Up to this point, we have
obtained clusters and sample snippets for each
cluster that will be used as training data for the
SVM classifier. Because this training data is
learned automatically, rather than hand-tagged,
we call it pseudo-training data.
? A second-stage Google search (SGS) is ap-
plied to resolve data sparseness in the pseudo-
training samples learned through the FGS. The
FGS data may have very few training snip-
pets in some clusters, so more snippets must
be collected. Note that this step just learns new
35
Google snippets into the clusters learned by the
FGS, but does not add new clusters.
For each candidate answer ci:
Combine the original query q = {qi} and
the candidate ci to form a new query q? =
{q, ci}.
Submit q? to Google and download the top
50 Google snippets.
Retain the snippets containing the candi-
date ci and at least one keyword qi.
Group the retained snippets into n clusters
to form the new pseudo-training data.
End
Here, we give an example illustrating the prin-
ciple of clustering web search results in the
FGS. In submitting TREC 2004 test question 1.1
?when was the first Crip gang started?? to Google
(http://www.google.com/apis), we extract n(= 8)
different candidates from the top m(= 30) Google
snippets. The Google snippets containing the same
candidates are aligned snippets, and thus the 12 re-
tained snippets are grouped into 8 clusters, as listed
in Table 2. This table roughly indicates that the snip-
pets with the same candidate answers contain the
same sub-meanings, so these snippets are considered
as aligned snippets. For example, all Google snip-
pets that contain the candidate answer 1969 express
the time of establishment of ?the first Crip gang?.
In summary, the U-SVM uses the result of ?clus-
tering web search results? as the pseudo-training
data of the SVM classifier, and then classifies user
question into one of the clusters for answer selec-
tion. On the one hand, the clusters and their names
are based on candidate answers to question; on the
other hand, candidates are dependent on question.
Therefore, the clusters are question-dependent.
3.2 Classifying Question
Using the pseudo-training data obtained by cluster-
ing web search results to train the SVM classifier,
we classify user questions into a set of question-
dependent clusters and assume that the correct an-
swer is the name of the question cluster that is as-
signed by the trained U-SVM classifier. For the
above example, if the U-SVM classifier, trained on
the pseudo-training data listed in Table 2, classifies
the above test question into a cluster whose name is
1969, then the cluster name 1969 is the answer to
the question.
This paper selects LIBSVM toolkit1 to implement
the SVM classifier. The kernel is the radical basis
function with the parameter ? = 0.001 in the exper-
iments.
3.3 Feature Extraction
To classify the question into a question-dependent
set of clusters, the U-SVM classifier extracts three
types of features.
? A similarity-based feature set (SBFS) is
extracted from the Google snippets. The SBFS
attempts to capture the word overlap between
a question and a snippet. The possible values
range from 0 to 1.
SBFS Features
percentage of matched keywords (KWs)
percentage of mismatched KWs
percentage of matched bi-grams of KWs
percentage of matched thesauruses
normalized distance between candidate and
KWs
To compute the matched thesaurus feature, we
adopt TONGYICICILIN 2 in the experiments.
? A Boolean match-based feature set (BMFS) is
also extracted from the Google snippets. The
BMFS attempts to capture the specific key-
word Boolean matches between a question and
a snippet. The possible values are true or false.
BMFS Features
person names are matched or not
location names are matched or not
organization names are matched or not
time words are matched or not
number words are matched or not
root verb is matched or not
candidate has or does not have bi-gram in
snippet matching bi-gram in question
candidate has or does not have desired
named entity type
? A window-based word feature set (WWFS)
is a set of words consisting of the words
1http://www.csie.ntu.edu.tw/ cjlin/libsvm/
2A Chinese Thesaurus Lexicon
36
Table 2: Clustering Web Search Results
Cluster Name Google Snippet
1969 It is believed that the first Crip gang was formed in late 1969. During this time in
Los Angeles there were ...
... the first Bloods and Crips gangs started forming in Los Angeles in late 1969, the
Island Bloods sprung up in north Pomona ...
... formed by 16 year old Raymond Lee Washington in 1969. Williams joined
Washington in 1971 ... had come to be called the Crips. It was initially started to
eliminate all street gangs ...
August 8, 2005 High Country News ? August 8, 2005: The Gangs of Zion
2004 2004 main 1 Crips 1.1 FACTOID When was the first Crip gang started? 1.2 FAC-
TOID What does the name mean or come...
1972 One of the first-known and publicized killings by Crip gang members occurred at
the Hollywood Bowl in March 1972.
1971 Williams joined Washington in 1971, forming the westside faction of what had
come to be called the Crips.
The Crips gang formed as a kind of community watchdog group in 1971 after the
demise of the Black Panthers. ...
... formed by 16 year old Raymond Lee Washington in 1969. Williams joined
Washington in 1971 ... had come to be called the Crips. It was initially started to
eliminate all street gangs ...
1982 Oceanside police first started documenting gangs in 1982, when five known gangs
were operating in the city: the Posole Locos...
mid-1990s Street Locos; Deep Valley Bloods and Deep Valley Crips. By the mid-1990s, gang
violence had ...
1970s The Blood gangs started up as opposition to the Crips gangs, also in the 1970s, and
the rivalry stands to this day ...
preceding {wi?5, . . . , wi?1} and following
{wi+1, . . . , wi+5} the candidate answer. The
WWFS features can be regarded as a kind of
relevant snippets-based question keywords ex-
pansion. By extracting the WWFS feature set,
the feature space in the U-SVM becomes ques-
tion dependent, which may be more suitable for
classifying the question. The number of classi-
fication features in the S-SVM must be fixed,
however, because all questions share the same
training data. This is one difference between
the U-SVM and the supervised SVM classifier
for answer selection. Each word feature in the
WWFS is weighted using its ISF value.
ISF (wj , Ci) =
N(wj , Ci) + 0.5
N(wj) + 0.5
(1)
where N(wj) is the total number of the
snippets containing word feature wj , and
N(wj , Ci) is the number of snippets in cluster
Ci containing word feature wj .
When constructing question vector, we assume
that the question is an ideal question that con-
tains all the extracted WWFS words. There-
fore, the values of the WWFS word features in
question vector are 1. Similarly, the values of
the SBFS and BMFS features in question vec-
tor are also estimated by self-similarity calcu-
lation.
4 Experiments
4.1 Data Sets
For the experiments, no English named entity recog-
nition (NER) tool is in our hand at the time of
the experiments; therefore, we validate the U-SVM
37
in terms of Chinese web QA using three test data
sets, which will be published with this paper3. Al-
though the U-SVM is independent of the question
types, for convenience in candidate extraction, only
those questions whose answers are named entities
are selected. The three test data sets are CTREC04,
CTREC05 and CTEST05. CTREC04 is a set of
178 Chinese questions translated from TREC 2004
FACTOID testing questions. CTREC05 is a set of
279 Chinese questions translated from TREC 2005
FACTOID testing questions. CTEST05 is a set of
178 Chinese questions found in [Wu et al 2004]
that are similar to TREC testing questions except
that they are written in Chinese. Figure 2 breaks
down the types of questions (manually assigned) in
the CTREC04 and CTREC05 data sets. Here, PER,
LOC, ORG, TIM, NUM, and CR refer to questions
whose answers are a person, location, organization,
time, number, and book or movie, respectively.
Figure 2: Statistics of CTEST05
To collect the question-answer training data for
the S-SVM, we submitted 807 Chinese questions to
Google and extracted the candidates for each ques-
tion from the top 50 Google snippets. We then man-
ually selected the snippets containing the correct
answers as positive snippets, and designated all of
the other snippets as negative snippets. Finally, we
collected 807 hand-tagged Chinese question-answer
pairs as the training data of S-SVM called CTRAIN-
DATA.
4.2 Evaluation Method
In the experiments, the top m(= 50) Google snip-
pets are adopted to extract candidates by using a
3Currently no public testing question set for simplified Chi-
nese QA is available.
Chinese NER tool [Wu et al 2005]. The number of
the candidates extracted from the top m(= 50) snip-
pets, n, is adaptive for different questions but it does
not exceed 30. The results are evaluated in terms
of two scores, top n and mrr 5. Here, top n is the
rate at which at least one correct answer is included
in the top n answers, while mrr 5 is the average re-
ciprocal rank (1/n) of the highest rank n(n ? 5) of
a correct answer to each question.
4.3 U-SVM vs. Retrieval-M
The Retrieval-M selects the candidate with the short-
est distances to all question keywords as the cor-
rect answer. In this experiment, the Retrieval-M
is implemented based on the snippets returned by
Google, while the U-SVM is based on the SGS data,
the SBFS and BMFS feature. Table 3 summarizes
the comparative performance.
Table 3: Comparison of Retrieval-M and U-SVM
Retrieval-M U-SVM
top 1 27.84% 53.61%
CTREC04 mrr 5 43.67% 66.25%
top 5 71.13% 88.66%
top 1 34.00% 50.00%
CTREC05 mrr 5 48.20% 62.38%
top 5 71.33% 82.67%
The table shows that the U-SVM greatly improves
the performance of the Retrieval-M: the top 1 im-
provements for CTREC04 and CTREC05 are about
25.8% and 16.0%, respectively. This experiment
demonstrates that the assumptions used here in clus-
tering web search results and in classifying the ques-
tion are effective in many cases, and that the U-SVM
benefits from these assumptions.
4.4 U-SVM vs. S-SVM
To explore the effectiveness of our unsupervised
model as compared with the supervised model, we
conduct a cross-model comparison of the S-SVM
and the U-SVM with the SBFS and BMFS feature
sets. The U-SVM results are compared with the S-
SVM results for CTREC04 and CTREC05 in Ta-
bles 4 and 5, respectively. The S-SVM is trained
on CTRAINDATA.
These tables show the following:
38
Table 4: Comparison of U-SVM and S-SVM on
CTREC04
FGS SGS
top 1 S-SVM 30.93% 39.18%
U-SVM 45.36% 53.61%
mrr 1 S-SVM 45.36% 53.54%
U-SVM 57.44% 66.25%
top 5 S-SVM 71.13% 79.38%
U-SVM 76.29% 88.66%
Table 5: Comparison of U-SVM and S-SVM on
CTREC05
FGS SGS
top 1 S-SVM 30.00% 33.33%
U-SVM 48.00% 50.00%
mrr 1 S-SVM 45.59% 48.67%
U-SVM 58.01% 62.38%
top 5 S-SVM 72.00% 74.67%
U-SVM 75.33% 82.67%
? The proposed U-SVM significantly outper-
forms the S-SVM for all measurements and
all test data sets. For the CTREC04 test data
set, the top1 improvements for the FGS and
SGS data are about 14.5% and 14.4%, respec-
tively. For the CTREC05 test data set, the top1
score for the FGS data increases from 30.0%
to 48.0%, and the top 1 score for the SGS data
increases from 33.3% to 50.0%. Note that the
SBFS and BMFS features here is fewer than the
features in [Ittycheriah et al 2001; Suzuki et
al. 2002], but the comparison is still effective
because the models are compared in terms of
the same features. In the S-SVM, all questions
share the same training data, while the U-SVM
uses the unique pseudo-training data for each
question. This is the main reason why the U-
SVM performs better than the S-SVM does.
? The SGS data is greatly helpful for both
the U-SVM and the S-SVM. Compared with
the FGS data, the top 1/mrr 5/top 5 im-
provements for the S-SVM and the U-SVM
on CTREC04 are 8.25%/8.18%/8.25% and
7.25%/8.81%/12.37%. The SGS can be re-
garded as a kind of query expansion. The rea-
sons for this improvement are: the data sparse-
ness in FGS data is partially resolved; and the
use of the Web to introduce data redundancy
is helpful. [Clarke et al 2001; Magnini et al
2002; and Dumais et al 2002].
In the S-SVM, all of the test questions share the
same hand-tagged training data, so the WWFS fea-
tures cannot be easily used [Ittycheriah et al 2002;
Suzuki, et al 2002]. Tables 6 and 7 compare
the performances of the U-SVM with the (SBFS +
BMFS) features, the WWFS features, and combina-
tion of three types of features for the CTREC04 and
CTREC05 test data sets, respectively.
Table 6: Performances of U-SVM for Different Fea-
tures on CTREC04
SBFS+BMFS WWFS Combination
top 1 53.61% 46.39% 60.82%
mrr 5 66.25% 59.19% 71.31%
top 5 88.66% 81.44% 88.66%
Table 7: Performances of U-SVM for Different Fea-
tures on CTREC05
SBFS+BMFS WWFS Combination
top 1 50.00% 49.33% 57.33%
mrr 5 62.38% 59.26% 65.61%
top 5 82.67% 74.00% 80.00%
These tables report that combining three types
of features can improve the performance of
the U-SVM. Using a combination of features
with the CTREC04 test data set results in the
best performances: 60.82%/71.31%/88.66% for
top 1/mrr 5/top 5. Similarly, as compared with
using the (SBFS + BMFS) and WWFS features, the
improvements from using a combination of features
with the CTREC05 test data set are 7.33%/3.23%/-
2.67% and 8.00%/6.35%/6.00%, respectively. The
results also demonstrate that the (SBFS + BMFS)
features are more important than the WWFS fea-
tures.
These comparative experiments indicate that the
U-SVM performs better than the S-SVM does, even
though the U-SVM is an unsupervised technique and
no hand-tagged training data is provided. The aver-
39
age top 1 improvements for both test data sets are
both more than 20%.
4.5 U-SVM vs. Pattern-M vs. S-SVM
To compare the U-SVM with the Pattern-M and
the S-SVM, we use the CTEST05 data set, shown
in Figure 3. The CTEST05 includes 14 different
question types, for example, Inventor Stuff (with
question like ?Who invented telephone??), Event-
Day (with question like ?when is World Day for Wa-
ter??), and so on. The Pattern-M uses the depen-
dency syntactic answer patterns learned in [Wu et
al. 2007] to extract the answer, and named entities
are also used to filter noise from the candidates.
Figure 3: Statistics of CTEST05
Table 8 summarizes the performances of the U-
SVM, Pattern-M, and S-SVM models on CTEST05.
Table 8: Comparison of U-SVM, Pattern-M and S-
SVM on CTEST05
S-SVM Pattern-M U-SVM
top 1 44.89% 53.14% 59.09%
mrr 5 56.49% 61.28% 67.34%
top 5 74.43% 73.14% 81.82%
The results in the table show that the U-SVM
significantly outperforms the S-SVM and Pattern-
M, while the S-SVM underperforms the Pattern-
M. Compared with the Pattern-M, the U-SVM in-
creases the top 1/mrr 5/top 5 scores by 5.95%/
6.06%/8.68%, respectively. The reasons may lie in
the following:
? The Chinese dependency parser influences de-
pendency syntactic answer-pattern extraction,
and thus degrades the performance of the
Pattern-M model.
? The imperfection of Google snippets affects
pattern matching, and thus adversely influences
the Pattern-M model. From the cross-model
comparison, we conclude that the performance
ranking of these models is: U-SVM > Pattern-
M > S-SVM > Retrieval-M.
5 Conclusion and Future Work
This paper presents an unsupervised machine learn-
ing technique (called the U-SVM) for answer selec-
tion that is validated in Chinese open-domain web
QA. Regarding answer selection as a kind of classifi-
cation task, the U-SVM automatically learns clusters
and pseudo-training data for each cluster by cluster-
ing web search results. It then selects the correct
answer from the candidates according to classifying
the question. The contribution of this paper is that
it presents an unsupervised machine learning tech-
nique for web QA that starts with only a user ques-
tion. The results of our experiments with three test
data sets are encouraging. As compared with the
S-SVM, the top 1 performances of the U-SVM for
the CTREC04 and CTREC05 data sets are signifi-
cantly improved, at more than 20%. Moreover, the
U-SVM performs better than the Retrieval-M and
the Pattern-M.
These experiments have only validated the U-
SVM on named entity types of questions that ac-
count for about 82% of all TREC2004 and 2005
FACTOID test questions. In fact, our technique is
independent of question types only if the candidates
can be extracted. In the future, we will explore the
effectiveness of our technique for the other types of
questions. The web search results clustering in the
U-SVM defines that a candidate in a Google snip-
pet can represent the ?signature? of its cluster. This
definition, however, is not always effective. To fil-
ter noise in the pseudo-training data, we will extract
relations between the candidates and the keywords
as the cluster signatures of Google snippets. More-
over, applying the U-SVM to QA systems in other
languages, like English and Japanese, will also be
included in our future work.
40
References
Abdessamad Echihabi, and Daniel Marcu. 2003. A
Noisy-Channel Approach to Question Answering. In
Proc. of ACL-2003, Japan.
Abraham Ittycheriah, Salim Roukos. 2002. IBM?s Sta-
tistical Question Answering System-TREC 11. In Proc.
of TREC-11, Gaithersburg, Maryland.
Bernardo Magnini, Matteo Negri, Roberto Prevete,
Hristo Tanev. 2002. Is It the Right Answer? Exploit-
ing Web Redundancy for Answer Validation. In Proc.
of ACL-2002, Philadelphia, pp. 425 432.
Charles L. A. Clarke, Gordon V. Cormack, Thomas R.
Lynam. Exploiting Redundancy in Question Answer-
ing In Proc. of SIGIR-2001, pp 358?365, 2001.
Christopher Pinchak, Dekang Lin. 2006. A Probabilistic
Answer Type Model. In Proc. of EACL-2006, Trento,
Italy, pp. 393-400.
Dan Moldovan, Sanda Harabagiu, Roxana Girju, et al
2002. LCC Tools for Question Answering. NIST Spe-
cial Publication: SP 500-251, TREC-2002.
Deepak Ravichandran, Eduard Hovy. 2002. Learning
Surface Text Patterns for a Question Answering Sys-
tem. In Proc. of the 40th ACL, Philadelphia, July
2002.
Eduard Hovy, Ulf Hermjakob, Chin-Yew Lin. 2001. The
Use of External Knowledge of Factoid QA. In Proc.
of TREC 2001, Gaithersburg, MD, U.S.A., November
13-16, 2001.
Hui Yang, Tat-Seng Chua. 2003. QUALIFIER: Question
Answering by Lexical Fabric and External Resources.
In Proc. of EACL-2003, page 363-370.
Hwee T. Ng, Jennifer L. P. Kwan, and Yiyuan Xia. 2001.
Question Answering Using a Large Text Database: A
Machine Learning Approach. In Proc. of EMNLP-
2001, pp66-73 (2001).
Jun Suzuki, Yutaka Sasaki, Eisaku Maeda. 2002. SVM
Answer Selection for Open-Domain Question Answer-
ing. In Proc. of Coling-2002, pp. 974 980 (2002).
Marius Pasca. 2001. A Relational and Logic Represen-
tation for Open-Domain Textual Question Answering.
In Proc. of ACL (Companion Volume) 2001: 37-42.
Martin M. Soubbotin, Sergei M. Soubbotin. 2002. Use of
Patterns for Detection of Likely Answer Strings: A Sys-
tematic Approach. In Proc. of TREC-2002, Gaithers-
burg, Maryland, November 2002.
Susan Dumais, Michele Banko, Eric Brill, Jimmy Lin,
and Andre Ng. Web Question Answering: Is More
Always Better?. In Proc. SIGIR-2002, pp 291?298,
2002.
Xin Li, and Dan Roth. 2002. Learning Question Classi-
fication. In Proc. of the 19th International Conference
on Computational Linguistics, Taibai, 2002.
Youzheng Wu, Hideki Kashioka, Jun Zhao. 2007. Us-
ing Clustering Approaches to Open-domain Question
Answering. In Proc. of CICLING-2007, Mexico City,
Mexico, pp506 517, 2007.
Youzheng Wu, Jun Zhao and Bo Xu. 2005. Chinese
Named Entity Recognition Model Based on Multiple
Features. In Proc. of HLT/EMNLP-2005, Vancouver,
Canada, pp.427-434.
Youzheng Wu, Jun Zhao, Xiangyu Duan and Bo Xu.
2004. Building an Evaluation Platform for Chinese
Question Answering Systems. In Proc. of the First
NCIRCS, China, December, 2004.
Yutaka Sasaki. 2005. Question Answering as Question-
Biased Term Extraction: A New Approach toward
Multilingual QA. In Proc. of ACL-2005, pp.215-222.
41
