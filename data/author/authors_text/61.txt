Proceedings of the Second Workshop on Statistical Machine Translation, pages 228?231,
Prague, June 2007. c?2007 Association for Computational Linguistics
Meteor: An Automatic Metric for MT Evaluation with High
Levels of Correlation with Human Judgments
Alon Lavie and Abhaya Agarwal
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
{alavie,abhayaa}@cs.cmu.edu
Abstract
Meteor is an automatic metric for Ma-
chine Translation evaluation which has been
demonstrated to have high levels of corre-
lation with human judgments of translation
quality, significantly outperforming the more
commonly used Bleu metric. It is one of
several automatic metrics used in this year?s
shared task within the ACL WMT-07 work-
shop. This paper recaps the technical de-
tails underlying the metric and describes re-
cent improvements in the metric. The latest
release includes improved metric parameters
and extends the metric to support evalua-
tion of MT output in Spanish, French and
German, in addition to English.
1 Introduction
Automatic Metrics for MT evaluation have been re-
ceiving significant attention in recent years. Evalu-
ating an MT system using such automatic metrics is
much faster, easier and cheaper compared to human
evaluations, which require trained bilingual evalua-
tors. Automatic metrics are useful for comparing
the performance of different systems on a common
translation task, and can be applied on a frequent
and ongoing basis during MT system development.
The most commonly used MT evaluation metric in
recent years has been IBM?s Bleu metric (Papineni
et al, 2002). Bleu is fast and easy to run, and it
can be used as a target function in parameter op-
timization training procedures that are commonly
used in state-of-the-art statistical MT systems (Och,
2003). Various researchers have noted, however, var-
ious weaknesses in the metric. Most notably, Bleu
does not produce very reliable sentence-level scores.
Meteor , as well as several other proposed metrics
such as GTM (Melamed et al, 2003), TER (Snover
et al, 2006) and CDER (Leusch et al, 2006) aim to
address some of these weaknesses.
Meteor , initially proposed and released in 2004
(Lavie et al, 2004) was explicitly designed to im-
prove correlation with human judgments of MT qual-
ity at the segment level. Previous publications on
Meteor (Lavie et al, 2004; Banerjee and Lavie,
2005) have described the details underlying the met-
ric and have extensively compared its performance
with Bleu and several other MT evaluation met-
rics. This paper recaps the technical details underly-
ing Meteor and describes recent improvements in
the metric. The latest release extends Meteor to
support evaluation of MT output in Spanish, French
and German, in addition to English. Furthermore,
several parameters within the metric have been opti-
mized on language-specific training data. We present
experimental results that demonstrate the improve-
ments in correlations with human judgments that re-
sult from these parameter tunings.
2 The Meteor Metric
Meteor evaluates a translation by computing a
score based on explicit word-to-word matches be-
tween the translation and a given reference trans-
lation. If more than one reference translation is
available, the translation is scored against each refer-
ence independently, and the best scoring pair is used.
Given a pair of strings to be compared, Meteor cre-
ates a word alignment between the two strings. An
alignment is mapping between words, such that ev-
ery word in each string maps to at most one word
in the other string. This alignment is incrementally
produced by a sequence of word-mapping modules.
The ?exact? module maps two words if they are ex-
actly the same. The ?porter stem? module maps two
words if they are the same after they are stemmed us-
ing the Porter stemmer. The ?WN synonymy? mod-
ule maps two words if they are considered synonyms,
based on the fact that they both belong to the same
?synset? in WordNet.
The word-mapping modules initially identify all
228
possible word matches between the pair of strings.
We then identify the largest subset of these word
mappings such that the resulting set constitutes an
alignment as defined above. If more than one maxi-
mal cardinality alignment is found, Meteor selects
the alignment for which the word order in the two
strings is most similar (the mapping that has the
least number of ?crossing? unigram mappings). The
order in which the modules are run reflects word-
matching preferences. The default ordering is to
first apply the ?exact? mapping module, followed by
?porter stemming? and then ?WN synonymy?.
Once a final alignment has been produced between
the system translation and the reference translation,
the Meteor score for this pairing is computed as
follows. Based on the number of mapped unigrams
found between the two strings (m), the total num-
ber of unigrams in the translation (t) and the total
number of unigrams in the reference (r), we calcu-
late unigram precision P = m/t and unigram recall
R = m/r. We then compute a parameterized har-
monic mean of P and R (van Rijsbergen, 1979):
Fmean =
P ? R
? ? P + (1? ?) ? R
Precision, recall and Fmean are based on single-
word matches. To take into account the extent to
which the matched unigrams in the two strings are
in the same word order, Meteor computes a penalty
for a given alignment as follows. First, the sequence
of matched unigrams between the two strings is di-
vided into the fewest possible number of ?chunks?
such that the matched unigrams in each chunk are
adjacent (in both strings) and in identical word or-
der. The number of chunks (ch) and the number of
matches (m) is then used to calculate a fragmenta-
tion fraction: frag = ch/m. The penalty is then
computed as:
Pen = ? ? frag?
The value of ? determines the maximum penalty
(0 ? ? ? 1). The value of ? determines the
functional relation between fragmentation and the
penalty. Finally, the Meteor score for the align-
ment between the two strings is calculated as:
score = (1? Pen) ? Fmean
In all previous versions of Meteor , the values of
the three parameters mentioned above were set to be:
? = 0.9, ? = 3.0 and ? = 0.5, based on experimen-
tation performed in early 2004. In the latest release,
we tuned these parameters to optimize correlation
with human judgments based on more extensive ex-
perimentation, as reported in section 4.
3 Meteor Implementations for
Spanish, French and German
We have recently expanded the implementation of
Meteor to support evaluation of translations in
Spanish, French and German, in addition to English.
Two main language-specific issues required adapta-
tion within the metric: (1) language-specific word-
matching modules; and (2) language-specific param-
eter tuning. The word-matching component within
the English version of Meteor uses stemming and
synonymy modules in constructing a word-to-word
alignment between translation and reference. The re-
sources used for stemming and synonymy detection
for English are the Porter Stemmer (Porter, 2001)
and English WordNet (Miller and Fellbaum, 2007).
In order to construct instances of Meteor for Span-
ish, French and German, we created new language-
specific ?stemming? modules. We use the freely
available Perl implementation packages for Porter
stemmers for the three languages (Humphrey, 2007).
Unfortunately, we have so far been unable to obtain
freely available WordNet resources for these three
languages. Meteor versions for Spanish, French
and German therefore currently include only ?exact?
and ?stemming? matching modules. We are investi-
gating the possibility of developing new synonymy
modules for the various languages based on alterna-
tive methods, which could then be used in place of
WordNet. The second main language-specific issue
which required adaptation is the tuning of the three
parameters within Meteor , described in section 4.
4 Optimizing Metric Parameters
The original version of Meteor (Banerjee and
Lavie, 2005) has instantiated values for three pa-
rameters in the metric: one for controlling the rela-
tive weight of precision and recall in computing the
Fmean score (?); one governing the shape of the
penalty as a function of fragmentation (?) and one
for the relative weight assigned to the fragmenta-
tion penalty (?). In all versions of Meteor to date,
these parameters were instantiated with the values
? = 0.9, ? = 3.0 and ? = 0.5, based on early data ex-
perimentation. We recently conducted a more thor-
ough investigation aimed at tuning these parameters
based on several available data sets, with the goal of
finding parameter settings that maximize correlation
with human judgments. Human judgments come in
the form of ?adequacy? and ?fluency? quantitative
scores. In our experiments, we looked at optimizing
parameters for each of these human judgment types
separately, as well as optimizing parameters for the
sum of adequacy and fluency. Parameter adapta-
229
Corpus Judgments Systems
NIST 2003 Ara-to-Eng 3978 6
NIST 2004 Ara-to-Eng 347 5
WMT-06 Eng-to-Fre 729 4
WMT-06 Eng-to-Ger 756 5
WMT-06 Eng-to-Spa 1201 7
Table 1: Corpus Statistics for Various Languages
tion is also an issue in the newly created Meteor
instances for other languages. We suspected that
parameters that were optimized to maximize corre-
lation with human judgments for English would not
necessarily be optimal for other languages.
4.1 Data
For English, we used the NIST 2003 Arabic-to-
English MT evaluation data for training and the
NIST 2004 Arabic-to-English evaluation data for
testing. For Spanish, German and French we used
the evaluation data provided by the shared task at
last year?s WMT workshop. Sizes of various corpora
are shown in Table 1. Some, but not all, of these data
sets have multiple human judgments per translation
hypothesis. To partially address human bias issues,
we normalize the human judgments, which trans-
forms the raw judgment scores so that they have sim-
ilar distributions. We use the normalization method
described in (Blatz et al, 2003). Multiple judgments
are combined into a single number by taking their
average.
4.2 Methodology
We performed a ?hill climbing? search to find the
parameters that achieve maximum correlation with
human judgments on the training set. We use Pear-
son?s correlation coefficient as our measure of corre-
lation. We followed a ?leave one out? training proce-
dure in order to avoid over-fitting. When n systems
were available for a particular language, we train the
parameters n times, leaving one system out in each
training, and pooling the segments from all other
systems. The final parameter values are calculated
as the mean of the n sets of trained parameters that
were obtained. When evaluating a set of parameters
on test data, we compute segment-level correlation
with human judgments for each of the systems in the
test set and then report the mean over all systems.
4.3 Results
4.3.1 Optimizing for Adequacy and Fluency
We trained parameters to obtain maximum cor-
relation with normalized adequacy and fluency judg-
Adequacy Fluency Sum
? 0.82 0.78 0.81
? 1.0 0.75 0.83
? 0.21 0.38 0.28
Table 2: Optimal Values of Tuned Parameters for
Different Criteria for English
Adequacy Fluency Sum
Original 0.6123 0.4355 0.5704
Adequacy 0.6171 0.4354 0.5729
Fluency 0.6191 0.4502 0.5818
Sum 0.6191 0.4425 0.5778
Table 3: Pearson Correlation with Human Judg-
ments on Test Data for English
ments separately and also trained for maximal corre-
lation with the sum of the two. The resulting optimal
parameter values on the training corpus are shown in
Table 2. Pearson correlations with human judgments
on the test set are shown in Table 3.
The optimal parameter values found are somewhat
different than our previous metric parameters (lower
values for all three parameters). The new parame-
ters result in moderate but noticeable improvements
in correlation with human judgments on both train-
ing and testing data. Tests for statistical significance
using bootstrap sampling indicate that the differ-
ences in correlation levels are all significant at the
95% level. Another interesting observation is that
precision receives slightly more ?weight? when op-
timizing correlation with fluency judgments (versus
when optimizing correlation with adequacy). Recall,
however, is still given more weight than precision.
Another interesting observation is that the value of
? is higher for fluency optimization. Since the frag-
mentation penalty reflects word-ordering, which is
closely related to fluency, these results are consistent
with our expectations. When optimizing correlation
with the sum of adequacy and fluency, optimal val-
ues fall in between the values found for adequacy and
fluency.
4.3.2 Parameters for Other Languages
Similar to English, we trained parameters for
Spanish, French and German on the available WMT-
06 training data. We optimized for maximum corre-
lation with human judgments of adequacy, fluency
and for the sum of the two. Resulting parameters
are shown in Table 4.3.2. For all three languages, the
parameters that were found to be optimal were quite
different than those that were found for English, and
using the language-specific optimal parameters re-
230
Adequacy Fluency Sum
French:? 0.86 0.74 0.76
? 0.5 0.5 0.5
? 1.0 1.0 1.0
German:? 0.95 0.95 0.95
? 0.5 0.5 0.5
? 0.6 0.8 0.75
Spanish:? 0.95 0.62 0.95
? 1.0 1.0 1.0
? 0.9 1.0 0.98
Table 4: Tuned Parameters for Different Languages
sults in significant gains in Pearson correlation levels
with human judgments on the training data (com-
pared with those obtained using the English opti-
mal parameters)1. Note that the training sets used
for these optimizations are comparatively very small,
and that we currently do not have unseen test data
to evaluate the parameters for these three languages.
Further validation will need to be performed once ad-
ditional data becomes available.
5 Conclusions
In this paper we described newly developed
language-specific instances of the Meteor metric
and the process of optimizing metric parameters for
different human measures of translation quality and
for different languages. Our evaluations demonstrate
that parameter tuning improves correlation with hu-
man judgments. The stability of the optimized pa-
rameters on different data sets remains to be inves-
tigated for languages other than English. We are
currently exploring broadening the set of features
used in Meteor to include syntax-based features
and alternative notions of synonymy. The latest re-
lease of Meteor is freely available on our website
at: http://www.cs.cmu.edu/~alavie/METEOR/
Acknowledgements
The work reported in this paper was supported by
NSF Grant IIS-0534932.
References
Satanjeev Banerjee and Alon Lavie. 2005. ME-
TEOR: An Automatic Metric for MT Evalua-
tion with Improved Correlation with Human Judg-
ments. In Proceedings of the ACL Workshop
on Intrinsic and Extrinsic Evaluation Measures
1Detailed tables are not included for lack of space.
for Machine Translation and/or Summarization,
pages 65?72, Ann Arbor, Michigan, June.
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. 2003. Confidence Es-
timation for Machine Translation. Technical Re-
port Natural Language Engineering Workshop Fi-
nal Report, Johns Hopkins University.
Marvin Humphrey. 2007. Perl In-
terface to Snowball Stemmers.
http://search.cpan.org/ creamyg/Lingua-Stem-
Snowball-0.941/lib/Lingua/Stem/Snowball.pm.
Alon Lavie, Kenji Sagae, and Shyamsundar Jayara-
man. 2004. The Significance of Recall in Auto-
matic Metrics for MT Evaluation. In Proceedings
of the 6th Conference of the Association for Ma-
chine Translation in the Americas (AMTA-2004),
pages 134?143, Washington, DC, September.
Gregor Leusch, Nicola Ueffing, and Hermann Ney.
2006. CDER: Efficient MT Evaluation Using
Block Movements. In Proceedings of the Thir-
teenth Conference of the European Chapter of the
Association for Computational Linguistics.
I. Dan Melamed, Ryan Green, and Joseph Turian.
2003. Precision and Recall of Machine Transla-
tion. In Proceedings of the HLT-NAACL 2003
Conference: Short Papers, pages 61?63, Edmon-
ton, Alberta.
George Miller and Christiane Fellbaum. 2007. Word-
Net. http://wordnet.princeton.edu/.
Franz Josef Och. 2003. Minimum Error Rate Train-
ing for Statistical Machine Translation. In Pro-
ceedings of the 41st Annual Meeting of the Asso-
ciation for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a Method for Auto-
matic Evaluation of Machine Translation. In Pro-
ceedings of 40th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 311?
318, Philadelphia, PA, July.
Martin Porter. 2001. The Porter Stem-
ming Algorithm. http://www.tartarus.org/ mar-
tin/PorterStemmer/index.html.
Matthew Snover, Bonnie Dorr, Richard Schwartz,
Linnea Micciulla, and John Makhoul. 2006. A
Study of Translation Edit Rate with Targeted Hu-
man Annotation. In Proceedings of the 7th Confer-
ence of the Association for Machine Translation in
the Americas (AMTA-2006), pages 223?231, Cam-
bridge, MA, August.
C. van Rijsbergen, 1979. Information Retrieval.
Butterworths, London, UK, 2nd edition.
231
Proceedings of the Third Workshop on Statistical Machine Translation, pages 115?118,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Meteor, m-bleu and m-ter: Evaluation Metrics for
High-Correlation with Human Rankings of Machine Translation
Output
Abhaya Agarwal and Alon Lavie
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
{abhayaa,alavie}@cs.cmu.edu
Abstract
This paper describes our submissions to the
machine translation evaluation shared task in
ACL WMT-08. Our primary submission is the
Meteor metric tuned for optimizing correla-
tion with human rankings of translation hy-
potheses. We show significant improvement
in correlation as compared to the earlier ver-
sion of metric which was tuned to optimized
correlation with traditional adequacy and flu-
ency judgments. We also describe m-bleu and
m-ter, enhanced versions of two other widely
used metrics bleu and ter respectively, which
extend the exact word matching used in these
metrics with the flexible matching based on
stemming and Wordnet in Meteor .
1 Introduction
Automatic Metrics for MT evaluation have been re-
ceiving significant attention in recent years. Evalu-
ating an MT system using such automatic metrics is
much faster, easier and cheaper compared to human
evaluations, which require trained bilingual evalua-
tors. The most commonly used MT evaluation met-
ric in recent years has been IBM?s Bleu metric (Pa-
pineni et al, 2002). Bleu is fast and easy to run,
and it can be used as a target function in parameter
optimization training procedures that are commonly
used in state-of-the-art statistical MT systems (Och,
2003). Various researchers have noted, however, var-
ious weaknesses in the metric. Most notably, Bleu
does not produce very reliable sentence-level scores.
Meteor , as well as several other proposed metrics
such as GTM (Melamed et al, 2003), TER (Snover
et al, 2006) and CDER (Leusch et al, 2006) aim to
address some of these weaknesses.
Meteor , initially proposed and released in 2004
(Lavie et al, 2004) was explicitly designed to im-
prove correlation with human judgments of MT qual-
ity at the segment level. Previous publications on
Meteor (Lavie et al, 2004; Banerjee and Lavie,
2005; Lavie and Agarwal, 2007) have described the
details underlying the metric and have extensively
compared its performance with Bleu and several
other MT evaluation metrics. In (Lavie and Agar-
wal, 2007), we described the process of tuning free
parameters within the metric to optimize the corre-
lation with human judgments and the extension of
the metric for evaluating translations in languages
other than English.
This paper provides a brief technical description of
Meteor and describes our experiments in re-tuning
the metric for improving correlation with the human
rankings of translation hypotheses corresponding to
a single source sentence. Our experiments show sig-
nificant improvement in correlation as a result of re-
tuning which shows the importance of having a met-
ric tunable to different testing conditions. Also, in
order to establish the usefulness of the flexible match-
ing based on stemming and Wordnet, we extend two
other widely used metrics bleu and ter which use
exact word matching, with the matcher module of
Meteor .
2 The Meteor Metric
Meteor evaluates a translation by computing a
score based on explicit word-to-word matches be-
tween the translation and a given reference trans-
lation. If more than one reference translation is
available, the translation is scored against each refer-
ence independently, and the best scoring pair is used.
Given a pair of strings to be compared, Meteor cre-
ates a word alignment between the two strings. An
alignment is mapping between words, such that ev-
ery word in each string maps to at most one word
in the other string. This alignment is incrementally
produced by a sequence of word-mapping modules.
The ?exact? module maps two words if they are ex-
actly the same. The ?porter stem? module maps two
words if they are the same after they are stemmed us-
115
ing the Porter stemmer. The ?WN synonymy? mod-
ule maps two words if they are considered synonyms,
based on the fact that they both belong to the same
?synset? in WordNet.
The word-mapping modules initially identify all
possible word matches between the pair of strings.
We then identify the largest subset of these word
mappings such that the resulting set constitutes an
alignment as defined above. If more than one maxi-
mal cardinality alignment is found, Meteor selects
the alignment for which the word order in the two
strings is most similar (the mapping that has the
least number of ?crossing? unigram mappings). The
order in which the modules are run reflects word-
matching preferences. The default ordering is to
first apply the ?exact? mapping module, followed by
?porter stemming? and then ?WN synonymy?.
Once a final alignment has been produced between
the system translation and the reference translation,
the Meteor score for this pairing is computed as
follows. Based on the number of mapped unigrams
found between the two strings (m), the total num-
ber of unigrams in the translation (t) and the total
number of unigrams in the reference (r), we calcu-
late unigram precision P = m/t and unigram recall
R = m/r. We then compute a parametrized har-
monic mean of P and R (van Rijsbergen, 1979):
Fmean =
P ?R
? ? P + (1? ?) ?R
Precision, recall and Fmean are based on single-
word matches. To take into account the extent to
which the matched unigrams in the two strings are
in the same word order, Meteor computes a penalty
for a given alignment as follows. First, the sequence
of matched unigrams between the two strings is di-
vided into the fewest possible number of ?chunks?
such that the matched unigrams in each chunk are
adjacent (in both strings) and in identical word or-
der. The number of chunks (ch) and the number of
matches (m) is then used to calculate a fragmenta-
tion fraction: frag = ch/m. The penalty is then
computed as:
Pen = ? ? frag?
The value of ? determines the maximum penalty
(0 ? ? ? 1). The value of ? determines the
functional relation between fragmentation and the
penalty. Finally, the Meteor score for the align-
ment between the two strings is calculated as:
score = (1 ? Pen) ? Fmean
The free parameters in the metric, ?, ? and ? are
tuned to achieve maximum correlation with the hu-
man judgments as described in (Lavie and Agarwal,
2007).
3 Extending Bleu and Ter with
Flexible Matching
Many widely used metrics like Bleu (Papineni et al,
2002) and Ter (Snover et al, 2006) are based on
measuring string level similarity between the refer-
ence translation and translation hypothesis, just like
Meteor . Most of them, however, depend on find-
ing exact matches between the words in two strings.
Many researchers (Banerjee and Lavie, 2005; Liu and
Gildea, 2006), have observed consistent gains by us-
ing more flexible matching criteria. In the following
experiments, we extend the Bleu and Ter metrics
to use the stemming and Wordnet based word map-
ping modules from Meteor .
Given a translation hypothesis and reference pair,
we first align them using the word mapping modules
from Meteor . We then rewrite the reference trans-
lation by replacing the matched words with the cor-
responding words in the translation hypothesis. We
now compute Bleu and Ter with these new refer-
ences without changing anything inside the metrics.
To get meaningful Bleu scores at segment level,
we compute smoothed Bleu as described in (Lin and
Och, 2004).
4 Re-tuning Meteor for Rankings
(Callison-Burch et al, 2007) reported that the inter-
coder agreement on the task of assigning ranks to
a given set of candidate hypotheses is much better
than the intercoder agreement on the task of assign-
ing a score to a hypothesis in isolation. Based on
that finding, in WMT-08, only ranking judgments
are being collected from the human judges.
The current version of Meteor uses parameters
optimized towards maximizing the Pearson?s corre-
lation with human judgments of adequacy scores. It
is not clear that the same parameters would be op-
timal for correlation with human rankings. So we
would like to re-tune the parameters in the metric
for maximizing the correlation with ranking judg-
ments instead. This requires computing full rankings
according to the metric and the humans and then
computing a suitable correlation measure on those
rankings.
4.1 Computing Full Rankings
Meteor assigns a score between 0 and 1 to every
translation hypothesis. This score can be converted
116
Language Judgments
Binary Sentences
English 3978 365
German 2971 334
French 1903 208
Spanish 2588 284
Table 1: Corpus Statistics for Various Languages
to rankings trivially by assuming that a higher score
indicates a better hypothesis.
In development data, human rankings are avail-
able as binary judgments indicating the preferred hy-
pothesis between a given pair. There are also cases
where both the hypotheses in the pair are judged to
be equal. In order to convert these binary judgments
into full rankings, we do the following:
1. Throw out all the equal judgments.
2. Construct a directed graph where nodes corre-
spond to the translation hypotheses and every
binary judgment is represented by a directed
edge between the corresponding nodes.
3. Do a topological sort on the resulting graph and
assign ranks in the sort order. The cycles in the
graph are broken by assigning same rank to all
the nodes in the cycle.
4.2 Measuring Correlation
Following (Ye et al, 2007), we first compute the
Spearman correlation between the human rankings
and Meteor rankings of the translation hypotheses
corresponding to a single source sentence. Let N be
the number of translation hypotheses and D be the
difference in ranks assigned to a hypothesis by two
rankings, then Spearman correlation is given by:
r = 1?
6
?
D2
N(N2 ? 1)
The final score for the metric is the average of the
Spearman correlations for individual sentences.
5 Experiments
5.1 Data
We use the human judgment data from WMT-07
which was released as development data for the eval-
uation shared task. Amount of data available for
various languages is shown in Table 1. Development
data contains the majority judgments (not every hy-
potheses pair was judged by same number of judges)
which means that in the cases where multiple judges
judged the same pair of hypotheses, the judgment
given by majority of the judges was considered.
English German French Spanish
? 0.95 0.9 0.9 0.9
? 0.5 3 0.5 0.5
? 0.45 0.15 0.55 0.55
Table 2: Optimal Values of Tuned Parameters for Various
Languages
Original Re-tuned
English 0.3813 0.4020
German 0.2166 0.2838
French 0.2992 0.3640
Spanish 0.2021 0.2186
Table 3: Average Spearman Correlation with Human
Rankings for Meteor on Development Data
5.2 Methodology
We do an exhaustive grid search in the feasible ranges
of parameter values, looking for parameters that
maximize the average Spearman correlation over the
training data. To get a fair estimate of performance,
we use 3-fold cross validation on the development
data. Final parameter values are chosen as the best
performing set on the data pooled from all the folds.
5.3 Results
5.3.1 Re-tuning Meteor for Rankings
The re-tuned parameter values are shown in Ta-
ble 2 while the average Spearman correlations for
various languages with original and re-tuned param-
eters are shown in Table 3. We get significant im-
provements for all the languages. Gains are specially
pronounced for German and French.
Interestingly, weight for recall becomes even higher
than earlier parameters where it was already high.
So it seems that ranking judgments are almost en-
tirely driven by the recall in all the languages. Also
the re-tuned parameters for all the languages except
German are quite similar.
5.3.2 m-bleu and m-ter
Table 4 shows the average Spearman correlations
of m-bleu and m-ter with human rankings. For
English, both m-bleu and m-ter show considerable
improvements. For other languages, improvements
in m-ter are smaller but consistent. m-bleu , how-
ever, doesn?t shows any improvements in this case.
A possible reason for this behavior can be the lack of
a ?WN synonymy? module for languages other than
English which results in fewer extra matches over the
exact matching baseline. Additionally, French, Ger-
man and Spanish have a richer morphology as com-
pared to English. The morphemes in these languages
117
Exact Match Flexible Match
English: Bleu 0.2486 0.2747
Ter 0.1598 0.2033
French: Bleu 0.2906 0.2889
Ter 0.2472 0.2604
German: Bleu 0.1829 0.1806
Ter 0.1509 0.1668
Spanish: Bleu 0.1804 0.1847
Ter 0.1787 0.1839
Table 4: Average Spearman Correlation with Human
Rankings for m-bleu and m-ter
carry much more information and different forms of
the same word may not be as freely replaceable as in
English. A more fine grained strategy for matching
words in these languages remains an area of further
investigation.
6 Conclusions
In this paper, we described the re-tuning of Me-
teor parameters to better correlate with human
rankings of translation hypotheses. Results on the
development data indicate that the re-tuned ver-
sion is significantly better at predicting ranking than
the earlier version. We also presented enhanced
Bleu and Ter that use the flexible word match-
ing module from Meteor and show that this re-
sults in better correlations as compared to the de-
fault exact matching versions. The new version of
Meteor will be soon available on our website at:
http://www.cs.cmu.edu/~alavie/METEOR/ . This
release will also include the flexible word matcher
module which can be used to extend any metric with
the flexible matching.
Acknowledgments
The work reported in this paper was supported by
NSF Grant IIS-0534932.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Translation
and/or Summarization, pages 65?72, Ann Arbor,
Michigan, June.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2007. (meta-)
evaluation of machine translation. In Proceedings of
the Second Workshop on Statistical Machine Transla-
tion, pages 136?158, Prague, Czech Republic, June.
Association for Computational Linguistics.
Alon Lavie and Abhaya Agarwal. 2007. METEOR: An
Automatic Metric for MT Evaluation with High Levels
of Correlation with Human Judgments. In Proceedings
of the Second ACL Workshop on Statistical Machine
Translation, pages 228?231, Prague, Czech Republic,
June.
Alon Lavie, Kenji Sagae, and Shyamsundar Jayaraman.
2004. The Significance of Recall in Automatic Metrics
for MT Evaluation. In Proceedings of the 6th Confer-
ence of the Association for Machine Translation in the
Americas (AMTA-2004), pages 134?143, Washington,
DC, September.
Gregor Leusch, Nicola Ueffing, and Hermann Ney. 2006.
CDER: Efficient MT Evaluation Using Block Move-
ments. In Proceedings of the Thirteenth Conference of
the European Chapter of the Association for Compu-
tational Linguistics.
Chin-Yew Lin and Franz Josef Och. 2004. Orange: a
method for evaluating automatic evaluation metrics
for machine translation. In COLING ?04: Proceedings
of the 20th international conference on Computational
Linguistics, page 501, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Ding Liu and Daniel Gildea. 2006. Stochastic itera-
tive alignment for machine translation evaluation. In
Proceedings of the COLING/ACL on Main conference
poster sessions, pages 539?546, Morristown, NJ, USA.
Association for Computational Linguistics.
I. Dan Melamed, Ryan Green, and Joseph Turian. 2003.
Precision and Recall of Machine Translation. In Pro-
ceedings of the HLT-NAACL 2003 Conference: Short
Papers, pages 61?63, Edmonton, Alberta.
Franz Josef Och. 2003. Minimum Error Rate Training
for Statistical Machine Translation. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 311?318, Philadelphia, PA,
July.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. In Proceedings of the 7th Conference of the
Association for Machine Translation in the Americas
(AMTA-2006), pages 223?231, Cambridge, MA, Au-
gust.
C. van Rijsbergen, 1979. Information Retrieval. Butter-
worths, London, UK, 2nd edition.
Yang Ye, Ming Zhou, and Chin-Yew Lin. 2007. Sen-
tence level machine translation evaluation as a rank-
ing. In Proceedings of the Second Workshop on Sta-
tistical Machine Translation, pages 240?247, Prague,
Czech Republic, June. Association for Computational
Linguistics.
118
Proceedings of the Third Workshop on Statistical Machine Translation, pages 163?166,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Statistical Transfer Systems for French?English
and German?English Machine Translation
Greg Hanneman and Edmund Huber and Abhaya Agarwal and Vamshi Ambati
and Alok Parlikar and Erik Peterson and Alon Lavie
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213 USA
{ghannema, ehuber, abhayaa, vamshi, aup, eepeter, alavie}@cs.cmu.edu
Abstract
We apply the Stat-XFER statistical transfer
machine translation framework to the task of
translating from French and German into En-
glish. We introduce statistical methods within
our framework that allow for the principled
extraction of syntax-based transfer rules from
parallel corpora given word alignments and
constituency parses. Performance is evaluated
on test sets from the 2007 WMT shared task.
1 Introduction
The Carnegie Mellon University statistical trans-
fer (Stat-XFER) framework is a general search-
based and syntax-driven framework for develop-
ing MT systems under a variety of data condi-
tions (Lavie, 2008). At its core is a transfer en-
gine using two language-pair-dependent resources:
a grammar of weighted synchronous context-free
rules (possibly augmented with unification-style fea-
ture constraints), and a probabilistic bilingual lexi-
con of syntax-based word- and phrase-level transla-
tions. The Stat-XFER framework has been used to
develop research MT systems for a number of lan-
guage pairs, including Chinese?English, Hebrew?
English, Urdu?English, and Hindi?English.
In this paper, we describe our use of the frame-
work to create new French?English and German?
English MT systems for the 2008 Workshop on Sta-
tistical Machine Translation shared translation task.
We first describe the acquisition and processing of
resources for each language pair and the roles of
those resources within the Stat-XFER system (Sec-
tion 2); we then report results on common test sets
(Section 3) and share some early analysis and future
directions (Section 4).
2 System Description
Building a new machine translation system under
the Stat-XFER framework involves constructing a
bilingual translation lexicon and a transfer gram-
mar. Over the past six months, we have developed
new methods for extracting syntax-based translation
lexicons and transfer rules fully automatically from
parsed and word-aligned parallel corpora. These
new methods are described in detail by Lavie et
al. (2008). Below, we detail the statistical meth-
ods by which these resources were extracted for our
French?English and German?English systems.
2.1 Lexicon
The bilingual lexicon is automatically extracted
from automatically parsed and word-aligned paral-
lel corpora. To obtain high-quality statistical word
alignments, we run GIZA++ (Och and Ney, 2003)
in both the source-to-target and target-to-source di-
rections, then combine the resulting alignments with
the Sym2 symmetric alignment heuristic of Ortiz-
Mart??nez et al (2005)1. From this data, we extract a
lexicon of both word-to-word and syntactic phrase-
to-phrase translation equivalents.
The word-level correspondences are extracted di-
rectly from the word alignments: parts of speech for
these lexical entries are obtained from the preter-
1We use Sym2 over more well-known heuristics such as
?grow-diag-final? because Sym2 has been shown to give the
best results for the node-alignment subtask that is part of our
processing chain.
163
ws cs wt ct r
paru V appeared V 0.2054
paru V seemed V 0.1429
paru V found V 0.0893
paru V published V 0.0804
paru V felt V 0.0714
.
.
.
.
.
.
.
.
.
paru V already ADV 0.0089
paru V appear V 0.0089
paru V authoritative ADJ 0.0089
Table 1: Part of the lexical entry distribution for the
French (source) word paru.
minal nodes of parse trees of the source and target
sentences. If parsers are unavailable for either lan-
guage, we have also experimented with determin-
ing parts of speech with independent taggers such
as TreeTagger (Schmid, 1995). Alternatively, parts
of speech may be projected through the word align-
ments from one language to the other if the infor-
mation is available on at least one side. Syntactic
phrase-level correspondences are extracted from the
parallel data by applying the PFA node alignment
algorithm described by Lavie et al (2008). The
yields of the aligned parse tree nodes are extracted
as constituent-level translation equivalents.
Each entry in the lexicon is assigned a rule score,
r, based on its source-side part of speech cs, source-
side text ws, target-side part of speech ct, and target-
side text wt. The score is a maximum-likelihood es-
timate of the distribution of target-language transla-
tion and source- and target-language parts of speech,
given the source word or phrase.
r = p(wt, ct, cs |ws) (1)
? #(wt, ct, ws, cs)#(ws) + 1
(2)
We employ add-one smoothing in the denominator
of Equation 2 to counteract overestimation in the
case that #(ws) is small. Rule scores provide a way
to promote the more likely translation alternatives
while still retaining a high degree of diversity in the
lexicon. Table 1 shows part of the lexical distribu-
tion for the French (source) word paru.
The result of the statistical word alignment pro-
cess and lexical extraction is a bilingual lexicon con-
taining 1,064,755 entries for French?English and
1,111,510 entries for German?English. Sample lex-
ical entries are shown in Figure 1.
2.2 Grammar
Transfer grammars for our earlier statistical transfer
systems were manually created by in-house experts
of the languages involved and were therefore small.
The Stat-XFER framework has since been extended
with procedures for automatic grammar acquisition
from a parallel corpus, given constituency parses for
source or target data or both. Our French and Ger-
man systems used the context-free grammar rule ex-
traction process described by Lavie et al (2008).
For French, we used 300,000 parallel sentences from
the Europarl training data parsed on the English side
with the Stanford parser (Klein and Manning, 2003)
and on the French side with the Xerox XIP parser
(A??t-Mokhtar et al, 2001). For German, we used
300,000 Europarl sentence pairs parsed with the En-
glish and German versions of the Stanford parser2.
The set of rules extracted from the parsed corpora
was filtered down after scoring to improve system
performance and run time. The final French rule set
was comprised of the 1500 most frequently occur-
ring rules. For German, rules that occurred less than
twice were filtered out, leaving a total of 16,469. In
each system, rule scores were again calculated by
Equation 2, with ws and wt representing the full
right-hand sides of the source and target grammar
rules.
A secondary version of our French system used a
word-level lexicon extracted from the intersection,
rather than the symmetricization, of the GIZA++
alignments in each direction; we hypothesize that
this tends to improve precision at the expense of re-
call. The word-level lexicon was supplemented with
syntax-based phrase-level entries obtained from the
PFA node alignment algorithm. The grammar
contained the 700 highest-frequency and the 500
highest-scoring rules extracted from the parallel
parsed corpus. This version had a total lexicon size
of 2,023,531 entries and a total grammar of 1034
rules after duplicates were removed. Figure 2 shows
2Due to a combination of time constraints and paucity of
computational resources, only a portion of the Europarl parallel
corpus was utilized, and none of the supplementary news com-
mentary training data was integrated.
164
)(
{VS,248840}
V::V |: ["paru"] ?> ["appeared"]
  (*score* 0.205357142857143)
)
  (*score* 0.763636363636364)
{NP,2000012}
NP::NP |: ["ein" "Beispiel"] ?> ["an" "example"]
(
Figure 1: Sample lexical entries for French and German.
sample grammar rules automatically learned by the
process described above.
2.3 Transfer Engine
The Stat-XFER transfer engine runs in a two-stage
process, first applying the grammar and lexicon
to an input sentence, then running a decoder over
the resulting lattice of scored translation pieces.
Scores for each translation piece are based on a
log-linear combination of several features: language
model probability, rule scores, source-given-target
and target-given-source lexical probabilities, parse
fragmentation, and length. For more details, see
Lavie (2008). The use of a German transfer gram-
mar an order of magnitude larger than the corre-
sponding French grammar was made possible due to
a recent optimization made in the engine. When en-
abled, it constrains the search of translation hypothe-
ses to the space of hypotheses whose structure satis-
fies the consituent structure of a source-side parse.
3 Evaluation
We trained our model parameters on a subset of
the provided ?dev2006? development set, optimiz-
ing for case-insensitive IBM-style BLEU (Papineni
et al, 2002) with several iterations of minimum error
rate training on n-best lists. In each iteration?s list,
we also included the lists from previous iterations in
order to maintain a diversity of hypothesis types and
scores. The provided ?test2007? and ?nc-test2007?
data sets, identical with the test data from the 2007
Workshop on Statistical Machine Translation shared
task, were used as internal development tests.
Tables 2, 3, and 4 report scores on these data sets
for our primary French, secondary French, and Ger-
man systems. We report case-insensitive scores for
version 0.6 of METEOR (Lavie and Agarwal, 2007)
with all modules enabled, version 1.04 of IBM-style
BLEU (Papineni et al, 2002), and version 5 of TER
(Snover et al, 2006).
Data Set METEOR BLEU TER
dev2006 0.5332 0.2063 64.81
test2007 0.5358 0.2078 64.75
nc-test2007 0.5369 0.1719 69.83
Table 2: Results for the primary French?English system
on provided development and development test sets.
Data Set METEOR BLEU TER
dev2006 0.5330 0.2086 65.02
test2007 0.5386 0.2129 64.29
nc-test2007 0.5311 0.1680 70.90
Table 3: Results for the secondary French?English sys-
tem on provided development and development test sets.
4 Analysis and Conclusions
From the development test results in Section 3, we
note that the Stat-XFER systems? performance cur-
rently lags behind the state-of-the-art scores on the
2007 test data3. This may be in part due to the low
volume of training data used for rule learning. A key
research question in our approach is how to distin-
guish low-frequency correct and useful transfer rules
from ?noisy? rules that are due to parser errors and
incorrect word alignments. We believe that learning
rules from more data will help alleviate this prob-
lem by proportionally increasing the counts of good
rules compared to incorrect ones. We also plan to
study methods for more effective rule set pruning,
regardless of the volume of training data used.
The difference in metric scores between in-
domain and out-of-domain data is partly due to ef-
fects of reference length on the metrics used. De-
tailed output from METEOR and BLEU shows that
the reference translations for the test2007 set are
about 94% as long as the primary French?English
3Top scores on the 2007 test data are approximately 0.60
METEOR, 0.33 BLEU, and 57.6 TER. See Callison-Burch et
al. (2007) for full results.
165
(
  (*score* 0.866050808314088
)
{PP,1627955}
PP:PP [PRE "d?" "autres" N] ?> [PRE "other" N]
  (X1::Y1)
  (X4::Y3)
)
(
{PP,3000085}
PP:ADVP ["vor" CARD "Monaten"] ?> [NUM "months" "ago"]
  (*score* 0.9375)
  (X2::Y1)
)
Figure 2: Sample grammar rules for French and German.
Data Set METEOR BLEU TER
dev2006 0.4967 0.1794 68.68
test2007 0.5052 0.1878 67.94
nc-test2007 0.4939 0.1347 74.38
Table 4: Results for the German?English system on pro-
vided development and development test sets.
system?s translations. On this set, our system has
approximately balanced precision (0.62) and recall
(0.66). However, the nc-test2007 references are only
84% as long as our output, a situation that hurts our
system?s precision (0.57) but boosts its recall (0.68).
METEOR, as a metric that favors recall, shows a
negligible increase in score between these two test
sets, while BLEU and TER report significant relative
drops of 17.3% and 7.8%. This behavior appears to
be consistent on the test2007 and nc-test2007 data
sets across systems (Callison-Burch et al, 2007).
Acknowledgments
This research was supported in part by NSF grants
IIS-0121631 (AVENUE) and IIS-0534217 (LE-
TRAS), and by the DARPA GALE program. We
thank the members of the Parsing and Semantics
group at Xerox Research Centre Europe for assisting
in parsing the French data using their XIP parser.
References
Salah A??t-Mokhtar, Jean-Pierre Chanod, and Claude
Roux. 2001. A multi-input dependency parser. In
Proceedings of the Seventh International Workshop on
Parsing Technologies, Beijing, China, October.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2007. (Meta-)
evaluation of machine translation. In Proceedings of
the Second Workshop on Statistical Machine Transla-
tion, pages 136?158, Prague, Czech Republic, June.
Dan Klein and Christopher D. Manning. 2003. Fast exact
inference with a factored model for natural language
parsing. In Advances in Neural Information Process-
ing Systems 15, pages 3?10. MIT Press, Cambridge,
MA.
Alon Lavie and Abhaya Agarwal. 2007. METEOR: An
automatic metric for MT evaluation with high levels of
correlation with human judgments. In Proceedings of
the Second Workshop on Statistical Machine Transla-
tion, pages 228?231, Prague, Czech Republic, June.
Alon Lavie, Alok Parlikar, and Vamshi Ambati. 2008.
Syntax-driven learning of sub-sentential translation
equivalents and translation rules from parsed paral-
lel corpora. In Proceedings of the Second Work-
shop on Syntax and Structure in Statistical Transla-
tion, Columbus, OH, June. To appear.
Alon Lavie. 2008. Stat-XFER: A general search-based
syntax-driven framework for machine translation. In
Computational Linguistics and Intelligent Text Pro-
cessing, Lecture Notes in Computer Science, pages
362?375. Springer.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Daniel Ortiz-Mart??nez, Ismael Garc??a-Varea, and Fran-
cisco Casacuberta. 2005. Thot: A toolkit to train
phrase-based models for statistical machine transla-
tion. In Proceedings of the 10th Machine Translation
Summit, pages 141?148, Phuket, Thailand, September.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eva-
lution of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics, pages 311?318, Philadelphia, PA,
July.
Helmut Schmid. 1995. Improvements in part-of-speech
tagging with an application to German. In Proceed-
ings of the ACL SIGDAT Workshop.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In
Proceedings of the Seventh Conference of the Associ-
ation for Machine Translation in the Americas, pages
223?231, Cambridge, MA, August.
166
