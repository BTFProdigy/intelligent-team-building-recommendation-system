Inducing Information Extraction Systems for New Languages
via Cross-Language Projection
Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT 84112
riloff@cs.utah.edu
Charles Schafer and David Yarowsky
Department of Computer Science
Johns Hopkins University
Baltimore, MD 21218
{cschafer,yarowsky}@cs.jhu.edu
Abstract
Information extraction (IE) systems are costly to
build because they require development texts, pars-
ing tools, and specialized dictionaries for each ap-
plication domain and each natural language that
needs to be processed. We present a novel
method for rapidly creating IE systems for new lan-
guages by exploiting existing IE systems via cross-
language projection. Given an IE system for a
source language (e.g., English), we can transfer its
annotations to corresponding texts in a target lan-
guage (e.g., French) and learn information extrac-
tion rules for the new language automatically. In
this paper, we explore several ways of realizing both
the transfer and learning processes using off-the-
shelf machine translation systems, induced word
alignment, attribute projection, and transformation-
based learning. We present a variety of experiments
that show how an English IE system for a plane
crash domain can be leveraged to automatically cre-
ate a French IE system for the same domain.
1 Introduction
Information extraction (IE) is an important appli-
cation for natural language processing, and recent
research has made great strides toward making IE
systems easily portable across domains. However,
IE systems depend on parsing tools and specialized
dictionaries that are language specific, so they are
not easily portable across languages. In this re-
search, we explore the idea of using an information
extraction system designed for one language to au-
tomatically create a comparable information extrac-
tion system for a different language.
To achieve this goal, we rely on the idea of cross-
language projection. The basic approach is the fol-
lowing. First, we create an artificial parallel cor-
pus by applying an off-the-shelf machine translation
(MT) system to source language text (here, English)
to produce target language text (here, French). Or
conversely, in some experiments we generate a par-
allel corpus by applying MT to a French corpus
to produce artificial English. We then run a word
alignment algorithm over the parallel corpus. Next,
we apply an English IE system to the English texts
and project the IE annotations over to the corre-
sponding French words via the induced word align-
ments. In effect, this produces an automatically an-
notated French corpus. We explore several strate-
gies for transferring the English IE annotations to
the target language, including evaluation of the
French annotations produced by the direct projec-
tion alone, as well as the use of transformation-
based learning to create French extraction rules
from the French annotations.
2 Information Extraction
The goal of information extraction systems is to
identify and extract facts from natural language text.
IE systems are usually designed for a specific do-
main, and the types of facts to be extracted are de-
fined in advance. In this paper, we will focus on the
domain of plane crashes and will try to extract de-
scriptions of the vehicle involved in the crash, vic-
tims of the crash, and the location of the crash.
Most IE systems use some form of extraction
patterns to recognize and extract relevant informa-
tion. Many techniques have been developed to gen-
erate extraction patterns for a new domain automat-
ically, including PALKA (Kim & Moldovan, 1993),
AutoSlog (Riloff, 1993), CRYSTAL (Soderland et
al., 1995), RAPIER (Califf, 1998), SRV (Freitag,
1998), meta-bootstrapping (Riloff & Jones, 1999),
and ExDisco (Yangarber et al, 2000). For this
work, we will use AutoSlog-TS (Riloff, 1996b) to
generate IE patterns for the plane crash domain.
AutoSlog-TS is a derivative of AutoSlog that auto-
matically generates extraction patterns by gathering
statistics from a corpus of relevant texts (within the
domain) and irrelevant texts (outside the domain).
Each extraction pattern represents a linguistic ex-
pression that can extract noun phrases from one of
three syntactic positions: subject, direct object, or
object of a prepositional phrase. For example, the
following patterns could extract vehicles involved
in a plane crash: ?<subject> crashed?, ?hijacked
<direct-object>?, and ?wreckage of <np>?.
We trained AutoSlog-TS using AP news stories
about plane crashes as the relevant text, and AP
news stories that do not mention plane crashes as
the irrelevant texts. AutoSlog-TS generates a list
of extraction patterns, ranked according to their as-
sociation with the domain. A human must review
this list to decide which patterns are useful for the
IE task and which ones are not. We manually re-
viewed the top patterns and used the accepted pat-
terns for the experiments described in this paper. To
apply the extraction patterns to new text, we used a
shallow parser called Sundance that also performs
information extraction.
3 Cross-Language Projection
3.1 Motivation and Previous Projection Work
Not all languages have received equal investment
in linguistic resources and tool development. For
a select few, resource-rich languages such as En-
glish, annotated corpora and text analysis tools are
readily available. However, for the large majority
of the world?s languages, resources such as tree-
banks, part-of-speech taggers, and parsers do not
exist. And even for many of the better-supported
languages, cutting edge analysis tools in areas such
as information extraction are not readily available.
One solution to this NLP-resource disparity is
to transfer linguistic resources, tools, and do-
main knowledge from resource-rich languages to
resource-impoverished ones. In recent years, there
has been a burst of projects based on this paradigm.
Yarowsky et al (2001) developed cross-language
projection models for part-of-speech tags, base
noun phrases, named-entity tags, and morpholog-
ical analysis (lemmatization) for four languages.
Resnik et al (2001) developed related models for
projecting dependency parsers from English to Chi-
nese. There has also been extensive work on the
cross-language transfer and development of ontolo-
gies and WordNets (e.g., (Atserias et al, 1997)).
3.2 Mechanics of Projection
The cross-language projection methodology em-
ployed in this paper is based on Yarowsky et al
(2001), with one important exception. Given the
absence of available naturally occurring bilingual




































 
 
 
 
LOCATION
VICTIM
tuant ses  20 occupants
was crushed Thursday evening in the south?east of Haiti  ,
A two?motor aircraft Beechcraft of the Air?Saint?Martin company
Un avion bi?moteur Beechcraft de la compagnie Air?Saint?Martin
VEHICLE
killing its 20 occupants 
s? est ?cras? jeudi soir dans le sud?est   d?   Haiti    ,
.
.
Figure 1: French text word aligned with its English
machine translation (extractions highlighted)
corpora in our target domain, we employ commer-
cial, off-the-shelf machine translation to generate
an artificial parallel corpus. While machine transla-
tion errors present substantial problems, MT offers
great opportunities because it frees cross-language
projection research from the relatively few large
existing bilingual corpora (such as the Canadian
Hansards). MT allows projection to be performed
on any corpus, such as the domain-specific plane-
crash news stories employed here. Section 5 gives
the details of the MT system and corpora that we
used.
Once the artificial parallel corpus has been cre-
ated, we apply an English IE system to the English
texts and transfer the IE annotations to the target
language as follows:
1. Sentence align the parallel corpus.1
2. Word-align the parallel corpus using the
Giza++ system (Och and Ney, 2000).
3. Transfer English IE annotations and noun-
phrase boundaries to French via the mecha-
nism described in Yarowsky et al (2001),
yielding annotated sentence pairs as illustrated
in Figure 1.
4. Train a stand-alone IE tagger on these pro-
jected annotations (described in Section 4).
4 Transformation-Based Learning
We used transformation-based learning (TBL)
(Brill, 1995) to learn information extraction rules
for French. TBL is well-suited for this task because
it uses rule templates as the basis for learning, which
can be easily modeled after English extraction pat-
terns. However, information extraction systems typ-
ically rely on a shallow parser to identify syntactic
elements (e.g., subjects and direct objects) and verb
1This is trivial because each sentence has a numbered an-
chor preserved by the MT system.
constructions (e.g., passive vs. active voice). Our
hope was that the rules learned by TBL would be ap-
plicable to new French texts without the need for a
French parser. One of our challenges was to design
rule templates that could approximate the recogni-
tion of syntactic structures well enough to duplicate
most of the functionality of a French shallow parser.
When our TBL training begins, the initial state is
that no words are annotated. We experimented with
two sets of ?truth? values: Sundance?s annotations
and human annotations. We defined 56 language-
independent rule templates, which can be broken
down into four sets designed to produce different
types of behavior. Lexical N-gram rule templates
change the annotation of a word if the word(s) im-
mediately surrounding it exactly match the rule. We
defined rule templates for 1, 2, and 3-grams. In
Table 1, Rules 1-3 are examples of learned Lexi-
cal N-gram rules. Lexical+POS N-gram rule tem-
plates can match exact words or part-of-speech tags.
Rules 4-5 are Lexical+POS N-gram rules. Rule 5
will match verb phrases such as ?went down in?,
?shot down in?, and ?came down in?.
One of the most important functions of a parser is
to identify the subject of a sentence, which may be
several words away from the main verb phrase. This
is one of the trickest behaviors to duplicate without
the benefit of syntactic parsing. We designed Sub-
ject Capture rule templates to identify words that
are likely to be a syntactic subject. As an example,
Rule 6 looks for an article at the beginning of a sen-
tence and the word ?crashed? a few words ahead2,
and infers that the article belongs to a vehicle noun
phrase. (The NP Chaining rules described next will
extend the annotation to include the rest of the noun
phrase.) Rule 7 attempts relative pronoun disam-
biguation when it finds the three tokens ?COMMA
which crashed? and infers that the word preceding
the comma is a vehicle.
Without the benefit of a parser, another challenge
is identifying noun phrase boundaries. We designed
NP Chaining rule templates to look at words that
have already been labelled and extend the bound-
aries of the annotation to cover a complete noun
phrase. As examples, Rules 8 and 9 extend loca-
tion and victim annotations to the right, and Rule 10
extends a vehicle annotation to the left.
2? is a start-of-sentence token. w
4?7
means that the item
occurs in the range of word
4
through word
7
.
Rule Condition Rule Effect
1. w
1
=crashed w
2
=in w
3
is LOC.
2. w
1
=wreckage w
2
=of w
3
is VEH.
3. w
1
=injuring w
2
is VIC.
4. w
1
=NOUN w
2
=crashed w
1
is VEH.
5. w
1
=VERB w
2
=down w
3
=in w
4
is LOC.
6. w
1
=? w
2
=ART w
4?7
=crashed w
2
is VEH.
7. w
2
=COMMA w
3
=which w
4
=crashed w
1
is VEH.
8. w
1
=in w
2
=LOCATION w
3
=NOUN w
3
is LOC.
9. w
1
=VERB w
2
=VICTIM w
3
=NOUN w
3
is VIC.
10. w
1
=ART w
2
=VEHICLE w
1
is VEH.
Table 1: Examples of Learned TBL Rules
(LOC.=location, VEH.=vehicle, VIC.=victim)
5 Resources
The corpora used in these experiments were ex-
tracted from English and French AP news stories.
We created the corpora automatically by searching
for articles that contain plane crash keywords. The
news streams for the two languages came from dif-
ferent years, so the specific plane crash events de-
scribed in the two corpora are disjoint. The En-
glish corpus contains roughly 420,000 words, and
the French corpus contains about 150,000 words.
For each language, we hired 3 fluent university
students to do annotation. We instructed the anno-
tators to read each story and mark relevant entities
with SGML-style tags. Possible labels were loca-
tion of a plane crash, vehicle involved in a crash,
and victim (any persons killed, injured, or surviv-
ing a crash). We asked the annotators to align their
annotations with noun phrase boundaries. The an-
notators marked up 1/3 of the English corpus and
about 1/2 of the French corpus.
We used a high-quality commercial machine
translation (MT) program (Systran Professional
Edition) to generate a translated parallel corpus for
each of our English and French corpora. These will
henceforth be referred to as MT-French (the Systran
translation of the English text) and MT-English (the
Systran translation of our French text).
6 Experiments and Evaluation
6.1 Scoring and Annotator Agreement
We explored two ways of measuring annotator
agreement and system performance. (1) The
exact-word-match measure considers annotations to
match if their start and end positions are exactly the
same. (2) The exact-NP-match measure is more for-
giving and considers annotations to match if they
both include the head noun of the same noun phrase.
The exact-word-match criterion is very conservative
because annotators may disagree about equally ac-
ceptable alternatives (e.g., ?Boeing 727? vs. ?new
Boeing 727?). Using the exact-NP-match measure,
?Boeing 727? and ?new Boeing 727? would con-
stitute a match. We used different tools to identify
noun phrases in English and French. For English,
we applied the base noun phrase chunker supplied
with the fnTBL toolkit (Ngai & Florian, 2001). In
French, we ran a part-of-speech tagger (Cucerzan
& Yarowsky, 2000) and applied regular-expression
heuristics to detect the heads of noun phrases.
We measured agreement rates among our human
annotators to assess the difficulty of the IE task. We
computed pairwise agreement scores among our 3
English annotators and among our 3 French anno-
tators. The exact-word-match scores ranged from
16-31% for French and 24-27% for English. These
relatively low numbers suggest that the exact-word-
match criterion is too strict. The exact-NP-match
agreement scores were much higher, ranging from
43-54% for French and 51-59% for English3.
These agreement numbers are still relatively low,
however, which partly reflects the fact that IE is a
subjective and difficult task. Inspection of the data
revealed some systematic differences of approach
among annotators. For example, one of the French
annotators marked 4.5 times as many locations as
another. On the English side, the largest disparity
was a factor of 1.4 in the tagging of victims.
6.2 Monolingual English & French Evaluation
As a key baseline for our cross-language projec-
tion studies, we first evaluated the AutoSlog-TS
and TBL training approaches on monolingual En-
glish and French data. Figure 2 shows (1) English
training by running AutoSlog-TS on unannotated
texts and then applying its patterns to the human-
annotated English test data, (2) English training and
testing by applying TBL to the human-annotated
English data with 5-fold cross-validation, (3) En-
glish training by applying TBL to annotations pro-
duced by Sundance (using AutoSlog-TS patterns)
and then testing the TBL rules on the human-
annotated English data, and (4) French training and
testing by applying TBL to human annotated French
data with 5-fold cross-validation.
Table 2 shows the performance in terms of Pre-
cision (P), Recall (R) and F-measure (F). Through-
3Agreement rates were computed on a subset of the data
annotated by multiple people; systems were scored against the
full corpus, of which each annotator provided the standard for
one third.
out our experiments, AutoSlog-TS training achieves
higher precision but lower recall than TBL training.
This may be due to the exhaustive coverage pro-
vided by the human annotations used by TBL, com-
pared to the more labor-efficient but less-complete
AutoSlog-TS training that used only unannotated
data.
English TEST
140K words
(English)
  SUNDANCE
English (plain)
English (plain)
(English)
  SUNDANCE
4/5Eng TEST Eng TEST1/5
French TEST1/5
French TEST4/5
  TBLES
S1
ES1TS1Train TBL
(1)
(3)
Test TBL
(4) (French)  TBL TF0
Train TBL
Test TBL
English TESTS0
140K words
280K words
280K words
Autoslo
g?TS
Autos
log?T
S
[+ 280K words irrel. text]
(2) (English)  TBL T0
112K words 28K words
Train TBL
Test TBL
[+ 280K words irrel. text]
[cross validation]
[cross validation]
16K words
64K words
Figure 2: Monolingual IE Evaluation pathways4
Monolingual Training Route P R F
English
(1) Train AutoSlog-TS on English-plain (ASE)
S0: Apply ASE to English Test .44 .42 .43
(2) Train TBL on 4/5 of English-Test (TBLE)
T0: Apply TBLE to 1/5 of English Test .35 .62 .45
(perform in 5-fold cross-validation)
(3) Train AutoSlog-TS on English-plain (ASE)
S1: Apply ASE to English-plain .31 .40 .35
TS1 Train TBL on Sundance annotations
ES1: Apply TBLES to English Test
French
(4) Train TBL on 4/5 of French-Test (TBLF)
TF0: Apply TBLF to 1/5 of French Test .47 .66 .54
(perform in 5-fold cross-validation)
Table 2: Monolingual IE Baseline Performance
6.3 TBL-based IE Projection and Induction
As noted in Section 5, both the English and French
corpora were divided into unannotated (?plain?)
and annotated (?antd? or ?Tst?) sections. Figure
3 illustrates these native-language data subsets in
white. Each native-language data subset alo has
a machine-translated mirror in French/English re-
spectively (shown in black), with an identical num-
ber of sentences to the original. By word-aligning
these 4 native/MT pairs, each becomes a potential
vehicle for cross-language information projection.
Consider the pathway TE1?P1 ? TF1 as a rep-
resentative example pathway for projection. Here
an English TBL classifier is trained on the 140K-
word human annotated data and the learned TBL
rules are applied to the unannotated English sub-
corpus. The annotations are then projected across
the Giza++ word alignments to their MT-French
mirror. Next, a French TBL classifier (TBL1) is
trained on the projected MT-French annotations and
the learned French TBL rules are subsequently ap-
plied to the native-French test data.
An alternative path (TE4 ? P4 ? French-Test)
is more direct, in that the English TBL classifier
is applied immediately to the word-aligned MT-
English translation of the French test data. The MT-
English annotations can then be directly projected
to the French test data, so no additional training
is necessary. Another short direct projection path
(PHA2 ? THA2 ? French-test) skips the need to
train an English TBL model by projecting the En-
glish human annotations directly onto MT-French
texts, which can then be used to train a French TBL
system which can be applied to the French test data.
HUMAN
Annotators
  TBL (English)
French TBL
Training and
Transfer to
Test Data
English
Annotation
P1
English (plain)
P    2HA
TBL1
T    2HA
TBL2h
English (antd)
TB
L 
Tr
ai
ni
ng
T  1
ET  1
F
Cross?
Language
Projection
P3
TBL3
French (plain)
T  3F
T   3E
P4
MT?English Tst
T   4E
(plain)
(plain)
MT?English
MT?French
MT?French
(annotated) French Test
Figure 3: TBL-based IE projection pathways
Table 3 shows the results of our TBL-based ex-
periments. The top performing pathway is the
TE4 ? P4 two-step projection pathway shown in
Figure 3. Note the F-measure of the best pathway
is .45, which is equal to the highest F-measure for
monolingual English and only 9% lower than the F-
measure for monolingual French.
4The irrelevant texts are needed to train AutoSlog-TS, but
not TBL.
Projection and Training Route P R F
TE1: Apply TBLE to English-plain
P1: Project to MT-French(English-Plain) .69 .24 .36
TF1: Train TBL & Apply to FrTest
? Use human Annos from Eng Antd
Pha2: Project to MT-French(English Antd) .56 .29 .39
Tha2: Train TBL & Apply to FrTest
TE3: Apply TBLE to MT-Eng(FrenchPlain)
P3: Project to French-Plain .49 .34 .40
TF3: Train TBL & Apply to FrTest
TE4: Apply TBLE to MT-Eng(FrenchTest)
P4: Direct Project to French-Test .49 .41 .45
Table 3: TBL-based IE projection performance
6.4 Sundance-based IE Projection and
Induction
Figure 4 shows the projection and induction model
using Sundance for English IE annotation, which is
almost isomorphic to that using TBL. One notable
difference is that Sundance was trained by apply-
ing AutoSlog-TS to the unannotated English text
rather than the human-annotated data. Figure 4 also
shows an additional set of experiments (SMT 3 and
SMT 4) in which AutoSlog-TS was trained on the
English MT translations of the unannotated French
data. The motivation was that native-English extrac-
tion patterns tend to achieve low recall when applied
to MT-English text (given frequent mistranslations
such as ?to crush? a plane rather than ?to crash? a
plane). By training AutoSlog-TS on the sentences
generated by an MT system (seen in the SMT 3 and
SMT 4 pathways), the F-measure increases.5
French TBL
Training and
Transfer to
Test Data
English
Annotation
P2P1
T1
T2
S1
S2
  SUNDANCE
(English)
English (plain)
TBL1
TBL2
English (antd)
Projection
Language
Cross?
P4
(MT?English)
SUNDANCE
MT?English Tst
P     3
S     4
P     4
MT
MT MT
French (plain)
S4
S     3MT
S3
P3
TBL3
T3 TBL3m
T     3MT
Au
tos
log
?T
S
Au
to
slo
g 
 ?T
S
(plain)
MT?French
MT?English
(plain)
MT?French
(annotated) French Test
Figure 4: Sundance-based projection pathways
5This is a ?fair? gain, in that the MT-trained AutoSlog-TS
patterns didn?t use translations of any of the French test data.
Projection and Training Route P R F
AutoSlog-TS trained on native English (AS E)
S2: Apply ASE to English-Antd
P2: Project to MT-French(English-Antd) .39 .24 .29
T2: Train TBLFP2 & Apply to FrTest
S(1+2): Apply ASE to English Antd+Plain
P (1+2): Project to MT-French(Eng-Ant+Pl) .43 .23 .30
T (1+2): Train TBLFP1+2 & Apply to FrTest
S3: Apply ASE to MT-Eng(FrenchPlain)
P3: Project to French-Plain .45 .04 .07
T3: Train TBLFP3 & Apply to FrTest
S4: Apply ASE to MT-Eng(FrenchTest)
P4: Direct Project to French-Test .48 .07 .13
AutoSlog-TS trained on MT English (AS MTE)
SMT 3: Apply ASMTE to MT-Eng(FrPlain)
PMT 3: Project to French-Plain .46 .25 .32
TMT 3: Train TBLFMT3 & Apply to FrTest
SMT 4: Apply ASMTE to MT-Eng(FrTest)
PMT 4: Direct Project to French-Test .55 .28 .37
Table 4: Sundance-based IE projection performance 6
Table 4 shows that the best Sundance pathway
achieved an F-measure of .37. Overall, Sundance
averaged 7% lower F-measures than TBL on com-
parable projection pathways. However, AutoSlog-
TS training required only 3-4 person hours to review
the learned extraction patterns while TBL training
required about 150 person-hours of manual IE an-
notations, so this may be a viable cost-reward trade-
off. However, the investment in manual English IE
annotations can be reused for projection to new for-
eign languages, so the larger time investment is a
fixed cost per-domain rather than per-language.
6.5 Analysis and Implications
? For both TBL and Sundance, the P1, P2 and
P3-family of projection paths all yield stand-alone
monolingual French IE taggers not specialized for
any particular test set. In contrast, the P4 series of
pathways (e.g. PMT 4 for Sundance), were trained
specifically on the MT output of the target test data.
Running an MT system on test data can be done au-
tomatically and requires no additional human lan-
guage knowledge, but it requires additional time
(which can be substantial for MT). Thus, the higher
performance of the P4 pathways has some cost.
? The significant performance gains shown by
Sundance when AutoSlog-TS is trained on MT-
English rather than native-English are not free be-
cause the MT data must be generated for each new
language and/or MT system to optimally tune to
6S(1+2) combines the training data in S1 (280K) and S2
(140K), yielding a 420K-word sample.
its peculiar language variants. No target-language
knowledge is needed in this process, however, and
reviewing AutoSlog-TS? patterns can be done suc-
cessfully by imaginative English-only speakers.
? In general, recall and F-measure drop as the
number of experimental steps increases. Averaged
over TBL and Sundance pathways, when compar-
ing 2 and 3-step projections, mean recall decreases
from 26.8 to 21.8 (5 points), and mean F-measure
drops from 32.6 to 28.8 (3.8 points). Viable extrac-
tion patterns may simply be lost or corrupted via too
many projection and retraining phases.
? One advantage of the projection path families
of P1 and P2 is that no domain-specific documents
in the foreign language are required (as they are in
the P3 family). A collection of domain-specific En-
glish texts can be used to project and induce new IE
systems even when no domain-specific documents
exist in the foreign language.
6.6 Multipath Projection
Finally, we explored the use of classifier combina-
tion to produce a premium system. We considered a
simple voting scheme over sets of individual IE sys-
tems. Every annotation of a head noun was consid-
ered a vote. We tried 4 voting combinations: (1) the
systems that used Sundance with English extraction
patterns, (2) the systems that used Sundance with
MT-English extraction patterns, (3) the systems that
used TBL trained on English human annotations,
(4) all systems. For each combination of n sys-
tems, n answer sets were produced using the voting
thresholds Tv = 1..n. For example, for Tv = 2 ev-
ery annotation receiving >= 2 votes (picked by at
least 2 individual systems) was output in the answer
set. This allowed us to explore a precision/recall
tradeoff based on varying levels of consensus.
Figure 5 shows the precision/recall curves. Vot-
ing yields some improvement in F-measure and pro-
vides a way to tune the system for higher preci-
sion or higher recall by choosing the Tv threshold.
When using all English knowledge sources, the F-
measure at Tv=1 (.48) is nearly 3% higher than the
strongest individual system. Figure 5 also shows
the performance of a 5th system (5), which is a
TBL system trained directly from the French anno-
tations under 5-fold cross-validation. It is remark-
able that the most effective voting-based projection
system from English to French comes within 6% F-
measure of the monolingually trained system, given
that this cross-validated French monolingual system
was trained directly on data in the same language
and source as the test data. This suggests that cross-
language projection of IE analysis capabilities can
successfully approach the performance of dedicated
systems in the target language.
Precision
Recall
(5)
(2)
(1) (3) (4)
(5) TBL Trained from French Annotations 
(4) English TBL + Sundance pathways
(3) English TBL pathways
(2) Sundance?MT pathways
(1) Sundance pathways
[under 5?fold cross?validation]
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
Figure 5: Precision/Recall curves for voting systems. Each
point represents performance for a particular voting threshold.
In all cases, precision increases and recall decreases as the
threshold is raised.
French Test-Set Performance P R F
Multipath projection from all English resources .43 .54 .48
Table 5: Best multipath English-French Projection Per-
formance (from English TBL and Sundance pathways)
7 Conclusions
We have used IE systems for English to automati-
cally derive IE systems for a second language. Even
with the quality of MT available today, our results
demonstrate that we can exploit translation tools to
transfer information extraction expertise from one
language to another. Given an IE system for a
source language, an MT system that can translate
between the source and target languages, and a word
alignment algorithm, our approach allows a user to
create a functionally comparable IE system for the
target language with very little human effort. Our
experiments demonstrated that the new IE system
can achieve roughly the same level of performance
as the source-language IE system. French and En-
glish are relatively close languages, however, so
how well these techniques will work for more dis-
tant language pairs is still an open question.
Additional performance benefits could be
achieved in two ways: (1) put more effort into
obtaining better resources for English, or (2)
implement (minor) specializations per language.
While it is expensive to advance the state of the art
in English IE or to buy annotated data for a new
domain, these additions will improve performance
not only in English but for other languages as
well. On the other hand, with minimal effort
(hours) it is possible to custom-train a system
such as Autoslog/Sundance to work relatively
well on noisy MT-English, providing a substantial
performance boost for the IE system learned for the
target language, and further gains are achieved via
voting-based classifier combination.
References
J. Atserias, S. Climent, X. Farreres, G. Rigau and H. Rodriguez.
1997. Combining multiple methods for the automatic con-
struction of multilingual WordNets. In Proceedings of the
International Conference on Recent Advances in Natural
Language Processing.
E. Brill. 1995. Transformation-based error-driven learning and
natural language processing: A case study in part of speech
tagging. Computational Linguistics, 21(4):543?565.
M. E. Califf. 1998. Relational learning techniques for natural
language information extraction. Ph.D. thesis, Tech. Rept.
AI98-276, Artificial Intelligence Laboratory, The University
of Texas at Austin.
S. Cucerzan and D. Yarowsky. 2000. Language independent
minimally supervised induction of lexical probabilities. In
Proceedings of ACL-2000, pages 270-277.
D. Freitag. 1998. Toward general-purpose learning for in-
formation extraction. In Proceedings of COLING-ACL?98,
pages 404-408.
J. Kim and D. Moldovan. 1993. Acquisition of semantic pat-
terns for information extraction from corpora. In Proceed-
ings of the Ninth IEEE Conference on Artificial Intelligence
for Applications, pages 171?176.
G. Ngai and R. Florian. 2001. Transformation-based learning
in the fast lane. In Proceedings of NAACL, pages 40-47.
F. J. Och and H. Ney. 2000. Improved statistical alignment
models. In Proceedings of the 38th Annual Meeting of the
Association for Computational Linguistics, pages 440?447.
E. Riloff. 1993. Automatically Constructing a dictionary for
information extraction tasks. In Proceedings of the Eleventh
National Conference on Artificial Intelligence, pages 811?
816.
E. Riloff. 1996b. Automatically generating extraction patterns
from untagged text. In Proceedings of the Thirteenth Na-
tional Conference on Artificial Intelligence, pages 1044?
1049. AAAI Press/MIT Press.
E. Riloff and R. Jones. 1999. Learning dictionaries for infor-
mation extraction by multi-level bootstrapping. In Proceed-
ings of the Sixteenth National Conference on Artificial Intel-
ligence, pages 474?479.
S. Soderland, D. Fisher, J. Aseltine, and W. Lehnert. 1995.
CRYSTAL: Inducing a conceptual dictionary. In Proceed-
ings of the Fourteenth International Joint Conference on Ar-
tificial Intelligence, pages 1314?1319.
R. Yangarber, R. Grishman, P. Tapanainen, and S. Huttunen.
2000. Automatic acquisiton of domain knowledge for infor-
mation extraction. In Proceedings of COLING-2000, pages
940-946.
Yarowsky, D., G. Ngai and R. Wicentowski. 2001. Inducing
multilingual text analysis tools via robust projection across
aligned corpora. In Proceedings of HLT-01, pages 161?168.
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1031?1040,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Mining and Modeling Relations between
Formal and Informal Chinese Phrases from Web Corpora
Zhifei Li and David Yarowsky
Department of Computer Science and Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD 21218, USA
zhifei.work@gmail.com and yarowsky@cs.jhu.edu
Abstract
We present a novel method for discovering
and modeling the relationship between in-
formal Chinese expressions (including collo-
quialisms and instant-messaging slang) and
their formal equivalents. Specifically, we pro-
posed a bootstrapping procedure to identify
a list of candidate informal phrases in web
corpora. Given an informal phrase, we re-
trieve contextual instances from the web us-
ing a search engine, generate hypotheses of
formal equivalents via this data, and rank
the hypotheses using a conditional log-linear
model. In the log-linear model, we incorpo-
rate as feature functions both rule-based intu-
itions and data co-occurrence phenomena (ei-
ther as an explicit or indirect definition, or
through formal/informal usages occurring in
free variation in a discourse). We test our
system on manually collected test examples,
and find that the (formal-informal) relation-
ship discovery and extraction process using
our method achieves an average 1-best preci-
sion of 62%. Given the ubiquity of informal
conversational style on the internet, this work
has clear applications for text normalization
in text-processing systems including machine
translation aspiring to broad coverage.
1 Introduction
Informal text (e.g., newsgroups, online chat, blogs,
etc.) is the majority of all text appearing on the Inter-
net. Informal text tends to have very different style
from formal text (e.g., newswire, magazine, etc.).
In particular, they are different in vocabulary, syn-
tactic structure, semantic interpretation, discourse
Formal Informal
?? (BaiBai)[bye-bye] 88 (BaBa)
?? (XiHuan)[like] ?, (XiFan)[gruel]
?? (GeGe)[elder brother] GG
?? (GeMi)[fans] N? (FenSi)[a food]
Table 1: Example Chinese Formal-informal Relations.
The PinYin pronunciation is in parentheses and an op-
tional literal gloss is in brackets.
structure, and so on. On the other hand, certain re-
lations exist between the informal and formal text,
and informal text often has a viable formal equiva-
lent. Table 1 shows several naturally occurring ex-
amples of informal expressions in Chinese, and Ta-
ble 2 provides a more detailed inventory and charac-
terization of this phenomena1. The first example of
informal phrase ?88? is used very often in Chinese
on-line chat when a person wants to say ?bye-bye?
to the other person. This can be explained as fol-
lows. In Chinese, the standard equivalent to ?bye-
bye? is ???? whose PinYin is ?BaiBai?. Coin-
cidentally, the PinYin of ?88? is ?BaBa?. Because
?BaBa? and ?BaiBai? are near homophones, people
often use ?88? to represent ????, either for input
convenience or just for fun. The other relations in
Table 1 are formed due to similar processes as will
be described later.
Due to the often substantial divergence between
1For clarity, we represent Chinese words in the format: Chi-
nese characters (optional PinYin equivalent in parentheses and
optional English gloss in brackets).
1031
informal and formal text, a text-processing system
trained on formal text does not typically work well
on informal genres. For example, in a machine
translation system (Koehn et al, 2007), if the bilin-
gual training data does not contain the word ??
,? (the second example in Table 1), it leaves the
word untranslated. On the other hand, if the word
??,? does appear in the training data but it has
only a translation ?gruel? as that is the meaning in
the formal text, the translation system may wrongly
translate ??,? into ?gruel? for the informal text
where the word ??,? is more likely to mean
?like?. Therefore, as a text-normalization step, it
is desirable to transform the informal text into its
standard formal equivalent before feeding it into a
general-purpose text-processing system. Unfortu-
nately, there are many processes for generating in-
formal expressions in common use today. Such
transformations are highly flexible/diverse, and new
phrases are invented on the Internet every day due to
major news events, popular movies, TV shows, ra-
dio talks, political activities, and so on. Therefore,
it is of great interest to have a data-driven method
that can automatically find the relations between in-
formal and formal expressions.
In this paper, we present a novel method for dis-
covering and modeling the relationship between in-
formal Chinese expressions found in web corpora
and their formal equivalents. Specifically, we im-
plement a bootstrapping procedure to identify a
list of candidate informal phrases. Given an indi-
vidual informal phrase, we retrieve contextual in-
stances from the web using a search engine (in this
case, www.baidu.com), generate hypotheses of for-
mal equivalents via this data, and rank the hypothe-
ses using a conditional log-linear model. In the log-
linear model, we incorporate as feature functions
both rule-based intuitions and data co-occurrence
phenomena (either as an explicit or indirect defini-
tion, or through formal/informal usages occurring in
free variation in a discourse). We test our system on
manually collected test examples2, and find that the
(formal-informal) relationship discovery and extrac-
tion process using our method achieves an average
precision of more than 60%. This work has applica-
2The training and test examples are freely available at
http://www.cs.jhu.edu/?zfli.
tions for text normalization in many general-purpose
text-processing tasks, e.g., machine translation.
To the best of our knowledge, our work is the
first published machine-learning approach to pro-
ductively model the broad types of relationships be-
tween informal and formal expressions in Chinese
using web corpora.
2 Formal to Informal: Phenomena and
Examples
In this section, we describe the phenomena and pro-
vide examples of the relations between formal and
informal expressions in Chinese (we refer to the
relation as formal-informal phrases hereafter, even
in the case of single-word expressions). We man-
ually collected 908 formal-informal relations, and
classified these relations into four categories. We
collected these pairs by investigating multiple web-
pages where the formal-informal relations are man-
ually compiled, and then merged these seed relations
and removed duplicates. In this way, the 908 exam-
ples should give good coverage on the typical cat-
egories in the formal-informal relations. Also, the
distribution of the categories found in the 908 exam-
ples should be representative of the actual distribu-
tion of the formal-informal relations occurring in the
real text. Table 2 presents these categories and ex-
amples in each category. In the last column, the table
also shows the relative frequency of each category,
computed based on the 908 examples. Recall that
we represent Chinese words in the format: Chinese
characters (optional PinYin equivalent in parenthe-
ses and optional English gloss in brackets).
2.1 Homophone
In general, a homophone is a word that is pro-
nounced the same as another word but differs in
meaning and/or written-form. Here, we use the word
?homophone? in a loose way. In particular, we re-
fer an informal phrase as a homophone of a formal
phrase if its pronunciation is the same or similar to
the formal phrase. In the three examples belonging
to the homophone category in Table 2, the first ex-
ample is a true homophone, while the other two are
loose homophones. The third example represents a
major sub-class where the informal phrase is a num-
ber (e.g., 88).
1032
Category Formal Informal %
Homophone ?? (BanZhu) [system administrator] ?? (BanZhu) [bamboo] 4.2
?? (XiHuan)[like] ?, (XiFan)[gruel] 4.4
?? (BaiBai)[bye-bye] 88 (BaBa) 21
Abbreviation ?)? (MeiGuoJunDui)[american army] ? (MeiJun)[american army] 3.8
Acronym ?? (GeGe)[elder brother] GG 12.3
E?? (Nu?PengYou)[girl friend] GF 7.2
Transliteration ?? (GeMi)[fans] N? (FenSi)[a Chinese food]
2.3
\\ (XieXie)[thank you] 3Q (SanQiu)
Others ?n?N? (XiLaLiFenSi)[fans of Hilary] ?, (XiFan)[gruel]
44.8??jN? (AoBaMaFenSi)[fans of Obama] QN (OuFen)[a food]
? (ChaoQiang)[super strong] P?? (ZouZhaoGongXu)
Table 2: Chinese Formal-informal Relations: Categories and Examples. Literal glosses in brackets.
For illustrative purposes, we can present the
transformation path showing how the informal
phrase is obtained from the formal phrase. In par-
ticular, the transformation path for this category is
?Formal ? PinYin ? Informal (similar or same
PinYin as the formal phrase)?.
2.2 Abbreviation and Acronym
A Chinese abbreviation of a formal phrase is ob-
tained by selecting one or more characters from this
formal phrase, and the selected characters can be at
any position in the formal phrase (Li and Yarowsky,
2008; Lee, 2005; Yin, 1999). In comparison, an
acronym is a special form of abbreviation, where
only the first character of each word in the formal
phrase is selected to form the informal phrase. Table
2 presents three examples belonging to this category.
While the first example is an abbreviation, and the
other two examples are acronyms.
The transformation path for the second exam-
ple is ?Formal ? PinYin ? Acronym?, and the
transformation path for the third example is ?For-
mal? English? Acronym?. Clearly, they differ in
whether PinYin or English is used as a bridge.
2.3 Transliteration
A transliteration is transcribing a word or text writ-
ten in one writing system into another writing sys-
tem. Table 2 presents examples belonging to this
category. In the first example, the Chinese infor-
mal phrase ?N? (FenSi)[a Chinese food]? can be
thought as a transliteration of the English phase
?fans? as the pronunciation of ?fans? is quite sim-
ilar to the PinYin ?FenSi?.
The transformation path for this category is ?For-
mal? English? Chinese Transliteration?.
2.4 Others
Due to the inherently informal and flexible nature of
expressions in informal genre, the formation of an
informal phrase can be very complex or ad-hoc. For
example, an informal phrase can be generated by ap-
plying the above transformation rules jointly. More
importantly, many relations cannot be described us-
ing a simple set of rules. Table 2 presents three such
examples, where the first two examples are gener-
ated by applying rules jointly and the third example
is created by decomposing the Chinese characters in
the formal form. The statistics collected from the
904 examples tells us that about 45% of the relations
belonging to this category. This motivates us to use
a data-driven method to automatically discover the
relations between informal and formal phrases.
3 Data Co-occurrence
In natural language, related words tend to appear to-
gether (i.e., co-occurrence). For example, Bill Gates
1033
tends to appear together with Microsoft more of-
ten than expected by chance. Such co-occurrence
may imply the existence of a relationship, and is ex-
ploited in formal-informal relation discovery under
different conditions.
3.1 Data Co-occurrence in Definitions
In general, for many informal phrases in popular use,
there is likely to be an explicit definition somewhere
that provides or paraphrases its meaning for an unfa-
miliar audience. People have created dedicated def-
inition web-pages to explain the relations between
formal and informal phrases. For example, the first
example in Table 3 is commonly explained in many
dedicated definition web-pages on the Internet. On
the other hand, in some formal text (e.g., research
papers), people tend to define the informal phrase
before it is used frequently in the later part of the
text. The second example of Table 3 illustrates this
phenomena. Clearly, the definition text normally
contains salient patterns. For example, the first ex-
ample follows the ?informal4formal{??? defi-
nition pattern, while the second example follows the
pattern ?formal (informal)?. This gives us a reliable
way to seed and bootstrap a list of informal phrases
as will be discussed in Section 4.1.
Relation Definition Text
(E??, GF) GF4E??{??
(-??	?,
-?)
&?{ yf~-??
	? (-?)X~yy
??Z
Table 3: Data Co-occurrence in Definitions
3.2 Data Co-occurrence in Online Chat
Informal phrases appear in online chat very often for
input convenience or just for fun. Since different
people may have different ways or traditions to ex-
press semantically-equivalent phrases, one may find
many nearby data co-occurrence examples in chat
text. For example, in Table 4, after a series of mes-
sage exchanges, person A wants to end the conver-
sation and types ???? (meaning ?bye-bye?), per-
son B later includes the same semantic content, but
in a different (more or less formal) expression (e.g.
?88?).
...
Person A: ?X???"?
Person A: ??
Person B: 88
Table 4: Data Co-occurrence in Online Chat for Relation
(??, 88) meaning ?bye-bye?
3.3 Data Co-occurrence in News Articles
For some formal-informal relations, since both of
the informal and formal phrases have been used in
public very often and people are normally aware
of these relations, an author may use the informal
and formal phrases interchangeably without bother-
ing to explain the relations. This is particularly true
in news articles for some well-known relations. Ta-
ble 5 shows an example, where the abbreviation ??
??? (meaning ?winter olympics?) appears in the
title and its full-form ?????? appears in the
text of the same document. In general, the relative
distance between an informal phrase and its formal
phrase varies. For example, they may appear in the
same sentence, or in neighboring sentences.
Title ?????*R?<??
Text c???2?9??(V?c?
?)?20?????{?*R
?h?-10?t8?????
?.??t*y ?{??
Table 5: Data Co-occurrence in News Article for Relation
(????,???) meaning ?winter olympics?
4 Mining Relations between Informal and
Formal Phrases from Web
In this section, we describe an approach that auto-
matically discovers the relation between a formal
phrase and an informal phrase from web corpora.
Specifically, we propose a bootstrapping procedure
to identify a list of candidate informal phrases.
Given a target informal phrase, we retrieve a large
set of of instances in context from the Web, generate
candidate hypotheses (i.e, candidate formal phrases)
from the data, and rank the hypotheses by using a
conditional log-linear model. The log-linear model
is very flexible to incorporate both the rule- and data-
1034
driven intuitions (described in Sections 2 and 3, re-
spectively) into the model as feature functions.
4.1 Identifying Informal Phrases
Before finding the formal phrase corresponding to
an informal phrase, we first need to identify infor-
mal phrases of interest. For example, one can collect
informal phrases manually. However, this is too ex-
pensive as new relations between informal and for-
mal phrases emerge every day on the Internet. Alter-
natively, one can employ a large amount of formal
text (e.g., newswire) and informal text (e.g., Inter-
net blogs) to derive such a list as follows. Specifi-
cally, from the informal corpus we can extract those
phrases whose frequency in the informal corpus is
significantly different from that in the formal cor-
pus. However, such a list may be quite noisy, i.e.,
many of them are not informal phrases at all.
An alternative approach to extracting the infor-
mal phrases is to use a bootstrapping algorithm (e.g.,
Yarowsky (1995)). Specifically, we first manually
collect a small set of example relations. Then, using
these relations as a seed set, we extract the text pat-
terns (e.g., the definition pattern showing how the
informal and formal phrases co-occur in the data as
discussed in Section 3.1). With these patterns, we
identify many more new relations from the data and
augment them into the seed set. The procedure it-
erates. Using such an approach, we should be able
to extract a large list of formal-informal relations.
Clearly, the list extracted in this way may be quite
noisy, and thus it is important to exploit both the
data- and rule-driven intuitions to rank these rela-
tions properly.
4.2 Retrieving Data from Web
Given an informal phrase, we retrieve training data
from the web on the fly. Specifically, we first use
a search engine to identify a set of hyper-links that
point to web pages containing contexts relevant to
the informal phrase, and then follow the hyper-links
to download the web pages. The input to the search
engine is a text query. One can simply use the infor-
mal phrase as a query. However, this may lead to a
set of pages that have nothing to do with the infor-
mal phrase. For example, if we search the informal
phrase ?88? (the third example in Table 2) using the
well-known Chinese search engine www.baidu.com,
none of the top-10 pages are related to the infor-
mal phrase ?88?. To avoid this situation, one can
use a search engine that is dedicated to informal text
search (e.g., blogsearch.baidu.com). Alternatively,
one can use the general-purpose search engine but
expanding the query with domain information. For
example, for the informal phrase ?88?, we can use
a query ?88 d???, where ?d??? means
internet language.
4.3 Generating Candidate Hypotheses
Given an informal phrase, we generate a set of hy-
potheses which are candidate formal phrases corre-
sponding to the informal phrase. We considered two
general approaches to the generation of hypotheses.
Rule-driven Hypothesis Generation: One can
use the rules described in Section 2 to generate a
set of hypotheses. However, with this approach, one
may generate an exponential number of hypotheses.
For example, assuming the number of English words
starting with a given letter is O(|V |), we can generate
O(|V |n) hypotheses given an acronym containing n
letters. Another problem with this approach is that
a relation between an informal phrase and a formal
phrase may not be explained by a specific rule. In
fact, as shown in the last row of Table 2, such rela-
tions consist of 44.8% of all corpus instances.
Data-driven Hypothesis Generation: With data
retrieved from the Web, we can generate hypotheses
by enumerating the frequent n-grams co-occurring
with the informal phrase within certain distance.
This exploits the data co-occurrence phenomena de-
scribed in Section 3, that is, the formal phrase tends
to co-occur with the informal phrase nearby in the
data, for the multiple reasons described above. This
can deal with the cases where the relation between
an informal phrase and a formal phrase cannot be
explained by a rule. However, it also suffers from
the over-generation problem as in the rule-driven ap-
proach.
In this paper, we use the data-driven method to
generate hypotheses, and rank the hypotheses using
a conditional log-linear model that incorporates both
the rule and data intuitions as feature functions.
1035
4.4 Ranking Hypotheses: Conditional
Log-linear Model
Log-linear models are known for flexible incorpora-
tion of features into the model. Each feature func-
tion reflects a hint/intuition that can be used to rank
the hypotheses. In this subsection, we develop a
conditional log-linear model that incorporates both
the rule and data intuitions as feature functions.
4.4.1 Conditional Log-linear Model
Given an informal phrase (say x) and a candidate
formal phrase (say y), the model assigns the pair a
score (say s(x, y)), which will be used to rank the
hypothesis y. The score s(x, y) is a linear combina-
tion of the feature scores (say ?i(x, y)) over a set of
feature functions indexed by i. Formally,
s(x, y) =
K?
i=1
?i(x, y)? ?i (1)
where K is the number of feature functions defined
and ?i is the weight assigned to the i-th feature func-
tion (i.e., ?i). To learn the weight vector ~?, we first
define a probability measure,
P~?(y|x) =
1
Z(x, ~?)
es(x,y) (2)
where Z(x, ~?) is a normalization constant. Now, we
define the regularized log-likelihood (LLR) of the
training data (i.e, a set of pairs of (x, y)), as follows,
LLR(~?) =
N?
j=1
log P~?(yj |xj)?
||~?||2
2?2
(3)
whereN is the number of training examples, and the
regularization term ||~?||
2
2?2 is a Gaussian prior with a
variance ?2 (Roark et al, 2007). The optimal weight
vector ~?? is obtained by maximizing the regularized
log-likelihood (LLR), that is,
~?? = arg max
~?
LLR(~?) (4)
To maximize the above function, we use a limited-
memory variable method (Benson and More, 2002)
that is implemented in the TAO package (Benson et
al., 2002) and has been shown to be very effective in
various natural language processing tasks (Malouf,
2002).
During test time, the following decision rule is
normally used to predict the optimal formal phrase
y? for a given informal phrase x,
y? = arg max
y
s(x, y). (5)
4.4.2 Feature Functions
As mentioned before, we incorporate both the
rule- and data-driven intuitions as feature functions
in the log-linear model.
Rule-driven feature functions: Clearly, if a pair
(x, y) matches the rule patterns described in Table 2,
the pair has a high possibility to be a true formal-
informal relation. To reflect this intuition, we de-
velop several feature functions as follows.
? LD-PinYin(x, y): the Levenshtein distance on
PinYin of x and y. The distance between
two PinYin characters is weighted based on
the similarity of pronunciation, for example,
the weight w(l, n) is smaller than the weight
w(a, z).
? LEN-PinYin(x, y): the difference in the num-
ber of PinYin characters between x and y.
? Is-PinYin-Acronym(x, y): is x a PinYin
acronym of y? For example,
Is-PinYin-Acronym(GG,??)=1,
Is-PinYin-Acronym(GG,w?)=0.
? Is-CN-Abbreviation(x, y): is x a Chinese ab-
breviation of y? For example,
Is-CN-Abbreviation(?,?)?)=1,
Is-CN-Abbreviation(?,?)?)=0.
Data-driven feature functions: As described in
Section 3, the informal and formal phrases tends to
co-occur in the data. Here, we develop several fea-
ture functions to reflect this intuition.
? n-gram co-occurrence relative frequency: we
collect the n-grams that occur in the data within
a window of the occurrence of the informal
phrase, and compute their relative frequency
as feature values. Since different orders of
grams will have quite different statistics, we
define 7 features in this category: 1-gram, 2-
gram, 3-gram, 4-gram, 5-gram, 6to10-gram,
and 11to15-gram. Note that the order n of a
n-gram is in terms of number of Chinese char-
acters instead of words.
1036
? Features on a definition pattern: we have dis-
cussed definition patterns in Section 3.1. For
each definition pattern, we can define a feature
function saying that if the co-occurrence of x
and y satisfies the definition pattern, the feature
value is one, otherwise is zero.
? Features on the number of relevant web-pages:
another interesting feature function can be de-
fined as follows. For each candidate relation
(x, y), we use the pair as a query to search the
web, and treat the number of pages returned by
the search engine as a feature value.3 However,
these features are quite expensive as millions of
queries may need to be served.
5 Experimental Results
Recall that in Section 2 we categorize the formal-
informal relations based on the manually collected
relations. In this section, we use a subset of them for
training and testing. In particular, we use 252 exam-
ples to train the log-linear model that is described
in Section 4, and use 249 examples as test data to
compute the precision.4
Table 6 shows the weights5 learned for the var-
ious feature functions described in Section 4.4.
Clearly, different feature functions get quite differ-
ent weights. This is intuitive as the feature functions
may differ in the scale of the feature values or in
their importance in ranking the hypotheses. In fact,
this shows the importance of using the log-linear
model to learn the optimal weights in a principled
and automatic manner, instead of manually tuning
the weights in an ad-hoc way.
Tables 7-9 show the precision results for different
categories as described in Section 2, using the rule-
driven, data-driven, or both rule and data-driven fea-
tures, respectively. In the tables, the precision corre-
sponding to the ?top-N? is computed in the follow-
ing way: if the true hypothesis is among the top-N
hypotheses ranked by the model, we tag the classi-
fication as correct, otherwise as wrong. Clearly, the
3Note that the number of pages relevant to a query can be
easily obtained as most search engines return this number.
4Again, the training and test examples are freely available at
http://www.cs.jhu.edu/?zfli.
5Note that we do not use the features on definition patterns
and on the number of relevant web pages, for efficiency.
Category Feature Weight
Rule-driven
LD-PinYin 0.800
Len-PinYin 0.781
Is-PinYin-Acronym 7.594
Is-CN-Abbreviation 7.464
Data-driven
1-gram 14.506
2-gram 108.193
3-gram 82.975
4-gram 66.872
5-gram 42.258
6to10-gram 21.229
11to15-gram 0.985
Table 6: Optimal Weights in the Log-linear Model
larger the N is, the higher the precision is. Comput-
ing the top-N precision (instead of just computing
the usual top-1 precision) is meaningful especially
when we consider our relation extractor as an inter-
mediate step in an end-to-end text-processing sys-
tem (e.g., machine translation) since the final deci-
sion can be delayed to later stage based on more ev-
idence. In general, our model gets quite respectably
high precision for such a task (e.g., more than 60%
for top-1 and more than 85% for top-100) when us-
ing both data and rule-driven features, as shown in
Table 9. Moreover, the data-driven features are more
helpful than the rule-driven features (e.g, 25.3% ab-
solute improvement in 1-best precision), while the
combination of these features does boost the perfor-
mance of any individual feature set (e.g., 10.4% ab-
solute improvement in 1-best precision over the case
using data-driven features only).
We also carried out experiments (see Table 10)
in the bootstrapping procedure described in Section
4.1. In particular, we start from a seed set having
130 relations. We identify the frequent patterns from
the data retrieved from the web for these seed exam-
ples. Then, we use these patterns to identify many
more new possible formal-informal relations. After
the first iteration, we select the top 3000 pairs of re-
lations matched by the patterns. The recall of a man-
ually collected test set (having 750 pairs) on these
3000 pairs is around 30%, which is quite promising
given the highly noisy data.
1037
Category
Precision (%)
Top-1 Top-10 Top-50 Top-100
Homophone Same PinYin 31.6 47.4 68.4 73.7
Similar PinYin 15.0 35.0 45.0 50.0
Number 31.6 64.2 84.2 90.5
Abbreviation Chinese abbreviation 11.8 35.3 41.2 41.2
Acronym PinYin Acronym 39.3 82.1 91.1 92.9
English Acronym 3.1 6.3 9.4 28.1
Transliteration 10.0 20.0 20.0 20.0
Average 26.1 53.4 66.3 72.3
Table 7: Rule-driven Features only: Precision on Chinese Formal-informal Relation Extraction
Category
Precision (%)
Top-1 Top-10 Top-50 Top-100
Homophone Same PinYin 52.6 73.7 73.7 78.9
Similar PinYin 45.0 65.0 75.0 75.0
Number 66.3 86.3 94.7 96.8
Abbreviation Chinese abbreviation 0.0 23.5 47.1 47.1
Acronym PinYin Acronym 58.9 78.6 85.7 87.5
English Acronym 25.0 46.9 68.6 68.8
Transliteration 50.0 50.0 50.0 50.0
Average 51.4 71.1 81.1 82.7
Table 8: Data-driven Features only: Precision on Chinese Formal-informal Relation Extraction
Category
Precision (%)
Top-1 Top-10 Top-50 Top-100
Homophone Same PinYin 63.2 73.7 84.2 84.2
Similar PinYin 40.0 60.0 70.0 80.0
Number 81.1 91.6 95.8 96.8
Abbreviation Chinese abbreviation 11.8 41.2 52.9 52.9
Acronym PinYin Acronym 82.1 94.6 96.4 96.4
English Acronym 21.9 46.9 56.3 59.4
Transliteration 20.0 40.0 50.0 50.0
Average 61.8 77.1 83.1 84.7
Table 9: Both Data and Rule-drive Features: Precision on Chinese Formal-informal Relation Extraction
1038
Size of seed set 130
Size of candidate set 3000
Size of test set 750
Recall 30%
Table 10: Recall of Test Set on a Candidate Set Extracted
by a Bootstrapping Procedure
6 Related Work
Automatically extracting the relations between full-
form Chinese phrases and their abbreviations is an
interesting and important task for many NLP appli-
cations (e.g., machine translation, information re-
trieval, etc.). Recently, Chang and Lai (2004), Lee
(2005), Chang and Teng (2006), Li and Yarowsky
(2008) have investigated this task. Specifically,
Chang and Lai (2004) describes a hidden markov
model (HMM) to model the relationship between
a full-form phrase and its abbreviation, by treat-
ing the abbreviation as the observation and the full-
form words as states in the model. Using a set
of manually-created full-abbreviation relations as
training data, they report experimental results on
a recognition task (i.e., given an abbreviation, the
task is to obtain its full-form, or the vice versa).
Chang and Teng (2006) extends the work in Chang
and Lai (2004) to automatically extract the relations
between full-form phrases and their abbreviations,
where both the full-form phrase and its abbrevia-
tion are not given. Clearly, the method in (Chang
and Lai, 2004; Chang and Teng, 2006) is super-
vised because it requires the full-abbreviation rela-
tions as training data. Li and Yarowsky (2008) pro-
pose an unsupervised method to extract the relations
between full-form phrases and their abbreviations.
They exploit the data co-occurrence phenomena in
the newswire text, as we have done in this paper.
Moreover, they augment and improve a statistical
machine translation by incorporating the extracted
relations into the baseline translation system.
Other interesting work that addresses a similar
task as ours includes the work on homophones (e.g.,
Lee and Chen (1997)), abbreviations with their defi-
nitions (e.g., Park and Byrd (2001)), abbreviations
and acronyms in the medical domain (Pakhomov,
2002), and transliteration (e.g., (Knight and Graehl,
1998; Virga and Khudanpur, 2003; Li et al, 2004;
Wu and Chang, 2007)).
While all the above work deals with the rela-
tions occurring within the formal text, we consider
the formal-informal relations that occur across both
formal and informal text, and we extract the rela-
tions from the web corpora, instead from just formal
text. Moreover, our method is semi-supervised in
the sense that the weights of the feature functions
are tuned in a supervised log-linear model using a
small number of seed relations while the generation
and ranking of the hypotheses are unsupervised by
exploiting the data co-occurrence phenomena.
7 Conclusions
In this paper, we have first presented a taxonomy of
the formal-informal relations occurring in Chinese
text. We have then proposed a novel method for
discovering and modeling the relationship between
informal Chinese expressions (including colloqui-
alisms and instant-messaging slang) and their formal
equivalents. Specifically, we have proposed a boot-
strapping procedure to identify a list of candidate
informal phrases in web corpora. Given an infor-
mal phrase, we retrieved contextual instances from
the web using a search engine, generated hypothe-
ses of formal equivalents via this data, and ranked
the hypotheses using a conditional log-linear model.
In the log-linear model, we incorporated as feature
functions both rule-based intuitions and data co-
occurrence phenomena (either as an explicit or in-
direct definition, or through formal/informal usages
occurring in free variation in a discourse). We tested
our system on manually collected test examples,
and found that the (formal-informal) relationship
discovery and extraction process using our method
achieves an average 1-best precision of 62%. Given
the ubiquity of informal conversational style on the
internet, this work has clear applications for text nor-
malization in text-processing systems including ma-
chine translation aspiring to broad coverage.
Acknowledgments
We would like to thank Yi Su, Sanjeev Khudanpur,
and the anonymous reviewers for their helpful com-
ments. This work was partially supported by the De-
fense Advanced Research Projects Agency?s GALE
program via Contract No
?
HR0011-06-2-0001.
1039
References
S. J. Benson, L. C. McInnes, J. J. More, and J. Sarich.
2002. Tao users manual, Technical Report ANL/MCS-
TM-242-Revision 1.4, Argonne National Laboratory.
S. J. Benson and J. J. More. 2002. A limited memory vari-
able metric method for bound constrained minimiza-
tion. preprint ANL/ACSP909-0901, Argonne National
Laboratory.
Jing-Shin Chang and Yu-Tso Lai. 2004. A preliminary
study on probabilistic models for Chinese abbrevia-
tions. In Proceedings of the 3rd SIGHAN Workshop
on Chinese Language Processing, Barcelona, Spain
(2004),pages 9-16.
Jing-Shin Chang and Wei-Lun Teng. 2006. Mining
Atomic Chinese Abbreviation Pairs: A Probabilistic
Model for Single Character Word Recovery. In Pro-
ceedings of the 5rd SIGHAN Workshop on Chinese
Language Processing, Sydney, Australia (2006), pages
17-24.
Kevin Knight and Jonathan Graehl. 1998. Machine
Transliteration. Computational Linguistics, 24(4):599-
612.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan,Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
strantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of ACL, Demonstration Session, pages 177-180.
H.W.D Lee. 2005. A study of automatic expansion of
Chinese abbreviations. MA Thesis, The University of
Hong Kong.
Yue-Shi Lee and Hsin-Hsi Chen. 1997. Applying Repair
Processing in Chinese Homophone Disambiguation.
In Proceedings of the Fifth Conference on Applied Nat-
ural Language Processing, pages 57-63.
Haizhou Li, Min Zhang, and Jian Su. 2004. A joint source
channel model for machine transliteration. In Proceed-
ings of ACL 2004, pages 159-166.
Zhifei Li and David Yarowsky. 2008. Unsupervised
Translation Induction for Chinese Abbreviations using
Monolingual Corpora. In Proceedings of ACL 2008,
pages 425-433.
R. Malouf. 2002. A comparison of algorithms for maxi-
mum entropy parameter estimation. In Proceedings of
CoNLL 2002, pages 49-55.
Serguei Pakhomov. 2002. Semi-Supervised Maximum
Entropy Based Approach to Acronym and Abbrevia-
tion Normalization in Medical Texts. In Proceedings
of ACL 2002, pages 160-167.
Youngja Park and Roy J. Byrd. 2001. Hybrid text min-
ing for finding abbreviations and their definitions. In
Proceedings of EMNLP 2001, pages 126-133.
Brian Roark, Murat Saraclar, and Michael Collins. 2007.
Discriminative n-gram language modeling. Computer
Speech and Language, 21(2):373-392.
Paola Virga and Sanjeev Khudanpur. 2003. Transliter-
ation of Proper Names in Cross lingual Information
Retrieval. In Proceedings of the ACL 2003 Workshop
on Multilingual and Mixed-language Named Entity
Recognition.
Jian-Cheng Wu and Jason S. Chang. 2007. Learning to
Find English to Chinese Transliterations on the Web.
In Proceedings of EMNLP-CoNLL 2007, pages 996-
1004.
David Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proceedings
of ACL 1995, pages 189-196.
Z.P. Yin. 1999. Methodologies and principles of Chi-
nese abbreviation formation. In Language Teaching
and Study, No.2 (1999) 73-82.
1040
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 300?308,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Structural, Transitive and Latent Models for Biographic Fact Extraction
Nikesh Garera and David Yarowsky
Department of Computer Science, Johns Hopkins University
Human Language Technology Center of Excellence
Baltimore MD, USA
{ngarera,yarowsky}@cs.jhu.edu
Abstract
This paper presents six novel approaches
to biographic fact extraction that model
structural, transitive and latent proper-
ties of biographical data. The ensem-
ble of these proposed models substantially
outperforms standard pattern-based bio-
graphic fact extraction methods and per-
formance is further improved by modeling
inter-attribute correlations and distribu-
tions over functions of attributes, achiev-
ing an average extraction accuracy of 80%
over seven types of biographic attributes.
1 Introduction
Extracting biographic facts such as ?Birthdate?,
?Occupation?, ?Nationality?, etc. is a critical step
for advancing the state of the art in information
processing and retrieval. An important aspect of
web search is to be able to narrow down search
results by distinguishing among people with the
same name leading to multiple efforts focusing
on web person name disambiguation in the liter-
ature (Mann and Yarowsky, 2003; Artiles et al,
2007, Cucerzan, 2007). While biographic facts are
certainly useful for disambiguating person names,
they also allow for automatic extraction of ency-
lopedic knowledge that has been limited to man-
ual efforts such as Britannica, Wikipedia, etc.
Such encyploedic knowledge can advance verti-
cal search engines such as http://www.spock.com
that are focused on people searches where one can
get an enhanced search interface for searching by
various biographic attributes. Biographic facts are
also useful for powerful query mechanisms such
as finding what attributes are common between
two people (Auer and Lehmann, 2007).
Figure 1: Goal: extracting attribute-value bio-
graphic fact pairs from biographic free-text
While there are a large quantity of biographic texts
available online, there are only a few biographic
fact databases available1, and most of them have
been created manually, are incomplete and are
available primarily in English.
This work presents multiple novel approaches
for automatically extracting biographic facts such
as ?Birthdate?, ?Occupation?, ?Nationality?, and
?Religion?, making use of diverse sources of in-
formation present in biographies.
In particular, we have proposed and evaluated the
following 6 distinct original approaches to this
1E.g.: http://www.nndb.com, http://www.biography.com,
Infoboxes in Wikipedia
300
task with large collective empirical gains:
1. An improvement to the Ravichandran and
Hovy (2002) algorithm based on Partially
Untethered Contextual Pattern Models
2. Learning a position-based model using ab-
solute and relative positions and sequential
order of hypotheses that satisfy the domain
model. For example, ?Deathdate? very often
appears after ?Birthdate? in a biography.
3. Using transitive models over attributes via
co-occurring entities. For example, other
people mentioned person?s biography page
tend to have similar attributes such as occu-
pation (See Figure 4).
4. Using latent wide-document-context models
to detect attributes that may not be mentioned
directly in the article (e.g. the words ?song,
hits, album, recorded,..? all collectively indi-
cate the occupation of singer or musician in
the article.
5. Using inter-attribute correlations, for filter-
ing unlikely biographic attribute combina-
tions. For example, a tuple consisting of <
?Nationality? = India, ?Religion? = Hindu >
has a higher probability than a tuple consist-
ing of < ?Nationality? = France, ?Religion?
= Hindu >.
6. Learning distributions over functions of at-
tributes, for example, using an age distri-
bution to filter tuples containing improbable
<deathyear>-<birthyear> lifespan values.
We propose and evaluate techniques for exploiting
all of the above classes of information in the next
sections.
2 Related Work
The literature for biography extraction falls into
two major classes. The first one deals with iden-
tifying and extracting biographical sentences and
treats the problem as a summarization task (Cowie
et al, 2000, Schiffman et al, 2001, Zhou et
al., 2004). The second and more closely related
class deals with extracting specific facts such as
?birthplace?, ?occupation?, etc. For this task,
the primary theme of work in the literature has
been to treat the task as a general semantic-class
learning problem where one starts with a few
seeds of the semantic relationship of interest and
learns contextual patterns such as ?<NAME>
was born in <Birthplace>? or ?<NAME> (born
<Birthdate>)? (Hearst, 1992; Riloff, 1996; The-
len and Riloff, 2002; Agichtein and Gravano,
2000; Ravichandran and Hovy, 2002; Mann and
Yarowsky, 2003; Jijkoun et al, 2004; Mann and
Yarowsky, 2005; Alfonseca et al, 2006; Pasca et
al., 2006). There has also been some work on ex-
tracting biographic facts directly from Wikipedia
pages. Culotta et al (2006) deal with learning
contextual patterns for extracting family relation-
ships from Wikipedia. Ruiz-Casado et al (2006)
learn contextual patterns for biographic facts and
apply them to Wikipedia pages.
While the pattern-learning approach extends well
for a few biography classes, some of the bio-
graphic facts like ?Gender? and ?Religion? do not
have consistent contextual patterns, and only a
few of the explicit biographic attributes such as
?Birthdate?, ?Deathdate?, ?Birthplace? and ?Oc-
cupation? have been shown to work well in the
pattern-learning framework (Mann and Yarowsky,
2005; Alfonesca, 2006; Pasca et al, 2006).
Secondly, there is a general lack of work that at-
tempts to utilize the typical information sequenc-
ing within biographic texts for fact extraction, and
we show how the information structure of biogra-
phies can be used to improve upon pattern based
models. Furthermore, we also present additional
novel models of attribute correlation and age dis-
tribution that aid the extraction process.
3 Approach
We first implement the standard pattern-based ap-
proach for extracting biographic facts from the raw
prose in Wikipedia people pages. We then present
an array of novel techniques exploiting different
classes of information including partially-tethered
contextual patterns, relative attribute position and
sequence, transitive attributes of co-occurring en-
tities, broad-context topical profiles, inter-attribute
correlations and likely human age distributions.
For illustrative purposes, we motivate each tech-
nique using one or two attributes but in practice
they can be applied to a wide range of attributes
and empirical results in Table 4 show that they
give consistent performance gains across multiple
attributes.
301
4 Contextual Pattern-Based Model
A standard model for extracting biographic facts
is to learn templatic contextual patterns such as
<NAME> ?was born in? <Birthplace>. Such
templatic patterns can be learned using seed ex-
amples of the attribute in question and, there has
been a plethora of work in the seed-based boot-
strapping literature which addresses this problem
(Ravichandran and Hovy, 2002; Thelen and Riloff,
2002; Mann and Yarowsky, 2005; Alfonseca et al,
2006; Pasca et al, 2006)
Thus for our baseline we implemented a stan-
dard Ravichandran and Hovy (2002) pattern
learning model using 100 seed2 examples from
an online biographic database called NNDB
(http://www.nndb.com) for each of the biographic
attributes: ?Birthdate?, ?Birthplace?, ?Death-
date?, ?Gender?, ?Nationality?, ?Occupation? and
?Religion?. Given the seed pairs, patterns for
each attribute were learned by searching for seed
<Name,Attribute Value> pairs in the Wikipedia
page and extracting the left, middle and right con-
texts as various contextual patterns3.
While the biographic text was obtained from
Wikipedia articles, all of the 7 attribute values
used as seed and test person names could not
be obtained from Wikipedia due to incomplete
and unnormalized (for attribute value format) in-
foboxes. Hence, the values for training/evaluation
were extracted from NNDB which provides a
cleaner set of gold truth, and is similar to an ap-
proach utilizing trained annotators for marking up
and extracting the factual information in a stan-
dard format. For consistency, only the people
names whose articles occur in Wikipedia where
selected as part of seed and test sets.
Given the attribute values of the seed names and
their text articles, the probability of a relationship
r(Attribute Name), given the surrounding context
?A1 p A2 q A3?, where p and q are <NAME>
and <Attrib Val> respectively, is given using the
rote extractor model probability as in (Ravichan-
dran and Hovy, 2002; Mann and Yarowsky 2005):
2The seed examples were chosen randomly, with a bias
against duplicate attribute values to increase training diver-
sity. Both the seed and test names and data will be made
available online to the research community for replication
and extension.
3We implemented a noisy model of coreference resolu-
tion by resolving any gender-correct pronoun used in the
Wikipedia page to the title person name of the article. Gender
is also extracted automatically as a biographic attribute.
P (r(p, q)|A1pA2qA3) =
?
x,y?r
c(A1xA2yA3)
?
x,z
c(A1xA2zA3)
Thus, the probability for each contextual pattern
is based on how often it correctly predicts a re-
lationship in the seed set. And, each extracted
attribute value q using the given pattern can thus
be ranked according to the above probability. We
tested this approach for extracting values for each
of the seven attributes on a test set of 100 held-out
names and report Precision, Pseudo-recall and F-
score for each attribute which are computed in the
standard way as follows, for say Attribute ?Birth-
place (bplace)?:
Precisionbplace =
# people with bplace correctly extracted
# of people with bplace extracted
Pseudo-recbplace =
# people with bplace correctly extracted
# of people with bplace in test set
F-scorebplace =
2?Precisionbplace?Pseudo-recbplace
Precisionbplace + Pseudo-recbplace
Since the true values of each attribute are obtained
from a cleaner and normalized person-database
(NNDB), not all the attribute values maybe present
in the Wikipedia article for a given name. Thus,
we also compute accuracy on the subset of names
for which the value of a given attribute is also ex-
plictly stated in the article. This is denoted as:
Acctruth pres =
# people with bplace correctly extracted
# of people with true bplace stated in article
We further applied a domain model for each at-
tribute to filter noisy targets extracted from lex-
ical patterns. Our domain models of attributes
include lists of acceptable values (such as lists
of places, occupations and religions) and struc-
tural constraints such as possible date formats for
?Birthdate? and ?Deathdate?. The rows with sub-
script ?RH02?in Table 4 shows the performance
of this Ravichandran and Hovy (2002) model with
additional attribute domain modeling for each at-
tribute, and Table 3 shows the average perfor-
mance across all attributes.
5 Partially Untethered Templatic
Contextual Patterns
The pattern-learning literature for fact extraction
often consists of patterns with a ?hook? and
?target? (Mann and Yarowsky, 2005). For ex-
ample, in the pattern ?<Name> was born in
<Birthplace>?, ?<NAME>? is the hook and
?<Birthplace>? is the target. The disadvantage
of this approach is that the intervening dually-
tethered patterns can be quite long and highly
variable, such as ?<NAME> was highly influ-
302
Figure 2: Distribution of the observed document
mentions of Deathdate, Nationality and Religion.
ential in his role as <Occupation>?. We over-
come this problem by modeling partially unteth-
ered variable-length ngram patterns adjacent to
only the target, with the only constraint being
that the hook entity appear somewhere in the sen-
tence4. Examples of these new contextual ngram
features include ?his role as <Occupation>? and
?role as <Occupation>?. The pattern probability
model here is essentially the same as in Ravichan-
dran and Hovy, 2002 and just the pattern repre-
sentation is changed. The rows with subscript
?RH02imp? in tables 4 and 3 show performance
gains using this improved templatic-pattern-based
model, yielding an absolute 21% gain in accuracy.
6 Document-Position-Based Model
One of the properties of biographic genres is that
primary biographic attributes5 tend to appear in
characteristic positions, often toward the begin-
ning of the article. Thus, the absolute position
(in percentage) can be modeled explicitly using a
Gaussian parametric model as follows for choos-
ing the best candidate value v? for a given attribute
A:
v? = argmaxv?domain(A)f(posnv|A)
where,
f(posnv|A)
= N (posnv; ??A, ??2A)
= 1
??A
?
2pi
e?(posnv???A)
2/2??A2
4This constraint is particularly viable in biographic text,
which tends to focus on the properties of a single individual.
5We use the hyperlinked phrases as potential values for all
attributes except ?Gender?. For ?Gender? we used pronouns
as potential values ranked according to the their distance from
the beginning of the page.
In the above equation, posnv is the absolute
position ratio (position/length) and ??A, ??A
2 are
the sample mean and variance based on the sam-
ple of correct position ratios of attribute values
in biographies with attribute A. Figure 2, for
example, shows the positional distribution of the
seed attribute values for deathdate, nationality and
religion in Wikipedia articles, fit to a Gaussian
distribution. Combining this empirically derived
position model with a domain model6 of accept-
able attribute values is effective enough to serve
as a stand-alone model.
Attribute Best rank P(Rank)
in seed set
Birthplace 1 0.61
Birthdate 1 0.98
Deathdate 2 0.58
Gender 1 1.0
Occupation 1 0.70
Nationality 1 0.83
Religion 1 0.80
Table 1: Majority rank of the correct attribute
value in the Wikipedia pages of the seed names
used for learning relative ordering among at-
tributes satisfying the domain model
6.1 Learning Relative Ordering in the
Position-Based Model
In practice, for attributes such as birthdate, the
first text pattern satisfying the domain model is
often the correct answer for biographical articles.
Deathdate also tends to occur near the beginning
of the article, but almost always some point
after the birthdate. This motivates a second,
sequence-based position model based on the rank
of the attribute values among other values in the
domain of the attribute, as follows:
v? = argmaxv?domain(A)P (rankv|A)
where P (rankv|A) is the fraction of biographies
having attribute a with the correct value occuring
at rank rankv, where rank is measured according
to the relative order in which the values belonging
to the attribute domain occur from the beginning
6The domain model is the same as used in Section 4 and
remains constant across all the models developed in this paper
303
of the article. We use the seed set to learn the rel-
ative positions between attributes, that is, in the
Wikipedia pages of seed names what is the rank of
the correct attribute.
Table 1 shows the most frequent rank of the correct
attribute value and Figure 3 shows the distribu-
tion of the correct ranks for a sample of attributes.
We can see that 61% of the time the first loca-
tion mentioned in a biography is the individuals?s
birthplace, while 58% of the time the 2nd date
in the article is the deathdate. Thus, ?Deathdate?
often appears as the second date in a Wikipedia
page as expected. These empirical distributions
for the correct rank provide a direct vehicle for
scoring hypotheses, and the rows with ?rel. posn?
as the subscript in Table 4 shows the improvement
in performance using the learned relative order-
ing. Averaging across different attributes, table
3 shows an absolute 11% average gain in accu-
racy of the position-sequence-based models rela-
tive to the improved Ravichandran and Hovy re-
sults achieved here.
Figure 3: Empirical distribution of the relative po-
sition of the correct (seed) answers among all text
phrases satisfying the domain model for ?birth-
place? and ?death date?.
7 Implicit Models
Some of the biographic attributes such as ?Nation-
ality?, ?Occupation? and ?Religion? can be ex-
tracted successfully even when the answer is not
directly mentioned in the biographic article. We
present two such models for doing so in the fol-
lowing subsections:
7.1 Extracting Attributes Transitively using
Neighboring Person-Names
Attributes such as ?Occupation? are transitive in
nature, that is, the people names appearing close
to the target name will tend to have the same
occupation as the target name. Based on this
intution, we implemented a transitive model that
predicts occupation based on consensus voting via
the extracted occupations of neighboring names7
as follows:
v? = argmaxv?domain(A)P (v|A,Sneighbors)
where,
P (v|A,Sneighbors) =
# neighboring names with attrib value v
# of neighboring names in the article
The set of neighboring names is represented
as Sneighbors and the best candidate value for
an attribute A is chosen based on the the fraction
of neighboring names having the same value
for the respective attribute. We rank candidates
according to this probability and the row labeled
?trans? in Table 4 shows that this model helps in
subsantially improving the recall of ?Occupation?
and ?Religion?, yielding a 7% and 3% average
improvement in F-measure respectively, on top of
the position model described in Section 6.
7.2 Latent Model based on Document-Wide
Context Profiles
In addition to modeling cross-entity attribute
transitively, attributes such as ?Occupation? can
also be modeled successfully using a document-
wide context or topic model. For example, the
distribution of words occuring in a biography
7We only use the neighboring names whose attribute
value can be obtained from an encylopedic database. Fur-
thermore, since we are dealing with biographic pages that
talk about a single person, all other person-names mentioned
in the article whose attributes are present in an encylopedia
were considered for consensus voting
304
Figure 4: Illustration of modeling ?occupation? and ?nationality? transitively via consensus from at-
tributes of neighboring names
of a politician would be different from that of
a scientist. Thus, even if the occupation is not
explicitly mentioned in the article, one can infer
it using a bag-of-words topic profile learned from
the seed examples.
Given a value v, for an attribute A, (for ex-
ample v = ?Politician? and A = ?Occupation?),
we learn a centroid weight vector:
Cv = [w1,v, w2,v, ..., wn,v] where,
wt,v = 1N tft,v ? log
|A|
|t?A|
tft,v is the frequency of word t in the articles of People
having attribute A = v
|A| is the total number of values of attribute A
|t ? A| is the total number of values of attribute A, such that
the articles of people having one of those values contain the
term t
N is the total number of People in the seed set
Given a biography article of a test name and
an attribute in question, we compute a similar
word weight vector C ? = [w?1, w
?
2, ..., w
?
n] for
the test name and measure its cosine similarity
to the centroid vector of each value of the given
attribute. Thus, the best value a? is chosen as:
v? =
argmaxv
w?1?w1,v+w
?
2?w2,v+....+w
?
n?wn,v?
w?21 +w
?2
2 +...+w
?2
n
?
w21,v+w
2
2,v+...+w
2
n,v
Tables 3 and 4 show performance using the la-
tent document-wide-context model. We see that
this model by itself gives the top performance
on ?Occupation?, outperforming the best alterna-
tive model by 9% absolute accuracy, indicating
the usefulness of implicit attribute modeling via
broad-context word frequencies.
This latent model can be further extended us-
ing the multilingual nature of Wikipedia. We
take the corresponding German pages of the train-
ing names and model the German word distribu-
tions characterizing each seed occupation. Table
4 shows that English attribute classification can be
successful using only the words in a parallel Ger-
man article. For some attributes, the performance
of latent model modeled via cross-language (noted
as latentCL) is close to that of English suggesting
potential future work by exploiting this multilin-
gual dimension.
It is interesting to note that both the transitive
model and the latent wide-context model do not
rely on the actual ?Occupation? being explicitly
mentioned in the article, they still outperform ex-
305
Occupation Weight Vector
English
Physicist <magnetic:32.7, electromagnetic:18.2, wire: 18.2, electricity: 17.7, optical:14.5, discovered:11.2>
Singer <song:40, hits:30.5, hit:29.6, reggae:23.6, album:17.1, francis:15.2, music:13.8, recorded:13.6, ...>
Politician <humphrey:367.4, soviet: 97.4, votes: 70.6, senate: 64.7, democratic: 57.2, kennedy: 55.9, ...>
Painter <mural:40.0, diego:14.7, paint:14.5, fresco:10.9. paintings:10.9, museum of modern art:8.83, ...>
Auto racing <renault:76.3, championship:32.7. schumacher:32.7, race:30.4, pole:29.1, driver:28.1 >
German
Physicist <faraday:25.4, chemie:7.3, vorlesungsserie:7.2, 1846:5.8, entdeckt:4.5, rotation:3.6 ...>
Singer <song:16.22, jamaikanischen:11.77, platz:7.3, hit: 6.7, solou?nstler:4.5, album:4.1, widmet:4.0, ...>
Politician <konservativen:26.5, wahlkreis:26.5, romano:21.8, stimmen:18.6, gewa?hlt:18.4, ...>
Painter <rivera:32.7, malerin:7.6, wandgema?lde:7.3, kunst:6.75, 1940:5.8, maler:5.1, auftrag:4.5, ...>
Auto racing <team:29.4,mclaren:18.1,teamkollegen:18.1,sieg:11.7, meisterschaft:10.9, gegner:10.9, ...>
Table 2: Sample of occupation weight vectors in English and German learned using the latent model.
plicit pattern-based and position-based models.
This implicit modeling also helps in improving the
recall of less-often directly mentioned attributes
such as a person?s ?Religion?.
8 Model Combination
While the pattern-based, position-based, transitive
and latent models are all stand-alone models, they
can complement each other in combination as they
provide relatively orthogonal sources of informa-
tion. To combine these models, we perform a sim-
ple backoff-based combination for each attribute
based on stand-alone model performance, and the
rows with subscript ?combined? in Tables 3 and 4
shows an average 14% absolute performance gain
of the combined model relative to the improved
Ravichandran and Hovy 2002 model.
9 Further Extensions: Reducing False
Positives
Since the position-and-domain-based models will
almost always posit an answer, one of the prob-
lems is the high number of false positives yielded
by these algorithms. The following subsections in-
troduce further extensions using interesting prop-
erties of biographic attributes to reduce the effect
of false positives.
9.1 Using Inter-Attribute Correlations
One of the ways to filter false positives is by
filtering empirically incompatible inter-attribute
pairings. The motivation here is that the at-
tributes are not independent of each other when
modeled for the same individual. For example,
P(Religion=Hindu | Nationality=India) is higher
than P(Religion=Hindu | Nationality=France) and
Model Fscore Acc
truth
pres
Ravichandran and Hovy, 2002 0.37 0.43
Improved RH02 Model 0.54 0.64
Position-Based Model 0.53 0.75
Combinedabove 3+trans+latent+cl 0.59 0.78
Combined + Age Dist + Corr 0.62 0.80
(+24%) (+37%)
Table 3: Average Performance of different models
across all biographic attributes
similarly we can find positive and negative cor-
relations among other attribute pairings. For im-
plementation, we consider all possible 3-tuples
of (?Nationality?, ?Birthplace?, ?Religion?)8 and
search on NNDB for the presence of the tuple for
any individual in the database (excluding the test
data of course). As an agressive but effective filter,
we filter the tuples for which no name in NNDB
was found containing the candidate 3-tuples. The
rows with label ?combined+corr? in Table 4 and
Table 3 shows substantial performaance gains us-
ing inter-attribute correlations, such as the 7% ab-
solute average gain for Birthplace over the Section
8 combined models, and a 3% absolute gain for
Nationality and Religion.
9.2 Using Age Distribution
Another way to filter out false positives is to con-
sider distributions on meta-attributes, for example:
while age is not explicitly extracted, we can use
the fact that age is a function of two extracted at-
tributes (<Deathyear>-<Birthyear>) and use the
age distribution to filter out false positives for
8The test of joint-presence between these three attributes
were used since they are strongly correlated
306
Figure 5: Age distribution of famous people on the
web (from www.spock.com)
<Birthdate> and<Deathdate>. Based on the age
distribution for famous people9 on the web shown
in Figure 5, we can bias against unusual candi-
date lifespans and filter out completely those out-
side the range of 25-100, as most of the probabil-
ity mass is concentrated in this range. Rows with
subscript ?comb+ age dist? in Table 4 shows the
performance gains using this feature, yielding an
average 5% absolute accuracy gain for Birthdate.
10 Conclusion
This paper has shown six successful novel ap-
proaches to biographic fact extraction using struc-
tural, transitive and latent properties of biographic
data. We first showed an improvement to the stan-
dard Ravichandran and Hovy (2002) model uti-
lizing untethered contextual pattern models, fol-
lowed by a document position and sequence-based
approach to attribute modeling.
Next we showed transitive models exploiting the
tendency for individuals occurring together in an
article to have related attribute values. We also
showed how latent models of wide document con-
text, both monolingually and translingually, can
capture facts that are not stated directly in a text.
Each of these models provide substantial per-
formance gain, and further performance gain is
achived via classifier combination. We also
showed how inter-attribution correlations can be
9Since all the seed and test examples were used from
nndb.com, we use the age distribution of famous people on
the web: http://blog.spock.com/2008/02/08/age-distribution-
of-people-on-the-web/
Attribute Prec P-Rec Fscore Acc
truth
pres
BirthdateRH02 0.86 0.38 0.53 0.88
BirthdateRH02imp 0.52 0.52 0.52 0.67
Birthdaterel. posn 0.42 0.40 0.41 0.93
Birthdatecombined 0.58 0.58 0.58 0.95
Birthdatecomb+age dist 0.63 0.60 0.61 1.00
DeathdateRH02 0.80 0.19 0.30 0.36
DeathdateRH02imp 0.50 0.49 0.49 0.59
Deathdaterel. posn 0.46 0.44 0.45 0.86
Deathdatecombined 0.49 0.49 0.49 0.86
Deathdatecomb+age dist 0.51 0.49 0.50 0.86
BirthplaceRH02 0.42 0.38 0.40 0.42
BirthplaceRH02imp 0.41 0.41 0.41 0.45
Birthplacerel. posn 0.47 0.41 0.44 0.48
Birthplacecombined 0.44 0.44 0.44 0.48
Birthplacecombined+corr 0.53 0.50 0.51 0.55
OccupationRH02 0.54 0.18 0.27 0.26
OccupationRH02imp 0.38 0.34 0.36 0.48
Occupationrel. posn 0.48 0.35 0.40 0.50
Occupationtrans 0.49 0.46 0.47 0.50
Occupationlatent 0.48 0.48 0.48 0.59
OccupationlatentCL 0.48 0.48 0.48 0.54
Occupationcombined 0.48 0.48 0.48 0.59
NationalityRH02 0.40 0.25 0.31 0.27
NationalityRH02imp 0.75 0.75 0.75 0.81
Nationalityrel. posn 0.73 0.72 0.71 0.78
Nationalitytrans 0.51 0.48 0.49 0.49
Nationalitylatent 0.56 0.56 0.56 0.56
NationalitylatentCL 0.55 0.48 0.51 0.48
Nationalitycombined 0.75 0.75 0.75 0.81
Nationalitycomb+corr 0.77 0.77 0.77 0.84
GenderRH02 0.76 0.76 0.76 0.76
GenderRH02imp 0.99 0.99 0.99 0.99
Genderrel. posn 1.00 1.00 1.00 1.00
Gendertrans 0.79 0.75 0.77 0.75
Genderlatent 0.82 0.82 0.82 0.82
GenderlatentCL 0.83 0.72 0.77 0.72
Gendercombined 1.00 1.00 1.00 1.00
ReligionRH02 0.02 0.02 0.04 0.06
ReligionRH02imp 0.55 0.18 0.27 0.45
Religionrel. posn 0.49 0.24 0.32 0.73
Religiontrans 0.38 0.33 0.35 0.48
Religionlatent 0.36 0.36 0.36 0.45
ReligionlatentCL 0.30 0.26 0.28 0.22
Religioncombined 0.41 0.41 0.41 0.76
Religioncombined+corr 0.44 0.44 0.44 0.79
Table 4: Attribute-wise performance comparison
of all the models across several biographic at-
tributes.
modeled to filter unlikely attribute combinations,
and how models of functions over attributes, such
as deathdate-birthdate distributions, can further
constrain the candidate space. These approaches
collectively achieve 80% average accuracy on a
test set of 7 biographic attribute types, yielding a
37% absolute accuracy gain relative to a standard
algorithm on the same data.
307
References
E. Agichtein and L. Gravano. 2000. Snowball: ex-
tracting relations from large plain-text collections.
Proceedings of ICDL, pages 85?94.
E. Alfonseca, P. Castells, M. Okumura, and M. Ruiz-
Casado. 2006. A rote extractor with edit distance-
based generalisation and multi-corpora precision
calculation. Proceedings of COLING-ACL, pages
9?16.
J. Artiles, J. Gonzalo, and S. Sekine. 2007. The
semeval-2007 weps evaluation: Establishing a
benchmark for the web people search task. In Pro-
ceedings of SemEval, pages 64?69.
S. Auer and J. Lehmann. 2007. What have Innsbruck
and Leipzig in common? Extracting Semantics from
Wiki Content. Proceedings of ESWC, pages 503?
517.
A. Bagga and B. Baldwin. 1998. Entity-Based Cross-
Document Coreferencing Using the Vector Space
Model. Proceedings of COLING-ACL, pages 79?
85.
R. Bunescu and M. Pasca. 2006. Using encyclopedic
knowledge for named entity disambiguation. Pro-
ceedings of EACL, pages 3?7.
J. Cowie, S. Nirenburg, and H. Molina-Salgado. 2000.
Generating personal profiles. The International
Conference On MT And Multilingual NLP.
S. Cucerzan. 2007. Large-scale named entity disam-
biguation based on wikipedia data. Proceedings of
EMNLP-CoNLL, pages 708?716.
A. Culotta, A. McCallum, and J. Betz. 2006. Integrat-
ing probabilistic extraction models and data mining
to discover relations and patterns in text. Proceed-
ings of HLT-NAACL, pages 296?303.
E. Filatova and J. Prager. 2005. Tell me what you do
and I?ll tell you what you are: Learning occupation-
related activities for biographies. Proceedings of
HLT-EMNLP, pages 113?120.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of COLING,
pages 539?545.
V. Jijkoun, M. de Rijke, and J. Mur. 2004. Infor-
mation extraction for question answering: improv-
ing recall through syntactic patterns. Proceedings of
COLING, page 1284.
G.S. Mann and D. Yarowsky. 2003. Unsupervised
personal name disambiguation. In Proceedings of
CoNLL, pages 33?40.
G.S. Mann and D. Yarowsky. 2005. Multi-field in-
formation extraction and cross-document fusion. In
Proceedings of ACL, pages 483?490.
A. Nenkova and K. McKeown. 2003. References to
named entities: a corpus study. Proceedings of HLT-
NAACL companion volume, pages 70?72.
M. Pasca, D. Lin, J. Bigham, A. Lifchits, and A. Jain.
2006. Organizing and searching the World Wide
Web of Facts Step one: The One-Million Fact Ex-
traction Challenge. Proceedings of AAAI, pages
1400?1405.
D. Ravichandran and E. Hovy. 2002. Learning sur-
face text patterns for a question answering system.
Proceedings of ACL, pages 41?47.
Y. Ravin and Z. Kazi. 1999. Is Hillary Rodham Clin-
ton the President? Disambiguating Names across
Documents. Proceedings of ACL.
M. Remy. 2002. Wikipedia: The Free Encyclopedia.
Online Information Review Year, 26(6).
E. Riloff. 1996. Automatically Generating Extraction
Patterns from Untagged Text. Proceedings of AAAI,
pages 1044?1049.
M. Ruiz-Casado, E. Alfonseca, and P. Castells.
2005. Automatic extraction of semantic relation-
ships for wordnet by means of pattern learning from
wikipedia. Proceedings of NLDB 2005.
M. Ruiz-Casado, E. Alfonseca, and P. Castells. 2006.
From Wikipedia to semantic relationships: a semi-
automated annotation approach. Proceedings of
ESWC.
B. Schiffman, I. Mani, and K.J. Concepcion. 2001.
Producing biographical summaries: combining lin-
guistic knowledge with corpus statistics. Proceed-
ings of ACL, pages 458?465.
M. Thelen and E. Riloff. 2002. A bootstrapping
method for learning semantic lexicons using extrac-
tion pattern contexts. In Proceedings of EMNLP,
pages 14?21.
N. Wacholder, Y. Ravin, and M. Choi. 1997. Disam-
biguation of proper names in text. Proceedings of
ANLP, pages 202?208.
C. Walker, S. Strassel, J. Medero, and K. Maeda. 2006.
Ace 2005 multilingual training corpus. Linguistic
Data Consortium.
R. Weischedel, J. Xu, and A. Licuanan. 2004. A
Hybrid Approach to Answering Biographical Ques-
tions. New Directions In Question Answering, pages
59?70.
M. Wick, A. Culotta, and A. McCallum. 2006. Learn-
ing field compatibilities to extract database records
from unstructured text. In Proceedings of EMNLP,
pages 603?611.
L. Zhou, M. Ticrea, and E. Hovy. 2004. Multidoc-
ument biography summarization. Proceedings of
EMNLP, pages 434?441.
308
Inducing Multilingual Text Analysis Tools
via Robust Projection across Aligned Corpora
David Yarowsky
Dept. of Computer Science
Johns Hopkins University
Baltimore, MD 21218 USA
yarowsky@cs.jhu.edu
Grace Ngai
Dept. of Computer Science
Johns Hopkins University
Baltimore, MD 21218 USA
gyn@cs.jhu.edu
Richard Wicentowski
Dept. of Computer Science
Johns Hopkins University
Baltimore, MD 21218 USA
richardw@cs.jhu.edu
ABSTRACT
This paper describes a system and set of algorithms for automati-
cally inducing stand-alone monolingual part-of-speech taggers, base
noun-phrase bracketers, named-entity taggers and morphological
analyzers for an arbitrary foreign language. Case studies include
French, Chinese, Czech and Spanish.
Existing text analysis tools for English are applied to bilingual
text corpora and their output projected onto the second language
via statistically derived word alignments. Simple direct annotation
projection is quite noisy, however, even with optimal alignments.
Thus this paper presents noise-robust tagger, bracketer and lemma-
tizer training procedures capable of accurate system bootstrapping
from noisy and incomplete initial projections.
Performance of the induced stand-alone part-of-speech tagger
applied to French achieves 96% core part-of-speech (POS) tag ac-
curacy, and the corresponding induced noun-phrase bracketer ex-
ceeds 91% F-measure. The induced morphological analyzer achie-
ves over 99% lemmatization accuracy on the complete French ver-
bal system.
This achievement is particularly noteworthy in that it required
absolutely no hand-annotated training data in the given language,
and virtually no language-specific knowledge or resources beyond
raw text. Performance also significantly exceeds that obtained by
direct annotation projection.
Keywords
multilingual, text analysis, part-of-speech tagging, noun phrase brac-
keting, named entity, morphology, lemmatization, parallel corpora
1. TASK OVERVIEW
A fundamental roadblock to developing statistical taggers, brack-
eters and other analyzers for many of the world?s 200+ major lan-
guages is the shortage or absence of annotated training data for the
large majority of these languages. Ideally, one would like to lever-
.
[][ ]
IN NNP NNP VBG VBG NNS NNSJJ JJ JJ
HongIn
Kong
national law(s)implementing of
109876543210
National   laws  applying  in  Hong  Kong
2 4 53
[ [] ]JJ VBG IN NNP NNP
0 1
NNS
un  producteur  important   de   petrole  brut 
DT[
NN
20
NN ]a  significant  producer    for    crude  oil[ ]
22 23
NNJJIN
21
JJ
1918
12
JJJJ NN
13 14 151110[ [ ]]
DT
[PLACE]
[PLACE]
Annotations From Existing English Tools
Annotations From Existing English Tools
Induced Annotations for Chinese
Induced Annotations for French
Figure 1: Projecting part-of-speech tags, named-entity tags and
noun-phrase structure from English to Chinese and French.
            	 
 
    

     	 


croyaient croissant croire croitre
believe growing growbelievingbelieved
French RootsFrench Inflections
BELIEVE
   
GROW
English Bridge Lemmas
V
   


  
 
Figure 2: French morphological analysis via English
age the large existing investments in annotated data and tools for
resource-rich languages (such as English and Japanese) to over-
come the annotated resource shortage in other languages.
To show the broad potential of our approach and methods, this
paper will investigate four fundamental language analysis tasks:
POS tagging, base noun phrase (baseNP) bracketing, named en-
tity tagging, and inflectional morphological analysis, as illustrated
in Figures 1 and 2. These bedrock tools are important components
of the language analysis pipelines for many applications, and their
low cost extension to new languages, as described here, can serve
as a broadly useful enabling resource.
2. BACKGROUND
Previous research on the word alignment of parallel corpora has
tended to focus on their use in translation model training for MT
rather than on monolingual applications. One exception is bilin-
gual parsing. Wu (1995, 1997) investigated the use of concurrent
parsing of parallel corpora in a transduction inversion framework,
helping to resolve attachment ambiguities in one language by the
coupled parsing state in the second language. Jones and Havrilla
(1998) utilized similar joint parsing techniques (twisted-pair gram-
mars) for word reordering in target language generation.
However, with these exceptions in the field of parsing, to our
knowledge no one has previously used linguistic annotation pro-
jection via aligned bilingual corpora to induce traditional stand-
alone monolingual text analyzers in other languages. Thus both
our proposed projection and induction methods, and their applica-
tion to multilingual POS tagging, named-entity classification and
morphological analysis induction, appears to be highly novel.
3. DATA RESOURCES
The data sets used in these experiments included the English-
French Canadian Hansards, the English-Chinese Hong Kong Han-
sards, and parallel Czech-English Reader?s Digest collection. In
addition, multiple versions of the Bible were used, including the
French Douay-Rheims Bible, Spanish Reina Valera Bible, and three
English Bible Versions (King James, New International and Re-
vised Standard), automatically verse-aligned in multiple pairings.
All corpora were automatically word-aligned by the now publicly
available EGYPT system (Al-Onaizan et al, 1999), based on IBM?s
Model 3 statistical MT formalism (Brown et al, 1990). The tag-
ging and bracketing tasks utilized approximately 2 million words
in each language, with the sample sizes for morphology induc-
tion given in Table 3. All word alignments utilized strictly raw-
word-based model variants for English/French/Spanish/Czech and
character-based model variants for Chinese, with no use of mor-
phological analysis or stemming, POS-tagging, bracketing or dic-
tionary resources.
4. PART-OF-SPEECHTAGGER INDUCTION
Part-of-speech tagging is the first of four applications covered in
this paper. The goal of this work is to project POS analysis capabil-
ities from one language to another via word-aligned parallel bilin-
gual corpora. To do so, we use an existing POS tagger (e.g. Brill,
1995) to annotate the English side of the parallel corpus. Then,
as illustrated in Figure 1 for Chinese and French, the raw tags are
transferred via the word alignments, yielding an extremely noisy
initial training set for the 2nd language. The third crucial step is to
generalize from these noisy projected annotations in a robust way,
yielding a stand-alone POS tagger for the new language that is con-
siderably more accurate than the initial projected tags.
Additional details of this algorithm are given in Yarowsky and
Ngai (2001). Due to lack of space, the following sections will serve
primarily as an overview of the algorithm and its salient issues.
4.1 Part-of-speech Projection Issues
First, because of considerable cross-language differences in fine-
grained tag set inventories, this work focuses on accurately assign-
ing core POS categories (e.g. noun, verb, adverb, adjective, etc.),
with additional distinctions in verb tense, noun number and pro-
noun type as captured in the English tagset inventory. Although
impoverished relative to some languages, and incapable of resolv-
ing details such as grammatical gender, this Brown-corpus-based
tagset granularity is sufficient for many applications. Furthermore,
many finer-grained part-of-speech distinctions are resolved primar-
ily by morphology, as handled in Section 7. Finally, if one desires
to induce a finer-grained tagging capability for case, for example,
one should project from a reference language such as Czech, where
case is lexically marked.
Figure 3 illustrates six scenarios encountered when projecting
POS tags from English to a language such as French. The first
two show straightforward 1-to-1 projections, which are encoun-
tered in roughly two-thirds of English words. Phrasal (1-to-N)
alignments offer greater challenges, as typically only a subset of
the aligned words accept the English tag. To distinguish these
cases, we initially assign position-sensitive phrasal parts-of-speech
via subscripting (e.g. Les/NNS  lois/NNS  ), and subsequently learn
a probablistic mapping to core, non-phrasal parts of speech (e.g.
P  DT  NNS  ) that is used along with tag sequence and lexical prior
models to re-tag these phrasal POS projections.
French
Induced Tags NN
O    ... salon ...Les lois ...
DT NNS
Tagger Output
English ... living room ...
VBG NN
The laws ...
DT NNS
... veterans ...
... anciens combattants ...
NNS NNS
NNS
(JJ) (NNS)
... potatoes ...
... pommes   de     terre ...
NNS
NNS NNSNNS
(IN) (NN)(NNS)
Tagger Output
English
French
Induced Tag
Correct Tag
Les   lois ...
NNS
(DT) (NNS)
O    Laws ...
NNS
O
Les     lois ...
Laws ...
NNS NNS
(DT) (NNS)
NNS
b a ba bc a
Figure 3: French POS tag projection scenarios
4.2 Noise-robust POS Tagger Training
Even at the relatively low tagset granularity of English, direct
projection of core POS tags onto French achieves only 76% ac-
curacy using EGYPT?s automatic word alignments (as shown in
Table 1). Part of this deficiency is due to word-alignment error;
when word alignments were manually corrected, direct projection
core-tag accuracy increased to 85%. Also, standard bigram taggers
trained on the automatically projected data achieve only modest
success at generalization (86% when reapplied to the noisy train-
ing data). More highly lexicalized learning algorithms exhibit even
greater potential for overmodeling the specific projection errors of
this data.
Thus our research has focused on noise-robust techniques for
distilling a conservative but effective tagger from this challenging
raw projection data. In particular, we modify standard n-gram mod-
eling to separate the training of the tag sequence model Translating Compounds by Learning Component
Gloss Translation Models via Multiple Languages
Nikesh Garera and David Yarowsky
Department of Computer Science
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218, USA
{ngarera,yarowsky}@cs.jhu.edu
Abstract
This paper presents an approach to the
translation of compound words without the
need for bilingual training text, by mod-
eling the mapping of literal component
word glosses (e.g. ?iron-path?) into flu-
ent English (e.g. ?railway?) across mul-
tiple languages. Performance is improved
by adding component-sequence and learned-
morphology models along with context sim-
ilarity from monolingual text and optional
combination with traditional bilingual-text-
based translation discovery.
1 Introduction
Compound words such as lighthouse and fireplace
are words that are composed of two or more compo-
nent words and are often a challenge for machine
translation due to their potentially complex com-
pounding behavior and ambiguous interpretations
(Rackow et al, 1992). For many languages, such
words form a significant portion of the lexicon and
the compounding process is further complicated by
diverse morphological processes (Levi, 1978) and
the properties of different compound sequences such
as Noun-Noun, Adj-Adj, Adj-Noun, Verb-Verb, etc.
Compounds also tend to have a high type frequency
but a low token frequency which makes their transla-
tion difficult to learn using corpus-based algorithms
(Tanaka and Baldwin, 2003). Furthermore, most of
the literature on compound translation has been re-
stricted to a few languages dealing with compound-
ing phenomena specific to the language in question.
Compound Splitting English Gloss Translation
Input: Distilled glosses from German-English dictionary
Krankenhaus Kranken-Haus sick-house hospital
Regenschirm Regen-Schirm rain-guard umbrella
Wo?rterBuch Wo?rter-Buch words-book dictionary
Eisenbahn Eisen-Bahn iron-path railroad
Input: Distilled glosses from Swedish-English dictionary
Sjukhus Sjhu-Khus sick-house hospital
Ja?rnva?g Ja?rn-va?g iron-path railway
Ordbok Ord-Bok words-book dictionary
Goal: To translate new Albanian compounds
Hekurudhe? Hekur-Udhe? iron-path ???
Table 1: Example lexical resources used in this task and their
application to translating compound words in new languages.
With these challenges in mind, the primary goal of
this work is to improve the coverage of translation
lexicons for compounds, as illustrated in Table 1
and Figure 1, in multiple new languages. We show
how using cross-language compound evidence ob-
tained from bilingual dictionaries can aid in com-
pound translation. A primary motivating idea for
this work is that the literal component glosses for
compound words (such as ?iron path? for railway)
is often replicated in multiple languages, providing
insight into the fluent translation of a similar literal
gloss in a new (often resource-poor) language.
2 Resources Utilized
The only resource utilized for our compound trans-
lation lexicon algorithm is a collection of bilingual
dictionaries. We used bilingual dictionary collec-
tions for 50 languages that were acquired in elec-
tronic form over the Internet or via optical character
recognition (OCR) on paper dictionaries. Note that
no parallel or even monolingual corpora is required,
their use described later in the paper is optional.
403
3 Related Work
The compound-translation literature typically deals
with these steps: 1) Compound splitting, 2) transla-
tion candidate generation and 3) translation candi-
date scoring. Compound splitting is generally done
using translation lexicon lookup and allowing for
different splitting options based on corpus frequency
(Zhang et al, 2000; Koehn and Knight, 2003).
Translation candidate generation is an important
phase and this is where our work differs signifi-
cantly from the previous literature. Most of the pre-
vious work has been focused on generating com-
positional translation candidates, that is, the trans-
lation candidates of the compound words are lexi-
cally composed of the component word translations.
This has been done by either just concatenating the
translations of component words to form a candi-
date (Grefenstette, 1999; Cao and Li, 2002), or us-
ing syntactic templates such as ?E2 in E1?, ?E1 of
E2? to form translation candidates from the transla-
tion of the component words E2 and E1 (Baldwin
and Tanaka, 2004), or using synsets of the compo-
nent word translations to include synonyms in the
compositional candidates (Navigli et al, 2003).
The above class of work in compositional-candidate
generation fails to translate compounds such as
Krankenhaus (hospital) whose component word
translations are Kranken (sick) and Haus (hospital),
and composing sick and house in any order will not
result in the correct translation (hospital). Another
problem with using fixed syntactic templates is that
they are restricted to the specific patterns occurring
in the target language. We show how one can use
the gloss patterns of compounds in multiple other
languages to hypothesize translation candidates that
are not lexically compositional.
4 Approach
Our approach to compound word translation is illus-
trated in Figure 1.
4.1 Splitting compound words and gloss
generation with translation lexicon lookup
We first split a given source word, such as the Al-
banian compound hekurudhe?, into a set of compo-
nent word partitions, such as hekur (iron) and udhe?
(path). Our initial approach is to consider all possi-
ble partitions based on contiguous component words
found in a small dictionary for the language, as in
Goal: To translate this 
Albanian compound word:
udh?
(English gloss)
Compound splitting
using lexicon lookup 
Using small Albanian
 English dictionary
Italian-English dictionary
   ferrovia      --->  ferro    via
   (railroad)   <--- (iron)  (path)
German-English dictionary
   eisenbahn  --->  eisen     bahn 
  (railroad)   <---  (iron)    (path)
Swedish-English dictionary
   j?rnv?g  --->    j?rn          v?g 
  (railway)    <--- (iron)    (path)
Uighur-English dictionary
  t?m?ryol   --->   t?m?r     yol     
 (railroad)    <--- (iron)     (path)
Lookup words in other 
languages that result in
"iron path" after splitting
Candidate 
translations 
of hekurudh?
Other dictionaries
iron path
hekur
hekurudh?
zog bird
udh? path
hekur iron
vadis water
0.19
0.14
0.05
railroad
railway
rail
Algorithm output
for hekurudh?  
(iron) (path)
Figure 1: Illustration of using cross-language evidence us-
ing bilingual dictionaries of different languages for compound
translation
404
Brown (2002) and Koehn and Knight (2003)1. For a
given split, we generate its English glosses by using
all possible English translations of the component
words given in the dictionary of that language2.
4.2 Using cross-language evidence from
different bilingual dictionaries
For many compound words (especially for borrow-
ings), the compounding process is identical across
several languages and the literal English gloss re-
mains the same across these languages. For ex-
ample, the English word railway is translated as a
compound word in many languages, and the English
gloss of those compounds is often ?iron path? or a
similar literal meaning3. Thus knowing the fluent
English translation of the literal gloss ?iron path?
in some relatively resource-rich language provides a
vehicle for the translation from all other languages
sharing that literal gloss4
4.3 Ranking translation candidates
The confidence in the correctness of a mapping be-
tween a literal gloss (e.g. ?iron path?) and fluent
translation (e.g. ?railroad?) can be based on the
number of distinct languages exhibiting this associa-
tion. Thus we rank the candidate translations gener-
ated via different languages as in Figure 1 as fol-
lows: For a given target compound word, say fc
with a set of English glosses G obtained via mul-
tiple splitting options or multiple component word
translations, the translation probability for a candi-
date translation can be computed as:
p(ec|fc) =
?
g?G
p(ec, g|fc)
=
?
g?G
p(g|fc) ? p(ec|g, fc)
=
?
g?G
p(g|fc) ? p(ec|g)
1In order to avoid inflections as component-words we limit
the component-word length to at least three characters.
2The algorithm is allowed to generate multiple glosses ?iron
way,? ?iron road,? etc. based on multiple translations of the
component words. Multiple glosses only add to the number of
translation candidates generated.
3For the gloss, ?iron path?, we found 10 other languages in
which some compound word has the English gloss after split-
ting and component-word translation
4We do assume an existing small translation lexicon in the
target language for the individual component-words, but these
are often higher frequency words and present either in a basic
dictionary or discoverable through corpus-based techniques.
where, p(g|fc) = p(g1|f1) ? p(g2|f2). f1, f2 are
the individual component-words of compound and
g1, g2 are their translations from the existing dic-
tionary. For human dictionaries, p(g|fc) is uni-
form for all g ? G, while variable probabilities
can also be acquired from bitext or other translation
discovery approaches. Also, p(ec|g) =
freq(g,ec)
freq(g) ,
where freq(g, ec) is the number of times the com-
pound word with English gloss g is translated as
ec in the bilingual dictionaries of other languages
and freq(g) is the total number of times the English
gloss appears in these dictionaries.
5 Evaluation using Exact-match
Translation Accuracy
For evaluation, we assess the performance of the
algorithm on the following 10 languages: Alba-
nian, Arabic, Bulgarian, Czech, Farsi, German,
Hungarian, Russian, Slovak and Swedish. We de-
tail both the average performance for these 10 lan-
guages (Avg10), as well as provide individual per-
formance details on Albanian, Bulgarian, German
and Swedish. For each of the compound trans-
lation models, we report coverage (the # of com-
pound words for which a hypothesis was generated
by the algorithm) and Top1/Top10 accuracy. Top1
and Top 10 accuracy are the fraction of words for
which a correct translation (listed in the evaluation
dictionary) appears in the Top 1 and Top 10 trans-
lation candidates respectively, as ranked by the al-
gorithm. Because evaluation dictionaries are often
missing acceptable translations (e.g. railroad rather
than railway), and any deviation from exact-match is
scored as incorrect, these measures will be a lower
bound on acceptable translation accuracy. Also,
target language models can often select effectively
among such hypothesis lists in context.
6 Comparison of different compound
translation models
6.1 A simple model using literal English gloss
concatenation as the translation
Our baseline model is a simple gloss concatenation
model for generating compositional translation can-
didates on the lines of Grefenstette (1999) and Cao
and Li (2002). We take the translations of the in-
dividual component-words (e.g. for the compound
word hekurudhe?, they would be hekur (iron) and
405
udhe? (path)) and hypothesizes three translation can-
didate variants: ?iron path?, ?iron-path? and ?iron-
path?. A test instance is scored as correct if any
of these translation candidates occur in the transla-
tions of hekurudhe? in the bilingual dictionary. This
baseline performance measures how well simple lit-
eral glosses serve as translation candidates. In cases
such as the German compoundNu?schale (nutshell),
which is a simple concatenation of the individual
components Nu?(nut) and Schale (shell), the literal
gloss is correct. For this baseline, if the component-
words have multiple translations, then each of the
possible English gloss is ranked randomly. While
Grefenstette (1999) and Cao and Li (2002) proposed
re-ranking these candidates using web-data, the po-
tential gains of this ranking are limited, as we see in
Table 2 that even the Found Acc. is very low5, that
is for most of the cases the correct translation does
not appear anywhere in the set of English glosses6
Language Cmpnd wrds Top1 Top10 Found
translated Acc. Acc. Acc.
Albanian 4472 (10.11%) 0.001 0.010 0.020
Bulgarian 9093 (12.50%) 0.001 0.015 0.031
German 15731 (29.11%) 0.004 0.079 0.134
Swedish 18316 (31.57%) 0.005 0.068 0.111
Avg10 14228 (17.84%) 0.002 0.030 0.055
Table 2: Baseline performance using unreordered literal En-
glish glosses as translations. The percentages in parentheses
indicate what fraction of all the words in the test (entire) vocab-
ulary were detected and translated as compounds.
6.2 Using bilingual dictionaries
This section describes the results from the model ex-
plained in Section 4. To recap, this model attempts
to translate every test word such that there is at least
one additional language whose bilingual dictionary
supports an equivalent split and literal English gloss,
and bases its translation hypotheses on the consen-
sus fluent translation(s) corresponding to the literal
glosses in these other languages. The performance
is shown in Table 3. The substantial increase in ac-
curacy over the baseline indicates the usefulness of
5Found Acc. is the fraction of examples for which the cor-
rect translation appears anywhere in the n-best list
6One explanation for this could be that for only a small per-
centage of compound words, their dictionary translations are
formed by concatenating their English glosses. Also, Grefen-
stette (1999) reports much higher accuracies for German on this
model because the 724 German test compounds were chosen in
such a way that their correct translation is a concatenation of the
possible component word translations.
such gloss-to-translation guidance from other lan-
guages. The rest of the sections detail our investi-
gation of improvements to this model.
Language Compound words Top1 Top10
translated Acc. Acc.
Albanian 3085 (6.97%) 0.185 0.332
Bulgarian 6719 (9.24%) 0.247 0.416
German 11103 (20.55%) 0.195 0.362
Swedish 12681 (21.86%) 0.188 0.346
Avg10 9320.9 (11.98%) 0.184 0.326
Table 3: Coverage and accuracy for the standard model us-
ing gloss-to-fluent translation mappings learned from bilingual
dictionaries in other languages (in forward order only).
6.3 Using forward and backward ordering for
English gloss search
In our standard model, the literal English gloss for
a source compound word (for example, iron path)
matches glosses in other language dictionaries only
in the identical order. But given that modifier/head
word order often differs between languages, we
test how searching for both orderings (e.g. ?iron
path? and ?path iron?) can improve performance,
as shown in Table 4. The percentages in parentheses
show relative increase from the performance of the
standard model in Section 6.2. We see a substantial
improvement in both coverage and accuracy.
Language Cmpnd wrds Top1 Top10
translated Acc. Acc.
Albanian 3229(+4.67%) .217(+17.30%) .409(+23.19%)
Bulgarian 6806(+1.29%) .255(+3.24%) .442(+6.25%)
German 11346(+2.19%) .199(+2.05%) .388(+7.18%)
Swedish 12970(+2.28%) .189(+0.53%) .361(+4.34%)
Avg10 9603(+3.03%) .193(+4.89%) .362(+11.04%)
Table 4: Performance for looking up English gloss via both
orderings. The percentages in parentheses are relative improve-
ments from the performance in Table 3
.
6.4 Increasing coverage by automatically
discovering compound morphology
For many languages, the compounding process in-
troduces its own morphology (Figure 2). For ex-
ample, in German, the word Gescha?ftsfu?hrer (man-
ager) consists of the lexemes Gescha?ft (business)
and Fu?hrer (guide) joined by the lexeme -s. For the
purposes of these experiments, we will call such lex-
emes fillers or middle glue characters. Koehn and
Knight (2003) used a fixed set of two known fillers s
and es for handling German compounds. To broaden
the applicability of this work to new languages with-
out linguistic guidance, we show how such fillers
406
Gesch?ft s F?hrer
Paterfamilias
Pater Familia s+ + + +
s as Middle Glue
     in German
s as End Glue
      in Latin
Gesch?ftsf?hrer
(Business) (Guide) (Father) (Family)
(Manager) (Household head)
Figure 2: Illustration of compounding morphology using
middle and end glue characters.
can be estimated directly from corpora in different
languages. In additional to fillers, compound can
also introduce morphology at the suffix or prefix of
compounds, for example, in the Latin language, the
lexeme paterfamilias contains the genitive form fa-
milias of the lexeme familia (family), thus s in this
case is referred to as the ?end glue? character. To
Albanian Bulgarian German Swedish
Top 5 Middle Glue Character(s)
j 0.059 O 0.129 s 0.133 s 0.132
s 0.048 N 0.046 n 0.090 l 0.051
t 0.042 H 0.036 k 0.066 n 0.049
r 0.042 E 0.025 h 0.042 t 0.045
i 0.038 A 0.025 f 0.037 r 0.035
Top 5 End Glue Character(s)
m 0.146 T 0.124 n 0.188 a 0.074
t 0.079 EH 0.092 t 0.167 g 0.073
s 0.059 H 0.063 en 0.130 t 0.059
k 0.048 M 0.049 e 0.069 e 0.057
r 0.037 AM 0.047 d 0.043 d 0.057
Table 5: Top 5 middle glues (fillers) and end glues discovered
for each language along with their probability scores.
augment the splitting step outlined in Section 4.1,
we allow deletion of up to two middle characters
and two end characters. Then, for each glue candi-
date (for example es), we estimate its probability as
the relative frequency of unique hypothesized com-
pound words successfully using that particular glue.
We rank the set of glues by their probability and take
the top 10 middle and end glues for each language.
A sample of glues discovered for some of the lan-
guages are shown in Table 5. The performance for
the morphology step is shown in Table 6. The rela-
tive percentage improvements are with respect to the
previous Section 6.3. We observe significant gain in
coverage as the flexibility of glue process allows dis-
covery of more compounds.
6.5 Re-ranking using context vector projection
We may further improve performance by re-ranking
candidate translations based on the goodness of se-
mantic ?fit? between two words, as measured by
Language Cmpnd wrds Top1 Top10
translated Acc. Acc.
Albanian 3272(+1.33%) .214(-1.38%) .407(-0.49%)
Bulgarian 7211(+5.95%) .258(+1.18%) .443(+0.23%)
German 13372(+17.86%) .200(+0.50%) .391(+0.77%)
Swedish 15094(+16.38%) .190(+0.53%) .363(+0.55%)
Avg10 10273(+6.98%) .194(+0.52%) .363(+0.28%)
Table 6: Performance for increasing coverage by including
compounding morphology. The percentages in parentheses are
relative improvements from the performance in Table 4
.
their context similarity. This can be accomplished as
in Rapp (1999) and Schafer and Yarowsky (2002) by
creating bag-of-words context vectors around both
the source and target language words and then pro-
jecting the source vectors into the (English) target
space via the current small translation dictionary.
Once in the same language space, source words and
their translation hypotheses are compared via co-
sine similarity using their surrounding context vec-
tors. We performed this experiment for German
and Swedish and report average accuracies with and
without this addition in Table 7. For monolingual
corpora, we used the German and Swedish side of
the Europarl corpus (Koehn, 2005) consisting of ap-
proximately 15 million and 21 million words respec-
tively. We were able to project context vectors for
an average of 4224.5 words in the two languages
among all the possible compound words detected in
Section 6.4. The poor Eurpoarl coverage could be
due to the fact that compound words are generally
technical words with low Europarl corpus frequency,
especially in parliamentary proceedings. We believe
that the small performance gains here are due to
these limitations of the monolingual corpora.
Method Top1avg Top10avg
Original ranking 0.196 0.388
Comb. with Context Sim 0.201 0.391
Table 7: Average performance on German and Swedish with
and without using context vector similarity from monolingual
corpora.
6.6 Using phrase-tables if a parallel corpus is
available
All previous results in this paper have been for trans-
lation lexicon discovery without the need for paral-
lel bilingual text (bitext), which is often in limited
supply for lower-resource languages. However, it
is useful to assess how this translation lexicon dis-
407
covery work compares with traditional bitext-based
lexicon induction (and how well the approaches can
be combined). For this purpose, we used phrase ta-
bles learned by the standard statistical MT Toolkit
Moses (Koehn et al, 2007). We tested the phrase-
table accuracy on two languages, one for which we
had a lot of parallel data available (German-English
Europarl corpus with approx. 15 million words) and
one for which we had relatively little parallel data
(Czech-English news-commentary corpus with ap-
prox. 1 million words). This was done to see how
the amount of parallel data available affects the ac-
curacy and coverage of compound translation. Table
8 shows the performance for this experiment. For
German, we see a significant improvement in accu-
racy and for Czech a small improvement in Top1 but
a decline in Top10 accuracy. Note that these ac-
curacies are still quite low as compared to general
performance of phrase tables in an end-to-end MT
system because we are measuring exact-match ac-
curacy on a generally more challenging and often-
lower-frequency lexicon subset. The third row in
Table 8 for each of the languages shows that if one
had a parallel corpus available, its n-best list can be
combined with the n-best list of Bilingual Dictio-
naries algorithm to provide much higher consensus
accuracy gains using weighted voting.
Method # of words Top1 Top10
translated Acc. Acc.
German
BiDict 13372 0.200 0.391
Parallel Corpus SMT 3281 0.423 0.576
Parallel + BiDict 3281 0.452 0.579
Czech
BiDictthresh=1 3455 0.276 0.514
Parallel Corpus SMT 309 0.285 0.404
Parallel + BiDict 309 0.359 0.599
Table 8: Performance of this paper?s BiDict approach com-
pared with and augmented with traditional statistical MT learn-
ing from bitext.
7 Quantifying the Role of Cross-languages
7.1 Coverage/Accuracy Trade off
The number of languages offering a translation hy-
pothesis for a given literal English gloss is a use-
ful parameter for measuring confidence in the algo-
rithm?s selection. The more distinct languages ex-
hibiting a translation for the gloss, the higher like-
lihood that the majority translation will be correct
Coverage/Accuracy Tradeoff
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0 200 400 600 800 1000 1200 1400 1600
# of words translated as compounds
E
x
a
c
t
 
m
a
t
c
h
 
a
c
c
u
r
a
c
y
Avg Top 1 Acc.
Avg Top 10 Acc.
>= 8
>= 5
>= 4
>= 3
>= 6
>= 14
>= x: threshold for 
# of languages
Figure 3: Coverage/Accuracy trade off curve by incrementing
the minimum number of languages exhibiting a candidate trans-
lation for the source-word?s literal English gloss. Accuracy here
is the Top1 accuracy averaged over all 10 test languages.
rather than noise. Varying this parameter yields the
coverage/accuracy trade off as shown in Figure 3.
7.2 Varying size of bilingual dictionaries
Figure 4 illustrates how the size of the bilingual
dictionaries used for providing cross-language evi-
dence affects translation performance. In order to
take both coverage and accuracy into account, per-
formance measure used was the F-score which is
a harmonic average of Precision (the accuracy on
the subset of words that could be translated) and
Psuedo-recall (which is the correctly translated frac-
tion out of total words that could be translated using
100% of the dictionary size). We can see in Figure 4
that increasing the percentage of dictionary size7 al-
ways helps without plateauing, suggesting substan-
tial extrapolation potential from large dictionaries.
7.3 Greedy vs Random Selection of Utilized
Languages
A natural question for our compound translation al-
gorithm is how does the choice of additional lan-
guages affect performance. We report two experi-
ments on this question. A simple experiment is to
use bilingual dictionaries of randomly selected lan-
guages and test the performance of K-randomly se-
lected languages8, incrementing K until it is the full
set of 50 languages. The dashed lines in Figures 5
7Each run of choosing a percentage of dictionary size was
averaged over 10 runs
8Each run of randomly selecting K languages was averaged
over 10 runs.
408
00.050.1
0.150.20.25
0.30.35
0 10 20 30 40 50 60 70 80 90 100% of dictionary used
F- score Top 1Top 10
Figure 4: F-measure performance given varying sizes of the
bilingual dictionaries used for cross-language evidence (as a
percentage of words randomly utilized from each dictionary).
00.020.040.060.08
0.10.120.140.160.18
0.2
0 10 20 30 40 50# of languages utilized (K)F-
score (Top 1) K-RandomK-Greedy
Figure 5: Top-1 match F-score performance utilizing K lan-
guages for cross-language evidence, for both a random K lan-
guages and greedy selection of the most effective K languages
(typically the closest or largest dictionaries)
00.050.10.15
0.20.250.30.35
0 10 20 30 40 50# of languages utilized (K)F-s
core (Top 10) K-RandomK-Greedy
Figure 6: The performance relationship detailed in Figure 5
caption for Top-10 match F-score.
and 6 show this trend. The performance is measured
by F-score as in section 7.1, where Pseudo-Recall
here is the fraction of correct candidates out of the
total candidates that could be translated had we used
bilingual dictionaries of all the languages. We can
see that adding random bilingual dictionaries helps
improve the performance in a close to linear fashion.
Furthermore, we observe that certain contributing
languages are much more effective than others (e.g.
Arabic/Farsi vs. Arabic/Czech). We use a greedy
heuristic for ranking an additional cross-language,
that is the number of test words for which the correct
English translation can be provided by the bilingual
dictionary of the respective cross-language. Figures
5 and 6 show that greedy selection of the most ef-
fective K utilized languages using this heuristic sub-
stantially accelerates performance. In fact, beyond
the best 10 languages, performance plateaus and ac-
tually decreases slightly, indicating that increased
noise is outweighing increased coverage.
Albanian Arabic
Russian 0.067 0.116 Farsi 0.051 0.090
+Spanish 0.100 0.169 +Spanish 0.059 0.111
+Bulgarian 0.119 0.201 +French 0.077 0.138
Bulgarian Czech
Russian 0.186 0.294 Slovak 0.177 0.289
+Hungarian 0.190 0.319 +Russian 0.222 0.368
+Swedish 0.203 0.339 +Hungarian 0.235 0.407
Farsi German
Arabic 0.031 0.047 Dutch 0.130 0.228
+Dutch 0.038 0.070 +Swedish 0.191 0.316
+Spanish 0.044 0.079 +Hungarian 0.204 0.355
Hungarian Russian
Swedish 0.073 0.108 Bulgarian 0.185 0.250
+Dutch 0.103 0.158 +Hungarian 0.199 0.292
+German 0.117 0.182 +Swedish 0.216 0.319
Slovak Swedish
Czech 0.145 0.218 German 0.120 0.188
+Russian 0.168 0.280 +Hungarian 0.152 0.264
+Hungarian 0.176 0.300 +Dutch 0.182 0.309
Table 9: Illustrating 3-best cross-languages obtained for each
test language (shown in bold). Each row shows the effect of
adding the respective cross-language to the set of languages in
the rows above it and the corresponding F-scores (Top 1 and
Top 10) achieved.
7.4 Languages found using Greedy selection
Table 9 shows the sets of the most effective three
cross-languages per test language selected using the
greedy heuristic explained in previous section. Un-
surprisingly, related languages tend to help more
than distant languages. For example, Dutch is most
409
effective for the test language German, and Slovak is
most effective for Czech. We can also see interest-
ing symmetries between related languages, for ex-
ample: Farsi is the top language used for test lan-
guage Arabic and vice-versa. Such symmetries can
also be seen for other pairs of related languages such
as (Czech, Slovak) and (Russian, Bulgarian). Thus,
related languages are most helpful and they can be
related in several ways such as etymologically, cul-
turally and physically (such as Hungarian contact
with the Germanic languages). The second point
to note is that languages having large dictionaries
also tend to be especially helpful, even when un-
related. This can be seen by the presence of Hun-
garian in top three cross-languages for most of the
test languages. This is likely because Hungarian was
one of the largest dictionaries and hence can provide
good coverage for obtaining translation candidates
of rarer or technical compounds, which may have
more language universal literal glosses.
8 Conclusion
This paper has shown that successful translation
of compounds can be achieved without the need
for bilingual training text, by modeling the map-
ping of literal component-word glosses (e.g. ?iron-
path?) into fluent English (e.g. ?railway?) across
multiple languages. An interesting property of us-
ing such cross-language evidence is that one does
need to restrict the candidate translations to compo-
sitional (or ?glossy?) translations, as our model al-
lows the successful generation of more fluent non-
compositional translations. We further show im-
proved performance by adding component-sequence
and learned-morphology models along with context
similarity from monolingual text and optional com-
bination with traditional bilingual-text-based trans-
lation discovery. These models show consistent per-
formance gains across 10 diverse test languages.
9 Acknowledgments
We thank Chris Callison-Burch for providing access
to phrase tables and giving valuable comments on
this work as well as suggesting useful additional ex-
periments. We also thankMarkus Dreyer for helping
with German examples and David Smith for giving
valuable comments on initial version of the paper.
References
T. Baldwin and T. Tanaka. 2004. Translation by Machine
of Complex Nominals: Getting it Right. Proceedings
of the ACL-2004 Workshop on Multiword Expressions,
pages 24?31.
R.D. Brown. 2002. Corpus-driven splitting of compound
words. Proceedings of TMI-2002.
Y. Cao and H. Li. 2002. Base Noun Phrase translation
using web data and the EM algorithm. Proceedings of
COLING-Volume 1, pages 1?7.
G. Grefenstette. 1999. The World Wide Web as a Re-
source for Example-Based Machine Translation Tasks.
In ASLIB?99 Translating and the Computer 21.
P. Koehn and K. Knight. 2003. Empirical methods for
compound splitting. Proceedings of the EACL-Volume
1, pages 187?193.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, et al 2007. Moses: Open source
toolkit for statistical machine translation. Proceedings
of ACL, companion volume, pages 177?180.
P. Koehn. 2005. Europarl: A parallel corpus for statisti-
cal machine translation. MT Summit X.
J.N. Levi. 1978. The Syntax and Semantics of Complex
Nominals.
R. Navigli, P. Velardi, and A. Gangemi. 2003. Ontology
learning and its application to automated terminology
translation. Intelligent Systems, IEEE, 18(1):22?31.
U. Rackow, I. Dagan, and U. Schwall. 1992. Auto-
matic translation of noun compounds. Proceedings of
COLING-Volume 4, pages 1249?1253.
R. Rapp. 1999. Automatic identification of word trans-
lations from unrelated English and German corpora.
Proceedings of ACL, pages 519?526.
C. Schafer and D. Yarowsky. 2002. Inducing translation
lexicons via diverse similarity measures and bridge
languages. Proceedings of COLING, pages 1?7.
C. Schafer and D. Yarowsky. 2004. Exploiting aggregate
properties of bilingual dictionaries for distinguishing
senses of English words and inducing English sense
clusters. Proceedings of ACL-2004, pages 118?121.
T. Tanaka and T. Baldwin. 2003. Noun-Noun Compound
Machine Translation: A Feasibility Study on Shallow
Processing. Proceedings of the ACL-2003 Workshop
on Multiword Expressions, pages 17?24.
J. Zhang, J. Gao, and M. Zhou. 2000. Extraction of
Chinese compound words: an experimental study on
a very large corpus. Proceedings of the Second Work-
shop on Chinese Language Processing, pages 132?
139.
410
Minimally Supervised Multilingual Taxonomy and
Translation Lexicon Induction
Nikesh Garera and David Yarowsky
Department of Computer Science
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218, USA
{ngarera,yarowsky}@cs.jhu.edu
Abstract
We present a novel algorithm for the acqui-
sition of multilingual lexical taxonomies (in-
cluding hyponymy/hypernymy, meronymy
and taxonomic cousinhood), from monolin-
gual corpora with minimal supervision in the
form of seed exemplars using discriminative
learning across the major WordNet seman-
tic relationships. This capability is also ex-
tended robustly and effectively to a second
language (Hindi) via cross-language projec-
tion of the various seed exemplars. We also
present a novel model of translation dic-
tionary induction via multilingual transitive
models of hypernymy and hyponymy, us-
ing these induced taxonomies. Candidate
lexical translation probabilities are based on
the probability that their induced hyponyms
and/or hypernyms are translations of one an-
other. We evaluate all of the above models
on English and Hindi.
1 Introduction
Taxonomy resources such as WordNet are limited
or non-existent for most of the world?s languages.
Building a WordNet manually from scratch requires
a huge amount of human effort and for rare lan-
guages the required human and linguistic resources
may simply not be available. Most of the automatic
approaches for extracting semantic relations (such as
hyponyms) have been demonstrated for English and
some of them rely on various language-specific re-
sources (such as supervised training data, language-
specific lexicosyntactic patterns, shallow parsers,
(grenade)
haathagolaa (explosive)
baaruuda
(bomb)
bama
(gun)
banduuka
explosivegrenade bomb gun
weapon
Induced Hindi Hypernymy (with glosses)
Induced English Hypernymy
hathiyaara
(weapon)
Figure 1: Goal: To induce multilingual taxonomy relation-
ships in parallel in multiple languages (such as Hindi and En-
glish) for information extraction and machine translation pur-
poses.
etc.). This paper presents a language independent
approach for inducing taxonomies such as shown
in Figure 1 using limited supervision and linguis-
tic resources. We propose a seed learning based ap-
proach for extracting semantic relations (hyponyms,
meronyms and cousins) that improves upon existing
induction frameworks by combining evidence from
multiple semantic relation types. We show that us-
ing a joint model for extracting different semantic
relations helps to induce more relation-specific pat-
terns and filter out the generic patterns1 . The pat-
1By generic patterns, we mean patterns that cannot distin-
guish between different semantic relations. For example, the
465
terns can then be used for extracting new wordpairs
expressing the relation. Note that the only training
data used in the algorithm are the few seed pairs re-
quired to start the bootstrapping process, which are
relatively easy to obtain. We evaluate the taxonomy
induction algorithm on English and a second lan-
guage (Hindi) and show that it can reliably and accu-
rately induce taxonomies in two diverse languages.
We further show how having induced parallel tax-
onomies in two languages can be used for augment-
ing a translation dictionary between those two lan-
guages. We make use of the automatically induced
hyponym/hypernym relations in each language to
create a transitive ?bridge? for dictionary induction.
Specifically, the dictionary induction task relies on
the key observation that words in two languages (e.g.
English and Hindi) have increased probabilities of
being translations of each other if their hypernyms
or hyponyms are translations of one another.
2 Related Work
While manually created WordNets for English (Fell-
baum, 1998) and Hindi (Narayan, 2002) have been
made available, a lot of time and effort is required
in building such semantic taxonomies from scratch.
Hence several automatic corpus based approaches
for acquiring lexical knowledge have been proposed
in the literature. Much of this work has been done
for English based on using a few evocative fixed
patterns including ?X and other Ys?, ?Y such as
X?, as in the classic work by Hearst (1992). The
problems with using a few fixed patterns is the of-
ten low coverage of such patterns; thus there is a
need for discovering additional informative patterns
automatically. There has been a plethora of work
in the area of information extraction using automat-
ically derived patterns contextual patterns for se-
mantic categories (e.g. companies, locations, time,
person-names, etc.) based on bootstrapping from
a small set of seed words (Riloff and Jones, 1999;
Agichtein and Gravano, 2000; Thelen and Riloff,
2002; Ravichandran and Hovy, 2002; Hasegawa et
al. 2004; Etzioni et al 2005; Pas?ca et al 2006).
This framework has been also shown to work for ex-
tracting semantic relations between entities: Pantel
et al (2004) proposed an approach based on edit-
pattern ?X and Y? is a generic pattern whereas the pattern ?Y
such as X? is a hyponym-specific pattern
distance to learn lexico-POS patterns for is-a and
part-of relations. Girju et al (2003) used 100 seed
words from WordNet to extract patterns for part-of
relations. While most of the above pattern induction
work has been shown to work well for specific rela-
tions (such as ?birthdates, companies, etc.?), Section
3.1 explains why directly applying seed learning for
semantic relations can result in high recall but low
precision patterns, a problem also noted by Pantel
and Pennacchiotti (2006). Furthermore, much of
the semantic relation extraction work has focused
on extracting a particular relation independently of
other relations. We show how this problem can be
solved by combining evidence from multiple rela-
tions in Section 3.2. Snow et al(2006) also de-
scribe a probablistic framework for combining ev-
idence using constraints from hyponymy and cousin
relations. However, they use a supervised logistic
regression model. Moreover, their features rely on
parsing dependency trees which may not be avail-
able for most languages.
The key contribution of this work is using evidence
from multiple relationship types in the seed learning
framework for inducing these relationships and con-
ducting a multilingual evaluation for the same. We
further show how extraction of semantic relations in
multiple languages can be applied to the task of im-
proving a dictionary between those languages.
3 Approach
To be able to automatically create taxonomies such
as WordNet, it is useful to be able to learn not only
hyponymy/hyponymy directly, but also the addi-
tional semantic relationships of meronymy and tax-
onomic cousinhood. Specifically, given a pair of
words (X, Y), the task is to answer the following
questions: 1. Is X a hyponym of Y (e.g. weapon,
gun)? 2. Is X a part/member of Y (e.g. trigger, gun)?
3. Is X a cousin/sibling2 of Y (e.g. gun, missile)? 4.
Do none of the above 3 relations apply but X is ob-
served in the context of Y (e.g. airplane,accident)?3
We will refer to class 4 as ?other?.
2Cousins/siblings are words that share a close common hy-
pernym
3Note that this does not imply X is unrelated or indepen-
dent of Y. On the contrary, the required sentential co-occurence
implies a topic similarity. Thus, this is a much harder class to
distinguish from classes 1-3 than non co-occuring unrelatedness
(such as gun, protazoa) and hence was included in the evalua-
tion.
466
Rank English Hindi
1 Y, the X Y aura X
(Gloss: Y and X)
2 Y and X Y va X
(Gloss: Y in addition to X)
3 X and other Y Y ne X
(Gloss: Y (case marker) X)
4 X and Y X ke Y
(Gloss: X?s Y)
5 Y, X Y me.n X
(Gloss: Y in X)
Table 1: Naive pattern scoring: Hyponymy patterns ranked by
their raw corpus frequency scores.
3.1 Independently Bootstrapping Lexical
Relationship Models
Following the pattern induction framework of
Ravichandran and Hovy (2002), one of the ways
of extracting different semantic relations is to learn
patterns for each relation independently using seeds
of that relation and extract new pairs using the
learned patterns. For example, to build an inde-
pendent model of hyponymy using this framework,
we collected approximately 50 seed exemplars of
hyponym pairs and extracted all the patterns that
match with the seed pairs4. As in Ravichandran
and Hovy (2002), the patterns were ranked by cor-
pus frequency and a frequency threshold was set to
select the final patterns. These patterns were then
used to extract new word pairs expressing the hy-
ponymy relation by finding word pairs that occur
with these patterns in an unlabeled corpus. How-
ever, the problem with this approach is that generic
patterns (like ?X and Y?) occur many times in a
corpus and thus low-precision patterns may end up
with high cumulative scores. This problem is illus-
trated more clearly in Table 1, which shows a list
of top five hyponymy patterns (ranked by their cor-
pus frequency) using this approach. We overcome
this problem by exploiting the multi-class nature of
our task and combine evidence from multiple rela-
tions in order to learn high precision patterns (with
high conditional probabilities) for each relation. The
key idea is to weed out the patterns that occur in
4A pattern is the ngrams occurring between the seedpair
(also called gluetext). The length of the pattern was thresholded
to 15 words.
Rank English Hindi
1 Y like X X aura anya Y
(Gloss: X and other Y)
2 Y such as X Y, X
(Gloss: Y, X)
3 X and other Y X jaise Y
(Gloss: X like Y)
4 Y and X Y tathaa X
(Gloss: Y or X)
5 Y, including X X va anya Y
(Gloss: X and other Y)
Table 2: Patterns for hypernymy class reranked using ev-
idence from other classes. Patterns distributed fairly evenly
across multiple relationship types (e.g. ?X and Y?) are dep-
recated more than patterns focused predominantly on a single
relationship type (e.g. ?Y such as X?).
more than one semantic relation and keep the ones
that are relation-specific5 , thus using the relations
meronymy, cousins and other as negative evidence
for hyponymy and vice versa. Table 2 shows the pat-
tern ranking by using the model developed in Sec-
tion 3.2 that makes use of evidence from different
classes. We can see more hyponymy specific pat-
terns ranked at the top6 suggesting the usefulness of
this method in finding class-specific patterns.
3.2 A minimally supervised multi-class
classifier for identifying different semantic
relations
First, we extract a list of patterns from an unla-
beled corpus7 independently for each relationship
type (class) using the seeds8 for the respective class
as in Section 3.1.9 In order to develop a multi-
5In the actual algorithm, we will not be entirely weeding
out the common patterns but will estimate the conditional class
probabilities for each pattern: p(class|pattern)
6It is interesting to see in Table 2 that the top learned Hindi
hyponymy patterns seem to be translations of the English pat-
terns suggested by Hearst (1992). This leads to an interesting
future work question: Are the most effective hyponym patterns
in other languages usually translations of the English hyponym
patterns proposed by Hearst (1992) and what are frequent ex-
ceptions?
7Unlabeled monolingual corpora were used for this task, the
English corpus was the LDC Gigaword corpus and the Hindi
corpus was newswire text extracted from the web containing a
total of 64 million words.
8The number of seeds used for classes {hyponym,
meronym, cousin, other} were {48,40,49,50} for English and
were {32,58,31,35} for Hindi respectively. A sample of seeds
used is shown in Table 5.
9We retained only the patterns that had seed frequency
greater one for extracting new word pairs. The total number
467
Hypo. Mero. Cous. Other
X of the Y 0 0.66 0.04 0.3
Y, especially X 1 0 0 0
Y, whose X 0 1 0 0
X and other Y 0.63 0.08 0.18 0.11
X and Y 0.23 0.3 0.33 0.14
Table 3: A sample of patterns and their relationship type
probabilities P (class|pattern) extracted at the end of training
phase for English.
Hypo. Mero. Cous. Other
X aura anya Y 1 0 0 0
(X and other Y)
X aura Y 0.09 0.09 0.71 0.11
(X and Y)
X jaise Y 1 0 0 0
(X like Y)
X va Y 0.11 0 0.89 0
(X and Y)
Y kii X 0.33 0.67 0 0
(Y?s X)
Table 4: A sample of patterns and their class probabilities
P (class|pattern) extracted at the end of training phase for
Hindi.
class probabilistic model, we obtain the probability
of each class c given the pattern p as follows:
P (c|p) = seedfreq(p,c)?
c? seedfreq(p,c
?
)
where seedfreq(p, c) is the number of seeds of class
c that were found with the pattern p in an unlabeled
corpus. A sample of the P (class|pattern) tables
for English and Hindi are shown in the Tables 3 and
4 respectively. It is clear how occurrence of a pattern
in multiple classes can be used for finding reliable
patterns for a particular class. For example, in Table
3: although the pattern ?X and Y? will get a higher
seed frequency than the pattern ?Y, especially X?,
the probability P (?X and Y ??|hyponymy) is much
lower than P (?Y, especially X ??|hyponymy),
since the pattern ?Y, especially X? is unlikely to oc-
cur with seeds of other relations.
Now, instead of using the seedfreq(p, c) as the
score for a particular pattern with respect to a
class, we can rescore patterns using the probabilities
P (class|pattern). Thus the final score for a pattern
of retained patterns across all classes for {English,Hindi} were
{455,117} respectively.
p with respect to class c is obtained as:
score(p, c) = seedfreq(p, c) ? P (c|p)
We can view this equation as balancing recall and
precision, where the first term is the frequency of
the pattern with respect to seeds of class c (repre-
senting recall), and the second term represents the
relation-specificness of the pattern with respect to
class c (representing precision). We recomputed the
score for each pattern in the above manner and ob-
tain a ranked list of patterns for each of the classes
for English and Hindi. Now, to extract new pairs
for each class, we take all the patterns with a seed
frequency greater than 2 and use them to extract
word pairs from an unlabeled corpus. The semantic
class for each extracted pair is then predicted using
the multi-class classifier as follows: Given a pair of
words (X1, X2), note all the patterns that matched
with this pair in the unlabeled corpus, denote this set
as P. Choose the predicted class c? for this pair as:
c? = argmaxc
?
p?P score(p, c)
3.3 Evaluation of the Classification Task
Over 10,000 new word relationship pairs were ex-
tracted based on the above algorithm. While it is
hard to evaluate all the extracted pairs manually, one
can certainly create a representative smaller test set
and evaluate performance on that set. The test set
was created by randomly identifying word pairs in
WordNet and newswire corpora and annotating their
correct semantic class relationships. Test set con-
struction was done entirely independently from the
algorithm application, and hence some of the test
pairs were missed entirely by the learning algorithm,
yielding only partial coverage.
The total number of test examples including all
classes were 200 and 140 for English and Hindi test-
sets respectively. The overall coverage10 on these
test-sets was 81% and 79% for English and Hindi
respectively. Table 6 reports the overall accuracy11
for the 4-way classification using different patterns
scoring methods. Baseline 1 is scoring patterns by
their corpus frequency as in Ravichandran and Hovy
(2002), Baseline 2 is another intutive method of
10Coverage is defined as the percentage of the test cases that
were present in the unlabeled corpus, that is, cases for which an
answer was given.
11Accuracy on a particular set of pairs is defined as the per-
centage of pairs in that set whose class was correctly predicted.
468
English Hindi
Seed Pairs Model Predictions Seed Pairs Model Predictions
tool,hammer gun,weapon khela,Tenisa kaa.ngresa,paarTii
(game,tennis) (congress,party)
Hypernym currency,yen hockey,sport appraadha,hatyaa passporTa,kaagajaata
(crime,murder) (passport,document)
metal,copper cancer,disease jaanvara,bhaaga a.ngrejii,bhaashhaa
(animal,tiger) (English,language)
wheel,truck room,hotel u.ngalii,haatha jeba,sharTa
(finger,hand) (pocket,shirt)
Meronym headline,newspaper bark,tree kamaraa,aspataala kaptaana,Tiima
(room,hospital) (captain,team)
wing,bird lens,camera ma.njila,imaarata darvaaja,makaana
(floor,building) (door,house)
dollar,euro guitar,drum bhaajapa,kaa.ngresa peTrola,Diijala
(bjp,congress) (petrol,diesel)
Cousin heroin,cocaine history, geography Hindii,a.ngrejii Daalara,rupayaa
(Hindi,English) (dollar,rupee)
helicopter,submarine diabetes,arthritis basa,Traka talaaba,nadii
(bus,truck) (pond,river)
Table 5: A sample of seeds used and model predictions for each class for the taxonomy induction task. For each of the model
predictions shown above, its Hyponym/Meronym/Cousin classification was correctly assigned by the model.
scoring patterns by the number of seeds they ex-
tract. The third row in Table 6 indicates the result
of rescoring patterns by their class conditional prob-
abilties, giving the best accuracy.
While this method yields some improvement over
other baselines, the main point to note here is that
the pattern-based methods which have been shown
to work well for English also perform reasonably
well on Hindi, inspite of the fact that the size of the
unlabeled corpus available for Hindi was 15 times
smaller than for English.
Table 7 shows detailed accuracy results for each re-
lationship type using the model developed in sec-
tion 3.2. It is also interesting to see in Table 8 that
most of the confusion is due to ?other? class being
classified as ?cousin? which is expected as cousin
words are only weakly semantically related and uses
more generic patterns such as ?X and Y? which can
often be associated with the ?other? class as well.
Strongly semantically clear classes like Hypernymy
and Meronymy seem to be well discriminated as
their induced patterns are less likely to occur in other
relationship types.
Model English Hindi
Accuracy Accuracy
Baseline 1
[RH02] 65% 63%
Baseline 2 seedfreq 70% 65%
seedfreq ? P (c|p) 73% 66%
Table 6: Overall accuracy for 4-way classification
{hypernym,meronym,cousin,other} using different pattern
scoring methods.
English Hindi
Total Cover. Acc. Total Cover. Acc.
Hypr. 83 74% 97% 59 82% 75%
Mero. 41 81% 88% 33 63% 81%
Cous. 42 91% 55% 23 91% 71%
Other 34 85% 31% 25 80% 20%
Overall 200 81% 73% 140 79% 66%
Table 7: Test set coverage and accuracy results for inducing
different semantic relationship types.
English Hindi
Hypo. Mero. Cous. Oth. Hypo. Mero. Cous. Oth.
Hypo. 59 1 1 0 36 1 10 1
Mero. 1 28 1 3 0 17 4 0
Cous. 14 3 21 0 6 0 15 0
Other 7 3 10 9 1 4 11 4
Table 8: Confusion matrix for English (left) Hindi (right) for
the four-way classification task
469
baaruuda
hathiyaara
bama
[via induced
hypernymy]
bomb explosive grenadegun
weapon
banduuka
hyponymy]
[via induced
Goal: To learn this translation
haathagolaa
[via existing dictionary entries or previous induced translations]
EnglishHindi
Figure 2: Illustration of the models of using induced hyponymy and hypernymy for translation lexicon induction.
4 Improving a partial translation
dictionary
In this section, we explore the application of
automatically generated multilingual taxonomies
to the task of translation dictionary induction. The
hypothesis is that a pair of words in two languages
would have increased probability of being transla-
tions of each other if their hypernyms or hyponyms
are translations of one another.
As illustrated in Figure 2, the probability that
weapon is a translation of the Hindi word hathiyaara
can be decomposed into the sum of the probabilities
that their hyponyms in both languages (as induced
in Section 3.2) are translations of each other. Thus:
PH?>E (WE |WH) =
?
i Phyper (WE |Eng(Hi)) Phypo(Hi|WH)
for induced hyponyms Hi of the source word
WH , and using an existing (and likely very incom-
plete) Hindi-English dictionary to generate Eng(Hi)
for these hyponyms, and the corresponding induced
hypernyms of these translations in English.12. We
conducted a very preliminary evaluation of this idea
for obtaining English translations of a set of 25
12One of the challenges of inducing a dictionary via using a
corpus based taxonomy is sense disambiguation of the words to
be translated. In the current model, the more dominant sense
(in terms of corpus frequency of its hyponyms) is likely to get
selected by this approach. While the current model can still
help in getting translations of the dominant sense, possible fu-
ture work would be to cluster all the hyponyms according to
contextual features such that each cluster can represent the hy-
ponyms for a particular sense. The current dictionary induction
model can then be applied again using the hyponym clusters to
distinguish different senses for translation.
Hindi words. The Hindi candidate hyponym space
had been pruned of function words and non-noun
words. The likely English translation candidates
for each Hindi word were ranked according to the
probability PH?>E(WE|WH).
The first column of Table 9 shows the stand-alone
performance for this model on the dictionary induc-
tion task. This standalone model has a reasonably
good accuracy for finding the correct translation in
the Top 10 and Top 20 English candidates.
Accuracy Accuracy Accuracy
(uni-d) (bi-d) bi-d + Other
Top 1 20% 36% 36%
Top 5 56% 64% 72%
Top 10 72% 72% 80%
Top 20 84% 84% 84%
Table 9: Accuracy on Hindi to English word translation using
different transitive hypernym algorithms. The additional model
components in the bi-d(irectional) plus Other model are only
used to rerank the top 20 candidates of the bidirectional model,
and are hence limited to its top-20 performance.
This approach can be further improved by also im-
plementing the above model in the reverse direction
and computing the P (WH |WEi) for each of the
English candidates Ei. We did so and computed
P (WH |WEi) for top 20 English candidate trans-
lations. The final score for an English candidate
translation given a Hindi word was combined by
a simple average of the two directions, that is, by
summing P (WEi |WH) + P (WH |WEi).
The second column of Table 9 shows how this
bidirectional approach helps in getting the right
470
translations in Top 1 and Top 5 as compared to the
unidirectional approach. Table 10 shows a sample
Correctly translated Incorrectly translated
aujaara vishaya
(tool) (topic)
biimaarii saamana
(disease) (stuff)
hathiyaara dala
(weapon) (group,union)
dastaaveja tyohaara
(documents) (festival)
aparaadha jagaha
(crime) (position,location)
Table 10: A sample of correct and incorrect translations using
transitive hypernymy/hyponym word translation induction
of correct and incorrect translations generated
by the above model. It is interesting to see that
the incorrect translations seem to be the words
that are very general (like ?topic?, ?stuff?, etc.)
and hence their hyponym space is very large and
diffuse, resulting in incorrect translations.While the
columns 1 and 2 of Table 9 show the standalone
application of our translation dictionary induction
method, we can also combine our model with
existing work on dictionary induction using other
translation induction measures such as using relative
frequency similarity in multilingual corpora and
using cross-language context similarity between
word co-occurrence vectors (Schafer and Yarowsky,
2002).We implemented the above dictionary induc-
tion measures and combined the taxonomy based
dictionary induction model with other measures by
just summing the two scores13. The preliminary
results for bidirectional hypernym/hyponym +
other features are shown in column 3 of Table
9. The results show that the hypernym/hyponym
features can be a useful orthogonal source of lexical
similarity in the translation-induction model space.
While the model shown in Figure 2 proposes
inducing translations of hypernyms, one can also go
in the other direction and induce likely translation
candidates for hyponyms by knowing the translation
of hypernyms. For example, to learn that rifle is
a likely translation candidate of the Hindi word
13after renormalizing each of the individual score to be in the
range 0 to 1.
raaiphala, is illustrated in Figure 3. But because
there is a much larger space of hyponyms for
weapon in this direction, the output serves more to
reduce the entropy of the translation candidate space
when used in conjunction with other translation
induction similarity measures. We would expect the
application of additional similarity measures to this
greatly narrowed and ranked hypothesis space to
yield improvement in future work.
5 Conclusion
This paper has presented a novel minimal-resource
algorithm for the acquisition of multilingual lex-
ical taxonomies (including hyponymy/hypernymy
and meronymy). The algorithm is based on cross
language projection of various monolingual indica-
tors of these taxonomic relationships in free text
and via bootstrapping thereof. Using only 31-58
seed examples, the algorithm achieves accuracies of
73% and 66% for English and Hindi respectively on
the tasks of hyponymy/meronomy/cousinhood/other
model induction. The robustness of this approach
is shown by the fact that the unannotated Hindi de-
velopment corpus was only 1/15th the size of the
utilized English corpus. We also present a novel
model of unsupervised translation dictionary induc-
tion via multilingual transitive models of hypernymy
and hyponymy, using these induced taxonomies and
evaluated on Hindi-English. Performance starting
from no multilingual dictionary supervision is quite
promising.
References
E. Agichtein and L. Gravano. 2000. Snowball: extract-
ing relations from large plain-text collections. In Pro-
ceedings of the 5th ACM International Conference on
Digital Libraries, pages 85?94.
M. J. Cafarella, D. Downey, S. Soderland, and O. Etzioni.
2005. Knowitnow: Fast, scalable information extrac-
tion from the web. In Proceedings of EMNLP/HLT-05,
pages 563?570.
S. Caraballo. 1999. Automatic construction of a
hypernym-labeled noun hierarchy from text. In Pro-
ceedings of ACL-99, pages 120?126.
B. Carterette, R. Jones, W. Greiner, and C. Barr. 2006. N
semantic classes are harder than two. In Proceedings
of ACL/COLING-06, pages 49?56.
471
raaiphala missile grenade bomb
rifle
weapon
(hypothesis space)
[via inducedhathiyaara
or previous induced translations]
[via existing dictionary entries
hypernymy]
[via induced
hyponymy]
Hindi English
Goal: To learn this translation
Figure 3: Reducing the space of likely translation candidates of the word raaiphala by inducing its hypernym, using a partial
dictionary to look up the translation of hypernym and generating the candidate translations as induced hyponyms in English space.
O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. S. Weld, and A. Yates.
2005. Unsupervised named-entity extraction from the
web: an experimental study. Artif. Intell., 165(1):91?
134.
C. Fellbaum. 1998. WordNet: An electronic lexical
database.
R. Girju, A. Badulescu, and D. Moldovan. 2003.
Learning semantic constraints for the automatic dis-
covery of part-whole relations. In Proceedings of
HLT/NAACL-03, pages 1?8.
R. Girju, A. Badulescu, and D. Moldovan. 2006. Au-
tomatic discovery of part-whole relations. Computa-
tional Linguistics, 21(1):83?135.
D. Graff, J. Kong, K. Chen, and K. Maeda. 2005. En-
glish Gigaword Second Edition. Linguistic Data Con-
sortium, catalog number LDC2005T12.
T. Hasegawa, S. Sekine, and R. Grishman. 2004. Discov-
ering relations among named entities from large cor-
pora. In Proceedings of ACL-04, pages 415?422.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of COLING-
92, pages 539?545.
D. Narayan, D. Chakrabarty, P. Pande, and P. Bhat-
tacharyya. 2002. An Experience in Building the Indo
WordNet-a WordNet for Hindi. International Confer-
ence on Global WordNet.
Marius Pas?ca, Dekang Lin, Jeffrey Bigham, Andrei Lif-
chits, and Alpa Jain. 2006. Names and similarities on
the web: Fact extraction in the fast lane. In Proceed-
ings of ACL/COLING-06, pages 809?816.
P. Pantel and M. Pennacchiotti. 2006. Espresso: Lever-
aging generic patterns for automatically harvesting
semantic relations. In Proceedings of ACL/COLING-
06, pages 113?120.
P. Pantel and D. Ravichandran. 2004. Automati-
cally labeling semantic classes. In Proceedings of
HLT/NAACL-04, pages 321?328.
P. Pantel, D. Ravichandran, and E. Hovy. 2004. Towards
terascale knowledge acquisition. In Proceedings of
COLING-04.
D. Ravichandran and E. Hovy. 2002. Learning surface
text patterns for a question answering system. In Pro-
ceedings of ACL-02, pages 41?47.
E. Riloff and R. Jones. 1999. Learning dictionaries for
information extraction by multi-level bootstrapping.
In Proceedings of AAAI/IAAI-99, pages 474?479.
E. Riloff and J. Shepherd. 1997. A corpus-based ap-
proach for building semantic lexicons. CoRR, cmp-
lg/9706013.
C. Schafer and D. Yarowsky. 2002. Inducing translation
lexicons via diverse similarity measures and bridge
languages. In Proceedings of CONLL-02, pages 146?
152.
R. Snow, D. Jurafsky, and A. Y. Ng. 2006. Semantic tax-
onomy induction from heterogenous evidence. In Pro-
ceedings of ACL/COLING-06, pages 801?808.
M. Thelen and E. Riloff. 2002. A bootstrapping method
for learning semantic lexicons using extraction pattern
contexts. In Proceedings of EMNLP-02, pages 214?
221.
D. Widdows. 2003. Unsupervised methods for devel-
oping taxonomies by combining syntactic and statisti-
cal information. In Proceedings of HLT/NAACL-03,
pages 197?204.
472
  	

	
ff 	
 

 	

Desparately Seeking Cebuano
Douglas W. Oard, David Doermann, Bonnie Dorr, Daqing He, Philip Resnik, and Amy Weinberg
UMIACS, University of Maryland, College Park, MD, 20742
(oard,doermann,bonnie,resnik,weinberg)@umiacs.umd.edu
William Byrne, Sanjeev Khudanpur and David Yarowsky
CLSP, Johns Hopkins University, 3400 North Charles Street, Barton Hall, Baltimore, MD 21218
(byrne,khudanpur,yarowsky)@jhu.edu
Anton Leuski, Philipp Koehn and Kevin Knight
USC Information Sciences Institute, 4676 Admiralty Way, Marina Del Rey, CA 90292
(leuski,koehn,knight)@isi.edu
Abstract
This paper describes an effort to rapidly de-
velop language resources and component tech-
nology to support searching Cebuano news sto-
ries using English queries. Results from the
first 60 hours of the exercise are presented.
1 Introduction
The Los Angeles Times reported that at about 5:20 P.M.
on Tuesday March 4, 2003, a bomb concealed in a back-
pack exploded at the airport in Davao City, the second
largest city in the Philippines. At least 23 people were
reported dead, with more than 140 injured, and Pres-
ident Arroyo of the Philippines characterized the blast
as a terrorist act. With the 13 hour time difference, it
was then 4:20 A.M on the same date in Washington, DC.
Twenty-four hours later, at 4:13 A.M. on March 5, partic-
ipants in the Translingual Information Detection, Extrac-
tion and Summarization (TIDES) program were notified
that Cebuano had been chosen as the language of interest
for a ?surprise language? practice exercise that had been
planned quite independently to begin on that date. The
notification observed that Cebuano is spoken by 24% of
the population of the Philippines, and that it is the lingua
franca in the south Philippines, where the event occurred.
One goal of the TIDES program is to develop the abil-
ity to rapidly deploy a broad array of language technolo-
gies for previously unforeseen languages in response to
unexpected events. That capability will be formally ex-
ercised for the first time during June 2003, in a month-
long ?Surprise Language Experiment.? To prepare for
that event, the Linguistic Data Consortium (LDC) orga-
nized a ?dry run? for March 5-14 in order to refine pro-
cedures for rapidly developing language resources of the
type that the TIDES community will need during the July
evaluation.
Development of interactive Cross-Language Informa-
tion Retrieval (CLIR) systems that can be rapidly adapted
to accommodate new languages has been the focus of
extensive collaboration between the University of Mary-
land and The Johns Hopkins University, and more re-
cently with the University of Southern California. The
capability for rapid development of necessary language
resources is an essential part of that process, so we had
been planning to participate in the surprise language dry
run to refine our procedures for sharing those resources
with other members of the TIDES community. Naturally,
we chose CLIR as a driving application to focus our ef-
fort. Our goal, therefore, was to build an interactive sys-
tem that would allow a searcher posing English queries
to find relevant Cebuano news articles from the period
immediately following the bombing.
2 Obtaining Language Resources
Our basic approach to development of an agile system for
interactive CLIR relies on three strategies: (1) create an
infrastructure in advance for English as a query language
that makes only minimal assumptions about the docu-
ment language; (2) leverage the asymmetry inherent in
the problem by assembling strong resources for English
in advance; and (3) develop a robust suite of capabilities
to exploit any language resources that can be found for
the ?surprise language.? We defer the first two topics to
the next section, and focus here on the third. We know of
five possible sources of translation expertise:
People. People who know the language are an excellent
source of insight, and universities are an excellent
place to find such people. We were able to locate
a speaker of Cebuano within 50 feet of one of our
offices, and to schedule an interview with a second
Cebuano speaker within 36 hours of the announce-
ment of the language.
Scholarly literature. Major research universities are
also an excellent place to find written materials de-
scribing a broad array of languages. Within 12 hours
of the announcement, reference librarians at the Uni-
versity of Maryland had identified a textbook on
?Beginning Cebuano,? and we had located a copy
at the University of Southern California. Together
with the excellent electronic resources located by the
LDC, this allowed us to develop a rudimentary stem-
mer within 36 hours.
Translation lexicons. Simple bilingual term lists are
available for many language pairs. Using links pro-
vided by the LDC and our own Web searches, we
were able to construct an English-Cebuano term list
with over 14,000 translation pairs within 12 hours of
the announcement. This largely duplicated a simul-
taneous effort at the LDC, and we later merged our
term list with theirs.
Parallel text. Translation-equivalent documents, when
aligned at the word level, provide an excellent
source of information about not just possible trans-
lations, but their relative predominance. Within 24
hours of the announcement, we had aligned Ce-
buano and English versions of the Holy Bible at
the word level using Giza++. An evaluation by a
native Cebuano speaker of a stratified random sam-
ple of 88 translation pairs showed remarkably high
precision. On a 4-point scale with 1=correct and
4=incorrect the most frequent 100 words averaged
1.3, the next 400 most frequent terms averaged 1.6,
and the 500 next most frequent terms after that aver-
aged 1.7. The Bible?s vocabulary covers only about
half of the words found in typical English news text
(counted by-token), so it is useful to have additional
sources of parallel text. For this reason, we have ex-
tended our previously developed STRAND system
to locate likely translations in the Internet Archive.
Those runs were not yet complete when this paper
was submitted.
Printed Dictionaries. People learning a new language
make extensive use of bilingual dictionaries, so we
have developed a system that mimics that process
to some extent. Within 12 hours of the announce-
ment we had zoned page images from a Cebuano-
English dictionary that was available commercially
in Adobe Page Description Format (PDF) to iden-
tify each dictionary entry, performed optical charac-
ter recognition, and parsed the entries to construct a
bilingual term list. We were aided in this process by
the fact that Cebuano is written in a Roman script.
Again, we achieved good precision, with a sampled
word error rate for OCR of 6.9% and a precision for
a random sample of translation pairs of 87%. Part of
speech tags were also extracted, although they are
not used in our process.
As this description illustrates, these five sources pro-
vide complementary information. Since there is some
uncertainty at the outset about how long it will be before
each delivers useful results, we chose a strategy based
on concurrency, balancing our investment over each the
five sources. This allowed us to use whatever resources
became available first to get an initial system running,
with refinements subsequently being made as additional
resources became available. Because Cebuano and En-
glish are written in the same script, we did not need char-
acter set conversion or phonetic cognate matching in this
case. The CLIR system described in the next section
was therefore constructed using only English resources
that were (or could have been) pre-assembled, plus a
Cebuano-English bilingual term list, a rule-based stem-
mer, and the Cebuano Bible.
3 Building a Cross-Language Retrieval
System
Ideally, we would like to build a system that would find
whatever documents the searcher would wish to read in a
fully automatic mode. In practice, fully automatic search
systems are imperfect even in monolingual applications.
We therefore have developed an interactive approach that
functions something like a typical Web search engine: (1)
the searcher poses their query in English, (2) the sys-
tem ranks the Cebuano documents in decreasing order
of likely relevance to the query, (3) the searcher exam-
ines a list of document titles in something approximat-
ing English, and (4) the searcher may optionally exam-
ine the full text of any document in something approx-
imating English. The intent is to support an iterative
process in which searchers learn to better express their
query through experience. We are only able to provide
very rough translations, so we expect that such a sys-
tem would be used in an environment where searchers
could send documents that appear promising off for pro-
fessional translation when necessary.
At the core of our system is the capability to au-
tomatically rank Cebuano documents based on an En-
glish query. We chose a query translation architecture
using backoff translation and Pirkola?s structured query
method, implemented using Inquery version 3.1p1. The
key idea in backoff translation is to first try to find con-
secutive sequences of query words on the English side
of the bilingual term list, where that fails to try to find
the surface form of each remaining English term, to fall
back to stem matching when necessary, and ultimately to
fall back to retaining the English term unchanged in the
hope that it might be a proper name or some other form
of cognate with Cebuano. Accents are stripped from the
documents and all language resources to facilitate match-
ing at that final step.
Although we have chosen techniques that are relatively
robust and therefore require relatively little domain-
specific tuning, stemmer design is an area of uncertainty
that could adversely affect retrieval effectiveness. We
therefore needed a test collection on which we could try
out variants of the Cebuano stemmer. We built this test
collection using 34,000 Cebuano Bible verses and 50 En-
glish questions that we found on the Web for which ap-
propriate Bible verses were known. Each question was
posed as a query using the batch mode of Inquery, and
the rank of the known relevant verse was taken as a mea-
sure of effectiveness. We took the mean reciprocal rank
(the inverse of the harmonic mean) as a figure of merit
for each configuration, and used a paired two-tailed   -
test (with p  0.05) to assess the statistical significance of
observed differences. Our initial configuration, without
stemming, obtained a mean inverse rank of 0.14, which
is a statistically significant improvement over no transla-
tion at all (mean inverse rank 0.02 from felicitous cognate
and loan word matches). The addition of Cebuano stem-
ming resulted in a reduction in mean inverse rank to 0.09.
Although the reduction is not statistically significant in
that case, the result suggests that our initial stemmer is
not yet useful for information retrieval tasks.
The other key capability that is needed is title and doc-
ument translation. We can accomplish this in one of two
ways. The simplest approach is to reverse the bilingual
term list, and to reverse the role of Cebuano and En-
glish in the process described above for query transla-
tion. Our user interface is capable of displaying multi-
ple translations for a single term (arranged horizontally
for compact depiction or vertically for clearer depiction),
but searchers can choose to display only the single most
likely translation. When reliable translation probability
statistics (from parallel text) are not available, we use the
relative word unigram frequency of each translation of a
Cebuano term in a representative English collection as a
substitute for that probability. A more sophisticated way
is to build a statistical machine translation system using
parallel text. We built our first statistical machine trans-
lation system within 40 hours of the announcement, and
one sentence of the resulting translation using each tech-
nique is shown below:
Cebuano: ?ang rebeldeng milf, kinsa
lakip sa nangamatay, nagdala og
backpack nga dunay explosives nga
niguba sa waiting lounge sa airport,
matod sa mga defense official.?
Term-by-term translation:
?(carelessness, circumference,
conveyence) rebeldeng milf, who lakip
(at in of) nangamatay, nagdala og
backpack nga valid explosives nga
niguba (at, in of) waiting lounge
(at, in, of) airport, matod (at, in,
of) mga defense official?
Statistical translation: ?who was
accused of rank, ehud og niguba
waiting lounge defense of those dumah
milf rebeldeng explosives backpack
airport matod official.?
At this point, term-by-term translation is clearly the bet-
ter choice. But as more parallel text becomes available,
we expect the situation to reverse. The LDC is prepar-
ing a set of human reference translations that will allow
us to detect that changeover point automatically using the
NIST variant of the BLEU measure for machine transla-
tion effectiveness.
4 Conclusion
The results reported in this paper were accomplished by
a team of 20 people with expertise in various facets of te
task that invested about 250 person-hours over two and
a half days. As additional Cebuano-specific evaluation
resources are developed, we expect to gain additional in-
sight into the quality of these early resources. Moreover,
once we see what works best for Cebuano by the end of
the process, we plan to revisit our process design with
an eye towards better optimizing our initial time invest-
ments. We expect to be able to address both of those
points in detail by the time of the conference.
This exercise was originally envisioned as a dry run to
work out the kinks in our process, and indeed we have
already learned a lot on that score. First, we learned that
our basic approach seems sound; we built the key com-
ponents of an interactive CLIR system in about 40 hours,
and by the 60-hour point we had some basis for believing
that each of those components could at least minimally
fulfill their role in a fully integrated system. Some of our
time was, however, spent on things that could have been
done in advance. Perhaps the most important of these
was the development of an information retrieval test col-
lection using the Bible. That job, and numerous smaller
ones, are now done, so we expect that we will be able to
obtain similar results with about half the effort next time
around.
Acknowledgments
Thanks to Clara Cabezas, Tim Hackman, Margie Hi-
nonangan, Burcu Karagol-Ayan, Okan Kolak, Huanfeng
Ma, Grazia Russo-Lassner, Michael Subotin, Jianqiang
Wang and the LDC! This work has been supported in part
by DARPA contract N660010028910.
  	

	  
		 	
			ffExploiting Aggregate Properties of Bilingual Dictionaries For Distinguishing
Senses of English Words and Inducing English Sense Clusters
Charles SCHAFER and David YAROWSKY
Department of Computer Science and
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD, 21218, USA
{cschafer,yarowsky}@cs.jhu.edu
Abstract
We propose a novel method for inducing monolingual
semantic hierarchies and sense clusters from numerous
foreign-language-to-English bilingual dictionaries. The
method exploits patterns of non-transitivity in transla-
tions across multiple languages. No complex or hierar-
chical structure is assumed or used in the input dictio-
naries: each is initially parsed into the ?lowest common
denominator? form, which is to say, a list of pairs of the
form (foreign word, English word). We then propose a
monolingual synonymy measure derived from this ag-
gregate resource, which is used to derive multilingually-
motivated sense hierarchies for monolingual English
words, with potential applications in word sense classifi-
cation, lexicography and statistical machine translation.
1 Introduction
In this work we consider a learning resource compris-
ing over 80 foreign-language-to-English bilingual dictio-
naries, collected by downloading electronic dictionaries
from the Internet and also scanning and running optical
character recognition (OCR) software on paper dictio-
naries. Such a diverse parallel lexical data set has not,
to our knowledge, previously been assembled and exam-
ined in its aggregate form as a lexical semantics training
resource. We show that this aggregate data set admits
of some surprising applications, including discovery of
synonymy relationships between words and automatic
induction of high-quality hierarchical word sense clus-
terings for English.
We perform and describe several experiments deriving
synonyms and sense groupings from the aggregate bilin-
gual dictionary, and subsequently suggest some possible
applications for the results.
Finally, we propose that sense taxonomies of the kind
introduced here, being of different provenance from
those produced explicitly by lexicographers or using un-
supervised corpus-driven methods, have significant value
because they add diversity to the set of available re-
sources.
2 Resources
First we collected, from Internet sources and via scan-
ning and running OCR on print dictionaries, 82 dictio-
naries between English and a total of 44 distinct foreign
languages from a variety of language families.
Over 213K distinct English word types were present
in a total of 5.5M bilingual dictionary entries, for an av-
fair
blond justS
SS are synonymous with
fair
differing senses of
blond and just
Figure 1: Detecting asynonymy via unbalanced synonymy relation-
ships among 3 words. The derived synonymy relation S holds between
fair and blond, and between fair and just. S does not hold between
blond and fair. We can infer that fair has at least 2 senses and, further,
we can represent them by blond and just.
English French Spanish German
fair blond, blondo, blond,
juste licito, recto gerecht
blond blond blondo blond
just juste licito; recto gerecht
Figure 2: This excerpt from the data set illustrates the kind of support
the aggregate bilingual dictionary provides for partitioning the mean-
ings of fair into distinct senses: blond and just.
erage of 26 and a median of 3 foreign entries per English
word. Roughly 15K English words had at least 100 for-
eign entries; over 64K had at least 10 entries.
No complex or hierarchical structure was assumed or
used in our input dictionaries. Each was initially parsed
into the ?lowest common denominator? form. This con-
sisted of a list of pairs of the form (foreign word, English
word). Because bilingual dictionary structure varies
widely, and even the availability and compatibility of
part-of-speech tags for entries is uncertain, we made the
decision to compile the aggregate resource only with data
that could be extracted from every individual dictionary
into a universally compatible format. The unique pairs
extracted from each dictionary were then converted to 4-
tuples of the form:
<foreign language, dictionary name, foreign word, English word>
before being inserted into the final, combined dictionary
data set.
3 A Synonymy Relation
We began by using the above-described data set to obtain
a synonymy relation between English words.
In general, in a paper bilingual dictionary, each for-
eign word can be associated with a list of English words
which are possible translations; in our reduced format
each entry lists a single foreign word and single possible
English translation, though taking a union of all English
translations for a particular foreign word recreates this
list.
We use the notion of coentry to build the synonymy
relation between English words. The per-entry coentry
count Cper?entry(e1,e2) for two English words e1 and e2
is simply the number of times e1 and e2 both appear as
the translation of the same foreign word (over all foreign
words, dictionaries and languages). The per-dictionary
coentry count Cper?dict(e1,e2), ignores the number
of individual coentries within a particular dictionary
and merely counts as 1 any number of coentries inside
a particular dictionary. Finally, per-language coentry
count Cper?lang(e1,e2) counts as 1 any number of
coentries for e1 and e2 for a particular language. Thus,
for the following snippet from the database:
Eng. Wd. Foreign Wd. Foreign Language Dict. ID
hit schlagen GERMAN ger.dict1
pound schlagen GERMAN ger.dict1
hit schlag GERMAN ger.dict1
pound schlag GERMAN ger.dict1
hit schlag GERMAN ger.dict2
pound schlag GERMAN ger.dict2
hit battere ITAL ital.dict1
pound battere ITAL ital.dict1
Cper?entry(hit,pound) = 4, while
Cper?dict(hit,pound) = 3, since the two individ-
ual coentries in ger.dict1 are only counted once.
Cper?lang(hit,pound) = 2; hit and pound are coentries in
the Italian and German languages. We found the more
conservative per-dictionary and per-language counts to
be a useful device, given that some dictionary creators
appear sometimes to copy and paste identical synonym
sets in a fairly indiscriminate fashion, spuriously
inflating the Cper?entry(e1,e2) counts.
Our algorithm for identifying synonyms was sim-
ple: we sorted all pairs of English words by decreas-
ing Cper?dict(e1,e2) and, after inspection of the resulting
list, cut it off at a per-dictionary and per-language count
threshold1 yielding qualitatively strong results. For all
word pairs e1,e2 above threshold, we say the symmetric
synonymy relation S(e1,e2) holds. The following tables
provide a clarifying example showing how synonymy
can be inferred from multiple bilingual dictionaries in a
way which is impossible with a single such dictionary
(because of idiosyncratic foreign language polysemy).
Lang. Dict. ID Foreign Wd English Translations
GERMAN ger.dict1 absetzen deposit drop deduct sell
GERMAN ger.dict1 ablagerung deposit sediment settlement
The table above displays entries from one
German-English dictionary. How can we tell
that ?sediment? is a better synonym for ?de-
posit? than ?sell?? We can build and examine the
1The threshold was 10 and 5 respectively for per-dictionary and per-
language coentry counts.
coentry counts Cper?lang(deposit,sediment) and
Cper?lang(deposit,sell) using dictionaries from many
languages, as illustrated below:
FRENCH fre.dict1 de?po?t arsenal deposit depository
depot entrusting filing
sludge store trust submission
repository scale sediment
TURKISH tk.dict1 tortu sediment deposit faeces
remainder dregs crust
CZECH cz.dict1 sedlina clot deposit sediment warp
Polysemy which is specific to German ? ?deposit?
and ?sell? senses coexisting in a particular word
form ?absetzen? ? will result in total coentry counts
Cper?lang(deposit,sell), over all languages and dictio-
naries, which are low. In fact, ?deposit? and ?sell?
are coentries under only 2 out of 44 languages in our
database (German and Swedish, which are closely re-
lated). On the other hand, near-synonymous English
translations of a particular sense across a variety of lan-
guages will result in high coentry counts, as is the case
with Cper?lang(deposit,sediment). As illustrated in the
tables, German, French, Czech and Turkish all support
the synonymy hypothesis for this pair of English words.
?deposit? Coentries Per Entry Per Dict. Per Lang.
sell 4 4 2
sediment 68 40 18
The above table, listing the various coentry counts
for ?deposit?, demonstrates the empirical motivation in
the aggregate dictionary for the synonymy relationship
between deposit and sediment, while the aggregate ev-
idence of synonymy between deposit and sell is weak,
limited to 2 languages, and is most likely the result of a
word polysemy restricted to a few Germanic languages.
4 Different Senses: Asymmetries of
Synonymy Relations
After constructing the empirically derived synonymy re-
lation S described in the previous section, we observed
that one can draw conclusions from the topology of the
graph of S relationships (edges) among words (vertices).
Specifically, consider the case of three words e1,e2, e3
for which S(e1,e2) and S(e1,e3) hold, but S(e2,e3) does
not. Figure 1 illustrates this situation with an example
from data (e1 = ?fair?), and more examples are listed
in Table 1. As Figure 1 suggests and inspection of the
random extracts presented in Table 1 will confirm, this
topology can be interpreted as indicating that e2 and e3
exemplify differing senses of e1.
We decided to investigate and apply it with more gen-
erality. This will be discussed in the next section.
5 Inducing Sense Taxonomies: Clustering
with Synonym Similarity
With the goal of using the aggregate bilingual dictionary
to induce interesting and useful sense distinctions of En-
glish words, we investigated the following strategy.
syn1(W) W syn2(W)
quiet still yet
desire want lack
delicate tender offer
conceal hide skin
nice kind sort
assault charge load
filter strain stretch
flow run manage
cloth fabric structure
blond fair just
foundation base ignoble
deny decline fall
hurl cast mould
bright clear open
harm wrong incorrect
crackle crack fissure
impeach charge load
enthusiastic keen sharp
coarse rough difficult
fling cast form
firm fast speedy
fashion mold mildew
incline lean meagre
arouse raise increase
digit figure shape
dye paint picture
spot stain tincture
shape cast toss
claim call shout
earth ground groundwork
associate fellow guy
arrest stop plug
Table 1: A representative sampling of high-confidence sense
distinctions derived via unbalanced synonymy relationships among
three words, W and two of its synonyms syn1(W) & syn2(W),
such that Cper?dict(W,syn1(W)) and Cper?dict(W,syn2(W)) are
high, whereas Cper?dict(syn1(W),syn2(W)) is low (0). Ex-
tracted from a list sorted by descending Cper?dict(W,syn1(W))
? Cper?dict(W,syn2(W)) / Cper?dict(syn1(W),syn2(W)) (counts
were smoothed to prevent division by zero).
For each target word Wt in English having a suffi-
ciently high dictionary occurrence count to allow inter-
esting results2, a list of likely synonym words Ws was
induced by the method described in Section 33. Addi-
tionally, we generated a list of all words Wc having non-
zero Cper?dict(Wt,Wc).
The synonym words Ws ? the sense exemplars for
target words Wt ? were clustered based on vectors of
coentry counts Cper?dict(Ws,Wc). This restriction on
vector dimension to only words that have nonzero co-
entries with the target word helps to exclude distractions
such as coentries of Ws corresponding to a sense which
doesn?t overlap with Wt. The example given in the fol-
lowing table shows an excerpt of the vectors for syn-
onyms of strike. The hit synonym overlaps strike in the
beat/bang/knock sense. Restricting the vector dimension
as described will help prevent noise from hit?s common
2For our experiments, English words occurring in at least 15 distinct
source dictionaries were considered.
3Again, the threshold for synonyms was 10 and 5 respectively for
per-dictionary and per-language coentry counts.
chart-topper/recording/hit single sense. The following
table also illustrates the clarity with which major sense
distinctions are reflected in the aggregate dictionary. The
induced clustering for strike (tree as well as flat cluster
boundaries) is presented in Figure 4.
attack bang hit knock walkout find
attack - 4 18 7 0 0
bang - 38 43 2 0 0
hit - 44 2 29
knock - 2 0
walkout - 0
find -
We used the CLUTO clustering toolkit (Karypis,
2002) to induce a hierarchical agglomerative clustering
on the vectors for Ws. Example results for vital and
strike are in Figures 3 and 4 respectively4. Figure 4 also
presents flat clusters automatically derived from the tree,
as well as a listing of some foreign words associated with
particular clusters.
Figure 3: Induced sense hierarchy for the word ?vital?
6 Related Work
There is a distinguished history of research extracting lexical
semantic relationships from bilingual dictionaries (Copestake
et al, 1995; Chen and Chang, 1998). There is also a long-
standing goal of mapping translations and senses in multiple
languages in a linked ontology structure (Resnik and Yarowsky,
1997; Risk, 1989; Vossen, 1998). The recent work of Ploux and
Ji (2003) has some similarities to the techniques presented here
in that it considers topological properties of the graph of syn-
onymy relationships between words. The current paper can be
distinguished on a number of dimensions, including our much
greater range of participating languages, and the fundamental
algorithmic linkage between multilingual translation distribu-
tions and monolingual synonymy clusters.
4In both ?vital? and ?strike? examples, the rendered hierarchical
clusterings were pruned (automatically) in order to fit in this paper.
Figure 4: Induced sense hierarchy for the word ?strike? and some translations of individual ?strike? synonyms. Flat clusters
automatically derived from the tree are denoted by the horizontal lines.
7 Analysis and Conclusions
This is the first presentation of a novel method for the induc-
tion of word sense inventories, which makes use of aggregate
information from a large collection of bilingual dictionaries.
One possible application of the induced sense inventories
presented here is as an aid to manual construction of mono-
lingual dictionaries or thesauri, motivated by translation dis-
tinctions across numerous world languages. While the desired
granularity of sense distinction will vary according to the re-
quirements of taste and differing applications, treating our out-
put as a proposal to be assessed and manually modified would
be a valuable labor-saving tool for lexicographers.
Another application of this work is a supplemental resource
for statistical machine translation (SMT). It is possible, as
shown graphically in Figure 4, to recover the foreign words
associated with a cluster (not just a single word). Given that
the clusters provide a more complete coverage of English word
types for a given sense than the English side of a particular
bilingual dictionary, clusters could be used to unify bitext co-
occurrence counts of foreign words with English senses in a
way that typical bilingual dictionaries cannot. Unifying counts
in this way would be a useful way of reducing data sparsity in
SMT training.
Finally, evaluation of induced sense taxonomies is always
problematic. First of all, there is no agreed ?correct? way to
classify the possible senses of a particular word. To some de-
gree this is because human experts disagree on particular judg-
ments of classification, though a larger issue, as pointed out
in Resnik and Yarowsky 1997, is that what constitutes an ap-
propriate set of sense distinctions for a word is, emphatically, a
function of the task at hand. The sense-distinction requirements
of English-to-French machine translation differ from those of
English-to-Arabic machine translation (due to differing degrees
of parallel polysemy across the language pairs), and both differ
from those of English dictionary construction.
We believe that the translingually-motivated word-sense tax-
onomies developed here will prove useful for the a variety
of tasks including those mentioned above. The fact that they
are derived from a novel resource, not constructed explicitly
by humans or derived in fully unsupervised fashion from text
corpora, makes them worthy of study and incorporation in fu-
ture lexicographic, machine translation, and word sense disam-
biguation efforts.
References
J. Chen and J. Chang. 1998. Topical Clustering of MRD
Senses Based on Information Retrieval Techniques.
Computational Linguistic, 29(2):61-95.
A. Copestake, E. Briscoe, P. Vossen, A. Ageno, I.
Castellan, F. Ribas, G. Rigau, H. Rodriguez and A.
Samiotou. 1995. Acquisition of Lexical Translation
Relations from MRDs. Machine Translation: Special
Issue on the Lexicon, 9(3):33-69.
G. Karypis. 2002. CLUTO: A Clustering Toolkit. Tech
Report 02-017, Dept. of Computer Science, University
of Minnesota. Available at http://www.cs.umn.edu?cluto
S. Ploux and H. Ji. 2003. A Model for Matching
Semantic Maps Between Languages (French/English,
English/French). Computational Linguistics, 29(2):155-
178.
P. Resnik and D. Yarowsky. 1997. A Perspective
on Word Sense Disambiguation Methods and Their
Evaluation. In Proceedings of SIGLEX-1997, pp. 79-86.
O. Risk. 1989. Sense Disambiguation of Word Trans-
lations in Bilingual Dictionaries: Trying to Solve The
Mapping Problem Automatically. RC 14666, IBM T.J.
Watson Research Center. Yorktown Heights.
P. Vossen (ed.). 1998. EUROWORDNET: A Multilingual
Database with Lexical Semantic Networks. Kluwer
Academic Publishers. Dordrecht, The Netherlands.
Improving Bitext Word Alignments
via Syntax-based Reordering of English
Elliott Franco Dra?bek and David Yarowsky
Department of Computer Science
Johns Hopkins University
Baltimore, MD 21218, USA
{edrabek,yarowsky}@cs.jhu.edu
Abstract
We present an improved method for automated word
alignment of parallel texts which takes advantage
of knowledge of syntactic divergences, while avoid-
ing the need for syntactic analysis of the less re-
source rich language, and retaining the robustness of
syntactically agnostic approaches such as the IBM
word alignment models. We achieve this by using
simple, easily-elicited knowledge to produce syntax-
based heuristics which transform the target lan-
guage (e.g. English) into a form more closely resem-
bling the source language, and then by using stan-
dard alignment methods to align the transformed
bitext. We present experimental results under vari-
able resource conditions. The method improves
word alignment performance for language pairs such
as English-Korean and English-Hindi, which exhibit
longer-distance syntactic divergences.
1 Introduction
Word-level alignment is a key infrastructural tech-
nology for multilingual processing. It is crucial for
the development of translation models and transla-
tion lexica (Tufis?, 2002; Melamed, 1998), as well as
for translingual projection (Yarowsky et al, 2001;
Lopez et al, 2002). It has increasingly attracted at-
tention as a task worthy of study in its own right
(Mihalcea and Pedersen, 2003; Och and Ney, 2000).
Syntax-light alignment models such as the five
IBM models (Brown et al, 1993) and their rela-
tives have proved to be very successful and robust
at producing word-level alignments, especially for
closely related languages with similar word order
and mostly local reorderings, which can be cap-
tured via simple models of relative word distortion.
However, these models have been less successful at
modeling syntactic distortions with longer distance
movement. In contrast, more syntactically informed
approaches have been constrained by the often weak
syntactic correspondences typical of real-world par-
allel texts, and by the difficulty of finding or induc-
ing syntactic parsers for any but a few of the world?s
most studied languages.
Our approach uses simple, easily-elicited knowl-
edge of divergences to produce heuristic syntax-
based transformations from English to a form
(English?) more closely resembling the source lan-
English Transform
Traces
RetraceSource| \|
English
English?
Run GIZA++
Source|/ |
English?
Source
Language-specific
Heuristics
Figure 1: System Architecture
guage, and then using standard alignment methods
to align the transformed version to the target lan-
guage. This approach retains the robustness of syn-
tactically agnostic models, while taking advantage
of syntactic knowledge. Because the approach relies
only on syntactic analysis of English, it can avoid
the difficulty of developing a full parser for a new
low-resource language.
Our method is rapid and low cost. It requires
only coarse-grained knowledge of basic word order,
knowledge which can be rapidly found in even the
briefest grammatical sketches. Because basic word
order changes very slowly with time, word order of
related languages tends to be very similar. For ex-
ample, even if we only know that a language is of
the Northern-Indian/Sanskrit family, we can easily
guess with high confidence that it is systematically
head-final. Because our method can be restricted
to only bi-text pre-processing and post-processing,
it can be used as a wrapper around any existing
word-alignment tool, without modification, to pro-
vide improved performance by minimizing alignment
distortion.
2 Prior Work
The 2003 HLT-NAACL Workshop on Building and
Using Parallel Texts (Mihalcea and Pedersen, 2003)
reflected the increasing importance of the word-
alignment task, and established standard perfor-
mance measures and some benchmark tasks.
There is prior work studying systematic cross-
English:
Hindi:
use of plutonium is to manufacture nuclear weapons
plutoniyama kaa
?s
istemaala
use
paramaanu
nuclear
hathiyaara banaane
manufacture
ke lie hotaa hai
is
the
NP
NP
PP
S
VP
VP
VP
NP
plutonium weapons to
Figure 2: Original Hindi-English sentence pair with gold-standard word-alignments.
English?:
Hindi: plutoniyama
plutonium
kaa
?s
istemaala
use
paramaanu
nuclear
hathiyaara banaane ke lie hotaa hai
is
plutonium of the use nuclear weapons manufacture to is
S
VP
PP
NP VP
VP
NP NP
weapons manufacture to
Figure 3: Transformed Hindi-English? sentence pair with gold-standard word-alignments. Rotated nodes are
marked with an arc.
linguistic structural divergences, such as the DUSTer
system (Dorr et al, 2002). While the focus on ma-
jor classes of structural variation such as manner-of-
motion verb-phrase transformations have facilitated
both transfer and generation in machine translation,
these divergences have not been integrated into a
system that produces automatic word alignments
and have tended to focus on more local phrasal varia-
tion rather than more comprehensive sentential syn-
tactic reordering.
Complementary prior work (e.g. Wu, 1995) has
also addressed syntactic transduction for bilingual
parsing, translation, and word-alignment. Much of
this work depends on high-quality parsing of both
target and source sentences, which may be unavail-
able for many ?lower density? languages of interest.
Tree-to-string models, such as (Yamada and Knight,
2001) remove this dependency, and such models are
well suited for situations with large, cleanly trans-
lated training corpora. By contrast, our method re-
tains the robustness of the underlying aligner to-
wards loose translations, and can if necessary use
knowledge of syntactic divergences even in the ab-
sence of any training corpora whatsoever, using only
a translation lexicon.
3 System
Figure 1 shows the system architecture. We start
by running the Collins parser (Collins, 1999) on the
English side of both training and testing data, and
apply our source-language-specific heuristics to the
Language VP AP NP
English VO AO AN, NR
Hindi OV OA AN, RN
Korean OV OA AN, RN
Chinese VO AOA AN, RN
Romanian VO AO NA, NR
Table 1: Basic word order for three major phrase
types ? VP: verb phrases with Verb and Object,
AP: appositional (prepositional or postpositional)
phrases with Apposition and Object, and NP: noun
phrases withNoun andAdjective orRelative clause.
Chinese has both prepositions and postpositions.
resulting trees. This yields English? text, along with
traces recording correspondences between English?
words and the English originals. We use GIZA++
(Och and Ney, 2000) to align the English? with the
source language text, yielding alignments in terms
of the English?. Finally, we use the traces to map
these alignments to the original English words.
Figure 2 shows an illustrative Hindi-English sen-
tence pair, with true word alignments, and parse-
tree over the English sentence. Although it is only
a short sentence, the large number of crossing align-
ments clearly show the high-degree of reordering,
and especially long-distance motion, caused by the
syntactic divergences between Hindi and English.
Figure 3 shows the same sentence pair after En-
glish has been transformed into English? by our sys-
tem. Tree nodes whose children have been reordered
 20
 25
 30
 35
 40
 45
 50
 55
 60
 65
 70
 3  3.2  3.4  3.6  3.8  4  4.2  4.4  4.6  4.8
F
-
m
e
a
s
u
r
e
log(number of training sentences)
E? MethodDirect
Figure 4: Hindi alignment performance
 0
 5
 10
 15
 20
 25
 3  3.2  3.4  3.6  3.8  4  4.2  4.4
F
-
m
e
a
s
u
r
e
log(number of training sentences)
E? MethodDirect
Figure 5: Korean alignment performance
are marked by a subtended arc. Crossings have been
eliminated, and the alignment is now monotonic.
Table 1 shows the basic word order of three major
phrase types for each of the languages we treated. In
each case, our heuristics transform the English trees
to achieve these same word orders. For the Chinese
case, we apply several more language-specific trans-
formations. Because Chinese has both prepositions
and postpositions, we retain the original preposition
and add an additional bracketing postposition. We
also move verb modifiers other than noun phrases to
the left of the head verb.
4 Experiments
For each language we treated, we assembled
sentence-aligned, tokenized training and test cor-
pora, with hand-annotated gold-standard word
alignments for the latter1. We did not apply any
sort of morphological analysis beyond basic word to-
kenization. We measured system performance with
wa eval align.pl, provided by Rada Mihalcea and
Ted Pedersen.
Each training set provides the aligner with infor-
mation about lexical affinities and reordering pat-
terns. For Hindi, Korean and Chinese, we also tested
our system under the more difficult situation of hav-
ing only a bilingual word list but no bitext available.
This is a plausible low-resource language scenario
 25
 30
 35
 40
 45
 50
 55
 3  3.5  4  4.5  5
F
-
m
e
a
s
u
r
e
log(number of training sentences)
E? MethodDirect
Figure 6: Chinese alignment performance
 35
 40
 45
 50
 55
 60
 65
 70
 75
 3  3.2  3.4  3.6  3.8  4  4.2  4.4  4.6
F
-
m
e
a
s
u
r
e
log(number of training sentences)
E? MethodDirect
Figure 7: Romanian alignment performance
# Train Direct English?
Sents P R F P R F
Hindi
Dict only 16.4 13.8 15.0 18.5 15.6 17.0
1000 26.8 23.0 24.8 28.4 24.4 26.2
3162 35.7 31.6 33.5 38.4 33.5 35.8
10000 46.6 42.7 44.6 50.4 45.2 47.6
31622 60.1 56.0 58.0 63.6 58.5 61.0
63095 64.7 61.7 63.2 66.3 62.2 64.2
Korean
Dict only 26.6 12.3 16.9 27.5 12.9 17.6
1000 9.4 7.3 8.2 11.3 8.7 9.8
3162 13.2 10.2 11.5 16.0 12.4 14.0
10000 15.2 12.0 13.4 17.0 13.3 14.9
30199 21.5 16.9 18.9 21.9 17.2 19.3
Chinese
Dict only 44.4 30.4 36.1 44.5 30.5 36.2
1000 33.0 22.2 26.5 30.8 22.6 26.1
3162 44.6 28.9 35.1 41.7 30.0 34.9
10000 51.1 34.0 40.8 50.7 35.8 42.0
31622 60.4 39.0 47.4 55.7 39.7 46.4
100000 66.0 43.7 52.6 63.7 45.4 53.0
Romanian
1000 49.6 27.7 35.6 50.1 28.0 35.9
3162 57.9 33.4 42.4 57.6 33.0 42.0
10000 72.6 45.5 55.9 71.3 45.0 55.2
48441 84.7 57.8 68.7 83.5 57.1 67.8
Table 2: Performance in Precision, Recall, and F-
measure (per cent) of all systems.
Source # Test Mean Correlation
Language Sents Length Direct E?
Hindi 46 16.3 54.1 60.1
Korean 100 20.2 10.2 31.6
Chinese 88 26.5 60.2 63.7
Romanian 248 22.7 81.1 80.6
Table 3: Test set characteristics, including number
of sentence pairs, mean length of English sentences,
and correlation r2 between English and source-
language normalized word positions in gold-standard
data, for direct and English? situations.
and a test of the ability of the system to take sole
responsibility for knowledge of reordering.
Table 3 describes the test sets and shows the cor-
relation in gold standard aligned word pairs between
the position of the English word in the English sen-
tence and the position of the source-language word
in the source-language sentence (normalizing the po-
sitions to fall between 0 and 1). The baseline (di-
rect) correlations give quantitative evidence of dif-
fering degrees of syntactic divergence with English,
and the English? correlations demonstrate that our
heuristics do have the effect of better fitting source
language word order.
5 Results
Figures 4, 5, 6 and 7 show learning curves for sys-
tems trained on parallel sentences with and with-
out the English? transforms. Table 2 provides fur-
ther detail, and also shows the performance of sys-
tems trained without any bitext, but only with ac-
cess to a bilingual translation lexicon. Our sys-
tem achieves consistent, substantial performance im-
provement under all situations for English-Hindi
and English-Korean language pairs, which exhibit
longer distance SOV?SVO syntactic divergence.
For English-Romanian and English-Chinese, neither
significant improvement nor degradation is seen, but
these are language pairs with quite similar sentential
word order to English, and hence have less opportu-
nity to benefit from our syntactic transformations.
6 Conclusions
We have developed a system to improve the per-
formance of bitext word alignment between English
and a source language by first reordering parsed
English into an order more closely resembling that
1Hindi training: news text from the LDC for the 2003
DARPA TIDES Surprise Language exercise; Hindi testing:
news text from Rebecca Hwa, then at the University of Mary-
land; Hindi dictionary: The Hindi-English Dictionary, v. 2.0
from IIIT (Hyderabad) LTRC; Korean training: Unbound
Bible; Korean testing: half from Penn Korean Treebank and
half from Universal declaration of Human Rights, aligned by
Woosung Kim at the Johns Hopkins University; Korean dic-
tionary: EngDic v. 4; Chinese training: news text from FBIS;
Chinese testing: Penn Chinese Treebank news text aligned by
Rebecca Hwa, then at the University of Maryland; Chinese
dictionary: from the LDC; Romanian training and testing:
(Mihalcea and Pedersen, 2003).
of the source language, based only on knowledge
of the coarse basic word order of the source lan-
guage, such as can be obtained from any cross-
linguistic survey of languages, and requiring no pars-
ing of the source language. We applied the sys-
tem to the task of aligning English with Hindi, Ko-
rean, Chinese and Romanian. Performance improve-
ment is greatest for Hindi and Korean, which exhibit
longer-distance constituent reordering with respect
to English. These properties suggest the proposed
English? word alignment method can be an effective
approach for word alignment to languages with both
greater cross-linguistic word-order divergence and an
absence of available parsers.
References
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra,
and R. L. Mercer. 1993. The mathematics of sta-
tistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
B. J. Dorr, L. Pearl, R. Hwa, and N. Habash. 2002.
DUSTer: A method for unraveling cross-language
divergences for statistical word-level alignment.
In Proceedings of AMTA-02, pages 31?43.
A. Lopez, M. Nosal, R. Hwa, and P. Resnik. 2002.
Word-level alignment for multilingual resource ac-
quisition. In Proceedings of the LREC-02 Work-
shop on Linguistic Knowledge Acquisition and
Representation.
I. D. Melamed. 1998. Empirical methods for MT
lexicon development. Lecture Notes in Computer
Science, 1529:18?9999.
R. Mihalcea and T. Pedersen. 2003. An evalua-
tion exercise for word alignment. In Rada Mi-
halcea and Ted Pedersen, editors, Proceedings of
the HLT-NAACL 2003 Workshop on Building and
Using Parallel Texts, pages 1?10.
F. J. Och and H. Ney. 2000. A comparison of align-
ment models for statistical machine translation.
In Proceedings of COLING-00, pages 1086?1090.
D. I. Tufis?. 2002. A cheap and fast way to build
useful translation lexicons. In Proceedings of
COLING-02, pages 1030?1036.
D. Wu. 1995. Stochastic inversion transduction
grammars, with application to segmentation,
bracketing, and alignment of parallel corpora. In
Proceedings of IJCAI-95, pages 1328?1335.
K. Yamada and K. Knight. 2001. A syntax-based
statistical translation model. In Proceedings of
ACL-01, pages 523?530.
D. Yarowsky, G. Ngai, and R. Wicentowski. 2001.
Inducing multilingual text analysis tools via ro-
bust projection across aligned corpora. In Pro-
ceedings of HLT-01, pages 161?168.
Proceedings of the 43rd Annual Meeting of the ACL, pages 483?490,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Multi-Field Information Extraction and Cross-Document Fusion
Gideon S. Mann and David Yarowsky
Department of Computer Science
The Johns Hopkins University
Baltimore, MD 21218 USA
{gsm,yarowsky}@cs.jhu.edu
Abstract
In this paper, we examine the task of extracting a
set of biographic facts about target individuals from
a collection of Web pages. We automatically anno-
tate training text with positive and negative exam-
ples of fact extractions and train Rote, Na??ve Bayes,
and Conditional Random Field extraction models
for fact extraction from individual Web pages. We
then propose and evaluate methods for fusing the
extracted information across documents to return a
consensus answer. A novel cross-field bootstrapping
method leverages data interdependencies to yield
improved performance.
1 Introduction
Much recent statistical information extraction re-
search has applied graphical models to extract in-
formation from one particular document after train-
ing on a large corpus of annotated data (Leek, 1997;
Freitag and McCallum, 1999).1 Such systems are
widely applicable, yet there remain many informa-
tion extraction tasks that are not readily amenable to
these methods. Annotated data required for training
statistical extraction systems is sometimes unavail-
able, while there are examples of the desired infor-
mation. Further, the goal may be to find a few inter-
related pieces of information that are stated multiple
times in a set of documents.
Here, we investigate one task that meets the above
criteria. Given the name of a celebrity such as
1Alternatively, Riloff (1996) trains on in-domain and
out-of-domain texts and then has a human filtering step.
Huffman (1995) proposes a method to train a different type of
extraction system by example.
?Frank Zappa?, our goal is to extract a set of bio-
graphic facts (e.g., birthdate, birth place and occupa-
tion) about that person from documents on the Web.
First, we describe a general method of automatic
annotation for training from positive and negative
examples and use the method to train Rote, Na??ve
Bayes, and Conditional Random Field models (Sec-
tion 2). We then examine how multiple extractions
can be combined to form one consensus answer
(Section 3). We compare fusion methods and show
that frequency voting outperforms the single high-
est confidence answer by an average of 11% across
the various extractors. Increasing the number of re-
trieved documents boosts the overall system accu-
racy as additional documents which mention the in-
dividual in question lead to higher recall. This im-
proved recall more than compensates for a loss in
per-extraction precision from these additional doc-
uments. Next, we present a method for cross-field
bootstrapping (Section 4) which improves per-field
accuracy by 7%. We demonstrate that a small train-
ing set with only the most relevant documents can be
as effective as a larger training set with additional,
less relevant documents (Section 5).
2 Training by Automatic Annotation
Typically, statistical extraction systems (such as
HMMs and CRFs) are trained using hand-annotated
data. Annotating the necessary data by hand is time-
consuming and brittle, since it may require large-
scale re-annotation when the annotation scheme
changes. For the special case of Rote extrac-
tors, a more attractive alternative has been proposed
by Brin (1998), Agichtein and Gravano (2000), and
Ravichandran and Hovy (2002).
483
Essentially, for any text snippet of the form
A1pA2qA3, these systems estimate the probability
that a relationship r(p, q) holds between entities p
and q, given the interstitial context, as2
P (r(p, q) | pA2q) = P (r(p, q) | pA2q)
=
?
x,y?T c(xA2y)
?
x c(xA2)
That is, the probability of a relationship r(p, q) is
the number of times that pattern xA2y predicts any
relationship r(x, y) in the training set T . c(.) is the
count. We will refer to x as the hook3 and y as the
target. In this paper, the hook is always an indi-
vidual. Training a Rote extractor is straightforward
given a set T of example relationships r(x, y). For
each hook, download a separate set of relevant doc-
uments (a hook corpus, Dx) from the Web.4 Then
for any particular pattern A2 and an element x, count
how often the pattern xA2 predicts y and how often
it retrieves a spurious y?.5
This annotation method extends to training other
statistical models with positive examples, for exam-
ple a Na??ve Bayes (NB) unigram model. In this
model, instead of looking for an exact A2 pattern
as above, each individual word in the pattern A2 is
used to predict the presence of a relationship.
P (r(p, q) | pA2q)
?P (pA2q | r(p, q))P (r(p, q))
=P (A2 | r(p, q))
=
?
a?A2
P (a | r(p, q))
We perform add-lambda smoothing for out-of-
vocabulary words and thus assign a positive prob-
ability to any sequence. As before, a set of relevant
2The above Rote models also condition on the preceding and
trailing words, for simplicity we only model interstitial words
A2.
3Following (Ravichandran and Hovy, 2002).
4In the following experiments we assume that there is one
main object of interest p, for whom we want to find certain
pieces of information r(p, q), where r denotes the type of re-
lationship (e.g., birthday) and q is a value (e.g., May 20th). We
require one hook corpus for each hook, not a separate one for
each relationship.
5Having a functional constraint ?q? 6= q, r?(p, q?) makes this
estimate much more reliable, but it is possible to use this method
of estimation even when this constraint does not hold.
documents is downloaded for each particular hook.
Then every hook and target is annotated. From that
markup, we can pick out the interstitial A2 patterns
and calculate the necessary probabilities.
Since the NB model assigns a positive probability
to every sequence, we need to pick out likely tar-
gets from those proposed by the NB extractor. We
construct a background model which is a basic un-
igram language model, P (A2) =
?
a?A2 P (a). We
then pick targets chosen by the confidence estimate
CNB(q) = log
P (A2 | r(p, q))
P (A2)
However, this confidence estimate does not work-
well in our dataset.
We propose to use negative examples to estimate
P (A2 | r?(p, q))6 as well as P (A2 | r(p, q)). For
each relationship, we define the target set Er to be
all potential targets and model it using regular ex-
pressions.7 In training, for each relationship r(p, q),
we markup the hook p, the target q, and all spuri-
ous targets (q? ? {Er ? q}) which provide negative
examples. Targets can then be chosen with the fol-
lowing confidence estimate
CNB+E(q) = log
P (A2 | r(p, q))
P (A2 | r?(p, q))
We call this NB+E in the following experiments.
The above process describes a general method for
automatically annotating a corpus with positive and
negative examples, and this corpus can be used to
train statistical models that rely on annotated data.8
In this paper, we test automatic annotation using
Conditional Random Fields (CRFs) (Lafferty et al,
2001) which have achieved high performance for in-
formation extraction. CRFs are undirected graphical
models that estimate the conditional probability of a
state sequence given an output sequence
P (s | o) =
1
Z
exp
( T?
t=1
?
k
?kfk(st?1, st, o, t)
)
6r? stands in for all other possible relationships (including no
relationship) between p and q. P (A2 | r?(p, q)) is estimated as
P (A2 | r(p, q)) is, except with spurious targets.
7e.g., Ebirthyear = {\d\d\d\d}. This is the only source of
human knowledge put into the system and required only around
4 hours of effort, less effort than annotating an entire corpus or
writing information extraction rules.
8This corpus markup gives automatic annotation that yields
noisier training data than manual annotation would.
484
p qA_2
B
p
A_2
A_2
q
B
q
Figure 1: CRF state-transition graphs for extracting a relation-
ship r(p, q) from a sentence pA2q. Left: CRF Extraction with
a background model (B). Right: CRF+E As before but with
spurious target prediction (pA2q?).
We use the Mallet system (McCallum, 2002) for
training and evaluation of the CRFs. In order to ex-
amine the improvement by using negative examples,
we train CRFs with two topologies (Figure 1). The
first, CRF, models the target relationship and back-
ground sequences and is trained on a corpus where
targets (positive examples) are annotated. The sec-
ond, CRF+E, models the target relationship, spu-
rious targets and background sequences, and it is
trained on a corpus where targets (positive exam-
ples) as well as spurious targets (negative examples)
are annotated.
Experimental Results
To test the performance of the different ex-
tractors, we collected a set of 152 semi-
structured mini-biographies from an online site
(www.infoplease.com), and used simple rules to
extract a biographic fact database of birthday and
month (henceforth birthday), birth year, occupation,
birth place, and year of death (when applicable).
An example of the data can be found in Table
1. In our system, we normalized birthdays, and
performed capitalization normalization for the
remaining fields. We did no further normalization,
such as normalizing state names to their two letter
acronyms (e.g., California ? CA). Fifteen names
were set aside as training data, and the rest were
used for testing. For each name, 150 documents
were downloaded from Google to serve as the hook
corpus for either training or testing.9
In training, we automatically annotated docu-
ments using people in the training set as hooks, and
in testing, tried to get targets that exactly matched
what was present in the database. This is a very strict
method of evaluation for three reasons. First, since
the facts were automatically collected, they contain
9Name polyreference, along with ranking errors, result in
the retrieval of undesired documents.
Aaron Neville Frank Zappa
Birthday January 24 December 21
Birth year 1941 1940
Occupation Singer Musician
Birthplace New Orleans Baltimore,Maryland
Year of Death - 1993
Table 1: Two of 152 entries in the Biographic Database. Each
entry contains incomplete information about various celebrities.
Here, Aaron Neville?s birth state is missing, and Frank Zappa
could be equally well described as a guitarist or rock-star.
errors and thus the system is tested against wrong
answers.10 Second, the extractors might have re-
trieved information that was simply not present in
the database but nevertheless correct (e.g., some-
one?s occupation might be listed as writer and the
retrieved occupation might be novelist). Third, since
the retrieved targets were not normalized, there sys-
tem may have retrieved targets that were correct but
were not recognized (e.g., the database birthplace is
New York, and the system retrieves NY).
In testing, we rejected candidate targets that were
not present in our target set models Er. In some
cases, this resulted in the system being unable to find
the correct target for a particular relationship, since
it was not in the target set.
Before fusion (Section 3), we gathered all the
facts extracted by the system and graded them in iso-
lation. We present the per-extraction precision
Pre-Fusion Precision = # Correct Extracted Targets
# Total Extracted Targets
We also present the pseudo-recall, which is the av-
erage number of times per person a correct target
was extracted. It is difficult to calculate true re-
call without manual annotation of the entire corpus,
since it cannot be known for certain how many times
the document set contains the desired information.11
Pre-Fusion Pseudo-Recall = # Correct Extracted Targets
#People
The precision of each of the various extraction
methods is listed in Table 2. The data show that
on average the Rote method has the best precision,
10These deficiencies in testing also have implications for
training, since the models will be trained on annotated data that
has errors. The phenomenon of missing and inaccurate data
was most prevalent for occupation and birthplace relationships,
though it was observed for other relationships as well.
11It is insufficient to count all text matches as instances that
the system should extract. To obtain the true recall, it is nec-
essary to decide whether each sentence contains the desired re-
lationship, even in cases where the information is not what the
biographies have listed.
485
Birthday Birth year Occupation Birthplace Year of Death Avg.
Rote .789 .355 .305 .510 .527 .497
NB+E .423 .361 .255 .217 .088 .269
CRF .509 .342 .219 .139 .267 .295
CRF+E .680 .654 .246 .357 .314 .450
Table 2: Pre-Fusion Precision of extracted facts for various extraction systems, trained on 15 people each with 150 documents, and
tested on 137 people each with 150 documents.
Birthday Birth year Occupation Birthplace Year of Death Avg.
Rote 4.8 1.9 1.5 1.0 0.1 1.9
NB+E 9.6 11.5 20.3 11.3 0.7 10.9
CRF 3.0 16.3 31.1 10.7 3.2 12.9
CRF+E 6.8 9.9 3.2 3.6 1.4 5.0
Table 3: Pre-Fusion Pseudo-Recall of extract facts with the identical training/testing set-up as above.
while the NB+E extractor has the worst. Train-
ing the CRF with negative examples (CRF+E) gave
better precision in extracted information then train-
ing it without negative examples. Table 3 lists the
pseudo-recall or average number of correctly ex-
tracted targets per person. The results illustrate that
the Rote has the worst pseudo-recall, and the plain
CRF, trained without negative examples, has the best
pseudo-recall.
To test how the extraction precision changes as
more documents are retrieved from the ranked re-
sults from Google, we created retrieval sets of 1, 5,
15, 30, 75, and 150 documents per person and re-
peated the above experiments with the CRF+E ex-
tractor. The data in Figure 2 suggest that there is a
gradual drop in extraction precision throughout the
corpus, which may be caused by the fact that doc-
uments further down the retrieved list are less rele-
vant, and therefore less likely to contain the relevant
biographic data.
Pre?F
usion P
recisio
n
# Retrieved Documents per Person 80  160 140
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 1
 60 40 20  120 0  100
Birthday
BirthplaceBirthyearOccupationDeathyear
Figure 2: As more documents are retrieved per person, pre-
fusion precision drops.
However, even though the extractor?s precision
drops, the data in Figure 3 indicate that there con-
tinue to be instances of the relevant biographic data.
# Retrieved Documents Per PersonPre
?Fusio
n Pseu
do?Re
call
 1 2 3
 4 5 6
 7 8 9
 10
 0
 0  20  40  60  80  100  120 140
 160
BirthyearBirthdayBirthplaceOccupationDeathyear
Figure 3: Pre-fusion pseudo-recall increases as more documents
are added.
3 Cross-Document Information Fusion
The per-extraction performance was presented in
Section 2, but the final task is to find the single cor-
rect target for each person.12 In this section, we ex-
amine two basic methodologies for combining can-
didate targets. Masterson and Kushmerick (2003)
propose Best which gives each candidate a
score equal to its highest confidence extraction:
Best(x) = argmax
x
C(x).13 We further consider
Voting, which counts the number of times each can-
didate x was extracted: Vote(x) = |C(x) > 0|.
Each of these methods ranks the candidate targets
by score and chooses the top-ranked one.
The experimental setup used in the fusion exper-
iments was the same as before: training on 15 peo-
ple, and testing on 137 people. However, the post-
fusion evaluation differs from the pre-fusion evalua-
tion. After fusion, the system returns one consensus
target for each person and thus the evaluation is on
the accuracy of those targets. That is, missing tar-
12This is a simplifying assumption, since there are many
cases where there might exist multiple possible values, e.g., a
person may be both a writer and a musician.
13C(x) is either the confidence estimate (NB+E) or the prob-
ability score (Rote,CRF,CRF+E).
486
Best Vote
Rote .364 .450
NB+E .385 .588
CRF .513 .624
CRF+E .650 .678
Table 4: Average Accuracy of the Highest Confidence (Best)
and Most Frequent (Vote) across five extraction fields.
gets are graded as wrong.14
Post-Fusion Accuracy = # People with Correct Target
# People
Additionally, since the targets are ranked, we also
calculated the mean reciprocal rank (MRR).15 The
data in Table 4 show the average system perfor-
mance with the different fusion methods. Frequency
voting gave anywhere from a 2% to a 20% improve-
ment over picking the highest confidence candidate.
CRF+E (the CRF trained with negative examples)
was the highest performing system overall.
Birth Day
Fusion Accuracy Fusion MRR
Rote Vote .854 .877
NB+E Vote .854 .889
CRF Vote .650 .703
CRF+E Vote .883 .911
Birth year
Rote Vote .387 .497
NB+E Vote .778 .838
CRF Vote .796 .860
CRF+E Vote .869 .876
Occupation
Rote Vote .299 .405
NB+E Vote .642 .751
CRF Vote .606 .740
CRF+E Vote .423 .553
Birthplace
Rote Vote .321 .338
NB+E Vote .474 .586
CRF Vote .321 .476
CRF+E Vote .467 .560
Year of Death
Rote Vote .389 .389
NB+E Vote .194 .383
CRF .750 .840
CRF+E Vote .750 .827
Table 5: Voting for information fusion, evaluated per person.
CRF+E has best average performance (67.8%).
Table 5 shows the results of using each of these
extractors to extract correct relationships from the
top 150 ranked documents downloaded from the
14For year of death, we only graded cases where the person
had died.
15The reciprocal rank = 1 / the rank of the correct target.
Web. CRF+E was a top performer in 3/5 of the
cases. In the other 2 cases, the NB+E was the most
successful, perhaps because NB+E?s increased re-
call was more useful than CRF+E?s improved pre-
cision.
Retrieval Set Size and Performance
As with pre-fusion, we performed a set of exper-
iments with different retrieval set sizes and used
the CRF+E extraction system trained on 150 docu-
ments per person. The data in Figure 4 show that
performance improves as the retrieval set size in-
creases. Most of the gains come in the first 30 doc-
uments, where average performance increased from
14% (1 document) to 63% (30 documents). Increas-
ing the retrieval set size to 150 documents per person
yielded an additional 5% absolute improvement.
Post?F
usion 
Accur
acy
# Retrieved Documents Per Person 0
 0.1 0.2 0.3
 0.4 0.5 0.6
 0.7 0.8 0.9
 0  20
 40  60  80  100 120 140 160 Occupation
BirthyearBirthdayDeathyearBirthplace
Figure 4: Fusion accuracy increases with more documents per
person
Post-fusion errors come from two major sources.
The first source is the misranking of correct relation-
ships. The second is the case where relevant infor-
mation is not retrieved at all, which we measure as
Post-Fusion Missing = # Missing Targets
# People
The data in Figure 5 suggest that the decrease in
missing targets is a significant contributing factor
to the improvement in performance with increased
document size. Missing targets were a major prob-
lem for Birthplace, constituting more than half the
errors (32% at 150 documents).
4 Cross-Field Bootstrapping
Sections 2 and 3 presented methods for training sep-
arate extractors for particular relationships and for
doing fusion across multiple documents. In this sec-
tion, we leverage data interdependencies to improve
performance.
The method we propose is to bootstrap across
fields and use knowledge of one relationship to im-
prove performance on the extraction of another. For
487
# Retrieved Documents Per PersonPost?F
usion 
Missin
g Targ
ets
 0 0.1 0.2
 0.3 0.4 0.5
 0.6 0.7 0.8
 0.9 1
 20 0  40  60  80  100 120 140 160
BirthplaceOccupationDeathyearBirthdayBirthyear
Figure 5: Additional documents decrease the number of post-
fusion missing targets, targets which are never extracted in any
document.
Birth year
Extraction Precision Fusion Accuracy
CRF .342 .797
+ birthday .472 .861
CRF+E .654 .869
+ birthday .809 .891
Occupation
Extraction Precision Fusion Accuracy
CRF .219 .606
+ birthday .217 .569
+ birth year(f) 21.9 .599
+ all .214 .591
CRF+E .246 .423
+ birthday .325 .577
+ birth year(f) .387 .672
+ all .382 .642
Birthplace
Extraction Precision Fusion Accuracy
CRF .139 .321
+ birthday .158 .372
+ birth year(f) .156 .350
CRF+E .357 .467
+ birthday .350 .474
+ birth year(f) .294 .350
+ occupation(f) .314 .354
+ all .362 .532
Table 6: Performance of Cross-Field Bootstrapping Models.
(f) indicates that the best fused result was taken. birth year(f)
means birth years were annotated using the system that discov-
ered the most accurate birth years.
example, to extract birth year given knowledge of
the birthday, in training we mark up each hook cor-
pus Dx with the known birthday b : birthday(x, b)
and the target birth year y : birthyear(x, y) and
add an additional feature to the CRF that indicates
whether the birthday has been seen in the sentence.16
In testing, for each hook, we first find the birthday
using the methods presented in the previous sec-
tions, annotate the corpus with the extracted birth-
day, and then apply the birth year CRF (see Figure 6
next page).
16The CRF state model doesn?t change. When bootstrapping
from multiple fields, we add the conjunctions of the fields as
features.
Table 6 shows the effect of using this bootstrapped
data to estimate other fields. Based on the relative
performance of each of the individual extraction sys-
tems, we chose the following schedule for perform-
ing the bootstrapping: 1) Birthday, 2) Birth year, 3)
Occupation, 4) Birthplace. We tried adding in all
knowledge available to the system at each point in
the schedule.17 There are gains in accuracy for birth
year, occupation and birthplace by using cross-field
bootstrapping. The performance of the plain CRF+E
averaged across all five fields is 67.4%, while for the
best bootstrapped system it is 74.6%, a gain of 7%.
Doing bootstrapping in this way improves for
people whose information is already partially cor-
rect. As a result, the percentage of people who
have completely correct information improves to
37% from 13.8%, a gain of 24% over the non-
bootstrapped CRF+E system. Additionally, erro-
neous extractions do not hurt accuracy on extraction
of other fields. Performance in the bootstrapped sys-
tem for birthyear, occupation and birth place when
the birthday is wrong is almost the same as perfor-
mance in the non-bootstrapped system.
5 Training Set Size Reduction
One of the results from Section 2 is that lower
ranked documents are less likely to contain the rel-
evant biographic information. While this does not
have an dramatic effect on the post-fusion accuracy
(which improves with more documents), it suggests
that training on a smaller corpus, with more relevant
documents and more sentences with the desired in-
formation, might lead to equivalent or improved per-
formance. In a final set of experiments we looked at
system performance when the extractor is trained on
fewer than 150 documents per person.
The data in Figure 7 show that training on 30 doc-
uments per person yields around the same perfor-
mance as training on 150 documents per person. Av-
erage performance when the system was trained on
30 documents per person is 70%, while average per-
formance when trained on 150 documents per per-
son is 68%. Most of this loss in performance comes
from losses in occupation, but the other relationships
17This system has the extra knowledge of which fused
method is the best for each relationship. This was assessed by
inspection.
488
Frank Zappa was born on December 21.
1. Birthday
Zappa : December 21, 1940.
2. Birthyear1. Birthday
2. Birthyear 3. Birthplace
Zappa was born in 1940 in Baltimore. 
Figure 6: Cross-Field Bootstrapping: In step (1) The birthday,
December 21, is extracted and the text marked. In step 2, cooc-
currences with the discovered birthday make 1940 a better can-
didate for birthyear. In step (3), the discovered birthyear ap-
pears in contexts where the discovered birthday does not and
improves extraction of birth place.
Post?F
usion 
Accur
acy
# Training Documents Per Person 0.2
 0.3 0.4
 0.5 0.6
 0.7 0.8
 0.9
 0
 20
 40  60  80  100  120 140 160
BirthdayBirthyearDeathyearOccupationBirthplace
Figure 7: Fusion accuracy doesn?t improve with more than 30
training documents per person.
have either little or no gain from training on addi-
tional documents. There are two possible reasons
why more training data may not help, and even may
hurt performance.
One possibility is that higher ranked retrieved
documents are more likely to contain biographical
facts, while in later documents it is more likely that
automatically annotated training instances are in fact
false positives. That is, higher ranked documents are
cleaner training data. Pre-Fusion precision results
(Figure 8) support this hypothesis since it appears
that later instances are often contaminating earlier
models.
Pre?F
usion 
Precis
ion
# Training Documents Per Person 0
 0.1 0.2 0.3
 0.4 0.5 0.6
 0.7 0.8
 0  20  40  60  80  100 120 140 160
BirthdayBirthyearBirthplace
OccupationDeathyear
Figure 8: Pre-Fusion precision shows slight drops with in-
creased training documents.
The data in Figure 9 suggest an alternate possibil-
ity that later documents also shift the prior toward
a model where it is less likely that a relationship is
observed as fewer targets are extracted.
Pre?F
usion 
Pseud
o?Rec
all
# Training Documents Per Person 0
 1 2 3
 4 5 6
 7 8 9
 10 11
 0
 20  40
 60  80  100  120 140 160
Birthday
BirthplaceDeathyear
Birthyear
Occupation
Figure 9: Pre-Fusion Pseudo-Recall also drops with increased
training documents.
6 Related Work
The closest related work to the task of biographic
fact extraction was done by Cowie et al (2000) and
Schiffman et al (2001), who explore the problem of
biographic summarization.
There has been rather limited published
work in multi-document information extrac-
tion. The closest work to what we present here is
Masterson and Kushmerick (2003), who perform
multi-document information extraction trained on
manually annotated training data and use Best
Confidence to resolve each particular template slot.
In summarizarion, many systems have examined
the multi-document case. Notable systems are
SUMMONS (Radev and McKeown, 1998) and
RIPTIDE (White et al, 2001), which assume per-
fect extracted information and then perform closed
domain summarization. Barzilay et al (1999) does
not explicitly extract facts, but instead picks out
relevant repeated elements and combines them to
obtain a summary which retains the semantics of
the original.
In recent question answering research, informa-
tion fusion has been used to combine multiple
candidate answers to form a consensus answer.
Clarke et al (2001) use frequency of n-gram occur-
rence to pick answers for particular questions. An-
other example of answer fusion comes in (Brill et
al., 2001) which combines the output of multiple
question answering systems in order to rank an-
swers. Dalmas and Webber (2004) use a WordNet
cover heuristic to choose an appropriate location
from a large candidate set of answers.
There has been a considerable amount of work in
training information extraction systems from anno-
tated data since the mid-90s. The initial work in the
field used lexico-syntactic template patterns learned
using a variety of different empirical approaches
(Riloff and Schmelzenbach, 1998; Huffman, 1995;
489
Soderland et al, 1995). Seymore et al (1999) use
HMMs for information extraction and explore ways
to improve the learning process.
Nahm and Mooney (2002) suggest a method to
learn word-to-word relationships across fields by do-
ing data mining on information extraction results.
Prager et al (2004) uses knowledge of birth year to
weed out candidate years of death that are impos-
sible. Using the CRF extractors in our data set,
this heuristic did not yield any improvement. More
distantly related work for multi-field extraction sug-
gests methods for combining information in graphi-
cal models across multiple extraction instances (Sut-
ton et al, 2004; Bunescu and Mooney, 2004) .
7 Conclusion
This paper has presented new experimental method-
ologies and results for cross-document information
fusion, focusing on the task of biographic fact ex-
traction and has proposed a new method for cross-
field bootstrapping. In particular, we have shown
that automatic annotation can be used effectively
to train statistical information extractors such Na??ve
Bayes and CRFs, and that CRF extraction accuracy
can be improved by 5% with a negative example
model. We looked at cross-document fusion and
demonstrated that voting outperforms choosing the
highest confidence extracted information by 2% to
20%. Finally, we introduced a cross-field bootstrap-
ping method that improved average accuracy by 7%.
References
E. Agichtein and L. Gravano. 2000. Snowball: Extracting re-
lations from large plain-text collections. In Proceedings of
ICDL, pages 85?94.
R. Barzilay, K. R. McKeown, and M. Elhadad. 1999. Informa-
tion fusion in the context of multi-document summarization.
In Proceedings of ACL, pages 550?557.
E. Brill, J. Lin, M. Banko, S. Dumais, and A. Ng. 2001. Data-
intensive question answering. In Proceedings of TREC,
pages 183?189.
S. Brin. 1998. Extracting patterns and relations from the world
wide web. In WebDB Workshop at 6th International Confer-
ence on Extending Database Technology, EDBT?98, pages
172?183.
R. Bunescu and R. Mooney. 2004. Collective information ex-
traction with relational markov networks. In Proceedings of
ACL, pages 438?445.
C. L. A. Clarke, G. V. Cormack, and T. R. Lynam. 2001. Ex-
ploiting redundancy in question answering. In Proceedings
of SIGIR, pages 358?365.
J. Cowie, S. Nirenburg, and H. Molina-Salgado. 2000. Gener-
ating personal profiles. In The International Conference On
MT And Multilingual NLP.
T. Dalmas and B. Webber. 2004. Information fusion
for answering factoid questions. In Proceedings of 2nd
CoLogNET-ElsNET Symposium. Questions and Answers:
Theoretical Perspectives.
D. Freitag and A. McCallum. 1999. Information extraction
with hmms and shrinkage. In Proceedings of the AAAI-99
Workshop on Machine Learning for Information Extraction,
pages 31?36.
S. B. Huffman. 1995. Learning information extraction patterns
from examples. In Working Notes of the IJCAI-95 Workshop
on New Approaches to Learning for Natural Language Pro-
cessing, pages 127?134.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and la-
beling sequence data. In Proceedings of ICML, pages 282?
289.
T. R. Leek. 1997. Information extraction using hidden markov
models. Master?s Thesis, UC San Diego.
D. Masterson and N. Kushmerick. 2003. Information ex-
traction from multi-document threads. In Proceedings of
ECML-2003: Workshop on Adaptive Text Extraction and
Mining, pages 34?41.
A. McCallum. 2002. Mallet: A machine learning for language
toolkit.
U. Nahm and R. Mooney. 2002. Text mining with information
extraction. In Proceedings of the AAAI 2220 Spring Sympo-
sium on Mining Answers from Texts and Knowledge Bases,
pages 60?67.
J. Prager, J. Chu-Carroll, and K. Czuba. 2004. Question an-
swering by constraint satisfaction: Qa-by-dossier with con-
straints. In Proceedings of ACL, pages 574?581.
D. R. Radev and K. R. McKeown. 1998. Generating natural
language summaries from multiple on-line sources. Compu-
tational Linguistics, 24(3):469?500.
D. Ravichandran and E. Hovy. 2002. Learning surface text
patterns for a question answering system. In Proceedings of
ACL, pages 41?47.
E. Riloff and M. Schmelzenbach. 1998. An empirical ap-
proach to conceptual case frame acquisition. In Proceedings
of WVLC, pages 49?56.
E. Riloff. 1996. Automatically Generating Extraction Patterns
from Untagged Text. In Proceedings of AAAI, pages 1044?
1049.
B. Schiffman, I. Mani, and K. J. Concepcion. 2001. Produc-
ing biographical summaries: Combining linguistic knowl-
edge with corpus statistics. In Proceedings of ACL, pages
450?457.
K. Seymore, A. McCallum, and R. Rosenfeld. 1999. Learning
hidden markov model structure for information extraction.
In AAAI?99 Workshop on Machine Learning for Information
Extraction, pages 37?42.
S. Soderland, D. Fisher, J. Aseltine, and W. Lehnert. 1995.
CRYSTAL: Inducing a conceptual dictionary. In Proceed-
ings of IJCAI, pages 1314?1319.
C. Sutton, K. Rohanimanesh, and A. McCallum. 2004. Dy-
namic conditional random fields: factorize probabilistic
models for labeling and segmenting sequence data. In Pro-
ceedings of ICML.
M. White, T. Korelsky, C. Cardie, V. Ng, D. Pierce, and
K. Wagstaff. 2001. Multi-document summarization via in-
formation extraction. In Proceedings of HLT.
490
Proceedings of ACL-08: HLT, pages 425?433,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Unsupervised Translation Induction for Chinese Abbreviations
using Monolingual Corpora
Zhifei Li and David Yarowsky
Department of Computer Science and Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD 21218, USA
zhifei.work@gmail.com and yarowsky@cs.jhu.edu
Abstract
Chinese abbreviations are widely used in
modern Chinese texts. Compared with
English abbreviations (which are mostly
acronyms and truncations), the formation of
Chinese abbreviations is much more complex.
Due to the richness of Chinese abbreviations,
many of them may not appear in available par-
allel corpora, in which case current machine
translation systems simply treat them as un-
known words and leave them untranslated. In
this paper, we present a novel unsupervised
method that automatically extracts the relation
between a full-form phrase and its abbrevia-
tion from monolingual corpora, and induces
translation entries for the abbreviation by us-
ing its full-form as a bridge. Our method does
not require any additional annotated data other
than the data that a regular translation system
uses. We integrate our method into a state-of-
the-art baseline translation system and show
that it consistently improves the performance
of the baseline system on various NIST MT
test sets.
1 Introduction
The modern Chinese language is a highly abbrevi-
ated one due to the mixed use of ancient single-
character words with modern multi-character words
and compound words. According to Chang and Lai
(2004), approximately 20% of sentences in a typical
news article have abbreviated words in them. Ab-
breviations have become even more popular along
with the development of Internet media (e.g., online
chat, weblog, newsgroup, and so on). While En-
glish words are normally abbreviated by either their
Full-form Abbreviation Translation
&? ? ?? Hong Kong Governor
?\ ?/? ??? Security Council
Figure 1: Chinese Abbreviations Examples
first letters (i.e. acronyms) or via truncation, the for-
mation of Chinese abbreviations is much more com-
plex. Figure 1 shows two examples for Chinese ab-
breviations. Clearly, an abbreviated form of a word
can be obtained by selecting one or more characters
from this word, and the selected characters can be at
any position in the word. In an extreme case, there
are even re-ordering between a full-form phrase and
its abbreviation.
While the research in statistical machine trans-
lation (SMT) has made significant progress, most
SMT systems (Koehn et al, 2003; Chiang, 2007;
Galley et al, 2006) rely on parallel corpora to extract
translation entries. The richness and complexness
of Chinese abbreviations imposes challenges to the
SMT systems. In particular, many Chinese abbrevi-
ations may not appear in available parallel corpora,
in which case current SMT systems treat them as
unknown words and leave them untranslated. This
affects the translation quality significantly.
To be able to translate a Chinese abbreviation that
is unseen in available parallel corpora, one may an-
notate more parallel data. However, this is very
expensive as there are too many possible abbrevia-
tions and new abbreviations are constantly created.
Another approach is to transform the abbreviation
425
into its full-form for which the current SMT system
knows how to translate. For example, if the baseline
system knows that the translation for ?&??? is
?Hong Kong Governor?, and it also knows that ??
?? is an abbreviation of ?&? ?? , then it can
translate ???? to ?Hong Kong Governor?.
Even if an abbreviation has been seen in parallel
corpora, it may still be worth to consider its full-
form phrase as an additional alternative to the ab-
breviation since abbreviated words are normally se-
mantically ambiguous, while its full-form contains
more context information that helps the MT system
choose a right translation for the abbreviation.
Conceptually, the approach of translating an ab-
breviation by using its full-form as a bridge in-
volves four components: identifying abbreviations,
learning their full-forms, inducing their translations,
and integrating the abbreviation translations into the
baseline SMT system. None of these components is
trivial to realize. For example, for the first two com-
ponents, we may need manually annotated data that
tags an abbreviation with its full-form. We also need
to make sure that the baseline system has at least
one valid translation for the full-form phrase. On
the other hand, integrating an additional component
into a baseline SMT system is notoriously tricky as
evident in the research on integrating word sense
disambiguation (WSD) into SMT systems: different
ways of integration lead to conflicting conclusions
on whether WSD helps MT performance (Chan et
al., 2007; Carpuat and Wu, 2007).
In this paper, we present an unsupervised ap-
proach to translate Chinese abbreviations. Our ap-
proach exploits the data co-occurrence phenomena
and does not require any additional annotated data
except the parallel and monolingual corpora that the
baseline SMT system uses. Moreover, our approach
integrates the abbreviation translation component
into the baseline system in a natural way, and thus is
able to make use of the minimum-error-rate training
(Och, 2003) to automatically adjust the model pa-
rameters to reflect the change of the integrated sys-
tem over the baseline system. We carry out experi-
ments on a state-of-the-art SMT system, i.e., Moses
(Koehn et al, 2007), and show that the abbreviation
translations consistently improve the translation per-
formance (in terms of BLEU (Papineni et al, 2002))
on various NIST MT test sets.
2 Background: Chinese Abbreviations
In general, Chinese abbreviations are formed based
on three major methods: reduction, elimination and
generalization (Lee, 2005; Yin, 1999). Table 1
presents examples for each category.
Among the three methods, reduction is the most
popular one, which generates an abbreviation by
selecting one or more characters from each of the
words in the full-form phrase. The selected char-
acters can be at any position of the word. Table 1
presents examples to illustrate how characters at dif-
ferent positions are selected to generate abbrevia-
tions. While the abbreviations mostly originate from
noun phrases (in particular, named entities), other
general phrases are also abbreviatable. For example,
the second example ?Save Energy? is a verb phrase.
In an extreme case, reordering may happen between
an abbreviation and its full-form phrase. For exam-
ple, for the seventh example in Table 1, a monotone
abbreviation should be ?X??, however, ?X
?? is a more popular ordering in Chinese texts.
In elimination, one or more words of the origi-
nal full-form phrase are eliminated and the rest parts
remain as an abbreviation. For example, in the full-
form phrase ?8?L??, the word ?L?? is elim-
inated and the remaining word ?8?? alone be-
comes the abbreviation.
In generalization, an abbreviation is created
by generalizing parallel sub-parts of the full-form
phrase. For example, ??3 (three preventions)? in
Table 1 is an abbreviation for the phrase ?3?3
x3b//? (fire prevention, theft prevention,
and traffic accident prevention)?. The character ?3
(prevention)? is common to the three sub-parts of the
full-form, so it is being generalized.
3 Unsupervised Translation Induction for
Chinese Abbreviations
In this section, we describe an unsupervised method
to induce translation entries for Chinese abbrevia-
tions, even when these abbreviations never appear in
the Chinese side of the parallel corpora. Our basic
idea is to automatically extract the relation between
a full-form phrase and its abbreviation (we refer the
relation as full-abbreviation) from monolingual cor-
pora, and then induce translation entries for the ab-
breviation by using its full-form phrase as a bridge.
426
Category Full-form Abbreviation Translation
Reduction ?? L? ?L Peking University
?? ? ? Save Energy
&? ? ?? Hong Kong Governor
ib \? i? Foreign Minister
|? ?? ?? People?s Police
?\ ?/? ??? Security Council
? X ?? X? No.1 Nuclear Energy Power Plant
Elimination 8? L? 8? Tsinghua University
Generalization 3?3x3b//? ?3 Three Preventions
Table 1: Chinese Abbreviation: Categories and Examples
Our approach involves five major steps:
? Step-1: extract a list of English entities from
English monolingual corpora;
? Step-2: translate the list into Chinese using a
baseline translation system;
? Step-3: extract full-abbreviation relations from
Chinese monolingual corpora by treating the
Chinese translations obtained in Step-2 as full-
form phrases;
? Step-4: induce translation entries for Chinese
abbreviations by using their full-form phrases
as bridges;
? Step-5: augment the baseline system with
translation entries obtained in Step-4.
Clearly, the main purpose of Step-1 and -2 is to
obtain a list of Chinese entities, which will be treated
as full-form phrases in Step-3. One may use a named
entity tagger to obtain such a list. However, this re-
lies on the existence of a Chinese named entity tag-
ger with high-precision. Moreover, obtaining a list
using a dedicated tagger does not guarantee that the
baseline system knows how to translate the list. On
the contrary, in our approach, since the Chinese en-
tities are translation outputs for the English entities,
it is ensured that the baseline system has translations
for these Chinese entities.
Regarding the data resource used, Step-1, -2, and
-3 rely on the English monolingual corpora, paral-
lel corpora, and the Chinese monolingual corpora,
respectively. Clearly, our approach does not re-
quire any additional annotated data compared with
the baseline system. Moreover, our approach uti-
lizes both Chinese and English monolingual data
to help MT, while most SMT systems utilizes only
the English monolingual data to build a language
model. This is particularly interesting since we nor-
mally have enormous monolingual data, but a small
amount of parallel data. For example, in the transla-
tion task between Chinese and English, both the Chi-
nese and English Gigaword have billions of words,
but the parallel data has only about 30 million words.
Step-4 and -5 are natural ways to integrate the ab-
breviation translation component with the baseline
translation system. This is critical to make the ab-
breviation translation get performance gains over the
baseline system as will be clear later.
In the remainder of this section, we will present a
specific instantiation for each step.
3.1 English Entity Extraction from English
Monolingual Corpora
Though one can exploit a sophisticated named-entity
tagger to extract English entities, in this paper we
identify English entities based on the capitalization
information. Specifically, to be considered as an en-
tity, a continuous span of English words must satisfy
the following conditions:
? all words must start from a capital letter except
for function words ?of?, ?the?, and ?and?;
? each function word can appear only once;
? the number of words in the span must be
smaller than a threshold (e.g., 10);
? the occurrence count of this span must be
greater than a threshold (e.g., 1).
427
3.2 English Entity Translation
For the Chinese-English language pair, most MT re-
search is on translation from Chinese to English, but
here we need the reverse direction. However, since
most of statistical translation models (Koehn et al,
2003; Chiang, 2007; Galley et al, 2006) are sym-
metrical, it is relatively easy to train a translation
system to translate from English to Chinese, except
that we need to train a Chinese language model from
the Chinese monolingual data.
It is worth pointing out that the baseline system
may not be able to translate all the English enti-
ties. This is because the entities are extracted from
the English monolingual corpora, which has a much
larger vocabulary than the English side of the par-
allel corpora. Therefore, we should remove all the
Chinese translations that contain any untranslated
English words before proceeding to the next step.
Moreover, it is desirable to generate an n-best list
instead of a 1-best translation for the English entity.
3.3 Full-abbreviation Relation Extraction from
Chinese Monolingual Corpora
We treat the Chinese entities obtained in Section 3.2
as full-form phrases. To identify their abbreviations,
one can employ an HMM model (Chang and Teng,
2006). Here we propose a much simpler approach,
which is based on the data co-occurrence intuition.
3.3.1 Data Co-occurrence
In a monolingual corpus, relevant words tend to
appear together (i.e., co-occurrence). For example,
Bill Gates tends to appear together with Microsoft.
The co-occurrence may imply a relationship (e.g.,
Bill Gates is the founder of Microsoft). By inspec-
tion of the Chinese text, we found that the data
co-occurrence phenomena also applies to the full-
Title ?????*R?<??
Text c???2?9??(V?c?
?)?20?????{?*R?
h?-10?t8??????.
??t*y ?{??
Table 2: Data Co-occurrence Example for the Full-
abbreviation Relation (????,???) meaning
?winter olympics?
abbreviation relation. Table 2 shows an example,
where the abbreviation ????? appears in the title
while its full-form ?????? appears in the text
of the same document. In general, the occurrence
distance between an abbreviation and its full-form
varies. For example, they may appear in the same
sentence, or in the neighborhood sentences.
3.3.2 Full-abbreviation Relation Extraction
Algorithm
By exploiting the data co-occurrence phenom-
ena, we identify possible abbreviations for full-form
phrases. Figure 2 presents the pseudocode of the
full-abbreviation relation extraction algorithm.
Relation-Extraction(Corpus ,Full-list)
1 contexts ? NIL
2 for i ? 1 to length[Corpus]
3 sent1 ? Corpus[i ]
4 contexts ? UPDATE(contexts ,Corpus , i)
5 for full in sent1
6 if full in Full-list
7 for sent2 in contexts
8 for abbr in sent2
9 if RL(full , abbr ) = TRUE
10 Count[abbr , full]++
11 return Count
Figure 2: Full-abbreviation Relation Extraction
Given a monolingual corpus and a list of full-form
phrases (i.e., Full-list, which is obtained in Sec-
tion 3.2), the algorithm returns a Count that con-
tains full-abbreviation relations and their occurrence
counts. Specifically, the algorithm linearly scans
over the whole corpus as indicated by line 1. Along
the linear scan, the algorithm maintains contexts of
the current sentence (i.e., sent1), and the contexts
remember the sentences from where the algorithm
identifies possible abbreviations. In our implemen-
tation, the contexts include current sentence, the ti-
tle of current document, and previous and next sen-
tence in the document. Then, for each ngram (i.e.,
full) of the current sentence (i.e., sent1) and for each
ngram (i.e., abbr) of a context sentence (i.e., sent2),
the algorithm calls a function RL, which decides
whether the full-abbreviation relation holds between
full and abbr. If RL returns TRUE, the count table
428
(i.e., Count) is incremented by one for this relation.
Note that the filtering through the full-form phrases
list (i.e., Full-list) as shown in line 6 is the key to
make the algorithm efficient enough to run through
large-size monolingual corpora.
In function RL, we run a simple alignment algo-
rithm that links the characters in abbr with the words
in full. In the alignment, we assume that there is no
reordering between full and abbr. To be considered
as a valid full-abbreviation relation, full and abbr
must satisfy the following conditions:
? abbr must be shorter than full by a relative
threshold (e.g., 1.2);
? each character in abbr must be aligned to full;
? each word in full must have at least one charac-
ter aligned to abbr;
? abbr must not be a continuous sub-part of full;
Clearly, due to the above conditions, our approach
may not be able to handle all possible abbreviations
(e.g., the abbreviations formed by the generalization
method described in Section 2). One can modify
the conditions and the alignment algorithm to handle
more complex full-abbreviation relations.
With the count table Count, we can calculate the
relative frequency and get the following probability,
P (full|abbr) = Count[abbr, full]?Count[abbr, ?] (1)
3.4 Translation Induction for Chinese
Abbreviations
Given a Chinese abbreviation and its full-form, we
induce English translation entries for the abbrevia-
tion by using the full-form as a bridge. Specifically,
we first generate n-best translations for each full-
form Chinese phrase using the baseline system.1 We
then post-process the translation outputs such that
they have the same format (i.e., containing the same
set of model features) as a regular phrase entry in
1In our method, it is guaranteed that each Chinese full-form
phrase will have at least one English translation, i.e., the En-
glish entity that has been used to produce this full-form phrase.
However, it does not mean that this English entity is the best
translation that the baseline system has for the Chinese full-
form phrase. This is mainly due to the asymmetry introduced
by the different LMs in different translation directions.
the baseline phrase table. Once we get the transla-
tion entries for the full-form, we can replace the full-
form Chinese with its abbreviation to generate trans-
lation entries for the abbreviation. Moreover, to deal
with the case that an abbreviation may have several
candidate full-form phrases, we normalize the fea-
ture values using the following equation,
?j(e, abbr) = ?j(e, full)? P (full|abbr) (2)
where e is an English translation, and ?j is the j-th
model feature indexed as in the baseline system.
3.5 Integration with Baseline Translation
System
Since the obtained translation entries for abbrevia-
tions have the same format as the regular transla-
tion entries in the baseline phrase table, it is rela-
tively easy to add them into the baseline phrase ta-
ble. Specifically, if a translation entry (signatured by
its Chinese and English strings) to be added is not in
the baseline phrase table, we simply add the entry
into the baseline table. On the other hand, if the en-
try is already in the baseline phrase table, then we
merge the entries by enforcing the translation prob-
ability as we obtain the same translation entry from
two different knowledge sources (one is from par-
allel corpora and the other one is from the Chinese
monolingual corpora).
Once we obtain the augmented phrase table, we
should run the minimum-error-rate training (Och,
2003) with the augmented phrase table such that the
model parameters are properly adjusted. As will be
shown in the experimental results, this is critical to
obtain performance gain over the baseline system.
4 Experimental Results
4.1 Corpora
We compile a parallel dataset which consists of var-
ious corpora distributed by the Linguistic Data Con-
sortium (LDC) for NIST MT evaluation. The paral-
lel dataset has about 1M sentence pairs, and about
28M words. The monolingual data we use includes
the English Gigaword V2 (LDC2005T12) and the
Chinese Gigaword V2 (LDC2005T14).
4.2 Baseline System Training
Using the toolkit Moses (Koehn et al, 2007), we
built a phrase-based baseline system by following
429
the standard procedure: running GIZA++ (Och and
Ney, 2000) in both directions, applying refinement
rules to obtain a many-to-many word alignment, and
then extracting and scoring phrases using heuristics
(Och and Ney, 2004). The baseline system has eight
feature functions (see Table 8). The feature func-
tions are combined under a log-linear framework,
and the weights are tuned by the minimum-error-rate
training (Och, 2003) using BLEU (Papineni et al,
2002) as the optimization metric.
To handle different directions of translation be-
tween Chinese and English, we built two tri-
gram language models with modified Kneser-Ney
smoothing (Chen and Goodman, 1998) using the
SRILM toolkit (Stolcke, 2002).
4.3 Statistics on Intermediate Steps
As described in Section 3, our approach involves
five major steps. Table 3 reports the statistics for
each intermediate step. While about 5M English en-
tities are extracted and 2-best Chinese translations
are generated for each English entity, we get only
4.7M Chinese entities. This is because many of the
English entities are untranslatable by the baseline
system. The number of full-abbreviation relations2
extracted from the Chinese monolingual corpora is
51K. For each full-form phrase we generate 5-best
English translations, however only 210k (<51K?5)
translation entries are obtained. This is because the
baseline system may have less than 5 unique trans-
lations for some of the full-form phrases. Lastly, the
number of translation entries added due to abbrevi-
ations is very small compared with the total number
of translation entries (i.e., 50M).
Measure Value
number of English entities 5M
number of Chinese entities 4.7M
number of full-abbreviation relations 51K
number of translation entries added 210K
total number of translation entries 50M
Table 3: Statistics on Intermediate Steps
2Note that many of the ?abbreviations? extracted by our al-
gorithm are not true abbreviations in the linguistic sense, instead
they are just continuous-span of words. This is analogous to the
concept of ?phrase? in phrase-based MT.
4.4 Precision on Full-abbreviation Relations
Table 4 reports the precision on the extracted full-
abbreviation relations. We classify the relations into
several classes based on their occurrence counts. In
the second column, we list the fraction of the rela-
tions in the given class among all the relations we
have extracted (i.e., 51K relations). For each class,
we randomly select 100 relations, manually tag them
as correct or wrong, and then calculate the precision.
Intuitively, a class that has a higher occurrence count
should have a higher precision, and this is generally
true as shown in the fourth column of Table 4. In
comparison, Chang and Teng (2006) reports a preci-
sion of 50% over relations between single-word full-
forms and single-character abbreviations. One can
imagine a much lower precision on general relations
(e.g., the relations between multi-word full-forms
and multi-character abbreviations) that we consider
here. Clearly, our results are very competitive3.
Count Fraction (%) Precision (%)Baseline Ours
(0, 1] 35.2 8.9 42.6
(1, 5] 33.8 7.8 54.4
(5, 10] 10.7 8.9 60.0
(10, 100] 16.5 7.6 55.9
(100,+?) 3.8 12.1 59.9
Average Precision (%) 8.4 51.3
Table 4: Full-abbreviation Relation Extraction Precision
To further show the advantage of our relation ex-
traction algorithm (see Section 3.3), in the third col-
umn of Table 4 we report the results on a simple
baseline. To create the baseline, we make use of the
dominant abbreviation patterns shown in Table 5,
which have been reported in Chang and Lai (2004).
The abbreviation pattern is represented using the
format ?(bit pattern|length)? where the bit pattern
encodes the information about how an abbreviated
form is obtained from its original full-form word,
and the length represents the number of characters in
the full-form word. In the bit pattern, a ?1? indicates
that the character at the corresponding position of
the full-form word is kept in the abbreviation, while
a ?0? means the character is deleted. Now we dis-
3However, it is not a strict comparison because the dataset is
different and the recall may also be different.
430
Pattern Fraction (%) Example
(1|1) 100 (?,?)
(10|2) 87 (??,?)
(101|3) 44 (?/?,??)
(1010|4) 56 (??=?,?=)
Table 5: Dominant Abbreviation Patterns reported in
Chang and Lai (2004)
cuss how to create the baseline. For each full-form
phrase in the randomly selected relations, we gener-
ate a baseline hypothesis (i.e., abbreviation) as fol-
lows. We first generate an abbreviated form for each
word in the full-form phrase by using the dominant
abbreviation pattern, and then concatenate these ab-
breviated words to form a baseline abbreviation for
the full-form phrase. As shown in Table 4, the base-
line performs significantly worse than our relation
extraction algorithm. Compared with the baseline,
our relation extraction algorithm allows arbitrary ab-
breviation patterns as long as they satisfy the align-
ment constraints. Moreover, our algorithm exploits
the data co-occurrence phenomena to generate and
rank hypothesis (i.e., abbreviation). The above two
reasons explain the large performance gain.
It is interesting to examine the statistics on abbre-
viation patterns over the relations automatically ex-
tracted by our algorithm. Table 6 reports the statis-
tics. We obtain the statistics on the relations that
are manually tagged as correct before, and there are
in total 263 unique words in the corresponding full-
form phrases. Note that the results here are highly
biased to our relation extraction algorithm (see Sec-
tion 3.3). For the statistics on manually collected
examples, please refer to Chang and Lai (2004).
4.5 Results on Translation Performance
4.5.1 Precision on Translations of Chinese
Full-form Phrases
For the relations manually tagged as correct in
Section 4.4, we manually look at the top-5 transla-
tions for the full-form phrases. If the top-5 transla-
tions contain at least one correct translation, we tag
it as correct, otherwise as wrong. We get a precision
of 97.5%. This precision is extremely high because
the BLEU score (precision with brevity penalty) that
one obtains for a Chinese sentence is normally be-
tween 30% to 50%. Two reasons explain such a high
Pattern Fraction (%) Example
(1|1) 100 (?,?)
(10|2) 74.3 (??,?)
(01|2) 7.6 (??,?)
(11|2) 18.1 ( j, j)
(100|3) 58.5 (n.,)
(010|3) 3.1 (qu?,u)
(001|3) 4.6 (???,?)
(110|3) 13.8 (???,??)
(101|3) 3.1 (?/?,??)
(111|3) 16.9 ()?,)?)
Table 6: Statistics on Abbreviation Patterns
precision. Firstly, the full-form phrase is short com-
pared with a regular Chinese sentence, and thus it is
easier to translate. Secondly, the full-form phrase it-
self contains enough context information that helps
the system choose a right translation for it. In fact,
this shows the importance of considering the full-
form phrase as an additional alternative to the ab-
breviation even if the baseline system already has
translation entries for the abbreviation.
4.5.2 BLEU on NIST MT Test Sets
We use MT02 as the development set4 for mini-
mum error rate training (MERT) (Och, 2003). The
MT performance is measured by lower-case 4-gram
BLEU (Papineni et al, 2002). Table 7 reports the re-
sults on various NIST MT test sets. As shown in the
table, our Abbreviation Augmented MT (AAMT)
systems perform consistently better than the base-
line system (described in Section 4.2).
Task Baseline AAMTNo MERT With MERT
MT02 29.87 29.96 30.46
MT03 29.03 29.23 29.71
MT04 29.05 29.88 30.55
Average Gain +0.52 +1.18
Table 7: MT Performance measured by BLEU Score
As clear in Table 7, it is important to re-run MERT
(on MT02 only) with the augmented phrase table
in order to get performance gains. Table 8 reports
4On the dev set, about 20K (among 210K) abbreviation
translation entries are matched in the Chinese side.
431
the MERT weights with different phrase tables. One
may notice the change of the weight in word penalty
feature. This is very intuitive in order to prevent the
hypothesis being too long due to the expansion of
the abbreviations into their full-forms.
Feature Baseline AAMT
language model 0.137 0.133
phrase translation 0.066 0.023
lexical translation 0.061 0.078
reverse phrase translation 0.059 0.103
reverse lexical translation 0.112 0.090
phrase penalty -0.150 -0.162
word penalty -0.327 -0.356
distortion model 0.089 0.055
Table 8: Weights obtained by MERT
5 Related Work
Though automatically extracting the relations be-
tween full-form Chinese phrases and their abbrevi-
ations is an interesting and important task for many
natural language processing applications (e.g., ma-
chine translation, question answering, information
retrieval, and so on), not much work is available
in the literature. Recently, Chang and Lai (2004),
Chang and Teng (2006), and Lee (2005) have in-
vestigated this task. Specifically, Chang and Lai
(2004) describes a hidden markov model (HMM) to
model the relationship between a full-form phrase
and its abbreviation, by treating the abbreviation as
the observation and the full-form words as states in
the model. Using a set of manually-created full-
abbreviation relations as training data, they report
experimental results on a recognition task (i.e., given
an abbreviation, the task is to obtain its full-form, or
the vice versa). Clearly, their method is supervised
because it requires the full-abbreviation relations as
training data.5 Chang and Teng (2006) extends the
work in Chang and Lai (2004) to automatically ex-
tract the relations between full-form phrases and
their abbreviations. However, they have only con-
sidered relations between single-word phrases and
single-character abbreviations. Moreover, the HMM
model is computationally-expensive and unable to
exploit the data co-occurrence phenomena that we
5However, the HMM model aligns the characters in the ab-
breviation to the words in the full-form in an unsupervised way.
have exploited efficiently in this paper. Lee (2005)
gives a summary about how Chinese abbreviations
are formed and presents many examples. Manual
rules are created to expand an abbreviation to its full-
form, however, no quantitative results are reported.
None of the above work has addressed the Chi-
nese abbreviation issue in the context of a machine
translation task, which is the primary goal in this
paper. To the best of our knowledge, our work is
the first to systematically model Chinese abbrevia-
tion expansion to improve machine translation.
The idea of using a bridge (i.e., full-form) to ob-
tain translation entries for unseen words (i.e., abbre-
viation) is similar to the idea of using paraphrases in
MT (see Callison-Burch et al (2006) and references
therein) as both are trying to introduce generaliza-
tion into MT. At last, the goal that we aim to exploit
monolingual corpora to help MT is in-spirit similar
to the goal of using non-parallel corpora to help MT
as aimed in a large amount of work (see Munteanu
and Marcu (2006) and references therein).
6 Conclusions
In this paper, we present a novel method that
automatically extracts relations between full-form
phrases and their abbreviations from monolingual
corpora, and induces translation entries for these ab-
breviations by using their full-form as a bridge. Our
method is scalable enough to handle large amount
of monolingual data, and is essentially unsupervised
as it does not require any additional annotated data
than the baseline translation system. Our method
exploits the data co-occurrence phenomena that is
very useful for relation extractions. We integrate our
method into a state-of-the-art phrase-based baseline
translation system, i.e., Moses (Koehn et al, 2007),
and show that the integrated system consistently im-
proves the performance of the baseline system on
various NIST machine translation test sets.
Acknowledgments
We would like to thank Yi Su, Sanjeev Khudan-
pur, Philip Resnik, Smaranda Muresan, Chris Dyer
and the anonymous reviewers for their helpful com-
ments. This work was partially supported by the De-
fense Advanced Research Projects Agency?s GALE
program via Contract No
?
HR0011-06-2-0001.
432
References
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne, 2006. Improved Statistical Machine Translation
Using Paraphrases. In Proceedings of NAACL 2006,
pages 17-24.
Marine Carpuat and Dekai Wu. 2007. Improving Statis-
tical Machine Translation using Word Sense Disam-
biguation. In Proceedings of EMNLP 2007, pages 61-
72.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word Sense Disambiguation Improves Statistical Ma-
chine Translation. In Proceedings of ACL 2007, pages
33-40.
Jing-Shin Chang and Yu-Tso Lai. 2004. A preliminary
study on probabilistic models for Chinese abbrevia-
tions. In Proceedings of the 3rd SIGHAN Workshop on
Chinese Language Processing, pages 9-16.
Jing-Shin Chang and Wei-Lun Teng. 2006. Mining
Atomic Chinese Abbreviation Pairs: A Probabilistic
Model for Single Character Word Recovery. In Pro-
ceedings of the 5rd SIGHAN Workshop on Chinese
Language Processing, pages 17-24.
Stanley F. Chen and Joshua Goodman. 1998. An empiri-
cal study of smoothing techniques for language mod-
eling. Technical Report TR-10-98, Harvard University
Center for Research in Computing Technology.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201-228.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of COLING/ACL 2006, pages 961-968.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan,Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
strantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of ACL, Demonstration Session, pages 177-180.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of NAACL 2003, pages 48-54.
H.W.D Lee. 2005. A study of automatic expansion of
Chinese abbreviations. MA Thesis, The University of
Hong Kong.
Dragos Stefan Munteanu and Daniel Marcu. 2006. Ex-
tracting Parallel Sub-Sentential Fragments from Non-
Parallel Corpora. In Proceedings of ACL 2006, pages
81-88.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL
2003, pages 160-167.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of ACL
2000, pages 440-447.
Franz Josef Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30:417-449.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of ACL
2002, pages 311-318.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of the International
Conference on Spoken Language Processing, pages
901-904.
Z.P. Yin. 1999. Methodologies and principles of Chi-
nese abbreviation formation. In Language Teaching
and Study, 2:73-82.
433
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 710?718,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Modeling Latent Biographic Attributes in Conversational Genres
Nikesh Garera and David Yarowsky
Department of Computer Science, Johns Hopkins University
Human Language Technology Center of Excellence
Baltimore MD, USA
{ngarera,yarowsky}@cs.jhu.edu
Abstract
This paper presents and evaluates several
original techniques for the latent classifi-
cation of biographic attributes such as gen-
der, age and native language, in diverse
genres (conversation transcripts, email)
and languages (Arabic, English). First,
we present a novel partner-sensitive model
for extracting biographic attributes in con-
versations, given the differences in lexi-
cal usage and discourse style such as ob-
served between same-gender and mixed-
gender conversations. Then, we explore
a rich variety of novel sociolinguistic and
discourse-based features, including mean
utterance length, passive/active usage, per-
centage domination of the conversation,
speaking rate and filler word usage. Cu-
mulatively up to 20% error reduction is
achieved relative to the standard Boulis
and Ostendorf (2005) algorithm for classi-
fying individual conversations on Switch-
board, and accuracy for gender detection
on the Switchboard corpus (aggregate) and
Gulf Arabic corpus exceeds 95%.
1 Introduction
Speaker attributes such as gender, age, dialect, na-
tive language and educational level may be (a)
stated overtly in metadata, (b) derivable indirectly
from metadata such as a speaker?s phone number
or userid, or (c) derivable from acoustic proper-
ties of the speaker, including pitch and f0 contours
(Bocklet et al, 2008). In contrast, the goal of
this paper is to model and classify such speaker
attributes from only the latent information found
in textual transcripts. In particular, we are inter-
ested in modeling and classifying biographic at-
tributes such as gender and age based on lexi-
cal and discourse factors including lexical choice,
mean utterance length, patterns of participation
in the conversation and filler word usage. Fur-
thermore, a speaker?s lexical choice and discourse
style may differ substantially depending on the
gender/age/etc. of the speaker?s interlocutor, and
hence improvements may be achived via dyadic
modeling or stacked classifiers.
There has been substantial work in the sociolin-
guistics literature investigating discourse style dif-
ferences due to speaker properties such as gender
(Coates, 1997; Eckert, McConnell-Ginet, 2003).
Analyzing such differences is not only interesting
from the sociolinguistic and psycholinguistic point
of view of language understanding, but also from
an engineering perspective, given the goal of pre-
dicting latent author/speaker attributes in various
practical applications such as user authenticaion,
call routing, user and population profiling on so-
cial networking websites such as facebook, and
gender/age conditioned language models for ma-
chine translation and speech recogntition. While
most of the prior work in sociolinguistics has been
approached from a non-computational perspec-
tive, Koppel et al (2002) employed the use of a
linear model for gender classification with manu-
ally assigned weights for a set of linguistically in-
teresting words as features, focusing on a small de-
velopment corpus. Another computational study
for gender classification using approximately 30
weblog entries was done by Herring and Paolillo
(2006), making use of a logistic regression model
to study the effect of different features.
While small-scale sociolinguistic studies on
monologues have shed some light on important
features, we focus on modeling attributes from
spoken conversations, building upon the work of
710
Boulis and Ostendorf (2005) and show how gen-
der and other attributes can be accurately predicted
based on the following original contributions:
1. Modeling Partner Effect: A speaker may
adapt his or her conversation style depending
on the partner and we show how conditioning
on the predicted partner class using a stacked
model can provide further performance gains
in gender classification.
2. Sociolinguistic features: The paper explores
a rich set of lexical and non-lexical features
motivated by the sociolinguistic literature for
gender classification, and show how they
can effectively augment the standard ngram-
based model of Boulis and Ostendorf (2005).
3. Application to Arabic Language: We also re-
port results for Arabic language and show
that the ngram model gives reasonably high
accuracy for Arabic as well. Furthmore, we
also get consistent performance gains due to
partner effect and sociolingusic features, as
observed in English.
4. Application to Email Genre: We show how
the models explored in this paper extend to
email genre, showing the wide applicability
of general text-based features.
5. Application to new attributes: We show how
the lexical model of Boulis and Ostendorf
(2005) can be extended to Age and Native
vs. Non-native prediction, with further im-
provements gained from our partner-sensitive
models and novel sociolinguistic features.
2 Related Work
Much attention has been devoted in the sociolin-
guistics literature to detection of age, gender, so-
cial class, religion, education, etc. from conversa-
tional discourse and monologues starting as early
as the 1950s, making use of morphological fea-
tures such as the choice between the -ing and
the -in variants of the present participle ending
of the verb (Fisher, 1958), and phonological fea-
tures such as the pronounciation of the ?r? sound
in words such as far, four, cards, etc. (Labov,
1966). Gender differences has been one of the
primary areas of sociolinguistic research, includ-
ing work such as Coates (1998) and Eckert and
McConnell-Ginet (2003). There has also been
some work in developing computational models
based on linguistically interesting clues suggested
by the sociolinguistic literature for detecting gen-
der on formal written texts (Singh, 2001; Koppel
et al, 2002; Herring and Paolillo, 2006) but it has
been primarily focused on using a small number of
manually selected features, and on a small number
of formal written texts. Another relevant line of
work has been on the blog domain, using a bag of
words feature set to discriminate age and gender
(Schler et al, 2006; Burger and Henderson, 2006;
Nowson and Oberlander, 2006).
Conversational speech presents a challenging do-
main due to the interaction of genders, recognition
errors and sudden topic shifts. While prosodic fea-
tures have been shown to be useful in gender/age
classification (e.g. Shafran et al, 2003), their work
makes use of speech transcripts along the lines of
Boulis and Ostendorf (2005) in order to build a
general model that can be applied to electronic
conversations as well. While Boulis and Osten-
dorf (2005) observe that the gender of the part-
ner can have a substantial effect on their classifier
accuracy, given that same-gender conversations
are easier to classify than mixed-gender classifi-
cations, they don?t utilize this observation in their
work. In Section 5.3, we show how the predicted
gender/age etc. of the partner/interlocutor can
be used to improve overall performance via both
dyadic modeling and classifier stacking. Boulis
and Ostendorf (2005) have also constrained them-
selves to lexical n-gram features, while we show
improvements via the incorporation of non-lexical
features such as the percentage domination of the
conversation, degree of passive usage, usage of
subordinate clauses, speaker rate, usage profiles
for filler words (e.g. ?umm?), mean-utterance
length, and other such properties.
We also report performance gains of our models
for a new genre (email) and a new language (Ara-
bic), indicating the robustness of the models ex-
plored in this paper. Finally, we also explore and
evaluate original model performance on additional
latent speaker attributes including age and native
vs. non-native English speaking status.
3 Corpus Details
Consistent with Boulis and Ostendorf (2005), we
utilized the Fisher telephone conversation corpus
(Cieri et al, 2004) and we also evaluated per-
formance on the standard Switchboard conversa-
tional corpus (Godfrey et al, 1992), both collected
and annotated by the Linguistic Data Consortium.
In both cases, we utilized the provided metadata
711
(including true speaker gender, age, native lan-
guage, etc.) as only class labels for both train-
ing and evaluation, but never as features in the
classification. The primary task we employed was
identical to Boulis and Ostendorf (2005), namely
the classification of gender, etc. of each speaker
in an isolated conversation, but we also evaluate
performance when classifying speaker attributes
given the combination of multiple conversations
in which the speaker has participated. The Fisher
corpus contains a total of 11971 speakers and each
speaker participated in 1-3 conversations, result-
ing in a total of 23398 conversation sides (i.e. the
transcript of a single speaker in a single conversa-
tion). We followed the preprocessing steps and ex-
perimental setup of Boulis and Ostendorf (2005)
as closely as possible given the details presented
in their paper, although some details such as the
exact training/test partition were not currently ob-
tainable from either the paper or personal commu-
nication. This resulted in a training set of 9000
speakers with 17587 conversation sides and a test
set of 1000 speakers with 2008 conversation sides.
The Switchboard corpus was much smaller and
consisted of 543 speakers, with 443 speakers used
for training and 100 speakers used for testing, re-
sulting in a total of 4062 conversation sides for
training and 808 conversation sides for testing.
4 Modeling Gender via Ngram features
(Boulis and Ostendorf, 2005)
As our reference algorithm, we used the current
state-of-the-art system developed by Boulis and
Ostendorf (2005) using unigram and bigram fea-
tures in a SVM framework. We reimplemented
this model as our reference for gender classifica-
tion, further details of which are given below:
4.1 Training Vectors
For each conversation side, a training example was
created using unigram and bigram features with
tf-idf weighting, as done in standard text classi-
fication approaches. However, stopwords were re-
tained in the feature set as various sociolinguis-
tic studies have shown that use of some of the
stopwords, for instance, pronouns and determin-
ers, are correlated with age and gender. Also, only
the ngrams with frequency greater than 5 were re-
tained in the feature set following Boulis and Os-
tendorf (2005). This resulted in a total of 227,450
features for the Fisher corpus and 57,914 features
for the Switchboard corpus.
Female Male
Fisher Corpus
husband -0.0291 my wife 0.0366
my husband -0.0281 wife 0.0328
oh -0.0210 uh 0.0284
laughter -0.0186 ah 0.0248
have -0.0169 er 0.0222
mhm -0.0169 i i 0.0201
so -0.0163 hey 0.0199
because -0.0160 you doing 0.0169
and -0.0155 all right 0.0169
i know -0.0152 man 0.0160
hi -0.0147 pretty 0.0156
um -0.0141 i see 0.0141
boyfriend -0.0134 yeah i 0.0125
oh my -0.0124 my girlfriend 0.0114
i have -0.0119 thats thats 0.0109
but -0.0118 mike 0.0109
children -0.0115 guy 0.0109
goodness -0.0114 is that 0.0108
yes -0.0106 basically 0.0106
uh huh -0.0105 shit 0.0102
Switchboard Corpus
oh -0.0122 wife 0.0078
laughter -0.0088 my wife 0.0077
my husband -0.0077 uh 0.0072
husband -0.0072 i i 0.0053
have -0.0069 actually 0.0051
uhhuh -0.0068 sort of 0.0041
and i -0.0050 yeah i 0.0041
feel -0.0048 got 0.0039
umhum -0.0048 a 0.0038
i know -0.0047 sort 0.0037
really -0.0046 yep 0.0036
women -0.0043 the 0.0036
um -0.0042 stuff 0.0035
would -0.0039 yeah 0.0034
children -0.0038 pretty 0.0033
too -0.0036 that that 0.0032
but -0.0035 guess 0.0031
and -0.0034 as 0.0029
wonderful -0.0032 is 0.0028
yeah yeah -0.0031 i guess 0.0028
Table 1: Top 20 ngram features for gender, ranked by the
weights assigned by the linear SVM model
4.2 Model
After extracting the ngrams, a SVM model was
trained via the SVMlight toolkit (Joachims, 1999)
using the linear kernel with the default toolkit
settings. Table 1 shows the most discriminative
ngrams for gender based on the weights assigned
by the linear SVM model. It is interesting that
some of the gender-correlated words proposed by
sociolinguistics are also found by this empirical
approach, including the frequent use of ?oh? by fe-
males and also obvious indicators of gender such
as ?my wife? or ?my husband?, etc. Also, named
entity ?Mike? shows up as a discriminative uni-
gram, this maybe due to the self-introduction at
the beginning of the conversations and ?Mike?
being a common male name. For compatibility
with Boulis and Ostendorf (2005), no special pre-
712
Figure 1: The effect of varying the amount of each con-
versation side utilized for training, based on the utilized % of
each conversation (starting from their beginning).
processing for names is performed, and they are
treated as just any other unigrams or bigrams1.
Furthermore, the ngram-based approach scales
well with varying the amount of conversation uti-
lized in training the model as shown in Figure 1.
The ?Boulis and Ostendorf, 05? rows in Table 3
show the performance of this reimplemented al-
gorithm on both the Fisher (90.84%) and Switch-
board (90.22%) corpora, under the identical train-
ing and test conditions used elsewhere in our paper
for direct comparison with subsequent results2.
5 Effect of Partner?s Gender
Our original contribution in this section is the suc-
cessful modeling of speaker properties (e.g. gen-
der/age) based on the prior and joint modeling of
the partner speaker?s gender/age in the same dis-
course. The motivation here is that people tend
to use stronger gender-specific, age-specific or
dialect-specific word/phrase usage and discourse
properties when speaking with someone of a sim-
ilar gender/age/dialect than when speaking with
someone of a different gender/age/dialect, when
they may adapt a more neutral speaking style.
Also, discourse properties such as relative use
of the passive and percentage of the conversa-
tion dominated may vary depending on the gen-
der or age relationship with the speaking partner.
We employ several varieties of classifier stacking
and joint modeling to be effectively sensitive to
these differences. To illustrate the significance of
1A natural extension of this work, however, would be to
do explicit extraction of self introductions and then do table-
lookup-based gender classification, although we did not do
so for consistency with the reference algorithm.
2The modest differences with their reported results may
be due to unreported details such as the exact training/test
splits or SVM parameterizations, so for the purposes of as-
sessing the relative gain of our subsequent enhancements
we base all reported experiments on the internally-consistent
configurations as (re-)implemented here.
Fisher Corpus
Same gender conversations 94.01
Mixed gender conversations 84.06
Switchboard Corpus
Same gender conversations 93.22
Mixed gender conversations 86.84
Table 2: Difference in Gender classification accuracy be-
tween mixed gender and same gender conversations using the
reference algorithm
Classifying speaker?s and partner?s
gender simultaneously
Male-Male 84.80
Female-Female 81.96
Male-Female 15.58
Female-Male 27.46
Table 3: Performance for 4-way classification of the entire
conversation into (mm, ff, mf, fm) classes using the reference
algorithm on Switchboard corpus.
the ?partner effect?, Table 2 shows the difference
in the standard algorithm performance between
same-gender conversations (when gender-specific
style flourishes) and mixed-gender conversations
(where more neutral styles are harder to classify).
Table 3 shows the classwise performance of classi-
fying the entire conversation into four possible cat-
egories. We can see that the mixed-gender cases
are also significantly harder to classify on a con-
versation level granularity.
5.1 Oracle Experiment
To assess the potential gains from full exploita-
tion of partner-sensitive modeling, we first report
the result from an oracle experiment, where we
assume we know whether the conversation is ho-
mogeneous (same gender) or heterogeneous (dif-
ferent gender). In order to effectively utilize this
information, we classify both the test conversa-
tion side and the partner side, and if the classi-
fier is more confident about the partner side then
we choose the gender of the test conversation side
based on the heterogeneous/homogeneous infor-
mation. The overall accuracy improves to 96.46%
on the Fisher corpus using this oracle (from
90.84%), leading us to the experiment where the
oracle is replaced with a non-oracle SVM model
trained on a subset of training data such that all test
conversation sides (of the speaker and the partner)
are excluded from the training set.
5.2 Replacing Oracle by a Homogeneous vs
Heterogenous Classifier
Given the substantial improvement using the Or-
acle information, we initially trained another bi-
713
nary classifier for classifying the conversation as
mixed or single-gender. It turns out that this task
is much harder than the single-side gender clas-
sification, task and achieved only a low accuracy
value of 68.35% on the Fisher corpus. Intuitively,
the homogeneous vs. hetereogeneous partition re-
sults in a much harder classification task because
the two diverse classes of male-male and female-
female conversations are grouped into one class
(?homogeneous?) resulting in linearly insepara-
ble classes3. This subsequently lead us to create
two different classifiers for conversations, namely,
male-male vs rest and female-female vs rest4 used
in a classifier combination framework as follows:
5.3 Modeling partner via conditional model
and whole-conversation model
The following classifiers were trained and each of
their scores was used as a feature in a meta SVM
classifier:
1. Male-Male vs Rest: Classifying the entire
conversation (using test speaker and partner?s
sides) as male-male or other5.
2. Female-Female vs Rest: Classifying the en-
tire conversation (using test speaker and part-
ner?s sides) as female-female or other.
3. Conditional model of gender given most
likely partner?s gender: Two separate clas-
sifiers were trained for classifying the gen-
der of a given conversation side, one where
the partner is male and other where the part-
ner is female. Given a test conversation side,
we first choose the most likely gender of the
partner?s conversation side using the ngram-
based model6 and then choose the gender of
the test conversation side using the appropri-
ate conditional model.
4. Ngram model as explained in Section 4.
The row labeled ?+ Partner Model? in Table 4
shows the performance gain obtained via this
meta-classifier incorporating conversation type
and partner-conditioned models.
3Even non-linear kernels were not able to find a good clas-
sification boundary
4We also explored training a 3-way classifier, male-male,
female-female, mixed and the results were similar to that of
the binarized setup
5For classifying the conversations as male-male vs rest or
female-female vs rest, all the conversations with either the
speaker or the partner present in any of the test conversations
were eliminated from the training set, thus creating a disjoint
training and test conversation partitions.
6All the partner conversation sides of test speakers were
removed from the training data and the ngram-based model
was retrained on the remaining subset.
Figure 2: Empirical differences in sociolinguistic features
for Gender on the Switchboard corpus
6 Incorporating Sociolinguistic Features
The sociolinguistic literature has shown gender
differences for speakers due to features such as
speaking rate, pronoun usage and filler word us-
age. While ngram features are able to reason-
ably predict speaker gender due to their high detail
and coverage and the overall importance of lexical
choice in gender differences while speaking, the
sociolinguistics literature suggests that other non-
lexical features can further help improve perfor-
mance, and more importantly, advance our under-
standing of gender differences in discourse. Thus,
on top of the standard Boulis and Ostendorf (2005)
model, we also investigated the following features
motivated by the sociolinguistic literature on gen-
der differences in discourse (Macaulay, 2005):
1. % of conversation spoken: We measured the
speaker?s fraction of conversation spoken via
three features extracted from the transcripts:
% of words, utterances and time.
2. Speaker rate: Some studies have shown that
males speak faster than females (Yuan et
al., 2006) as can also be observed in Fig-
ure 2 showing empirical data obtained from
Switchboard corpus. The speaker rate was
measured in words/sec., using starting and
ending time-stamps for the discourse.
3. % of pronoun usage: Macaulay (2005) argues
that females tend to use more third-person
male/female pronouns (he, she, him, her and
his) as compared to males.
4. % of back-channel responses such as
?(laughter)? and ?(lipsmacks)?.
5. % of passive usage: Passives were detected
by extracting a list of past-participle verbs
from Penn Treebank and using occurences of
?form of ?to be? + past participle?.
714
6. % of short utterances (<= 3 words).
7. % of modal auxiliaries, subordinate clauses.
8. % of ?mm? tokens such as ?mhm?, ?um?,
?uh-huh?, ?uh?, ?hm?, ?hmm?,etc.
9. Type-token ratio
10. Mean inter-utterance time: Avg. time taken
between utterances of the same speaker.
11. % of ?yeah? occurences.
12. % of WH-question words.
13. % Mean word and utterance length.
The above classes resulted in a total of 16 sociolin-
guistic features which were added based on feature
ablation studies as features in the meta SVM clas-
sifier along with the 4 features as explained previ-
ously in Section 5.3.
The rows in Table 4 labeled ?+ (any sociolinguis-
tic feature)? show the performance gain using the
respective features described in this section. Each
row indicates an additive effect in the feature ab-
lation, showing the result of adding the current so-
ciolinguistic feature with the set of features men-
tioned in the rows above.
7 Gender Classification Results
Table 4 combines the results of the experiments re-
ported in the previous sections, assessed on both
the Fisher and Switchboard corpora for gender
classification. The evaluation measure was the
standard classifier accuracy, that is, the fraction of
test conversation sides whose gender was correctly
predicted. Baseline performance (always guessing
female) yields 57.47% and 51.6% on Fisher and
Switchboard respectively. As noted before, the
standard reference algorithm is Boulis and Osten-
dorf (2005), and all cited relative error reductions
are based on this established standard, as imple-
mented in this paper. Also, as a second reference,
performance is also cited for the popular ?Gender
Genie?, an online gender-detector7, based on the
manually weighted word-level sociolinguistic fea-
tures discussed in Argamon et al (2003). The ad-
ditional table rows are described in Sections 4-6,
and cumulatively yield substantial improvements
over the Boulis and Ostendorf (2005) standard.
7.1 Aggregating results over per-speaker via
consensus voting
While Table 4 shows results for classifying the
gender of the speaker on a per conversation ba-
sis (to be consistent and enable fair comparison
7http://bookblog.net/gender/genie.php
Model Acc. Error
Reduc.
Fisher Corpus (57.5% of sides are female)
Gender Genie 55.63 -384%
Ngram (Boulis & Ostendorf, 05) 90.84 Ref.
+ Partner Model 91.28 4.80%
+ % of ?yeah? 91.33
+ % of (laughter) 91.38
+ % of short utt. 91.43
+ % of auxiliaries 91.48
+ % of subord-clauses, ?mm? 91.58
+ % of Participation (in utt.) 91.63
+ % of Passive usage 91.68 9.17%
Switchboard Corpus (51.6% of sides are female)
Gender Genie 55.94 -350%
Ngram (Boulis & Ostendorf, 05) 90.22 Ref.
+ Partner Model 91.58 13.91%
+ Speaker rate, % of fillers 91.71
+ Mean utt. len., % of Ques. 91.96
+ % of Passive usage 92.08
+ % of (laughter) 92.20 20.25%
Table 4: Results showing improvement in accuracy of gen-
der classifier using partner-model and sociolinguistic features
Model Acc. Error
Reduc.
Fisher Corpus
Ngram (Boulis & Ostendorf, 05) 90.50 Ref.
+ Partner Model 91.60 11.58%
+ Socioling. Features 91.70 12.63%
Switchboard Corpus
Ngram (Boulis & Ostendorf, 05) 92.78 Ref.
+ Partner Model 93.81 14.27%
+ Socioling. Features 96.91 57.20%
Table 5: Aggregate results on a ?per-speaker? basis via ma-
jority consensus on different conversations for the respective
speaker. The results on Switchboard are significantly higher
due to more conversations per speaker as compared to the
Fisher corpus
with the work reported by Boulis and Ostendorf
(2005)), all of the above models can be easily
extended to per-speaker evaluation by pooling in
the predictions from multiple conversations of the
same speaker. Table 5 shows the result of each
model on a per-speaker basis using a majority vote
of the predictions made on the individual conver-
sations of the respective speaker. The consen-
sus model when applied to Switchboard corpus
show larger gains as it has 9.38 conversations per
speaker on average as compared to 1.95 conversa-
tions per speaker on average in Fisher. The results
715
on Switchboard corpus show a very large reduc-
tion in error rate of more than 57% with respect to
the standard algorithm, further indicating the use-
fulness of the partner-sensitive model and richer
sociolinguistic features when more conversational
evidence is available.
8 Application to Arabic Language
It would be interesting to see how the Boulis and
Ostendorf (2005) model along with the partner-
based model and sociolinguistic features would
extend to a new language. We used the LDC Gulf
Arabic telephone conversation corpus (Linguistic
Data Consortium, 2006). The training set con-
sisted of 499 conversations, and the test set con-
sisted of 200 conversations. Each speaker partic-
ipated in only one conversation, resulting in the
same number of training/test speakers as conver-
sations, and thus there was no overlap in speak-
ers/partners between training and test sets. Only
non-lexical sociolinguistic features were used for
Arabic in addition to the ngram features. The re-
sults for Arabic are shown in table 6. Based on
prior distribution, always guessing the most likely
class for gender (?male?) yielded 52.5% accuracy.
We can see that the Boulis and Ostendorf (2005)
model gives a reasonably high accuracy in Arabic
as well. More importantly, we also see consistent
performance gains via partner modeling and so-
ciolinguistic features, indicating the robustness of
these models and achieving final accuracy of 96%.
9 Application to Email Genre
A primary motivation for using only the speaker
transcripts as compared to also using acoustic
properties of the speaker (Bocklet et al, 2008) was
to enable the application of the models to other
new genres. In order to empirically support this
motivation, we also tested the performance of the
models explored in this paper on the Enron email
corpus (Klimt and Yang, 2004). We manually an-
notated the sender?s gender on a random collec-
tion of emails taken from the corpus. The resulting
training and test sets after preprocessing for header
information, reply-to?s, forwarded messages con-
sisted of 1579 and 204 emails respectively.
In addition to ngram features, a subset of so-
ciolinguistic features that could be extracted for
email were also utilized. Based on the prior dis-
tribution, always guessing the most likely class
(?male?) resulted in 63.2% accuracy. We can see
from Table 7 that the Boulis and Ostendorf (2005)
Model Acc. Error
Reduc.
Gulf Arabic (52.5% sides are male)
Ngram (Boulis & Ostendorf, 05) 92.00 Ref.
+ Partner Model 95.00
+ Mean word len. 95.50
+ Mean utt. len. 96.00 50.00%
Table 6: Gender classification results for a new
language (Gulf Arabic) showing consistent im-
provement gains via partner-model and sociolin-
guistic features.
Model Acc. Error
Reduc.
Enron Email Corpus (63.2% sides are male)
Ngram (Boulis & Ostendorf, 05) 76.78 Ref.
+ % of subor-claus., Mean 80.19
word len., Type-token ratio
+ % of pronouns. 80.50 16.02%
Table 7: Application of Ngram model and soci-
olinguistic features for gender classification in a
new genre (Email)
model based on lexical features yields a reason-
able performance with further improvements due
to the addition of sociolingustic features, resulting
in 80.5% accuracy.
10 Application to New Attributes
While gender has been studied heavily in the lit-
erature, other speaker attributes such as age and
native/non-native status also correlate highly with
lexical choice and other non-lexical features. We
applied the ngram-based model of Boulis and Os-
tendorf (2005) and our improvements using our
partner-sensitive model and richer sociolinguistic
features for a binary classification of the age of the
speaker, and classifying into native speaker of En-
glish vs non-native.
Corpus details for Age and Native Language:
For age, we used the same training and test speak-
ers from Fisher corpus as explained for gender in
section 3 and binarized into greater-than or less-
than-or-equal-to 40 for more parallel binary eval-
uation. For predicting native/non-native status, we
used the 1156 non-native speakers in the Fisher
corpus and pooled them with a randomly selected
equal number of native speakers. The training and
test partitions consisted of 2000 and 312 speakers
respectively, resulting in 3267 conversation sides
for training and 508 conversation sides for testing.
716
Age >= 40 Age < 40
well 0.0330 im thirty -0.0266
im forty 0.0189 actually -0.0262
thats right 0.0160 definitely -0.0226
forty 0.0158 like -0.0223
yeah well 0.0153 wow -0.0189
uhhuh 0.0148 as well -0.0183
yeah right 0.0144 exactly -0.0170
and um 0.0130 oh wow -0.0143
im fifty 0.0126 everyone -0.0137
years 0.0126 i mean -0.0132
anyway 0.0123 oh really -0.0128
isnt 0.0118 mom -0.0112
daughter 0.0117 im twenty -0.0110
well i 0.0116 cool -0.0108
in fact 0.0116 think that -0.0107
whether 0.0111 so -0.0107
my daughter 0.0111 mean -0.0106
pardon 0.0110 pretty -0.0106
gee 0.0109 thirty -0.0105
know laughter 0.0105 hey -0.0103
this 0.0102 right now -0.0100
oh 0.0102 cause -0.0096
young 0.0100 im actually -0.0096
in 0.0100 my mom -0.0096
when they 0.0100 kinda -0.0095
Table 8: Top 25 ngram features for Age ranked by weights
assigned by the linear SVM model
Results for Age and Native/Non-Native:
Based on the prior distribution, always guessing
the most likely class for age ( age less-than-or-
equal-to 40) results in 62.59% accuracy and al-
ways guessing the most likely class for native lan-
guage (non-native) yields 50.59% accuracy.
Table 9 shows the results for age and native/non-
native speaker status. We can see that the ngram-
based approach for gender also gives reasonable
performance on other speaker attributes, and more
importantly, both the partner-model and sociolin-
guistic features help in reducing the error rate on
age and native language substantially, indicating
their usefulness not just on gender but also on
other diverse latent attributes.
Table 8 shows the most discriminative ngrams for
binary classification of age, it is interesting to see
the use of ?well? right on top of the list for older
speakers, also found in the sociolinguistic studies
for age (Macaulay, 2005). We also see that older
speakers talk about their children (?my daughter?)
and younger speakers talk about their parents (?my
mom?), the use of words such as ?wow?, ?kinda?
and ?cool? is also common in younger speakers.
To give maximal consistency/benefit to the Boulis
and Ostendorf (2005) n-gram-based model, we did
not filter the self-reporting n-grams such as ?im
forty? and ?im thirty?, putting our sociolinguistic-
literature-based and discourse-style-based features
at a relative disadvantage.
Model Accuracy
Age (62.6% of sides have age <= 40)
Ngram Model 82.27
+ Partner Model 82.77
+ % of passive, mean inter-utt. time 83.02
, % of pronouns
+ % of ?yeah? 83.43
+ type/token ratio, + % of lipsmacks 83.83
+ % of auxiliaries, + % of short utt. 83.98
+ % of ?mm? 84.03
(Reduction in Error) (9.93%)
Native vs Non-native (50.6% of sides are non-native)
Ngram 76.97
+ Partner 80.31
+ Mean word length 80.51
(Reduction in Error) (15.37%)
Table 9: Results showing improvement in the accuracy of
age and native language classification using partner-model
and sociolinguistic features
11 Conclusion
This paper has presented and evaluated several
original techniques for the latent classification of
speaker gender, age and native language in diverse
genres and languages. A novel partner-sensitve
model shows performance gains from the joint
modeling of speaker attributes along with partner
speaker attributes, given the differences in lexical
usage and discourse style such as observed be-
tween same-gender and mixed-gender conversa-
tions. The robustness of the partner-model is sub-
stantially supported based on the consistent per-
formance gains achieved in diverse languages and
attributes. This paper has also explored a rich va-
riety of novel sociolinguistic and discourse-based
features, including mean utterance length, pas-
sive/active usage, percentage domination of the
conversation, speaking rate and filler word usage.
In addition to these novel models, the paper also
shows how these models and the previous work
extend to new languages and genres. Cumula-
tively up to 20% error reduction is achieved rel-
ative to the standard Boulis and Ostendorf (2005)
algorithm for classifying individual conversations
on Switchboard, and accuracy for gender detection
on the Switchboard corpus (aggregate) and Gulf
Arabic exceeds 95%.
Acknowledgements
We would like to thank Omar F. Zaidan for valu-
able discussions and feedback during the initial
stages of this work.
717
References
S. Argamon, M. Koppel, J. Fine, and A.R. Shimoni.
2003. Gender, genre, and writing style in formal
written texts. Text-Interdisciplinary Journal for the
Study of Discourse, 23(3):321?346.
T. Bocklet, A. Maier, and E. No?th. 2008. Age Determi-
nation of Children in Preschool and Primary School
Age with GMM-Based Supervectors and Support
Vector Machines/Regression. In Proceedings of
Text, Speech and Dialogue; 11th International Con-
ference, volume 1, pages 253?260.
C. Boulis and M. Ostendorf. 2005. A quantitative
analysis of lexical differences between genders in
telephone conversations. Proceedings of ACL, pages
435?442.
J.D. Burger and J.C. Henderson. 2006. An ex-
ploration of observable features related to blogger
age. In Computational Approaches to AnalyzingWe-
blogs: Papers from the 2006 AAAI Spring Sympo-
sium, pages 15?20.
C. Cieri, D. Miller, and K. Walker. 2004. The
Fisher Corpus: a resource for the next generations
of speech-to-text. In Proceedings of LREC.
J. Coates. 1998. Language and Gender: A Reader.
Blackwell Publishers.
Linguistic Data Consortium. 2006. Gulf Arabic Con-
versational Telephone Speech Transcripts.
P. Eckert and S. McConnell-Ginet. 2003. Language
and Gender. Cambridge University Press.
J.L. Fischer. 1958. Social influences on the choice of a
linguistic variant. Word, 14:47?56.
JJ Godfrey, EC Holliman, and J. McDaniel. 1992.
Switchboard: Telephone speech corpus for research
and development. Proceedings of ICASSP, 1.
S.C. Herring and J.C. Paolillo. 2006. Gender and
genre variation in weblogs. Journal of Sociolinguis-
tics, 10(4):439?459.
J. Holmes and M. Meyerhoff. 2003. The Handbook of
Language and Gender. Blackwell Publishers.
H. Jing, N. Kambhatla, and S. Roukos. 2007. Extract-
ing social networks and biographical facts from con-
versational speech transcripts. Proceedings of ACL,
pages 1040?1047.
B. Klimt and Y. Yang. 2004. Introducing the En-
ron corpus. In First Conference on Email and Anti-
Spam (CEAS).
M. Koppel, S. Argamon, and A.R. Shimoni. 2002.
Automatically Categorizing Written Texts by Au-
thor Gender. Literary and Linguistic Computing,
17(4):401?412.
W. Labov. 1966. The Social Stratification of English
in New York City. Center for Applied Linguistics,
Washington DC.
H. Liu and R. Mihalcea. 2007. Of Men, Women, and
Computers: Data-Driven Gender Modeling for Im-
proved User Interfaces. In International Conference
on Weblogs and Social Media.
R.K.S. Macaulay. 2005. Talk that Counts: Age, Gen-
der, and Social Class Differences in Discourse. Ox-
ford University Press, USA.
S. Nowson and J. Oberlander. 2006. The identity of
bloggers: Openness and gender in personal weblogs.
Proceedings of the AAAI Spring Symposia on Com-
putational Approaches to Analyzing Weblogs.
J. Schler, M. Koppel, S. Argamon, and J. Pennebaker.
2006. Effects of age and gender on blogging. Pro-
ceedings of the AAAI Spring Symposia on Computa-
tional Approaches to Analyzing Weblogs.
I. Shafran, M. Riley, and M. Mohri. 2003. Voice sig-
natures. Proceedings of ASRU, pages 31?36.
S. Singh. 2001. A pilot study on gender differences in
conversational speech on lexical richness measures.
Literary and Linguistic Computing, 16(3):251?264.
718
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 357?360,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
Arabic Cross-Document Coreference Detection
Asad Sayeed,
1,2
Tamer Elsayed,
1,2
Nikesh Garera,
1,6
David Alexander,
1,3
Tan Xu,
1,4
Douglas W. Oard,
1,4,5
David Yarowsky,
1,6
Christine Piatko
1
1
Human Language Technology Center of Excellence, Johns Hopkins University, Baltimore,
MD, USA?
2
Dept. of Computer Science, University of Maryland, College Park, MD,
USA?
3
BBN Technologies, Cambridge, MA, USA?
4
College of Information Studies,
University of Maryland, College Park, MD, USA?
5
UMIACS, University of Maryland, College
Park, MD, USA?
6
Dept. of Computer Science, Johns Hopkins University, Baltimore, MD, USA
{asayeed,telsayed}@cs.umd.edu, ngarera@cs.jhu.edu, dalexand@bbn.com,
{tanx,oard}@umd.edu, yarowsky@cs.jhu.edu, Christine.Piatko@jhuapl.edu
Abstract
We describe a set of techniques for Ara-
bic cross-document coreference resolu-
tion. We compare a baseline system of
exact mention string-matching to ones that
include local mention context information
as well as information from an existing
machine translation system. It turns out
that the machine translation-based tech-
nique outperforms the baseline, but local
entity context similarity does not. This
helps to point the way for future cross-
document coreference work in languages
with few existing resources for the task.
1 Introduction
Our world contains at least two noteworthy
George Bushes: President George H. W. Bush and
President George W. Bush. They are both fre-
quently referred to as ?George Bush.? If we wish
to use a search engine to find documents about
one of them, we are likely also to find documents
about the other. Improving our ability to find all
documents referring to one and none referring to
the other in a targeted search is a goal of cross-
document entity coreference detection. Here we
describe some results from a system we built to
perform this task on Arabic documents. We base
our work partly on previous work done by Bagga
and Baldwin (Bagga and Baldwin, 1998), which
has also been used in later work (Chen and Mar-
tin, 2007). Other work such as Lloyd et al (Lloyd,
2006) focus on techniques specific to English.
The main contribution of this work to cross-
document coreference lies in the conditions under
which it was done. Even now, there is no large-
scale resource?in terms of annotated data?for
cross-document coreference in Arabic as there is
in English (e.g. WebPeople (Artiles, 2008)). Thus,
we employed techniques for high-performance
processing in a resource-poor environment. We
provide early steps in cross-document coreference
detection for resource-poor languages.
2 Approach
We treat cross-document entities as a set of graphs
consisting of links between within-document enti-
ties. The graphs are disjoint. Each of our systems
produces a list of such links as within-document
entity pairs (A,B). We obtain within-document
entities by running the corpus through a within-
document coreference resolver?in this case, Serif
from BBN Technologies.
To create the entity clusters, we use a union-
find algorithm over the pairs. If links (A,B)
and (C,B) appear in the system output, then
{A,B,C} are one entity. Similarly, if (X,Y )
and (Z, Y ) appear in the output, then it will find
that {X,Y, Z} are one entity. If the algorithm
later discovers link (B,Z) in the system output, it
will decide that {A,B,C,X, Y, Z} are an entity.
This is efficiently implemented via a hash table
whose keys and values are both within-document
entity IDs, allowing the implementation of easily-
searched linked lists.
2.1 The baseline system
The baseline system uses a string matching cri-
terion to determine whether two within-document
entities are similar enough to be considered as part
of the same cross-document entity. Given within-
document entities A and B, the criterion is imple-
mented as follows:
1. Find the mention strings {a
1
, a
2
, . . .} and
357
{b
1
, b
2
, . . .} of A and B, respectively that are
the longest for that within-document entity
in the given document. (There may be more
than one longest mention of equal length for
a given entity.)
2. If any longest mention strings a
n
and b
m
exist
such that a
n
= b
m
(exact string match), then
A and B are considered to be part of the same
cross-document entity. Otherwise, they are
considered to be different entities.
When the system decides that two within-
document entities are connected as a single cross-
document entity, it emits a link between within-
document entities A and B represented as the pair
(A, B). We maintain a list of such links, but we
omit all links between within-document entities in
the same document.
The output of the system is a list of pairwise
links. The following two experimental systems
also produce lists of pairwise links. Union is per-
formed between the baseline system?s list and the
lists produced by the other systems to create lists
of pairs that include the information in the base-
line. However, each of the following systems?
outputs are merged separately with the baseline.
By including the baseline results in each system,
we are able to clarify the potential of each addi-
tional technique to improve performance over a
technique that is cheap to run under any circum-
stances, especially given that our experiments are
focused on increasing the number of links in an
Arabic context where links are likely to be dis-
rupted by spelling variations.
2.2 Translingual projection
We implement a novel cross-language approach
for Arabic coreference resolution by expanding
the space of exact match comparisons to approxi-
mate matches of English translations of the Arabic
strings. The intuition for this approach is that of-
ten the Arabic strings of the same named entity
may differ due to misspellings, titles, or aliases
that can be corrected in the English space. The
English translations were obtained using a stan-
dard statistical machine translation system (Chi-
ang, 2007; Li, 2008) and then compared using an
alias match.
The algorithm below describes the approach,
applied to any Arabic named entities that fail the
baseline string-match test:
1. For a given candidate Arabic named entity
pair (A,B), we project them into English by
translating the mentions using a standard sta-
tistical machine translation toolkit. Using the
projected English pair, say, (A
?
, B
?
) we per-
form the following tests to determine whether
A and B are co-referent:
(a) We do an exact string-match test in the
English space using the projected enti-
ties (A
?
, B
?
). The exact string match test
is done exactly as in the baseline system,
using the set of longest named entities in
their respective co-reference chains.
(b) If (A
?
, B
?
) fail in the exact string-match
test as in the baseline, then we test
whether they belong to a list of high con-
fidence co-referent named-entity pairs
1
precomputed for English using alias-
lists derived from Wikipedia.
(c) If (A
?
, B
?
) fails (a) and (b) then (A,B)
is deemed as non-coreferent.
While we hypothesize that translingual projection
via English should help in increasing recall since
it can work with non-exact string matches, it may
also help in increasing precision based on the as-
sumption that a name of American or English ori-
gin might have different variants in Arabic and that
translating to English can help in merging those
variants, as shown in figure 1.
 ????? ??????
 ????? 
?????? 
 ??????? 
 ???????
(Ms. Aisha)
(Aisha)
(Clenton)
(Clinton)
(Cilinton)
Aisha
Aisha
Clinton
Clinton
Clinton
Translate
via SMT
Figure 1: Illustration of translingual projection
method for resolving Arabic named entity strings
via English space. The English strings in paren-
theses indicate the literal glosses of the Arabic
strings prior to translation.
2.3 Entity context similarity
The context of mentions can play an important role
in merging or splitting potential coreferent men-
1
For example: (Sean Michael Waltman, Sean Waltman)
are high confidence-matches even though they are not an
exact-string match.
358
tions. We hypothesize that two mentions in two
different documents have a good chance of refer-
ring to the same entity if they are mentioned in
contexts that are topically very similar. A way of
representing a mention context is to consider the
words in the mention?s neighborhood. The con-
text of a mention can be defined as the words that
surround the mention in a window of n (50 in our
experiments) tokens centered by the mention. In
our experiments, we used highly similar contexts
to link mentions that might be coreferent.
Computing context similarity between every
pair of large number of mentions requires a highly
scalable and efficient mechanism. This can be
achieved using MapReduce, a distributed comput-
ing framework (Dean, 2004)
Elsayed et al (Elsayed, 2008) proposed an ef-
ficient MapReduce solution for the problem of
computing the pairwise similarity matrix in large
collections. They considered a ?bag-of-words?
model where similarity of two documents d
i
and d
j
is measured as follows: sim(d
i
, d
j
) =
?
t?d
i
?d
j
w
t,d
i
? w
t,d
j
, where w(t, d) is the weight
of term t in document d. A term contributes to
each pair that contains it. The list of documents
that contain a term is what is contained in the post-
ings of an inverted index. Thus, by processing
all postings, the pairwise similarity matrix can be
computed by summing term contributions. We use
the MapReduce framework for two jobs, inverted
indexing and pairwise similarity.
Elsayed et al suggested an efficient df-cut strat-
egy that eliminates terms that appear in many doc-
uments (having high df ) and thus contribute less
in similarity but cost in computation (e.g., a 99%
df-cut means that the most frequent 1% of the
terms were discarded). We adopted that approach
for computing similarities between the contexts
of two mentions. The processing unit was rep-
resented as a bag of n words in a window sur-
rounding each mention of a within-document en-
tity. Given a relatively small mention context, we
used a high df-cut value of 99.9%.
3 Experiments
We performed our experiments in the context of
the Automatic Content Extraction (ACE) eval-
uation of 2008, run by the National Institute
of Standards and Technology (NIST). The eval-
uation corpus contained approximately 10,000
documents from the following domains: broad-
cast conversation transcripts, broadcast news tran-
scripts, conversational telephone speech tran-
scripts, newswire, Usenet Newsgroup/Discussion
Groups, and weblogs. Systems were required to
process the large source sets completely. For per-
formance measurement after the evaluation, NIST
selected 412 of the Arabic source documents out
of the larger set (NIST, 2008).
For development purposes we used the NIST
ACE 2005 Arabic data with within-document
ground truth. This consisted of 1,245 documents.
We also used exactly 12,000 randomly selected
documents from the LDC Arabic Gigaword Third
Edition corpus, processed through Serif. The Ara-
bic Gigaword corpus was used to select a thresh-
old of 0.4956 for the context similarity technique
via inspection of (A,B) link scores by a native
speaker of Arabic.
It must be emphasized that there was no ground
truth available for this task in Arabic. Performing
this task in the absence of significant training or
evaluation data is one emphasis of this work.
3.1 Evaluation measures
We used NIST?s scoring techniques to evaluate the
performance of our systems. Scoring for the ACE
evaluation is done using an scoring script provided
by NIST which produces many kinds of statistics.
NIST mainly uses a measure called the ACE value,
but it also computes B-cubed.
B-Cubed represents the task of finding cross-
document entities in the following way: if a user
of the system is searching for a particular Bush
and finds document D, he or she should be able to
find all of the other documents with the same Bush
in them as links from D?that is, cross-document
entities represent graphs connecting documents.
Bagga and Baldwin are able to define precision,
recall, and F-measure over a collection of docu-
ments in this way.
The ACE Value represents a score similar to
B-Cubed, except that every mention and within-
document entity is weighted in NIST?s specifica-
tion by a number of factors. Every entity is worth 1
point, a missing entity worth 0, and attribute errors
are discounted by multiplying by a factor (0.75 for
CLASS, 0.5 for TYPE, and 0.9 for SUBTYPE).
Before scoring can be accomplished, the enti-
ties found by the system must be mapped onto
those found in the reference provided by NIST.
The ACE scorer does this document-by-document,
359
selecting the mapping that produces the highest
score. A description of the evaluation method and
entity categorization is available at (NIST, 2008).
3.2 Results and discussion
The results of running the ACE evaluation script
on the system output are shown in table 1. The
translingual projection system achieves higher
scores than all other systems on all measures. Al-
though it achieves only a 2 point improvement
over the baseline ACE value, it should be noted
that this represents a substantial number of at-
tributes per cross-document entity that it is getting
right.
Thresh B-Cubed ACE
System hold Prec Rec F Val.
Baseline 37.5 44.1 40.6 19.2
TrnsProj 38.4 44.8 41.3 21.2
CtxtSim 0.2 37.6 35.2 36.4 15.9
CtxtSim 0.3 37.4 43.8 40.3 18.9
CtxtSim 0.4 37.5 44.1 40.6 19.3
CtxtSim 0.4956 37.5 44.1 40.6 19.3
CtxtSim 0.6 37.5 44.1 40.6 19.2
Table 1: Scores from ACE evaluation script.
On the other hand, as the context similarity
threshold increases, we notice that the B-Cubed
measures reach identical values with the baseline
but never exceed it. But as it decreases, it loses
B-Cubed recall and ACE value.
While two within-document entities whose
longest mention strings match exactly and are le-
gitimately coreferent are likely to be mentioned in
the same contexts, it seems that a lower (more lib-
eral) threshold introduces spurious links and cre-
ates a different entity clustering.
Translingual projection appears to include links
that exact string matching in Arabic does not?
part of its purpose is to add close matches to those
found by exact string matching. It is able to in-
clude these links partly because it allows access to
resources in English that are not available for Ara-
bic such as Wikipedia alias lists.
4 Conclusions and Future Work
We have evaluated and discussed a set of tech-
niques for cross-document coreference in Arabic
that can be applied in the absence of significant
training and evaluation data. As it turns out, an
approach based on machine translation is slightly
better than a string-matching baseline, across all
measures. It worked by using translations from
Arabic to English in order to liberalize the string-
matching criterion, suggesting that using further
techniques via English to discover links may be
a fruitful future research path. This also seems
to suggest that a Bagga and Baldwin-style vector-
space model may not be the first approach to pur-
sue in future work on Arabic.
However, varying other parameters in the con-
text similarity approach should be tried in order
to gain a fuller picture of performance. One of
them is the df-cut of the MapReduce-based sim-
ilarity computation. Another is the width of the
word token window we used?we may have used
one that is too tight to be better than exact Arabic
string-matching.
References
Javier Artiles and Satoshi Sekine and Julio Gonzalo
2008. Web People Search?Results of the first eval-
uation and the plan for the second. WWW 2008.
A. Bagga and B. Baldwin. 1998. Entity-based cross-
document coreferencing using the vector space
model. COLING-ACL 1998.
Y. Chen and J. Martin. 2007. Towards robust unsuper-
vised personal name disambiguation. EMNLP.
D. Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2).
J. Dean and S. Ghemawat. 2004. MapReduce: Simpli-
fied Data Processing on Large Clusters. OSDI.
T. Elsayed and J. Lin and D. W. Oard. 2008. Pair-
wise Document Similarity in Large Collections with
MapReduce. ACL/HLT.
Z. Li and S. Khudanpur. 2008. A Scalable Decoder for
Parsing-based Machine Translation with Equivalent
Language Model State Maintenance. ACL SSST.
L. Lloyd and Andrew Mehler and Steven Skiena. 2006.
Identifying Co-referential Names Across Large Cor-
pora. Combinatorial Pattern Matching.
NIST. 2008. Automatic Content Extraction 2008 Eval-
uation Plan (ACE08).
360
Modeling Consensus: Classifier Combination
for Word Sense Disambiguation
Radu Florian and David Yarowsky
Department of Computer Science and
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218, USA
{rflorian,yarowsky}@cs.jhu.edu
Abstract
This paper demonstrates the substantial empirical
success of classifier combination for the word sense
disambiguation task. It investigates more than 10
classifier combination methods, including second
order classifier stacking, over 6 major structurally
different base classifiers (enhanced Na?ve Bayes,
cosine, Bayes Ratio, decision lists, transformation-
based learning and maximum variance boosted mix-
ture models). The paper also includes in-depth per-
formance analysis sensitive to properties of the fea-
ture space and component classifiers. When eval-
uated on the standard SENSEVAL1 and 2 data sets
on 4 languages (English, Spanish, Basque, and
Swedish), classifier combination performance ex-
ceeds the best published results on these data sets.
1 Introduction
Classifier combination has been extensively stud-
ied in the last decade, and has been shown to be
successful in improving the performance of diverse
NLP applications, including POS tagging (Brill and
Wu, 1998; van Halteren et al, 2001), base noun
phrase chunking (Sang et al, 2000), parsing (Hen-
derson and Brill, 1999) and word sense disambigua-
tion (Kilgarriff and Rosenzweig, 2000; Stevenson
and Wilks, 2001). There are several reasons why
classifier combination is useful. First, by consulting
the output of multiple classifiers, the system will im-
prove its robustness. Second, it is possible that the
problem can be decomposed into orthogonal feature
spaces (e.g. linguistic constraints and word occur-
rence statistics) and it is often better to train dif-
ferent classifiers in each of the feature spaces and
then combine their output, instead of designing a
complex system that handles the multimodal infor-
mation. Third, it has been shown by Perrone and
Cooper (1993) that it is possible to reduce the clas-
sification error by a factor of 

( is the number of
classifiers) by combination, if the classifiers? errors
are uncorrelated and unbiased.
The target task studied here is word sense disam-
biguation in the SENSEVAL evaluation framework
(Kilgarriff and Palmer, 2000; Edmonds and Cotton,
2001) with comparative tests in English, Spanish,
Swedish and Basque lexical-sample sense tagging
over a combined sample of 37730 instances of 234
polysemous words.
This paper offers a detailed comparative evalu-
ation and description of the problem of classifier
combination over a structurally and procedurally
diverse set of six both well established and orig-
inal classifiers: extended Na?ve Bayes, BayesRa-
tio, Cosine, non-hierarchical Decision Lists, Trans-
formation Based Learning (TBL), and the MMVC
classifiers, briefly described in Section 4. These
systems have different space-searching strategies,
ranging from discriminant functions (BayesRatio)
to data likelihood (Bayes, Cosine) to decision rules
(TBL, Decision Lists), and therefore are amenable
to combination.
2 Previous Work
Related work in classifier combination is discussed
throughout this article. For the specific task of
word sense disambiguation, the first empirical study
was presented in Kilgarriff and Rosenzweig (2000),
where the authors combined the output of the par-
ticipating SENSEVAL1 systems via simple (non-
weighted) voting, using either Absolute Majority,
Relative Majority, or Unanimous voting. Steven-
son and Wilks (2001) presented a classifier com-
bination framework where 3 disambiguation meth-
ods (simulated annealing, subject codes and selec-
tional restrictions) were combined using the TiMBL
memory-based approach (Daelemans et al, 1999).
Pedersen (2000) presents experiments with an en-
semble of Na?ve Bayes classifiers, which outper-
form all previous published results on two ambigu-
ous words (line and interest).
3 The WSD Feature Space
The feature space is a critical factor in classifier de-
sign, given the need to fuel the diverse strengths of
the component classifiers. Thus its quality is of-
ten highly correlated with performance. For this
                                            Association for Computational Linguistics.
                      Language Processing (EMNLP), Philadelphia, July 2002, pp. 25-32.
                         Proceedings of the Conference on Empirical Methods in Natural
An ancient stone church stands amid the fields,
the sound of bells ...
Feat. Type Word POS Lemma
Context ancient JJ ancient/J
Context stone NN stone/N
Context church NNP church/N
Context stands VBZ stand/V
Context amid IN amid/I
Context fields NN field/N
Context ... ... ...
Syntactic (predicate-argument) features
SubjectTo stands_Sbj VBZ stand_Sbj/V
Modifier stone_mod JJ ancient_mod/J
Ngram collocational features
-1 bigram stone_L JJ ancient_L/J
+1 bigram stands_R VBZ stand_R/V
1 trigram stone  stands JJVBZ stone/Jstands/V
... ... ... ...
Figure 1: Example sentence and extracted features from
the SENSEVAL2 word church
reason, we used a rich feature space based on raw
words, lemmas and part-of-speech (POS) tags in a
variety of positional and syntactical relationships to
the target word. These positions include traditional
unordered bag-of-word context, local bigram and
trigram collocations and several syntactic relation-
ships based on predicate-argument structure. Their
use is illustrated on a sample English sentence for
the target word church in Figure 1. While an exten-
sive evaluation of feature type to WSD performance
is beyond the scope of this paper, Section 6 sketches
an analysis of the individual feature contribution to
each of the classifier types.
3.1 Part-of-Speech Tagging and
Lemmatization
Part-of-speech tagger availability varied across the
languages that are studied here. An electronically
available transformation-based POS tagger (Ngai
and Florian, 2001) was trained on standard labeled
data for English (Penn Treebank), Swedish (SUC-
1 corpus), and Basque. For Spanish, an minimally
supervised tagger (Cucerzan and Yarowsky, 2000)
was used. Lemmatization was performed using an
existing trie-based supervised models for English,
and a combination of supervised and unsupervised
methods (Yarowsky and Wicentowski, 2000) for all
the other languages.
3.2 Syntactic Features
The syntactic features extracted for a target word
depend on the word?s part of speech:
 verbs: the head noun of the verb?s object, par-
ticle/preposition and prepositional object;
 nouns: the headword of any verb-object,
subject-verb or noun-noun relationships iden-
tified for the target word;
 adjectives: the head noun modified by the ad-
jective.
The extraction process was performed using heuris-
tic patterns and regular expressions over the parts-
of-speech surrounding the target word1.
4 Classifier Models for Word Sense
Disambiguation
This section briefly introduces the 6 classifier mod-
els used in this study. Among these models, the
Na?ve Bayes variants (NB henceforth) (Pedersen,
1998; Manning and Sch?tze, 1999) and Cosine dif-
fer slightly from off-the-shelf versions, and only the
differences will be described.
4.1 Vector-based Models: Enhanced Na?ve
Bayes and Cosine Models
Many of the systems used in this research share
a common vector representation, which captures
traditional bag-of-words, extended ngram and
predicate-argument features in a single data struc-
ture. In these models, a vector is created for each
document in the collection:   


 

 







, where 

is the number of times the feature


appears in document ,  is the number of words
in  and 

is a weight associated with the feature


2
. Confusion between the same word participat-
ing in multiple feature roles is avoided by append-
ing the feature values with their positional type (e.g.
stands_Sbj, ancient_L are distinct from stands and
ancient in unmarked bag-of-words context).
The notable difference between the extended
models and others described in the literature, aside
from the use of more sophisticated features than
the traditional bag-of-words, is the variable weight-
ing of feature types noted above. These differences
yield a boost in the NB performance (relative to ba-
sic Na?ve Bayes) of between 3.5% (Basque) and
10% (Spanish), with an average improvement of
7.25% over the four languages.
4.2 The BayesRatio Model
The BayesRatio model (BR henceforth) is a vector-
based model using the likelihood ratio framework
described in Gale et al (1992):
1The feature extraction on the in English data was per-
formed by first identifying text chunks, and then using heuris-
tics on the chunks to extract the syntactic information.
2The weight 

depends on the type of the feature 

: for
the bag-of-word features, this weight is inversely proportional
to the distance between the target word and the feature, while
for predicate-argument and extended ngram features it is a em-
pirically estimated weight (on a per language basis).
  

 	
 	
 

 	
 	


  	
  	
where  is the selected sense,  denotes documents
and  denotes features. By utilizing the binary ra-
tio for k-way modeling of feature probabilities, this
approach performs well on tasks where the data is
sparse.
4.3 The MMVC Model
The Mixture Maximum Variance Correction classi-
fier (MMVC henceforth) (Cucerzan and Yarowsky,
2002) is a two step classifier. First, the sense proba-
bility is computed as a linear mixture
  


    




   
where the probability  	 is estimated from
data and  	 is computed as a weighted normal-
ized similarity between the word 	 and the target
word 
 (also taking into account the distance in the
document between 	 and 
). In a second pass, the
sense whose variance exceeds a theoretically moti-
vated threshold is selected as the final sense label
(for details, see Cucerzan and Yarowsky (2002)).
4.4 The Discriminative Models
Two discriminative models are used in the exper-
iments presented in Section 5 - a transformation-
based learning system (TBL henceforth) (Brill,
1995; Ngai and Florian, 2001) and a non-
hierarchical decision lists system (DL henceforth)
(Yarowsky, 1996). For prediction, these systems
utilize local n-grams around the target word (up to
3 words/lemma/POS to the left/right), bag-of-words
and lemma/collocation (20 words around the tar-
get word, grouped by different window sizes) and
the syntactic features listed in Section 3.2.
The TBL system was modified to include redun-
dant rules that do not improve absolute accuracy on
training data in the traditional greedy training al-
gorithm, but are nonetheless positively correlated
with a particular sense. The benefit of this approach
is that predictive but redundant features in training
context may appear by themselves in new test con-
texts, improving coverage and increasing TBL base
model performance by 1-2%.
5 Models for Classifier Combination
One necessary property for success in combining
classifiers is that the errors produced by the com-
ponent classifiers should not be positively corre-
lated. On one extreme, if the classifier outputs are
0.0 0.2 0.4 0.6 0.8 1.0
 MMVC
 Cosine
 Bayes
 BayesRatio
 TBL
 DecisionLists
Figure 2: Empirically-derived classifier similarity
strongly correlated, they will have a very high inter-
agreement rate and there is little to be gained from
the joint output. On the other extreme, Perrone and
Cooper (1993) show that, if the errors made by the
classifiers are uncorrelated and unbiased, then by
considering a classifier that selects the class that
maximizes the posterior class probability average
  	

   	









 (1)
the error is reduced by a factor of 

. This case
is mostly of theoretical interest, since in practice
all the classifiers will tend to make errors on the
?harder? samples.
Figure 3(a) shows the classifier inter-agreement
among the six classifiers presented in Section 4, on
the English data. Only two of them, BayesRatio and
cosine, have an agreement rate of over 80%3, while
the agreement rate can be as low as 63% (BayesRa-
tio and TBL). The average agreement is 71.7%. The
fact that the classifiers? output are not strongly cor-
related suggests that the differences in performance
among them can be systematically exploited to im-
prove the overall classification. All individual clas-
sifiers have high stand-alone performance; each is
individually competitive with the best single SEN-
SEVAL2 systems and are fortuitously diverse in rel-
ative performance, as shown in Table 3(b). A den-
dogram of the similarity between the classifiers is
shown in Figure 2, derived using maximum linkage
hierarchical agglomerative clustering.
5.1 Major Types of Classifier Combination
There are three major types of classifier combina-
tion (Xu et al, 1992). The most general type is the
case where the classifiers output a posterior class
probability distribution for each sample (which can
be interpolated). In the second case, systems only
output a set of labels, together with a ordering of
preference (likelihood). In the third and most re-
strictive case, the classifications consist of just a sin-
gle label, without rank or probability. Combining
classifiers in each one of these cases has different
properties; the remainder of this section examines
models appropriate to each situation.
3The performance is measured using 5-fold cross validation
on training data.
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85





































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































Cosine
Bayes
TBL
DL
BayesRatio
Cl
as
sif
ie
r A
gg
re
em
en
t (
% 
of 
da
ta)
Bayes Cosine BayesRatio DL TBL MMVC
MMVC
(a) Classifier inter-agreement on SENSEVAL2
English data
System SENSEVAL1 SENSEVAL2
EN EN ES EU SV
Baseline 63.2 48.3 45.9 62.7 46.2
NB 80.4 65.7 67.9 71.2 66.7
BR 79.8 65.3 69.0 69.6 68.0
Cosine 74.0 62.2 65.9 66.0 66.4
DL 79.9 63.2 65.1 70.7 61.5
TBL 80.7 64.4 64.7 69.4 62.7
MMVC 81.1 66.7 66.7 69.7 61.9
(b) Individual classifier performance; best performers are
shown in bold
Figure 3: Individual Classifier Properties (cross-validation on SENSEVAL training data)
5.2 Combining the Posterior Sense Probability
Distributions
One of the simplest ways to combine the poste-
rior probability distributions is via direct averaging
(Equation (1)). Surprisingly, this method obtains
reasonably good results, despite its simplicity and
the fact that is not theoretically motivated under a
Bayes framework. Its success is highly dependent
on the condition that the classifiers? errors are un-
correlated (Tumer and Gosh, 1995).
The averaging method is a particular case of
weighted mixture:4
 	  



 
	   

	  





	   

	  (2)
where 

  is the weight assigned to the clas-
sifier  in the mixture and 


  is the poste-
rior probability distribution output by classifier ;
for 


  


we obtain Equation (1).
The mixture interpolation coefficients can be
computed at different levels of granularity. For
instance, one can make the assumption that
 
    
 and then the coefficients will
be computed at word level; if  
    
then the coefficients will be estimated on the entire
data.
One way to estimate these parameters is by linear
regression (Fuhr, 1989): estimate the coefficients
that minimize the mean square error (MSE)










 	  





	    	 






(3)
where  
  is the target vector of the cor-
rect classification of word 
 in document d:
4Note that we are computing a probability conditioned both
on the target word  and the document , because the docu-
ments are associated with a particular target word ; this for-
malization works mainly for the lexical choice task.
 
    ?  

 , 

being the goldstan-
dard sense of 
 in  and ? the Kronecker function:
? 
  

 if 
  
 if 
  
As shown in Fuhr (1989), Perrone and Cooper
(1993), the solution to the optimization problem (3)
can be obtained by solving a linear set of equations.
The resulting classifier will have a lower square er-
ror than the average classifier (since the average
classifier is a particular case of weighted mixture).
Another common method to compute the  pa-
rameters is by using the Expectation-Maximization
(EM) algorithm (Dempster et al, 1977). One
can estimate the coefficients such as to max-
imize the log-likelihood of the data,  




 

	 . In this particular opti-
mization problem, the search space is convex, and
therefore a solution exists and is unique, and it can
be obtained by the usual EM algorithm (see Berger
(1996) for a detailed description).
An alternative method for estimating the parame-
ters 

is to approximate them with the performance
of the th classifier (a performance-based combiner)
(van Halteren et al, 1998; Sang et al, 2000)



    

_is_correct
  (4)
therefore giving more weight to classifiers that have
a smaller classification error (the method will be re-
ferred to as PB). The probabilities in Equation (4)
are estimated directly from data, using the maxi-
mum likelihood principle.
5.3 Combination based on Order Statistics
In cases where there are reasons to believe that the
posterior probability distribution output by a clas-
sifier is poorly estimated5, but that the relative or-
dering of senses matches the truth, a combination
5For instance, in sparse classification spaces, the Na?ve
Bayes classifier will assign a probability very close to 1 to the
most likely sense, and close to 0 for the other ones.
strategy based on the relative ranking of sense pos-
terior probabilities is more appropriate. The sense
posterior probability can be computed as
 
  





  


 

	






  




  (5)
where the rank of a sense  is inversely proportional
to the number of senses that are (strictly) more prob-
able than sense :
	


 	 











 
	
 

 	





 
	

This method will tend to prefer senses that appear
closer to the top of the likelihood list for most of the
classifiers, therefore being more robust both in cases
where one classifier makes a large error and in cases
where some classifiers consistently overestimate the
posterior sense probability of the most likely sense.
5.4 The Classifier Republic: Voting
Some classification methods frequently used in
NLP directly minimize the classification error and
do not usually provide a probability distribution
over classes/senses (e.g. TBL and decision lists).
There are also situations where the user does not
have access to the probability distribution, such as
when the available classifier is a black-box that only
outputs the best classification. A very common
technique for combination in such a case is by vot-
ing (Brill and Wu, 1998; van Halteren et al, 1998;
Sang et al, 2000). In the simplest model, each clas-
sifier votes for its classification and the sense that
receives the most number of votes wins. The behav-
ior is identical to selecting the sense with the highest
posterior probability, computed as
 
  





   ?  


 








   ?  


  (6)
where ? is the Kronecker function and 


  is
the classification of the th classifier. The 

co-
efficients can be either equal (in a perfect classifier
democracy), or they can be estimated with any of
the techniques presented in Section 5.2. Section
6 presents an empirical evaluation of these tech-
niques.
Van Halteren et al (1998) introduce a modified
version of voting called TagPair. Under this model,
the conditional probability that the word sense is 
given that classifier  outputs 

and classifier  out-
puts 

,  


   

 


   

, is com-
puted on development data, and the posterior prob-
ability is estimated as
  	 



?  

 		 



?  
	
 		
(7)
where 
	
	   	


 

	   
	
	 .
Each classifier votes for its classification and every
pair of classifiers votes for the sense that is most
likely given the joint classification. In the experi-
ments presented in van Halteren et al (1998), this
method was the best performer among the presented
methods. Van Halteren et al (2001) extend this
method to arbitrarily long conditioning sequences,
obtaining the best published POS tagging results on
four corpora.
6 Empirical Evaluation
To empirically test the combination methods pre-
sented in the previous section, we ran experiments
on the SENSEVAL1 English data and data from four
SENSEVAL2 lexical sample tasks: English(EN),
Spanish(ES), Basque(EU) and Swedish(SV). Un-
less explicitly stated otherwise, all the results in the
following section were obtained by performing 5-
fold cross-validation6 . To avoid the potential for
over-optimization, a single final evaluation system
was run once on the otherwise untouched test data,
as presented in Section 6.3.
The data consists of contexts associated with a
specific word to be sense tagged (target word); the
context size varies from 1 sentence (Spanish) to
5 sentences (English, Swedish). Table 1 presents
some statistics collected on the training data for the
five data sets. Some of the tasks are quite challeng-
ing (e.g. SENSEVAL2 English task) ? as illustrated
by the mean participating systems? accuracies in Ta-
ble 5.
Outlining the claim that feature selection is im-
portant for WSD, Table 2 presents the marginal loss
in performance of either only using one of the po-
sitional feature classes or excluding one of the po-
sitional feature classes relative to the algorithm?s
full performance using all available feature classes.
It is interesting to note that the feature-attractive
methods (NB,BR,Cosine) depend heavily on the
BagOfWords features, while discriminative methods
are most dependent on LocalContext features. For
an extensive evaluation of factors influencing the
WSD performance (including representational fea-
tures), we refer the readers to Yarowsky and Florian
(2002).
6.1 Combination Performance
Table 3 shows the fine-grained sense accuracy (per-
cent of exact correct senses) results of running the
6When parameters needed to be estimated, a 3-1-1 split was
used: the systems were trained on three parts, parameters esti-
mated on the fourth (in a round-robin fashion) and performance
tested on the fifth; special care was taken such that no ?test?
data was used in training classifiers or parameter estimation.
SE1 SENSEVAL2
EN EN ES EU SV
#words 42 73 39 40 40
#samples 12479 8611 4480 3444 8716
avg #senses/word 11.3 10.7 4.9 4.8 11.1
avg #samples/sense 26.21 9.96 23.4 17.9 19.5
Table 1: Training set characteristics
Performance drop relative to full system (%)
NB Cosine BR TBL DL
BoW Ftrs Only -6.4 -4.8 -4.8 -6.0 -3.2
Local Ftrs Only -18.4 -11.5 -6.1 -1.5 -3.3
Syntactic Ftrs Only -28.1 -14.9 -5.4 -5.4 -4.8
No BoW Ftrs -14.7 -8.1 -5.3 -0.5 -2.0
No Local Ftrs -3.5 -0.8 -2.2 -2.9 -4.5
No Syntactic Ftrs -1.1 -0.8 -1.3 -1.0 -2.3
Table 2: Individual feature type contribution to perfor-
mance. Fields marked with  indicate that the difference
in performance was not statistically significant at a 

level (paired McNemar test).
classifier combination methods for 5 classifiers, NB
(Na?ve Bayes), BR (BayesRatio), TBL, DL and
MMVC, including the average classifier accuracy
and the best classification accuracy. Before examin-
ing the results, it is worth mentioning that the meth-
ods which estimate parameters are doing so on a
smaller training size (3/5, to be precise), and this
can have an effect on how well the parameters are
estimated. After the parameters are estimated, how-
ever, the interpolation is done between probability
distributions that are computed on 4/5 of the train-
ing data, similarly to the methods that do not esti-
mate any parameters.
The unweighted averaging model of probability
interpolation (Equation (1)) performs well, obtain-
ing over 1% mean absolute performance over the
best classifier7, the difference in performance is
statistically significant in all cases except Swedish
and Spanish. Of the classifier combination tech-
niques, rank-based combination and performance-
based voting perform best. Their mean 2% absolute
improvement over the single best classifier is signif-
icant in all languages. Also, their accuracy improve-
ment relative to uniform-weight probability interpo-
lation is statistically significant in aggregate and for
all languages except Basque (where there is gener-
ally a small difference among all classifiers).
To ensure that we benefit from the performance
improvement of each of the stronger combination
methods and also to increase robustness, a final av-
eraging method is applied to the output of the best
performing combiners (creating a stacked classi-
fier). The last line in Table 3 shows the results ob-
tained by averaging the rank-based, EM-vote and
7The best individual classifier differs with language, as
shown in Figure 3(b).
SE1 SENSEVAL2
Method EN EN ES EU SV
Individual Classifiers
Mean Acc 79.5 65.0 66.6 70.4 65.9
Best Acc 81.1 66.7 68.8 71.2 68.0
Probability Interpolation
Averaging 82.7 68.0 69.3 72.2 68.16
MSE 82.8 68.1 69.7 71.0 69.2
EM 82.7 68.4 69.6 72.1 69.1
PB 82.8 68.0 69.4 72.2 68.7
Rank-based Combination
rank 83.1 68.6 71.0 72.1 70.3
Count-based Combination (Voting)
Simple Vote 82.8 68.1 70.9 72.1 70.0
TagPair 82.9 68.3 70.9 72.1 70.0
EM 83.0 68.4 70.5 71.7 70.0
PB 83.1 68.5 70.8 72.0 70.3
Stacking (Meta-Combination)
Prob. Interp. 83.2 68.6 71.0 72.3 70.4
Table 3: Classifier combination accuracy over 5 base
classifiers: NB, BR, TBL, DL, MMVC. Best perform-
ing methods are shown in bold.
Estimation Level word POS ALL Interp
Accuracy 68.1 68.2 68.0 68.4
CrossEntropy 1.623 1.635 1.646 1.632
Table 4: Accuracy for different EM-weighted probability
interpolation models for SENSEVAL2
PB-vote methods? output. The difference in perfor-
mance between the stacked classifier and the best
classifier is statistically significant for all data sets
at a significance level of at least , as measured
by a paired McNemar test.
One interesting observation is that for all meth-
ods of -parameter estimation (EM, PB and uniform
weighting) the count-based and rank-based strate-
gies that ignore relative probability magnitudes out-
perform their equivalent combination models using
probability interpolation. This is especially the case
when the base classifier scores have substantially
different ranges or variances; using relative ranks
effectively normalizes for such differences in model
behavior.
For the three methods that estimate the interpo-
lation weights ? MSE, EM and PB ? three vari-
ants were investigated. These were distinguished by
the granularity at which the weights are estimated:
at word level (


   


), at POS level
(


   

 
) and over the entire train-
ing set (


   

). Table 4 displays the results
obtained by estimating the parameters using EM at
different sample granularities for the SENSEVAL2
English data. The number in the last column is ob-
tained by interpolating the first three systems. Also
displayed is cross-entropy, a measure of how well
?1.2
?1
?0.8
?0.6
?
?
0.4
0 2
0
0.2
0.4
0.6 English Spanish Swedish Basque
Bayes BayesRatio Cosine DL TBL MMVC





 



 














































































































































































































































































































































































































Senseval2 dataset
D
if
fe
re
nc
e 
in
 A
cc
ur
ac
y 
vs
 
6?
w
ay
 C
om
bi
na
tio
n
(a) Performance drop when eliminating one classifier
(marginal performance contribution)
?3.5
?3
?2.5
?2
?1.5
?1
?0.5
0
0.5
1
Bayes BayesRatio
Cosine
DL
TBL
MMVC
Percent of available training data
10 20 40 50 80
D
iff
er
en
ce
 in
 c
la
ss
ifi
ca
tio
n 
ac
cu
ra
cy
 (%
)
(b) Performance drop when eliminating one classifer,
versus training data size
Figure 4: Individual basic classifiers? contribution to the final classifier combination performance.
the combination classifier estimates the sense prob-
abilities,   


 

 	

 
 .
6.2 Individual Systems Contribution to
Combination
An interesting issue pertaining to classifier combi-
nation is what is the marginal contribution to final
combined performance of the individual classifier.
A suitable measure of this contribution is the dif-
ference in performance between a combination sys-
tem?s behavior with and without the particular clas-
sifier. The more negative the accuracy difference on
omission, the more valuable the classifier is to the
ensemble system.
Figure 4(a) displays the drop in performance ob-
tained by eliminating in turn each classifier from the
6-way combination, across four languages, while
Figure 4(b) shows the contribution of each classifier
on the SENSEVAL2 English data for different train-
ing sizes (10%-80%)8. Note that the classifiers with
the greatest marginal contribution to the combined
system performance are not always the best single
performing classifiers (Table 3(b)), but those with
the most effective original exploitation of the com-
mon feature space. On average, the classifier that
contributes the most to the combined system?s per-
formance is the TBL classifier, with an average im-
provement of 

 across the 4 languages. Also,
note that TBL and DL offer the greatest marginal
contribution on smaller training sizes (Figure 4(b)).
6.3 Performance on Test Data
At all points in this article, experiments have been
based strictly on the original SENSEVAL1 and SEN-
SEVAL2 training sets via cross-validation. The of-
ficial SENSEVAL1 and SENSEVAL2 test sets were
8The latter graph is obtained by sampling repeatedly a
prespecified ratio of training samples from 3 of the 5 cross-
validation splits, and testing on the other 2.
unused and unexamined during experimentation to
avoid any possibility of indirect optimization on this
data. But to provide results more readily compara-
ble to the official benchmarks, a single consensus
system was created for each language using linear
average stacking on the top three classifier combi-
nation methods in Table 3 for conservative robust-
ness. The final frozen consensus system for each
language was applied once to the SENSEVAL test
sets. The fine-grained results are shown in Table
5. For each language, the single new stacked com-
bination system outperforms the best previously re-
ported SENSEVAL results on the identical test data9.
As far as we know, they represent the best published
results for any of these five SENSEVAL tasks.
7 Conclusion
In conclusion, we have presented a comparative
evaluation study of combining six structurally and
procedurally different classifiers utilizing a rich
common feature space. Various classifier combi-
nation methods, including count-based, rank-based
and probability-based combinations are described
and evaluated. The experiments encompass super-
vised lexical sample tasks in four diverse languages:
English, Spanish, Swedish, and Basque.
9To evaluate systems on the full disambiguation task, it is
appropriate to compare them on their accuracy at 100% test-
data coverage, which is equivalent to system recall in the offi-
cial SENSEVAL scores. However, it can also be useful to con-
sider performance on only the subset of data for which a sys-
tem is confident enough to answer, measured by the secondary
measure precision. One useful byproduct of the CBV method
is the confidence it assigns to each sample, which we measured
by the number of classifiers that voted for the sample. If one
restricts system output to only those test instances where all
participating classifiers agree, consensus system performance
is 83.4% precision at a recall of 43%, for an F-measure of 56.7
on the SENSEVAL2 English lexical sample task. This outper-
forms the two supervised SENSEVAL2 systems that only had
partial coverage, which exhibited 82.9% precision at a recall of
28% (F=41.9) and 66.5% precision at 34.4% recall (F=47.9).
SENSEVAL1 SENSEVAL2 Sense Classification Accuracy
English English Spanish Swedish Basque
Mean Official SENSEVAL Systems Accuracy 73.12.9 55.75.3 59.65.0 58.46.6 74.41.8
Best Previously Published SENSEVAL Accuracy 77.1% 64.2% 71.2% 70.1% 75.7%
Best Individual Classifier Accuracy 77.1% 62.5% 69.6% 68.6% 75.6%
New (Stacking) Accuracy 79.7% 66.5% 72.4% 71.9% 76.7%
Table 5: Final Performance (Frozen Systems) on SENSEVAL Lexical Sample WSD Test Data
The experiments show substantial variation in
single classifier performance across different lan-
guages and data sizes. They also show that this
variation can be successfully exploited by 10 differ-
ent classifier combination methods (and their meta-
voting consensus), each of which outperforms both
the single best classifier system and standard classi-
fier combination models on each of the 4 focus lan-
guages. Furthermore, when the stacking consensus
systems were frozen and applied once to the other-
wise untouched test sets, they substantially outper-
formed all previously known SENSEVAL1 and SEN-
SEVAL2 results on 4 languages, obtaining the best
published results on these data sets.
8 Acknowledgements
The authors would like to thank Noah Smith for his
comments on an earlier version of this paper, and
the anonymous reviewers for their useful comments.
This work was supported by NSF grant IIS-9985033
and ONR/MURI contract N00014-01-1-0685.
References
A. Berger. 1996. Convexity, maximum likelihood
and all that. http://www.cs.cmu.edu/afs/cs/user/aberger/
www/ps/convex.ps.
E. Brill and J. Wu. 1998. Classifier combination for improved
lexical disambiguation. In Proceedings of COLING-ACL?98,
pages 191?195.
E. Brill. 1995. Transformation-based error-driven learning and
natural language processing: A case study in part of speech
tagging. Computational Linguistics, 21(4):543?565.
S. Cucerzan and D. Yarowsky. 2000. Language independent
minimally supervised induction of lexical probabilities. In
Proceedings of ACL-2000, pages 270?277.
S. Cucerzan and D. Yarowsky. 2002. Augmented mixture models
for lexical disambiguation. In Proceedings of EMNLP-2002.
W. Daelemans, A. van den Bosch, and J. Zavrel. 1999. Timbl:
Tilburg memory based learner - version 1.0. Technical Report
ilk9803, Tilburg University, The Netherlands.
A.P. Dempster, N.M. Laird, , and D.B. Rubin. 1977. Maximum
likelihood from incomplete data via the EM algorithm. Jour-
nal of the Royal statistical Society, 39(1):1?38.
P. Edmonds and S. Cotton. 2001. SENSEVAL-2: Overview. In
Proceedings of SENSEVAL-2, pages 1?6.
N. Fuhr. 1989. Optimum polynomial retrieval funcions based
on the probability ranking principle. ACM Transactions on
Information Systems, 7(3):183?204.
W. Gale, K. Church, and D. Yarowsky. 1992. A method for
disambiguating word senses in a large corpus. Computers and
the Humanities, 26:415?439.
J. Henderson and E. Brill. 1999. Exploiting diversity in natural
language processing: Combining parsers. In Proceedings on
EMNLP99, pages 187?194.
A. Kilgarriff and M. Palmer. 2000. Introduction to the special
issue on senseval. Computer and the Humanities, 34(1):1-13.
A. Kilgarriff and J. Rosenzweig. 2000. Framework and re-
sults for English Senseval. Computers and the Humanities,
34(1):15-48.
C.D. Manning and H. Sch?tze. 1999. Foundations of Statistical
Natural Language Processing. MIT Press.
G. Ngai and R. Florian. 2001. Transformation-based learning in
the fast lane. In Proceedings of NAACL?01, pages 40?47.
T. Pedersen. 1998. Na?ve Bayes as a satisficing model. In Work-
ing Notes of the AAAI Symposium on Satisficing Models.
T. Pedersen. 2000. A simple approach to building ensembles of
naive bayesian classifiers for word sense disambiguation. In
Proceedings of NAACL?00, pages 63?69.
M. P. Perrone and L. N. Cooper. 1993. When networks disagree:
Ensemble methods for hybrid neural networks. In R. J. Mam-
mone, editor, Neural Networks for Speech and Image Process-
ing, pages 126?142. Chapman-Hall.
E. F. Tjong Kim Sang, W. Daelemans, H. Dejean, R. Koeling,
Y. Krymolowsky, V. Punyakanok, and D. Roth. 2000. Apply-
ing system combination to base noun phrase identification. In
Proceedings of COLING 2000, pages 857?863.
M. Stevenson and Y. Wilks. 2001. The interaction of knowl-
edge sources in word sense disambiguation. Computational
Linguistics, 27(3):321?349.
K. Tumer and J. Gosh. 1995. Theoretical foundations of linear
and order statistics combiners for neural pattern classifiers.
Technical Report TR-95-02-98, University of Texas, Austin.
H. van Halteren, J. Zavrel, and W. Daelemans. 1998. Improv-
ing data driven wordclass tagging by system combination. In
Proceedings of COLING-ACL?98, pages 491?497.
H. van Halteren, J. Zavrel, and W. Daelemans. 2001. Im-
proving accuracy in word class tagging through the combina-
tion fo machine learning systems. Computational Linguistics,
27(2):199?230.
L. Xu, A. Krzyzak, and C. Suen. 1992. Methods of com-
bining multiple classifires and their applications to handwrit-
ing recognition. IEEE Trans. on Systems, Man. Cybernet,
22(3):418?435.
D. Yarowsky and R. Florian. 2002. Evaluating sense disambigua-
tion performance across diverse parameter spaces. To appear
in Journal of Natural Language Engineering.
D. Yarowsky and R. Wicentowski. 2000. Minimally supervised
morphological analysis by multimodal alignment. In Pro-
ceedings of ACL-2000, pages 207?216.
D. Yarowsky. 1996. Homograph disambiguation in speech
synthesis. In J. Olive J. van Santen, R. Sproat and
J. Hirschberg, editors, Progress in Speech Synthesis, pages
159?175. Springer-Verlag.
Augmented Mixture Models for Lexical Disambiguation
Silviu Cucerzan and David Yarowsky
Department of Computer Science and
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218, USA
{silviu,yarowsky}@cs.jhu.edu
Abstract
This paper investigates several augmented mixture
models that are competitive alternatives to standard
Bayesian models and prove to be very suitable to
word sense disambiguation and related classifica-
tion tasks. We present a new classification correc-
tion technique that successfully addresses the prob-
lem of under-estimation of infrequent classes in the
training data. We show that the mixture models are
boosting-friendly and that both Adaboost and our
original correction technique can improve the re-
sults of the raw model significantly, achieving state-
of-the-art performance on several standard test sets
in four languages. With substantially different out-
put to Na?ve Bayes and other statistical methods, the
investigated models are also shown to be effective
participants in classifier combination.
1 Introduction
The focus tasks of this paper are two re-
lated problems in lexical ambiguity resolution:
Word Sense Disambiguation (WSD) and Context-
Sensitive Spelling Correction (CSSC).
Word Sense Disambiguation has a long history as
a computational task (Kelly and Stone, 1975), and
the field has recently supported large-scale interna-
tional system evaluation exercises in multiple lan-
guages (SENSEVAL-1, Kilgarriff and Palmer (2000),
and SENSEVAL-2, Edmonds and Cotton (2001)).
General purpose Spelling Correction is also a
long-standing task (e.g. McIlroy, 1982), tradi-
tionally focusing on resolving typographical errors
such as transposition and deletion to find the clos-
est ?valid? word (in a dictionary or a morpholog-
ical variant), typically ignoring context. Yet Ku-
kich (1992) observed that about 25-50% of the
spelling errors found in modern documents are ei-
ther context-inappropriate misuses or substitutions
of valid words (such as principal and principle)
which are not detected by traditional spelling cor-
rectors. Previous work has addressed the problem
of CSSC from a machine learning perspective, in-
cluding Bayesian and Decision List models (Gold-
ing, 1995), Winnow (Golding and Roth, 1996) and
Transformation-Based Learning (Mangu and Brill,
1997).
Generally, both tasks involve the selection be-
tween a relatively small set of alternatives per key-
word (e.g. sense id?s such as church/BUILDING
and church/INSTITUTION or commonly confused
spellings such as quiet and quite), and are dependent
on local and long-distance collocational and syntac-
tic patterns to resolve between the set of alterna-
tives. Thus both tasks can share a common feature
space, data representation and algorithm infrastruc-
ture. We present a framework of doing so, while in-
vestigating the use of mixture models in conjunction
with a new error-correction technique as competi-
tive alternatives to Bayesian models. While several
authors have observed the fundamental similarities
between CSSC and WSD (e.g. Berleant, 1995 and
Roth, 1998), to our knowledge no previous com-
parative empirical study has tackled these two prob-
lems in a single unified framework.
2 Problem Formulation. Feature Space
The problem of lexical disambiguation can be mod-
eled as a classification task, in which each in-
stance of the word to be disambiguated (target word,
henceforth), identified by its context, has to be la-
beled with one of the established sense labels
 
	
	
.
1 The approaches we investigate
are statistical methods 
 ffBootstrapping a Multilingual Part-of-speech Tagger
in One Person-day
Silviu Cucerzan and David Yarowsky
Department of Computer Science and
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218 USA
{silviu,yarowsky}@cs.jhu.edu
Abstract
This paper presents a method for bootstrapping a
fine-grained, broad-coverage part-of-speech (POS)
tagger in a new language using only one person-
day of data acquisition effort. It requires only three
resources, which are currently readily available in
60-100 world languages: (1) an online or hard-copy
pocket-sized bilingual dictionary, (2) a basic library
reference grammar, and (3) access to an existing
monolingual text corpus in the language. The al-
gorithm begins by inducing initial lexical POS dis-
tributions from English translations in a bilingual
dictionary without POS tags. It handles irregular,
regular and semi-regular morphology through a ro-
bust generative model using weighted Levenshtein
alignments. Unsupervised induction of grammatical
gender is performed via global modeling of context-
window feature agreement. Using a combination of
these and other evidence sources, interactive train-
ing of context and lexical prior models are accom-
plished for fine-grained POS tag spaces. Experi-
ments show high accuracy, fine-grained tag resolu-
tion with minimal new human effort.
1 Introduction
Previous work in minimally supervised language
learning has defined minimal using several different
criteria. Some have assumed only partially tagged
training corpora (Merialdo, 1994), while others
have begin with small tagged seed wordlists (such
as Collins and Singer (1999) and Cucerzan and
Yarowsky (1999) for named-entity tagging). Oth-
ers have exploited the automatic transfer of some
already existing annotated resource in a different
medium or language (such as the translingual pro-
jection of part-of-speech tags, syntactic bracket-
ing and inflectional morphology in Yarowsky et al
(2001), requiring no direct supervision in the for-
eign language). Ngai and Yarowsky (2000) ob-
served that an often more practical measure of the
degree of supervision is not simply the quantity of
annotated words, but the total weighted human la-
bor and resource costs of different modes of su-
pervision (allowing manual rule writing to be com-
pared directly with active learning on a common
cost-performance learning curve).
In this paper we observe that another useful mea-
sure of (minimal) supervision is the additional cost
of obtaining a desired functionality from existing
commonly available knowledge sources. In particu-
lar, we note that for a remarkably wide range of lan-
guages, academic libraries, many booksellers and
websites offer a foundation of linguistic wisdom in
reference grammars and dictionaries. Thus starting
from this baseline, what is the marginal cost of dis-
tilling from and augmenting this existing knowledge
to achieve a desired new task functionality?
2 Inducing POS Tag Candidates from
Unlabeled Bilingual Dictionaries
A substantial percentage of foreign language dic-
tionaries that are available on line or in smaller pa-
perback format are simple bilingual word or phrase
translation lists which fail to specify part of speech.1
Thus one component question of this work is how
can one extract preliminary part-of-speech distribu-
tions from untagged monolingual translation lists.
Figure 1 illustrates such a bilingual dictionary, also
specifying the true part of speech for each possible
translation, which we do not assume to be generally
available.
One approach is to take an unweighted mixture
of the prior part-of-speech distributions for the En-
glish words 

given in the translation list (TL) as
illustrated in Figure 2. These probabilities may be
estimated from a large and preferably balanced, cor-
pus. In this work, we used statistics from the Brown
and WSJ corpora combined.
1In this section, we will use the term POS tag to denote
only the main part-of-speech tags (noun, verb, adjective, ad-
verb, preposition, etc.) and not the fine-grained tags (such as
Noun-Genitive-fem-plur-def).
True
Romanian POS English translation list
mandat N warrant; proxy; mandate;
money order;
power of attorney
manechin N model, dummy
manifesta V arise, express itself, show
manual Adj manual;
N manual; textbook;
handbook
mare Adj large; big; great; tall;
old; important;
N sea
maro Adj brown, chestnut
Figure 1: A sample Romanian-English dictionary.
The POS tags are used only for evaluation and are
not available in many bilingual dictionaries.
MANDAT  Warrant
Proxy
Mandate
.55  .00    .45
.66   .34    .00
.80  .20    .00
.67   .18   .15
N AV
N AV
e i jP(Pos  | e  )iFW P(Pos  | FW)j
(via English treebank)dictionary
bilingual
via
Figure 2: Inducing a preliminary POS distribution
for the Romanian word mandat via a simple English
translation list.
However, when a translation candidate is phrasal
(e.g. mandat  money order), one can model the
more general probability of the foreign word?s part
of speech tag (

) given the part of speech sequence
of the English phrasal translation (





).
For example, one could model P(T

money or-
der) via P(T





 and P(T

manifest itself) via
P(T





. However, because English words
often have multiple parts of speech (e.g. order may
be a verb), one may weight phrasal POS sequence
probabilities (making an independence assumption)
as:
 

	
  
 





   

	
   


 





   

	
   


 





   

	
   


 





   

	
   



And in general:
 







 








 







   





   






where  





 is estimated from the dictionary
as above. Without an independence assumption:
 







 
 







   












There are two major options via which one can
estimate  







. The first is to assume
that the part-of-speech usage of phrasal (English)
translations is generally consistent across dictionar-
ies (e.g.  







 remains high regardless
of publisher or language). Hence one could use
any foreign-English bilingual dictionary that also
includes the true foreign word part of speech in ad-
dition to its translations to train these probabilities.
Alternately, one could do a first-pass assignment
of foreign-word part of speech based on only sin-
gle word translations as in Figure 2, and use this to
train  







 for those foreign words hav-
ing both phrasal and single-word definitions (such
as mandat). The advantage of this approach is that it
may benefit dictionaries with different phrasal trans-
lation styles from the training dictionary (e.g. use
or omission of the word ?to? in verb definitions).
However, given the assumption of relatively consis-
tent dictionary formatting styles (which was unfor-
tunately not the case for Kurdish), we evaluated this
work based on supervised phrasal training from a
single independent third language dictionary.
Table 1 measures the POS induction performance
on three languages, where the true POS tags were
given in the dictionary (as in Figure 1), but ignored
except for evaluation. The accuracy values in this
table are based on exact matches between a word?s
dictionary-provided POS and the most probable tag
in its induced distribution.
For our target application of part-of-speech tag-
ging, what matters is to have a robust tag probabil-
ity distribution that includes the true candidate with
sufficiently large probability to seed further train-
ing. By setting this baseline threshold to 0.1 and
deleting lower ranked candidates, up to 98% of the
true POS were found to be above this threshold and
hence were considered in future training.
The Mean Probability of Truth, as shown in Ta-
ble 1, is another measure of the quality of the POS
predictions made by the algorithm, representing the
probability mass associated with the true POS tag
averaged over all words.
In some cases the algorithm could not predict a
POS tag, primarily due to English translations for
which no POS distribution was known (often an ob-
scure word, proper name or OCR error). This oc-
Target Training Accuracy Correct POS Coverage Mean Probability
Language Dictionary Exact POS Over Threshold of Truth
Romanian Spanish - English 92.9 97.8 98 .91
Kurdish Spanish - English 76.8 93.1 95 .82
Spanish Romanian - English 83.3 94.9 97 .86
Table 1: Performance of inducing candidate part-of-speech distributions derived solely from untagged En-
glish translation lists. Results are measured by type (all dictionary entries are weighted equally).
casional omission is measured by the coverage col-
umn.
Most of the observed errors are due to differences
in phrasal definitional conventions in the training
and testing dictionaries, long phrasal idioms, single-
word definitions with ambiguous English parts-of-
speech and OCR errors. The Kurdish dictionary was
particularly hindered by frequent long phrasal trans-
lations which often included an explanation or def-
inition in their translation. Because all dictionary
entries are equally weighted, errors on rare words
such as mythological characters or kinship terms
can substantially downgrade performance. But for
the purposes of providing seed POS distributions to
context-sensitive taggers, performance is quite ade-
quate for this follow-on task.
3 Inducing Morphological Analyses
There has been extensive previous work in the
supervised and minimally supervised induction of
both affix paradigms (e.g. Goldsmith, 2000; Snover
and Brent, 2001) and diverse models of regular and
irregular concatenative and non-concatenative mor-
phology (e.g. Schone and Jurafsky, 2000; van den
Bosch and Daelemans, 1999; Yarowsky and Wicen-
towski, 2000). While such approaches are impor-
tant from the perspective of learning theory or broad
coverage handling of irregular forms, another pos-
sible paradigm for minimal supervision is to begin
with whatever knowledge can be efficiently manu-
ally entered from the grammar book in several hours
work.
We defined such grammar-based ?supervision? as
entry of regular inflectional affix changes and their
associated part of speech in standardized ordering of
fine-grained attributes, as in Table 2 for Spanish and
Romanian. The full tables have approximately 200
lines each and required roughly 1.5-2 person-hours
for entry.
Given a dictionary marked with core parts of
speech, it is trivial to generate hypothesized in-
flected forms following the regular paradigms, as
shown in the left size of Figure 3. However, due
to irregularities and semi-regularities such as stem-
Root Inflected
Affix Affix Part-of-speech Tag
Spanish:
o$ o$ Adj-masc-sing
o$ os$ Adj-masc-plur
o$ a$ Adj-fem-sing
o$ as$ Adj-fem-plur
e$ e$ Adj-masc,fem-sing
e$ es$ Adj-masc,fem-plur
ar$ o$ Verb-Indic_Pres-p1-sing
ar$ as$ Verb-Indic_Pres-p2-sing
ar$ a$ Verb-Indic_Pres-p3-sing
ar$ amos$ Verb-Indic_Pres-p1-plur
ar$ ?is$ Verb-Indic_Pres-p2-plur
ar$ an$ Verb-Indic_Pres-p3-plur
Romanian:
a?$ e$ Noun-Nomin-p3-fem-plur-indef
e$ i$ Noun-Nomin-p3-fem-plur-indef
ea$ ele$ Noun-Nomin-p3-fem-plur-indef
i$ ile$ Noun-Nomin-p3-fem-plur-indef
a$ ale$ Noun-Nomin-p3-fem-plur-indef
$ $ Adj-masc,neut-sing
$ a?$ Adj-fem-sing
$ i$ Adj-masc,neut,fem-plur
$ e$ Adj-fem,neut-plur
ru$ ra$ Adj-fem-sing
ru$ ri$ Adj-masc,neut,fem-plur
ru$ re$ Adj-fem-plur
... ... ...
e$ $ Verb-Indic_Pres-p1-sing
e$ i$ Verb-Indic_Pres-p2-sing
e$ e$ Verb-Indic_Pres-p3-sing
e$ em$ Verb-Indic_Pres-p1-plur
e$ et?i$ Verb-Indic_Pres-p2-plur
e$ $ Verb-Indic_Pres-p3-plur
Table 2: Sample extracted regular inflectional
paradigms (suffix context is marked by $).
changes, such generation will clearly have substan-
tial inaccuracies and overgenerations.
However, through weighted-Levenshtein-based
iterative alignment models, such as described in
Yarowsky and Wicentowski (2000), one can per-
form a probabilistic string match from all lexical to-
kens actually observed in a monolingual corpus, as
z->cdestrozan destroc?
destroz?
V-pres-3pl
V-pret-1sg
V-subj-3pl destrozen
destrocen
destrozan
destrozar/V
z->c
destruo
destru?
destruen
V-pres-1sg
V-pres-1sg
V-pret-1sg
destrue destru?
destruyo
destruye
destruyen
destruir/V
V-pres-3sg
->y
->y?
?
 
->y
V-pres-3pl
V-pret-3pl
doler/V
dormir/V
V-pres-1sg
dormenV-pres-3pl
dormo
dolen
doli?
o->ue
o-
>u
e
doli?
duelen
duermen
duermo
dormi?
dorm?an
dorm?an
durmi?
V-pret-3pl
V-imprf-3pl
o->
ue
o->u
Observed
Rootword
Dictionary Corpus
Regular
Words
Inflection
Generation
?
Figure 3: Inflectional analysis induction via
weighted string alignment to noisy generations from
dictionary roots under regular paradigms
in the right side of Figure 32.
For example, when looking for a potential anal-
ysis path for the Spanish irregular inflection de-
strocen, the closest string match is the regular hy-
pothesis destrozar/V  destrozen/V-pres_subj-3pl.
Likewise, the closest string match for destruyen is
destruir/V  destruen/V-pres_indic-3pl. The dif-
ferences between these regular hypotheses and ob-
served inflected forms are the relatively productive
stem changes  and , neither of which was
listed in the inflectional supervision table, and yet
they were correctly handled. Note that a traditional
 POSsuffix) model would fail to handle this case
given that the common inflection suffix -en corre-
sponds to two different parts of speech here (present
indicative or subjunctive depending on -ir or -ar
paradigm).
Also note that the irregular stem change pro-
cesses such as dormirduermen have a correct
best-fit analysis, despite the absence of any internal
stem change exemplars (e.g. oue) in the human-
generated inflectional supervision table.
For further robustness, the consensus model of
 

  is estimated as a weighted mixture of
the part-of-speech tags of the most closely aligned
2For processing efficiency, one additional constraint is that
potential hypothesizedobserved string pair candidates must
exactly match in both initial consonant cluster and suffix of the
generated hypothesis.
pseudo-regular generated inflections.
The inflections of closed-class words (such as
pronouns, determiners and auxiliary verbs) are not
well handled by this generative-alignment model,
both due to their often very high irregularity (e.g.
the Spanish verb ser (to be)) and/or their typ-
ical shortness (e.g. the pronominal inflections
of mi, tu, su). Thus as one final amount of
supervision, lists of closed-class words, paired
with their inflections and fine-grained part-of-
speech tags were entered manually from the gram-
mar book (e.g. aquellas#(aquel)Adj_Dem-
fem-plur-p3). This final source of supervision
utilized an average of 400 lines and 3 person-hours
per language.
4 POS Model Induction
The non-traditional supervision methodology in
Sections 2 and 3 yields a noisy but broad-coverage
candidate space of parts of speech with little human
effort.
We then perform a noise-robust combination of
model estimation and re-estimation techniques for
the syntagmatic trigram models  



 


and lexical priors  



 using the word co-
occurrence information from a raw corpus.
 A suffix-based part-of-speech probability
model  

suffix

 using hierarchically
smoothed tries is trained on the raw initial
tag distributions, yielding coverage to unseen
words and smoothing of low-confidence initial
tag assignments.
 Paradigmatic cross-context tag modeling is
performed as in Cucerzan and Yarowsky
(2000) when sufficiently large unannotated
corpora are available.
 Sub-part-of-speech contextual agreement for
features such as gender is performed as de-
scribed in Section 4.1.
 The part-of-speech tag sequence models
 



 

 utilize a weighted backoff
between fine-grained and coarse-grained tags.
 Both the tag-sequence and lexical prior models
are iteratively retrained using these additional
evidence sources and first-pass probability dis-
tributions.
The success of this model is based on the as-
sumption that (a) words of the same part of speech
tend to have similar tag sequence behavior, and (b)
there are sufficient instances of each POS tag la-
beled by either the morphology models or closed-
class entries described in Section 3. One example
where these assumptions do not hold is for the Ro-
manian word a, which has 5 possible POS tags, in-
cluding Infinitive_Marker (corresponding to
the English word to). But because the Infini-
tive_Marker tag has no other word instances in
Romanian, no other filial supervision exists to re-
solve the ambiguity of a if no context-sensitive tag-
ging is provided (such as the preference for a to
be labeled Infinitive_Markerwhen followed
by a Verb-Infinitive). Thus one avenue of
potential improvement to these models would be
to include limited tagged contexts for ambiguous
small class (or singleton class) words, although such
supervision is less readily extractable from gram-
mar books by non-native speakers, and was not em-
ployed here.
4.1 Contextual-agreement models for
part-of-speech subtags
Traditional part-of-speech models assume a strict
Markovian sequential dependency. However, Adj-
Noun, Det-Noun and Noun-Verb agreement at the
subtag-level (e.g. for person, number, case and gen-
der) often do not require direct adjacency, and are
based on the selective matching of isolated subfea-
tures. This is particularly important for grammatical
gender, where the lack of gender features projected
from English rootwords in a bilingual dictionary (as
in Section 2) require contextual agreement to assign
gender to many inflected and root forms.
However, given the assumptions of minimal su-
pervision, it is not reasonable to require a parser or
dependency model to identify non-adjacent agree-
ing pairs explicitly. Rather, we utilize a much more
general tendency for words exhibiting a property
such as grammatical gender to co-occur in a rela-
tively narrow window with other words of the same
gender (etc.) with a probability greater than chance.
Empirically, we observe this in Figures 4-5, which
show the gender-agreement ratio between a target
noun/adjective and other gender marked words ap-
pearing in context at relative position . Adjec-
tives in Romanian exhibit a stronger agreement ten-
dency with words to their left (5/1 ratio), while for
nouns the agreement ratio is quite closely balanced
between -1 (primarily determiners) and +1 (primar-
ily adjectives), although weaker (2.4/1 ratio), per-
haps due to a greater relative tendency for nouns to
juxtapose directly with other independent clauses of
different gender. Also, both parts of speech con-
0
1
2
3
4
5
6
-10 -9 -8 -7 -6 -5 -4 -3 -2 -1
Relative Position
0
Ag
re
em
en
t/N
on
-A
gr
ee
m
en
t R
at
io
1 2 3 4 5 6 7 8 9 10
Adjectives
0
1
2
2.5
-10 -9 -8 -7 -6 -5 -4 -3 -2 -1
Nouns
Ag
ree
me
nt/
No
n-A
gre
em
en
t R
atio
0
Relative Position
1 2 3 4 5 6 7 8 9 10
Figure 4: Ratio of the frequency that a gender-
marked adjective (above) or noun (below) agrees
in gender with another noun/adjective/determiner at
relative position i over the frequency of gender dis-
agreement at that relative position.
0.4
0.5
0.6
0.7
0.8
0.9
1.0
1 2 3 4
to
ke
ns
 w
ith
in
 c
on
te
xt
 w
in
do
w
Pr
ob
ab
ilit
y 
of
 e
xis
te
nc
e 
of
  g
en
de
r-m
ar
ke
d
Context Width
5 6 7 8 9 10
Figure 5: The probability that at least one gender-
marked word will occur within a window of 
words relative to another gender marked word (of
any part of speech).
verge on the agreement ratio expected by chance
(0.82) relatively quickly. Thus while any individ-
ual context may suggest incorrect gender based on
agreement, if one aggregates over all occurrences of
a word in a corpus, a consensus gender preference
emerges, with the true gender agreement signal ex-
ceeding nearby spurious gender noise.
Formally, we can model this window-weighted
global feature consensus as:
 


 



	




 





The  window-size parameter was selected
prior to the studies shown in Figures 4-5, but is sup-
ported by them. Beyond this window the agree-
ment/disagreement ratio approaches chance, but
with a smaller window the probability of finding any
gender-marked word in the window drops below the
80% coverage observed for , trading lower cov-
erage for increased accuracy.
If one makes the assumption that the overwhelm-
ing majority of nouns have a single grammatical
gender independent of context, we perform smooth-
ing to force nouns with sufficient global context fre-
quency towards their single most likely gender.
Finally, the trie-based suffix model noted in Sec-
tion 3 can be utilized here to further generalize gen-
der affixal tendencies for use in smoothing poorly
represented single words. Through this approach
we successfully discover a wide space of low-
entropy gender affix tendencies, including the com-
mon -a, -dad and -ci?n feminine affixes in Span-
ish, without any human or dictionary supervision
of nominal gender. But even those words with-
out gender-distinguishing affixes (e.g. parte, cabal)
can be successfully learned via global context max-
imization.
5 Evaluation of the Full Part-of-speech
Tagger
One problem with minimally supervised learning of
foreign languages is that annotated evaluation data
are often not available for the features being in-
duced, or are otherwise difficult to obtain. Thus we
have used for initial test languages two languages
familiar to the authors (Romanian and Spanish) for
which sufficient evaluation resources could be ob-
tained. However, the monolingual corpora utilized
for bootstrapping were quite small (123 thousand
words of the book 1984 for Romanian and 3.2 mil-
lion words of newswire for Spanish), which are eas-
ily comparable to the sizes that can be accessed on-
line for 60-100 world languages. The seed dictio-
naries were located online (for Spanish - 42k en-
tries) and via OCR (for Romanian - 7k entries), and
small grammar references were obtained at a local
bookstore. 1000 words of test data were annotated
with a standardized, finely detailed part-of-speech
tag inventory including the full complex distinctions
for gender, person, number, case, detailed tense and
nominal definiteness (an inventory of 259 and 230
fine-grained tags were used for Spanish and Roma-
nian respectively).
The minimal supervision in this study consisted
of an average total of 4 person-hours per language
for manually entering the inflectional paradigms
and associated parts of speech from a grammar as
in Section 3, and an additional average of 3 person-
hours per language for dictionary extraction and en-
try parsing. OCR itself on our high-speed 2-sided
scanner with OmniPage Pro took under 30 min-
utes). As would be expected given that data en-
try was done by computer scientists which were
not native speakers of the test languages, significant
analysis errors or gaps were introduced when rather
blindly transferring from the reference grammar.
Thus to test the relative contributions of limited na-
tive speaker help when available, for roughly 4 addi-
tional total person hours in a second test condition
for Romanian a native speaker corrected and aug-
mented gaps in the patterns previously entered from
the grammar book, focusing almost exclusively on
the complex inflections of closed-class words.
A summary of the results for these three super-
vision modes is given in Table 3. Performance is
broken down by fine-grained part of speech. Exact-
match accuracy is measured over both the full fine-
grained (up to 5-feature) part-of-speech space, as
well as the 12-class core POS tag (noun and proper
noun, pronoun, verb, adjective, adverb, numeral,
determiner, conjunction, preposition, interjection,
particle, punctuation). The feature of grammatical
gender was specifically isolated because it is rarely
salient for cross-language applications such as ma-
chine translation (where grammatical gender rarely
transfers), and because its induction algorithm in
Section 4.1 depends heavily on the size of the mono-
lingual corpus (which is small in these experiments,
suggesting size-dependent potential for significant
further improvement here).
Finally, a post-hoc analysis of the system vs. test
data discrepancies showed that a significant number
were simply arbitrary differences in annotation con-
vention between the grammar-book analyses and
the test data tagging policy. For example, one such
?error?/discrepancy is the rather arbitrary distinc-
tion of whether the Romanian word oricare (mean-
ing any) should be considered an adjective (as listed
in a standard bilingual dictionary) or a determiner.
Another difference is whether proper-name citations
of common nouns (e.g. Casa Blanca) should be an-
notated for gender/number etc. or not.
Yet regardless of exactly how many system-test
discrepancies are just policy differences rather than
errors, even the raw accuracy here is very promising
given the very fined-grained part-of-speech inven-
tory and small monolingual data size used for boot-
strapping. And ultimately the performance is quite
Spanish Romanian
NNS NNS NNS-8h
8h 8h NS-4h
All words
core-tag 93.1 86.3 89.2
exact-match 86.5 68.6 75.5
exact w/o gender 87.0 76.7 83.0
Nouns
core-tag 90.3 97.4 97.4
*number 100.0 97.4 98.9
*gender 100.0 54.9 64.7
*definiteness ? 96.6 93.7
*case ? 97.4 97.4
Verbs
core-tag 94.7 87.9 89.5
*tense 93.0 92.6 93.2
*number 100.0 91.5 91.2
*person 97.2 92.6 93.2
Adjectives
core-tag 79.7 78.6 81.5
*gender 100.0 81.3 82.2
*number 100.0 98.3 98.3
Table 3: Performance of POS tagger induction
based on 1 person-day of supervision, no tagged
training corpora and a fine-grained (250 tags)
tagset. NNS and NN refer to non-native-speaker and
native-speaker effort.
remarkable given that it is the result of less than 1
total person day of data collection and supervision,
in contrast to the thousands of hours and $100,000-
$1,000,000 spent on some annotated training data
in a much more limited tagset inventories. Thus
in terms of cost-benefit analysis, the supervision
paradigm and associated bootstrapping models pre-
sented here offer quite a good value of new func-
tionality per labor invested.
6 Conclusion
This paper has presented an alternative to tradi-
tional corpus annotation-based supervision of part-
of-speech taggers. Given that even obscure lan-
guages have reference grammars and dictionaries
available in large bookstores, libraries or even on-
line, the focus of this work is on using human su-
pervision for efficient structured entry of this seed
knowledge (in the form of regular and semi-regular
inflectional paradigms and often irregular closed-
class part-of-speech entries). Minimally supervised
bootstrapping procedures then used corpus-derived
distributional data to induce lexical tag probabilities
from dictionaries, irregular morphological analyses
via weighted Levenshtein-based alignment models,
tag sequence probability induction and grammati-
cal gender agreement modeling. Experiments show
high accuracy coarse and fine-grained ( 250 tag)
part-of-speech analyses using only one person day
of new human supervision based on readily avail-
able linguistic resources.
Acknowledgements
This work was partially supported by NSF grant
IIS-9985033 and ONR/MURI contract N00014-01-
1-0685.
References
Baum, L. 1972. An inequality and associated maximiza-
tion technique in statistical estimation of probabilistic
functions of a Markov process. Inequalities, 3:1?8.
Collins, M., and Y. Singer, 1999 Unsupervised models
for named entity classification. In Proceedings of the
Joint SIGDAT Conference on EMNLP and VLC 1999,
pp. 100-110.
Cucerzan, S., and D. Yarowsky, 1999. Language inde-
pendent named entity recognition combining morpho-
logical and contextual evidence. In Proceedings of the
Joint SIGDAT Conference on EMNLP and VLC 1999,
pp. 90-99.
Cucerzan, S., and D. Yarowsky, 2000. Language in-
dependent minimally supervised induction of lexical
probabilities. In Proceedings of ACL 2000, pp. 270-
277.
Goldsmith, J. A., 2000 Unsupervised learning of the
morphology of a natural language. Computational
Linguistics 27(2):153?198.
Merialdo, B., 1994. Tagging English text with a prob-
abilistic model. Computational Linguistics 20:155?
171.
Ngai, G., and D. Yarowsky, 2000. Inducing multilin-
gual POS taggers and NP bracketers via robust projec-
tion across aligned corpora. In Proceedings of NAACL
2000, pp. 200-207.
Schone, P., and D. Jurafsky, 2000. Knowledge-free in-
duction of morphology using latent semantic analysis.
In Proceedings of CoNLL 2000.
Snover, M. G., and M. R. Brent, 2001. A Bayesian model
for morpheme and paradigm identification. In Pro-
ceedings of ACL 2001, pp. 482-490.
Van den Bosch, A., and W. Daelemans, 1999. Memory-
based morphological analysis. In Proceedings of ACL
1999, pp.285-292
Yarowsky, D., G. Ngai, and R. Wicentowski, 2001. In-
ducing Multilingual Text Analysis Tools via Robust
Projection across Aligned Corpora. In Proceedings of
HLT 2001, pp. 161-168.
Yarowsky, D., and R. Wicentowski, 2000. Minimally su-
pervised morphological analysis by multimodal align-
ment. In Proceedings of ACL 2000, pp. 207-216.
Language Independent NER using a Unified Model of Internal and
Contextual Evidence
Silviu Cucerzan and David Yarowsky
Department of Computer Science and
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218, USA
{silviu,yarowsky}@cs.jhu.edu
Abstract
This paper investigates the use of a language inde-
pendent model for named entity recognition based
on iterative learning in a co-training fashion, using
word-internal and contextual information as inde-
pendent evidence sources. Its bootstrapping pro-
cess begins with only seed entities and seed con-
texts extracted from the provided annotated corpus.
F-measure exceeds 77 in Spanish and 72 in Dutch.
1. Introduction
Our aim has been to build a maximally language-
independent system for named-entity recognition
using minimal supervision or knowledge of the
source language. The core model utilized, ex-
tended and evaluated here is based on Cucerzan and
Yarowsky (1999). It assumes that only an entity ex-
emplar list is provided as a bootstrapping seed set.
For the particular task of CoNLL-2002, the seed
entities are extracted from the provided annotated
corpus. As a consequence, the seed examples may
be ambiguous and the system must therefore han-
dle seeds with probability distribution over entity
classes rather than unambiguous seeds. Another
consequence is that this approach of extracting only
the entity seeds from the annotated text does not use
the full potential of the training data, ignoring con-
textual information. For example, Bosnia appears
labeled 9 times as LOC and 5 times as ORG and
the only information that would be used is that the
word Bosnia denotes a location 64% of the time,
and an organization 36% of the time, but not in
which contexts is labeled one way or the other. In
order to correct this problem, an improved system
also uses context seeds if available (for this particu-
lar task, they are extracted from the annotated cor-
pus). Because the representations of entity candi-
dates and contexts are identical, this modification
imposes only minor changes in algorithm and code.
Because the core model has been presented in de-
tail in Cucerzan and Yarowsky (1999), this paper
focuses primarily on the modifications of the algo-
rithm and its adaptation to the current task. The ma-
jor modifications besides the seed handling include
a different method of smoothing the distributions
along the paths in the tries, a new ?soft? discourse
segmentation method, and use of a different label-
ing methodology, as required by the current task i.e.
no overlapping entities are allowed (for example,
the correct labeling of colegio San Juan Bosco de
M?rida is considered to be ORG(colegio San Juan
Bosco) de LOC(M?rida) rather than ORG(colegio
PER(San Juan Bosco) de LOC(M?rida))).
2. Entity-Internal Information
Two types of entity-internal evidence are used in a
unified framework. The first consists of the pre-
fixes and suffixes of candidate entities. For exam-
ple, in Spanish, names ending in -ez (e.g. Alvarez
and Guti?rrez) are often surnames; names ending in
-ia are often locations (e.g. Austria, Australia, and
Italia). Likewise, common beginnings and endings
of multiword entities (e.g. Asociaci?n de la Prensa
de Madrid and Asociaci?n para el Desarrollo Rural
Jerez-Sierra Suroeste, which are both organizations)
are good indicators for entity type.
3. Contextual Information
An entity?s left and right context provides an essen-
tially independent evidence source for model boot-
strapping. This information is also important for en-
tities that do not have a previously seen word struc-
ture, are of foreign origin, or polysemous. Rather
than using word bigrams or trigrams, the system
handles the context in the same way it handles the
entities, allowing for variable-length contexts. The
advantages of this unified approach are presented in
the next paragraph.
4. A Unified Structure for both Internal and
Contextual Information
Character-based tries provide an effective, efficient
and flexible data structure for storing both con-
textual and morphological patterns and statistics.
... organizada por la Concejal?a de Cultura , tienen un ...
PREFIX RIGHT CONTEXTLEFT CONTEXT
SUFFIX
Figure 1: An example of entity candidate and context and
the way the information is introduced in the four tries (arrows
indicate the direction letters are considered)
They are very compact representations and support
a natural hierarchical smoothing procedure for dis-
tributional class statistics. In our implementation,
each terminal or branching node contains a prob-
ability distribution which encodes the conditional
probability of entity classes given the sistring cor-
responding to the path from the root to that node.
Each such distribution also has two standard classes,
named ?questionable? (unassigned probability mass
in terms of entity classes, to be motivated below)
and ?non-entity? (common words).
Two tries (denoted PT and ST) are used for in-
ternal representation of the entity candidates in pre-
fix, respectively suffix form, respectively. Other two
tries are used for left (LCT) and right (RCT) con-
text. Right contexts are introduced in RCT by con-
sidering their component letters from left to right,
left contexts are introduced in LCT using the re-
versed order of letters, from right to left (Figure 1).
In this way, the system handles variable length con-
texts and it attempts to match in each instance the
longest known context (as longer contexts are more
reliable than short contexts, and also the longer con-
text statistics incorporate the shorter context statis-
tics through smoothing along the paths in the tries).
The tries are linked together into two bipartite
structures, PT with LCT, and ST with RCT, by at-
taching to each node a list of links to the entity can-
didates or contexts with, respectively in which the
sistring corresponding to that node has been seen in
the text (Figure 2).
5. Unassigned Probability Mass
When faced with a highly skewed observed class
distribution for which there is little confidence due
to small sample size, a typical response is to back-
off or smooth to the more general class distribution.
Unfortunately, this representation makes problem-
atic the distinction between a back-off conditional
distribution and one based on a large sample (and
hence estimated with confidence). We address this
problem by explicitly representing the uncertainty
as a class, called "questionable". Probability mass
continues to be distributed among the primary en-
tity classes proportional to the observed distribu-
tion in the data, but with a total sum that reflects
ST
a
i
r
t
s
u
A
bA
... ...
z
... ...
RCT
#
...
,
h
i
#
C
h
i
r
a
#
H
o
l
a
n
rz
o
p
a
t
i
a
#
...
c
#
...
d
a
#
.
.
.
......
...
...
...
...
Figure 2: An example of links between the Suffix Trie and the
Right Context Trie for the entity candidate Austria and some of
its right contexts as observed in the corpus (< , Holanda >,
< , hizo >, < a Chirac >)
the confidence in the distribution and is equal to
 
	
.
Incremental learning essentially becomes the pro-
cess of gradually shifting probability mass from
questionable to one of the primary classes.
6. Smoothing
The probability of an entity candidate or context as
being or indicating a certain type of entity is com-
puted along the path from the root to the node in
the trie structure described above. In this way, ef-
fective smoothing can be realized for rare entities
or contexts. A smoothing formula taking advantage
of the distributional representation of uncertainty is
presented below.
For a sistring  	

Unsupervised Personal Name Disambiguation
Gideon S. Mann and David Yarowsky
Department of Computer Science
Johns Hopkins University
Baltimore, MD 21218 USA
{gsm,yarowsky}@cs.jhu.edu
Abstract
This paper presents a set of algorithms for
distinguishing personal names with mul-
tiple real referents in text, based on little
or no supervision. The approach utilizes
an unsupervised clustering technique over
a rich feature space of biographic facts,
which are automatically extracted via a
language-independent bootstrapping pro-
cess. The induced clustering of named
entities are then partitioned and linked to
their real referents via the automatically
extracted biographic data. Performance is
evaluated based on both a test set of hand-
labeled multi-referent personal names and
via automatically generated pseudonames.
1 Introduction
One open problem in natural language ambiguity
resolution is the task of proper noun disambigua-
tion1. While word senses and translation ambigu-
ities may typically have 2-20 alternative meanings
that must be resolved through context, a personal
name such as ?Jim Clark? may potentially refer to
hundreds or thousands of distinct individuals. Each
different referent typically has some distinct contex-
tual characteristics. These characteristics can help
distinguish, resolve and trace the referents when the
surface names appear in online documents.
A search of Google shows 76,000 web pages
mentioning Jim Clark, of which the first 10 unique
referents are:
1This has been recognized even by the popular press.
Reuters (March 13, 2003) observed the problem of name am-
biguity to be a major stumbling block in personal name web
searches.
1. Jim Clark - Race car driver from Scotland
2. Jim Clark - Clockmaker from Colorado
3. Jim Clark - Film Editor
4. Jim Clark - Netscape Founder
5. Jim Clark - Disaster Survivor
6. Jim Clark - Car Salesman in Kansas
7. Jim Clark - Fishing Instructor in Canada
8. Jim Clark - Computer Science student in Hong Kong
9. Jim Clark - Professor at McGill
10. Jim Clark - Gun Dealer in Louisiana
In this paper, we present a method for dis-
tinguishing the real world referent of a given
name in context. Approaches to this problem
include Wacholder et al (1997), focusing on the
variation of surface name for a given referent,
and Smith and Crane (2002), resolving geographic
name ambiguity. We present preliminary evaluation
on pseudonames: conflations of multiple personal
names, constructed in the same way pseudowords
are used for word sense disambiguation (Gale et al,
1992). We then present corroborating evidence from
real personal name polysemy to show that this tech-
nique works in practice.
Miles Davis
birth day May 26(5), May 25(5)
birth year 1926(82), 1967(18), 1969(9)...
occupation trumpeter(38), artist(10), player(5)...
birth place Alton(7), Illinois(6)
Joerg Haider
birth year 1950(6)
occupation leader(198) politician(93) chairman(6)...
birth place Austria(1)
Table 1: Extracted Biographical Information from
1000 Web Pages
Another topic of recent interest is in producing
biographical summaries from corpora (Schiffman et
al., 2001). Along with disambiguation, our system
simultaneously collects biographic information (Ta-
ble 1). The relevant biographical attributes are de-
picted along with a clustering which shows the dis-
tinct referents (Section 4.1).
2 Robust Extraction of Categorical
Biographic Data
Past work on this task (e.g. Bagga and Baldwin,
1998) has primarily approached personal name dis-
ambiguation using document context profiles or vec-
tors, which recognize and distinguish identical name
instances based on partially indicative words in con-
text such as computer or car in the Clark case. How-
ever, in the specialized case of personal names, there
is more precise information available.
In particular, information extraction techniques
can add high precision, categorical information such
as approximate age/date-of-birth, nationality and
occupation. This categorical data can support or
exclude a candidate name?referent matches with
higher confidence and greater pinpoint accuracy
than via simple context vector-style features alone.
Another major source of disambiguation infor-
mation for proper nouns is the space of associated
names. While these names could be used in a undif-
ferentiated vector-based bag-of-words model, fur-
ther accuracy can be gained by extracting specific
types of association, such as familial relationships
(e.g. son, wife), employment relationships (e.g.
manager of), and nationality as distinct from sim-
ple term co-occurrence in a window. The Jim Clark
married to ?Vickie Parker-Clark? is likely not the
same Jim Clark married to ?Patty Clark?. Addi-
tionally, information about one?s associates can help
predict information about the person in question.
Someone who frequently associates with Egyptians
is likely to be Egyptian, or at the very least, has a
close connection to Egypt.
2.1 Generating Extraction Patterns
One standard method for generating extraction pat-
terns is simply to write them by hand. In this paper,
we have experimented with generating patterns au-
tomatically from data. This has the advantage of be-
ing more flexible, portable and scalable, and poten-
tially having higher precision and recall. It also has
the advantage of being applicable to new languages
for which no developer with sufficient knowledge of
the language is available.
String Patterns with <person> and <birth year>
Extractions where <birth year> is a year
Extraction graded by #correct extractions/#total extractions
<person> ( <birth year> ? \d\d\d\d)
<person> <birthyear>?\d\d\d\d
<person> was born in <birth year>
<person> ( b.<birth year>
Extractions of <person> with potential <birth years>
Web Pages w/<person> and <birth year>
Sentences with <person> and <birth year>
Substrings with <person> and <birth year>
Web Pages with <person>
Sentences with <person>
1642
1685
1869
1770
1899
Ludwig van Beethoven
Humphrey Bogart
Mohandas Gandhi 
John Sebastian Bach
Isaac Newton 
<person>                       <birth year>
Figure 1: Learning Extraction Patterns from Filled
Templates and Web Pages
In the late 90s, there was a substantial body of
research on learning information extraction patterns
from templates (Huffman, 1995; Brin, 1998; Califf
and Mooney, 1998; Freitag and McCallum, 1999;
Yangarber et al, 2000; Ravichandran and Hovy,
2002). These techniques provide a way to bootstrap
information extraction patterns from a set of exam-
ple extractions or seed facts, where a tuple with the
filled roles for the desired pattern are given. For
the task of extracting biographical information, each
example would include the personal name and the
biographic feature. For example, training data for
the pattern born in might be (?Wolfgang Amadeus
Mozart?,1756). Given this set of examples, each
method generates patterns differently.
In this paper, we employ and extend the method
described by Ravichandran and Hovy (2002) shown
in Figure 1. For each seed fact pair for a given tem-
plate (such as (Mozart,1756)), a web query is made
which in turn leads to sentences in which the roles
are observed in nearby association (e.g. ?Mozart
was born in 1756?). All substrings from these sen-
tences are then extracted. The substrings are then
subject to simple generalization, to produce can-
didate patterns: Mozart is replaced by <name>,
1756 is replaced by <birth year>, and all digits
are replaced by #. These substring templates can
English Spanish
Purely Syntactic Patterns
Pattern Template Precision Count
<name> ( <birth year> - #### ) 1 31
<name> ( <birth year> - #### 1 31
- <name> ( <birth year>-#### ) 1 30
- <name> ( <birth year>-#### 1 30
<name> <birth year>-#### 1 27
<name> ( <birth year>-#### ) - 1 26
<name> <name> ( <birth year> 1 18
Syntactic & Lexical
Pattern Template Precision Count
<name> was born in <birth year> 1 19
<name> was born in <birth year> in 1 12
by <name> ( <birth year>-#### ) 1 10
by <name> ( <birth year>-#### 1 10
of <name> ( <birth year>-#### ) 0.933 15
of <name> ( <birth year>-#### 0.933 15
<name> ( <birth year>-#### ) was 0.833 12
Purely Syntactic Patterns
Pattern Template Precision Count
. <name> ( <birth year>- 1 62
. <name> ( <birth year>-## 1 58
. <name> ( <birth year>-#### 1 55
. <name> ( <birth year>-#### ) 1 54
<name> ( <birth year>-#### ) : 1 38
<name> <birth year>-#### , 1 26
<name> , <birth year>-#### 1 25
Syntactic & Lexical
Pattern Template Precision Count
a <name> ( <birth year>-#### 1 30
a <name> ( <birth year>-#### ) 1 29
<birth year> . - Nace <name> 1 21
<birth year> . - Nace <name> , 1 17
<name> ( <birth year>-#### ) , con 1 15
<name> ( <birth year>-#### ) se 1 12
, de <name> ( <birth year>-#### ) 1 12
Table 2: Highest Precision Patterns Extracted for English and Spanish using Suffix Tree Methodology
then serve as extraction patterns for previously un-
known fact pairs, and their precision in fact extrac-
tion can be calculated with respect to a set of cur-
rently known facts.
We examined a subset of the available and desir-
able extracted information. We learned patterns for
birth year and occupation, and hand-coded patterns
for birth location, spouse, birthday, familial relation-
ships, collegiate affiliations and nationality. Other
potential patterns currently under investigation in-
clude employer/employee and place of residence.
2.2 Multilingual Information Extraction
We adapted the information extraction pattern gen-
eration techniques described above to multiple lan-
guages. In particular, the methodology proposed by
Ravichandran and Hovy (2002) requires no parsing
or other language specific resources, so is an ideal
candidate for multilingual use. In this paper, we
conducted an initial test test of the viability of in-
ducing these information extraction patterns across
languages. To test, we constructed a initial database
of 5 people and their birthdays, and used this to in-
duce the English patterns. We then increased the
database to 50 people and birthdays and induced pat-
terns for Spanish, presenting the results above. Fig-
ure 2 shows the top precision patterns extracted for
English and for Spanish.
It can be seen that the Spanish patterns are of
the same length, with similar estimated precision, as
well as similar word and punctuation distribution as
the English ones. In fact, the purely syntactic pat-
terns look identical. The only difference being that
to generate equivalent Spanish data, a database of
training examples an order of magnitude larger was
required. This may be because for each database en-
try more pages were available on English websites
than on Spanish websites.
3 Using Unsupervised Clustering to
Identify the Referents of Personal Names
This section examines clustering of web pages
which containing an ambiguous personal name
(with multiple real referents). The cluster method
we employed is bottom-up centroid agglomerative
clustering. In this method, each document is as-
signed a vector of automatically extracted features.
At each stage of the clustering, the two most similar
vectors are merged, to produce a new cluster, with
a vector equal to the centroid of the vectors in the
cluster. This step is repeated until all documents are
clustered.
To generate the vectors for each document, we ex-
plored a variety of methods:
1. Baseline : All words (plain) or only Proper
Nouns (nnp)
2. Most Relevant words (mi and tf-idf)
3. Basic biographical features (feat)
4. Extended biographical Features (extfeat)
word weight(mi) weight(extfeat)
adderley 5.30 0
snipes 5.16 0
coltrane 5.06 0
montreux 5.01 0
bitches 4.99 0
danson 4.97 0
hemp 4.97 0
mullally 4.95 0
porgy 4.94 0
remastered 4.92 0
actor 3.50 2.40
1926 0 2.20
trumpeter 0 2.20
midland 0 1.39
Table 3: The 10 words with highest mutual infor-
mation with the document collection and all of ex-
tended feature words for DAVIS/HARRELSON pseudon-
ame
3.1 Baseline Models
In our baseline models, we used term vectors com-
posed either of all words (minus a set of closed class
?stop? words) or of only proper nouns. To assess
similarity between vectors we utilized standard co-
sine similarity (cos(a, b) = a?b||a||?||b||).
We experimentally determined that the use of
proper nouns alone led to more pure clustering. As
a result, for the remainder of the experiments, we
used only proper nouns in the vectors, except for
those common words introduced by the various fea-
ture sets.
3.2 Relevant Words (mi and tf-idf)
Selective term weighting has been shown to be
highly effective for information retrieval. For this
study, we investigated both the use of standard TF-
IDF weighting and weighting based on the mutual
information, where given a document collection c,
for each word w, we calculate I(w; c) = p(w|c)p(w) .
From these, we select words which appear more
than ?1 = 20 times in the collection, and have a
I(w; c) greater than ?2 = 10. These words are to
the document?s feature vector with a weight equal to
log(I(w; c)).
3.3 Extracted Biographical Features (feat)
The next set of models use the features extracted
using the methodology described in Section 2. Bi-
ographical information such as birth year, and oc-
cupation, when found, is quite useful in connecting
documents. If a document connects a name with a
birth year, and another document connects the same
name with the same birth year, typically, those two
documents refer to the same person.
Type Extracted Feature
birth place Midland(4), Texas (3), Alton(1),
Illinois(1)
birth year 1926 (9), 1967(3), 1973(2),
1947(1), 1958(1), 1969(1)
occupation actor (11), trumpeter(9),
heavyweight(2) ...
spouse Demi Moore(1)
Table 4: feat: Features Extracted for
DAVIS/HARRELSON pseudoname
These extracted features were used to categori-
cally cluster documents in which they appeared. Be-
cause of their high degree of precision and speci-
ficity, documents which contained similar extracted
features are virtually guaranteed to have the same
referent. By clustering these documents first, large
high quality clusters formed, which then then pro-
vided an anchor for the remaining pages. By ex-
amining the dendrogram in Figure 3, it is clear that
the clusters start with documents with matching fea-
tures, and then the other documents cluster around
this core.
In addition to improving disambiguation perfor-
mance, these extracted features help distinguish the
different clusters, and provide information about the
different people.
3.4 Extended Biographical Features (extfeat)
Another method for using these extracted features is
to give higher weight to words which have ever been
seen as filling a pattern. For example, if 1756 is ex-
tracted as a birth year from a syntactic-based pattern
for the polysemous name, then whenever 1756 is
observed anywhere in context (outside an extraction
pattern), it is given a higher weighting and added to
the document vector as a potential biographic fea-
ture. In our experiments, we did this only for words
which appeared as values for a feature more than a
threshold of 4 times. Then, whenever the word was
seen in a document, it was given a weight equal to
the log of the number of times the word was seen as
an extracted feature.
actor comedy  |     spouse:Demi Moore  |       Woody Harrelson|                        |       Woody Harrelson|                        |       Woody Harrelson
comedy starring  |                        |       Woody Harrelson
movie  |                        |       Woody Harrelson
actor movie  |                        |       Woody Harrelson|             occ:actor  |       Woody Harrelson|             occ:actor  |       Woody Harrelson|             occ:actor  |       Woody Harrelson
movie  |             occ:actor  |       Woody Harrelson
actor sleeve  |             occ:actor  |       Woody Harrelson
actor  |             occ:actor  |       Woody Harrelson|                        |       Woody Harrelson|          occ:resident  |       Woody Harrelson
movie  |                        |       Woody Harrelson|                        |       Woody Harrelson|                        |       Woody Harrelson|                        |       Woody Harrelson|                        |       Woody Harrelson|                        |       Woody Harrelson
actor star  |                        |       Woody Harrelson
actor star  |                        |       Woody Harrelson
Seed1
formats  |                        |       Woody Harrelson
formats  |                        |           Miles Davis|            byear:1926  |           Miles Davis
quintet  |            byear:1926  |           Miles Davis
formats  |                        |           Miles Davis|                        |           Miles Davis|                        |           Miles Davis|                        |           Miles Davis|                        |           Miles Davis|            byear:1973  |           Miles Davis|            byear:1973  |           Miles Davis
click title  |                        |           Miles Davis
click title  |                        |           Miles Davis|         occ:trumpeter  |           Miles Davis|         occ:trumpeter  |           Miles Davis
interviews...  |         occ:trumpeter  |           Miles Davis
biography...  |  byear:1967,occ:trumpeter  |           Miles Davis|         occ:trumpeter  |           Miles Davis|            byear:1967  |           Miles Davis
recordings  |            byear:1967  |           Miles Davis
recordings  |                        |           Miles Davis
quintet trumpet  |      brthloc:Alton,IL  |           Miles Davis|                        |           Miles Davis
trumpet  |                        |           Miles Davis|                        |           Miles Davis|                        |           Miles Davis
artist  |                        |           Miles Davis|                        |           Miles Davis
album jazz  |                        |           Miles Davis
album  |                        |           Miles Davis
album music  |                        |           Miles Davis|                        |           Miles Davis|                        |           Miles Davis
de  |                        |           Miles Davis
quintet trumpet  |                        |           Miles Davis
music nbsp  |          occ:musician  |           Miles Davis
album  |          occ:musician  |           Miles Davis|                        |           Miles Davis|            byear:1969  |           Miles Davis|                        |           Miles Davis
artist  |                        |           Miles Davis|                        |           Miles Davis
Seed2
0.0 0.05 0.1 0.15 0.2
keywords features referent
Induced Seed 1
Induced Seed 2
Figure 2: nnp+feat+extfeat+mi Clustering Visual-
ization for DAVIS/HARRELSON pseudoname
3.5 Cluster Refactoring
Ideally, the raw unsupervised clustering would yield
a top level distinction between the different refer-
ents. However, this is rarely the case. With this
type of agglomerative clustering, the most similar
pages are clustered first, and outliers are assigned
as stragglers at the top levels of the cluster tree.
This typically leads to a full clustering where the
top-level clusters are significantly less discrimina-
tive than those at the roots. In order to compensate
for this effect, we performed a type of tree refac-
toring, which attempted to pick out and utilize seed
clusters from within the entire clustering.
In the refactoring, the clustering is stopped be-
fore it runs to completion, based on the percentage
of documents clustered and the relative size of the
clusters achieved. At this intermediate stage, rel-
atively large and high-precision clusters are found
(e.g. Figure 2). These automatically-induced clus-
ters are then used as seeds for the next stage, where
the unclustered documents are assigned to the seed
with the closest distance measure (Figure 3).
An alternative to this form of cluster refactoring
would be to initially cluster only pages with ex-
tracted features. This would yield a set of cluster
seeds, divided by features, which could then be used
for further clustering. However, this method relies
on having a number of pages with extracted features
that overlap from each referent. This can only be
actor comedy  |     spouse:Demi Moore  |       Woody Harrelson|                        |       Woody Harrelson|                        |       Woody Harrelson
comedy starring  |                        |       Woody Harrelson
movie  |                        |       Woody Harrelson
actor movie  |                        |       Woody Harrelson|             occ:actor  |       Woody Harrelson|             occ:actor  |       Woody Harrelson|             occ:actor  |       Woody Harrelson
movie  |             occ:actor  |       Woody Harrelson
actor sleeve  |             occ:actor  |       Woody Harrelson
actor  |             occ:actor  |       Woody Harrelson|                        |       Woody Harrelson|          occ:resident  |       Woody Harrelson
movie  |                        |       Woody Harrelson|                        |       Woody Harrelson|                        |       Woody Harrelson|                        |       Woody Harrelson|                        |       Woody Harrelson|                        |       Woody Harrelson
actor star  |                        |       Woody Harrelson
actor star  |                        |       Woody Harrelson
Seed1
hemp  |  brthloc:Midland,Texas  |       Woody Harrelson
hemp  |  brthloc:Midland,Texas  |       Woody Harrelson|         brthloc:Texas  |       Woody Harrelson|                        |       Woody Harrelson
film movie  |                        |       Woody Harrelson
solo  |                        |           Miles Davis|                        |       Woody Harrelson|                        |       Woody Harrelson
formats killers  |             occ:lover  |       Woody Harrelson|                        |       Woody Harrelson
comedy reviews  |                        |       Woody Harrelson
com  |                        |       Woody Harrelson|                        |           Miles Davis|                        |           Miles Davis
actor  |                        |       Woody Harrelson|                        |       Woody Harrelson|                        |       Woody Harrelson|                        |       Woody Harrelson
encoding  |                        |       Woody Harrelson|                        |       Woody Harrelson
marijuana  |                        |       Woody Harrelson
movie  |                        |       Woody Harrelson
film marijuana  |                        |       Woody Harrelson
marijuana  |                        |       Woody Harrelson|                        |       Woody Harrelson
actor marijuana  |                        |       Woody Harrelson
upcoming  |                        |       Woody Harrelson
starring  |                        |       Woody Harrelson|                        |       Woody Harrelson
film movie  |       occ:heavyweight  |       Woody Harrelson
upcoming  |                        |       Woody Harrelson|                        |       Woody Harrelson
biography  |                        |       Woody Harrelson
biography  |                        |       Woody Harrelson|                        |           Miles Davis
movie  |                        |       Woody Harrelson|                        |       Woody Harrelson|                        |       Woody Harrelson|                        |       Woody Harrelson
bowling  |                        |       Woody Harrelson
bowling soundtrack...  |                        |       Woody Harrelson|                        |       Woody Harrelson|                        |       Woody Harrelson
music  |                        |           Miles Davis
actor album  |                        |       Woody Harrelson|                        |       Woody Harrelson|                        |           Miles Davis|                        |       Woody Harrelson
com musician reviews  |                        |           Miles Davis|                        |       Woody Harrelson|                        |       Woody Harrelson
marijuana  |            occ:farmer  |       Woody Harrelson
hemp  |                        |       Woody Harrelson
hemp tune  |                        |       Woody Harrelson
hemp  |          occ:activist  |       Woody Harrelson
title ~  |                        |       Woody Harrelson
title ~  |                        |       Woody Harrelson
title ~  |                        |       Woody Harrelson
title  |                        |           Miles Davis|                        |       Woody Harrelson
com  |                        |       Woody Harrelson
com reviews  |                        |       Woody Harrelson
blues  |                        |           Miles Davis|                        |       Woody Harrelson|                        |       Woody Harrelson|                        |       Woody Harrelson|                        |       Woody Harrelson
actors  |                        |       Woody Harrelson
review  |                        |       Woody Harrelson
click  |                        |       Woody Harrelson|                        |           Miles Davis|                        |       Woody Harrelson|                        |       Woody Harrelson|                        |       Woody Harrelson
comedy guest  |                        |       Woody Harrelson
amp  |                        |       Woody Harrelson
w/  |                        |       Woody Harrelson
nbsp w/  |                        |       Woody Harrelson|                        |           Miles Davis
nbsp  |                        |           Miles Davis|                        |           Miles Davis|                        |       Woody Harrelson
arrange celebrity...  |                        |       Woody Harrelson|                        |           Miles Davis|                        |           Miles Davis
evil smile smiles  |                        |           Miles Davis|                        |           Miles Davis|                        |           Miles Davis|                        |           Miles Davis
de  |                        |       Woody Harrelson|                        |           Miles Davis|                        |           Miles Davis
formats  |                        |       Woody Harrelson
formats  |                        |           Miles Davis|            byear:1926  |           Miles Davis
quintet  |            byear:1926  |           Miles Davis
formats  |                        |           Miles Davis|                        |           Miles Davis|                        |           Miles Davis|                        |           Miles Davis|                        |           Miles Davis|            byear:1973  |           Miles Davis|            byear:1973  |           Miles Davis
click title  |                        |           Miles Davis
click title  |                        |           Miles Davis|         occ:trumpeter  |           Miles Davis|         occ:trumpeter  |           Miles Davis
interviews...  |         occ:trumpeter  |           Miles Davis
biography...  |  byear:1967,occ:trumpeter  |           Miles Davis|         occ:trumpeter  |           Miles Davis|            byear:1967  |           Miles Davis
recordings  |            byear:1967  |           Miles Davis
recordings  |                        |           Miles Davis
quintet trumpet  |      brthloc:Alton,IL  |           Miles Davis|                        |           Miles Davis
trumpet  |                        |           Miles Davis|                        |           Miles Davis|                        |           Miles Davis
artist  |                        |           Miles Davis|                        |           Miles Davis
album jazz  |                        |           Miles Davis
album  |                        |           Miles Davis
album music  |                        |           Miles Davis|                        |           Miles Davis|                        |           Miles Davis
de  |                        |           Miles Davis
quintet trumpet  |                        |           Miles Davis
music nbsp  |          occ:musician  |           Miles Davis
album  |          occ:musician  |           Miles Davis|                        |           Miles Davis|            byear:1969  |           Miles Davis|                        |           Miles Davis
artist  |                        |           Miles Davis|                        |           Miles Davis
Seed2
tracks  |            byear:1958  |           Miles Davis|                        |       Woody Harrelson
formats rehkram  |                        |           Miles Davis|                        |       Woody Harrelson
fusion music...  |                        |           Miles Davis|                        |           Miles Davis
idem  |                        |           Miles Davis
biography videos  |                        |       Woody Harrelson
nbsp  |                        |           Miles Davis|                        |       Woody Harrelson
discography  |                        |           Miles Davis
solos  |                        |           Miles Davis|                        |           Miles Davis|                        |           Miles Davis
disc  |                        |           Miles Davis|                        |           Miles Davis|                        |           Miles Davis|                        |           Miles Davis
music  |                        |           Miles Davis|                        |           Miles Davis|                        |           Miles Davis
music  |                        |           Miles Davis
com poster  |                        |           Miles Davis
poster  |                        |           Miles Davis
poster  |                        |           Miles Davis|                        |           Miles Davis
soundtrack  |                        |           Miles Davis|                        |           Miles Davis
discography  |            byear:1947  |           Miles Davis|                        |           Miles Davis|            occ:artist  |           Miles Davis|                        |           Miles Davis|                        |           Miles Davis|                        |           Miles Davis
bowling  |                        |       Woody Harrelson
musicians recordings...  |            occ:player  |           Miles Davis
music trumpet  |                        |           Miles Davis
discography trumpet  |                        |           Miles Davis
flute saxophone...  |                        |           Miles Davis
musicians  |                        |           Miles Davis
studio  |                        |           Miles Davis|                        |       Woody Harrelson|                        |           Miles Davis|                        |           Miles Davis
0.0 0.2 0.4 0.6 0.8
keywords referentfeatures
Induced
Cluster 1
Induced
Cluster
2
Figure 3: nnp+feat+extfeat+mi Clustering Visual-
ization for DAVIS/HARRELSON pseudoname
assured when the feature set is rich, or a large docu-
ment space is assumed.
4 Experiments
To test these clustering methods, we collected web
pages by making requests to the Google website for
a set of target personal names (up to a maximum
of 1000 pages per name). There was no require-
ment that the web page be focused on that name, nor
was there a minimum number of name occurrences.
As a result, some pages clustered only mentioned
the name in passing, or in a specialized, commercial
context (e.g. Amazon sales product).
The pseudonames were created as follows. The
retrieval results from two different randomly-
selected people were taken, and all references to
either name (in full or part) replaced by a unique,
shared pseudoname. The resulting collection then
consisted of documents which were ambiguous as
to whom they talked about. The aim of the clus-
tering was then to distinguish this artificially con-
flated pseudoname. In addition, a test set of four
naturally occurring polysemous names (such as Jim
Clark), containing an average of 60 instances each,
was manually annotated with distinguishing nameID
numbers and used for a parallel evaluation.
The experiments consist of two parts. The first
output is the clustering visualizations whose utility
can be judged by inspection. The second is a quanti-
tative analysis of the different methodologies. Both
are conducted over test sets of pseudonames and nat-
urally occurring ambiguities.
4.1 Clustering Visualizations
Figures 2/3 and 4 each have two subfigures. The
left/top figure shows the extracted seed sets. The
right/bottom figure shows the final clustering of the
entire document collection. In each figure, there
are three columns of information before the dendro-
gram. The first column contains high weighted doc-
ument content words. The second column contains
the extracted features from the document. The third
column indicates the real referent. This is either the
real name of the conflated pseudoname (e.g. Woody
Harrelson or Miles Davis), or a number indicating
the referent (e.g. 1 - 20 in the case of Jim Clark).
This presentation allows a quick scan of the cluster-
ing to reveal correlations.
In general, the visualizations are informative. Oc-
casionally, the extractions err. One time when the
patterns themselves cannot be syntactically faulted
comes in the case where Woody Harrelson?s wife
is extracted as Demi Moore. The information was
extracted from the sentence: ?Architect Woody Har-
relson and his wife realtor Demi Moore ...? which
appears as a plot description for the movie ?Inde-
driver racing  |            occ:driver  |      1
racing  |           brthyr:1968  |      1
racing  |                        |      1
championship  |                        |      1
championship  |                        |      1|                        |      1
racing  |                        |      1
racing  |                        |      1
road  |                        |      1
statistics  |                        |      1|                        |      1
championship driver  |                        |      1
championship rally  |                        |      1|                        |      1
link  |                        |      1|                        |      1
rally  |                        |      1|                        |      1|                        |      1|                        |      1
rally  |                        |      1
promoter rally  |                        |      1|                        |      1
championship  |                        |      1
Seed1
|                        |      4|                        |      4|                        |      4|                        |      4
founder  |                        |      4
software  |                        |      4|                        |    All|                        |      4|           occ:founder  |      4|           occ:founder  |      4|           occ:founder  |      4|                        |      4
software  |                        |      4|       occ:billionaire  |      4
graphics  |                        |      4|                        |      4|                        |      4|                        |      4|                        |      4|      occ:entrepreneur  |      4|                        |      4|                        |      4
browser graphics  |                        |      4
software  |                        |      4|                        |      9|                        |     11|                        |     11|                        |     11
prevention  |                        |     19|                        |     17|                        |     17
Seed2
0.0 0.1 0.2 0.3 0.4 0.5
Induced Seed 1 (1 = Racing Jim Clark)
Induced Seed 2 (4 = Netscape Jim Clark)
keywords features referent
driver racing  |            occ:driver  |      1
racing  |           brthyr:1968  |      1
racing  |                        |      1
championship  |                        |      1
championship  |                        |      1|                        |      1
racing  |                        |      1
racing  |                        |      1
road  |                        |      1
statistics  |                        |      1|                        |      1
championship driver  |                        |      1
championship rally  |                        |      1|                        |      1
link  |                        |      1|                        |      1
rally  |                        |      1|                        |      1|                        |      1|                        |      1
rally  |                        |      1
promoter rally  |                        |      1|                        |      1
championship  |                        |      1
Seed1
|                        |      4|                        |      4|                        |      4|                        |      4
founder  |                        |      4
software  |                        |      4|                        |    All|                        |      4|           occ:founder  |      4|           occ:founder  |      4|           occ:founder  |      4|                        |      4
software  |                        |      4|       occ:billionaire  |      4
graphics  |                        |      4|                        |      4|                        |      4|                        |      4|                        |      4|      occ:entrepreneur  |      4|                        |      4|                        |      4
browser graphics  |                        |      4
software  |                        |      4|                        |      9|                        |     11|                        |     11|                        |     11
prevention  |                        |     19|                        |     17|                        |     17
Seed2
|                        |     15|                        |     18|                        |     20|                        |     10|                        |      3|                        |      3
links  |                        |     14|                        |     14|                        |      2|                        |      5|                        |      5|                        |     13|                        |      1
software  |                        |      9|                        |      6|                        |     12|                        |      7|                        |     16|                        |      8
0.0 0.2 0.4 0.6 0.8
Induced Cluster 1 
(1 = Racing Jim Clark)
Induced Cluster 2
keywords features referent
(4 = Netscape Jim Clark)
Figure 4: nnp+feat+extfeat+mi Clustering Visual-
ization of Jim Clark Pages: ?1?=Race Car Driver,
?4?=Netscape Founder, ?A?=multiple referents
cent Proposal?. Here, untangling of synecdoche is
needed. For Miles Davis, the incorrectly extracted
birth years refer to record release dates, which take
the same surface form as birth years in some genres.
Figure 4 shows a clustering for a naturally occur-
ing name ambiguity, in particular that of web pages
which refer to ?Jim Clark?. The set was constructed
by retrieving 100 web pages, and then labeling the
pages with respect to their referent. As can be seen,
the clusterings are highly coherent. All of the rele-
vant pages are included in the seed set, and few in-
appropriate pages are added. This type of clustering
would be useful to someone searching for a specific
individual named Jim Clark. Once the clustering had
been performed, a user could scan the output, and
identity the ?Jim Clark? of interest, based both on
extracted features and key words.
4.2 Evaluation on Pseudonames
For automated pseudoname evaluation purposes, we
selected a set of 8 different people for confla-
tion, who we presumed had one vastly predominant
sense. We selected these people giving room for his-
torical figures, figures from pop culture and mod-
ern media culture, as well as ?ordinary? people. We
added people with similar backgrounds (born close
to each other, or having the same profession). The
full list was composed of these 8 individuals:
Haifa Al-Faisal, William Blake, Tom
Cruise, Woody Harrelson, Hermann
Hesse, Wolfgang Amadeus Mozart, Anna
Shusterman, Bryon Tosoff
For each, we submitted Google queries, and re-
trieved up to 1000 pages each. We then took these
hit returns, and subsampled to a maximum of 100
pages per person. The person with the smallest rep-
resentation was Anna Shusterman with 26 pages.
We subsampled by taking the first 100 as ordered
lexically. This may have biased the results some-
what towards unreliable web pages, since pages with
numeric addresses tend to be newer and more tran-
sient.
We evaluated two guanularities of feature extrac-
tion. The small feature set uses high precision
rules to extract occupation (occ), birthday (brthyr),
spouse, birth location (brthloc), and school. The
large feature set adds higher recall (and therefore
noisier) patterns for the previous relationships and
as well as parent/child relationships.
As can be seen from the table, the highest per-
forming system combines proper nouns, relevant
words, and the high precision extracted features
(nnp+feat+mi and nnp+feat+tfidf). The extended
features (nnp+feat+extfeat+mi) do not give addi-
tional benefit to this combination. As can be seen
from the table, the large feature set yields better
overall performance than the smaller feature set.
Clustering Method Disambiguation Accuracy
no extracted features
majority sense 62.5
plain 74.5
tfidf 76.7
nnp 79.7
nnp+tfidf 79.7
nnp+mi 82.9
w/ extracted features feature set size
small large
nnp+feat 82.5 85.1
nnp+feat+extfeat 82.0 84.6
nnp+feat+mi 85.6 85.2
nnp+feat+tfidf 82.9 86.4
Table 5: Disambiguation Accuracy of different
Clustering Methods over 28 pseudonames
This suggests that the increased coverage outweighs
the introduced noise.
For the feat+tfidf system, accuracy at the two-
class disambiguation was above 80% for 25 out of
the 28 pairs. Without these pairs, the average two-
class disambiguation performance over the remain-
ing pairs is 90%. In two of the problematic cases, the
contexts of the names are easily confusable, as the
individuals share the same profession and many of
the same keywords. More complete biographic pro-
files and different clustering biases would be helpful
in fully partitioning these cases. However, in prac-
tice these pseudoname pair situations may be more
difficult than expected for naturally occurring name
pairs. In many occupations that are typically news-
worthy (such as actors, authors, musicians, politi-
cians, etc.), there may be a tendency for individu-
als to avoid using identical names (or entering the
field entirely) to minimize confusion. When people
with identical names do indeed share the same field
one would expect a greater effort to providing dis-
ambiguating contextual features to distinguish them.
We have made some preliminary investigations
into selecting pages according to the number of
mentions, as opposed to by random. The results
have not been conclusive, and continuing work is in-
vestigating the cause.
4.3 Evaluation on Naturally Ambiguous Names
The above results have utilized pseudoname test sets
where high accuracy ground truth is automatically
available in large quantities [O(1000) examples per
name] to better distinguish model performance. Ta-
ble 6 shows the performance on the four O(60) ex-
ample hand-labeled test sets for naturally occurring
polysemous person names. Given that this is an
n-ary classification task, for consistency with the
above experiments the data were assigned to one
of 3 clusters, corresponding to the 2 automatically
derived first-pass majority seed sets and the resid-
ual ?other-use? classification, but evaluated strictly
on performance for the two major senses. While
additional analyses could be accomplished on the
residual sets, this is difficult given their small size
(remaining personal exemplars were mostly single-
tons) and lack of evidence on many single-mention
web pages. Thus the task of accurately partitioning
the two most common uses and clustering the resid-
ual examples for visual exploration may be a natural
and practical use for these classification and visual-
ization technologies.
Weighting Method Precision Recall
TF-IDF .81 .70
Mutual Information .88 .73
Table 6: Classification performance for naturally
occurring name ambiguities on 3-way classification
task (Majority-Use, Secondary-Use, Other-Use).
5 Conclusion
In this paper we have presented a set of algorithms
for finding the real referents for ambiguous per-
sonal names in text using unsupervised clustering
and feature extraction methods. In particular, we
have shown how to learn and use automatically ex-
tracted biographic information to improve clustering
results, and have demonstrated this improvement by
evaluating on pseudonames. We have presented ini-
tial results on learning these patterns to extract bio-
graphic information for multiple languages, and in-
tend to use these techniques for large-scale multilin-
gual polysemous name clustering.
The results presented here support the automatic
clustering of polysemous personal name referents
and visualization of these induced clusters and their
motivating features. These distinct referents can
be verified by inspection both of extracted features
and of the high weighted terms for each document.
These clusterings may be useful in two ways. First
as a useful visualization tool themselves, and second
as seeds for disambiguating further entities.
References
A. Bagga and B. Baldwin. 1998. Entity-based cross-document
coreferencing using the vector space model. In Chris-
tian Boitet and Pete Whitelock, editors, Proceedings of the
Thirty-Sixth Annual Meeting of the Association for Compu-
tational Linguistics and Seventeenth International Confer-
ence on Computational Linguistics, pages 79?85, San Fran-
cisco, California. Morgan Kaufmann Publishers.
S. Brin. 1998. Extracting patterns and relations from the world
wide web. In WebDB Workshop at 6th International Confer-
ence on Extending Database Technology, EDBT?98.
M. E. Califf and R. J. Mooney. 1998. Relational learning
of pattern-match rules for information extraction. In Work-
ing Notes of AAAI Spring Symposium on Applying Machine
Learning to Discourse Processing, pages 6?11, Menlo Park,
CA. AAAI Press.
D. Freitag and A. McCallum. 1999. Information extraction
with hmms and shrinkage. In Proceedings of the AAAI-99
Workshop on Machine Learning for Information Extraction.
B. Gale, K. Church, and D. Yarowsky. 1992. Work on statisti-
cal methods for word sense disambiguation. In AAAI Fall
Symposium on Probabilistic Approaches to Natural Lan-
guage Processing, pages 54?60, Cambridge, MA.
S. B. Huffman. 1995. Learning information extraction patterns
from examples. In Learning for Natural Language Process-
ing, pages 246?260.
D. Ravichandran and E. Hovy. 2002. Learning surface text pat-
terns for a question answering system. In Proceedings of the
40th Annual Meeting of the Association for Computational
Linguistics.
B. Schiffman, I. Mani, and K. J. Concepcion. 2001. Producing
biographical summaries: Combining linguistic knowledge
with corpus statistics. In Proceedings of the 39th Annual
Meeting of the Association for Computational Linguistics.
D. A. Smith and G. Crane. 2002. Disambiguating geographic
names in a historic digital library. In Proceedings of ECDL,
pages 127?136.
N. Wacholder, Y. Ravin, and M. Choi. 1997. Disambiguation
of proper names in text. In Proceedings of Fifth Conference
on Applied Natural Language Processing, pages 202?208.
R. Yangarber, R. Grishman, P. Tapanainen, and S. Huttunen.
2000. Unsupervised discovery of scenario-level patterns for
information extraction. In Proceedings of the Sixth Con-
ference on Applied Natural Language Processing, (ANLP-
NAACL 2000), pages 282?289.
Statistical Machine Translation Using Coercive Two-Level Syntactic
Transduction
Charles Schafer and David Yarowsky
Center for Language and Speech Processing / Department of Computer Science
Johns Hopkins University
Baltimore, MD 21218 USA
{cschafer,yarowsky}@cs.jhu.edu
Abstract
We define, implement and evaluate a novel model for
statistical machine translation, which is based on shal-
low syntactic analysis (part-of-speech tagging and phrase
chunking) in both the source and target languages. It
is able to model long-distance constituent motion and
other syntactic phenomena without requiring a full parse
in either language. We also examine aspects of lexical
transfer, suggesting and exploring a concept of transla-
tion coercion across parts of speech, as well as a transfer
model based on lemma-to-lemma translation probabili-
ties, which holds promise for improving machine trans-
lation of low-density languages. Experiments are per-
formed in both Arabic-to-English and French-to-English
translation demonstrating the efficacy of the proposed
techniques. Performance is automatically evaluated via
the Bleu score metric.
1 Introduction
In this work we define, implement and evaluate a novel
model for statistical machine translation (SMT).
Our goal was to produce a SMT system for translat-
ing foreign languages into English which utilizes some
syntactic information in both the foreign language and
English without, however, requiring a full parse in either
language. Some advantages of not relying on full parses
include that (1) there is a lack of availability of parsers
for many languages of interest; (2) parsing time com-
plexity represents a potential bottleneck for both model
training and testing.
Intuitively, the explicit modeling of syntactic phenom-
ena should be of benefit in the machine translation task;
the ability to handle long-distance motion in an intelli-
gently constrained way is a salient example of such a
benefit. Allowing unconstrained translation reorderings
at the word level generates a very large set of permu-
tations that pose a difficult search problem at decoding
time. We propose a model that makes use of shallow
parses (text chunking) to support long-distance motion
of phrases without requiring deeper analysis of syntax.
The resources required to train this system on a new lan-
guage are minimal, and we gain the ability to model
long-distance movement and some interesting proper-
ties of lexical translation across parts of speech. One of
the source languages we examine in this paper, Arabic,
has a canonical sentence-level order of Verb-Subject-
Object, which means that translation into English (with
a standard ordering of Subject-Verb-Object) commonly
requires motion of entire phrasal constituents, which is
not true of French-to-English translation, to cite one lan-
guage pair whose characteristics have wielded great in-
fluence in the history of work on statistical machine
translation. A key motivation for and objective of this
work was to build a translation model and feature space
to handle the above-described phenomenon effectively.
2 Prior Work
Statistical machine translation, as pioneered by IBM
(e.g. Brown et al, 1993), is grounded in the noisy chan-
nel model. And similar to the related channel problems
of speech and handwriting recognition, the original SMT
language pair French-English exhibits a relatively close
linear correlation in source and target sequence. Much
common local motion that is observed for French, such
as adjective-noun swapping, is adequately modeled by
the relative-position-based distortion models of the clas-
sic IBM approach. Unfortunately, these distortion mod-
els are less effective for languages such as Japanese or
Arabic, which have substantially different top-level sen-
tential word orders from English, and hence longer dis-
tance constituent motion.
Wu (1997) and Jones and Havrilla (1998) have sought
to more closely tie the allowed motion of constituents
between languages to those syntactic transductions sup-
ported by the independent rotation of parse tree con-
stituents. Yamada and Knight (2000, 2001) and Alshawi
et al (2000) have effectively extended such syntactic
transduction models to fully functional SMT systems,
based on channel model tree transducers and finite state
head transducers respectively. While these models are
well suited for the effective handling of highly divergent
sentential word orders, the above frameworks have a lim-
itation shared with probabilistic context free grammars
that the preferred ordering of subtrees is insufficiently
constrained by their embedding context, which is espe-
cially problematic for very deep syntactic parses.
In contrast, Och et al (1999) have avoided the con-
straints of tree-based syntactic models and allow the rel-
atively flat motion of empirically derived phrasal chunks,
which need not adhere to traditional constituent bound-
aries.
Our current paper takes a middle path, by grounding
motion in syntactic transduction, but in a much flatter 2-
level model of syntactic analysis, based on flat embed-
ded noun-phrases in a flat sentential constituent-based
chunk sequence that can be driven by syntactic brack-
eters and POS tag models rather than a full parser, facili-
tating its transfer to lower density languages. The flatter
2-level structures also better support transductions condi-
tioned to full sentential context than do deeply embedded
tree models, while retaining the empirically observed ad-
vantages of translation ordering independence of noun-
phrases.
Another improvement over Och et al and Yamada and
Knight is the use of the finite state machine (FSM) mod-
elling framework (e.g. Bangalore and Riccardi, 2000),
which offers the considerable advantage of a flexible
framework for decoding, as well as a representation
which is suitable for the fixed two-level phrasal mod-
elling employed here.
Finally, the original cross-part-of-speech lexical coer-
cion models presented in Section 4.3.3 have related work
in the primarily-syntactic coercion models utilized by
Dorr and Habash (2002) and Habash and Door (2003),
although their induction and modelling are quite differ-
ent from the approach here.
3 Resources
As in other SMT approaches, the primary training re-
source is a sentence-aligned parallel bilingual corpus.
We further require that each side of the corpus be part-
of-speech (POS) tagged and phrase chunked; our lab
has previously developed techniques for rapid training
of such tools (Cucerzan and Yarowsky, 2002). Our trans-
lation experiments were carried out on two languages:
Arabic and French. The Arabic training corpus was a
subset of the United Nations (UN) parallel corpus which
is being made available by the Linguistic Data Consor-
tium. For French-English training, we used a portion of
the Canadian Hansards. Both corpora utilized sentence-
level alignments publicly distributed by the Linguistic
Data Consortium.
POS tagging and phrase chunking in English were
done using the trained systems provided with the fnTBL
Toolkit (Ngai and Florian, 2001); both were trained
from the annotated Penn Treebank corpus (Marcus et al,
1993). French POS tagging was done using the trained
French lexical tagger also provided with the fnTBL soft-
ware. For Arabic, we used a colleague?s POS tagger and
tokenizer (clitic separation was also performed prior to
POS tagging), which was rapidly developed in our lab-
oratory. Simple regular-expression-based phrase chun-
kers were developed by the authors for both Arabic and
French, requiring less than a person-day each using ex-
isting multilingual learning tools.
A further input to our system is a set of word alignment
links on the parallel corpus. These are used to compute
word translation probabilities and phrasal alignments.
The word alignments can in principle come from any
source: a dictionary, a specialized alignment program,
or another SMT system. We used alignments generated
by Giza++ (Och and Ney, 2000) by running it in both di-
rections (e.g., Arabic ? English and English ? Arabic)
on our parallel corpora. The union of these bidirectional
alignments was used to compute cross-language phrase
correspondences by simple majority voting, and for pur-
poses of estimating word translation probabilities, each
link in this union was treated as an independent instance
of word translation.
4 Translation Model
Now we turn to a detailed description of the proposed
translation model. The exposition will give a formal
specification and also will follow a running example
throughout, using one of the actual Arabic test set sen-
tences. This example, its gloss, system translation and
reference human translation are shown in Table 1.
The translation model (TM) we describe is trained di-
rectly from counts in the data, and is a direct model, not
a noisy channel model. It consists of three nested com-
ponents: (1) a sentence-level model of phrase correspon-
dence and reordering, (2) a model of intra-phrase trans-
lation, and (3) models of lexical transfer, or word transla-
tion. We make a key assumption in our construction that
translation at each of these three levels is independent of
the others.
4.1 Sentence Translation
As mentioned, both the foreign language and English
corpora are input with ?hard? phrase bracketings and la-
beled with ?hard? phrase types (e.g., NP, VP1, PPNP2,
etc.) as given by the output of the phrase chunker. These
are denoted in the top-level model presentation in Table
2(1). Given word alignment links, as described in Sec-
tion 2, we compute phrasal alignments on training data.
We contrain these to have cardinality
(foreign)N ? 1(English). Next, we collect counts over
aligned phrase sequences and use the relative frequen-
cies to estimate the probability distribution in Table 2(2).
Particularly for smaller training corpora, unseen foreign-
language phrase sequences are a problem, so we imple-
mented a simple backoff method which assigns proba-
bility to translations of unseen foreign-language phrase
sequences. Table 2(3) encapsulates the remainder of the
translation model, which is described below.
As an example, Table 3 shows the most probable
aligned English phrase sequence generations given an
Arabic simple sentence having the canonical VSO or-
dering. Also, note that all probabilities in the following
1VP in our parlance is perhaps more properly called a verb chunk:
it consists of a verb, its auxiliaries, and contiguous adverbs.
2PPNP consists of a NP with its prepositional head attached.
Arabic Example Sentence From Test Set
(ARABIC) twSy Al- ljnp Al- sAdsp Al- jmEyp Al- EAmp b- AEtmAd m$rwE Al- mqrr Al- tAly :
(PHR.-BRACKETED AR.) [twSy] [Al- ljnp Al- sAdsp] [Al- jmEyp Al- EAmp] [b- AEtmAd m$rwE Al- mqrr Al- tAly] [:]
(AN ENG. GLOSS) [recommends] [the committee the sixth] [the assembly the general] [to adoption draft the decision the following] [:]
(ENG. MT OUTPUT) [the sixth committee] [recommends] [the general assembly] [in the adoption of the following draft resolution] [:]
(REFERENCE TRANS.) the sixth committee recommends to the general assembly the adoption of the following draft decision :
Table 1: An Arabic translation from the test set. We revisit portions of this example throughout the text. All Arabic
strings in this paper are rendered in the reversible Buckwalter transliteration. In addition, all words or symbols referring
to Arabic and French in this paper are italicized.
figures and tables are from the actual Arabic and French
trained systems.
Arabic Phrase Aligned English Prob.
Sequence Phrase Sequence
VP1 NP2 NP3 NP2 VP1 NP3 0.23
VP1 NP2 NP3 VP1 NP2 PP3 0.10
VP1 NP2 NP3 NP3 VP1,2 0.06
Table 3: Top learned sentence-level reorderings for Ara-
bic, for canonical Arabic simple sentence structure VP
(verb) NP (subject) NP (object). Subscripts in English
phrase sequence are alignments to positions in the corre-
sponding Arabic phrase sequence.
4.2 Phrase Translation
Given an Arabic test sentence, a distribution of aligned
English phrase sequences is proposed by the sentence-
level model described in the previous section and in Ta-
ble 2. Each proposed English phrase in each of the phrase
sequence possibilities, therefore, comes to the middle
level of the translation model with access to the identity
of the French phrases aligned to it. Phrase translation is
implemented as shown in Table 4. The phrase transla-
tion model is structured with several levels of backoff: if
no observations exist from training data for a particular
level, the model backs off to the next-more-general level.
In all cases, generation of an English phrase is condi-
tioned on the foreign phrase as well as the type
(NP, VP, etc.) of the English phrase.
Table 4 (1) describes the initial phrase translation
model. It comes into play if the precise sequence of
foreign words has been observed aligning to an En-
glish phrase of the appropriate type. In the example,
we are trying to generate an NP given the Arabic word
string ?Al- ljnp Al- sAdsp? (literally: ?the committee the
sixth?). If this has been observed in data, then that rela-
tive frequency distribution serves as the translation prob-
ability distribution. Table 11 contains examples of some
of these literal phrase translations from the French data.
The next stage of backoff from the above, literal level
is a model that generates aligned English POS tag se-
quences given foreign POS tag sequences: details and
an example can be found in Table 4(2). The sequence
alignments determine the position in English phrase and
the part-of-speech into which we translate the foreign
word. Again, translation is also conditioned on the En-
glish phrase type. Table 5 and Table 6 show the most
probable aligned English sequence generations for two
of the phrases in the example sentence.
If there were no counts for (foreign-POS-sequence,
english-phrase-type) then we back off to counts
collected over (foreign-coarse-POS-sequence, english-
phrase-type), where a coarse POS is, for example, N in-
stead of NOUN-SG. This is shown in Table 4(3).
In case further backoff is needed, as shown in Table
4(4), we begin stripping POS-tags off the ?less signifi-
cant? (non-head) end of the foreign POS-sequence until
we are left with a phrase sequence that has been seen in
training, and from this a corresponding English phrase
distribution is observable. We define the ?less signifi-
cant? end of a phrase to be the end if it is head-initial,
or the beginning if it is head-final, and at this point ig-
nore issues such as nested structure in French and Arabic
NP?s.
Aligned English POS-tag Sequence Translation Probabilities
(conditioned on Arabic POS-tag sequence from NP in example)
P ( DT? JJ4 NN2 | DET1 NOUN-SG2 DET3 ADJ4 , NP) = 0.22
P ( JJ4 NN1 | DET1 NOUN-SG2 DET3 ADJ4 , NP) = 0.20
P ( DT? NN1 | DET1 NOUN-SG2 DET3 ADJ4 , NP) = 0.13
P ( DT? VBN4 NNS2 | DET1 NOUN-SG2 DET3 ADJ4 , NP) = 0.13
P ( DT1 NN2 | DET1 NOUN-SG2 DET3 ADJ4 , NP) = 0.04
P ( DT3 JJ4 NN2 | DET1 NOUN-SG2 DET3 ADJ4 , NP) = 0.03
P ( DT1 VBN4 NNS2 | DET1 NOUN-SG2 DET3 ADJ4 , NP) = 0.03
P ( DT? NN4 NN2 | DET1 NOUN-SG2 DET3 ADJ4 , NP) = 0.02
P ( JJ4 NNS2 | DET1 NOUN-SG2 DET3 ADJ4 , NP) = 0.02
P ( DT1 JJ4 NN2 | DET1 NOUN-SG2 DET3 ADJ4 , NP) = 0.02
P ( NN2 | DET1 NOUN-SG2 DET3 ADJ4 , NP) = 0.02
P ( NN4 NN2 | DET1 NOUN-SG2 DET3 ADJ4 , NP) = 0.02
Table 5: From the running Arabic example, top English
NP generations given an Arabic phrase DET NOUN-SG
DET ADJ. Note: ? denotes a null alignment (generation
from null). Generation from a null alignment is allowed
for specified parts of speech, such as determiners and
prepositions.
4.3 Lexical Transfer
4.3.1 The Basic Model
In the basic model of word generation, phrases may be
translated directly as single atomic entities (as in Table
4(1)), or via phrasal decomposition to individual words
translated independently, conditioned only on the source
word and target POS. Word translation in the latter case
Top-level Definition of Translation Model
Example Instantiation of Model Variables Model Description
P ( the sixth committee recommends the general assembly .. | P ( english words | foreign words ) =
twSy Al- ljnp Al- sAdsp Al- jmEyp Al- EAmp .. ) =
P ( [twSy]V P1 [Al- ljnp Al- sAdsp]NP1 [Al- jmEyp Al- EAmp]NP2 .. | (1) P ( foreign bracketing , foreign phrase sequence | foreign words )
twSy Al- ljnp Al- sAdsp Al- jmEyp Al- EAmp .. )
?P ( NP2 VP1 NP3 PPNP4 PUNC5 | (2) P ( english phrase sequence , phrase alignment matrix |
VP1 NP2 NP3 PPNP4 PUNC5 ) foreign phrase sequence )
?P ( [the sixth committee]NP2 [recommends]V P1 (3) P ( english words , english bracketing , english phrase sequence |
[the general assembly]NP3 .. | foreign words , foreign bracketing , foreign phrase sequence ,
[twSy]V P1 [Al- ljnp Al- sAdsp]NP1 [Al- jmEyp Al- EAmp]NP2 .. , english phrase sequence , phrase alignment matrix )
NP2 VP1 NP3 PPNP4 PUNC5 )
Table 2: Statement of the translation model at top level.
.
Phrase Translation Model with Backoff Pathways
Example Instantiations Model Statement
P ( the sixth committee | Al- ljnp Al- sAdsp , NP ) =
P ( the sixth committee | Al- ljnp Al- sAdsp , NP ) (1) P ( WE1 WE2 .. WEn | WF1 WF2 .. WFm , phr typeE )
? ? (backoff if C( WF1 WF2 .. WFm , phr typeE) = 0)
P ( DT1 JJ4 NN2 | DET1 NOUN-SG2 DET3 ADJ4 , NP ) (2) P ( TfineE1
TfineE2
.. TfineEn
, ?i | TfineF1
TfineF2
.. TfineFm
, phr typeE )
. ?P ( the | Al- , DT ) . ?P ( WE1 | WF?i(1)
, TfineE1
)
. ?P ( committee | ljnp , NN ) . ?P ( WE2 | WF?i(2)
, TfineE2
)
. ?P ( sixth | sAdsp , JJ ) . ?.. ? P ( WEn | WF?i(n)
, TfineEn
)
? ? (backoff if C( TfineF1
TfineF2
.. TfineFm
, phr typeE) = 0)
P ( DT1 JJ4 NN2 | D1 N2 D3 A4 , NP ) (3) P ( TfineE1
TfineE2
.. TfineEn
, ?i | TcoarseF1
TcoarseF2
.. TcoarseFm
, phr typeE )
. ?P ( the | Al- , DT ) . ?P ( WE1 | WF?i(1)
, TfineE1
)
. ?P ( committee | ljnp , NN ) . ?P ( WE2 | WF?i(2)
, TfineE2
)
. ?P ( sixth | sAdsp , JJ ) . ?.. ? P ( WEn | WF?i(n)
, TfineEn
)
? ? (backoff if C( TcoarseF1 TcoarseF2 .. TcoarseFm , phr typeE ) = 0)
P ( ? | D1 N2 D3 , NP ) (4) P ( TfineE1
TfineE2
.. TfineEn
, ?i | TcoarseF1
TcoarseF2
.. TcoarseFm?1
, phr typeE )
. * ? * .. * ? . * ? * .. * ?
? ? (backoff if C( TcoarseF1 TcoarseF2 .. TcoarseFm?1 , phr typeE) = 0)
P ( ? | D1 N2 , NP ) (4) P ( TfineE1
TfineE2
.. TfineEn
, ?i | TcoarseF1
TcoarseF2
.. TcoarseFm?2
, phr typeE )
. * ? * .. * ? . * ? * .. * ?
? ? (backoff if C( TcoarseF1 TcoarseF2 .. TcoarseFm?2 , phr typeE) = 0)
.... ....
Table 4: The phrase translation model, with backoff. Examples on the left side are from one of the Arabic test
sentences. (1) is the direct, lexical translation level. (2) - (4) constitute the backoff path to handle detailed phenomena
unseen in the training set. (2) is a model of fine POS-tag reordering and lexical generation; (3) is similar, but conditions
generation on coarse POS-tag sequences in the foreign language. (4) is a model for progressively stripping off POS-
tags from the ?less significant? end of a foreign sequence. The idea is to do this until we reach a subsequence that has
been seen in training data, and which we therefore have a distribution of valid generatons for. The term ?i in (2) - (4)
is a position alignment matrix. At all times, we generate not just an English POS-tag sequence, but rather an aligned
sequence. Similarly, in the lexical transfer probabilities shown in this table, there is a function ?i() which takes an
English sequence position index and returns the (unique) foreign word position to which it is aligned4.
Aligned English POS-tag Sequence Translation Probabilities
(conditioned on Arabic POS-tag sequence from VP in example)
P ( VBZ1 | VERB-IMP1 , VP ) = .28
P ( VBP1 | VERB-IMP1 , VP ) = .17
P ( VBD1 | VERB-IMP1 , VP ) = .09
...
P ( MD? VB1 | VERB-IMP1 , VP ) = .06
Table 6: From the Arabic example, top English VP gen-
erations given an Arabic phrase VERB-IMP.
is done in the context that the model has already pro-
posed a sequence of POS tags for the phrase. Thus we
know the English POS of the word we are trying to gen-
erate in addition to the foreign word that is generating it.
Consequently, we condition translation on English POS
as well as the foreign word. Table 7 describes the backoff
path for basic lexical transfer and presents a motivating
example in the French word droit. Translation probabili-
ties for one of the words in the example Arabic sentence
can be found in Table 8.
4.3.2 Generation via a Lemma Model
To counter sparse data problems in estimating word
translation probabilities, we also implemented a lemma-
Word Generation
Examples Model with Backoff Pathways
P (WE | droit , NNS) P (WE |WF , TfineE )
rights 0.4389 p(rights | droit , NNS)
benefits 0.0690
people 0.0533
laws 0.0188
? (backoff if C(WF , TfineE ) = 0)
P (WE | droit , N) P (WE |WF , TcoarseE )
right 0.4970
law 0.1318
rights 0.0424 p(rights | droit , N)
property 0.0115
? (backoff if C(WF , TcoarseE ) = 0)
P (WE | droit) P (WE |WF )
right 0.2919
entitled 0.0663
law 0.0652
the 0.0249
to 0.0240
rights 0.0210 p( rights | droit )
? (backoff if C(WF ) = 0)
p( UNKNOWN WORD |WF ) = 1
Table 7: Description of the conditioning for different lev-
els of backoff in the lexical transfer model. The exam-
ple shows translations for the French word droit (?right?)
conditioned on decreasingly specific values. The pro-
gressively lower ranking of the correct translation as we
move from fine, to coarse, to no POS, illustrates the ben-
efit of conditioning generation on the English part of
speech.
Arabic Word English POS English Wd. Prob.
ljnp NN committee 0.591
ljnp NN commission 0.233
ljnp NN subcommittee 0.035
ljnp NN acc 0.013
ljnp NN report 0.005
ljnp NN ece 0.004
ljnp NN icrc 0.004
ljnp NN aalcc 0.004
ljnp NN escap 0.004
ljnp NN escwa 0.004
ljnp NN eca 0.003
ljnp NNS members 0.088
ljnp NNS recommendations 0.033
ljnp NNS copuos 0.033
ljnp NNS questions 0.027
ljnp NNS representatives 0.024
ljnp N committee 0.577
ljnp N commission 0.227
ljnp N subcommittee 0.035
Table 8: From running example, translation probabilities for Arabic
noun ljnp, ?committee?.
based model for word translation. Under this model,
translation distributions are estimated by counting word
alignment links between foreign and English lemmas, as-
suming a lemmatization of both sides of the parallel cor-
pus as input. The form of the model is illustrated below:
P ( WE | WF ,TcoarseF ,TfineE ) =
P ( WE | lemmaE , TcoarseF , TfineE )?
P ( lemmaE | lemmaF , TcoarseF , TfineE )?
P ( lemmaF | WF , TcoarseF , TfineE )
? approximated by
P ( WE | lemmaE , TfineE )?
P ( lemmaE | lemmaF , TcoarseE )?
P ( lemmaF | WF , TcoarseF )
First, note that P ( lemmaF | WF , TcoarseF ) is very
simply a hard lemma assignment by the foreign lan-
guage lemmatizer. Second, English word generation
from English lemma and coarse POS (P ( WE | lemmaE
, TfineE )) is programmatic, and can be handled by
means of rules in conjunction with a lookup table for
irregular forms. The only distribution here that must be
estimated from data is P ( lemmaE | lemmaF , TcoarseE
). This is done as described above. Furthermore, given
an electronic translation dictionary, even this distribution
can be pre-loaded: indeed, we expect this to be an
advantage of the lemma model, and an example of
a good opportunity for integrating compiled human
knowledge about language into an SMT system. Some
examples of the lemma model combating sparse data
problems inherent in the basic word-to-word models can
be found in Table 9.
4.3.3 Coercion
Lexical coercion is a phenomenon that sometimes occurs
when we condition translation of a foreign word on the
word and the English part-of-speech. We find that the
system we have described frequently learns this behav-
ior: specifically, the model learns in some cases how
to generate, for instance, a nominal form with similar
meaning from a French adjective, or an adjectival real-
ization of a French verb?s meaning; some examples of
this phenomenon are shown in Table 10. We find this
coercion effect to be of interest because it identifies in-
teresting associations of meaning. For example, in Table
10 ?willing? and ?ready? are both sensible ways to re-
alize the meaning of the action ?to accept? in a passive,
descriptive mode. droit behaves similarly. Though the
English verb ?to right? or ?to be righted? does not have
the philosophical/judicial entitlement sense of the noun
?right?, we see that the model has learned to realize the
meaning in an active, verbal form: e.g., VBG ?receiving?
and VB ?qualify?.
5 Decoding
Decoding was implemented by constructing finite-state
machines (FSMs) per evaluation sentence to encode
relevant portions (for the individual sentence in ques-
tion) of the component translation distributions described
above. Operations on these FSMs are performed using
the AT&T FSM Toolkit (Mohri et al, 1997). The FSM
constructed for a test sentence is subsequently composed
with a FSM trigram language model created via the SRI
Language Modeling Toolkit (Stolcke, 2002). Thus we
use the trigram language model to implement rescoring
of the (direct) translation probabilities for the English
word sequences in the translation model lattice.
We found that using the finite-state framework and the
general-purpose AT&T toolkit greatly facilitates decoder
development by freeing the implementation from details
of machine composition and best-path searching, etc.
The structure of the translation model finite-state ma-
chines is as illustrated in Figure 1. The sentence-level
(aligned phrase sequence generation) and phrase-level
(aligned intra-phrase sequence generation) translation
probabilities are encoded on epsilon arcs in the ma-
chines. Word translation probabilities are placed onto
arcs emitting the word as an output symbol (in the fig-
ure, note the arcs emitting ?committee?, ?the?, etc.). The
FSM in Figure 1 corresponds to the Arabic example sen-
tence used throughout this paper. In the portion of the
machine shown, the (best) path which generated the ex-
ample sentence is drawn in bold. Finally, Figure 2 is
a rendering of the actual FSM (aggressively pruned for
display purposes) that generated the example Arabic sen-
tence; although labels and details are not visible, it may
provide a visual aid for better understanding the structure
of the FSM lattices generated here.
As a practical matter in decoding, during translation
model FSM construction we modified arc costs for out-
put words in the following way: a fixed bonus was as-
signed for generating a ?content? word translating to a
?content? word. Determining what qualifies as a con-
tent word was done on the basis of a list of content POS
tags for each language. For example, all types of nouns,
verbs and adjectives were listed as content tags; deter-
miners, prepositions, and most other closed-class parts of
speech were not. This implements a reasonable penalty
on undesirable output sentence lengths. Without such a
penalty, translation outputs tend to be very short: long
sentence hypotheses are penalized de facto merely by
containing many word translation probabilities. An ad-
ditional trick in decoding is to use only the N-best trans-
lation options for sentence-level, phrase-level, and word-
level translation. We found empirically (and very consis-
tently) in dev-test experiments that restricting the syntac-
tic transductions to a 30-best list and word translations to
a 15-best list had no negative impact on Bleu score. The
benefit, of course, is that the translation lattices are dra-
matically reduced in size, speeding up composition and
search operations.
.
.
.
.
.
.
etc ...
... next phrase,
.
.
.
.
.
.
S
P(the | NULL)
P(an | NULL) P(committee | ljnp)
P(commission | ljnp)
NULLP( DT       NN      |   NOUN?SG    )1
1 1
P(NP    VP    NP  .. |VP   NP   NP  .. )
32
"commission"
"committee"
"the"
"an"
1
2 3
Figure 1: An illustration of the translation model structure for an
Arabic test sentence.
319 321W__:the/-0
0
1
S__:<s>/5.411
2
S__:<s>/5.629
3
S__:<s>/5.835
4
S__:<s>/5.868
5
S__:<s>/5.940
6
S__:<s>/6.226
7
S__:<s>/6.381
8
S__:<s>/6.385
9
S__:<s>/6.396
10
S__:<s>/6.529
11S__:<s>/6.546
12
S__:<s>/6.620
13
S__:<s>/6.657
14S__:<s>/6.698
15
S__:<s>/6.711
16
S__:<s>/6.717
17S__:<s>/6.748
18
S__:<s>/6.825
19S__:<s>/6.875
20S__:<s>/6.942
21
S__:<s>/6.946
116
137
W__:committee/-0
224
247P__ADJP:<epsilon>/-0
412 439T__-:<epsilon>/0.129
702
714
W__:of/1.969
W__:in/2.215
22
P__NP:<epsilon>/-0
117
138
G__:<epsilon>/-0
225 248G__:<epsilon>/-0
272 273W__:general/-0
413
440T__recommends:<epsilon>/0.227
474
501W__:the/1.188
W__:a/1.736
W__:this/2.266
703
715
W__:on/2.917
W__:of/1.969
W__:against/4.731
W__:by/2.975
W__:under/4.248
W__:at/3.201
W__:in/2.215
W__:that/2.614
W__:after/4.880
W__:before/4.435
W__:with/3.075
W__:for/2.660
W__:because/4.073
W__:from/3.270
W__:as/3.201
W__:through/4.435
W__:into/3.971
W__:about/3.426
W__:between/4.602
W__:like/4.665
W__:during/4.572
W__:whether/4.697
W__:if/3.704
W__:than/4.147
W__:over/4.385
W__:per/3.988
W__:out/4.410
W__:within/4.766
521
555W__:of/1.969
558
590
W__:the/0.132
631
630
W__:rapporteur/0.699
W__:decision/1.000
23
P__NP:<epsilon>/-0
118
139G__:<epsilon>/-0
226
227
W__:committee/-0
322
W__:assembly/-0
414 441T__recommends:<epsilon>/0.227
522 556W__:of/1.969
W__:in/2.215
525
559
P__O:<epsilon>/-0
24
P__NP:<epsilon>/-0
119 140P__O:<epsilon>/-0
249
G__:<epsilon>/-0
415
442
T__2:<epsilon>/0.631
443
T__2#xslax/NULL:<epsilon>/1.194
632 647P__NP:<epsilon>/-0
367
G__:<epsilon>/-0
523
557
W__:on/2.917
W__:of/1.969
W__:by/2.975
W__:under/4.248
W__:at/3.201
W__:in/2.215
W__:that/2.614
W__:before/4.435
W__:with/3.075
W__:for/2.660
W__:because/4.073
W__:from/3.270
W__:as/3.201
W__:through/4.435
W__:into/3.971
W__:about/3.426
W__:if/3.704
W__:than/4.147
W__:over/4.385
W__:per/3.988
W__:out/4.410
592
T__::<epsilon>/0.028
704
716
W__:of/1.969
W__:in/2.215
W__:that/2.614
W__:for/2.660
25
P__O:<epsilon>/-0
228 229
W__:sixth/-0
416
444
T__IN#NGEN+DT#NN#NGEN+IN#NGEN+DT#JJ#NN#NN:<epsilon>/3.545
445T__IN#NGEN+DT#NN#NGEN+IN#JJ#NN#NNS:<epsilon>/4.133
446
T__IN#NGEN+DT#NN#NGEN+IN#DT#JJ#NN#NN:<epsilon>/4.895
475 502G__:<epsilon>/-0
633
648
P__VP:<epsilon>/-0
120
141
P__NP:<epsilon>/-0
560
561
W__:the/-0
705
717
P__O:<epsilon>/-0
26
P__NP:<epsilon>/-0
417 447T__-:<epsilon>/0.129
476 503P__NP:<epsilon>/-0
634
635
W__:draft/0.404
W__:project/1.841
121
142
G__:<epsilon>/-0
230
251
P__VP:<epsilon>/-0
323
325
W__:by/-0
27P__NP:<epsilon>/-0
524
W__:of/1.969
W__:in/2.215
593W__:sixth/-0
122 143G__:<epsilon>/-0
324 W__:to/-0
477
504W__:adoption/0.711
W__:provision/1.929
532
G__:<epsilon>/-60
706
718W__:police/-0
28
P__ADJP:<epsilon>/-0
231
252
G__:<epsilon>/-0
418
448
W__:-/-0
526
T__the/NULL#sixth#committee:<epsilon>/0.454
T__the#sixth#committee:<epsilon>/1.871
T__sixth#committee:<epsilon>/1.871
562
534
W__:by/-0
123 144
P__O:<epsilon>/-0
29P__NP:<epsilon>/-0
232
253
W__:adoption/0.711
W__:provision/1.929
527 T__by/NULL#the/NULL#general#assembly:<epsilon>/1.085
563T__to/NULL#the/NULL#general#assembly:<epsilon>/1.254 W__:to/-0
636 649P__ADJP:<epsilon>/-0
327W__:the/-0
419 W__:2/-0
30
P__NP:<epsilon>/-0
124 145P__O:<epsilon>/-0
528 565T__-:<epsilon>/0.129
573
W__:-/-0
637
650
W__:on/2.046
W__:of/2.238
W__:by/1.908
W__:under/3.942
W__:at/3.536
W__:in/1.425
W__:that/4.161
W__:with/1.853
W__:for/3.134
W__:as/2.616
326
328
W__:the/-0
420 449W__:2/-0
707
696
W__:in/-0
31
P__NP:<epsilon>/-0
32
P__VP:<epsilon>/-0
33
P__SBAR:<epsilon>/-0
529 566
W__:on/2.917
W__:of/1.969
W__:by/2.975
W__:at/3.201
W__:in/2.215
W__:that/2.614
W__:with/3.075
W__:for/2.660
W__:from/3.270
W__:as/3.201
W__:about/3.426
594
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:this/2.266
638 651
W__:on/2.046
W__:of/2.238
W__:by/1.908
W__:under/3.942
W__:at/3.536
W__:in/1.425
W__:that/4.161
W__:with/1.853
W__:for/3.134
W__:as/2.616
125
146
G__:<epsilon>/-0
421
450
W__:on/2.046
W__:of/2.238
W__:by/1.908
W__:at/3.536
W__:in/1.425
W__:with/1.853
W__:for/3.134
W__:as/2.616
34P__NP:<epsilon>/-0
329
T__the/NULL#general#assembly:<epsilon>/0.938
369
T__had#decided:<epsilon>/2.442
370
T__will#consider:<epsilon>/2.442
639
652
W__:in/1.425
708 719
W__:world/-0
104 126G__:<epsilon>/-0
422
451
W__:on/2.046
W__:of/2.238
W__:by/1.908
W__:at/3.536
W__:in/1.425
W__:with/1.853
W__:for/3.134
W__:as/2.616
530 567
W__:on/2.917
W__:of/1.969
W__:by/2.975
W__:at/3.201
W__:in/2.215
W__:that/2.614
W__:with/3.075
W__:for/2.660
W__:from/3.270
W__:as/3.201
35P__NP:<epsilon>/-0
330
W__:decided/-0
478
505
W__:adoption/0.711
W__:provision/1.929
640
653
W__:on/2.046
W__:by/1.908
W__:in/1.425
W__:with/1.853
709 720P__VP:<epsilon>/-0
423
452
W__:on/2.046
W__:of/2.238
W__:by/1.908
W__:in/1.425
W__:with/1.853
W__:for/3.134
W__:as/2.616
36
P__NP:<epsilon>/-0
233
254W__:adoption/0.711
W__:provision/1.929
331
W__:consider/-0
531 568
W__:on/2.917
W__:of/1.969
W__:by/2.975
W__:in/2.215
W__:that/2.614
W__:with/3.075
W__:for/2.660
641
654W__:on/2.046
W__:of/2.238
W__:by/1.908
W__:under/3.942
W__:at/3.536
W__:in/1.425
W__:that/4.161
W__:with/1.853
W__:for/3.134
W__:as/2.616
710
721
P__O:<epsilon>/-0
147P__VP:<epsilon>/-0
148
P__PPNP:<epsilon>/-0
424
453
W__:on/2.046
W__:of/2.238
W__:by/1.908
W__:under/3.942
W__:at/3.536
W__:in/1.425
W__:with/1.853
W__:for/3.134
W__:as/2.616
37
P__O:<epsilon>/-0
332
307W__:to/-0
533 570
T__accountable:<epsilon>/0.693
571T__present/NULL:<epsilon>/0.693
642
655
W__:on/2.046
W__:of/2.238
W__:by/1.908
W__:in/1.425
W__:with/1.853
127
P__PPNP:<epsilon>/-0
149P__NP:<epsilon>/-0
425
454W__:on/2.046
W__:of/2.238
W__:by/1.908
W__:under/3.942
W__:at/3.536
W__:in/1.425
W__:with/1.853
W__:for/3.134
W__:as/2.616
38
P__O:<epsilon>/-0
333 371
W__:with/-0
537
W__:the/-0
711
722
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:those/3.154
W__:all/3.201
W__:an/2.837
W__:no/3.547
W__:any/3.434
W__:some/3.097
W__:this/2.266
W__:these/3.201
W__:every/4.241
W__:another/4.082
128 P__VP:<epsilon>/-0
426
455
W__:in/1.425
39
P__O:<epsilon>/-0
535
538
W__:the/-0
595W__:following/0.227
129 150
P__O:<epsilon>/-0
334
372
W__:next/1.854
W__:following/0.227
427
456
W__:on/2.046
W__:of/2.238
W__:by/1.908
W__:at/3.536
W__:in/1.425
W__:with/1.853
W__:for/3.134
W__:as/2.616
643
656
T__to/NULL#2/NULL:<epsilon>/1.299
T__to/NULL#2:<epsilon>/2.397
657T__with/NULL#article/NULL#2:<epsilon>/2.397
658
T__under/NULL#review/NULL:<epsilon>/3.091
659
T__by/NULL#2#police/NULL#officers/NULL:<epsilon>/3.091
660T__in/NULL#only/NULL#2:<epsilon>/3.091
661
T__in/NULL#order/NULL:<epsilon>/3.091
662T__for/NULL#just/NULL#2:<epsilon>/3.091
663
T__on/NULL#2:<epsilon>/3.091
664T__at/NULL#2:<epsilon>/3.091
665
T__with/NULL#regard/NULL:<epsilon>/3.091
666
T__for/NULL#2:<epsilon>/3.091
667
T__to/NULL#2.7/NULL#in/NULL:<epsilon>/3.091
668
T__in/NULL#document:<epsilon>/3.091
669
T__into/NULL#the/NULL#world/NULL#economy/NULL:<epsilon>/3.091
40
P__NP:<epsilon>/-0
479
506
W__:adoption/0.711
W__:provision/1.929
428
457
W__:in/1.425
536 572
W__:the/-0
41
P__PPNP:<epsilon>/-0
130 151P__NP:<epsilon>/-0
234
255
W__:adoption/0.711
W__:provision/1.929
429
458
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:those/3.154
W__:all/3.201
W__:an/2.837
W__:no/3.547
W__:any/3.434
W__:some/3.097
W__:this/2.266
W__:these/3.201
42P__VP:<epsilon>/-0
131
152G__:<epsilon>/-0
539
574
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:those/3.154
W__:all/3.201
W__:an/2.837
W__:any/3.434
W__:some/3.097
W__:this/2.266
W__:these/3.201
644 670T__-:<epsilon>/0.129
43
P__NP:<epsilon>/-0
132
153P__PPNP:<epsilon>/-0
645
646
W__:the/-0
44
P__NP:<epsilon>/-0
133
154
P__VP:<epsilon>/-0
480
508
W__:adoption/0.711
W__:provision/1.929
W__:appropriation/3.108
W__:accreditation/3.18
235
257
P__NP:<epsilon>/-0
335
373
W__:rapporteur/0.699
W__:decision/1.000
W__:the/0.132
45
T__the/NULL#sixth#committee:<epsilon>/0.454
T__the#sixth#committee:<epsilon>/1.871
46
T__sixth#committee:<epsilon>/1.871
134
155
P__NP:<epsilon>/-0
671W__:sixth/-0
236
258
P__NP:<epsilon>/-0
47
T__the/NULL#sixth#committee:<epsilon>/0.454
T__the#sixth#committee:<epsilon>/1.871
48T__sixth#committee:<epsilon>/1.871
135
156
G__:<epsilon>/-0
430
459
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:those/3.154
W__:all/3.201
W__:an/2.837
W__:no/3.547
W__:any/3.434
W__:some/3.097
W__:this/2.266
W__:these/3.201
336
374W__:2/-0
49T__2:<epsilon>/0.631
50
T__2#xslax/NULL:<epsilon>/1.194
136
157
G__:<epsilon>/-0
237
259
G__:<epsilon>/-0
337
375
W__:2/-0
51
T__-:<epsilon>/0.129
T__-/NULL:<epsilon>/2.988
158
G__:<epsilon>/-0
238
260
G__:<epsilon>/-0
569P__O:<epsilon>/-0
287
338
W__:sixth/-0
672
T__2:<epsilon>/0.631
673
T__2#xslax/NULL:<epsilon>/1.194
52T__the/NULL#sixth#committee:<epsilon>/0.454
T__the#sixth#committee:<epsilon>/1.871
53
T__sixth#committee:<epsilon>/1.871
159
P__VP:<epsilon>/-0
239
261
G__:<epsilon>/-0
596T__::<epsilon>/0.028
376W__:committee/-0
674
T__recommends:<epsilon>/0.227
54T__the/NULL#sixth#committee:<epsilon>/0.454
T__the#sixth#committee:<epsilon>/1.871
55T__sixth#committee:<epsilon>/1.871
160
P__VP:<epsilon>/-0
240
262
P__PPNP:<epsilon>/-0
481
509
W__:adoption/0.711
W__:provision/1.929
W__:appropriation/3.108
W__:accreditation/3.18
W__:accountable/-0
431
460
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:those/3.154
W__:all/3.201
W__:an/2.837
W__:some/3.097
W__:this/2.266
W__:these/3.201
540
575
W__:next/1.854
W__:following/0.227
56T__asean/NULL:<epsilon>/1.098
57
T__imf-2:<epsilon>/1.098
58
T__concerned/NULL#with/NULL:<epsilon>/1.098
161T__-:<epsilon>/0.129
241
263P__NP:<epsilon>/-0
339
377
W__:committee/-0
W__:present/-0
59
T__2:<epsilon>/0.631
60T__2#xslax/NULL:<epsilon>/1.194
162
T__the/NULL#sixth#committee:<epsilon>/0.454
T__the#sixth#committee:<epsilon>/1.871
163
T__sixth#committee:<epsilon>/1.871
242
264
T__by/NULL#the/NULL#general#assembly:<epsilon>/1.085
T__by/NULL#the#general#assembly:<epsilon>/2.725
265T__to/NULL#the/NULL#general#assembly:<epsilon>/1.254
T__to/NULL#the#general#assembly:<epsilon>/2.748
266
T__with/NULL#general#assembly#decision/NULL:<epsilon>/3.313
340 342W__:the/-0
W__:general/-0
675
T__asean/NULL:<epsilon>/1.098
676T__imf-2:<epsilon>/1.098
677T__concerned/NULL#with/NULL:<epsilon>/1.098
61
T__the/NULL#sixth#committee:<epsilon>/0.454
T__the#sixth#committee:<epsilon>/1.871
62
T__sixth#committee:<epsilon>/1.871
164
P__VP:<epsilon>/-0
243
267
P__VP:<epsilon>/-0
341
343
W__:the/-0
W__:assembly/-0
678
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:those/3.154
W__:all/3.201
W__:an/2.837
W__:no/3.547
W__:any/3.434
W__:some/3.097
W__:this/2.266
W__:these/3.201
W__:every/4.241
W__:another/4.082
63T__728#xslax/NULL:<epsilon>/1.421
64
T__the/NULL#committee:<epsilon>/1.981 65
T__the/NULL#advisory#committee:<epsilon>/2.268
66T__729#xslax/NULL:<epsilon>/2.674
165P__VP:<epsilon>/-0
244
268
P__NP:<epsilon>/-0
564
W__:wish/-0
344 346
W__:the/-0
67
T__recommends:<epsilon>/0.227
166
T__-:<epsilon>/0.129
245
269
P__NP:<epsilon>/-0
432
461
W__:the/1.188
345
347
W__:the/-0
597
G__:<epsilon>/-0
712
723
W__:next/1.854
W__:following/0.227
68T__that:<epsilon>/-0
167T__-:<epsilon>/0.129
246
270
T__the/NULL#general#assembly:<epsilon>/0.530
T__the#general#assembly:<epsilon>/2.122
271
T__the/NULL#assembly:<epsilon>/1.941
433
462
W__:on/2.046
W__:of/2.238
W__:by/1.908
W__:under/3.942
W__:at/3.536
W__:in/1.425
W__:that/4.161
W__:with/1.853
W__:for/3.134
W__:as/2.616
482
510
W__:adoption/0.711
348
368
W__:accountable/-0
598
W__:next/1.854
W__:following/0.227
69T__2:<epsilon>/0.631
70T__2#xslax/NULL:<epsilon>/1.194
168P__VP:<epsilon>/-0
274T__accountable:<epsilon>/0.693
275T__present/NULL:<epsilon>/0.693
434
463
W__:on/2.046
W__:of/2.238
W__:by/1.908
W__:under/3.942
W__:at/3.536
W__:in/1.425
W__:that/4.161
W__:with/1.853
W__:for/3.134
W__:as/2.616
483
511
W__:adoption/0.711
W__:provision/1.929
349 W__:present/-0
72T__the/NULL#sixth#committee:<epsilon>/0.454
T__the#sixth#committee:<epsilon>/1.871
73T__sixth#committee:<epsilon>/1.871
169T__the/NULL#sixth#committee:<epsilon>/0.154
170
T__will#establish:<epsilon>/1.945
276P__PPNP:<epsilon>/-0
435
464
W__:in/1.425
350
380
W__:2/-0
541
W__:the/0.132
679
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:those/3.154
W__:all/3.201
W__:an/2.837
W__:no/3.547
W__:any/3.434
W__:some/3.097
W__:this/2.266
W__:these/3.201
W__:every/4.241
W__:another/4.082
74T__the/NULL#general#assembly:<epsilon>/0.530
T__the#general#assembly:<epsilon>/2.122
75
T__the/NULL#assembly:<epsilon>/1.941
171T__by/NULL#the/NULL#sixth#committee:<epsilon>/0.490
T__by/NULL#the/NULL#sixth/NULL#committee:<epsilon>/1.589
436
465W__:in/1.425
277
P__NP:<epsilon>/-0
351
W__:article/-0
78T__-:<epsilon>/0.129
172
T__the/NULL#sixth#committee:<epsilon>/0.454
T__the#sixth#committee:<epsilon>/1.871
173
T__sixth#committee:<epsilon>/1.871
437
466
W__:on/2.046
W__:of/2.238
W__:by/1.908
W__:under/3.942
W__:at/3.536
W__:in/1.425
W__:with/1.853
W__:for/3.134
W__:as/2.616
250
W__:committee/-0
352
W__:review/-0
71
T__-:<epsilon>/0.129
438
467
W__:in/1.425
W__:with/1.853
278
G__:<epsilon>/-0
353
381
W__:2/-0
79
T__-:<epsilon>/0.129
174
T__-:<epsilon>/0.129
468
W__:-/-0
599
W__:rapporteur/0.699
W__:decision/1.000
T__the/NULL#general#assembly:<epsilon>/0.938
279
T__had#decided:<epsilon>/2.442
280T__will#consider:<epsilon>/2.442
354 W__:only/-0
80
T__the/NULL#sixth#committee:<epsilon>/0.454
T__the#sixth#committee:<epsilon>/1.871 81
T__sixth#committee:<epsilon>/1.871
175
T__2:<epsilon>/0.631
176T__2#xslax/NULL:<epsilon>/1.194
W__:recommends/-0
484
512
W__:adoption/0.711
281
P__PPNP:<epsilon>/-0
355 W__:order/-0
82
T__to/NULL#2/NULL:<epsilon>/1.299
177
P__VP:<epsilon>/-0
469W__:recommends/-0
485
514
W__:on/2.917
W__:of/1.969
W__:by/2.975
W__:under/4.248
W__:at/3.201
W__:in/2.215
W__:that/2.614
W__:before/4.435
W__:with/3.075
W__:for/2.660
W__:because/4.073
W__:from/3.270
W__:as/3.201
W__:through/4.435
W__:into/3.971
W__:about/3.426
W__:if/3.704
W__:than/4.147
W__:over/4.385
W__:per/3.988
W__:out/4.410
576
600W__::/-0
680
W__:the/1.188
282
W__:on/2.917
W__:of/1.969
W__:by/2.975
W__:at/3.201
W__:in/2.215
W__:that/2.614
W__:with/3.075
W__:for/2.660
W__:because/4.073
W__:from/3.270
W__:as/3.201
W__:into/3.971
W__:about/3.426
W__:if/3.704
W__:than/4.147
W__:per/3.988
356 W__:just/-0
507
542
P__O:<epsilon>/-0
83T__add:<epsilon>/1.704
84
T__take/NULL:<epsilon>/2.397
178
T__IN#NGEN+DT#NN#NGEN+IN#NGEN+DT#JJ#NN#NN:<epsilon>/3.545
179
T__IN#NGEN+DT#NN#NGEN+IN#JJ#NN#NNS:<epsilon>/4.133
180
T__IN#NGEN+DT#NN#NGEN+IN#DT#JJ#NN#NN:<epsilon>/4.895
470
W__:2/-0
577
601
W__:next/1.854
W__:following/0.227
681
W__:the/1.188
W__:a/1.736
T__::<epsilon>/0.028
85
T__the/NULL#sixth#committee:<epsilon>/0.454
T__the#sixth#committee:<epsilon>/1.871
86
T__sixth#committee:<epsilon>/1.871
181T__add:<epsilon>/1.704
182T__take/NULL:<epsilon>/2.397
283
W__:on/2.917
W__:of/1.969
W__:by/2.975
W__:at/3.201
W__:in/2.215
W__:that/2.614
W__:with/3.075
W__:for/2.660
W__:from/3.270
W__:as/3.201
W__:into/3.971
W__:about/3.426
W__:if/3.704
W__:per/3.988
471W__:2/-0
357 W__:regard/-0
486
515
W__:on/2.917
W__:of/1.969
W__:by/2.975
W__:under/4.248
W__:at/3.201
W__:in/2.215
W__:that/2.614
W__:with/3.075
W__:for/2.660
W__:because/4.073
W__:from/3.270
W__:as/3.201
W__:into/3.971
W__:about/3.426
W__:if/3.704
W__:than/4.147
W__:per/3.988
543
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:those/3.154
W__:all/3.201
W__:an/2.837
W__:no/3.547
W__:any/3.434
W__:some/3.097
W__:this/2.266
W__:these/3.201
87
T__the/NULL#sixth#committee:<epsilon>/0.454
T__the#sixth#committee:<epsilon>/1.871
88T__sixth#committee:<epsilon>/1.871
183T__2:<epsilon>/0.631
184T__2#xslax/NULL:<epsilon>/1.194
472
W__:on/2.046
W__:of/2.238
W__:by/1.908
W__:in/1.425
W__:with/1.853
W__:as/2.616
284
W__:on/2.917
W__:of/1.969
W__:by/2.975
W__:at/3.201
W__:in/2.215
W__:that/2.614
W__:with/3.075
W__:for/2.660
W__:from/3.270
W__:as/3.201
W__:about/3.426
W__:if/3.704
682
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:those/3.154
W__:all/3.201
W__:an/2.837
W__:no/3.547
W__:any/3.434
W__:some/3.097
W__:this/2.266
W__:these/3.201
W__:another/4.082
W__:the/-0
185
P__VP:<epsilon>/-0
358 382W__:2.7/-0
473
W__:on/2.046
W__:of/2.238
W__:by/1.908
W__:in/1.425
W__:with/1.853
W__:as/2.616
487
516
W__:for/2.660
W__:because/4.073
W__:from/3.270
W__:as/3.201
W__:into/3.971
W__:about/3.426
W__:if/3.704
W__:than/4.147
W__:per/3.988
W__:on/2.917
W__:of/1.969
W__:by/2.975
W__:at/3.201
W__:in/2.215
W__:that/2.614
W__:with/3.075
186
P__VP:<epsilon>/-0
286
T__the/NULL#sixth#committee:<epsilon>/0.454
359 W__:document/-0
W__:on/2.046
W__:of/2.238
W__:by/1.908
W__:in/1.425
W__:with/1.853
89
W__:sixth/-0
488
517
W__:of/1.969
187
P__VP:<epsilon>/-0
288T__the/NULL#sixth#committee:<epsilon>/0.454
T__the#sixth#committee:<epsilon>/1.871
289
T__sixth#committee:<epsilon>/1.871
360
383
W__:the/-0
W__:-/-0
W__:the/-0
490
519
W__:adoption/0.711
W__:provision/1.929
W__:appropriation/3.108
W__:accreditation/3.18
188T__recommends:<epsilon>/0.227
189T__recommended:<epsilon>/2.759
290
P__NP:<epsilon>/-0
361
384
G__:<epsilon>/-0
G__:<epsilon>/-0
578
602
W__:rapporteur/0.699
W__:decision/1.000
713 724W__:the/1.188
90
W__:sixth/-0
190T__recommends:<epsilon>/0.227
191T__recommended:<epsilon>/2.759
192T__recommend:<epsilon>/3.200
291
P__NP:<epsilon>/-0
362
W__:xslax/-0
W__:xslax/-0
580 604W__:sponsors/1.131
91
W__:2/-0
193W__:-/-0
292P__ADJP:<epsilon>/-0
683
W__:the/1.188
W__:a/1.736
725
W__:following/0.227
363
385W__:assembly/-0
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:those/3.154
W__:all/3.201
W__:an/2.837
W__:any/3.434
W__:some/3.097
W__:this/2.266
W__:these/3.201
92W__:2/-0
W__:the/-0
293
T__to/NULL#2/NULL:<epsilon>/1.299
T__to/NULL#2:<epsilon>/2.397
294
T__with/NULL#article/NULL#2:<epsilon>/2.397
295T__under/NULL#review/NULL:<epsilon>/3.091
296T__by/NULL#2#police/NULL#officers/NULL:<epsilon>/3.091
297T__in/NULL#only/NULL#2:<epsilon>/3.091
298
T__in/NULL#order/NULL:<epsilon>/3.091
299T__for/NULL#just/NULL#2:<epsilon>/3.091
300T__on/NULL#2:<epsilon>/3.091
301
T__at/NULL#2:<epsilon>/3.091
302
T__with/NULL#regard/NULL:<epsilon>/3.091
303
T__for/NULL#2:<epsilon>/3.091
304
T__to/NULL#2.7/NULL#in/NULL:<epsilon>/3.091
305T__in/NULL#document:<epsilon>/3.091
306T__into/NULL#the/NULL#world/NULL#economy/NULL:<epsilon>/3.091
579 603W__:following/0.227
364
386
G__:<epsilon>/-0
W__:the/0.132
W__:a/3.066
93
W__:-/-0
T__2:<epsilon>/0.631
308T__2#xslax/NULL:<epsilon>/1.194
309
T__it/NULL:<epsilon>/3.818
581 605W__:2/-0
194W__:sixth/-0
W__:by/-0
582 606W__:2/-0
W__:to/-0
W__:the/-0
195
T__recommends:<epsilon>/0.227
316
317
W__:general/-0
491
520
W__:adoption/0.711
W__:provision/1.929
W__:appropriation/3.108
W__:accreditation/3.18
W__:to/-0
583
607W__:rapporteur/0.699
W__:decision/1.000
685
W__:with/-0
196T__recommends:<epsilon>/0.227
197
T__recommended:<epsilon>/2.759
365
W__:assembly/-0
94
W__:sixth/-0
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:those/3.154
W__:all/3.201
W__:an/2.837
W__:some/3.097
W__:this/2.266
W__:these/3.201
198
W__:-/-0
544
W__:next/1.854
W__:following/0.227
584
608
W__:implications/2.139
W__:sponsors/1.131
W__:projects/2.972
686
W__:under/-0
W__:the/-0
310W__:with/-0
387
G__:<epsilon>/-0
199W__:-/-0
687W__:by/-0
311T__recommends:<epsilon>/0.227
312
T__recommended:<epsilon>/2.759
313T__recommend:<epsilon>/3.200
320
W__:general/-0
585
586
W__:draft/0.404
95W__:sixth/-0
200T__recommends:<epsilon>/0.227
688W__:in/-0
314
T__the/NULL#general#assembly:<epsilon>/0.530
T__the#general#assembly:<epsilon>/2.122
315
T__the/NULL#assembly:<epsilon>/1.941
366W__:assembly/-0
587 610G__:<epsilon>/-0
96
W__:asean/-0
201W__:the/-0
689
W__:in/-0
318
T__the/NULL#general#assembly:<epsilon>/0.530
T__the#general#assembly:<epsilon>/2.122
T__the/NULL#assembly:<epsilon>/1.941
588
611
W__:next/1.854
W__:following/0.227
W__:imf-2/-0
202W__:will/-0
388G__:<epsilon>/-0
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:those/3.154
W__:an/2.837
W__:some/3.097
W__:this/2.266
492
W__:adoption/0.711
690W__:for/-0
W__:the/-0
97
W__:concerned/-0
203W__:by/-0
389
P__PPNP:<epsilon>/-0
493
W__:adoption/0.711
W__:on/-0
W__:the/-0
98W__:2/-0
494 W__:adoption/0.711
W__:provision/1.929
W__:appropriation/3.108
W__:accreditation/3.18
W__:at/-0
W__:the/-0
99W__:2/-0
W__:accountable/-0
W__:general/-0
691W__:with/-0
W__:the/-0 204
W__:sixth/-0
W__:present/-0
W__:assembly/-0
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:those/3.154
W__:all/3.201
W__:an/2.837
W__:no/3.547
W__:any/3.434
W__:some/3.097
W__:this/2.266
W__:these/3.201
W__:for/-0
205
W__:-/-0
T__by/NULL#the/NULL#general#assembly:<epsilon>/1.085
T__by/NULL#the#general#assembly:<epsilon>/2.725
T__to/NULL#the/NULL#general#assembly:<epsilon>/1.254
T__to/NULL#the#general#assembly:<epsilon>/2.748
692W__:to/-0
100
W__:sixth/-0
390
G__:<epsilon>/-0
589
612
W__:rapporteur/0.699
W__:decision/1.000
W__:2/-0
T__the/NULL#general#assembly:<epsilon>/0.530
T__the#general#assembly:<epsilon>/2.122
T__the/NULL#assembly:<epsilon>/1.941
693W__:in/-0
101W__:728/-0
W__:the/0.132
206W__:2/-0
P__VP:<epsilon>/-0
391
W__:had/-0
545 W__:following/0.227
613
W__:following/0.227
694W__:into/-0
102
W__:the/-0
207
T__recommends:<epsilon>/0.227
392W__:will/-0
495
W__:adoption/0.711
546
W__:the/0.132
591
614
W__:sponsors/1.131
684W__:-/-0
103W__:the/-0
W__:had/-0
208
W__:on/2.046
W__:of/2.238
W__:by/1.908
W__:at/3.536
W__:in/1.425
W__:with/1.853
W__:for/3.134
W__:as/2.616
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:those/3.154
W__:all/3.201
W__:an/2.837
W__:no/3.547
W__:any/3.434
W__:some/3.097
W__:this/2.266
W__:these/3.201
496 P__NP:<epsilon>/-0
726
T__::<epsilon>/0.028
W__:729/-0
W__:will/-0
695W__:committee/-0
209
W__:on/2.046
W__:of/2.238
W__:by/1.908
W__:in/1.425
W__:with/1.853
W__:for/3.134
W__:as/2.616
497 P__PPNP:<epsilon>/-0
615W__::/-0
W__:officers/-0
105W__:recommends/-0
T__to/NULL#2/NULL:<epsilon>/1.299
T__to/NULL#2:<epsilon>/2.397
T__with/NULL#article/NULL#2:<epsilon>/2.397
W__:article/-0
W__:2/-0
210
W__:on/2.046
W__:of/2.238
W__:by/1.908
W__:in/1.425
W__:with/1.853
W__:for/3.134
W__:as/2.616
498 P__O:<epsilon>/-0
106
W__:that/-0
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:those/3.154
W__:all/3.201
W__:an/2.837
W__:some/3.097
W__:this/2.266
W__:these/3.201 393W__:rapporteur/0.699
W__:decision/1.000
616
W__:committee/-0
697
W__:2/-0
W__:economy/-0
211W__:add/-0
107W__:2/-0
499 W__:adoption/0.711
W__:provision/1.929
W__:recommends/-0
728T__recommends:<epsilon>/0.227
W__:take/-0
394W__:implications/2.139
W__:sponsors/1.131
W__:projects/2.972
W__:of/-0
108W__:2/-0
W__:asean/-0
727
T__-:<epsilon>/0.129
T__-/NULL:<epsilon>/2.988
212
W__:2/-0
W__:the/1.188
W__:the/-0
395G__:<epsilon>/-0
W__:imf-2/-0
729
W__:next/1.854
W__:following/0.227
213W__:2/-0
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:those/3.154
W__:all/3.201
W__:an/2.837
W__:no/3.547
W__:any/3.434
W__:some/3.097
W__:this/2.266
W__:these/3.201
500
W__:adoption/0.711
W__:provision/1.929
617
W__:following/0.227
W__:xslax/-0
698W__:concerned/-0
110W__:sixth/-0
214T__recommends:<epsilon>/0.227
396
G__:<epsilon>/-0
699
W__:adoption/0.711
W__:provision/1.929
W__:approval/3.758
W__:appropriation/3.108
W__:accreditation/3.18
W__:dependence/3.758
W__:reliance/3.746
76W__:the/-0
215
T__recommends:<epsilon>/0.227
W__:adoption/0.711
547 W__:the/0.132
397
G__:<epsilon>/-0
513 548P__NP:<epsilon>/-0
618
W__:rapporteur/0.699
W__:decision/1.000
77W__:the/-0
216
T__recommends:<epsilon>/0.227
W__:general/-0
T__2:<epsilon>/0.631
T__2#xslax/NULL:<epsilon>/1.194
217W__:recommends/-0
P__ADJP:<epsilon>/-0
619W__::/-0
112
W__:-/-0
378W__:assembly/-0
W__:the/1.188
549
W__:next/1.854
W__:following/0.227
W__:recommended/-0
W__:next/1.854
W__:following/0.227
T__the/NULL#general#assembly:<epsilon>/0.530
T__the#general#assembly:<epsilon>/2.122
T__the/NULL#assembly:<epsilon>/1.941
T__the#assembly:<epsilon>/3.641
T__the/NULL#wish/NULL#of/NULL#the/NULL#general#assembly:<epsilon>/3.421
109
W__:-/-0
W__:adoption/0.711
W__:provision/1.929
W__:appropriation/3.108
W__:accreditation/3.18
218
W__:recommends/-0
398G__:<epsilon>/-0
W__:on/2.917
W__:of/1.969
W__:by/2.975
W__:at/3.201
W__:in/2.215
W__:that/2.614
W__:with/3.075
W__:for/2.660
W__:because/4.073
W__:from/3.270
W__:as/3.201
W__:into/3.971
W__:about/3.426
W__:if/3.704
W__:than/4.147
W__:per/3.988
113
W__:-/-0
620
P__PPNP:<epsilon>/-0
730
W__:rapporteur/0.699
W__:decision/1.000
W__:recommended/-0
W__:general/-0
W__:the/-0
W__:on/2.917
W__:of/1.969
W__:by/2.975
W__:at/3.201
W__:in/2.215
W__:that/2.614
W__:with/3.075
W__:for/2.660
W__:because/4.073
W__:from/3.270
W__:as/3.201
W__:into/3.971
W__:about/3.426
W__:if/3.704
W__:per/3.988
621
W__:rapporteur/0.699
W__:decision/1.000
W__:recommend/-0
379
W__:assembly/-0
731
W__:following/0.227
700
W__:adoption/0.711
W__:provision/1.929
W__:approval/3.758
W__:appropriation/3.108
W__:accreditation/3.18
W__:dependence/3.758
W__:reliance/3.746
114W__:sixth/-0
219G__:<epsilon>/-0
W__:on/2.917
W__:of/1.969
W__:by/2.975
W__:at/3.201
W__:in/2.215
W__:that/2.614
W__:with/3.075
W__:for/2.660
W__:from/3.270
W__:as/3.201
W__:about/3.426
W__:if/3.704
622W__:implications/2.139
W__:sponsors/1.131
W__:projects/2.972
732
W__:sponsors/1.131
399
G__:<epsilon>/-0
W__:to/-0
220
W__:committee/-0
W__:on/2.917
W__:of/1.969
W__:by/2.975
W__:under/4.248
W__:at/3.201
W__:in/2.215
W__:that/2.614
W__:before/4.435
W__:with/3.075
W__:for/2.660
W__:because/4.073
W__:from/3.270
W__:as/3.201
W__:through/4.435
W__:into/3.971
W__:about/3.426
W__:between/4.602
W__:during/4.572
W__:if/3.704
W__:than/4.147
W__:over/4.385
W__:per/3.988
W__:out/4.410
550
W__:rapporteur/0.699
W__:decision/1.000
623
G__:<epsilon>/-0
W__:add/-0
221
W__:recommends/-0
400G__:<epsilon>/-0
W__:adoption/0.711
W__:provision/1.929
624W__:rapporteur/0.699
W__:decision/1.000
733
W__::/-0
W__:take/-0
222
W__:recommends/-0
W__:the/0.132
551
W__:following/0.227
W__:the/-0
W__:on/2.917
W__:of/1.969
W__:by/2.975
W__:under/4.248
W__:at/3.201
W__:in/2.215
W__:that/2.614
W__:before/4.435
W__:with/3.075
W__:for/2.660
W__:because/4.073
W__:from/3.270
W__:as/3.201
W__:through/4.435
W__:into/3.971
W__:about/3.426
W__:if/3.704
W__:than/4.147
W__:over/4.385
W__:per/3.988
W__:out/4.410
625
W__:implications/2.139
W__:sponsors/1.131
W__:projects/2.972
W__:recommended/-0
401W__:police/-0
552 W__::/-0
W__:recommends/-0
115
W__:sixth/-0
223G__:<epsilon>/-0
553
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:those/3.154
W__:all/3.201
W__:an/2.837
W__:no/3.547
W__:any/3.434
W__:some/3.097
W__:this/2.266
W__:these/3.201
W__:draft/0.404
W__:-/-0
W__:the/-0
G__:<epsilon>/-0
W__:of/1.969
626
W__:of/1.969
701W__:adoption/0.711
734
W__:rapporteur/0.699
W__:decision/1.000
W__:recommends/-0
W__:in/-0
W__:on/2.917
W__:of/1.969
W__:by/2.975
W__:under/4.248
W__:at/3.201
W__:in/2.215
W__:that/2.614
W__:with/3.075
W__:for/2.660
W__:because/4.073
W__:from/3.270
W__:as/3.201
W__:into/3.971
W__:about/3.426
W__:if/3.704
W__:than/4.147
W__:per/3.988
627G__:<epsilon>/-0
W__:adoption/0.711
W__:sixth/-0
W__:sixth/-0
W__:adoption/0.711
W__:provision/1.929
W__:xslax/-0
735
W__:articles/3.975
W__:implications/2.139
W__:sponsors/1.131
W__:projects/2.972
W__:co-sponsors/3.808
402W__:world/-0
W__:of/1.969
W__:adoption/0.711
W__:provision/1.929
W__:appropriation/3.108
W__:accreditation/3.18
W__:committee/-0
W__:establish/-0
256
285P__NP:<epsilon>/-0
W__:draft/0.404
W__:project/1.841
403P__O:<epsilon>/-0
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:those/3.154
W__:all/3.201
W__:an/2.837
W__:no/3.547
W__:any/3.434
W__:some/3.097
W__:this/2.266
W__:these/3.201
W__:draft/0.404
W__:the/-0
T__2:<epsilon>/0.631
T__2#xslax/NULL:<epsilon>/1.194
W__:committee/-0
489G__:<epsilon>/-60
736
W__:of/1.969
W__:in/2.215
W__:that/2.614
W__:for/2.660
W__:the/-0
W__:decision/-0G__:<epsilon>/-0
W__:committee/-0 737G__:<epsilon>/-0
W__:the/-0
404P__NP:<epsilon>/-0
W__:xslax/-0
G__:<epsilon>/-0
609
W__:of/1.969
405
P__PPNP:<epsilon>/-0
W__:adoption/0.711
G__:<epsilon>/-0
W__:sixth/-0
W__:rapporteur/0.699
W__:draft/0.404
W__:resolution/3.339
W__:project/1.841
W__:xslax/-0
406
P__PPNP:<epsilon>/-0
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:those/3.154
W__:all/3.201
W__:an/2.837
W__:no/3.547
W__:any/3.434
W__:some/3.097
W__:this/2.266
W__:these/3.201
T__the/NULL#general#assembly:<epsilon>/0.530
T__the#general#assembly:<epsilon>/2.122
T__the/NULL#assembly:<epsilon>/1.941
628P__NP:<epsilon>/-0
W__:committee/-0
W__:recommends/-0
407T__IN#NGEN+DT#NN#NGEN+IN#NGEN+DT#JJ#NN#NN:<epsilon>/3.545
408
T__IN#NGEN+DT#NN#NGEN+IN#JJ#NN#NNS:<epsilon>/4.133
409
T__IN#NGEN+DT#NN#NGEN+IN#DT#JJ#NN#NN:<epsilon>/4.895
410
T__IN#NGEN+DT#NN#NGEN+IN#DT#JJ#NN#NGEN+IN#NN:<epsilon>/5.049
W__:adoption/0.711
G__:<epsilon>/-60
T__the/NULL#general#assembly:<epsilon>/0.530
T__the/NULL#assembly:<epsilon>/1.941
629
W__:rapporteur/0.699
W__:decision/1.000
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:those/3.154
W__:all/3.201
W__:an/2.837
W__:some/3.097
W__:this/2.266
W__:these/3.201
411
P__PPNP:<epsilon>/-0
W__:committee/-0
T__accountable:<epsilon>/0.693
T__present/NULL:<epsilon>/0.693
W__:decided/-0
W__:implications/2.139
W__:sponsors/1.131
W__:projects/2.972
W__:co-sponsors/3.808
W__:article/-0
G__:<epsilon>/-0
W__:to/-0
W__:consider/-0
W__:review/-0
W__:with/-0
W__:draft/0.404
W__:rapporteur/0.699
W__:decision/1.000
W__:with/-0
554
W__:next/1.854
W__:following/0.227
W__:2/-0
W__:draft/0.404
W__:project/1.841
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:those/3.154
W__:all/3.201
W__:an/2.837
W__:no/3.547
W__:any/3.434
W__:some/3.097
W__:this/2.266
W__:these/3.201
W__:of/1.969
W__:in/2.215
G__:<epsilon>/-0
W__:under/-0
W__:only/-0
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:those/3.154
W__:all/3.201
W__:an/2.837
W__:some/3.097
W__:this/2.266
W__:these/3.201
W__:next/1.854
W__:following/0.227
G__:<epsilon>/-0
W__:xslax/-0
W__:by/-0
G__:<epsilon>/-60
W__:order/-0
738/0<epsilon>:</s>/-0
G__:<epsilon>/-0
W__:in/-0
W__:just/-0
W__:committee/-0
W__:rapporteur/0.699
W__:decision/1.000
W__:in/-0
P__O:<epsilon>/-0
W__:xslax/-0
W__:implications/2.139
W__:sponsors/1.131
W__:for/-0
W__:regard/-0
W__:committee/-0
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:an/2.837
W__:some/3.097
W__:this/2.266
P__VP:<epsilon>/-0
W__:on/-0
W__:the/1.188
G__:<epsilon>/-0
W__:advisory/-0
P__VP:<epsilon>/-0
W__:2.7/-0
W__:at/-0
W__:the/1.188
T__IN#NGEN+DT#NN#NGEN+IN#NGEN+DT#JJ#NN#NN:<epsilon>/3.545
T__IN#NGEN+DT#NN#NGEN+IN#JJ#NN#NNS:<epsilon>/4.133
T__IN#NGEN+DT#NN#NGEN+IN#NGEN+DT#JJ#NN#NGEN+IN#NN:<epsilon>/4.443
T__IN#NGEN+DT#NN#NGEN+IN#JJ#NNS#NGEN+IN#NN:<epsilon>/4.644
T__IN#NGEN+DT#NN#NGEN+IN#DT#JJ#NN#NN:<epsilon>/4.895
T__IN#NGEN+DT#NN#NGEN+IN#DT#JJ#NN#NGEN+IN#NN:<epsilon>/5.049
P__NP:<epsilon>/-0
W__:document/-0
G__:<epsilon>/-0
W__:with/-0
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:those/3.154
W__:all/3.201
W__:an/2.837
W__:no/3.547
W__:any/3.434
W__:some/3.097
W__:this/2.266
W__:these/3.201
W__:draft/0.404
W__:project/1.841
P__PPNP:<epsilon>/-0
W__:the/-0
G__:<epsilon>/-0
W__:for/-0
G__:<epsilon>/-0
P__O:<epsilon>/-0
G__:<epsilon>/-60
G__:<epsilon>/-0
G__:<epsilon>/-0
W__:to/-0
W__:officers/-0
W__:the/0.132
G__:<epsilon>/-0
W__:xslax/-0
G__:<epsilon>/-0
W__:in/-0
P__PPNP:<epsilon>/-0
G__:<epsilon>/-0
G__:<epsilon>/-0
W__:xslax/-0
W__:into/-0
W__:economy/-0
W__:draft/0.404
W__:project/1.841
W__:xslax/-0
W__:recommends/-0
W__:2/-0
T__-:<epsilon>/0.129
T__-/NULL:<epsilon>/2.988
W__:the/1.188
W__:the/1.188
W__:committee/-0
W__:recommends/-0
W__:2/-0
W__:following/0.227
G__:<epsilon>/-60
W__:general/-0
T__2:<epsilon>/0.631
T__2#xslax/NULL:<epsilon>/1.194
G__:<epsilon>/-0
W__:recommends/-0
W__:it/-0
W__:with/-0
111W__:assembly/-0
T__IN#NGEN+DT#NN#NGEN+IN#NGEN+DT#JJ#NN#NN:<epsilon>/3.545
T__IN#NGEN+DT#NN#NGEN+IN#JJ#NN#NNS:<epsilon>/4.133
T__IN#NGEN+DT#NN#NGEN+IN#DT#JJ#NN#NN:<epsilon>/4.895
W__:the/0.132
G__:<epsilon>/-0
G__:<epsilon>/-0
W__:on/2.917
W__:of/1.969
W__:against/4.731
W__:by/2.975
W__:under/4.248
W__:at/3.201
W__:in/2.215
W__:that/2.614
W__:after/4.880
W__:before/4.435
W__:with/3.075
W__:for/2.660
W__:because/4.073
W__:from/3.270
W__:as/3.201
W__:up/5.208
W__:without/5.054
W__:since/5.054
W__:so/5.103
W__:through/4.435
W__:into/3.971
W__:about/3.426
W__:between/4.602
W__:like/4.665
W__:during/4.572
W__:whether/4.697
W__:if/3.704
W__:than/4.147
W__:over/4.385
W__:per/3.988
W__:out/4.410
W__:within/4.766
W__:upon/5.054
W__:across/5.208
T__IN#NGEN+DT#NN#NGEN+IN#NGEN+DT#JJ#NN#NN:<epsilon>/3.545
T__IN#NGEN+DT#NN#NGEN+IN#JJ#NN#NNS:<epsilon>/4.133
T__IN#NGEN+DT#NN#NGEN+IN#JJ#NNS#NGEN+IN#NN:<epsilon>/4.644
T__IN#NGEN+DT#NN#NGEN+IN#DT#JJ#NN#NN:<epsilon>/4.895
T__IN#NGEN+DT#NN#NGEN+IN#DT#JJ#NN#NGEN+IN#NN:<epsilon>/5.049
W__:rapporteur/0.699
G__:<epsilon>/-0
W__:general/-0
G__:<epsilon>/-0
G__:<epsilon>/-0
W__:on/2.046
W__:of/2.238
W__:by/1.908
W__:at/3.536
W__:in/1.425
W__:with/1.853
W__:for/3.134
W__:as/2.616
P__O:<epsilon>/-0
G__:<epsilon>/-0
W__:recommends/-0
W__:xslax/-0
W__:on/2.046
W__:of/2.238
W__:by/1.908
W__:at/3.536
W__:in/1.425
W__:with/1.853
W__:for/3.134
W__:as/2.616
G__:<epsilon>/-0
W__:recommended/-0
W__:the/1.188
W__:that/2.597
W__:a/1.736
W__:this/2.266
W__:the/0.132
P__PPNP:<epsilon>/-0
W__:on/2.046
W__:of/2.238
W__:by/1.908
W__:at/3.536
W__:in/1.425
W__:with/1.853
W__:for/3.134
W__:as/2.616
518P__O:<epsilon>/-0
W__:recommend/-0
W__:on/2.917
W__:of/1.969
W__:against/4.731
W__:by/2.975
W__:under/4.248
W__:at/3.201
W__:in/2.215
W__:that/2.614
W__:after/4.880
W__:before/4.435
W__:with/3.075
W__:for/2.660
W__:because/4.073
W__:from/3.270
W__:as/3.201
W__:without/5.054
W__:since/5.054
W__:through/4.435
W__:into/3.971
W__:about/3.426
W__:between/4.602
W__:like/4.665
W__:during/4.572
W__:whether/4.697
W__:if/3.704
W__:than/4.147
W__:over/4.385
W__:per/3.988
W__:out/4.410
W__:within/4.766
W__:upon/5.054
W__:committee/-0
G__:<epsilon>/-0
W__:in/1.425
T__::<epsilon>/0.028
W__:the/-0
T__the/NULL#sixth#committee:<epsilon>/0.454
T__the#sixth#committee:<epsilon>/1.871
T__sixth#committee:<epsilon>/1.871
G__:<epsilon>/-0
T__IN#NGEN+DT#NN#NGEN+IN#NGEN+DT#JJ#NN#NN:<epsilon>/3.545
T__IN#NGEN+DT#NN#NGEN+IN#JJ#NN#NNS:<epsilon>/4.133
T__IN#NGEN+DT#NN#NGEN+IN#NGEN+DT#JJ#NN#NGEN+IN#NN:<epsilon>/4.443
T__IN#NGEN+DT#NN#NGEN+IN#JJ#NNS#NGEN+IN#NN:<epsilon>/4.644
T__IN#NGEN+DT#NN#NGEN+IN#DT#JJ#NN#NN:<epsilon>/4.895
T__IN#NGEN+DT#NN#NGEN+IN#DT#JJ#NN#NGEN+IN#NN:<epsilon>/5.049
W__:on/2.917
W__:of/1.969
W__:against/4.731
W__:by/2.975
W__:under/4.248
W__:at/3.201
W__:in/2.215
W__:that/2.614
W__:before/4.435
W__:with/3.075
W__:for/2.660
W__:because/4.073
W__:from/3.270
W__:as/3.201
W__:through/4.435
W__:into/3.971
W__:about/3.426
W__:between/4.602
W__:like/4.665
W__:during/4.572
W__:whether/4.697
W__:if/3.704
W__:than/4.147
W__:over/4.385
W__:per/3.988
W__:out/4.410
W__:within/4.766
W__:the/-0
W__:draft/0.404
W__:project/1.841
G__:<epsilon>/-0
W__:the/1.188
W__:a/1.736
W__:this/2.266
W__:committee/-0
W__:the/-0
G__:<epsilon>/-60
P__NP:<epsilon>/-0
W__:on/2.917
W__:of/1.969
W__:by/2.975
W__:under/4.248
W__:at/3.201
W__:in/2.215
W__:that/2.614
W__:before/4.435
W__:with/3.075
W__:for/2.660
W__:because/4.073
W__:from/3.270
W__:as/3.201
W__:through/4.435
W__:into/3.971
W__:about/3.426
W__:between/4.602
W__:like/4.665
W__:during/4.572
W__:whether/4.697
W__:if/3.704
W__:than/4.147
W__:over/4.385
W__:per/3.988
W__:out/4.410
W__:of/1.969
W__:in/2.215
Figure 2: A portion of the translation model for an Arabic test sen-
tence, compacted and aggressively pruned by path probability for dis-
play purposes.
6 Evaluation
Results Tables A and B below list evaluation results for
translation on the Arabic and French test sets respec-
tively. In each case, results for a comparison system ?
the Giza++ IBM Model 4 implementation (Och and Ney,
2000) with the ReWrite decoder (Marcu and Germann,
2002) ? are included as a benchmark. Results were gen-
erated for training corpora of varying sizes. For Arabic,
we ran our system on two large subsets of the UN cor-
pus and evaluated on a 200-sentence held-out set (refer
to Results Table A below). For the 150K sentence Ara-
bic training set, Giza++ and the shallow syntax model
achieved very similar performance. We were unable to
obtain evaluation numbers for Giza++/ReWrite on the
large Arabic training set, however, since its language
model component has a vocabulary size limit which was
exceeded in the larger corpus. In French we observed the
systems to perform similarly on the small training sets
we used (Results Table B). We performed some exper-
iments in classifier combination using the two compat-
ible (150K-training-sentence) Arabic systems, wherein
a small devtest set was used to identify simple system
combination parameters based on model confidence and
sentence length. In situations where our system was con-
fident we used its output, and used Giza++ output other-
wise. We achieved a 3% boost in Bleu score over Giza++
performance on the evaluation set with these very sim-
ple classifier combination techniques, and anticipate that
research in this direction ? classifier combination of di-
versely trained SMT systems ? could yield significant
performance improvements.
Bleu Score
System 150K 500K
Trn. Sent. Trn. Sent.
Giza++/ReWrite Decoder 0.17 *
2-level Syntax Model 0.17 0.18
Results Table A: Results comparison for Arabic to English
translation on the UN corpus, with a 200-sentence evaluation
set. Note that Giza++/ReWrite cannot be run for the 500K
sentence training set; the CMU Language Modeling Toolkit,
which ReWrite uses, has a vocabulary size limit which is
exceeded in the 500K corpus.
Bleu Score
System 5K 20K
Trn. Sent. Trn. Sent.
Giza++/ReWrite Decoder 0.08 0.11
2-level Syntax Model 0.08 0.09
Results Table B: Results comparison for French to English
translation on the Canadian Hansards corpus (200-sentence
evaluation set).
7 Conclusions
We have described and implemented an original syntax-
based statistical translation model that yields baseline re-
sults which compete successfully with other state-of-the-
art SMT models. This is particularly encouraging in that
the authors are not well-versed in Arabic or French and
it appears that the quality of the rule-based phrase chun-
kers we developed in a single person-day offers substan-
tial room for improvement. We expect to be able to at-
tain improved bracketings from native speakers and, in
addition, via translingual projection of existing brack-
eters. Secondly, the lemma model we have proposed for
lexical transfer provides an efficient framework for in-
tegrating electronic dictionaries into SMT models. Al-
though we have at this time no large electronic dictionar-
ies for either Arabic or French, efforts are underway to
acquire electronic or scanned paper dictionaries for this
purpose. We did evaluate the lemma models in isola-
tion for French and Arabic without dictionary inclusion,
but in each experiment the results did not differ signifi-
cantly from the word-specific lexical transfer models, de-
spite their substantially reduced dimensionality. We an-
ticipate that the relatively seamless direct incorporation
of dictionaries into the lemma-based models will be par-
ticularly effective for translating low-density languages,
which suffer from data sparseness in the face of limited
parallel text. Finally, we incorporated lexical translation
coercion models into this full SMT framework, the in-
duction of which is a phenomenon of interest in its own
right.
8 Acknowledgements
This work was supported in part by NSF grant number IIS-9985033. In
addition, we owe many thanks to colleagues who generously lent their
time and insights. David Smith shared his tools for Arabic part-of-
speech tagging and morphological analysis and answered many ques-
tions about the Arabic language. Thanks to Skankar Kumar and San-
jeev Khudanpur for numerous helpful discussions.
9 References
H. Alshawi, S. Bangalore, and S. Douglas. 2000. Learning depen-
dency translation models as collections of finite state head transducers
Computational Linguistics, 26(1), 45?60.
S. Bangalore and G. Riccardi. 2000. Stochastic finite-state models for
spoken language machine translation. In Proceedings of the Workshop
on Embedded Machine Translation Systems., pp. 52?59.
P. Brown, S. Della Pietra, V. Della Pietra and R. Mercer. 1993. The
mathematics of statistical machine translation: Parameter estimation.
Computational Linguistics, 12(2), 263?312.
S. Cucerzan and D. Yarowsky. 2002. Bootstrapping a Multilingual
Part-of-speech Tagger in One Person-day. Proceedings of the Sixth
Conference on Natural Language Learning (CoNLL), Taipei, 2002.
B. Dorr and N. Habash. 2002. Interlingua approximation: A
generation-heavy approach. In Proceedings of AMTA-2002.
W. A. Gale and K. W. Church. 1991. A Program for Aligning
Sentences in Bilingual Corpora. In 29th Annual Meeting of the ACL,
Berkeley, CA.
N. Habash and B. Dorr. 2003. A categorial variation database for
English. In Proceedings of NAACL-HLT 2003
D. Jones and R. Havrilla. 1998. Twisted pair grammar: Support for
rapid development of machine translation for low density languages.
In Proceedings of AMTA98, pp. 318?332.
D. Marcu and U. Germann. 2002. The ISI ReWrite Decoder Release
0.7.0b. http://www.isi.edu/licensed-sw/rewrite-decoder/.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Building a
large annotated corpus of English: the Penn Treebank. Computational
Linguistics, Vol. 19.
M. Mohri, F. Pereira, and M. Riley. 1997.
ATT General-purpose finite-state machine software tools.
http://www.research.att.com/sw/tools/fsm/.
G. Ngai and R. Florian. Transformation-based learning in the fast lane.
In Proceedings of North Americal ACL 2001, pages 40-47, June 2001.
F. J. Och and H. Ney. 2000. Improved statistical alignment models.
In Proceedings of the 38th Annual Meeting of the Association for
Computational Linguistics, pages 440?447.
F.J. Och, C. Tillmann, H. Ney. Improved Alignment Models for
Statistical Machine Translation. In Proceedings of EMNLP 1999, pp.
20-28.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001. Bleu: a method
for automatic evaluation of machine translation. Technical Report
RC22176 (W0109-022), IBM Research Division.
A. Stolcke. 2002. SRILM - an extensible language modeling
toolkit. In Proceedings of the International Conference on Spo-
ken Language Processing, pages 901-904. Denver, CO, USA.
http://www.speech.sri.com/projects/srilm/.
D. Wu. 1997. Stochastic inversion transduction grammars and
bilingual parsing of parallel corpora. Computational Linguistics,
23(3), 377?404.
K. Yamada and K. Knight. 2001. A syntax-based statistical translation
model. In Proceedings of ACL-2001, pp. 523?529.
K. Yamada and K. Night. 2002. A decoder for syntax-based statistical
MT In Proceedings of ACL-2002, pp. 303?310.
Word Translation Probabilities
Word translation for mangeait conditioned on
French Word, EnglishPOS
mangeait VBG eating 1.00
mangeait VB go 0.50
mangeait VB anticipate 0.50
mangeait VBD were 1.00
mangeait VBP knelt 1.00
mangeait NN bill 1.00
Word translation for mangeait conditioned on
French Word, English Coarse POS
mangeait V eating 0.44
mangeait V were 0.22
mangeait V knelt 0.11
mangeait V go 0.11
mangeait V anticipate 0.11
mangeait N bill 1.00
Word translation for mangeait conditioned on
French Word only
mangeait eating 0.29
mangeait were 0.14
mangeait go 0.07
mangeait bill 0.07
Word translation for mangeant
conditioned on French Word, EnglishPOS
mangeant RB mostly 1.00
mangeant JJ final 1.00
mangeant VBN obtained 1.00
mangeant VBG eating 1.00
mangeant WP who 1.00
mangeant IN through 1.00
mangeant NN lard 1.00
mangeant VBZ eats 0.50
mangeant VBZ comes 0.50
Lemma Translation Probabilities
Generation of a verb lemma given manger
manger V eat 0.60
manger V feed 0.05
manger V have 0.04
Generation of a noun lemma given manger
manger N meal 0.06
manger N trough 0.04
manger N loaf 0.04
manger N food 0.04
Generation of an adj. lemma given manger
manger J hungry 0.33
Raw lemma translation probabilities
(ignoring English Coarse POS)
manger eat 0.28
manger to 0.03
manger feed 0.03
manger out 0.02
manger have 0.02
manger are 0.02
manger , 0.02
manger you 0.01
manger meal 0.01
Table 9: Direct generation (word-to-word translation probabilities
at the various levels of backoff) is contrasted with lemma generation.
Manger (?to eat?) is a relatively rare word in the Hansards. Note that
due to low counts, the desired verb POS (target of generation) for ?eat?
may not have been observed as a translation in training data. In addi-
tion, in this situation, noisy word alignments may cause an incorrect
translation to have similar estimated translation probability. This prob-
lem is addressed by the lemma model; note the much sharper probabil-
ity distribution for verb lemmas given manger. Generation of English
inflections given lemma and target POS is algorithmic (and irregular
exceptions are handled via a lookup table).
French Wd. Eng. POS Eng. Wd. Prob.
accepter JJ unacceptable 0.12
accepter JJ acceptable 0.12
accepter JJ willing 0.11
accepter JJ ready 0.03
accepter NN acceptance 0.09
accepter NN amendment 0.03
droit VBN entitled 0.66
droit VBN allowed 0.09
droit VBN denied 0.03
droit VBN given 0.02
droit VBN permitted 0.02
droit VBN justified 0.01
droit VBN qualified 0.01
droit VBN allotted 0.01
droit VB qualify 0.14
droit VB be 0.11
droit VB have 0.09
droit VB receive 0.08
droit VB get 0.07
droit VB expect 0.03
droit VBG receiving 0.11
droit VBG getting 0.08
droit NNS rights 0.44
droit NNS benefits 0.69
Table 10: Examples of word translation coercions. Co-
ercions of the French verb accepter ?to accept? and the
French noun droit ?right? (there is parallel polysemy be-
tween the two languages for this word, but the predom-
inant sense in our corpus is the philosophical/judicial
sense, as opposed to the direction).
Eng. Phrase French Eng. Prob.
Type Phrase Phrase
NP dans le cas pre?sent a situation 0.25
NP dans le cas pre?sent the subject of debate 0.25
NP dans le cas pre?sent the position 0.25
NP dans le cas pre?sent it 0.25
VP dans le cas pre?sent should apply 1.00
ADVP dans le cas pre?sent really 1.00
PPNP dans le cas pre?sent in this case 0.63
PPNP dans le cas pre?sent in this instance 0.04
PPNP dans le cas pre?sent in this actual case 0.04
PPNP dans le cas pre?sent in this particular case 0.04
PPNP dans le cas pre?sent in that case 0.04
PPNP dans le cas pre?sent in the present circumstances 0.04
VP acceptons accept 0.48
VP acceptons agree 0.14
NP acceptons this consent 1.00
PPNP par an per year 0.67
PPNP par an in each year 0.03
PPNP par an for a year 0.03
ADVP par an annually 1.00
NP par an a year 0.79
NP par an each year 0.02
NP un discours a speech 0.83
NP un discours an address 0.05
VP un discours to speak 1.00
Table 11: Examples of direct phrase translations (see Ta-
ble 4(1)), including some coercions.
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 49?56,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Induction of Fine-grained Part-of-speech Taggers via
Classifier Combination and Crosslingual Projection
Elliott Franco Dra?bek
Department of Computer Science
Johns Hopkins University
Baltimore, MD 21218
edrabek@cs.jhu.edu
David Yarowsky
Department of Computer Science
Johns Hopkins University
Baltimore, MD 21218
yarowsky@cs.jhu.edu
Abstract
This paper presents an original approach
to part-of-speech tagging of fine-grained
features (such as case, aspect, and adjec-
tive person/number) in languages such as
English where these properties are gener-
ally not morphologically marked.
The goals of such rich lexical tagging
in English are to provide additional fea-
tures for word alignment models in bilin-
gual corpora (for statistical machine trans-
lation), and to provide an information
source for part-of-speech tagger induction
in new languages via tag projection across
bilingual corpora.
First, we present a classifier-combination
approach to tagging English bitext with
very fine-grained part-of-speech tags nec-
essary for annotating morphologically
richer languages such as Czech and
French, combining the extracted fea-
tures of three major English parsers,
and achieve fine-grained-tag-level syntac-
tic analysis accuracy higher than any indi-
vidual parser.
Second, we present experimental results
for the cross-language projection of part-
of-speech taggers in Czech and French via
word-aligned bitext, achieving success-
ful fine-grained part-of-speech tagging of
these languages without any Czech or
French training data of any kind.
1 Introduction
Most prior research in part-of-speech (POS) tag-
ging has focused on supervised learning over a
tagset such as the Penn Treebank tagset for En-
glish, which is restricted to features that are mor-
phologically distinguished in the focus language.
Thus the only verb person/number distinction made
in the Brown Corpus/Penn Treebank tagset is VBZ
(3rd-person-singular-present), with no correspond-
ing person/number distinction in other tenses. Sim-
ilarly, adjectives in English POS tagsets typically
have no distinctions for person, number or case be-
cause such properties have no morphological surface
distinction, although they do for many other lan-
guages.
This essential limitation of the Brown/Penn POS
subtag inventory to morphologically realized dis-
tinctions in English dramatically simplifies the prob-
lem by reducing the tag entropy per surface form
(the adjective tall has only one POS tag (JJ) rather
than numerous singular, plural, nominative, ac-
cusative, etc. variants), increasing both the stand-
alone effectiveness of lexical prior models and word-
suffix models for part-of-speech tagging.
However, for many multilingual applications, in-
cluding feature-based word alignment in bilingual
corpora and machine translation into morphologi-
cally richer languages, it is helpful to extract finer-
grained lexical analyses on the English side that
more closely parallel the morphologically realized
tagset of the second (source or target) language.
In particular, prior work on translingual part-of-
speech tagger projection via parallel bilingual cor-
pora (e.g. Yarowsky et al, 2001) has been limited
to inducing part-of-speech taggers in second lan-
guages (such as French or Czech) that only assign
tags at the granularity of their source language (i.e.
49
the Penn Treebank-granularity distinctions from En-
glish). The much richer English tagsets achieved
here can allow these tagger projection techniques to
transfer richer tag distinctions (such as case and verb
person/number) that are important to the full analy-
sis of these languages, using only bilingual corpora
with the morphologically impoverished English.
For quickly retargetable machine translation, the
primary focus of effort is overcoming the extreme
scarcity of resources for the low density source lan-
guage. Sparsity of conditioning events for a transla-
tion model can be greatly reduced by the availabil-
ity of automatic source-language analysis. In this
research we attempt to induce models for the au-
tomatic analysis of morphological features such as
case, tense, number, and polarity in both the source
and target languages with this end in mind.
2 Prior Work
2.1 Fine-grained part-of-speech tagging
Most prior work in fine-grained part-of-speech tag-
ging has been limited to languages such as Czech
(e.g. Hajic? and Hladka?, 1998) or French (e.g. Fos-
ter etc.) where finer-grained tagset distinctions are
morphologically marked and hence natural for the
language. In support of supervised tagger learn-
ing of these languages, fine-trained tagset inven-
tories have been developed by the teams above
at Charles University (Czech) and Universite? de
Montre?al (French). The tagset developed by Hajic?
forms the basis of the distinctions used in this paper.
The other major approach to fine-grained tagging
involves using tree-based tags that capture grammat-
ical structure. Bangalore and Joshi (1999) have uti-
lized ?supertags? based on tree-structures of various
complexity in the tree-adjoining grammar model.
Using such tags, Brants (2000) has achieved the au-
tomated tagging of a syntactic-structure-based set of
grammatical function tags including phrase-chunk
and syntactic-role modifiers trained in supervised
mode from a treebank of German.
2.2 Classifier combination for part-of-speech
tagging
There has been broad work in classifier combination
at the tag-level for supervised POS tagging mod-
els. For example, Ma`rquez and Rodr??guez (1998)
have performed voting over an ensemble of decision
tree and HMM-based taggers for supervised En-
glish tagging. Murata et al (2001) have combined
neural networks, support vector machines, decision
lists and transformation-based-learning approaches
for Thai part-of-speech tagging. In each of these
cases, annotated corpora containing the full tagset
granularity are required for supervision.
Henderson and Brill (1999) have approached
parsing through classifier combination, using bag-
ging and boosting for the performance-weighted
voting over the parse-trees from three anonymous
statistical phrase-structure-based parsers. However,
as their switching and voting models assumed equiv-
alent phrase-structure conventions for merger com-
patibility, it is not clear how a dependency parsing
model or other divergent syntactic models could be
integrated into this framework. In contrast, the ap-
proach presented below can readily combine syntac-
tic analyses from highly diverse parse structure mod-
els by first projecting out all syntactic analyses onto
a common fine-grained lexical tag inventory.
2.3 Projection-based Bootstrapping
Yarowsky et al (2001) performed early work in the
cross-lingual projection of part-of-speech tag anno-
tations from English to French and Czech, by way of
word-aligned parallel bilingual corpora. They also
used noise-robust supervised training techniques to
train stand-alone French and Czech POS taggers
based on these projected tags. Their projected
tagsets, however, were limited to those distinctions
captured in the English Penn treebank inventory,
and hence failed to make many of the finer grained
distinctions traditionally assumed for French and
Czech POS tagging, such as verb person, number,
and polarity and noun/adjective case.
Probst (2003) pursued a similar methodology for
the purposes of tag projection, using a somewhat
expanded tagset inventory (e.g. including adjec-
tive number but not case), and focusing on target-
language monolingual modeling using morpheme
analysis. Cucerzan and Yarowsky (2003) addressed
the problem of grammatical gender projection via
the use of small seed sets based on natural gender.
Another distinct body of work addresses the prob-
lem of parser bootstrapping based on syntactic de-
pendency projection (e.g. Hwa et al 2002), often
using approaches based in synchronous parsing (e.g.
Smith and Smith, 2004).
50
Word Core Prsn Num. Case Tns/ Pol. Voi.
POS Asp.
The DT 3 PL. NOM.
books NN 3 PL. NOM.
were VB 3 PL. PAST + ACT.
provoking VB 3 PL. PAST- + ACT.
PROG.
laughter NN 3 S. ACC.
with IN
their DT 3 PL. ?WITH?
curious JJ 3 PL. ?WITH?
titles NN 3 PL. ?WITH?
Figure 1: Example of fine-grained English POS tags
Word Core Prsn Num. Case Tns/ Pol. Voice
POS Asp.
Les DT 3 PL. NOM.
livres NN 3 PL. NOM.
provoquaient VB 3 PL. PAST- + ACT.
PROGR.
des DT 3 PL. ACC.
rires NN 3 PL. ACC.
avec IN
ses DT 3 PL. ?WITH?
titres NN 3 PL. ?WITH?
curieux JJ 3 PL. ?WITH?
Figure 2: Example of fined-grained POS tags pro-
jected onto a French translation
3 Tagsets
We use Penn treebank-style part-of-speech tags as
a substrate for further enrichment (for all of the ex-
periments described here, text was first tagged us-
ing the fnTBL part-of-speech tagger (Ngai and Flo-
rian, 2001)). Each Penn tag is mapped to a core
part-of-speech tag, which determines the set of fine-
grained tags further applicable to each word. The
fine-grained tags applicable to nouns, verbs, and ad-
jective are shown in Table 1. This paper concentrates
on these most important core parts-of-speech.
The example English sentence in Figure 1 illus-
trates several key points about our tagset. Some of
the information we are interested in is already ex-
pressed by the Penn-style tags ? the NN titles is plu-
ral; the VBD were is in the past tense. For these, our
goal is simply to make these facts explicit.
On the other hand, curious could also be meaning-
fully said to be semantically plural, and most impor-
tantly for us, the corresponding word in a translation
of this sentence into many other languages would
be morphologically plural. Similarly, the head verb
provoking is also semantically in the past tense, and
is likely to be translated to a past-tense form in many
languages, even though in this example the actual
tense marking is on were. We expect the ?past-
ness? of the action to be much more stable cross-
linguistically, than the particular division of labor
between the head word and the auxiliary. By prop-
VB JJ NN Range
Person       1 / 2 / 3
Number       SINGULAR
PLURAL
Case     NOMINATIVE
ACCUSATIVE
GENITIVE
PREPOSITION-?IN?
PREPOSITION-?OF?
. . .
Degree   POSITIVE
COMPARATIVE
SUPERLATIVE
Tense   PAST
PRESENT
FUTURE
Perfectivity   + / ?
Progressivity   + / ?
Polarity   + / ?
Voice   ACTIVE / PASSIVE
Table 1: The fine-grained POS inventory used for
English
agating these features from where they are explicit
to where they are not, we hope to make information
more directly available for projection. Another im-
portant class of information we would like to make
available concerns syntactic relations, which many
languages mark with morphological case. This is an
issue that involves deep, complex, and ambiguous
mappings, which we are not yet prepared to treat in
their fullness. For now, we observe that curious and
titles are both dominated by with.
Because of intent to mark whatever information
is recoverable, some of our tags require some in-
terpretation. For example, English has little or no
morphological realization of syntactic case, but the
essential information of case, relationship of a noun
with its governor, is recoverable from contextual
information, so we defined it in these terms. To
avoid loss of information, we chose to remain ag-
nostic about deeper analyses, such as the identifi-
cation of theta roles or predicate-argument relation-
ships, and restricted ourselves to a direct represen-
tation of surface relationships. We identified sub-
jects, direct and indirect objects, non-heads of noun
compounds, possessives, and temporal adjuncts, and
created a distinct tag for the objects of each distinct
preposition.
Our ideal would be to have as expansive and de-
tailed a tagset as possible, a ?quasi-universal? tagset
which could cover whatever set of distinctions might
be relevant for any language onto which we might
51
Feature Antecedent   CONSEQUENT
Noun Number NN   SINGULAR
NNS   PLURAL
Verb Tense VBD   PAST
(will

shall) RB* VB   FUTURE
Figure 3: Examples of locally recoverable features
project our analysis. A completely universal tagset
would require that the morphological distinctions
made by the world?s languages come from a limited
pool of possibilities, based on non-arbitrary seman-
tic distinctions, and further would require that the
relevant semantic information be recoverable from
English text. The tagset we are using now is shaped
in part by exceptions to these conditions. For ex-
ample, we have put off implementing tagging of
gender given the notoriously arbitrary and inconsis-
tent assignment of grammatical gender across lan-
guages (although Cucerzan and Yarowsky (2003)
were able to show success on projection-based anal-
ysis of grammatical gender as well).
In the end, we have settled on a set of distinc-
tions very similar to those realized by the morpho-
logically richer of the European languages, with the
noticeable absence of gender. Table 1 describes the
features we chose on this basis (definiteness and
mood features were developed for English but not
projected to French or Czech, and are not treated in
this paper).
4 Methods ? English Tagging
The features we tagged vary widely in their degree
of morphological versus syntactic marking, and the
difficulty of their monolingual English detection.
For some, tagging is simply a matter of explicitly
separating information contained in the Penn part-
of-speech tags, while others can be tagged to a high
degree of accuracy with simple heuristics based on
local word and part-of-speech tag patterns. These
include number for nouns and adjectives, person
(trivially) for nouns, degree for adjectives, polarity,
voice, and aspect (perfectivity and progressivity) for
verbs, as well as tense for some verbs. Figure 3
shows example rules for some of these easier cases.
The more difficult features are those whose de-
tection requires some degree of syntactic analysis.
These include case, which summarizes the relation
of each noun with its governor, and the agreement-
based features: we define person, number, and case
for attributive adjectives by agreement with their
head nouns, number and person for verbs and predi-
cate adjectives by agreement with their subjects, and
tense for some verbs by agreement with their in-
flected auxiliaries.
We investigated four individual approaches for
the syntax-features ? a regular-expression-based
quasi-parser, a system based on Dekang Lin?s Mini-
Par (Lin, 1993), a system based on the Collins parser
(Collins, 1999), and one based on the CMU Link
Grammar Parser (Sleator and Temperley, 1993),
as well as a family of voting-based combination
schemes.
4.1 Regular-expression Quasi-parser
The regular-expression ?quasi-parser? takes a direct
approach, using several dozen heuristics based on
regular-expression-like patterns over words, Penn
part-of-speech tags, and the output of the fnTBL
noun chunker. Use of the noun chunker fa-
cilitates identification of noun/dependent relation-
ships within chunks, and extends the range of pat-
terns identifying noun/governor relationships across
chunks.
The output of the quasi-parser consists of two
parts: a case tag for each noun in a sentence, and
a set of agreement links across which other features
are then spread. We call this a direct approach be-
cause the links are defined operationally, directly in-
dicating the spreading action, rather than represent-
ing any deeper syntactic analysis.
In the diagram of the example sentence below, an
arrow from one word to another indicates that the
former takes features from the latter. The example
also shows the context patterns by which the nouns
in the sentence receive case.
+<<<<<+ +>>>>>>>>>>>>>+
| | | |
+>>>>+ +<<<<<<<+ | +>>>>>>+
| | | | | | |
<The books> were provoking laughter with <their curious titles>
Word Context Pattern   CASE TAG
laughter VB (genitive-NP)*     ACCUSATIVE
titles with (genitive-NP)*     PREP-WITH
books default   NOMINATIVE
4.2 MiniPar and the CMU Link Grammar
Parser
For MiniPar, the Collins parser, and the CMU
link grammar parser, we developed for each a set
of minimal-complexity heuristics to transform the
parser output into the specific conceptions of depen-
dency and case we had developed for the first pass.
52
MiniPar produces a labeled dependency graph,
which yields a straightforward extraction of the in-
formation needed for this task. Case tagging is a
simple matter of mapping the set of dependency la-
bels to our case inventory. Our agreement links
are almost a subset of MiniPar?s dependencies (with
some special treatment of subject/auxiliary/main-
verb triads, as shown in the example sentence).
The figure below presents MiniPar?s raw output
for the example sentence, along with some exam-
ple dependency-label/case-tag rules. The agreement
links extracted from the dependency graph are iden-
tical (in this case) to those produced by the regular-
expression quasi-parser.
mod pcomp-n
+<<<<<<+<<<<<<<<<<<<<<<<<<<+
| | |
s | | gen |
+>>>>>>>>>>>>>+ | | +>>>>>>>>>>>>>+
| | | | | |
det| be | obj | | | mod |
+>>>+ +>>>>>>>+<<<<<<<<+ | | +>>>>>>+
| | | | | | | | |
| | | | | | | | |
The books were provoking laughter with their curious titles
Word Dependency Label   CASE TAG
books s   NOMINATIVE
laughter obj   ACCUSATIVE
titles pcomp-n:with   PREP-WITH
The output of the CMU link grammar parser has
properties similar to MiniPar, and thus tag extraction
was handled in a similar fashion.
4.3 Collins Parser
The Collins Parser produces a Penn-Treebank-style
constituency tree, with head labels. Although we
could have used the head-labels to operate on the
dependency graph as with MiniPar, we chose to con-
centrate on addressing the weakest point of our pre-
vious systems, the identification of case. Our algo-
rithm traces the path from each noun to the root of
the tree, stopping at the first node which we judged
to reliably indicate case.
We did not directly extract any further informa-
tion from the Collins parser output. Instead, the
remainder of the system is identical to the regular-
expression quasi-parser. However, because the sys-
tem uses nominative case to identify verb sub-
jects, we did expect to see some improvements in
agreement-based features as well.
S
NPB
The books
VP
were VP
provoking NPB
laughter
PP
with NPB
their curious titles
Word Path to Root   CASE TAG
books NPB:S   NOMINATIVE
laughter NPB:VP:VP:VP:S   ACCUSATIVE
titles NPB:PP(with):VP:VP:S   PREP-WITH
4.4 Parser Combination
The fine-grained taggers based on the four partic-
ipating parsers exhibited significant differences in
their strengths and weaknesses, suggesting poten-
tial benefit from combining them. Lacking tag-level
numerical scores and development data for weight-
training, we restricted ourselves to simple voting
mechanisms. We chose to do all of the combinations
at the end of the process, voting separately on tags
for specific features of specific words. Without tag-
level probabilities from the one-best parser outputs,
we were still able to use the combination protocols
to achieve a coarse-grained confidence measure.
We compared a series of seven combination pro-
tocols of increasing leniency to investigate preci-
sion/recall tradeoffs. The strictest, ?4:0?, produces
an output only when there are four votes for the fa-
vored tag, and no votes for any other. Analogously,
protocols ?3:0?, ?2:0? and ?1:0? also allow no dissent,
but allow progressively more abstentions. Continu-
ing the sequence, protocol ?2:1? proposes a tag as
long as there is a clear majority, ?2:2? as long as sup-
porters are not outnumbered by dissenters, and ?1:3?
whenever possible. To break ties in the latter two
protocols, we favored first the CMU Link Parser,
then Collins, then MiniPar, then Regexp. (Lacking
sufficient labeled data for fine-tuning, we ordered
them arbitrarily.)
5 Evaluation of English POS Tagging
Before we began the development of our taggers, we
created standard tagging guidelines, and hand anno-
tated a 3013-word segment of the English side of the
Canadian Hansards, to be used for evaluation.
53
Core Feature MiniPar Regexp Collins CMU Link 1:3
POS
num 86.8 87.7 87.7 87.9 88.4
case 65.1 74.5 76.4 79.2 80.6
JJ deg 100 100 100 100 100
?French? 86.8 87.7 87.7 87.9 88.4
?Czech? 57.9 64.3 67.1 68.1 70.5
num 99.7 99.7 99.7 99.7 99.7
NN case 65.9 74.8 77.8 77.3 80.0
?French? 99.7 99.7 99.7 99.7 99.7
?Czech? 65.0 74.8 77.8 77.2 79.9
num 77.2 64.8 65.5 66.8 78.1
tns 77.2 66.8 67.1 67.1 76.3
prsn 88.0 75.0 74.3 73.4 86.5
VB pol 96.3 96.6 96.6 96.6 96.6
voice 88.0 88.0 88.0 88.0 88.0
?French? 61.8 61.3 61.0 61.3 67.5
?Czech? 61.3 61.1 60.8 61.1 67.1
overall ?French? 82.6 82.5 82.4 83.2 85.2
?Czech? 62.5 67.8 69.4 70.5 73.3
Table 2: English tagging forced-choice accuracy
Core Feature Mini Regexp Collins CMU 2:0 1:0 1:2
POS Par Link
num 79.1 81.3 81.3 82.2 81.2 83.8 83.9
JJ case 72.1 79.2 83.0 78.9 78.1 79.1 84.2
deg 100 100 100 100 100 100 100
?Czech? 67.6 72.2 76.0 74.3 70.4 73.4 77.9
num 99.7 99.7 99.7 99.7 99.7 99.7 99.7
NN case 68.5 75.5 78.6 77.9 72.6 72.5 78.1
?Czech? 68.1 75.2 78.3 77.7 72.2 72.1 77.8
tns 78.0 68.5 68.7 68.0 68.7 78.3 78.3
num 72.7 61.3 61.2 61.3 61.1 76.1 77.1
prsn 77.2 66.5 65.4 63.9 64.0 78.3 79.0
VB pol 96.3 96.6 96.6 96.5 96.5 96.5 96.6
voice 88.0 88.0 88.0 88.0 88.0 88.0 88.0
?French? 61.7 50.7 50.2 50.1 50.6 64.8 65.6
?Czech? 61.1 50.5 49.9 49.8 50.4 64.5 65.2
all ?French? 81.9 78.7 78.5 78.5 83.6 78.9 83.9
?Czech? 65.4 66.0 67.8 69.3 68.9 63.5 72.9
Table 3: English tagging F-measure
3:0
 55
 60
 65
 70
 75
 80
 85
 90
 95
 70  75  80  85  90  95  100
R
ec
al
l
Consensus
MiniPar
2:0
Precision
Noun Case
CMULink
Collins
RegExp
2:21:3
2:1
1:0
4:0
 50
Figure 4: Precision versus Recall ? Noun case
Verb number
 40
 45
 50
 55
 60
 65
 70
 75
 80
 86  88  90  92  94
R
ec
al
l
Precision
Consensus
2:2
1:3 2:1 1:0
MiniPar
Collins
CMULink RegExp
3:0
2:0
4:0
 35
Figure 5: Precision versus Recall ? Verb number
Table 2 shows system accuracy on test data in
a forced-choice evaluation, where abstentions were
replaced by the most common tag for the each situa-
tion (the combination system is that one biased most
heavily towards recall.)
In addition to the individual features, we also list
?pseudo-French? and ?pseudo-Czech?. These rep-
resent exact-match accuracies for composite fea-
tures comprising those features typically realized in
French or Czech POS taggers. For example, pseudo-
Czech verb accuracy of 67.1% indicates that for
67.1% of verb instances, the Czech-realized features
of number, tense, perfectivity, progressivity, polar-
ity, and voice were all correct. These give an indica-
tion of the quality of the starting point for crosslin-
gual bootstrapping to the respective languages.
Besides the forced-choice scenario, we were also
interested in the effect of allowing abstentions for
low-confidence cases. Table 3 shows the F-measure
of precision and recall for the individual systems, as
well as a range of combination systems. Figures 4
and 5 show (for two example features) the clear pre-
cision/recall tradeoff. Performance of the consensus
systems is higher than the individual parser-based
taggers at all levels of tag precision or recall.
Unfortunately, because MiniPar does its own inte-
grated tokenization and part-of-speech tagging, we
found that a significant portion of the errors seemed
to stem from discrepancies where MiniPar disagreed
on the segmentation or the core part-of-speech of the
words in question.
6 Cross-lingual POS Tag Projection and
Bootstrapping
Our cross-lingual POS tag projection process is sim-
ilar to Yarowsky et al (2001). It begins by perform-
ing a statistical sentence and word alignment of the
bilingual corpora (described below), and then trans-
fers both the coarse- and fine-grained tags achieved
from classifier combination on the English side via
the higher confidence word alignments (based on the
intersection of the 1-best word alignments induced
from French to English and English to French. The
projected tags then serve as noisy monolingual train-
ing data in the source language.
There are several notable differences and exten-
sions: The first major difference is that the projected
fine-grained tag set is much more detailed, including
such additional properties as noun case, adjective
54
case and number, and verb person, number, voice,
and polarity. Because these span the subtag features
normally assumed for Czech and French part-of-
speech taggers, the projection work presented here
for the first time shows the translingual projection
and induction of full-granularity Czech and French
taggers, rather than the much less complete and
coarser-grained prior projection work.
The other major differences are in the method
of target-language monolingual tagger generaliza-
tion from the projected tags. We pursue a combi-
nation of trie-based lexical prior models and local-
agreement-based context models. The lexical prior
trie model, as illustrated in Figure 6 for noun num-
ber, shows how the hierarchically-smoothed lexical
prior conditioned on variable length suffixes can as-
sign noun number probabilities to both previously
seen words (with full-word-length suffixes stored)
and to new words in test data, based on backoff to
partially matching suffixes.
The context models are based on exploiting agree-
ment phenomena of the fine-grained tag features in
local context.  
	 	Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 37?44, New York City, June 2006. c?2006 Association for Computational Linguistics
Resolving and Generating Definite Anaphora
by Modeling Hypernymy using Unlabeled Corpora
Nikesh Garera and David Yarowsky
Department of Computer Science
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218, USA
{ngarera,yarowsky}@cs.jhu.edu
Abstract
We demonstrate an original and success-
ful approach for both resolving and gen-
erating definite anaphora. We propose
and evaluate unsupervised models for ex-
tracting hypernym relations by mining co-
occurrence data of definite NPs and po-
tential antecedents in an unlabeled cor-
pus. The algorithm outperforms a stan-
dard WordNet-based approach to resolv-
ing and generating definite anaphora. It
also substantially outperforms recent re-
lated work using pattern-based extraction
of such hypernym relations for corefer-
ence resolution.
1 Introduction
Successful resolution and generation of definite
anaphora requires knowledge of hypernym and hy-
ponym relationships. For example, determining the
antecedent to the definite anaphor ?the drug? in text
requires knowledge of what previous noun-phrase
candidates could be drugs. Likewise, generating a
definite anaphor for the antecedent ?Morphine? in
text requires both knowledge of potential hypernyms
(e.g. ?the opiate?, ?the narcotic?, ?the drug?, and
?the substance?), as well as selection of the most ap-
propriate level of generality along the hypernym tree
in context (i.e. the ?natural? hypernym anaphor).
Unfortunately existing manual hypernym databases
such as WordNet are very incomplete, especially
for technical vocabulary and proper names. Word-
Nets are also limited or non-existent for most of the
world?s languages. Finally, WordNets also do not
include notation of the ?natural? hypernym level for
anaphora generation, and using the immediate par-
ent performs quite poorly, as quantified in Section 5.
In first part of this paper, we propose a novel ap-
proach for resolving definite anaphora involving hy-
ponymy relations. We show that it performs substan-
tially better than previous approaches on the task of
antecedent selection. In the second part we demon-
strate how this approach can be successfully ex-
tended to the problem of generating a natural def-
inite NP given a specific antecedent.
In order to explain the antecedent selection task for
definite anaphora clearly, we provide the follow-
ing example taken from the LDC Gigaword corpus
(Graff et al, 2005).
(1)...pseudoephedrine is found in an allergy treat-
ment, which was given to Wilson by a doctor when
he attended Blinn junior college in Houston. In a
unanimous vote, the Norwegian sports confedera-
tion ruled that Wilson had not taken the drug to en-
hance his performance...
In the above example, the task is to resolve
the definite NP the drug to its correct antecedent
pseudoephedrine, among the potential antecedents
<pseudoephedrine, allergy, blinn, college, hous-
ton, vote, confederation, wilson>. Only Wilson can
be ruled out on syntactic grounds (Hobbs, 1978).
To be able to resolve the correct antecedent from
the remaining potential antecedents, the system re-
quires the knowledge that pseudoephedrine is a
drug. Thus, the problem is to create such a knowl-
edge source and apply it to this task of antecedent
selection. A total of 177 such anaphoric examples
37
were extracted randomly from the LDC Gigaword
corpus and a human judge identified the correct an-
tecedent for the definite NP in each example (given a
context of previous sentences).1 Two human judges
were asked to perform the same task over the same
examples. The agreement between the judges was
92% (of all 177 examples), indicating a clearly de-
fined task for our evaluation purposes.
We describe an unsupervised approach to this task
that extracts examples containing definite NPs from
a large corpus, considers all head words appearing
before the definite NP as potential antecedents and
then filters the noisy<antecedent, definite-NP> pair
using Mutual Information space. The co-occurence
statistics of such pairs can then be used as a mecha-
nism for detecting a hypernym relation between the
definite NP and its potential antecedents. We com-
pare this approach with a WordNet-based algorithm
and with an approach presented by Markert and Nis-
sim (2005) on resolving definite NP coreference that
makes use of lexico-syntactic patterns such as ?X
and Other Ys? as utilized by Hearst (1992).
2 Related work
There is a rich tradition of work using lexical and se-
mantic resources for anaphora and coreference res-
olution. Several researchers have used WordNet as
a lexical and semantic resource for certain types of
bridging anaphora (Poesio et al, 1997; Meyer and
Dale, 2002). WordNet has also been used as an im-
portant feature in machine learning of coreference
resolution using supervised training data (Soon et
al., 2001; Ng and Cardie, 2002). However, sev-
eral researchers have reported that knowledge incor-
porated via WordNet is still insufficient for definite
anaphora resolution. And of course, WordNet is not
available for all languages and is missing inclusion
of large segments of the vocabulary even for cov-
ered languages. Hence researchers have investigated
use of corpus-based approaches to build a Word-
Net like resource automatically (Hearst, 1992; Cara-
1The test examples were selected as follows: First, all
the sentences containing definite NP ?The Y? were extracted
from the corpus. Then, the sentences containing instances
of anaphoric definite NPs were kept and other cases of defi-
nite expressions (like existential NPs ?The White House?,?The
weather?) were discarded. From this anaphoric set of sentences,
177 sentence instances covering 13 distinct hypernyms were
randomly selected as the test set and annotated for the correct
antecedent by human judges.
ballo, 1999; Berland and Charniak, 1999). Also,
several researchers have applied it to resolving dif-
ferent types of bridging anaphora (Clark, 1975).
Poesio et al (2002) have proposed extracting lexical
knowledge about part-of relations using Hearst-style
patterns and applied it to the task of resolving bridg-
ing references. Poesio et al (2004) have suggested
using Google as a source of computing lexical dis-
tance between antecedent and definite NP for mere-
ological bridging references (references referring to
parts of an object already introduced). Markert et al
(2003) have applied relations extracted from lexico-
syntactic patterns such as ?X and other Ys? for Other-
Anaphora (referential NPs with modifiers other or
another) and for bridging involving meronymy.
There has generally been a lack of work in the exist-
ing literature for automatically building lexical re-
sources for definite anaphora resolution involving
hyponyms relations such as presented in Example
(1). However, this issue was recently addressed by
Markert and Nissim (2005) by extending their work
on Other-Anaphora using lexico syntactic pattern ?X
and other Y?s to antecedent selection for definite NP
coreference. However, our task is more challeng-
ing since the anaphoric definite NPs in our test set
include only hypernym anaphors without including
the much simpler cases of headword repetition and
other instances of string matching. For direct eval-
uation, we also implemented their corpus-based ap-
proach and compared it with our models on identical
test data.
We also describe and evaluate a mechanism for com-
bining the knowledge obtained from WordNet and
the six corpus-based approaches investigated here.
The resulting models are able to overcome the weak-
nesses of a WordNet-only model and substantially
outperforms any of the individual models.
3 Models for Lexical Acquisition
3.1 TheY-Model
Our algorithm is motivated by the observation that in
a discourse, the use of the definite article (?the?) in a
non-deictic context is primarily licensed if the con-
cept has already been mentioned in the text. Hence a
sentence such as ?The drug is very expensive? gen-
erally implies that either the word drug itself was
previously mentioned (e.g. ?He is taking a new drug
for his high cholesterol.?) or a hyponym of drug was
38
previously mentioned (e.g. ?He is taking Lipitor for
his high cholesterol.?). Because it is straightforward
to filter out the former case by string matching, the
residual instances of the phrase ?the drug? (without
previous mentions of the word ?drug? in the dis-
course) are likely to be instances of hypernymic def-
inite anaphora. We can then determine which nouns
earlier in the discourse (e.g. Lipitor) are likely an-
tecedents by unsupervised statistical co-occurrence
modeling aggregated over the entire corpus. All we
need is a large corpus without any anaphora annota-
tion and a basic tool for noun tagging and NP head
annotation. The detailed algorithm is as follows:
1. Find each sentence in the training corpus that
contains a definite NP (?the Y?) and does not
contain ?a Y?, ?an Y? or other instantiations of
Y2 appearing before the definite NP within a
fixed window.3
2. In the sentences that pass the above definite NP
and a/an test, regard all the head words (X) oc-
curring in the current sentence before the defi-
nite NP and the ones occurring in previous two
sentences as potential antecedents.
3. Count the frequency c(X,Y) for each pair ob-
tained in the above two steps and pre-store it in
a table.4 The frequency table can be modified
to give other scores for pair(X,Y) such as stan-
dard TF-IDF and Mutual Information scores.
4. Given a test sentence having an anaphoric def-
inite NP Y, consider the nouns appearing be-
fore Y within a fixed window as potential an-
tecedents. Rank the candidates by their pre-
computed co-occurence measures as computed
in Step 3.
Since we consider all head words preceding the defi-
nite NP as potential correct antecedents, the raw fre-
quency of the pair (X ,Y ) can be very noisy. This
can be seen clearly in Table 1, where the first col-
umn shows the top potential antecedents of definite
NP the drug as given by raw frequency. We nor-
malize the raw frequency using standard TF-IDF
2While matching for both ?the Y? and ?a/an Y?, we also ac-
count for Nouns getting modified by other words such as adjec-
tives. Thus ?the Y? will still match to ?the green and big Y?.
3Window size was set to two sentences, we also experi-
mented with a larger window size of five sentences and the re-
sults obtained were similar.
4Note that the count c(X,Y) is asymmetric
Rank Raw freq TF-IDF MI
1 today kilogram amphetamine
2 police heroin cannabis
3 kilogram police cocaine
4 year cocaine heroin
5 heroin today marijuana
6 dollar trafficker pill
7 country officer hashish
8 official amphetamine tablet
Table 1: A sample of ranked hyponyms proposed for
the definite NP The drug by TheY-Model illustrat-
ing the differences in weighting methods.
Acc Acctag Av Rank
MI 0.531 0.577 4.82
TF-IDF 0.175 0.190 6.63
Raw Freq 0.113 0.123 7.61
Table 2: Results using different normalization tech-
niques for the TheY-Model in isolation. (60 million
word corpus)
and Mutual Information scores to filter the noisy
pairs.5 In Table 2, we report our results for an-
tecedent selection using Raw frequency c(X,Y), TF-
IDF 6 and MI in isolation. Accuracy is the fraction
of total examples that were assigned the correct an-
tecedent and Accuracytag is the same excluding the
examples that had POS tagging errors for the cor-
rect antecedent.7 Av Rank is the rank of the true
antecedent averaged over the number of test exam-
ples.8 Based on the above experiment, the rest of
this paper assumesMutual Information scoring tech-
nique for TheY-Model.
5Note that MI(X,Y ) = log P (X,Y )P (X)P (Y ) and this is directly
proportional to P (Y |X) = c(X,Y )c(X) for a fixed Y . Thus, we
can simply use this conditional probability during implementa-
tion since the definite NP Y is fixed for the task of antecedent
selection.
6For the purposes of TF-IDF computation, document fre-
quency df(X) is defined as the number of unique definite NPs
for which X appears as an antecedent.
7Since the POS tagging was done automatically, it is possi-
ble for any model to miss the correct antecedent because it was
not tagged correctly as a noun in the first place. There were 14
such examples in the test set and none of the model variants can
find the correct antecdent in these instances.
8Knowing average rank can be useful when a n-best ranked
list from coreference task is used as an input to other down-
stream tasks such as information extraction.
39
Acc Acctag Av Rank
TheY+WN 0.695 0.755 3.37
WordNet 0.593 0.644 3.29
TheY 0.531 0.577 4.82
Table 3: Accuracy and Average Rank showing com-
bined model performance on the antecedent selec-
tion task. Corpus Size: 60 million words.
3.2 WordNet-Model (WN)
Because WordNet is considered as a standard re-
source of lexical knowledge and is often used in
coreference tasks, it is useful to know how well
corpus-based approaches perform as compared to
a standard model based on the WordNet (version
2.0).9 The algorithm for the WordNet-Model is as
follows:
Given a definite NP Y and its potential antecedent
X, choose X if it occurs as a hyponym (either direct
or indirect inheritance) of Y. If multiple potential an-
tecedents occur in the hierarchy of Y, choose the one
that is closest in the hierarchy.
3.3 Combination: TheY+WordNet Model
Most of the literature on using lexical resources
for definite anaphora has focused on using individ-
ual models (either corpus-based or manually build
resources such as WordNet) for antecedent selec-
tion. Some of the difficulties with using WordNet is
its limited coverage and its lack of empirical rank-
ing model. We propose a combination of TheY-
Model andWordNet-Model to overcome these prob-
lems. Essentially, we rerank the hypotheses found
in WordNet-Model based on ranks of TheY-model
or use a backoff scheme if WordNet-Model does not
return an answer due to its limited coverage. Given
a definite NP Y and a set of potential antecedents Xs
the detailed algorithm is specified as follows:
1. Rerank with TheY-Model: Rerank the potential
antecedents found in the WordNet-Model ta-
ble by assiging them the ranks given by TheY-
Model. If TheY-Model does not return a rank
for a potential antecedent, use the rank given by
9We also computed the accuracy using a weaker baseline,
namely, selecting the closest previous headword as the correct
antecedent. This recency based baseline obtained a low accu-
racy of 15% and hence we used the stronger WordNet based
model for comparison purposes.
the WordNet-Model. Now pick the top ranked
antecedent after reranking.
2. Backoff: If none of the potential antecedents
were found in the WordNet-Model then pick
the correct antecedent from the ranked list of
The-Y model. If none of the models return an
answer then assign ranks uniformly at random.
The above algorithm harnesses the strength of
WordNet-Model to identify good hyponyms and the
strength of TheY-model to identify which are more
likely to be used as an antecedent. Note that this
combination algorithm can be applied using any
corpus-based technique to account for poor-ranking
and low-coverage problems of WordNet and the
Sections 3.4, 3.5 and 3.6 will show the results for
backing off to a Hearst-style hypernym model. Ta-
ble 4 shows the decisions made by TheY-model,
WordNet-Model and the combined model for a sam-
ple of test examples. It is interesting to see how both
the models mutually complement each other in these
decisions. Table 3 shows the results for the models
presented so far using a 60 million word training text
from the Gigaword corpus. The combined model re-
sults in a substantially better accuracy than the indi-
vidual WordNet-Model and TheY-Model, indicating
its strong merit for the antecedent selection task.10
3.4 OtherY-Modelfreq
This model is a reimplementation of the corpus-
based algorithm proposed by Markert and Nissim
(2005) for the equivalent task of antecedent selec-
tion for definite NP coreference. We implement their
approach of using the lexico-syntactic pattern X and
A* other B* Y{pl} for extracting (X,Y) pairs.The A*
and B* allow for adjectives or other modifiers to be
placed in between the pattern. The model presented
in their article uses the raw frequency as the criteria
for selecting the antecedent.
3.5 OtherY-ModelMI (normalized)
We normalize the OtherY-Model using Mutual In-
formation scoring method. Although Markert and
Nissim (2005) report that using Mutual Information
performs similar to using raw frequency, Table 5
shows that using Mutual Information makes a sub-
stantial impact on results using large training cor-
pora relative to using raw frequency.
10The claim is statistically significant with a p < 0.01 ob-
tained by sign-test
40
Summary Keyword True TheY Truth WordNet Truth TheY+WN Truth
(Def. Ana) Antecedent Choice Rank Choice Rank Choice Rank
Both metal gold gold 1 gold 1 gold 1
correct sport soccer soccer 1 soccer 1 soccer 1
TheY-Model drug steroid steroid 1 NA NA steroid 1
helps drug azt azt 1 medication 2 azt 1
WN-Model instrument trumpet king 10 trumpet 1 trumpet 1
helps drug naltrexone alcohol 14 naltrexone 1 naltrexone 1
Both weapon bomb artillery 3 NA NA artillery 3
incorrect instrument voice music 9 NA NA music 9
Table 4: A sample of output from different models on antecedent selection (60 million word corpus).
3.6 Combination: TheY+OtherYMI Model
Our two corpus-based approaches (TheY and Oth-
erY) make use of different linguistic phenomena and
it would be interesting to see whether they are com-
plementary in nature. We used a similar combina-
tion algorithm as in Section 3.3 with the WordNet-
Model replaced with the OtherY-Model for hyper-
nym filtering, and we used the noisy TheY-Model
for reranking and backoff. The results for this ap-
proach are showed as the entry TheY+OtherYMI in
Table 5. We also implemented a combination (Oth-
erY+WN) of Other-Y model and WordNet-Model
by replacing TheY-Model with OtherY-Model in the
algorithm described in Section 3.3. The respective
results are indicated as OtherY+WN entry in Table
5.
4 Further Anaphora Resolution Results
Table 5 summarizes results obtained from all the
models defined in Section 3 on three different sizes
of training unlabeled corpora (from Gigaword cor-
pus). The models are listed from high accuracy to
low accuracy order. The OtherY-Model performs
particularly poorly on smaller data sizes, where cov-
erage of the Hearst-style patterns maybe limited,
as also observed by Berland and Charniak (1999).
We further find that the Markert and Nissim (2005)
OtherY-Model and our MI-based improvement do
show substantial relative performance growth at in-
creased corpus sizes, although they still underper-
form our basic TheY-Model at all tested corpus
sizes. Also, the combination of corpus-based mod-
els (TheY-Model+OtherY-model) does indeed per-
forms better than either of them in isolation. Fi-
nally, note that the basic TheY-algorithm still does
Acc Acctag Av Rank
60 million words
TheY+WN 0.695 0.755 3.37
OtherYMI+WN 0.633 0.687 3.04
WordNet 0.593 0.644 3.29
TheY 0.531 0.577 4.82
TheY+OtherYMI 0.497 0.540 4.96
OtherYMI 0.356 0.387 5.38
OtherYfreq 0.350 0.380 5.39
230 million words
TheY+WN 0.678 0.736 3.61
OtherYMI+WN 0.650 0.705 2.99
WordNet 0.593 0.644 3.29
TheY+OtherYMI 0.559 0.607 4.50
TheY 0.519 0.564 4.64
OtherYMI 0.503 0.546 4.37
OtherYfreq 0.418 0.454 4.52
380 million words
TheY+WN 0.695 0.755 3.47
OtherYMI+WN 0.644 0.699 3.03
WordNet 0.593 0.644 3.29
TheY+OtherYMI 0.554 0.601 4.20
TheY 0.537 0.583 4.26
OtherYMI 0.525 0.571 4.20
OtherYfreq 0.446 0.485 4.36
Table 5: Accuracy and Average Rank of Models de-
fined in Section 3 on the antecedent selection task.
41
relatively well by itself on smaller corpus sizes,
suggesting its merit on resource-limited languages
with smaller available online text collections and the
unavailability of WordNet. The combined models
of WordNet-Model with the two corpus-based ap-
proaches still significantly (p < 0.01) outperform
any of the other individual models.11
5 Generation Task
Having shown positive results for the task of an-
tecedent selection, we turn to a more difficult task,
namely generating an anaphoric definite NP given
a nominal antecedent. In Example (1), this would
correspond to generating ?the drug? as an anaphor
knowing that the antecedent is pseudoephedrine.
This task clearly has many applications: current gen-
eration systems often limit their anaphoric usage to
pronouns and thus an automatic system that does
well on hypernymic definite NP generation can di-
rectly be helpful. It also has strong potential appli-
cation in abstractive summarization where rewriting
a fluent passage requires a good model of anaphoric
usage.
There are many interesting challenges in this prob-
lem: first of all, there maybe be multiple acceptable
choices for definite anaphor given a particular an-
tecedent, complicating automatic evaluation. Sec-
ond, when a system generates a definite anaphora,
the space of potential candidates is essentially un-
bounded, unlike in antecdent selection, where it is
limited only to the number of potential antecedents
in prior context. In spite of the complex nature
of this problem, our experiments with the human
judgements, WordNet and corpus-based approaches
show a simple feasible solution. We evaluate our
automatic approaches based on exact-match agree-
ment with definite anaphora actually used in the cor-
pus (accuracy) and also by agreement with definite
anaphora predicted independently by a human judge
in an absence of context.
11Note that syntactic co-reference candidate filters such as
the Hobbs algorithm were not utilized in this study. To assess
the performance implications, the Hobbs algorithm was applied
to a randomly selected 100-instance subset of the test data. Al-
though the Hobbs algorithm frequently pruned at least one of
the coreference candidates, in only 2% of the data did such can-
didate filtering change system output. However, since both of
these changes were improvements, it could be worthwhile to
utilize Hobbs filtering in future work, although the gains would
likely be modest.
5.1 Human experiment
We extracted a total of 103 <true antecedent, defi-
nite NP> pairs from the set of test instances used in
the resolution task. Then we asked a human judge (a
native speaker of English) to predict a parent class
of the antecedent that could act as a good definite
anaphora choice in general, independent of a par-
ticular context. Thus, the actual corpus sentence
containing the antecedent and definite NP and its
context was not provided to the judge. We took
the predictions provided by the judge and matched
them with the actual definite NPs used in the corpus.
The agreement between corpus and the human judge
was 79% which can thus be considered as an upper
bound of algorithm performance. Table 7 shows a
sample of decisions made by the human and how
they agree with the definite NPs observed in the cor-
pus. It is interesting to note the challenge of the
sense variation and figurative usage. For example,
?corruption? is refered to as a ?tool? in the actual
corpus anaphora, a metaphoric usage that would be
difficult to predict unless given the usage sentence
and its context. However, a human agreement of
79% indicate that such instances are relatively rare
and the task of predicting a definite anaphor with-
out its context is viable. In general, it appears from
our experiements that humans tend to select from
a relatively small set of parent classes when gener-
ating hypernymic definite anaphora. Furthermore,
there appears to be a relatively context-independent
concept of the ?natural? level in the hypernym hi-
erarchy for generating anaphors. For example, al-
though <?alkaloid?, ?organic compound?, ?com-
pound?, ?substance?, ?entity?> are all hypernyms
of ?Pseudoephederine? in WordNet, ?the drug?
appears to be the preferred hypernym for definite
anaphora in the data, with the other alternatives be-
ing either too specific or too general to be natural.
This natural level appears to be difficult to define by
rule. For example, using just the immediate parent
hypernym in the WordNet hierarchy only achieves
4% match with the corpus data for definite anaphor
generation.
5.2 Algorithms
The following sections presents our corpus-based al-
gorithms as more effective alternatives.
42
Agreement Agreement
w/ human w/ corpus
judge
TheY+OtherY+WN 47% 46%
OtherY +WN 43% 43%
TheY+WN 42% 37%
TheY +OtherY 39% 36%
OtherY 39% 36%
WordNet 4% 4%
Human judge 100% 79%
Corpus 79% 100%
Table 6: Agreement of different generation models
with human judge and with definite NP used in the
corpus.
5.2.1 Individual Models
For the corpus-based approaches, the TheY-Model
and OtherY-Model were trained in the same manner
as for the antecedent selection task. The only differ-
ence was that in the generation case, the frequency
statistics were reversed to provide a hypernym given
a hyponym. Additionally, we found that raw fre-
quency outperformed either TF-IDF or Mutual In-
formation and was used for all results in Table 6.
The stand-alone WordNet model is also very simple:
Given an antecedent, we lookup its direct hypernym
(using first sense) in the WordNet and use it as the
definite NP, for lack of a better rule for preferred hy-
pernym location.
5.2.2 Combining corpus-based approaches and
WordNet
Each of the corpus-based approaches was combined
with WordNet resulting in two different models as
follows: Given an antecedent X, the corpus-based
approach looks up in its table the hypernym of X,
for example Y, and only produces Y as the output if
Y also occurs in the WordNet as hypernym. Thus
WordNet is used as a filtering tool for detecting vi-
able hypernyms. This combination resulted in two
models: ?TheY+WN? and ?OtherY+WN?.
We also combined all the three approaches, ?TheY?,
?OtherY? and WordNet resulting in a single model
?TheY+OtherY+WN?. This was done as follows: We
first combine the models ?TheY? and ?OtherY? using
a backoff model. The first priority is to use the hy-
Antecedent Corpus Human TheY+OtherY
Def Ana Choice +WN
racing sport sport sport
azt drug drug drug
missile weapon weapon weapon
alligator animal animal animal
steel metal metal metal
osteporosis disease disease condition
grenade device weapon device
baikonur site city station
corruption tool crime activity
Table 7: Sample of decisions made by hu-
man judge and our best performing model
(TheY+OtherY+WN) on the generation task.
pernym from the model ?OtherY?, if not found then
use the hypernym from the model ?TheY?. Given a
definite NP from the backoff model, apply theWord-
Net filtering technique, specifically, choose it as the
correct definite NP if it also occurs as a hypernym in
the WordNet hierarchy of the antecedent.
5.3 Evaluation of Anaphor Generation
We evaluated the resulting algorithms from Section
5.2 on the definite NP prediction task as described
earlier. Table 6 shows the agreement of the algo-
rithm predictions with the human judge as well as
with the definite NP actually observed in the corpus.
It is interesting to see that WordNet by itself per-
forms very poorly on this task since it does not have
any word-specific mechanism to choose the correct
level in the hierarchy and the correct word sense for
selecting the hypernym. However, when combined
with our corpus-based approaches, the agreement
increases substantially indicating that the corpus-
based approaches are effectively filtering the space
of hypernyms that can be used as natural classes.
Likewise, WordNet helps to filter the noisy hyper-
nyms from the corpus predictions. Thus, this inter-
play between the corpus-based and WordNet alo-
rithm works out nicely, resulting in the best model
being a combination of all three individual models
and achieving a substantially better agreement with
both the corpus and human judge than any of the in-
dividual models. Table 7 shows decisions made by
this algorithm on a sample test data.
43
6 Conclusion
This paper provides a successful solution to the
problem of incomplete lexical resources for definite
anaphora resolution and further demonstrates how
the resources built for resolution can be naturally ex-
tended for the less studied task of anaphora genera-
tion. We first presented a simple and noisy corpus-
based approach based on globally modeling head-
word co-occurrence around likely anaphoric definite
NPs. This was shown to outperform a recent ap-
proach byMarkert and Nissim (2005) that makes use
of standard Hearst-style patterns extracting hyper-
nyms for the same task. Even with a relatively small
training corpora, our simple TheY-model was able
to achieve relatively high accuracy, making it suit-
able for resource-limited languages where annotated
training corpora and full WordNets are likely not
available. We then evaluated several variants of this
algorithm based on model combination techniques.
The best combined model was shown to exceed 75%
accuracy on the resolution task, beating any of the
individual models. On the much harder anaphora
generation task, where the stand-alone WordNet-
based model only achieved an accuracy of 4%, we
showed that our algorithms can achieve 35%-47%
accuracy on blind exact-match evaluation, thus mo-
tivating the use of such corpus-based learning ap-
proaches on the generation task as well.
Acknowledgements
Thanks to Charles Schafer for sharing his tools on
POS/Headword tagging for the Gigaword corpus.
References
M. Berland and E. Charniak. 1999. Finding parts in
very large corpora. In Proceedings of the 37th Annual
Meeting of the Association for Computational Linguis-
tics, pages 57?64.
S. Caraballo. 1999. Automatic construction of a
hypernym-labeled noun hierarchy from text. In Pro-
ceedings of the 37th Annual Meeting of the Association
for Computational Linguistics, pages 120?126.
H. H. Clark. 1975. Bridging. In Proceedings of the
Conference on Theoretical Issues in Natural Language
Processing, pages 169?174.
D. Connoly, J. D. Burger, and D. S. Day. 1997. A ma-
chine learning approach to anaphoric reference. In
Proceedings of the International Conference on New
Methods in Language Processing, pages 133?144.
D. Graff, J. Kong, K. Chen, and K. Maeda. 2005. En-
glish Gigaword Second Edition. Linguistic Data Con-
sortium, catalog number LDC2005T12.
S. Harabagiu, R. Bunescu, and S. J. Maiorano. 2001.
Text and knowledge mining for coreference resolu-
tion. In Proceedings of the Second Meeting of the
North American Chapter of the Association for Com-
putational Linguistics, pages 55?62.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the 14th In-
ternational Conference on Computational Linguistics,
pages 539?545.
J. Hobbs. 1978. Resolving pronoun references. Lingua,
44:311?338.
K. Markert and M. Nissim. 2005. Comparing knowl-
edge sources for nominal anaphora resolution. Com-
putational Linguistics, 31(3):367?402.
K. Markert, M. Nissim, and N. N. Modjeska. 2003. Us-
ing the web for nominal anaphora resolution. In Pro-
ceedings of the EACL Workshop on the Computational
Treatment of Anaphora, pages 39?46.
J. Meyer and R. Dale. 2002. Mining a corpus to sup-
port associative anaphora resolution. In Proceedings
of the Fourth International Conference on Discourse
Anaphora and Anaphor Resolution.
V. Ng and C. Cardie. 2002. Improving machine learn-
ing approaches to coreference resolution. In Proceed-
ings of the 40th Annual Meeting of the Association for
Computational Linguistics, pages 104?111.
M. Poesio, R. Vieira, and S. Teufel. 1997. Resolving
bridging references in unrestricted text. In Proceed-
ings of the ACL Workshop on Operational Factors in
Robust Anaphora, pages 1?6.
M. Poesio, T. Ishikawa, S. Schulte im Walde, and
R. Viera. 2002. Acquiring lexical knowledge for
anaphora resolution. In Proccedings of the Third Con-
ference on Language Resources and Evaluation, pages
1220?1224.
M. Poesio, R. Mehta, A. Maroudas, and J. Hitzeman.
2004. Learning to resolve bridging references. In Pro-
ceedings of the 42nd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 143?150.
W. M. Soon, H. T. Ng, and D. C. Y. Lim. 2001. A ma-
chine learning approach to coreference resolution of
noun phrases. Computational Linguistics, 27(4):521?
544.
M. Strube, S. Rapp, and C. Mu?ller. 2002. The influ-
ence of minimum edit distance on reference resolution.
In Proceedings of the 2002 Conference on Empirical
Methods in Natural Language Processing, pages 312?
319.
R. Vieira and M. Poesio. 2000. An empirically-based
system for processing definite descriptions. Computa-
tional Linguistics, 26(4):539?593.
X. Yang, G. Zhou, J. Su, and C. L. Tan. 2003. Corefer-
ence resolution using competition learning approach.
In Proceedings of the 41st Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 176?183.
44
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 199?202,
Prague, June 2007. c?2007 Association for Computational Linguistics
JHU1 : An Unsupervised Approach to Person Name Disambiguation
using Web Snippets
Delip Rao Nikesh Garera David Yarowsky
Dept. of Computer Science
Johns Hopkins University
Baltimore, MD 21218
{delip, ngarera, yarowsky}@cs.jhu.edu
Abstract
This paper presents an approach to person
name disambiguation using K-means clus-
tering on rich-feature-enhanced document
vectors, augmented with additional web-
extracted snippets surrounding the polyse-
mous names to facilitate term bridging. This
yields a significant F-measure improvement
on the shared task training data set. The pa-
per also illustrates the significant divergence
between the properties of the training and
test data in this shared task, substantially
skewing results. Our system optimized on
F0.2 rather than F0.5 would have achieved
top performance in the shared task.
1 Introduction
Being able to automatically distinguish between
John Doe, the musician, and John Doe, the actor, on
the Web is a task of significant importance with ap-
plications in IR and other information management
tasks. Mann and Yarowsky (2004) used bigograph-
ical data annotated with named entitities and per-
form fusion of extracted information across multiple
documents. Bekkerman and McCallum (2005) stud-
ied the problem in a social network setting exploit-
ing link topology to disambiguate namesakes. Al-
Kamha and Embley (2004) used a combination of
attributes (like zipcodes, state, etc.), links, and page
similarity to derive the name clusters while Wan et.
al. (2005) used lexical features and named entities.
2 Approaches
Our framework focuses on the K-means clustering
model using both bag of words as features and vari-
ous augumented feature sets. We experimented with
several similarity functions and chose Pearson?s cor-
relation coefficient1 as the distance measure for clus-
tering. The weights for the features were set to the
term frequency of their respective words in the doc-
ument.2
2.1 Submitted system: Clustering using Web
Snippets
We queried the Google search engine with the
target person names and extracted up to the top
one thousand results. For each result we also
extracted the snippet associated with it. An example
is shown below in Figure 2.1. As can be seen the
Figure 1: Google snippet for ?Dekang Lin?
snippets contain high quality, low noise features that
could be used to improve the performance of the
system. Each snippet was treated as a document and
1This performs better than the standard measures like Eu-
clidean and Cosine with K-means clustering on this data.
2We found that using TF weights instead of TF-IDF weights
gives a better performance on this task.
199
clustered along with the supplied documents. This
process is illustrated in Figure 2. The following
example illustrates how these web snippets can
improve performance by lexical transitivity. In
this hypothetical example, a short test document
contains a Canadian postal code (T6G 2H1) not
found in any of the training documents. However,
there may exist an additional web page not in the
training or test data which contains both this term
and also overlap with other terms in the training data
(e.g. 492-9920), serving as an effective transitive
bridge between the two.
Training Document 1 492-9920, not(T6G 2H1)
Web Snippet 2 both 492-9920, T6G 2H1
Test Document 3 T6G 2H1, not(492-9920)
Thus K-means clustering is likely to cluster the
three documents above together while without this
transitive bridge the association between training
and test documents is much less strong. The final
clustering of the test data is simply a projection with
the training documents and web snippets removed.
Projection of test documents
Initial clusters of web snippets + test documents
Web snippet document
Test document
Figure 2: Clustering using Web Snippets
2.2 Baselines
In this section we describe several trivial baselines:
1. Singletons: A clustering where each cluster
has only one document hence number of clus-
ters is same as the number of documents.
2. One Cluster: A clustering with only one clus-
ter containing all documents.
3. Random: A clustering scheme which parti-
tions the documents uniformly at random into
K clusters, where the value of K were the op-
timal K on the training and test data.
These results are summarized in Table 1. Note that
all average F-scores mentioned in this table and the
rest of the paper are microaverages obtained by av-
eraging the purity and invese purity over all names
and then calculating the F-score.
Train Test
Baseline F0.2 F0.5 F0.2 F0.5
Singletons .676 .511 .843 .730
One Cluster .688 .638 .378 .327
Random .556 .493 .801 .668
Table 1: Baseline performance
2.3 K-means on Bag of Words model
The standard unaugumented Bag of Words model
achieves F0.5 of 0.666 on training data, as shown
in Table 2.
2.4 Part of speech tag features
We then consider only terms that are nouns (NN,
NNP) and adjectives (JJ) with the intuition that
most of the content bearing words and descriptive
words that disambiguate a person would fall in these
classes. The result then improves to 0.67 on the
training data.
2.5 Rich features
Another variant of this system, that we call Rich-
Feats, gives preferential weighting to terms that are
immediately around all variants of the person name
in question, place names, occupation names, and
titles. For marking up place names, occupation
names, and titles we used gazetteer3 lookup with-
out explicit named entity disambiguation. The key-
words that appeared in the HTML tag <META ..>
were also given higher weights. This resulted in an
F0.5 of 0.664.
2.6 Snippets from the Web
The addition of web snippets as described in Sec-
tion 2.1 yeilds a significant F0.5 improvement to
0.72.
3Totalling 19646 terms, gathered from publicly available re-
sources on the web. Further details are available on request.
200
2.7 Snippets and Rich features
This is a combination of the models mentioned in
Sections 2.5 and 2.6. This model combination re-
sulted in a slight degradation of performance over
snippets by themselves on the training data but a
slight improvement on test data.
Model K F0.2 F0.5
Vanilla BOW 10% 0.702 0.666
BOW + PoS 10% 0.706 0.670
BOW + RichFeats 10% 0.700 0.664
Snippets 10 0.721 0.718
Snippets + RichFeats 10 0.714 0.712
Table 2: Performance on Training Data
3 Selection of Parameters
The main parameter for K-means clustering is
choosing the number of clusters, K. We optimized
K over the training data varying K from 10%,
20%,? ? ?,100% of the number of documents as well
as varying absolute K values from 10, 20, ? ? ? to 100
documents.4 The evaluation score of F-measure can
be highly sensitive to this parameter K, as shown
in Table 3. The value of K that gives the best F-
measure on training set using vanilla bag of words
(BOW) model is K = 10%, however we see in Ta-
ble 3 that this value of K actually performs much
worse on the test data as compared to other K val-
ues.
4 Training/Test discrepancy and
re-evaluation using cross validation on
test data
Table 4 compares cluster statistics between the train-
ing and test data. This data was derived from Artiles
et. al (2007). The large difference between aver-
age number of clusters in training and test sets in-
dicates that the parameter K, optimized on training
set cannot be transferred to test set as these two sets
belong to a very different distribution. This can be
emprically seen in Table 3 where applying the best
K on training results in a significant performance
4We discard the training and test documents that have no text
content, thus the absolute value K = 10 and percentage value K
= 10% can result in different K?s, even if name had originally
100 documents to begin with.
drop on test set given this divergence when param-
eters are optimized for F0.5 (although performance
does transfer well when parameters are optimized on
F0.2). This was observed in our primary evaluation
system which was optimized for F0.5 and resulted in
a low official score of F0.5 = .53 and F0.2 = .65.
Train Test
K F0.2 F0.5 F0.2 F0.5
10% .702 .666 .527 .600
20% .716 .644 .617 .630
30% .724 .631 .683 .676
40% .724 .618 .728 .705
50% .732 .614 .762 .724
60% .731 .601 .798 .747
70% .730 .593 .832 .766
80% .732 .586 .855 .773
90% .714 .558 .861 .764
100% .670 .502 .843 .730
Table 3: Selecting the optimal parameter on training
data and application to test data
Thus an interesting question is to measure per-
formance when parameters are chosen on data shar-
ing the distributional character of the test data rather
than the highly divergent training set. To do this, we
used a standard 2-fold cross validation to estimate
clustering parameters from a held-out, alternate-half
portion of the test data5, which more fairly repre-
sents the character of the other half of the test data
than does the very different training data. We di-
vide the test set into two equal halves (taking first
fifteen names alphabetically in one set and the rest
in another). We optimize K on the first half, test
on the other half and vice versa. We report the two
K-values and their corresponding F-measures in Ta-
ble 5 and we also report the average in order to com-
pare it with the results on the test set obtained using
K optimized on training. Further, we also report
what would be oracle best K, that is, if we optimize
K on the entire test data 6. We can see in Table 5
that how optimizing K on a devlopment set with
5This also prevents overfitting as the two halves for training
and testing are disjoint.
6By oracle best K we mean the K obtained by optimizing
over the entire test data. Note that, the oracle best K is just
for comparison because it would be unfair to claim results by
optimizing K on the entire test set, all our claimed results for
different models are based on 2-fold cross validation.
201
same distribution as test set can give us F-measure
in the range of 77%, a significant increase as com-
pared to the F-measure obtained by optimizing K on
given training data. Further, Table 5, also indicates
results by a custom clustering method, that takes the
best K-means clustering using vanilla bag of words
model, retains the largest cluster and splits all the
other clusters into singleton clusters. This method
gives an improved 2-fold F-measure score over the
simple bag of words model, implying that most of
the namesakes in test data have one (or few) domi-
nant cluster and a lot of singleton clusters. Table 6
shows a full enumeration of model variance under
this cross validated test evaluation. POS and Rich-
Feats yield small gains, and a best F0.5 performance
of .776.
Data set cluster size # of clusters
Mean Variance Mean Variance
Train 5.4 144.0 10.8 146.3
Test 3.1 26.5 45.9 574.1
Table 4: Cluster statistics from the test and training
data
Data set K F0.2 F0.5
F0.5 Best K on train 10% .702 .666
F0.2 Best K on train 10 .707 .663
Best K on train 10% .527 .560
applied to test 10 .540 .571
2Fold on Test 80 .847 .748
80% .862 .793
.854* .771*
2Fold on Single 80 .847 .749
Largest Cluster 80 .866 .795
.856* .772*
Oracle on Test 80 .858 .774
Table 5: Comparision of training and test results us-
ing Vanilla Bag-of-words model. The values indi-
cated with * represent the average value.
5 Conclusion
We presented a K-means clustering approach for the
task of person name disambiguation using several
augmented feature sets including HTML meta fea-
tures, part-of-speech-filtered features, and inclusion
of additional web snippets extracted from Google
to facilitate term bridging. The latter showed sig-
nificant empirical gains on the training data. Best
Model K F0.2 F0.5
Vanilla BOW 80/ .847/.862 .749/.793
80% Avg = .854 Avg = .771
BOW + PoS 80%/ .844/.865 .749/.795
80% Avg = .854 Avg = .772
BOW 80%/ .847/.868 .754/.798
RichFeats 80% Avg = .858 Avg = .776
Snippets 50%/ .842/.875 .746/.800
50% Avg = .859 Avg = .773
Snippets + 40%/ .836/.874 .750/.798
RichFeats 50% Avg = .855 Avg = .774
Table 6: Performance on 2Fold Test Data
performance on test data, when parameters are op-
timized for F0.2 on training (Table 3), yielded a top
performing F0.2 of .855 on test data (and F0.5=.773
on test data). We also explored the striking discrep-
ancy between training and test data characteristics
and showed how optimizing the clustering param-
eters on given training data does not transfer well
to the divergent test data. To control for similar
training and test distributional characteristics, we re-
evaluated our test results estimating clustering pa-
rameters from alternate held-out portions of the test
set. Our models achieved cross validated F0.5 of .77-
.78 on test data for all feature combinations, further
showing the broad strong performance of these tech-
niques.
References
Reema Al-Kamha and David W. Embley. 2004. Grouping
search-engine returned citations for person-name queries. In
Proceedings of the 6th annual ACM international workshop
on Web information and data management, pages 96?103.
Javier Artiles, Julio Gonzalo, and Felisa Verdejo. 2007. Eval-
uation: Establishing a benchmark for the web people search
task. In Proceedings of Semeval 2007, Association for Com-
putational Linguistics.
Ron Bekkerman and Andrew McCallum. 2005. Disambiguat-
ing web appearances of people in a social network. In Pro-
ceedings of the 14th international conference on World Wide
Web, pages 463?470.
Gideon S. Mann and David Yarowsky. 2004. Unsupervised
personal name disambiguation. In Proceedings of the sev-
enth conference on Natural language learning (CONLL),
pages 33?40.
Xiaojun Wan, Jianfeng Gao, Mu Li, and Binggong Ding. 2005.
Person resolution in person search results: Webhawk. In
Proceedings of the 14th ACM international conference on
Information and knowledge management, pages 163?170.
202
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 129?137,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Improving Translation Lexicon Induction from Monolingual Corpora via
Dependency Contexts and Part-of-Speech Equivalences
Nikesh Garera, Chris Callison-Burch, David Yarowsky
Department of Computer Science, Johns Hopkins University
Baltimore MD, USA
{ngarera,ccb,yarowsky}@cs.jhu.edu
Abstract
This paper presents novel improvements
to the induction of translation lexicons
from monolingual corpora using multilin-
gual dependency parses. We introduce a
dependency-based context model that in-
corporates long-range dependencies, vari-
able context sizes, and reordering. It pro-
vides a 16% relative improvement over
the baseline approach that uses a fixed
context window of adjacent words. Its
Top 10 accuracy for noun translation is
higher than that of a statistical translation
model trained on a Spanish-English par-
allel corpus containing 100,000 sentence
pairs. We generalize the evaluation to
other word-types, and show that the per-
formance can be increased to 18% rela-
tive by preserving part-of-speech equiva-
lencies during translation.
1 Introduction
Recent trends in machine translation illustrate that
highly accurate word and phrase translations can be
learned automatically given enough parallel training
data (Koehn et al, 2003; Chiang, 2007). However,
large parallel corpora exist for only a small frac-
tion of the world?s languages, leading to a bottleneck
for building translation systems in low-density lan-
guages such as Swahili, Uzbek or Punjabi. While
parallel training data is uncommon for such lan-
guages, more readily available resources include
small translation dictionaries, comparable corpora,
and large amounts of monolingual data.
The marked difference in the availability of
monolingual vs parallel corpora has led several
researchers to develop methods for automatically
learning bilingual lexicons, either by using mono-
lingual corpora (Rapp, 1999; Koehn and Knight,
2002; Schafer and Yarowsky, 2002; Haghighi et al,
2008) or by exploiting the cross-language evidence
of closely related ?bridge? languages that have more
resources (Mann and Yarowsky, 2001).
This paper investigates new ways of learning
translations from monolingual corpora. We extend
the Rapp (1999) model of context vector projection
using a seed lexicon. It is based on the intuition that
translations will have similar lexical context, even in
unrelated corpora. For example, in order to translate
the word ?airplane?, the algorithm builds a context
vector which might contain terms such as ?passen-
gers?, ?runway?, ?airport?, etc. and words in tar-
get language that have their translations (obtained
via seed lexicon) in surrounding context can be con-
sidered as likely translations. We extend the basic
approach by formulating a context model that uses
dependency trees. The use of dependencies has the
following advantages:
? Long distance dependencies allow associated
words to be included in the context vector even
if they fall outside of the fixed-window used in
the baseline model.
? Using relationships like parent and child in-
stead of absolute positions alleviates problems
when projecting vectors between languages
with different word orders.
? It achieves better performance than baseline
context models across the board, and better
performance than statistical translation models
on Top-10 accuracy for noun translation when
trained on identical data.
129
We further show that an extension based on part-
of-speech clustering can give similar accuracy gains
for learning translations of all word-types, deepen-
ing the findings of previous literature which mainly
focused on translating nouns (Rapp, 1999; Koehn
and Knight, 2002; Haghighi et al, 2008).
2 Related Work
The literature on translation lexicon induction for
low-density languages falls in to two broad cate-
gories: 1) Effectively utilizing similarity between
languages by choosing a high-resource ?bridge? lan-
guage for translation (Mann and Yarowsky, 2001;
Schafer and Yarowsky, 2002) and 2) Extracting
noisy clues (such as similar context) from mono-
lingual corpora with help of a seed lexicon (Rapp,
1999; Koehn and Knight, 2002; Schafer and
Yarowsky, 2002, Haghighi et al, 2008). The lat-
ter category is more relevant to this work and is ex-
plained in detail below.
The idea of words with similar meaning having
similar contexts in the same language comes from
the Distributional Hypothesis (Harris, 1985) and
Rapp (1999) was the first to propose using context of
a given word as a clue to its translation. Given a Ger-
man word with an unknown translation, a German
context vector is constructed by counting its sur-
rounding words in a monolingual German corpus.
Using an incomplete bilingual dictionary, the counts
of the German context words with known transla-
tions are projected onto an English vector. The pro-
jected vector for the German word is compared to
the vectors constructed for all English words using
a monolingual English corpus. The English words
with the highest vector similarity are treated as trans-
lation candidates. The original work employed a rel-
atively large bilingual dictionary containing approx-
imately 16,000 words and tested only on a small col-
lection of 100 manually selected nouns.
Koehn and Knight (2002) tested this idea on a
larger test set consisting of the 1000 most frequent
words from a German-English lexicon. They also
incorporated clues such as frequency and ortho-
graphic similarity in addition to context. Schafer
and Yarowsky, (2002) independently proposed us-
ing frequency, orthographic similarity and also
showed improvements using temporal and word-
burstiness similarity measures, in addition to con-
text. Haghighi et al, (2008) made use of contex-
tual and orthographic clues for learning a generative
model from monolingual corpora and a seed lexicon.
All of the aforementioned work defines context
similarity in terms of the adjacent words over a win-
dow of some arbitary size (usually 2 to 4 words), as
initially proposed by Rapp (1999). We show that the
model for surrounding context can be improved by
using dependency information rather than strictly re-
lying on adjacent words, based on the success of de-
pendency trees for monolingual clustering and dis-
ambiguation tasks (Lin and Pantel, 2002; Pado and
Lapata, 2007) and the recent developments in multi-
lingual dependency parsing literature (Buchholz and
Marsi, 2006; Nivre et al, 2007).
We further differentiate ourselves from previous
work by conducting a second evaluation which ex-
amines the accuracy of translating all word types,
rather than just nouns. While the straightforward ap-
plication of context-based model gives a lower over-
all accuracy than nouns alone, we show how learn-
ing a mapping of part-of-speech tagsets between the
source and target language can result in comparable
performance to that of noun translation.
3 Translation by Context Vector
Projection
This section details how translations are discovered
from monolingual corpora through context vector
projection. Section 3.1 defines alternative ways of
modeling context vectors, and including baseline
models and our dependency-based model.
The central idea of Rapp?s method for learning
translations is that of context vector projection and
vector similarity. The goodness of semantic ?fit? of
candidate translations is measured as the vector sim-
ilarity between two words. Those vectors are drawn
from two different languages, so the vector for one
word must first be projected onto the language space
of the other. The algorithm for creating, projecting
and comparing vectors is described below, and illus-
trated in Figure 1.
Algorithm:
1. Extract context vectors:
Given a word in source language, say sw, create
a vector using the surrounding context words
and call this reference source vector rssw for
130
Figure 1: Illustration of (Rapp, 1999) model for translating spanish word ?crecimiento (growth)? via dependency context vectors
extracted from respective monolingual corpora as explained in Section 3.1.2
source word sw. The actual composition of this
vector varies depending on how the surround-
ing context is modeled. The context model is
independent of the algorithm, and various mod-
els are explained in later sections.
2. Project reference source vector:
Project all the source vector words contained in
the projection dictionary onto the vector space
for the target language, retaining the counts
from source corpus. This vector now exists in
the target language space and is called the ref-
erence target vector rtsw . This vector may be
sparse, depending on how complete the bilin-
gual dictionary is, because words without dic-
tionary entires will receive zero counts in the
reference target vector.
3. Rank candidates by vector similarity:
For each word twi in the target language a con-
text vector is created using the target language
monolingual corpora as in Step 1. Compute a
similarity score between the context vector of
twi = ?ci1, ci2, ...., cin? and reference target vec-
tor rtsw = ?r1, r2, ...., rn?. The word with the
maximum similarity score t?wi is chosen as thecandidate translation of sw.
The vector similarity can be computed in a
number of ways. Our setup we used cosine
similarity:
t?wi = argmaxtwi ci1?r1+ci2?r2+....+cin?rn?c2i1+c2i2+...+c2in?r21+r22+...+r2n
Rapp (1999) used l1-norm metric after nor-
malizing the vectors to unit length, Koehn and
Knight (2002) used Spearman rank order cor-
relation, and Schafer and Yarowsky (2002) use
cosine similarity. We found that cosine simi-
larity gave the best results in our experimental
conditions. Other similarity measures may be
used equally well.
3.1 Models of Context
We compared several context models. Empirical re-
sults for their ability to find accurate translations are
given in Section 5.
3.1.1 Baseline model
In the baseline model, the context is computed
using adjacent words as in (Rapp,1999; Koehn
and Knight, 2002; Schafer and Yarowsky, 2002;
Haghighi et al, 2008). Given a word in source lan-
guage, say sw, count all its immediate context words
appearing in a window of four words. The counts
are collected seperately for each position by keeping
track of four seperate vectors for positions -2, -1, +1
and +2. Thus each vector is a sparse vector, having
the # of dimensions as the size of source language
vocabulary. Each dimension is also reweighted by
multiplying the inverse document frequency (IDF)
131
Figure 2: Illustration of using dependency trees to model richer contexts for projection
as in the standard TF.IDF weighting scheme1. These
vectors are then concatenated into a single vector,
having dimension four times the size of the vocabu-
lary. This vector is called the reference source vector
rssw for source word sw.
3.1.2 Modeling context using dependency trees
We use dependency parsing to extend the con-
text model. Our context vectors use contexts derived
from head-words linked by dependency trees instead
of using the immediate adjacent lexical words. The
use of dependency trees for modeling contexts has
been shown to help in monolingual clustering tasks
of finding words with similar meaning (Lin and Pan-
tel, 2002) and we show how they can be effectively
used for translation lexicon induction.
Position Adjacent Dependency
Context Context
-2 para camino
-1 el para
+1 y prosperidad, y, el
+2 la econo?mica
Table 1: Contrasting context words derived from the adjacent
vs dependency models for the above example
The four vectors for positions -1, +1, -2 and +2
in the baseline model get mapped to immediate par-
ent (-1), immediate child (+1), grandparent (-2) and
grandchild (+2). An example of using the depen-
dency tree context is shown in Figure 2, and the de-
pendency context is shown in contrast with the ad-
jacent context in Table 1, showing the selection of
more salient words by using the dependencies.
Note that while we are limiting to four positions
in the tree, it does not imply that only a maximum of
four context words are selected since the word can
have multiple immediate children depending upon
the dependency parse of the sentence. Hence, this
approach allows for a dynamic context size, with the
1In order to compute the IDF, while there were no clear doc-
ument boundaries in our corpus, a virtual document boundary
was created by binning after every 1000 words.
number of context words varying with the number of
children and parents at the two levels.
Another advantage of this method is that it al-
leviates the reordering problem as we use tree po-
sitions (consisting of head-words) as compared to
the adjacent position in the baseline context model.
For example, if the source spanish word to be trans-
lated was ?prosperidad?, then in the example shown
in Figure 2, in case of adjacent context, the con-
text word ?econo?mica? will show up in +1 position
in Spanish and -1 position in English (as adjectives
come before nouns in English) but in case of depen-
dency context, the adjective will be the child of noun
and hence will show up in +1 position in both lan-
guages. Thus, we do not need to use a bag of word
model as in Section 3 in order to avoid learning the
explicit mapping that adjectives and nouns in Span-
ish and English are reversed.
4 Experimental Design
For our initial set of experiments we compared sev-
eral different vector-based context models:
? Adjbow ? A baseline model which used bag of
words model with a fixed window of 4 words,
two on either side of the word to be translated.
? Adjposn ? A second baseline that used a fixed
window of 4 words but which took positional
into account.
? Depbow ? A dependency model which did not
distinguish between grandparent, parent, child
and grandparent relations, analogous to the bag
of words model.
? Depposn ? A dependency model which did in-
clude such relationships, and was analogous to
the position-based baseline.
? Depposn + rev ? The above Depposn model ap-
plied in both directions (Spanish-to-English
and English-to-Spanish) using their sum as the
final translation score.
We contrasted the accuracy of the above methods,
which use monolingual corpora, with a statistical
132
model trained on bilingual parallel corpora. We re-
fer to that model as Mosesen-es-100k, because it was
trained using the Moses toolkit (Koehn et al, 2007).
4.1 Training Data
All context models were trained on a Spanish cor-
pus containing 100,000 sentences with 2.13 million
words and an English corpus containing 100,000
sentences with 2.07 million words. The Spanish cor-
pus was parsed using the MST dependency parser
(McDonald et al, 2005) trained using dependency
trees generated from the the English Penn Treebank
(Marcus et al, 1993) and Spanish CoNLL-X data
(Buchholz and Marsi, 2006).
So that we could directly compare against sta-
tistical translation models, our Spanish and English
monolingual corpora were drawn from the Europarl
parallel corpus (Koehn, 2005). The fact that our
two monolingual corpora are taken from a parallel
corpus ensures that the assumption that similar con-
texts are a good indicator of translation holds. This
assumption underlies in all work of translation lex-
icon induction from comparable monolingual cor-
pora, and here we strongly bias toward that assump-
tion. Despite the bias, the comparison of different
context models holds, since all models are trained
on the same data.
4.2 Evaluation Criterion
The models were evaluated in terms of exact-match
translation accuracy of the 1000 most frequent
nouns in a English-Spanish dictionary. The accuracy
was calculated by counting how many mappings ex-
actly match one of the entries in the dictionary. This
evaluation criterion is similar to the setup used by
Koehn and Knight (2002). We compute the Top N
accuracy in the standard way as the number of Span-
ish test words whose Top N English translation can-
didates contain a lexicon translation entry out of the
total number of Spanish words that can be mapped
correctly using the lexicon entries. Thus if ?crec-
imiento, growth? is the correct mapping based on the
lexicon entries, the translation for ?crecimiento? will
be counted as correct if ?growth? occurs in the Top
N English translation candidates for ?crecimiento?.
Note that the exact-match accuracy is a conser-
vative estimate as it is possible that the algorithm
may propose a reasonable translation for the given
camino
Depposn Cntxt Model Adjbow Cntxt Model
way 0.124 intentions 0.22
solution 0.097 way 0.21
steps 0.094 idea 0.20
path 0.093 thing 0.20
debate 0.085 faith 0.18
account 0.082 steps 0.17
means 0.080 example 0.17
work 0.079 news 0.16
approach 0.074 work 0.16
issue 0.073 attitude 0.15
Table 2: Top 10 translation candidates for the spanish word
?camino (way)? for the best adjacent context model (Adjbow)
and best dependency context model (Depposn). The bold English
terms show the acceptable translations.
Figure 3: Precision/Recall curve showing superior perfor-
mance of dependency context model as compared to adjacent
context at different recall points. Precision is the fraction of
tested Spanish words with Top 1 translation correct and Recall
is fraction of the 1000 Spanish words tested upon.
Spanish word but is marked incorrect if it does not
exist in the lexicon. Because it would be intractable
to compare each projected vector against the vectors
for all possible English words, we limited ourselves
to comparing the projected vector from each Spanish
word against the vectors for the 1000 most frequent
English nouns, following along the lines of previ-
ous work (Koehn and Knight, 2002; Haghighi et al,
2008).
5 Results
Table 3 gives the Top 1 and Top 10 accuracy for
each of the models on their ability to translate Span-
ish nouns into English. Examples of the top 10
translations using the best performing baseline and
dependency-based models are shown in Table 2. The
baseline models Adjposn and Adjbow differ in that the
133
Model AccTop 1 AccTop 10
Adjbow 35.3% 59.8%
Adjposn 20.9% 46.9%
Depbow 41.0% 62.0%
Depposn 41.0% 64.1%
Depposn + rev 42.9% 65.5%
Mosesen-es-100k 56.4% 62.7%
Table 3: Performance of various context-based models
learned from monolingual corpora and phrase-table learned
from parallel corpora on Noun translation.
latter disregards the position information in the con-
text vector and simply uses a bag of words instead.
Table 3 shows that Adjbow gains using this simplifi-
cation. A bag of words vector approach pools counts
together, which helps to reduce data sparsity. In
the position based model the vector is four times as
long. Additionally, the bag of words model can help
when there is local re-ordering between the two lan-
guages. For instance, Spanish adjectives often fol-
low nouns whereas in English the the ordering is
reversed. Thus, one can either learn position map-
pings, that is, position +1 for adjectives in Spanish is
the same as position -1 in English or just add the the
word counts from different positions into one com-
mon vector as considered in the bag of words ap-
proach.
Using dependency trees also alleviates the prob-
lem of position mapping between source and target
language. Table 3 shows the performance using the
dependency based models outperforms the baseline
models substantially. Comparing Depbow to Depposn
shows that ignoring the tree depth and treating it as
a bag of words does not increase the performance.
This contrasts with the baseline models. The de-
pendency positions account for re-ordering automat-
ically. The precision-recall curve in Figure 3 shows
that the dependency-based context performs better
than adjancet context at almost all recall levels.
The Mosesen-es-100k model shows the performance
of the statistical translation model trained on a bilin-
gual parallel corpus. While the system performs best
in Top 1 accuracy, the dependency context-based
model that ignores the sentence alignments surpris-
ingly performs better in case of Top 10 accuracy,
showing substantial promise.
While computing the accuracy using the phrase-
table learned from parallel corpora (Mosesen-es-100k),
the translation probabilities from both directions
(p(es|en) and p(en|es)) were used to rank the can-
didates. We also apply the monolingual context-
based model in the reverse direction (from English
to Spanish) and the row with label Depposn + rev in
Table 3 shows further gains using both directions.
Spanish English Sim Is present
Score in lexicon
sen?ores gentlemen 0.99 NO
xenofobia xenophobia 0.87 YES
diversidad diversity 0.73 YES
chipre cyprus 0.66 YES
mujeres women 0.65 YES
alemania germany 0.65 YES
explotacio?n exploitation 0.63 YES
hombres men 0.62 YES
repu?blica republic 0.60 YES
racismo racism 0.59 YES
comercio commerce 0.58 YES
continente continent 0.53 YES
gobierno government 0.52 YES
israel israel 0.52 YES
francia france 0.52 YES
fundamento certainty 0.51 NO
suecia sweden 0.50 YES
tra?fico space 0.49 NO
televisio?n tv 0.48 YES
francesa portuguese 0.48 NO
Table 4: List of 20 most confident mappings using the de-
pendency context based model for noun translation. Note that
although the first mapping is the correct one, it was not present
in the lexicon used for evaluation and hence is marked as incor-
rect.
6 Further Extensions: Generalizing to
other word types via tagset mapping
Most of the previous literature on this problem fo-
cuses on evaluating on nouns (Rapp, 1999; Koehn
and Knight 2002; Haghighi et al, 2008). However
the vector projection approach is general, and should
be applicable to other word-types as well. We eval-
uated the models with new test set containing 1000
most frequent words (not just nouns) in the English-
Spanish lexicon.
We used the dependency-based context model to
create translations for this new set. The row labeled
Depposn in Table 5 shows that the accuracy on this
set is lower when compared to evaluating only on
nouns. The main reason for lower accuracy is that
closed class words are often the most frequent and
tend to have a wide range of contexts resulting in
reasonable translation for most words include open
class words via the context model. For instance, the
English preposition ?to? appears as the most confi-
dent translation for 147 out of the 1000 Spanish test
134
Figure 4: Illustration of using part-of-speech tag mapping to
restrict candidate space of translations
words and in none (rightly so) after restricting the
translations by part-of-speech categories.
This problem can be greatly reduced by making
use of the intuition that part-of-speech is often pre-
served in translation, thus the space of possible can-
didate translation can be largely reduced based on
the part-of-speech restrictions. For example, a noun
in source language will usually be translated as noun
in target language, determiner will be translated as
determiner and so on. This idea is more clearly il-
lustrated in in Figure 4. We do not impose a hard
restriction but rather compute a ranking based on
the conditional probability of candidate translation?s
part-of-speech tag given source word?s tag.
An interesting problem in using part-of-speech re-
strictions is that corpora in different languages have
been tagged using widely different tagsets and the
following subsection explains this problem in detail:
6.1 Mapping Part-of-Speech tagsets in
different languages
The English tagset was derived from the Penn tree-
bank consisting of 53 tags (including punctuation
markers) and the Spanish tagset was derived from
the Cast3LB dataset consisting of 57 tags but there
is a large difference in the morphological and syn-
tactic features marked by the tagset. For example,
the Spanish tagset as different tags for masculine and
feminine nouns and also has a different tag for coor-
dinated nouns, all of which need to be mapped to the
singular or plural noun category available in English
tagset. Figure 5 shows an illustration of the mapping
problem between the Spanish and English POS tags.
Figure 5: Illustration of mapping Spanish part-of-speech
tagset to English tagset. The tagsets vary greatly in notation and
the morphological/syntactic constituents represented and need
to be mapped first, using the algorithm described in Section 6.1.
We now describe an empirical approach for learn-
ing the mapping between tagsets using the English-
Spanish projection dictionary used in the monolin-
gual context-based models for translation. Given a
small English-Spanish bilingual dictionary and a n-
best list of part-of-speech tags for each word in the
dictionary2, we compute conditional probability of
translating a source word with pos tag sposi to a tar-
get with pos tag tposj as follows:
p(tposj |sposi) =
c(sposi , tposj )
c(sposi) =?
sw?S, tw?T p(sposi |sw) ? p(tposj |tw) ? Idict(sw, tw)?
sw?S p(sposi |sw)
where
? S and T are the source and target vocabulary in
the seed dictionary, with sw and tw being any
of the words in the respective sets.
? p(sposi |sw), p(tposj |tw) are obtained using rel-
ative frequencies in a part-of-speech tagged
corpus in the source/target languages respec-
tively, and are used as soft counts.
? Idict(sw, tw) is the indicator function with
value 1 if the pair (sw, tw) occurs in the seed
dictionary and 0 otherwise.
In essence, the mapping between tagsets is
learned using the known translations from a small
dictionary.
Given a source word sw to translate, its most
likely tag s?pos, and the most likely mapping of this
tag into English t?pos computed as above, the transla-
tion candidates with part-of-speech tag t?pos are con-
sidered for comparison with vector similarity and
2The n-best part-of-speech tag list for any word in the dic-
tionary was derived using the relative frequencies in a part-of-
speech annotated corpora in the respective languages
135
Figure 6: Precision/Recall curve showing superior perfor-
mance of using part-of-speech equivalences for translating all
word-types. Precision is the fraction of tested Spanish words
with Top 1 translation correct and Recall is fraction of the 1000
Spanish words tested upon.
the other candidates with tposj 6= t?pos are discarded
from the candidate space. Figure 4 shows an exam-
ple of restricting the candidate space using POS tags.
Model AccTop 1 AccTop 10
Depposn 35.1% 62.9%
+ POS 41.3% 66.4%
Table 5: Performance of dependency context-based model
along with addition of part-of-speech mapping model on trans-
lating all word-types.
The row labeled +POS in Table 5 shows the part-
of-speech tags provides substantial gain as com-
pared to direct application of dependency context-
based model and is also comparable to the accuracy
obtained evaluating just on nouns in Table 3.
7 Conclusion
This paper presents a novel contribution to the stan-
dard context models used when learning transla-
tion lexicons from monolingual corpora by vector
projection. We show that using contexts based on
dependency parses can provide more salient con-
texts, allow for dynamic context size, and account
for word reordering in the source and target lan-
guage. An exact-match evaluation shows 16% rela-
tive improvement by using a dependency-based con-
text model over the standard approach. Furthermore,
we show that our model, which is trained only on
monolingual corpora, outperforms the standard sta-
Spanish English Sim Is present
Score in lexicon
sen?ores gentlemen 0.99 NO
chipre cyprus 0.66 YES
mujeres women 0.65 YES
alemania germany 0.65 YES
hombres men 0.62 YES
expresar express 0.60 YES
racismo racism 0.59 YES
interior internal 0.55 YES
gobierno government 0.52 YES
francia france 0.52 YES
cultural cultural 0.51 YES
suecia sweden 0.50 YES
fundamento basis 0.48 YES
francesa french 0.48 YES
entre between 0.47 YES
origen origin 0.46 YES
tra?fico traffic 0.45 YES
de of 0.44 YES
social social 0.43 YES
ruego thank 0.43 NO
Table 6: List of 20 most confident mappings using the depen-
dency context with the part-of-speech mapping model translat-
ing all word-types. Note that although the second best mapping
in Table4 for noun-translation is for xenofobia with score 0.87,
xenofobia is not among the 1000 most frequent words (of all
word-types) and thus is not in this test set.
tistical MT approach to learning phrase tables when
trained on the same amount of sentence-aligned par-
allel corpora, when evaluated on Top 10 accuracy.
As a second contribution, we go beyond previ-
ous literature which evaluated only on nouns. We
showed how preserving a word?s part-of-speech in
translation can improve performance. We further
proposed a solution to an interesting sub-problem
encountered on the way. Since part-of-speeech
tagsets are not identical across two languages, we
propose a way of learning their mapping automat-
ically. Restricting candidate space based on this
learned tagset mapping resulted in 18% improve-
ment over the direct application of context-based
model to all word-types.
Dependency trees help improve the context for
translation substantially and their use opens up the
question of how the context can be enriched further
making use of the hidden structure that may provide
clues for a word?s translation. We also believe that
the problem of learning the mapping between tagsets
in two different languages can be used in general for
other NLP tasks making use of projection of words
and its morphological/syntactic properties between
languages.
136
References
S. Buchholz and E. Marsi. 2006. Conll-X shared task
on multilingual dependency parsing. Proceedings of
CoNLL, pages 189?210.
Y. Cao and H. Li. 2002. Base Noun Phrase translation
using web data and the EM algorithm. Proceedings of
COLING-Volume 1, pages 1?7.
D. Chiang. 2007. Hierarchical Phrase-Based Transla-
tion. Computational Linguistics, 33(2):201?228.
P. Fung and L.Y. Yee. 1998. An IR Approach for
Translating New Words from Nonparallel, Compara-
ble Texts. Proceedings of ACL, 36:414?420.
A. Haghighi, P. Liang, T. Berg-Kirkpatrick, and D. Klein.
2008. Learning bilingual lexicons from monolingual
corpora. Proceedings of ACL-HLT, pages 771?779.
Z. Harris. 1985. Distributional structure. Katz, J. J. (ed.),
The Philosophy of Linguistics, pages 26?47.
P. Koehn and K. Knight. 2002. Learning a translation
lexicon from monolingual corpora. Proceedings of
ACL Workshop on Unsupervised Lexical Acquisition,
pages 9?16.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proceedings of NAACL-
HLT, pages 48?54.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, et al 2007. Moses: Open source
toolkit for statistical machine translation. Proceedings
of ACL, companian volume, pages 177?180.
P. Koehn. 2005. Europarl: A parallel corpus for statisti-
cal machine translation. MT Summit X.
D. Lin and P. Pantel. 2002. Discovery of inference rules
for question-answering. Natural Language Engineer-
ing, 7(04):343?360.
G.S. Mann and D. Yarowsky. 2001. Multipath transla-
tion lexicon induction via bridge languages. Proceed-
ings of NAACL, pages 151?158.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: the Penn treebank. Computational Linguistics,
19(2):313?330.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 2005.
Non-projective dependency parsing using spanning
tree algorithms. Proceedings of EMNLP-HLT, pages
523?530.
J. Nivre, J. Hall, S. Kubler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The conll 2007
shared task on dependency parsing. Proceedings of
the CoNLL Shared Task Session of EMNLP-CoNLL,
pages 915?932.
S. Pado and M. Lapata. 2007. Dependency-Based Con-
struction of Semantic Space Models. Computational
Linguistics, 33(2):161?199.
R. Rapp. 1999. Automatic identification of word trans-
lations from unrelated English and German corpora.
Proceedings of ACL, pages 519?526.
C. Schafer and D. Yarowsky. 2002. Inducing translation
lexicons via diverse similarity measures and bridge
languages. Proceedings of COLING, pages 1?7.
137
Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing, ACL-IJCNLP 2009, pages 58?65,
Suntec, Singapore, 7 August 2009.
c
?2009 ACL and AFNLP
Ranking and Semi-supervised Classification
on Large Scale Graphs Using Map-Reduce
Delip Rao
Dept. of Computer Science
Johns Hopkins University
delip@cs.jhu.edu
David Yarowsky
Dept. of Computer Science
Johns Hopkins University
yarowsky@cs.jhu.edu
Abstract
Label Propagation, a standard algorithm
for semi-supervised classification, suffers
from scalability issues involving memory
and computation when used with large-
scale graphs from real-world datasets. In
this paper we approach Label Propagation
as solution to a system of linear equations
which can be implemented as a scalable
parallel algorithm using the map-reduce
framework. In addition to semi-supervised
classification, this approach to Label Prop-
agation allows us to adapt the algorithm to
make it usable for ranking on graphs and
derive the theoretical connection between
Label Propagation and PageRank. We pro-
vide empirical evidence to that effect using
two natural language tasks ? lexical relat-
edness and polarity induction. The version
of the Label Propagation algorithm pre-
sented here scales linearly in the size of
the data with a constant main memory re-
quirement, in contrast to the quadratic cost
of both in traditional approaches.
1 Introduction
Natural language data often lend themselves to a
graph-based representation. Words can be linked
by explicit relations as in WordNet (Fellbaum,
1989), and documents can be linked to one an-
other via hyperlinks. Even in the absence of such a
straightforward representation it is possible to de-
rive meaningful graphs such as the nearest neigh-
bor graphs, as done in certain manifold learning
methods, e.g. Roweis and Saul (2000); Belkin and
Niyogi (2001). Typically, these graphs share the
following properties:
? They are edge-weighted.
? The edge weight encodes some notion of re-
latedness between the vertices.
? The relation represented by edges is at least
weakly transitive. Examples of such rela-
tions include, ?is similar to?, ?is more gen-
eral than?, and so on. It is important that the
relations selected are transitive for the graph-
based learning methods using random walks.
Such graphs present several possibilities for
solving natural language problems involving rank-
ing, classification, and clustering. Graphs have
been successfully employed in machine learning
in a variety of supervised, unsupervised, and semi-
supervised tasks. Graph based algorithms perform
better than their counterparts as they capture the
latent structure of the problem. Further, their ele-
gant mathematical framework allows simpler anal-
ysis to gain a deeper understanding of the prob-
lem. Despite these advantages, implementations
of most graph-based learning algorithms do not
scale well on large datasets from real world prob-
lems in natural language processing. With large
amounts of unlabeled data available, the graphs
can easily grow to millions of nodes and most ex-
isting non-parallel methods either fail to work due
to resource constraints or find the computation in-
tractable.
In this paper we describe a scalable implemen-
tation of Label Propagation, a popular random
walk based semi-supervised classification method.
We show that our framework can also be used for
ranking on graphs. Our parallel formulation shows
a theoretical connection between Label Propaga-
tion and PageRank. We also confirm this em-
pirically using the lexical relatedness task. The
58
proposed Parallel Label Propagation scales up lin-
early in the data and the number of processing ele-
ments available. Also, the main memory required
by the method does not grow with the size of the
graph.
The outline of this paper is as follows: Section 2
introduces the manifold assumption and explains
why graph-based learning algorithms perform bet-
ter than their counterparts. Section 3 motivates
the random walk based approach for learning on
graphs. Section 4 introduces the Label Propaga-
tion method by Zhu et al (2003). In Section 5 we
describe a method to scale up Label Propagation
using Map-Reduce. Section 6 shows how Label
Propagation could be used for ranking on graphs
and derives the relation between Label Propaga-
tion and PageRank. Parallel Label Propagation is
evaluated on ranking and semi-supervised classifi-
cation problems in natural language processing in
Section 8. We study scalability of this algorithm in
Section 9 and describe related work in the area of
parallel algorithms and machine learning in Sec-
tion 10.
2 Manifold Assumption
The training data D can be considered as a collec-
tion of tuples D = (X ,Y) where Y are the labels
and X are the features, and the learned modelM
is a surrogate for an underlying physical process
which generates the data D. The data D can be
considered as a sampling from a smooth surface or
a manifold which represents the physical process.
This is known as the manifold assumption (Belkin
et al, 2005). Observe that even in the simple case
of Euclidean data (X = {x : x ? R
d
}) as shown
in Figure 1, points that lie close in the Euclidean
space might actually be far off on the manifold.
A graph, as shown in Figure 1c, approximates the
structure of the manifold which was lost in vector-
ized algorithms operating in the Euclidean space.
This explains the better performance of graph al-
gorithms for learning as seen in the literature.
3 Distance measures on graphs
Most learning tasks on graphs require some notion
of distance or similarity to be defined between the
vertices of a graph. The most obvious measure of
distance in a graph is the shortest path between the
vertices, which is defined as the minimum number
of intervening edges between two vertices. This is
also known as the geodesic distance. To convert
this distance measure to a similarity measure, we
take the reciprocal of the shortest-path length. We
refer to this as the geodesic similarity.
Figure 2: Shortest path distances on graphs ignore
the connectivity structure of the graph.
While shortest-path distances are useful in
many applications, it fails to capture the following
observation. Consider the subgraph of WordNet
shown in Figure 2. The term moon is con-
nected to the terms religious leader
and satellite.
1
Observe that both
religious leader and satellite are
at the same shortest path distance from moon.
However, the connectivity structure of the graph
would suggest satellite to be more similar
than religious leader as there are multiple
senses, and hence multiple paths, connecting
satellite and moon.
Thus it is desirable to have a measure that cap-
tures not only path lengths but also the connectiv-
ity structure of the graph. This notion is elegantly
captured using random walks on graphs.
4 Label Propagation: Random Walk on
Manifold Graphs
An efficient way to combine labeled and unla-
beled data involves construction of a graph from
the data and performing a Markov random walk
on the graph. This has been utilized in Szummer
and Jaakkola (2001), Zhu et. al. (2003), and Azran
(2007). The general idea of Label Propagation in-
volves defining a probability distribution F over
the labels for each node in the graph. For labeled
nodes, this distribution reflects the true labels and
the aim is to recover this distribution for the unla-
beled nodes in the graph.
Consider a graph G(V,E,W ) with vertices V ,
edges E, and an n ? n edge weight matrix W =
1
The religious leader sense of moon is due to Sun
Myung Moon, a US religious leader.
59
(a) (b) (c)
Figure 1: Manifold Assumption [Belkin et al, 2005]: Data lies on a manifold (a) and points along the
manifold are locally similar (b).
[w
ij
], where n = |V |. The Label Propagation al-
gorithm minimizes a quadratic energy function
E =
1
2
?
(i, j) ? E
w
ij
(F
i
? F
j
)
2
(1)
The general recipe for using random walks
for classification involves constructing the graph
Laplacian and using the pseudo-inverse of the
Laplacian as a kernel (Xiao and Gutman, 2003).
Given a weighted undirected graph, G(V,E,W ),
the Laplacian is defined as follows:
L
ij
=
?
?
?
d
i
if i = j
?w
ij
if i is adjacent to j
0 otherwise
(2)
where d
i
=
?
j
w
ij
.
It has been shown that the pseudo-inverse of the
Laplacian L is a kernel (Xiao and Gutman, 2003),
i.e., it satisfies the Mercer conditions. However,
there is a practical limitation to this approach. For
very large graphs, even if the graph Laplacians are
sparse, their pseudo-inverses are dense matrices
requiring O(n
2
) space. This can be prohibitive in
most computing environments.
5 Parallel Label Propagation
In developing a parallel algorithm for Label
Propagation we instead take an alternate approach
and completely avoid the use of inverse Lapla-
cians for the reasons stated above. Our approach
follows from the observation made from Zhu et
al.?s (2003) Label Propagation algorithm:
Observation: In a weighted graph G(V,E,W )
with n = |V | vertices, minimization of Equation
(1) is equivalent to solving the following system
of linear equations.
?
(i, j) ? E
w
ij
F
i
=
?
(i, j) ? E
w
ij
F
j
(3)
?
c ? classes(i)
F
i
(c) = 1 ?i, j ? V.
We use this observation to derive an iterative
Label Propagation algorithm that we will later par-
allelize. Consider a weighted undirected graph
G(V,E,W ) with the vertex set partitioned into V
L
and V
U
(i.e., V = V
L
?V
U
) such that all vertices in
V
L
are labeled and all vertices in V
U
are unlabeled.
Typically only a small set of vertices are labeled,
i.e., |V
U
|  |V
L
|. Let F
u
denote the probability
distribution over the labels associated with vertex
u ? V . For v ? V
L
, F
v
is known, and we also
add a ?dummy vertex? v
?
to the graph G such that
w
vv
?
= 1 and F
v
?
= F
v
. This is equivalent to the
?clamping? done in (Zhu et al, 2003). Let V
D
be
the set of dummy vertices.
Algorithm 1: Iterative Label Propogation
repeat
forall v ? (V ? V
D
) do
F
v
=
?
(v,u)?E
w
uv
F
v
Row normalize F
v
.
end
until convergence or maxIterations
Observe that every iteration of Algorithm 1 per-
forms certain operations on each vertex of the
graph. Further, these operations only rely on
local information (from neighboring vertices of
the graph). This leads to the parallel algorithm
(Algorithm 2) implemented using the map-reduce
model. Map-Reduce (Dean and Ghemawat, 2004)
is a paradigm for implementing distributed algo-
rithms with two user supplied functions ?map? and
?reduce?. The map function processes the input
key/value pairs with the key being a unique iden-
60
tifier for a node in the graph and the value corre-
sponds to the data associated with the node. The
mappers run on different machines operating on
different parts of the data and the reduce function
aggregates results from various mappers.
Algorithm 2: Parallel Label Propagation
map(key, value):
begin
d = 0
neighbors = getNeighbors(value);
foreach n ? neighbors do
w = n.weight();
d += w ? n.getDistribution();
end
normalize(d);
value.setDistribution(d);
Emit(key, value);
end
reduce(key, values): Identity Reducer
Algorithm 2 represents one iteration of Algo-
rithm 1. This is run repeatedly until convergence
or for a specified number of iterations. The al-
gorithm is considered to have converged if the la-
bel distributions associated with each node do not
change significantly, i.e.,
?
?
?
?
F
(i+1)
? F
(i)
?
?
?
?
2
< 
for a fixed  > 0.
6 Label Propagation for Ranking
Graph ranking is applicable in a variety of prob-
lems in natural language processing and informa-
tion retrieval. Given a graph, we would like to
rank the vertices of a graph with respect to a node,
called the pivot node or query node. Label Prop-
agation and its variants (Szummer and Jaakkola,
2001; Zhu et al, 2003; Azran, 2007) have been
traditionally used for semi-supervised classifica-
tion. Our view of Label Propagation (via Algo-
rithm 1) suggests a way to perform ranking on
graphs.
Ranking on graphs can be performed in the Par-
allel Label Propagation framework by associating
a single point distribution with all vertices. The
pivot node has a mass fixed to the value 1 at all it-
erations. In addition, the normalization step in Al-
gorithm 2 is omitted. At the end of the algorithm,
the mass associated with each node determines its
rank.
6.1 Connection to PageRank
It is interesting to note that Algorithm 1 brings
out a connection between Label Propagation and
PageRank (Page et al, 1998). PageRank is a ran-
dom walk model that allows the random walk to
?jump? to its initial state with a nonzero proba-
bility (?). Given the probability transition matrix
P = [P
rs
], where P
rs
is the probability of jumping
from node r to node s, the weight update for any
vertex (say v) is derived as follows
v
t+1
= ?v
t
P + (1? ?)v
0
(4)
Notice that when ? = 0.5, PageRank is reduced
to Algorithm 1, by a constant factor, with the ad-
ditional (1? ?)v
0
term corresponding to the con-
tribution from the ?dummy vertices? V
D
in Algo-
rithm 1.
We can in fact show that Algorithm 1 reduces to
PageRank as follows:
v
t+1
= ?v
t
P + (1? ?)v
0
? v
t
P +
(1? ?)
?
v
0
= v
t
P + ?v
0
(5)
where ? =
(1??)
?
. Thus by setting the edge
weights to the dummy vertices to ?, i.e., ?(z, z
?
) ?
E and z
?
? V
D
, w
zz
?
= ?, Algorithm 1, and hence
Algorithm 2, reduces to PageRank. Observe that
when ? = 1 we get the original Algorithm 1.
We?ll refer to this as the ??-correction?.
7 Graph Representation
Since Parallel Label Propagation algorithm uses
only local information, we use the adjacency list
representation (which is same as the sparse adja-
cency matrix representation) for the graph. This
representation is important for the algorithm to
have a constant main memory requirement as no
further lookups need to be done while comput-
ing the label distribution at a node. The interface
definition for the graph is listed in Appendix A.
Often graph data is available in an edge format,
as <source, destination, weight> triples. We use
another map-reduce step (Algorithm 3) to convert
that data to the form shown in Appendix A.
8 Evaluation
We evaluate the Parallel Label Propagation algo-
rithm for both ranking and semi-supervised clas-
sification. In ranking our goal is to rank the ver-
tices of a graph with respect to a given node called
the pivot/query node. In semi-supervised classi-
fication, we are given a graph with some vertices
61
Algorithm 3: Graph Construction
map(key, value):
begin
edgeEntry = value;
Node n(edgeEntry);
Emit(n.id, n);
end
reduce(key, values):
begin
Emit(key, serialize(values));
end
labeled and would like to predict labels for the re-
maining vertices.
8.1 Ranking
To evaluate ranking, we consider the problem
of deriving lexical relatedness between terms.
This has been a topic of interest with applica-
tions in word sense disambiguation (Patwardhan
et al, 2005), paraphrasing (Kauchak and Barzilay,
2006), question answering (Prager et al, 2001),
and machine translation (Blatz et al, 2004), to
name a few. Following the tradition in pre-
vious literature we evaluate on the Miller and
Charles (1991) dataset. We compare our rankings
with the human judegments using the Spearman
rank correlation coefficient. The graph for this
task is derived from WordNet, an electronic lex-
ical database. We compare Algorithm 2 with re-
sults from using geodesic similarity as a baseline.
As observed in Table 1, the parallel implemen-
tation in Algorithm 2 performs better than rank-
ing using geodesic similarity derived from short-
est path lengths. This reinforces the motivation of
using random walks as described in Section 3.
Method Spearman
Correlation
Geodesic (baseline) 0.28
Parallel Label 0.36
Propagation
Table 1: Lexical-relatedness results: Comparison
with geodesic similarity.
We now empirically verify the equivalence of
the ?-corrected Parallel Label Propagation and
PageRank established in Equation 4. To do this,
we use ? = 0.1 in the PageRank algorithm and
set ? =
(1??)
?
= 9 in the ?-corrected Parallel La-
bel Propagation algorithm. The results are seen in
Table 2.
Method Spearman
Correlation
PageRank (? = 0.1) 0.39
Parallel Label 0.39
Propagation (? = 9)
Table 2: Lexical-relatedness results: Comparision
of PageRank and ?-corrected Parallel Label Prop-
agation
8.2 Semi-supervised Classification
Label Propagation was originally developed as a
semi-supervised classification method. Hence Al-
gorithm 2 can be applied without modification.
After execution of Algorithm 2, every node v in
the graph will have a distribution over the labels
F
v
. The predicted label is set to argmax
c?classes(v)
F
v
(c).
To evaluate semi-supervised classification we
consider the problem of learning sentiment polar-
ity lexicons. We consider the polarity of a word to
be either positive or negative. For example, words
such as good, beautiful , and wonderful are consid-
ered as positive sentiment words; whereas words
such as bad, ugly, and sad are considered negative
sentiment words. Learning such lexicons has ap-
plications in sentiment detection and opinion min-
ing. We treat sentiment polarity detection as a
semi-supervised Label Propagation problem in a
graph. In the graph, each node represents a word
whose polarity is to be determined. Each weighted
edge encodes a relation that exists between two
words. Each node (word) can have two labels:
positive or negative. It is important to note that La-
bel Propagation, and hence Algorithms 1&2, sup-
port multi-class classification but for the purpose
of this task we have two labels. The graph for the
task is derived from WordNet. We use the Gen-
eral Inquirer (GI)
2
data for evaluation. General
Inquirer is lexicon of English words hand-labeled
with categorical information along several dimen-
sions. One such dimension is called valence, with
1915 words labeled ?Positiv? (sic) and 2291 words
labeled ?Negativ? for words with positive and neg-
ative sentiments respectively. We used a random
20% of the data as our seed labels and the rest
as our unlabeled data. We compare our results
2
http://www.wjh.harvard.edu/?inquirer/
62
(a) (b)
Figure 3: Scalability results: (a) Scaleup (b) Speedup
(F-scores) with another scalable previous work by
Kim and Hovy (Kim and Hovy, 2006) in Table 2
for the same seed set. Their approach starts with a
few seeds of positive and negative terms and boot-
straps the list by considering all synonyms of pos-
itive word as positive and antonyms of positive
words as negative. This procedure is repeated mu-
tatis mutandis for negative words in the seed list
until there are no more words to add.
Method Nouns Verbs Adjectives
Kim & Hovy 34.80 53.36 47.28
Parallel Label 58.53 83.40 72.95
Propagation
Table 3: Polarity induction results (F-scores)
The performance gains seen in Table 3 should
be attributed to the Label Propagation in general
as the previous work (Kim and Hovy, 2006) did
not utilize a graph based method.
9 Scalability experiments
We present some experiments to study the scala-
bility of the algorithm presented. All our experi-
ments were performed on an experimental cluster
of four machines to test the concept. The machines
were Intel Xeon 2.4 GHz with 1Gb main memory.
All performance measures were averaged over 20
runs.
Figure 3a shows scaleup of the algorithm which
measures how well the algorithm handles increas-
ing data sizes. For this experiment, we used all
nodes in the cluster. As observed, the increase in
time is at most linear in the size of the data. Fig-
ure 3b shows speedup of the algorithm. Speedup
shows how well the algorithm performs with in-
crease in resources for a fixed input size. In
this case, we progressively increase the number of
nodes in the cluster. Again, the speedup achieved
is linear in the number of processing elements
(CPUs). An appealing factor of Algorithm 2 is that
the memory used by each mapper process is fixed
regardless of the size of the graph. This makes the
algorithm feasible for use with large-scale graphs.
10 Related Work
Historically, there is an abundance of work in par-
allel and distributed algorithms for graphs. See
Grama et al (2003) for survey chapters on the
topic. In addition, the emergence of open-source
implementations of Google?s map-reduce (Dean
and Ghemawat, 2004) such as Hadoop
3
has made
parallel implementations more accessible.
Recent literature shows tremendous interest in
application of distributed computing to scale up
machine learning algorithms. Chu et al (2006)
describe a family of learning algorithms that fit
the Statistical Query Model (Kearns, 1993). These
algorithms can be written in a special summation
form that is amenable to parallel speed-up. Exam-
ples of such algorithms include Naive Bayes, Lo-
gistic Regression, backpropagation in Neural Net-
works, Expectation Maximization (EM), Princi-
pal Component Analysis, and Support Vector Ma-
chines to name a few. The summation form can be
easily decomposed so that the mapper can com-
pute the partial sums that are then aggregated by a
reducer. Wolfe et al (2008) describe an approach
to estimate parameters via the EM algorithm in a
setup aimed to minimize communication latency.
The k-means clustering algorithm has been an
archetype of the map-reduce framework with sev-
eral implementations available on the web. In
3
http://hadoop.apache.org/core
63
addition, the Netflix Million Dollar Challenge
4
generated sufficient interest in large scale cluster-
ing algorithms. (McCallum et al, 2000), describe
algorithmic improvements to the k-means algo-
rithm, called canopy clustering, to enable efficient
parallel clustering of data.
While there is earlier work on scalable map-
reduce implementations of PageRank (E.g., Gle-
ich and Zhukov (2005)) there is no existing liter-
ature on parallel algorithms for graph-based semi-
supervised learning or the relationship between
PageRank and Label Propagation.
11 Conclusion
In this paper, we have described a parallel algo-
rithm for graph ranking and semi-supervised clas-
sification. We derived this by first observing that
the Label Propagation algorithm can be expressed
as a solution to a set of linear equations. This is
easily expressed as an iterative algorithm that can
be cast into the map-reduce framework. This al-
gorithm uses fixed main memory regardless of the
size of the graph. Further, our scalability study re-
veals that the algorithm scales linearly in the size
of the data and the number of processing elements
in the cluster. We also showed how Label Prop-
agation can be used for ranking on graphs and
the conditions under which it reduces to PageR-
ank. We evaluated our implementation on two
learning tasks ? ranking and semi-supervised clas-
sification ? using examples from natural language
processing including lexical-relatedness and senti-
ment polarity lexicon induction with a substantial
gain in performance.
A Appendix A: Interface definition for
Undirected Graphs
In order to guarantee the constant main memory
requirement of Algorithm 2, the graph represen-
tation should encode for each node, the complete
information about it?s neighbors. We represent
our undirected graphs in the Google?s Protocol
Buffer format.
5
Protocol Buffers allow a compact,
portable on-disk representation that is easily ex-
tensible. This definition can be compiled into effi-
cient Java/C++ classes.
The interface definition for undirected graphs is
listed below:
4
http://www.netflixprize.com
5
Implementation available at
http://code.google.com/p/protobuf/
package graph;
message NodeNeighbor {
required string id = 1;
required double edgeWeight = 2;
repeated double labelDistribution = 3;
}
message UndirectedGraphNode {
required string id = 1;
repeated NodeNeighbor neighbors = 2;
repeated double labelDistribution = 3;
}
message UndirectedGraph {
repeated UndirectedGraphNode nodes = 1;
}
References
Arik Azran. 2007. The rendezvous algorithm: Multi-
class semi-supervised learning with markov random
walks. In Proceedings of the International Confer-
ence on Machine Learning (ICML).
Micheal. Belkin, Partha Niyogi, and Vikas Sindhwani.
2005. On manifold regularization. In Proceedings
of AISTATS.
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. 2004. Confidence es-
timation for machine translation. In Proceeding of
COLING.
Cheng T. Chu, Sang K. Kim, Yi A. Lin, Yuanyuan Yu,
Gary R. Bradski, Andrew Y. Ng, and Kunle Oluko-
tun. 2006. Map-reduce for machine learning on
multicore. In Proceedings of Neural Information
Processing Systems.
Jeffrey Dean and Sanjay Ghemawat. 2004. Map-
reduce: Simplified data processing on large clusters.
In Proceedings of the symposium on Operating sys-
tems design and implementation (OSDI).
Christaine Fellbaum, editor. 1989. WordNet: An Elec-
tronic Lexical Database. The MIT Press.
D. Gleich and L. Zhukov. 2005. Scalable comput-
ing for power law graphs: Experience with parallel
pagerank. In Proceedings of SuperComputing.
Ananth Grama, George Karypis, Vipin Kumar, and An-
shul Gupta. 2003. Introduction to Parallel Comput-
ing (2nd Edition). Addison-Wesley, January.
David Kauchak and Regina Barzilay. 2006. Para-
phrasing for automatic evaluation. In Proceedings
of HLT-NAACL.
Michael Kearns. 1993. Efficient noise-tolerant learn-
ing from statistical queries. In Proceedings of the
Twenty-Fifth Annual ACM Symposium on Theory of
Computing (STOC).
64
Soo-Min Kim and Eduard H. Hovy. 2006. Identifying
and analyzing judgment opinions. In Proceedings of
HLT-NAACL.
Andrew McCallum, Kamal Nigam, and Lyle H. Un-
gar. 2000. Efficient clustering of high-dimensional
data sets with application to reference matching.
In Knowledge Discovery and Data Mining (KDD),
pages 169?178.
G. Miller and W. Charles. 1991. Contextual correlates
of semantic similarity. In Language and Cognitive
Process.
Larry Page, Sergey Brin, Rajeev Motwani, and Terry
Winograd. 1998. The pagerank citation ranking:
Bringing order to the web. Technical report, Stan-
ford University, Stanford, CA.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted
Pedersen. 2005. Senserelate::targetword - A gen-
eralized framework for word sense disambiguation.
In Proceedings of ACL.
John M. Prager, Jennifer Chu-Carroll, and Krzysztof
Czuba. 2001. Use of wordnet hypernyms for an-
swering what-is questions. In Proceedings of the
Text REtrieval Conference.
M. Szummer and T. Jaakkola. 2001. Clustering and
efficient use of unlabeled examples. In Proceedings
of Neural Information Processing Systems (NIPS).
Jason Wolfe, Aria Haghighi, and Dan Klein. 2008.
Fully distributed EM for very large datasets. In Pro-
ceedings of the International Conference in Machine
Learning.
W. Xiao and I. Gutman. 2003. Resistance distance and
laplacian spectrum. Theoretical Chemistry Associa-
tion, 110:284?289.
Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty.
2003. Semi-supervised learning using Gaussian
fields and harmonic functions. In Proceedings of
the International Conference on Machine Learning
(ICML).
65
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1815?1827,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Exploring Demographic Language Variations to Improve Multilingual
Sentiment Analysis in Social Media
Svitlana Volkova
Center for Language and
Speech Processing
Johns Hopkins University
Baltimore, MD
svitlana@jhu.edu
Theresa Wilson
Human Language Technology
Center of Excellence
Johns Hopkins University
Baltimore, MD
taw@jhu.edu
David Yarowsky
Center for Language and
Speech Processing
Johns Hopkins University
Baltimore, MD
yarowsky@cs.jhu.edu
Abstract
Different demographics, e.g., gender or age,
can demonstrate substantial variation in their
language use, particularly in informal contexts
such as social media. In this paper we focus on
learning gender differences in the use of sub-
jective language in English, Spanish, and Rus-
sian Twitter data, and explore cross-cultural
differences in emoticon and hashtag use for
male and female users. We show that gen-
der differences in subjective language can ef-
fectively be used to improve sentiment anal-
ysis, and in particular, polarity classification
for Spanish and Russian. Our results show
statistically significant relative F-measure im-
provement over the gender-independent base-
line 1.5% and 1% for Russian, 2% and 0.5%
for Spanish, and 2.5% and 5% for English for
polarity and subjectivity classification.
1 Introduction
Sociolinguistics and dialectology have been study-
ing the relationships between language and speech at
the phonological, lexical and morphosyntactic lev-
els and social identity for decades (Picard, 1997;
Gefen and Ridings, 2005; Holmes and Meyerhoff,
2004; Macaulay, 2006; Tagliamonte, 2006). Re-
cent studies have focused on exploring demographic
language variations in personal email communica-
tion, blog posts, and public discussions (Boneva et
al., 2001; Mohammad and Yang, 2011; Eisenstein
et al, 2010; O?Connor et al, 2010; Bamman et al,
2012). However, one area that remains largely unex-
plored is the effect of demographic language varia-
tion on subjective language use, and whether these
differences may be exploited for automatic senti-
ment analysis. With the growing commercial im-
portance of applications such as personalized rec-
ommender systems and targeted advertising (Fan
and Chang, 2009), detecting helpful product review
(Ott et al, 2011), tracking sentiment in real time
(Resnik, 2013), and large-scale, low-cost, passive
polling (O?Connor et al, 2010), we believe that sen-
timent analysis guided by user demographics is a
very important direction for research.
In this paper, we focus on gender demographics
and language in social media to investigate differ-
ences in the language used to express opinions in
Twitter for three languages: English, Spanish, and
Russian. We focus on Twitter data because of its vol-
ume, dynamic nature, and diverse population world-
wide.1 We find that some words are more or less
likely to be positive or negative in context depend-
ing on the the gender of the author. For example, the
word weakness is more likely to be used in a pos-
itive way by women (Chocolate is my weakness!)
but in a negative way by men (Clearly they know
our weakness. Argggg). The Russian word ???????
(achieve) is used in a positive way by male users and
in a negative way by female users.
Our goals of this work are to (1) explore the gen-
der bias in the use of subjective language in so-
cial media, and (2) incorporate this bias into models
to improve sentiment analysis for English, Spanish,
and Russian. Specifically, in this paper we:
? investigate multilingual lexical variations in the
use of subjective language, and cross-cultural
1As of May 2013, Twitter has 500m users (140m of them
in the US) from more than 100 countries.
1815
emoticon and hashtag usage on a large scale in
Twitter data;2
? show that gender bias in the use of subjec-
tive language can be used to improve sentiment
analysis for multiple languages in Twitter.
? demonstrate that simple, binary features repre-
senting author gender are insufficient; rather, it
is the combination of lexical features, together
with set-count features representing gender-
dependent sentiment terms that is needed for
statistically significant improvements.
To the best of our knowledge, this work is the first
to show that incorporating gender leads to signifi-
cant improvements for sentiment analysis, particu-
larly subjectivity and polarity classification, for mul-
tiple languages in social media.
2 Related Work
Numerous studies since the early 1970?s have inves-
tigated gender-language differences in interaction,
theme, and grammar among other topics (Schiffman,
2002; Sunderland et al, 2002). More recent research
has studied gender differences in telephone speech
(Cieri et al, 2004; Godfrey et al, 1992) and emails
(Styler, 2011). Mohammad and Yang (2011) ana-
lyzed gender differences in the expression of senti-
ment in love letters, hate mail, and suicide notes, and
emotional word usage across genders in email.
There has also been a considerable amount of
work in subjectivity and sentiment analysis over
the past decade, including, more recently, in mi-
croblogs (Barbosa and Feng, 2010; Bermingham
and Smeaton, 2010; Pak and Paroubek, 2010; Bifet
and Frank, 2010; Davidov et al, 2010; Li et
al., 2010; Kouloumpis et al, 2011; Jiang et al,
2011; Agarwal et al, 2011; Wang et al, 2011;
Calais Guerra et al, 2011; Tan et al, 2011; Chen
et al, 2012; Li et al, 2012). In spite of the surge of
research in both sentiment and social media, only a
limited amount of work focusing on gender identi-
fication has looked at differences in subjective lan-
guage across genders within social media. Thel-
wall (2010) found that men and women use emoti-
cons to differing degrees on MySpace, e.g., female
2Gender-dependent and independent lexical resources of
subjective terms in Twitter for Russian, Spanish and English can
be found here: http://www.cs.jhu.edu/~svitlana/
users express positive emoticons more than male
users. Other researchers included subjective patterns
as features for gender classification of Twitter users
(Rao et al, 2010). They found that the majority of
emotion-bearing features, e.g., emoticons, repeated
letters, exasperation, are used more by female than
male users, which is consistent with results reported
in other recent work (Garera and Yarowsky, 2009;
Burger et al, 2011; Goswami et al, 2009; Argamon
et al, 2007). Other related work is that of Otter-
bacher (2010), who studied stylistic differences be-
tween male and female reviewers writing product
reviews, and Mukherjee and Liu (2010), who ap-
plied positive, negative and emotional connotation
features for gender classification in microblogs.
Although previous work has investigated gen-
der differences in the use of subjective language,
and features of sentiment have been used in gender
identification, to the best of our knowledge no one
has yet investigated whether gender differences in
the use of subjective language can be exploited to
improve sentiment classification in English or any
other language. In this paper we seek to answer this
question for the domain of social media.
3 Data
For the experiments in this paper, we use three sets
of data for each language: a large pool of data (800K
tweets) labeled for gender but unlabeled for senti-
ment, plus 2K development data and 2K test data
labeled for both sentiment and gender. We use the
unlabeled data to bootstrap Twitter-specific lexicons
and investigate gender differences in the use of sub-
jective language. We use the development data for
parameter tuning while bootstrapping, and the test
data for sentiment classification.
For English, we download tweets from the corpus
created by Burger et al (2011). This dataset con-
tains 2,958,103 tweets from 184K users, excluding
retweets. Retweets are omitted because our focus is
on the sentiment of the person tweeting; in retweets,
the words originate from a different user. All users
in this corpus have gender labels, which Burger et
al. automatically extracted from self-reported gen-
der on Facebook or MySpace profiles linked to by
the Twitter users. English tweets are identified using
a compression-based language identification (LID)
1816
tool (Bergsma et al, 2012). According to LID,
there are 1,881,620 (63.6%) English tweets from
which we select a random, gender-balanced sample
of 0.8M tweets. Burger?s corpus does not include
Russian and Spanish data on the same scale as En-
glish. Therefore, for Russian and Spanish we con-
struct a new Twitter corpus by downloading tweets
from followers of region-specific news and media
Twitter feeds. We use LID to identify Russian and
Spanish tweets, and remove retweets as before. In
this data, gender is labeled automatically based on
user first and last name morphology with a precision
above 0.98 for all languages.
Sentiment labels for tweets in the development
and test sets are obtained using Amazon Mechanical
Turk. For each tweet we collect annotations from
five workers and use majority vote to determine the
final label for the tweet. Snow et al (2008) show
that for a similar task, labeling emotion and valence,
on average four non-expert labelers are needed to
achieve an expert level of annotation. Below are the
example Russian tweets labeled for sentiment:
? Pos: ??? ?? ??????? ?????? ???? ? ??-
????? ????? ???????? ???... (It is a great
pleasure to go to bed after a long day at work...)
? Neg: ????????? ???????? ???????? ??-
???? ??? ??????! (Dear Mr. Prokhorov just
buy the elections!)
? Both: ????????? ???? ?? ??????? ?????!
?? ???? ?????????? ????????? ??? ????
????? :) (It was crowded at the local market!
But I got presents for my family:-))
? Neutral: ???? ????? ?????? ????? (Kiev is
a very old city).
Table 1 gives the distribution of tweets over senti-
ment and gender labels for the development and test
sets for English (EDEV, ETEST), Spanish (SDEV,
STEST), and Russian (RDEV, RTEST).
Data Pos Neg Both Neut ? ?
EDEV 617 357 202 824 1,176 824
ETEST 596 347 195 862 1,194 806
SDEV 358 354 86 1,202 768 1,232
STEST 317 387 93 1203 700 1,300
RDEV 452 463 156 929 1,016 984
RTEST 488 380 149 983 910 1,090
Table 1: Gender and sentiment label distribution in the
development and test sets for all languages.
4 Subjective Language and Gender
To study the intersection of subjective language and
gender in social media, ideally we would have a
large corpus labeled for both. Although our large
corpus is labeled for gender, it is not labeled for sen-
timent. Only the 4K tweets for each language that
compose the development and test sets are labeled
for both gender and sentiment. Obtaining sentiment
labels for all tweets would be both impractical and
expensive. Instead we use large multilingual senti-
ment lexicons developed specifically for Twitter as
described below. Using these lexicons we can begin
to explore the relationship between subjective lan-
guage and gender in the large pool of data labeled
for gender but unlabeled for sentiment. We also
look at the relationship between gender and the use
of different hashtags and emoticons. These can be
strong indicators of sentiment in social media, and in
fact are sometimes used to create noisy training data
for sentiment analysis in Twitter (Pak and Paroubek,
2010; Kouloumpis et al, 2011).
4.1 Bootstrapping Subjectivity Lexicons
Recent work by Banea et.al (2012) classifies meth-
ods for bootstrapping subjectivity lexicons into two
types: corpus-based and dictionary-based. Corpus-
based methods extract subjectivity lexicons from
unlabeled data using different similarity metrics
to measure the relatedness between words, e.g.,
Pointwise Mutual Information (PMI). Corpus-based
methods have been used to bootstrap lexicons
for ENGLISH (Turney, 2002) and other languages,
including ROMANIAN (Banea et al, 2008) and
JAPANESE (Kaji and Kitsuregawa, 2007).
Dictionary-based methods rely on relations be-
tween words in existing lexical resources. For exam-
ple, Rao and Ravichandran (2009) construct HINDI
and FRENCH sentiment lexicons using relations in
WordNet (Miller, 1995), Rosas et. al. (2012) boot-
strap a SPANISH lexicon using SentiWordNet (Bac-
cianella et al, 2010) and OpinionFinder,3 Clematide
and Klenner (2010), Chetviorkin et al (2012) and
Abdul-Mageed et. al. (2011) automatically expand
and evaluate GERMAN, RUSSIAN and ARABIC sub-
jective lexicons.
3www.cs.pitt.edu/mpqa/opinionfinder
1817
We use the corpus-based, language-independent
approach proposed by Volkova et al (2013) to boot-
strap Twitter-specific subjectivity lexicons. To start,
the new lexicon is seeded with terms from the initial
lexicon LI . On each iteration, tweets in the unla-
beled data are labeled using the current lexicon. If a
tweet contains one or more terms from the lexicon it
is marked subjective, otherwise neutral. Tweet po-
larity is determined in a similar way, but takes into
account negation. For every term not in the lexi-
con with a frequency threshold, the probability of
that word appearing in a subjective sentence is cal-
culated. The top k terms with a subjective probabil-
ity are then added to the lexicon. Bootstrapping con-
tinues until there are no more new terms meeting the
criteria to add to the lexicon. The parameters are op-
timized using a grid search on the development data
using F-measure for subjectivity classification. In
Table 2 we report size and term polarity from the ini-
tial LI and the bootstrapped LB lexicons. Although
more sophisticated bootstrapping methods exist, this
approach has been shown to be effective for atomi-
cally learning subjectivity lexicons in multiple lan-
guages on a large scale without any external, rich,
lexical resources, e.g., WordNet, or advanced NLP
tools, e.g., syntactic parsers (Wiebe, 2000) or infor-
mation extraction tools (Riloff and Wiebe, 2003).
For English, seed terms for bootstrapping are
the strongly subjective terms in the MPQA lexicon
(Wilson et al, 2005). For Spanish and Russian, the
seed terms are obtained by translating the English
seed terms using a bi-lingual dictionary, collecting
subjectivity judgments from MTurk on the transla-
tions, filtering out translations that are not strongly
subjective, and expanding the resulting word lists
with plurals and inflectional forms.
To verify that bootstrapping does provide a bet-
ter resource than existing dictionary-expanded lexi-
cons, we compare our Twitter-specific lexicons LB
English Spanish Russian
LEI L
E
B L
S
I L
S
B L
R
I L
R
B
Pos 2.3 16.8 2.9 7.7 1.4 5.3
Neg 2.8 4.7 5.2 14.6 2.3 5.5
Total 5.1 21.5 8.1 22.3 3.7 10.8
Table 2: The initial LI and the bootstrapped LB (high-
lighted) lexicon term count (LI ? LB) with polarity
across languages (thousands).
to the corresponding initial lexicons LI and the ex-
isting state-of-the-art subjective lexicons including:
? 8K strongly subjective English terms from Sen-
tiWordNet ?E (Baccianella et al, 2010);
? 1.5K full strength terms from the Spanish sen-
timent lexicon ?S (Perez-Rosas et al, 2012);
? 5K terms from the Russian sentiment lexicon
?R (Chetviorkin and Loukachevitch, 2012).
For that we apply rule-based subjectivity classi-
fication on the test data.4 This subjectivity classi-
fier predicts that a tweet is subjective if it contains
at least one, or at least two subjective terms from
the lexicon. To make a fair comparison, we auto-
matically expand ?E with plurals and inflectional
forms, ?S with the inflectional forms for verbs, and
?R with the inflectional forms for adverbs, adjec-
tives and verbs. We report precision, recall and F-
measure results in Table 3 and show that our boot-
strapped lexicons outperform the corresponding ini-
tial lexicons and the external resources.
Subj ? 1 Subj ? 2
P R F P R F
?E 0.67 0.49 0.57 0.76 0.16 0.27
LEI 0.69 0.73 0.71 0.79 0.34 0.48
LEB 0.64 0.91 0.75 0.7 0.74 0.72
?S 0.52 0.39 0.45 0.62 0.07 0.13
LSI 0.50 0.73 0.59 0.59 0.36 0.45
LSB 0.44 0.91 0.59 0.51 0.71 0.59
?R 0.61 0.49 0.55 0.74 0.17 0.29
LRI 0.72 0.34 0.46 0.83 0.07 0.13
LRB 0.64 0.58 0.61 0.74 0.23 0.35
Table 3: Precision, recall and F-measure results for sub-
jectivity classification using the external ?, initial LI and
bootstrapped LB lexicons for all languages.
4.2 Lexical Evaluation
With our Twitter-specific sentiment lexicons, we
can now investigate how the subjective use of these
terms differs depending on gender for our three lan-
guages. Figure 1 illustrates what we expect to find.
{F} and {M} are the sets of subjective terms used
by females and males, respectively. We expect that
some terms will be used by males, but never by fe-
males, and vice-versa. The vast majority, however,
will be used by both genders. Within this set of
shared terms, many words will show little difference
4A similar rule-based approach using terms from the
MPQA lexicon is suggested by (Riloff and Wiebe, 2003).
1818
Figure 1: Gender-dependent vs. independent subjectivity
terms (+ and - indicates term polarity).
Figure 2: The distribution of gender-dependent GDep
and gender-independent GInd sentiment terms.
in their subjective use when considering gender, but
there will be some words for which gender will have
an influence. Of particular interest for our work are
words in which the polarity of a term as it is used in
context is gender-influenced, the extreme case being
terms that flip their polarity depending on the gender
of the user. Polarity may be different because the
concept represented by the term tends to be viewed
in a different light depending on gender. There are
also words like weakness in which a more positive or
more negative word sense tends to be used by men
or women. In Figure 2 we show the distribution of
gender-specific and gender-independent terms from
the LB lexicons for all languages.
To identify gender-influenced terms in our lexi-
cons, we start by randomly sampling 400K male and
400K female tweets for each language from the data.
Next, for both genders we calculate the probability
of term ti appearing in a tweet with another subjec-
tive term (Eq.1), and the probability of it appearing
with a positive or negative term (Eq.2-3) from LB .
pti(subj?g) =
c(ti, P, g) + c(ti,N, g)
c(ti, g)
, (1)
where g ? F,M and P and N are positive and nega-
tive sets of terms from the initial lexicon LI .
pti(+?g) =
c(ti, P, g)
c(ti, P, g) + c(ti,N, g)
(2)
pti(??g) =
c(ti,N, g)
c(ti, P, g) + c(ti,N, g)
(3)
We introduce a novel metric ?p+ti to measure po-
larity change across genders. For every subjective
term ti we want to maximize the difference5:
?p+ti = ?pti(+?F ) ? pti(+?M)?
s.t.
RRRRRRRRRRRR
1 ?
tfsubjti (F )
tfsubjti (M)
RRRRRRRRRRRR
? ?, tfsubjti (M) ? 0, (4)
where p(+?F ) and p(+?M) are probabilities that
term ti is positive for females and males respec-
tively; tfsubjti (F ) and tf
subj
ti (M) are correspond-
ing term frequencies (if tfsubjti (F ) > tf
subj
ti (M) the
fraction is flipped); ? is a threshold that controls
the level of term frequency similarity6. The terms
in which polarity is most strongly gender-influenced
are those with ?? 0 and ?p+ti ? 1.
Table 4 shows a sample of the most strongly
gender-influenced terms from the initial LI and the
bootstrapped LB lexicons for all languages. A plus
(+) means that the term tends to be used positively
by women and minus (?) means that the term tends
to be used positively by men. For instance, in En-
glish we found that perfecting is used with negative
polarity by male users but with positive polarity by
female users; the term dogfighting has negative po-
larity for women but positive polarity for men.
4.3 Hashtags
People may also express positive or negative senti-
ment in their tweets using hashtags. From our bal-
anced samples of 800K tweets for each language,
we extracted 611, 879, and 71 unique hashtags for
English, Spanish, and Russian, respectively. As we
did for terms in the previous section, we evaluated
the subjective use of the hashtags. Some of these are
clearly expressing sentiment (#horror), while others
seem to be topics that people are frequently opinion-
ated about (#baseball, #latingrammy, #spartak).
5One can also maximize ?p?ti = ?pti(??F ) ? pti(??M)?.
6? = 0 means term frequencies are identical for both gen-
ders; ?? 1 indicates increasing gender divergence.
1819
English Initial Terms LEI ?p
+ ? English Bootstrapped Terms LEB ?p
+ ?
perfecting + 0.7 0.2 pleaseeeeee + 0.7 0.0
weakened + 0.1 0.0 adorably + 0.6 0.4
saddened ? 0.1 0.0 creatively ? 0.6 0.5
misbehaving ? 0.4 0.0 dogfighting ? 0.7 0.5
glorifying ? 0.7 0.5 overdressed ? 1.0 0.3
Spanish Initial Terms LSI Spanish Bootstrapped Terms L
S
B
fiasco (fiasco) + 0.7 0.3 cafe?na (caffeine) + 0.7 0.5
triunfar (succeed) + 0.7 0.0 claro (clear) + 0.7 0.3
inconsciente (unconscious) ? 0.6 0.2 cancio (dog) ? 0.3 0.3
horroriza (horrifies) ? 0.7 0.3 llevara (take) ? 0.8 0.3
groseramente (rudely) ? 0.7 0.3 recomendarlo (recommend) ? 1.0 0.0
Russian Initial Terms LRI Russian Bootstrapped Terms L
R
B
?????????? (magical) + 0.7 0.3 ???????? (dream!) + 0.7 0.3
???????????? (sensational) + 0.7 0.3 ???????? (dancing) + 0.7 0.3
????????? (adorable) ? 0.7 0.0 ?????? (complicated) ? 1.0 0.0
????????? (temptation) ? 0.7 0.3 ??????????? (young) ? 1.0 0.0
??????????? (deserve) ? 1.0 0.0 ??????? (achieve) ? 1.0 0.0
Table 4: Sample of subjective terms sorted by ?p+ to show lexical differences and polarity change across genders
(module is not applied as defined in Eq.1 to demonstrate the polarity change direction).
English ?p+ ? Spanish ?p+ ? Russian ?p+ ?
#parenting + 0.7 0.0 #rafaelnarro (politician) + 1.0 0.0 #????? (advise) + 1.0 0.0
#vegas ? 0.2 0.8 #amores (loves) + 0.2 1.0 #ukrlaw + 1.0 1.0
#horror ? 0.6 0.7 #britneyspears + 0.1 0.3 #spartak (soccer team) ? 0.7 0.9
#baseball ? 0.6 0.9 #latingrammy ? 0.5 0.1 #??? (dreams) ? 1.0 0.0
#wolframalpha ? 0.7 1.0 #metallica (music band) ? 0.5 0.8 #iphones ? 1.0 1.0
Table 5: Hashtag examples with opposite polarity across genders for English, Spanish, and Russian.
Table 5 gives the hashtags, correlated with sub-
jective language, that are most strongly gender-
influenced. Analogously to ?p+ values in Table 4, a
plus (+) means the hashtag is more likely to be used
positively by women, and a minus (?) means the
hashtag is more likely to be used positively by men.
For example, in English we found that male users
tend to express positive sentiment in tweets men-
tioning #baseball, while women tend to be nega-
tive about this hashtag. The opposite is true for the
hashtag #parenting.
4.4 Emoticons
We investigate how emoticons are used differently
by men and women in social media following the
work by (Bamman et al, 2012). For that we rely on
the lists of emoticons from Wikipedia7 and present
the cross-cultural and gender emoticon differences
in Figure 3. The frequency of each emoticon is given
7List of emoticons from Wikipedia http://en.
wikipedia.org/wiki/List_of_emoticons
on the right of each language chart, with probability
of use by a male user in that language given on the
x-axis. The top 8 emoticons are the same across lan-
guages and sorted by English frequency.
We found that emoticons in English data are used
more overall by female users, which is consistent
with previous findings in Schnoebelen?s work.8 In
addition, we found that some emoticons like :-)
(smile face) and :-o (surprised) are used equally by
both genders, at least in Twitter. When comparing
English emoticon usage to other languages, there are
some similarities, but also some clear differences. In
Spanish data, several emoticons are more likely to be
used by male than by female users, e.g., :-o (sur-
prised) and :-& (tongue-tied), and the difference in
probability of use by males and females is greater
for the emoticons, as compared to the same emoti-
cons for English. Interestingly, in Russian Twitter
8Language and emotion (talks, essays and reading notes)
www.stanford.edu/~tylers/emotions.shtml
1820
p(Male|Emoticon)
0.0 0.2 0.4 0.6 0.8 1.0
 34.4K
 8.7K 
 4.1K 
 2.7K 
 0.9K 
 0.7K 
 0.4K 
 0.1K 
 0.1K 
 0.1K 
:)  
:(  
:-)  
:-&  
:-(  
:[  
:-/  
8)  
()  
:-o  
p(Male|Emoticon)
0.0 0.2 0.4 0.6 0.8 1.0
 19.1K
 9.5K 
 1.5K 
 0.1K 
 0.3K 
 0.3K 
 0.1K 
 1.5K 
 0.1K 
 0.1K 
:)  
:(  
:-)  
:-&  
:-(  
:[  
:-/  
8)  
%)  
:-o  
p(Male|Emoticon)
0.0 0.2 0.4 0.6 0.8 1.0
 41.5K
 4.5K 
 4.6K 
 0.4K 
 0.4K 
 0.1K 
 0.1K 
 0.4K 
 0.4K 
 0.1K 
:)  
:(  
:-)  
:-&  
:-(  
:[  
:-/  
8)  
%)  
()  
Figure 3: Probability of gender and emoticons for English, Spanish and Russian (from left to right).
data emoticons tend to be used more or equally by
male users rather than female users.
5 Experiments
The previous section showed that there are gender
differences in the use of subjective language, hash-
tags, and emoticons in Twitter. We aim leverage
these differences to improve subjectivity and po-
larity classification for the informal, creative and
dynamically changing multilingual Twitter data.9
For that we conduct experiments using gender-
independent GInd and gender-dependent GDep
features and compare the results to evaluate the in-
fluence of gender on sentiment classification.
We experiment with two classification ap-
proaches: (I) rule-based classifier which uses only
subjective terms from the lexicons designed to verify
if the gender differences in subjective language cre-
ate enough of a signal to influence sentiment classifi-
cation; (II) state-of-the-art supervised models which
rely on lexical features as well as lexicon set-count
features.10,11 Moreover, to show that the gender-
9For polarity classification we distinguish between positive
and negative instances, which is the approach typically reported
in the literature for recognizing polarity (Velikovich et al, 2010;
Yessenalina and Cardie, 2011; Taboada et al, 2011)
10A set-count feature is a count of the number of instances
from a set of terms that appears in a tweet.
11We also experimented with repeated punctuation (!!, ??)
and letters (nooo, reealy), which are often used in sentiment
classification in social media. However, we found these features
sentiment signal can be learned by more than one
classifier we apply a variety of classifiers imple-
mented in Weka (Hall et al, 2009). For that we do
10-fold cross validation over English, Spanish, and
Russian test data (ETEST, STEST and RTEST) la-
beled with subjectivity (pos, neg, both vs. neut) and
polarity (pos vs. neg) as described in Section 3.
5.1 Models
For the rule-basedGIndRBsubj classifier, tweets are la-
beled as subjective or neutral as follows:
GIndRBsubj = {
1 if w? ? f? ? 0.5,
0 otherwise
(5)
where w? ? f? stands for weighted set features, e.g.,
terms from LI only, emoticons E, or different part-
of-speech tags (POS) from LB weighted using w =
p(subj) = p(subj?M) + p(subj?F ) subjectivity
score as shown in Eq.1. We experiment with the
POS tags to show the contribution of each POS to
sentiment classification.
Similarly, for the rule-based GIndRBpol classifier,
tweets are labeled as positive or negative:
GIndRBpol = {
1 if w?+ ? f?+ ? w?? ? f??,
0 otherwise
(6)
where f?+, f?? are feature sets that include only posi-
tive and negative features fromLI orLB;w+ andw?
to be noisy and adding them decreased performance.
1821
0.6 0.7 0.8 0.9
0.62
0.64
0.66
0.68
0.70
Recall
Preci
sion
+E
+A
+R
+V
+N
L_I
+E
+A
+R
+V
+N
L_I
(a) Rule-based subjectivity
0.65 0.70 0.75 0.80 0.85 0.90
0.65
0.70
0.75
0.80
0.85
Recall
Preci
sion
L_I
+E
+A
+R
+V
+N
L_I
+E
+A
+R
+V
+N
(b) Rule-based polarity
BL R N B AB RF J48 SVM
C lassifiers
F-me
asure
0.55
0.60
0.65
0.70
0.75
0.80
0.85
GIn d Subj
AN D
GDepSubj
AN D
GIndPol
AN D
GDepPol
AN D
(c) SL subjectivity and polarity
Figure 4: Rule-based (RB) and Supervised Learning (SL) sentiment classification results for English. LI - the initial
lexicon, E - emoticons, A,R,V,N are adjectives, adverbs, verbs, nouns from LB .
are positive and negative polarity scores estimated
using Eq.2 - 3 such as: w+ = p(+?M) + p(+?F ) and
w? = p(??M) + p(??F ).
The gender-dependent rule-based classifiers are
defined in a similar way. Specifically, f? is replaced
by f?M and f?F in Eq.5 and f??, f?+ are replaced
by f?M?, f?F? and f?M+, f?F+ respectively in Eq.6.
We learn subjectivity s? and polarity p? score vectors
using Eq.1-3. The difference between GInd and
GDep models is that GInd scores w?, w?+ and w??
are not conditioned on gender.
For gender-independent classification using su-
pervised models, we build feature vectors using lex-
ical features V represented as term frequencies, to-
gether with set-count features from the lexicons:
f?GIndsubj = [LI , LB,E, V ];
f?GIndpol = [L
+
I , L
+
B,E
+, L?I , L
?
B,E
?, V ].
Finally, for gender-dependent supervised models,
we try different feature combinations. (A) We ex-
tract set-count features for gender-dependent subjec-
tive terms from LI , LB, and E jointly:
f?GDep?Jsubj = [L
M
I , L
M
B ,E
M , LFI , L
F
B,E
F , V ];
f?Dep?Jpol = [L
M+
I , L
M+
B ,E
M+, LF+I , L
F+
B ,E
F+
LM?I , L
M?
B ,E
M?, LF?I , L
F?
B ,E
F?, V ].
(B) We extract disjoint (prefixed) gender-specific
features (in addition to lexical features V ) by rely-
ing only on female set-count features when classify-
ing female tweets; and only male set-count features
for male tweets. We refer to the joint features as
GInd?J andGDep?J , and to the disjoint features
GInd ?D and GDep ?D.
5.2 Results
Figures 4a and 4b show performance improvements
for subjectivity and polarity classification under the
rule-based approach when taking into account gen-
der. The left figure shows precision-recall curves
for subjective vs. neutral classification, and the mid-
dle figure shows precision-recall curves for positive
vs. negative classification. We measure performance
starting with features from LI , and then incremen-
tally add emoticon features E and features from LB
one part of speech at a time to show the contribution
of each part of speech for sentiment classification.12
This experiment shows that there is a clear improve-
ment for the models parameterized with gender, at
least for the simple, rule-based model.
For the supervised models we experiment with
a variety of learners for English to show that gen-
der differences in subjective language improve sen-
timent classification for many learning algorithms.
We present the results in Figure 4c. For subjectiv-
ity classification, Support Vector Machines (SVM),
Naive Bayes (NB) and Bayesian Logistic Regres-
sion (BLR) achieve the best results, with improve-
ments in F-measure ranging from 0.5 - 5%. The po-
larity classifiers overall achieve much higher scores,
with improvements for GDep features ranging from
1-2%. BLR with Gaussian prior is the top scorer
12POS from the Twitter POSTagger (Gimpel et al, 2011).
1822
P R F A Arand P R F A Arand
English subj vs. neutral p(subj)=0.57 English pos vs. neg p(pos)=0.63
GIndLR 0.62 0.58 0.60 0.66 ? 0.78 0.83 0.80 0.71 ?
GDep ? J 0.64 0.62 0.63 0.68 0.66 0.80 0.83 0.82 0.73 0.70
?R,% +3.23 +6.90 +5.00 +3.03 3.03? +2.56 0.00 +2.50 +2.82 4.29?
GIndSVM 0.66 0.70 0.68 0.72 ? 0.79 0.86 0.82 0.77 ?
GDep ?D 0.66 0.71 0.68 0.72 0.70 0.80 0.87 0.83 0.78 0.76
?R,% ?0.45 +0.71 0.00 ?0.14 2.85? +0.38 +0.23 +0.24 +0.41 2.63?
Spanish subj vs. neutral p(subj)=0.40 Spanish pos vs. neg p(pos)=0.45
GIndLL 0.67 0.71 0.68 0.61 ? 0.71 0.63 0.67 0.71 ?
GDep ? J 0.67 0.72 0.69 0.62 0.61 0.72 0.65 0.68 0.71 0.68
?R,% 0.00 +1.40 +0.58 +0.73 1.64? +2.53 +3.17 +1.49 0.00 4.41?
GIndSVM 0.68 0.79 0.73 0.65 ? 0.66 0.65 0.65 0.69 ?
GDep ?D 0.68 0.79 0.73 0.66 0.65 0.68 0.67 0.67 0.71 0.68
?R,% +0.35 +0.21 +0.26 +0.54 1.54? +2.43 +2.44 +2.51 +2.08 4.41?
Russian subj vs. neutral p(subj)=0.51 Russian pos vs. neg p(pos)=0.58
GIndLR 0.66 0.68 0.67 0.67 ? 0.66 0.72 0.69 0.62 ?
GDep ? J 0.66 0.69 0.68 0.67 0.66 0.68 0.73 0.70 0.64 0.63
?R,% 0.00 +1.47 +0.75 0.00 1.51? +3.03 +1.39 +1.45 +3.23 1.58?
GIndSVM 0.67 0.75 0.71 0.70 ? 0.64 0.73 0.68 0.62 ?
GDep ?D 0.67 0.76 0.71 0.70 0.69 0.65 0.74 0.69 0.63 0.62
?R,% ?0.30 +1.46 +0.56 +0.14 1.44? +0.93 +1.92 +1.46 +1.49 1.61?
Table 6: Sentiment classification results obtained using gender-dependent and gender-independent joint and disjoint
features for Logistic Regression (LR) and SVM models.
for polarity classification with an F-measure of 82%.
We test our results for statistical significance us-
ing McNemar?s Chi-squared test (p-value < 0.01) as
suggested by Dietterich (1998). Only three classi-
fiers, J48, AdaBoostM1 (AB) and Random Forest
(RF) do not always show significant improvements
for GDep features over GInd features. However,
for the majority of classifiers, GDep models outper-
formGIndmodels for both tasks, demonstrating the
robustness of GDep features for sentiment analysis.
In Table 6 we report results for subjectivity and
polarity classification using the best performing
classifiers (as shown in Figure 4c) :
- Logistic Regression (LR) (Genkin et al, 2007)
for GInd ? J and GDep ? J models.
- SVM model with radial-based kernel for
GInd ? D and GDep ? D models. We use
LibSVM implementation (EL-Manzalawy and
Honavar, 2005).
Each ?R(%) row shows the relative percent im-
provements in terms of precision P , recall R, F-
measure F and accuracy A for GDep compared to
GInd models. Our results show that differences in
subjective language across genders can be exploited
to improve sentiment analysis, not only for English
but for multiple languages. For Spanish and Russian
results are lower for subjectivity classification, we
suspect, because lexical features V are already in-
flected for gender and set-count features are down-
weighted by the classifier. For polarity classifica-
tion, on the other hand, gender-dependent features
provide consistent, significant improvements (1.5-
2.5%) across all languages.
As a reality check, Table 6 also reports accuracies
(in Arand columns) for experiments that use random
permutations of male and female subjective terms,
which are then encoded as gender-dependent set-
count features as before. We found that all gender-
dependent models, GDep ? J and GDep ?D, out-
performed their random equivalents for both subjec-
tivity and polarity classification (as reflected by rel-
ative accuracy decrease ? forArand compared toA).
These results further confirm the existence of gen-
der bias in subjective language for any of our three
languages and its importance for sentiment analysis.
Finally, we check whether encoding gender as
a binary feature would be sufficient to improve
sentiment classification. For that we encode fea-
1823
English Spanish Russian
P R P R P R
(a) 0.73 0.93 0.68 0.63 0.66 0.74
(b) 0.72 0.94 0.69 0.64 0.66 0.74
(c) 0.78 0.83 0.71 0.63 0.66 0.72
(d) 0.69 0.93 0.71 0.62 0.65 0.76
(e) 0.80 0.83 0.72 0.65 0.68 0.73
Table 7: Precision and recall results for polarity classifi-
cation: encoding gender as a binary feature vs. gender-
dependent features GDep ? J .
tures such as: (a) unigram term frequencies V , (b)
term frequencies and gender binary V +GBin, (c)
gender-independent GInd, (d) gender-independent
and gender binary GBin + GInd, and (e) gender-
dependent GDep ? J . We train logistic-regression
model for polarity classification and report precision
and recall results in Table 7. We observe that includ-
ing gender as a binary feature does not yield signif-
icant improvements compared to GDep ? J for all
three languages.
6 Conclusions
We presented a qualitative and empirical study that
analyses substantial and interesting differences in
subjective language between male and female users
in Twitter, including hashtag and emoticon usage
across cultures. We showed that incorporating au-
thor gender as a model component can significantly
improve subjectivity and polarity classification for
English (2.5% and 5%), Spanish (1.5% and 1%) and
Russian (1.5% and 1%). In future work we plan to
develop new models for joint modeling of personal-
ized sentiment, user demographics e.g., age and user
preferences e.g., political favorites in social media.
Acknowledgments
The authors thank the anonymous reviewers for
helpful comments and suggestions.
References
Muhammad Abdul-Mageed, Mona T. Diab, and Mo-
hammed Korayem. 2011. Subjectivity and sentiment
analysis of modern standard Arabic. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies: short papers - Volume 2, pages 587?591.
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow,
and Rebecca Passonneau. 2011. Sentiment analysis
of twitter data. In Proceedings of the Workshop on
Languages in Social Media (LSM?11), pages 30?38.
Shlomo Argamon, Moshe Koppel, James W. Pen-
nebaker, and Jonathan Schler. 2007. Min-
ing the blogosphere: Age, gender and the va-
rieties of self-expression. First Monday, 12(9).
http://www.firstmonday.org/ojs/index.php/fm/article/
view/2003/1878.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining. In
Proceedings of the Seventh International Conference
on Language Resources and Evaluation (LREC?10),
pages 2200?2204.
David Bamman, Jacob Eisenstein, and Tyler Schnoebe-
len. 2012. Gender in Twitter: styles, stances, and so-
cial networks. Computing Research Repository.
Carmen Banea, Rada Mihalcea, and Janyce Wiebe. 2008.
A bootstrapping method for building subjectivity lex-
icons for languages with scarce resources. In Pro-
ceedings of the Sixth International Conference on Lan-
guage Resources and Evaluation (LREC?08), pages
2764?2767.
Luciano Barbosa and Junlan Feng. 2010. Robust sen-
timent detection on Twitter from biased and noisy
data. In Proceedings of the 23rd International Con-
ference on Computational Linguistics (COLING?10),
pages 36?44.
Shane Bergsma, Paul McNamee, Mossaab Bagdouri,
Clayton Fink, and Theresa Wilson. 2012. Language
identification for creating language-specific Twitter
collections. In Proceedings of the Second Workshop
on Language in Social Media (LSM?12), pages 65?74.
Adam Bermingham and Alan F. Smeaton. 2010. Clas-
sifying sentiment in microblogs: Is brevity an advan-
tage? In Proceedings of the 19th ACM International
Conference on Information and Knowledge Manage-
ment (CIKM?10), pages 1833?1836.
Albert Bifet and Eibe Frank. 2010. Sentiment knowl-
edge discovery in Twitter streaming data. In Proceed-
ings of the 13th International Conference on Discovery
Science (DS?10), pages 1?15.
Bonka Boneva, Robert Kraut, and David Frohlich. 2001.
Using email for personal relationships: The differ-
ence gender makes. American Behavioral Scientist,
45(3):530?549.
John D. Burger, John C. Henderson, George Kim, and
Guido Zarrella. 2011. Discriminating gender on Twit-
ter. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1301?1309.
1824
Pedro Henrique Calais Guerra, Adriano Veloso, Wagner
Meira Jr, and Virg?lio Almeida. 2011. From bias to
opinion: a transfer-learning approach to real-time sen-
timent analysis. In Proceedings of the17th Interna-
tional Conference on Knowledge Discovery and Data
Mining (KDD?11), pages 150?158.
Lu Chen, Wenbo Wang, Meenakshi Nagarajan, Shaojun
Wang, and Amit P. Sheth. 2012. Extracting diverse
sentiment expressions with target-dependent polarity
from Twitter. In Proceedings of the Sixth Interna-
tional AAAI Conference on Weblogs and Social Media
(ICWSM?12), pages 50?57.
Ilia Chetviorkin and Natalia V. Loukachevitch. 2012.
Extraction of Russian sentiment lexicon for prod-
uct meta-domain. In Proceedings of the 25rd In-
ternational Conference on Computational Linguistics
(COLING?12), pages 593?610.
Christopher Cieri, David Miller, and Kevin Walker.
2004. The Fisher corpus: a resource for the next gen-
erations of speech-to-text. In Proceedings of the 4th
International Conference on Language Resources and
Evaluation (LREC?04), pages 69?71.
Simon Clematide and Manfred Klenner. 2010. Eval-
uation and extension of a polarity lexicon for Ger-
man. In Proceedings of the Workshop on Computa-
tional Approaches to Subjectivity and Sentiment Anal-
ysis (WASSA?10), pages 7?13.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using Twitter hashtags
and smileys. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics
(COLING?10), pages 241?249.
Thomas G. Dietterich. 1998. Approximate statistical
tests for comparing supervised classification learning
algorithms. Neural Computation, 10(7):1895?1923.
Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model for ge-
ographic lexical variation. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP?10), pages 1277?1287.
Yasser EL-Manzalawy and Vasant Honavar, 2005.
WLSVM: Integrating LibSVM into Weka Environment.
http://www.cs.iastate.edu/ yasser/wlsvm.
Teng-Kai Fan and Chia-Hui Chang. 2009. Sentiment-
oriented contextual advertising. Advances in Informa-
tion Retrieval, 5478:202?215.
Nikesh Garera and David Yarowsky. 2009. Modeling la-
tent biographic attributes in conversational genres. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 710?718.
David Gefen and Catherine M. Ridings. 2005. If you
spoke as she does, sir, instead of the way you do: a so-
ciolinguistics perspective of gender differences in vir-
tual communities. SIGMIS Database, 36(2):78?92.
Alexander Genkin, David D. Lewis, and David Madigan.
2007. Large-scale Bayesian logistic regression for text
categorization. Technometrics, 49:291?304.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for
Twitter: annotation, features, and experiments. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies: short papers - Volume 2, pages 42?47.
John J. Godfrey, Edward C. Holliman, and Jane Mc-
Daniel. 1992. Switchboard: telephone speech corpus
for research and development. In Proceedings of the
IEEE International Conference on Acoustics, Speech,
and Signal Processing (ICASSP?92), pages 517?520.
Sumit Goswami, Sudeshna Sarkar, and Mayur Rustagi.
2009. Stylometric analysis of bloggers age and gen-
der. In Proceedings of AAAI Conference on Weblogs
and Social Media, pages 214?217.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
SIGKDD Exploratory Newsletter, 11(1):10?18.
Janet Holmes and Miriam Meyerhoff. 2004. The Hand-
book of Language and Gender. Blackwell Publishing.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and Tiejun
Zhao. 2011. Target-dependent Twitter sentiment clas-
sification. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies, pages 151?160.
Nobuhiro Kaji and Masaru Kitsuregawa. 2007. Building
lexicon for sentiment analysis from massive collection
of HTML documents. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP?07), pages 1075?1083.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the OMG! In Proceedings of the Fifth In-
ternational AAAI Conference on Weblogs and Social
Media (ICWSM?11), pages 538?541.
Guangxia Li, Steven Hoi, Kuiyu Chang, and Ramesh
Jain. 2010. Micro-blogging sentiment detection
by collaborative online learning. In Proceedings of
IEEE 10th International Conference on Data Mining
(ICDM?10), pages 893?898.
Hao Li, Yu Chen, Heng Ji, Smaranda Muresan, and
Dequan Zheng. 2012. Combining social cognitive
theories with linguistic features for multi-genre senti-
ment analysis. In Proceedings of the 26th Pacific Asia
1825
Conference on Language,Information and Computa-
tion (PACLIC?12), pages 27?136.
Ronald Macaulay. 2006. Pure grammaticalization: The
development of a teenage intensifier. Language Varia-
tion and Change, 18(03):267?283.
Rada Mihalcea, Carmen Banea, and Janyce Wiebe. 2012.
Multilingual subjectivity and sentiment analysis. In
Proceedings of the Association for Computational Lin-
guistics (ACL?12).
George A. Miller. 1995. Wordnet: a lexical database for
english. Communications of the ACM, 38(11):39?41.
Saif Mohammad and Tony Yang. 2011. Tracking senti-
ment in mail: How genders differ on emotional axes.
In Proceedings of the 2nd Workshop on Computa-
tional Approaches to Subjectivity and Sentiment Anal-
ysis (WASSA?11), pages 70?79.
Arjun Mukherjee and Bing Liu. 2010. Improving gen-
der classification of blog authors. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP?10), pages 207?217.
Brendan O?Connor, Jacob Eisenstein, Eric P. Xing, and
Noah A. Smith. 2010. A mixture model of de-
mographic lexical variation. In Proceedings of NIPS
Workshop on Machine Learning in Computational So-
cial Science, pages 1?7.
Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T. Han-
cock. 2011. Finding deceptive opinion spam by any
stretch of the imagination. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
309?319.
Jahna Otterbacher. 2010. Inferring gender of movie re-
viewers: exploiting writing style, content and meta-
data. In Proceedings of the 19th ACM International
Conference on Information and Knowledge Manage-
ment (CIKM?10), pages 369?378.
Alexander Pak and Patrick Paroubek. 2010. Twitter as a
corpus for sentiment analysis and opinion mining. In
Proceedings of the Seventh International Conference
on Language Resources and Evaluation (LREC?10),
pages 1320?1326.
Veronica Perez-Rosas, Carmen Banea, and Rada Mihal-
cea. 2012. Learning sentiment lexicons in Spanish.
In Proceedings of the 8th International Conference
on Language Resources and Evaluation (LREC?12),
pages 3077?3081.
Rosalind W. Picard. 1997. Affective computing. MIT
Press.
Delip Rao and Deepak Ravichandran. 2009. Semi-
supervised polarity lexicon induction. In Proceedings
of the 12th Conference of the European Chapter of the
Association for Computational Linguistics (EACL?09),
pages 675?682.
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in Twitter. In Proceedings of the Work-
shop on Search and Mining User-generated Contents
(SMUC?10), pages 37?44.
Philip Resnik. 2013. Getting real(-time) with live
polling. http://vimeo.com/68210812.
Ellen Riloff and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP?03), pages 105?
112.
Harold Schiffman. 2002. Bibliography of gender and
language. http://ccat.sas.upenn.edu/ haroldfs/popcult/
bibliogs/gender/genbib.htm.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast?but is it
good?: evaluating non-expert annotations for natural
language tasks. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP?08), pages 254?263.
Will Styler. 2011. The EnronSent Corpus.
Technical report, University of Colorado
at Boulder Institute of Cognitive Science.
http://verbs.colorado.edu/enronsent/.
Jane Sunderland, Ren-Feng Duann, and Paul
Bake. 2002. Gender and genre bibliography.
www.ling.lancs.ac.uk/pubs/clsl/clsl122.pdf.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly
Voll, and Manfred Stede. 2011. Lexicon-based meth-
ods for sentiment analysis. Computational Linguis-
tics, 37(2):267?307.
Sali A. Tagliamonte. 2006. Analysing Sociolinguistic
Variation. Cambridge University Press, 1st. Edition.
Chenhao Tan, Lillian Lee, Jie Tang, Long Jiang, Ming
Zhou, and Ping Li. 2011. User-level sentiment anal-
ysis incorporating social networks. In Proceedings of
the 17th International Conference on Knowledge Dis-
covery and Data Mining (KDD?11), pages 1397?1405.
Mike Thelwall, David Wilkinson, and Sukhvinder Uppal.
2010. Data mining emotion in social network com-
munication: Gender differences in MySpace. Journal
of the American Society for Information Science and
Technology, 61(1):190?199.
Peter D. Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the 40th Annual
Meeting on Association for Computational Linguistics
(ACL?02), pages 417?424.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Han-
nan, and Ryan McDonald. 2010. The viability
of web-derived polarity lexicons. In Proceedings of
1826
the Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL?10), pages 777?785.
Svitlana Volkova, Theresa Wilson, and David Yarowsky.
2013. Exploring sentiment in social media: Boot-
strapping subjectivity clues from multilingual Twitter
streams. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguistics
(ACL?13), pages 505?510.
Xiaolong Wang, Furu Wei, Xiaohua Liu, Ming Zhou, and
Ming Zhang. 2011. Topic sentiment analysis in Twit-
ter: A graph-based hashtag sentiment classification ap-
proach. In Proceedings of the 20th ACM International
Conference on Information and Knowledge Manage-
ment (CIKM?11), pages 1031?1040.
Janyce Wiebe. 2000. Learning subjective adjectives
from corpora. In Proceedings of the Seventeenth
National Conference on Artificial Intelligence and
Twelfth Conference on Innovative Applications of Ar-
tificial Intelligence (AAAI?00, pages 735?740.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP?05), pages 347?354.
Ainur Yessenalina and Claire Cardie. 2011. Composi-
tional matrix-space models for sentiment analysis. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP?11), pages
172?182.
1827
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 130?140,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Toward Statistical Machine Translation without Parallel Corpora
Alexandre Klementiev Ann Irvine Chris Callison-Burch David Yarowsky
Center for Language and Speech Processing
Johns Hopkins University
Abstract
We estimate the parameters of a phrase-
based statistical machine translation sys-
tem from monolingual corpora instead of a
bilingual parallel corpus. We extend exist-
ing research on bilingual lexicon induction
to estimate both lexical and phrasal trans-
lation probabilities for MT-scale phrase-
tables. We propose a novel algorithm to es-
timate reordering probabilities from mono-
lingual data. We report translation results
for an end-to-end translation system us-
ing these monolingual features alone. Our
method only requires monolingual corpora
in source and target languages, a small
bilingual dictionary, and a small bitext for
tuning feature weights. In this paper, we ex-
amine an idealization where a phrase-table
is given. We examine the degradation in
translation performance when bilingually
estimated translation probabilities are re-
moved and show that 80%+ of the loss can
be recovered with monolingually estimated
features alone. We further show that our
monolingual features add 1.5 BLEU points
when combined with standard bilingually
estimated phrase table features.
1 Introduction
The parameters of statistical models of transla-
tion are typically estimated from large bilingual
parallel corpora (Brown et al 1993). However,
these resources are not available for most lan-
guage pairs, and they are expensive to produce in
quantities sufficient for building a good transla-
tion system (Germann, 2001). We attempt an en-
tirely different approach; we use cheap and plen-
tiful monolingual resources to induce an end-to-
end statistical machine translation system. In par-
ticular, we extend the long line of work on in-
ducing translation lexicons (beginning with Rapp
(1995)) and propose to use multiple independent
cues present in monolingual texts to estimate lex-
ical and phrasal translation probabilities for large,
MT-scale phrase-tables. We then introduce a
novel algorithm to estimate reordering features
from monolingual data alone, and we report the
performance of a phrase-based statistical model
(Koehn et al 2003) estimated using these mono-
lingual features.
Most of the prior work on lexicon induction
is motivated by the idea that it could be applied
to machine translation but stops short of actu-
ally doing so. Lexicon induction holds the po-
tential to create machine translation systems for
languages which do not have extensive parallel
corpora. Training would only require two large
monolingual corpora and a small bilingual dictio-
nary, if one is available. The idea is that intrin-
sic properties of monolingual data (possibly along
with a handful of bilingual pairs to act as exam-
ple mappings) can provide independent but infor-
mative cues to learn translations because words
(and phrases) behave similarly across languages.
This work is the first attempt to extend and apply
these ideas to an end-to-end machine translation
pipeline. While we make an explicit assumption
that a table of phrasal translations is given a priori,
we induce every other parameter of a full phrase-
based translation system from monolingual data
alone. The contributions of this work are:
? In Section 2.2 we analyze the challenges
of using bilingual lexicon induction for sta-
tistical MT (performance on low frequency
items, and moving from words to phrases).
? In Sections 3.1 and 3.2 we use multiple cues
present in monolingual data to estimate lexi-
cal and phrasal translation scores.
? In Section 3.3 we propose a novel algo-
rithm for estimating phrase reordering fea-
tures from monolingual texts.
? Finally, in Section 5 we systematically drop
feature functions from a phrase table and
then replace them with monolingually es-
timated equivalents, reporting end-to-end
translation quality.
130
2 Background
We begin with a brief overview of the stan-
dard phrase-based statistical machine translation
model. Here, we define the parameters which
we later replace with monolingual alternatives.
We continue with a discussion of bilingual lex-
icon induction; we extend these methods to es-
timate the monolingual parameters in Section 3.
This approach allows us to replace expensive/rare
bilingual parallel training data with two large
monolingual corpora, a small bilingual dictionary,
and ?2,000 sentence bilingual development set,
which are comparatively plentiful/inexpensive.
2.1 Parameters of phrase-based SMT
Statistical machine translation (SMT) was first
formulated as a series of probabilistic mod-
els that learn word-to-word correspondences
from sentence-aligned bilingual parallel corpora
(Brown et al 1993). Current methods, includ-
ing phrase-based (Och, 2002; Koehn et al 2003)
and hierarchical models (Chiang, 2005), typically
start by word-aligning a bilingual parallel cor-
pus (Och and Ney, 2003). They extract multi-
word phrases that are consistent with the Viterbi
word alignments and use these phrases to build
new translations. A variety of parameters are es-
timated using the bitexts. Here we review the pa-
rameters of the standard phrase-based translation
model (Koehn et al 2007). Later we will show
how to estimate them using monolingual texts in-
stead. These parameters are:
? Phrase pairs. Phrase extraction heuristics
(Venugopal et al 2003; Tillmann, 2003;
Och and Ney, 2004) produce a set of phrase
pairs (e, f) that are consistent with the word
alignments. In this paper we assume that the
phrase pairs are given (without any scores),
and we induce every other parameter of the
phrase-based model from monolingual data.
? Phrase translation probabilities. Each
phrase pair has a list of associated fea-
ture functions (FFs). These include phrase
translation probabilities, ?(e|f) and ?(f |e),
which are typically calculated via maximum
likelihood estimation.
? Lexical weighting. Since MLE overestimates
? for phrase pairs with sparse counts, lexi-
cal weighting FFs are used to smooth. Aver-
How
much
should
you
charge
for
your
W
i
e
v
i
e
l
s
o
l
l
t
e
m
a
n
a
u
f
r
g
u
n
d
s
e
i
n
e
s
P
r
o
fi
l
s
i
n
F
a
c
e
b
o
o
k
v
e
r
d
i
e
n
e
n
Facebook
profile
s
d
m
m
m
d
d
Figure 1: The reordering probabilities from the phrase-
based models are estimated from bilingual data by cal-
culating how often in the parallel corpus a phrase pair
(f, e) is orientated with the preceding phrase pair in
the 3 types of orientations (monotone, swapped, and
discontinuous).
age word translation probabilities, w(ei|fj),
are calculated via phrase-pair-internal word
alignments.
? Reordering model. Each phrase pair (e, f)
also has associated reordering parameters,
po(orientation|f, e), which indicate the dis-
tribution of its orientation with respect to the
previously translated phrase. Orientations
are monotone, swap, discontinuous (Tillman,
2004; Kumar and Byrne, 2004), see Figure 1.
? Other features. Other typical features are
n-gram language model scores and a phrase
penalty, which governs whether to use fewer
longer phrases or more shorter phrases.
These are not bilingually estimated, so we
can re-use them directly without modifica-
tion.
The features are combined in a log linear model,
and their weights are set through minimum error
rate training (Och, 2003). We use the same log
linear formulation and MERT but propose alterna-
tives derived directly from monolingual data for
all parameters except for the phrase pairs them-
selves. Our pipeline still requires a small bitext of
approximately 2,000 sentences to use as a devel-
opment set for MERT parameter tuning.
131
2.2 Bilingual lexicon induction for SMT
Bilingual lexicon induction describes the class of
algorithms that attempt to learn translations from
monolingual corpora. Rapp (1995) was the first
to propose using non-parallel texts to learn the
translations of words. Using large, unrelated En-
glish and German corpora (with 163m and 135m
words) and a small German-English bilingual dic-
tionary (with 22k entires), Rapp (1999) demon-
strated that reasonably accurate translations could
be learned for 100 German nouns that were not
contained in the seed bilingual dictionary. His al-
gorithm worked by (1) building a context vector
representing an unknown German word by count-
ing its co-occurrence with all the other words
in the German monolingual corpus, (2) project-
ing this German vector onto the vector space of
English using the seed bilingual dictionary, (3)
calculating the similarity of this sparse projected
vector to vectors for English words that were con-
structed using the English monolingual corpus,
and (4) outputting the English words with the
highest similarity as the most likely translations.
A variety of subsequent work has extended the
original idea either by exploring different mea-
sures of vector similarity (Fung and Yee, 1998)
or by proposing other ways of measuring simi-
larity beyond co-occurence within a context win-
dow. For instance, Schafer and Yarowsky (2002)
demonstrated that word translations tend to co-
occur in time across languages. Koehn and Knight
(2002) used similarity in spelling as another kind
of cue that a pair of words may be translations of
one another. Garera et al(2009) defined context
vectors using dependency relations rather than ad-
jacent words. Bergsma and Van Durme (2011)
used the visual similarity of labeled web images
to learn translations of nouns. Additional related
work on learning translations from monolingual
corpora is discussed in Section 6.
In this paper, we apply bilingual lexicon in-
duction methods to statistical machine translation.
Given the obvious benefits of not having to rely
on scarce bilingual parallel training data, it is sur-
prising that bilingual lexicon induction has not
been used for SMT before now. There are sev-
eral open questions that make its applicability to
SMT uncertain. Previous research on bilingual
lexicon induction learned translations only for a
small number of high frequency words (e.g. 100
l
llll
lll
l
0 100 200 300 400 500 600
0
10
20
30
40
Accu
racy
, %
Corpus Frequency
l Top 1Top 10
Figure 2: Accuracy of single-word translations in-
duced using contextual similarity as a function of the
source word corpus frequency. Accuracy is the pro-
portion of the source words with at least one correct
(bilingual dictionary) translation in the top 1 and top
10 candidate lists.
nouns in Rapp (1995), 1,000 most frequent words
in Koehn and Knight (2002), or 2,000 most fre-
quent nouns in Haghighi et al(2008)). Although
previous work reported high translation accuracy,
it may be misleading to extrapolate the results to
SMT, where it is necessary to translate a much
larger set of words and phrases, including many
low frequency items.
In a preliminary study, we plotted the accuracy
of translations against the frequency of the source
words in the monolingual corpus. Figure 2 shows
the result for translations induced using contex-
tual similarity (defined in Section 3.1). Unsur-
prisingly, frequent terms have a substantially bet-
ter chance of being paired with a correct transla-
tion, with words that only occur once having a low
chance of being translated accurately.1 This prob-
lem is exacerbated when we move to multi-token
phrases. As with phrase translation features esti-
mated from parallel data, longer phrases are more
sparse, making similarity scores less reliable than
for single words.
Another impediment (not addressed in this
paper) for using lexicon induction for SMT is
the number of translations that must be learned.
Learning translations for all words in the source
language requires n2 vector comparisons, since
each word in the source language vocabulary must
1For a description of the experimental setup used to pro-
duce these translations, see Experiment 8 in Section 5.2.
132
s1
s
2
s
3
s
N-1
s
N
?
?
?
t
1
t
2
t
3
t
M-1
t
M
?
?
dict.
project
?
?
?
?
?
?
c
o
m
p
a
r
e
para crecer
to expand
activity of
economic
activity
policy
growth
foreign
economico
tasa
planeta
empleo
extranjero
policy
para crecer
(projected)
ES Context
Vector
Projected ES
Context Vector
EN Context
Vectors
Figure 3: Scoring contextual similarity of phrases:
first, contextual vectors are projected using a small
seed dictionary and then compared with the target lan-
guage candidates.
be compared against the vectors for all words in
the target language vocabulary. The size of the n2
comparisons hugely increases if we compare vec-
tors for multi-word phrases instead of just words.
In this work, we avoid this problem by assuming
that a limited set of phrase pairs is given a pri-
ori (but without scores). By limiting ourselves
to phrases in a phrase table, we vastly limit the
search space of possible translations. This is an
idealization because high quality translations are
guaranteed to be present. However, as our lesion
experiments in Section 5.1 show, a phrase table
without accurate translation probability estimates
is insufficient to produce high quality translations.
We show that lexicon induction methods can be
used to replace bilingual estimation of phrase- and
lexical-translation probabilities, making a signifi-
cant step towards SMT without parallel corpora.
3 Monolingual Parameter Estimation
We use bilingual lexicon induction methods to es-
timate the parameters of a phrase-based transla-
tion model from monolingual data. Instead of
scores estimated from bilingual parallel data, we
make use of cues present in monolingual data to
provide multiple orthogonal estimates of similar-
ity between a pair of phrases.
3.1 Phrasal similarity features
Contextual similarity. We extend the vector
space approach of Rapp (1999) to compute sim-
ilarity between phrases in the source and tar-
get languages. More formally, assume that
(s1, s2, . . . sN ) and (t1, t2, . . . tM ) are (arbitrarily
indexed) source and target vocabularies, respec-
tively. A source phrase f is represented with an
terrorist (en)terrorista (es)
Occ
urre
nces
terrorist (en)riqueza (es)
Occ
urre
nces
Time
Figure 4: Temporal histograms of the English phrase
terrorist, its Spanish translation terrorista, and riqueza
(wealth) collected from monolingual texts spanning a
13 year period. While the correct translation has a
good temporal match, the non-translation riqueza has
a distinctly different signature.
N - and target phrase e with an M -dimensional
vector (see Figure 3). The component values of
the vector representing a phrase correspond to
how often each of the words in that vocabulary
appear within a two word window on either side
of the phrase. These counts are collected using
monolingual corpora. After the values have been
computed, a contextual vector f is projected onto
the English vector space using translations in a
seed bilingual dictionary to map the component
values into their appropriate English vector posi-
tions. This sparse projected vector is compared
to the vectors representing all English phrases e.
Each phrase pair in the phrase table is assigned
a contextual similarity score c(f, e) based on the
similarity between e and the projection of f .
Various means of computing the component
values and vector similarity measures have been
proposed in literature (e.g. Rapp (1999), Fung and
Yee (1998)). Following Fung and Yee (1998), we
compute the value of the k-th component of f ?s
contextual vector as follows:
wk = nf,k ? (log(n/nk) + 1)
where nf,k and nk are the number of times sk ap-
pears in the context of f and in the entire corpus,
and n is the maximum number of occurrences of
any word in the data. Intuitively, the more fre-
quently sk appears with f and the less common
it is in the corpus in general, the higher its com-
ponent value. Similarity between two vectors is
measured as the cosine of the angle between them.
Temporal similarity. In addition to contex-
tual similarity, phrases in two languages may
133
be scored in terms of their temporal similarity
(Schafer and Yarowsky, 2002; Klementiev and
Roth, 2006; Alfonseca et al 2009). The intu-
ition is that news stories in different languages
will tend to discuss the same world events on the
same day. The frequencies of translated phrases
over time give them particular signatures that will
tend to spike on the same dates. For instance, if
the phrase asian tsunami is used frequently dur-
ing a particular time span, the Spanish transla-
tion maremoto asia?tico is likely to also be used
frequently during that time. Figure 4 illustrates
how the temporal distribution of terrorist is more
similar to Spanish terrorista than to other Span-
ish phrases. We calculate the temporal similar-
ity between a pair of phrases t(f, e) using the
method defined by Klementiev and Roth (2006).
We generate a temporal signature for each phrase
by sorting the set of (time-stamped) documents in
the monolingual corpus into a sequence of equally
sized temporal bins and then counting the number
of phrase occurrences in each bin. In our exper-
iments, we set the window size to 1 day, so the
size of temporal signatures is equal to the num-
ber of days spanned by our corpus. We use cosine
distance to compare the normalized temporal sig-
natures for a pair of phrases (f, e).
Topic similarity. Phrases and their translations
are likely to appear in articles written about the
same topic in two languages. Thus, topic or cat-
egory information associated with monolingual
data can also be used to indicate similarity be-
tween a phrase and its candidate translation. In
order to score a pair of phrases, we collect their
topic signatures by counting their occurrences in
each topic and then comparing the resulting vec-
tors. We again use the cosine similarity mea-
sure on the normalized topic signatures. In our
experiments, we use interlingual links between
Wikipedia articles to estimate topic similarity. We
treat each linked article pair as a topic and collect
counts for each phrase across all articles in its cor-
responding language. Thus, the size of a phrase
topic signature is the number of article pairs with
interlingual links in Wikipedia, and each compo-
nent contains the number of times the phrase ap-
pears in (the appropriate side of) the correspond-
ing pair. Our Wikipedia-based topic similarity
feature, w(f, e), is similar in spirit to polylingual
topic models (Mimno et al 2009), but it is scal-
able to full bilingual lexicon induction.
3.2 Lexical similarity features
In addition to the three phrase similarity features
used in our model ? c(f, e), t(f, e) and w(f, e) ?
we include four additional lexical similarity fea-
tures for each of phrase pair. The first three lex-
ical features clex(f, e), tlex(f, e) and wlex(f, e)
are the lexical equivalents of the phrase-level con-
textual, temporal and wikipedia topic similarity
scores. They score the similarity of individual
words within the phrases. To compute these
lexical similarity features, we average similarity
scores over all possible word alignments across
the two phrases. Because individual words are
more frequent than multiword phrases, the accu-
racy of clex, tlex, and wlex tends to be higher than
their phrasal equivalents (this is similar to the ef-
fect observed in Figure 2).
Orthographic / phonetic similarity. The final
lexical similarity feature that we incorporate is
o(f, e), which measures the orthographic similar-
ity between words in a phrase pair. Etymolog-
ically related words often retain similar spelling
across languages with the same writing system,
and low string edit distance sometimes signals
translation equivalency. Berg-Kirkpatrick and
Klein (2011) present methods for learning cor-
respondences between the alphabets of two lan-
guages. We can also extend this idea to language
pairs not sharing the same writing system since
many cognates, borrowed words, and names re-
main phonetically similar. Transliterations can be
generated for tokens in a source phrase (Knight
and Graehl, 1997), with o(f, e) calculating pho-
netic similarity rather than orthographic.
The three phrasal and four lexical similarity
scores are incorporated into the log linear trans-
lation model as feature functions, replacing the
bilingually estimated phrase translation probabil-
ities ? and lexical weighting probabilities w. Our
seven similarity scores are not the only ones that
could be incorporated into the translation model.
Various other similarity scores can be computed
depending on the available monolingual data and
its associated metadata (see, e.g. Schafer and
Yarowsky (2002)).
3.3 Reordering
The remaining component of the phrase-based
SMT model is the reordering model. We
introduce a novel algorithm for estimating
134
Input: Source and target phrases f and e,
Source and target monolingual corpora Cf and Ce,
Phrase table pairs T = {(f (i), e(i))}Ni=1.
Output: Orientation features (pm, ps, pd).
Sf ? sentences containing f in Cf ;
Se ? sentences containing e in Ce;
(Bf ,?,?)? CollectOccurs(f,?Ni=1f
(i), Sf );
(Be, Ae, De)? CollectOccurs(e,?Ni=1e
(i), Se);
cm = cs = cd = 0;
foreach unique f ? in Bf do
foreach translation e? of f ? in T do
cm = cm + #Be (e
?);
cs = cs + #Ae (e
?);
cd = cd + #De (e
?);
c? cm + cs + cd;
return ( cmc ,
cs
c ,
cd
c )
CollectOccurs(r, R, S)
B ? (); A? (); D ? ();
foreach sentence s ? S do
foreach occurrence of phrase r in s do
B ? B + (longest preceding r and in R);
A? A + (longest following r and in R);
D ? D + (longest discontinuous w/ r and in
R);
return (B, A, D);
Figure 5: Algorithm for estimating reordering
probabilities from monolingual data.
po(orientation|f, e) from two monolingual cor-
pora instead a bitext.
Figure 1 illustrates how the phrase pair orienta-
tion statistics are estimated in the standard phrase-
based SMT pipeline. For a phrase pair like (f =
?Profils?, e = ?profile?), we count its orien-
tation with the previously translated phrase pair
(f ? = ?in Facebook?, e? = ?Facebook?) across
all translated sentence pairs in the bitext.
In our pipeline we do not have translated sen-
tence pairs. Instead, we look for monolingual
sentences in the source corpus which contain
the source phrase that we are interested in, like
f = ?Profils?, and at least one other phrase
that we have a translation for, like f ? = ?in
Facebook?. We then look for all target lan-
guage sentences in the target monolingual cor-
pus that contain the translation of f (here e =
?profile?) and any translation of f ?. Figure 6 il-
lustrates that it is possible to find evidence for
po(swapped|Profils, profile), even from the non-
parallel, non-translated sentences drawn from two
independent monolingual corpora. By looking for
foreign sentences containing pairs of adjacent for-
eign phrases (f, f ?) and English sentences con-
D
a
s
A
n
l
e
g
e
n
e
i
n
e
s
P
r
o
fi
l
s
i
n
F
a
c
e
b
o
o
k
i
s
t
e
i
n
f
a
c
h
s
What
does
your
Facebook
profile
reveal
Figure 6: Collecting phrase orientation statistics for
a English-German phrase pair (?profile?, ?Profils?)
from non-parallel sentences (the German sentence
translates as ?Creating a Facebook profile is easy?).
taining their corresponding translations (e, e?), we
are able to increment orientation counts for (f, e)
by looking at whether e and e? are adjacent,
swapped, or discontinuous. The orientations cor-
respond directly to those shown in Figure 1.
One subtly of our method is that shorter and
more frequent phrases (e.g. punctuation) are more
likely to appear in multiple orientations with a
given phrase, and therefore provide poor evi-
dence of reordering. Therefore, we (a) collect
the longest contextual phrases (which also appear
in the phrase table) for reordering feature estima-
tion, and (b) prune the set of sentences so that
we only keep a small set of least frequent contex-
tual phrases (this has the effect of dropping many
function words and punctuation marks and and re-
lying more heavily on multi-word content phrases
to estimate the reordering).2
Our algorithm for learning the reordering pa-
rameters is given in Figure 5. The algorithm
estimates a probability distribution over mono-
tone, swap, and discontinuous orientations (pm,
ps, pd) for a phrase pair (f, e) from two mono-
lingual corpora Cf and Ce. It begins by calling
CollectOccurs to collect the longest match-
ing phrase table phrases that precede f in source
monolingual data (Bf ), as well as those that pre-
cede (Be), follow (Ae), and are discontinuous
(De) with e in the target language data. For each
unique phrase f ? preceding f , we look up transla-
tions in the phrase table T. Next, we count3 how
2The pruning step has an additional benefit of minimizing
the memory needed for orientation feature estimations.
3#L(x) returns the count of object x in list L.
135
Monolingual training corpora
Europarl Gigaword Wikipedia
date range 4/96-10/09 5/94-12/08 n/a
uniq shared dates 829 5,249 n/a
Spanish articles n/a 3,727,954 59,463
English articles n/a 4,862,876 59,463
Spanish lines 1,307,339 22,862,835 2,598,269
English lines 1,307,339 67,341,030 3,630,041
Spanish words 28,248,930 774,813,847 39,738,084
English words 27,335,006 1,827,065,374 61,656,646
Spanish-English phrase table
Phrase pairs 3,093,228
Spanish phrases 89,386
English phrases 926,138
Spanish unigrams 13,216
Avg # translations 98.7
Spanish bigrams 41,426
Avg # translations 31.9
Spanish trigrams 34,744
Avg # translations 13.5
Table 1: Statistics about the monolingual training data and the phrase table that was used in all of the experiments.
many translations e? of f ? appeared before, after
or were discontinuous with e in the target lan-
guage data. Finally, the counts are normalized and
returned. These normalized counts are the values
we use as estimates of po(orientation|f, e).
4 Experimental Setup
We use the Spanish-English language pair to test
our method for estimating the parameters of an
SMT system from monolingual corpora. This al-
lows us to compare our method against the nor-
mal bilingual training procedure. We expect bilin-
gual training to result in higher translation qual-
ity because it is a more direct method for learn-
ing translation probabilities. We systematically
remove different parameters from the standard
phrase-based model, and then replace them with
our monolingual equivalents. Our goal is to re-
cover as much of the loss as possible for each of
the deleted bilingual components.
The standard phrase-based model that we use
as our top-line is the Moses system (Koehn et
al., 2007) trained over the full Europarl v5 par-
allel corpus (Koehn, 2005). With the exception
of maximum phrase length (set to 3 in our ex-
periments), we used default values for all of the
parameters. All experiments use a trigram lan-
guage model trained on the English side of the
Europarl corpus using SRILM with Kneser-Ney
smoothing. To tune feature weights in minimum
error rate training, we use a development bitext
of 2,553 sentence pairs, and we evaluate per-
formance on a test set of 2,525 single-reference
translated newswire articles. These development
and test datasets were distributed in the WMT
shared task (Callison-Burch et al 2010).4 MERT
4Specifcially, news-test2008 plus news-syscomb2009 for
dev and newstest2009 for test.
was re-run for every experiment.
We estimate the parameters of our model from
two sets of monolingual data, detailed in Table 1:
? First, we treat the two sides of the Europarl
parallel corpus as independent, monolingual
corpora. Haghighi et al(2008) also used
this method to show how well translations
could be learned from monolingual corpora
under ideal conditions, where the contextual
and temporal distribution of words in the two
monolingual corpora are nearly identical.
? Next, we estimate the features from truly
monolingual corpora. To estimate the con-
textual and temporal similarity features, we
use the Spanish and English Gigaword cor-
pora.5 These corpora are substantially larger
than the Europarl corpora, providing 27x as
much Spanish and 67x as much English for
contextual similarity, and 6x as many paired
dates for temporal similarity. Topical simi-
larity is estimated using Spanish and English
Wikipedia articles that are paired with inter-
language links.
To project context vectors from Spanish to En-
glish, we use a bilingual dictionary containing en-
tries for 49,795 Spanish words. Note that end-to-
end translation quality is robust to substantially
reducing dictionary size, but we omit these ex-
periments due to space constraints. The con-
text vectors for words and phrases incorporate co-
occurrence counts using a two-word window on
either side.
The title of our paper uses the word towards be-
cause we assume that an inventory of phrase pairs
is given. Future work will explore inducing the
5We use the afp, apw and xin sections of the corpora.
136
BLE
U
0
5
10
15
20
25 21.87 21.54
12.86
4.00
10.52
15.35 14.02 14.78
16.85 17.50
22.92
-
/
-
B
/
B
B
/
-
-
/
B
-
/
M
t
/
-
o
/
-
c
/
-
M
/
-
M
/
M
B
M
/
B
1 2 3 4 5 6 7 8 9 10 11
            Exp      Phrase scores / orientation scores
   1        B/B       bilingual / bilingual (Moses)
   2        B/-        bilingual / distortion
   3        -/B        none / bilingual
   4        -/-         none / distortion
5, 12     -/M       none / mono
6, 13      t/-        temporal mono / distortion
7,14       o/-       orthographic mono / distortion
8, 15      c/-       contextual mono / distortion
  16        w/-      Wikipedia topical mono / distorion
9, 17      M/-      all mono / distortion
10, 18   M/M     all mono / mono
11, 19   BM/B   bilingual + all mono / bilingual
Estimated Using Europarl
Estimated Using Monolingual Corpora
B
L
E
U
.
BLE
U
0
5
10
15
20
25
10.15
13.13 14.02 14.07
17.00 17.92 18.79
23.36
23.
18.
17.
17
B
M
/
B
M
/
M
M
/
-
w
/
-
c
/
-
o
/
-
t
/
-
-
/
M
0
5
1
0
1
5
2
0
2
5
B
L
E
U
14
14
13.
10.
18 19171615141312
BLE
U
0
5
10
15
20
25 21.87 21.54
12.86
4.00
10.52
15.35 14.02 14.78
16.85 17.50
22.92
-
/
-
B
/
B
B
/
-
-
/
B
-
/
M
t
/
-
o
/
-
c
/
-
M
/
-
M
/
M
B
M
/
B
1 2 3 4 5 6 7 8 9 10 11
            Exp      Phrase scores / orientation scores
   1        B/B       bilingual / bilingual (Moses)
   2        B/-        bilingual / distortion
   3        -/B        none / bilingual
   4        -/-         none / distortion
5, 12     -/M       none / mono
6, 13      t/-        temporal mono / distortion
7,14       o/-       orthographic mono / distortion
8, 15      c/-       contextual mono / distortion
  16        w/-      Wikipedia topical mono / distorion
9, 17      M/-      all mono / distortion
10, 18   M/M     all mono / mono
11, 19   BM/B   bilingual + all mono / bilingual
Estimated Using Europarl
Estimated Using Monolingual Corpora
B
L
E
U
.
BLE
U
0
5
10
15
20
25
10.15
13.13 14.02 14.07
17.00 17.92 18.79
23.36
23.
18.
17.
17
B
M
/
B
M
/
M
M
/
-
w
/
-
c
/
-
o
/
-
t
/
-
-
/
M
0
5
1
0
1
5
2
0
2
5
B
L
E
U
14
14
13.
10.
18 19171615141312
Figure 7: Much of the loss in BLEU score when bilingually estimated features are removed from a Spanish-
English translation system (experiments 1-4) can be recovered when they are replaced with monolingual equiva-
lents estimated from monolingual Eur parl data (experiments 5-10). The labels indicate how the different types
of parameters are estimated, the first part is for phrase-table features, the second is for reordering probabilities.
BLE
U
0
5
10
15
20
25 21.87 21.54
12.86
4.00
10.52
15.35 14.02 14.78
16.85 17.50
22.92
-
/
-
B
/
B
B
/
-
-
/
B
-
/
M
t
/
-
o
/
-
c
/
-
M
/
-
M
/
M
B
M
/
B
1 2 3 4 5 6 7 8 9 10 11
            Exp      Phrase scores / orientation scores
   1        B/B       bilingual / bilingual (Moses)
   2        B/-        bilingual / distortion
   3        -/B        none / bilingual
   4        -/-         none / distortion
5, 12     -/M       none / mono
6, 13      t/-        temporal mono / distortion
7,14       o/-       orthographic mono / distortion
8, 15      c/-       contextual mono / distortion
  16        w/-      Wikipedia topical mono / distorion
9, 17      M/-      all mono / distortion
10, 18   M/M     all mono / mono
11, 19   BM/B   bilingual + all mono / bilingual
Estimated Using Europarl
Estimated Using Monolingual Corpora
B
L
E
U
.
BLE
U
0
5
10
15
20
25
10.15
13.13 14.02 14.07
17.00 17.92 18.79
23.36
23.
18.
17.
17
B
M
/
B
M
/
M
M
/
-
w
/
-
c
/
-
o
/
-
t
/
-
-
/
M
0
5
1
0
1
5
2
0
2
5
B
L
E
U
14
14
13.
10.
18 19171615141312
Figure 8: Performance of monolingual features de-
rived from truly monolingual corpora. Over 82% of
the BLEU score loss can be recovered.
phrase table itself from monolingual texts. Across
all of our experiments, we use the phrase table
that the bilingual model learned from the Europarl
parallel corpus. We keep its phrase pairs, but we
drop all of its scores. Table 1 gives details of the
phrase pairs. In our experiments, we estimated
similarity and reordering scores for more than 3
million phrase pairs. For each source phrase, the
set of possible translations was constrained and
likely to contain good translations. However, the
average number of possible translations was high
(ranging from nearly 100 translations for each un-
igram to 14 for each trigram). These contain a
lot of noise and result in low end-to-end transla-
tion quality without good estimates of translation
quality, as the experiments in Section 5.1 show.
Software. Because many details of our estima-
tion procedures must be omitted for space, we dis-
tribute our full set of code along with scripts for
running our experiments and output translations.
These may be downed from http://www.cs.
jhu.edu/?anni/papers/lowresmt/
5 Experimental Results
Figures 7 and 8 give experimental results. Figure
7 shows the performance of the standard phrase-
based model when each of the bilingually esti-
mated features are removed. It shows how much
of the performance loss can be recovered using
our monolingual features when they are estimated
from the Europarl training corpus but treating
each side as an independent, monolingual cor-
pus. Figure 8 shows the recovery when using truly
monolingual corpora to estimate the parameters.
5.1 Lesion experiments
Experiments 1-4 remove bilingually estimated pa-
rameters from the standard model. For Spanish-
English, the relative contribution of the phrase-
table features (which include the phrase transla-
tion probabilities ? and the lexical weights w) is
greater than the reordering probabilities. When
the reordering probability po(orientation|f, e) is
eliminated and replaced with a simple distance-
based distortion feature that does not require a
bitext to estimate, the score dips only marginally
since word order in English and Spanish is simi-
lar. However, when both the reordering and the
phrase table features are dropped, leaving only
the LM feature and the phrase penalty, the result-
ing translation quality is abysmal, with the score
dropping a total of over 17 BLEU points.
5.2 Adding equivalent monolingual features
estimated using Europarl
Experiments 5-10 show how much our monolin-
gual equivalents could recover when the monolin-
gual corpora are drawn from the two sides of the
bitext. For instance, our algorithm for estimating
137
reordering probabilities from monolingual data (?
/M) adds 5 BLEU points, which is 73% of the po-
tential recovery going from the model (?/?) to the
model with bilingual reordering features (?/B).
Of the temporal, orthographic, and contextual
monolingual features the temporal feature per-
forms the best. Together (M/?), they recover
more than each individually. Combining mono-
lingually estimated reordering and phrase table
features (M/M) yields a total gain of 13.5 BLEU
points, or over 75% of the BLEU score loss that
occurred when we dropped all features from the
phrase table. However, these results use ?mono-
lingual? corpora which have practically identical
phrasal and temporal distributions.
5.3 Estimating features using truly
monolingual corpora
Experiments 12-18 estimate all of the features
from truly monolingual corpora. Our novel al-
gorithm for estimating reordering holds up well
and recovers 69% of the loss, only 0.4 BLEU
points less than when estimated from the Europarl
monolingual texts. The temporal similarity fea-
ture does not perform as well as when it was esti-
mated using Europarl data, but the contextual fea-
ture does. The topic similarity using Wikipedia
performs the strongest of the individual features.
Combining the monolingually estimated re-
ordering features with the monolingually esti-
mated similarity features (M/M) yields a total
gain of 14.8 BLEU points, or over 82% of the
BLEU point loss that occurred when we dropped
all features from the phrase table. This is equiv-
alent to training the standard system on a bi-
text with roughly 60,000 lines or nearly 2 million
words (learning curve omitted for space).
Finally, we supplement the standard bilingually
estimated model parameters with our monolin-
gual features (BM/B), and we see a 1.5 BLEU
point increase over the standard model. There-
fore, our monolingually estimated scores capture
some novel information not contained in the stan-
dard feature set.
6 Additional Related Work
Carbonell et al(2006) described a data-driven
MT system that used no parallel text. It produced
translation lattices using a bilingual dictionary
and scored them using an n-gram language model.
Their method has no notion of translation similar-
ity aside from a bilingual dictionary. Similarly,
Sa?nchez-Cartagena et al(2011) supplement an
SMT phrase table with translation pairs extracted
from a bilingual dictionary and give each a fre-
quency of one for computing translation scores.
Ravi and Knight (2011) treat MT without paral-
lel training data as a decipherment task and learn
a translation model from monolingual text. They
translate corpora of Spanish time expressions and
subtitles, which both have a limited vocabulary,
into English. Their method has not been applied
to broader domains of text.
Most work on learning translations from mono-
lingual texts only examine small numbers of fre-
quent words. Huang et al(2005) and Daume? and
Jagarlamudi (2011) are exceptions that improve
MT by mining translations for OOV items.
A variety of past research has focused on min-
ing parallel or comparable corpora from the web
(Munteanu and Marcu, 2006; Smith et al 2010;
Uszkoreit et al 2010). Others use an existing
SMT system to discover parallel sentences within
independent monolingual texts, and use them to
re-train and enhance the system (Schwenk, 2008;
Chen et al 2008; Schwenk and Senellart, 2009;
Rauf and Schwenk, 2009; Lambert et al 2011).
These are complementary but orthogonal to our
research goals.
7 Conclusion
This paper has demonstrated a novel set of tech-
niques for successfully estimating phrase-based
SMT parameters from monolingual corpora, po-
tentially circumventing the need for large bitexts,
which are expensive to obtain for new languages
and domains. We evaluated the performance of
our algorithms in a full end-to-end translation sys-
tem. Assuming that a bilingual-corpus-derived
phrase table is available, we were able utilize our
monolingually-estimated features to recover over
82% of BLEU loss that resulted from removing
the bilingual-corpus-derived phrase-table proba-
bilities. We also showed that our monolingual fea-
tures add 1.5 BLEU points when combined with
standard bilingually estimated features. Thus our
techniques have stand-alone efficacy when large
bilingual corpora are not available and also make
a significant contribution to combined ensemble
performance when they are.
138
References
Enrique Alfonseca, Massimiliano Ciaramita, and
Keith Hall. 2009. Gazpacho and summer rash:
lexical relationships from temporal patterns of web
search queries. In Proceedings of EMNLP.
Taylor Berg-Kirkpatrick and Dan Klein. 2011. Simple
effective decipherment via combinatorial optimiza-
tion. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP-2011), Edinburgh, Scotland, UK.
Shane Bergsma and Benjamin Van Durme. 2011.
Learning bilingual lexicons using the visual simi-
larity of labeled web images. In Proceedings of the
International Joint Conference on Artificial Intelli-
gence.
Peter Brown, John Cocke, Stephen Della Pietra, Vin-
cent Della Pietra, Frederick Jelinek, Robert Mercer,
and Paul Poossin. 1988. A statistical approach to
language translation. In 12th International Confer-
ence on Computational Linguistics (CoLing-1988).
Peter Brown, Stephen Della Pietra, Vincent Della
Pietra, and Robert Mercer. 1993. The mathemat-
ics of machine translation: Parameter estimation.
Computational Linguistics, 19(2):263?311, June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Workshop on Sta-
tistical Machine Translation.
Jaime Carbonell, Steve Klein, David Miller, Michael
Steinbaum, Tomer Grassiany, and Jochen Frey.
2006. Context-based machine translation. In Pro-
ceedings of AMTA.
Boxing Chen, Min Zhang, Aiti Aw, and Haizhou Li.
2008. Exploiting n-best hypotheses for SMT self-
enhancement. In Proceedings of ACL/HLT, pages
157?160.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of ACL.
Hal Daume? and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by mining
unseen words. In Proceedings of ACL/HLT.
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compa-
rable texts. In Proceedings of ACL/CoLing.
Nikesh Garera, Chris Callison-Burch, and David
Yarowsky. 2009. Improving translation lexicon in-
duction from monolingual corpora via dependency
contexts and part-of-speech equivalences. In Thir-
teenth Conference On Computational Natural Lan-
guage Learning (CoNLL-2009), Boulder, Colorado.
Ulrich Germann. 2001. Building a statistical machine
translation system from scratch: How much bang
for the buck can we expect? In ACL 2001 Workshop
on Data-Driven Machine Translation, Toulouse,
France.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexi-
cons from monolingual corpora. In Proceedings of
ACL/HLT.
Fei Huang, Ying Zhang, and Stephan Vogel. 2005.
Mining key phrase translations from web corpora.
In Proceedings of EMNLP.
Alexandre Klementiev and Dan Roth. 2006. Weakly
supervised named entity transliteration and discov-
ery from multilingual comparable corpora. In Pro-
ceedings of the ACL/Coling.
Kevin Knight and Jonathan Graehl. 1997. Machine
transliteration. In Proceedings of ACL.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
ACL Workshop on Unsupervised Lexical Acquisi-
tion.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT/NAACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proceedings of the ACL-2007 Demo
and Poster Sessions.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of
the Machine Translation Summit.
Shankar Kumar and William Byrne. 2004. Local
phrase reordering models for statistical machine
translation. In Proceedings of HLT/NAACL.
Patrik Lambert, Holger Schwenk, Christophe Ser-
van, and Sadaf Abdul-Rauf. 2011. Investigations
on translation model adaptation using monolingual
data. In Proceedings of the Workshop on Statistical
Machine Translation, pages 284?293, Edinburgh,
Scotland, UK.
David Mimno, Hanna Wallach, Jason Naradowsky,
David Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proceedings of
EMNLP.
Dragos Stefan Munteanu and Daniel Marcu. 2006.
Extracting parallel sub-sentential fragments from
non-parallel corpora. In Proceedings of the
ACL/Coling.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417?449.
139
Franz Joseph Och. 2002. Statistical Machine Transla-
tion: From Single-Word Models to Alignment Tem-
plates. Ph.D. thesis, RWTH Aachen.
Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings
of ACL.
Reinhard Rapp. 1995. Identifying word translations
in non-parallel texts. In Proceedings of ACL.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated English and Ger-
man corpora. In Proceedings of ACL.
Sadaf Abdul Rauf and Holger Schwenk. 2009. On the
use of comparable corpora to improve SMT perfor-
mance. In Proceedings of EACL.
Sujith Ravi and Kevin Knight. 2011. Deciphering for-
eign language. In Proceedings of ACL/HLT.
Vctor M. Sa?nchez-Cartagena, Felipe Sa?nchez-
Martnez, and Juan Antonio Pe?rez-Ortiz. 2011.
Integrating shallow-transfer rules into phrase-based
statistical machine translation. In Proceedings of
the XIII Machine Translation Summit.
Charles Schafer and David Yarowsky. 2002. Inducing
translation lexicons via diverse similarity measures
and bridge languages. In Proceedings of CoNLL.
Holger Schwenk and Jean Senellart. 2009. Transla-
tion model adaptation for an Arabic/French news
translation system by lightly-supervised training. In
MT Summit.
Holger Schwenk. 2008. Investigations on large-scale
lightly-supervised training for statistical machine
translation. In Proceedings of IWSLT.
Jason R. Smith, Chris Quirk, and Kristina Toutanova.
2010. Extracting parallel sentences from compa-
rable corpora using document level alignment. In
Proceedings of HLT/NAACL.
Christoph Tillman. 2004. A unigram orientation
model for statistical machine translation. In Pro-
ceedings of HLT/NAACL.
Christoph Tillmann. 2003. A projection extension al-
gorithm for statistical machine translation. In Pro-
ceedings of EMNLP.
Jakob Uszkoreit, Jay M. Ponte, Ashok C. Popat, and
Moshe Dubiner. 2010. Large scale parallel docu-
ment mining for machine translation. In Proceed-
ings of CoLing.
Ashish Venugopal, Stephan Vogel, and Alex Waibel.
2003. Effective phrase translation extraction from
alignment models. In Proceedings of ACL.
140
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 327?337,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Stylometric Analysis of Scientific Articles
Shane Bergsma, Matt Post, David Yarowsky
Department of Computer Science and Human Language Technology Center of Excellence
Johns Hopkins University
Baltimore, MD 21218, USA
sbergsma@jhu.edu, post@cs.jhu.edu, yarowsky@cs.jhu.edu
Abstract
We present an approach to automatically re-
cover hidden attributes of scientific articles,
such as whether the author is a native English
speaker, whether the author is a male or a fe-
male, and whether the paper was published in
a conference or workshop proceedings. We
train classifiers to predict these attributes in
computational linguistics papers. The classi-
fiers perform well in this challenging domain,
identifying non-native writing with 95% accu-
racy (over a baseline of 67%). We show the
benefits of using syntactic features in stylom-
etry; syntax leads to significant improvements
over bag-of-words models on all three tasks,
achieving 10% to 25% relative error reduction.
We give a detailed analysis of which words
and syntax most predict a particular attribute,
and we show a strong correlation between our
predictions and a paper?s number of citations.
1 Introduction
Stylometry aims to recover useful attributes of doc-
uments from the style of the writing. In some do-
mains, statistical techniques have successfully de-
duced author identity (Mosteller and Wallace, 1984),
gender (Koppel et al, 2003), native language (Kop-
pel et al, 2005), and even whether an author has de-
mentia (Le et al, 2011). Stylometric analysis is im-
portant to marketers, analysts and social scientists
because it provides demographic data directly from
raw text. There has been growing interest in apply-
ing stylometry to the content generated by users of
Internet applications, e.g., detecting author ethnic-
ity in social media (Eisenstein et al, 2011; Rao et
Native/Non-native
Male/Female
Conference/Workshop
Stylometric Analysis of Scientific Articles
Abstract
We present an approach to automatically re-
cover hidden attributes of scientific articles,
such as whether the author is a native En-
glish speaker. We train classifiers to predict
these attributes in computational linguistics
papers. The classifiers perform well in this
challenging domain, identifying non-native
writing with 95% accuracy (over a baseline
of 67%), and outperforming reasonable base-
lines on two other difficult tasks. We show the
benefits of using syntactic features in stylom-
etry; syntax leads to significant improvements
over bag-of-words models on all three tasks,
achieving 10% to 25% relative error reduction.
We give an insightful analysis of which words
and syntax most predict a particular attribute,
and we show a strong correlation between our
predictions and a paper?s number of citations.
1 Introduction
Stylometry aims to recover useful attributes of doc-
uments from the style of their writing. In some
domains, statistical techniques have successfully
deduced author identities (Mosteller and Wallace,
1984), gender (Koppel et al, 2003), native language
(Koppel et al, 2005), and even whether an author
has dementia (Le et al, 2011). Stylometric analysis
is important to marketers, analysts and social scien-
tists because it provides demographic data directly
from raw text. There has been growing interest in
applying stylometry in Web 2.0 applications, e.g.,
detecting the ethnicity of Twitter users (Eisenstein
et al, 2011; Rao et al, 2011), or whether a person is
writing deceptive online reviews (Ott et al, 2011).
We evaluate stylometric techniques in the novel
domain of scientific writing. Science is a diffi-
cult domain; authors are compelled, often explic-
itly by reviewers/submission guidelines, to comply
!"#$%&'!(!)!"#$%&
*"+&',&*"+&
*"$!'-(./01(2
!"#$%&'"()* +,-$#.). %/ !*)',")0* +(")*$'.
+1."(-*"
!" #$"%"&' (& (##$)(*+ ') (,')-('.*(//0 $"1
*)2"$ +.33"& (''$.4,'"% )5 %*."&'.6* ($'.*/"%7
%,*+ (% 8+"'+"$ '+" (,'+)$ .% ( &('.2" 9&1
:/.%+ %#"(;"$< !" '$(.& */(%%.6"$% ') #$"3.*'
'+"%" (''$.4,'"% .& *)-#,'('.)&(/ /.&:,.%'.*%
#(#"$%< =+" */(%%.6"$% #"$5)$- 8"// .& '+.%
*+(//"&:.&: 3)-(.&7 .3"&'.50.&: &)&1&('.2"
8$.'.&: 8.'+ >?@ (**,$(*0 A)2"$ ( 4(%"/.&"
)5 BC@D7 (&3 ),'#"$5)$-.&: $"(%)&(4/" 4(%"1
/.&"% )& '8) )'+"$ 3.56*,/' '(%;%< !" %+)8 '+"
4"&"6'% )5 ,%.&: !"#$%&$'& ()%$*+)! .& %'0/)-1
"'$0E %0&'(F /"(3% ') %.:&.6*(&' .-#$)2"-"&'%
)2"$ 4(:1)518)$3% -)3"/% )& (// '+$"" '(%;%7
(*+."2.&: GH@ ') I?@ $"/('.2" "$$)$ $"3,*'.)&<
!" :.2" (& .&%.:+'5,/ (&(/0%.% )5 8+.*+ 8)$3%
(&3 %0&'(F -)%' #$"3.*' ( #($'.*,/($ (''$.4,'"7
(&3 8" %+)8 ( %'$)&: *)$$"/('.)& 4"'8""& ),$
#$"3.*'.)&% (&3 ( #(#"$J% &,-4"$ )5 *.'('.)&%<
2 3,"(%45*")%,
K'0/)-"'$0 (.-% ') $"*)2"$ ,%"5,/ (''$.4,'"% )5 3)*1
,-"&'% 5$)- '+" %'0/" )5 '+".$ 8$.'.&:< L& %)-"
3)-(.&%7 %'('.%'.*(/ '"*+&.M,"% +(2" %,**"%%5,//0
3"3,*"3 (,'+)$ .3"&'.'."% AN)%'"//"$ (&3 !(//(*"7
G>OPD7 :"&3"$ AQ)##"/ "' (/<7 IHHRD7 &('.2" /(&:,(:"
AQ)##"/ "' (/<7 IHH?D7 (&3 "2"& 8+"'+"$ (& (,'+)$
+(% 3"-"&'.( AS" "' (/<7 IHGGD< K'0/)-"'$.* (&(/0%.%
.% .-#)$'(&' ') -($;"'"$%7 (&(/0%'% (&3 %)*.(/ %*."&1
'.%'% 4"*(,%" .' #$)2.3"% 3"-):$(#+.* 3('( 3.$"*'/0
5$)- $(8 '"F'< =+"$" +(% 4""& :$)8.&: .&'"$"%' .&
(##/0.&: %'0/)-"'$0 .& !"4 I<H (##/.*('.)&%7 "<:<7
3"'"*'.&: '+" "'+&.*.'0 )5 =8.''"$ ,%"$% A9.%"&%'".&
"' (/<7 IHGGE T() "' (/<7 IHGGD7 )$ 8+"'+"$ ( #"$%)& .%
8$.'.&: 3"*"#'.2" )&/.&" $"2."8% AU'' "' (/<7 IHGGD<
!" "2(/,('" %'0/)-"'$.* '"*+&.M,"% .& '+" &)2"/
3)-(.& )5 !&')#$',& -+'$'#.< K*."&*" .% ( 3.561
*,/' 3)-(.&E (,'+)$% ($" *)-#"//"37 )5'"& "F#/.*1
.'/0 40 $"2."8"$%V%,4-.%%.)& :,.3"/.&"%7 ') *)-#/0
!"#$%&'!(!)!"#$%&
*"+&',&*"+&
*"$!'-(./01(2
!"#$%&'"()* +,-$#.). %/ !*)',")0* +(")*$'.
+1."(-*"!" #$"%"&' (& (##$)(*+ ') (,')-('.*(//0 $"1*)2"$ +.33"& (''$.4,'"% )5 %*."&'.6* ($'.*/"%7%,*+ (% 8+"'+"$ '+" (,'+)$ .% ( &('.2" 9&1:/.%+ %#"(;"$< !" '$(.& */(%%.6"$% ') #$"3.*''+"%" (''$.4,'"% .& *)-#,'('.)&(/ /.&:,.%'.*%#(#"$%< =+" */(%%.6"$% #"$5)$- 8"// .& '+.%*+(//"&:.&: 3)-(.&7 .3"&'.50.&: &)&1&('.2"8$.'.&: 8.'+ >?@ (**,$(*0 A)2"$ ( 4(%"/.&")5 BC@D7 (&3 ),'#"$5)$-.&: $"(%)&(4/" 4(%"1/.&"% )& '8) )'+"$ 3.56*,/' '(%;%< !" %+)8 '+"4"&"6'% )5 ,%.&: !"#$%&$'& ()%$*+)! .& %'0/)-1"'$0E %0&'(F /"(3% ') %.:&.6*(&' .-#$)2"-"&'%)2"$ 4(:1)518)$3% -)3"/% )& (// '+$"" '(%;%7(*+."2.&: GH@ ') I?@ $"/('.2" "$$)$ $"3,*'.)&<!" :.2" (& .&%.:+'5,/ (&(/0%.% )5 8+.*+ 8)$3%(&3 %0&'(F -)%' #$"3.*' ( #($'.*,/($ (''$.4,'"7(&3 8" %+)8 ( %'$)&: *)$$"/('.)& 4"'8""& ),$#$"3.*'.)&% (&3 ( #(#"$J% &,-4"$ )5 *.'('.)&%<2 3,"(%45*")%,K'0/)-"'$0 (.-% ') $"*)2"$ ,%"5,/ (''$.4,'"% )5 3)*1,-"&'% 5$)- '+" %'0/" )5 '+".$ 8$.'.&:< L& %)-"3)-(.&%7 %'('.%'.*(/ '"*+&.M,"% +(2" %,**"%%5,//03"3,*"3 (,'+)$ .3"&'.'."% AN)%'"//"$ (&3 !(//(*"7G>OPD7 :"&3"$ AQ)##"/ "' (/<7 IHHRD7 &('.2" /(&:,(:"AQ)##"/ "' (/<7 IHH?D7 (&3 "2"& 8+"'+"$ (& (,'+)$+(% 3"-"&'.( AS" "' (/<7 IHGGD< K'0/)-"'$.* (&(/0%.%.% .-#)$'(&' ') -($;"'"$%7 (&(/0%'% (&3 %)*.(/ %*."&1'.%'% 4"*(,%" .' #$)2.3"% 3"-):$(#+.* 3('( 3.$"*'/05$)- $(8 '"F'< =+"$" +(% 4""& :$)8.&: .&'"$"%' .&(##/0.&: %'0/)-"'$0 .& !"4 I<H (##/.*('.)&%7 "<:<73"'"*'.&: '+" "'+&.*.'0 )5 =8.''"$ ,%"$% A9.%"&%'".&"' (/<7 IHGGE T() "' (/<7 IHGGD7 )$ 8+"'+"$ ( #"$%)& .%8$.'.&: 3"*"#'.2" )&/.&" $"2."8% AU'' "' (/<7 IHGGD<!" "2(/,('" %'0/)-"'$.* '"*+&.M,"% .& '+" &)2"/3)-(.& )5 !&')#$',& -+'$'#.< K*."&*" .% ( 3.561*,/' 3)-(.&E (,'+)$% ($" *)-#"//"37 )5'"& "F#/.*1.'/0 40 $"2."8"$%V%,4-.%%.)& :,.3"/.&"%7 ') *)-#/0
!"#$%&'!(!)!"#$%&*"+&',&*"+&*"$!'-(./01(2
!"#$%&'"()* +,-$#.). %/ !*)',")0* +(")*$'.+1."(-*"!" #$"%"&' (& (##$)(*+ ') (,')-('.*(//0 $"1*)2"$ +.33"& (''$.4,'"% )5 %*."&'.6* ($'.*/"%7%,*+ (% 8+"'+"$ '+" (,'+)$ .% ( &('.2" 9&1:/.%+ %#"(;"$< !" '$(.& */(%%.6"$% ') #$"3.*''+"%" (''$.4,'"% .& *)-#,'('.)&(/ /.&:,.%'.*%#(#"$%< =+" */(%%.6"$% #"$5)$- 8"// .& '+.%*+(//"&:.&: 3)-(.&7 .3"&'.50.&: &)&1&('.2"8$.'.&: 8.'+ >?@ (**,$(*0 A)2"$ ( 4(%"/.&")5 BC@D7 (&3 ),'#"$5)$-.&: $"(%)&(4/" 4(%"1/.&"% )& '8) )'+"$ 3.56*,/' '(%;%< !" %+)8 '+"4"&"6'% )5 ,%.&: !"#$%&$'& ()%$*+)! .& %'0/)-1"'$0E %0&'(F /"(3% ') %.:&.6*(&' .-#$)2"-"&'%)2"$ 4(:1)518)$3% -)3"/% )& (// '+$"" '(%;%7(*+."2.&: GH@ ') I?@ $"/('.2" "$$)$ $"3,*'.)&<!" :.2" (& .&%.:+'5,/ (&(/0%.% )5 8+.*+ 8)$3%(&3 %0&'(F -)%' #$"3.*' ( #($'.*,/($ (''$.4,'"7(&3 8" %+)8 ( %'$)&: *)$$"/('.)& 4"'8""& ),$#$"3.*'.)&% (&3 ( #(#"$J% &,-4"$ )5 *.'('.)&%<2 3,"(%45*")%,K'0/)-"'$0 (.-% ') $"*)2"$ ,%"5,/ (''$.4,'"% )5 3)*1,-"&'% 5$)- '+" %'0/" )5 '+".$ 8$.'.&:< L& %)-"3)-(.&%7 %'('.%'.*(/ '"*+&.M,"% +(2" %,**"%%5,//03"3,*"3 (,'+)$ .3"&'.'."% AN)%'"//"$ (&3 !(//(*"7G>OPD7 :"&3"$ AQ)##"/ "' (/<7 IHHRD7 &('.2" /(&:,(:"AQ)##"/ "' (/<7 IHH?D7 (&3 "2"& 8+"'+"$ (& (,'+)$+(% 3"-"&'.( AS" "' (/<7 IHGGD< K'0/)-"'$.* (&(/0%.%.% .-#)$'(&' ') -($;"'"$%7 (&(/0%'% (&3 %)*.(/ %*."&1'.%'% 4"*(,%" .' #$)2.3"% 3"-):$(#+.* 3('( 3.$"*'/05$)- $(8 '"F'< =+"$" +(% 4""& :$)8.&: .&'"$"%' .&(##/0.&: %'0/)-"'$0 .& !"4 I<H (##/.*('.)&%7 "<:<73"'"*'.&: '+" "'+&.*.'0 )5 =8.''"$ ,%"$% A9.%"&%'".&"' (/<7 IHGGE T() "' (/<7 IHGGD7 )$ 8+"'+"$ ( #"$%)& .%8$.'.&: 3"*"#'.2" )&/.&" $"2."8% AU'' "' (/<7 IHGGD<!" "2(/,('" %'0/)-"'$.* '"*+&.M,"% .& '+" &)2"/3)-(.& )5 !&')#$',& -+'$'#.< K*."&*" .% ( 3.561*,/' 3)-(.&E (,'+)$% ($" *)-#"//"37 )5'"& "F#/.*1.'/0 40 $"2."8"$%V%,4-.%%.)& :,.3"/.&"%7 ') *)-#/0
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`$%'7 G>>?D< U,$ 8)$; .&*/,3"%X6'7 !"#$%&'"()* 8-.9.:!" #$"3.*' 8+"'+"$ (#(#"$ .% 8$.''"&X AGD 40 ( &('.2" )$ &)&1&('.2" 9&1:/.%+ %#"(;"$7 AID 40 ( -(/" )$ 5"-(/"7 (&3 ARD .& '+"%'0/" )5 ( *)&5"$"&*" )$ (8)$;%+)# #(#"$< =+" /(''"$.% ( &)2"/ %'0/)-"'$.* (&3 4.4/.)-"'$.* #$"3.*'.)&<6'7 !"#$%&'"()* ;'-"5('.:!" %+)8 '+" 2(/,")5!"#$%&$'& ()%$*+)!5)$ %'0/)-"'$0< ^-)&: )'+"$%78" 3"%*$.4"$+)) !*/!$'$*$'0# .+%11%+ 5$(:-"&'%78+.*+ +(2" &)' #$"2.),%/0 4""& ,%"3 .& %'0/)-"'$0<=K_ 5$(:-"&'% ($" .&'"$#$"'(4/"7 "56*."&'7 (&3 #($1'.*,/($/0 "55"*'.2" 5)$ 3"'"*'.&: &)&1&('.2" 8$.'.&:<!+./" $"*"&' %',3."% +(2" -)%'/0 "2(/,('"3 %.&1:/" #$"3.*'.)& '(%;%7 8" *)-#($" 3.55"$"&' %'$('":."%(*$)%% 3.55"$"&' '(%;% )& ( *)--)& 3('(%"' (&3 8.'+( *)--)& .&5$(%'$,*',$"< L& (33.'.)& ') *)&'$(%'.&:3.55"$"&' 5"(',$" '0#"%7 8" (/%) *)-#($" 3.55"$"&'W.:,$" GX Y$"3.*'.&: +.33"& (''$.4,'"% .& %*."&'.6* ($'.*/"%8.'+ &)$-('.2" #$(*'.*"% .& %#"//.&: (&3 :$(--($<N)$")2"$7 ')#.*(/ */,"% ($" /"%% %(/."&' '+(& .& 3)1-(.&% /.;" %)*.(/ -"3.(< Z"' %*."&*" .% -)$" '+(&[,%' ( :))3 *+(//"&:" 5)$ %'0/)-"'$0E .' .% (& .-#)$1'(&' ($"( .& .'%"/5< K0%'"-% 5)$ %*."&'.6* %'0/)-"'$08),/3 :.2" %)*.)/):.%'% &"8 '))/% 5)$ (&(/0\.&: (*(13"-.* *)--,&.'."%7 (&3 &"88(0% ') $"%)/2" '+" &(1',$" )5 *)//(4)$('.)& .& %#"*.6* ($'.*/"% A])+$. "' (/<7IHGGD< ^,'+)$% -.:+' (/%) ,%" '+"%" '))/%7 "<:< ')+"/# "&%,$" ( *)&%.%'"&' %'0/" .& -,/'.1(,'+)$"3 #(1#"$% A_/)2"$ (&3 `.$%'7 G>>?D< U,$ 8)$; .&*/,3"%X6'7 !"#$%&'"()* 8-.9.: !" #$"3.*' 8+"'+"$ (#(#"$ .% 8$.''"&X AGD 40 ( &('.2" )$ &)&1&('.2" 9&1:/.%+ %#"(;"$7 AID 40 ( -(/" )$ 5"-(/"7 (&3 ARD .& '+"%'0/" )5 ( *)&5"$"&*" )$ ( 8)$;%+)# #(#"$< =+" /(''"$.% ( &)2"/ %'0/)-"'$.* (&3 4.4/.)-"'$.* #$"3.*'.)&<6'7 !"#$%&'"()* ;'-"5('.: !" %+)8 '+" 2(/,")5 !"#$%&$'& ()%$*+)! 5)$ %'0/)-"'$0< ^-)&: )'+"$%78" 3"%*$.4" $+)) !*/!$'$*$'0# .+%11%+ 5$(:-"&'%78+.*+ +(2" &)' #$"2.),%/0 4""& ,%"3 .& %'0/)-"'$0<=K_ 5$(:-"&'% ($" .&'"$#$"'(4/"7 "56*."&'7 (&3 #($1'.*,/($/0 "55"*'.2" 5)$ 3"'"*'.&: &)&1&('.2" 8$.'.&:<!+./" $"*"&' %',3."% +(2" -)%'/0 "2(/,('"3 %.&1:/" #$"3.*'.)& '(%;%7 8" *)-#($" 3.55"$"&' %'$('":."%(*$)%% 3.55"$"&' '(%;% )& ( *)--)& 3('(%"' (&3 8.'+( *)--)& .&5$(%'$,*',$"< L& (33.'.)& ') *)&'$(%'.&:3.55"$"&' 5"(',$" '0#"%7 8" (/%) *)-#($" 3.55"$"&'W.:,$" GX Y$"3.*'.&: +.33"& (''$.4,'"% .& %*."&'.6* ($'.*/"%8.'+ &)$-('.2" #$(*'.*"% .& %#"//.&: (&3 :$(--($<N)$")2"$7 ')#.*(/ */,"% ($" /"%% %(/."&' '+(& .& 3)1-(.&% /.;" %)*.(/ -"3.(< Z"' %*."&*" .% -)$" '+(&[,%' ( :))3 *+(//"&:" 5)$ %'0/)-"'$0E .' .% (& .-#)$1'(&' ($"( .& .'%"/5< K0%'"-% 5)$ %*."&'.6* %'0/)-"'$08),/3 :.2" %)*.)/):.%'% &"8 '))/% 5)$ (&(/0\.&: (*(13"-.* *)--,&.'."%7 (&3 &"8 8(0% ') $"%)/2" '+" &(1',$" )5 *)//(4)$('.)& .& %#"*.6* ($'.*/"% A])+$. "' (/<7IHGGD< ^,'+)$% -.:+' (/%) ,%" '+"%" '))/%7 "<:< ')+"/# "&%,$" ( *)&%.%'"&' %'0/" .& -,/'.1(,'+)$"3 #(1#"$% A_/)2"$ (&3 `.$%'7 G>>?D< U,$ 8)$; .&*/,3"%X6'7 !"#$%&'"()* 8-.9.: !" #$"3.*' 8+"'+"$ (#(#"$ .% 8$.''"&X AGD 40 ( &('.2" )$ &)&1&('.2" 9&1:/.%+ %#"(;"$7 AID 40 ( -(/" )$ 5"-(/"7 (&3 ARD .& '+"%'0/" )5 ( *)&5"$"&*" )$ ( 8)$;%+)# #(#"$< =+" /(''"$.% ( &)2"/ %'0/)-"'$.* (&3 4.4/.)-"'$.* #$"3.*'.)&<6'7 !"#$%&'"()* ;'-"5('.: !" %+)8 '+" 2(/,")5 !"#$%&$'& ()%$*+)! 5)$ %'0/)-"'$0< ^-)&: )'+"$%78" 3"%*$.4" $+)) !*/!$'$*$'0# .+%11%+ 5$(:-"&'%78+.*+ +(2" &)' #$"2.),%/0 4""& ,%"3 .& %'0/)-"'$0<=K_ 5$(:-"&'% ($" .&'"$#$"'(4/"7 "56*."&'7 (&3 #($1'.*,/($/0 "55"*'.2" 5)$ 3"'"*'.&: &)&1&('.2" 8$.'.&:<!+./" $"*"&' %',3."% +(2" -)%'/0 "2(/,('"3 %.&1:/" #$"3.*'.)& '(%;%7 8" *)-#($" 3.55"$"&' %'$('":."%(*$)%% 3.55"$"&' '(%;% )& ( *)--)& 3('(%"' (&3 8.'+( *)--)& .&5$(%'$,*',$"< L& (33.'.)& ') *)&'$(%'.&:3.55"$"&' 5"(',$" '0#"%7 8" (/%) *)-#($" 3.55"$"&'
W.:,$" GX Y$"3.*'.&: +.33"& (''$.4,'"% .& %*."&'.6* ($'.*/"%
8.'+ &)$-('.2" #$(*'.*"% .& %#"//.&: (&3 :$(--($<
N)$")2"$7 ')#.*(/ */,"% ($" /"%% %(/."&' '+(& .& 3)1
-(.&% /.;" %)*.(/ -"3.(< Z"' %*."&*" .% -)$" '+(&
[,%' ( :))3 *+(//"&:" 5)$ %'0/)-"'$0E .' .% (& .-#)$1
'(&' ($"( .& .'%"/5< K0%'"-% 5)$ %*."&'.6* %'0/)-"'$0
8),/3 :.2" %)*.)/):.%'% &"8 '))/% 5)$ (&(/0\.&: (*(1
3"-.* *)--,&.'."%7 (&3 &"8 8(0% ') $"%)/2" '+" &(1
',$" )5 *)//(4)$('.)& .& %#"*.6* ($'.*/"% A])+$. "' (/<7
IHGGD< ^,'+)$% -.:+' (/%) ,%" '+"%" '))/%7 "<:< ')
+"/# "&%,$" ( *)&%.%'"&' %'0/" .& -,/'.1(,'+)$"3 #(1
#"$% A_/)2"$ (&3 `.$%'7 G>>?D< U,$ 8)$; .&*/,3"%X
6'7 !"#$%&'"()* 8-.9.: !" #$"3.*' 8+"'+"$ (
#(#"$ .% 8$.''"&X AGD 40 ( &('.2" )$ &)&1&('.2" 9&1
:/.%+ %#"(;"$7 AID 40 ( -(/" )$ 5"-(/"7 (&3 ARD .& '+"
%'0/" )5 ( *)&5"$"&*" )$ ( 8)$;%+)# #(#"$< =+" /(''"$
.% ( &)2"/ %'0/)-"'$.* (&3 4.4/.)-"'$.* #$"3.*'.)&<
6'7 !"#$%&'"()* ;'-"5('.: !" %+)8 '+" 2(/,"
)5 !"#$%&$'& ()%$*+)! 5)$ %'0/)-"'$0< ^-)&: )'+"$%7
8" 3"%*$.4" $+)) !*/!$'$*$'0# .+%11%+ 5$(:-"&'%7
8+.*+ +(2" &)' #$"2.),%/0 4""& ,%"3 .& %'0/)-"'$0<
=K_ 5$(:-"&'% ($" .&'"$#$"'(4/"7 "56*."&'7 (&3 #($1
'.*,/($/0 "55"*'.2" 5)$ 3"'"*'.&: &)&1&('.2" 8$.'.&:<
!+./" $"*"&' %',3."% +(2" -)%'/0 "2(/,('"3 %.&1
:/" #$"3.*'.)& '(%;%7 8" *)-#($" 3.55"$"&' %'$('":."%
(*$)%% 3.55"$"&' '(%;% )& ( *)--)& 3('(%"' (&3 8.'+
( *)--)& .&5$(%'$,*',$"< L& (33.'.)& ') *)&'$(%'.&:
3.55"$"&' 5"(',$" '0#"%7 8" (/%) *)-#($" 3.55"$"&'
Figure 1: Predicting hidden attributes in scientific articles
with normative practices in spelling and grammar.
Moreover, topical clues are less salient than in do-
mains like social media. Yet science is more than
just a good challenge for stylometry; it is an impor-
tant area in itself. Systems for scientific stylometry
would give sociologists new tools for analyzing aca-
demic communities, and new ways to resolve the na-
ture of collaboration in specific articles (Johri et al,
2011). Authors might also use these tools, e.g. to
help ensure a consistent style in multi-authored pa-
pers (Glover and Hirst, 1995). Our work includes:
New Stylometric Tasks: We predict whether a
paper is written: (1) by a native or non-native En-
glish speaker, (2) by a male or female, and (3) in the
style of a conference or a workshop paper. The latter
is a novel stylometric and bibliometric prediction.
New Stylometric Features: We show the value
of syntactic features for stylometry. Among others,
we describe tree substitution grammar fragments,
which have not previously been used in stylometry.
TSG fragments are interpretable, efficient, and par-
ticularly effective for detecting non-native writing.
While recent studies have mostly evaluated sin-
gle prediction tasks, we compare different strategies
across different tasks on a common dataset and with
a common infrastructure. In addition to contrasting
different feature types, we also compare different
Figure 1: Predicting hidden attributes in scientific articles
al., 2011), or whether someone is writing deceptive
online reviews (Ott et al, 2011).
We evaluate stylometric techniques in the novel
domain of scientific writing. Science is a difficult
domain; authors are encouraged, often explicitly
by reviewers/submission-guidelines, to comply with
normative practices in style, spelling and grammar.
Moreover, topical clues are less salient than in do-
mains like social media. Success in this challenging
domain can bring us closer to correctly analyzing
the huge volumes of online text that are currently
unmarked for useful author attributes such as gender
and native-language.
Yet science is more than just a good stepping-
stone for stylometry; it is an important area in itself.
Systems for scientific stylometry would give sociol-
ogists new tools for analyzing academic communi-
ties, and new ways to resolve the nature of collab-
oration in specific articles (Johri et al, 2011). Au-
thors might also use these tools, e.g., to help ensure
a consistent style in multi-authored papers (Glover
and Hirst, 1995), or to determine sections of a paper
needing revision.
327
The contributions of our paper include:
New Stylometric Tasks: We predict whether
a paper is written: (1) by a native or non-native
speaker, (2) by a male or female, and (3) in the style
of a conference or workshop paper. The latter is a
fully novel stylometric and bibliometric prediction.
New Stylometric Features: We show the value
of syntactic features for stylometry. Among others,
we describe tree substitution grammar fragments,
which have not previously been used in stylometry.
TSG fragments are interpretable, efficient, and par-
ticularly effective for detecting non-native writing.
While recent studies have mostly evaluated sin-
gle prediction tasks, we compare different strategies
across different tasks on a common dataset and with
a common infrastructure. In addition to contrasting
different feature types, we compare different train-
ing strategies, exploring ways to make use of train-
ing instances with label uncertainty.
We also provide a detailed analysis that is inter-
esting from a sociolinguistic standpoint. Precisely
what words distinguish non-native writing? How
does the syntax of female authors differ from males?
What are the hallmarks of top-tier papers? Finally,
we identify some strong correlations between our
predictions and a paper?s citation count, even when
controlling for paper venue and origin.
2 Related Work
Bibliometrics is the empirical analysis of scholarly
literature; citation analysis is a well-known bib-
liometric approach for ranking authors and papers
(Borgman and Furner, 2001). Bibliometry and sty-
lometry can share goals but differ in techniques.
For example, in a work questioning the blindness
of double-blind reviewing, Hill and Provost (2003)
predict author identities. They ignore the article
body and instead consider (a) potential self-citations
and (b) similarity between the article?s citation list
and the citation lists of known papers. Radev et al
(2009a) perform a bibliometric analysis of compu-
tational linguistics. Teufel and Moens (2002) and
Qazvinian and Radev (2008) summarize scientific
articles, the latter by automatically finding and fil-
tering sentences in other papers that cite the target
article.
Our system does not consider citations; it is most
similar to work that uses raw article text. Hall et
al. (2008) build per-year topic models over scientific
literature to track the evolution of scientific ideas.
Gerrish and Blei (2010) assess the influence of indi-
vidual articles by modeling their impact on the con-
tent of future papers. Yogatama et al (2011) pre-
dict whether a paper will be cited based on both its
content and its meta-data such as author names and
publication venues. Johri et al (2011) use per-author
topic models to assess the nature of collaboration in
a particular article (e.g., apprenticeship or synergy).
One of the tasks in Sarawgi et al (2011) concerned
predicting gender in scientific writing, but they use a
corpus of only ten ?highly established? authors and
make the prediction using twenty papers for each.
Finally, Dale and Kilgarriff (2010) initiated a shared
task on automatic editing of scientific papers written
by non-native speakers, with the objective of devel-
oping ?tools which can help non-native speakers of
English (NNSs) (and maybe some native ones) write
academic English prose of the kind that helps a pa-
per get accepted.?
Lexical and pragmatic choices in academic writ-
ing have also been analyzed within the applied lin-
guistics community (Myers, 1989; Vassileva, 1998).
3 ACL Dataset and Preprocessing
We use papers from the ACL Anthology Network
(Radev et al, 2009b, Release 2011) and exploit its
manually-curated meta-data such as normalized au-
thor names, affiliations (including country, avail-
able up to 2009), and citation counts. We con-
vert each PDF to text1 but remove text before the
Abstract (to anonymize) and after the Acknowledg-
ments/References headings. We split the text into
sentences2 and filter any documents with fewer than
100 (this removes some short/demo papers, mal-
converted PDFs, etc. ? about 23% of the 13K pa-
pers with affiliation information). In case the text
was garbled, we then filtered the first 3 lines from
every file and any line with an ?@? symbol (which
might be part of an affiliation). We remove foot-
ers like Proceedings of ..., table/figure captions, and
any lines with non-ASCII characters (e.g. math
equations). Papers are then parsed via the Berke-
1Via the open-source utility pdftotext
2Splitter from cogcomp.cs.illinois.edu/page/tools
328
Task Training Set: Dev Test
Strict Lenient Set Set
NativeL 2127 3963 450 477
Venue 2484 3991 400 421
Gender 2125 3497 400 409
Table 1: Number of documents for each task
ley parser (Petrov et al, 2006), and part-of-speech
(PoS) tagged using CRFTagger (Phan, 2006).
Training sets always comprise papers from 2001-
2007, while test sets are created by randomly shuf-
fling the 2008-2009 portion and then dividing it into
development/test sets. We also use papers from
1990-2000 for experiments in ?7.3 and ?7.4.
4 Stylometric Tasks
Each task has both a Strict training set, using only
the data for which we are most confident in the la-
bels (as described below), and a Lenient set, which
forcibly assigns every paper in the training period
to some class (Table 1). All test papers are anno-
tated using a Strict rule. While our approaches for
automatically-assigning labels can be coarse, they
allow us to scale our analysis to a realistic cross-
section of academic papers, letting us discover some
interesting trends.
4.1 NativeL: Native vs. Non-Native English
We introduce the task of predicting whether a sci-
entific paper is written by a native English speaker
(NES) or non-native speaker (NNS). Prior work has
mostly made this prediction in learner corpora (Kop-
pel et al, 2005; Tsur and Rappoport, 2007; Wong
and Dras, 2011), although there have been attempts
in elicited speech transcripts (Tomokiyo and Jones,
2001) and e-mail (Estival et al, 2007). There has
also been a large body of work on correcting er-
rors in non-native writing, with a specific focus on
difficulties in preposition and article usage (Han et
al., 2006; Chodorow et al, 2007; Felice and Pul-
man, 2007; Tetreault and Chodorow, 2008; Gamon,
2010).
We annotate papers using two pieces of associated
meta-data: (1) author first names and (2) countries
of affiliation. We manually marked each country for
whether English is predominantly spoken there. We
then built a list of common first names of English
speakers via the top 150 male and female names
from the U.S. census.3 If the first author of a pa-
per has an English first name and English-speaking-
country affiliation, we mark as NES.4 If none of the
authors have an English first name nor an English-
speaking-country affiliation, we mark as NNS. We
use this rule to label our development and test data,
as well as our Strict training set. For Lenient train-
ing, we decide based solely on whether the first au-
thor is from an English-speaking country.
4.2 Venue: Top-Tier vs. Workshop
This novel task aims to distinguish top-tier papers
from those at workshops, based on style. We use
the annual meeting of the ACL as our canonical top-
tier venue. For evaluation and Strict training, we la-
bel all main-session ACL papers as top-tier, and all
workshop papers as workshop. For Lenient training,
we assign all conferences (LREC, Coling, EMNLP,
etc.) to be top-tier except for their non-main-session
papers, which we label as workshop.
4.3 Gender: Male vs. Female
Because we are classifying an international set of
authors, U.S. census names (the usual source of
gender ground-truth) provide incomplete informa-
tion. We therefore use the data of Bergsma and Lin
(2006).5 This data has been widely used in corefer-
ence resolution but never in stylometry. Each line
in the data lists how often a noun co-occurs with
male, female, neutral and plural pronouns; this is
commonly taken as an approximation of the true
gender distribution. E.g., ?bill clinton? is 98% male
(in 8344 instances) while ?elsie wayne? is 100% fe-
male (in 23). The data also has aggregate counts
over all nouns with the same first token, e.g., ?elsie
...? is 94% female (in 255 instances). For Strict
training/evaluation, we label papers with the fol-
lowing rule based on the first author?s first name:
3
www.census.gov/genealogy/names/names_files.
html We also manually added common nicknames for these,
e.g. Rob for Robert, Chris for Christopher, Dan for Daniel, etc.
4Of course, assuming the first author writes each paper is
imperfect. In fact, for some native/non-native collaborations,
our system ultimately predicts the 2nd (non-native) author to be
the main writer; in one case we confirmed the accuracy of this
prediction by personal communication with the authors.
5
www.clsp.jhu.edu/
?
sbergsma/Gender/
329
if the name has an aggregate count >30 and fe-
male probability >0.85, label as female; otherwise
if the aggregate count is >30 and male probabil-
ity >0.85, label male. This rule captures many of
ACL?s unambiguously-gendered names, both male
(Nathanael, Jens, Hiroyuki) and female (Widad,
Yael, Sunita). For Lenient training, we assign all
papers based only on whether the male or female
probability for the first author is higher. While po-
tentially noisy, there is precedent for assigning a sin-
gle gender to papers ?co-authored by researchers of
mixed gender? (Sarawgi et al, 2011).
5 Models and Training Strategies
Model: We take a discriminative approach to sty-
lometry, representing articles as feature vectors (?6)
and classifying them using a linear, L2-regularized
SVM, trained via LIBLINEAR (Fan et al, 2008).
SVMs are state-of-the-art and have been used pre-
viously in stylometry (Koppel et al, 2005).
Strategy: We test whether it?s better to train with
a smaller, more accurate Strict set, or a larger but
noisier Lenient set. We also explore a third strategy,
motivated by work in learning from noisy web im-
ages (Bergamo and Torresani, 2010), in which we
fix the Strict labels, but also include the remaining
examples as unlabeled instances. We then optimize
a Transductive SVM, solving an optimization prob-
lem where we not only choose the feature weights,
but also labels for unlabeled training points. Like
a regular SVM, the goal is to maximize the margin
between the positive and negative vectors, but now
the vectors have both fixed and imputed labels. We
optimize using Joachims (1999)?s software. While
the classifier is trained using a transductive strategy,
it is still tested inductively, i.e., on unseen data.
6 Stylometric Features
Koppel et al (2003) describes a range of features
that have been used in stylometry, ranging from
early manual selection of potentially discriminative
words, to approaches based on automated text cat-
egorization (Sebastiani, 2002). We use the follow-
ing three feature classes; the particular features were
chosen based on development experiments.
6.1 Bow Features
A variety of ?discouraging results? in the text cate-
gorization literature have shown that simple bag-of-
words (Bow) representations usually perform better
than ?more sophisticated? ones (e.g. using syntax)
(Sebastiani, 2002). This was also observed in sen-
timent classification (Pang et al, 2002). One key
aim of our research is to see whether this is true of
scientific stylometry. Our Bow representation uses
a feature for each unique lower-case word-type in
an article. We also preprocess papers by making all
digits ?0?. Normalizing digits and filtering capital-
ized words helps ensure citations and named-entities
are excluded from our features. The feature value is
the log-count of how often the corresponding word
occurs in the document.
6.2 Style Features
While text categorization relies on keywords, sty-
lometry focuses on topic-independent measures like
function word frequency (Mosteller and Wallace,
1984), sentence length (Yule, 1939), and PoS (Hirst
and Feiguina, 2007). We define a style-word to be:
(1) punctuation, (2) a stopword, or (3) a Latin abbre-
viation.6 We create Style features for all unigrams
and bigrams, replacing non-style-words separately
with both PoS-tags and spelling signatures.7 Each
feature is an N-gram, the value is its log-count in the
article. We also include stylistic meta-features such
as mean-words-per-sentence and mean-word-length.
6.3 Syntax Features
Unlike recent work using generative PCFGs (Ragha-
van et al, 2010; Sarawgi et al, 2011), we use syntax
directly as features in discriminative models, which
can easily incorporate arbitrary and overlapping syn-
tactic clues. For example, we will see that one indi-
cator of native text is the use of certain determin-
ers as stand-alone noun phrases (NPs), like this in
Figure 2. This contrasts with a proposed non-native
phrase, ?this/DT growing/VBG area/NN,? where this
instead modifies a noun. The Bow features are
clearly unhelpful: this occurs in both cases. The
6The stopword list is the standard set of 524 SMART-system
stopwords (following Tomokiyo and Jones (2001)). Latin ab-
breviations are i.e., e.g., etc., c.f., et or al.
7E.g., signature ?LC-ing? means lower-case, ending in ing.
These are created via a script included with the Berkeley parser.
330
we did this using . . .
PRP VBD DT VBG
NPNP
VP
. . .
Figure 2: Motivating deeper syntactic features: The
shaded TSG fragment indicates native English, but is not
directly encoded in Bow, Style, nor standard CFG-rules.
Style features are likewise unhelpful; this-VBG also
occurs in both cases. We need the deeper knowledge
that a specific determiner is used as a complete NP.
We evaluate three feature types that aim to cap-
ture such knowledge. In each case, we aggregate the
feature counts over all the parse trees constituting a
document. The feature value is the log-count of how
often each feature occurs. To remove content infor-
mation from the features, we preprocess the parse
tree terminals: all non-style-word terminals are re-
placed with their spelling signature (see ?6.2).
CFG Rules: We include a feature for every unique,
single-level context-free-grammar (CFG) rule appli-
cation in a paper (following Baayen et al (1996),
Gamon (2004), Hirst and Feiguina (2007), Wong
and Dras (2011)). The Figure 2 tree would have
features: NP?PRP, NP?DT, DT?this, etc. Such fea-
tures do capture that a determiner was used as an NP,
but they do not jointly encode which determiner was
used. This is an important omission; we?ll see that
other determiners acting as stand-alone NPs indicate
non-native writing (e.g., the word that, see ?7.2).
TSG Fragments: A tree-substitution grammar is a
generalization of CFGs that allow rewriting to tree
fragments rather than sequences of non-terminals
(Joshi and Schabes, 1997). Figure 2 gives the exam-
ple NP?(DT this). This fragment captures both the
identity of the determiner and its syntactic function
as an NP, as desired. Efficient Bayesian procedures
have recently been developed that enable the train-
ing of large-scale probabilistic TSG grammars (Post
and Gildea, 2009; Cohn et al, 2010).
While TSGs have not been used previously in sty-
lometry, Post (2011) uses them to predict sentence
grammaticality (i.e. detecting pseudo-sentences fol-
lowing Okanohara and Tsujii (2007) and Cherry and
Quirk (2008)). We use Post?s TSG training settings
and his public code.8 We parse with the TSG gram-
mar and extract the fragments as features. We also
follow Post by having features for aggregate TSG
statistics, e.g., how many fragments are of a given
size, tree-depth, etc. These syntactic meta-features
are somewhat similar to the manually-defined stylo-
metric features of Stamatatos et al (2001).
C&J Reranking Features: We also extracted the
reranking features of Charniak and Johnson (2005).
These features were hand-crafted for reranking the
output of a parser, but have recently been used for
other NLP tasks (Post, 2011; Wong and Dras, 2011).
They include lexicalized features for sub-trees and
head-to-head dependencies, and aggregate features
for conjunct parallelism and the degree of right-
branching. We get the features using another script
from Post.9 While TSG fragments tile a parse tree
into a few useful fragments, C&J features can pro-
duce thousands of features per sentence, and are thus
much more computationally-demanding.
7 Experiments and Results
We take the minority class as the positive class:
NES for NativeL, top-tier for Venue and female for
Gender, and calculate the precision/recall of these
classes. We tune three hyperparameters for F1-
score on development data: (1) the SVM regular-
ization parameter, (2) the threshold for classifying
an instance as positive (using the signed hyperplane-
distance as the score), and (3) for transductive train-
ing (?5), the fraction of unlabeled data to label as
positive. Statistical significance on held-out test data
is assessed with McNemar?s test, p<0.05. For F1-
score, we use the following reasonable Baseline: we
label all instances with the label of the minority class
(achieving 100% recall but low precision).
7.1 Selection of Syntax and Training Strategy
Development experiments showed that using all fea-
tures, Bow+Style+Syntax, works best on all tasks,
but there was no benefit in combining different
8http://github.com/mjpost/dptsg
9http://github.com/mjpost/extract-spfeatures.
331
Syntax Strategy NativeL Venue Gender
Baseline 50.5 45.0 28.7
CFG Strict 93.5 59.9 42.5
CFG Lenient 89.9 64.9 39.5
TSG Strict 93.6 60.7 40.0
TSG Lenient 90.9 64.4 39.1
C&J Strict 90.5 62.3 37.1
C&J Lenient 86.2 65.2 39.0
Table 2: F1 scores for Bow+Style+Syntax system on de-
velopment data: The best training strategy and the best
syntactic features depend on the task.
Syntax features. We also found no gain from trans-
ductive training, but greater cost, with more hyper-
parameter tuning and a slower SVM solver. The
best Syntax features depend on the task (Table 2).
Whether Strict or Lenient training: TSG was best
for NativeL, C&J was best for Venue, and CFG was
best for Gender. These trends continue on test data,
where TSG exceeds CFG (91.6% vs. 91.2%). For
the training strategy, Strict was best on NativeL and
Gender, while Lenient was best on Venue (Table 2).
This latter result is interesting: recall that for Venue,
Lenient training considers all conferences to be top-
tier, but evaluation is just on detecting ACL papers.
We suggest some reasons for this below, highlight-
ing some general features of conference papers that
extend beyond particular venues.
For the remainder of experiments on each task,
we fix the syntactic features and training strategy to
those that performed best on development data.
7.2 Test Results and Feature Analysis
Gender remains the most difficult task on test data,
but our F1 still substantially outperforms the base-
line (Table 3). Results on NativeL are particu-
larly impressive; in terms of accuracy, we classify
94.6% of test articles correctly (the majority-class
baseline is 66.9%). Regarding features, just using
Style+Syntax always works better than using Bow.
Combining all features always works better still.
The gains of Bow+Style+Syntax over vanilla Bow are
statistically significant in each case.
We also highlight important individual features:
NativeL: Table 4 gives Bow and Style features
for NativeL. Some reflect differences in common
Features NativeL Venue Gender
Baseline 49.8 45.5 33.1
Bow 88.8 60.7 42.5
Style 90.6 61.9 39.8
Syntax 88.7 64.6 41.2
Bow+Style 90.4 64.0 45.1
Bow+Syntax 90.3 65.8 42.9
Style+Syntax 89.4 65.5 43.3
Bow+Style+Syntax 91.6 66.7 48.2
Table 3: F1 scores with different features on held-out test
data: Including style and syntactic features is superior to
standard Bow features in all cases.
native/non-native topics; e.g., ?probabilities? pre-
dicts native while ?morphological? predicts non-
native. Several features, like ?obtained?, indicate L1
interference; i.e., many non-natives have a cognate
for obtain in their native language and thus adopt the
English word. As an example, the word obtained
occurs 3.7 times per paper from Spanish-speaking
areas (cognate obtenir) versus once per native paper
and 0.8 times per German-authored paper.
Natives also prefer certain abbreviations (e.g.
?e.g.?) while non-natives prefer others (?i.e.?, ?c.f.?,
?etc.?). Exotic punctuation also suggests native text:
the semi-colon, exclamation and question mark all
predict NES. Note this also varies by region; semi-
colons are most popular in NES countries but papers
from Israel and Italy are close behind.
Table 5 gives highly-weighted TSG features for
predicting NativeL. Note the determiner-as-NP us-
age described earlier (? 6.3): these, this and each
predict native when used as an NP; that-as-an-NP
predicts non-native. Furthermore, while not all na-
tive speakers use a comma before a conjunction in
a list, it?s nevertheless a good flag for native writ-
ing (?NP?NP, NP, (CC and) NP?). In terms of non-
native syntax, the passive voice is more common
(?VP?(VBZ is) VP? and ?VP?VBN (PP (IN as) NP)?).
We also looked for features involving determiners
since correct determiner usage is a common diffi-
culty for non-native speakers. We found cases where
determiners were missing where natives might have
used one (?NP?JJ JJ NN?), but also those where a de-
terminer might be optional and skipped by a native
speaker (?NP?(DT the) NN NNS?). Note that Table 5
332
Predicts native Predicts non-native
Bow feature Wt. Bow feature Wt.
initial 2.25 obtained -2.15
techniques 2.11 proposed -2.06
probabilities 1.38 method -2.06
additional 1.23 morphological -1.96
fewer 1.02 languages -1.23
Style feature Wt. Style feature Wt.
used to 1.92 , i.e. -2.60
JJR NN 1.90 have to -1.65
has VBN 1.90 the xxxx-ing -1.61
example , 1.75 thus -1.61
all of 1.73 usually -1.24
?s 1.69 mainly -1.21
allow 1.47 , because -1.12
has xxxx-ed 1.45 the VBN -1.12
may be 1.35 JJ for -1.11
; and 1.21 cf -0.97
e.g. 1.10 etc. -0.55
must VB 0.99 associated to -0.23
Table 4: NativeL: Examples of highly-weighted style and
content features in the Bow+Style+Syntax system.
examples are based on actual usage in ACL papers.
We also found that complex NPs were more asso-
ciated with native text. Features such as ?NP?DT JJ
NN NN NN?, and ?NP?DT NN NN NNS? predict native
writing.
Non-natives also rely more on boilerplate. For
example, the exact phrase ?The/This paper is orga-
nized as follows? occurs 3 times as often in non-
native compared to native text (in 7.5% of all non-
native papers). Sentence re-use is only indirectly
captured by our features; it would be interesting to
encode flags for it directly.
In general, we found very few highly-weighted
features that pinpoint ?ungrammatical? non-native
writing (the feature ?associated to? in Table 4 is a
rare example). Our classifiers largely detect non-
native writing on a stylistic rather than grammatical
basis.
Venue: Table 6 provides important Bow and Style
features for the Venue task (syntactic features omit-
ted due to space). While some features are topical
(e.g. ?biomedical?), the table gives a blueprint for
writing a solid main-conference paper. That is, good
papers often have an explicit probability model (or
algorithm), experimental baselines, error analysis,
TSG Fragment Example
Predicts native English author:
NP?NNP CD (Model) (1)
NP?(DT these) six of (these)
NP?(DT that) NN in (that) (language)
NP?(DT this) we did (this) using ...
VP?(VBN used) S (used) (to describe it)
NP?NP, NP, (CC and) NP (X), (Y), (and) (Z)
NP?(DT each) (each) consists of ...
Predicts non-native English author:
VP?(VBZ is) VP it (is) (shown below)
VP?VBN (PP (IN as) NP) (considered) (as) (a term)
NP?JJ JJ NN in (other) (large) (corpus)
NP?DT JJ (CD one) (a) (correct) (one)
NP?(DT the) NN NNS seen in (the) (test) (data)
NP?(DT that) larger than (that) of ...
QP?(IN about) CD (about) (200,000) words
Table 5: NativeL: Highly-weighted syntactic features
(descending order of absolute weight) and examples in
the Bow+Style+Syntax system.
and statistical significance checking. On the other
hand, there might be a bias at main conferences for
focused, incremental papers; features of workshop
papers highlight the exploration of ?interesting? new
ideas/domains. Here, the objective might only be to
show what is ?possible? or what one is ?able to? do.
Main conference papers prefer work that improves
?performance? by ?#%? on established tasks.
Gender: The CFG features for Gender are given
in Table 7. Several of the most highly-weighted
female features include pronouns (e.g. PRP$). A
higher frequency of pronouns in female writing has
been attested previously (Argamon et al, 2003), but
has not been traced to particular syntactic construc-
tions. Likewise, we observe a higher frequency of
not just negation (noted previously) but adverbs (RB)
in general (e.g. ?VP?MD RB VP?). In terms of Bow
features (not shown), the words contrast and com-
parison highly predict female, as do topical clues
like verb and resource. The top-three male Bow fea-
tures are (in order): simply, perform, parsing.
7.3 Author Rankings
While our objective is to predict attributes of pa-
pers, we also show how that we can identify author
attributes using a larger body of work. We make
NativeL and Gender predictions for all papers in the
333
Predicts ACL Predicts Workshop
Bow feature Wt. Bow feature Wt.
model 2.64 semantic -2.16
probability 1.66 analysis -1.65
performance 1.40 verb -1.35
baseline 1.36 lexical -1.33
= 1.26 study -0.92
algorithm 1.18 biomedical -0.87
large 1.16 preliminary -0.69
error 1.15 interesting -0.69
outperforms 1.02 aim -0.64
significant 0.96 manually -0.62
statistically 0.75 appears -0.54
Style feature Wt. Style feature Wt.
by VBG 1.04 able to -0.99
#% 0.82 xxxx-ed out -0.77
NN over 0.79 further NN -0.71
than the 0.79 NN should -0.69
improvement 0.75 will be -0.61
best 0.71 possible -0.57
xxxx-s by 0.70 have not -0.56
much JJR 0.67 currently -0.56
Table 6: Venue: Examples of highly-weighted style con-
tent features in the Bow+Style+Syntax system.
1990-2000 era using our Bow+Style+Syntax system.
For each author+affiliation with ?3 first-authored
papers, we take the average classifier score on these
papers.
Table 8 shows cases where our model strongly
predicts native, showing top authors with foreign af-
filiations and top authors in English-speaking coun-
tries.10 While not perfect, the predictions correctly
identify some native authors that would be difficult
to detect using only name and location data. For ex-
ample, Dekai Wu (Hong Kong) speaks English na-
tively; Christer Samuelsson lists near-native English
on his C.V.; etc. Likewise, we have also been able
to accurately identify a set of non-native speakers
with common American names that were working
at American universities.
Table 9 provides some of the extreme predictions
of our system on Gender. The extreme male and fe-
male predictions are based on both style and content;
females tend to work on summarization, discourse,
10Note again that this is based on the affiliation of these au-
thors during the 1990s; e.g. Gerald Penn published three papers
while at the University of Tu?bingen.
CFG Rule Example
Predicts female author:
NP?PRP$ NN NN (our) (upper) (bound)
QP?RB CD (roughly) (6000)
NP?NP, CC NP (a new NE tag), (or) (no NE tag)
NP?PRP$ JJ JJ NN (our) (first) (new) (approach)
VP?MD RB VP (may) (not) (be useful)
ADVP?RB RBR (significantly) (more)
Predicts male author:
ADVP?RB RB (only) (superficially)
NP?NP, SBAR we use (XYZ), (which is ...)
S?S: S. (Trust me): (I?m a doctor)
S?S, NP VP (To do so), (it) (needs help)
WHNP?WP NN depending on (what) (path) is ...
PP?IN PRN (in) ((Jelinek, 1976))
Table 7: Gender: Highly-weighted syntactic features
(descending order of weight) and examples in the
Bow+Style+Syntax system.
Highest NES Scores, non-English-country: Gerald
Penn,10 Ezra W. Black, Nigel Collier, Jean-Luc Gauvain,
Dan Cristea, Graham J. Russell, Kenneth R. Beesley,
Dekai Wu, Christer Samuelsson, Raquel Martinez
Highest NES Scores, English-country: Eric V. Siegel,
Lance A. Ramshaw, Stephanie Seneff, Victor W. Zue,
Joshua Goodman, Patti J. Price, Stuart M. Shieber, Jean
Carletta, Lynn Lambert, Gina-Anne Levow
Table 8: Authors scoring highest on NativeL, in descend-
ing order, based exclusively on article text.
etc., while many males focus on parsing. We also
tried making these lists without Bow features, but
the extreme examples still reflect topic to some ex-
tent. Topics themselves have their own style, which
the style features capture; it is difficult to fully sepa-
rate style from topic.
7.4 Correlation with Citations
We also test whether our systems? stylometric scores
correlate with the most common bibliometric mea-
sure: citation count. To reduce the impact of topic,
we only use Style+Syntax features. We plot re-
sults separately for ACL, Coling and Workshop pa-
pers (1990-2000 era). Papers at each venue are
sorted by their classifier scores and binned into five
score bins. Each point in the plot is the mean-
score/mean-number-of-citations for papers in a bin
(within-community citation data is via the AAN ?3
334
Highest Model Scores (Male): John Aberdeen,
Chao-Huang Chang, Giorgio Satta, Stanley F. Chen,
GuoDong Zhou, Carl Weir, Akira Ushioda, Hideki
Tanaka, Koichi Takeda, Douglas B. Paul, Hideo Watan-
abe, Adam L. Berger, Kevin Knight, Jason M. Eisner
Highest Model Scores (Female): Julia B. Hirschberg,
Johanna D. Moore, Judy L. Delin, Paola Merlo,
Rebecca J. Passonneau, Bonnie Lynn Webber, Beth
M. Sundheim, Jennifer Chu-Carroll, Ching-Long Yeh,
Mary Ellen Okurowski, Erik-Jan Van Der Linden
Table 9: Authors scoring highest (absolute values) on
Gender, in descending order, based exclusively on arti-
cle text.
and excludes self citations). We use a truncated
mean for citation counts, leaving off the top/bottom
five papers in each bin.
For NativeL, we only plot papers marked as na-
tive by our Strict rule (i.e. English name/country).
Papers with the lowest NativeL-scores receive many
fewer citations, but they soon level off (Figure 3(a)).
Many junior researchers at English universities are
non-native speakers; early-career non-natives might
receive fewer citations than well-known peers. The
correlation between citations and Venue-scores is
even stronger (Figure 3(b)); the top-ranked work-
shop papers receive five times as many citations
as the lowest ones, and are cited better than a
good portion of ACL papers. These figures sug-
gest that citation-predictors can get useful informa-
tion beyond typical Bow features (Yogatama et al,
2011). Although we focused on a past era, stylis-
tic/syntactic features should also be more robust to
the evolution of scientific topics; we plan to next test
whether we can better forecast future citations. It
would also be interesting to see whether these trends
transfer to other academic disciplines.
7.5 Further Experiments on NativeL
For NativeL, we also created a special test corpus of
273 papers written by first-time ACL authors (2008-
2009 era). This set closely aligns with the system?s
potential use as a tool to help new authors compose
papers. Two (native-speaking) annotators manually
annotated each paper for whether it was primarily
written by a native or non-native speaker (consid-
ering both content and author names/affiliations).
The annotators agreed on 90% of decisions, with an
 1
 10
 0.3  0.4  0.5  0.6  0.7  0.8
NativeL-Score
ACL
Coling
Workshop
(a)
 1
 10
 0.2  0.3  0.4  0.5  0.6
Venue-Score
ACL
Coling
Workshop
(b)
Figure 3: Correlation between predictions (x-axis) and
mean number of citations (y-axis, log-scale).
inter-annotator kappa of 66%. We divided the papers
into a test set and a development set. We applied our
Bow+Style+Syntax system exactly as trained above,
except we tuned its hyperparameters on the new de-
velopment data. The system performed quite well
on this set, reaching 68% F1 over a baseline of only
27%. Moreover, the system also reached 90% accu-
racy, matching the level of human agreement.
8 Conclusion
We have proposed, developed and successfully eval-
uated significant new tasks and methods in the sty-
lometric analysis of scientific articles, including the
novel resolution of publication venue based on pa-
per style, and novel syntactic features based on tree
substitution grammar fragments. In all cases, our
syntactic and stylistic features significantly improve
over a bag-of-words baseline, achieving 10% to 25%
relative error reduction in all three major tasks. We
have included a detailed and insightful analysis of
discriminative stylometric features, and we showed
a strong correlation between our predictions and a
paper?s number of citations. We observed evidence
for L1-interference in non-native writing, for dif-
ferences in topic between males and females, and
for distinctive language usage which can success-
fully identify papers published in top-tier confer-
ences versus wokrshop proceedings. We believe that
this work can stimulate new research at the intersec-
tion of computational linguistics and bibliometrics.
335
References
Shlomo Argamon, Moshe Koppel, Jonathan Fine, and
Anat Rachel Shimoni. 2003. Gender, genre, and writ-
ing style in formal written texts. Text, 23(3), August.
Harald Baayen, Fiona Tweedie, and Hans van Halteren.
1996. Outside the cave of shadows: Using syntactic
annotation to enhance authorship attribution. Literary
and Linguistic Computing, 11(3):121?132.
Alessandro Bergamo and Lorenzo Torresani. 2010. Ex-
ploiting weakly-labeled web images to improve object
classification: a domain adaptation approach. In Proc.
NIPS, pages 181?189.
Shane Bergsma and Dekang Lin. 2006. Bootstrapping
path-based pronoun resolution. In Proc. Coling-ACL,
pages 33?40.
Christine L. Borgman and Jonathan Furner. 2001. Schol-
arly communication and bibliometrics. Annual Review
of Information Science and Technology, 36:3?72.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proc. ACL, pages 173?180.
Colin Cherry and Chris Quirk. 2008. Discriminative,
syntactic language modeling through latent SVMs. In
Proc. AMTA.
Martin Chodorow, Joel R. Tetreault, and Na-Rae Han.
2007. Detection of grammatical errors involving
prepositions. In Proc. ACL-SIGSEM Workshop on
Prepositions, pages 25?30.
Trevor Cohn, Phil Blunsom, and Sharon Goldwater.
2010. Inducing tree-substitution grammars. J. Mach.
Learn. Res., 11:3053?3096.
Robert Dale and Adam Kilgarriff. 2010. Helping our
own: Text massaging for computational linguistics as
a new shared task. In Proc. 6th International Natural
Language Generation Conference, pages 261?265.
Jacob Eisenstein, Noah A. Smith, and Eric P. Xing.
2011. Discovering sociolinguistic associations with
structured sparsity. In Proc. ACL, pages 1365?1374.
Dominique Estival, Tanja Gaustad, Son-Bao Pham, Will
Radford, and Ben Hutchinson. 2007. Author profiling
for English emails. In Proc. PACLING, pages 263?
272.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. J. Mach. Learn.
Res., 9:1871?1874.
Rachele De Felice and Stephen G. Pulman. 2007. Au-
tomatically acquiring models of preposition use. In
Proc. ACL-SIGSEM Workshop on Prepositions, pages
45?50.
Michael Gamon. 2004. Linguistic correlates of style:
authorship classification with deep linguistic analysis
features. In Proc. Coling, pages 611?617.
Michael Gamon. 2010. Using mostly native data to cor-
rect errors in learners? writing: a meta-classifier ap-
proach. In Proc. HLT-NAACL, pages 163?171.
Sean Gerrish and David M. Blei. 2010. A language-
based approach to measuring scholarly impact. In
Proc. ICML, pages 375?382.
Angela Glover and Graeme Hirst. 1995. Detecting
stylistic inconsistencies in collaborative writing. In
Writers at work: Professional writing in the comput-
erized environment, pages 147?168.
David Hall, Daniel Jurafsky, and Christopher D. Man-
ning. 2008. Studying the history of ideas using topic
models. In Proc. EMNLP, pages 363?371.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by non-
native speakers. Nat. Lang. Eng., 12(2):115?129.
Shawndra Hill and Foster Provost. 2003. The myth of
the double-blind review?: Author identification using
only citations. SIGKDD Explor. Newsl., 5:179?184.
Graeme Hirst and Ol?ga Feiguina. 2007. Bigrams of
syntactic labels for authorship discrimination of short
texts. Literary and Linguistic Computing, 22(4):405?
417.
Thorsten Joachims. 1999. Transductive inference for
text classification using support vector machines. In
Proc. ICML, pages 200?209.
Nikhil Johri, Daniel Ramage, Daniel McFarland, and
Daniel Jurafsky. 2011. A study of academic collabo-
rations in computational linguistics using a latent mix-
ture of authors model. In Proc. 5th ACL-HLT Work-
shop on Language Technology for Cultural Heritage,
Social Sciences, and Humanities, pages 124?132.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
adjoining grammars. In G. Rozenberg and A. Salo-
maa, editors, Handbook of Formal Languages: Beyond
Words, volume 3, pages 71?122.
Moshe Koppel, Shlomo Argamon, and Anat Rachel Shi-
moni. 2003. Automatically categorizing written texts
by author gender. Literary and Linguistic Computing,
17(4):401?412.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005.
Determining an author?s native language by mining a
text for errors. In Proc. KDD, pages 624?628.
Xuan Le, Ian Lancashire, Graeme Hirst, and Regina
Jokel. 2011. Longitudinal detection of dementia
through lexical and syntactic changes in writing: A
case study of three British novelists. Literary and Lin-
guistic Computing, 26(4):435?461.
Frederick Mosteller and David L. Wallace. 1984. Ap-
plied Bayesian and Classical Inference: The Case of
the Federalist Papers. Springer-Verlag.
Greg Myers. 1989. The pragmatics of politeness in sci-
entific articles. Applied Linguistics, 10(1):1?35.
336
Daisuke Okanohara and Jun?ichi Tsujii. 2007. A
discriminative language model with pseudo-negative
samples. In Proc. ACL, pages 73?80.
Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T. Han-
cock. 2011. Finding deceptive opinion spam by any
stretch of the imagination. In Proc. ACL, pages 309?
319.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: Sentiment classification using
machine learning techniques. In Proc. EMNLP, pages
79?86.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. Coling-ACL, pages
433?440.
Xuan-Hieu Phan. 2006. CRFTagger: CRF English POS
Tagger. crftagger.sourceforge.net.
Matt Post and Daniel Gildea. 2009. Bayesian learning
of a tree substitution grammar. In Proc. ACL-IJCNLP,
pages 45?48.
Matt Post. 2011. Judging grammaticality with tree sub-
stitution grammar derivations. In Proc. ACL, pages
217?222.
Vahed Qazvinian and Dragomir R. Radev. 2008. Scien-
tific paper summarization using citation summary net-
works. In Proc. Coling, pages 689?696.
Dragomir R. Radev, Mark Thomas Joseph, Bryan Gib-
son, and Pradeep Muthukrishnan. 2009a. A biblio-
metric and network analysis of the field of computa-
tional linguistics. Journal of the American Society for
Information Science and Technology.
Dragomir R. Radev, Pradeep Muthukrishnan, and Vahed
Qazvinian. 2009b. The ACL anthology network cor-
pus. In Proc. ACL Workshop on Natural Language
Processing and Information Retrieval for Digital Li-
braries, pages 54?61.
Sindhu Raghavan, Adriana Kovashka, and Raymond
Mooney. 2010. Authorship attribution using proba-
bilistic context-free grammars. In Proc. ACL, pages
38?42.
Delip Rao, Michael Paul, Clay Fink, David Yarowsky,
Timothy Oates, and Glen Coppersmith. 2011. Hierar-
chical bayesian models for latent attribute detection in
social media. In Proc. ICWSM, pages 598?601.
Ruchita Sarawgi, Kailash Gajulapalli, and Yejin Choi.
2011. Gender attribution: tracing stylometric evidence
beyond topic and genre. In Proc. CoNLL, pages 78?
86.
Fabrizio Sebastiani. 2002. Machine learning in auto-
mated text categorization. ACM Comput. Surv., 34:1?
47.
Efstathios Stamatatos, Nikos Fakotakis, and George
Kokkinakis. 2001. Automatic text categorization in
terms of genre and author. Computational Linguistics,
26(4):471?495.
Joel R. Tetreault and Martin Chodorow. 2008. The ups
and downs of preposition error detection in ESL writ-
ing. In Proc. Coling, pages 865?872.
Simone Teufel and Marc Moens. 2002. Summariz-
ing scientific articles - experiments with relevance
and rhetorical status. Computational Linguistics,
28(4):409?445.
Laura Mayfield Tomokiyo and Rosie Jones. 2001.
You?re not from ?round here, are you? Naive Bayes
detection of non-native utterances. In Proc. NAACL.
Oren Tsur and Ari Rappoport. 2007. Using classi-
fier features for studying the effect of native language
on the choice of written second language words. In
Proc. Workshop on Cognitive Aspects of Computa-
tional Language Acquisition, pages 9?16.
Irena Vassileva. 1998. Who am I/who are we in aca-
demic writing? International Journal of Applied Lin-
guistics, 8(2):163?185.
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploiting
parse structures for native language identification. In
Proc. EMNLP, pages 1600?1610.
Dani Yogatama, Michael Heilman, Brendan O?Connor,
Chris Dyer, Bryan R. Routledge, and Noah A. Smith.
2011. Predicting a scientific community?s response to
an article. In Proc. EMNLP, pages 594?604.
G. Udny Yule. 1939. On sentence-length as a statis-
tical characteristic of style in prose: With applica-
tion to two cases of disputed authorship. Biometrika,
30(3/4):363?390.
337
Proceedings of NAACL-HLT 2013, pages 1010?1019,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Broadly Improving User Classification via
Communication-Based Name and Location Clustering on Twitter
Shane Bergsma, Mark Dredze, Benjamin Van Durme, Theresa Wilson, David Yarowsky
Department of Computer Science and Human Language Technology Center of Excellence
Johns Hopkins University
Baltimore, MD 21218, USA
shane.a.bergsma@gmail.com, mdredze@cs.jhu.edu, vandurme@cs.jhu.edu, taw@jhu.edu, yarowsky@cs.jhu.edu
Abstract
Hidden properties of social media users, such
as their ethnicity, gender, and location, are of-
ten reflected in their observed attributes, such
as their first and last names. Furthermore,
users who communicate with each other of-
ten have similar hidden properties. We pro-
pose an algorithm that exploits these insights
to cluster the observed attributes of hundreds
of millions of Twitter users. Attributes such
as user names are grouped together if users
with those names communicate with other
similar users. We separately cluster millions
of unique first names, last names, and user-
provided locations. The efficacy of these clus-
ters is then evaluated on a diverse set of clas-
sification tasks that predict hidden users prop-
erties such as ethnicity, geographic location,
gender, language, and race, using only pro-
file names and locations when appropriate.
Our readily-replicable approach and publicly-
released clusters are shown to be remarkably
effective and versatile, substantially outper-
forming state-of-the-art approaches and hu-
man accuracy on each of the tasks studied.
1 Introduction
There is growing interest in automatically classify-
ing users in social media by various hidden prop-
erties, such as their gender, location, and language
(e.g. Rao et al (2010), Cheng et al (2010), Bergsma
et al (2012)). Predicting these and other proper-
ties for users can enable better advertising and per-
sonalization, as well as a finer-grained analysis of
user opinions (O?Connor et al, 2010), health (Paul
and Dredze, 2011), and sociolinguistic phenomena
(Eisenstein et al, 2011). Classifiers for user prop-
erties often rely on information from a user?s social
network (Jernigan and Mistree, 2009; Sadilek et al,
2012) or the textual content they generate (Pennac-
chiotti and Popescu, 2011; Burger et al, 2011).
Here, we propose and evaluate classifiers that bet-
ter exploit the attributes that users explicitly provide
in their user profiles, such as names (e.g., first names
like Mary, last names like Smith) and locations (e.g.,
Brasil). Such attributes have previously been used as
?profile features? in supervised user classifiers (Pen-
nacchiotti and Popescu, 2011; Burger et al, 2011;
Bergsma et al, 2012). There are several motivations
for exploiting these data. Often the only informa-
tion available for a user is a name or location (e.g.
for a new user account). Profiles also provide an
orthogonal or complementary source of information
to a user?s social network and textual content; gains
based on profiles alone should therefore add to gains
based on other data. The decisions of profile-based
classifiers could also be used to bootstrap training
data for other classifiers that use complementary fea-
tures.
Prior work has encoded profile attributes via lex-
ical or character-based features (e.g. Pennacchiotti
and Popescu (2011), Burger et al (2011), Bergsma
et al (2012)). Unfortunately, due to the long-tailed
distribution of user attributes, a profile-based classi-
fier will encounter many examples at test time that
were not observed during training. For example,
suppose a user wassim hassan gives their location as
tanger. If the attribute tokens wassim, hassan, and
tanger do not occur in training (nor indicative sub-
1010
strings), then a classifier can only guess at the user?s
ethnicity and location. In social media, the preva-
lence of fake names and large variations in spelling,
slang, and language make matters worse.
Our innovation is to enhance attribute-based clas-
sifiers with new data, derived from the communica-
tions of Twitter users with those attributes. Users
with the name tokens wassim and hassan often talk
to users with Arab names like abdul and hussein.
Users listing their location as tanger often talk to
users from morocco. Since users who communicate
often share properties such as ethnicity and location
(?8), the user wassim hassan might be an Arab who
uses the French spelling of the city Tangier.
Our challenge is to encode these data in a form
readily usable by a classifier. Our approach is to
represent each unique profile attribute (e.g. tanger
or hassan) as a vector that encodes the communi-
cation pattern of users with that attribute (e.g. how
often they talk to users from morocco, etc.); we then
cluster the vectors to discover latent groupings of
similar attributes. Based on transitive (third party)
connections, tanger and tangier can appear in the
same cluster, even if no two users from these loca-
tions talk directly. To use the clusters in an attribute-
based classifier, we add new features that indicate
the cluster memberships of the attributes. Clustering
thus lets us convert a high-dimensional space of all
attribute pairs to a low-dimensional space of cluster
memberships. This makes it easier to share our data,
yields fewer parameters for learning, and creates at-
tribute groups that are interpretable to humans.
We cluster names and locations in a very large
corpus of 168 million Twitter users (?2) and use a
distributed clustering algorithm to separately clus-
ter millions of first names, last names, and user-
provided locations (?3). We evaluate the use of our
cluster data as a novel feature in supervised classi-
fiers, and compare our result to standard classifiers
using character and token-level features (?4). The
cluster data enables significantly improved perfor-
mance in predicting the gender, location, and lan-
guage of social media users, exceeding both ex-
isting state-of-the-art machine and human perfor-
mance (?6). Our cluster data can likewise im-
prove performance in other domains, on both es-
tablished and new NLP tasks as further evaluated
in this paper (?6). We also propose a way to
First names: maria, david, ana, daniel, michael, john,
alex, jessica, carlos, jose, chris, sarah, laura, juan
Last names: silva, santos, smith, garcia, oliveira, ro-
driguez, jones, williams, johnson, brown, gonzalez
Locations: brasil, indonesia, philippines, london,
jakarta, s?o paulo, rio de janeiro, venezuela, brazil
Table 1: Most frequent profile attributes for our collection
of 168 million Twitter users, in descending order
enhance a geolocation system by using commu-
nication patterns, and show strong improvements
over a hand-engineered baseline (?7). We share
our clusters with the community to use with other
tasks. The clusters, and other experimental data, are
available for download from www.clsp.jhu.edu/
~sbergsma/TwitterClusters/.
2 Attribute Associations on Twitter
Data and Processing Our raw Twitter data com-
prises the union of 2.2 billion tweets from 05/2009
to 10/2010 (O?Connor et al, 2010), 1.8 billion
tweets collected from 07/2011 to 08/2012, and 80
million tweets collected from followers of 10 thou-
sand location and language-specific Twitter feeds.
We implemented each stage of processing using
MapReduce (Dean and Ghemawat, 2008). The total
computation (from extracting profiles to clustering
attributes) was 1300 days of wall-clock CPU time.
Attribute Extraction Tweets provide the name
and self-reported location of the tweeter. We find
126M unique users with these attributes in our data.
When tweets mention other users via an @user con-
struction, Twitter also includes the profile name of
the mentioned user; we obtain a further 42M users
from these cases. We then normalize the extracted
attributes by converting to lower-case, deleting sym-
bols, numbers, and punctuation, and removing com-
mon honorifics and suffixes like mr/mrs and jr/sr.
Common prefixes like van and de la are joined to
the last-name token.1 This processing yields 8.3M
1www.clsp.jhu.edu/~sbergsma/TwitterClusters/
also provides our scripts for normalizing attributes. The scripts
can be used to ensure consistency/compatibility between
arbitrary datasets and our shared cluster data. Note we use no
special processing for the companies, organizations, and spam-
mers among our users, nor for names arising from different
conventions (e.g. 1-word names, reversed first/last names).
1011
henrik: fredrik 5.87, henrik 5.82, anders 5.73, johan
5.69, andreas 5.59, martin 5.54, magnus 5.41
courtney: taylor 8.03, ashley 7.92, courtney 7.92,
emily 7.91, lauren 7.82, katie 7.72, brittany 7.69
ilya: sergey 5.85, alexey 5.62, alexander 5.59, dmitry
5.51, ????????? 5.46, anton 5.44, andrey 5.40
Table 2: Top associates and PMIs for three first names.
unique locations, 7.4M unique last names, and 5.5M
unique first names. These three sets provide the tar-
get attributes that we cluster in ?3. Table 1 shows
the most frequent names in each of these three sets.
User-User Links We extract each user mention as
an undirected communication link between the user
tweeting and the mentioned user (including self-
mentions but not retweets). We consider each user-
user link as a single event; we count it once no mat-
ter how often two specific users interact. We extract
436M user-user links in total.
Attribute-Attribute Pairs We use our profile data
to map each user-user link to an attribute-attribute
pair; we separately count each pair of first names,
last names, and locations. For example, the first-
name pair (henrik, fredrik) occurs 181 times. Rather
than using the raw count, we calculate the associa-
tion between attributes a1 and a2 via their pointwise
mutual information (PMI), following prior work in
distributional clustering (Lin and Wu, 2009):
PMI(a1, a2) = log
P(a1, a2)
P(a1)P(a2)
PMI essentially normalizes the co-occurrence by
what we would expect if the attributes were indepen-
dently distributed. We smooth the PMI by adding a
count of 0.5 to all co-occurrence events.
The most highly-associated name attributes re-
flect similarities in ethnicity and gender (Table 2).
The most highly-ranked associates for locations are
often nicknames and alternate/misspellings of those
locations. For example, the locations charm city,
bmore, balto, westbaltimore, b a l t i m o r e, bal-
timoreee, and balitmore each have the U.S. city of
baltimore as their highest-PMI associate. We show
how this can be used to help geolocate users (?7).
3 Attribute Clustering
Representation We first represent each target at-
tribute as a feature vector, where each feature corre-
sponds to another attribute of the same type as the
target and each value gives the PMI between this at-
tribute and the target (as in Table 2).2 To help cluster
the long-tail of infrequent attributes, we also include
orthographic features. For first and last names, we
have binary features for the last 2 characters in the
string. For locations, we have binary features for
(a) any ideographic characters in the string and (b)
each token (with diacritics removed) in the string.
We normalize the feature vectors to unit length.
Distributed K-Means Clustering Our approach
to clustering follows Lin and Wu (2009) who used k-
means to cluster tens of millions of phrases. We also
use cosine similarity to compute the closest centroid
(i.e., we use the spherical k-means clustering algo-
rithm (Dhillon and Modha, 2001)). We keep track
of the average cosine similarity between each vector
and its nearest centroid; this average is guaranteed
to increase at each iteration.
Like Lin and Wu (2009), we parallelize the al-
gorithm using MapReduce. Each mapper finds the
nearest centroids for a portion of the vectors, while
also computing the partial sums of the vectors as-
signed to each centroid. The mappers emit the cen-
troid IDs as keys and the partial sums as values.
The Reducer aggregates the partial sums from each
partition and re-normalizes each sum vector to unit
length to obtain the new centroids. We also use an
inverted index at each iteration that, for each input
feature, lists which centroids each feature belongs
to. Using this index greatly speeds up the centroid
similarity computations.
Clustering Details We cluster with nine separate
configurations: over first names, last names, and lo-
cations, and each with 50, 200, and 1000 cluster
centroids (denoted C50, C200, and C1000). Since k-
2We decided to restrict the features for a target to be at-
tributes of the same type (e.g., we did not use last name as-
sociations for a first name target) because each attribute type
conveys distinct information. For example, first names convey
gender and age more than last names. By separately cluster-
ing representations using first names, last names, and locations,
each clustering can capture its own distinct latent-class associa-
tions.
1012
Cluster 463 (Serbian): pavlovic?, jovanovic, jo-
vanovic?, stankovic?, srbija, markovic?, petrovic?,
radovic, nenad, milenkovic, nikolic, sekulic, todor-
ovic, stojanovic, petrovic, aleksic, ilic, markovic
Cluster 544 (Black South African): ngcobo, nkosi,
dlamini, ndlovu, mkhize, mtshali, sithole, mathebula,
mthembu, khumalo, ngwenya, shabangu, nxumalo,
buthelezi, radebe, mabena, zwane, mbatha, sibiya
Cluster 449 (Turkish): s?ahin, ?elik, ?zt?rk, ko?, ?ak?r,
karatas?, aktas?, g?ng?r, ?zkan, balc?, g?m?s?, akkaya,
gen?, sar?, y?ksel, g?nes?, yig?it, yal??n, orhan, sag?lam,
g?ler, demirci, k???k, yavuz, bayrak, ?zcan, altun
Cluster 656 (Indonesian): utari, oktaviana, apriani,
mustika, septiana, febrianti, kurniawati, indriani, nur-
janah, septian, cahya, anggara, yuliani, purnamasari,
sukma, wijayanti, pramesti, ningrum, yanti, wulansari
Table 3: Example C1000 last-name clusters
Cluster 56 [sim=0.497]: gregg, bryn, bret, stewart,
lyndsay, howie, elyse, jacqui, becki, rhett, meaghan,
kirstie, russ, jaclyn, zak, katey, seamus, brennan,
fraser, kristie, stu, jaimie, kerri, heath, carley, griffin
Cluster 104 [sim=0.442]: stephon, devonte, deion,
demarcus, janae, tyree, jarvis, donte, dewayne, javon,
destinee, tray, janay, tyrell, jamar, iesha, chyna,
jaylen, darion, lamont, marquise, domonique, alexus
Cluster 132 [sim=0.292]: moustafa, omnya, menna-
tallah, ?C?@, shorouk, ragab, ?


??, radwa, moemen,
mohab, hazem, yehia, ? K
Q k, Z @Q?? @, mennah, ?
 Q?? ?,
abdelrahman, ?


	
????, H. 	Qk, Q?A

K, nermeen, hebatallah
...
Table 4: C200 soft clustering for first name yasmeen
means is not guaranteed to reach a global optimum,
we use ten different random initializations for each
configuration, and select the one with the highest av-
erage similarity after 20 iterations. We run this one
for an additional 30 iterations and take the output as
our final set of centroids for that configuration.
The resulting clusters provide data that could help
classify hidden properties of social media users. For
example, Table 3 shows that last names often clus-
ter by ethnicity, even at the sub-national level (e.g.
Zulu tribe surnames nkosi, dlamini, mathebula, etc.).
Note the Serbian names include two entries that are
not last names: srbija, the Serbian word for Serbia,
and nenad, a common Serbian first name.
Soft Clustering Rather than assigning each at-
tribute to its single highest-similarity cluster, we can
assign each vector to its N most similar clusters.
These soft-cluster assignments often reflect different
social groups where a name or location is used. For
example, the name yasmeen is similar to both com-
mon American names (Cluster 56), African Ameri-
can names (Cluster 104), and Arabic names (Clus-
ter 132) (Table 4). As another example, the C1000
assignments for the location trujillo comprise sep-
arate clusters containing towns and cities in Peru,
Venezuela, Colombia, etc., reflecting the various
places in the Latin world with this name. In general,
the soft cluster assignment is a low-dimensional rep-
resentation of each of our attributes. Although it can
be interpretable to humans, it need not be in order to
be useful to a classifier.
4 Classification with Cluster Features
Our motivating problem is to classify users for hid-
den properties such as their gender, location, race,
ethnicity, and language. We adopt a discriminative
solution. We encode the relevant data for each in-
stance in a feature vector and train a (linear) support
vector machine classifier (Cortes and Vapnik, 1995).
SVMs represent the state-of-the-art on many NLP
classification tasks, but other classifiers could also
be used. For multi-class classification, we use a one-
versus-all strategy, a competitive approach on most
multi-class problems (Rifkin and Klautau, 2004).
The input to our system is one or more observed
user attributes (e.g. name and location fields from
a user profile). We now describe how features are
created from these attributes in both state-of-the-art
systems and via our new cluster data.
Token Features (Tok) are binary features that in-
dicate the presence of a specific attribute (e.g., first-
name=bob). Burger et al (2011) and Bergsma et al
(2012) used Tok features to encode user profile fea-
tures. For multi-token fields (e.g. location), our Tok
features also indicate the specific position of each
token (e.g., loc1=s?o, loc2=paulo, locN=brasil).
Character N-gram Features (Ngm) give the
count of all character n-grams of length 1-to-4 in the
input. Ngm features have been used in user classifi-
cation (Burger et al, 2011) and represent the state-
1013
of-the-art in detecting name ethnicity (Bhargava and
Kondrak, 2010). We add special begin/end charac-
ters to the attributes to mark the prefix and suffix po-
sitions. We also use a smoothed log-count; we found
this to be most effective in preliminary work.
Cluster Features (Clus) indicate the soft-cluster
memberships of the attributes. We have features for
the top-2, 5, and 20 most similar clusters in the C50,
C200, and C1000 clusterings, respectively. Like Lin
and Wu (2009), we ?side-step the matter of choos-
ing the optimal value k in k-means? by using fea-
tures from clusterings at different granularities. Our
feature dimensions correspond to cluster IDs; fea-
ture values give the similarity to the cluster centroid.
Other strategies (e.g. hard clustering, binary fea-
tures) were less effective in preliminary work.
5 Classification Experiments
5.1 Methodology
Our main objective is to assess the value of us-
ing cluster features (Clus). We add these features
to classifiers using Tok+Ngm features, which repre-
sents the current state-of-the-art. We compare these
feature settings on both Twitter tasks (?5.2) and
tasks not related to social-media (?5.3). For each
task, we randomly divide the gold standard data into
50% train, 25% development and 25% test, unless
otherwise noted. As noted above, the gold-standard
datasets for all of our experiments are available for
download. We train our SVM classifiers using the
LIBLINEAR package (Fan et al, 2008). We optimize
the classifier?s regularization parameter on develop-
ment data, and report our final results on the held-
out test examples. We report accuracy: the propor-
tion of test examples classified correctly. For com-
parison, we report the accuracy of a majority-class
baseline on each task (Base).
Classifying hidden properties of social media
users is challenging (Table 5). Pennacchiotti and
Popescu (2011) even conclude that ?profile fields do
not contain enough good-quality information to be
directly used for user classification.? To provide in-
sight into the difficulty of the tasks, we had two hu-
mans annotate 120 examples from each of the test
sets, and we average their results to give a ?Human?
performance number. The two humans are experts in
Country: 53 possible countries
United States courtland dante cali baby
United States tinas twin on the court
Brazil thamires gomez macap? ap
Denmark marte clason NONE
Lang. ID: 9 confusable languages
Bulgarian valentina getova NONE
Russian borisenko yana edinburgh
Bulgarian NONE blagoevgrad
Ukrainian andriy kupyna ternopil
Farsi kambiz barahouei NONE
Urdu musadiq sanwal jammu
Ethnicity: 13 European ethnicities
German dennis hustadt
Dutch bernhard hofstede
French david coste
Swedish mattias bjarsmyr
Portuguese helder costa
Race: black or white
black kerry swain
black darrell foskey
white ty j larocca
black james n jones
white sean p farrell
Table 5: Examples of class (left) and input (names, loca-
tions) for some of our evaluation tasks.
this domain and have very wide knowledge of global
names and locations.
5.2 Twitter Applications
Country A number of recent papers have consid-
ered the task of predicting the geolocation of users,
using both user content (Cheng et al, 2010; Eisen-
stein et al, 2010; Hecht et al, 2011; Wing and
Baldridge, 2011; Roller et al, 2012) and social net-
work (Backstrom et al, 2010; Sadilek et al, 2012).
Here, we first predict user location at the level of
the user?s location country. To our knowledge, we
are the first to exploit user locations and names for
this prediction. For this task, we obtain gold data
from the portion of Twitter users who have GPS en-
abled (geocoded tweets). We were able to obtain a
very large number of gold instances for this task, so
selected only 10K for testing, 10K for development,
and retained the remaining 782K for training.
Language ID Identifying the language of users
is an important prerequisite for building language-
specific social media resources (Tromp and Pech-
1014
enizkiy, 2011; Carter et al, 2013). Bergsma et al
(2012) recently released a corpus of tweets marked
for one of nine languages grouped into three confus-
able character sets: Arabic, Farsi, and Urdu tweets
written in Arabic characters; Hindi, Nepali, and
Marathi written in Devanagari, and Russian, Bulgar-
ian, and Ukrainian written in Cyrillic. The tweets
were marked for language by native speakers via
Amazon Mechanical Turk. We again discard the
tweet content and extract each user?s first name, last
name, and user location as our input data, while tak-
ing the annotated language as the class label.
Gender We predict whether a Twitter user is male
or female using data from Burger et al (2011). This
data was created by linking Twitter users to struc-
tured profile pages on other websites where users
must select their gender. Unlike prior systems using
this data (Burger et al, 2011; Van Durme, 2012), we
make the predictions using only user names.
5.3 Other Applications
Origin Knowing the origin of a name can improve
its automatic pronunciation (Llitjos and Black,
2001) and transliteration (Bhargava and Kondrak,
2010). We evaluate our cluster data on name-origin
prediction using a corpus of names marked as ei-
ther Indian or non-Indian by Bhargava and Kondrak
(2010). Since names in this corpus are not marked
for entity type, we include separate cluster features
from both our first and last name clusters.
Ethnicity We also evaluate on name-origin data
from Konstantopoulos (2007). This data derives
from lists of football players on European national
teams; it marks each name (with diacritics removed)
as arising from one of 13 European languages. Fol-
lowing prior work, we test in two settings: (1) using
last names only, and (2) using first and last names.
Race We also evaluate our ability to identify eth-
nic groups at a sub-national level. To obtain data
for this task, we mined the publicly-available arrest
records on mugshots.com for the U.S. state of New
Jersey (a small but diverse and densely-populated
area). Over 99% of users were listed as either black
or white, and we structure the task as a binary clas-
sification problem between these two classes. We
predict the race of each person based purely on their
name; this contrasts with prior work in social media
which looked at identifying African Americans on
the basis of their Twitter content (Eisenstein et al,
2011; Pennacchiotti and Popescu, 2011).
6 Classification Results
Table 6 gives the results on each task. The system in-
corporating our novel Clus features consistently im-
proves over the Ngm+Tok system; all differences be-
tween All and Ngm+Tok are significant (McNemar?s,
p<0.01). The relative reduction in error from adding
Clus features ranges between 7% and 51%. The All
system including Clus features also exceeds human
performance on all studied tasks.
On Country, the U.S. is the majority class, oc-
curring in 42.5% of cases.3 It is impressive that
All so significantly exceeds Tok+Ngm (86.7% vs.
84.8%); with 782K training examples, we did not
expect such room for improvement. Both names and
locations play an important role: All achieves 66%
using names alone and 70% with only location. On
the subset of data where all three attributes are non-
empty, the full system achieves 93% accuracy.
Both feature classes are likewise important for
Lang. ID; All achieves 67% with only first+last
names, 72% with just locations, but 83% with both.
Our smallest improvement is on Gender. This
task is easier (with higher human/system accuracy)
and has plenty of training data (more data per class
than any other task); there is thus less room to im-
prove. Looking at the feature weights, the strongest-
weighted female cluster apparently captures a sub-
community of Justin Bieber fans (showing loyalty
with ?first names? jbieber, belieb, biebz, beliebing,
jbiebs, etc.). Just because a first name like madison
has a high similarity to this cluster does not imply
girls named Madison are Justin Bieber fans; it sim-
ply means that Madisons have similar names to the
friends of Justin Bieber fans (who tend to be girls).
Also, note that while the majority of the 34K users in
our training data are assigned this cluster somewhere
in their soft clustering, only 6 would be assigned this
3We tried other baselines: e.g., we predict countries if they
are substrings of the location (otherwise predicting U.S.); and
we predict countries if they often occur as a string following
the given location in our profile data (e.g., we predict Spain for
Madrid since Madrid, Spain is common). Variations on these
approaches consistently performed between 48% and 56%.
1015
Task Input
Num. Num.
Base Human Tok Ngm Clus
Tok+
All ?
Train Class Ngm
Country first+last+loc 781920 53 42.5 71.7 83.0 84.5 80.2 84.8 86.7 12.5
Lang. ID first+last+loc 2492 9 27.0 74.2 74.6 80.6 71.1 80.4 82.7 11.7
Gender first+last 33805 2 52.4 88.3 85.3 88.6 79.5 89.5 90.2 6.7
Origin entity name 500 2 52.4 80.4 - 75.6 81.2 75.6 88.0 50.8
Ethnicity last 6026 13 20.8 47.9 - 54.6 48.5 54.6 62.4 17.2
Ethnicity first+last 7457 13 21.2 53.3 67.6 77.5 73.6 78.4 81.3 13.4
Race first+last 7977 2 54.7 71.4 80.4 81.6 84.6 82.4 84.6 12.5
Table 6: Task details and accuracy (%) for attribute-based classification tasks. ? = relative error reduction (%) of All
(Tok+Ngm+Clus) over Ngm+Tok. All always exceeds both Tok+Ngm and the human performance.
cluster in a hard clustering. This clearly illustrates
the value of the soft clustering representation.
Note the All system performed between 83% and
90% on each Twitter task. This level of performance
strongly refutes the prevailing notion that Twitter
profile information is useless in general (Pennac-
chiotti and Popescu, 2011) and especially for geolo-
cation (Cheng et al, 2010; Hecht et al, 2011).
We now move to applications beyond social me-
dia. Bhargava and Kondrak (2010) have the current
state-of-the-art on Origin and Ethnicity based on an
SVM using character-n-gram features; we reimple-
mented this as Ngm. We obtain a huge improvement
over their work using Clus, especially on Origin
where we reduce error by >50%.4 This improve-
ment can partly be attributed to the small amount of
training data; with fewer parameters to learn, Clus
learns more from limited data than Ngm. We like-
wise see large improvements over the state-of-the-
art on Ethnicity, on both last name and full name
settings.
Finally, Clus features also significantly improve
accuracy on the new Race task. Our cluster data can
therefore help to classify names into sub-national
groups, and could potentially be used to infer other
interesting communities such as castes in India and
religious divisions in many countries.
In general, the relative value of our cluster models
varies with the amount of training data; we see huge
gains on the smaller Origin data but smaller gains
on the large Gender set. Figure 1 shows how per-
formance of Clus and Ngm varies with training data
on Race. Again, Clus is especially helpful with less
4Note Tok is not used here because the input is a single token
and training and test splits have distinct instances.
 60
 65
 70
 75
 80
 85
 10  100  1000  10000
A
cc
ur
ac
y
Number of training examples
Clus
Ngm
Figure 1: Learning curve on Race: Clus perform as well
with 30 training examples as Ngm features do with 1000.
data; thousands of training examples are needed for
Ngm to rival the performance of Clus using only a
handful. Since labeled data is generally expensive
to obtain or in short supply, our method for exploit-
ing unlabeled Twitter data can both save money and
improve top-end performance.
7 Geolocation by Association
There is a tradition in computational linguistics of
grouping words both by the similarity of their con-
text vectors (Hindle, 1990; Pereira et al, 1993; Lin,
1998) and directly by their statistical association in
text (Church and Hanks, 1990; Brown et al, 1992).
While the previous sections explored clusters built
by vector similarity, we now explore a direct appli-
cation of our attribute association data (?2).
We wish to use this data to improve an existing
Twitter geolocation system based on user profile lo-
cations. The system operates as follows: 1) normal-
1016
ize user-provided locations using a set of regular ex-
pressions (e.g. remove extra spacing, punctuation);
2) look up the normalized location in an alias list;
3) if found, map the alias to a unique string (target
location), corresponding to a structured location ob-
ject that includes geo-coordinates.
The alias list we are currently using is based on
extensive work in hand-writing aliases for the most
popular Twitter locations. For example, the current
aliases for Nashville, Tennessee include nashville,
nashville tn, music city, etc. Our objective is to im-
prove on this human-designed list by automatically
generating aliases using our association data.
Aliases by Association For each target, we pro-
pose new aliases from the target?s top-PMI asso-
ciates (?2). To become an alias, the PMI between
the alias and target must be above a threshold,
the alias must occur more than a fixed number of
times in our profile data, the alias must be within
the top-N1 associates of the target, and the target
must be within the top-N2 associates of the alias.
We merge our automatic aliases with the manually-
written aliases. The new aliases for Nashville, Ten-
nessee include east nashville, nashville tenn, music
city usa, nashvegas, cashville tn, etc.
Experiments To evaluate the geolocation system,
we use tweets from users with GPS enabled (?5.2).
For each tweet, we resolve the location using the
system and compare to the gold coordinates. The
system can skip a location if it does not match the
alias list; more than half of the locations are skipped,
which is consistent with prior work (Hecht et al,
2011). We evaluate the alias lists using two mea-
sures: (1) its coverage: the percentage of locations it
resolves, and (2) its precision: of the ones resolved,
the percentage that are correct. We define a correct
resolution to be one where the resolved coordinates
are within 50 miles of the gold coordinates.
We use 56K gold tweets to tune the parameters of
our automatic alias-generator, trading off coverage
and precision. We tune such that the system using
these aliases obtains the highest possible coverage,
while being at least as precise as the baseline system.
We then evaluate both the baseline set of aliases and
our new set on 56K held-out examples.
Results On held-out test data, the geolocation sys-
tem using baseline aliases has a coverage of 38.7%
and a precision of 59.5%. Meanwhile, the system
using the new aliases has a coverage of 44.6% and
a precision of 59.4%. With virtually the same pre-
cision, the new aliases are thus able to resolve 15%
more users. This provides an immediate benefit to
our existing Twitter research efforts.
Note that our alias lists can be viewed as clus-
ters of locations. In ongoing work, we are exploring
techniques based on discriminative learning to infer
alias lists using not only Clus information but also
Ngm and Tok features as in the previous sections.
8 Related Work
In both real-world and online social networks, ?peo-
ple socialize with people who are like them in terms
of gender, sexual orientation, age, race, education,
and religion? (Jernigan and Mistree, 2009). So-
cial media research has exploited this for two main
purposes: (1) to predict friendships based on user
properties, and (2) to predict user properties based
on friendships. Friendship prediction systems (e.g.
Facebook?s friend suggestion tool) use features such
as whether both people are computer science ma-
jors (Taskar et al, 2003) or whether both are at the
same location (Crandall et al, 2010; Sadilek et al,
2012). The inverse problem has been explored in the
prediction of a user?s location given the location of
their peers (Backstrom et al, 2010; Cho et al, 2011;
Sadilek et al, 2012). Jernigan and Mistree (2009)
predict a user?s sexuality based on the sexuality of
their Facebook friends, while Garera and Yarowsky
(2009) predict a user?s gender partly based on the
gender of their conversational partner. Jha and El-
hadad (2010) predict the cancer stage of users of
an online cancer discussion board; they derive com-
plementary information for prediction from both the
text a user generates and the cancer stage of the peo-
ple that a user interacts with.
The idea of clustering data in order to provide fea-
tures for supervised systems has been successfully
explored in a range of NLP tasks, including named-
entity-recognition (Miller et al, 2004; Lin and Wu,
2009; Ratinov and Roth, 2009), syntactic chunking
(Turian et al, 2010), and dependency parsing (Koo
et al, 2008; T?ckstr?m et al, 2012). In each case,
1017
the clusters are derived from the distribution of the
words or phrases in text, not from their communica-
tion pattern. It would be interesting to see whether
prior distributional clusters can be combined with
our communication-based clusters to achieve even
better performance. Indeed, there is evidence that
features derived from text can improve the predic-
tion of name ethnicity (Pervouchine et al, 2010).
There has been an explosion of work in recent
years in predicting user properties in social net-
works. Aside from the work mentioned above that
analyzes a user?s social network, a large amount
of work has focused on inferring user properties
based on the content they generate (e.g. Burger
and Henderson (2006), Schler et al (2006), Rao
et al (2010), Mukherjee and Liu (2010), Pennac-
chiotti and Popescu (2011), Burger et al (2011), Van
Durme (2012)).
9 Conclusion and Future Work
We presented a highly effective and readily repli-
cable algorithm for generating language resources
from Twitter communication patterns. We clustered
user attributes based on both the communication of
users with those attributes as well as substring sim-
ilarity. Systems using our clusters significantly out-
perform state-of-the-art algorithms on each of the
tasks investigated, and exceed human performance
on each task as well. The power and versatility of
our clusters is exemplified by the fact we reduce er-
ror by a larger margin on each of the non-Twitter
tasks than on any Twitter task itself.
Twitter provides a remarkably large sample and
effectively a partial census of much of the world?s
population, with associated metadata, descriptive
content and sentiment information. Our ability to
accurately assign numerous often unspecified prop-
erties such as race, gender, language and ethnicity to
such a large user sample substantially increases the
sociological insights and correlations one can derive
from such data.
References
Lars Backstrom, Eric Sun, and Cameron Marlow. 2010.
Find me if you can: improving geographical predic-
tion with social and spatial proximity. In Proc. WWW,
pages 61?70.
Shane Bergsma, Paul McNamee, Mossaab Bagdouri,
Clayton Fink, and Theresa Wilson. 2012. Language
identification for creating language-specific Twitter
collections. In Proceedings of the Second Workshop
on Language in Social Media, pages 65?74.
Aditya Bhargava and Grzegorz Kondrak. 2010. Lan-
guage identification of names with SVMs. In Proc.
HLT-NAACL, pages 693?696.
Peter F. Brown, Vincent J. Della Pietra, Peter V. de Souza,
Jennifer C. Lai, and Robert L. Mercer. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467?479.
John D. Burger and John C. Henderson. 2006. An ex-
ploration of observable features related to blogger age.
In Proc. AAAI Spring Symposium: Computational Ap-
proaches to Analyzing Weblogs, pages 15?20.
John D. Burger, John Henderson, George Kim, and Guido
Zarrella. 2011. Discriminating gender on Twitter. In
Proc. EMNLP, pages 1301?1309.
Simon Carter, Wouter Weerkamp, and Manos Tsagkias.
2013. Microblog Language Identification: Overcom-
ing the Limitations of Short, Unedited and Idiomatic
Text. Language Resources and Evaluation Journal.
(forthcoming).
Zhiyuan Cheng, James Caverlee, and Kyumin Lee. 2010.
You are where you tweet: a content-based approach
to geo-locating Twitter users. In Proc. CIKM, pages
759?768.
Eunjoon Cho, Seth A. Myers, and Jure Leskovec. 2011.
Friendship and mobility: user movement in location-
based social networks. In Proc. KDD, pages 1082?
1090.
Kenneth W. Church and Patrick Hanks. 1990. Word as-
sociation norms, mutual information, and lexicogra-
phy. Computational Linguistics, 16(1):22?29.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Mach. Learn., 20(3):273?297.
David J. Crandall, Lars Backstrom, Dan Cosley, Sid-
dharth Suri, Daniel Huttenlocher, and Jon Kleinberg.
2010. Inferring social ties from geographic coinci-
dences. Proceedings of the National Academy of Sci-
ences, 107(52):22436?22441.
Jeffrey Dean and Sanjay Ghemawat. 2008. MapReduce:
simplified data processing on large clusters. Commun.
ACM, 51(1):107?113.
Inderjit S. Dhillon and Dharmendra S. Modha. 2001.
Concept decompositions for large sparse text data us-
ing clustering. Mach. Learn., 42(1-2):143?175.
Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model for
geographic lexical variation. In Proc. EMNLP, pages
1277?1287.
1018
Jacob Eisenstein, Noah A. Smith, and Eric P. Xing. 2011.
Discovering sociolinguistic associations with struc-
tured sparsity. In Proc. ACL, pages 1365?1374.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. J. Mach. Learn.
Res., 9:1871?1874.
Nikesh Garera and David Yarowsky. 2009. Modeling la-
tent biographic attributes in conversational genres. In
Proc. ACL-IJCNLP, pages 710?718.
Brent Hecht, Lichan Hong, Bongwon Suh, and Ed H. Chi.
2011. Tweets from Justin Bieber?s heart: the dynamics
of the location field in user profiles. In Proc. CHI,
pages 237?246.
Donald Hindle. 1990. Noun classification from
predicate-argument structures. In Proc. ACL, pages
268?275.
Carter Jernigan and Behram F. T. Mistree. 2009. Gaydar:
Facebook friendships expose sexual orientation. First
Monday, 14(10). [Online].
Mukund Jha and Noemie Elhadad. 2010. Cancer stage
prediction based on patient online discourse. In Proc.
2010 Workshop on Biomedical Natural Language Pro-
cessing, pages 64?71.
Stasinos Konstantopoulos. 2007. What?s in a name? In
Proc. Computational Phonology Workshop, RANLP.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Proc.
ACL-08: HLT, pages 595?603.
Dekang Lin and Xiaoyun Wu. 2009. Phrase clustering
for discriminative learning. In Proc. ACL-IJCNLP,
pages 1030??1038.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In Proc. Coling-ACL, pages 768?774.
Ariadna Font Llitjos and Alan W. Black. 2001. Knowl-
edge of language origin improves pronunciation accu-
racy of proper names. In Proceedings of EuroSpeech-
01, pages 1919?1922.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and discrimi-
native training. In Proc. HLT-NAACL, pages 337?342.
Arjun Mukherjee and Bing Liu. 2010. Improving gender
classification of blog authors. In Proc. EMNLP, pages
207?217.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From tweets to polls: Linking text sentiment to public
opinion time series. In Proc. ICWSM, pages 122?129.
Michael Paul and Mark Dredze. 2011. You are what you
tweet: Analyzing Twitter for public health. In Proc.
ICWSM, pages 265?272.
Marco Pennacchiotti and Ana-Maria Popescu. 2011. A
machine learning approach to Twitter user classifica-
tion. In Proc. ICWSM, pages 281?288.
Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993.
Distributional clustering of English words. In Proc.
ACL, pages 183?190.
Vladimir Pervouchine, Min Zhang, Ming Liu, and
Haizhou Li. 2010. Improving name origin recogni-
tion with context features and unlabelled data. In Col-
ing 2010: Posters, pages 972?978.
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in Twitter. In Proc. International Workshop on
Search and Mining User-Generated Contents, pages
37?44.
Lev Ratinov and Dan Roth. 2009. Design challenges and
misconceptions in named entity recognition. In Proc.
CoNLL, pages 147?155.
Ryan Rifkin and Aldebaro Klautau. 2004. In defense of
one-vs-all classification. J. Mach. Learn. Res., 5:101?
141.
Stephen Roller, Michael Speriosu, Sarat Rallapalli, Ben-
jamin Wing, and Jason Baldridge. 2012. Supervised
text-based geolocation using language models on an
adaptive grid. In Proc. EMNLP-CoNLL, pages 1500?
1510.
Adam Sadilek, Henry Kautz, and Jeffrey P. Bigham.
2012. Finding your friends and following them to
where you are. In Proc. WSDM, pages 723?732.
Jonathan Schler, Moshe Koppel, Shlomo Argamon, and
James W. Pennebaker. 2006. Effects of age and
gender on blogging. In Proc. AAAI Spring Sympo-
sium: Computational Approaches to Analyzing We-
blogs, pages 199?205.
Oscar T?ckstr?m, Ryan McDonald, and Jakob Uszkoreit.
2012. Cross-lingual word clusters for direct transfer
of linguistic structure. In Proc. NAACL-HLT, pages
477?487.
Ben Taskar, Ming-Fai Wong, Pieter Abbeel, and Daphne
Koller. 2003. Link prediction in relational data. In
Proc. NIPS, volume 15.
Erik Tromp and Mykola Pechenizkiy. 2011. Graph-
based n-gram language identication on short texts. In
Proc. 20th Machine Learning conference of Belgium
and The Netherlands, pages 27?34.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proc. ACL, pages
384?394.
Benjamin Van Durme. 2012. Streaming analysis of dis-
course participants. In Proc. EMNLP-CoNLL, pages
48?58.
Benjamin Wing and Jason Baldridge. 2011. Simple su-
pervised document geolocation with geodesic grids.
In Proc. ACL, pages 955?964.
1019
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1346?1355,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Using Large Monolingual and Bilingual Corpora to
Improve Coordination Disambiguation
Shane Bergsma, David Yarowsky, Kenneth Church
Deptartment of Computer Science and Human Language Technology Center of Excellence
Johns Hopkins University
sbergsma@jhu.edu, yarowsky@cs.jhu.edu, kenneth.church@jhu.edu
Abstract
Resolving coordination ambiguity is a clas-
sic hard problem. This paper looks at co-
ordination disambiguation in complex noun
phrases (NPs). Parsers trained on the Penn
Treebank are reporting impressive numbers
these days, but they don?t do very well on this
problem (79%). We explore systems trained
using three types of corpora: (1) annotated
(e.g. the Penn Treebank), (2) bitexts (e.g. Eu-
roparl), and (3) unannotated monolingual (e.g.
Google N-grams). Size matters: (1) is a mil-
lion words, (2) is potentially billions of words
and (3) is potentially trillions of words. The
unannotated monolingual data is helpful when
the ambiguity can be resolved through associ-
ations among the lexical items. The bilingual
data is helpful when the ambiguity can be re-
solved by the order of words in the translation.
We train separate classifiers with monolingual
and bilingual features and iteratively improve
them via co-training. The co-trained classifier
achieves close to 96% accuracy on Treebank
data and makes 20% fewer errors than a su-
pervised system trained with Treebank anno-
tations.
1 Introduction
Determining which words are being linked by a co-
ordinating conjunction is a classic hard problem.
Consider the pair:
+ellipsis rocket\w1 and mortar\w2 attacks\h
?ellipsis asbestos\w1 and polyvinyl\w2 chloride\h
+ellipsis is about both rocket attacks and mortar at-
tacks, unlike ?ellipsis which is not about asbestos
chloride. We use h to refer to the head of the phrase,
and w1 and w2 to refer to the other two lexical items.
Natural Language Processing applications need to
recognize NP ellipsis in order to make sense of new
sentences. For example, if an Internet search en-
gine is given the phrase rocket attacks as a query, it
should rank documents containing rocket and mor-
tar attacks highly, even though rocket and attacks
are not contiguous in the document. Furthermore,
NPs with ellipsis often require a distinct type of re-
ordering when translated into a foreign language.
Since coordination is both complex and produc-
tive, parsers and machine translation (MT) systems
cannot simply memorize the analysis of coordinate
phrases from training text. We propose an approach
to recognizing ellipsis that could benefit both MT
and other NLP technology that relies on shallow or
deep syntactic analysis.
While the general case of coordination is quite
complicated, we focus on the special case of com-
plex NPs. Errors in NP coordination typically ac-
count for the majority of parser coordination errors
(Hogan, 2007). The information needed to resolve
coordinate NP ambiguity cannot be derived from
hand-annotated data, and we follow previous work
in looking for new information sources to apply
to this problem (Resnik, 1999; Nakov and Hearst,
2005; Rus et al, 2007; Pitler et al, 2010).
We first resolve coordinate NP ambiguity in a
word-aligned parallel corpus. In bitexts, both mono-
lingual and bilingual information can indicate NP
structure. We create separate classifiers using mono-
lingual and bilingual feature views. We train the
two classifiers using co-training, iteratively improv-
ing the accuracy of one classifier by learning from
the predictions of the other. Starting from only two
1346
initial labeled examples, we are able to train a highly
accurate classifier using only monolingual features.
The monolingual classifier can then be used both
within and beyond the aligned bitext. In particular,
it achieves close to 96% accuracy on both bitext data
and on out-of-domain examples in the Treebank.
2 Problem Definition and Related Tasks
Our system operates over a part-of-speech tagged in-
put corpus. We attempt to resolve the ambiguity in
all tag sequences matching the expression:
[DT|PRP$] (N.*|J.*) and [DT|PRP$] (N.*|J.*) N.*
e.g. [the] rocket\w1 and [the] mortar\w2 attacks\h
Each example ends with a noun, h. Preceding h
are a pair of possibly-conjoined words, w1 and w2,
either nouns (rocket and mortar), adjectives, or a
mix of the two. We allow determiners or possessive
pronouns before w1 and/or w2. This pattern is very
common. Depending on the domain, we find it in
roughly one of every 10 to 20 sentences. We merge
identical matches in our corpus into a single exam-
ple for labeling. Roughly 38% of w1,w2 pairs are
both adjectives, 26% are nouns, and 36% are mixed.
The task is to determine whether w1 and w2 are
conjoined or not. When they are not conjoined, there
are two cases: 1) w1 is actually conjoined with w2 h
as a whole (e.g. asbestos and polyvinyl chloride),
or 2) The conjunction links something higher up in
the parse tree, as in, ?farmers are getting older\w1
and younger\w2 people\h are reluctant to take up
farming.? Here, and links two separate clauses.
Our task is both narrower and broader than pre-
vious work. It is broader than previous approaches
that have focused only on conjoined nouns (Resnik,
1999; Nakov and Hearst, 2005). Although pairs
of adjectives are usually conjoined (and mixed tags
are usually not), this is not always true, as in
older/younger above. For comparison, we also state
accuracy on the noun-only examples (? 8).
Our task is more narrow than the task tackled
by full-sentence parsers, but most parsers do not
bracket NP-internal structure at all, since such struc-
ture is absent from the primary training corpus for
statistical parsers, the Penn Treebank (Marcus et al,
1993). We confirm that standard broad-coverage
parsers perform poorly on our task (? 7).
Vadas and Curran (2007a) manually annotated NP
structure in the Penn Treebank, and a few custom NP
parsers have recently been developed using this data
(Vadas and Curran, 2007b; Pitler et al, 2010). Our
task is more narrow than the task handled by these
parsers since we do not handle other, less-frequent
and sometimes more complex constructions (e.g.
robot arms and legs). However, such constructions
are clearly amenable to our algorithm. In addition,
these parsers have only evaluated coordination res-
olution within base NPs, simplifying the task and
rendering the aforementioned older/younger prob-
lem moot. Finally, these custom parsers have only
used simple count features; for example, they have
not used the paraphrases we describe below.
3 Supervised Coordination Resolution
We adopt a discriminative approach to resolving co-
ordinate NP ambiguity. For each unique coordinate
NP in our corpus, we encode relevant information
in a feature vector, x?. A classifier scores these vec-
tors with a set of learned weights, w?. We assume N
labeled examples {(y1, x?1), ..., (yN , x?N )} are avail-
able to train the classifier. We use ?y = 1? as the
class label for NPs with ellipsis and ?y = 0? for
NPs without. Since our particular task requires a bi-
nary decision, any standard learning algorithm can
be used to learn the feature weights on the train-
ing data. We use (regularized) logistic regression
(a.k.a. maximum entropy) since it has been shown
to perform well on a range of NLP tasks, and also
because its probabilistic interpretation is useful for
co-training (? 4). In binary logistic regression, the
probability of a positive class takes the form of the
logistic function:
Pr(y = 1) = exp(w? ? x?)1 + exp(w? ? x?)
Ellipsis is predicted if Pr(y = 1) > 0.5 (equiva-
lently, w? ? x? > 0), otherwise we predict no ellipsis.
Supervised classifiers easily incorporate a range
of interdependent information into a learned deci-
sion function. The cost for this flexibility is typically
the need for labeled training data. The more features
we use, the more labeled data we need, since for
linear classifiers, the number of examples needed to
reach optimum performance is at most linear in the
1347
Phrase Evidence Pattern
dairy and meat English: ... production of dairy and meat... h of w1 and w2
production English: ... dairy production and meat production... w1 h and w2 h
(ellipsis) English: ... meat and dairy production... w2 and w1 h
Spanish: ... produccio?n la?ctea y ca?rnica... h w1 ... w2
? production dairy and meat
Finnish: ... maidon- ja lihantuotantoon... w1- ... w2h
? dairy- and meatproduction
French: ... production de produits laitiers et de viande... h ... w1 ... w2
? production of products dairy and of meat
asbestos and English: ... polyvinyl chloride and asbestos... w2 h and w1
polyvinyl English: ... asbestos , and polyvinyl chloride... w1 , and w2 h
chloride English: ... asbestos and chloride... w1 and h
(no ellipsis) Portuguese: ... o amianto e o cloreto de polivinilo... w1 ... h ... w2
? the asbestos and the chloride of polyvinyl
Italian: ... l? asbesto e il polivinilcloruro... w1 ... w2h
? the asbestos and the polyvinylchloride
Table 1: Monolingual and bilingual evidence for ellipsis or lack-of-ellipsis in coordination of [w1 and w2 h] phrases.
number of features (Vapnik, 1998). In ? 4, we pro-
pose a way to circumvent the need for labeled data.
We now describe the particular monolingual and
bilingual information we use for this problem. We
refer to Table 1 for canonical examples of the two
classes and also to provide intuition for the features.
3.1 Monolingual Features
Count features These real-valued features encode
the frequency, in a large auxiliary corpus, of rel-
evant word sequences. Co-occurrence frequencies
have long been used to resolve linguistic ambigui-
ties (Dagan and Itai, 1990; Hindle and Rooth, 1993;
Lauer, 1995). With the massive volumes of raw
text now available, we can look for very specific
and indicative word sequences. Consider the phrase
dairy and meat production (Table 1). A high count
in raw text for the paraphrase ?production of dairy
and meat? implies ellipsis in the original example.
In the third column of Table 1, we suggest a pat-
tern that generalizes the particular piece of evidence.
It is these patterns and other English paraphrases
that we encode in our count features (Table 2). We
also use (but do not list) count features for the four
paraphrases proposed in Nakov and Hearst (2005,
? 3.2.3). Such specific paraphrases are more com-
mon than one might think. In our experiments, at
least 20% of examples have non-zero counts for a
5-gram pattern, while over 70% of examples have
counts for a 4-gram pattern.
Our features also include counts for subsequences
of the full phrase. High counts for ?dairy produc-
tion? alone or just ?dairy and meat? also indicate el-
lipsis. On the other hand, like Pitler et al (2010), we
have a feature for the count of ?dairy and produc-
tion.? Frequent conjoining of w1 and h is evidence
that there is no ellipsis, that w1 and h are compatible
and heads of two separate and conjoined NPs.
Many of our patterns are novel in that they include
commas or determiners. The presence of these of-
ten indicate that there are two separate NPs. E.g.
seeing asbestos , and polyvinyl chloride or the as-
bestos and the polyvinyl chloride suggests no ellip-
sis. We also propose patterns that include left-and-
right context around the NP. These aim to capture
salient information about the NP?s distribution as an
entire unit. Finally, patterns involving prepositions
look for explicit paraphrasing of the nominal rela-
tions; the presence of ?h PREP w1 and w2? in a cor-
pus would suggest ellipsis in the original NP.
In total, we have 48 separate count features, re-
quiring counts for 315 distinct N-grams for each ex-
ample. We use log-counts as the feature value, and
use a separate binary feature to indicate if a partic-
ular count is zero. We efficiently acquire the counts
using custom tools for managing web-scale N-gram
1348
Real-valued count features. C(p) ? count of p
C(w1) C(w2) C(h)
C(w1 CC w2) C(w1 h) C(w2 h)
C(w2 CC w1) C(w1 CC h) C(h CC w1)
C(DT w1 CC w2) C(w1 , CC w2)
C(DT w2 CC w1) C(w2 , CC w1)
C(DT w1 CC h) C(w1 CC w2 ,)
C(DT h CC w1) C(w2 CC w1 ,)
C(DT w1 and DT w2) C(w1 CC DT w2)
C(DT w2 and DT w1) C(w2 CC DT w1)
C(DT h and DT w1) C(w1 CC DT h)
C(DT h and DT w2) C(h CC DT w1)
C(?L-CTXTi? w1 and w2 h) C(w1 CC w2 h)
C(w1 and w2 h ?R-CTXTi?) C(h PREP w1)
C(h PREP w1 CC w2) C(h PREP w2)
Count feature filler sets
DT = {the, a, an, its, his} CC = {and, or, ?,?}
PREP = {of, for, in, at, on, from, with, about}
Binary features and feature templates ? {0, 1}
wrd1=?wrd(w1)? tag1=?tag(w1)?
wrd2=?wrd(w2)? tag2=?tag(w2)?
wrdh=?wrd(h)? tagh=?tag(h)?
wrd12=?wrd(w1),wrd(w2)? wrd(w1)=wrd(w2)
tag12=?tag(w1),tag(w2)? tag(w1)=tag(w2)
tag12h=?tag(w1),tag(w1),tag(h)?
Table 2: Monolingual features. For counts using the
filler sets CC, DT and PREP, counts are summed across
all filler combinations. In contrast, feature templates are
denoted with ???, where the feature label depends on the
?bracketed argument?. E.g., we have separate count fea-
ture for each item in the L/R context sets, where
{L-CTXT} = {with, and, as, including, on, is, are, &},
{R-CTXT} = {and, have, of, on, said, to, were, &}
data (? 5). Previous approaches have used search
engine page counts as substitutes for co-occurrence
information (Nakov and Hearst, 2005; Rus et al,
2007). These approaches clearly cannot scale to use
the wide range of information used in our system.
Binary features Table 2 gives the binary features
and feature templates. These are templates in the
sense that every unique word or tag fills the tem-
plate and corresponds to a unique feature. We can
thus learn if particular words or tags are associated
with ellipsis. We also include binary features to flag
the presence of any optional determiners before w1
or w2. We also have binary features for the context
words that precede and follow the tag sequence in
the source corpus. These context features are analo-
gous to the L/R-CTXT features that were counted in
the auxiliary corpus. Our classifier learns, for exam-
Monolingual: x?m Bilingual: x?b
C(w1):14.4 C(detl=h * w1 * w2),Dutch:1
C(w2):15.4 C(detl=h * * w1 * * w2),Fr.:1
C(h):17.2 C(detl=h w1 h * w2),Greek:1
C(w1 CC w2):9.0 C(detl=h w1 * w2),Spanish:1
C(w1 h):9.8 C(detl=w1- * w2h),Swedish:1
C(w2 h):10.2 C(simp=h w1 w2),Dutch:1
C(w2 CC w1):10.5 C(simp=h w1 w2),French:1
C(w1 CC h):3.5 C(simp=h w1 h w2),Greek:1
C(h CC w1):6.8 C(simp=h w1 w2),Spanish:1
C(DT w2 CC w1:7.8 C(simp=w1 w2h),Swedish:1
C(w1 and w2 h and):2.4 C(span=5),Dutch:1
C(h PREP w1 CC w2):2.6 C(span=7),French:1
wrd1=dairy:1 C(span=5),Greek:1
wrd2=meat:1 C(span=4),Spanish:1
wrdh=production:1 C(span=3),Swedish:1
tag1=NN:1 C(ord=h w1 w2),Dutch:1
tag2=NN:1 C(ord=h w1 w2),French:1
tagh=NN:1 C(ord=h w1 h w2),Greek:1
wrd12=dairy,meat:1 C(ord=h w1 w2),Spanish:1
tag12=NN,NN:1 C(ord=w1 w2 h),Swedish:1
tag(w1)=tag(w2):1 C(ord=h w1 w2):4
tag12h=NN,NN,NN:1 C(ord=w1 w2 h):1
Table 3: Example of actual instantiated feature vectors
for dairy and meat production (in label:value format).
Monolingual feature vector, x?m, on the left (both count
and binary features, see Table 2), Bilingual feature vec-
tor, x?b, on the right (see Table 4).
ple, that instances preceded by the words its and in
are likely to have ellipsis: these words tend to pre-
cede single NPs as opposed to conjoined NP pairs.
Example Table 3 provides part of the actual in-
stantiated monolingual feature vector for dairy and
meat production. Note the count features have log-
arithmic values, while only the non-zero binary fea-
tures are included.
A later stage of processing extracts a list of feature
labels from the training data. This list is then used
to map feature labels to integers, yielding the stan-
dard (sparse) format used by most machine learning
software (e.g., 1:14.4 2:15.4 3:17.2 ... 7149:1 24208:1).
3.2 Bilingual Features
The above features represent the best of the infor-
mation available to a coordinate NP classifier when
operating on an arbitrary text. In some domains,
however, we have additional information to inform
our decisions. We consider the case where we seek
to predict coordinate structure in parallel text: i.e.,
English text with a corresponding translation in one
1349
or more target languages. A variety of mature NLP
tools exists in this domain, allowing us to robustly
align the parallel text first at the sentence and then
at the word level. Given a word-aligned parallel cor-
pus, we can see how the different types of coordinate
NPs are translated in the target languages.
In Romance languages, examples with ellipsis,
such as dairy and meat production (Table 1), tend to
correspond to translations with the head in the first
position, e.g. ?produccio?n la?ctea y ca?rnica? in Span-
ish (examples taken from Europarl (Koehn, 2005)).
When there is no ellipsis, the head-first syntax leads
to the ?w1 and h w2? ordering, e.g. amianto e o
cloreto de polivinilo in Portuguese. Another clue
for ellipsis is the presence of a dangling hyphen, as
in the Finnish maidon- ja lihantuotantoon. We find
such hyphens especially common in Germanic lan-
guages like Dutch. In addition to language-specific
clues, a translation may resolve an ambiguity by
paraphrasing the example in the same way it may
be paraphrased in English. E.g., we see hard and
soft drugs translated into Spanish as drogas blandas
y drogas duras with the head, drogas, repeated (akin
to soft drugs and hard drugs in English).
One could imagine manually defining the rela-
tionship between English NP coordination and the
patterns in each language, but this would need to be
repeated for each language pair, and would likely
miss many useful patterns. In contrast, by represent-
ing the translation patterns as features in a classifier,
we can instead automatically learn the coordination-
translation correspondences, in any language pair.
For each occurrence of a coordinate NP in a word-
aligned bitext, we inspect the alignments and de-
termine the mapping of w1, w2 and h. Recall that
each of our examples represents all the occurrences
of a unique coordinate NP in a corpus. We there-
fore aggregate translation information over all the
occurrences. Since the alignments in automatically-
aligned parallel text are noisy, the more occurrences
we have, the more translations we have, and the
more likely we are to make a correct decision. For
some common instances in Europarl, like Agricul-
ture and Rural Development, we have thousands of
translations in several languages.
Table 4 provides the bilingual feature templates.
The notation indicates that, for a given coordi-
nate NP, we count the frequency of each transla-
C?detl(w1,w2,h)?,?LANG?
C?simp(w1,w2,h)?,?LANG?
C?span(w1,w2,h)?,?LANG?
C?ord(w1,w2,h)?,?LANG?
C?ord(w1,w2,h)?
Table 4: Real-valued bilingual feature templates. The
shorthand is detl=?detailed pattern,? simp=?simple pat-
tern,? span=?span of pattern,? ord=?order of words.? The
notation C?p?,?LANG?means the number of times we see
the pattern (or span) ?p? as the aligned translation of the
coordinate NP in the target language ?LANG?.
tion pattern in each target language, and generate
real-valued features for these counts. The feature
counts are indexed to the particular pattern and lan-
guage. We also have one language-independent fea-
ture, C?ord(w1,w2,h)?, which gives the frequency of
each ordering across all languages. The span is the
number of tokens collectively spanned by the trans-
lations of w1, w2 and h. The ?detailed pattern? rep-
resents the translation using wildcards for all other
foreign words, but maintains punctuation. Letting
?*? stand for the wildcard, the detailed patterns for
the translations of dairy and meat production in Ta-
ble 1 would be [h w1 * w2] (Spanish), [w1- * w2h]
(Finnish) and [h * * w1 * * w2] (French). Four
or more consecutive wildcards are converted to ?...?.
For the ?simple pattern,? we remove the wildcards
and punctuation. Note that our aligner allows the
English word to map to multiple target words. The
simple pattern differs from the ordering in that it de-
notes how many tokens each of w1, w2 and h span.
Example Table 3 also provides part of the actual
instantiated bilingual feature vector for dairy and
meat production.
4 Bilingual Co-training
We exploit the orthogonality of the monolingual
and bilingual features using semi-supervised learn-
ing. These features are orthogonal in the sense that
they look at different sources of information for each
example. If we had enough training data, a good
classifier could be trained using either monolingual
or bilingual features on their own. With classifiers
trained on even a little labeled data, it?s feasible that
for a particular example, the monolingual classifier
might be confident when the bilingual classifier is
1350
Algorithm 1 The bilingual co-training algorithm: subscript m corresponds to monolingual, b to bilingual
Given: ? a set L of labeled training examples in the bitext, {(x?i, yi)}
? a set U of unlabeled examples in the bitext, {x?j}
? hyperparams: k (num. iterations), um and ub (size smaller unlabeled pools), nm and nb
(num. new labeled examples each iteration), C: regularization param. for classifier training
Create Lm ? L
Create Lb ? L
Create a pool Um by choosing um examples randomly from U .
Create a pool Ub by choosing ub examples randomly from U .
for i = 0 to k do
Use Lm to train a classifier hm using only x?m, the monolingual features of x?
Use Lb to train a classifier hb using only x?b, the bilingual features of x?
Use hm to label Um, move the nm most-confident examples to Lb
Use hb to label Ub, move the nb most-confident examples to Lm
Replenish Um and Ub randomly from U with nm and nb new examples
end for
uncertain, and vice versa. This suggests using a
co-training approach (Yarowsky, 1995; Blum and
Mitchell, 1998). We train separate classifiers on the
labeled data. We use the predictions of one classi-
fier to label new examples for training the orthogo-
nal classifier. We iterate this training and labeling.
We outline how this procedure can be applied to
bitext data in Algorithm 1 (above). We follow prior
work in drawing predictions from smaller pools, Um
and Ub, rather than from U itself, to ensure the la-
beled examples ?are more representative of the un-
derlying distribution? (Blum and Mitchell, 1998).
We use a logistic regression classifier for hm and
hb. Like Blum and Mitchell (1998), we also create
a combined classifier by making predictions accord-
ing to argmaxy=1,0 Pr(y|xm)Pr(y|xb).
The hyperparameters of the algorithm are 1) k,
the number of iterations, 2) um and ub, the size of
the smaller unlabeled pools, 3) nm and nb, the num-
ber of new labeled examples to include at each itera-
tion, and 4) the regularization parameter of the logis-
tic regression classifier. All such parameters can be
tuned on a development set. Like Blum and Mitchell
(1998), we ensure that we maintain roughly the true
class balance in the labeled examples added at each
iteration; we also estimate this balance using devel-
opment data.
There are some differences between our approach
and the co-training algorithm presented in Blum and
Mitchell (1998, Table 1). One of our key goals is to
produce an accurate classifier that uses only mono-
lingual features, since only this classifier can be ap-
plied to arbitrary monolingual text. We thus break
the symmetry in the original algorithm and allow hb
to label more examples for hm than vice versa, so
that hm will improve faster. This is desirable be-
cause we don?t have unlimited unlabeled examples
to draw from, only those found in our parallel text.
5 Data
Web-scale text data is used for monolingual feature
counts, parallel text is used for classifier co-training,
and labeled data is used for training and evaluation.
Web-scale N-gram Data We extract our counts
from Google V2: a new N-gram corpus (with
N-grams of length one-to-five) created from the
same one-trillion-word snapshot of the web as the
Google 5-gram Corpus (Brants and Franz, 2006),
but with enhanced filtering and processing of the
source text (Lin et al, 2010, Section 5). We get
counts using the suffix array tools described in (Lin
et al, 2010). We add one to all counts for smooth-
ing.
Parallel Data We use the Danish, German, Greek,
Spanish, Finnish, French, Italian, Dutch, Por-
tuguese, and Swedish portions of Europarl (Koehn,
2005). We also use the Czech, German, Span-
ish and French news commentary data from WMT
1351
2010.1 Word-aligned English-Foreign bitexts are
created using the Berkeley aligner.2 We run 5 itera-
tions of joint IBM Model 1 training, followed by 3-
to-5 iterations of joint HMM training, and align with
the competitive-thresholding heuristic. The English
portions of all bitexts are part-of-speech tagged with
CRFTagger (Phan, 2006). 94K unique coordinate
NPs and their translations are then extracted.
Labeled Data For experiments within the paral-
lel text, we manually labeled 1320 of the 94K co-
ordinate NP examples. We use 605 examples to set
development parameters, 607 examples as held-out
test data, and 2, 10 or 100 examples for training.
For experiments on the WSJ portion of the Penn
Treebank, we merge the original Treebank annota-
tions with the NP annotations provided by Vadas and
Curran (2007a). We collect all coordinate NP se-
quences matching our pattern and collapse them into
a single example. We label these instances by deter-
mining whether the annotations have w1 and w2 con-
joined. In only one case did the same coordinate NP
have different labels in different occurrences; this
was clearly an error and resolved accordingly. We
collected 1777 coordinate NPs in total, and divided
them into 777 examples for training, 500 for devel-
opment and 500 as a final held-out test set.
6 Evaluation and Settings
We evaluate using accuracy: the percentage of ex-
amples classified correctly in held-out test data.
We compare our systems to a baseline referred to
as the Tag-Triple classifier. This classifier has a
single feature: the tag(w1), tag(w2), tag(h) triple.
Tag-Triple is therefore essentially a discriminative,
unlexicalized parser for our coordinate NPs.
All classifiers use L2-regularized logistic regres-
sion training via LIBLINEAR (Fan et al, 2008). For
co-training, we fix regularization at C = 0.1. For all
other classifiers, we optimize the C parameter on the
development data. At each iteration, i, classifier hm
annotates 50 new examples for training hb, from a
pool of 750 examples, while hb annotates 50 ? i new
examples for hm, from a pool of 750 ? i examples.
This ensures hm gets the majority of automatically-
labeled examples.
1
www.statmt.org/wmt10/translation-task.html
2
nlp.cs.berkeley.edu/pages/wordaligner.html
 86
 88
 90
 92
 94
 96
 98
 100
 0  10  20  30  40  50  60
Ac
cu
ra
cy
 (%
)
Co-training iteration
Bilingual View
Monolingual View
Combined
Figure 1: Accuracy on Bitext development data over the
course of co-training (from 10 initial seed examples).
We also set k, the number of co-training itera-
tions. The monolingual, bilingual, and combined
classifiers reach their optimum levels of perfor-
mance after different numbers of iterations (Fig-
ure 1). We therefore set k separately for each, stop-
ping around 16 iterations for the combined, 51 for
the monolingual, and 57 for the bilingual classifier.
7 Bitext Experiments
We evaluate our systems on our held-out bitext data.
The majority class is ellipsis, in 55.8% of exam-
ples. For comparison, we ran two publicly-available
broad-coverage parsers and analyzed whether they
correctly predicted ellipsis. The parsers were the
C&C parser (Curran et al, 2007) and Minipar (Lin,
1998). They achieved 78.6% and 77.6%.3
Table 5 shows that co-training results in much
more accurate classifiers than supervised training
alone, regardless of the features or amount of ini-
tial training data. The Tag-Triple system is the
weakest system in all cases. This shows that better
monolingual features are very important, but semi-
supervised training can also make a big difference.
3We provided the parsers full sentences containing the NPs. We
directly extracted the labels from the C&C bracketing, while
for Minipar we checked whether w1 was the head of w2. Of
course, the parsers performed very poorly on ellipsis involving
two nouns (partly because NP structure is absent from their
training corpora (see ? 2 and also Vadas and Curran (2008)),
but neither exceeded 88% on adjective or mixed pairs either.
1352
# of Examples
System 2 10 100
Tag-Triple classifier 67.4 79.1 82.9
Monolingual classifier 69.9 90.8 91.6
Co-trained Mono. classifier 96.4 95.9 96.0
Relative error reduction via co-training 88% 62% 52%
Bilingual classifier 76.8 85.5 92.1
Co-trained Bili. classifier 93.2 93.2 93.9
Relative error reduction via co-training 71% 53% 23%
Mono.+Bili. classifier 69.9 91.4 94.9
Co-trained Combo classifier 96.7 96.7 96.7
Relative error reduction via co-training 89% 62% 35%
Table 5: Co-training improves accuracy (%) over stan-
dard supervised learning on Bitext test data for different
feature types and number of training examples.
System Accuracy ?
Monolingual alone 91.6 -
+ Bilingual 94.9 39%
+ Co-training 96.0 54%
+ Bilingual & Co-training 96.7 61%
Table 6: Net benefits of bilingual features and co-training
on Bitext data, 100-training-example setting. ? = rela-
tive error reduction over Monolingual alone.
Table 6 shows the net benefit of our main contri-
butions. Bilingual features clearly help on this task,
but not as much as co-training. With bilingual fea-
tures and co-training together, we achieve 96.7% ac-
curacy. This combined system could be used to very
accurately resolve coordinate ambiguity in parallel
data prior to training an MT system.
8 WSJ Experiments
While we can now accurately resolve coordinate NP
ambiguity in parallel text, it would be even better
if this accuracy carried over to new domains, where
bilingual features are not available. We test the ro-
bustness of our co-trained monolingual classifier by
evaluating it on our labeled WSJ data.
The Penn Treebank and the annotations added by
Vadas and Curran (2007a) comprise a very special
corpus; such data is clearly not available in every
domain. We can take advantage of the plentiful la-
beled examples to also test how our co-trained sys-
tem compares to supervised systems trained with in-
System Training WSJ Acc.Set # Nouns All
Nakov & Hearst - - 79.2 84.8
Tag-Triple WSJ 777 76.1 82.4
Pitler et al WSJ 777 92.3 92.8
MonoWSJ WSJ 777 92.3 94.4
Co-trained Bitext 2 93.8 95.6
Table 7: Coordinate resolution accuracy (%) on WSJ.
domain labeled examples, and also other systems,
like Nakov and Hearst (2005), which although un-
supervised, are tuned on WSJ data.
We reimplemented Nakov and Hearst (2005)4 and
Pitler et al (2010)5 and trained the latter on WSJ an-
notations. We compare these systems to Tag-Triple
and also to a supervised system trained on the WSJ
using only our monolingual features (MonoWSJ).
The (out-of-domain) bitext co-trained system is the
best system on the WSJ data, both on just the ex-
amples where w1 and w2 are nouns (Nouns), and on
all examples (All) (Table 7).6 It is statistically sig-
nificantly better than the prior state-of-the-art Pitler
et al system (McNemar?s test, p<0.05) and also
exceeds the WSJ-trained system using monolingual
features (p<0.2). This domain robustness is less sur-
prising given its key features are derived from web-
scale N-gram data; such features are known to gen-
eralize well across domains (Bergsma et al, 2010).
We tried co-training without the N-gram features,
and performance was worse on the WSJ (85%) than
supervised training on WSJ data alone (87%).
9 Related Work
Bilingual data has been used to resolve a range of
ambiguities, from PP-attachment (Schwartz et al,
2003; Fossum and Knight, 2008), to distinguishing
grammatical roles (Schwarck et al, 2010), to full
dependency parsing (Huang et al, 2009). Related
4Nakov and Hearst (2005) use an unsupervised algorithm that
predicts ellipsis on the basis of a majority vote over a number
of pattern counts and established heuristics.
5Pitler et al (2010) uses a supervised classifier to predict brack-
etings; their count and binary features are a strict subset of the
features used in our Monolingual classifier.
6For co-training, we tuned k on the WSJ dev set but left other
parameters the same. We start from 2 training instances; results
were the same or slightly better with 10 or 100 instances.
1353
work has also focused on projecting syntactic an-
notations from one language to another (Yarowsky
and Ngai, 2001; Hwa et al, 2005), and jointly pars-
ing the two sides of a bitext by leveraging the align-
ments during training and testing (Smith and Smith,
2004; Burkett and Klein, 2008) or just during train-
ing (Snyder et al, 2009). None of this work has fo-
cused on coordination, nor has it combined bitexts
with web-scale monolingual information.
Most prior work has focused on leveraging the
alignments between a single pair of languages. Da-
gan et al (1991) first articulated the need for ?a mul-
tilingual corpora based system, which exploits the
differences between languages to automatically ac-
quire knowledge about word senses.? Kuhn (2004)
used alignments across several Europarl bitexts to
devise rules for identifying parse distituents. Ban-
nard and Callison-Burch (2005) used multiple bi-
texts as part of a system for extracting paraphrases.
Our co-training algorithm is well suited to using
multiple bitexts because it automatically learns the
value of alignment information in each language. In
addition, our approach copes with noisy alignments
both by aggregating information across languages
(and repeated occurrences within a language), and
by only selecting the most confident examples at
each iteration. Burkett et al (2010) also pro-
posed exploiting monolingual-view and bilingual-
view predictors. In their work, the bilingual view
encodes the per-instance agreement between mono-
lingual predictors in two languages, while our bilin-
gual view encodes the alignment and target text to-
gether, across multiple instances and languages.
The other side of the coin is the use of syntax to
perform better translation (Wu, 1997). This is a rich
field of research with its own annual workshop (Syn-
tax and Structure in Translation).
Our monolingual model is most similar to pre-
vious work using counts from web-scale text, both
for resolving coordination ambiguity (Nakov and
Hearst, 2005; Rus et al, 2007; Pitler et al, 2010),
and for syntax and semantics in general (Lapata
and Keller, 2005; Bergsma et al, 2010). We do
not currently use semantic similarity (either tax-
onomic (Resnik, 1999) or distributional (Hogan,
2007)) which has previously been found useful for
coordination. Our model can easily include such in-
formation as additional features. Adding new fea-
tures without adding new training data is often prob-
lematic, but is promising in our framework, since the
bitexts provide so much indirect supervision.
10 Conclusion
Resolving coordination ambiguity is hard. Parsers
are reporting impressive numbers these days, but
coordination remains an area with room for im-
provement. We focused on a specific subcase, com-
plex NPs, and introduced a new evaluation set. We
achieved a huge performance improvement from
79% for state-of-the-art parsers to 96%.7
Size matters. Most parsers are trained on a mere
million words of the Penn Treebank. In this work,
we show how to take advantage of billions of words
of bitexts and trillions of words of unlabeled mono-
lingual text. Larger corpora make it possible to
use associations among lexical items (compare dairy
production vs. asbestos chloride) and precise para-
phrases (production of dairy and meat). Bitexts are
helpful when the ambiguity can be resolved by some
feature in another language (such as word order).
The Treebank is convenient for supervised train-
ing because it has annotations. We show that even
without such annotations, high-quality supervised
models can be trained using co-training and features
derived from huge volumes of unlabeled data.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proc. ACL,
pages 597?604.
Shane Bergsma, Emily Pitler, and Dekang Lin. 2010.
Creating robust supervised classifiers via web-scale n-
gram data. In Proc. ACL, pages 865?874.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proc.
COLT, pages 92?100.
Thorsten Brants and Alex Franz. 2006. The Google Web
1T 5-gram Corpus Version 1.1. LDC2006T13.
David Burkett and Dan Klein. 2008. Two languages
are better than one (for syntactic parsing). In Proc.
EMNLP, pages 877?886.
David Burkett, Slav Petrov, John Blitzer, and Dan Klein.
2010. Learning better monolingual models with unan-
notated bilingual text. In Proc. CoNLL, pages 46?53.
7Evaluation scripts and data are available online:
www.clsp.jhu.edu/?sbergsma/coordNP.ACL11.zip
1354
James Curran, Stephen Clark, and Johan Bos. 2007. Lin-
guistically motivated large-scale NLP with C&C and
Boxer. In Proc. ACL Demo and Poster Sessions, pages
33?36.
Ido Dagan and Alan Itai. 1990. Automatic processing of
large corpora for the resolution of anaphora references.
In Proc. COLING, pages 330?332.
Ido Dagan, Alon Itai, and Ulrike Schwall. 1991. Two
languages are more informative than one. In Proc.
ACL, pages 130?137.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. JMLR, 9:1871?
1874.
Victoria Fossum and Kevin Knight. 2008. Using bilin-
gual Chinese-English word alignments to resolve PP-
attachment ambiguity in English. In Proc. AMTA Stu-
dent Workshop, pages 48?53.
Donald Hindle and Mats Rooth. 1993. Structural ambi-
guity and lexical relations. Computational Linguistics,
19(1):103?120.
Deirdre Hogan. 2007. Coordinate noun phrase disam-
biguation in a generative parsing model. In Proc. ACL,
pages 680?687.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proc. EMNLP, pages 1222?1231.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering, 11(3):311?325.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proc. MT Summit X.
Jonas Kuhn. 2004. Experiments in parallel-text based
grammar induction. In Proc. ACL, pages 470?477.
Mirella Lapata and Frank Keller. 2005. Web-based
models for natural language processing. ACM Trans.
Speech and Language Processing, 2(1):1?31.
Mark Lauer. 1995. Corpus statistics meet the noun com-
pound: Some empirical results. In Proc. ACL, pages
47?54.
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil, Emily
Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani,
and Sushant Narsale. 2010. New tools for web-scale
N-grams. In Proc. LREC.
Dekang Lin. 1998. Dependency-based evaluation of
MINIPAR. In Proc. LREC Workshop on the Evalu-
ation of Parsing Systems.
Mitchell P. Marcus, Beatrice Santorini, and Mary
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Preslav Nakov and Marti Hearst. 2005. Using the web as
an implicit training set: application to structural ambi-
guity resolution. In Proc. HLT-EMNLP, pages 17?24.
Xuan-Hieu Phan. 2006. CRFTagger: CRF English POS
Tagger. crftagger.sourceforge.net.
Emily Pitler, Shane Bergsma, Dekang Lin, and Kenneth
Church. 2010. Using web-scale N-grams to improve
base NP parsing performance. In In Proc. COLING,
pages 886?894.
Philip Resnik. 1999. Semantic similarity in a taxonomy:
An information-based measure and its application to
problems of ambiguity in natural language. Journal of
Artificial Intelligence Research, 11:95?130.
Vasile Rus, Sireesha Ravi, Mihai C. Lintean, and
Philip M. McCarthy. 2007. Unsupervised method for
parsing coordinated base noun phrases. In Proc. CI-
CLing, pages 229?240.
Florian Schwarck, Alexander Fraser, and Hinrich
Schu?tze. 2010. Bitext-based resolution of German
subject-object ambiguities. In Proc. HLT-NAACL,
pages 737?740.
Lee Schwartz, Takako Aikawa, and Chris Quirk. 2003.
Disambiguation of English PP attachment using mul-
tilingual aligned data. In Proc. MT Summit IX, pages
330?337.
David A. Smith and Noah A. Smith. 2004. Bilingual
parsing with factored estimation: Using English to
parse Korean. In Proc. EMNLP, pages 49?56.
Benjamin Snyder, Tahira Naseem, and Regina Barzilay.
2009. Unsupervised multilingual grammar induction.
In Proc. ACL-IJCNLP, pages 1041?1050.
David Vadas and James R. Curran. 2007a. Adding noun
phrase structure to the Penn Treebank. In Proc. ACL,
pages 240?247.
David Vadas and James R. Curran. 2007b. Large-scale
supervised models for noun phrase bracketing. In PA-
CLING, pages 104?112.
David Vadas and James R. Curran. 2008. Parsing noun
phrase structure with CCG. In Proc. ACL, pages 104?
112.
Vladimir N. Vapnik. 1998. Statistical Learning Theory.
John Wiley & Sons.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
David Yarowsky and Grace Ngai. 2001. Inducing multi-
lingual POS taggers and NP bracketers via robust pro-
jection across aligned corpora. In Proc. NAACL, pages
1?8.
David Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proc. ACL,
pages 189?196.
1355
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 514?518,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Typed Graph Models for Semi-Supervised Learning of Name Ethnicity
Delip Rao
Dept. of Computer Science
Johns Hopkins University
delip@cs.jhu.edu
David Yarowsky
Dept. of Computer Science
Johns Hopkins University
yarowsky@cs.jhu.edu
Abstract
This paper presents an original approach to
semi-supervised learning of personal name
ethnicity from typed graphs of morphophone-
mic features and first/last-name co-occurrence
statistics. We frame this as a general solu-
tion to an inference problem over typed graphs
where the edges represent labeled relations be-
tween features that are parameterized by the
edge types. We propose a framework for
parameter estimation on different construc-
tions of typed graphs for this problem us-
ing a gradient-free optimization method based
on grid search. Results on both in-domain
and out-of-domain data show significant gains
over 30% accuracy improvement using the
techniques presented in the paper.
1 Introduction
In the highly relational world of NLP, graphs are
a natural way to represent relations and constraints
among entities of interest. Even problems that are
not obviously graph based can be effectively and
productively encoded as a graph. Such an encoding
will often be comprised of nodes, edges that repre-
sent the relation, and weights on the edges that could
be a metric or a probability-based value, and type
information for the nodes and edges. Typed graphs
are a frequently-used formalism in natural language
problems including dependency parsing (McDonald
et al, 2005), entity disambiguation (Minkov and Co-
hen, 2007), and social networks to just mention a
few.
In this paper, we consider the problem of iden-
tifying a personal attribute such as ethnicity from
only an observed first-name/last-name pair. This has
important consequences in targeted advertising and
personalization in social networks, and in gathering
intelligence for business and government research.
We propose a parametrized typed graph framework
for this problem and perform the hidden attribute in-
ference using random walks on typed graphs. We
also propose a novel application of a gradient-free
optimization technique based on grid search for pa-
rameter estimation in typed graphs. Although, we
describe this in the context of person-attribute learn-
ing, the techniques are general enough to be applied
to various typed graph based problems.
2 Data for Person-Ethnicity Learning
Name ethnicity detection is a particularly challeng-
ing (and practical) problem in Nigeria given that
it has more than 250 ethnicities1 with minor vari-
ations. We constructed a dictionary of Nigerian
names and their associated ethnicity by crawling
baby name sites and other Nigerian diaspora web-
sites (e.g. onlinenigeria.com) to compile a name dic-
tionary of 1980 names with their ethnicity. We re-
tained the top 4 ethnicities ? Yoruba, Igbo, Efik
Ibibio, and Benin Edo2. In addition we also crawled
Facebook to identify Nigerians from different com-
munities. There are more details to this dataset that
1https://www.cia.gov/library/publications/the-world-
factbook/geos/ni.html
2Although the Hausa-Fulani is a populous community from
the north of Nigeria, we did not include it as our dictionary had
very few Hausa-Fulani names. Further, Hausa-Fulani names are
predominantly Arabic or Arabic derivatives and stand out from
the rest of the ethnic groups, making their detection easier.
514
will be made available with the data itself for future
research.
3 Random Walks on Typed Graphs
Consider a graph G = (V,E), with edge set E de-
fined on the vertices in V . A typed graph is one
where every vertex v in V has an associated type
tv ? TV . Analogously, we also use edge types
TE ? TV ? TV . Some examples of typed edges
and vertices used in this paper are shown in Table 1.
These will be elaborated further in Section 4.
Vertices POSITIONAL BIGRAM, BIGRAM,
TRIGRAM, FIRST NAME, LAST NAME, . . .
Edges POSITION (POSITIONAL BIGRAM? BIGRAM),
32BACKOFF (TRIGRAM? BIGRAM),
CONCURRENCE (FIRST NAME? LAST NAME),
. . .
Table 1: Example types for vertices and edges in the
graph for name morpho-phonemics
With every edge type te ? TE we associate a real-
valued parameter ? ? [0, 1]. Thus our graph is pa-
rameterized by a set of parameters ? with |?| =
|TE |. We will need to learn these parameters from
the training data; more on this in Section 5. We re-
lax the estimation problem by forcing the graph to
be undirected. This effectively reduces the number
of parameters by half.
We now have a weighted graph with a weight
matrix W(?). The probability transition matrix
P(?) for the random walk is derived by noting
P(?) = D(?)?1W(?) where D(?) is the diagonal
weighted-degree matrix, i.e, dii(?) =
?
j wij(?).
From this point on, we rely on standard label-
propagation based semi-supervised classification
techniques (Zhu et al, 2003; Baluja et al, 2008;
Talukdar et al, 2008) that work by spreading proba-
bility mass across the edges in the graph. While tra-
ditional label propagation methods proceed by con-
structing graphs using some kernel or arbitrary sim-
ilarity measures, our method estimates the appro-
priate weight matrix from training data using grid
search.
4 Graph construction
Our graphs have two kinds of nodes ? nodes we want
to classify ? called target nodes and feature nodes
which correspond to different feature types. Some
of the target nodes can optionally have label infor-
mation, these are called seed nodes and are excluded
from evaluation. Every feature instance has its own
node and an edge exists between a target node and
a feature node if the target node instantiates the fea-
ture. Features are not independent. For example the
trigram aba also indicates the presence of the bi-
grams ab and ba . We encode this relationship
between features by adding typed edges. For in-
stance, in the previous case, a typed edge (32BACK-
OFF) is added between the trigram aba and the bi-
gram ab representing the backoff relation. In the
absence of these edges between features, our graph
would have been bipartite. We experimented with
three kinds of graphs for this task:
First name/Last name (FN LN) graph
As a first attempt, we only considered first and last
names as features generated by a name. The name
we wish to classify is treated as a target node. There
are two typed relations 1) between the first and last
name, called CONCURRENCE, where the first and
last names occur together and 2) Where an edge,
SHARED NAME, exists between two first (last)
names if they share a last (first) name. Hence there
are only two parameters to estimate here.
Figure 1: A part of the First name/Last name graph:
Edges indicate co-occurrence or a shared name.
Character Ngram graph
The ethnicity of personal names are often indi-
cated by morphophonemic features of the individ-
ual?s given/first or family/last names. For exam-
ple, the last names Polanski, Piotrowski, Soszyn-
ski, Sikorski with the suffix ski indicate Polish de-
scent. Instead of writing suffix rules, we generate
character n-gram features from names ranging from
515
Figure 2: A part of the character n-gram graph: Ob-
serve how the suffix osun contributes to the inference
of adeosun as a Yoruba name even though it was never
seen in training. The different colors on the edges rep-
resent edge types whose weights are estimated from the
data.
bigrams to 5-grams and all orders in-between. We
further distinguish n-grams that appear in the begin-
ning (corresponding to prefixes), middle, and end
(corresponding to suffixes). Thus the last name,
mosun in the graph is connected to the follow-
ing positional trigrams mos-BEG , osu-MID ,
sun-END besides positional n-grams of other or-
ders. The positional trigram mos-BEG connected
to the position-independent trigram mos using the
typed edge POSITION. Further, the trigram mos
is connected to the bigrams mo and os using
a 32BACKOFF edge. The resulting graph has
four typed relations ? 32BACKOFF, 43BACKOFF,
45BACKOFF, and POSITION ? and four corre-
sponding parameters to be estimated.
Combined graph
Finally, we consider the union of the character n-
gram graph and the FirstName-LastName graph. Ta-
ble 2 lists some summary statistics for the various
graphs.
#Vertices #Edges Avg. degree
FN LN 22.8K 137.2K 3.6
CHAR. NGRAM 282.6K 1.2M 8.7
COMBINED 282.6K 1.3M 9.2
Table 2: Graphs for person name ethnicity classification
5 Grid Search for Parameter Estimation
The typed graph we constructed in the previous sec-
tion has as many parameters as the number of edge
types, i.e, |?| = |TE |. We further constrain the val-
ues taken by the parameters to be in the range [0, 1].
Note that there is no loss of representation in doing
so, as arbitrary real-valued weights on edges can be
normalized to the range [0, 1]. Our objective is to
find a set of values for ? that maximizes the classi-
fication accuracy. Towards that effect, we quantize
the range [0, 1] into k equally sized bins and con-
vert this to a discrete-valued optimization problem.
While this is an approximation, our experience finds
that relative values of the various ?i ? ? are more
important than the absolute values for label propa-
gation.
Figure 3: Grid search on a unit 2-simplex with k = 4.
The complexity of this search procedure is O(kn)
for k bins and n parameters. For problems with
small number of parameters, like ours (n = 4 or
n = 2 depending on the graph model), and with
fewer bins this search is still tractable although com-
putationally expensive. We set k = 4; this results
in 256 combinations to be searched at most and we
evaluate each combination in parallel on a cluster.
Clearly, this exhaustive search works only for prob-
lems with few parameters. However, grid search can
still be used in problems with large number of edge
types using one of the following two techniques: 1)
Randomly sample with replacement from a Dirichlet
distribution with same order as the number of bins.
Evaluate using parameter values from each sample
on the development set. Select the parameter values
that result in highest accuracy on the development
set from a large number of samples. 2) Perform a
516
coarse grained search first using a small k on the
range [0, 1] and use that result to shrink the search
range. Perform grid search again on this smaller
range. We simply search exhaustively given the na-
ture of our problem.
6 Experiments & Results
We evaluated our three different model variants un-
der two settings: 1) When only a weak prior from
the dictionary data is present; we call this ?out-of-
domain? since we don?t use any labels from Face-
book and 2) when both the dictionary prior and some
labels from the Facebook data is present; we call this
?in-domain?. The results are reported using 10-fold
cross-validation. In addition to the proposed typed
graph models, we show results from a smoothed-
Na??ve Bayes implementation and two standard base-
lines 1) where labels are assigned uniformly at ran-
dom (UNIFORM) and 2) where labels are assigned
according the empirical prior distribution (PRIOR).
The baseline accuracies are shown in Table 3.
Out-of-domain In-domain
UNIFORM 25.0 25.0
PRIOR 42.6 42.6
Na??ve Bayes 75.1 77.2
Table 3: Ethnicity-classification accuracy from baseline
classifiers.
We performed similar in-domain and out-of-
domain experiments for each of the graph models
proposed in Section 4 and list the results in Table 4,
without using grid search.
Out-of-domain In-domain
FN LN 57.6 60.2
CHAR. NGRAM 73.2 76.8
%gain over FN LN 27% 27.6%
COMBINED 77.1 78.7
%gain over CHAR. NGRAM 5.3% 2.5%
Table 4: Ethnicity-classification accuracy without grid
search
Some points to note about the results reported in
Table 4: 1) These results were obtained without us-
ing parameters from the grid search based optimiza-
tion. 2) The character n-gram graph model performs
better than the first-name/last-name graph model by
itself, as expected due to the smoothing induced by
the backoff edge types. 3) The combination of first-
name/last-name graph and the n-gram improves ac-
curacy by over 30%.
Table 5 reports results from using parameters es-
timated using grid search. The parameter estimation
was done on a development set that was not used
in the 10-fold cross-validation results reported in the
table. Observe that the parameters estimated via grid
search always improved performance of label prop-
agation.
Out-of-domain In-domain
FN LN 59.1 61.4
CHAR. NGRAM 76.7 78.5
COMBINED 78.6 80.1
Improvements by grid search (c.f., Table 4)
FN LN 2.6% 2%
CHAR. NGRAM 4.8% 2.2%
COMBINED 1.5% 1.7%
Table 5: Ethnicity-classification accuracy with grid
search
7 Conclusions
We considered the problem of learning a person?s
ethnicity from his/her name as an inference prob-
lem over typed graphs, where the edges represent la-
beled relations between features that are parameter-
ized by the edge types. We developed a framework
for parameter estimation on different constructions
of typed graphs for this problem using a gradient-
free optimization method based on grid search. We
also proposed alternatives to scale up grid search for
large problem instances. Our results show a sig-
nificant performance improvement over the baseline
and this performance is further improved by param-
eter estimation resulting over 30% improvement in
accuracy using the conjunction of techniques pro-
posed for the task.
References
Shumeet Baluja, Rohan Seth, D. Sivakumar, Yushi Jing,
Jay Yagnik, Shankar Kumar, Deepak Ravichandran,
and Mohamed Aly. 2008. Video suggestion and dis-
covery for youtube: taking random walks through the
view graph. In Proceeding of the 17th international
conference on World Wide Web.
Jonathan Chang, Itamar Rosenn, Lars Backstrom, and
Cameron Marlow. 2010. epluribus: Ethnicity on so-
517
cial networks. In Proceedings of the International
Conference in Weblogs and Social Media (ICWSM).
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Processing.
Association for Computational Linguistics.
Einat Minkov and William Cohen. 2007. Learning to
rank typed graph walks: local and global approaches.
In Proceedings of the 9th WebKDD and 1st SNA-KDD
2007 workshop on Web mining and social network
analysis, New York, NY, USA. ACM.
Partha Pratim Talukdar, Joseph Reisinger, Marius Pas?ca,
Deepak Ravichandran, Rahul Bhagat, and Fernando
Pereira. 2008. Weakly-supervised acquisition of la-
beled class instances using graph random walks. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing. Association for
Computational Linguistics.
Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty.
2003. Semi-supervised learning using gaussian fields
and harmonic functions. In Proceedings of the Inter-
national Conference in Machine Learning, pages 912?
919.
518
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 505?510,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Exploring Sentiment in Social Media: Bootstrapping Subjectivity Clues
from Multilingual Twitter Streams
Svitlana Volkova
CLSP
Johns Hopkins University
Baltimore, MD
svitlana@jhu.edu
Theresa Wilson
HLTCOE
Johns Hopkins University
Baltimore, MD
taw@jhu.edu
David Yarowsky
CLSP
Johns Hopkins University
Baltimore, MD
yarowsky@cs.jhu.edu
Abstract
We study subjective language in social
media and create Twitter-specific lexi-
cons via bootstrapping sentiment-bearing
terms from multilingual Twitter streams.
Starting with a domain-independent, high-
precision sentiment lexicon and a large
pool of unlabeled data, we bootstrap
Twitter-specific sentiment lexicons, us-
ing a small amount of labeled data to
guide the process. Our experiments on
English, Spanish and Russian show that
the resulting lexicons are effective for
sentiment classification for many under-
explored languages in social media.
1 Introduction
The language that people use to express opinions
and sentiment is extremely diverse. This is true for
well-formed data, such as news and reviews, and
it is particularly true for data from social media.
Communication in social media is informal, ab-
breviations and misspellings abound, and the per-
son communicating is often trying to be funny,
creative, and entertaining. Topics change rapidly,
and people invent new words and phrases.
The dynamic nature of social media together
with the extreme diversity of subjective language
has implications for any system with the goal
of analyzing sentiment in this domain. General,
domain-independent sentiment lexicons have low
coverage. Even models trained specifically on so-
cial media data may degrade somewhat over time
as topics change and new sentiment-bearing terms
crop up. For example, the word ?occupy? would
not have been indicative of sentiment before 2011.
Most of the previous work on sentiment lexicon
construction relies on existing natural language
processing tools, e.g., syntactic parsers (Wiebe,
2000), information extraction (IE) tools (Riloff
and Wiebe, 2003) or rich lexical resources such
as WordNet (Esuli and Sebastiani, 2006). How-
ever, such tools and lexical resources are not avail-
able for many languages spoken in social media.
While English is still the top language in Twitter,
it is no longer the majority. Thus, the applicabil-
ity of these approaches is limited. Any method for
analyzing sentiment in microblogs or other social
media streams must be easily adapted to (1) many
low-resource languages, (2) the dynamic nature of
social media, and (3) working in a streaming mode
with limited or no supervision.
Although bootstrapping has been used for learn-
ing sentiment lexicons in other domains (Turney
and Littman, 2002; Banea et al, 2008), it has not
yet been applied to learning sentiment lexicons for
microblogs. In this paper, we present an approach
for bootstrapping subjectivity clues from Twitter
data, and evaluate our approach on English, Span-
ish and Russian Twitter streams. Our approach:
? handles the informality, creativity and the dy-
namic nature of social media;
? does not rely on language-dependent tools;
? scales to the hundreds of new under-explored
languages and dialects in social media;
? classifies sentiment in a streaming mode.
To bootstrap subjectivity clues from Twitter
streams we rely on three main assumptions:
i. sentiment-bearing terms of similar orienta-
tion tend to co-occur at the tweet level (Tur-
ney and Littman, 2002);
ii. sentiment-bearing terms of opposite orienta-
tion do not co-occur at the tweet level (Ga-
mon and Aue, 2005);
iii. the co-occurrence of domain-specific and
domain-independent subjective terms serves
as a signal of subjectivity.
505
2 Related Work
Mihalcea et.al (2012) classifies methods for boot-
strapping subjectivity lexicons into two types:
corpus-based and dictionary-based.
Dictionary-based methods rely on existing lex-
ical resources to bootstrap sentiment lexicons.
Many researchers have explored using relations in
WordNet (Miller, 1995), e.g., Esuli and Sabastiani
(2006), Andreevskaia and Bergler (2006) for En-
glish, Rao and Ravichandran (2009) for Hindi and
French, and Perez-Rosas et al (2012) for Spanish.
Mohammad et al (2009) use a thesaurus to aid
in the construction of a sentiment lexicon for En-
glish. Other works (Clematide and Klenner, 2010;
Abdul-Mageed et al, 2011) automatically expands
and evaluates German and Arabic lexicons. How-
ever, the lexical resources that dictionary-based
methods need, do not yet exist for the majority of
languages in social media. There is also a mis-
match between the formality of many language re-
sources, such as WordNet, and the extremely in-
formal language of social media.
Corpus-based methods extract subjectivity and
sentiment lexicons from large amounts of unla-
beled data using different similarity metrics to
measure the relatedness between words. Hatzivas-
siloglou and McKeown (1997) were the first to ex-
plore automatically learning the polarity of words
from corpora. Early work by Wiebe (2000) iden-
tifies clusters of subjectivity clues based on their
distributional similarity, using a small amount of
data to bootstrap the process. Turney (2002) and
Velikovich et al (2010) bootstrap sentiment lexi-
cons for English from the web by using Pointwise
Mutual Information (PMI) and graph propaga-
tion approach, respectively. Kaji and Kitsuregawa
(2007) propose a method for building sentiment
lexicon for Japanese from HTML pages. Banea
et al (2008) experiment with Lexical Semantic
Analysis (LSA) (Dumais et al, 1988) to bootstrap
a subjectivity lexicon for Romanian. Kanayama
and Nasukawa (2006) bootstrap subjectivity lexi-
cons for Japanese by generating subjectivity can-
didates based on word co-occurrence patterns.
In contrast to other corpus-based bootstrapping
methods, we evaluate our approach on multiple
languages, specifically English, Spanish, and Rus-
sian. Also, as our approach relies only on the
availability of a bilingual dictionary for translating
an English subjectivity lexicon and crowdsourcing
for help in selecting seeds, it is more scalable and
better able to handle the informality and the dy-
namic nature of social media. It also can be effec-
tively used to bootstrap sentiment lexicons for any
language for which a bilingual dictionary is avail-
able or can be automatically induced from parallel
corpora.
3 Data
For the experiments in this paper, we use three
sets of data for each language: 1M unlabeled
tweets (BOOT) for bootstrapping Twitter-specific
lexicons, 2K labeled tweets for development data
(DEV), and 2K labeled tweets for evaluation
(TEST). DEV is used for parameter tuning while
bootstrapping, and TEST is used to evaluating the
quality of the bootstrapped lexicons.
We take English tweets from the corpus con-
structed by Burger et al (2011) which con-
tains 2.9M tweets (excluding retweets) from 184K
users.1 English tweets are identified automati-
cally using a compression-based language identifi-
cation (LID) tool (Bergsma et al, 2012). Accord-
ing to LID, there are 1.8M (63.6%) English tweets,
which we randomly sample to create BOOT, DEV
and TEST sets for English. Unfortunately, Burger?s
corpus does not include Russian and Spanish data
on the same scale as English. Therefore, for
other languages we construct a new Twitter corpus
by downloading tweets from followers of region-
specific news and media feeds.
Sentiment labels for tweets in DEV and TEST
sets for all languages are obtained using Amazon
Mechanical Turk. For each tweet we collect an-
notations from five workers and use majority vote
to determine the final label for the tweet. Snow
et al (2008) show that for a similar task, labeling
emotion and valence, on average four non-expert
labelers are needed to achieve an expert level of
annotation. Table 1 gives the distribution of tweets
over sentiment labels for the development and test
sets for English (E-DEV, E-TEST), Spanish (S-
DEV, S-TEST), and Russian (R-DEV, R-TEST).
Below are examples of tweets in Russian with En-
glish translations labeled with sentiment:
? Positive: ? ?????? ??????? ???????
? ???? ??????? (Planning for delicious
breakfast and lots of movies);
? Negative: ???? ????????, ? ? ??? ??????
(I want to die and I will do that);
1They provided the tweet IDs, and we used the Twitter
Corpus Tools to download the tweets.
506
Data Positive Neg Both Neutral
E-DEV 617 357 202 824
E-TEST 596 347 195 862
S-DEV 358 354 86 1,202
S-TEST 317 387 93 1203
R-DEV 452 463 156 929
R-TEST 488 380 149 983
Table 1: Sentiment label distribution in develop-
ment DEV and test TEST datasets across languages.
? Both: ??????? ???????? ?????? ???
????? ?? ?? ????. ???? ?????? ????-
?? (I want to write about the movie rougher
but I will not. Although the actors are good);
? Neutral: ?????? ????? ????? ????????
?????? ?????? (Why clever thoughts come
only at night?).
4 Lexicon Bootstrapping
To create a Twitter-specific sentiment lexicon for
a given language, we start with a general-purpose,
high-precision sentiment lexicon2 and bootstrap
from the unlabeled data (BOOT) using the labeled
development data (DEV) to guide the process.
4.1 High-Precision Subjectivity Lexicons
For English we seed the bootstrapping pro-
cess with the strongly subjective terms from the
MPQA lexicon3 (Wilson et al, 2005). These
terms have been previously shown to be high-
precision for recognizing subjective sentences
(Riloff and Wiebe, 2003).
For the other languages, the subjective seed
terms are obtained by translating English seed
terms using a bilingual dictionary, and then col-
lecting judgments about term subjectivity from
Mechanical Turk. Terms that truly are strongly
subjective in translation are used for seed terms
in the new language, with term polarity projected
from the English. Finally, we expand the lexicons
with plurals and inflectional forms for adverbs, ad-
jectives and verbs.
4.2 Bootstrapping Approach
To bootstrap, first the new lexicon LB(0) is seeded
with the strongly subjective terms from the orig-
inal lexicon LI . On each iteration i ? 1, tweets
in the unlabeled data are labeled using the lexicon
2Other works on generating domain-specific sentiment
lexicons e.g., from blog data (Jijkoun et al, 2010) also start
with a general, domain-specific lexicon.
3http://www.cs.pitt.edu/mpqa/
from the previous iteration, LB(i?1). If a tweet
contains one or more terms from LB(i?1) it is con-
sidered subjective, otherwise objective. The polar-
ity of subjective tweets is determined in a similar
way: if the tweet contains ? 1 positive terms, tak-
ing into account the negation, it is considered neg-
ative; if it contains ? 1 negative terms, taking into
account the negation, it is considered positive.4 If
it contains both positive and negative terms, it is
considered to be both. Then, for every term not in
LB(i?1) that has a frequency ? ?freq, the proba-
bility of that term being subjective is calculated as
shown in Algorithm 1 line 10. The top ?k terms
with a subjective probability ? ?pr are then added
to LB(i). The polarity of new terms is determined
based on the probability of the term appearing in
positive or negative tweets as shown in line 18.5
The bootstrapping process terminates when there
are no more new terms meeting the criteria to add.
Algorithm 1 BOOTSTRAP (?, ?pr, ?freq, ?topK )
1: iter = 0, ? = 0.5, LB(~?)? LI(?)
2: while (stop 6= true) do
3: LiterB (~?)? ?,?LiterB (~?)? ?
4: for each new term w ? {V \ LB(~?)} do
5: for each tweet t ? T do
6: if w ? t then
7: UPDATE c(w,LB(~?)), c(w,LposB (~?)), c(w)8: end if
9: end for
10: psubj(w)? c(w,LB(~?))c(w)
11: ppos(w)? c(w,L
pos
B (~?))
c(w,LB(~?))
12: LiterB (~?)? w, psubj(w), ppol(w)13: end for
14: SORT LiterB (~?) by psubj(w)15: while (K ? ?topK) do
16: for each new term w ? LiterB (~?) do
17: if [psubj(w) ? ?pr and cw ? ?freq then
18: if [ppos(w) ? 0.5] then
19: wpol ? positive
20: else
21: wpol ? negative
22: end if
23: ?LiterB (~?)? ?LiterB (~?) + wpol24: end if
25: end for
26: K = K + 1
27: end while
28: if [?LiterB (~?) == 0] then29: stop? true
30: end if
31: LB(~?)? LB(~?) + ?LiterB (~?)32: iter = iter + 1
33: end while
4If there is a negation in the two words before a sentiment
term, we flip its polarity.
5Polarity association probabilities should sum up to 1
ppos(w|LB(~?)) + pneg(w|LB(~?)) = 1.
507
English Spanish Russian
LEI LEB LSI LSB LRI LRB
Pos 2.3 16.8 2.9 7.7 1.4 5.3
Neg 2.8 4.7 5.2 14.6 2.3 5.5
Total 5.1 21.5 8.1 22.3 3.7 10.8
Table 2: The original and the bootstrapped (high-
lighted) lexicon term count (LI ? LB) with polar-
ity across languages (thousands).
The set of parameters ~? is optimized using a grid
search on the development data using F-measure
for subjectivity classification. As a result, for En-
glish ~? = [0.7, 5, 50] meaning that on each itera-
tion the top 50 new terms with a frequency ? 5
and probability ? 0.7 are added to the lexicon.
For Spanish, the set of optimal parameters ~? =
[0.65, 3, 50] and for Russian - ~? = [0.65, 3, 50]. In
Table 2 we report size and term polarity from the
original LI and the bootstrapped LB lexicons.
5 Lexicon Evaluations
We evaluate our bootstrapped sentiment lexicons
English LEB , Spanish LSB and Russian LRB by com-
paring them with existing dictionary-expanded
lexicons that have been previously shown to be ef-
fective for subjectivity and polarity classification
(Esuli and Sebastiani, 2006; Perez-Rosas et al,
2012; Chetviorkin and Loukachevitch, 2012). For
that we perform subjectivity and polarity classifi-
cation using rule-based classifiers6 on the test data
E-TEST, S-TEST and R-TEST.
We consider how the various lexicons perform
for rule-based classifiers for both subjectivity and
polarity. The subjectivity classifier predicts that
a tweet is subjective if it contains a) at least one,
or b) at least two subjective terms from the lexi-
con. For the polarity classifier, we predict a tweet
to be positive (negative) if it contains at least one
positive (negative) term taking into account nega-
tion. If the tweet contains both positive and nega-
tive terms, we take the majority label.
For English we compare our bootstrapped lex-
icon LEB against the original lexicon LEI and
strongly subjective terms from SentiWordNet 3.0
(Esuli and Sebastiani, 2006). To make a fair
comparison, we automatically expand SentiWord-
Net with noun plural forms and verb inflectional
forms. In Figure 1 we report precision, recall
6Similar approach to a rule-based classification using
terms from he MPQA lexicon (Riloff and Wiebe, 2003).
and F-measure results. They show that our boot-
strapped lexicon significantly outperforms Senti-
WordNet for subjectivity classification. For polar-
ity classification we get comparable F-measure but
much higher recall for LEB compared to SWN .
(a) Subj ? 1 (b) Subj ? 2 (c) Polarity
Lexicon Fsubj?1 Fsubj?2 Fpolarity
SWN 0.57 0.27 0.78
LEI 0.71 0.48 0.82
LEB 0.75 0.72 0.78
Figure 1: Precision (x-axis), recall (y-axis) and
F-measure (in the table) for English: LEI = ini-
tial lexicon, LEB = bootstrapped lexicon, SWN =
strongly subjective terms from SentiWordNet.
For Spanish we compare our bootstrapped lex-
icon LSB against the original LSI lexicon, and the
full and medium strength terms from the Span-
ish sentiment lexicon constructed by Perez-Rosas
et el. (2012). We report precision, recall and F-
measure in Figure 2. We observe that our boot-
strapped lexicon yields significantly better perfor-
mance for subjectivity classification compared to
both full and medium strength terms. However,
our bootstrapped lexicon yields lower recall and
similar precision for polarity classification.
(a) Subj ? 1 (b) Subj ? 2 (c) Polarity
Lexicon Fsubj?1 Fsubj?2 Fpolarity
SM 0.44 0.17 0.64
SF 0.47 0.13 0.66
LSI 0.59 0.45 0.58
LSB 0.59 0.59 0.55
Figure 2: Precision (x-axis), recall (y-axis) and F-
measure (in the table) for Spanish: LSI = initial
lexicon, LSB = bootstrapped lexicon, SF = full
strength terms; SM = medium strength terms.
508
For Russian we compare our bootstrapped lex-
icon LRB against the original LRI lexicon, and the
Russian sentiment lexicon constructed by Chetv-
iorkin and Loukachevitchet (2012). The external
lexicon in Russian P was built for the domain
of product reviews and does not include polarity
judgments for subjective terms. As before, we
expand the external lexicon with the inflectional
forms for adverbs, adjectives and verbs. We report
results for Russian in Figure 3. We find that for
subjectivity our bootstrapped lexicon shows better
performance compared to the external lexicon (5k
terms). However, the expanded external lexicon
(17k terms) yields higher recall with a significant
drop in precision. Note that for Russian, we report
polarity classification results for LRB and LRI lexi-
cons only because P does not have polarity labels.
(a) Subj ? 1 (b) Subj ? 2 (c) Polarity
Lexicon Fsubj?1 Fsubj?2 Fpolarity
P 0.55 0.29 ?
PX 0.62 0.47 ?
LRI 0.46 0.13 0.73
LRB 0.61 0.35 0.73
Figure 3: Precision (x-axis), recall (y-axis) and F-
measure for Russian: LRI = initial lexicon, LRB =
bootstrapped lexicon, P = external sentiment lex-
icon, PX = expanded external lexicon.
We next perform error analysis for subjectiv-
ity and polarity classification for all languages and
identify common errors to address them in future.
For subjectivity classification we observe that
applying part-of-speech tagging during the boot-
strapping could improve results for all languages.
We could further improve the quality of the lex-
icon and reduce false negative errors (subjec-
tive tweets classified as neutral) by focusing on
sentiment-bearing terms such as adjective, adverbs
and verbs. However, POS taggers for Twitter are
only available for a limited number of languages
such as English (Gimpel et al, 2011). Other false
negative errors are often caused by misspellings.7
7For morphologically-rich languages, our approach cov-
ers different linguistic forms of terms but not their mis-
spellings. However, it can be fixed by an edit-distance check.
We also find subjective tweets with philosophi-
cal thoughts and opinions misclassified, especially
in Russian, e.g., ?????? ?? ?????? ?? ??????
? ?????????? ???????? ????? ?? ??? ???-
?? ??? ?? ??????? ?? ???????? (Sometimes we
are not ready to fulfill our dreams yet but, at the
same time, we do not want to scare them). Such
tweets are difficult to classify using lexicon-based
approaches and require deeper linguistic analysis.
False positive errors for subjectivity classifica-
tion happen because some terms are weakly sub-
jective and can be used in both subjective and
neutral tweets e.g., the Russian term ??????????
(brag) is often used as subjective, but in a tweet
??????? ?? ????? ?????????? ??????? (never
brag about your future) it is used as neutral. Simi-
larly, the Spanish term buenas (good) is often used
subjectively but it is used as neutral in the follow-
ing tweet ?@Diveke me falto el buenas! jaja que
onda que ha pasado? (I miss the good times we
had, haha that wave has passed!).
For polarity classification, most errors happen
because our approach relies on either positive or
negative polarity scores for a term but not both.8
However, in the real world terms may sometimes
have both usages. Thus, some tweets are misclas-
sified (e.g., ?It is too warm outside?). We can
fix this by summing over weighted probabilities
rather than over term counts. Additional errors
happen because tweets are very short and convey
multiple messages (e.g., ?What do you mean by
unconventional? Sounds exciting!?) Thus, our ap-
proach can be further improved by adding word
sense disambiguation and anaphora resolution.
6 Conclusions
We propose a scalable and language independent
bootstrapping approach for learning subjectivity
clues from Twitter streams. We demonstrate the
effectiveness of the bootstrapping procedure by
comparing the resulting subjectivity lexicons with
state-of the-art sentiment lexicons. We perform
error analysis to address the most common error
types in the future. The results confirm that the
approach can be effectively exploited and further
improved for subjectivity classification for many
under-explored languages in social media.
8During the bootstrapping we calculate probability for a
term to be positive and negative, e.g., p(warm|+) = 0.74
and p(warm|?) = 0.26. But during polarity classification
we rely on the highest probability score and consider it to be
?the polarity? for the term e.g., positive for warm.
509
References
Muhammad Abdul-Mageed, Mona T. Diab, and Mo-
hammed Korayem. 2011. Subjectivity and senti-
ment analysis of modern standard arabic. In Pro-
ceedings of ACL/HLT.
Alina Andreevskaia and Sabine Bergler. 2006. Min-
ing wordnet for fuzzy sentiment: Sentiment tag ex-
traction from WordNet glosses. In Proceedings of
EACL.
Carmen Banea, Rada Mihalcea, and Janyce Wiebe.
2008. A bootstrapping method for building subjec-
tivity lexicons for languages with scarce resources.
In Proceedings of LREC.
Shane Bergsma, Paul McNamee, Mossaab Bagdouri,
Clayton Fink, and Theresa Wilson. 2012. Language
identification for creating language-specific Twitter
collections. In Proceedings of 2nd Workshop on
Language in Social Media.
John D. Burger, John C. Henderson, George Kim, and
Guido Zarrella. 2011. Discriminating gender on
Twittier. In Proceedings of EMNLP.
Ilia Chetviorkin and Natalia V. Loukachevitch. 2012.
Extraction of Russian sentiment lexicon for product
meta-domain. In Proceedings of COLING.
Simon Clematide and Manfred Klenner. 2010. Eval-
uation and extension of a polarity lexicon for Ger-
man. In Proceedings of the 1st Workshop on Com-
putational Approaches to Subjectivity and Sentiment
Analysis.
Susan T. Dumais, George W. Furnas, Thomas K. Lan-
dauer, Scott Deerwester, and Richard Harshman.
1988. Using latent semantic analysis to improve
access to textual information. In Proceedings of
SIGCHI.
Andrea Esuli and Fabrizio Sebastiani. 2006. Senti-
WordNet: A publicly available lexical resource for
opinion mining. In Proceedings of LREC.
Michael Gamon and Anthony Aue. 2005. Automatic
identification of sentiment vocabulary: exploiting
low association with known sentiment terms. In
Proceedings of the ACL Workshop on Feature Engi-
neering for Machine Learning in Natural Language
Processing.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for Twittier: annotation, features, and experiments.
In Proceedings of ACL.
Vasileios Hatzivassiloglou and Kathy McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of ACL.
Valentin Jijkoun, Maarten de Rijke, and Wouter
Weerkamp. 2010. Generating focused topic-
specific sentiment lexicons. In Proceedings of ACL.
Nobuhiro Kaji and Masaru Kitsuregawa. 2007. Build-
ing lexicon for sentiment analysis from massive
collection of html documents. In Proceedings of
EMNLP.
Hiroshi Kanayama and Tetsuya Nasukawa. 2006.
Fully automatic lexicon expansion for domain-
oriented sentiment analysis. In Proceedings of
EMNLP.
Rada Mihalcea, Carmen Banea, and Janyce Wiebe.
2012. Multilingual subjectivity and sentiment anal-
ysis. In Proceedings of ACL.
George A. Miller. 1995. Wordnet: a lexical database
for English. Communications of the ACM, 38(11).
Saif Mohammad, Cody Dunne, and Bonnie Dorr.
2009. Generating high-coverage semantic orienta-
tion lexicons from overtly marked words and a the-
saurus. In Proceedings of EMNLP.
Veronica Perez-Rosas, Carmen Banea, and Rada Mi-
halcea. 2012. Learning sentiment lexicons in Span-
ish. In Proceedings of LREC.
Delip Rao and Deepak Ravichandran. 2009. Semi-
supervised polarity lexicon induction. In Proceed-
ings of EACL.
Ellen Riloff and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In Proceed-
ings of EMNLP.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast ? but is it
good?: Evaluating non-expert annotations for natu-
ral language tasks. In Proceedings of EMNLP.
Peter D. Turney and Michael L. Littman. 2002. Un-
supervised learning of semantic orientation from a
hundred-billion-word corpus. Computing Research
Repository.
Peter D. Turney. 2002. Thumbs up or thumbs down?:
Semantic orientation applied to unsupervised classi-
fication of reviews. In Proceedings of ACL.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry
Hannan, and Ryan McDonald. 2010. The viabil-
ity of web-derived polarity lexicons. In Proceedings
of NAACL.
Janyce Wiebe. 2000. Learning subjective adjectives
from corpora. In Proceedings of AAAI.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of EMNLP.
510
Coling 2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Processing, pages 41?48
Manchester, August 2008
Affinity Measures based on the Graph Laplacian
Delip Rao
Dept. of Computer Science
Johns Hopkins University
delip@cs.jhu.edu
David Yarowsky
Dept. of Computer Science
Johns Hopkins University
yarowsky@cs.jhu.edu
Chris Callison-Burch
Dept. of Computer Science
Johns Hopkins University
ccb@cs.jhu.edu
Abstract
Several language processing tasks can be
inherently represented by a weighted graph
where the weights are interpreted as a
measure of relatedness between two ver-
tices. Measuring similarity between ar-
bitary pairs of vertices is essential in solv-
ing several language processing problems
on these datasets. Random walk based
measures perform better than other path
based measures like shortest-path. We
evaluate several random walk measures
and propose a new measure based on com-
mute time. We use the psuedo inverse
of the Laplacian to derive estimates for
commute times in graphs. Further, we
show that this pseudo inverse based mea-
sure could be improved by discarding the
least significant eigenvectors, correspond-
ing to the noise in the graph construction
process, using singular value decomposi-
tion.
1 Introduction
Natural language data lend themselves to a graph
based representation. Words could be linked by
explicit relations as in WordNet (Fellbaum, 1989)
or documents could be linked to one another via
hyperlinks. Even in the absence of such a straight-
forward representation it is possible to derive
meaningful graphs such as the nearest neighbor
graphs as done in certain manifold learning meth-
ods (Roweis and Saul, 2000; Belkin and Niyogi,
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
2001). All of these graphs share the following
properties:
? They are edge-weighted.
? The edge weight encodes some notion of re-
latedness between the vertices.
? The relation represented by edges is at least
weakly transitive. Examples of such rela-
tions include, ?is similar to?, ?is more general
than?, and so on. It is important that the re-
lations selected are transitive for the random
walk to make sense.
Such graphs present several possibilities in solv-
ing language problems on the data. One such task
is, given two vertices in the graph we would like
to know how related the two vertices are. There
is an abundance of literature on this topic, some
of which will be reviewed here. Finding similarity
between vertices in a graph could be an end in it-
self, as in the lexical similarity task, or could be a
stage before solving other problems like clustering
and classification.
2 Contributions of this paper
The major contributions of this paper are
? A comprehensive evaluation of various ran-
dom walk based measures
? Propose a new similarity measure based on
commute time.
? An improvement to the above measure by
eliminating noisy features via singular value
decomposition.
41
3 Problem setting
Consider an undirected graph G(V,E,W) with
vertices V , edges E, and W = [w
ij
] be the sym-
metric adjacency weight matrix with w
ij
as the
weight of the edge connecting vertices i and j. The
weight, w
ij
= 0 for vertices i and j that are not
neighbors and when w
ij
> 0 it is interpreted as an
indication of relatedness between i and j. In our
case, we consider uniformly weighted graphs, i.e,
w
ij
= 1 for neighbors but this need not be the case.
Let n = |V | be the order of the graph. We define
a relation sim : V ? V ? R
+
such that sim(i, j)
is the relatedness between vertices i and j. There
are several ways to define sim; the ones explored
in this paper are:
? sim
G
(i, j) is the reciprocal of the shortest
path length between vertices i and j. Note
that this is not a random walk based mea-
sure but a useful baseline for comparison pur-
poses.
? sim
B
(i, j) is the probability of a random walk
from vertex i to vertex j using all paths of
length less than m.
? sim
P
(i, j) is the probability of a random walk
from vertex i to vertex j defined via a pager-
ank model.
? sim
C
(i, j) is a function of the commute time
between vertex i and vertex j.
4 Data and Evaluation
We evaluate each of the similarity measure we
consider by using a linguistically motivated task
of finding lexical similarity. Deriving lexical
relatedness between terms has been a topic of
interest with applications in word sense disam-
biguation (Patwardhan et al, 2005), paraphras-
ing (Kauchak and Barzilay, 2006), question an-
swering (Prager et al, 2001), and machine trans-
lation (Blatz et al, 2004) to name a few. Lex-
ical relatedness between terms could be derived
either from a thesaurus like WordNet or from
raw monolingual corpora via distributional simi-
larity (Pereira et al, 1993). WordNet is an inter-
esting graph-structured thesaurus where the ver-
tices are the words and the edges represent rela-
tions between the words. For the purpose of this
work, we only consider relations like hypernymy,
hyponymy, and synonymy. The importance of this
problem has generated copious literature in the
past ? see (Pedersen et al, 2004) or (Budanitsky
and Hirst, 2006) for a detailed review of various
lexical relatedness measures on WordNet. Our fo-
cus in this paper is not to derive the best similar-
ity measure for WordNet but to use WordNet and
the lexical relatedness task as a method to evalu-
ate the various random walk based similarity mea-
sures. Following the tradition in previous litera-
ture we evaluate on the Miller and Charles (1991)
dataset. This data consists of 30 word-pairs along
with human judgements which is a real value be-
tween 1 and 4. For every measure we consider,
we derive similarity scores and compare with the
human judgements using the Spearman rank cor-
relation coefficient.
5 Graph construction
For the purpose of evaluation of the random walk
measures, we construct a graph for every pair of
words for which similarity has to be computed.
This graph is derived from WordNet as follows:
? For each word w in the pair (w
1
, w
2
):
? Add an edge between w and all of its
parts of speech. For example, if the word
is coast, add edges between coast and
coast#noun and coast#verb.
? For each word#pos combination,
add edges to all of its senses (For
example, coast#noun#1 through
coast#noun#4.
? For each word sense, add edges to all of
its hyponyms
? For each word sense, add edges to all of
its hypernyms recursively.
In this paper we consider uniform weights on all
edges as our main aim is to illustrate the differ-
ent random walk measures rather than fine tune the
graph construction process.
6 Shortest path based measure
The most obvious measure of distance in a graph is
the shortest path between the vertices which is de-
fined as the minimum number of intervening edges
between two vertices. This is also known as the
geodesic distance. To convert this distance mea-
sure to a similarity measure, we take the recipro-
cal of the shortest-path length. We refer to this as
the geodesic similarity. This is not a random walk
42
Figure 1: Shortest path distances on graphs
measure but will serve as an important baseline for
our work. As can be observed from Table 1, the
Method Spearman correlation
Geodesic 0.275
Table 1: Similarity using shortest-path measure.
correlation is rather poor for the shortest path mea-
sure.
7 Why are shortest path distances bad?
While shortest-path distances are useful in many
applications, it fails to capture the following obser-
vation. Consider the subgraph of WordNet shown
in Figure 1. The term moon is connected to the
terms religious leader and satellite
1
.
Observe that both religious leader and
satellite are at the same shortest path dis-
tance from moon. However, the connectivity
structure of the graph would suggest satellite
to be ?more? similar than religious leader
as there are multiple senses, and hence multiple
paths, connecting satellite and moon.
Thus it is desirable to have a measure that cap-
tures not only path lengths but also the connectiv-
ity structure of the graph. This notion is elegantly
captured using random walks on graphs.
7.1 Similarity via Random walks
A random walk is a stochastic process that consists
of a sequence of discrete steps taken at random de-
fined by a distribution. Random walks have inter-
esting connections to Brownian motion, heat diffu-
sion and have been used in semi-supervised learn-
ing ? for example, see (Zhu et al, 2003). Certain
properties of random walks are defined for ergodic
processes only
2
. In our work, we assume these
1
The religious leader sense of moon is due to Sun
Myung Moon, a US religious leader.
2
A stochastic process is ergodic if the underlying Markov
chain is irreducible and aperiodic. A Markov chain is irre-
hold true as the graphs we deal with are connected,
undirected, and non-bipartite.
7.1.1 Bounded length walks
As our first random walk measure, we consider
the bounded length walk ? i.e., all random walks of
length less than or equal to a bound m. We derive
a probability transition matrix P from the weight
matrix W as follows:
P = D
?1
W
where, D is a diagonal matrix with d
ii
=
?
n
j = 1
w
ij
. Observe that:
? p
ij
= P[i, j] ? 0, and
?
?
n
j = 1
p
ij
= 1
Hence p
ij
can be interpreted as the probability
of transition from vertex i to vertex j in one step. It
is easy to observe that P
k
gives the transition prob-
ability from vertex i to vertex j in k steps. This
leads to the following similarity measure:
S = P + P
2
+ P
3
+ ...+ P
m
Observe that S[i, j] derives the total probability of
transition from vertex i to vertex j in at most m
steps
3
. Given S, we can derive several measures of
similarity:
1. Bounded Walk: S[i, j]
2. Bounded Walk Cosine: dot product of
rowvectors S
i
and S
j
.
When we evaluate these measures on the Miller-
Charles data the results shown in Table 2. are ob-
served. For this experiment, we consider all walks
that are at most 20 steps long, i.e., m = 20. Ob-
serve that these results are significantly better than
the Geodesic similarity based on shortest-paths.
ducible if there exists a path between any two states and it is
aperiodic if the GCD of all cycle lengths is one.
3
The matrix S is row normalized to ensure that the entries
can be interpreted as probabilities.
43
Method Spearman correlation
Bounded Walk 0.346
Bounded Walk Cosine 0.365
Table 2: Similarity using bounded random walks
(m = 20).
7.1.2 How many paths are sufficient?
In the previous experiment, we arbitrarily fixed
m = 20. However, as observed in Figure 2. , be-
yond a certain value the choice of m does not affect
the result as the random walk converges to its sta-
tionary distribution. The choice of m depends on
Figure 2: Effect of m in Bounded walk
the amount of computation available. A reason-
ably large value of m (m > 10) should be suffi-
cient for most purposes and one could use lower
values of m to derive an approximation for this
measure. One could derive an upper bound on the
value of m using the mixing time of the underlying
Markov chain (Aldous and Fill, 2001).
7.1.3 Similarity via pagerank
Pagerank (Page et al, 1998) is the celebrated ci-
tation ranking algorithm that has been applied to
several natural language problems from summa-
rization (Erkan and Radev, 2004) to opinion min-
ing (Esuli and Sebastiani, 2007) to our task of
lexical relatedness (Hughes and Ramage, 2007).
Pagerank is yet another random walk model with a
difference that it allows the random walk to ?jump?
to its initial state with a nonzero probability (?).
Given the probability transition matrix P as defined
above, a stationary distribution vector for any ver-
tex (say i) could be derived as follows:
1. Let e
i
be a vector of all zeros with e
i
(i) = 1
2. Let v
0
= e
i
3. Repeat until ?v
t
? v
t?1
?
F
< ?
? v
t+1
= ?v
t
P + (1? ?)v
0
? t = t+ 1
4. Assign v
t+1
as the stationary distribution for
vertex i.
Armed with the stationary distribution vectors for
vertices i and j, we define pagerank similarity ei-
ther as the cosine of the stationary distribution vec-
tors or the reciprocal Jensen-Shannon (JS) diver-
gence
4
between them. Table 3. shows results on
the Miller-Charles data. We use ? = 0.1, the best
value on this data. Observe that these results are
Method Spearman correlation
Pagerank JS-Divergence 0.379
Pagerank Cosine 0.393
Table 3: Similarity via pagerank (? = 0.1).
better than the best bounded walk result. We fur-
ther note that our results are different from that
of (Hughes and Ramage, 2007) as they use exten-
sive feature engineering and weight tuning during
the graph generation process that we have not been
able to reproduce. Hence for simplicity we stuck to
a simpler graph generation process. Nevertheless,
the result in Table 3. is still useful as we are in-
terested in the performance of the various spectral
similarity measures rather than achieving the best
performance on the lexical relatedness task. The
graphs we use in all methods are identical making
comparisons across methods possible.
7.2 Similarity via Hitting Time
Given a graph with the transition probability ma-
trix P as defined above, the hitting time between
vertices i and j, denoted as h(i, j), is defined as
the expected number of steps taken by a random
walker to first encounter vertex j starting from ver-
tex i. This can be recursively defined as follows:
h(i, j) =
?
?
?
1 +
?
k : w
ik
> 0
p
ik
h(k, j) if i 6= j
0 if i = j
(1)
4
The Jensen-Shannon divergence between two distribu-
tions p and q is defined as D(p ? a)+D(q ? a), where D(. ?
.) is the Kullback-Liebler divergence and a = (p + q)/2.
Note that unlike KL-divergence this measure is symmetric.
See (Lin, 1991) for additional details.
44
The lower the hitting times of two vertices, the
more similar they are. It can be easily verified
that hitting time is not a symmetric relation hence
graph theory literature suggests another symmet-
ric measure ? the commute time.
5
The commute
time, c(i, j), is the expected number of steps taken
to leave vertex i, reach vertex j, and return back to
i. Thus,
c(i, j) = h(i, j) + h(j, i) (2)
Observe that, the commute time is a metric in that
it is positive definite, symmetric, and satisifies tri-
angle inequality. Hence, commute time could be
used as a distance measure as well. We derive a
similarity measure from this distance measure us-
ing the following lemma.
Lemma 1. For every edge (i, j), c(i, j) ? 2l
where l = |E|, the number of edges.
Proof. This can be easily observed by defining a
Markov chain on the edges with probability tran-
sition matrix Q with 2l states, such that Q
e
1
e
2
=
1/degree(e
1
? e
2
). Since this matrix is doubly
stochastic, the stationary distribution on this chain
will be uniform with a probability 1/2l. Now
c(i, j) = h(i, j)+h(j, i), is the expected time for a
walk to start at i, visit j, and return back to i. When
the stationary probability at each edge is 1/2l, this
expected time evaluates to 2l. Hence the commute
time can be at most 2l.
This lemma allows us to define a similarity mea-
sure as follows:
sim
C
(i, j) = 1?
c(i, j)
2l
(3)
Observe that the measure defined in Equation 3 is
a metric and further its range is defined in [0, 1].
We now only need a way to compute the commute
times to use Equation 3. One could compute the
hitting times and hence the commute times from
the Equations 1 and 2 using dynamic program-
ming, akin to shortest paths in graphs. In this pa-
per, we instead choose to derive commute times
via the graph Laplacian. This also allows us to
handle ?noise? in the graph construction process
which cannot be taken care by naive dynamic pro-
gramming.
5
Note that distance measures, in general, need not be sym-
metric but we interpret distance as proximity which mandates
symmetry.
Chandra et. al. (1989) show that the commute
time between two vertices is equal to the resis-
tance distance between them. Resistance distance,
as proposed by Klein and Randic (1993), is the
effective resistance between two vertices in the
electrical network represented by the graph, where
the edges have resistance 1/w
ij
. Xiao and Gut-
man (2003), show the relation between resistance
distances in graphs to the Laplacian spectrum, thus
enabling a way to derive commute times from the
graph Laplacian in closed form.
We now introduce graph Laplacians, which are
interesting in their own right besides being related
to commute time. The Laplacian of a graph could
be viewed as a discrete version of the Laplace-
Beltrami operator on Riemannian manifolds. It is
defined as
L = D ? W
The graph Laplacian has interesting properties and
a wide range of applications, in semi-supervised
learning (Zhu et al, 2003), non-linear dimension-
ality reduction (Roweis and Saul, 2000; Belkin and
Niyogi, 2001), and so on. See (Chung, 1997) for
a thorough introduction on Laplacians and their
properties. We depend on the fact that L is:
1. symmetric (since D and W are for undirected
graphs)
2. positive-semidefinite : since it is symmet-
ric, all of the eigenvalues are real and by
the Greshgorin circle theorem, the eigenval-
ues must also be non-negative and hence L is
positive-semidefinite.
Throughout this paper we use normalized Lapla-
cians as defined below:
L = D
?1/2
LD
?1/2
= I ? D
?1/2
WD
?1/2
The normalized Laplacians preserve all properties
of the Laplacian by construction.
As noted in Xiao and Gutman (2003), the re-
sistance distances can be derived from the gener-
alized Moore-Penrose pseudo-inverse of the graph
Laplacian(L
?
) ? also called the inverse Laplacian.
Like Laplacians, their pseudo inverse counterparts
are also symmetric, and positive semi-definite.
Lemma 2. L
?
is symmetric
Proof. The Moore-Penrose pseudo-inverse is de-
fined as L
?
= (L
T
L)
?1
L
T
. From this definition,
it is clear that (L
?
)
T
= (L
T
)
?
. By the symmetry
45
property of graph Laplacians, L
T
= L. Hence,
(L
?
)
T
= L
?
.
Lemma 3. L
?
is positive semi-definite
Proof. We make use of the following properties
from (Chung, 1997):
? The Laplacian, L, is positive semi-definite
(also shown above).
? If the Eigen-decomposition of L is Q?Q
T
,
then the Eigen-decomposition of the pseudo-
inverse L
?
is Q?
?1
Q
T
. If any of the eigenval-
ues of L is zero then the corresponding eigen-
value for L
?
is also zero.
Since L is positive semi-definite, and the eigen-
values of L
?
have the same sign as L, the pseudo
inverse L
?
has to be positive semi-definite.
Lemma 4. The inverse Laplacian is a gram matrix
Proof. To prove this, we use the fact that the
Laplacian Matrix is symmetric and positive semi-
definite. Hence by Cholesky decomposition we
can write L = UU
T
.
Therefore L
?
= (U
T
)
?
U
?
= (U
?
)
T
(U
?
).
Hence L
?
is a matrix of dot-products or a gram-
matrix.
Thus, from Lemmas 2, 3 and 4, the inverse
Laplacian L
?
is a valid Kernel.
7.2.1 Similarity measures from the Laplacian
The pseudo inverse of the Laplacian allows us
to compute the following similarity measures.
1. Since L
?
is a kernel, L
?
ij
can be interpreted a
similarity value of vertices i and j.
2. Commute time: This is due to (Aldous and
Fill, 2001). The commute time, c(i, j) ?
(L
?
ii
+ L
?
jj
? 2L
?
ij
). This allows us to derive
similarities using Equation 3.
Evaluating the above measures with the Miller-
Charles data yields results shown in Table 4.
Again, these results are better than the other ran-
dom walk methods compared in the paper.
Method Spearman correlation
L
?
ij
0.469
Commute Time (sim
C
) 0.520
Table 4: Similarity via inverse Laplacian.
7.2.2 Noise in the graph construction process
The graph construction process outlined in Sec-
tion 5 is not necessarily the best one. In fact, any
method that constructs graphs from existing data
incorporates ?noise? or extraneous features. These
could be spurious edges between vertices, miss-
ing edges, or even improper edge weights. It is
however impossible to know any of this a priori
and some noise is inevitable. The derivation of
commute times via the pseudo inverse of a noisy
Laplacian matrix makes it even worse because the
pseudo inverse amplifies the noise in the original
matrix. This is because the largest singular value
of the pseudo inverse of a matrix is equal to the in-
verse of the smallest singular value of the original
matrix. A standard technique in signal processing
and information retrieval to eliminate noise or han-
dle missing values is to use singular value decom-
position (Deerwester et al, 1990). We apply SVD
to handle noise in the graph construction process.
For a given matrix A, SVD decomposes A into
three matrices U, S, and V such that A = USV
T
,
where S is a diagonal matrix of eigenvalues of A,
and U and V are orthonormal matrices containing
the left and the right eigenvectors respectively. The
top-k eigenvectors and eigenvalues are computed
using the iterative method by Lanczos-Arnoldi (us-
ing LAPACK) and the product of these matrices
represents a ?smoothed? version of the original
Laplacian. The pseudo inverse is then computed
on this smooth Laplacian. Table 5., shows the im-
provements obtained by discarding bottom 20% of
the eigenvalues.
Method Original After SVD
L
?
ij
0.469 0.472
Commute Time (sim
C
) 0.520 0.542
Table 5: Denoising graph Laplacian via SVD
Figure 3. shows the dependence on the num-
ber of eigenvalues selected. As can be observed in
both curves there is a reduction in performance by
adding the last few eigenvectors and hence may be
safely discarded. This observation is true in other
text processing tasks like document clustering or
classification using Latent Semantic Indexing.
8 Related Work
Apart from the related work cited throughout this
paper, we would also like to note the paper by Yen
46
Figure 3: Noise reduction via SVD.
et al(2007) on using sigmoid commute time kernel
on a graph for document clustering but our work
differs in that our goal was to study various ran-
dom walk measures rather than a specific task and
we provide a new similarity measure (ref. Eqn
3) based on an upper bound on the commute time
(Lemma 1). Our work also suggests a way to han-
dle noise in the graph construction process.
9 Conclusions and Future Work
This paper presented an evaluation of random
walk based similarity measures on weighted undi-
rected graphs. We provided an intuitive explana-
tion of why random walk based measures perform
better than shortest-path or geodesic measures,
and backed it with empirical evidence. The ran-
dom walk measures we consider include bounded
length walks, pagerank based measures, and a new
measure based on the commute times in graphs.
We derived the commute times via pseudo inverse
of the graph Laplacian. This enables a new method
of graph similarity using SVD that is robust to the
noise in the graph construction process. Further,
the inverse Laplacian is also interesting in that it is
a kernel by itself and could be used for other tasks
like word clustering, for example.
Acknowledgements
The authors would like to thank David Smith and
Petros Drineas for useful discussions and to Fan
Chung for the wonderful book on Spectral Graph
theory.
References
Aldous and Fill. 2001. Reversible Markov Chains and
Random Walks on Graphs. In preparation.
Belkin, Mikhail and Partha Niyogi. 2001. Laplacian
eigenmaps and spectral techniques for embedding
and clustering. In Proceedings of the NIPS.
Blatz, John, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto San-
chis, and Nicola Ueffing. 2004. Confidence estima-
tion for machine translation. In Proceeding of the
COLING.
Budanitsky, Alexander and Graeme Hirst. 2006. Eval-
uating wordnet-based measures of lexical semantic
relatedness. Computational Linguistics, 32(1):13?
47.
Chandra, Ashok, Prabhakar Raghavan, Walter Ruzzo,
Roman Smolensky, and Prasoon Tiwari. 1989. The
electrical resistance of a graph captures its commute
and cover times. In Proceedings of the STOC.
Chung, Fan. 1997. Spectral graph theory. In CBMS:
Conference Board of the Mathematical Sciences, Re-
gional Conference Series.
Deerwester, Scott, Susan Dumais, George Furnas,
Thomas Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American Society for Information Science, 41.
Erkan, G?unes and Dragomir Radev. 2004. Lexrank:
Graph-based lexical centrality as salience in text
summarization. Journal of Artificial Intelligence Re-
search (JAIR), 22:457?479.
Esuli, Andrea and Fabrizio Sebastiani. 2007. Pager-
anking wordnet synsets: An application to opinion
mining. In Proceedings of the ACL, pages 424?431.
Fellbaum, Christaine, editor. 1989. WordNet: An Elec-
tronic Lexical Database. The MIT Press.
Hughes, Thad and Daniel Ramage. 2007. Lexical
semantic relatedness with random graph walks. In
Proceedings of the EMNLP.
Kauchak, David and Regina Barzilay. 2006. Para-
phrasing for automatic evaluation. In Proceedings
HLT-NAACL.
Klein, D. and M. Randic. 1993. Resistance distance.
Journal of Mathematical Chemistry, 12:81?95.
Lin, Jianhua. 1991. Divergence measures based on the
shannon entropy. IEEE Transactions on Information
Theory, 37(1).
Miller, G. and W. Charles. 1991. Contextual correlates
of semantic similarity. In Language and Cognitive
Process.
Page, Larry, Sergey Brin, Rajeev Motwani, and Terry
Winograd. 1998. The pagerank citation ranking:
Bringing order to the web. Technical report, Stan-
ford University, Stanford, CA.
47
Patwardhan, Siddharth, Satanjeev Banerjee, and Ted
Pedersen. 2005. Senserelate:: Targetword-A gen-
eralized framework for word sense disambiguation.
In Proceedings of the ACL.
Pedersen, Ted, Siddharth Patwardhan, and Jason
Michelizzi. 2004. Wordnet::similarity - measuring
the relatedness of concepts. In Proceedings of the
AAAI.
Pereira, Fernando, Naftali Tishby, and Lillian Lee.
1993. Distributional clustering of english words. In
Proceedings of the ACL.
Prager, John M., Jennifer Chu-Carroll, and Krzysztof
Czuba. 2001. Use of wordnet hypernyms for an-
swering what-is questions. In Proceedings of the
Text REtrieval Conference.
Roweis, Sam and Lawrence Saul. 2000. Nonlinear di-
mensionality reduction by locally linear embedding.
Science, 290:2323?2326.
Xiao, W. and I. Gutman. 2003. Resistance distance and
laplacian spectrum. Theoretical Chemistry Associa-
tion, 110:284?289.
Yen, Luh, Francois Fouss, Christine Decaestecker, Pas-
cal Francq, and Marco Saerens. 2007. Graph nodes
clustering based on the commute-time kernel. In
Proceedings of the PAKDD.
Zhu, Xiaojin, Zoubin Ghahramani, and John Lafferty.
2003. Semi-supervised learning using gaussian
fields and harmonic functions. In Proceedings of the
ICML.
48
