Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1037?1045,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Fast Translation Rule Matching for Syntax-based Statistical 
Machine Translation 
 
 
 
Hui Zhang1, 2   Min Zhang1   Haizhou Li1   Chew Lim Tan2    
1Institute for Infocomm Research                    2National University of Singapore                    
zhangh1982@gmail.com   {mzhang, hli}@i2r.a-star.edu.sg   tancl@comp.nus.edu.sg 
 
 
 
 
 
 
 
Abstract 
In a linguistically-motivated syntax-based trans-
lation system, the entire translation process is 
normally carried out in two steps, translation 
rule matching and target sentence decoding us-
ing the matched rules. Both steps are very time-
consuming due to the tremendous number of 
translation rules, the exhaustive search in trans-
lation rule matching and the complex nature of 
the translation task itself. In this paper, we pro-
pose a hyper-tree-based fast algorithm for trans-
lation rule matching. Experimental results on 
the NIST MT-2003 Chinese-English translation 
task show that our algorithm is at least 19 times 
faster in rule matching and is able to help to 
save 57% of overall translation time over previ-
ous methods when using large fragment transla-
tion rules. 
1 Introduction 
Recently linguistically-motivated syntax-based 
translation method has achieved great success in 
statistical machine translation (SMT) (Galley et al, 
2004; Liu et al, 2006, 2007; Zhang et al, 2007, 
2008a; Mi et al, 2008; Mi and Huang 2008; 
Zhang et al, 2009). It translates a source sentence 
to its target one in two steps by using structured 
translation rules. In the first step, which is called 
translation rule matching step, all the applicable1 
translation rules are extracted from the entire rule 
set by matching the source parse tree/forest. The 
second step is to decode the source sentence into 
its target one using the extracted translation rules. 
Both of the two steps are very time-consuming 
due to the exponential number of translation rules 
and the complex nature of machine translation as 
                                                           
1 Given a source structure (either a parse tree or a parse 
forest), a translation rule is applicable if and only if the 
left hand side of the translation rule exactly matches a 
tree fragment of the given source structure. 
an NP-hard search problem (Knight, 1999). In the 
SMT research community, the second step has 
been well studied and many methods have been 
proposed to speed up the decoding process, such 
as node-based or span-based beam search with 
different pruning strategies (Liu et al, 2006; 
Zhang et al, 2008a, 2008b) and cube pruning 
(Huang and Chiang, 2007; Mi et al, 2008). How-
ever, the first step attracts less attention. The pre-
vious solution to this problem is to do exhaustive 
searching with heuristics on each tree/forest node 
or on each source span. This solution becomes 
computationally infeasible when it is applied to 
packed forests with loose pruning threshold or rule 
sets with large tree fragments of large rule height 
and width. This not only overloads the translation 
process but also compromises the translation per-
formance since as shown in our experiments the 
large tree fragment rules are also very useful.  
To solve the above issue, in this paper, we pro-
pose a hyper-tree-based fast algorithm for transla-
tion rule matching. Our solution includes two 
steps. In the first step, all the translation rules are 
re-organized using our proposed hyper-tree struc-
ture, which is a compact representation of the en-
tire translation rule set, in order to make the com-
mon parts of translation rules shared as much as 
possible. This enables the common parts of differ-
ent translation rules to be visited only once in rule 
matching. Please note that the first step can be 
easily done off-line very fast. As a result, it does 
not consume real translation time. In the second 
step, we design a recursive algorithm to traverse 
the hyper-tree structure and the input source forest 
in a top-down manner to do the rule matching be-
tween them. As we will show later, the hyper-tree 
structure and the recursive algorithm significantly 
improve the speed of the rule matching and the 
entire translation process compared with previous 
methods. 
With the proposed algorithm, we are able to 
carry out experiments with very loose pruning 
1037
thresholds and larger tree fragment rules effi-
ciently. Experimental results on the NIST MT-
2003 Chinese-English translation task shows that 
our algorithm is 19 times faster in rule matching 
and is able to save 57% of overall translation time 
over previous methods when using large fragment 
translation rules with height up to 5. It also shows 
that the larger rules with height of up to 5 signifi-
cantly outperforms the rules with height of up to 3 
by around 1 BLEU score. 
The rest of this paper is organized as follows. 
Section 2 introduces the syntax-based translation 
system that we are working on. Section 3 reviews 
the previous work. Section 4 explains our solution 
while section 5 reports the experimental results. 
Section 6 concludes the paper. 
2 Syntax-based Translation 
This section briefly introduces the forest/tree-
based tree-to-string translation model which 
serves as the translation platform in this paper. 
2.1 Tree-to-string model 
   
                                                    
 
                                                      
XNA declaration is related to some regulation 
 
Figure 1. A tree-to-string translation process. 
 
The tree-to-string model (Galley et al 2004; Liu et 
al. 2006) views the translation as a structure map-
ping process, which first breaks the source syntax 
tree into many tree fragments and then maps each 
tree fragment into its corresponding target transla-
tion using translation rules, finally combines these 
target translations into a complete sentence. Fig. 1 
illustrates this process. In real translation, the 
number of possible tree fragment segmentations 
for a given input tree is exponential in the number 
of tree nodes.  
2.2 Forest-based translation 
To overcome parse error for SMT, Mi and Huang 
(2008) propose forest-based translation by using a 
packed forest instead of a single syntax tree as the 
translation input. A packed forest (Tomita 1987; 
Klein and Manning, 2001; Huang and Chiang, 
2005) is a compact representation of many possi-
ble parse trees of a sentence, which can be for-
mally described as a triple , where V is 
the set of non-terminal nodes, E is the set of hy-
per-edges and S is a sentence represented as an 
ordered word sequence. A hyper-edge in a packed 
forest is a group of edges in a tree which connects 
a father node to all its children nodes, representing 
a CFG-based parse rule. Fig. 2 is a packed forest 
incorporating two parse trees T1 and T2 of a sen-
tence as shown in Fig. 3 and Fig. 4. Given a hy-
per-edge e, let h be its father node, then we say 
that e is attached to h. 
A non-terminal node in a packed forest can be 
represented as ?label [start, stop]?, where ?label? 
is its syntax category and ?[start, stop]? is the 
range of words it covers. For example, the node in 
Fig. 5 pointed by the dark arrow is labelled as 
?NP[3,4]?, where NP is its label and [3,4] means 
that it covers the span from the 3rd word to the 4th  
word. In forest-based translation, rule matching is 
much more complicated than the tree-based one.  
 
 
 
Figure 2. A packed forest 
 
Zhang et al (2009) reduce the tree sequence 
problem into tree problem by introducing virtual 
node and related forest conversion algorithms, so 
1038
the algorithm proposed in this paper is also appli-
cable to the tree sequence-based models. 
 
     
 
Figure 3. Tree 1 (T1)      Figure 4. Tree 2 (T2) 
3 Matching Methods in Previous Work  
In this section, we discuss the two typical rule 
matching algorithms used in previous work. 
3.1 Exhaustive search by tree fragments 
This method generates all possible tree fragments 
rooted by each node in the source parse tree or 
forest, and then matches all the generated tree 
fragments against the source parts (left hand side) 
of translation rules to extract the useful rules 
(Zhang et al, 2008a).  
 
 
 
Figure 5. Node NP[3,4] in packed forest 
 
 
 
Figure 6. Candidate fragments on NP[3,4] 
For example, if we want to extract useful rules 
for node NP[3,4] in Fig 5, we have to generate all 
the tree fragments rooted at node NP[3,4] as 
shown in Fig 6, and then query each fragment in 
the rule set. Let  be a node in the packed forest, 
 represents the number of possible tree frag-
ments rooted at node , then we have: 
 
 
?
?? ?? ??? 
??? ????????
 ???? ?? ?
? ?? ? ?????????? 
???????? ?? ?
 
 
 
The above equation shows that the number of 
tree fragments is exponential to the span size, the 
height and the number of hyper-edges it covers. In 
a real system, one can use heuristics, e.g. the max-
imum number of nodes and the maximum height 
of fragment, to limit the number of possible frag-
ments. However, these heuristics are very subjec-
tive and hard to optimize. In addition, they may 
filter out some ?good? fragments.  
3.2 Exhaustive search by rules 
This method does not generate any source tree 
fragments. Instead, it does top-down recursive 
matching from each node one-by-one with each 
translation rule in the rule set (Mi and Huang 
2008). 
For example, given a translation rule with its 
left hand side as shown in Fig. 7, the rule match-
ing between the given rule and the node IP[1,4] in 
Fig. 2 can be done as follows.  
1. Decompose the left hand side of the transla-
tion rule as shown in Fig. 7 into a sequence of hy-
per-edges in top-down, left-to-right order as fol-
lows: 
IP => NP VP;  NP => NP NP;  NP => NN; 
NN => ?? 
 
 
 
Figure 7. The left hand side of a rule 
 
2. Pattern match these hyper-edges(rule) one-
by-one in top-down left-to-right order from node 
IP[1,4]. If there is a continuous path in the forest 
matching all of these hyper-edges in order, then 
we can say that the rule is useful and matchable 
1039
with the tree fragment covered by the continuous 
path. The following illustrates the matching steps: 
1. Match hyper-edge ?IP => NP VP? with node 
IP[1,4]. There are two hyper-edges in the forest 
matching it: ?IP[1,4] => NP[1,1] VP[2,4]? and 
?IP[1,4] => NP[1,2] VP [3,4]?, which generates 
two candidate paths. 
2. Since hyper-edge ?NP => NP NP? fails to 
match NP[1,1], the path initiated with ?IP[1,4] => 
NP[1,1] VP[2,4]? is pruned out. 
3.  Since there is a hyper-edge ?NP[1,2] => 
NP[1,1] NP[2,2]? matching ?NP => NP NP? on 
NP[1,2], then continue for further matching. 
4. Since ?NP=>NN? on NP[2,2] matches 
?NP[2,2] => NN[2,2]?, then continue for further 
matching. 
5. ?NN=>??? on NN[2,2] matches ?NN[2,2] 
=>??? and it is the last hyper-edge in the input 
rules. Finally, there is one continuous path suc-
cessfully matching the left hand side of the input 
rule.  
This method is able to avoid the exponential 
problem of the first method as described in the 
previous subsection. However, it has to do one-by-
one pattern matching for each rule on each node. 
When the rule set is very large (indeed it is very 
large in the forest-based model even with a small 
training set), it becomes very slow, and even much 
slower than the first method. 
4 The Proposed Hyper-tree-based Rule 
Matching Algorithm 
In this section, we first explain the motivation why 
we re-organize the translation rule sets, and then 
elaborate how to re-organize the translation rules 
using our proposed hyper-tree structure. Finally 
we discuss the top-down rule matching algorithm 
between forest and hyper-tree.  
4.1 Motivation 
 
 
              Figure 8.  Two rules? left hand side 
 
 
Figure 9. Common part of the two rules? left hand  
sides in Figure 8 
 
Fig. 9 shows the common part of the left hand 
sides of two translation rules as shown in Fig. 8. 
In previous rule matching algorithm, the common 
parts are matched as many times as they appear in 
the rule set, which reduces the rule matching 
speed significantly. This motivates us to propose 
the hyper-tree structure and the rule matching al-
gorithm to make the common parts shared by mul-
tiple translation rules to be visited only once in the 
entire rule matching process. 
4.2 Hyper-node, hyper-path and hyper-tree 
A hyper-tree is a compact representation of a 
group of tree translation rules with common parts 
shared. It consists of a set of hyper-nodes with 
edges connecting different hyper-nodes into a big 
tree. A hyper-tree is constructed from the transla-
tion rule sets in two steps: 
1) Convert each tree translation rule into a hy-
per-path; 
2) Construct the hyper-tree by incrementally 
adding each individual hyper-path into the 
hyper-tree. 
A tree rule can be converted into a hyper-path 
without losing information. Fig. 10 demonstrates 
the conversion process:  
1) We first fill the rule tree with virtual nodes  
to make all its leaves have the same depth 
to the root; 
2) We then group all the nodes in the same 
tree level to form a single hyper-node, 
where we use a comma as a delimiter to 
separate the tree nodes with different father 
nodes; 
3) A hyper-path is a set of hyper-nodes linked 
in a top-down manner. 
The commas and virtual nodes  are introduced 
to help to recover the original tree from the hyper-
path. Given a tree node in a hyper-node, if there 
are n commas before it, then its father node is the 
(n+1)th tree node in the father hyper-node. If we 
could find father node for each node in hyper-
nodes, then it is straightforward to recover the 
original tree from the hyper-path by just adding 
the edges between original father and children 
nodes except the virtual node .  
1040
After converting each tree rule into a hyper-
path, we can organize the entire rule set into a big 
hyper-tree as shown in Figure 11. The concept of 
hyper-path and hyper-tree could be viewed as an 
extension of the "prefix merging" ideas for CFG 
rules (Klein and Manning 2001). 
 
         
 
 
 
 
Figure 10. Convert tree to hyper-path 
 
 
 
Figure 11. A hyper-tree example 
 
Algorithm 1 shows how to organize the rule set 
into a big hyper-tree. The general process is that 
for each rule we convert it into a hyper-path and 
then add the hyper-path into a hyper-tree incre-
mentally. However, there are many different hy-
per-trees generated given a big rule set. We then 
introduce a TOP label as the root node to link all 
the individual hyper-trees to a single big hyper-
tree. Algorithm 2 shows the process of adding a 
hyper-path into a hyper-tree. Given a hyper-path, 
we do a top-down matching between the hyper-
tree and the input hyper-path from root hyper-
node until a leaf hyper-node is reached or there is 
no matching hyper-node at some level found. 
Then we add the remaining unmatchable part of 
the input hyper-path as the descendants of the last 
matchable hyper-node. 
Please note that in Fig. 10 and Fig. 11, we ig-
nore the target side (right hand side) of translation 
rules for easy discussion. Indeed, we can easily 
represent all the complete translation rules (not 
only left hand side) in Fig. 11 by simply adding 
the corresponding rule target sides into each hy-
per-node as done by line 5 of Algorithm 1.  
Any hyper-path from the root to any hyper-
node (not necessarily be a leaf of the hyper-tree) 
in a hyper-tree can represent a tree fragment. As a 
result, the hyper-tree in Fig. 11 can represent up to 
6 candidate tree fragments. It is easy to understand 
that the maximum number of tree fragments that a 
hyper-tree can represent is equal to the number of 
hyper-nodes in it except the root. It is worth not-
ing that a hyper-node in a hyper-tree without any 
target side rule attached means there is no transla-
tion rule corresponding to the tree fragment repre-
sented by the hyper-path from the root to the cur-
rent hyper-node. The compact representation of 
the rule set by hyper-tree enables a fast algorithm 
to do translation rule matching. 
 
Algorithm 1. Compile rule set into hyper-tree 
Input: rule set 
Output: hyper-tree 
 
1.  Initialize hyper-tree as a TOP node  
2.  for  each rule in rule set  do 
3.          Convert the left hand side tree to a hyper-path p 
4.          Add hyper-path p into hyper-tree 
5. Add rule?s right hand side to the leaf hyper-node of  
a hyper-path in the hyper-tree  
6. end for 
 
Algorithm  2. Add hyper-path into hyper-tree 
Input: hyper-path p and hyper-tree t 
Notation:  
   h: the height of hyper-path p 
   p(i) : the hyper-node of ith level (top-down) of p 
   TN: the hyper-node in hyper-tree  
Output: updated hyper-tree t  
 
1. Initialize TN as TOP 
2. for  i := 1 to h  do 
3.       if there is a child c of TN has the same label as p(i)    
              then 
4.             TN := c 
5.       else  
6.             Add a child c to TN, label c as p(i) 
7.             TN := c 
4.3 Translation rule matching between forest 
and hyper-tree 
Given the source parse forest and the translation 
rules represented in the hyper-tree structure, here 
we present a fast matching algorithm to extract so-
called useful translation rules from the entire rule 
set in a top-down manner for each node of the for-
est.  
As shown in Algorithm 3, the general process 
of the matching algorithm is as follows: 
 
1041
Algorithm 3. Rule matching on one node  
Input: hyper-tree T, forest F, and node n 
Notation:   
      FP: a pair <FNS, TN>, FNS is the frontier nodes of      
             matched tree fragment,  
             TN is the hyper-tree node matching it 
      SFP: the queue of FP 
Output: Available rules on node n 
 
1. if there is no child c of TOP having the same label as n      
   then 
2.        Return failure. 
3. else  
4.      Initialize FP as <{n},c> and put it into SFP 
5.      for each FP in SFP do 
6.                 SFP ? PropagateNextLevel(FP.FNS, FP.TN)  
7.      for each FP in SFP do 
8.          if the rule set attached to FP.TN is not empty   
         then 
9.               Add FP to result 
 
Algorithm 4. PropagateNextLevel  
Input: Frontier node sequence FNS, hyper-tree node TN 
Notation: 
           CT: a child node of TN 
                  the number of node sequence (separated by  
                  comma, see Fig 11) in CT is equal to the number  
                  of node in TN.   
           CT(i) : the ith node sequence in hyper-node CT 
           FNS(i): the ith node in FNS 
           TFNS: the temporary set of frontier node sequence 
           RFNS: the result set of frontier node sequence  
           FP:  a pair of frontier node sequence  
                   and hyper-tree node 
           RFP: the result set of FP 
Output: RFP  
 
1. for each child hyper-node CT of TN do 
2.        for i:= 1 to the number of node sequence in CT do 
3.              empty TFNS 
4.              if CT(i) ==  then 
5.                      Add FNS(i) to TFNS. 
6.              else 
7.                   for each hyper-edge e attached to FNS(i) do 
8.                         if e.children match CT(i) then 
9.                                Add e.children to TFNS 
10.              if TFNS is empty then 
11.                      empty RFNS 
12.                      break 
13.              else if i == 1 then  
14.                       RFNS := TFNS 
15.              else  
16.                       RFNS := RFNS  TFNS 
17.        for each FNS in RFNS do 
18.                add <FNS, CT > into RFP 
 
1) For each node n of the source forest if no 
child node of TOP in hyper-tree has the same label 
with it, it means that no rule matches any tree 
fragments rooted at the node n (i.e., no useful 
rules to be used for the node n) (line 1-2) 
2) Otherwise, we match the sub-forest starting 
from the node n against a sub-hyper-tree starting 
from the matchable child node of TOP layer by 
layer in a top-down manner. There may be many 
possible tree fragments rooted at node n and each 
of them may have multiple useful translation rules. 
In our implementation, we maintain a data struc-
ture of FP = <FNS, TN> to record the currently 
matched tree fragment of forest and its corres-
ponding hyper-tree node in the rule set, where 
FNS is the frontier node set of the current tree 
fragment and TN is the hyper-tree node. The data 
structure FP is used to help extract useful transla-
tion rules and is also used for further matching of 
larger tree fragments. Finally, all the FPs for the 
node n are kept in a queue. During the search, the 
queue size is dynamically increased. The matching 
algorithm terminates when all the FPs have been 
visited (line 5-6 and Algorithm 4). 
3) In the final queue, each element (FP) of the 
queue contains the frontier node sequence of the 
matched tree fragment and its corresponding hy-
per-tree node. If the target side of a rule in the hy-
per-tree node is not empty, we just output the 
frontier nodes of the matched tree fragment, its 
root node n and all the useful translation rules for 
later translation process. 
Algorithm 4 describes the detailed process of 
how to propagate the matching process down to 
the next level.  <FNS, TN> is the current level 
frontier node sequence and hyper-tree node. Given 
a child hyper-node CT of TN (line 1), we try to 
find the group of next level frontier node sequence 
to match it (line 2-18). As shown in Fig 11, a hy-
per-node consists of a sequence of node sequence 
with comma as delimiter. For the ith node se-
quence CT(i) in CT, If CT(i) is , that means 
FNS(i) is a leaf/frontier node in the matched tree 
fragment and thus no need to propagate to the next 
level (line 4-5). Otherwise, we try each hyper-
edge e of FNS(i) to see whether its children match 
CT(i), and put the children of the matched hyper-
edge into a temp set TFNS (line 7-9). If the temp 
set is empty, that means the current matching fails 
and no further expansion needs (line 10-12). Oth-
erwise, we integrate current matched children into 
the final group of frontier node sequence (line 13-
16) by Descartes Product ( ). Finally, we con-
struct all the <FNS, TN> pair for next level 
matching (line 17-18). 
It would be interesting to study the time com-
plexity of our Algorithm 3 and 4. Suppose the 
maximum number of children of each hyper-node 
in hyper-tree is N (line 1), the maximum number 
of node sequence in CT is M (line 2), the maxi-
mum number of hyper-edge in each node in 
packed forest is K (line 7), the maximum number 
of hyper-edge with same children representation 
in each node in packed forest is C (i.e. the maxi-
mum size of TFNS in line 16, and the maximum 
complexity of the Descartes Product in line 16 
1042
would be CM), then the time complexity upper-
bound of Algorithm 4 is O(NM(K+CM)). For Al-
gorithm 3, its time complexity is O(RNM(K+CM)), 
where R is the maximum number of tree fragment 
matched in each node.  
5 Experiment 
5.1 Experimental settings 
We carry out experiment on Chinese-English 
NIST evaluation tasks. We use FBIS corpus 
(250K sentence pairs) as training data with the 
source side parsed by a modified Charniak parser 
(Charniak 2000) which can output a packed forest. 
The Charniak Parser is trained on CTB5, tuned on 
301-325 portion, with F1 score of 80.85% on 271-
300 portion. We use GIZA++ (Och and Ney, 2003) 
to do m-to-n word-alignment and adopt heuristic 
?grow-diag-final-and? to do refinement. A 4-gram 
language model is trained on Gigaword 3 Xinhua 
portion by SRILM toolkit (Stolcke, 2002) with 
Kneser-Ney smoothing. We use NIST 2002 as 
development set and NIST 2003 as test set. The 
feature weights are tuned by the modified Koehn?s 
MER (Och, 2003, Koehn, 2007) trainer. We use 
case-sensitive BLEU-4 (Papineni et al, 2002) to 
measure the quality of translation result. Zhang et 
al. 2004?s implementation is used to do significant 
test. 
Following (Mi and Huang 2008), we use viterbi 
algorithm to prune the forest. Instead of using a 
static pruning threshold (Mi and Huang 2008), we 
set the threshold as the distance of the probabili-
ties of the nth best tree and the 1st best tree. It 
means the pruned forest is able to at least keep all 
the top n best trees. However, because of the shar-
ing nature of the packed forest, it may still contain 
a large number of additional trees. Our statistic 
shows that when we set the threshold as the 100th 
best tree, the average number of all possible trees 
in the forest is 1.2*105 after pruning. 
In our experiments, we compare our algorithm 
with the two traditional algorithms as discussed in 
section 3. For the ?Exhaustive search by tree? al-
gorithm, we use a bottom-up dynamic program-
ming algorithm to generate all the candidate tree 
fragments rooted at each node. For the ?Exhaus-
tive search by rule? algorithm, we group all rules 
with the same left hand side in order to remove the 
duplicated matching for the same left hand side 
rules. All these settings aim for fair comparison. 
5.2 Accuracy, speed vs. rule heights 
We first compare the three algorithms? perfor-
mance by setting the maximum rule height from 1 
to 5. We set the forest pruning threshold to the 
100th best parse tree.  
Table 1 compares the speed of the three algo-
rithms. It clearly shows that the speed of both of 
the two traditional algorithms increases dramati-
cally while the speed of our hyper-tree based algo-
rithm is almost linear to the tree height. In the case 
of rule height of 5, the hyper-tree algorithm is at 
least 19 times (9.329/0.486) faster than the two 
traditional algorithms and saves 8.843(9.329 - 
0.486) seconds in rule matching for each sentence 
on average, which contributes 57% (8.843/(9.329 
+ 6.21)) speed improvement to the overall transla-
tion.  
 
H 
Rule Matching 
D Exhaus-
tive 
by tree 
Exhaus-
tive 
by rule 
Hyper- 
tree-
based 
1 0.043 0.077 0.083   2.96 
2 0.047 0.920 0.173   3.56 
3 0.237 9.572 0.358   4.02 
4 2.300 48.90 0.450   5.27 
5 9.329 90.80 0.486   6.21 
 
Table 1. Speed in seconds per sentence vs. rule 
height; ?H? is rule height, ?D? represents the de-
coding time after rule matching 
 
 
Height BLEU 
1 0.1646 
2 0.2498 
3 0.2824 
4 0.2874 
5 0.2925 
Moses 0.2625 
 
Table 2. BLEU vs. rule height 
 
Table 2 reports the BLEU score with different 
rule heights, where Moses, a state-of-the-art 
phrase-based SMT system, serves as the baseline 
system.  It shows the BLEU score consistently 
improves as the rule height increases. In addition, 
one can see that the rules with maximum height of 
5 are able to outperform the rules with maximum 
height of 3 by 1 BLEU score (p<0.05) and signifi-
cantly outperforms Moses by 3 BLEU score 
(p<0.01). To our knowledge, this is the first time 
to report the performance of rules up to height of 5 
for forest-based translation model.  
1043
We also study the distribution of the rules used 
in the 1-best translation output. The results are 
shown in Table 3; we could see something inter-
esting that is as the rule height increases, the total 
number of rules with that height decreases, while 
the percentage of partial-lexicalized increases 
dramatically. And one thing needs to note is the 
percentage of partial-lexicalized rules with height 
of 1 is 0, since there is no partial-lexicalized rule 
with height of 1 in the rule set (the father node of 
a word is a pos tag node).  
 
H Total 
Rule Type Percentage (%) 
F P U 
1 9814   76.58     0 23.42 
2 5289   44.99     46.40 8.60 
3 3925   18.39     77.25 4.35 
4 1810   7.90      87.68 4.41 
5 511    6.46 90.50 3.04 
 
Table 3. statistics of rules used in the 1-best trans-
lation output, ?F? means full-lexicalized, ?P? 
means partial-lexicalized, ?U? means unlexiclaizd. 
5.3 Speed vs. forest pruning threshold 
This section studies the impact of the forest prun-
ing threshold on the rule matching speed when 
setting the maximum rule height to 5. 
 
Threshold 
Rule Matching  
Exhaus-
tive 
by tree 
Exhaus-
tive 
by rule 
Hyper- 
tree- 
based 
1 1.2 23.66 0.171 
10 3.1 36.42 0.234 
50 5.7 66.20 0.405 
100 9.3 90.80 0.486 
200 27.3 104.86 0.598 
500 133.6 148.54 0.873 
 
Table 4. Speed in seconds per sentence vs. for-
est  pruning threshold 
 
In Table 4, we can see that our hyper-tree based 
algorithm is the fastest among the three algorithms 
in all pruning threshold settings and even 150 
times faster than both of the two traditional algo-
rithms with threshold of 500th best. Table 5 shows 
the average number of parse trees embedded in a 
packed forest with different pruning thresholds per 
sentence. We can see that the number of trees in-
creases exponentially when the pruning threshold 
increases linearly. When the threshold is 500th best, 
the average number of trees per sentence is 
1.49*109. However, even in this extreme case, the 
hyper-tree based algorithm is still capable of com-
pleting rule matching within 1 second.  
 
Threshold Number of Trees  
1 1 
10 32 
50 5922 
100 128860 
200 2.75*106 
500 1.49*109 
 
Table 5. Average number of trees in packed 
forest with different pruning threshold. 
5.4 Hyper-tree compression rate 
As we describe in section 4.2, theoretically the 
number of tree fragments that a hyper-tree can 
represent is equal to the number of hyper-nodes in 
it. However, in real rule set, there is no guarantee 
that each tree fragment in the hyper-tree has cor-
responding translation rules. To gain insights into 
how effective the compact representation of the 
hyper-tree and how many hyper-nodes without 
translation rules, we define the compression rate 
as follows.  
 
 
 
 
Table 6 reports the different statistics on the 
rule sets with different maximum rule heights 
ranging from 1 to 5. The reported statistics are the 
number of rules, the number of unique left hand 
side (since there may be more than one rules hav-
ing the same left hand side), the number of hyper-
nodes and the compression rate.  
 
H n_rules n_LHS n_nodes c_rate 
1 21588 10779 10779 100% 
2 141632 51807 51903 99.8% 
3 1.73*106 491268 494919 99.2% 
4 8.65*106 2052731 2083296 98.5% 
5 1.89*107 3966742 4043824 98.1% 
 
Table 6. Statistics of rule set and hyper-tree. ?H? 
is rule height, ?n_rules? is the number of rules, 
?n_LHS? is the number of unique left hand side, 
?n_nodes? is the number of hyper-nodes in hyper-
tree and ?c_rate? is the compression rate. 
 
Table 6 shows that in all the five cases, the 
compression rates of hyper-tree are all more than 
1044
98%. It means that almost all the tree fragments 
embedded in the hyper-tree have corresponding 
translation rules. As a result, we are able to use 
almost only one hyper-edge (i.e. only the frontier 
nodes of a tree fragment without any internal 
nodes) to represent all the rules with the same left 
hand side. This suggests that our hyper-tree is par-
ticularly effective in representing the tree transla-
tion rules compactly. It also shows that there are a 
lot of common parts among different translation 
rules. 
All the experiments reported in this section 
convincingly demonstrate the effectiveness of our 
proposed hyper-tree representation of translation 
rules and the hyper-tree-based rule matching algo-
rithm. 
6 Conclusion   
In this paper2, we propose the concept of hyper-
tree for compact rule representation and a hyper-
tree-based fast algorithm for translation rule 
matching in a forest-based translation system. We 
compare our algorithm with two previous widely-
used rule matching algorithms.  Experimental re-
sults on the NIST Chinese-English MT 2003 eval-
uation data set show the rules with maximum rule 
height of 5 outperform those with height 3 by 1.0 
BLEU and outperform MOSES by 3.0 BLEU. In 
the same test cases, our algorithm is at least 19 
times faster than the two traditional algorithms, 
and contributes 57% speed improvement to the 
overall translation. We also show that in a more 
challenging setting (forest containing 1.49*109 
trees on average) our algorithm is 150 times faster 
than the two traditional algorithms. Finally, we 
show that the hyper-tree structure has more than 
98% compression rate. It means the compact re-
presentation by the hyper-tree is very effective for 
translation rules. 
References  
Eugene Charniak. 2000. A maximum-entropy inspired 
parser. NAACL-00. 
Michel Galley, Mark Hopkins, Kevin Knight and Da-
niel Marcu. 2004. What?s in a translation rule? 
HLT-NAACL-04.  
Liang Huang and David Chiang. 2005. Better k-best 
Parsing. IWPT-05. 
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. ACL-07. 144?151 
                                                           
The corresponding authors of this paper are Hui Zhang 
(zhangh1982@gmail.com) and Min Zhang 
(mzhang@i2r.a-star.edu.sg) 
Dan Klein and Christopher D. Manning. 2001. Parsing 
and Hypergraphs. IWPT-2001. 
Dan Klein and Christopher D. Manning. 2001. Parsing 
with Treebank Grammars: Empirical Bounds, Theo-
retical Models, and the Structure of the Penn Tree-
bank. ACL - 2001. 338-345. 
Kevin Knight. 1999. Decoding Complexity in Word-
Replacement Translation Models. CL: J99-4005. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran, Ri-
chard Zens, Chris Dyer, Ondrej Bojar, Alexandra 
Constantin and Evan Herbst. 2007. Moses: Open 
Source Toolkit for Statistical Machine Translation. 
ACL-07. 177-180. (poster) 
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-
String Alignment Template for Statistical Machine 
Translation. COLING-ACL-06. 609-616. 
Yang Liu, Yun Huang, Qun Liu and Shouxun Lin. 
2007. Forest-to-String Statistical Translation Rules. 
ACL-07. 704-711. 
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. ACL-HLT-08. 192-199. 
Haitao Mi and Liang Huang. 2008. Forest-based 
Translation Rule Extraction. EMNLP-08. 206-214 
Franz J. Och. 2003. Minimum error rate training in 
statistical machine translation. ACL-03. 160-167. 
Franz Josef Och and Hermann Ney. 2003. A Systematic 
Comparison of Various Statistical Alignment Mod-
els. Computational Linguistics. 29(1) 19-51  
Kishore Papineni, Salim Roukos, ToddWard and Wei-
Jing Zhu. 2002. BLEU: a method for automatic 
evaluation of machine translation. ACL-02.311-318. 
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. ICSLP-02. 901-904. 
Masaru Tomita. 1987. An Efficient Augmented-
Context-Free Parsing Algorithm. Computational 
Linguistics 13(1-2): 31-46. 
Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw and Chew 
Lim Tan. 2009. Forest-based Tree Sequence to 
String Translation Model. ACL-IJCNLP-09. 
Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Sheng 
Li and Chew Lim Tan. 2007. A Tree-to-Tree Align-
ment-based Model for Statistical Machine Transla-
tion. MT-Summit-07. 535-542. 
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew 
Lim Tan, Sheng Li. 2008a. A Tree Sequence Align-
ment-based Tree-to-Tree Translation Model. ACL-
HLT-08. 559-567. 
Min Zhang, Hongfei Jiang, Haizhou Li, Aiti Aw, Sheng 
Li. 2008b. Grammar Comparison Study for Transla-
tional Equivalence Modeling and Statistical Ma-
chine Translation. COLING-08. 1097-1104. 
Ying Zhang, Stephan Vogel, Alex Waibel. 2004. Inter-
preting BLEU/NIST scores: How much improvement 
do we need to have a better system? LREC-04 
1045
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1552?1560,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
K-Best Combination of Syntactic Parsers  
 
Hui Zhang1, 2   Min Zhang1   Chew Lim Tan2   Haizhou Li1   
1Institute for Infocomm Research                 2National University of Singapore                    
zhangh1982@gmail.com   {mzhang, hli}@i2r.a-star.edu.sg   tancl@comp.nus.edu.sg 
 
 
 
Abstract 
In this paper, we propose a linear model-based 
general framework to combine k-best parse 
outputs from multiple parsers. The proposed 
framework leverages on the strengths of pre-
vious system combination and re-ranking 
techniques in parsing by integrating them into 
a linear model. As a result, it is able to fully 
utilize both the logarithm of the probability of 
each k-best parse tree from each individual 
parser and any additional useful features. For 
feature weight tuning, we compare the simu-
lated-annealing algorithm and the perceptron 
algorithm. Our experiments are carried out on 
both the Chinese and English Penn Treebank 
syntactic parsing task by combining two state-
of-the-art parsing models, a head-driven lexi-
calized model and a latent-annotation-based 
un-lexicalized model. Experimental results 
show that our F-Scores of 85.45 on Chinese 
and 92.62 on English outperform the previ-
ously best-reported systems by 1.21 and 0.52, 
respectively. 
1 Introduction 
Statistical models have achieved great success in 
language parsing and obtained the state-of-the-
art results in a variety of languages. In general, 
they can be divided into two major categories, 
namely lexicalized models (Collins 1997, 1999; 
Charniak 1997, 2000) and un-lexicalized models 
(Klein and Manning 2003; Matsuzaki et al 2005; 
Petrov et al 2006; Petrov and Klein 2007). In 
lexicalized models, word information play a key 
role in modeling grammar rule generation, while 
un-lexicalized models usually utilize latent in-
formation derived from the parse structure diver-
sity. Although the two models are different from 
each other in essence, both have achieved state-
of-the-art results in a variety of languages and 
are complementary to each other (this will be 
empirically verified later in this paper). There-
fore, it is natural to combine the two models for 
better parsing performance.  
Besides individual parsing models, many sys-
tem combination methods for parsing have been 
proposed (Henderson and Brill 1999; Zeman and 
?abokrtsk? 2005; Sagae and Lavie 2006) and 
promising performance improvements have been 
reported. In addition, parsing re-ranking (Collins 
2000; Riezler et al 2002; Charniak and Johnson 
2005; Huang 2008) has also been shown to be 
another effective technique to improve parsing 
performance. This technique utilizes a bunch of 
linguistic features to re-rank the k-best (Huang 
and Chiang 2005) output on the forest level or 
tree level. In prior work, system combination 
was applied on multiple parsers while re-ranking 
was applied on the k-best outputs of individual 
parsers. 
In this paper, we propose a linear model-based 
general framework for multiple parsers combina-
tion. The proposed framework leverages on the 
strengths of previous system combination and re-
ranking methods and is open to any type of fea-
tures. In particular, it is capable of utilizing the 
logarithm of the parse tree probability from each 
individual parser while previous combination 
methods are unable to use this feature since the 
probabilities from different parsers are not com-
parable. In addition, we experiment on k-best 
combination while previous methods are only 
verified on 1-best combination. Finally, we apply 
our method in combining outputs from both the 
lexicalized and un-lexicalized parsers while pre-
vious methods only carry out experiments on 
multiple lexicalized parsers. We also compare 
two learning algorithms in tuning the feature 
weights for the linear model. 
We perform extensive experiments on the 
Chinese and English Penn Treebank corpus. Ex-
perimental results show that our final results, an 
F-Score of 92.62 on English and 85.45 on Chi-
nese, outperform the previously best-reported 
systems by 0.52 point and 1.21 point, respec-
tively. This convincingly demonstrates the effec-
tiveness of our proposed framework. Our study 
also shows that the simulated-annealing algo-
rithm (Kirkpatrick et al 1983) is more effective 
1552
than the perceptron algorithm (Collins 2002) for 
feature weight tuning. 
The rest of this paper is organized as follows. 
Section 2 briefly reviews related work. Section 3 
discusses our method while section 4 presents 
the feature weight tuning algorithm. In Section 5, 
we report our experimental results and then con-
clude in Section 6. 
2 Related Work  
As discussed in the previous section, system 
combination and re-ranking are two techniques 
to improve parsing performance by post-
processing parsers? k-best outputs.  
Regarding the system combination study, 
Henderson and Brill (1999) propose two parser 
combination schemes, one that selects an entire 
tree from one of the parsers, and one that builds a 
new tree by selecting constituents suggested by 
the initial trees. According to the second scheme, 
it breaks each parse tree into constituents, calcu-
lates the count of each constituent, then applies 
the majority voting to decide which constituent 
would appear in the final tree. Sagae and Lavie 
(2006) improve this second scheme by introduc-
ing a threshold for the constituent count, and 
search for the tree with the largest number of 
count from all the possible constituent combina-
tion. Zeman and ?abokrtsk? (2005) study four 
combination techniques, including voting, stack-
ing, unbalanced combining and switching, for 
constituent selection on Czech dependency pars-
ing. Promising results have been reported in all 
the above three prior work. Henderson and Brill 
(1999) combine three parsers and obtained an F1 
score of 90.6, which is better than the score of 
88.6 obtained by the best individual parser as 
reported in their paper. Sagae and Lavie (2006) 
combine 5 parsers to obtain a score of 92.1, 
while they report a score of 91.0 for the best sin-
gle parser in their paper. Finally, Zeman and 
?abokrtsk? (2005) reports great improvements 
over each individual parsers and show that a 
parser with very low accuracy can also help to 
improve the performance of a highly accurate 
parser. However, there are two major limitations 
in these prior works. First, only one-best output 
from each individual parsers are utilized. Second, 
none of these works uses the parse probability of 
each parse tree output from the individual parser.  
Regarding the parser re-ranking, Collins (2000) 
proposes a dozen of feature types to re-rank k-
best outputs of a single head-driven parser. He 
uses these feature types to extract around half a 
million different features on the training set, and 
then examine two loss functions, MRF and 
Boosting, to do feature selection. Charniak and 
Johnson (2005) generate a more accurate k-best 
output and adopt MaxEnt method to estimate the 
feature weights for more than one million fea-
tures extracted from the training set. Huang 
(2008) further improves the re-ranking work of 
Charniak and Johnson (2005) by re-ranking on 
packed forest, which could potentially incorpo-
rate exponential number of k-best list. The re-
ranking techniques also achieve great improve-
ment over the original individual parser. Collins 
(2002) improves the F1 score from 88.2% to 
89.7%, while Charniak and Johnson (2005) im-
prove from 90.3% to 91.4%. This latter work 
was then further improved by Huang (2008) to 
91.7%, by utilizing the benefit of forest structure. 
However, one of the limitations of these tech-
niques is the huge number of features which 
makes the training very expensive and inefficient 
in space and memory usage.  
3 K-best Combination of Lexicalized 
and Un-Lexicalized Parsers with 
Model Probabilities 
In this section, we first introduce our proposed k-
best combination framework. Then we apply this 
framework to the combination of two state-of-
the-art lexicalized and un-lexicalized parsers 
with an additional feature inspired by traditional 
combination techniques. 
3.1 K-best Combination Framework 
Our proposed framework consists of the follow-
ing steps: 
1) Given an input sentence and N different 
parsers, each parser generates K-best parse 
trees. 
2) We combine the N*K output trees and 
remove any duplicates to obtain M unique 
tress. 
3) For each of the M unique trees, we re-
evaluate it with all the N models which are 
used by the N parsers. It is worth noting 
that this is the key point (i.e. one of the 
major advantages) of our method since 
some parse trees are only generated from 
one or I (I<N) parsers. For example, if a 
tree is only generated from head-driven 
lexicalized model, then it only has the 
head-driven model score. Now we re-
evaluate it with the latent-annotation un-
lexicalized model to reflect the latent-
1553
annotation model?s confidence for this 
tree. This enables our method to effec-
tively utilize the confidence measure of all 
the individual models without any bias. 
Without this re-evaluation step, the previ-
ous combination methods are unable to 
utilize the various model scores. 
4) Besides model scores, we also compute 
some additional feature scores for each 
tree, such as the widely-used ?constituent 
count? feature. 
5) Then we adopt the linear model to balance 
and combine these feature scores and gen-
erate an overall score for each parse tree.  
6) Finally we re-rank the M best trees and 
output the one with the highest score. 
 
 
? ? ? ? ? ?
? ?  
 
The above is the linear function used in our 
method, where t is the tree to be evaluated,  to 
 are the model confidence scores (in this paper, 
we use logarithm of the parse tree probability) 
from the N models,  to  are their weights, 
?  to ?  are the L additional features, ?  to ?  
are their weights.  
In this paper, we employ two individual pars-
ing model scores and only one additional feature. 
Let  be the head-driven model score,  be the 
latent-annotation model score, ?  be the consti-
tuent count feature and ?  is the weight of fea-
ture ? .  
3.2 Confidences of Lexicalized and Un-
lexicalized Model 
The term ?confidence? was used in prior parser 
combination studies to refer to the accuracy of 
each individual parser. This reflects how much 
we can trust the parse output of each parser. In 
this paper, we use the term ?confidence? to refer 
to the logarithm of the tree probability computed 
by each model, which is a direct measurement of 
the model?s confidence on the target tree being 
the best or correct parse output. In fact, the fea-
ture weight ? in our linear model functions simi-
larly as the traditional ?confidence?. However, 
we do not directly use parser?s accuracy as its 
value. Instead we tune it automatically on devel-
opment set to optimize it against the parsing per-
formance directly. In the following, we introduce 
the state-of-the-art head-driven lexicalized and 
latent-annotation un-lexicalized models (which 
are used as two individual models in this paper), 
and describe how they compute the tree probabil-
ity briefly. 
Head-driven model is one of the most repre-
sentative lexicalized models. It attaches the head 
word to each non-terminal and views the genera-
tion of each rule as a Markov process first from 
father to head child, and then to the head child?s 
left and right siblings. 
Take following rule r as example,  
 
 
 
 is the rule?s left hand side (i.e. father label), 
 is the head child,  is M?s left sibling and  
is M?s right sibling. Let h be M?s head word, the 
probability of this rule is 
 
 
 
The probability of a tree is just the product of the 
probabilities of all the rules in it. The above is 
the general framework of head-driven model. For 
a specific model, there may be some additional 
features and modification. For example, the 
model2 in Collins (1999) introduces sub-
categorization and model3 introduces gap as ad-
ditional features. Charniak (2000)?s model intro-
duces pre-terminal as additional features. 
The latent-annotation model (Matsuzaki et al 
2005; Petrov et al 2006) is one of the most ef-
fective un-lexicalized models. Briefly speaking, 
latent-annotation model views each non-terminal 
in the Treebank as a non-terminal followed by a 
set of latent variables, and uses EM algorithms to 
automatically learn the latent variables? probabil-
ity functions to maximize the probability of the 
given training data. Take the following binarized 
rule as example, 
 
 
 
could be viewed as the set of rules  
 
 
 
The process of computing the probability of a 
normal tree is to first binarized all the rules in it, 
and then replace each rule to the corresponding 
set of rules with latent variables. Now the pre-
vious tree becomes a packed forest (Klein and 
Manning 2001; Petrov et al 2007) in the latent-
annotation model, and its probability is the inside 
probability of the root node. This model is quite 
different from the head-driven model in which 
1554
the probability of a tree is just the product all the 
rules? probability. 
3.3 Constituent Counts 
Besides the two model scores, we also adopt 
constituent count as an additional feature in-
spired by (Henderson and Brill 1999) and (Sagae 
and Lavie 2006). A constituent is a non-terminal 
node covering a special span. For example, 
?NP[2,4]? means a constituent labelled as ?NP? 
which covers the span from the second word to 
the fourth word. If we have 100 trees and NP[2,4] 
appears in 60 of them, then its constituent count 
is 60. For each tree, its constituent count is the 
sum of all the counts of its constituent. However, 
as suggested in (Sagae and Lavie 2006), this fea-
ture favours precision over recall. To solve this 
issue, Sagae and Lavie (2006) use a threshold to 
balance them. For any constituent, we calculate 
its count if and only if it appears more than X 
times in the k-best trees; otherwise we set it as 0. 
In this paper, we normalize this feature by divid-
ing the constituent count by the number of k-best. 
Note that the threshold value and the additional 
feature value are not independent. Once the 
threshold changes, the feature value has to be re-
calculated. 
In conclusion, we have four parameters to es-
timate: two model score weights, one additional 
feature weight and a threshold for the additional 
feature.  
4 Parameter Estimation  
We adopt the minimum error rate principle to 
tune the feature weights by minimizing the error 
rate (i.e. maximizing the F1 score) on the devel-
opment set. In our study, we implement and 
compare two algorithms, the simulated-annealing 
style algorithm and the average perceptron algo-
rithm. 
4.1 Simulated Annealing 
Simulated-annealing algorithm has been proved 
to be a powerful and efficient algorithm in solv-
ing NP problem (?ern? 1985). Fig 1 is the pseu-
do code of the simulated-annealing algorithm 
that we apply.   
In a single iteration (line 4-11), the simulated 
algorithm selects some random points (the Mar-
kov link) for hill climbing. However, it accepts 
some bad points with a threshold probability 
controlled by the annealing temperature (line 7-
10). The hill climbing nature gives this algorithm 
the ability of converging at local maximal point 
and the random nature offers it the chance to 
jump from some local maximal points to global 
maximal point. We do a slight modification to 
save the best parameter so far across all the fi-
nished iterations and let it be the initial point for 
upcoming iterations (line 12-17). 
RandomNeighbour(p) is the function to gener-
ate a random neighbor for the p (the four-tuple 
parameter to be estimated). F1(p) is the function 
to calculate the F1 score over the entire test set. 
Given a fixed parameter p, it selects the candi-
date tree with best score for each sentence and 
computes the F1 score with the PARSEVAL me-
trics. 
 
Pseudo code 1. Simulated-annealing algorithm 
Input: k-best trees combined from two model output 
Notation:  
   p: the current parameter value 
   F1(p): the F1 score with the parameter value p 
   TMF: the max F1 score of each iteration 
   TMp: the optimal parameter value during iteration 
   MaxF1: the max F1 score on dev set 
   Rp: the parameter value which maximizes the F1 score 
of the dev set 
   T: annealing temperature 
   L: length of Markov link 
Output: Rp 
 
1. MaxF1:= 0, Rp:= (0,0,0,0), T:=1, L=100 // initialize 
2. Repeat                                                       // iteration 
3.      TMp :=Rp 
4.      for  i := 1 to L  do 
5.            p := RandomNeighbour(TMp) 
6.            d= F1(p)- TMF 
7.            if d>0 or exp(d/T) > random[0,1) then  
8.                  TMF:=F1(p) 
9.                  TMp:=p 
10.            end if 
11.      end for 
12.      if TMF > MaxF1 then 
13.            MaxF:=TMF 
14.            Rp:=TMp 
15.      else  
16.            TMp:=Rp 
17.      end if 
18.      T=T*0.9 
19. Until convergence 
 
Fig 1. Simulated Annealing Algorithm 
4.2 Averaged Perceptron 
Another algorithm we apply is the averaged per-
ceptron algorithm. Fig 2 is the pseudo code of 
this algorithm. Averaged perceptron is an online 
algorithm. It iterates through each instance. In 
each instance, it selects the candidate answer 
with the maximum function score. Then it up-
dates the weight by the margin of feature value 
between the select answer and the oracle answer 
(line 5-9). After each iteration, it does average to 
generate a new weight (line 10). The averaged 
1555
perceptron has a solid theoretical fundamental 
and was proved to be effective across a variety of 
NLP tasks (Collins 2002). 
However, it needs a slightly modification to 
adapt to our problem. Since the threshold and the 
constituent count are not independent, they are 
not linear separable. In this case, the perceptron 
algorithm cannot be guaranteed to converge. To 
solve this issue, we introduce an outer loop (line 
2) to iterate through the value range of threshold 
with a fixed step length and in the inner loop we 
use perceptron to estimate the other three para-
meters. Finally we select the final parameter 
which has maximum F1 score across all the itera-
tion (line 14-17). 
 
Pseudo code 2. Averaged perceptron algorithm 
Input: k-best trees combined from two model output 
Notation:  
   MaxF1, Rp: already defined in pseudo code 1 
   T: the max number of iterations 
   I: the number of instances 
   Threshold: the threshold for constituent count 
   w: the three feature weights other than threshold 
   ?: the candidate tree with max function score given a 
fixed weight w 
   ?: the candidate tree with the max F1 score (since the 
oracle tree may not appeared in our candidate set, 
we choose this one as the pseudo orcale tree) 
   : the set of candidate tree for ith sentence 
Output: Rp 
 
1. MaxF1:=0, T=30 
2. for  Threshold :=0 to 1 with step 0.01 do  
3.     Initialize w 
4.     for iter : 1 to T do 
5.           for  i := 1 to I  do 
6.               ? ?????????  
7.               ? ?  
8.               ?:= w 
9.           end for  
10.           ? ??I???I  
11.           if converged  then break 
12.     end for 
13.     p := (Threshold, w) 
14.     if F1(p) > MaxF1 then 
15.         MaxF1 := F1(p) 
16.         Rp:=p 
17.     end if 
18. end for 
 
Fig 2. Averaged Perceptron Algorithm 
5 Experiments 
We evaluate our method on both Chinese and 
English syntactic parsing task with the standard 
division on Chinese Penn Treebank Version 5.0 
and WSJ English Treebank 3.0 (Marcus et al 
1993) as shown in Table 1.  
We use Satoshi Sekine and Michael Collins? 
EVALB script modified by David Ellis for accu-
racy evaluation. We use Charniak?s parser 
(Charniak 2000) and Berkeley?s parser (Petrov 
and Klein 2007) as the two individual parsers, 
where Charniak?s parser represents the best per-
formance of the lexicalized model and the Berke-
ley?s parser represents the best performance of 
the un-lexicalized model. We retrain both of 
them according to the division in Table. 1. The 
number of EM iteration process for Berkeley?s 
parser is set to 5 on English and 6 on Chinese. 
Both the Charniak?s parser and Berkeley?s parser 
provide function to evaluate an input parse tree?s 
probability and output the logarithm of the prob-
ability. 
 
        Div. 
Lang. Train Dev Test 
English Sec.02-21 Sec. 22 Sec. 23 
 
Chinese 
Art. 
001-270, 
400-1151 
Art. 
301-325 
Art. 
271-300 
 
          Table 1. Data division 
5.1 Effectiveness of our Combination Me-
thod 
This sub-section examines the effectiveness of 
our proposed methods. The experiment is set up 
as follows: 1) for each sentence in the dev and 
test sets, we generate 50-best from Charniak?s 
parser (Charniak 2000) and Berkeley?s parser 
(Petrov and Klein 2007), respectively; 2) the two 
50-best trees are merged together and duplication 
was removed; 3) we tune the parameters on the 
dev set and test on the test set. (Without specific 
statement, we use simulated-annealing as default 
weight tuning algorithm.)  
The results are shown in Table 2 and Table 3. 
?P? means precision, ?R? means recall and ?F? is 
the F1-measure (all is in % percentage metrics); 
?Charniak? represents the parser of (Charniak 
2000), ?Berkeley? represents the parser of (Pe-
trov and Klein 2007), ?Comb.? represents the 
combination of the two parsers. 
 
         parser 
accuracy Charniak Berkeley Comb. 
<=40 
words 
P 85.20 86.65 90.44 
R 83.70 84.18 85.96 
F 84.44 85.40 88.15 
All 
P 82.07 84.63 87.76 
R 79.66 81.69 83.27 
F 80.85 83.13 85.45 
 
Table 2. Results on Chinese 
1556
         parser 
accuracy Charniak Berkeley Comb. 
<=40 
words 
P 90.45 90.27 92.36 
R 90.14 89.76 91.42 
F 90.30 90.02 91.89 
All 
P 89.86 89.77 91.89 
R 89.53 89.26 90.97 
F 89.70 89.51 91.43 
 
Table 3. Results on English 
 
From Table 2 and Table 3, we can see our me-
thod outperforms the single systems in all test 
cases with all the three evaluation metrics. Using 
the entire Chinese test set, our method improves 
the performance by 2.3 (85.45-83.13) point in 
F1-Score, representing 13.8% error rate reduc-
tion. Using the entire English test set, our method 
improves the performance by 1.7 (91.43-89.70) 
point in F1-Score, representing 16.5% error rate 
reduction. These improvements convincingly 
demonstrate the effectiveness of our method. 
5.2 Effectiveness of K 
Fig 3 and Fig. 4 show the relationship between 
F1 score and the number of K-best used when 
doing combination on Chinese and English re-
spectively.  
From Fig 3 and Fig. 4, we could see that the 
F1 score first increases with the increasing of K 
(there are some vibration points, this may due to 
statistical noise) and reach the peak when K is 
around 30-50, then it starts to drop.  It shows that 
k-best list did provide more information than 
one-best and thus can help improve the accuracy; 
however more k-best list may also contain more 
noises and these noises may hurt the final com-
bination quality. 
 
 
 
       Fig 3. F1-measure vs. K on Chinese 
 
 
 
       Fig 4. F1-measure vs. K on English 
5.3 Diversity on the K-best Output of the 
Head-driven and Latent-annotation-
driven Model  
In this subsection, we examine how different of 
the 50-best trees generated from Charnriak?s 
parser (head-driven model) (Charnriak, 2000) 
and Berkeley?s parser (latent-annotation model) 
(Petrov and Klein, 2007).   
Table 4 reports the statistics on the 50-best 
output for Chinese and English test set. Since for 
some short sentences the parser cannot generate 
up to 50 best trees, the average number of trees is 
less than 50 for each sentence. Each cell reports 
the total number of trees generated over the en-
tire test set followed by the average count for 
each sentence in bracket. ?Total? means simply 
combine the number of trees from the two pars-
ers while ?Unique? means the number after re-
moving the duplicated trees for each sentence. In 
the last row, we report the averaged redundant 
rate for each sentence, which is derived by divid-
ing the figures in the row ?Duplicated? by those 
in the row ?Total?. 
 
 Chinese English 
Charniak 14577 (41.9) 120438 (49.9) 
Berkeley 14524 (41.7) 114299 (47.3) 
Total 29101 (83.6) 234737 (97.2) 
Unique 27747 (79.7) 221633 (91.7) 
Duplicated 1354 (3.9) 13104 (5.4) 
Redundant rate 4.65% 5.58% 
 
          Table 4. The statistics on the 50-best out-
put for Chinese and English test set.  
 
The small redundant rate clearly suggests that 
the two parsing models are quite different and 
are complementary to each other.  
1557
         parser 
Oracle Charniak Berkeley Comb. 
Chinese 
P 88.95 90.07 92.45 
R 86.51 87.12 89.67 
F 87.71 88.57 91.03 
English 
P 97.06 95.86 98.10 
R 96.57 95.53 97.68 
F 96.82 95.70 97.89 
 
Table 5. The oracle over 50-best output for in-
dividual parser and our method 
 
The k-best oracle score is the upper bound of 
the quality of the k-best trees. Table 5 reports the 
oracle score for the 50-best of the two individual 
parsers and our method.  Similar to Table 4, Ta-
ble 5 shows again that the two models are com-
plementary to each other and our method is able 
to take the strength of the two models. 
5.4 Effectiveness of Model Confidence 
One of the advantages of our method that we 
claim is that we can utilize the feature of the 
model confidence score (logarithm of the parse 
tree probability). 
Table 6 shows that all the three features con-
tribute to the final accuracy improvement. Even 
if we only use the ?B+C? confidence scores, it 
also outperforms the baseline individual parser 
(as reported in Table 2 and Table 3) greatly. All 
these together clearly verify the effective of the 
model confidence feature and our method can 
effectively utilize this feature. 
 
         Feat.  
Lang    I B+C B+C+I 
Chinese 82.34 84.67 85.45 
English 90.20 91.02 91.43 
 
Table 6. F1 score on 50-best combination with 
different feature configuration. ?I? means the 
constituent count, ?B? means Berkeley parser 
confidence score and ?C? means Charniak parser 
confidence score. 
5.5 Comparison of the Weight Tuning Al-
gorithms 
In this sub-section, we compare the two weight 
tuning algorithms on 50-best combination tasks 
on both Chinese and English. Dan Bikel?s ran-
domized parsing evaluation comparator (Bikel 
2004) was used to do significant test on precision 
and recall metrics. The results are shown in Ta-
ble 7.  
We can see, simulated annealing outperforms 
the averaged perceptron significantly in both 
precision (p<0.005) and recall (p<0.05) metrics 
of Chinese task and precision (p<0.005) metric 
of English task. Though averaged perceptron got 
slightly better recall score on English task, it is 
not significant according to the p-value (p>0.2). 
From table 8, we could see the simulated an-
nealing algorithm is around 2-4 times slower 
than averaged perceptron algorithm. 
 
         Algo. 
Lang SA. AP. P-value 
Chinese 
P 87.76 86.85 0.003 
R 83.27 82.90 0.030 
English 
P 91.89 91.72 0.004 
R 90.97 91.02 0.205 
 
Table 7. Precision and Recall score on 50-best 
combination by the two parameter estimation 
algorithms with significant test; ?SA.? is simu-
lated annealing, ?AP.? is averaged perceptron, 
?P-value? is the significant test p-value. 
 
           Algo. 
Lang 
Simulated 
Annealing 
Averaged 
Perceptron 
Chinese 2.3 0.6 
English 12 6 
  
   Table 8. Time taken (in minutes) on 50-best 
combination of the two parameter estimation 
algorithms 
5.6 Performance-Enhanced Individual  
Parsers on English  
For Charniak?s lexicalized parser, there are two 
techniques to improve its performance. One is re-
ranking as explained in section 2. The other is 
the self-training (McClosky et al 2006) which 
first parses and reranks the NANC corpus, and 
then use them as additional training data to re-
train the model. In this sub-section, we apply our 
method to combine the Berkeley parser and the 
enhanced Charniak parser by using the new 
model confidence score output from the en-
hanced Charniak parser.  
Table 9 and Table 10 show that the Charniak 
parser enhanced by re-ranking and self-training 
is able to help to further improve the perfor-
mance of our method. This is because that the 
enhanced Charniak parser provides more accu-
rate model confidence score.  
 
1558
         parser 
accuracy reranking Comb. baseline 
<=40 
words 
P 92.34 93.41 92.36 
R 91.61 92.15 91.42 
F 91.97 92.77 91.89 
All 
P 91.78 92.92 91.89 
R 91.03 91.70 90.97 
F 91.40 92.30 91.43 
 
Table 9. Performance with Charniak parser 
enhanced by re-ranking; ?baseline? is the per-
formance of the combination of Table 3. 
 
         parser 
accuracy 
self-train+ 
reranking Comb. baseline 
<=40 
words 
P 92.87 93.69 92.36 
R 92.12 92.44 91.42 
F 92.49 93.06 91.89 
All 
P 92.41 93.25 91.89 
R 91.64 92.00 90.97 
F 92.02 92.62 91.43 
 
 Table 10. Performance with Charniak parser 
enhanced by re-ranking plus self-training 
5.7 Comparison with Other State-of-the-art 
Results 
Table 11 and table 12 compare our method with 
the other state-of-the-art methods; we use I, B, R, 
S and C to denote individual model (Charniak 
2000; Collins 2000; Bod 2003; Petrov and Klein 
2007), bilingual-constrained model (Burkett and 
Klein 2008)1, re-ranking model (Charniak and 
Johnson 2005, Huang 2008), self-training model 
(David McClosky 2006) and combination model 
(Sagae and Lavie 2006) respectively. The two 
tables clearly show that our method advance the 
state-of-the-art results on both Chinese and Eng-
lish syntax parsing. 
 
System  F1-Measure 
I 
Charniak (2000) 80.85 
Petrov and Klein (2007) 83.13 
B Burkett and Klein (2008)1 84.24 
C Our method 85.45 
 
Table 11. Accuracy comparison on Chinese 
 
                                                           
1 Burkett and Klein (2008) use the additional know-
ledge from Chinese-English parallel Treebank to im-
prove Chinese parsing accuracy. 
System  F1-Measure 
I 
Petrov and Klein (2007) 89.5 
Charniak (2000) 89.7 
Bod (2003) 90.7 
R 
Collins (2000) 89.7 
Charniak and Johnson (2005) 91.4 
Huang (2008) 91.7 
S David McClosky (2006) 92.1 
C 
Sagae and Lavie (2006) 92.1 
Our method 92.6 
 
  Table 12. Accuracy comparison on English. 
6 Conclusions   
In this paper2, we propose a linear model-based 
general framework for multiple parser combina-
tion. Compared with previous methods, our me-
thod is able to use diverse features, including 
logarithm of the parse tree probability calculated 
by the individual systems. We verify our method 
by combining the two representative parsing 
models, lexicalized model and un-lexicalized 
model, on both Chinese and English. Experimen-
tal results show our method is very effective and 
advance the state-of-the-art results on both Chi-
nese and English syntax parsing. In the future, 
we will explore more features and study the for-
est-based combination methods for syntactic 
parsing. 
Acknowledgement  
We would like to thank Prof. Hwee Tou Ng for 
his help and support; Prof. Charniak for his sug-
gestion on doing the experiments with the self-
trained parser and David McCloksy for his help 
on the self-trained model; Yee Seng Chan and 
the anonymous reviewers for their valuable 
comments. 
References  
Dan  Bikel. 2004. On the Parameter Space of Genera-
tive Lexicalized Statistical Parsing Models. Ph.D. 
Thesis, University of Pennsylvania 2004. 
Rens Bod. 2003. An efficient implementation of a new 
DOP model. EACL-04. 
David Burkett and Dan Klein. 2008. Two Languages 
are Better than One (for Syntactic Parsing). 
EMNLP-08. 
                                                           
The corresponding authors of this paper are Hui 
Zhang (zhangh1982@gmail.com) and Min Zhang 
(mzhang@i2r.a-star.edu.sg) 
1559
V ?ern? 1985. Thermodynamical approach to the 
travelling salesman problem: an efficient simula-
tion algorithm. Journal of Optimization Theory and 
Applications, 45:41-51.1985. 
Eugene Charniak. 1997. Statistical parsing with a 
context-free grammar and word statistics. AAAI-
97, pages 598-603. 
Eugene Charniak. 2000. A maximum-entropy-inspired 
parser. NAACL-2000. 
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine-grained n-best parsing and discriminative 
reranking. ACL-05, Pages 173-180. 
Michael Collins. 1997. Three generative, lexicalised 
models for statistical parsing. ACL-97, pages 16-
23.  
Michael Collins.1999. Head-drivenstatistical models 
for natural language parsing. Doctoral Disserta-
tion, Dept. of Computer and Information Science, 
University of Pennsylvania, Philadelphia 1999. 
Michael Collins. 2000. Discriminative reranking for 
natural language parsing. ICML-00, pages 175-
182. 
Michael Collins. 2002. Discriminative training me-
thods for hidden markov models: Theory and expe-
riments with perceptron algorithms. EMNLP-02. 
Liang Huang. 2008. Forest Reranking: Discriminative 
Parsing with Non-Local Features. ACL-HLT-08, 
pages 586-594. 
Liang Huang and David Chiang. 2005. Better k-best 
Parsing. IWPT-05. 
S. Kirkpatrick, C. D. Gelatt, Jr. and M. P. Vecchi. 
1983. Optimization by Simulated Annealing. 
Science. New Series 220 (4598): 671-680. 
Dan Klein and Christopher D. Manning. 2001. Pars-
ing and Hypergraphs. IWPT-01. 
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. ACL-03, pages 423-
430. 
John Henderson and Eric Brill. 1999. Exploiting di-
versity in natural language processing: combining 
parsers. EMNLP-99. 
Mitchell P. Marcus, Beatrice Santorini and Mary Ann 
Marcinkiewicz. 1993. Building a large annotated 
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19:313-330. 
Takuya Matsuzaki. Yusuke Miyao and Jun'ichi Tsujii. 
2005. Probabilistic CFG with latent annotations. 
ACL-05, pages 75-82. 
David McClosky, Eugene Charniak and Mark John-
son. 2006. Effective self-training for parsing. 
NAACL-06, pages 152-159. 
Slav Petrov, Leon Barrett, Romain Thibaux and Dan 
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. COLING-ACL-06, 
pages 443-440. 
Slav Petrov and Dan Klein. 2007. Improved inference 
for unlexicalized parsing. HLT-NAACL-07, pages 
401-411. 
Stefan Riezler, Tracy H. King, Ronald M. Kaplan, 
Richard Crouch, John T. III Maxwell and Mark 
Johnson. 2002. Parsing the wall street journal us-
ing a lexical-functional grammar and discrimina-
tive estimation techniques. ACL-02, pages 271?
278.  
Kenji Sagae and Alon Lavie. 2006. Parser combina-
tion by reparsing. HLT-NAACL-06, pages 129-
132. 
Daniel Zeman and Zden?k ?abokrtsk?. Improving 
Parsing Accuracy by Combining Diverse Depen-
dency Parsers. IWPT-05. 
 
1560
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 172?180,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Forest-based Tree Sequence to String Translation Model 
 
Hui Zhang1, 2   Min Zhang1   Haizhou Li1   Aiti Aw1   Chew Lim Tan2 
1Institute for Infocomm Research                    2National University of Singapore                    
zhangh1982@gmail.com   {mzhang, hli, aaiti}@i2r.a-star.edu.sg   tancl@comp.nus.edu.sg  
 
 
 
 
Abstract 
This paper proposes a forest-based tree se-
quence to string translation model for syntax- 
based statistical machine translation, which 
automatically learns tree sequence to string 
translation rules from word-aligned source-
side-parsed bilingual texts. The proposed 
model leverages on the strengths of both tree 
sequence-based and forest-based translation 
models. Therefore, it can not only utilize forest 
structure that compactly encodes exponential 
number of parse trees but also capture non-
syntactic translation equivalences with linguis-
tically structured information through tree se-
quence. This makes our model potentially 
more robust to parse errors and structure di-
vergence. Experimental results on the NIST 
MT-2003 Chinese-English translation task 
show that our method statistically significantly 
outperforms the four baseline systems. 
1 Introduction 
Recently syntax-based statistical machine trans-
lation (SMT) methods have achieved very prom-
ising results and attracted more and more inter-
ests in the SMT research community. Fundamen-
tally, syntax-based SMT views translation as a 
structural transformation process. Therefore, 
structure divergence and parse errors are two of 
the major issues that may largely compromise 
the performance of syntax-based SMT (Zhang et 
al., 2008a; Mi et al, 2008).  
Many solutions have been proposed to address 
the above two issues. Among these advances, 
forest-based modeling (Mi et al, 2008; Mi and 
Huang, 2008) and tree sequence-based modeling 
(Liu et al, 2007; Zhang et al, 2008a) are two 
interesting modeling methods with promising 
results reported. Forest-based modeling aims to 
improve translation accuracy through digging the 
potential better parses from n-bests (i.e. forest) 
while tree sequence-based modeling aims to 
model non-syntactic translations with structured 
syntactic knowledge. In nature, the two methods 
would be complementary to each other since 
they manage to solve the negative impacts of 
monolingual parse errors and cross-lingual struc-
ture divergence on translation results from dif-
ferent viewpoints. Therefore, one natural way is 
to combine the strengths of the two modeling 
methods for better performance of syntax-based 
SMT. However, there are many challenges in 
combining the two methods into a single model 
from both theoretical and implementation engi-
neering viewpoints. In theory, one may worry 
about whether the advantage of tree sequence has 
already been covered by forest because forest 
encodes implicitly a huge number of parse trees 
and these parse trees may generate many differ-
ent phrases and structure segmentations given a 
source sentence. In system implementation, the 
exponential combinations of tree sequences with 
forest structures make the rule extraction and 
decoding tasks much more complicated than that 
of the two individual methods.  
In this paper, we propose a forest-based tree 
sequence to string model, which is designed to 
integrate the strengths of the forest-based and the 
tree sequence-based modeling methods. We pre-
sent our solutions that are able to extract transla-
tion rules and decode translation results for our 
model very efficiently. A general, configurable 
platform was designed for our model. With this 
platform, we can easily implement our method 
and many previous syntax-based methods by 
simple parameter setting. We evaluate our 
method on the NIST MT-2003 Chinese-English 
translation tasks. Experimental results show that 
our method significantly outperforms the two 
individual methods and other baseline methods. 
Our study shows that the proposed method is 
able to effectively combine the strengths of the 
forest-based and tree sequence-based methods, 
and thus having great potential to address the 
issues of parse errors and non-syntactic transla-
172
tions resulting from structure divergence. It also 
indicates that tree sequence and forest play dif-
ferent roles and make contributions to our model 
in different ways. 
The remainder of the paper is organized as fol-
lows. Section 2 describes related work while sec-
tion 3 defines our translation model. In section 4 
and section 5, the key rule extraction and decod-
ing algorithms are elaborated. Experimental re-
sults are reported in section 6 and the paper is 
concluded in section 7. 
2 Related work  
As discussed in section 1, two of the major chal-
lenges to syntax-based SMT are structure diver-
gence and parse errors. Many techniques have 
been proposed to address the structure diver-
gence issue while only fewer studies are reported 
in addressing the parse errors in the SMT re-
search community. 
To address structure divergence issue, many 
researchers (Eisner, 2003; Zhang et al, 2007) 
propose using the Synchronous Tree Substitution 
Grammar (STSG) grammar in syntax-based 
SMT since the STSG uses larger tree fragment as 
translation unit. Although promising results have 
been reported, STSG only uses one single sub-
tree as translation unit which is still committed to 
the syntax strictly. Motivated by the fact that 
non-syntactic phrases make non-trivial contribu-
tion to phrase-based SMT, the tree sequence-
based translation model is proposed (Liu et al, 
2007; Zhang et al, 2008a) that uses tree se-
quence as the basic translation unit, rather than 
using single sub-tree as in the STSG. Here, a tree 
sequence refers to a sequence of consecutive 
sub-trees that are embedded in a full parse tree. 
For any given phrase in a sentence, there is at 
least one tree sequence covering it. Thus the tree 
sequence-based model has great potential to ad-
dress the structure divergence issue by using tree 
sequence-based non-syntactic translation rules. 
Liu et al (2007) propose the tree sequence con-
cept and design a tree sequence to string transla-
tion model. Zhang et al (2008a) propose a tree 
sequence-based tree to tree translation model and 
Zhang et al (2008b) demonstrate that the tree 
sequence-based modelling method can well ad-
dress the structure divergence issue for syntax-
based SMT. 
To overcome the parse errors for SMT, Mi et 
al. (2008) propose a forest-based translation 
method that uses forest instead of one best tree as 
translation input, where a forest is a compact rep-
resentation of exponentially number of n-best 
parse trees. Mi and Huang (2008) propose a for-
est-based rule extraction algorithm, which learn 
tree to string rules from source forest and target 
string. By using forest in rule extraction and de-
coding, their methods are able to well address the 
parse error issue. 
From the above discussion, we can see that 
traditional tree sequence-based method uses sin-
gle tree as translation input while the forest-
based model uses single sub-tree as the basic 
translation unit that can only learn tree-to-string 
(Galley et al 2004; Liu et al, 2006) rules. There-
fore, the two methods display different strengths, 
and which would be complementary to each 
other. To integrate their strengths, in this paper, 
we propose a forest-based tree sequence to string 
translation model.  
3 Forest-based tree sequence to string 
model  
In this section, we first explain what a packed 
forest is and then define the concept of the tree 
sequence in the context of forest followed by the 
discussion on our proposed model. 
3.1 Packed Forest 
A packed forest (forest in short) is a special kind 
of hyper-graph (Klein and Manning, 2001; 
Huang and Chiang, 2005), which is used to rep-
resent all derivations (i.e. parse trees) for a given 
sentence under a context free grammar (CFG). A 
forest F is defined as a triple ? ?, ?, ? ?, where 
? is non-terminal node set, ?  is hyper-edge set 
and ? is leaf node set (i.e. all sentence words). A 
forest F satisfies the following two conditions: 
 
1) Each node ?  in ?  should cover a phrase, 
which is a continuous word sub-sequence in ?. 
2) Each hyper-edge ?  in ?  is defined as 
?? ? ?? ??? ? ??, ??? ? ?? ? ??, ?? ? ?? , 
where ?? ? ?? ???  covers a sequence of conti-
nuous and non-overlap phrases, ??  is the father 
node of the children sequence ?? ??? ???. The 
phrase covered by ??  is just the sum of all the 
phrases covered by each child node ??. 
 
We here introduce another concept that is used 
in our subsequent discussions. A complete forest 
CF is a general forest with one additional condi-
tion that there is only one root node N in CF, i.e., 
all nodes except the root N in a CF must have at 
least one father node. 
Fig. 1 is a complete forest while Fig. 7 is a 
non-complete forest due to the virtual node 
?VV+VV? introduced in Fig. 7. Fig. 2 is a hyper-
edge (IP => NP VP) of Fig. 1, where NP covers 
173
the phrase ?Xinhuashe?, VP covers the phrase 
?shengming youguan guiding? and IP covers the 
entire sentence. In Fig.1, only root IP has no fa-
ther node, so it is a complete forest. The two 
parse trees T1 and T2 encoded in Fig. 1 are 
shown separately in Fig. 3 and Fig. 41.  
Different parse tree represents different deri-
vations and explanations for a given sentence. 
For example, for the same input sentence in Fig. 
1, T1 interprets it as ?XNA (Xinhua News 
Agency) declares some regulations.? while T2 
interprets it as ?XNA declaration is related to 
some regulations.?.  
 
 
 
Figure 1. A packed forest for sentence ????
/Xinhuashe ?? /shengming ?? /youguan ??
/guiding? 
             
Figure 2.  A hyper-edge used in Fig. 1 
 
       
 
Figure 3. Tree 1 (T1)            Figure 4. Tree 2 (T2) 
3.2 Tree sequence in packed forest 
Similar to the definition of tree sequence used in 
a single parse tree defined in Liu et al (2007) 
and Zhang et al (2008a), a tree sequence in a 
forest also refers to an ordered sub-tree sequence 
that covers a continuous phrase without overlap-
ping. However, the major difference between 
                                                          
1 Please note that a single tree (as T1 and T2 shown in Fig. 
3 and Fig. 4) is represented by edges instead of hyper-edges. 
A hyper-edge is a group of edges satisfying the 2nd condi-
tion as shown in the forest definition. 
them lies in that the sub-trees of a tree sequence 
in forest may belongs to different single parse 
trees while, in a single parse tree-based model, 
all the sub-trees in a tree sequence are committed 
to the same parse tree.  
The forest-based tree sequence enables our 
model to have the potential of exploring addi-
tional parse trees that may be wrongly pruned out 
by the parser and thus are not encoded in the for-
est. This is because that a tree sequence in a for-
est allows its sub-trees coming from different 
parse trees, where these sub-trees may not be 
merged finally to form a complete parse tree in 
the forest. Take the forest in Fig. 1 as an exam-
ple, where ((VV shengming) (JJ youguan)) is a 
tree sequence that all sub-trees appear in T1 
while ((VV shengming) (VV youguan)) is a tree 
sequence whose sub-trees do not belong to any 
single tree in the forest. But, indeed the two sub-
trees (VV shengming) and (VV youguan) can be 
merged together and further lead to a complete 
single parse tree which may offer a correct inter-
pretation to the input sentence (as shown in Fig. 
5). In addition, please note that, on the other 
hand, more parse trees may introduce more noisy 
structures. In this paper, we leave this problem to 
our model and let the model decide which sub-
structures are noisy features. 
 
          
 
 Figure 5. A parse tree that was wrongly 
pruned out 
 
            
    Figure 6. A tree sequence to string rule 
 
174
A tree-sequence to string translation rule in a 
forest is a triple <L, R, A>, where L is the tree 
sequence in source language, R is the string con-
taining words and variables in target language, 
and A is the alignment between the leaf nodes of 
L and R. This definition is similar to that of (Liu 
et al 2007, Zhang et al 2008a) except our tree-
sequence is defined in forest. The shaded area of 
Fig. 6 exemplifies a tree sequence to string trans-
lation rule in the forest.  
3.3 Forest-based tree-sequence to string 
translation model 
Given a source forest F and target translation TS 
as well as word alignment A, our translation 
model is formulated as: 
  
 Pr??, ??, ?? ? ? ? ????????????? ?,???????, ??,??  
 
By the above Eq., translation becomes a tree 
sequence structure to string mapping issue. Giv-
en the F, TS and A, there are multiple derivations 
that could map F to TS under the constraint A. 
The mapping probability Pr??, ??, ??  in our 
study is obtained by summing over the probabili-
ties of all derivations ?. The probability of each 
derivation ?? is given as the product of the prob-
abilities of all the rules ( )ip r  used in the deriva-
tion (here we assume that each rule is applied 
independently in a derivation). 
Our model is implemented under log-linear 
framework (Och and Ney, 2002). We use seven 
basic features that are analogous to the common-
ly used features in phrase-based systems (Koehn, 
2003): 1) bidirectional rule mapping probabilities, 
2) bidirectional lexical rule translation probabili-
ties, 3) target language model, 4) number of rules 
used and 5) number of target words. In addition, 
we define two new features: 1) number of leaf 
nodes in auxiliary rules (the auxiliary rule will be 
explained later in this paper) and 2) product of 
the probabilities of all hyper-edges of the tree 
sequences in forest. 
4 Training  
This section discusses how to extract our transla-
tion rules given a triple ? ?, ??, ? ? . As we 
know, the traditional tree-to-string rules can be 
easily extracted from ? ?, ??, ? ? using the algo-
rithm of Mi and Huang (2008)2. We would like 
                                                          
2 Mi and Huang (2008) extend the tree-based rule extraction 
algorithm (Galley et al, 2004) to forest-based by introduc-
ing non-deterministic mechanism. Their algorithm consists 
of two steps, minimal rule extraction and composed rule 
generation. 
to leverage on their algorithm in our study. Un-
fortunately, their algorithm is not directly appli-
cable to our problem because tree rules have only 
one root while tree sequence rules have multiple 
roots. This makes the tree sequence rule extrac-
tion very complex due to its interaction with for-
est structure. To address this issue, we introduce 
the concepts of virtual node and virtual hyper-
edge to convert a complete parse forest ?  to a 
non-complete forest ? which is designed to en-
code all the tree sequences that we want. There-
fore, by doing so, the tree sequence rules can be 
extracted from a forest in the following two 
steps: 
1) Convert the complete parse forest ? into a 
non-complete forest ?  in order to cover those 
tree sequences that cannot be covered by a single 
tree node. 
2) Employ the forest-based tree rule extraction 
algorithm (Mi and Huang, 2008) to extract our 
rules from the non-complete forest. 
To facilitate our discussion, here we introduce 
two notations:  
? Alignable: A consecutive source phrase is 
an alignable phrase if and only if it can be 
aligned with at least one consecutive target 
phrase under the word-alignment con-
straint. The covered source span is called 
alignable span. 
? Node sequence: a sequence of nodes (ei-
ther leaf or internal nodes) in a forest cov-
ering a consecutive span. 
Algorithm 1 illustrates the first step of our rule 
extraction algorithm, which is a CKY-style Dy-
namic Programming (DP) algorithm to add vir-
tual nodes into forest. It includes the following 
steps: 
1) We traverse the forest to visit each span in 
bottom-up fashion (line 1-2), 
1.1) for each span [u,v] that is covered by 
single tree nodes3, we put these tree 
nodes into the set NSS(u,v) and go 
back to step 1 (line 4-6). 
1.2) otherwise we concatenate the tree se-
quences of sub-spans to generate the 
set of tree sequences covering the cur-
rent larger span (line 8-13). Then, we 
prune the set of node sequences (line 
14). If this span is alignable, we 
create virtual father nodes and corres-
ponding virtual hyper-edges to link 
the node sequences with the virtual 
father nodes (line 15-20). 
                                                          
3 Note that in a forest, there would be multiple single tree 
nodes covering the same span as shown Fig.1.  
175
2) Finally we obtain a forest with each align-
able span covered by either original tree 
nodes or the newly-created tree sequence 
virtual nodes. 
Theoretically, there is exponential number of 
node sequences in a forest. Take Fig. 7 as an ex-
ample. The NSS of span [1,2] only contains ?NP? 
since it is alignable and covered by the single 
tree node NP. However, span [2,3] cannot be 
covered by any single tree node, so we have to 
create the NSS of span[2,3] by concatenating the 
NSSs of span [2,2] and span [3,3]. Since NSS of 
span [2,2] contains 4 element {?NN?, ?NP?, 
?VV?, ?VP?} and NSS of span [3, 3] also con-
tains 4 element {?VV?, ?VP?, ?JJ?, ?ADJP?}, 
NSS of span [2,3] contains 16=4*4 elements. To 
make the NSS manageable, we prune it with the 
following thresholds: 
? each node sequence should contain less 
than n nodes 
? each node sequence set should contain less 
than m node sequences 
? sort node sequences according to their 
lengths and only keep the k shortest ones 
Each virtual node is simply labeled by the 
concatenation of all its children?s labels as 
shown in Fig. 7. 
 
Algorithm 1. add virtual nodes into forest 
Input: packed forest F, alignment A 
Notation:  
   L: length of source sentence 
   NSS(u,v): the set of node sequences covering span [u,v] 
  VN(ns): virtual father node for node sequence ns. 
Output: modified forest F with virtual nodes 
 
 
1. for length := 0 to L - 1 do 
2.      for start := 1 to L - length do 
3.          stop := start + length 
4.          if span[start, stop] covered by tree nodes then 
5.                for each node n of span [start, stop] do 
6.                    add n into NSS(start, stop) 
7.          else  
8.                for pivot := start to stop - 1 
9.                     for each ns1 in NSS(start, pivot) do 
10.                          for each ns2 in NSS(pivot+1, stop) do 
11.                               create ?? ?? ?1? ?  ?2?  
12.                                if ns is not in NSS(start, stop) then 
13.                                      add ns into NSS(start, stop) 
14.                do pruning on NSS(start, stop) 
15.                if the span[start, stop] is alignable then 
16.                    for each ns of NSS(start, stop) do 
17.                   if node VN(ns) is not in F then 
18.                                add node VN(ns) into F 
19.                          add a hyper-edge h into F,  
20.                          let lhs(h) := VN(ns), rhs(h) := ns 
 
Algorithm 1 outputs a non-complete forest CF 
with each alignable span covered by either tree 
nodes or virtual nodes. Then we can easily ex-
tract our rules from the CF using the tree rule 
extraction algorithm (Mi and Huang, 2008). 
Finally, to calculate rule feature probabilities 
for our model, we need to calculate the fractional 
counts (it is a kind of probability defined in Mi 
and Huang, 2008) of each translation rule in a 
parse forest. In the tree case, we can use the in-
side-outside-based methods (Mi and Huang 
2008) to do it. In the tree sequence case, since 
the previous method cannot be used directly, we 
provide another solution by making an indepen-
dent assumption that each tree in a tree sequence 
is independent to each other. With this assump-
tion, the fractional counts of both tree and tree 
sequence can be calculated as follows: 
 
???? ? ?????????????????   
 
???????? ? ? ????
????????????
? ? ????
??????
? ? ????
??????????????
 
 
where ???? is the fractional counts to be calcu-
lated for rule r, a frag is either lhs(r) (excluding 
virtual nodes and virtual hyper-edges) or any tree 
node in a forest, TOP is the root of the forest, 
??. ? and ??.) are the outside and inside probabil-
ities of nodes, ?????. ? returns the root nodes of a 
tree sequence fragment, ???????. ?  returns the 
leaf nodes of a tree sequence fragment, ???? is 
the hyper-edge probability. 
 
 
 
              Figure 7. A virtual node in forest 
5 Decoding  
We benefit from the same strategy as used in our 
rule extraction algorithm in designing our decod-
ing algorithm, recasting the forest-based tree se-
quence-to-string decoding problem as a forest-
based tree-to-string decoding problem. Our de-
coding algorithm consists of four steps: 
1) Convert the complete parse forest to a non-
complete one by introducing virtual nodes. 
176
2) Convert the non-complete parse forest into 
a translation forest4 ?? by using the translation 
rules and the pattern-matching algorithm pre-
sented in Mi et al (2008). 
3) Prune out redundant nodes and add auxil-
iary hyper-edge into the translation forest for 
those nodes that have either no child or no father. 
By this step, the translation forest ?? becomes a 
complete forest.  
4) Decode the translation forest using our 
translation model and a dynamic search algo-
rithm. 
The process of step 1 is similar to Algorithm 1 
except no alignment constraint used here. This 
may generate a large number of additional virtual 
nodes; however, all redundant nodes will be fil-
tered out in step 3. In step 2, we employ the tree-
to-string pattern match algorithm (Mi et al, 
2008) to convert a parse forest to a translation 
forest. In step 3, all those nodes not covered by 
any translation rules are removed. In addition, 
please note that the translation forest is already 
not a complete forest due to the virtual nodes and 
the pruning of rule-unmatchable nodes. We, 
therefore, propose Algorithm 2 to add auxiliary 
hyper-edges to make the translation forest com-
plete.  
In Algorithm 2, we travel the forest in bottom-
up fashion (line 4-5). For each span, we do: 
1) generate all the NSS for this span (line 7-12)  
2) filter the NSS to a manageable size (line 13) 
3) add auxiliary hyper-edges for the current 
span (line 15-19) if it can be covered by at least 
one single tree node, otherwise go to step 1 . This 
is the key step in our Algorithm 2. For each tree 
node and each node sequences covering the same 
span (stored in the current NSS), if the tree node 
has no children or at least one node in the node 
sequence has no father, we add an auxiliary hy-
per-edge to connect the tree node as father node 
with the node sequence as children. Since Algo-
rithm 2 is DP-based and traverses the forest in a 
bottom-up way, all the nodes in a node sequence 
should already have children node after the lower 
level process in a small span. Finally, we re-build 
the NSS of current span for upper level NSS 
combination use (line 20-22). 
 
 In Fig. 8, the hyper-edge ?IP=>NP VV+VV 
NP? is an auxiliary hyper-edge introduced by 
Algorithm 2. By Algorithm 2, we convert the 
translation forest into a complete translation for-
est. We then use a bottom-up node-based search 
                                                          
4 The concept of translation forest is proposed in Mi et 
al. (2008). It is a forest that consists of only the hyper-
edges induced from translation rules. 
algorithm to do decoding on the complete trans-
lation forest. We also use Cube Pruning algo-
rithm (Huang and Chiang 2007) to speed up the 
translation process. 
 
 
 
Figure 8. Auxiliary hyper-edge in a translation 
forest 
 
Algorithm 2. add auxiliary hyper-edges into mt forest F 
Input:  mt forest F 
Output: complete forest F with auxiliary hyper-edges 
 
1. for i := 1 to L do 
2.      for each node n of span [i, i] do 
3.          add n into NSS(i, i) 
4. for length := 1 to L - 1 do 
5.      for start := 1 to L - length do 
6.          stop := start + length 
7.          for pivot := start to stop-1 do 
8.               for each ns1 in NSS (start, pivot) do 
9.                    for each ns2 in NSS (pivot+1,stop) do 
10.                 create ?? ?? ?1? ?  ?2? 
11.                          if ns is not in NSS(start, stop) then 
12.                                add ns into NSS (start, stop) 
13.           do pruning on NSS(start, stop) 
14.           if there is tree node cover span [start, stop] then 
15.         for each tree node n of span [start,stop] do 
16.                      for each ns of NSS(start, stop) do 
17.                     if node n have no children or  
there is node in ns with no father  
then 
18.                                add auxiliary hyper-edge h into F 
19.                                let lhs(h) := n, rhs(h) := ns 
20.          empty NSS(start, stop) 
21.          for each node n of span [start, stop] do 
22.                 add n into NSS(start, stop) 
6 Experiment 
6.1 Experimental Settings 
We evaluate our method on Chinese-English 
translation task. We use the FBIS corpus as train-
ing set, the NIST MT-2002 test set as develop-
ment (dev) set and the NIST MT-2003 test set as 
test set. We train Charniak?s parser (Charniak 
2000) on CTB5 to do Chinese parsing, and modi-
fy it to output packed forest. We tune the parser 
on section 301-325 and test it on section 271-
300. The F-measure on all sentences is 80.85%. 
A 3-gram language model is trained on the Xin-
177
hua portion of the English Gigaword3 corpus and 
the target side of the FBIS corpus using the 
SRILM Toolkits (Stolcke, 2002) with modified 
Kneser-Ney smoothing (Kenser and Ney, 1995). 
GIZA++ (Och and Ney, 2003) and the heuristics 
?grow-diag-final-and? are used to generate m-to-
n word alignments. For the MER training (Och, 
2003), Koehn?s MER trainer (Koehn, 2007) is 
modified for our system. For significance test, 
we use Zhang et al?s implementation (Zhang et 
al, 2004). Our evaluation metrics is case-
sensitive BLEU-4 (Papineni et al, 2002). 
For parse forest pruning (Mi et al, 2008), we 
utilize the Margin-based pruning algorithm pre-
sented in (Huang, 2008). Different from Mi et al 
(2008) that use a static pruning threshold, our 
threshold is sentence-depended. For each sen-
tence, we compute the Margin between the n-th 
best and the top 1 parse tree, then use the Mar-
gin-based pruning algorithm presented in 
(Huang, 2008) to do pruning. By doing so, we 
can guarantee to use at least all the top n best 
parse trees in the forest. However, please note 
that even after pruning there is still exponential 
number of additional trees embedded in the for-
est because of the sharing structure of forest. 
Other parameters are set as follows: maximum 
number of roots in a tree sequence is 3, maxi-
mum height of a translation rule is 3, maximum 
number of leaf nodes is 7, maximum number of 
node sequences on each span is 10, and maxi-
mum number of rules extracted from one node is 
10000. 
6.2 Experimental Results 
We implement our proposed methods as a gen-
eral, configurable platform for syntax-based 
SMT study. Based on this platform, we are able 
to easily implement most of the state-of-the-art 
syntax-based x-to-string SMT methods via sim-
ple parameter setting. For training, we set forest 
pruning threshold to 1 best for tree-based me-
thods and 100 best for forest-based methods. For 
decoding, we set: 
1) TT2S: tree-based tree-to-string model by 
setting the forest pruning threshold to 1 best and 
the number of sub-trees in a tree sequence to 1. 
2) TTS2S: tree-based tree-sequence to string 
system by setting the forest pruning threshold to 
1 best and the maximum number of sub-trees in a 
tree sequence to 3. 
3) FT2S: forest-based tree-to-string system by 
setting the forest pruning threshold to 500 best, 
the number of sub-trees in a tree sequence to 1. 
4) FTS2S: forest-based tree-sequence to string 
system by setting the forest pruning threshold to 
500 best and the maximum number of sub-trees 
in a tree sequence to 3. 
 
Model BLEU(%) 
Moses 25.68 
TT2S 26.08 
TTS2S 26.95 
FT2S 27.66 
FTS2S 28.83 
 
Table 1. Performance Comparison 
 
We use the first three syntax-based systems 
(TT2S, TTS2S, FT2S) and Moses (Koehn et al, 
2007), the state-of-the-art phrase-based system, 
as our baseline systems. Table 1 compares the 
performance of the five methods, all of which are 
fine-tuned.  It shows that: 
1) FTS2S significantly outperforms (p<0.05) 
FT2S. This shows that tree sequence is very use-
ful to forest-based model. Although a forest can 
cover much more phrases than a single tree does, 
there are still many non-syntactic phrases that 
cannot be captured by a forest due to structure 
divergence issue. On the other hand, tree se-
quence is a good solution to non-syntactic trans-
lation equivalence modeling. This is mainly be-
cause tree sequence rules are only sensitive to 
word alignment while tree rules, even extracted 
from a forest (like in FT2S), are also limited by 
syntax according to grammar parsing rules. 
2) FTS2S shows significant performance im-
provement (p<0.05) over TTS2S due to the con-
tribution of forest. This is mainly due to the fact 
that forest can offer very large number of parse 
trees for rule extraction and decoder. 
3) Our model statistically significantly outper-
forms all the baselines system. This clearly de-
monstrates the effectiveness of our proposed 
model for syntax-based SMT. It also shows that 
the forest-based method and tree sequence-based 
method are complementary to each other and our 
proposed method is able to effectively integrate 
their strengths. 
4) All the four syntax-based systems show bet-
ter performance than Moses and three of them 
significantly outperforms (p<0.05) Moses. This 
suggests that syntax is very useful to SMT and 
translation can be viewed as a structure mapping 
issue as done in the four syntax-based systems. 
Table 2 and Table 3 report the distribution of 
different kinds of translation rules in our model 
(training forest pruning threshold is set to 100 
best) and in our decoding (decoding forest prun-
ing threshold is set to 500 best) for one best 
translation generation. From the two tables, we 
can find that: 
178
Rule Type Tree 
to String 
Tree Sequence 
to String 
L 4,854,406 20,526,674 
P 37,360,684 58,826,261 
U 3,297,302 3,775,734 
All 45,512,392 83,128,669 
 
Table 2. # of rules extracted from training cor-
pus. L means fully lexicalized, P means partially 
lexicalized, U means unlexicalized. 
 
Rule Type Tree 
to String 
Tree Sequence 
to String 
L 10,592 1,161 
P 7,132 742 
U 4,874 278 
All 22,598 2,181 
 
Table 3. # of rules used to generate one-best 
translation result in testing 
 
1) In Table 2, the number of tree sequence 
rules is much larger than that of tree rules al-
though our rule extraction algorithm only ex-
tracts those tree sequence rules over the spans 
that tree rules cannot cover. This suggests that 
the non-syntactic structure mapping is still a big 
challenge to syntax-based SMT. 
2) Table 3 shows that the tree sequence rules 
is around 9% of the tree rules when generating 
the one-best translation. This suggests that 
around 9% of translation equivalences in the test 
set can be better modeled by tree sequence to 
string rules than by tree to string rules. The 9% 
tree sequence rules contribute 1.17 BLEU score 
improvement (28.83-27.66 in Table 1) to FTS2S 
over FT2S.  
3) In Table 3, the fully-lexicalized rules are 
the major part (around 60%), followed by the 
partially-lexicalized (around 35%) and un-
lexicalized (around 15%). However, in Table 2, 
partially-lexicalized rules extracted from training 
corpus are the major part (more than 70%). This 
suggests that most partially-lexicalized rules are 
less effective in our model. This clearly directs 
our future work in model optimization. 
 
BLEU (%)    
N-best \ model FT2S FTS2S 
100 Best 27.40 28.61 
500 Best  27.66 28.83 
2500 Best  27.66 28.96 
5000 Best  27.79 28.89 
 
Table 4. Impact of the forest pruning  
 
Forest pruning is a key step for forest-based 
method. Table 4 reports the performance of the 
two forest-based models using different values of 
the forest pruning threshold for decoding. It 
shows that: 
1) FTS2S significantly outperforms (p<0.05) 
FT2S consistently in all test cases. This again 
demonstrates the effectiveness of our proposed 
model. Even if in the 5000 Best case, tree se-
quence is still able to contribute 1.1 BLEU score 
improvement (28.89-27.79). It indicates the ad-
vantage of tree sequence cannot be covered by 
forest even if we utilize a very large forest.  
2) The BLEU scores are very similar to each 
other when we increase the forest pruning thre-
shold. Moreover, in one case the performance 
even drops. This suggests that although more 
parse trees in a forest can offer more structure 
information, they may also introduce more noise 
that may confuse the decoder. 
7 Conclusion   
In this paper, we propose a forest-based tree-
sequence to string translation model to combine 
the strengths of forest-based methods and tree-
sequence based methods. This enables our model 
to have the great potential to address the issues 
of structure divergence and parse errors for syn-
tax-based SMT. We convert our forest-based tree 
sequence rule extraction and decoding issues to 
tree-based by introducing virtual nodes, virtual 
hyper-edges and auxiliary rules (hyper-edges). In 
our system implementation, we design a general 
and configurable platform for our method, based 
on which we can easily realize many previous 
syntax-based methods. Finally, we examine our 
methods on the FBIS corpus and the NIST MT-
2003 Chinese-English translation task. Experi-
mental results show that our model greatly out-
performs the four baseline systems. Our study 
demonstrates that forest-based method and tree 
sequence-based method are complementary to 
each other and our proposed method is able to 
effectively combine the strengths of the two in-
dividual methods for syntax-based SMT. 
Acknowledgement  
We would like to thank Huang Yun for preparing 
the pictures in this paper; Run Yan for providing 
the java version modified MERT program and 
discussion on the details of MOSES; Mi Haitao 
for his help and discussion on re-implementing 
the FT2S model; Sun Jun and Xiong Deyi for 
their valuable suggestions. 
179
References  
Eugene Charniak. 2000. A maximum-entropy inspired 
parser. NAACL-00. 
Jason Eisner. 2003. Learning non-isomorphic tree 
mappings for MT. ACL-03 (companion volume). 
Michel Galley, Mark Hopkins, Kevin Knight and Da-
niel Marcu. 2004. What?s in a translation rule? 
HLT-NAACL-04. 273-280. 
Liang Huang. 2008. Forest Reranking: Discriminative 
Parsing with Non-Local Features. ACL-HLT-08. 
586-594 
Liang Huang and David Chiang. 2005. Better k-best 
Parsing. IWPT-05. 
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language 
models. ACL-07. 144?151 
Liang Huang, Kevin Knight and Aravind Joshi. 2006. 
Statistical Syntax-Directed Translation with Ex-
tended Domain of Locality. AMTA-06. (poster) 
Reinhard Kenser and Hermann Ney. 1995. Improved 
backing-off for M-gram language modeling. 
ICASSP-95. 181-184 
Dan Klein and Christopher D. Manning. 2001. Pars-
ing and Hypergraphs. IWPT-2001. 
Philipp Koehn, F. J. Och and D. Marcu. 2003. Statis-
tical phrase-based translation. HLT-NAACL-03. 
127-133. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertol-
di, Brooke Cowan, Wade Shen, Christine Moran, 
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra 
Constantin and Evan Herbst. 2007. Moses: Open 
Source Toolkit for Statistical Machine Translation. 
ACL-07. 177-180. (poster) 
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-
String Alignment Template for Statistical Machine 
Translation. COLING-ACL-06. 609-616. 
Yang Liu, Yun Huang, Qun Liu and Shouxun Lin. 
2007. Forest-to-String Statistical Translation 
Rules. ACL-07. 704-711. 
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. ACL-HLT-08. 192-199. 
Haitao Mi and Liang Huang. 2008. Forest-based 
Translation Rule Extraction. EMNLP-08. 206-214. 
Franz J. Och and Hermann Ney. 2002. Discriminative 
training and maximum entropy models for statis-
tical machine translation. ACL-02. 295-302. 
Franz J. Och. 2003. Minimum error rate training in 
statistical machine translation. ACL-03. 160-167. 
Franz Josef Och and Hermann Ney. 2003. A Syste-
matic Comparison of Various Statistical Alignment 
Models. Computational Linguistics. 29(1) 19-51.  
Kishore Papineni, Salim Roukos, ToddWard and 
Wei-Jing Zhu. 2002. BLEU: a method for automat-
ic evaluation of machine translation. ACL-02. 311-
318. 
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. ICSLP-02. 901-904. 
Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Sheng 
Li and Chew Lim Tan. 2007. A Tree-to-Tree 
Alignment-based Model for Statistical Machine 
Translation. MT-Summit-07. 535-542. 
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, 
Chew Lim Tan, Sheng Li. 2008a. A Tree Sequence 
Alignment-based Tree-to-Tree Translation Model. 
ACL-HLT-08. 559-567. 
Min Zhang, Hongfei Jiang, Haizhou Li, Aiti Aw, 
Sheng Li. 2008b. Grammar Comparison Study for 
Translational Equivalence Modeling and Statistic-
al Machine Translation. COLING-08. 1097-1104. 
Ying Zhang, Stephan Vogel, Alex Waibel. 2004. In-
terpreting BLEU/NIST scores: How much im-
provement do we need to have a better system? 
LREC-04. 2051-2054. 
 
180
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 440?450,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Non-isomorphic Forest Pair Translation 
 
 
Hui Zhang1, 2, 3   Min Zhang1   Haizhou Li1   Eng Siong Chng2 
 
1Institute for Infocomm Research   
2Nanyang Technological University  
3USC Information Science Institute  
huizhang.fuan@gmail.com   {mzhang, hli}@i2r.a-star.edu.sg   aseschng@ntu.edu.sg 
 
 
 
 
 
 
 
 
 
 
Abstract 
This paper studies two issues, non-isomorphic 
structure translation and target syntactic structure 
usage, for statistical machine translation in the 
context of forest-based tree to tree sequence trans-
lation. For the first issue, we propose a novel 
non-isomorphic translation framework to capture 
more non-isomorphic structure mappings than tra-
ditional tree-based and tree-sequence-based trans-
lation methods. For the second issue, we propose a 
parallel space searching method to generate hypo-
thesis using tree-to-string model and evaluate its 
syntactic goodness using tree-to-tree/tree sequence 
model. This not only reduces the search complexity 
by merging spurious-ambiguity translation paths 
and solves the data sparseness issue in training, but 
also serves as a syntax-based target language mod-
el for better grammatical generation. Experiment 
results on the benchmark data show our proposed 
two solutions are very effective, achieving signifi-
cant performance improvement over baselines 
when applying to different translation models. 
1 Introduction 
Recently syntax-based methods have achieved very 
promising results and attracted increasing interests in 
statistical machine translation (SMT) research com-
munity due to their ability to provide informative 
context structure information and convenience in 
carrying out word transformation and sub-span reor-
dering. Fundamentally, syntax-based SMT views 
translation as a structural transformation process. 
Generally speaking, from modeling viewpoint, a 
syntax-based model tries to convert the source struc-
tures into target structures iteratively and recursively 
while from decoding viewpoint a syntax-based sys-
tem segments an input tree/forest into many 
sub-fragments, translates each of them separately, 
combines the translated sub-fragments and then finds 
out the best combinations. Therefore, from bilingual 
viewpoint, we face two fundamental problems: the 
mapping between bilingual structures and the way of 
carrying out the target structures combination.  
For the first issue, a number of models have been 
proposed to model the structure mapping between 
tree and string (Galley et al, 2004; Liu et al, 2006; 
Yamada and Knight, 2001; DeNeefe and Knight, 
2009) and between tree and tree (Eisner, 2003; 
Zhang et al, 2007 & 2008; Liu et al, 2009). How-
ever, one of the major challenges is that all the cur-
rent models only allow one-to-one mapping from one 
source frontier non-terminal node (Galley et al, 2004) 
to one target frontier non-terminal node in a bilingual 
translation rule. Therefore, all those translation equi-
valents with one-to-many frontier non-terminal node 
mapping cannot be covered by the current 
state-of-the-art models. This may largely compro-
mise the modeling ability of translation rules. 
For the second problem, currently, the combina-
tion is driven by only the source side (both 
tree-to-string model and tree-to-tree model only 
check the source span compatibility when combining 
different target structures in decoding) or only the 
440
target side (string to tree model). There is no well 
study in considering both the source side information 
and the compatibility between different target syn-
tactic structures during combination. In addition, it is 
well known that the traditional tree-to-tree models 
suffer heavily from the data sparseness issue in 
training and the spurious-ambiguity translation path 
issue (the same translation with different syntactic 
structures) in decoding. 
In addition, because of the performance limitation 
of automatic syntactic parser, researchers propose 
using packed forest (Tomita, 1987; Klein and Man-
ning, 2001; Huang, 2008)1 instead of 1-best parse 
tree to carry out training (Mi and Huang, 2008) and 
decoding (Mi et al, 2008) in order to reduce the side 
effect caused by parsing errors of the one-best tree. 
However, when we apply the tree-to-tree model to 
the bilingual forest structures, both training and de-
coding become very complicated. 
In this paper, to address the first issue, we propose 
a framework to model the non-isomorphic translation 
process from source tree fragment to target tree se-
quence, allowing any one source frontier 
non-terminal node to be translated into any number 
of target frontier non-terminal nodes. For the second 
issue, we propose a technology to model the combi-
nation task by considering both sides? syntactic 
structure information. We evaluate and integrate the 
two technologies into forest-based tree to tree se-
quence translation. Experimental results on the 
NIST-2003 and NIST-2005 Chinese-English transla-
tion tasks show that our methods significantly out-
perform the forest-based tree to string and previous 
tree to tree models as well as the phrase-based model.  
The remaining of the paper is organized as fol-
lowing. Section 2 reviews the related work. In sec-
tion 3 and section 4, we discuss the proposed for-
est-based rule extraction (non-isomorphic mapping) 
and decoding algorithms (target syntax information 
usage). Finally we report the experimental results in 
section 5 and conclude the paper in section 6. 
2 Related Work 
Much effort has been done in the syntax-based trans-
lation modeling. Yamada and Knight (2001) propose 
                                                          
1 A packed forest is a compact representation of a set of trees 
with sharing substructures; formally, it is defined as a triple a 
triple? ?, ?, ? ?, where ? is non-terminal node set, ? is hy-
per-edge set and ? is leaf node set (i.e. all sentence words). 
Every node in ? covers a consecutive sequence of leaf, every 
hyper-edge in ? connect the father node to its children nodes as 
in a tree. Figure 8 is a packed forest contains two trees. 
a string to tree model. Galley et al (2004) propose 
the GHKM scheme to model the string-to-tree map-
ping. Liu et al (2006) propose a tree-to-string trans-
lation model. Liu et al (2007) propose the tree se-
quence to string model to capture rules covered by 
continuous sequence of trees. Shieber (2007), De-
Neefe and Knight (2009) and Carreras and Collins 
(2009) propose synchronous tree adjoin grammar to 
capture more tree-string mapping beyond the GHKM 
scheme. Zhang et al (2009a) propose the concept of 
virtual node to reform a tree sequence as a tree, and 
design efficient algorithms for tree sequence model 
in forest context. All these works only consider either 
the source side or the target side syntax information. 
To capture both side syntax contexts, Eisner (2003) 
studies the bilingual dependency tree-to-tree map-
ping in conceptual level. Zhang et al (2008) propose 
tree sequence-based tree-to-tree modeling. Liu et al 
(2009) propose efficient algorithms for tree-to-tree 
model in the forest-based training and decoding 
scheme. One common limitation of the above works 
is they only allow the one-to-one mapping between 
each non-terminal frontier node, and thus they suffer 
from the issue of rule coverage. On the other hand, 
due to the data sparseness issue and model coverage 
issue, previous tree-to-tree (Zhang et al, 2008; Liu et 
al., 2009) decoder has to rely solely on the span in-
formation or source side information to combine the 
target syntactic structures, without checking the 
compatibility of the merging nodes, in order not to 
fail many translation paths. Thus, this solution fails 
to effectively utilize the target structure information. 
To address this issue, tree sequence (Liu et al, 
2007; Zhang et al, 2008) and virtual node (Zhang et 
al., 2009a) are two concepts with promising results 
reported. In this paper, with the help of these two 
concepts, we propose a novel framework to solve the 
one-to-many non-isomorphic mapping issue. In addi-
tion, our proposed solution of using target syntax 
information enables our forest-based tree-to-tree se-
quence translation decoding algorithm to not only 
capture bilingual forest information but also have 
almost the same complexity as forest-based 
tree-to-string translation. This reduces the time/space 
complexity exponentially. 
3 Tree to Tree Sequence Rules 
The motivation of introducing tree to tree sequence 
rules is to add target syntax information to 
tree-to-string rules. Following, we first briefly review 
the definition of tree-to-string rules, and then de-
scribe the tree-to-tree sequence rules. 
441
3.1 Tree to String Rules 
VP
ADVP
AD
VP
VV
??
(try hard to)
??
(study)
try to studyhard  
    
Fig. 1. A word-aligned sentence pair with source tree 
 
 
   
   Fig. 2 Examples of tree to string rules 
 
Fig. 2 illustrates the examples of tree to string rules 
extracted from Fig. 1. The tree-to-string rule is very 
simple. Its source side is a sub-tree of source parse 
tree and its target side is a string with only one varia-
ble/non-terminal X. The source side and the target 
side is translation of each other with the constraint of 
word alignments. Please note that there is no any 
target syntactic or linguistic information used in the 
tree-to-string model. 
3.2 Tree to Tree Sequence Rules 
It is more challenging when extracting rules with 
target tree structure as constraint. Fig. 3 extends Fig. 
1 with target tree structure. The problem is that, giv-
en a source tree node, we are able to find its target 
string translation, but these target string may not 
form a linguistic sub-tree. For example, in Fig. 3, the 
source tree node ?ADVP? in solid eclipse is trans-
lated to ?try hard to? in the target sentence, but there 
is no corresponding sub-tree covering and only cov-
ering it in the target side.  
Given the example rules in Fig. 2, what are their 
corresponding rules with target syntax information? 
The answer is that the previous tree or tree se-
quence-based models fail to model the Rule 1 and 
Rule 2 at Fig. 2, since at frontier node level they only 
allow one-to-one node mapping but the solution is 
one-to-many non-terminal frontier node mapping. 
The concept of ?virtual node? (Zhang et al 2009a) is 
a solution to this issue. To facilitate discussion, we 
first introduce three concepts. 
 
 
 
Fig. 3. A word-aligned bi-parsed tree 
 
 
Fig. 4. A restructured tree with a virtual span root 
 
? Def. 1. The ?node sequence? is a sequence of 
nodes (either leaf or internal nodes) covering a 
consecutive span. For example, in Fig 3, ?VBP 
RB TO? and ?VBP ADVP TO? are two ?node 
sequence? covering the same span ?try hard to?. 
 
442
? Def. 2. The ?root node sequence? of a span is 
such a node sequence that any node in this se-
quence could not be a child of a node in other 
node sequence of the span. Intuitively, the ?root 
node sequence? of a span is the node sequence 
with the highest topology level. For example, 
?VBP ADVP TO? is the ?root node sequence? 
of the span of ?try hard to?. It is easy to prove 
that given any span, there exist one and only one 
?root node sequence?. 
 
? Def. 3. The ?span root? of a span is such a node 
that if the ?root node sequence? contains only 
one tree node, then the ?span root? is this tree 
node; otherwise, the ?span root? is the virtual 
father node (Zhang et al, 2009a) of the ?root 
node sequence?. Fig. 4 illustrates the reformed 
Fig. 3 by introducing the virtual node 
?VBP+ADVP+TO? as the ?span root? of the 
span of ?try hard to?. 
 
 
The ?span root? facilitates us to extract rules with 
target side structure information. Given a sub-tree of 
the source tree, we have a set of non-terminal frontier 
nodes. For each such frontier node, we can find its 
corresponding target ?span root?. If the ?span root? 
is a virtual node, then we add it into the target tree as 
a virtual segmentation joint point. After adding the 
?span root? as joint point, we are able to ensure that 
each frontier source node has only one corresponding 
target node, then we can use any traditional rule ex-
traction algorithm to extract rules, including those 
rules with one-to-many non-terminal frontier map-
pings. 
 
 
Fig. 5. Tree-to-tree sequence rules 
Fig. 5 lists the corresponding rules with target 
structure information of the tree-to-string rules in Fig 
2. All the three rules cannot be extracted by previous 
tree-to-tree mapping methods (Liu et al, 2009). The 
previous tree-sequence-based methods (Zhang et al, 
2008; Zhang et al, 2009a) can extracted rule 3 since 
they allow one-to-many mapping in root node level. 
But they cannot extract rule 1 and rule 2. Therefore, 
for any tree-to-string rule, our method can always 
find the corresponding tree-to-tree sequence rule. As 
a result, our rule coverage is the same as 
tree-to-string framework while our rules contain 
more informative target syntax information. Later we 
will show that using our decoding algorithm the 
tree-to-tree sequence search space is exponentially 
reduced to the same as tree-to-string search space. 
That is to say, we do not need to worry about the ex-
ponential search space issue of tree-to-tree sequence 
model existing in previous work. 
3.3 Rule Extraction in Tree Context 
Given a word aligned tree pair, we first extract the 
set of minimum tree to string rules (Galley et al 
2004), then for each tree-to-string rule, we can easily 
extract its corresponding tree-to-tree sequence rule 
by introducing the virtual span root node. After that, 
we generate the composite rules by iteratively com-
bining small rules.  
 
    
 
Fig. 6. Rule combination and virtual node removing 
443
  Please note that in generating composite rules, if 
the joint node is a virtual node, we have to recover 
the original link and remove this virtual node to 
avoid unnecessary ambiguity. Fig. 6 illustrates the 
combination process of rule 2 and rule 3 in Fig. 5. As 
a result, all of our extract rules do not contain any 
internal virtual nodes. 
3.4 Rule Extraction in Forest Context 
In forest pair context, we also first generate the 
minimum tree-to-string rule set as Mi et al (2008), 
and for each tree-to-string rule, we find its corres-
ponding tree-to-tree sequence rules, and then do rule 
composition. 
In tree pair context, given a tree-to-string rule, 
there is one and only one corresponding tree-to-tree 
sequence rule. But in forest pair context, given one 
such tree-to-string rule, there are many correspond-
ing tree-to-tree sequence rules. All these sub-trees 
form one or more sub-forests2 of the entire big target 
forest. If we can identify the sub-forests, i.e., all of 
the hyper-edges of the sub-forests, we can retrieve all 
the sub-trees from the sub-forests as the target sides 
of the corresponding tree-to-tree sequence rules. 
Given a source sub-tree, we can obtain the target 
root span where the target sub-forests start and the 
frontier spans where the target sub-forests stop. To 
indentify all the hyper-edges in the sub-forests, we 
start from every node covering the root span, traverse 
from top to down, mark all the hyper-edges visited 
and stop at the node if its span is a sub-span of one of 
the forest frontier spans or if it is a word node. The 
reason we stop at the node once it fell into a frontier 
span (i.e. the span of the node is a sub-span of the 
frontier span) is to guarantee that given any frontier 
span, we could stop at the ?root node sequence? of 
this span by Def. 2. 
For example, Fig. 7 is a source sub-tree of rule 2 
in Fig. 5 and the circled part in Fig. 8 is one of its 
corresponding target sub-forests. Its corresponding 
target root span is [1,4] (corresponding to source root 
?VP? ) and its corresponding target frontier span is 
{[1,3], study[4,4]}. Now given the target forest, we 
start from node VP[1,4] and traverse from top to 
down, finally stop at following nodes: VBP[1,1], 
ADVP[2,2], TO[3,3], study .  
                                                          
2 All the sub-forests cover the same span. But their roots have 
different grammar tags as the roots? names. The root may be a 
virtual span root node in the case of the one-to-many frontier 
non-terminal node mappings. 
Please note that the starting root node must be a 
single node, being either a normal forest node or a 
virtual ?span root? node. The virtual ?span root? 
node serves as the frontier node of upper rules and 
root node of the currently being extracted rules. Be-
cause we extract rules in a top-to-down manner, the 
necessary virtual ?span root? node for current 
sub-forest has already been added into the global 
forest when extracting upper level rules. 
 
 
 
Figure 7. A source sub-tree in rule 2 
 
 
 
Fig. 8. The corresponding target sub-forest for the tree of 
Figure 7. 
3.5 Fractional Count of Rule 
Following Mi and Huang (2008) and Liu et al 
(2009), we assign a fractional count to a rule to 
measure how likely it appears given the context of 
the forest pair. In following equation, ?S? means 
source sub-tree, ?T? means target sub-tree, ?SF? is 
source forest and ?TF? is the target forest. 
 
 
???, ? |??, ??? ? ???|??, ??? ? ???|??, ???
? ???|??? ? ???|??? 
 
444
The above equation means the fractional count of 
a source-target tree pair is just the product of each of 
their fractional count in corresponding forest context 
in following equation. 
 
????????| ???????? ? ??????????????????? ?????? 
? ?????? ???????? ? ? ????????????? ? ? ????????????????????????????? ??????  
 
 
where ? and ? are the outside and inside probabil-
ities. In addition, if a sub-tree root is a virtual node 
(formed by a root node sequence), then we use fol-
lowing equation to approximate the outside probabil-
ity of the virtual node. 
 
????? ???????? ???? ? ?? ????
? ? ??
# ?? ????? ?? ??
 
4 Decoding 
4.1 Traditional Forest-based Decoding 
A typical translation process of a forest-based system 
is to first convert the source packed forest into a tar-
get translation forest, and then apply search algo-
rithm to find the best translation result from this tar-
get translation forest (Mi et al, 2008).  
For the tree-to-string model, the forest conversion 
process is as following: given an input packed forest, 
we do pattern matching (Zhang et al, 2009b) with 
the source side structures in the rule set. For each 
matched rule, we establish its target side as a hy-
per-edge in the target forest.   
 
 
 
Fig. 9. A forest conversion step in a tree to string model 
 
Fig. 9 exemplifies a conversion step in the tree to 
string model. A sub-tree structure with two hy-
per-edge ?VP[2,4] => ADVP[2,2] VP[3,4]? and 
?VP[3,4] => ADVP[3,4] VP[4,4]? is converted into 
a target hyper-edge ?X-VP[2,4] => X-ADVP[3,3] 
X-ADVP[2,2]  X-VP[4,4] ?.  The node ?X-VP[4,4]? 
in the target forest means that its syntactic label in 
target forest is ?X? and it is translated from the 
source node ?VP[4,4]? in the source forest. In this 
target hyper-edge, ?X-ADVP[3,3] X-ADVP[2,2]? 
means the translation from source node ?ADVP[3,3]? 
is put before the translation from ?ADVP[2,2]?, 
representing a structure reordering. 
4.2 Toward Bilingual Syntax-aware Trans-
lation Generation 
As we could see in section 4.1, there is only one kind 
of non-terminal symbol ?X? in the target side. It is a 
big challenge to rely on such a coarse label to gener-
ate a translation with fine syntactic quality. For ex-
ample, a source node may be translated into a ?NP? 
(noun phrase) in target side. However, in this rule set 
with the only symbol ?X?, it may be merged with 
upper structure as a ?VP? (verb phrase) instead, be-
cause there is no way to favor one over another. In 
this case, the target tree does not well model the 
translation syntactically. In addition, all of the inter-
nal structure information in the target side is ignored 
by the tree-to-string rules. 
One natural solution to the above issue is to use 
the tree to tree/tree sequence model, which have 
richer target syntax structures for more discrimina-
tive probability and finer labels to guide the combi-
nation process. However, the tree to tree/tree se-
quence model may face very severe computational 
problem and so-called ?spurious ambiguities? issue.  
Theoretically, if in the tree-to-tree sequence mod-
el-based decoding, we just give a penalty to the in-
compatible-node combinations instead of pruning out 
the translation paths, then the set of sentences gener-
ated by the tree-to-tree sequence model is identical to 
that of the tree-to-string model since every 
tree-to-tree sequence rule can be projected into a 
tree-to-string rule. Motivated by this, we propose a 
solution call parallel hypothesis spaces searching to 
solve the computational and ?spurious ambiguities? 
issues mentioned above. In the meanwhile, we can 
fully utilize the target structure information to guide 
translation.  
We restructure the tree-to-tree sequence rule set by 
grouping all the rules according to their correspond-
ing tree-to-string rules. This behaves like a 
?tree-to-forest? rule. The ?forest? encodes all the tree 
sequences with same corresponding string. With the 
re-constructed rule set, during decoding, we generate 
two target translation hypothesis spaces (in the form 
of packed forests) synchronously by the tree-to-string 
445
rules and tree-to-tree sequence rules, and maintain 
the projection between them. In other words, we 
generate hypothesis (searching) from the 
tree-to-string forest and calculate the probability 
(evaluating syntax goodness) for each hypothesis by 
the hyper-edges in the tree-to-tree sequence forest.  
4.3 Parallel Hypothesis Spaces 
 
 
Fig. 10. Mapping from tree-to-tree sequence into 
tree-to-string rule 
 
In this subsection, we describe what the parallel 
search spaces are and how to construct them. As 
shown at Fig. 10, given a tree-to-tree sequence rule, 
it is easy to find its corresponding tree-to-string rule 
by simply ignoring the target inside structure and 
renaming the root and leaves non-terminal labels into 
?X?. We iterate through the tree-to-tree sequence rule 
set, find its corresponding tree-to-string rule and then 
group those rules with the same tree-to-string projec-
tion. After that, the original tree-to-tree sequence rule 
set becomes a set of smaller rule sets. Each of them is 
indexed by a unique tree-to-string rule.  
We apply the tree-to-string rules to generate an 
explicit target translation forest to represent the target 
sentences space. At the same time, whenever a 
tree-to-string rule is applied, we also retrieve its cor-
responding tree-to-tree sequence rule set and gener-
ate a set of latent hyper-edges with fine-grained syn-
tax information. In this case, we have two parallel 
forests, one with coarse explicit hyper-edges and the 
other fine and latent. Given a hyper-edge (or a node) 
in the coarse forest, there are a group of correspond-
ing latent hyper-edges (or nodes) with finer syntax 
labels in the fine forest. Accordingly, given a tree in 
the coarse forest, there is a corresponding sub-forest 
in the latent fine forest. We can view the latent fine 
forest as imbedded inside the explicit coarse forest. If 
an explicit hyper-edge is viewed as a big cable, then 
the group of its corresponding latent hyper-edges is 
the small wires inside it. 
We rely on the explicit hyper-edges to enumerate 
possible hypothesis while using the latent hy-
per-edges to measure its translation probability and 
syntax goodness. Thus, the complexity of the search 
space is reduced into the tree-to-string model level, 
while keeping the target language generation syntac-
tic aware. More importantly, we thoroughly avoid 
those spurious ambiguities introduced by the 
tree-to-tree sequence rules. 
4.4 Decoding with Parallel Hypothesis 
Spaces 
 
 
 
Fig. 11. Derivation path and derivation forest 
 
In this subsection, we show exactly how our decoder 
finds the best result from the parallel spaces. We 
generate hypothesis by traversing the coarse forest in 
the parallel spaces with cube-pruning (Huang and 
Chiang, 2007). Given a newly generated hypothesis, 
it is affiliated with a derivation path (tree) in the 
coarse forest and a group of derivation paths 
(sub-forest) in the finer forest. As shown in Fig. 11, 
the left part is the derivation path formed by a coarse 
hyper-edge, consisting the newly-generated sub-tree 
?X => X X X? connecting with three previous-
ly-generated sub paths while the right part is the de-
rivation forest formed by newly-generated finer hy-
per-edges rooted at ?VP? and ?S?, and previous-
ly-generated sub-forests.  
In this paper, we use the sum of probabilities of all 
the derivation paths in the finer forest to measure the 
quality of the candidate translation suggested by the 
hypothesis. From Fig. 11, we can see there may be 
more than one corresponding finer forests, it is easy 
to understand that the sum of all the trees? probabili-
ties in these finer forests is equal to the sum of the 
inside probability of all these root nodes of these fin-
er forests. We adopt the dynamic programming to 
compute the probability of the finer forest: whenever 
we generate a new hypothesis by concatenating a 
446
coarse hyper-edge and its sub-path, we find its cor-
responding finer hyper-edges and sub-forests, do the 
combination and accumulate probabilities from bot-
tom to up. For the coarse hyper-edge, because there 
is only one label ?X?, any sub-path could be easily 
concatenated with upper structure covering the same 
sub-span without the need of checking label compa-
tibility. While for the finer hyper-edges, we only link 
the root nodes of sub-forests to upper hyper-edges 
with the same linking node label. This is to guarantee 
syntactic goodness. In case there are some leaf nodes 
of the upper hyper-edges fail to find corresponding 
sub-forest roots with the same label (e.g. the ?NP? in 
red color in the rightmost of Fig 11), we simply link 
it into the nodes with the least inside probability 
(among these sub-forests), and at the same time give 
a penalty score to this combination. If some root 
nodes of some sub-forest still cannot find upper leaf 
nodes to concatenate (e.g. the ?CP? in red color in 
Fig. 11), we simply ignore them. After the combina-
tion process, it is straightforward to accumulate the 
inside probability dynamically from bottom up. 
5 Experiment 
5.1 Experimental Settings 
We evaluate our method on the Chinese-English 
translation task. We first carry out a series empirical 
study on a set of parallel data with 30K sentence 
pairs, and then do experiment on a larger data set to 
ensure that the effectiveness of our method is consis-
tent across data set of different size. We use the 
NIST 2002 test set as our dev set, and NIST 2003 
and NIST 2005 test sets as our test set. A 3-gram 
language model is trained on the target side of the 
training data by the SRILM Toolkits (Stolcke, 2002) 
with modified Kneser-Ney smoothing (Kneser and 
Ney, 1995). We train Charniak?s parser (Charniak, 
2000) on CTB5.0 for Chinese and ETB3.0 for Eng-
lish and modify it to output packed forest. GIZA++ 
(Och and Ney, 2003) and the heuristics 
?grow-diag-final-and? are used to generate m-to-n 
word alignments. For the MER training (Och, 2003), 
Koehn?s MER trainer (Koehn, 2007) is modified for 
our system. For significance test, we use Zhang et 
al.?s implementation (Zhang et al 2004). Our evalu-
ation metrics is case-sensitive closest BLEU-4 (Pa-
pineni et al, 2002). We use following features in our 
systems: 1) bidirectional tree-to-tree sequence proba-
bility, 2) bidirectional tree-to-string probability, 3) 
bidirectional lexical translation probability, 4) target 
language model, 5) source tree probability 6) the av-
erage number of unmatched nodes in the target forest. 
7) the length of the target translation, 8) the number 
of glue rules used. 
5.2 Empirical Study on Small Data 
We set forest pruning threshold (Mi et al, 2008) to 8 
on both source and target forests for rule extraction. 
For each source sub-tree, we set its height up to 3, 
width up to 7 and extract up to 10-best target struc-
tures. In decoding, we set the pruning threshold to 10 
for the input source forest. Table 1 compares the 
performance in NIST 2003 data set of our method 
and several state-of-the-art systems as our baseline. 
 
1) MOSES: phrase-based system (Koehn et al, 
2007) 
2) FT2S: forest-based tree-to-string system (Mi 
and Huang, 2008; Mi et al, 2008) 
3) FT2T: forest-based tree-to-tree system (Liu et 
al., 2009).  
4) FT2TS (1to1): our forest-based tree-to-tree 
sequence system, where 1to1 means only 
one-to-one frontier non-terminal node map-
ping is allowed, thus the system does not fol-
low our non-isomorphic mapping framework.  
5) FT2TS (1toN): our forest-based tree-to-tree 
sequence system that allows one-to-many 
frontier non-terminal node mapping by fol-
lowing our non-isomorphic mapping frame-
work   
 
In addition, our proposed parallel searching space 
(PSS) technology can be applied to both tree to tree 
and tree to sequence systems. Thus in table 1, for the 
tree-to-tree/tree sequence systems, we report two 
BLEU scores, one uses this technology (withPSS) 
and one does not (noPSS). 
 
Model BLEU-4 
MOSES 23.39 
FT2S 26.10 
FT2T noPSS 23.40 withPSS 24.46 
FT2TS (1to1) noPSS 25.39 withPSS 26.58 
FT2TS (1toN) noPSS 26.30 withPSS 27.70 
 
Table 1. Performance comparison of different methods 
 
 From Table 1, we can see that:  
447
1) All the syntax-based systems (except FT2T 
(noPSS) (23.40)) consistently outperform the 
phrase-based system MOSES significantly 
(? ? 0.01 ), indicating that syntactic know-
ledge is very useful to SMT. 
2) The PSS technology shows significant perfor-
mance improvement ?? ? 0.01? in all mod-
els, which clearly shows effectiveness of the 
PSS technology in utilizing target structures 
for target language generation.  
3) FT2TS (1toN) significantly outperforms 
(? ? 0.01) FT2TS (1to1) in both cases (noPSS 
and withPSS). This convincingly shows the 
effectiveness of our non-isomorphic mapping 
framework in capturing the non-isomorphic 
structure translation equivalences. 
4) Both FT2TS systems significantly outperform 
FT2T( ? ? 0.01). This verifies the effective-
ness of tree sequence rules. 
5) FT2TS shows different level of performance 
improvements over FT2S with the best case 
having 1.6 (27.70-26.10) BLEU score im-
provement over FT2S. This suggests that the 
target structure information is very useful, but 
we need to find a correct way to effectively 
utilize it. 
 
1to1 1toN ratio 
1735871 2363771 1:1.36 
 
Table 2. Statistics on node mapping in forest, where 
?1to1? means the number of nodes in source forest 
that can be translated into one node in target forest 
and ?1toN? means the number of nodes in source 
forest that have to be translated into more than one 
node in target forest, where the node refers to 
non-terminal nodes only 
 
Model # of rules T2S covered 
FT2T 295732 26.8% 
FT2TS(1to1) 631487 57.1% 
FT2TS (1toN) 1945168 100% 
 
Table 3. Statistics of rule coverage, where ?T2S 
covered? means the percentage of tree-to-string 
rules that can be covered by the model 
 
Table 2 studies the node isomorphism between bi-
lingual forest pair. We can see that the 
non-isomorphic node translation mapping (1toN) 
accounts for 57.6% (=1.36/(1+1.36)) of all the forest 
non-terminal nodes with target translation. This 
means that the one-to-many node mapping is a major 
issue in structure transformation. It also empirically 
justifies the importance of our non-isomorphic map-
ping framework.  
Table 3 shows the rule coverage of different bi-
lingual structure mapping model. FT2T only covers 
26.8% tree-to-string rules, so it performs worse than 
FT2S as shown in Table 1. FT2TS (1to1) does not 
allow one-to-many frontier node mapping, so it could 
only recover the non-isomorphic node mapping in 
the root level, while FT2TS (1toN) could make it at 
both root and leaf levels. Therefore, it is not surpris-
ing that in Table 3, FT2TS (1toN) cover many more 
rules than FT2TS (1to1) because given a source tree, 
there are many leaves, if any one of them is 
non-isomorphic, then it could not be covered by the 
FT2TS (1to1).  
 
Decoding Method BLEU-4 Speed (sec/sent)
Traditional: 
FT2TS (1toN) (noPPS) 26.30 152.6 
Ours: 
FT2TS (1toN) (withPPS) 27.70 5.22 
 
Table 4. Performance and speed comparison  
 
Table 4 clearly shows the advantage of our decod-
er over the traditional one. Ours could not only gen-
erate better translation result, but also be 
152.6/5.22>30 times faster. This mainly attributes to 
two reasons: 1) one-to-many frontier node mapping 
equipments the model with more ability to capture 
more non-isomorphic structure mappings than tradi-
tional models, and 2) ?parallel search space? enables 
the decoder to fully utilize target syntactic informa-
tion, but keeping the size of search space the same as 
that a ?tree to string? model explores. 
5.3 Results on Larger Data Set 
We also carry out experiment on a larger dataset 
consisting of the small dataset used in last section 
and the FBIS corpus. In total, there are 280K parallel 
sentence pairs with 9.3M Chinese words and 11.8M 
English words. A 3-gram language model is trained 
on the target side of the parallel corpus and the GI-
GA3 Xinhua portion. We compare our system 
(FT2TS with 1toN and withPPS) with two 
state-of-the-art baselines: the phrase-based system 
MOSES and the forest-based tree-to-string system 
448
implemented by us. Table 5 clearly shows the effec-
tiveness of our method is consistent across small and 
larger corpora, outperforming FT2S by 1.6-1.8 
BLEU and the MOSES by 3.3-4.0 BLEU statistically 
significantly (p<0.01). 
 
Model BLEU 
NIST2003 NIST2005 
MOSES 29.51 27.53 
FT2S 31.21 29.72 
FT2TS 32.88 31.50 
   
Table 5. Performance on larger data set 
6 Conclusions 
In this paper, we propose a framework to address the 
issue of bilingual non-isomorphic structure mapping 
and a novel parallel searching space scheme to effec-
tively utilize target syntactic structure information in 
the context of forest-based tree to tree sequence ma-
chine translation. Based on this framework, we de-
sign an efficient algorithm to extract tree-to-tree se-
quence translation rules from word aligned bilingual 
forest pairs. We also elaborate the parallel searching 
space-based decoding algorithm and the node label 
checking scheme, which leads to very efficient de-
coding speed as fast as the forest-based tree-to-string 
model does, at the same time is able to utilize infor-
mative target structure knowledge. We evaluate our 
methods on both small and large training data sets 
and two NIST test sets. Experimental results show 
our methods statistically significantly outperform the 
state-of-the-art models across different size of cor-
pora and different test sets. In the future, we are in-
terested in testing our algorithm at forest-based tree 
sequence to tree sequence translation. 
References  
Eugene Charniak. 2000. A maximum-entropy inspired 
parser. NAACL-00. 
Eugene Charniak, Kevin Knight, and Kenji Yamada. 2003. 
Syntax-based language models for statistical machine 
translation. MT Summit IX. 40?46. 
David Chiang. 2007. Hierarchical phrase-based transla-
tion.Computational Linguistics, 33(2). 
Steve DeNeefe, Kevin Knight. 2009. Synchronous Tree 
Adjoining Machine Translation. EMNLP-2009. 
727-736. 
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for MT. ACL-03 (companion volume). 
Michel Galley, Mark Hopkins, Kevin Knight and Daniel 
Marcu. 2004. What?s in a translation rule? 
HLT-NAACL-04. 273-280. 
Liang Huang. 2008. Forest Reranking: Discriminative 
Parsing with Non-Local Features. ACL-HLT-08. 
586-594 
Liang Huang and David Chiang. 2005. Better k-best Pars-
ing. IWPT-05. 53-64 
Liang Huang and David Chiang. 2007. Forest rescoring: 
Faster decoding with integrated language models. 
ACL-07. 144?151 
Dan Klein and Christopher D. Manning. 2001. Parsing 
and Hypergraphs. IWPT-2001. 
Reinhard Kneser and Hermann Ney. 1995. Improved 
backing-off for M-gram language modeling. 
ICASSP-95, 181-184 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Cal-
lison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran, Richard 
Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin 
and Evan Herbst. 2007. Moses: Open Source Toolkit for 
Statistical Machine Translation. ACL-07. 177-180. 
(poster) 
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-String 
Alignment Template for Statistical Machine Transla-
tion. COLING-ACL-06. 609-616. 
Yang Liu, Yun Huang, Qun Liu and Shouxun Lin. 2007. 
Forest-to-String Statistical Translation Rules. ACL-07. 
704-711. 
Yang Liu, Yajuan L?, Qun Liu. 2009. Improving 
Tree-to-Tree Translation with Packed Forests. ACL-09. 
558-566 
Haitao Mi, Liang Huang, and Qun Liu. 2008. For-
est-based translation. ACL-HLT-08. 192-199. 
Haitao Mi and Liang Huang. 2008. Forest-based Transla-
tion Rule Extraction. EMNLP-08. 206-214. 
Franz J. Och. 2003. Minimum error rate training in statis-
tical machine translation. ACL-03. 160-167. 
Franz Josef Och and Hermann Ney. 2003. A Systematic 
Comparison of Various Statistical Alignment Models. 
Computational Linguistics. 29(1) 19-51.  
Kishore Papineni, Salim Roukos, Todd Ward and 
Wei-Jing Zhu. 2002. BLEU: a method for automatic 
evaluation of machine translation. ACL-02. 311-318. 
Andreas Stolcke. 2002. SRILM - an extensible language 
modeling toolkit. ICSLP-02. 901-904. 
Masaru Tomita. 1987. An Efficient Aug-
mented-Context-Free Parsing Algorithm. Computation-
al Linguistics 13(1-2): 31-46 
449
Xavier Carreras and Michael Collins. 2009. 
Non-projective Parsing for Statistical Machine Trans-
lation. EMNLP-2009. 200-209. 
K. Yamada and K. Knight. 2001. A Syntax-Based Statis-
tical Translation Model. ACL-01. 523-530. 
Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw and Chew 
Lim Tan. 2009a. Forest-based Tree Sequence to String 
Translation Model. ACL-IJCNLP-09. 172-180. 
Hui Zhang, Min Zhang, Haizhou Li, and Chew Lim Tan. 
2009b. Fast Translation Rule Matching for Syn-
tax-based Statistical Machine Translation. EMNLP-09. 
1037-1045. 
Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Chew Lim 
Tan and Sheng Li. 2007. A Tree-to-Tree Align-
ment-based model for statistical Machine translation. 
MT-Summit-07. 535-542 
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew 
Lim Tan, Sheng Li. 2008. A Tree Sequence Align-
ment-based Tree-to-Tree Translation Model. 
ACL-HLT-08. 559-567. 
Ying Zhang, Stephan Vogel, Alex Waibel. 2004. Inter-
preting BLEU/NIST scores: How much improvement do 
we need to have a better system? LREC-04. 2051-2054. 
450
Proceedings of NAACL-HLT 2013, pages 12?21,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Beyond Left-to-Right: Multiple Decomposition Structures for SMT
Hui Zhang?
USC/ISI
Los Angeles, CA 90089
hzhang@isi.edu
Kristina Toutanova
Microsoft Research
Redmond, WA 98502
kristout@microsoft.com
Chris Quirk
Microsoft Research
Redmond, WA 98502
chrisq@microsoft.com
Jianfeng Gao
Microsoft Research
Redmond, WA 98502
jfgao@microsoft.com
Abstract
Standard phrase-based translation models do
not explicitly model context dependence be-
tween translation units. As a result, they rely
on large phrase pairs and target language mod-
els to recover contextual effects in translation.
In this work, we explore n-gram models over
Minimal Translation Units (MTUs) to explic-
itly capture contextual dependencies across
phrase boundaries in the channel model. As
there is no single best direction in which con-
textual information should flow, we explore
multiple decomposition structures as well as
dynamic bidirectional decomposition. The
resulting models are evaluated in an intrin-
sic task of lexical selection for MT as well
as a full MT system, through n-best rerank-
ing. These experiments demonstrate that ad-
ditional contextual modeling does indeed ben-
efit a phrase-based system and that the direc-
tion of conditioning is important. Integrating
multiple conditioning orders provides consis-
tent benefit, and the most important directions
differ by language pair.
1 Introduction
The translation procedure of a classical phrase-
based translation model (Koehn et al, 2003) first di-
vides the input sentence into a sequence of phrases,
translates each phrase, explores reorderings of these
translations, and then scores the resulting candi-
dates with a linear combination of models. Conven-
tional models include phrase-based channel models
that effectively model each phrase as a large uni-
gram, reordering models, and target language mod-
els. Of these models, only the target language model
?This research was conducted during the author?s internship
at Microsoft Research
(and, to some weak extent, the lexicalized reordering
model) captures some lexical dependencies that span
phrase boundaries, though it is not able to model in-
formation from the source side. Larger phrases cap-
ture more contextual dependencies within a phrase,
but individual phrases are still translated almost in-
dependently.
To address this limitation, several researchers
have proposed bilingual n-gram Markov models
(Marino et al, 2006) to capture contextual depen-
dencies between phrase pairs. Much of their work
is limited by the requirement ?that the source and
target side of a tuple of words are synchronized, i.e.
that they occur in the same order in their respective
languages? (Crego and Yvon, 2010).
For language pairs with significant typological di-
vergences, such as Chinese-English, it is quite dif-
ficult to extract a synchronized sequence of units;
in the limit, the smallest synchronized unit may be
the whole sentence. Other approaches explore incor-
poration into syntax-based MT systems or replacing
the phrasal translation system altogether.
We investigate the addition of MTUs to a phrasal
translation system to improve modeling of con-
text and to provide more robust estimation of long
phrases. However, in a phrase-based system there
is no single synchronized traversal order; instead,
we may consider the translation units in many pos-
sible orders: left-to-right or right-to-left according
to either the source or the target are natural choices.
Alternatively we consider translating a particularly
unambiguous unit in the middle of the sentence
and building outwards from there. We investigate
both consistent and dynamic decomposition orders
in several language pairs, looking at distinct orders
in isolation and combination.
12
2 Related work
Marino et al (2006) proposed a translation model
using a Markov model of bilingual n-grams, demon-
strating state-of-the-art performance compared to
conventional phrase-based models. Crego and
Yvon (2010) further explored factorized n-gram ap-
proaches, though both models considered rather
large n-grams; this paper focuses on small units with
asynchronous orders in source and target. Durrani
et al (2011) developed a joint model that captures
translation of contiguous and gapped units as well as
reordering. Two prior approaches explored similar
models in syntax based systems. MTUs have been
used in dependency translation models (Quirk and
Menezes, 2006) to augment syntax directed trans-
lation systems. Likewise in target language syntax
systems, one can consider Markov models over min-
imal rules, where the translation probability of each
rule is adjusted to include context information from
parent rules (Vaswani et al, 2011).
Most prior work tends to replace the existing
probabilities rather than augmenting them. We be-
lieve that Markov rules provide an additional sig-
nal but are not a replacement. Their distributions
should be more informative than the so-called ?lex-
ical weighting? models, and less sparse than rela-
tive frequency estimates, though potentially not as
effective for truly non-compositional units. There-
fore, we explore the inclusion of all such informa-
tion. Also, unlike prior work, we explore combina-
tions of multiple decomposition orders, as well as
dynamic decompositions. The most useful context
for translation differs by language pair, an important
finding when working with many language pairs.
We build upon a standard phrase-based approach
(Koehn et al, 2003). This acts as a proposal dis-
tribution for translations; the MTU Markov models
provide additional signal as to which translations are
correct.
3 MTU n-gram Markov models
We begin by defining Minimal Translation Units
(MTUs) and describing how to identify them in
word-aligned text. Next we define n-gram Markov
models over MTUs, which requires us to define
traversal orders over MTUs.
?Yu ??ZuoTian ??JuXing
held the meeting
??HuiTan
yesterdaynull
null
M1 M2 M3 M5M4
M1: Yu => null                               M2: ZuoTian => yesterdayM3:                                  JuXing => heldM4:                                       null => theM5:                                 HuiTan => meeting
?Yu ??ZuoTian ??JuXing
? ? ?
??HuiTan
??
null
Figure 1: Word alignment and minimum translation units.
3.1 Definition of an MTU
Informally, the notion of a minimal translation unit
is simple: it is a translation rule that cannot be
broken down any further without violating the con-
straints of the rules. We restrict ourselves to contigu-
ous MTUs. They are similar to small phrase pairs,
though unlike phrase pairs we allow MTUs to have
either an empty source or empty target side, thereby
allowing insertion and deletion phrases. Conven-
tional phrase pairs may be viewed as compositions
of these MTUs up to a given size limit.
Consider a word-aligned sentence pair consisting
of a sequence of source words s = s1 . . . sm, a se-
quence of target words t = t1 . . . tn, and a word align-
ment relation between the source and target words
? ? {1..m} ? {1..n}. A translation unit is a sequence
of source words si..s j and a sequence of target words
tk..tl (one of which may be empty) such that for all
aligned pairs i? ? k?, we have i ? i? ? j if and only
if k ? k? ? l. This definition, nearly identical to
that of a phrase pair (Koehn et al, 2003), relaxes the
constraint that one aligned word must be present.
A set of translation units is a partition of the sen-
tence pair if each source and target word is covered
exactly once. Minimal translation units is the par-
tition with the smallest average unit size, or, equiv-
alently, the largest number of units. For example,
Figure 1 shows a word-aligned sentence pair and its
corresponding set of MTUs. We extract these min-
imal translation units with an algorithm similar to
that of phrase extraction.
We train n-gram Markov models only over min-
13
imal rules for two reasons. First, the segmentation
of the sentence pair is not unique under composed
rules, which makes probability estimation compli-
cated. Second, some phrase pairs are very large,
which results in sparse data issues and compromises
the model quality. Therefore, training an n-gram
model over minimal translation units turns out to
be a simple and clean choice: the resulting segmen-
tation is unique, and the distribution is smooth. If
we want to capture more context, we can simply in-
crease the order of the Markov model.
Such Markov models address issues in large
phrase-based translation approaches. Where stan-
dard phrase-based models rely upon large unigrams
to capture contextual information, n-grams of mini-
mal translation units allow a robust contextual model
that is less constrained by segmentation.
3.2 MTU enumeration orders
When defining a joint probability distribution over
MTUs of an aligned sentence pair, it is necessary
to define a decomposition, or generation order for
the sentence pair. For a single sequence in lan-
guage modeling or synchronized sequences in chan-
nel modeling, the default enumeration order has
been left-to-right.
Different decomposition orders have been used
in part-of-speech tagging and named entity recog-
nition (Tsuruoka and Tsujii, 2005). Intuitively, in-
formation from the left or right could be more use-
ful for particular disambiguation choices. Our re-
search on different decomposition orders was moti-
vated by this work. When applying such ideas to
machine translation, there are additional challenges
and opportunities. The task exhibits much more am-
biguity ? the number of possible MTUs is in the
millions. An opportunity arises from the reordering
phenomenon in machine translation: while in POS
tagging the natural decomposition orders to study
are only left-to-right and right-to-left, in machine
translation we can further distinguish source and tar-
get sentence orders.
We first define the source left-to-right and the tar-
get left-to-right orders of the aligned sets of MTUs.
The definition is straightforward when there are no
inserted or deleted word. To place the nulls corre-
sponding to such word we use the following defi-
nition: the source position of the null for a target
inserted word is just after the position of the last
source word aligned to the closest preceding non-
null aligned target word. The target position for a
null corresponding to a source deleted MTU is de-
fined analogously. In Figure 1 we define the posi-
tion of M4 to be right after M3 (because ?the? is
after ?held? in left-to-right order on the target side).
The complete MTU sequence in source left-to-
right order is M1-M2-M3-M4-M5. The sequence
in target left-to-right order is M3-M4-M5-M1-M2.
This illustrates that decomposition structure may
differ significantly depending on which language is
used to define the enumeration order.
Once a sentence pair is represented as a sequence
of MTUs, we can define the probability of the
sentence pair using a conventional n-gram Markov
model (MM) over MTUs. For example, the 3-gram
MM probability of the sentence pair in Figure 1
under the source left-to-right order is as follows:
P(M1)?P(M2|M1)?P(M3|M1,M2)?P(M4|M2,M3)?
P(M5|M3,M4).
Different decomposition orders use different con-
text for disambiguation and it is not clear apriori
which would perform best. We compare all four
decomposition orders (source order left-to-right and
right-to-left, and target order left-to-right and right-
to-left). Although the independence assumptions of
left-to-right and right-to-left are the same, the result-
ing models may be different due to smoothing.
In addition to studying these four basic decompo-
sition orders, we report performance of two cyclic
orders: cyclic in source or target sentence order.
These models are inspired by the cyclic depen-
dency network model proposed for POS tagging
(Toutanova et al, 2003) and also used as a baseline
in previous work on dynamic decomposition orders
(Tsuruoka and Tsujii, 2005). 1
The probability according to the cyclic orders is
defined by conditioning each MTU on both its left
and right neighbor MTUs. For example, the prob-
ability of the sentence pair in Figure 1 under the
source cyclic order, using a 3-gram model is defined
as: P(M1|M2) ? P(M2|M1,M3) ? P(M3|M2,M4) ?
P(M4|M3,M5) ? P(M5|M4).
All n-gram Markov models over MTUs are esti-
1The correct application of such models requires sampling
to find the highest scoring sequence, but we apply the max prod-
uct approximation as done in previous work.
14
mated using Kneser-Ney smoothing. Each MTU is
treated as an atomic unit in the vocabulary of the
n-gram model. Counts of all n-grams are obtained
from the parallel MT training data, using different
MTU enumeration orders.
Note that if we use a target-order decomposition,
the model provides a distribution over target sen-
tences and the corresponding source sides of MTUs,
albeit unordered. Likewise source order based mod-
els provide distributions over source sentences and
unordered target sides of MTUs. We attempted to
introduce reordering models to predict an order over
the resulting MTU sequences using approaches sim-
ilar to reordering models for phrases. Although
these models produced gains in some language pairs
when used without translation MTU MMs, there
were no additional gains over a model using mul-
tiple translation MTU MMs.
4 Lexical selection
We perform an empirical evaluation of different
MTU decomposition orders on a simplified machine
translation task: lexical selection. In this task we
assume that the source sentence segmentation into
minimal translation units is given and that the or-
der of the corresponding target sides of the minimal
translation units is also given. The problem is to
predict the target sides of the MTUs, called target
MTUs for brevity (see Figure 2). The lexical selec-
tion task is thus similar to sequence tagging tasks
like part-of-speech tagging, though much more dif-
ficult: the predicted variables are sequences of target
language words with millions of possible outcomes.
?Yu ??ZuoTian ??JuXing
held the meet ng
??HuiTan
yesterdaynull
null
M1 M2 M3 M5M4
M1: Yu => null                               M2: ZuoTian => yesterdayM3:                                  JuXing => heldM4:                                       null => theM5:                                 HuiTan => meeting
?Yu ??ZuoTian ??JuXing
? ? ?
??HuiTan
??
null
Figure 2: Lexical selection.
We use this constrained MT setting to evaluate the
performance of models using different MTU decom-
position orders and models using combinations of
decomposition orders. The simplified setting allows
controlled experimentation while lessening the im-
pact of complicating factors in a full machine trans-
lation setting (search error, reordering limits, phrase
table pruning, interaction with other models).
To perform the tagging task, we use trigram MTU
models. The four basic decomposition orders for
MTU Markov models we use are left-to-right in tar-
get sentence order, right-to-left in target sentence or-
der, left-to-right in source sentence order, and right-
to-left in source sentence order. We also consider
cyclic orders in source and target.
Regardless of the decomposition order used, we
perform decoding using a beam search decoder, sim-
ilar to ones used in phrase-based machine transla-
tion. The decoder builds target hypotheses in left-
to-right target sentence order. At each step, it fills in
the translation of the next source MTU, in the con-
text of the already predicted MTUs to its left. The
top scoring complete hypotheses covering the first m
MTUs are maintained in a beam. When scoring with
a target left-to-right MTU Markov model (L2RT),
we can score each partial hypothesis exactly at each
step. When scoring using a R2LT model or a source
order model, we use lower-order approximations to
the trigram MTU Markov model scores as future
scores, since not all needed context is available for a
hypothesis at the time of construction. As additional
context becomes available, the exact score can be
computed. 2
4.1 Basic decomposition order combinations
We first introduce two methods of combining differ-
ent decomposition orders: product and system com-
bination.
The product method arises naturally in the ma-
chine translation setting, where probabilities from
different models are multiplied together and further
weighted to form the log-linear model for machine
translation (Och and Ney, 2002). We define a similar
scoring function using a set of MTU Markov models
MM1, ...,MMk for a hypothesis h as follows:
Score(h) = ?1logPMM1(h) + ... + ?klogPMMk (h)
2We apply hypothesis recombination, which can merge hy-
potheses that are indistinguishable with respect to future contin-
uations. This is similar to recombination in a standard-phrase
based decoder with the difference that it is not always the last
two target MTUs that define the context needed by future ex-
tensions.
15
The weights ? of different models are trained on a
development set using MER training to maximize
the BLEU score of the resulting model. Note that
this method of model combination was not consid-
ered in any of the previous works comparing differ-
ent decompositions.
The system combination method is motivated
by prior work in machine translation which com-
bined left-to-right and right-to-left machine trans-
lation systems (Finch and Sumita, 2009). Simi-
larly, we perform sentence-level system combina-
tion between systems using different MTU Markov
models to come up with most likely translations.
If we have k systems guessing hypotheses based
on MM1, . . . ,MMk respectively, we generate 1000-
best lists from each system, resulting in a pool of
up to 1000k possible distinct translations. Each of
the candidate hypotheses from MMi is scored with
its Markov model log-probability logPMMi(h). We
compute normalized probabilities for each system?s
n-best by exponentiating and normalizing: Pi(h) ?
PMMi(h). If a hypothesis h is not in system i?s n-
best list, we assume its probability is zero according
to that system. The final scoring function for each
hypothesis in the combined list of candidates is:
Score(h) = ?1P1(h) + ... + ?kPk(h)
The weights ? for the combination are tuned using
MERT as for the product model.
4.2 Dynamic decomposition orders
A more complex combination method chooses the
best possible decomposition order for each transla-
tion dynamically, using a set of constraints to de-
fine the possible decomposition orders, and a set of
features to score the candidate decompositions. We
term this method dynamic combination. The score
of each translation is defined as its score according
to the highest-scoring decomposition order for that
translation.
This method is very similar to the bidirectional
tagging approach of Tsuruoka and Tsujii (2005).
For this approach we only explored combinations of
target language orders (L2RT, CycT, and R2LT). If
source language orders were included, the complex-
ity of decoding would increase substantially.
Figure 3 shows two possible decompositions for
a short MTU sequence. The structures displayed are
1
 1
2
1  2| 1
3
2  3| 2,	 1
4
2  4| 3,	 2
 1  2| 1,	 3 1  3| 4  4
Figure 3: Different decompositions.
directed graphical models. They define the set of
parents (context) used to predict each target MTU.
The decomposition structures we consider are lim-
ited to acyclic graphs where each node can have one
of the following parent configurations: no parents
(C = 0 in the Figure), one left parent (C = 1L),
one right parent (C = 1R), one left and one right
parent (C = LR), two left parents (C = 2L), and
two right parents (C = 2R). If all nodes have two
left parents, we recover the left-to-right decomposi-
tion order, and if all nodes have two right parents,
the right-to-left decomposition order. A mixture of
parent configurations defines a mixed, dynamic de-
composition order. The decomposition order chosen
varies from translation to translation.
A directed graphical model defines the probability
of an assignment of MTUs to the variable nodes as a
product of local probabilities of MTUs given their
parents. Here we extend this definition to scores
of assignments by using a linear model with con-
figuration features and log-probability features. The
configuration features are indicators of which par-
ent configuration is active at a node and the settings
of these features for the decompositions in Figure
3 are shown as assignments to the C variables. The
log-probability feature values are obtained by query-
ing the appropriate n-gram model: L2RT, CycT, or
R2LT. For a node with one or two left parents, the
log-probability is computed according to the L2RT
model. For a node with one or two right parents, the
R2LT model is queried. The CycT model is used for
nodes with one left and one right parent.
To find the best translation of a sentence the
model now searches over hidden decomposition or-
16
ders in addition to assignments to target MTUs. The
final score of a translation and decomposition is a
linear combination of the two types of feature values
? model log-probabilities and configuration types.
There is one feature weight for each parent con-
figuration (six configuration weights) and one fea-
ture weight for each component model (three model
weights). The final score of the second decomposi-
tion and assignment in Figure 3 is:
Score(h)
= 2 ? wC0 + wCLR + wC1R
+ wL2RlogPLR(m1) + wCyclogPCyc(m2|m1,m3)
+ wR2LlogPRL(m3|m4) + wL2RlogPLR(m4)
There are two main differences between our ap-
proach and that of Tsuruoka and Tsujii (2005): we
perform beam search with hypothesis recombination
instead of exact decoding (due to the larger size of
the hypothesis set), and we use parameters to be
able to globally weight the probabilities from dif-
ferent models and to develop preferences for using
certain types of decompositions. For example, the
model can learn to prefer right-to-left decomposi-
tions for one language pair, and left-to-right decom-
positions for another. An additional difference from
prior work is the definition of the possible decompo-
sition orders that are searched over.
Compared to the structures allowed in (Tsuruoka
and Tsujii, 2005) for a trigram baseline model, our
allowed structures are a subset; in (Tsuruoka and
Tsujii, 2005) there are sixteen possible parent con-
figurations (up to two left and two right parents),
whereas we allow only six. We train and use only
three n-gram Markov models to assign probabilities:
L2RT, R2LT, and CycT, whereas the prior work used
sixteen models. One could potentially see additional
gains from considering a larger space of structures
but the training time and runtime memory require-
ments might become prohibitive for the machine
translation task.
Because of the maximization over decomposition
structures, the score of a translation is not a simple
linear function of the features, but rather a maximum
over linear functions. The score of a translation for
a fixed decomposition is a linear function of the fea-
tures, but the score of a translation is a maximum of
linear functions (over decompositions). Therefore,
if we define hypotheses as just containing transla-
tions, MERT training does not work directly for op-
timizing the weights of the dynamic combination
method. 3 We used a combination of approaches;
we did MERT training followed by local simplex-
method search starting from three starting points:
the MERT solution, a weight vector that strongly
prefers left-to-right decompositions, and a weight-
vector that strongly prefers right-to-left decomposi-
tions. In the Experiments section, we report results
for the weights that achieved the best development
set performance.
5 N-best reranking
To evaluate the impact of these models in a full MT
system, we investigate n-best reranking. We use a
phrase-based MT system to output 1000-best can-
didate translations. For each candidate translation,
we have access to the phrase pairs it used as well as
the alignments inside each phrase pair. Thus, each
source sentence and its candidate translation form a
word-aligned parallel sentence pair. We can extract
MTU sequences from this sentence pair and com-
pute its probability according to MTU Markov mod-
els. These MTU MM log-probabilities are appended
to the original MT features and used to rerank the
1000-best list. The weight vectors for systems using
the original features along with one or more MTU
Markov model log-probabilities are trained on the
development set using MERT.
6 Experiments
We report experimental results on the lexical selec-
tion task and the reranking task on three language
pairs. The datasets used for the different languages
are described in detail in Section 6.2.
6.1 Lexical selection experiments
The data used for the lexical selection experiments
consists of the training portion of the datasets used
for MT. These training sets are split into three sec-
tions: lex-train, for training MTU Markov models
and extracting possible translations for each source
3If we include the decompositions in the hypotheses we
could use MERT but then the n-best lists used for training might
not contain much variety in terms of translation options. This is
an interesting direction for future research.
17
Model Chs-En Deu-En En-Bgr
Dev Test Dev Test Dev Test
Baseline 06.45 06.30 11.60 10.98 15.09 14.40
Oracle 69.79 70.78 72.28 75.39 85.15 84.32
L2RT 24.02 25.09 28.69 28.70 49.86 46.45
R2LT 23.79 24.91 30.14 30.14* 49.22 46.58
CycT 18.59 20.33 25.91 26.83 41.30 38.85
L2RS 25.81 27.89* 25.52 25.10 45.69 43.98
R2LS 26.48 27.96* 26.03 26.30 47.36 43.91
CycS 21.62 23.38 22.68 23.58 39.11 36.44
Table 1: Lexical selection results for individual MTU
Markov models.
MTU, lex-dev for tuning combination weights for
systems using several MTU MMs, and lex-test, for
final evaluation results. The possible translations for
each source MTU are defined as the most frequent
100 translations seen in lex-train. The lex-dev sets
contain 200 sentence pairs each and the lex-test sets
contains 1000 sentence pairs each. These develop-
ment and test sets consist of equally spaced sen-
tences taken from the full MT training sets.
We start by reporting BLEU scores of the six in-
dividual MTU MMs on the three language pairs in
Table 1. The baseline predicts the most frequent tar-
get MTU for each source MTU (unigram MM not
using context). The oracle looks at the correct trans-
lation and always chooses the correct target MTU if
it is in the vocabulary of available MTUs.
We can see that there is a large difference between
the baseline and oracle performance, underscoring
the importance of modeling context for accurate pre-
diction. The best decomposition order varies from
language to language: right-to-left in source order is
best for Chinese-English, right-to-left in target order
is best for German-English and left-to-right or right-
to-left in target order are best in English-Bulgarian.
We computed statistical significance tests, testing
the difference between the L2RT model (the stan-
dard in prior work) and models achieving higher test
set performance. The models that are significantly
better at significance ? < 0.01 are marked with a
star in the table. We used a paired bootstrap test with
10,000 trials (Koehn, 2004).
Next we evaluate the methods for combining de-
composition orders introduced in Sections 4.1 and
4.2. The results are reported in Table 2. The up-
per part of the table focuses on combining different
Model Chs-En Deu-En En-Bgr
Dev Test Dev Test Dev Test
Baseline-1 24.04 25.09 30.14 30.14 49.86 46.45
TgtProduct 25.27 25.84* 30.47 30.49 51.04 47.27*
TgtSysComb 24.49 25.27 30.20 30.15 50.46 46.31
TgtDynamic 24.07 25.10 30.60 30.41 49.99 46.52
Baseline-2 26.48 27.96 30.14 30.14 49.86 46.45
AllProduct 28.68 29.59* 31.54 31.36* 51.50 48.10*
AllSyscomb 27.02 28.30 30.20 30.17 50.90 46.53
Table 2: Lexical selection results for combinations of
MTU Markov models.
target-order decompositions. The lower part of the
table looks at combining all six decomposition or-
ders. The baseline for the target order combinations,
Baseline-1, is the best single target MTU Markov
model from Table 1. Baseline-2 in the lower part
of the table is the best individual model out of all
six. We can see that the product models TgtProduct
(a product of the three target-order MTU MMs) and
AllProduct (a product of all six MTU MMs) are con-
sistently best. The dynamic decomposition models
TgtDynamic achieve slight but not significant gains
over the baseline. The combination models that are
statistically significantly better than corresponding
baselines (? < 0.01) are marked with a star.
Our takeaway from these experiments is that mul-
tiple decomposition orders are good, and thus taking
a product (which encourages agreement among the
models) is a good choice for this task. The dynamic
decomposition method shows some promise, but it
does not outperform the simpler product approach.
Perhaps a lager space of decompositions would
achieve better results, especially given a larger pa-
rameter set to trade off decompositions and better
tuning for those parameters.
6.2 Datasets and reranking settings
For Chinese-English, the training corpus consists
of 1 million sentence pairs from the FBIS and
HongKong portions of the LDC data for the NIST
MT evaluation. We used the union of the NIST
2002 and 2003 test sets as the development set and
the NIST 2005 test set as our test set. The baseline
phrasal system uses a 5-gram language model with
modified Kneser-Ney smoothing (Kenser and Ney,
1995), trained on the Xinhua portion of the English
Gigaword corpus (238M English words).
For German-English we used the dataset from
18
Language Training Dev Test
Chs-En 1 Mln NIST02+03 NIST05
Deu-En 751 K WMT06dev WMT06test
En-Bgr 4 Mln 1,497 2,498
Table 3: Data sets for different language pairs.
the WMT 2006 shared task on machine translation
(Koehn and Monz, 2006). The parallel training set
contains approximately 751K sentences. We also
used the English monolingual data of around 1 mil-
lion sentences for language model training. The de-
velopment set contains 2000 sentences. The final
test set (the in-domain test set for the shared task)
also contains 2000 sentences. Two Kneser-Ney lan-
guage models were used as separate features: a 4-
gram LM trained on the parallel portion of the data,
and a 5-gram LM trained on the monolingual corpus.
For English-Bulgarian we used a dataset con-
taining sentences from several data sources: JRC-
Acquis (Steinberger et al, 2006), TAUS4, and web-
scraped data. The development set consists of 1,497
sentences, the English side from WMT 2009 news
test data, and the Bulgarian side a human translation
thereof. The test set comes from the same mixture of
sources as the training set. For this system we used
a single four-gram target language model trained on
the target side of the parallel corpus.
All systems used phrase tables with a maximum
length of seven words on either side and lexicalized
reordering models. For the Chinese-English sys-
tem we used GIZA++ alignments, and for the other
two we used alignments by an HMM model aug-
mented with word-based distortion (He, 2007). The
alignments were symmetrized and then combined
with the heuristics ?grow-diag-final-and?. 5 We tune
parameters using MERT (Och, 2003) with random
restarts (Moore and Quirk, 2008) on the develop-
ment set. Case-insensitive BLEU-4 is our evaluation
metric (Papineni et al, 2002).
3-gram models 5-gram models
Model Dev Test Dev Test
Baseline 32.58 31.78 32.58 31.78
L2RT 33.05 32.78* 33.16 32.88*
R2LT 33.05 32.96* 33.16 32.81*
L2RS 32.90 33.00* 32.98 32.98*
R2LS 32.94 32.98* 33.09 32.96*
4 MMs 33.22 33.07* 33.37 33.00*
4 MMs phrs 32.58 31.78 32.58 31.78
Table 4: Reranking with 3-gram and 5-gram MTU trans-
lation models on Chinese-English. Starred results on the
test set indicate significantly better performance than the
baseline.
6.3 MT reranking experiments
We first report detailed experiments on Chinese-
English, and then verify our main conclusions on the
other language pairs. Table 4 looks at the impact of
individual 3-gram and 5-gram MTU Markov models
and their combination. Amongst the decomposition
orders tested (L2RT, R2LT, L2RS, and R2LS), each
of the individual MTU MMs was able to achieve
significant improvement over the baseline, around 1
BLEU point.6 The results achieved by the individ-
ual models differ, and the combination of four direc-
tions is better than the best individual direction, but
the difference is not statistically significant.
We ran an additional experiment to test whether
MTU MMs make effective use of context across
phrase boundaries, or whether they simply pro-
vide better smoothed estimates of phrasal transla-
tion probabilities. The last row of the table reports
the results achieved by a combination of MTU MMs
that do not use context across the phrasal bound-
aries. Since an MTU MM limited to look only inside
phrases can provide improved smoothing compared
to whole phrase relative frequency counts, it is con-
ceivable it could provide a large improvement. How-
ever, there is no improvement in practice for this lan-
guage pair; the additional improvements from MTU
MMs stem from modeling cross-phrase context.
4www.tausdata.org
5The combination heuristic was further refined to disallow
crossing one-to-many alignments, which would result in the ex-
traction of larger minimum translation units. We found that this
further refinement on the combination heuristic consistently im-
proved the BLEU scores by between 0.3 and 0.7.
6Here again we call a difference significant if the paired
bootstrap p-value is less than 0.01.
19
Table 5 shows the test set results of individ-
ual 3-gram MTU Markov models and the com-
bination of 3-gram and 5-gram models on the
English-Bulgarian and German-English datasets.
For English-Bulgarian all individual 3-gram Markov
models achieve significant improvements of close to
one point; their combination is better than the best
individual model (but not significantly). The indi-
vidual 5-gram models and their combination bring
much larger improvement, for a total increase of
2.82 points over the baseline. We believe the 5-
gram models were more effective in this setting be-
cause the larger training set alowed for successful
training of models of larger capacity. Also the in-
creased context size helps to resolve ambiguity in
the forms of morphologically-rich Bulgarian words.
For German-English we see a similar pattern, with
the combination of models outperforming the in-
dividual ones, and the 5-gram models being better
than the 3-gram. Here the individual 3-gram models
are better than the baseline at significance level 0.02
and their combination is better than the baseline at
our earlier defined threshold of 0.01. The within-
phrase MTU MMs (results shown in the last two
rows) improve upon the baseline slightly, but here
again the improvements mostly stem from the use of
context across phrase boundaries. Our final results
on German-English are better than the best result of
27.30 from the shared task (Koehn and Monz, 2006).
Thanks to the reviewers for referring us to re-
cent work by (Clark et al, 2011) that pointed out
problems with significance tests for machine trans-
lation, where the randomness and local optima in the
MERT weight tuning method lead to a large vari-
ance in development and test set performance across
different runs of optimization (using a different ran-
dom seed or starting point). (Clark et al, 2011) pro-
posed a stratified approximate randomization statis-
tical significance test, which controls for optimizer
instability. Using this test, for the English-Bulgarian
system, we confirmed that the combination of four
3-gram MMs and the combination of 5-gram MMs
is better than the baseline (p = .0001 for both, using
five runs of parameter tuning). We have not run the
test for the other language pairs.
Model En-Bgr Deu-En
Baseline 45.75 27.92
L2RT 3-gram 47.07* 28.15
R2LT 3-gram 47.06* 28.19
L2RS 3-gram 46.44* 28.15
R2LS 3-gram 47.04* 28.18
4 3-gram 47.17* 28.37*
4 5-gram 48.57* 28.47*
4 3-gram phrs 46.08 27.92
4 5-gram phrs 46.17* 27.93
Table 5: English-Bulgarian and German-English test set
results: reranking with MTU translation models.
7 Conclusions
We introduced models of Minimal Translation Units
for phrasal systems, and showed that they make a
substantial and statistically significant improvement
on three distinct language-pairs. Additionally we
studied the importance of decomposition order when
defining the probability of MTU sequences. In a
simplified lexical selection task, we saw that there
were large differences in performance among the
different decompositions, with the best decomposi-
tions differing by language. We investigated multi-
ple methods to combine decompositions and found
that a simple product approach was most effective.
Results in the lexical selection task were consistent
with those obtained in a full MT system, although
the differences among decompositions were smaller.
In future work, perhaps we would see larger gains
by including additional decomposition orders (e.g.,
top-down in a dependency tree), and taking this idea
deeper into the machine translation model, down to
the word-alignment and language-modeling levels.
We were surprised to find n-best reranking so ef-
fective. We are incorporating the models into first
pass decoding, in hopes of even greater gains.
References
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better hypothesis testing for statistical
machine translation: Controlling for optimizer insta-
bility. In Proc. ACL-11.
JM Crego and F Yvon. 2010. Factored bilingual n-
gram language models for statistical machine transla-
tion. Machine Translation, Special Issue: Pushing the
frontiers of SMT, 24(2):159?175.
20
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A joint sequence translation model with inte-
grated reordering. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1045?
1054, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Andrew Finch and Eiichiro Sumita. 2009. Bidirectional
phrase-based machine translation. In In proceedings
of EMNLP.
Xiaodong He. 2007. Using word-dependent transition
models in hmm based word alignment for statistical
machine translation. In WMT workshop.
Reinhard Kenser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Proc.
ICASSP 1995, pages 181?184.
Philipp Koehn and Christof Monz. 2006. Manual and au-
tomatic evaluation of machine translation between eu-
ropean languages. In Proceedings on the Workshop on
Statistical Machine Translation, pages 102?121, June.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
HLT-NAACL 2003, pages 127?133.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In In Proceedings of
EMNLP.
JB Marino, RE Banchs, JM Crego, A de Gispert, P Lam-
bert, JA Fonollosa, and MR Costa-Jussa. 2006. N-
gram-based machine translation. Computational Lin-
guistics, 32(4):527?549.
Robert C. Moore and Chris Quirk. 2008. Random
restarts in minimum error training for statistical ma-
chine translation. In Proc. Coling-08.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In In Proceedings of ACL,
pages 295?302.
Franz Joseph Och. 2003. Minimum error training in sta-
tistical machine translation. In Proc. ACL-03.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proc. 40th Annual
Meeting of the ACL, pages 311?318.
Chris Quirk and Arul Menezes. 2006. Do we need
phrases? challenging the conventional wisdom in sta-
tistical machine translation. In Proceedings of the Hu-
man Language Technology Conference of the NAACL,
Main Conference, pages 9?16, New York City, USA,
June. Association for Computational Linguistics.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Toma Erjavec, Dan Tufis, and Dniel
Varga. 2006. The JRC-Acquis: A multilingual
aligned parallel corpus with 20+ languages. In LREC,
Genoa, Italy.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In In Pro-
ceedings of HLT-NAACL.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2005. Bidi-
rectional inference with the easiest-first strategy
for tagging sequence data. In In proceedings of
HLT/EMNLP.
Ashish Vaswani, Haitao Mi, Liang Huang, and David
Chiang. 2011. Rule markov models for fast tree-to-
string translation. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 856?864,
Portland, Oregon, USA, June. Association for Compu-
tational Linguistics.
21
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 875?885,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Convolution Kernel over Packed Parse Forest 
 
 
Min Zhang     Hui Zhang    Haizhou Li 
Institute for Infocomm Research 
A-STAR, Singapore 
{mzhang,vishz,hli}@i2r.a-star.edu.sg 
 
  
 
Abstract 
This paper proposes a convolution forest ker-
nel to effectively explore rich structured fea-
tures embedded in a packed parse forest. As 
opposed to the convolution tree kernel, the 
proposed forest kernel does not have to com-
mit to a single best parse tree, is thus able to 
explore very large object spaces and much 
more structured features embedded in a forest. 
This makes the proposed kernel more robust 
against parsing errors and data sparseness is-
sues than the convolution tree kernel. The pa-
per presents the formal definition of convolu-
tion forest kernel and also illustrates the com-
puting algorithm to fast compute the proposed 
convolution forest kernel. Experimental results 
on two NLP applications, relation extraction 
and semantic role labeling, show that the pro-
posed forest kernel significantly outperforms 
the baseline of the convolution tree kernel. 
1 Introduction 
Parse tree and packed forest of parse trees are 
two widely used data structures to represent the 
syntactic structure information of sentences in 
natural language processing (NLP). The struc-
tured features embedded in a parse tree have 
been well explored together with different ma-
chine learning algorithms and proven very useful 
in many NLP applications (Collins and Duffy, 
2002; Moschitti, 2004; Zhang et al, 2007). A 
forest (Tomita, 1987) compactly encodes an ex-
ponential number of parse trees. In this paper, we 
study how to effectively explore structured fea-
tures embedded in a forest using convolution 
kernel (Haussler, 1999). 
As we know, feature-based machine learning 
methods are less effective in modeling highly 
structured objects (Vapnik, 1998), such as parse 
tree or semantic graph in NLP. This is due to the 
fact that it is usually very hard to represent struc-
tured objects using vectors of reasonable dimen-
sions without losing too much information. For 
example, it is computationally infeasible to enu-
merate all subtree features (using subtree a fea-
ture) for a parse tree into a linear feature vector. 
Kernel-based machine learning method is a good 
way to overcome this problem. Kernel methods 
employ a kernel function, that must satisfy the 
properties of being symmetric and positive, to 
measure the similarity between two objects by 
computing implicitly the dot product of certain 
features of the input objects in high (or even in-
finite) dimensional feature spaces without enu-
merating all the features (Vapnik, 1998).  
Many learning algorithms, such as SVM 
(Vapnik, 1998), the Perceptron learning algo-
rithm (Rosenblatt, 1962) and Voted Perceptron 
(Freund and Schapire, 1999), can work directly 
with kernels by replacing the dot product with a 
particular kernel function. This nice property of 
kernel methods, that implicitly calculates the dot 
product in a high-dimensional space over the 
original representations of objects, has made 
kernel methods an effective solution to modeling 
structured objects in NLP. 
In the context of parse tree, convolution tree 
kernel (Collins and Duffy, 2002) defines a fea-
ture space consisting of all subtree types of parse 
trees and counts the number of common subtrees 
as the syntactic similarity between two parse 
trees. The tree kernel has shown much success in 
many NLP applications like parsing (Collins and 
Duffy, 2002), semantic role labeling (Moschitti, 
2004; Zhang et al, 2007), relation extraction 
(Zhang et al, 2006), pronoun resolution (Yang et 
al., 2006), question classification (Zhang and 
Lee, 2003) and machine translation (Zhang and 
Li, 2009), where the tree kernel is used to com-
pute the similarity between two NLP application 
instances that are usually represented by parse 
trees. However, in those studies, the tree kernel 
only covers the features derived from single 1-
875
best parse tree. This may largely compromise the 
performance of tree kernel due to parsing errors 
and data sparseness. 
To address the above issues, this paper con-
structs a forest-based convolution kernel to mine 
structured features directly from packed forest. A 
packet forest compactly encodes exponential 
number of n-best parse trees, and thus containing 
much more rich structured features than a single 
parse tree. This advantage enables the forest ker-
nel not only to be more robust against parsing 
errors, but also to be able to learn more reliable 
feature values and help to solve the data sparse-
ness issue that exists in the traditional tree kernel. 
We evaluate the proposed kernel in two real NLP 
applications, relation extraction and semantic 
role labeling. Experimental results on the 
benchmark data show that the forest kernel sig-
nificantly outperforms the tree kernel. 
The rest of the paper is organized as follows. 
Section 2 reviews the convolution tree kernel 
while section 3 discusses the proposed forest 
kernel in details. Experimental results are re-
ported in section 4. Finally, we conclude the pa-
per in section 5. 
 
2 Convolution Kernel over Parse Tree 
Convolution kernel was proposed as a concept of 
kernels for discrete structures by Haussler (1999) 
and related but independently conceived ideas on 
string kernels first presented in (Watkins, 1999). 
The framework defines the kernel function be-
tween input objects as the convolution of ?sub-
kernels?, i.e. the kernels for the decompositions 
(parts) of the input objects.  
The parse tree kernel (Collins and Duffy, 2002) 
is an instantiation of convolution kernel over 
syntactic parse trees. Given a parse tree, its fea-
tures defined by a tree kernel are all of its subtree 
types and the value of a given feature is the 
number of the occurrences of the subtree in the 
parse tree. Fig. 1 illustrates a parse tree with all 
of its 11 subtree features covered by the convolu-
tion tree kernel. In the tree kernel, a parse tree T  
is represented by a vector of integer counts of 
each subtree type (i.e., subtree regardless of its 
ancestors, descendants and span covered):  
 
( )T? ? (# subtreetype1(T), ?, # subtreetypen(T))         
 
where # subtreetypei(T) is the occurrence number 
of the ith subtree type in T. The tree kernel counts 
the number of common subtrees as the syntactic 
similarity between two parse trees. Since the 
number of subtrees is exponential with the tree 
size, it is computationally infeasible to directly 
use the feature vector ( )T? . To solve this com-
putational issue, Collins and Duffy (2002) pro-
posed the following tree kernel to calculate the 
dot product between the above high dimensional 
vectors implicitly. 
 
1 1 2 2
1 1 2 2
1 2 1 2
1 2
1 2
1 2
( , ) ( ), ( )
   # ( ) # ( )
   ( ) ( )
   ( , )
i i
i i
i
subtree subtree
i n N n N
n N n N
K T T T T
subtreetype T subtreetype T
I n I n
n n
? ?
? ?
? ?
?? ?
?
? ? ? ?
? ? ? ? ?
? ? ? ?
? ?
?
?
?
? ? ?
? ?
 
 
where N1 and N2 are the sets of nodes in trees T1 
and T2, respectively, and ( )isubtreeI n
 is a function 
that is 1 iff the subtreetypei occurs with root at 
node n and zero otherwise, and 
1 2( , )n n?  is the 
number of the common subtrees rooted at n1 and 
n2, i.e., 
 
1 2 1 2( , ) ( ) ( )i isubtree subtreein n I n I n? ? ??  
 
1 2( , )n n? can be computed by the following recur-
sive rules:  
IN
in the bank
DT NN
PP
IN
the bank
DT NN
PP
IN
in bank
DT NN
PP
IN
in the
DT NN
PP
IN
in
DT NN
PP
IN
the
DT
PP
NN IN
bank
DT NN
PP
IN DT NN
PP
IN
in
the bank
DT NN
IN
in the bank
DT NN
PP
 
Figure 1. A parse tree and its 11 subtree features covered by convolution tree kernel 
876
Rule 1: if the productions (CFG rules) at 
1n  and 
2n  are different, 1 2( , ) 0n n? ? ; 
 
Rule 2: else if both
1n  and 2n  are pre-terminals 
(POS tags), 
1 2( , ) 1n n ?? ? ? ; 
 
Rule 3: else,  
 
1( )
1 2 1 2
1
( , ) (1 ( ( , ), ( , )))
nc n
j
n n ch n j ch n j?
?
? ? ? ? ??
,  
 
where 
1( )nc n is the child number of 1n , ch(n,j) is 
the jth child of node n  and? (0<? ?1) is the de-
cay factor in order to make the kernel value less 
variable with respect to the subtree sizes (Collins 
and Duffy, 2002). The recursive Rule 3 holds 
because given two nodes with the same children, 
one can construct common subtrees using these 
children and common subtrees of further 
offspring. The time complexity for computing 
this kernel is
1 2(| | | |)O N N? . 
As discussed in previous section, when convo-
lution tree kernel is applied to NLP applications, 
its performance is vulnerable to the errors from 
the single parse tree and data sparseness. In this 
paper, we present a convolution kernel over 
packed forest to address the above issues by ex-
ploring structured features embedded in a forest. 
3 Convolution Kernel over Forest 
In this section, we first illustrate the concept of 
packed forest and then give a detailed discussion 
on the covered feature space, fractional count, 
feature value and the forest kernel function itself. 
3.1 Packed forest of parse trees 
Informally, a packed parse forest, or (packed) 
forest in short, is a compact representation of all 
the derivations (i.e. parse trees) for a given sen-
tence under context-free grammar (Tomita, 1987; 
Billot and Lang, 1989; Klein and Manning, 
2001). It is the core data structure used in natural 
language parsing and other downstream NLP 
applications, such as syntax-based machine 
translation (Zhang et al, 2008; Zhang et al, 
2009a). In parsing, a sentence corresponds to 
exponential number of parse trees with different 
tree probabilities, where a forest can compact all 
the parse trees by sharing their common subtrees 
in a bottom-up manner. Formally, a packed for-
est ? can be described as a triple: 
 
? = < ?,?, ? > 
 
where  ?is the set of non-terminal nodes, ? is the 
set of hyper-edges and ?  is a sentence 
 
NNP[1,1] VV[2,2] NN[4,4] IN[5,5]
John saw a man
NP[3,4]
in the bank
DT[3,3] DT[6,6]
NN[7,7]
PP[5,7]
VP[2,4] NP[3,7]
VP[2,7]
IP[1,7]
NNP VV NN     IN     DT   NN
John saw a man in the bank
DT
VP
NP
VP
IP
PP
NNP VV NN     IN     DT   NN
John saw a man in the bank
DT
NP
NP
VP
IP
PP
IP[1,7]
VP[2,7]
NNP[1,1]
a) A Forest f
b) A Hyper-edge e
c) A Parse Tree T1
d) A Parse Tree T2
 
 
Figure 2. An example of a packed forest, a hyper-edge and two parse trees covered by the packed forest 
 
877
represented as an ordered word sequence. A hy-
per-edge ?  is a group of edges in a parse tree 
which connects a father node and its all child 
nodes, representing a CFG rule. A non-terminal 
node in a forest is represented as a ?label [start, 
end]?, where the ?label? is its syntax category 
and ?[start, end]? is the span of words it covers. 
As shown in Fig. 2, these two parse trees (?1 
and ?2) can be represented as a single forest by 
sharing their common subtrees (such as NP[3,4] 
and PP[5,7]) and merging common non-terminal 
nodes covering the same span (such as VP[2,7], 
where there are two hyper-edges attach to it). 
Given the definition of forest, we introduce 
the concepts of inside probability ? .   and out-
side probability ?(. )  that are widely-used in 
parsing (Baker, 1979; Lari and Young, 1990) and 
are also to be used in our kernel calculation. 
 
? ? ?,?  = ?(? ? ?[?]) 
 
? ? ?, ?  =   
 
 
 
 ? ? 
?  ??  ?  ????? ?????  
????? ???  ??  ?
?  ?(??[?? , ??])
?? ?? ,?? ??  ?  ????  
????  ??  ?  
 
 
 
? ????(?) = 1       
? ? ?, ?  =   
 
 
 
 
? ???? ?    ? ? ? 
?  ??  ?  ????? ?  
????  ???  ? 
??  ???  ???
 ????  ????
?   ?(??[?? , ??]))
?? ?? ,?? ??  ?  
????????  ????  
??  ? ??????  ?
 
 
where ? is a forest node, ?[?] is the ???  word of 
input sentence ?, ?(? ? ?[?]) is the probability 
of the CFG rule ? ? ?[?] , ????(. )  returns the 
root node of input structure, [?? , ??] is a sub-span 
of  ?, ? , being covered by ?? , and ? ? is the 
PCFG probability of ? . From these definitions, 
we can see that the inside probability is total 
probability of generating words ? ?, ?  from 
non-terminal node ? ?, ?  while the outside 
probability is the total probability of generating 
node ? ?, ?  and words outside ?[?, ?] from the 
root of forest. The inside probability can be cal-
culated using dynamic programming in a bottom-
up fashion while the outside probability can be 
calculated using dynamic programming in a top-
to-down way. 
3.2 Convolution forest kernel 
In this subsection, we first define the feature 
space covered by forest kernel, and then define 
the forest kernel function. 
3.2.1 Feature space, object space and fea-
ture value 
The forest kernel counts the number of common 
subtrees as the syntactic similarity between two 
forests. Therefore, in the same way as tree kernel, 
its feature space is also defined as all the possible 
subtree types that a CFG grammar allows. In a 
forest kernel, forest ? is represented by a vector 
of fractional counts of each subtree type (subtree 
regardless of its ancestors, descendants and span 
covered):  
 
( )F? ? (# subtreetype1(F), ?,  
              # subtreetypen(F)) 
   = (#subtreetype1(n-best parse trees), ?,   (1) 
      # subtreetypen(n-best parse trees))  
 
where # subtreetypei(F) is the occurrence number 
of the ith subtree type (subtreetypei) in forest F, 
i.e., a n-best parse tree lists with a huge n.  
Although the feature spaces of the two kernels 
are the same, their object spaces (tree vs. forest) 
and feature values (integer counts vs. fractional 
counts) differ very much. A forest encodes expo-
nential number of parse trees, and thus contain-
ing exponential times more subtrees than a single 
parse tree. This ensures forest kernel to learn 
more reliable feature values and is also able to 
help to address the data sparseness issues in a 
better way than tree kernel does. Forest kernel is 
also expected to yield more non-zero feature val-
ues than tree kernel. Furthermore, different parse 
tree in a forest represents different derivation and 
interpretation for a given sentence. Therefore, 
forest kernel should be more robust to parsing 
errors than tree kernel. 
In tree kernel, one occurrence of a subtree 
contributes 1 to the value of its corresponding 
feature (subtree type), so the feature value is an 
integer count. However, the case turns out very 
complicated in forest kernel. In a forest, each of 
its parse trees, when enumerated, has its own 
878
probability. So one subtree extracted from differ-
ent parse trees should have different fractional 
count with regard to the probabilities of different 
parse trees. Following the previous work (Char-
niak and Johnson, 2005; Huang, 2008), we de-
fine the fractional count of the occurrence of a 
subtree in a parse tree ??  as  
 
? ???????, ?? =  
0                      ?? ??????? ? ??  
? ???????, ??|?, ?   ?????????
  
 
                           =  
0                         ?? ??????? ? ??  
? ??|?, ?                    ?????????
  
 
where we have ? ???????, ??|?, ? = ? ??|?, ?  if 
??????? ? ?? . Then we define the fractional count 
of the occurrence of a subtree in a forest f as 
 
 ? ???????,? = ? ???????|?, ?  
                            =   ? ???????, ?? |?, ? ??         (2) 
                            =   ????????  ?? ? ? ??|?, ? ??   
 
where ????????  ??  is a binary function that is 1 
iif the ??????? ? ??  and zero otherwise. Ob-
viously, it needs exponential time to compute the 
above fractional counts. However, due to the 
property of forest that compactly represents all 
the parse trees, the posterior probability of a 
subtree in a forest, ? ???????|?, ? , can be easi-
ly computed in an Inside-Outside fashion as the 
product of three parts: the outside probability of 
its root node, the probabilities of parse hyper-
edges involved in the subtree, and the inside 
probabilities of its leaf nodes (Lari and Young, 
1990; Mi and Huang, 2008).  
 
? ???????,? = ? ???????|?, ?             (3) 
 
=
??(???????)
??(???? ? )
     
where 
 
?? ??????? = ? ???? ???????         (4) 
?  ? ? 
?????????
           
?  ? ? 
??????  ???????  
            
and 
 
        ?? ???? ?  = ? ???? ?  ? ? ???? ?   
       = ? ???? ?   
 
where ? .   and ?(. ) denote the outside and in-
side probabilities. They can be easily obtained 
using the equations introduced at section 3.1.  
Given a subtree, we can easily compute its 
fractional count (i.e. its feature value) directly 
using eq. (3) and (4) without the need of enume-
rating each parse trees as shown at eq. (2) 1 .  
Nonetheless, it is still computationally infeasible 
to directly use the feature vector ?(?) (see eq. 
(1)) by explicitly enumerating all subtrees  al-
though its fractional count is easily calculated. In 
the next subsection, we present the forest kernel 
that implicitly calculates the dot-product between 
two ?(?)s in a polynomial time. 
3.2.2 Convolution forest kernel 
The forest kernel counts the fractional numbers 
of common subtrees as the syntactic similarity 
between two forests. We define the forest kernel 
function ?? ?1 ,?2  in the following way. 
 
   ?? ?1 ,?2 =< ? ?1 ,? ?2 >                       (5) 
  = #????????????(?1). #????????????(?2)
?
 
  =      ???  ???????1, ???????2 
??????? 1??1
??????? 2??2
? ? ???????1,?1 
? ? ???????2,?2                 
   =   ?? ?1 ,?2  ?2??2?1??1   
 
where 
? ???  ?,?  is a binary function that is 1 iif 
the input two subtrees are identical (i.e. 
they have the same typology and node 
labels) and zero otherwise; 
? ? ?,?  is the fractional count defined at 
eq. (3); 
? ?1  and ?2  are the sets of nodes in fo-
rests ?1 and ?2; 
? ?? ?1,?2  returns the accumulated value 
of products between each two fractional 
counts of the common subtrees rooted at 
?1 and ?2, i.e.,  
 
?? ?1,?2  
=      ???  ???????1, ???????2 
????  ??????? 1 =?1
????  ??????? 2 =?2
? ? ???????1,?1     
? ? ???????2,?2                 
                                                 
1  It has been proven in parsing literatures (Baker, 
1979; Lari and Young, 1990) that eq. (3) defined by 
Inside-Outside probabilities is exactly to compute the 
sum of those parse tree probabilities that cover the 
subtree of being considered as defined at eq. (2). 
879
We next show that ?? ?1 ,?2  can be computed 
recursively in a polynomial time as illustrated at 
Algorithm 1. To facilitate discussion, we tempo-
rarily ignore all fractional counts in Algorithm 1. 
Indeed, Algorithm 1 can be viewed as a natural 
extension of convolution kernel from over tree to 
over forest. In forest2, a node can root multiple 
hyper-edges and each hyper-edge is independent 
to each other. Therefore, Algorithm 1 iterates 
each hyper-edge pairs with roots at ?1  and ?2 
(line 3-4), and sums over (eq. (7) at line 9) each 
recursively-accumulated sub-kernel scores of 
subtree pairs extended from the hyper-edge pair 
 ?1 , ?2  (eq. (6) at line 8). Eq. (7) holds because 
the hyper-edges attached to the same node are 
independent to each other. Eq. (6) is very similar 
to the Rule 3 of tree kernel (see section 2) except 
its inputs are hyper-edges and its further expan-
sion is based on forest nodes. Similar to tree ker-
nel (Collins and Duffy, 2002), eq. (6) holds be-
cause a common subtree by extending from 
(?1 , ?2) can be formed by taking the hyper-edge 
(?1 , ?2), together with a choice at each of their 
leaf nodes of simply taking the non-terminal at 
the leaf node, or any one of the common subtrees 
with root at the leaf node. Thus there are 
 1 + ?? ???? ?1 , ? , ???? ?2 , ?   possible 
choices at the jth leaf node. In total, there are 
???  ?1 , ?2  (eq. (6)) common subtrees by extend-
ing from (?1 , ?2)  and ?
? ?1,?2  (eq. (7)) com-
mon subtrees with root at  ?1 ,?2 .  
Obviously ?? ?1 ,?2  calculated by Algorithm 
1 is a proper convolution kernel since it simply 
counts the number of common subtrees under the 
root  ?1 ,?2 . Therefore, ?? ?1,?2  defined at eq. 
(5) and calculated through ?? ?1,?2  is also a 
proper convolution kernel. From eq. (5) and Al-
gorithm 1, we can see that each hyper-edge pair 
(?1 , ?2) is only visited at most one time in com-
puting the forest kernel. Thus the time complexi-
ty for computing ?? ?1,?2  is ?(|?1| ? |?2|) , 
where ?1  and ?2 are the set of hyper-edges in 
forests ?1  and ?2 , respectively. Given a forest 
and the best parse trees, the number of hyper-
edges is only several times (normally <=3 after 
pruning) than that of tree nodes in the parse tree3. 
                                                 
2 Tree can be viewed as a special case of forest with 
only one hyper-edge attached to each tree node. 
3 Suppose there are K forest nodes in a forest, each 
node has M associated hyper-edges fan out and each 
hyper-edge has N children. Then the forest is capable 
of encoding ?
??1
??1  parse trees at most (Zhang et al, 
2009b). 
Same as tree kernel, forest kernel is running 
more efficiently in practice since only two nodes 
with the same label needs to be further processed 
(line 2 of Algorithm 1). 
Now let us see how to integrate fractional 
counts into forest kernel. According to Algo-
rithm 1 (eq. (7)), we have (?1/?2  are attached to 
?1/?2, respectively) 
 
?? ?1, ?2 =  ?
??  ?1, ?2 ?1=?2   
 
Recall eq. (4), a fractional count consists of 
outside, inside and subtree probabilities. It is 
more straightforward to incorporate the outside 
and subtree probabilities since all the subtrees 
with roots at  ?1 , ?2  share the same outside 
probability and each hyper-edge pair is only vi-
sited one time. Thus we can integrate the two 
probabilities into ?? ?1,?2  as follows. 
 
     ?? ?1,?2 = ? ? ? ?1 ? ? ?2  
        ?   ? ?1 ? ? ?2 ? ?
??  ?1, ?2  ?1=?2    (8) 
 
where, following tree kernel, a decay factor 
?(0 < ? ? 1) is also introduced in order to make 
the kernel value less variable with respect to the 
subtree sizes (Collins and Duffy, 2002). It func-
tions like multiplying each feature value by 
????? ? , where ?????  is the number of hyper-edges 
in ???????? . 
Algorithm 1.  
Input:  
        ?1 ,?2: two packed forests 
        ?1 ,?2: any two nodes of ?1 and ?2 
Notation:  
    ???  ?,? : defined at eq. (5) 
   ?? ?1 : number of leaf node of ?1    ???? ?1 , ? : the jth leaf node of ?1 
Output:  ?? ?1,?2  
 
1. ?? ?1,?2 = 0 
2. if  ?1 . ????? ? ?2 . ?????  exit 
3. for each hyper-edge ?1 attached to ?1 do 
4.      for each hyper-edge ?2 attached to ?2 do 
5.           if ???  ?1, ?2 == 0 do 
6.                 goto line 3 
7.           else do 
8.                  ???  ?1 , ?2  =    1 +
??  ?1 
?=1
                        ?? ???? ?1 , ? , ???? ?2 , ?     (6) 
9.                   ?? ?1,?2  +=  ?
??  ?1, ?2            (7) 
10.            end if  
11.       end for 
12. end for 
 
880
The inside probability is only involved when a 
node does not need to be further expanded. The 
integer 1 at eq. (6) represents such case. So the 
inside probability is integrated into eq. (6) by 
replacing the integer 1 as follows. 
  
 ???  ?1, ?2 =   ? ???? ?1, ?   ? ? ???? ?2, ?  
??  ?1 
?=1
+
 ?? ???? ?1 , ? , ???? ?2, ?  
? ???? ?1 , ?  ? ? ???? ?2, ?  
  (9) 
 
where in the last expression the two outside 
probabilities ? ???? ?1 , ?   and ? ???? ?2 , ?   
are removed. This is because  ???? ?1 , ? and 
???? ?2 , ?  are not roots of the subtrees of being 
explored (only outside probabilities of the root of 
a subtree should be counted in its fractional 
count), and  ?? ???? ?1 , ? , ???? ?2 , ?   already 
contains the two outside probabilities of 
???? ?1 , ?  and ???? ?2 , ? . 
Referring to eq. (3), each fractional count 
needs to be normalized by ??(???? ? ). Since 
??(???? ? ) is independent to each individual 
fractional count, we do the normalization outside 
the recursive function ???  ?1 , ?2 . Then we can 
re-formulize eq. (5) as 
 
     ?? ?1,?2 =< ? ?1 ,? ?2 >  
=
   ?? ?1,?2  ?2??2?1??1  
?? ???? ?1  ? ?? ???? ?2  
   (10) 
 
Finally, since the size of input forests is not 
constant, the forest kernel value is normalized 
using the following equation.  
 
        ? ? ?1,?2 =
   ?? ?1,?2 
 ?? ?1,?1 ? ?? ?2,?2 
     (11) 
 
From the above discussion, we can see that the 
proposed forest kernel is defined together by eqs. 
(11), (10), (9) and (8). Thanks to the compact 
representation of trees in forest and the recursive 
nature of the kernel function, the introduction of 
fractional counts and normalization do not 
change the convolution property and the time 
complexity of the forest kernel. Therefore, the 
forest kernel ? ? ?1 ,?2  is still a proper convolu-
tion kernel with quadratic time complexity. 
3.3 Comparison with previous work 
To the best of our knowledge, this is the first 
work to address convolution kernel over packed 
parse forest. 
Convolution tree kernel is a special case of the 
proposed forest kernel. From feature exploration 
viewpoint, although theoretically they explore 
the same subtree feature spaces (defined recur-
sively by CFG parsing rules), their feature values 
are different. Forest encodes exponential number 
of trees. So the number of subtree instances ex-
tracted from a forest is exponential number of 
times greater than that from its corresponding 
parse tree. The significant difference of the 
amount of subtree instances makes the parame-
ters learned from forests more reliable and also 
can help to address the data sparseness issue. To 
some degree, forest kernel can be viewed as a 
tree kernel with very powerful back-off mechan-
ism. In addition, forest kernel is much more ro-
bust against parsing errors than tree kernel. 
Aiolli et al (2006; 2007) propose using Direct 
Acyclic Graphs (DAG) as a compact representa-
tion of tree kernel-based models. This can largely 
reduce the computational burden and storage re-
quirements by sharing the common structures 
and feature vectors in the kernel-based model. 
There are a few other previous works done by 
generalizing convolution tree kernels (Kashima 
and Koyanagi, 2003; Moschitti, 2006; Zhang et 
al., 2007). However, all of these works limit 
themselves to single tree structure from modeling 
viewpoint in nature. 
From a broad viewpoint, as suggested by one 
reviewer of the paper, we can consider the forest 
kernel as an alternative solution proposed for the 
general problem of noisy inference pipelines (eg. 
speech translation by composition of FSTs, ma-
chine translation by translating over 'lattices' of 
segmentations (Dyer  et al, 2008) or using parse 
tree info for downstream applications in our cas-
es) . Following this line, Bunescu (2008) and 
Finkel et al (2006) are two typical related works 
done in reducing cascading noisy. However, our 
works are not overlapped with each other as 
there are two totally different solutions for the 
same general problem. In addition, the main mo-
tivation of this paper is also different from theirs. 
4 Experiments 
Forest kernel has a broad application potential in 
NLP. In this section, we verify the effectiveness 
of the forest kernel on two NLP applications, 
semantic role labeling (SRL) (Gildea, 2002) and 
relation extraction (RE) (ACE, 2002-2006). 
In our experiments, SVM (Vapnik, 1998) is 
selected as our classifier and the one vs. others 
strategy is adopted to select the one with the 
881
largest margin as the final answer. In our imple-
mentation, we use the binary SVMLight (Joa-
chims, 1998) and borrow the framework of the 
Tree Kernel Tools (Moschitti, 2004) to integrate 
our forest kernel into the SVMLight. We modify 
Charniak parser (Charniak, 2001) to output a 
packed forest. Following previous forest-based 
studies (Charniak and Johnson, 2005), we use the 
marginal probabilities of hyper-edges (i.e., the 
Viterbi-style inside-outside probabilities and set 
the pruning threshold as 8) for forest pruning. 
4.1 Semantic role labeling 
Given a sentence and each predicate (either a 
target verb or a noun), SRL recognizes and maps 
all the constituents in the sentence into their cor-
responding semantic arguments (roles, e.g., A0 
for Agent, A1 for Patient ?) of the predicate or 
non-argument. We use the CoNLL-2005 shared 
task on Semantic Role Labeling (Carreras and 
Ma rquez, 2005) for the evaluation of our forest 
kernel method. To speed up the evaluation 
process, the same as Che et al (2008), we use a 
subset of the entire training corpus (WSJ sections 
02-05 of the entire sections 02-21) for training, 
section 24 for development and section 23 for 
test, where there are 35 roles including 7 Core 
(A0?A5, AA), 14 Adjunct (AM-) and 14 Refer-
ence (R-) arguments. 
The state-of-the-art SRL methods (Carreras 
and Ma rquez, 2005) use constituents as the labe-
ling units to form the labeled arguments. Due to 
the errors from automatic parsing, it is impossi-
ble for all arguments to find their matching con-
stituents in the single 1-best parse trees. Statistics 
on the training data shows that 9.78% of argu-
ments have no matching constituents using the 
Charniak parser (Charniak, 2001), and the num-
ber increases to 11.76% when using the Collins 
parser (Collins, 1999). In our method, we break 
the limitation of 1-best parse tree and regard each 
span rooted by a single forest node (i.e., a sub-
forest with one or more roots) as a candidate ar-
gument. This largely reduces the unmatched ar-
guments from 9.78% to 1.31% after forest prun-
ing. However, it also results in a very large 
amount of argument candidates that is 5.6 times 
as many as that from 1-best tree. Fortunately, 
after the pre-processing stage of argument prun-
ing (Xue and Palmer, 2004) 4 , although the 
                                                 
4  We extend (Xue and Palmer, 2004)?s argument 
pruning algorithm from tree-based to forest-based. 
The algorithm is very effective. It can prune out 
around 90% argument candidates in parse tree-based 
amount of unmatched argument increases a little 
bit to 3.1%, its generated total candidate amount 
decreases substantially to only 1.31 times of that 
from 1-best parse tree. This clearly shows the 
advantages of the forest-based method over tree-
based in SRL. 
The best-reported tree kernel method for SRL 
??????? = ? ? ???? ? + (1? ?) ? ???  (0 ? ? ?
1), proposed by Che et al (2006)5, is adopted as 
our baseline kernel. We implemented the ???????  
in tree case (????????? , using tree kernel to 
compute ???? ?  and ??? ) and in forest case 
(????????? , using tree kernel to compute ???? ?  
and ??? ).  
 
 Precision Recall  F-Score 
?????????  (Tree) 76.02 67.38  71.44 
?????????  (Forest) 79.06 69.12 73.76 
Table 1: Performance comparison of SRL (%) 
 
Table 1 shows that the forest kernel significant-
ly outperforms (?2 test with p=0.01) the tree ker-
nel with an absolute improvement of 2.32 (73.76-
71.42) percentage in F-Score, representing a rela-
tive error rate reduction of 8.19% (2.32/(100-
71.64)). This convincingly demonstrates the ad-
vantage of the forest kernel over the tree kernel. It 
suggests that the structured features represented 
by subtree are very useful to SRL. The perfor-
mance improvement is mainly due to the fact that 
forest encodes much more such structured features 
and the forest kernel is able to more effectively 
capture such structured features than the tree ker-
nel. Besides F-Score, both precision and recall 
also show significantly improvement (?2 test with 
p=0.01). The reason for recall improvement is 
mainly due to the lower rate of unmatched argu-
ment (3.1% only) with only a little bit overhead 
(1.31 times) (see the previous discussion in this 
section). The precision improvement is mainly 
attributed to fact that we use sub-forest to 
represent argument instances, rather than sub-
tree used in tree kernel, where the sub-tree is on-
ly one tree encoded in the sub-forest. 
                                                                          
SRL and thus makes the amounts of positive and neg-
ative training instances (arguments) more balanced. 
We apply the same pruning strategies to forest plus 
our heuristic rules to prune out some of the arguments 
with span overlapped with each other and those ar-
guments with very small inside probabilities, depend-
ing on the numbers of candidates in the span. 
5 Kpath and Kcs are two standard convolution tree ker-
nels to describe predicate-argument path substructures 
and argument syntactic substructures, respectively. 
882
4.2 Relation extraction  
As a subtask of information extraction, relation 
extraction is to extract various semantic relations 
between entity pairs from text. For example, the 
sentence ?Bill Gates is chairman and chief soft-
ware architect of Microsoft Corporation? con-
veys the semantic relation ?EMPLOY-
MENT.executive? between the entities ?Bill 
Gates? (person) and ?Microsoft Corporation? 
(company). We adopt the method reported in 
Zhang et al (2006) as our baseline method as it 
reports the state-of-the-art performance using 
tree kernel-based composite kernel method for 
RE. We replace their tree kernels with our forest 
kernels and use the same experimental settings as 
theirs. We carry out the same five-fold cross va-
lidation experiment on the same subset of ACE 
2004 data (LDC2005T09, ACE 2002-2004) as 
that in Zhang et al (2006). The data contain 348 
documents and 4400 relation instances.  
In SRL, constituents are used as the labeling 
units to form the labeled arguments. However, 
previous work (Zhang et al, 2006) shows that if 
we use complete constituent (MCT) as done in 
SRL to represent relation instance, there is a 
large performance drop compared with using the 
path-enclosed tree (PT)6. By simulating PT, we 
use the minimal fragment of a forest covering the 
two entities and their internal words to represent 
a relation instance by only parsing the span cov-
ering the two entities and their internal words. 
 
 
 Precision  Recall  F-Score 
Zhang et al (2006):Tree 68.6 59.3 6  63.6 
Ours: Forest  70.3 60.0   64.7 
 
Table 2: Performance Comparison of RE (%) 
over 23 subtypes on the ACE 2004 data 
  
Table 2 compares the performance of the for-
est kernel and the tree kernel on relation extrac-
tion. We can see that the forest kernel significant-
ly outperforms (?2 test with p=0.05) the tree ker-
nel by 1.1 point of F-score. This further verifies 
the effectiveness of the forest kernel method for 
                                                 
6 MCT is the minimal constituent rooted by the near-
est common ancestor of the two entities under consid-
eration while PT is the minimal portion of the parse 
tree (may not be a complete subtree) containing the 
two entities and their internal lexical words. Since in 
many cases, the two entities and their internal words 
cannot form a grammatical constituent, MCT may 
introduce too many noisy context features and thus 
lead to the performance drop. 
modeling NLP structured data. In summary, we 
further observe the high precision improvement 
that is consistent with the SRL experiments. How-
ever, the recall improvement is not as significant 
as observed in SRL. This is because unlike SRL, 
RE has no un-matching issues in generating rela-
tion instances. Moreover, we find that the perfor-
mance improvement in RE is not as good as that 
in SRL. Although we know that performance is 
task-dependent, one of the possible reasons is 
that SRL tends to be long-distance grammatical 
structure-related while RE is local and semantic-
related as observed from the two experimental 
benchmark data. 
5 Conclusions and Future Work 
Many NLP applications have benefited from the 
success of convolution kernel over parse tree. 
Since a packed parse forest contains much richer 
structured features than a parse tree, we are mo-
tivated to develop a technology to measure the 
syntactic similarity between two forests. 
To achieve this goal, in this paper, we design a 
convolution kernel over packed forest by genera-
lizing the tree kernel. We analyze the object 
space of the forest kernel, the fractional count for 
feature value computing and design a dynamic 
programming algorithm to realize the forest ker-
nel with quadratic time complexity. Compared 
with the tree kernel, the forest kernel is more ro-
bust against parsing errors and data sparseness 
issues. Among the broad potential NLP applica-
tions, the problems in SRL and RE provide two 
pointed scenarios to verify our forest kernel. Ex-
perimental results demonstrate the effectiveness 
of the proposed kernel in structured NLP data 
modeling and the advantages over tree kernel.  
In the future, we would like to verify the forest 
kernel in more NLP applications. In addition, as 
suggested by one reviewer, we may consider res-
caling the probabilities (exponentiating them by 
a constant value) that are used to compute the 
fractional counts. We can sharpen or flatten the 
distributions. This basically says "how seriously 
do we want to take the very best derivation" 
compared to the rest. However, the challenge is 
that we compute the fractional counts together 
with the forest kernel recursively by using the 
Inside-Outside probabilities. We cannot differen-
tiate the individual parse tree?s contribution to a 
fractional count on the fly. One possible solution 
is to do the probability rescaling off-line before 
kernel calculation. This would be a very interest-
ing research topic of our future work. 
883
References  
ACE (2002-2006). The Automatic Content Extraction 
Projects. http://www.ldc.upenn.edu/Projects/ACE/ 
Fabio Aiolli, Giovanni Da San Martino, Alessandro 
Sperduti and Alessandro Moschitti. 2006. Fast On-
line Kernel Learning for Trees. ICDM-2006 
Fabio Aiolli, Giovanni Da San Martino, Alessandro 
Sperduti and Alessandro Moschitti. 2007. Efficient 
Kernel-based Learning for Trees. IEEE Sympo-
sium on Computational Intelligence and Data Min-
ing (CIDM-2007) 
J. Baker. 1979. Trainable grammars for speech rec-
ognition. The 97th meeting of the Acoustical So-
ciety of America  
S. Billot and S. Lang. 1989. The structure of shared 
forest in ambiguous parsing. ACL-1989  
Razvan Bunescu. 2008. Learning with Probabilistic 
Features for Improved Pipeline Models. EMNLP-
2008 
X. Carreras and Llu?s Ma rquez. 2005. Introduction to 
the CoNLL-2005 shared task: SRL. CoNLL-2005 
E. Charniak. 2001. Immediate-head Parsing for Lan-
guage Models. ACL-2001 
E. Charniak and Mark Johnson. 2005. Corse-to-fine-
grained n-best parsing and discriminative re-
ranking. ACL-2005 
Wanxiang Che, Min Zhang, Ting Liu and Sheng Li. 
2006. A hybrid convolution tree kernel for seman-
tic role labeling. COLING-ACL-2006 (poster) 
WanXiang Che, Min Zhang, Aiti Aw, Chew Lim Tan, 
Ting Liu and Sheng Li. 2008. Using a Hybrid 
Convolution Tree Kernel for Semantic Role Labe-
ling. ACM Transaction on Asian Language Infor-
mation Processing 
M. Collins. 1999. Head-driven statistical models for 
natural language parsing. Ph.D. dissertation, 
Pennsylvania University 
M. Collins and N. Duffy. 2002. Convolution Kernels 
for Natural Language. NIPS-2002 
Christopher Dyer, Smaranda Muresan and Philip Res-
nik. 2008. Generalizing Word Lattice Translation. 
ACL-HLT-2008 
Jenny Rose Finkel, Christopher D. Manning and And-
rew Y. Ng. 2006. Solving the Problem of Cascad-
ing Errors: Approximate Bayesian Inference for 
Linguistic Annotation Pipelines. EMNLP-2006 
Y. Freund and R. E. Schapire. 1999. Large margin 
classification using the perceptron algorithm. Ma-
chine Learning, 37(3):277-296 
D. Guldea. 2002. Probabilistic models of verb-
argument structure. COLING-2002 
D. Haussler. 1999. Convolution Kernels on Discrete 
Structures. Technical Report UCS-CRL-99-10, 
University of California, Santa Cruz 
Liang Huang. 2008. Forest reranking: Discriminative 
parsing with non-local features. ACL-2008 
Karim Lari and Steve J. Young. 1990. The estimation 
of stochastic context-free grammars using the in-
side-outside algorithm. Computer Speech and Lan-
guage. 4(35?56) 
H. Kashima and T. Koyanagi. 2003. Kernels for Semi-
Structured Data. ICML-2003 
Dan Klein and Christopher D. Manning. 2001. Pars-
ing and Hypergraphs. IWPT-2001 
T. Joachims. 1998. Text Categorization with Support 
Vecor Machine: learning with many relevant fea-
tures. ECML-1998 
Haitao Mi and Liang Huang. 2008. Forest-based 
Translation Rule Extraction. EMNLP-2008 
Alessandro Moschitti. 2004. A Study on Convolution 
Kernels for Shallow Semantic Parsing. ACL-2004 
Alessandro Moschitti. 2006. Syntactic kernels for 
natural language learning: the semantic role labe-
ling case. HLT-NAACL-2006 (short paper) 
Martha Palmer, Dan Gildea and Paul Kingsbury. 
2005. The proposition bank: An annotated corpus 
of semantic roles. Computational Linguistics. 31(1) 
F. Rosenblatt. 1962. Principles of Neurodynamics: 
Perceptrons and the theory of brain mechanisms. 
Spartan Books, Washington D.C. 
Masaru Tomita. 1987. An Efficient Augmented-
Context-Free Parsing Algorithm. Computational 
Linguistics 13(1-2): 31-46 
Vladimir N. Vapnik. 1998. Statistical Learning 
Theory. Wiley 
C. Watkins. 1999. Dynamic alignment kernels. In A. J. 
Smola, B. Sch o?lkopf, P. Bartlett, and D. Schuur-
mans (Eds.), Advances in kernel methods. MIT 
Press 
Nianwen Xue and Martha Palmer. 2004. Calibrating 
features for semantic role labeling. EMNLP-2004  
Xiaofeng Yang, Jian Su and Chew Lim Tan. 2006. 
Kernel-Based Pronoun Resolution with Structured 
Syntactic Knowledge. COLING-ACL-2006 
Dell Zhang and W. Lee. 2003. Question classification 
using support vector machines. SIGIR-2003 
Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw and 
Chew Lim Tan. 2009a. Forest-based Tree Se-
quence to String Translation Model. ACL-
IJCNLP-2009 
Hui Zhang, Min Zhang, Haizhou Li and Chew Lim 
Tan. 2009b. Fast Translation Rule Matching for 
884
Syntax-based Statistical Machine Translation. 
EMNLP-2009 
Min Zhang, Jie Zhang, Jian Su and GuoDong Zhou. 
2006. A Composite Kernel to Extract Relations be-
tween Entities with Both Flat and Structured Fea-
tures. COLING-ACL-2006 
Min Zhang, W. Che, A. Aw, C. Tan, G. Zhou, T. Liu 
and S. Li. 2007. A Grammar-driven Convolution 
Tree Kernel for Semantic Role Classification. 
ACL-2007 
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, 
Chew Lim Tan and Sheng Li. 2008. A Tree Se-
quence Alignment-based Tree-to-Tree Translation 
Model. ACL-2008 
Min Zhang and Haizhou Li. 2009. Tree Kernel-based 
SVM with Structured Syntactic Knowledge for 
BTG-based Phrase Reordering. EMNLP-2009 
885
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 317?321,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
An Exploration of Forest-to-String Translation:
Does Translation Help or Hurt Parsing?
Hui Zhang
University of Southern California
Department of Computer Science
hzhang@isi.edu
David Chiang
University of Southern California
Information Sciences Institute
chiang@isi.edu
Abstract
Syntax-based translation models that operate
on the output of a source-language parser have
been shown to perform better if allowed to
choose from a set of possible parses. In this
paper, we investigate whether this is because it
allows the translation stage to overcome parser
errors or to override the syntactic structure it-
self. We find that it is primarily the latter, but
that under the right conditions, the transla-
tion stage does correct parser errors, improv-
ing parsing accuracy on the Chinese Treebank.
1 Introduction
Tree-to-string translation systems (Liu et al, 2006;
Huang et al, 2006) typically employ a pipeline of
two stages: a syntactic parser for the source lan-
guage, and a decoder that translates source-language
trees into target-language strings. Originally, the
output of the parser stage was a single parse tree, and
this type of system has been shown to outperform
phrase-based translation on, for instance, Chinese-
to-English translation (Liu et al, 2006). More recent
work has shown that translation quality is improved
further if the parser outputs a weighted parse forest,
that is, a representation of a whole distribution over
possible parse trees (Mi et al, 2008). In this paper,
we investigate two hypotheses to explain why.
One hypothesis is that forest-to-string translation
selects worse parses. Although syntax often helps
translation, there may be situations where syntax, or
at least syntax in the way that our models use it, can
impose constraints that are too rigid for good-quality
translation (Liu et al, 2007; Zhang et al, 2008).
For example, suppose that a tree-to-string system
encounters the following correct tree (only partial
bracketing shown):
(1) [NP j??ngj?`
economy
ze?ngzha?ng]
growth
de
DE
su`du`
rate
?economic growth rate?
Suppose further that the model has never seen this
phrase before, although it has seen the subphrase
ze?ngzha?ng de su`du` ?growth rate?. Because this sub-
phrase is not a syntactic unit in sentence (1), the sys-
tem will be unable to translate it. But a forest-to-
string system would be free to choose another (in-
correct but plausible) bracketing:
(2) j??ngj?`
economy
[NP ze?ngzha?ng
growth
de
DE
su`du`]
rate
and successfully translate it using rules learned from
observed data.
The other hypothesis is that forest-to-string trans-
lation selects better parses. For example, if a Chi-
nese parser is given the input ca?njia? bia?ojie? de hu?nl??,
it might consider two structures:
(3) [VP ca?njia?
attend
bia?ojie?]
cousin
de
DE
hu?nl??
wedding
?wedding that attends a cousin?
(4) ca?njia?
attend
[NP bia?ojie?
cousin
de
DE
hu?nl??]
wedding
?attend a cousin?s wedding?
The two structures have two different translations
into English, shown above. While the parser prefers
structure (3), an n-gram language model would eas-
ily prefer translation (4) and, therefore, its corre-
sponding Chinese parse.
317
(a) f f f
parser
????? .
f f f
decoder?????? e e e e
source source target
string tree string
(b) f f f
parser
????? .
f f f
decoder?????? e e e e
source source target
string forest string
Figure 1: (a) In tree-to-string translation, the parser gen-
erates a single tree which the decoder must use to gen-
erate a translation. (b) In forest-to-string translation, the
parser generates a forest of possible trees, any of which
the decoder can use to generate a translation.
Previous work has shown that an observed target-
language translation can improve parsing of source-
language text (Burkett and Klein, 2008; Huang et al,
2009), but to our knowledge, only Chen et al (2011)
have explored the case where the target-language
translation is unobserved.
Below, we carry out experiments to test these
two hypotheses. We measure the accuracy (using
labeled-bracket F1) of the parses that the translation
model selects, and find that they are worse than the
parses selected by the parser. Our basic conclusion,
then, is that the parses that help translation (accord-
ing to Bleu) are, on average, worse parses. That is,
forest-to-string translation hurts parsing.
But there is a twist. Neither labeled-bracket F1
nor Bleu is a perfect metric of the phenomena it is
meant to measure, and our translation system is op-
timized to maximize Bleu. If we optimize our sys-
tem to maximize labeled-bracket F1 instead, we find
that our translation system selects parses that score
higher than the baseline parser?s. That is, forest-to-
string translation can help parsing.
2 Background
We provide here only a cursory overview of tree-
to-string and forest-to-string translation. For more
details, the reader is referred to the original papers
describing them (Liu et al, 2006; Mi et al, 2008).
Figure 1a illustrates the tree-to-string transla-
tion pipeline. The parser stage can be any phrase-
structure parser; it computes a parse for each source-
language string. The decoder stage translates the
source-language tree into a target-language string,
using a synchronous tree-substitution grammar.
In forest-to-string translation (Figure 1b), the
parser outputs a forest of possible parses of each
source-language string. The decoder uses the same
rules as in tree-to-string translation, but is free to se-
lect any of the trees contained in the parse forest.
3 Translation hurts parsing
The simplest experiment to carry out is to exam-
ine the parses actually selected by the decoder, and
see whether they are better or worse than the parses
selected by the parser. If they are worse, this sup-
ports the hypothesis that syntax can hurt translation.
If they are better, we can conclude that translation
can help parsing. In this initial experiment, we find
that the former is the case.
3.1 Setup
The baseline parser is the Charniak parser (Char-
niak, 2000). We trained it on the Chinese Treebank
(CTB) 5.1, split as shown in Table 1, following
Duan et al (2007).1 The parser outputs a parse forest
annotated with head words and other information.
Since the decoder does not use these annotations,
we use the max-rule algorithm (Petrov et al, 2006)
to (approximately) sum them out. As a side bene-
fit, this improves parsing accuracy from 77.76% to
78.42% F1. The weight of a hyperedge in this for-
est is its posterior probability, given the input string.
We retain these weights as a feature in the translation
model.
The decoder stage is a forest-to-string system (Liu
et al, 2006; Mi et al, 2008) for Chinese-to-English
translation. The datasets used are listed in Ta-
ble 1. We generated word alignments with GIZA++
and symmetrized them using the grow-diag-final-
and heuristic. We parsed the Chinese side using
the Charniak parser as described above, and per-
formed forest-based rule extraction (Mi and Huang,
2008) with a maximum height of 3 nodes. We used
the same features as Mi and Huang (2008). The
language model was a trigram model with modi-
fied Kneser-Ney smoothing (Kneser and Ney, 1995;
Chen and Goodman, 1998), trained on the target
1The more common split, used by Bikel and Chiang (2000),
has flaws that are described by Levy and Manning (2003).
318
Parsing Translation
Train CTB 1?815 FBIS
CTB 1101?1136
Dev CTB 900?931 NIST 2002
CTB 1148?1151
Test CTB 816?885 NIST 2003
CTB 1137?1147
Table 1: Data used for training and testing the parsing and
translation models.
Parsing Translation
System Objective F1% Bleu%
Charniak n/a 78.42 n/a
tree-to-string max-Bleu 78.42 23.07
forest-to-string max-Bleu 77.75 24.60
forest-to-string max-F1 78.81 19.18
Table 2: Forest-to-string translation outperforms tree-to-
string translation according to Bleu, but the decreases
parsing accuracy according to labeled-bracket F1. How-
ever, when we train to maximize labeled-bracket F1,
forest-to-string translation yields better parses than both
tree-to-string translation and the original parser.
side of the training data. We used minimum-error-
rate (MER) training to optimize the feature weights
(Och, 2003) to maximize Bleu.
At decoding time, we select the best derivation
and extract its source tree. In principle, we ought
to sum over all derivations for each source tree; but
the approximations that we tried (n-best list crunch-
ing, max-rule decoding, minimum Bayes risk) did
not appear to help.
3.2 Results
Table 2 shows the main results of our experiments.
In the second and third line, we see that the forest-
to-string system outperforms the tree-to-string sys-
tem by 1.53 Bleu, consistent with previously pub-
lished results (Mi et al, 2008; Zhang et al, 2009).
However, we also find that the trees selected by the
forest-to-string system score much lower according
to labeled-bracket F1. This suggests that the reason
the forest-to-string system is able to generate better
translations is that it can soften the constraints im-
posed by the syntax of the source language.
4 Translation helps parsing
We have found that better translations can be ob-
tained by settling for worse parses. However, trans-
lation accuracy is measured using Bleu and pars-
ing accuracy is measured using labeled-bracket F1,
and neither of these is a perfect metric of the phe-
nomenon it is meant to measure. Moreover, we op-
timized the translation model in order to maximize
Bleu. It is known that when MER training is used
to optimize one translation metric, other translation
metrics suffer (Och, 2003); much more, then, can
we expect that optimizing Bleu will cause labeled-
bracket F1 to suffer. In this section, we try optimiz-
ing labeled-bracket F1, and find that, in this case, the
translation model does indeed select parses that are
better on average.
4.1 Setup
MER training with labeled-bracket F1 as an objec-
tive function is straightforward. At each iteration of
MER training, we run the parser and decoder over
the CTB dev set to generate an n-best list of possible
translation derivations (Huang and Chiang, 2005).
For each derivation, we extract its Chinese parse tree
and compute the number of brackets guessed and
the number matched against the gold-standard parse
tree. A trivial modification of the MER trainer then
optimizes the feature weights to maximize labeled-
bracket F1.
A technical challenge that arises is ensuring di-
versity in the n-best lists. The MER trainer re-
quires that each list contain enough unique transla-
tions (when maximizing Bleu) or source trees (when
maximizing labeled-bracket F1). However, because
one source tree may lead to many translation deriva-
tions, the n-best list may contain only a few unique
source trees, or in the extreme case, the derivations
may all have the same source tree. We use a variant
of the n-best algorithm that allows efficient genera-
tion of equivalence classes of derivations (Huang et
al., 2006). The standard algorithm works by gener-
ating, at each node of the forest, a list of the best
subderivations at that node; the variant drops a sub-
derivation if it has the same source tree as a higher-
scoring subderivation.
319
Maximum
rule height F1%
3 78.81
4 78.93
5 79.14
LM data
(lines) F1%
none 78.78
100 78.79
30k 78.67
300k 79.14
13M 79.24
Features F1%
monolingual 78.89
+ bilingual 79.24
Parallel data
(lines) F1%
60k 78.00
120k 78.16
300k 79.24
(a) (b) (c) (d)
Table 3: Effect of variations on parsing performance. (a) Increasing the maximum translation rule height increases
parsing accuracy further. (b) Increasing/decreasing the language model size increases/decreases parsing accuracy.
(c) Decreasing the parallel text size decreases parsing accuracy. (d) Removing all bilingual features decreases parsing
accuracy, but only slightly.
4.2 Results
The last line of Table 2 shows the results of this
second experiment. The system trained to opti-
mize labeled-bracket F1 (max-F1) obtains a much
lower Bleu score than the one trained to maximize
Bleu (max-Bleu)?unsurprisingly, because a single
source-side parse can yield many different transla-
tions, but the objective function scores them equally.
What is more interesting is that the max-F1 system
obtains a higher F1 score, not only compared with
the max-Bleu system but also the original parser.
We then tried various settings to investigate what
factors affect parsing performance. First, we found
that increasing the maximum rule height increases
F1 further (Table 3a).
One of the motivations of our method is that bilin-
gual information (especially the language model)
can help disambiguate the source side structures. To
test this, we varied the size of the corpus used to train
the language model (keeping a maximum rule height
of 5 from the previous experiment). The 13M-line
language model adds the Xinhua portion of Giga-
word 3. In Table 3b we see that the parsing perfor-
mance does increase with the language model size,
with the largest language model yielding a net im-
provement of 0.82 over the baseline parser.
To test further the importance of bilingual infor-
mation, we compared against a system built only
from the Chinese side of the parallel text (with each
word aligned to itself). We removed all features that
use bilingual information, retaining only the parser
probability and the phrase penalty. In their place
we added a new feature, the probability of a rule?s
source side tree given its root label, which is essen-
tially the same model used in Data-Oriented Parsing
(Bod, 1992). Table 3c shows that this system still
outperforms the original parser. In other words, part
of the gain is not attributable to translation, but ad-
ditional source-side context and data that the trans-
lation model happens to capture.
Finally, we varied the size of the parallel text
(keeping a maximum rule height of 5 and the largest
language model) and found that, as expected, pars-
ing performance correlates with parallel data size
(Table 3d).
5 Conclusion
We set out to investigate why forest-to-string trans-
lation outperforms tree-to-string translation. By
comparing their performance as Chinese parsers, we
found that forest-to-string translation sacrifices pars-
ing accuracy, suggesting that forest-to-string trans-
lation works by overriding constraints imposed by
syntax. But when we optimized the system to max-
imize labeled-bracket F1, we found that, in fact,
forest-to-string translation is able to achieve higher
accuracy, by 0.82 F1%, than the baseline Chinese
parser, demonstrating that, to a certain extent, forest-
to-string translation is able to correct parsing errors.
Acknowledgements
We are grateful to the anonymous reviewers for
their helpful comments. This research was sup-
ported in part by DARPA under contract DOI-NBC
D11AP00244.
320
References
Daniel M. Bikel and David Chiang. 2000. Two statis-
tical parsing models applied to the Chinese Treebank.
In Proc. Second Chinese Language Processing Work-
shop, pages 1?6.
Rens Bod. 1992. A computational model of language
performance: Data Oriented Parsing. In Proc. COL-
ING 1992, pages 855?859.
David Burkett and Dan Klein. 2008. Two languages
are better than one (for syntactic parsing). In Proc.
EMNLP 2008, pages 877?886.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proc. NAACL, pages 132?139.
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. Technical Report TR-10-98, Harvard University
Center for Research in Computing Technology.
Wenliang Chen, Jun?ichi Kazama, Min Zhang, Yoshi-
masa Tsuruoka, Yujie Zhang, Yiou Wang, Kentaro
Torisawa, and Haizhou Li. 2011. SMT helps bitext
dependency parsing. In Proc. EMNLP 2011, pages
73?83.
Xiangyu Duan, Jun Zhao, and Bo Xu. 2007. Probabilis-
tic models for action-based Chinese dependency pars-
ing. In Proc. ECML 2007, pages 559?566.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proc. IWPT 2005, pages 53?64.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proc. AMTA 2006, pages 65?
73.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proc. EMNLP 2009, pages 1222?1231.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for M-gram language modeling. In Proc.
ICASSP 1995, pages 181?184.
Roger Levy and Christopher D. Manning. 2003. Is it
harder to parse Chinese, or the Chinese Treebank? In
Proc. ACL 2003, pages 439?446.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proc. COLING-ACL 2006, pages 609?616.
Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin. 2007.
Forest-to-string statistical translation rules. In Proc.
ACL 2007, pages 704?711.
Haitao Mi and Liang Huang. 2008. Forest-based trans-
lation rule extraction. In Proc. EMNLP 2008, pages
206?214.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. ACL-08: HLT, pages 192?
199.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proc. ACL 2003,
pages 160?167.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. COLING-ACL 2006,
pages 433?440.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008. A tree se-
quence alignment-based tree-to-tree translation model.
In Proc. ACL-08: HLT, pages 559?567.
Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw, and
Chew Lim Tan. 2009. Forest-based tree sequence to
string translation model. In Proc. ACL-IJCNLP 2009,
pages 172?180.
321
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 765?774,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Kneser-Ney Smoothing on Expected Counts
Hui Zhang
Department of Computer Science
University of Southern California
hzhang@isi.edu
David Chiang
Information Sciences Institute
University of Southern California
chiang@isi.edu
Abstract
Widely used in speech and language pro-
cessing, Kneser-Ney (KN) smoothing has
consistently been shown to be one of
the best-performing smoothing methods.
However, KN smoothing assumes integer
counts, limiting its potential uses?for ex-
ample, inside Expectation-Maximization.
In this paper, we propose a generaliza-
tion of KN smoothing that operates on
fractional counts, or, more precisely, on
distributions over counts. We rederive all
the steps of KN smoothing to operate
on count distributions instead of integral
counts, and apply it to two tasks where
KN smoothing was not applicable before:
one in language model adaptation, and the
other in word alignment. In both cases,
our method improves performance signifi-
cantly.
1 Introduction
In speech and language processing, smoothing is
essential to reduce overfitting, and Kneser-Ney
(KN) smoothing (Kneser and Ney, 1995; Chen
and Goodman, 1999) has consistently proven to be
among the best-performing and most widely used
methods. However, KN smoothing assumes inte-
ger counts, whereas in many NLP tasks, training
instances appear with possibly fractional weights.
Such cases have been noted for language model-
ing (Goodman, 2001; Goodman, 2004), domain
adaptation (Tam and Schultz, 2008), grapheme-to-
phoneme conversion (Bisani and Ney, 2008), and
phrase-based translation (Andr?es-Ferrer, 2010;
Wuebker et al, 2012).
For example, in Expectation-Maximization
(Dempster et al, 1977), the Expectation (E) step
computes the posterior distribution over possi-
ble completions of the data, and the Maximiza-
tion (M) step reestimates the model parameters as
if that distribution had actually been observed. In
most cases, the M step is identical to estimating
the model from complete data, except that counts
of observations from the E step are fractional. It
is common to apply add-one smoothing to the
M step, but we cannot apply KN smoothing.
Another example is instance weighting. If we
assign a weight to each training instance to indi-
cate how important it is (say, its relevance to a par-
ticular domain), and the counts are not integral,
then we again cannot train the model using KN
smoothing.
In this paper, we propose a generalization of KN
smoothing (called expected KN smoothing) that
operates on fractional counts, or, more precisely,
on distributions over counts. We rederive all the
steps of KN smoothing to operate on count distri-
butions instead of integral counts. We demonstrate
how to apply expected KN to two tasks where KN
smoothing was not applicable before. One is lan-
guage model domain adaptation, and the other is
word alignment using the IBM models (Brown et
al., 1993). In both tasks, expected KN smoothing
improves performance significantly.
2 Smoothing on integral counts
Before presenting our method, we review KN
smoothing on integer counts as applied to lan-
guage models, although, as we will demonstrate
in Section 7, KN smoothing is applicable to other
tasks as well.
2.1 Maximum likelihood estimation
Let uw stand for an n-gram, where u stands for
the (n ? 1) context words and w, the predicted
word. Let c(uw) be the number of occurrences
of uw. We use a bullet (?) to indicate summa-
tion over words, that is, c(u?) =
?
w
c(uw). Under
maximum-likelihood estimation (MLE), we max-
765
imize
L =
?
uw
c(uw) log p(w | u),
obtaining the solution
p
mle
(w | u) =
c(uw)
c(u?)
. (1)
2.2 Absolute discounting
Absolute discounting (Ney et al, 1994) ? on which
KN smoothing is based ? tries to generalize bet-
ter to unseen data by subtracting a discount from
each seen n-gram?s count and distributing the sub-
tracted discounts to unseen n-grams. For now, we
assume that the discount is a constant D, so that
the smoothed counts are
c?(uw) =
?
?
?
?
?
?
?
c(uw) ? D if c(uw) > 0
n
1+
(u?)Dq
u
(w) otherwise
where n
1+
(u?) = |{w | c(uw) > 0}| is the number
of word types observed after context u, and q
u
(w)
specifies how to distribute the subtracted discounts
among unseen n-gram types. Maximizing the like-
lihood of the smoothed counts c?, we get
p(w | u) =
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
c(uw) ? D
c(u?)
if c(uw) > 0
n
1+
(u?)Dq
u
(w)
c(u?)
otherwise.
(2)
How to choose D and q
u
(w) are described in the
next two sections.
2.3 Estimating D by leaving-one-out
The discount D can be chosen by various means;
in absolute discounting, it is chosen by the method
of leaving one out. Given N training instances, we
form the probability of each instance under the
MLE using the other (N ? 1) instances as train-
ing data; then we maximize the log-likelihood of
all those instances. The probability of an n-gram
token uw using the other tokens as training data is
p
loo
(w | u) =
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
c(uw) ? 1 ? D
c(u?) ? 1
c(uw) > 1
(n
1+
(u?) ? 1)Dq
u
(w)
c(u?) ? 1
c(uw) = 1.
We want to find the D that maximizes the
leaving-one-out log-likelihood
L
loo
=
?
uw
c(uw) log p
loo
(w | u)
=
?
uw|c(uw)>1
c(uw) log
c(uw) ? 1 ? D
c(u?) ? 1
+
?
uw|c(uw)=1
log
(n
1+
(u?) ? 1)Dq
u
(w)
c(u?) ? 1
=
?
r>1
rn
r
log(r ? 1 ? D) + n
1
logD +C, (3)
where n
r
= |{uw | c(uw) = r}| is the number of n-
gram types appearing r times, and C is a constant
not depending on D. Setting the partial derivative
with respect to D to zero, we have
?L
loo
?D
= ?
?
r>1
rn
r
r ? 1 ? D
+
n
1
D
n
1
D
=
?
r>1
rn
r
r ? 1 ? D
?
2n
2
1 ? D
.
Solving for D, we have
D ?
n
1
n
1
+ 2n
2
. (4)
Theoretically, we can use iterative methods to op-
timize D. But in practice, setting D to this upper
bound is effective and simple (Ney et al, 1994;
Chen and Goodman, 1999).
2.4 Estimating the lower-order distribution
Finally, q
u
(w) is defined to be proportional to an
(n ? 1)-gram model p
?
(w | u
?
), where u
?
is the
(n ? 2)-gram suffix of u. That is,
q
u
(w) = ?(u)p
?
(w | u
?
),
where ?(u) is an auxiliary function chosen to make
the distribution p(w | u) in (2) sum to one.
Absolute discounting chooses p
?
(w | u
?
) to be
the maximum-likelihood unigram distribution; un-
der KN smoothing (Kneser and Ney, 1995), it is
chosen to make p in (2) satisfy the following con-
straint for all (n ? 1)-grams u
?
w:
p
mle
(u
?
w) =
?
v
p(w | vu
?
)p
mle
(vu
?
). (5)
Substituting in the definition of p
mle
from (1) and
p from (2) and canceling terms, we get
c(u
?
w) =
?
v|c(vu
?
w)>0
(c(vu
?
w) ? D)
+
?
v|c(vu
?
w)=0
n
1+
(vu
?
?)D?(vu
?
)p
?
(w | u
?
).
766
Solving for p
?
(w | u
?
), we have
p
?
(w | u
?
) =
?
v|c(vu
?
w)>0
1
?
v|c(vu
?
w)=0
n
1+
(vu
?
?)?(vu
?
)
.
Kneser and Ney assume the denominator is con-
stant in w and renormalize to get an approximation
p
?
(w | u
?
) ?
n
1+
(?u
?
w)
n
1+
(?u
?
?)
, (6)
where
n
1+
(?u
?
w) = |{v | c(vu
?
w) > 0}|
n
1+
(?u
?
?) =
?
w
n
1+
(?u
?
w).
3 Count distributions
The computation of D and p
?
above made use of
n
r
and n
r+
, which presupposes integer counts. But
in many applications, the counts are not integral,
but fractional. How do we apply KN smoothing in
such cases? In this section, we introduce count dis-
tributions as a way of circumventing this problem.
3.1 Definition
In the E step of EM, we compute a probability dis-
tribution (according to the current model) over all
possible completions of the observed data, and the
expected counts of all types, which may be frac-
tional. However, note that in each completion of
the data, the counts are integral. Although it does
not make sense to compute n
r
or n
r+
on fractional
counts, it does make sense to compute them on
possible completions.
In other situations where fractional counts arise,
we can still think of the counts as expectations un-
der some distribution over possible ?realizations?
of the data. For example, if we assign a weight
between zero and one to every instance in a cor-
pus, we can interpret each instance?s weight as the
probability of that instance occurring or not, yield-
ing a distribution over possible subsets of the data.
Let X be a random variable ranging over pos-
sible realizations of the data, and let c
X
(uw) be
the count of uw in realization X. The expecta-
tion E[c
X
(uw)] is the familiar fractional expected
count of uw, but we can also compute the proba-
bilities p(c
X
(uw) = r) for any r. From now on, for
brevity, we drop the subscript X and understand
c(uw) to be a random variable depending on X.
The n
r
(u?) and n
r+
(u?) and related quantities also
become random variables depending on X.
For example, suppose that our data consists of
the following bigrams, with their weights:
(a) fat cat 0.3
(b) fat cat 0.8
(c) big dog 0.9
We can interpret this as a distribution over eight
subsets (not all distinct), with probabilities:
? 0.7 ? 0.2 ? 0.1 = 0.014
{a} 0.3 ? 0.2 ? 0.1 = 0.006
{b} 0.7 ? 0.8 ? 0.1 = 0.056
{a, b} 0.3 ? 0.8 ? 0.1 = 0.024
{c} 0.7 ? 0.2 ? 0.9 = 0.126
{a, c} 0.3 ? 0.2 ? 0.9 = 0.054
{b, c} 0.7 ? 0.8 ? 0.9 = 0.504
{a, b, c} 0.3 ? 0.8 ? 0.9 = 0.216
Then the count distributions and the E[n
r
] are:
r = 1 r = 2 r > 0
p(c(fat cat) = r) 0.62 0.24 0.86
p(c(big dog) = r) 0.9 0 0.9
E[n
r
] 1.52 0.24
3.2 Efficient computation
How to compute these probabilities and expecta-
tions depends in general on the structure of the
model. If we assume that all occurrences of uw
are independent (although in fact they are not al-
ways), the computation is very easy. If there are
k occurrences of uw, each occurring with proba-
bility p
i
, the count c(uw) is distributed according
to the Poisson-binomial distribution (Hong, 2013).
The expected count E[c(uw)] is just
?
i
p
i
, and the
distribution of c(uw) can be computed as follows:
p(c(uw) = r) = s(k, r)
where s(k, r) is defined by the recurrence
s(k, r) =
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
s(k ? 1, r)(1 ? p
k
)
+ s(k ? 1, r ? 1)p
k
if 0 ? r ? k
1 if k = r = 0
0 otherwise.
We can also compute
p(c(uw) ? r) = max
{
s(m, r), 1 ?
?
r
?
<r
s(m, r
?
)
}
,
the floor operation being needed to protect against
rounding errors, and we can compute
E[n
r
(u?)] =
?
w
p(c(uw) = r)
E[n
r+
(u?)] =
?
w
p(c(uw) ? r).
Since, as we shall see, we only need to compute
these quantities up to a small value of r (2 or 4),
this takes time linear in k.
767
4 Smoothing on count distributions
We are now ready to describe how to apply KN
smoothing to count distributions. Below, we reca-
pitulate the derivation of KN smoothing presented
in Section 2, using the expected log-likelihood
in place of the log-likelihood and applying KN
smoothing to each possible realization of the data.
4.1 Maximum likelihood estimation
The MLE objective function is the expected log-
likelihood,
E[L] = E
?
?
?
?
?
?
?
?
uw
c(uw) log p(w | u)
?
?
?
?
?
?
?
=
?
uw
E[c(uw)] log p(w | u)
whose maximum is
p
mle
(w | u) =
E[c(uw)]
E[c(u?)]
. (7)
4.2 Absolute discounting
If we apply absolute discounting to every realiza-
tion of the data, the expected smoothed counts are
E[c?(uw)] =
?
r>0
p(c(uw) = r)(r ? D)
+ p(c(uw) = 0)E[n
1+
(u?)]Dq
u
(w)
= E[c(uw)] ? p(c(uw) > 0)D
+ p(c(uw) = 0)E[n
1+
(u?)]Dq
u
(w) (8)
where, to be precise, the expectation E[n
1+
(u?)]
should be conditioned on c(uw) = 0; in practice, it
seems safe to ignore this. The MLE is then
p(w | u) =
E[c?(uw)]
E[c?(u?)]
. (9)
4.3 Estimating D by leaving-one-out
It would not be clear how to perform leaving-
one-out estimation on fractional counts, but here
we have a distribution over realizations of the
data, each with integral counts, and we can
perform leaving-one-out estimation on each of
these. In other words, our goal is to find the D
that maximizes the expected leaving-one-out log-
likelihood, which is just the expected value of (3):
E[L
loo
] = E
[
n
1
logD +
?
r>1
rn
r
log(r ? 1 ? D) +C
]
= E[n
1
] logD
+
?
r>1
rE[n
r
] log(r ? 1 ? D) +C,
where C is a constant not depending on D. We
have made the assumption that the n
r
are indepen-
dent.
By exactly the same reasoning as before, we ob-
tain an upper bound for D:
D ?
E[n
1
]
E[n
1
] + 2E[n
2
]
. (10)
In our example above, D =
1.52
1.52+2?0.24
= 0.76.
4.4 Estimating the lower-order distribution
We again require p
?
to satisfy the marginal con-
straint (5). Substituting in (7) and solving for p
?
as
in Section 2.4, we obtain the solution
p
?
(w | u
?
) =
E[n
1+
(?u
?
w)]
E[n
1+
(?u
?
?)]
. (11)
For the example above, the estimates for the un-
igram model p
?
(w) are
p
?
(cat) =
0.86
0.86+0.9
? 0.489
p
?
(dog) =
0.9
0.86+0.9
? 0.511.
4.5 Extensions
Chen and Goodman (1999) introduce three exten-
sions to Kneser-Ney smoothing which are now
standard. For our experiments, we used all three,
for both integral counts and count distributions.
4.5.1 Interpolation
In interpolated KN smoothing, the subtracted dis-
counts are redistributed not only among unseen
events but also seen events. That is,
c?(uw) = max{0, c(uw) ? D} + n
1+
(u?)Dp
?
(w | u
?
).
In this case, ?(u) is always equal to one, so that
q
u
(w) = p
?
(w | u
?
). (Also note that (6) becomes
an exact solution to the marginal constraint.) The-
oretically, this requires us to derive a new estimate
for D. However, as this is not trivial, nearly all im-
plementations simply use the original estimate (4).
On count distributions, the smoothed counts be-
come
E[c?(uw)] = E[c(uw)] ? p(c(uw) > 0)D
+ E[n
1+
(u?)]Dp
?
(w | u
?
). (12)
In our example, the smoothed counts are:
uw E[c?]
fat cat 1.1 ? 0.86 ? 0.76 + 0.86 ? 0.76 ? 0.489 ? 0.766
fat dog 0 ? 0 ? 0.76 + 0.86 ? 0.76 ? 0.511 ? 0.334
big cat 0 ? 0 ? 0.76 + 0.9 ? 0.76 ? 0.489 ? 0.334
big dog 0.9 ? 0.9 ? 0.76 + 0.9 ? 0.76 ? 0.511 ? 0.566
768
which give the smoothed probability estimates:
p(cat | fat) =
0.766
0.766+0.334
= 0.696
p(dog | fat) =
0.334
0.766+0.334
= 0.304
p(dog | big) =
0.334
0.334+0.556
= 0.371
p(cat | big) =
0.556
0.334+0.556
= 0.629.
4.5.2 Modified discounts
Modified KN smoothing uses a different discount
D
r
for each count r < 3, and a discount D
3+
for
counts r ? 3. On count distributions, a similar ar-
gument to the above leads to the estimates:
D
1
? 1 ? 2Y
E[n
2
]
E[n
1
]
D
2
? 2 ? 3Y
E[n
3
]
E[n
2
]
D
3+
? 3 ? 4Y
E[n
4
]
E[n
3
]
Y =
E[n
1
]
E[n
1
] + 2E[n
2
]
.
(13)
One side-effect of this change is that (6) is no
longer the correct solution to the marginal con-
straint (Teh, 2006; Sundermeyer et al, 2011). Al-
though this problem can be fixed, standard imple-
mentations simply use (6).
4.5.3 Recursive smoothing
In the original KN method, the lower-order
model p
?
was estimated using (6); recursive KN
smoothing applies KN smoothing to p
?
. To do this,
we need to reconstruct counts whose MLE is (6).
On integral counts, this is simple: we generate, for
each n-gram type vu
?
w, an (n?1)-gram token u
?
w,
for a total of n
1+
(?u
?
w) tokens. We then apply KN
smoothing to these counts.
Analogously, on count distributions, for each n-
gram type vu
?
w, we generate an (n ? 1)-gram to-
ken u
?
w with probability p(c(vu
?
w) > 0). Since
E[c(u
?
w)] =
?
v
p(c(vu
?
w) > 0) = E[n
1+
(?u
?
w)],
this has (11) as its MLE and therefore satisfies the
marginal constraint. We then apply expected KN
smoothing to these count distributions.
For the example above, the count distributions
used for the unigram distribution would be:
r = 0 r = 1
p(c(cat) = r) 0.14 0.86
p(c(dog) = r) 0.1 0.9
4.6 Summary
In summary, to perform expected KN smoothing
(either the original version or Chen and Good-
man?s modified version), we perform the steps
listed below:
orig. mod.
compute count distributions ?3.2
estimate discount D (10) (13)
estimate lower-order model p
?
(11) ?4.5.3
compute smoothed counts c? (8) (12)
compute probabilities p (9)
The computational complexity of expected KN
is almost identical to KN on integral counts. The
main addition is computing and storing the count
distributions. Using the dynamic program in Sec-
tion 3.2, computing the distributions for each r is
linear in the number of n-gram types, and we only
need to compute the distributions up to r = 2 (or
r = 4 for modified KN), and store them for r = 0
(or up to r = 2 for modified KN).
5 Related Work
Witten-Bell (WB) smoothing is somewhat easier
than KN to adapt to fractional counts. The SRI-
LM toolkit (Stolcke, 2002) implements a method
which we call fractional WB:
p(w | u) = ?(u)p
mle
(w | u) + (1 ? ?(u))p
?
(w | u
?
)
?(u) =
E[c(u)]
E[c(u)] + n
1+
(u?)
,
where n
1+
(u?) is the number of word types ob-
served after context u, computed by ignoring all
weights. This method, although simple, inconsis-
tently uses weights for counting tokens but not
types. Moreover, as we will see below, it does not
perform as well as expected KN.
The only previous adaptation of KN smoothing
to fractional counts that we are aware of is that
of Tam and Schultz (2008) and Bisani and Ney
(2008), called fractional KN. This method sub-
tracts D directly from the fractional counts, zero-
ing out counts that are smaller than D. The dis-
count D must be set by minimizing an error metric
on held-out data using a line search (Tam, p. c.) or
Powell?s method (Bisani and Ney, 2008), requiring
repeated estimation and evaluation of the language
model. By contrast, we choose D by leaving-one-
out. Like KN on integral counts, our method has
a closed-form approximation and requires neither
held-out data nor trial and error.
769
6 Language model adaptation
N-gram language models are widely used in appli-
cations like machine translation and speech recog-
nition to select fluent output sentences. Although
they can easily be trained on large amounts of data,
in order to perform well, they should be trained on
data containing the right kind of language. For ex-
ample, if we want to model spoken language, then
we should train on spoken language data. If we
train on newswire, then a spoken sentence might
be regarded as ill-formed, because the distribution
of sentences in these two domains are very differ-
ent. In practice, we often have limited-size training
data from a specific domain, and large amounts
of data consisting of language from a variety of
domains (we call this general-domain data). How
can we utilize the large general-domain dataset to
help us train a model on a specific domain?
Many methods (Lin et al, 1997; Gao et al,
2002; Klakow, 2000; Moore and Lewis, 2010; Ax-
elrod et al, 2011) rank sentences in the general-
domain data according to their similarity to the
in-domain data and select only those with score
higher than some threshold. Such methods are ef-
fective and widely used. However, sometimes it is
hard to say whether a sentence is totally in-domain
or out-of-domain; for example, quoted speech in a
news report might be partly in-domain if the do-
main of interest is broadcast conversation. Here,
we propose to assign each sentence a probability
to indicate how likely it is to belong to the domain
of interest, and train a language model using ex-
pected KN smoothing. We show that this approach
yields models with much better perplexity than the
original sentence-selection approach.
6.1 Method
One of the most widely used sentence-selection
approaches is that of Moore and Lewis (2010).
They first train two language models, p
in
on a set
of in-domain data, and p
out
on a set of general-
domain data. Then each sentence w is assigned a
score
H(w) =
log(p
in
(w)) ? log(p
out
(w))
|w|
.
They set a threshold on the score to select a subset.
We adapt this approach as follows. After selec-
tion, for each sentence in the subset, we use a sig-
moid function to map the scores into probabilities:
p(w is in-domain) =
1
1 + exp(?H(w))
.
.
.
0 0.2 0.4
0.6
0.8 1 1.2 1.4
140
160
180
200
220
240
260
sentences selected (?10
7
)
p
e
r
p
l
e
x
i
t
y
. .
fractional KN
. .
fractional WB
. .
integral KN
. .
expected KN
Figure 1: On the language model adaptation task,
expected KN outperforms all other methods across
all sizes of selected subsets. Integral KN is ap-
plied to unweighted instances, while fractional
WB, fractional KN and expected KN are applied
to weighted instances.
Then we use the weighted subset to train a lan-
guage model with expected KN smoothing.
6.2 Experiments
Moore and Lewis (2010) test their method by
partitioning the in-domain data into training data
and test data, both of which are disjoint from
the general-domain data. They use the in-domain
training data to select a subset of the general-
domain data, build a language model on the se-
lected subset, and evaluate its perplexity on the in-
domain test data. Here, we follow this experimen-
tal framework and compare Moore and Lewis?s
unweighted method to our weighted method.
For our experiments, we used all the English
data allowed for the BOLT Phase 1 Chinese-
English evaluation. We took 60k sentences (1.7M
words) of web forum data as in-domain data,
further subdividing it into 54k sentences (1.5M
words) for training, 3k sentences (100k words)
for testing, and 3k sentences (100k words) for fu-
ture use. The remaining 12.7M sentences (268M
words) we treated as general-domain data.
We trained trigram language models and com-
pared expected KN smoothing against integral KN
smoothing, fractional WB smoothing, and frac-
tional KN smoothing, measuring perplexity across
various subset sizes (Figure 1). For fractional KN,
for each subset size, we optimized D to mini-
770
mize perplexity on the test set to give it the great-
est possible advantage; nevertheless, it is clearly
the worst performer. Expected KN consistently
gives the best perplexity, and, at the optimal sub-
set size, obtains better perplexity (148) than the
other methods (156 for integral KN, 162 for frac-
tional WB and 197 for fractional KN). Finally, we
note that integral KN is very sensitive to the subset
size, whereas expected KN and the other methods
are more robust.
7 Word Alignment
In this section, we show how to apply expected KN
to the IBM word alignment models (Brown et al,
1993). This illustrates both how to use expected
KN inside EM and how to use it beyond language
modeling. Of course, expected KN can be applied
to other instances of EM besides word alignment.
7.1 Problem
Given a French sentence f = f
1
f
2
? ? ? f
m
and its
English translation e = e
1
e
2
? ? ? e
n
, an alignment a
is a sequence a
1
, a
2
, . . . , a
m
, where a
i
is the index
of the English word which generates the French
word f
i
, or NULL. As is common, we assume that
each French word can only be generated from one
English word or from NULL (Brown et al, 1993;
Och and Ney, 2003; Vogel et al, 1996).
The IBM models and related models define
probability distributions p(a, f | e, ?), which model
how likely a French sentence f is to be generated
from an English sentence ewith word alignment a.
Different models parameterize this probability dis-
tribution in different ways. For example, Model 1
only models the lexical translation probabilities:
p(a, f | e, ?) ?
m
?
j=1
p( f
j
| e
a
j
).
Models 2?5 and the HMM model introduce addi-
tional components to model word order and fer-
tility. All, however, have the lexical translation
model p( f
j
| e
i
) in common. It also contains most
of the model?s parameters and is where overfit-
ting occurs most. Thus, here we only apply KN
smoothing to the lexical translation probabilities,
leaving the other model components for future
work.
7.2 Method
The f and e are observed, while a is a latent vari-
able. Normally, in the E step, we collect expected
counts E[c(e, f )] for each e and f . Then, in the M
step, we find the parameter values that maximize
their likelihood. However, MLE is prone to over-
fitting, one symptom of which is the ?garbage col-
lection? phenomenon where a rare English word is
wrongly aligned to many French words.
To reduce overfitting, we use expected KN
smoothing during the M step. That is, during the
E step, we calculate the distribution of c(e, f ) for
each e and f , and during the M step, we train a
language model on bigrams e f using expected KN
smoothing (that is, with u = e and w = f ). This
gives a smoothed probability estimate for p( f | e).
One question that arises is: what distribution to
use as the lower-order distribution p
?
? Following
common practice in language modeling, we use
the unigram distribution p( f ) as the lower-order
distribution. We could also use the uniform distri-
bution over word types, or a distribution that as-
signs zero probability to all known word types.
(The latter case is equivalent to a backoff language
model, where, since all bigrams are known, the
lower-order model is never used.) Below, we com-
pare the performance of all three choices.
7.3 Alignment experiments
We modified GIZA++ (Och and Ney, 2003) to
perform expected KN smoothing as described
above. Smoothing is enabled or disabled with a
command-line switch, making direct comparisons
simple. Our implementation is publicly available
as open-source software.
1
We carried out experiments on two language
pairs: Arabic to English and Czech to English.
For Arabic-English, we used 5.4+4.3 million
words of parallel text from the NIST 2009 con-
strained task,
2
and 346 word-aligned sentence
pairs (LDC2006E86) for evaluation. For Czech-
English, we used all 2.0+2.2 million words of
training data from the WMT 2009 shared task,
and 515 word-aligned sentence pairs (Bojar and
Prokopov?a, 2006) for evaluation.
For all methods, we used five iterations of IBM
Models 1, 2, and HMM, followed by three iter-
ations of IBM Models 3 and 4. We applied ex-
pected KN smoothing to all iterations of all mod-
els. We aligned in both the foreign-to-English
1
https://github.com/hznlp/giza-kn
2
All data was used except for: United Nations pro-
ceedings (LDC2004E13), ISI Automatically Extracted Par-
allel Text (LDC2007E08), and Ummah newswire text
(LDC2004T18).
771
Alignment F1 Bleu
Smoothing p
?
Ara-Eng Cze-Eng Ara-Eng Cze-Eng
none (baseline) ? 66.5 67.2 37.0 16.6
variational Bayes uniform 65.7 65.5 36.5 16.6
fractional WB
unigram 60.1 63.7 ? ?
uniform 60.8 66.5 37.8 16.9
zero 60.8 65.2 ? ?
fractional KN unigram 67.7 70.2 37.2 16.5
expected KN
unigram 69.7 71.9 38.2 17.0
uniform 69.4 71.3 ? ?
zero 69.2 71.9 ? ?
Table 1: Expected KN (interpolating with the unigram distribution) consistently outperforms all other
methods. For variational Bayes, we followed Riley and Gildea (2012) in setting ? to zero (so that the
choice of p
?
is irrelevant). For fractional KN, we chose D to maximize F1 (see Figure 2).
and English-to-foreign directions and then used
the grow-diag-final method to symmetrize them
(Koehn et al, 2003), and evaluated the alignments
using F-measure against gold word alignments.
As shown in Table 1, for KN smoothing, in-
terpolation with the unigram distribution performs
the best, while for WB smoothing, interestingly,
interpolation with the uniform distribution per-
forms the best. The difference can be explained by
the way the two smoothing methods estimate p
?
.
Consider again a training example with a word e
that occurs nowhere else in the training data. In
WB smoothing, p
?
( f ) is the empirical unigram
distribution. If f contains a word that is much
more frequent than the correct translation of e,
then smoothing may actually encourage the model
to wrongly align e with the frequent word. This
is much less of a problem in KN smoothing,
where p
?
is estimated from bigram types rather
than bigram tokens.
We also compared with variational Bayes (Ri-
ley and Gildea, 2012) and fractional KN. Overall,
expected KN performs the best. Variational Bayes
is not consistent across different language pairs.
While fractional KN does beat the baseline for
both language pairs, the value of D, which we op-
timized D to maximize F1, is not consistent across
language pairs: as shown in Figure 2, on Arabic-
English, a smaller D is better, while for Czech-
English, a larger D is better. By contrast, expected
KN uses a closed-form expression for D that out-
performs the best performance of fractional KN.
Table 2 shows that, if we apply expected KN
smoothing to only selected stages of training,
adding smoothing always brings an improvement,
.
0 0.2 0.4
0.6
0.8 1
64
66
68
70
72
D
a
l
i
g
n
m
e
n
t
F
1
. .
Cze-Eng
. .
Ara-Eng
Figure 2: Alignment F1 vs. D of fractional KN
smoothing for word alignment.
Smoothed models Alignment F1
1 2 H 3 4 Ara-Eng Cze-Eng
? ? ? ? ? 66.5 67.2
? ? ? ? ? 67.3 67.9
? ? ? ? ? 68.0 68.7
? ? ? ? ? 68.6 70.0
? ? ? ? ? 66.9 68.4
? ? ? ? ? 67.0 68.6
? ? ? ? ? 69.7 71.9
Table 2: Smoothing more stages of training makes
alignment accuracy go up. For each row, we
smoothed all iterations of the models indicated.
Key: H = HMM model; ? = smoothing enabled;
? = smoothing disabled.
772
with the best setting being to smooth all stages.
This shows that expected KN smoothing is consis-
tently effective. It is also interesting to note that
smoothing is less helpful for the fertility-based
Models 3 and 4. Whether this is because modeling
fertility makes them less susceptible to ?garbage
collection,? or the way they approximate the E step
makes them less amenable to smoothing, or an-
other reason, would require further investigation.
7.4 Translation experiments
Finally, we ran MT experiments to see whether the
improved alignments also lead to improved trans-
lations. We used the same training data as before.
For the Arabic-English tasks, we used the NIST
2008 test set as development data and the NIST
2009 test set as test data; for the Czech-English
tasks, we used the WMT 2008 test set as develop-
ment data and the WMT 2009 test set as test data.
We used the Moses toolkit (Koehn et al, 2007)
to build MT systems using various alignments
(for expected KN, we used the one interpolated
with the unigram distribution, and for fractional
WB, we used the one interpolated with the uni-
form distribution). We used a trigram language
model trained on Gigaword (AFP, AP World-
stream, CNA, and Xinhua portions), and minimum
error-rate training (Och, 2003) to tune the feature
weights.
Table 1 shows that, although the relationship
between alignment F1 and Bleu is not very con-
sistent, expected KN smoothing achieves the best
Bleu among all these methods and is significantly
better than the baseline (p < 0.01).
8 Conclusion
For a long time, and as noted by many authors,
the usage of KN smoothing has been limited by its
restriction to integer counts. In this paper, we ad-
dressed this issue by treating fractional counts as
distributions over integer counts and generalizing
KN smoothing to operate on these distributions.
This generalization makes KN smoothing, widely
considered to be the best-performing smoothing
method, applicable to many new areas. We have
demonstrated the effectiveness of our method in
two such areas and showed significant improve-
ments in both.
Acknowledgements
We thank Qing Dou, Ashish Vaswani, Wilson Yik-
Cheung Tam, and the anonymous reviewers for
their input to this work. This research was sup-
ported in part by DOI IBC grant D12AP00225.
References
Jes?us Andr?es-Ferrer. 2010. Statistical approaches for
natural language modelling and monotone statisti-
cal machine translation. Ph.D. thesis, Universidad
Polit?ecnica de Valencia.
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In Proc. EMNLP, pages 355?362.
Maximilian Bisani and Hermann Ney. 2008. Joint-
sequence models for grapheme-to-phoneme conver-
sion. Speech Communication, 50(5):434?451.
Ondr?ej Bojar and Magdalena Prokopov?a. 2006.
Czech-English word alignment. In Proc. LREC,
pages 1236?1239.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19:263?311.
Stanley F. Chen and Joshua Goodman. 1999. An
empirical study of smoothing techniques for lan-
guage modeling. Computer Speech and Language,
13:359?394.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical So-
ciety, Series B, 39:1?38.
Jianfeng Gao, Joshua Goodman, Mingjing Li, and Kai-
Fu Lee. 2002. Toward a unified approach to statisti-
cal language modeling for Chinese. ACM Transac-
tions on Asian Language Information, 1:3?33.
Joshua T. Goodman. 2001. A bit of progress in lan-
guage modeling: Extended version. Technical Re-
port MSR-TR-2001-72, Microsoft Research.
Joshua Goodman. 2004. Exponential priors for maxi-
mum entropy models. In Proc. HLT-NAACL, pages
305?312.
Yili Hong. 2013. On computing the distribution func-
tion for the Poisson binomial distribution. Compu-
tational Statistics and Data Analysis, 59:41?51.
Dietrich Klakow. 2000. Selecting articles from the
language model training corpus. In Proc. ICASSP,
pages 1695?1698.
773
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for M-gram language modeling. In
Proc. ICASSP 1995, pages 181?184.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
HLT-NAACL, pages 127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. ACL, Companion Volume, pages 177?180.
Sung-Chien Lin, Chi-Lung Tsai, Lee-Feng Chien, Keh-
Jiann Chen, and Lin-Shan Lee. 1997. Chinese lan-
guage model adaptation based on document classifi-
cation and multiple domain-specific language mod-
els. In Proc. Eurospeech, pages 1463?1466.
Robert Moore and William Lewis. 2010. Intelligent
selection of language model training data. In Proc.
ACL, pages 220?224.
Hermann Ney, Ute Essen, and Reinhard Kneser.
1994. On structuring probabilistic dependencies in
stochastic language modelling. Computer Speech
and Language, 8:1?38, 1.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. ACL, pages
160?167.
Darcey Riley and Daniel Gildea. 2012. Improving
the IBM alignment models using variational Bayes.
In Proc. ACL (Volume 2: Short Papers), pages 306?
310.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proc. International Con-
ference on Spoken Language Processing, volume 2,
pages 901?904.
Martin Sundermeyer, Ralf Schl?uter, and Hermann Ney.
2011. On the estimation of discount parameters for
language model smoothing. In Proc. Interspeech,
pages 1433?1436.
Yik-Cheung Tam and Tanja Schultz. 2008. Correlated
bigram LSA for unsupervised language model adap-
tation. In Proc. NIPS, pages 1633?1640.
Yee Whye Teh. 2006. A hierarchical Bayesian lan-
guage model based on Pitman-Yor processes. In
Proc. COLING-ACL, pages 985?992.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical
translation. In Proc. COLING, pages 836?841.
Joern Wuebker, Mei-Yuh Hwang, and Chris Quirk.
2012. Leave-one-out phrase model training for
large-scale deployment. In Proc. WMT, pages 460?
467.
774
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 816?821,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Observational Initialization of Type-Supervised Taggers
Hui Zhang
?
Department of Computer Science
University of Southern California
hzhang@isi.edu
John DeNero
Google, Inc.
denero@google.com
Abstract
Recent work has sparked new interest
in type-supervised part-of-speech tagging,
a data setting in which no labeled sen-
tences are available, but the set of allowed
tags is known for each word type. This
paper describes observational initializa-
tion, a novel technique for initializing EM
when training a type-supervised HMM
tagger. Our initializer allocates probabil-
ity mass to unambiguous transitions in an
unlabeled corpus, generating token-level
observations from type-level supervision.
Experimentally, observational initializa-
tion gives state-of-the-art type-supervised
tagging accuracy, providing an error re-
duction of 56% over uniform initialization
on the Penn English Treebank.
1 Introduction
For many languages, there exist comprehensive
dictionaries that list the possible parts-of-speech
for each word type, but there are no corpora la-
beled with the part-of-speech of each token in con-
text. Type-supervised tagging (Merialdo, 1994)
explores this scenario; a model is provided with
type-level information, such as the fact that ?only?
can be an adjective, adverb, or conjunction, but
not any token-level information about which in-
stances of ?only? in a corpus are adjectives. Re-
cent research has focused on using type-level su-
pervision to infer token-level tags. For instance,
Li et al (2012) derive type-level supervision from
Wiktionary, Das and Petrov (2011) and T?ackstr?om
et al (2013) project type-level tag sets across lan-
guages, and Garrette and Baldridge (2013) solicit
type-level annotations directly from speakers. In
all of these efforts, a probabilistic sequence model
is trained to disambiguate token-level tags that are
?
Research conducted during an internship at Google.
constrained to match type-level tag restrictions.
This paper describes observational initialization,
a simple but effective learning technique for train-
ing type-supervised taggers.
A hidden Markov model (HMM) can be used
to disambiguate tags of individual tokens by max-
imizing corpus likelihood using the expectation
maximization (EM) algorithm. Our approach is
motivated by a suite of oracle experiments that
demonstrate the effect of initialization on the fi-
nal tagging accuracy of an EM-trained HMM tag-
ger. We show that initializing EM with accurate
transition model parameters is sufficient to guide
learning toward a high-accuracy final model.
Inspired by this finding, we introduce obser-
vational initialization, which is a simple method
to heuristically estimate transition parameters for
a corpus using type-level supervision. Transi-
tion probabilities are estimated from unambiguous
consecutive tag pairs that arise when two consec-
utive words each have only a single allowed tag.
These unambiguous word pairs can be tagged cor-
rectly without any statistical inference. Initializing
EM with the relative frequency of these unambigu-
ous pairs improves tagging accuracy dramatically
over uniform initialization, reducing errors by
56% in English and 29% in German. This efficient
and data-driven approach gives the best reported
tagging accuracy for type-supervised sequence
models, outperforming the minimized model of
Ravi and Knight (2009), the Bayesian LDA-based
model of Toutanova and Johnson (2008), and an
HMM trained with language-specific initialization
described by Goldberg et al (2008).
2 Type-Supervised Tagging
A first-order Markov model for part-of-speech
tagging defines a distribution over sentences for
which a single tag is given to each word token.
Let w
i
?W refer to the ith word in a sentence w,
drawn from language vocabulary W . Likewise,
816
ti
? T is the tag in tag sequence t of the ith word,
drawn from tag inventory T . The joint probabil-
ity of a sentence can be expressed in terms of two
sets of parameters for conditional multinomial dis-
tributions: ? defines the probability of a tag given
its previous tag and ? defines the probability of a
word given its tag.
P
?,?
(w, t) =
|w|
?
i=1
P
?
(t
i
|t
i?1
) ? P
?
(w
i
|t
i
)
Above, t
0
is a fixed start-of-sentence tag.
For a set of sentences S, the EM algorithm can
be used to iteratively find a local maximum of the
corpus log-likelihood:
`(?, ?;S) =
?
w?S
ln
[
?
t
P
?,?
(w, t)
]
The parameters ? and ? can then be used to predict
the most likely sequence of tags for each sentence
under the model:
?
t(w) = arg max
t
P
?,?
(w, t)
Tagging accuracy is the fraction of these tags in
?
t(w) that match hand-labeled oracle tags t
?
(w).
Type Supervision. In addition to an unlabeled
corpus of sentences, type-supervised models also
have access to a tag dictionary D ? W ? T that
contains all allowed word-tag pairs. For an EM-
trained HMM, initially setting P
?
(w|t) = 0 for all
(w, t) /? D ensures that all words will be labeled
with allowed tags.
Tag dictionaries can be derived from various
sources, such as lexicographic resources (Li et
al., 2012) and cross-lingual projections (Das and
Petrov, 2011). In this paper, we will follow pre-
vious work in deriving the tag dictionary from
a labeled corpus (Smith and Eisner, 2005); this
synthetic setting maximizes experiment repeata-
bility and allows for direct comparison of type-
supervised learning techniques.
Transductive Applications. We consider a
transductive data setting in which the test set is
available during training. In this case, the model
is not required to generalize to unseen examples or
unknown words, as in the typical inductive setting.
Transductive learning arises in document clus-
tering and corpus analysis applications. For ex-
ample, before running a document clustering al-
gorithm on a fixed corpus of documents, it may be
useful to tag each word with its most likely part-
of-speech in context, disambiguating the lexical
features in a bag-of-words representation. In cor-
pus analysis or genre detection, it may be useful
to determine for a fixed corpus the most common
part-of-speech for each word type, which could be
inferred by tagging each word with its most likely
part-of-speech. In both cases, the set of sentences
to tag is known in advance of learning.
3 Initializing HMM Taggers
The EM algorithm is sensitive to initialization. In
a latent variable model, different parameter values
may yield similar data likelihoods but very differ-
ent predictions. We explore this issue via exper-
iments on the Wall Street Journal section of the
English Penn Treebank (Marcus et al, 1993). We
adopt the transductive data setting introduced by
Smith and Eisner (2005) and used by Goldwa-
ter and Griffiths (2007), Toutanova and Johnson
(2008) and Ravi and Knight (2009); models are
trained on all sections 00-24, the tag dictionary D
is constructed by allowing all word-tag pairs ap-
pearing in the entire labeled corpus, and the tag-
ging accuracy is evaluated on a 1005 sentence sub-
set sampled from the corpus.
The degree of variation in tagging accuracy due
to initialization can be observed most clearly by
two contrasting initializations. UNIFORM initial-
izes the model with uniform distributions over al-
lowed outcomes:
P
?
(t|t
?
) =
1
|T |
P
?
(w|t) =
1
|{w : (w, t) ? D}|
SUPERVISED is an oracle setting that initializes
the model with the relative frequency of observed
pairs in a labeled corpus:
P
?
(t|t
?
) ?
?
(w,t
?
)
|w|
?
i=1
?((t
?
i
, t
?
i?1
), (t, t
?
))
P
?
(w|t) ?
?
(w,t
?
)
|w|
?
i=1
?((w
i
, t
?
i
), (w, t))
where the Kronecker ?(x, y) function is 1 if x and
y are equal and 0 otherwise.
Figure 1 shows that while UNIFORM and
SUPERVISED achieve nearly identical data log-
likelihoods, their final tagging accuracy differs by
817
70
80
90
100
0 5 10 15 20 25 30
-13
-11
-9
-7
SUPERVISED
UNIFORM
70
80
90
100
0 5 10 15 20 25 30
Data Log-Likelihood (10
6
)
Tagging Accuracy (%)
94.1%
82.1%
96.7%
72.0%
93.7%
91.0%
92.8%
93.5%
Number of Iterations of Expectation Maximization
-13
-11
-9
-7
SUPERVISED TRANSITIONS
SUPERVISED EMISSIONS
Data Log-Likelihood (10
6
)
Tagging Accuracy (%)
Number of Iterations of Expectation Maximization
70
80
90
100
0 5 10 15 20 25 30
93.7%
92.1%
89.2%
93.5%
-13
-11
-9
-7
SUPERVISED TRANSITIONS
OBSERVATIONAL
Data Log-Likelihood (10
6
)
Tagging Accuracy (%)
Number of Iterations of Expectation Maximization
Figure 1: The data log-likelihood (top) and tag-
ging accuracy (bottom) of two contrasting initial-
izers, UNIFORM and SUPERVISED, compared on
the Penn Treebank.
70
80
90
100
0 5 10 15 20 25 30
-13
-11
-9
-7
SUPERVISED
UNIFORM
70
80
90
100
0 5 10 15 20 25 30
Data Log-Likelihood (10
6
)
Tagging Accuracy (%)
94.1%
82.1%
96.7%
72.0%
93.7%
91.0%
92.8%
93.5%
Number of Iterations of Expectation Maximization
-13
-11
-9
-7
SUPERVISED TRANSITIONS
SUPERVISED EMISSIONS
Data Log-Likelihood (10
6
)
Tagging Accuracy (%)
Number of Iterations of Expectation Maximization
70
80
90
100
0 5 10 15 20 25 30
93.7%
92.1%
89.2%
93.5%
-13
-11
-9
-7
SUPERVISED TRANSITIONS
OBSERVATIONAL
Data Log-Likelihood (10
6
)
Tagging Accuracy (%)
Number of Iterations of Expectation Maximization
Figure 2: The data log-likelihood (top) and tag-
ging accuracy (bottom) of two partially supervised
initializers, one with SUPERVISED TRANSITIONS
and one with SUPERVISED EMISSIONS, compared
on the Penn Treebank.
12%. Accuracy degrades somewhat from the SU-
PERVISED initialization, since the data likelihood
objective differs from the objective of maximizing
tagging accuracy. However, the final SUPERVISED
performance of 94.1% shows that there is substan-
tial room for improvement over the UNIFORM ini-
tializer.
Figure 2 compares two partially supervised ini-
tializations. SUPERVISED TRANSITIONS initial-
izes the transition model with oracle counts, but
the emission model uniformly. Conversely, SU-
PERVISED EMISSIONS initializes the emission pa-
rameters from oracle counts, but initializes the
transition model uniformly. There are many more
emission parameters (57,390) than transition pa-
rameters (1,858). Thus, it is not surprising that
SUPERVISED EMISSIONS gives a higher initial
likelihood. Again, both initializers lead to solu-
tions with nearly the same likelihood as SUPER-
VISED and UNIFORM.
Figure 2 shows that SUPERVISED TRANSI-
TIONS outperforms SUPERVISED EMISSIONS in
tagging accuracy, despite the fact that fewer pa-
rameters are set with supervision. With fixed D,
an accurate initialization of the transition distribu-
tions leads to accurate tagging after EM training.
We therefore concentrate on developing an effec-
tive initialization for the transition distribution.
4 Observational Initialization
The SUPERVISED TRANSITIONS initialization is
estimated from observations of consecutive tags in
a labeled corpus. Our OBSERVATIONAL initializer
is likewise estimated from the relative frequency
of consecutive tags, taking advantage of the struc-
ture of the tag dictionary D. However, it does not
require a labeled corpus.
Let D(w, ?) = {t : (w, t) ? D} denote the
allowed tags for word w. The set
U = {w : |D(w, ?)| = 1}
contains all words that have only one allowed tag.
When a token of some w ? U is observed in a
corpus, its tag is unambiguous. Therefore, its tag
is observed as well, and a portion of the tag se-
quence is known. When consecutive pairs of to-
kens are both in U , we can observe a transition in
the latent tag sequence. The OBSERVATIONAL ini-
tializer simply estimates a transition distribution
from the relative frequency of these unambiguous
observations that occur whenever two consecutive
tokens both have a unique tag.
We now formally define the observational ini-
tializer. Let g(w, t) = ?(D(w, ?), {t}) be an indi-
cator function that is 1 whenever w ? U and its
single allowed tag is t, and 0 otherwise. Then, we
initialize ? such that:
P
?
(t|t
?
) ?
?
w?S
|w|
?
i=1
g(w
i
, t) ? g(w
i?1
, t
?
)
The emission parameters ? are set to be uniform
over allowed words for each tag, as in UNIFORM
initialization.
Figure 3 compares the OBSERVATIONAL ini-
tializer to the SUPERVISED TRANSITIONS initial-
izer, and the top of Table 1 summarizes the perfor-
mance of all initializers discussed so far for the
818
70
80
90
100
0 5 10 15 20 25 30
-13
-11
-9
-7
SUPERVISED
UNIFORM
70
80
90
100
0 5 10 15 20 25 30
Data Log-Likelihood (10
6
)
Tagging Accuracy (%)
94.1%
82.1%
96.7%
72.0%
93.7%
91.0%
92.8%
93.5%
Number of Iterations of Expectation Maximization
-13
-11
-9
-7
SUPERVISED TRANSITIONS
SUPERVISED EMISSIONS
Data Log-Likelihood (10
6
)
Tagging Accuracy (%)
Number of Iterations of Expectation Maximization
70
80
90
100
0 5 10 15 20 25 30
93.7%
92.1%
89.2%
93.5%
-13
-11
-9
-7
SUPERVISED TRANSITIONS
OBSERVATIONAL
Data Log-Likelihood (10
6
)
Tagging Accuracy (%)
Number of Iterations of Expectation Maximization
Figure 3: The data log-likelihood (top) and tag-
ging accuracy (bottom) of initializing with SU-
PERVISED TRANSITIONS compared to the unsu-
pervised OBSERVATIONAL initialization that re-
quires only a tag dictionary and an unlabeled train-
ing corpus.
English Penn Treebank. The OBSERVATIONAL
initializer provides an error reduction over UNI-
FORM of 56%, surpassing the performance of an
initially supervised emission model and nearing
the performance of a supervised transition model.
The bottom of Table 1 shows a similar compar-
ison on the T?ubingen treebank of spoken German
(Telljohann et al, 2006). Both training and test-
ing were performed on the entire treebank. The
observational initializer provides an error reduc-
tion over UNIFORM of 29%, and again outper-
forms SUPERVISED EMISSIONS. On this dataset
OBSERVATIONAL initialization matches the final
performance of SUPERVISED TRANSITIONS.
5 Discussion
The fact that observations and prior knowledge are
useful for part-of-speech tagging is well under-
stood (Brill, 1995), but the approach of estimating
an initial transition model only from unambiguous
word pairs is novel.
Our experiments show that for EM-trained
HMM taggers in a type-supervised transductive
data setting, observational initialization is an ef-
fective technique for guiding training toward high-
accuracy solutions, approaching the oracle accu-
racy of SUPERVISED TRANSITIONS initialization.
The fact that models with similar data likeli-
hood can vary dramatically in accuracy has been
observed in other learning problems. For instance,
Toutanova and Galley (2011) show that optimal
English Initial EM-trained
UNIFORM 72.0 82.1
OBSERVATIONAL 89.2 92.1
SUP. EMISSIONS 92.8 91.0
SUP. TRANSITIONS 93.5 93.7
FULLY SUPERVISED 96.7 94.1
German Initial EM-trained
UNIFORM 77.2 88.8
OBSERVATIONAL 92.7 92.1
SUP. EMISSIONS 90.7 89.0
SUP. TRANSITIONS 94.8 92.0
FULLY SUPERVISED 97.0 92.9
Table 1: Accuracy of English (top) and German
(bottom) tagging models at initialization (left) and
after 30 iterations of EM training (right) using var-
ious initializers.
parameters for IBM Model 1 are not unique, and
alignments predicted from different optimal pa-
rameters vary significantly in accuracy.
However, the effectiveness of observational ini-
tialization is somewhat surprising because EM
training includes these unambiguous tag pairs in
its expected counts, even with uniform initializa-
tion. Our experiments indicate that this signal is
not used effectively unless explicitly encoded in
the initialization.
In our English data, 48% of tokens and 74% of
word types have only one allowed tag. 28% of
pairs of adjacent tokens have only one allowed tag
pair and contribute to observational initialization.
In German, 49% of tokens and 87% of word types
are unambiguous, and 26% of adjacent token pairs
are unambiguous.
6 Related Work
We now compare with several previous published
results on type-supervised part-of-speech tagging
trained using the same data setting on the English
WSJ Penn Treebank, introduced by Smith and Eis-
ner (2005).
Contrastive estimation (Smith and Eisner, 2005)
is a learning technique that approximates the par-
tition function of the EM objective in a log-linear
model by considering a neighborhood around ob-
served training examples. The Bayesian HMM
of Goldwater and Griffiths (2007) is a second-
order HMM (i.e., likelihood factors over triples
of tags) that is estimated using a prior distribu-
tion that promotes sparsity. Sparse priors have
819
45 tag set 17 tag set
All train 973k train All train 973k train
Observational initialization (this work) 92.1 92.8 93.9 94.8
Contrastive Estimation (Smith and Eisner, 2005) ? ? 88.7 ?
Bayesian HMM (Goldwater and Griffiths, 2007) 86.8 ? 87.3 ?
Bayesian LDA-HMM (Toutanova and Johnson, 2008) ? ? 93.4 ?
Linguistic initialization (Goldberg et al, 2008) 91.4 ? 93.8 ?
Minimal models (Ravi and Knight, 2009) ? 92.3 ? 96.8
Table 2: Tagging accuracy of different approaches on English Penn Treebank. Columns labeled 973k
train describe models trained on the subset of 973k tokens used by Ravi and Knight (2009).
been motivated empirically for this task (Johnson,
2007). The Bayesian HMM model predicts tag se-
quences via Gibbs sampling, integrating out model
parameters. The Bayesian LDA-based model of
Toutanova and Johnson (2008) models ambiguity
classes of words, which allows information shar-
ing among words in the tag dictionary. In addition,
it incorporates morphology features and a sparse
prior of tags for a word. Inference approximations
are required to predict tags, integrating out model
parameters.
Ravi and Knight (2009) employs integer linear
programming to select a minimal set of parame-
ters that can generate the test sentences, followed
by EM to set parameter values. This technique
requires the additional information of which sen-
tences will be used for evaluation, and its scalabil-
ity is limited. In addition, this work used a sub-
set of the WSJ Penn Treebank for training and se-
lecting a tag dictionary. This restriction actually
tends to improve performance, because a smaller
tag dictionary further constrains model optimiza-
tion. We compare directly to their training set,
kindly provided to us by the authors.
The linguistic initialization of Goldberg et al
(2008) is most similar to the current work, in
that it estimates maximum likelihood parameters
of an HMM using EM, but starting with a well-
chosen initialization with language specific lin-
guistic knowledge. That work estimates emission
distributions using a combination of suffix mor-
phology rules and corpus context counts.
Table 2 compares our results to these related
techniques. Each column represents a variant of
the experimental setting used in prior work. Smith
and Eisner (2005) introduced a mapping from the
full 45 tag set of the Penn Treebank to 17 coarse
tags. We report results on this coarse set by pro-
jecting from the full set after learning and infer-
ence.
1
Using the full tag set or the full training
data, our method offers the best published perfor-
mance without language-specific assumptions or
approximate inference.
7 Future Work
This paper has demonstrated a simple and effec-
tive learning method for type-supervised, trans-
ductive part-of-speech tagging. However, it is an
open question whether the technique is as effec-
tive for tag dictionaries derived from more natural
sources than the labels of an existing treebank.
All of the methods to which we compare ex-
cept Goldberg et al (2008) focus on learning and
modeling techniques, while our method only ad-
dresses initialization. We look forward to inves-
tigating whether our technique can be used as an
initialization or prior for these other methods.
References
Eric Brill. 1995. Unsupervised learning of disam-
biguation rules for part of speech tagging. In In Nat-
ural Language Processing Using Very Large Cor-
pora, pages 1?13. Kluwer Academic Press.
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based
projections. In Proceedings of the Assocation for
Computational Linguistics.
Dan Garrette and Jason Baldridge. 2013. Learning a
part-of-speech tagger from two hours of annotation.
In Proceedings of the North American Chapter of
the Assocation for Computational Linguistics.
Yoav Goldberg, Meni Adler, and Michael Elhadad.
2008. EM can find pretty good HMM POS-taggers
1
Training with the reduced tag set led to lower perfor-
mance of 91.0% accuracy, likely because the coarse projec-
tion drops critical information about allowable English tran-
sitions, such as what verb forms can follow to be (Goldberg
et al, 2008).
820
(when given a good start). In Proceedings of the As-
sociation for Computational Linguistics.
Sharon Goldwater and Tom Griffiths. 2007. A fully
Bayesian approach to unsupervised part-of-speech
tagging. In Proceedings of the Association for Com-
putational Linguistics.
Mark Johnson. 2007. Why doesnt EM nd good HMM
POS-taggers? In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing.
Shen Li, Jo?ao V. Grac?a, and Ben Taskar. 2012. Wiki-ly
supervised part-of-speech tagging. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional Linguistics.
Bernard Merialdo. 1994. Tagging English text with a
probabilistic model. Computational Linguistics.
Sujith Ravi and Kevin Knight. 2009. Minimized mod-
els for unsupervised part-of-speech tagging. In Pro-
ceedings of the Association for Computational Lin-
guistics.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of the Association for Compu-
tational Linguistics.
Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
type constraints for cross-lingual part-of-speech tag-
ging. Transactions of the Association for Computa-
tional Linguistics.
Heike Telljohann, Erhard Hinrichs, Sandra K?ubler, and
Heike Zinsmeister. 2006. Stylebook for the tbingen
treebank of written german.
Kristina Toutanova and Michel Galley. 2011. Why
initialization matters for ibm model 1: Multiple op-
tima and non-strict convexity. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 461?466, Portland, Oregon, USA, June.
Association for Computational Linguistics.
Kristina Toutanova and Mark Johnson. 2008. A
Bayesian LDA-based model for semi-supervised
part-of-speech tagging. In Proceedings of Neural
and Information Processing Systems.
821
