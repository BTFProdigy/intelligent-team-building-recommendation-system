Tagging Portuguese with a Spanish Tagger Using Cognates
Jirka Hana
Department of Linguistics
The Ohio State University
hana.1@osu.edu
Anna Feldman
Department of Linguistics
The Ohio State University
afeldman@ling.osu.edu
Luiz Amaral
Department of Spanish and Portuguese
The Ohio State University
amaral.1@osu.edu
Chris Brew
Department of Linguistics
The Ohio State University
cbrew@acm.org
Abstract
We describe a knowledge and resource
light system for an automatic morpholog-
ical analysis and tagging of Brazilian Por-
tuguese.1 We avoid the use of labor in-
tensive resources; particularly, large anno-
tated corpora and lexicons. Instead, we
use (i) an annotated corpus of Peninsular
Spanish, a language related to Portuguese,
(ii) an unannotated corpus of Portuguese,
(iii) a description of Portuguese morphol-
ogy on the level of a basic grammar book.
We extend the similar work that we have
done (Hana et al, 2004; Feldman et al,
2006) by proposing an alternative algo-
rithm for cognate transfer that effectively
projects the Spanish emission probabili-
ties into Portuguese. Our experiments use
minimal new human effort and show 21%
error reduction over even emissions on a
fine-grained tagset.
1 Introduction
Part of speech (POS) tagging is an important step
in natural language processing. Corpora that have
been POS-tagged are very useful both for linguis-
tic research, e.g. finding instances or frequencies
of particular constructions (Meurers, 2004) and for
further computational processing, such as syntac-
tic parsing, speech recognition, stemming, word-
sense disambiguation. Morphological tagging is
the process of assigning POS, case, number, gen-
der and other morphological information to each
word in a corpus. Despite the importance of mor-
phological tagging, there are many languages that
1We thank the anonymous reviewers for their constructive
comments on an earlier version of the paper.
lack annotated resources of this kind, mainly due
to the lack of training corpora which are usually
required for applying standard statistical taggers.
Applications of taggers include syntactic pars-
ing, stemming, text-to-speech synthesis, word-
sense disambiguation, information extraction. For
some of these getting all the tags right is inessen-
tial, e.g. the input to noun phrase chunking does
not necessarily require high accuracy fine-grained
tag resolution.
Cross-language information transfer is not new;
however, most of the existing work relies on par-
allel corpora (e.g. Hwa et al, 2004; Yarowsky
and Ngai, 2001) which are difficult to find, es-
pecially for lesser studied languages. In this pa-
per, we describe a cross-language method that re-
quires neither training data of the target language
nor bilingual lexicons or parallel corpora. We re-
port the results of the experiments done on Brazil-
ian Portuguese and Peninsular Spanish, however,
our system is not tied to these particular languages.
The method is easily portable to other (inflected)
languages. Our method assumes that an anno-
tated corpus exists for the source language (here,
Spanish) and that a text book with basic linguis-
tic facts about the source language is available
(here, Portuguese). We want to test the generality
and specificity of the method. Can the systematic
commonalities and differences between two ge-
netically related languages be exploited for cross-
language applications? Is the processing of Por-
tuguese via Spanish different from the processing
of Russian via Czech (Hana et al, 2004; Feldman
et al, 2006)?
33
Spanish Portuguese
1. sg. canto canto
2. sg. cantas cantas
3. sg. canta canta
1. pl. catamos cantamos
2. pl. cantais cantais
3. pl. cantan cantam
Table 1: Verb conjugation present indicative: -ar
regular verb: cantar ?to sing?
2 Brazilian Portuguese (BP)
vs. Peninsular Spanish (PS)
Portuguese and Spanish are both Romance lan-
guages from the Iberian Peninsula, and share
many morpho-syntactic characteristics. Both lan-
guages have a similar verb system with three main
conjugations (-ar, -er, -ir), nouns and adjectives
may vary in number and gender, and adverbs are
invariable. Both are pro-drop languages, they have
a similar pronominal system, and certain phenom-
ena, such as clitic climbing, are prevalent in both
languages. They also allow rather free constituent
order; and in both cases there is considerable de-
bate in the literature about the appropriate char-
acterization of their predominant word order (the
candidates being SVO and VSO).
Sometimes the languages exhibit near-complete
parallelism in their morphological patterns, as
shown in Table 1.
The languages are also similar in their lexicon and
syntactic word order:
(1) Os
Los
The
estudantes
estudiantes
students
ja?
ya
already
comparam
compraron
bought
os
los
the
livros.
libros.
books
[BP]
[PS]
?The students have already bought the
books.?
One of the main differences is the fact that
Brazilian Portuguese (BP) accepts object drop-
ping, while Peninsular Spanish (PS) doesn?t. In
addition, subjects in BP tend to be overt while in
PS they tend to be omitted.
(2) a. A: O que
What
voce?
you
fez
did
com
with
o
the
livro?
book?
[BP]
A: ?What did you do with the book??
B: Eu
I
dei
gave
para
to
Maria.
Mary
B: ?I gave it to Mary.?
b. A: ?Que?
What
hiciste
did
con
with
el
the
libro?
book?
[PS]
A: ?What did you do with the book??
B: Se
Her.dat
lo
it.acc
di
gave
a
to
Mar??a.
Mary.
B: ?I gave it to Mary.?
Notice also that in the Spanish example (2b) the
dative pronoun se ?her? is obligatory even when
the prepositional phrase a Mar??a ?to Mary? is
present.
3 Resources
3.1 Tagset
For both Spanish and Portuguese, we used posi-
tional tagsets developed on the basis of Spanish
CLiC-TALP tagset (Torruella, 2002). Every tag is
a string of 11 symbols each corresponding to one
morphological category. For example, the Por-
tuguese word partires ?you leave? is assigned the
tag VM0S---2PI-, because it is a verb (V), main
(M), gender is not applicable to this verb form (0),
singular (S), case, possesor?s number and form are
not applicable to this category(-), 2nd person (2),
present (P), indicative (I) and participle type is not
applicable (-).
A comparison of the two tagsets is in Table 2.2
When possible the Spanish and Portuguese tagsets
use the same values, however some differences are
unavoidable. For instance, the pluperfect is a com-
pound verb tense in Spanish, but a separate word
that needs a tag of its own in Portuguese. In ad-
dition, we added a tag for ?treatment? Portuguese
pronouns.
The Spanish tagset has 282 tags, while that for
Portuguese has 259 tags.
3.2 Training corpora
Spanish training corpus. The Spanish corpus
we use for training the transition probabilities as
well as for obtaining Spanish-Portuguese cognate
pairs is a fragment (106,124 tokens, 18,629 types)
of the Spanish section of CLiC-TALP (Torruella,
2Notice that we have 6 possible values for the gender po-
sition: M (masc.), F (fem.), N (neutr., for certain pronouns), C
(common, either M or F), 0 (unspecified for this form within
the category), - (the category does not distinguish gender)
34
No. Description No. of values
Sp Po
1 POS 14 11
2 SubPOS ? detailed POS 30 29
3 Gender 6 6
4 Number 5 5
5 Case 6 6
6 Possessor?s Number 4 4
7 Form 3 3
8 Person 5 5
9 Tense 7 9
10 Mood 8 9
11 Participle 3 3
Table 2: Overview and comparison of the tagsets
2002). CLiC-TALP is a balanced corpus, contain-
ing texts of various genres and styles. We automat-
ically translated the CLiC-TALP tagset into our
system (see Sect. 3.1) for easier detailed evalua-
tion and for comparison with our previous work
that used a similar approach for tagging (Hana
et al, 2004; Feldman et al, 2006).
Raw Portuguese corpus. For automatic lexi-
con acquisition, we use NILC corpus,3 containing
1.2M tokens.
3.3 Evaluation corpus
For evaluation purposes, we selected and manually
annotated a small portion (1,800 tokens) of NILC
corpus.
4 Morphological Analysis
Our morphological analyzer (Hana, 2005) is an
open and modular system. It allows us to com-
bine modules with different levels of manual in-
put ? from a module using a small manually pro-
vided lexicon, through a module using a large lex-
icon automatically acquired from a raw corpus, to
a guesser using a list of paradigms, as the only
resource provided manually. The general strat-
egy is to run modules that make fewer errors and
less overgenerate before modules that make more
errors and overgenerate more. This, for exam-
ple, means that modules with manually created
resources are used before modules with resources
3Nu?cleo Interdisciplinar de Lingu???stica Computacional;
available at http://nilc.icmc.sc.usp.br/nilc/,
we used the version with POS tags assigned by PALAVRAS.
We ignored the POS tags.
automatically acquired. In the experiments below,
we used the following modules ? lookup in a list
of (mainly) closed-class words, a paradigm-based
guesser and an automatically acquired lexicon.
4.1 Portuguese closed class words
We created a list of the most common preposi-
tions, conjunctions, and pronouns, and a number
of the most common irregular verbs. The list con-
tains about 460 items and it required about 6 hours
of work. In general, the closed class words can be
derived either from a reference grammar book, or
can be elicited from a native speaker. This does
not require native-speaker expertise or intensive
linguistic training. The reason why the creation
of such a list took 6 hours is that the words were
annotated with detailed morphological tags used
by our system.
4.2 Portuguese paradigms
We also created a list of morphological paradigms.
Our database contains 38 paradigms. We just en-
coded basic facts about the Portuguese morphol-
ogy from a standard grammar textbook (Cunha
and Cintra, 2001). The paradigms include all three
regular verb conjugations (-ar, -er, -ir), the most
common adjective and nouns paradigms and a rule
for adverbs of manner that end with -mente (anal-
ogous to the English -ly). We ignore majority of
exceptions. The creation of the paradigms took
about 8 h of work.
4.3 Lexicon Acquisition
The morphological analyzer supports a module or
modules employing a lexicon containing informa-
tion about lemmas, stems and paradigms. There is
always the possibility to provide this information
manually. That, however, is very costly. Instead,
we created such a lexicon automatically.
Usually, automatically acquired lexicons and
similar systems are used as a backup for large
high-precision high-cost manually created lexi-
cons (e.g. Mikheev, 1997; Hlava?c?ova?, 2001). Such
systems extrapolate the information about the
words known by the lexicon (e.g. distributional
properties of endings) to unknown words. Since
our approach is resource light, we do not have any
such large lexicon to extrapolate from.
The general idea of our system is very sim-
ple. The paradigm-based Guesser, provides all the
possible analyses of a word consistent with Por-
tuguese paradigms. Obviously, this approach mas-
35
sively overgenerates. Part of the ambiguity is usu-
ally real but most of it is spurious. We use a large
corpus to weed the spurious analyses out of the
real ones. In such corpus, open-class lemmas are
likely to occur in more than one form. Therefore,
if a lemma+paradigm candidate suggested by the
Guesser occurs in other forms in other parts of the
corpus, it increases the likelihood that the candi-
date is real and vice versa. If we encounter the
word cantamos ?we sing? in a Portuguese corpus,
using the information about the paradigms we can
analyze it in two ways, either as being a noun in
the plural with the ending -s, or as being a verb in
the 1st person plural with the ending -amos. Based
on this single form we cannot say more. However
if we also encounter the forms canto, canta, can-
tam the verb analysis becomes much more prob-
able; and therefore, it will be chosen for the lex-
icon. If the only forms that we encounter in our
Portuguese corpus were cantamos and (the non-
existing) cantamo (such as the existing word ramo
and ramos) then we would analyze it as a noun and
not as a verb.
With such an approach, and assuming that the
corpus contains the forms of the verb matar ?to
kill?, mato1sg matas2sg, mata3sg, etc., we would
not discover that there is also a noun mata ?forest?
with a plural form matas ? the set of the 2 noun
forms is a proper subset of the verb forms. A sim-
ple solution is to consider not the number of form
types covered in a corpus, but the coverage of the
possible forms of the particular paradigm. How-
ever this brings other problems (e.g. it penalizes
paradigms with large number of forms, paradigms
with some obsolete forms, etc.). We combine both
of these measures in Hana (2005).
Lexicon Acquisition consists of three steps:
1. A large raw corpus is analyzed with a
lexicon-less MA (an MA using a list of
mainly closed-class words and a paradigm
based guesser);
2. All possible hypothetical lexical entries over
these analyses are created.
3. Hypothetical entries are filtered with aim to
discard as many nonexisting entries as possi-
ble, without discarding real entries.
Obviously, morphological analysis based on
such a lexicon still overgenerates, but it overgener-
ates much less than if based on the endings alone.
Lexicon no yes
recall 99.0 98.1
avg ambig (tag/word) 4.3 3.5
Tagging (cognates) ? accuracy 79.1 82.1
Table 3: Evaluation of Morphological analysis
Consider for example, the form func?o?es ?func-
tions? of the feminine noun func?a?o. The analyzer
without a lexicon provides 11 analyses (6 lemmas,
each with 1 to 3 tags); only one of them is cor-
rect. In contrast, the analyzer with an automati-
cally acquired lexicon provides only two analyses:
the correct one (noun fem. pl.) and an incorrect
one (noun masc. pl., note that POS and number
are still correct). Of course, not all cases are so
persuasive.
The evaluation of the system is in Table 3. The
98.1% recall is equivalent to the upper bound for
the task. It is calculated assuming an oracle-
Portuguese tagger that is always able to select the
correct POS tag if it is in the set of options given
by the morphological analyzer. Notice also that
for the tagging accuracy, the drop of recall is less
important than the drop of ambiguity.
5 Tagging
We used the TnT tagger (Brants, 2000), an im-
plementation of the Viterbi algorithm for second-
order Markov model. In the traditional approach,
we would train the tagger?s transitional and emis-
sion probabilities on a large annotated corpus of
Portuguese. However, our resource-light approach
means that such corpus is not available to us and
we need to use different ways to obtain this infor-
mation.
We assume that syntactic properties of Spanish
and Portuguese are similar enough to be able to
use the transitional probabilities trained on Span-
ish (after a simple tagset mapping).
The situation with the lexical properties as cap-
tured by emission probabilities is more complex.
Below we present three different ways how to ob-
tains emissions, assuming:
1. they are the same: we use the Spanish emis-
sions directly (?5.1).
2. they are different: we ignore the Spanish
emissions and instead uniformly distribute
36
the results of our morphological analyzer.
(?5.2)
3. they are similar: we map the Spanish emis-
sions onto the result of morphological analy-
sis using automatically acquired cognates.
(?5.3)
5.1 Tagging ? Baseline
Our lowerbound measurement consists of training
the TnT tagger on the Spanish corpus and apply-
ing this model directly to Portuguese.4 The overall
performance of such a tagger is 56.8% (see the the
min column in Table 4). That means that half of
the information needed for tagging of Portuguese
is already provided by the Spanish model. This
tagger has seen no Portuguese whatsoever, and is
still much better than nothing.
5.2 Tagging ? Approximating Emissions I
The opposite extreme to the baseline, is to assume
that Spanish emissions are useless for tagging Por-
tuguese. Instead we use the morphological an-
alyzer to limit the number of possibilities, treat-
ing them all equally ? The emission probabilities
would then form a uniform distribution of the tags
given by the analyzer. The results are summarized
in Table 4 (the e-even column) ? accuracy 77.2%
on full tags, or 47% relative error reduction against
the baseline.
5.3 Tagging ? Approximating Emissions II
Although it is true that forms and distributions of
Portuguese and Spanish words are not the same,
they are also not completely unrelated. As any
Spanish speaker would agree, the knowledge of
Spanish words is useful when trying to understand
a text in Portuguese.
Many of the corresponding Portuguese and
Spanish words are cognates, i.e. historically they
descend from the same ancestor root or they are
mere translations. We assume two things: (i) cog-
nate pairs have usually similar morphological and
distributional properties, (ii) cognate words are
similar in form.
Obviously both of these assumptions are ap-
proximations:
1. Cognates could have departed in their mean-
ings, and thus probably also have dif-
4Before training, we translated the Spanish tagset into the
Portuguese one.
ferent distributions. For example, Span-
ish embarazada ?pregnant? vs. Portuguese
embarac?ada ?embarrassed?.
2. Cognates could have departed in their mor-
phological properties. For example, Span-
ish cerca ?near?.adverb vs. Portuguese cerca
?fence?.noun (from Latin circa, circus ?cir-
cle?).
3. There are false cognates ? unrelated,
but similar or even identical words. For
example, Spanish salada ?salty?.adj vs. Por-
tuguese salada ?salad?.noun, Spanish doce
?twelve?.numeral vs. Portuguese doce
?candy?.noun
Nevertheless, we believe that these examples
are true exceptions from the rule and that in major-
ity of cases, the cognates would look and behave
similarly. The borrowings, counter-borrowings
and parallel developments of the various Romance
languages have of course been extensively studied,
and we have no space for a detailed discussion.
Identifying cognates. For the present work,
however, we do not assume access to philologi-
cal erudition, or to accurate Spanish-Portuguese
translations or even a sentence-aligned corpus. All
of these are resources that we could not expect to
obtain in a resource poor setting. In the absence
of this knowledge, we automatically identify cog-
nates, using the edit distance measure (normalized
by word length).
Unlike in the standard edit distance, the cost of
operations is dependent on the arguments. Simi-
larly as Yarowsky and Wicentowski (2000), we as-
sume that, in any language, vowels are more muta-
ble in inflection than consonants, thus for example
replacing a for i is cheaper that replacing s by r.
In addition, costs are refined based on some well
known and common phonetic-orthographic regu-
larities, e.g. replacing a q with c is less costly than
replacing m with, say s. However, we do not want
to do a detailed contrastive morpho-phonological
analysis, since we want our system to be portable
to other languages. So, some facts from a simple
grammar reference book should be enough.
Using cognates. Having a list of Spanish-
Portuguese cognate pairs, we can use these to
map the emission probabilities acquired on Span-
ish corpus to Portuguese.
37
Let?s assume Spanish word ws and Portuguese
word wp are cognates. Let Ts denote the tags that
ws occurs within the Spanish corpus, and let ps(t)
be the emission probability of a tag t (t 6? Ts ?
ps(t) = 0). Let Tp denote tags assigned to the
Portuguese word wp by our morphological ana-
lyzer, and the pp(t) is the even emission proba-
bility: pp(t) = 1|Tp| . Then we can assign the new
emission probability p?p(t) to every tag t ? Tp in
the following way (followed by normalization):
p?p(t) =
ps(t) + pp(t)
2
(1)
Results. This method provides the best results.
The full-tag accuracy is 82.1%, compared to
56.9% for baseline (58% error rate reduction) and
77.2% for even-emissions (21% reduction). The
accuracy for POS is 87.6%. Detailed results are in
column e-cognates of Table 4.
6 Evaluation & Comparison
The best way to evaluate our results would be to
compare it against the TnT tagger used the usual
way ? trained on Portuguese and applied on Por-
tuguese. We do not have access to a large Por-
tuguese corpus annotated with detailed tags. How-
ever, we believe that Spanish and Portuguese are
similar enough (see Sect. 2) to justify our assump-
tion that the TnT tagger would be equally success-
ful (or unsuccessful) on them. The accuracy of
TnT trained on 90K tokens of the CLiC-TALP cor-
pus is 94.2% (tested on 16K tokens). The accuracy
of our best tagger is 82.1%. Thus the error-rate is
more than 3 times bigger (17.9% vs. 5.4%).
Branco and Silva (2003) report 97.2% tagging
accuracy on 23K testing corpus. This is clearly
better than our results, on the other hand they
needed a large Portuguese corpus of 207K tokens.
The details of the tagset used in the experiments
are not provided, so precise comparison with our
results is difficult.
7 Related work
Previous research in resource-light language
learning has defined resource-light in different
ways. Some have assumed only partially tagged
training corpora (Merialdo, 1994); some have be-
gun with small tagged seed wordlists (Cucerzan
and Yarowsky, 1999) for named-entity tagging,
while others have exploited the automatic trans-
fer of an already existing annotated resource in a
min e-even e-cognates
Tag: 56.9 77.2 82.1
POS: 65.3 84.2 87.6
SubPOS: 61.7 83.3 86.9
gender: 70.4 87.3 90.2
number: 78.3 95.3 96.0
case: 93.8 96.8 97.2
possessor?s num: 85.4 96.7 97.0
form: 92.9 99.2 99.2
person: 74.5 91.2 92.7
tense: 90.7 95.1 96.1
mood: 91.5 95.0 96.0
participle: 99.9 100.0 100.0
Table 4: Tagging Brazilian Portuguese
different genres or a different language (e.g. cross-
language projection of morphological and syn-
tactic information in (Yarowsky et al, 2001;
Yarowsky and Ngai, 2001), requiring no direct su-
pervision in the target language).
Ngai and Yarowsky (2000) observe that the to-
tal weighted human and resource costs is the most
practical measure of the degree of supervision.
Cucerzan and Yarowsky (2002) observe that an-
other useful measure of minimal supervision is the
additional cost of obtaining a desired functional-
ity from existing commonly available knowledge
sources. They note that for a remarkably wide
range of languages, there exist a plenty of refer-
ence grammar books and dictionaries which is an
invaluable linguistic resource.
7.1 Resource-light approaches to Romance
languages
Cucerzan and Yarowsky (2002) present a method
for bootstrapping a fine-grained, broad coverage
POS tagger in a new language using only one
person-day of data acquisition effort. Similarly
to us, they use a basic library reference gram-
mar book, and access to an existing monolingual
text corpus in the language, but they also use a
medium-sized bilingual dictionary.
In our work, we use a paradigm-based mor-
phology, including only the basic paradigms from
a standard grammar textbook. Cucerzan and
Yarowsky (2002) create a dictionary of regular in-
flectional affix changes and their associated POS
and on the basis of it, generate hypothesized in-
flected forms following the regular paradigms.
38
Clearly, these hypothesized forms are inaccurate
and overgenerated. Therefore, the authors perform
a probabilistic match from all lexical tokens actu-
ally observed in a monolingual corpus and the hy-
pothesized forms. They combine these two mod-
els, a model created on the basis of dictionary in-
formation and the one produced by the morpho-
logical analysis. This approach relies heavily on
two assumptions: (i) words of the same POS tend
to have similar tag sequence behavior; and (ii)
there are sufficient instances of each POS tag la-
beled by either the morphology models or closed-
class entries. For richly inflectional languages,
however, there is no guarantee that the latter as-
sumption would always hold.
The accuracy of their model is comparable to
ours. On a fine-grained (up to 5-feature) POS
space, they achieve 86.5% for Spanish and 75.5%
for Romanian. With a tagset of a similar size (11
features) we obtain the accuracy of 82.1% for Por-
tuguese.
Carreras et al (2003) present work on develop-
ing low-cost Named Entity recognizers (NER) for
a language with no available annotated resources,
using as a starting point existing resources for a
similar language. They devise and evaluate several
strategies to build a Catalan NER system using
only annotated Spanish data and unlabeled Cata-
lan text, and compare their approach with a classi-
cal bootstrapping setting where a small initial cor-
pus in the target language is hand tagged. It turns
out that the hand translation of a Spanish model is
better than a model directly learned from a small
hand annotated training corpus of Catalan. The
best result is achieved using cross-linguistic fea-
tures. Solorio and Lo?pez (2005) follow their ap-
proach; however, they apply the NER system for
Spanish directly to Portuguese and train a classi-
fier using the output and the real classes.
7.2 Cognates
Mann and Yarowsky (2001) present a method for
inducing translation lexicons based on trasduction
modules of cognate pairs via bridge languages.
Bilingual lexicons within language families are in-
duced using probabilistic string edit distance mod-
els. Translation lexicons for abitrary distant lan-
guage pairs are then generated by a combination
of these intra-family translation models and one
or more cross-family online dictionaries. Simi-
larly to Mann and Yarowsky (2001), we show that
languages are often close enough to others within
their language family so that cognate pairs be-
tween the two are common, and significant por-
tions of the translation lexicon can be induced with
high accuracy where no bilingual dictionary or
parallel corpora may exist.
8 Conclusion
We have shown that a tagging system with a small
amount of manually created resources can be suc-
cessful. We have previously shown that this ap-
proach can work for Czech and Russian (Hana
et al, 2004; Feldman et al, 2006). Here we have
shown its applicability to a new language pair.
This can be done in a fraction of the time needed
for systems with extensive manually created re-
sources: days instead of years. Three resources
are required: (i) a reference grammar (for infor-
mation about paradigms and closed class words);
(ii) a large amount of text (for learning a lexicon;
e.g. newspapers from the internet); (iii) a limited
access to a native speaker ? reference grammars
are often too vague and a quick glance at results
can provide feedback leading to a significant in-
crease of accuracy; however both of these require
only limited linguistic knowledge.
In this paper we proposed an algorithm for cog-
nate transfer that effectively projects the source
language emission probabilities into the target lan-
guage. Our experiments use minimal new human
effort and show 21% error reduction over even
emissions on a fine-grained tagset.
In the near future, we plan to compare the ef-
fectiveness (time and price) of our approach with
that of the standard resource-intensive approach to
annotating a medium-size corpus (on a corpus of
around 100K tokens). A resource-intensive sys-
tem will be more accurate in the labels which it of-
fers to the annotator, so annotator can work faster
(there are fewer choices to make, fewer keystrokes
required). On the other hand, creation of the in-
frastructure for such a system is very time con-
suming and may not be justified by the intended
application.
The experiments that we are running right now
are supposed to answer the question of whether
training the system on a small corpus of a closely
related language is better than training on a larger
corpus of a less related language. Some prelim-
inary results (Feldman et al, 2006) suggest that
using cross-linguistic features leads to higher pre-
39
cision, especially for the source languages which
have target-like properties complementary to each
other.
9 Acknowledgments
We would like to thank Maria das Grac?as
Volpe Nunes, Sandra Maria Alu??sio, and Ricardo
Hasegawa for giving us access to the NILC cor-
pus annotated with PALAVRAS and to Carlos
Rodr??guez Penagos for letting us use the Spanish
part of the CLiC-TALP corpus.
References
Branco, A. and J. Silva (2003). Portuguese-
specific Issues in the Rapid Development of
State-of-the-art Taggers. In Workshop on Tag-
ging and Shallow Processing of Portuguese:
TASHA?2000.
Brants, T. (2000). TnT ? A Statistical Part-
of-Speech Tagger. In Proceedings of ANLP-
NAACL, pp. 224?231.
Carreras, X., L. Ma`rquez, and L. Padro? (2003).
Named Entity Recognition for Catalan Using
Only Spanish Resources and Unlabelled Data.
In Proceedings of EACL-2003.
Cucerzan, S. and D. Yarowsky (1999). Lan-
guage Independent Named Entity Recognition
Combining Morphological and Contextual Ev-
idence. In Proceedings of the 1999 Joint SIG-
DAT Conference on EMNLP and VLC, pp. 90?
99.
Cucerzan, S. and D. Yarowsky (2002). Boot-
strapping a Multilingual Part-of-speech Tagger
in One Person-day. In Proceedings of CoNLL
2002, pp. 132?138.
Cunha, C. and L. F. L. Cintra (2001). Nova
Grama?tica do Portugue?s Contempora?neo. Rio
de Janeiro, Brazil: Nova Fronteira.
Feldman, A., J. Hana, and C. Brew (2006). Experi-
ments in Morphological Annotation Transfer. In
Proceedings of Computational Linguistics and
Intelligent Text Processing (CICLing).
Hana, J. (2005). Knowledge and labor light mor-
phological analysis. Unpublished manuscript.
Hana, J., A. Feldman, and C. Brew (2004). A
Resource-light Approach to Russian Morphol-
ogy: Tagging Russian using Czech resources.
In Proceedings of EMNLP 2004, Barcelona,
Spain.
Hlava?c?ova?, J. (2001). Morphological Guesser
or Czech Words. In V. Matous?ek (Ed.), Text,
Speech and Dialogue, Lecture Notes in Com-
puter Science, pp. 70?75. Berlin: Springer-
Verlag.
Hwa, R., P. Resnik, A. Weinberg, C. Cabezas,
and O. Kolak (2004). Bootstrapping Parsers via
Syntactic Projection across Parallel Texts. Nat-
ural Language Engineering 1(1), 1?15.
Mann, G. S. and D. Yarowsky (2001). Multipath
Translation Lexicon via Bridge Languages. In
Proceedings of NAACL 2001.
Merialdo, B. (1994). Tagging English Text with
a Probabilistic Model. Computational Linguis-
tics 20(2), 155?172.
Meurers, D. (2004). On the Use of Electronic Cor-
pora for Theoretical Linguistics. Case Studies
from the Syntax of German. Lingua.
Mikheev, A. (1997). Automatic Rule Induction
for Unknown Word Guessing. Computational
Linguistics 23(3), 405?423.
Ngai, G. and D. Yarowsky (2000). Rule Writing or
Annotation: Cost-efficient Resource Usage for
Base Noun Phrase Chunking. In Proceedings of
the 38th Meeting of ACL, pp. 117?125.
Solorio, T. and A. L. Lo?pez (2005). Learning
named entity recognition in Portuguese from
Spanish. In Proceedings of Computational Lin-
guistics and Intelligent Text Processing (CI-
CLing).
Torruella, M. (2002). Gu??a para la anotacio?n mor-
folo?gica del corpus CLiC-TALP (Versio?n 3).
Technical Report WP-00/06, X-Tract Working
Paper.
Yarowsky, D. and G. Ngai (2001). Inducing Mul-
tilingual POS Taggers and NP Bracketers via
Robust Projection Across Aligned Corpora. In
Proceedings of NAACL-2001, pp. 200?207.
Yarowsky, D., G. Ngai, and R. Wicentowski
(2001). Inducing Multilingual Text Analy-
sis Tools via Robust Projection across Aligned
Corpora. In Proceedings of HLT 2001, First
International Conference on Human Language
Technology Research.
Yarowsky, D. and R. Wicentowski (2000). Min-
imally supervised morphological analysis by
multimodal alignment. In Proceedings of the
38th Meeting of the Association for Computa-
tional Linguistics, pp. 207?216.
40
Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 10?18,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Enhancing Authentic Web Pages for Language Learners
Detmar Meurers1, Ramon Ziai1,
Luiz Amaral2, Adriane Boyd3, Aleksandar Dimitrov1, Vanessa Metcalf3, Niels Ott1
1 Universita?t Tu?bingen
2 University of Massachusetts Amherst
3 The Ohio State University
Abstract
Second language acquisition research since
the 90s has emphasized the importance of
supporting awareness of language categories
and forms, and input enhancement techniques
have been proposed to make target language
features more salient for the learner.
We present an NLP architecture and web-
based implementation providing automatic vi-
sual input enhancement for web pages. Learn-
ers freely choose the web pages they want to
read and the system displays an enhanced ver-
sion of the pages. The current system supports
visual input enhancement for several language
patterns known to be problematic for English
language learners, as well as fill-in-the-blank
and clickable versions of such pages support-
ing some learner interaction.
1 Introduction
A significant body of research into the effectiveness
of meaning-focused communicative approaches to
foreign language teaching has shown that input
alone is not sufficient to acquire a foreign lan-
guage, especially for older learners (cf., e.g., Light-
bown and Spada, 1999). Recognizing the important
role of consciousness in second-language learning
(Schmidt, 1990), learners have been argued to ben-
efit from (Long, 1991) or even require (Lightbown,
1998) a so-called focus on form to overcome incom-
plete or incorrect knowledge of specific forms or
regularities. Focus on form is understood to be ?an
occasional shift of attention to linguistic code fea-
tures? (Long and Robinson, 1998, p. 23).
In an effort to combine communicative and struc-
turalist approaches to second language teaching,
Rutherford and Sharwood Smith (1985) argued for
the use of consciousness raising strategies drawing
the learner?s attention to specific language proper-
ties. Sharwood Smith (1993, p. 176) coined the term
input enhancement to refer to strategies highlighting
the salience of language categories and forms.
Building on this foundational research in second
language acquisition and foreign language teaching,
in this paper we present an NLP architecture and a
system for automatic visual input enhancement of
web pages freely selected by language learners. We
focus on learners of English as a Second Language
(ESL), and the language patterns enhanced by the
system include some of the well-established diffi-
culties: determiners and prepositions, the distinction
between gerunds and to-infinitives, wh-question for-
mation, tense in conditionals, and phrasal verbs.
In our approach, learners can choose any web
page they like, either by using an ordinary search-
engine interface to search for one or by entering the
URL of the page they want to enhance. In contrast to
textbooks and other pre-prepared materials, allow-
ing the learner to choose up-to-date web pages on
any topic they are interested in and enhancing the
page while keeping it intact (with its links, multi-
media, and other components working) clearly has
a positive effect on learner motivation. Input en-
hanced web pages also are attractive for people out-
side a traditional school setting, such as in the vol-
untary, self-motivated pursuit of knowledge often
referred to as lifelong learning. The latter can be
particularly relevant for adult immigrants, who are
10
already functionally living in the second language
environment, but often stagnate in their second lan-
guage acquisition and lack access or motivation to
engage in language classes or other explicit lan-
guage learning activities. Nevertheless, they do use
the web to obtain information that is language-based
and thus can be enhanced to also support language
acquisition while satisfying information needs.
In terms of paper organization, in section 2 we
first present the system architecture and in 2.1 the
language phenomena handled, before considering
the issues involved in evaluating the approach in 2.2.
The context of our work and related approaches are
discussed in section 3, and we conclude and discuss
several avenues for future research in section 4.
2 The Approach
The WERTi system (Working with English Real
Texts interactively) we developed follows a client-
server paradigm where the server is responsible for
fetching the web page and enriching it with annota-
tions, and the client then receives the annotated web
page and transforms it into an enhanced version.
The client here is a standard web browser, so on the
learner?s side no additional software is needed.
The system currently supports three types of input
enhancement: i) color highlighting of the pattern or
selected parts thereof, ii) a version of the page sup-
porting identification of the pattern through clicking
and automatic color feedback, and iii) a version sup-
porting practice, such as a fill-in-the-blank version
of the page with automatic color feedback.
The overall architecture is shown in Figure 1.
Essentially, the automated input enhancement pro-
cess consists of the following steps:
1. Fetch the page.
2. Find the natural language text portions in it.
3. Identify the targeted language pattern.
4. Annotate the web page, marking up the lan-
guage patterns identified in the previous step.
5. Transform the annotated web page into the out-
put by visually enhancing the targeted pattern
or by generating interaction possibilities.
Steps 1?4 take place on the server side, whereas step
5 happens in the learner?s browser.1 As NLP is only
involved in step 3, we here focus on that step.
1As an alternative to the server-based fetching of web pages,
Server
UIMA                     
Browser                                         
URL Fetching
HTML Annotation
Identifying text in HTML page
Tokenization
Sentence Boundary Detection
POS Tagging
Pattern-specific NLP
Colorize Click Practice
Figure 1: Overall WERTi architecture. Grey components
are the same for all patterns and activities, cf. section 2.1.
While the first prototype of the WERTi system2
presented at CALICO (Amaral, Metcalf and Meur-
ers, 2006) and EUROCALL (Metcalf and Meurers,
2006) was implemented in Python, the current sys-
tem is Java-based, with all NLP being integrated in
the UIMA framework (Ferrucci and Lally, 2004).
UIMA is an architecture for the management and
analysis of unstructured information such as text,
which is built on the idea of referential annotation
and can be seen as an NLP analysis counterpart
to current stand-off encoding standards for anno-
tated corpora (cf., e.g., Ide et al 2000). The input
we are developing a Firefox plugin, leaving only the NLP up to
the server. This increases compatibility with web pages using
dynamically generated contents and special session handling.
2http://purl.org/icall/werti-v1
11
can be monotonically enriched while passing from
one NLP component to the next, using a flexible
data repository common to all components (Go?tz
and Suhre, 2004). Such annotation-based processing
is particularly useful in the WERTi context, where
keeping the original text intact is essential for dis-
playing it in enhanced form.
A second benefit of using the UIMA framework is
that it supports a flexible combination of individual
NLP components into larger processing pipelines.
To obtain a flexible approach to input enhancement
in WERTi, we need to be able to identify and an-
alyze phenomena from different levels of linguistic
analysis. For example, lexical classes can be iden-
tified by a POS tagger, whereas other patterns to be
enhanced require at least shallow syntactic chunk-
ing. The more diverse the set of phenomena, the
less feasible it is to handle all of them within a
single processing strategy or formalism. Using the
UIMA framework, we can re-use the same basic
processing (e.g., tokenizing, POS tagging) for all
phenomena and still be able to branch into pattern-
specific NLP in a demand-driven way. Given that
NLP components in UIMA include self-describing
meta-information, the processing pipeline to be run
can dynamically be obtained from the module con-
figuration instead of being hard-wired into the core
system. The resulting extensible, plugin-like archi-
tecture seems particularly well-suited for the task of
visual input enhancement of a wide range of hetero-
geneous language properties.
Complementing the above arguments for the
UIMA-based architecture of the current WERTi sys-
tem, a detailed discussion of the advantages of an
annotation-based, demand-driven NLP architecture
for Intelligent Computer-Assisted Language Learn-
ing can be found in Amaral, Meurers, and Ziai (To
Appear), where it is employed in an Intelligent Lan-
guage Tutoring System.
2.1 Implemented Modules
The modules implemented in the current system
handle a number of phenomena commonly judged
as difficult for second language learners of English.
In the following we briefly characterize each mod-
ule, describing the nature of the language pattern,
the required NLP, and the input enhancement results,
which will be referred to as activities.
Lexical classes
Lexical classes are the most basic kind of linguis-
tic category we use for input enhancement. The in-
ventory of lexical categories to be used and which
ones to focus on should be informed by second
language acquisition research and foreign language
teaching needs. The current system focuses on func-
tional elements such as prepositions and determiners
given that they are considered to be particularly dif-
ficult for learners of English (cf. De Felice, 2008 and
references therein).
We identify these functional elements using the
LingPipe POS tagger (http://alias-i.com/
lingpipe) employing the Brown tagset (Francis
and Kucera, 1979). As we show in section 2.2, the
tagger reliably identifies prepositions and determin-
ers in native English texts such as those expected for
input enhancement.
The input enhancement used for lexical classes is
the default set of activities provided by WERTi. In
the simplest case, Color, all automatically identified
instances in the web page are highlighted by color-
ing them; no learner interaction is required. This is
illustrated by Figure 2, which shows the result of en-
hancing prepositions in a web page from the British
Figure 2: Screenshot of color activity for prepositions, cf.
http://purl.org/icall/werti-color-ex
12
newspaper The Guardian.3
In this and the following screenshots, links al-
ready present in the original web page appear in light
blue (e.g., Vauban in Germany). This raises an im-
portant issue for future research, namely how to de-
termine the best visual input enhancement for a par-
ticular linguistic pattern given a specific web page
with its existing visual design features (e.g., bold-
facing in the text or particular colors used to indicate
links), which includes the option of removing or al-
tering some of those original visual design features.
A more interactive activity type is Click, where
the learner during reading can attempt to identify in-
stances of the targeted language form by clicking on
it. Correctly identified instances are colored green
by the system, incorrect guesses red.
Thirdly, input can be turned into Practice activi-
ties, where in its simplest form, WERTi turns web
pages into fill-in-the-blank activities and provides
immediate color coded feedback for the forms en-
tered by the learner. The system currently accepts
only the form used in the original text as correct.
In principle, alternatives (e.g., other prepositions)
can also be grammatical and appropriate. The ques-
tion for which cases equivalence classes of target an-
swers can automatically be determined is an interest-
ing question for future research.4
Gerunds vs. to-infinitives
Deciding when a verb is required to be realized as
a to-infinitive and when as a gerund -ing form can be
difficult for ESL learners. Current school grammars
teach students to look for certain lexical clues that
reliably indicate which form to choose. Examples
of such clues are prepositions such as after and of,
which can only be followed by a gerund.
In our NLP approach to this language pattern, we
use Constraint Grammar rules (Karlsson et al, 1995)
on top of POS tagging, which allow for straightfor-
ward formulation of local disambiguation rules such
as: ?If an -ing form immediately follows the prepo-
sition by, select the gerund reading.? Standard POS
3Given the nature of the input enhancement using colors, the
highlighting in the figure is only visible in a color printout.
4The issue bears some resemblance to the task of identify-
ing paraphrases (Androutsopoulos and Malakasiotis, 2009) or
classes of learner answers which differ in form but are equiva-
lent in terms of meaning (Bailey and Meurers, 2008).
tagsets for English contain a single tag for all -ing
forms. In order to identify gerunds only, we in-
troduce all possible readings for all -ing forms and
wrote 101 CG rules to locally disambiguate them.
The to-infinitives, on the other hand, are relatively
easy to identify based on the surface form and re-
quire almost no disambiguation.
For the implementation of the Constraint Gram-
mar rules, we used the freely available CG3 system.5
While simple local disambiguation rules are suffi-
cient for the pattern discussed here, through iterative
application of rules, Constraint Grammar can iden-
tify a wide range of phenomena without the need to
provide a full grammatical analysis.
The Color activity resulting from input enhance-
ment is similar to that for lexical classes described
above, but the system here enhances both verb forms
and clue phrases. Figure 3 shows the system high-
lighting gerunds in orange, infinitives in purple, and
clue phrases in blue.
Figure 3: Color activity for gerunds vs. to-infinitives, cf.
http://purl.org/icall/werti-color-ex2
For the Click activity, the web page is shown
with colored gerund and to-infinitival forms and the
learner can click on the corresponding clue phrases.
For the Practice activity, the learner is presented
with a fill-in-the-black version of the web page, as
in the screenshot in Figure 4. For each blank, the
learner needs to enter the gerund or to-infinitival
form of the base form shown in parentheses.
Wh-questions
Question formation in English, with its particu-
lar word order, constitutes a well-known challenge
for second language learners and has received sig-
nificant attention in the second language acquisi-
5http://beta.visl.sdu.dk/cg3.html
13
Figure 4: Practice activity for gerunds vs. to-infinitives,
cf. http://purl.org/icall/werti-cloze-ex
tion literature (cf., e.g., White et al, 1991; Spada
and Lightbown, 1993). Example (1) illustrates the
use of do-support and subject-aux inversion in wh-
questions as two aspects challenging learners.
(1) What do you think it takes to be successful?
In order to identify the wh-question patterns, we
employ a set of 126 hand-written Constraint Gram-
mar rules. The respective wh-word acts as the lex-
ical clue to the question as a whole, and the rules
then identify the subject and verb phrase based on
the POS and lexical information of the local context.
Aside from the Color activity highlighting the rel-
evant parts of a wh-question, we adapted the other
activity types to this more complex language pattern.
The Click activity prompts learners to click on either
the subject or the verb phrase of the question. The
Practice activity presents the words of a wh-question
in random order and requires the learner to rearrange
them into the correct one.
Conditionals
English has five types of conditionals that are used
for discussing hypothetical situations and possible
outcomes. The tenses used in the different condi-
tional types vary with respect to the certainty of the
outcome as expressed by the speaker/writer. For ex-
ample, one class of conditionals expresses high cer-
tainty and uses present tense in the if -clause and fu-
ture in the main clause, as in example (2).
(2) If the rain continues, we will return home.
The recognition of conditionals is approached us-
ing a combination of shallow and deep methods. We
first look for lexical triggers of a conditional, such as
the word if at the beginning of a sentence. This first
pass serves as a filter to the next, more expensive
processing step, full parsing of the candidate sen-
tences using Bikel?s statistical parser (Bikel, 2002).
The parse trees are then traversed to identify and
mark the verb forms and the trigger word.
For the input enhancement, we color all relevant
parts of a conditional, namely the trigger and the
verb forms. The Click activity for conditionals re-
quires the learner to click on exactly these parts. The
Practice activity prompts users to classify the condi-
tional instances into the different classes.
Phrasal verbs
Another challenging pattern for English language
learners are phrasal verbs consisting of a verb and
either a preposition, an adverb or both. The meaning
of a phrasal verb often differs considerably from that
of the underlying verb, as in (3) compared to (4).
(3) He switched the glasses without her noticing.
(4) He switched off the light before he went to bed.
This distinction is difficult for ESL learners, who
often confuse phrasal and non-phrasal uses.
Since this is a lexical phenomenon, we ap-
proached the identification of phrasal verbs via a
database lookup in a large online collection of verbs
known to occur in phrasal form.6 In order to find out
about noun phrases and modifying adverbs possibly
occurring in between the verb and its particles, we
run a chunker and use this information in specifying
a filter for such intervening elements.
The visual input enhancement activities targeting
phrasal verbs are the same as for lexical classes, with
the difference that for the Practice activity, learners
have to fill in only the particle, not the particle and
the main verb, since otherwise the missing contents
may be too difficult to reconstruct. Moreover, we
want the activity to focus on distinguishing phrasal
from non-phrasal uses, not verb meaning in general.
2.2 Evaluation issues
The success of a visual input enhancement approach
such as the one presented in this paper depends on
a number of factors, each of which can in principle
6http://www.usingenglish.com/reference/
phrasal-verbs
14
be evaluated. The fundamental but as far as we are
aware unanswered question in second language ac-
quisition research is for which language categories,
forms, and patterns input enhancement can be effec-
tive. As Lee and Huang (2008) show, the study of
visual input enhancement sorely needs more experi-
mental studies. With the help of the WERTi system,
which systematically produces visual input enhance-
ment for a range of language properties, it becomes
possible to conduct experiments in a real-life foreign
language teaching setting to test learning outcomes7
with and without visual input enhancement under a
wide range of parameters. Relevant parameters in-
clude the linguistic nature of the language property
to be enhanced as well as the nature of the input en-
hancement to be used, be it highlighting through col-
ors or fonts, engagement in different types of activi-
ties such as clicking, entering fill-in-the-blank infor-
mation, reordering language material, etc.
A factor closely related to our focus in this pa-
per is the impact of the quality of the NLP analysis.8
For a quantitative evaluation of the NLP, one signif-
icant problem is the mismatch between the phenom-
ena focused on in second language learning and the
available gold standards where these phenomena are
actually annotated. For example, standard corpora
such as the Penn Treebank contain almost no ques-
tions and thus do not constitute a useful gold stan-
dard for wh-question identification. Another prob-
lem is that some grammatical distinctions taught to
language learners are disputed in the linguistic liter-
ature. For example, Huddleston and Pullum (2002,
p. 1120) eliminate the distinction between gerunds
and present participles, combining them into a class
called ?gerund-participle?. And in corpus annota-
tion practice, gerunds are not identified as a class by
the tagsets used to annotate large corpora, making it
unclear what gold standard our gerund identification
component should be evaluated against.
While the lack of available gold standards means
that a quantitative evaluation of all WERTi mod-
ules is beyond the scope of this paper, the deter-
miner and preposition classes focused on in the lex-
ical classes module can be identified using the stan-
7Naturally, online measures of noticing, such as eye tracking
or Event-Related Potentials (ERP) would also be relevant.
8The processing time for the NLP analysis as other relevant
aspect is negligible for most of the activities presented here.
dard CLAWS-7 or Brown tagsets, for which gold-
standard corpora are available. We thus decided
to evaluate this WERTi module against the BNC
Sampler Corpus (Burnard, 1999), which contains
a variety of genres, making it particularly appro-
priate for evaluating a tool such as WERTi, which
learners are expected to use with a wide range of
web pages as input. The BNC Sampler corpus is
annotated with the fine-grained CLAWS-7 tagset9
where, e.g., prepositions are distinguished from sub-
ordinating conjunctions. By mapping the relevant
POS tags from the CLAWS-7 tagset to the Brown
tagset used by the LingPipe tagger as integrated in
WERTi, it becomes possible to evaluate WERTi?s
performance for the specific lexical classes focused
on for input enhancement, prepositions and deter-
miners. For prepositions, precision was 95.07% and
recall 90.52% while for determiners, precision was
97.06% with a recall of 94.07%.
The performance of the POS tagger on this refer-
ence corpus thus seems to be sufficient as basis for
visual input enhancement, but the crucial question
naturally remains whether identification of the target
patterns is reliable in the web pages that language
learners happen to choose. For a more precise quan-
titative study, it will thus be important to try the sys-
tem out with real-life users in order to identify a set
of web pages which can constitute an adequate test
set. Interestingly, which web pages the users choose
depends on the search engine front-end we provide
for them. As discussed under outlook in section 4,
we are exploring the option to implicitly guide them
towards web pages containing enough instances of
the relevant language patterns in text at the appro-
priate reading difficulty.
3 Context and related work
Contextualizing our work, one can view the auto-
matic visual input enhancement approach presented
here as an enrichment of Data-Driven Learning
(DDL). Where DDL has been characterized as an
?attempt to cut out the middleman [the teacher] as
far as possible and to give the learner direct access
to the data? (Boulton 2009, p. 82, citing Tim Johns),
in visual input enhancement the learner stays in con-
9http://www.natcorp.ox.ac.uk/docs/
c7spec.html
15
trol, but the NLP uses ?teacher knowledge? about rel-
evant and difficult language properties to make those
more prominent and noticeable for the learner.
In the context of Intelligent Computer-Assisted
Language Learning (ICALL), NLP has received
most attention in connection with Intelligent Lan-
guage Tutoring Systems, where NLP is used to ana-
lyze learner data and provide individual feedback on
that basis (cf. Heift and Schulze, 2007). Demands
on such NLP are high given that it needs to be able
to handle learner language and provide high-quality
feedback for any sentence entered by the learner.
In contrast, visual input enhancement makes use
of NLP analysis of authentic, native-speaker text and
thus applies the tools to the native language they
were originally designed and optimized for. Such
NLP use, which we will refer to as Authentic Text
ICALL (ATICALL), also does not need to be able
to correctly identify and manipulate all instances of
a language pattern for which input enhancement is
intended. Success can be incremental in the sense
that any visual input enhancement can be beneficial,
so that one can focus on enhancing those instances
which can be reliably identified in a text. In other
words, for ATICALL, precision of the NLP tools is
more important than recall. It is not necessary to
identify and enhance all instances of a given pattern
as long as the instances we do identify are in fact
correct, i.e., true positives. As the point of our sys-
tem is to enhance the reading experience by raising
language awareness, pattern occurrences we do not
identify are not harmful to the overall goal.10
We next turn to a discussion of some interest-
ing approaches in two closely related fields, exercise
generation and reading support tools.
3.1 Exercise Generation
Exercise generation is widely studied in CALL re-
search and some of the work relates directly to the
input enhancement approach presented in this paper.
For instance, Antoniadis et al (2004) describe the
plans of the MIRTO project to support ?gap-filling?
and ?lexical spotting? exercises in combination with
a corpus database. However, MIRTO seems to fo-
10While identifying all instances of a pattern indeed is not
crucial in this context, representativeness remains relevant to
some degree. Where only a skewed subset of a pattern is high-
lighted, learners may not properly conceptualize the pattern.
cus on a general architecture supporting instructor-
determined activity design. Visual input enhance-
ment or language awareness are not mentioned. The
VISL project (Bick, 2005) offers games and visual
presentations in order to foster knowledge of syntac-
tic forms and rules, and its KillerFiller tool can cre-
ate slot-filler exercises from texts. However, Killer-
Filler uses corpora and databases as the text base and
it presents sentences in isolation in a testing setup.
In contrast to such exercise generation systems, we
aim at enhancing the reader?s second language input
using the described web-based mash-up approach.
3.2 Reading Support Tools
Another branch of related approaches consists of
tools supporting the reading of texts in a foreign lan-
guage. For example, the Glosser-RuG project (Ner-
bonne et al, 1998) supports reading of French texts
for Dutch learners with an online, context-dependent
dictionary, as well as morphological analysis and ex-
amples of word use in corpora. A similar system,
focusing on multi-word lexemes, was developed in
the COMPASS project (Breidt and Feldweg, 1997).
More recently, the ALPHEIOS project11 has pro-
duced a system that can look up words in a lexi-
con and provide aligned translations. While such
lexicon-based tools are certainly useful to learners,
they rely on the learner asking for help instead of
enhancing specific structures from the start and thus
clearly differ from our approach.
Finally, the REAP project12 supports learners in
searching for texts that are well-suited for provid-
ing vocabulary and reading practice (Heilman et al,
2008). While it differs in focus from the visual input
enhancement paradigm underlying our approach, it
shares with it the emphasis on providing the learner
with authentic text in support of language learning.
4 Conclusion and Outlook
In this paper we presented an NLP architecture and
a concrete system for the enhancement of authen-
tic web pages in order to support language aware-
ness in ESL learners. The NLP architecture is flexi-
ble enough to integrate any processing approach that
lends itself to the treatment of the language phe-
11http://alpheios.net
12http://reap.cs.cmu.edu
16
nomenon in question, without confining the devel-
oper to a particular formalism. The WERTi system
illustrates this with five language patterns typically
considered difficult for ESL learners: lexical classes,
gerunds vs. to-infinitives, wh-questions, condition-
als and phrasal verbs.
Looking ahead, we already mentioned the funda-
mental open question where input enhancement can
be effective in section 2.2. A system such as WERTi,
systematically producing visual input enhancement,
can help explore this question under a wide range of
parameters in a real-life language teaching setting.
A more specific future research issue is the auto-
matic computation of equivalence classes of target
forms sketched in section 2.1. Not yet mentioned
but readily apparent is the goal to integrate more
language patterns known to be difficult for language
learners into WERTi (e.g., active/passive, tense and
aspect distinctions, relative clauses), and to explore
the approach for other languages, such as German.
A final important avenue for future research con-
cerns the starting point of the system, the step where
learners search for a web page they are interested
in and select it for presentation with input enhance-
ment. Enhancing of patterns presupposes that the
pages contain instances of the pattern. The less
frequent the pattern, the less likely we are to find
enough instances of it in web pages returned by the
standard web search engines typically used by learn-
ers to find pages of interest to them. The issue is re-
lated to research on providing learners with texts at
the right level of reading difficulty (Petersen, 2007;
Miltsakaki and Troutt, 2008), but the focus for us
is on ensuring that texts which include instances of
the specific language pattern targeted by a given in-
put enhancement are ranked high in the search re-
sults. Ott (2009) presents a search engine prototype
which, in addition to the content-focused document-
term information and traditional readability mea-
sures, supports indexing based on a more general no-
tion of a text model into which the patterns relevant
to input enhancement can be integrated ? an idea we
are exploring further (Ott and Meurers, Submitted).
Acknowledgments
We benefited from the feedback we received at
CALICO 06, EUROCALL 06, and the ICALL
course13 at ESSLLI 09, where we discussed our
work on the Python-based WERTi prototype. We
would like to thank Chris Hill and Kathy Corl
for their enthusiasm and encouragement. We are
grateful to Magdalena Leshtanska, Emma Li, Iliana
Simova, Maria Tchalakova and Tatiana Vodolazova
for their good ideas and WERTi module contribu-
tions in the context of a seminar at the University of
Tu?bingen in Summer 2008. Last but not least, the
paper benefited from two helpful workshop reviews.
References
Luiz Amaral, Vanessa Metcalf, and Detmar Meur-
ers. 2006. Language awareness through re-use
of NLP technology. Presentation at the CALICO
Workshop on NLP in CALL ? Computational and
Linguistic Challenges, May 17, 2006. University
of Hawaii. http://purl.org/dm/handouts/
calico06-amaral-metcalf-meurers.pdf.
Luiz Amaral, Detmar Meurers, and Ramon Ziai.
To Appear. Analyzing learner language: To-
wards a flexible NLP architecture for intelligent
language tutors. Computer-Assisted Language
Learning. http://purl.org/dm/papers/
amaral-meurers-ziai-10.html.
Ion Androutsopoulos and Prodromos Malakasiotis.
2009. A survey of paraphrasing and textual entailment
methods. Technical report, NLP Group, Informatics
Dept., Athens University of Economics and Business,
Greece. http://arxiv.org/abs/0912.3747.
Georges Antoniadis, Sandra Echinard, Olivier Kraif,
Thomas Lebarbe?, Mathieux Loiseau, and Claude Pon-
ton. 2004. NLP-based scripting for CALL activities.
In Proceedings of the COLING Workshop on eLearn-
ing for CL and CL for eLearning, Geneva.
Stacey Bailey and Detmar Meurers. 2008. Diagnosing
meaning errors in short answers to reading compre-
hension questions. In (Tetreault et al, 2008), pages
107?115.
Eckhard Bick. 2005. Grammar for fun: IT-based gram-
mar learning with VISL. In P. Juel, editor, CALL for
the Nordic Languages, pages 49?64. Samfundslitter-
atur, Copenhagen.
Daniel M. Bikel. 2002. Design of a multi-lingual,
parallel-processing statistical parsing engine. In Pro-
ceedings of the Second Int. Conference on Human
Language Technology Research, San Francisco.
Alex Boulton. 2009. Data-driven learning: Reasonable
fears and rational reassurance. Indian Journal of Ap-
plied Linguistics, 35(1):81?106.
13http://purl.org/dm/09/esslli/
17
Elisabeth Breidt and Helmut Feldweg. 1997. Accessing
foreign languages with COMPASS. Machine Transla-
tion, 12(1?2):153?174.
L. Burnard, 1999. Users Reference Guide for the BNC
Sampler. Available on the BNC Sampler CD.
Rachele De Felice. 2008. Automatic Error Detection in
Non-native English. Ph.D. thesis, St Catherine?s Col-
lege, University of Oxford.
Catherine Doughty and J. Williams, editors. 1998. Fo-
cus on form in classroom second language acquisition.
Cambridge University Press, Cambridge.
David Ferrucci and Adam Lally. 2004. UIMA: an ar-
chitectural approach to unstructured information pro-
cessing in the corporate research environment. Natu-
ral Language Engineering, 10(3?4):327?348.
W. Nelson Francis and Henry Kucera, 1979. Brown cor-
pus manual. Dept. of Linguistics, Brown University.
Thilo Go?tz and Oliver Suhre. 2004. Design and im-
plementation of the UIMA Common Analysis System.
IBM Systems Journal, 43(3):476?489.
Trude Heift and Mathias Schulze. 2007. Errors and In-
telligence in Computer-Assisted Language Learning:
Parsers and Pedagogues. Routledge.
Michael Heilman, Le Zhao, Juan Pino, and Maxine Eske-
nazi. 2008. Retrieval of reading materials for vocab-
ulary and reading practice. In (Tetreault et al, 2008),
pages 80?88.
Rodney Huddleston and Geoffrey K. Pullum. 2002. The
Cambridge Grammar of the English Language. Cam-
bridge University Press.
Nancy Ide, Patrice Bonhomme, and Laurent Romary.
2000. XCES: An XML-based encoding standard for
linguistic corpora. In Proceedings of the 2nd Int. Con-
ference on Language Resources and Evaluation.
Tim Johns. 1994. From printout to handout: Grammar
and vocabulary teaching in the context of data-driven
learning. In T. Odlin, editor, Perspectives on Pedagog-
ical Grammar, pages 293?313. CUP, Cambridge.
Fred Karlsson, Atro Voutilainen, Juha Heikkila?, and
Arto Anttila, editors. 1995. Constraint Grammar:
A Language-Independent System for Parsing Unre-
stricted Text. Mouton de Gruyter, Berlin, New York.
Sang-Ki Lee and Hung-Tzu Huang. 2008. Visual in-
put enhancement and grammar learning: A meta-
analytic review. Studies in Second Language Acqui-
sition, 30:307?331.
Patsy M. Lightbown and Nina Spada. 1999. How lan-
guages are learned. Oxford University Press, Oxford.
Patsy M. Lightbown. 1998. The importance of timing
in focus on form. In (Doughty and Williams, 1998),
pages 177?196.
Michael H. Long and Peter Robinson. 1998. Focus on
form: Theory, research, and practice. In (Doughty and
Williams, 1998), pages 15?41.
M. H. Long. 1991. Focus on form: A design feature
in language teaching methodology. In K. De Bot,
C. Kramsch, and R. Ginsberg, editors, Foreign lan-
guage research in cross-cultural perspective, pages
39?52. John Benjamins, Amsterdam.
Vanessa Metcalf and Detmar Meurers. 2006.
Generating web-based English preposition
exercises from real-world texts. Presenta-
tion at EUROCALL, Sept. 7, 2006. Granada,
Spain. http://purl.org/dm/handouts/
eurocall06-metcalf-meurers.pdf.
Eleni Miltsakaki and Audrey Troutt. 2008. Real time
web text classification and analysis of reading diffi-
culty. In (Tetreault et al, 2008), pages 89?97.
John Nerbonne, Duco Dokter, and Petra Smit. 1998.
Morphological processing and computer-assisted lan-
guage learning. Computer Assisted Language Learn-
ing, 11(5):543?559.
Niels Ott and Detmar Meurers. Submitted. Information
retrieval for education: Making search engines lan-
guage aware. http://purl.org/dm/papers/
ott-meurers-10.html.
Niels Ott. 2009. Information retrieval for language learn-
ing: An exploration of text difficulty measures. Mas-
ter?s thesis, International Studies in Computational
Linguistics, University of Tu?bingen.
Sarah E. Petersen. 2007. Natural Language Processing
Tools for Reading Level Assessment and Text Simplifi-
cation for Bilingual Education. Ph.D. thesis, Univer-
sity of Washington.
William E. Rutherford and Michael Sharwood Smith.
1985. Consciousness-raising and universal grammar.
Applied Linguistics, 6(2):274?282.
Richard W. Schmidt. 1990. The role of conscious-
ness in second language learning. Applied Linguistics,
11:206?226.
Michael Sharwood Smith. 1993. Input enhancement in
instructed SLA: Theoretical bases. Studies in Second
Language Acquisition, 15:165?179.
Nina Spada and Patsy M. Lightbown. 1993. Instruction
and the development of questions in l2 classrooms.
Studies in Second Language Acquisition, 15:205?224.
Joel Tetreault, Jill Burstein, and Rachele De Felice, ed-
itors. 2008. Proceedings of the Third Workshop on
Innovative Use of NLP for Building Educational Ap-
plications. ACL, Columbus, Ohio, June.
Lydia White, Nina Spada, Patsy M. Lightbown, and Leila
Ranta. 1991. Input enhancement and L2 question for-
mation. Applied Linguistics, 12(4):416?432.
18
