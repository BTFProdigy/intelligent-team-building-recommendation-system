Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 28?29,
Vancouver, October 2005.
THE MIT SPOKEN LECTURE PROCESSING PROJECT 
 
James R. Glass, Timothy J. Hazen, D. Scott Cyphers, Ken Schutte and Alex Park 
The MIT Computer Science and Artificial Intelligence Laboratory 
32 Vassar Street, Cambridge, Massachusetts, 02476, USA 
{hazen,jrg,cyphers}@csail.mit.edu 
 
Abstract 
We will demonstrate the MIT Spoken Lecture 
Processing Server and an accompanying lecture 
browser that students can use to quickly locate and 
browse lecture segments that apply to their query. 
We will show how lecturers can upload recorded 
lectures and companion text material to our server 
for automatic processing. The server automatically 
generates a time-aligned word transcript of the lec-
ture which can be downloaded for use within a 
browser. We will also demonstrate a browser we 
have created which allows students to quickly lo-
cate and browse audio segments that are relevant to 
their query. These tools can provide students with 
easier access to audio (or audio/visual) lectures, 
hopefully improving their educational experience. 
 
1 Introduction 
Over the past decade there has been increasing 
amounts of educational material being made avail-
able on-line. Projects such as MIT OpenCourse-
Ware provide continuous worldwide access to 
educational materials to help satisfy our collective 
thirst for knowledge. While the majority of such 
material is currently text-based, we are beginning 
to see dramatic increases in the amount of audio 
and visual recordings of lecture material. Unlike 
text materials, untranscribed audio data can be te-
dious to browse, making it difficult to utilize the 
information fully without time-consuming data 
preparation. Moreover, unlike some other forms of 
spoken communication such as telephone conver-
sations or television and radio broadcasts, lecture 
processing has until recently received little atten-
tion or benefit from the development of human 
language technology. The single biggest effort, to 
date, is on-going work in Japan using the Corpus 
of Spontaneous Japanese [1,3,4]. 
Lectures are particularly challenging for auto-
matic speech recognizers because the vocabulary 
used within a lecture can be very technical and 
specialized, yet the speaking style can be very 
spontaneous. As a result, even if parallel text mate-
rials are available in the form of textbooks or re-
lated papers, there are significant linguistic 
differences between written and oral communica-
tion styles. Thus, it is a challenge to predict how a 
written passage might be spoken, and vice versa. 
By helping to focus a research spotlight on spoken 
lecture material, we hope to begin to overcome 
these and many other fundamental issues. 
While audio-visual lecture processing will per-
haps be ultimately most useful, we have initially 
focused our attention on the problem of spoken 
lecture processing. Within this realm there are 
many challenging research issues pertaining to the 
development of effective automatic transcription, 
indexing, and summarization. For this project, our 
goals have been to a) help create a corpus of spo-
ken lecture material for the research community, b) 
analyze this corpus to better understand the lin-
guistic characteristics of spoken lectures, c) per-
form speech recognition and information retrieval 
experiments on these data to benchmark perform-
ance on these data, d) develop a prototype spoken 
lecture processing server that will allow educators 
to automatically annotate their recorded lecture 
data, and e) develop prototype software that will 
allow students to browse the resulting annotated 
lectures. 
2 Project Details 
As mentioned earlier, we have developed a web-
based Spoken Lecture Processing Server 
(http://groups.csail.mit.edu/sls/lectures) in which 
users can upload audio files for automatic tran-
scription and indexing. In our work, we have ex-
28
perimented with collecting audio data using a 
small personal digital audio recorder (an iRiver 
N10). To help the speech recognizer, users can 
provide their own supplemental text files, such as 
journal articles, book chapters, etc., which can be 
used to adapt the language model and vocabulary 
of the system.  Currently, the key steps of the tran-
scription process are as follows: a) adapt a topic-
independent vocabulary and language model using 
any supplemental text materials, b) automatically 
segment the audio file into short chunks of pause-
delineated speech, and c) automatically annotate 
these chunks using a speech recognition system. 
Language model adaptation is performed is two 
steps. First the vocabulary of any supplemental text 
material is extracted and added to an existing 
topic-independent vocabulary of nearly 17K 
words. Next, the recognizer merges topic-
independent word sequence statistics from an 
existing corpus of lecture material with the topic-
dependent statistics of the supplemental material to 
create a topic-adapted language model. 
The segmentation algorithm is performed in two 
steps. First the audio file is arbitrarily broken into 
10-second chunks for speech detection processing 
using an efficient speaker-independent phonetic 
recognizer. To help improve its speech detection 
accuracy, this recognizer contains models for non-
lexical artifacts such as laughs and coughs as well 
as a variety of other noises. Contiguous regions of 
speech are identified from the phonetic recognition 
output (typically 6 to 8 second segments of speech) 
and passed alone to our speech recognizer for 
automatic transcription. The speech segmentation 
and transcription steps are currently performed in a 
distributed fashion over a bank of computation 
servers. Once recognition is completed, the audio 
data is indexed (based on the recognition output) in 
preparation for browsing by the user. 
The lecture browser provides a graphical user in-
terface to one or more automatically transcribed 
lectures. A user can type a text query to the 
browser and receive a list of hits within the in-
dexed lectures. When a hit is selected, it is shown 
in the context of the lecture transcription. The user 
can adjust the duration of context preceding and 
following the hit, navigate to and from the preced-
ing and following parts of the lecture, and listen to 
the displayed segment. Orthographic segments are 
highlighted as they are played. 
 
3 Experimental Results 
To date we have collected and analyzed a corpus 
of approximately 300 hours of audio lectures in-
cluding 6 full MIT courses and 80 hours of semi-
nars from the MIT World web site [2]. We are 
currently in the process of expanding this corpus. 
From manual transcriptions we have generated and 
verified time-aligned transcriptions for 169 hours 
of our corpus, and we are in the process of time-
aligning transcriptions for the remainder of our 
corpus.  
We have performed initial speech recognition 
experiments using 10 computer science lectures. In 
these experiments we have discovered that, despite 
high word error rates (in the area of 40%), retrieval 
of short audio segments containing important key-
words and phrases can be performed with a high-
degree of reliability (over 90% F-measure when 
examining precision and recall results) [5]. These 
results are similar in nature to the findings in the 
SpeechBot project (which performs a similar ser-
vice for online broadcast news archives) [6]. 
References  
[1] S. Furui, ?Recent advances in spontaneous speech 
recognition and understanding,? in Proc. ISCA & IEEE 
Workshop on Spontaneous Speech Processing and Rec-
ognition (SSPR), pp. 1-6, Tokyo, April 2003. 
[2] J. Glass, T. Hazen, L. Hetherington, and C. Wang, 
?Analysis and Processing of Lecture Audio Data: Pre-
liminary Investigations,? in Proc. HLT/NAACL Speech 
Indexing Workshop, 9-12, Boston, May 2004. 
[3] T. Kawahara, H. Nanjo. And S. Furui,  ?Automatic 
transcription of spontaneous lecture speech,? in IEEE 
Workshop on Automatic Speech Recognition and Un-
derstanding, pp. 186-189, Trento, Italy, December 
2001. 
[4] H. Nanjo and T. Kawahara, ?Language model and 
speaking rate adaptation for spontaneous speech recog-
nition,? IEEE Transactions of Speech and Audio Proc-
essing, vol. 12, no. 4, pp. 391-400, July 2004. 
[5] A. Park, T. Hazen, and J. Glass, "Automatic 
Processing of Audio Lectures for Information Retrieval: 
Vocabulary Selection and Language Modeling," Proc. 
ICASSP, Philadelphia, PA, March 2005. 
[6] J.-M. Van Thong, et al ?SpeechBot: An experimen-
tal speech-based search engine for multimedia content 
on the web. IEEE Transactions of Multimedia, vol. 4, 
no. 1, pp. 88-96, March 2002.  
29
Proceedings of the ACL-08: HLT Workshop on Mobile Language Processing, pages 1?9,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Multimodal Home Entertainment Interface via a Mobile Device
Alexander Gruenstein Bo-June (Paul) Hsu James Glass Stephanie Seneff
Lee Hetherington Scott Cyphers Ibrahim Badr Chao Wang Sean Liu
MIT Computer Science and Artificial Intelligence Laboratory
32 Vassar St, Cambridge, MA 02139 USA
http://www.sls.csail.mit.edu/
Abstract
We describe a multimodal dialogue system for
interacting with a home entertainment center
via a mobile device. In our working proto-
type, users may utilize both a graphical and
speech user interface to search TV listings,
record and play television programs, and listen
to music. The developed framework is quite
generic, potentially supporting a wide variety
of applications, as we demonstrate by integrat-
ing a weather forecast application. In the pro-
totype, the mobile device serves as the locus
of interaction, providing both a small touch-
screen display, and speech input and output;
while the TV screen features a larger, richer
GUI. The system architecture is agnostic to
the location of the natural language process-
ing components: a consistent user experience
is maintained regardless of whether they run
on a remote server or on the device itself.
1 Introduction
People have access to large libraries of digital con-
tent both in their living rooms and on their mobile
devices. Digital video recorders (DVRs) allow peo-
ple to record TV programs from hundreds of chan-
nels for subsequent viewing at home?or, increas-
ingly, on their mobile devices. Similarly, having
accumulated vast libraries of digital music, people
yearn for an easy way to sift through them from the
comfort of their couches, in their cars, and on the go.
Mobile devices are already central to accessing
digital media libraries while users are away from
home: people listen to music or watch video record-
ings. Mobile devices also play an increasingly im-
portant role in managing digital media libraries. For
instance, a web-enabled mobile phone can be used to
remotely schedule TV recordings through a web site
or via a custom application. Such management tasks
often prove cumbersome, however, as it is challeng-
ing to browse through listings for hundreds of TV
channels on a small display. Indeed, even on a large
screen in the living room, browsing alphabetically,
or by time and channel, for a particular show using
the remote control quickly becomes unwieldy.
Speech and multimodal interfaces provide a nat-
ural means of addressing many of these challenges.
It is effortless for people to say the name of a pro-
gram, for instance, in order to search for existing
recordings. Moreover, such a speech browsing ca-
pability is useful both in the living room and away
from home. Thus, a natural way to provide speech-
based control of a media library is through the user?s
mobile device itself.
In this paper we describe just such a prototype
system. A mobile phone plays a central role in pro-
viding a multimodal, natural language interface to
both a digital video recorder and a music library.
Users can interact with the system?presented as a
dynamic web page on the mobile browser?using
the navigation keys, the stylus, or spoken natural
language. In front of the TV, a much richer GUI is
also available, along with support for playing video
recordings and music.
In the prototype described herein, the mobile de-
vice serves as the locus of natural language in-
teraction, whether a user is in the living room or
walking down the street. Since these environments
may be very different in terms of computational re-
1
sources and network bandwidth, it is important that
the architecture allows for multiple configurations in
terms of the location of the natural language pro-
cessing components. For instance, when a device
is connected to a Wi-Fi network at home, recogni-
tion latency may be reduced by performing speech
and natural language processing on the home me-
dia server. Moreover, a powerful server may enable
more sophisticated processing techniques, such as
multipass speech recognition (Hetherington, 2005;
Chung et al, 2004), for improved accuracy. In sit-
uations with reduced network connectivity, latency
may be improved by performing speech recognition
and natural language processing tasks on the mobile
device itself. Given resource constraints, however,
less detailed acoustic and language models may be
required. We have developed just such a flexible ar-
chitecture, with many of the natural language pro-
cessing components able to run on either a server or
the mobile device itself. Regardless of the configu-
ration, a consistent user experience is maintained.
2 Related Work
Various academic researchers and commercial busi-
nesses have demonstrated speech-enabled interfaces
to entertainment centers. A good deal of the work
focuses on adding a microphone to a remote con-
trol, so that speech input may be used in addition
to a traditional remote control. Much commercial
work, for example (Fujita et al, 2003), tends to fo-
cus on constrained grammar systems, where speech
input is limited to a small set of templates corre-
sponding to menu choices. (Berglund and Johans-
son, 2004) present a remote-control based speech
interface for navigating an existing interactive tele-
vision on-screen menu, though experimenters man-
ually transcribed user utterances as they spoke in-
stead of using a speech recognizer. (Oh et al, 2007)
present a dialogue system for TV control that makes
use of concept spotting and statistical dialogue man-
agement to understand queries. A version of their
system can run independently on low-resource de-
vices such as PDAs; however, it has a smaller vo-
cabulary and supports a limited set of user utterance
templates. Finally, (Wittenburg et al, 2006) look
mainly at the problem of searching for television
programs using speech, an on-screen display, and a
remote control. They explore a Speech-In List-Out
interface to searching for episodes of television pro-
grams.
(Portele et al, 2003) depart from the model of
adding a speech interface component to an exist-
ing on-screen menu. Instead, they a create a tablet
PC interface to an electronic program guide, though
they do not use the television display as well. Users
may search an electronic program guide using con-
straints such as date, time, and genre; however, they
can?t search by title. Users can also perform typi-
cal remote-control tasks like turning the television
on and off, and changing the channel. (Johnston et
al., 2007) also use a tablet PC to provide an inter-
face to television content?in this case a database of
movies. The search can be constrained by attributes
such as title, director, or starring actors. The tablet
PC pen can be used to handwrite queries and to point
at items (such as actor names) while the user speaks.
We were also inspired by previous prototypes in
which mobile devices have been used in conjunc-
tion with larger, shared displays. For instance, (Paek
et al, 2004) demonstrate a framework for building
such applications. The prototype we demonstrate
here fits into their ?Jukebox? model of interaction.
Interactive workspaces, such as the one described in
(Johanson et al, 2002), also demonstrate the utility
of integrating mobile and large screen displays. Our
prototype is a departure from these systems, how-
ever, in that it provides for spoken interactions.
Finally, there is related work in the use of mobile
devices for various kinds of search. For instance, of-
ferings from Microsoft (Acero et al, 2008), Vlingo,1
and Promptu2 allow users to search for items like
businesses and songs using their mobile phones.
These applications differ from ours in that speech
is used only for search, without any accompanying
command and control capabilities. Also, these ser-
vices do not allow interaction with your own de-
vices at home. Efforts have been made to use mo-
bile devices for control of devices in the home, such
as in (Nichols and Myers, 2006), however these ef-
forts have not involved the use of speech as an input
modality.
1http://www.vlingo.com
2http://www.promptu.com
2
Na
vig
ati
on
 pa
d o
r s
tyl
us 
can
 
be
 us
ed
 to
 na
vig
ate
 G
UI
Sta
tus
 ic
on
 in
dic
ate
s c
on
ne
cti
on
 to
 se
rve
r, 
mi
cro
ph
on
e i
co
n w
he
n l
ist
en
ing
Ho
ld-
to-
tal
k 
bu
tto
n
Op
era we
b 
bro
ws
er
Sy
nc
hro
niz
ed
 se
lec
tio
n o
n m
ob
ile
 an
d 
tel
ev
isi
on
 di
spl
ay
s
Fir
efo
x
we
b b
row
ser
Ta
bs 
all
ow
 ea
sy 
sw
itc
hin
g 
am
on
g d
om
ain
s
Me
dia
 pl
ay
ba
ck
 
co
ntr
ols
Figure 1: User interface overview.
(a) Program Guide
(b) Scheduled Recordings
(c) Music Library
U: What?s on tonight at seven o?clock?
S: I found seventy entries. [Displays program guide.]
U: Are there any action movies on HBO tomorrow?
S: I found two entries. [Displays results.]
U: When is American Idol on?
S: I found three entries. [Displays results.]
U: Record the first one.
S: I have added American Idol to the list of programs to be
recorded. [Displays recording settings.]
U: Please record Nova on Wednesday night.
S: I have added Nova to the list of programs to be recorded.
[Displays recording settings.]
U: Show all my scheduled recordings.
S: There are seven scheduled recordings. [Shows list.]
U: Delete BBC World News.
S: I have deleted the scheduled recording: BBC World
News. [Updates List.]
U: Show my videos.
S: [Displays list of videos.]
U: Delete Frasier.
S: I have deleted the recording Frasier. [Updates list.]
U: Play Grey?s Anatomy.
S: Playing Grey?s Anatomy. [Plays recording.]
U: Stop Playing.
S: Stopping. [Stops playback.]
U: [Clicks on Music tab.]
S: [Displays artist list.]
U: Show albums by Billie Holiday.
S: I found three albums by Billie Holiday. [Shows albums.]
U: Please play A Hard Rain?s A-Gonna Fall by Bob Dylan.
S: Playing A Hard Rain?s A-Gonna Fall. [Plays song.]
Figure 2: Screenshots and an example interaction. Utterances are labeled with U for user and S for system.
3
3 User Experience
Our current prototype system implements the basic
functionalities that one expects from a home enter-
tainment center. Users can navigate through and
record programs from the television?s electronic pro-
gram guide, manage recording settings, and play
recorded videos. They can also browse and listen to
selections from their music libraries. However, un-
like existing prototypes, ours employs a smartphone
with a navigation pad, touch-sensitive screen, and
built-in microphone as the remote control. Figure 1
provides an overview of the graphical user interface
on both the TV and mobile device.
Mirroring the TV?s on-screen display, the proto-
type system presents a reduced view on the mobile
device with synchronized cursors. Users can navi-
gate the hierarchical menu structure using the arrow
keys or directly click on the target item with the sty-
lus. While away from the living room, or when a
recording is playing full screen, users can browse
and manage their media libraries using only the mo-
bile device.
While the navigation pad and stylus are great for
basic navigation and control, searching for media
with specific attributes, such as title, remains cum-
bersome. To facilitate such interactions, the cur-
rent system supports spoken natural language inter-
actions. For example, the user can press the hold-to-
talk button located on the side of the mobile device
and ask ?What?s on the National Geographic Chan-
nel this afternoon?? to retrieve a list of shows with
the specified channel and time. The system responds
with a short verbal summary ?I found six entries on
January seventh? and presents the resulting list on
both the TV and mobile displays. The user can then
browse the list using the navigation pad or press the
hold-to-talk button to barge in with another com-
mand, e.g. ?Please record the second one.? Depress-
ing the hold-to-talk button not only terminates any
current spoken response, but also mutes the TV to
minimize interference with speech recognition. As
the previous example demonstrates, contextual in-
formation is used to resolve list position references
and disambiguate commands.
The speech interface to the user?s music library
works in a similar fashion. Users can search by
artist, album, and song name, and then play the
songs found. To demonstrate the extensibility of
the architecture, we have also integrated an exist-
ing weather information system (Zue et al, 2000),
which has been previously deployed as a telephony
application. Users simply click on the Weather tab
to switch to this domain, allowing them to ask a wide
range of weather queries. The system responds ver-
bally and with a simple graphical forecast.
To create a natural user experience, we designed
the multimodal interface to allow users to seam-
lessly switch among the different input modalities
available on the mobile device. Figure 2 demon-
strates an example interaction with the prototype, as
well as several screenshots of the user interface.
4 System Architecture
The system architecture is quite flexible with re-
gards to the placement of the natural language pro-
cessing components. Figure 3 presents two possible
configurations of the system components distributed
across the mobile device, home media server, and
TV display. In 3(a), all speech recognition and nat-
ural language processing components reside on the
server, with the mobile device acting as the micro-
phone, speaker, display, and remote control. In 3(b),
the speech recognizer, language understanding com-
ponent, language generation component, and text-
to-speech (TTS) synthesizer run on the mobile de-
vice. Depending on the capabilities of the mobile
device and network connection, different configu-
rations may be optimal. For instance, on a power-
ful device with slow network connection, recogni-
tion latency may be reduced by performing speech
recognition and natural language processing on the
device. On the other hand, streaming audio via a fast
wireless network to the server for processing may
result in improved accuracy.
In the prototype system, flexible and reusable
speech recognition and natural language processing
capabilities are provided via generic components de-
veloped and deployed in numerous spoken dialogue
systems by our group, with the exception of an off-
the-shelf speech synthesizer. Speech input from the
mobile device is recognized using the landmark-
based SUMMIT system (Glass, 2003). The result-
ing N-best hypotheses are processed by the TINA
language understanding component (Seneff, 1992).
4
Gala
xy
Spee
ch?R
ecog
nize
r
Lang
uage
?Und
ersta
ndin
g
Dialo
gue?
Man
ager
Lang
uage
?Gen
erat
ion
Text
?To?S
peec
h
Web
?
Serv
er
Tele
visio
n Web
?Bro
wse
r
Med
ia?Pl
ayer
TV?G
uide
?
Med
ia
Mob
ile?D
evic
e
Web
?Bro
wse
r
Hom
e?M
edia
?Serv
er
Wea
ther
Mob
ile?
Man
ager
Aud
io?In
put?
/?Ou
tput
Gala
xy
Dialo
gue?
Man
ager
Web
?
Serv
er Tele
visio
n Web
?Bro
wse
r
Med
ia?Pl
ayer
TV?G
uide
?
Med
ia
Mob
ile?D
evic
e
Web
?Bro
wse
r
Hom
e?M
edia
?Serv
er
Wea
ther
Mob
ile?
Man
ager
Aud
io?In
put?
/?Ou
tput
Spee
ch?R
ecog
nize
r
Lang
uage
?Gen
erat
ion
Text
?To?S
peec
h
Lang
uage
?Und
ersta
ndin
g
(a) (b)
Figure 3: Two architecture diagrams. In (a) speech recognition and natural language processing occur on the server,
while in (b) processing is primarily performed on the device.
Based on the resulting meaning representation, the
dialogue manager (Polifroni et al, 2003) incorpo-
rates contextual information (Filisko and Seneff,
2003), and then determines an appropriate response.
The response consists of an update to the graph-
ical display, and a spoken system response which
is realized via the GENESIS (Baptist and Seneff,
2000) language generation module. To support on-
device processing, all the components are linked via
the GALAXY framework (Seneff et al, 1998) with
an additional Mobile Manager component responsi-
ble for coordinating the communication between the
mobile device and the home media server.
In the currently deployed system, we use a mo-
bile phone with a 624 MHz ARM processor run-
ning the Windows Mobile operating system and
Opera Mobile web browser. The TV program and
music databases reside on the home media server
running GNU/Linux. The TV program guide data
and recording capabilities are provided via MythTV,
a full-featured, open-source digital video recorder
software package.3 Daily updates to the program
guide information typically contain hundreds of
unique channel names and thousands of unique pro-
gram names. The music library is comprised of
5,000 songs from over 80 artists and 13 major gen-
res, indexed using the open-source text search en-
gine Lucene.4 Lastly, the TV display can be driven
by a web browser on either the home media server or
a separate computer connected to the server via a fast
Ethernet connection, for high quality video stream-
ing.
3http://www.mythtv.org/
4http://lucene.apache.org/
While the focus of this paper is on the natural lan-
guage processing and user interface aspects of the
system, our work is actually situated within a larger
collaborative project at MIT that also includes sim-
plified device configuration (Mazzola Paluska et al,
2008; Mazzola Paluska et al, 2006), transparent ac-
cess to remote servers (Ford et al, 2006), and im-
proved security.
5 Mobile Natural Language Components
Porting the implementation of the various speech
recognizer and natural language processing com-
ponents to mobile devices with limited computa-
tion and memory presents both a research and en-
gineering challenge. Instead of creating a small vo-
cabulary, fixed phrase dialogue system, we aim to
support?on the mobile device?the same flexible
and natural language interactions currently available
on our desktop, tablet, and telephony systems; see
e.g., (Gruenstein et al, 2006; Seneff, 2002; Zue et
al., 2000). In this section, we summarize our ef-
forts thus far in implementing the SUMMIT speech
recognizer and TINA natural language parser. Ports
of the GENESIS language generation system and of
our dialogue manager are well underway, and we ex-
pect to have these components working on the mo-
bile device in the near future.
5.1 PocketSUMMIT
To significantly reduce the memory footprint and
overall computation, we chose to reimplement our
segment-based speech recognizer from scratch, uti-
lizing fixed-point arithmetic, parameter quantiza-
tion, and bit-packing in the binary model files.
The resulting PocketSUMMIT recognizer (Hether-
5
ington, 2007) utilizes only the landmark features,
initially forgoing segment features such as phonetic
duration, as they introduce algorithmic complexities
for relatively small word error rate (WER) improve-
ments.
In the current system, we quantize the mean and
variance of each Gaussian mixture model dimension
to 5 and 3 bits, respectively. Such quantization not
only results in an 8-fold reduction in model size, but
also yields about a 50% speedup by enabling table
lookups for Gaussian evaluations. Likewise, in the
finite-state transducers (FSTs) used to represent the
language model, lexical, phonological, and class di-
phone constraints, quantizing the FST weights and
bit-packing not only compress the resulting binary
model files, but also reduce the processing time with
improved processor cache locality.
In the aforementioned TV, music, and weather do-
mains with a moderate vocabulary of a few thou-
sand words, the resulting PocketSUMMIT recog-
nizer performs in approximately real-time on 400-
600 MHz ARM processors, using a total of 2-4
MB of memory, including 1-2 MB for memory-
mapped model files. Compared with equivalent non-
quantized models, PocketSUMMIT achieves dra-
matic improvements in speed and memory while
maintaining comparable WER performance.
5.2 PocketTINA
Porting the TINA natural language parser to mobile
devices involved significant software engineering to
reduce the memory and computational requirements
of the core data structures and algorithms. TINA
utilizes a best-first search that explores thousands of
partial parses when processing an input utterance.
To efficiently manage memory allocation given the
unpredictability of pruning invalid parses (e.g. due
to subject-verb agreement), we implemented a mark
and sweep garbage collection mechanism. Com-
bined with a more efficient implementation of the
priority queue and the use of aggressive ?beam?
pruning, the resulting PocketTINA system provides
identical output as server-side TINA, but can parse
a 10-best recognition hypothesis list into the corre-
sponding meaning representation in under 0.1 sec-
onds, using about 2 MB of memory.
6 Rapid Dialogue System Development
Over the course of developing dialogue systems for
many domains, we have built generic natural lan-
guage understanding components that enable the
rapid development of flexible and natural spoken di-
alogue systems for novel domains. Creating such
prototype systems typically involves customizing
the following to the target domain: recognizer lan-
guage model, language understanding parser gram-
mar, context resolution rules, dialogue management
control script, and language generation rules.
Recognizer Language Model Given a new do-
main, we first identify a set of semantic classes
which correspond to the back-end application?s
database, such as artist, album, and genre. Ideally,
we would have a corpus of tagged utterances col-
lected from real users. However, when building pro-
totypes such as the one described here, little or no
training data is usually available. Thus, we create
a domain-specific context-free grammar to generate
a supplemental corpus of synthetic utterances. The
corpus is used to train probabilities for the natural
language parsing grammar (described immediately
below), which in turn is used to derive a class n-
gram language model (Seneff et al, 2003).
Classes in the language model which corre-
spond to contents of the database are marked as
dynamic, and are populated at runtime from the
database (Chung et al, 2004; Hetherington, 2005).
Database entries are heuristically normalized into
spoken forms. Pronunciations not in our 150,000
word lexicon are automatically generated (Seneff,
2007).
Parser Grammar The TINA parser uses a prob-
abilistic context-free grammar enhanced with sup-
port for wh-movement and grammatical agreement
constraints. We have developed a generic syntac-
tic grammar by examining hundreds of thousands
of utterances collected from real user interactions
with various existing dialogue systems. In addition,
we have developed libraries which parse and inter-
pret common semantic classes like dates, times, and
numbers. The grammar and semantic libraries pro-
vide good coverage for spoken dialogue systems in
database-query domains.
6
To build a grammar for a new domain, a devel-
oper extends the generic syntactic grammar by aug-
menting it with domain-specific semantic categories
and their lexical entries. A probability model which
conditions each node category on its left sibling and
parent is then estimated from a training corpus of
utterances (Seneff et al, 2003).
At runtime, the recognizer tags the hypothesized
dynamic class expansions with their class names,
allowing the parser grammar to be independent of
the database contents. Furthermore, each semantic
class is designated either as a semantic entity, or as
an attribute associated with a particular entity. This
enables the generation of a semantic representation
from the parse tree.
Dialogue Management & Language Generation
Once an utterance is recognized and parsed, the
meaning representation is passed to the context res-
olution and dialogue manager component. The con-
text resolution module (Filisko and Seneff, 2003)
applies generic and domain-specific rules to re-
solve anaphora and deixis, and to interpret frag-
ments and ellipsis in context. The dialogue man-
ager then interacts with the application back-end
and database, controlled by a script customized for
the domain (Polifroni et al, 2003). Finally, the
GENESIS module (Baptist and Seneff, 2000) ap-
plies domain-specific rules to generate a natural lan-
guage representation of the dialogue manager?s re-
sponse, which is sent to a speech synthesizer. The
dialogue manager also sends an update to the GUI,
so that, for example, the appropriate database search
results are displayed.
7 Mobile Design Challenges
Dialogue systems for mobile devices present a
unique set of design challenges not found in tele-
phony and desktop applications. Here we describe
some of the design choices made while developing
this prototype, and discuss their tradeoffs.
7.1 Client/Server Tradeoffs
Towards supporting network-less scenarios, we have
begun porting various natural language processing
components to mobile platforms, as discussed in
Section 5. Having efficient mobile implementations
further allows the natural language processing tasks
to be performed on either the mobile device or the
server. While building the prototype, we observed
that the Wi-Fi network performance can often be un-
predictable, resulting in erratic recognition latency
that occasionally exceeds on-device recognition la-
tency. However, utilizing the mobile processor for
computationally intensive tasks rapidly drains the
battery. Currently, the component architecture in the
prototype system is pre-configured. A more robust
implementation would dynamically adjust the con-
figuration to optimize the tradeoffs among network
use, CPU utilization, power consumption, and user-
perceived latency/accuracy.
7.2 Speech User Interface
As neither open-mic nor push-to-talk with automatic
endpoint detection is practical on mobile devices
with limited battery life, our prototype system em-
ploys a hold-to-talk hardware button for microphone
control. To guide users to speak commands only
while the button is depressed, a short beep is played
as an earcon both when the button is pushed and
released. Since users are less likely to talk over
short audio clips, the use of earcons mitigates the
tendency for users to start speaking before pushing
down the microphone button.
In the current system, media audio is played over
the TV speakers, whereas TTS output is sent to
the mobile device speakers. To reduce background
noise captured from the mobile device?s far-field mi-
crophone, the TV is muted while the microphone
button is depressed. Unlike telephony spoken di-
alogue systems where the recognizer has to con-
stantly monitor for barge-in, the use of a hold-to-
talk button significantly simplifies barge-in support,
while reducing power consumption.
7.3 Graphical User Interface
In addition to supporting interactive natural lan-
guage dialogues via the spoken user interface, the
prototype system implements a graphical user in-
terface (GUI) on the mobile device to supplement
the TV?s on-screen interface. To faciliate rapid pro-
totyping, we chose to implement both the mobile
and TV GUI using web pages with AJAX (Asyn-
chronous Javascript and XML) techniques, an ap-
proach we have leveraged in several existing mul-
timodal dialogue systems, e.g. (Gruenstein et al,
7
2006; McGraw and Seneff, 2007). The resulting in-
terface is largely platform-independent and allows
display updates to be ?pushed? to the client browser.
As many users are already familiar with the TV?s
on-screen interface, we chose to mirror the same in-
terface on the mobile device and synchronize the
selection cursor. However, unlike desktop GUIs,
mobile devices are constrained by a small display,
limited computational power, and reduced network
bandwidth. Thus, both the page layout and infor-
mation detail were adjusted for the mobile browser.
Although AJAX is more responsive than traditional
web technology, rendering large formatted pages?
such as the program guide grid?is often still un-
acceptably slow. In the current implementation, we
addressed this problem by displaying only the first
section of the content and providing a ?Show More?
button that downloads and renders the full content.
While browser-based GUIs expedite rapid prototyp-
ing, deployed systems may want to take advantage
of native interfaces specific to the device for more
responsive user interactions. Instead of limiting the
mobile interface to reflect the TV GUI, improved us-
ability may be obtained by designing the interface
for the mobile device first and then expanding the
visual content to the TV display.
7.4 Client/Server Communication
In the current prototype, communication between
the mobile device and the media server consists of
AJAX HTTP and XML-RPC requests. To enable
server-side ?push? updates, the client periodically
pings the server for messages. While such an im-
plementation provides a responsive user interface, it
quickly drains the battery and is not robust to net-
work outages resulting from the device being moved
or switching to power-saving mode. Reestablish-
ing connection with the server further introduces la-
tency. In future implementations, we would like to
examine the use of Bluetooth for lower power con-
sumption, and infrared for immediate response to
common controls and basic navigation.
8 Conclusions & Future Work
We have presented a prototype system that demon-
strates the feasibility of deploying a multimodal,
natural language interface on a mobile device for
browsing and managing one?s home media library.
In developing the prototype, we have experimented
with a novel role for a mobile device?that of a
speech-enabled remote control. We have demon-
strated a flexible natural language understanding ar-
chitecture, in which various processing stages may
be performed on either the server or mobile device,
as networking and processing power considerations
require.
While the mobile platform presents many chal-
lenges, it also provides unique opportunities.
Whereas desktop computers and TV remote controls
tend to be shared by multiple users, a mobile device
is typically used by a single individual. By collect-
ing and adapting to the usage data, the system can
personalize the recognition and understanding mod-
els to improve the system accuracy. In future sys-
tems, we hope to not only explore such adaptation
possibilities, but also study how real users interact
with the system to further improve the user interface.
Acknowledgments
This research is sponsored by the TParty Project,
a joint research program between MIT and Quanta
Computer, Inc.; and by Nokia, as part of a joint MIT-
Nokia collaboration. We are also thankful to three
anonymous reviewers for their constructive feed-
back.
References
A. Acero, N. Bernstein, R. Chambers, Y. C. Jui, X. Li,
J. Odell, P. Nguyen, O. Scholz, and G. Zweig. 2008.
Live search for mobile: Web services by voice on the
cellphone. In Proc. of ICASSP.
L. Baptist and S. Seneff. 2000. Genesis-II: A versatile
system for language generation in conversational sys-
tem applications. In Proc. of ICSLP.
A. Berglund and P. Johansson. 2004. Using speech and
dialogue for interactive TV navigation. Universal Ac-
cess in the Information Society, 3(3-4):224?238.
G. Chung, S. Seneff, C. Wang, and L. Hetherington.
2004. A dynamic vocabulary spoken dialogue inter-
face. In Proc. of INTERSPEECH, pages 327?330.
E. Filisko and S. Seneff. 2003. A context resolution
server for the GALAXY conversational systems. In
Proc. of EUROSPEECH.
B. Ford, J. Strauss, C. Lesniewski-Laas, S. Rhea,
F. Kaashoek, and R. Morris. 2006. Persistent personal
names for globally connected mobile devices. In Pro-
ceedings of the 7th USENIX Symposium on Operating
Systems Design and Implementation (OSDI ?06).
8
K. Fujita, H. Kuwano, T. Tsuzuki, and Y. Ono. 2003.
A new digital TV interface employing speech recog-
nition. IEEE Transactions on Consumer Electronics,
49(3):765?769.
J. Glass. 2003. A probabilistic framework for segment-
based speech recognition. Computer Speech and Lan-
guage, 17:137?152.
A. Gruenstein, S. Seneff, and C. Wang. 2006. Scalable
and portable web-based multimodal dialogue interac-
tion with geographical databases. In Proc. of INTER-
SPEECH.
I. L. Hetherington. 2005. A multi-pass, dynamic-
vocabulary approach to real-time, large-vocabulary
speech recognition. In Proc. of INTERSPEECH.
I. L. Hetherington. 2007. PocketSUMMIT: Small-
footprint continuous speech recognition. In Proc. of
INTERSPEECH, pages 1465?1468.
B. Johanson, A. Fox, and T. Winograd. 2002. The in-
teractive workspaces project: Experiences with ubiq-
uitous computing rooms. IEEE Pervasive Computing,
1(2):67?74.
M. Johnston, L. F. D?Haro, M. Levine, and B. Renger.
2007. A multimodal interface for access to content in
the home. In Proc. of ACL, pages 376?383.
J. Mazzola Paluska, H. Pham, U. Saif, C. Terman, and
S. Ward. 2006. Reducing configuration overhead with
goal-oriented programming. In PerCom Workshops,
pages 596?599. IEEE Computer Society.
J. Mazzola Paluska, H. Pham, U. Saif, G. Chau, C. Ter-
man, and S. Ward. 2008. Structured decomposition of
adapative applications. In Proc. of 6th IEEE Confer-
ence on Pervasive Computing and Communications.
I. McGraw and S. Seneff. 2007. Immersive second lan-
guage acquisition in narrow domains: A prototype IS-
LAND dialogue system. In Proc. of the Speech and
Language Technology in Education Workshop.
J. Nichols and B. A. Myers. 2006. Controlling home and
office appliances with smartphones. IEEE Pervasive
Computing, special issue on SmartPhones, 5(3):60?
67, July-Sept.
H.-J. Oh, C.-H. Lee, M.-G. Jang, and Y. K. Lee. 2007.
An intelligent TV interface based on statistical dia-
logue management. IEEE Transactions on Consumer
Electronics, 53(4).
T. Paek, M. Agrawala, S. Basu, S. Drucker, T. Kristjans-
son, R. Logan, K. Toyama, and A. Wilson. 2004. To-
ward universal mobile interaction for shared displays.
In Proc. of Computer Supported Cooperative Work.
J. Polifroni, G. Chung, and S. Seneff. 2003. Towards
the automatic generation of mixed-initiative dialogue
systems from web content. In Proc. EUROSPEECH,
pages 193?196.
T. Portele, S. Goronzy, M. Emele, A. Kellner, S. Torge,
and J. te Vrugt. 2003. SmartKom-Home - an ad-
vanced multi-modal interface to home entertainment.
In Proc. of INTERSPEECH.
S. Seneff, E. Hurley, R. Lau, C. Pao, P. Schmid, and
V. Zue. 1998. GALAXY-II: A reference architecture
for conversational system development. In Proc. IC-
SLP.
S. Seneff, C. Wang, and T. J. Hazen. 2003. Automatic in-
duction of n-gram language models from a natural lan-
guage grammar. In Proceedings of EUROSPEECH.
S. Seneff. 1992. TINA: A natural language system
for spoken language applications. Computational Lin-
guistics, 18(1):61?86.
S. Seneff. 2002. Response planning and generation in
the MERCURY flight reservation system. Computer
Speech and Language, 16:283?312.
S. Seneff. 2007. Reversible sound-to-letter/letter-to-
sound modeling based on syllable structure. In Proc.
of HLT-NAACL.
K. Wittenburg, T. Lanning, D. Schwenke, H. Shubin, and
A. Vetro. 2006. The prospects for unrestricted speech
input for TV content search. In Proc. of AVI?06.
V. Zue, S. Seneff, J. Glass, J. Polifroni, C. Pao, T. J.
Hazen, and L. Hetherington. 2000. JUPITER: A
telephone-based conversational interface for weather
information. IEEE Transactions on Speech and Audio
Processing, 8(1), January.
9
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 193?202, Dublin, Ireland, August 23-29 2014.
A Study of using Syntactic and Semantic Structures
for Concept Segmentation and Labeling
Iman Saleh
?
, Shafiq Joty, Llu??s M
`
arquez,
Alessandro Moschitti, Preslav Nakov
ALT Research Group
Qatar Computing Research Institute
{sjoty,lmarquez,amoschitti,pnakov}
@qf.org.qa
Scott Cyphers, Jim Glass
MIT CSAIL
Cambridge, Massachusetts 02139
USA
{cyphers,glass}@mit.edu
Abstract
This paper presents an empirical study on using syntactic and semantic information for Concept
Segmentation and Labeling (CSL), a well-known component in spoken language understand-
ing. Our approach is based on reranking N -best outputs from a state-of-the-art CSL parser. We
perform extensive experimentation by comparing different tree-based kernels with a variety of
representations of the available linguistic information, including semantic concepts, words, POS
tags, shallow and full syntax, and discourse trees. The results show that the structured representa-
tion with the semantic concepts yields significant improvement over the base CSL parser, much
larger compared to learning with an explicit feature vector representation. We also show that
shallow syntax helps improve the results and that discourse relations can be partially beneficial.
1 Introduction
Spoken Language Understanding aims to interpret user utterances and to convert them to logical forms,
or, equivalently, database queries, which can then be used to satisfy the user?s information needs. This
process is known as Concept Segmentation and Labeling (CSL): it maps utterances into meaning repre-
sentations based on semantic constituents. The latter are basically sequences of semantic entities, often
referred to as concepts, attributes or semantic tags. Traditionally, grammar-based methods have been
used for CSL, but more recently machine learning approaches to semantic structure computation have
been shown to yield higher accuracy. However, most previous work did not exploit syntactic/semantic
structures of the utterances, and the state-of-the-art is represented by conditional models for sequence la-
beling, such as Conditional Random Fields (Lafferty et al., 2001) trained with simple morphological and
lexical features. In our study, we measure the impact of syntactic and discourse structures by also com-
bining them with innovative features. In the following subsections, we present the application context
for our CSL task and then we outline the challenges and the findings of our research.
1.1 Semantic parsing for the ?restaurant? domain
We experiment with the dataset of McGraw et al. (2012), containing spoken and typed questions about
restaurants, which are to be answered using a database of free text such as reviews, categorical data such
as names and locations, and semi-categorical data such as user-reported cuisines and amenities.
Semantic parsing, in the form of sequential segmentation and labeling, makes it easy to convert spoken
and typed questions such as ?cheap lebanese restaurants in doha with take out? into database queries.
First, a language-specific semantic parser tokenizes, segments and labels the question:
[
Price
cheap] [
Cuisine
lebanese] [
Other
restaurants in] [
City
doha] [
Other
with] [
Amenity
take out]
Then, label-specific normalizers are applied to the segments, with the option to possibly relabel mis-
labeled segments; at this point, discourse history may be incorporated as well.
[
Price
low] [
Cuisine
lebanese] [
City
doha] [
Amenity
carry out]
?
Iman Saleh (iman.saleh@fci-cu.edu.eg) is affiliated to Faculty of Computers and Information, Cairo University.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
193
Finally, a database query is formed from the list of labels and values, and is then executed against the
database, e.g., MongoDB; a backoff mechanism may be used if the query does not succeed.
{$and [{cuisine:"lebanese"}, {city:"doha"}, {price:"low"}, {amenity:"carry out"}]}
1.2 Related work on CSL
Pieraccini et al. (1991) used Hidden Markov Models (HMMs) for CSL, where the observations were
word sequences and the hidden states were meaning units, i.e, concepts. In subsequent work (Rubinstein
and Hastie, 1997; Santaf?e et al., 2007; Raymond and Riccardi, 2007; De Mori et al., 2008), other genera-
tive models were applied, which model the joint probability of a word sequence and a concept sequence,
as well as discriminative models, which directly model a conditional probability over the concepts in the
input text.
Seneff (1989) and Miller et al. (1994) used stochastic grammars for CSL. In particular, they applied
stochastic Finite State Transducers (FST) for recognizing constituent annotations. FSTs describe local
syntactic structures with a sequence of words, e.g., noun phrases or even constituents. Papineni et al.
(1998) proposed and evaluated exponential models, but, nowadays, Conditional Random Fields (Lafferty
et al., 2001) are considered to be the state-of-the-art. More recently, Wang et al. (2009) illustrated an
approach for CSL that is specific to query understanding for web applications. A general survey of CSL
approaches can be found in (De Mori et al., 2008). CSL is also connected to a large body of work on
shallow semantic parsing; see (Gildea and Jurafsky, 2002; M`arquez et al., 2008) for an overview.
Another relevant line of research with a considerable body of work is reranking in NLP. Tree kernels
for reranking syntactic parse trees were first proposed in (Collins and Duffy, 2002). Some variants used
explicit spaces (Kudo et al., 2005), and feature vector approaches were proposed in (Koo and Collins,
2005). Other reranking work using tree kernels regards predicate argument structures (Moschitti et al.,
2006) and named entities (Nguyen and Moschitti, 2012). In (Dinarelli et al., 2011), we rerank CSL
hypotheses using structures built on top of concepts, words and features that are simpler than those
studied in this paper. The work of Ge and Mooney (2006) and Kate and Mooney (2006) is also similar
to ours, as it models the extraction of semantics as a reranking task using string kernels.
1.3 Syntactic and semantic structures for CSL
The related work has highlighted that automatic CSL is mostly based on powerful machine learning al-
gorithms and simple feature representations based on word and tag n-grams. In this paper, we study the
impact of more advanced linguistic processing on CSL, such as shallow and full syntactic parsing and
discourse structure. We use a reranking approach to select the best hypothesis annotated with concepts
derived by a local model, where the hypotheses are represented as trees enriched with semantic con-
cepts similarly to (Dinarelli et al., 2011). These tree-based structures can capture dependencies between
sentence constituents and concepts. However, extracting features from them is rather difficult as their
number is exponentially large. Thus, we rely on structural kernels (e.g., see (Moschitti, 2006)) for au-
tomatically encoding tree fragments, which represent syntactic and semantic dependencies from words
and concepts, and we train the reranking functions with Support Vector Machines (e.g., see (Joachims,
1999)). Additionally, we experiment with several types of kernels and newly designed feature vectors.
We test our models on the above-mentioned Restaurant domain. The results show that (i) the basic
CRF model, in fact semi-CRF (see below), is very accurate, achieving more than 83% in F
1
-score, which
indicates that improving over the semi-CRF approach is very hard; (ii) the upper-bound performance
of the reranking approach is very high as well, i.e., the correct annotation is generated in the first 100
hypotheses in 98.72% of the cases; (iii) our feature vectors show improvement only when all feature
groups are used together; otherwise, we only observe marginal improvement; (iv) structural kernels yield
a 10% relative error reduction from the semi-CRF baseline, which is more than double the feature vector
result; (v) syntactic information significantly improves on the best model, but only when using shallow
syntax; and finally, (vi) although, discourse structures provide good improvement over the semi-CRF
model, they perform lower than shallow syntax (thus, a valuable use of discourse features is still an open
problem that we plan to pursue in future work).
194
2 CSL reranking
Reranking is based on a list of N annotation hypotheses, which are generated and sorted by probability
using local classifiers. Then a reranker, typically a meta-classifier, tries to select the best hypothesis from
the list. The reranker can exploit global information, and, specifically, the dependencies between the
different concepts that are made available by the local model. We use semi-CRF as our local model since
it yields the highest accuracy in CSL (when using a single model), and preference reranking with kernel
machines to rerank the N hypotheses generated by the semi-CRF.
2.1 Basic parser using semi-CRF
We use a semi-Markov CRF (Sarawagi and Cohen, 2004), or semi-CRF, a variation of a linear-chain
CRF (Lafferty et al., 2001), to produce the N -best list of labeled segment hypotheses that serve as the
input to reranking. In a linear-chain CRF, with a sequence of tokens x and labels y, we approximate
p(y|x) as a product of factors of the form p(y
i
|y
i?1
, x), which corresponds to features of the form
f
j
(y
i?1
, y
i
, i, x), where i iterates over the token/label positions. This supports a Viterbi search for the
approximateN best values of y. WithM label values, if for each label y
m
we know the bestN sequences
of labels y
1
, y
2
, . . . , y
i?1
= y
m
, then we can use p(y
i
|y
i?1
, x) to get the probability for extending each
path by each possible label y
i
= y
?
m
. Then for each label y
?
m
, we will have MN paths and scores, one
from each of the paths of length i? 1 ending with y
m
. For each y
?
m
, we pick the N best extended paths.
With semi-CRF, we want a labeled segmentation s rather than a sequence of labels. Each segment
s
i
= (y
i
, t
i
, u
i
) has a label y
i
as well as a starting and ending token position for the segment, t
i
and
u
i
respectively, where u
i
+ 1 = t
i+1
. We approximate p(s|x), with factors of the form p(s
i
|s
i?1
, x),
which we simplify to p(y
i
, u
i
|y
i?1
, t
i
), so features take the form f
j
(y
i?1
, y
i
, t
i
, u
i
), i.e., they can use the
previous segment?s label and the current segment?s label and endpoints. The Viterbi search is extended
to search for a pair of label and segment end. Whereas for M labels we kept track of MN paths, we
must keep track of MLN paths, where L is the maximum segment length.
We use token n-gram features relative to the segment boundaries, n-grams within the segment, token
regular expression and lexicon features within a segment. Each of these features also includes the labels
of the previous and current segment, and the segment length.
2.2 Preference reranking with kernel machines
Preference reranking (PR) uses a classifier C of pairs of hypotheses ?H
i
, H
j
?, which decides if H
i
is
better thanH
j
. Given each training question Q, positive and negative examples are generated for training
the classifier. We adopt the following approach for example generation: the pairs ?H
1
, H
i
? constitute
positive examples, where H
1
has the lowest error rate with respect to the gold standard among the
hypotheses for Q, and vice versa, ?H
i
, H
1
? are considered as negative examples. At testing time, given
a new question Q
?
, C classifies all pairs ?H
i
, H
j
? generated from the annotation hypotheses of Q
?
: a
positive classification is a vote for H
i
, otherwise the vote is for H
j
. Also, the classifier score can be used
as a weighted vote. H
k
are then ranked according to the number (sum) of the (weighted) votes they get.
We build our reranker with kernel machines. The latter, e.g., SVMs, classify an input object o using
the following function: C(o) =
?
i
?
i
y
i
K(o, o
i
), where ?
i
are model parameters estimated from the
training data, o
i
are support objects and y
i
are the labels of the support objects. K(?, ?) is a kernel
function, which computes the scalar product between the two objects in an implicit vector space. In the
case of the reranker, the objects o are ?H
i
, H
j
?, and the kernel is defined as follow:
K(?H
1
, H
2
?, ?H
?
1
, H
?
2
?) = S(H
1
, H
?
1
) + S(H
2
, H
?
2
)? S(H
1
, H
?
2
)? S(H
2
, H
?
1
).
Our reranker also includes traditional feature vectors in addition to the trees. Therefore, we define each
hypothesis H as a tuple ?T,~v? composed of a tree T and a feature vector ~v. We then define a structural
kernel (similarity) between two hypotheses H and H
?
as follows: S(H,H
?
) = S
TK
(T, T
?
) + S
v
(~v,~v
?
),
where S
TK
is one of the tree kernel functions defined in Section 3.1, and S
v
is a kernel over feature
vectors (see Section 3.3), e.g., linear, polynomial, gaussian, etc.
195
(a) Basic Tree (BT). (b) Discourse Tree (DT).
(c) Shallow Syntactic Tree (ShT).
(d) Syntactic Tree (ST).
(e) BT with POS (BTP).
Figure 1: Syntactic/semantic trees. The numeric semantic tagset is defined in Table 7.
3 Structural kernels for semantic parsing
In this section, we briefly describe the kernels we use in S(H,H
?
) for preference reranking. We engineer
them by combining three aspects: (i) different types of existing tree kernels, (ii) new syntactic/semantic
structures for representing CSL, and (iii) new feature vectors.
3.1 Tree kernels
Structural kernels, e.g., tree and sequence kernels, measure the similarity between two structures in terms
of their shared substructures. One interesting aspect is that these kernels correspond to a scalar product
in the fragment space, where each substructure is a feature. Therefore, they can be used in the training
and testing algorithms of kernel machines (see Section 2.2). Below, we briefly describe different types of
kernels we tested in our study, which are made available in the SVM-Light-TK toolkit (Moschitti, 2006).
Subtree Kernel (K0) is one of the simplest tree kernels, as it only generates complete subtrees, i.e., tree
fragments that, given any arbitrary starting node, necessarily include all its descendants.
Syntactic Tree Kernel (K1), also known as a subset tree kernel (Collins and Duffy, 2002), maps ob-
jects in the space of all possible tree fragments constrained by the rule that the sibling nodes cannot
be separated from their parents. In other words, substructures are composed of atomic building blocks
corresponding to nodes, along with all of their direct children. In the case of a syntactic parse tree, these
are complete production rules for the associated parser grammar.
Syntactic Tree Kernel + BOW (K2) extends ST by allowing leaf nodes to be part of the feature space.
The leaves of the trees correspond to words, i.e., we allow bag-of-words (BOW).
Partial Tree Kernel (K3) can be effectively applied to both constituency and dependency parse trees.
It generates all possible connected tree fragments, e.g., sibling nodes can be also separated and be part
of different tree fragments. In other words, a fragment is any possible tree path from whose nodes other
tree paths can depart. Thus, it can generate a very rich feature space.
Sequence Kernel (K4) is the traditional string kernel applied to the words of a sentence. In our case, we
apply it to the sequence of concepts.
3.2 Semantic/syntactic structures
As mentioned before, tree kernels allow us to compute structural similarities between two trees without
explicitly representing them as feature vectors. For the CSL task, we experimented with a number of tree
representations that incorporate different levels of syntactic and semantic information.
To capture the structural dependencies between the semantic tags, we use a basic tree (Figure 1a)
where the words of a sentence are tagged with their semantic tags. More specifically, the words in the
sentence constitute the leaves of the tree, which are in turn connected to the pre-terminals containing the
semantic tags in BIO notation (?B?=begin, ?I?=inside, ?O?=outside). The BIO tags are then generalized
in the upper level, and so on. The basic tree does not include any syntactic information.
196
However, part-of-speech (POS) and phrasal information could be informative for both segmentation
and labeling in semantic parsing. To incorporate this information, we use two extensions of the basic
tree: one that includes the POS tags of the words (Figure 1e), and another one that includes both POS
tags and syntactic chunks (Figure 1c). The POS tags are children of the semantic tags, whereas the
chunks (i.e., phrasal information) are included as parents of the semantic tags.
We also experiment with full syntactic trees (Figure 1d) to see the impact of deep syntactic informa-
tion. The semantic tags are attached to the pre-terminals (i.e., POS tags) in the syntactic tree. We use the
Stanford POS tagger and syntactic parser and the Twitter NLP tool
1
for the shallow trees.
A sentence containing multiple clauses exhibits a coherence structure. For instance, in our example,
the first clause ?along my route tell me the next steak house? is elaborated by the second clause ?that is
within a mile?. The relations by which clauses in a text are linked are called coherence relations (e.g.,
Elaboration, Contrast). Discourse structures capture this coherence structure of text and provide addi-
tional semantic information that could be useful for the CSL task (Stede, 2011). To build the discourse
structure of a sentence, we use a state-of-the-art discourse parser (Joty et al., 2012) which generates
discourse trees in accordance with the Rhetorical Structure Theory of discourse (Mann and Thompson,
1988), as exemplified in Figure 1b. Notice that a text span linked by a coherence relation can be either a
nucleus (i.e., the core part) or a satellite (i.e., a supportive one) depending on how central the claim is.
3.3 New features
In order to compare to the structured representation, we also devoted significant effort towards engineer-
ing a set of features to be used in a flat feature-vector representation; they can be used in isolation or in
combination with the kernel-based approach (as a composite kernel using a linear combination):
CRF-based: these include the basic features used to train the initial semi-CRF model (cf. Section 2.1).
n-gram based: we collected 3- and 4-grams of the output label sequence at the level of concepts, with
artificial tags inserted to identify the start (?S?) and end (?E?) of the sequence.
2
Probability-based: two features computing the probability of the label sequence as an average of the
probabilities at the word level p(l
i
|w
i
) (i.e., assuming independence between words). The unigram prob-
abilities are estimated by frequency counts using maximum likelihood in two ways: (i) from the complete
100-best list of hypotheses; (ii) from the training set (according to the gold standard annotation).
DB-based: a single feature encoding the number of results returned from the database when constructing
a query using the conjunction of all semantic segments in the hypothesis. Three possible values are
considered by using a threshold t: 0 (if the query result is void), 1 (if the number of results is in [1, t]),
and 2 (if the number of results is greater than t). In our case, t is empirically set to 10,000.
4 Experiments
The experiments aim at investigating which structures, and thus which linguistic models and combination
with other models, are the most appropriate for our reranker. We first calculate the oracle accuracy in
order to compute an upper bound of the reranker. Then we present experiments with the feature vectors,
tree kernels, and representations of linguistic information introduced in the previous sections.
4.1 Experimental setup
In our experiments, we use questions annotated with semantic tags in the restaurant domain,
3
which were
collected by McGraw et al. (2012) through crowdsourcing on Amazon Mechanical Turk.
4
We split the
dataset into training, development and test sets. Table 1 shows statistics about the dataset and about the
size of the parts we used for training, development and testing (see the semi-CRF line).
We subsequently split the training data randomly into ten folds. We generated the N -best lists on
the training set in a cross-validation fashion, i.e., iteratively training on nine folds and annotating the
remaining fold. We computed the 100-best hypotheses for each example.
1
Available from http://nlp.stanford.edu/software/index.shtml and https://github.com/aritter/twitter nlp, respectively.
2
For instance, if the output sequence is Other-Rating-Other-Amenity the 3-gram patterns would be: S-Other-Rating, Other-
Rating-Other, Rating-Other-Amenity, and Other-Amenity-E.
3
http://www.sls.csail.mit.edu/downloads/restaurant
4
We could not use the datasets used by Dinarelli et al. (2011), because they use French and Italian corpora for which there
are no reliable syntactic and discourse parsers.
197
Train Devel. Test Total
semi-CRF 6,922 739 1,521 9,182
Reranker 28,482 3,695 7,605 39,782
Table 1: Number of instances and pairs used to
train the semi-CRF and rerankers, respectively.
N 1 2 5 10 100
F
1
83.03 87.76 92.63 95.23 98.72
Table 2: Oracle F
1
-score for N -best lists
of different lengths.
We used the development set to experiment and tune the hyper-parameters of the reranking model. The
results on the development set presented in Section 4.2 were obtained by semi-CRF and reranking models
learned on the training set. The results on the test set were obtained by models trained on the training
plus development sets. Similarly, the N -best lists for the development and test sets were generated using
a single semi-CRF model trained on the training set and the training+development sets, respectively.
Each generated hypothesis is represented using a semantic tree and a feature vector (explained in
Section 3) and two extra features accounting for (i) the semi-CRF probability of the hypothesis, and
(ii) the hypothesis reciprocal rank in the N -best list. SVM-Light-TK
5
is used to train the reranker with
a combination of tree kernels and feature vectors (Moschitti, 2006; Joachims, 1999). Although we
tried several parameters on the validation set, we observed that the default values yielded the highest
results. Thus, we used the default c (trade-off) and tree kernel parameters and a linear kernel for the
feature vectors. Table 1 shows the sizes of the train, the development and the test sets used for the
semi-CRF as well as the number of pairs generated for the reranker. As a baseline, we picked the best-
scored hypothesis in the list, according to the semi-CRF tagger. The evaluation measure used in all
the experiments is the harmonic mean of precision and recall, i.e., the F
1
-score (van Rijsbergen, 1979),
computed at the token level and micro-averaged over the different semantic types.
6
We used paired t-test
to measure the statistical significance of the improvements: we split the test set into 31 equally-sized
samples and performed t-tests based on the F
1
-scores of different models on the resulting samples.
4.2 Results
Oracle accuracy. Table 2 shows the oracle F
1
-score for N -best lists of different lengths, i.e., which
can be achieved by picking the best candidate of the N -best list for various values of N . We can see that
going to 5-best increases the oracle F
1
-score by almost ten points absolute. Going down to 10-best only
adds 2.5 extra F
1
points absolute, and a 100-best list adds 3.5 F
1
points more to yield a respectable F
1
-
score of 98.72. This high result can be explained considering that the size of the complete hypothesis set
is smaller than 100 for most questions. Thus, we can conclude that theN -best lists do include many good
options and do offer quite a large space for potential improvement. We can further observe that going to
5-best lists offers a good balance between the length of the list and the possibility to improve F
1
-score:
generally, we do not want too long N -best lists since they slow down computation and also introduce
more opportunities to make the wrong choice for a reranker (since there are just more candidates to
choose from). In our experiments with larger N , we observed improvements only for 10 and only on the
development set; thus, we will focus on 5-best lists in our experiments below.
K0 K1 K2 K3 K4
Dev 84.21 82.92 83.07 85.07 83.78
Test 84.08 83.19 83.20 84.61 82.93
Table 3: Results for using different tree kernels on the basic tree (BT) representation.
Choosing the best tree kernel. We first select the most appropriate tree kernel to limit the number
of experiment variables. Table 3 shows the results of different tree kernels using the basic tree (BT)
representation (see Figure 1a). We can observe that for both the development set and the test set, kernel
K3 (see Section 3.1) yields the highest F
1
-score.
Impact of feature vectors. Table 4 presents the results for the feature vector experiments in terms
of F
1
-scores and relative error reductions (row RER). The first column shows the baseline, when no
reranking is used; the following four columns contain the results when using vectors including different
5
http://disi.unitn.it/moschitti/Tree-Kernel.htm
6
?Other? is not considered a semantic type, thus ?Other? tokens are not included in the F
1
calculation.
198
Baseline n-grams CRF features Count DB ProbBased AllFeat
Dev 83.86 83.79 83.96 83.80 83.86 83.87 84.49
RER -0.4 0.6 -0.4 0.0 0.0 3.9
Test 83.03 82.90 83.44 82.90 83.01 83.09 83.86
RER -0.7 2.4 -0.7 -0.1 0.3 4.8
Table 4: Feature vector experiments: F
1
score and relative error reduction (in %).
Combining AllFeat and
Baseline BT BTP ShT ST AllFeat +BT +ShT +ShT +BT
Dev 83.86 85.07 85.41 85.06 84.30 84.49 85.57 85.58 85.33
RER 7.5 9.6 7.4 2.8 3.9 10.6 10.7 9.1
Test 83.03 84.61 84.63 84.07 83.81 83.86 84.67 84.79 84.76
RER 9.3 9.4 6.1 4.5 4.8 9.6 10.2 10.2
p.v. 0.00049 0.0002 0.012 0.032 0.00018 0.00028 0.00004 0.000023
Table 5: Tree kernel experiments: F
1
-score, relative error reduction (in %), and p-values.
kinds of features: (i) n-gram features, (ii) all features used by the semi-CRF, (iii) count features, and
(iv) database (DB) features. In each case, we include two additional features: the semi-CRF score
(i.e., the probability) and the reciprocal rank of the hypothesis in the N -best list. Among (i)?(iv), only
the semi-CRF features seem to help; the rest either show no improvements or degrade the performance.
However, putting all these features together (AllFeat) yields sizable gains in terms of F
1
-score and a
relative error reduction of 4-5%; the improvement is statistically significant, and it is slightly larger on
the test dataset compared to the development dataset.
Impact of structural kernels and combinations. Table 5 shows the results when experimenting with
various tree structures (see columns 2-5): (i) the basic tree (BT), (ii) the basic tree augmented with
part-of-speech information (BTP), (iii) shallow syntactic tree (ShT), and (iv) syntactic tree (ST). We
can see that the basic tree works rather well, yielding +1.6 F
1
-score on the test dataset, but adding POS
information can help a bit more, especially for the tuning dataset. Interestingly, the syntactic tree kernels,
ShT and ST, perform worse than BT and BTP, especially on the test dataset. The last three columns in the
table show the results when we combine the AllFeat feature vector (see Table 4) with BT and ShT. We can
see that combining AllFeat with ShT works better, on both development and test sets, than combining it
with BT or with both ShT and BT. Also note the big jump in performance from AllFeat to AllFeat+ShT.
Overall, we can conclude that shallow syntax has a lot to offer over AllFeat, and it is preferable over BT
in the combination with AllFeat. The improvements reported in Tables 5 and 6 are statistically significant
when compared to the semi-CRF baseline as shown by the p.v. (value) row. Moreover, the improvement
of AllFeat + ShT over BT is also statistically significant (p.v.<0.05).
Combining AllFeat and
Baseline DS +DS +DS +BT +DS +ShT
Dev 83.86 84.61 85.14 85.43 85.46
RER 4.7 7.9 9.7 9.9
Test 83.03 84.38 84.55 84.63 84.67
RER 7.9 8.9 9.4 9.6
p.v. 0.0005 0.0001 0.00066 0.00015
Table 6: Experiments with discourse kernels: F
1
score, relative error reduction (in %), and p-values.
Discourse structure. Finally, Table 6 shows the results for the discourse tree kernel (DS), which we
designed and experimented with for the first time in this paper. We see that DS yields sizable improve-
ments over the baseline. We also see that further gains can be achieved by combining DS with AllFeat,
and also with BT and ShT, the best combination being AllFeat+DS+ShT (see last column). However,
comparing to Table 5, we see that it is better to use just AllFeat+ShT and leave DS out. We would like
to note though that the discourse parser produced non-trivial trees for only 30% of the hypotheses (due
to the short, simple nature of the questions); in the remaining cases, it probably hurt rather than helped.
We conclude that discourse structure has clear potential, but how to make best use of it, especially in the
case of short simple questions, remains an open question that deserves further investigation.
199
Tag ID Other Rating Restaurant Amenity Cuisine Dish Hours Location Price
0 Other 8260 35 43 110 15 19 55 113 9
1 Rating 29 266 0 14 3 6 0 0 8
2 Restaurant 72 6 657 20 19 15 0 5 0
3 Amenity 117 9 10 841 27 27 7 12 7
4 Cuisine 36 2 12 26 543 44 3 1 0
5 Dish 23 0 4 20 33 324 1 4 0
6 Hours 61 0 1 2 6 1 426 9 1
7 Location 104 1 14 20 2 1 1 1457 0
8 Price 22 1 0 7 0 2 0 1 204
Table 7: Confusion matrix for the output of the best performing system.
4.3 Error analysis and discussion
Table 7 shows the confusion matrix for our best-performing model AllFeat+ShT (rows = gold standard
tags; columns = system predicted tags). Given the good results of the semantic parser, the numbers in the
diagonal are clearly dominating the weight of the matrix. The largest errors correspond to missed (first
column) and over-generated (first row) entity tokens. Among the proper confusions between semantic
types, Dish and Cuisine tend to mislead each other most. This is due to the fact that these two tags
are semantically similar, thus making them hard to distinguish. We can also notice that it is difficult to
identify Amenity correctly, and the model mistakenly tags many other tags as Amenity. We looked into
some examples to further investigate the errors. Our findings are as follow:
Inaccuracies and inconsistencies in human annotations. Since the annotations were done in Me-
chanical Turk, they have many inaccuracies and inconsistencies. For example, the word good with
exactly the same sense was tagged as both Other and Rating by the Turkers in the following examples:
Gold: [
Other
any good] [
Price
cheap] [
Cuisine
german] [
Other
restaurants] [
Location
nearby]
Model: [
Other
any] [
Rating
good] [
Price
cheap] [
Cuisine
german] [
Other
restaurants] [
Location
nearby]
Gold: [
Other
any place] [
Location
along the road] [
Other
has a] [
Rating
good] [
Dish
beer] [
Other
selection that also serves] ...
Requires lexical semantics and more coverage. In some cases our model fails to generalize well. For
instance, it fails to correctly tag establishments and tameles for the following examples. This suggests
that we need to consider other forms of semantic information, e.g., distributional and compositional
semantics computed from large corpora and/or using Web resources such as Wikipedia.
Gold: [
Other
any] [
Location
dancing establishments] [
Other
with] [
Price
reasonable] [
Other
pricing]
Model: [
Other
any] [
Amenity
dancing] [
Other
establishments] [
Other
with] [
Price
reasonable] [
Other
pricing]
Gold: [
Other
any] [
Cuisine
mexican] [
Other
places have a] [
Dish
tameles] [
Amenity
special today]
Model: [
Other
any] [
Cuisine
mexican] [
Other
places have a] [
Amenity
tameles] [
Other
special] [
Hours
today]
5 Conclusions
We have presented a study on the usage of syntactic and semantic structured information for improved
Concept Segmentation and Labeling (CSL). Our approach is based on reranking a set of N -best se-
quences generated by a state-of-the-art semi-CRF model for CSL. The syntactic and semantic informa-
tion was encoded in tree-based structures, which we used to train a reranker with kernel-based Support
Vector Machines. We empirically compared several variants of syntactic/semantic structured representa-
tions and kernels, including also a vector of manually engineered features.
The first and foremost conclusion from our study is that structural kernels yield significant improve-
ment over the strong baseline system, with a relative error reduction of ?10%. This more than doubles
the improvement when using the explicit feature vector. Second, we observed that shallow syntactic
information also improves results significantly over the best model. Unfortunately, the results obtained
using full syntax and discourse trees are not so clear. This is probably explained by the fact that user
queries are rather short and linguistically not very complex. We also observed that the upper bound per-
formance for the reranker still leaves large room for improvement. Thus, it remains to be seen whether
some alternative kernel representations can be devised to make better use of discourse and other syntac-
tic/semantic information. Also, we think that some innovative features based on analyzing the results
obtained from our database (or the Web) when querying with the segments represented in each hypothe-
ses have the potential to improve the results. All these concerns will be addressed in future work.
200
Acknowledgments
This research is developed by the Arabic Language Technologies (ALT) group at Qatar Computing Re-
search Institute (QCRI) within the Qatar Foundation in collaboration with MIT. It is part of the Interactive
sYstems for Answer Search (Iyas) project.
References
Michael Collins and Nigel Duffy. 2002. New ranking algorithms for parsing and tagging: Kernels over discrete
structures, and the voted perceptron. In Proceedings of the 40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 263?270, Philadelphia, PA, USA.
Renato De Mori, Dilek Hakkani-T?ur, Michael McTear, Giuseppe Riccardi, and Gokhan Tur. 2008. Spoken
language understanding: a survey. IEEE Signal Processing Magazine, 25:50?58.
Marco Dinarelli, Alessandro Moschitti, and Giuseppe Riccardi. 2011. Discriminative reranking for spoken lan-
guage understanding. IEEE Transactions on Audio, Speech and Language Processing, 20(2):526?539.
Ruifang Ge and Raymond Mooney. 2006. Discriminative reranking for semantic parsing. In Proceedings of the
21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association
for Computational Linguistics, COLING-ACL?06, pages 263?270, Sydney, Australia.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics,
28(3):245?288.
Thorsten Joachims. 1999. Advances in kernel methods. In Bernhard Sch?olkopf, Christopher J. C. Burges, and
Alexander J. Smola, editors, Making Large-scale Support Vector Machine Learning Practical, pages 169?184,
Cambridge, MA, USA. MIT Press.
Shafiq Joty, Giuseppe Carenini, and Raymond Ng. 2012. A novel discriminative framework for sentence-level dis-
course analysis. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing
and Computational Natural Language Learning, EMNLP-CoNLL ?12, pages 904?915, Jeju Island, Korea.
Rohit Kate and Raymond Mooney. 2006. Using string-kernels for learning semantic parsers. In Proceedings of
the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association
for Computational Linguistics, COLING-ACL ?06, pages 913?920, Sydney, Australia.
Terry Koo and Michael Collins. 2005. Hidden-variable models for discriminative reranking. In Proceedings
of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing,
HLT-EMNLP ?05, pages 507?514, Vancouver, British Columbia, Canada.
Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005. Boosting-based parse reranking with subtree features. In
Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ?05, pages 189?
196, Ann Arbor, MI, USA.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proceedings of the 18th International Conference on Machine
Learning, ICML ?01, pages 282?289, Williamstown, MA, USA.
William Mann and Sandra Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text
organization. Text, 8(3):243?281.
Llu??s M`arquez, Xavier Carreras, Kenneth Litkowski, and Suzanne Stevenson. 2008. Semantic Role Labeling: An
Introduction to the Special Issue. Computational Linguistics, 34(2):145?159.
Ian McGraw, Scott Cyphers, Panupong Pasupat, Jingjing Liu, and Jim Glass. 2012. Automating crowd-supervised
learning for spoken language systems. In Proceedings of 13th Annual Conference of the International Speech
Communication Association, INTERSPEECH ?12, Portland, OR, USA.
Scott Miller, Richard Schwartz, Robert Bobrow, and Robert Ingria. 1994. Statistical language processing using
hidden understanding models. In Proceedings of the workshop on Human Language Technology, HLT ?94,
pages 278?282, Morristown, NJ, USA.
Alessandro Moschitti, Daniele Pighin, and Roberto Basili. 2006. Semantic role labeling via tree kernel joint
inference. In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL-X),
pages 61?68, New York City, June.
201
Alessandro Moschitti. 2006. Efficient convolution kernels for dependency and constituent syntactic trees. In
Proceedings of the 17th European Conference on Machine Learning, ECML?06, pages 318?329, Berlin, Hei-
delberg. Springer-Verlag.
Truc-Vien T. Nguyen and Alessandro Moschitti. 2012. Structural reranking models for named entity recognition.
Intelligenza Artificiale, 6(2):177?190.
Kishore Papineni, Salim Roukos, and Todd Ward. 1998. Maximum likelihood and discriminative training of
direct translation models. In Proceedings of the IEEE International Conference on Acoustics, Speech and
Signal Processing, volume 1, pages 189?192, Seattle, WA, USA.
Roberto Pieraccini, Esther Levin, and Chin-Hui Lee. 1991. Stochastic representation of conceptual structure in
the ATIS task. In Proceedings of the Workshop on Speech and Natural Language, HLT ?91, pages 121?124,
Pacific Grove, CA, USA.
Christian Raymond and Giuseppe Riccardi. 2007. Generative and discriminative algorithms for spoken language
understanding. In Proceedings of 8th Annual Conference of the International Speech Communication Associa-
tion, INTERSPEECH ?07, pages 1605?1608, Antwerp, Belgium.
Yigal Dan Rubinstein and Trevor Hastie. 1997. Discriminative vs informative learning. In Proceedings of the
Third International Conference on Knowledge Discovery and Data Mining, KDD ?97, pages 49?53, Newport
Beach, CA, USA.
Guzm?an Santaf?e, Jose Lozano, and Pedro Larra?naga. 2007. Discriminative vs. generative learning of Bayesian
network classifiers. Lecture Notes in Computer Science, 4724:453?464.
Sunita Sarawagi and William Cohen. 2004. Semi-Markov conditional random fields for information extraction.
In Proceedings of the 18th Annual Conference on Neural Information Processing Systems, NIPS ?04, pages
1185?1192, Vancouver, British Columbia, Canada.
Stephanie Seneff. 1989. TINA: A probabilistic syntactic parser for speech understanding systems. In Proceedings
of the Workshop on Speech and Natural Language, HLT ?89, pages 168?178, Philadelphia, PA, USA.
Manfred Stede. 2011. Discourse Processing. Synthesis Lectures on Human Language Technologies. Morgan and
Claypool Publishers.
Cornelis Joost van Rijsbergen. 1979. Information Retrieval. Butterworth.
Ye-Yi Wang, Raphael Hoffmann, Xiao Li, and Jakub Szymanski. 2009. Semi-supervised learning of semantic
classes for query understanding: from the web and for the web. In Proceedings of the 18th ACM Conference on
Information and Knowledge Management, CIKM ?09, pages 37?46, New York, NY, USA.
202
