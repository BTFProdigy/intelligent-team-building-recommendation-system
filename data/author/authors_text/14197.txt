Proceedings of NAACL-HLT 2013, pages 391?400,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Parser lexicalisation through self-learning
Marek Rei
Computer Labratory
University of Cambridge
United Kingdom
Marek.Rei@cl.cam.ac.uk
Ted Briscoe
Computer Laboratory
University of Cambridge
United Kingdom
Ted.Briscoe@cl.cam.ac.uk
Abstract
We describe a new self-learning framework
for parser lexicalisation that requires only a
plain-text corpus of in-domain text. The
method first creates augmented versions of de-
pendency graphs by applying a series of mod-
ifications designed to directly capture higher-
order lexical path dependencies. Scores are
assigned to each edge in the graph using statis-
tics from an automatically parsed background
corpus. As bilexical dependencies are sparse,
a novel directed distributional word similar-
ity measure is used to smooth edge score es-
timates. Edge scores are then combined into
graph scores and used for reranking the top-
n analyses found by the unlexicalised parser.
The approach achieves significant improve-
ments on WSJ and biomedical text over the
unlexicalised baseline parser, which is origi-
nally trained on a subset of the Brown corpus.
1 Introduction
Most parsers exploit supervised machine learning
methods and a syntactically annotated dataset (i.e.
treebank), incorporating a wide range of features in
the training process to deliver competitive perfor-
mance. The use of lexically-conditioned features,
such as relations between lemmas or word forms,
is often critical when choosing the correct syntac-
tic analysis in ambiguous contexts. However, util-
ising such features leads the parser to learn infor-
mation that is often specific to the domain and/or
genre of the training data. Several experiments have
demonstrated that many lexical features learnt in
one domain provide little if any benefit when pars-
ing text from different domains and genres (Sekine,
1997; Gildea, 2001). Furthermore, manual creation
of in-domain treebanks is an expensive and time-
consuming process, which can only be performed by
experts with sufficient linguistic and domain knowl-
edge.
In contrast, unlexicalised parsers avoid using lex-
ical information and select a syntactic analysis us-
ing only more general features, such as POS tags.
While they cannot be expected to achieve optimal
performance when trained and tested in a single do-
main, unlexicalised parsers can be surprisingly com-
petitive with their lexicalised counterparts (Klein
and Manning, 2003; Petrov et al, 2006). In this
work, instead of trying to adapt a lexicalised parser
to new domains, we explore how bilexical features
can be integrated effectively with any unlexicalised
parser. As our novel self-learning framework re-
quires only a large unannotated corpus, lexical fea-
tures can be easily tuned to a specific domain or
genre by selecting a suitable dataset. In addition,
we describe a graph expansion process that captures
selected bilexical relations which improve perfor-
mance but would otherwise require sparse higher-
order dependency path feature types in most ap-
proaches to dependency parsing. As many bilex-
ical features will still be sparse, we also develop
an approach to estimating confidence scores for de-
pendency relations using a directional distributional
word similarity measure. The final framework in-
tegrates easily with any unlexicalised (and therefore
potentially less domain/genre-biased) parser capable
of returning ranked dependency analyses.
391
2 Background
We hypothesise that a large corpus will often contain
examples of dependency relations in non-ambiguous
contexts, and these will mostly be correctly parsed
by an unlexicalised parser. Lexical statistics derived
from the corpus can then be used to select the cor-
rect parse in a more difficult context. For example,
consider the following sentences:
(1) a. Government projects interest researchers
b. Government raises interest rates
c. Government projects receive funding
d. Interest rates are increasing
Noun-verb ambiguities over projects and interest
might erroneously result in the unlexicalised parser
returning similar dependency graphs for both a and
b. However, sentences c and d contain less ambigu-
ous instances of the same phrases and can provide
clues to correctly parsing the first two examples. In
a large in-domain corpus we are likely to find more
cases of researchers being the object for interest and
fewer cases where it is the object of project. In con-
trast, rates is more likely to have interest as a mod-
ifier than as a head in an object relation. Exploiting
this lexical information, we can assign the correct
derivation to each of the more ambiguous sentences.
Similar intuitions have been used to motivate the
acquisition of bilexical features from background
corpora for improving parser accuracy. However,
previous work has focused on including these statis-
tics as auxiliary features during supervised training.
For example, van Noord (2007) incorporated bilex-
ical preferences as features via self-training to im-
prove the Alpino parser for Dutch. Plank and van
Noord (2008) investigated the application of aux-
iliary distributions for domain adaptation. They
incorporated information from both in-domain and
out-of-domain sources into their maximum entropy
model and found that the out-of-domain auxiliary
distributions did not contribute to parsing accuracy
in the target domain. Zhou et al (2011) extracted n-
gram counts from Google queries and a large corpus
to improve the MSTParser. In contrast to previous
work, we refer to our approach as self-learning be-
cause it differs from self-training by utilising statis-
tics found using an initial parse ranking model to
create a separate unsupervised reranking compo-
nent, without retraining the baseline unlexicalised
model.
We formulate our self-learning framework as a
reranking process that assigns new scores to the top-
n ranked analyses found by the original parser. Parse
reranking has been successfully used in previous
work as a method of including a wider range of fea-
tures to rescore a smaller selection of highly-ranked
candidate parses. Collins (2000) was one of the first
to propose supervised reranking as an additional step
to increase parser accuracy and achieved 1.55% ac-
curacy improvement for his parser. Charniak and
Johnson (2005) utilise a discriminative reranker and
show a 1.3% improvement for the Charniak parser.
McClosky et al (2006) extend their work by adding
new features and further increase the performance
by 0.3%. Ng et al (2010) implemented a dis-
criminative maximum entropy reranker for the C&C
parser and showed a 0.23% improvement over the
baseline. Bansal and Klein (2011) discriminatively
rerank derivations from the Berkeley unlexicalised
parser (Petrov et al, 2006) demonstrating that lex-
ical features derived from the Google n-gram cor-
pus improve accuracy even when used in conjunc-
tion with other reranking features. They have all
treated reranking as a supervised task and trained a
discriminative classifier using parse tree features and
annotated in-domain data. In contrast, our reranker
only uses statistics from an unlabelled source and
requires no manual annotation or training of the
reranking component. As we utilise an unlexicalised
parser, our baseline performance on WSJ text is
lower compared to some fully-lexicalised parsers.
However, an unlexicalised parser is also likely to be
less biased to domains or genres manifested in the
text used to train its original ranking model. This
may allow the reranker to adapt it to a new domain
and/or genre more effectively.
3 Reordering dependency graphs
For our experiments, we make use of the unlexi-
calised RASP parser (Briscoe et al, 2006) as the
baseline system. For every sentence s the parser
returns a list of dependency graphs Gs, ranked by
the log probability of the associated derivation in the
structural ranking model. Our goal is to reorder this
392
list to improve ranking accuracy and, most impor-
tantly, to improve the quality of the highest-ranked
dependency graph. This is done by assigning a con-
fidence score to every graph gs,r ? Gs where r is the
rank of gs for sentence s. The method treats each
sentence independently, therefore we can omit the
sentence identifiers and refer to gs,r as gr.
We first calculate confidence scores for all the in-
dividual edges and then combine them into an over-
all score for the dependency graph. In the following
sections, we describe a series of graph modifications
that incorporates selected higher-order dependency
path relations, without introducing unwanted noise
or complexity into the reranker. Next, we outline
different approaches for calculating and smoothing
the confidence scores for bilexical relations. Finally,
we describe methods for combining together these
scores and calculating an overall score for a depen-
dency graph. We make publically available all the
code developed for performing these steps in the
parse reranking system.1
3.1 Graph modifications
For every dependency graph gr the graph expan-
sion procedure creates a modified representation g?r
which contains a wider range of bilexical relations.
The motivation for this graph expansion step is sim-
ilar to that motivating the move from first-order to
higher-order dependency path feature types (e.g.,
Carreras (2007)). However, compared to using all
nth-order paths, these rules are chosen to maximise
the utility and minimise the sparsity of the result-
ing bilexical features. In addition, the cascading na-
ture of the expansion steps means in some cases the
expansion captures useful 3rd and 4th order depen-
dencies. Similar approaches to graph modifications
have been successfully used for several NLP tasks
(van Noord, 2007; Arora et al, 2010).
For any edge e we also use notation (rel, w1, w2),
referring to an edge from w1 to w2 with the label
rel. We perform the following modifications on ev-
ery dependency graph:
1. Normalising lemmas. All lemmas are converted
to lowercase. Numerical lemmas are replaced
with more generic tags to reduce sparsity.
1www.marekrei.com/projects/lexicalisation
2. Bypassing conjunctions. For every edge pair
(rel1, w1, w2) and (rel2, w2, w3) where w2 is
tagged as a conjunction, we create an additional
edge (rel1, w1, w3). This bypasses the conjunc-
tion node and creates direct edges between the
head and dependents of the conjunctive lemma.
3. Bypassing prepositions. For every edge pair
(rel1, w1, w2) and (rel2, w2, w3) where w2 is
tagged as a preposition, we create an additional
edge (rel3, w1, w3). rel3 = rel1 +? prep?, where
? prep? is added as a marker to indicate that the
relation originally contained a preposition.
4. Bypassing verbs. For every edge pair
(rel1, w1, w2) and (rel2, w1, w3) where w1 is
tagged as a verb, w2 and w3 are both tagged
as open-class lemmas, rel1 starts with a subject
relation, and rel2 starts with an object relation,
we create an additional edge (rel3, w2, w3) where
rel3 = rel1 + ?-? + rel2. This creates an additional
edge between the subject and the object, with the
new edge label containing both of the original la-
bels.
5. Duplicating nodes. For every existing node in
the graph, containing the lemma and POS for
each token (lemma pos), we create a parallel node
without the POS information (lemma). Then, for
each existing edge, we create three correspond-
ing edges, interconnecting the parallel nodes to
each other and the original graph. This allows the
reranker to exploit both specific and more generic
instantiations of each lemma.
Figure 1 illustrates the graph modification pro-
cess. It is important to note that each of these mod-
ifications gets applied in the order that they are de-
scribed above. For example, when creating edges for
bypassing verbs, the new edges for prepositions and
conjunctions have already been created and also par-
ticipate in this step. We performed ablation tests on
the development data and verified that each of these
modifications contributes positively to the final per-
formance.
3.2 Edge scoring methods
We start the scoring process by assigning individual
confidence scores to every bilexical relation in the
393
italian pm meet with cabinet member and senior official
JJ NP1 VVZ IW NN1 NN2 CC JJ NN2
ncmod ncsubj iobj
dobj
ncmod conj
conj
ncmod
ncsubj-iobj prepncsubj-iobj prep
iobj prepiobj prep
iobj prep
dobjdobj
Figure 1: Modified graph for the sentence ?Italian PM meets with Cabinet members and senior officials? after steps
1-4. Edges above the text are created by the parser, edges below the text are automatically created using the operations
described in Section 3.1. The 5th step will create 9 new nodes and 45 additional edges (not shown).
modified graph. In this section we give an overview
of some possible strategies for performing this task.
The parser returns a ranked list of graphs and this
can be used to derive an edge score without requir-
ing any additional information. We estimate that the
likelihood of a parse being the best possible parse for
a given sentence is roughly inversely proportional
to the rank that it is assigned by the parser. These
values can be summed for all graphs that contain a
specific edge, normalised to approximate a proba-
bility. We then calculate the score for edge e as the
Reciprocal Edge Score (RES) ? the probability of e
belonging to the best possible parse:
RES(e) =
?R
r=1[ 1r ? contains(g?r, e)]
?R
r=1
1
r
whereR is the total number of parses for a sentence,
and contains(g?r, e) returns 1 if graph g?r contains
edge e, and 0 otherwise. The value is normalised,
so that an edge which is found in all parses will have
a score of 1.0, but occurrences at higher ranks will
have a considerably larger contribution.
The score of an edge can also be assigned by es-
timating the probability of that edge using a parsed
reference corpus. van Noord (2007) improved over-
all parsing performance in a supervised self-training
framework using feature weights based on pointwise
mutual information:
I(e) = log P(rel, w1, w2)
P(rel, w1, ?)? P(?, ?, w2)
where P(rel, w1, w2) is the probability of seeing an
edge from w1 to w2 with label rel, P(rel, w1, ?) is
the probability of seeing an edge from w1 to any
node with label rel, and P(?, ?, w2) is the prob-
ability of seeing any type of edge linking to w2.
Plank and van Noord (2008) used the same approach
for semi-supervised domain adaptation but were not
able to achieve similar performance benefits. In our
implementation we omit the logarithm in the equa-
tion, as this improves performance and avoids prob-
lems with log(0) for unseen edges.
I(e) compares the probability of the complete
edge to the probabilities of partially specified edges,
but it assumes that w2 will have an incoming rela-
tion, and that w1 will have an outgoing relation of
type rel to some unknown node. These assumptions
may or may not be true ? given the input sentence,
we have observed w1 and w2 but do not know what
relations they are involved in. Therefore, we create
a more general version of the measure that compares
the probability of the complete edge to the individual
probabilities of the two lemmas ? the Conditional
Edge Score (CES1):
CES1(e) =
P(rel, w1, w2)
P(w1)? P(w2)
where P(w1) is the probability of seeing w1 in text,
estimated from a background corpus using maxi-
mum likelihood.
Finally, we know that w1 and w2 are in a sen-
tence together but cannot assume that there is a de-
pendency relation between them. However, we can
choose to think of each sentence as a fully connected
graph, with an edge going from every lemma to ev-
ery other lemma in the same sentence. If there exists
394
ECES1(rel, w1, w2) =
1
2 ? (
?
c1?C1
sim(c1, w1)? P(rel,c1,w2)P(c1)?P(w2)?
c1?C1
sim(c1, w1)
+
?
c2?C2
sim(c2, w2)? P(rel,w1,c2)P(w1)?P(c2)?
c2?C2
sim(c2, w2)
)
ECES2(rel, w1, w2) =
1
2 ? (
?
c1?C1
sim(c1, w1)? P(rel,c1,w2)P(?,c1,w2)?
c1?C1
sim(c1, w1)
+
?
c2?C2
sim(c2, w2)? P(rel,w1,c2)P(?,w1,c2)?
c2?C2
sim(c2, w2)
)
Figure 2: Expanded edge score calculation methods using the list of distributionally similar lemmas
no genuine relation between the lemmas, the edge is
simply considered a null edge. We can then find the
conditional probability of the relation type given the
two lemmas:
CES2(e) =
P(rel, w1, w2)
P(?, w1, w2)
where P(rel, w1, w2) is the probability of the fully-
specified relation, and P(?, w1, w2) is the probability
of there being an edge of any type fromw1 tow2, in-
cluding a null edge. Using fully connected graphs,
the latter is equivalent to the probability of w1 and
w2 appearing in a sentence together, which again can
be calculated from the background corpus.
3.3 Smoothing edge scores
Apart from RES, all the scoring methods from
the previous section rely on correctly estimat-
ing the probability of the fully-specified edge,
P(rel, w1, w2). Even in a large background corpus
these triples will be very sparse, and it can be useful
to find approximate methods for estimating the edge
scores.
Using smoothing techniques derived from work
on language modelling, we could back-off to a more
general version of the relation. For example, if
(dobj, read, publication) is not frequent enough, the
value could be approximated using the probabilities
of (dobj, read, *) and (dobj, *, publication). How-
ever, this can lead to unexpected results due to com-
positionality ? while (dobj, read, *) and (dobj, *,
rugby) can be fairly common, (dobj, read, rugby) is
an unlikely relation.
Instead, we can consider looking at other lemmas
which are similar to the rare lemmas in the relation.
If (dobj, read, publication) is infrequent in the data,
the system might predict that book is a reasonable
substitute for publication and use (dobj, read, book)
to estimate the original probability.
Given that we have a reliable way of finding likely
substitutes for a given lemma, we can create ex-
panded versions of CES1 and CES2, as shown in
Figure 2. C1 is the list of substitute lemmas for w1,
and sim(c1, w1) is a measure showing how similar
c1 is to w1. The methods iterate over the list of sub-
stitutes and calculate the CES score for each of the
modified relations. The values are then combined by
using the similarity score as a weight ? more similar
lemmas will have a higher contribution to the final
result. This is done for both the head and the depen-
dent in the original relation, and the scores are then
normalised and averaged.
Experiments with a wide variety of distributional
word similarity measures revealed that WeightedCo-
sine (Rei, 2013), a directional similarity measure
designed to better capture hyponymy relations, per-
formed best. Hyponyms are more specific versions
of a word and normally include the general proper-
ties of the hypernym, making them well-suited for
lexical substitution. The WeightedCosine measure
incorporates an additional directional weight into
the standard cosine similarity, assigning different
importance to individual features for the hyponymy
relation. We retain the 10 most distributionally simi-
lar putative hyponyms for each lemma and substitute
them in the relation. The original lemma is also in-
cluded with similarity 1.0, thereby assigning it the
highest weight. The lemma vectors are built from
the same vector space model that is used for cal-
culating edge probabilities, which includes all the
graph modifications described in Section 3.1.
3.4 Combining edge scores
While the CES and ECES measures calculate con-
fidence scores for bilexical relations using statistics
from a large background corpus, they do not include
any knowledge about grammar, syntax, or the con-
395
CMB1(e) = 3
?
RES(e) ? CES1(e) ? CES2(e) CMB2(e) = 3
?
RES(e) ? ECES1(e) ? ECES2(e)
Figure 3: Edge score combination methods
text in a specific sentence. In contrast, the RES score
implicitly includes some of this information, as it is
calculated based on the original parser ranking. In
order to take advantage of both information sources,
we combine these scores into CMB1 and CMB2, as
shown in Figure 3.
3.5 Graph scoring
Every edge in graph g?r is assigned a score indicat-
ing the reranker?s confidence in that edge belonging
to the best parse. We investigated different strate-
gies for combining these values together into a con-
fidence score for the whole graph. The simplest so-
lution is to sum together individual edge scores, but
this would lead to always preferring graphs that have
a larger number of edges. Interestingly, averaging
the edge scores does not produce good results either
because it is biased towards smaller graph fragments
containing only highly-confident edges.
We created a new scoring method which prefers
graphs that cover all the nodes, but does not create
bias for a higher number of edges. For every node
in the graph, it finds the average score of all edges
which have that node as a dependent. These scores
are then averaged again over all nodes:
NScore(n) =
?
e?Eg
EdgeScore(e)? isDep(e, n)
?
e?Eg
isDep(e, n)
GraphScore(g) =
?
n?Ng
NScore(n)
|Ng|
where g is the graph being scored, n ? Ng is a
node in graph g, e ? Eg is an edge in graph g,
isDep(e, n) is a function returning 1.0 if n is the de-
pendent in edge e, and 0.0 otherwise. NScore(n) is
set to 0 if the node does not appear as a dependent in
any edges. We found this metric performs well, as
it prefers graphs that connect together many nodes
without simply rewarding a larger number of edges.
While the score calculation is done using the
modified graph g?r, the resulting score is directly as-
signed to the corresponding original graph gr, and
the reordering of the original dependency graphs is
used for evaluation.
4 Experiments
4.1 Evaluation methods
In order to evaluate how much the reranker improves
the highest-ranked dependency graph, we calculate
the microaveraged precision, recall and F-score over
all dependencies from the top-ranking parses for
the test set. Following the official RASP evalua-
tion (Briscoe et al, 2006) we employ the hierarchi-
cal edge matching scheme which aggregates counts
up the dependency relation subsumption hierarchy
and thus rewards the parser for making more fine-
grained distinctions.2 Statistical significance of the
change in F-score is calculated by using the Approx-
imate Randomisation Test (Noreen, 1989; Cohen,
1995) with 106 iterations.
We also wish to measure how well the reranker
does at the overall task of ordering dependency
graphs. For this we make use of an oracle that cre-
ates the perfect ranking for a set of graphs by calcu-
lating their individual F-scores; this ideal ranking is
then compared to the output of our system. Spear-
man?s rank correlation coefficient between the two
rankings is calculated for each sentence and then av-
eraged over all sentences. If the scores for all of the
returned analyses are equal, this coefficient cannot
be calculated and is set to 0.
4.2 DepBank
We evaluated our self-learning framework using
the DepBank/GR reannotation (Briscoe and Carroll,
2006) of the PARC 700 Dependency Bank (King
et al, 2003). The dataset is provided with the
open-source RASP distribution3 and has been used
for evaluating different parsers, including RASP
(Briscoe and Carroll, 2006; Watson et al, 2007) and
2Slight changes in the performance of the baseline parser
compared to previous publications are due to using a more re-
cent version of the parser and minor corrections to the gold stan-
dard annotation.
3ilexir.co.uk/2012/open-source-rasp-3-1/
396
C&C (Clark and Curran, 2007). It contains 700 sen-
tences, randomly chosen from section 23 of the WSJ
Penn Treebank (Marcus et al, 1993), divided into
development (140 sentences) and test data (560 sen-
tences). We made use of the development data to
experiment with a wider selection of edge and graph
scoring methods, and report the final results on the
test data.
For reranking we collect up to 1000 top-ranked
analyses for each sentence. The actual number of
analyses that the RASP parser outputs depends on
the sentence and can be smaller. As the parser first
constructs parse trees and converts them to depen-
dency graphs, several parse trees may result in iden-
tical graphs; we remove any duplicates to obtain a
ranking of unique dependency graphs.
Our approach relies on a large unannotated corpus
of in-domain text, and for this we used the BLLIP
corpus containing 50M words of in-domain WSJ ar-
ticles. Our version of this corpus excludes texts that
are found in the Penn Treebank, thereby also exclud-
ing the section that we use for evaluation.
The baseline system is the unlexicalised RASP
parser with default settings. In order to construct
the upper bound, we use an oracle to calculate the F-
score for each dependency graph individually, and
then create the best possible ranking using these
scores.
Table 1 contains evaluation results on the Dep-
Bank/GR test set. The baseline system achieves
76.41% F-score on the test data, with 32.70% av-
erage correlation. I and RES scoring methods give
comparable results, with RES improving correlation
by 9.56%. The CES and ECES scores all make use
of corpus-based statistics and all significantly im-
prove over the baseline system, with absolute in-
creases in F-score of more than 2% for the fully-
connected edge score variants.
Finally, we combine the RES score with the
corpus-based methods and the fully-connected
CMB2 variant again delivers the best overall results.
The final F-score is 79.21%, an absolute improve-
ment of 2.8%, corresponding to 33.65% relative er-
ror reduction with respect to the upper bound. Cor-
relation is also increased by 16.32%; this means the
methods not only improve the chances of finding the
best dependency graph, but also manage to create
a better overall ranking. The F-scores for all the
corpus-based scoring methods are statistically sig-
nificant when compared to the baseline (p < 0.05).
By using our self-learning framework, we were
able to significantly improve the original unlexi-
calised parser. To put the overall result in a wider
perspective, Clark and Curran (2007) achieve an
F-score of 81.86% on the DepBank/GR test sen-
tences using the C&C lexicalised parser, trained
on 40,000 manually-treebanked sentences from the
WSJ. The unlexicalised RASP parser, using a
manually-developed grammar and a parse ranking
component trained on 4,000 partially-bracketed un-
labelled sentences from a domain/genre balanced
subset of Brown (Watson et al, 2007), achieves an
F-score of 76.41% on the same test set. The method
introduced here improves this to 79.21% F-score
without using any further manually-annotated data,
closing more than half of the gap between the perfor-
mance of a fully-supervised in-domain parser and a
more weakly-supervised more domain-neutral one.
We also performed an additional detailed analysis
of the results and found that, with the exception of
the auxiliary dependency relation, the reranking pro-
cess was able to improve the F-score of all other in-
dividual dependency types. Complements and mod-
ifiers are attached with much higher accuracy, result-
ing in 3.34% and 3.15% increase in the correspond-
ing F-scores. The non-clausal modifier relation (nc-
mod), which is the most frequent label in the dataset,
increases by 3.16%.
4.3 Genia
One advantage of our reranking framework is that
it does not rely on any domain-dependent manually
annotated resources. Therefore, we are interested in
seeing how it performs on text from a completely
different domain and genre.
The GENIA-GR dataset (Tateisi et al, 2008) is
a collection of 492 sentences taken from biomedi-
cal research papers in the GENIA corpus (Kim et
al., 2003). The sentences have been manually anno-
tated with dependency-based grammatical relations
identical to those output by the RASP parser. How-
ever, it does not contain dependencies for all tokens
and many multi-word phrases are treated as single
units. For example, the tokens ?intracellular redox
status? are annotated as one node with label intra-
cellular redox status. We retain this annotation and
397
DepBank/GR GENIA-GR
Prec Rec F ? Prec Rec F ?
Baseline 77.91 74.97 76.41 32.70 79.91 78.86 79.38 36.54
Upper Bound 86.74 82.82 84.73 75.36 86.33 84.71 85.51 78.66
I 77.77 75.00 76.36 33.32 77.18 76.21 76.69 30.23
RES 78.13 74.94 76.50 42.26 80.06 78.89 79.47 47.52
CES1 79.68 76.40 78.01 41.95 78.64 77.50 78.07 36.06
CES2 80.48 77.28 78.85 48.43 79.92 78.92 79.42 43.09
ECES1 79.96 76.68 78.29 42.41 79.09 78.11 78.60 38.02
ECES2 80.71 77.52 79.08 49.05 79.84 78.95 79.39 43.64
CMB1 80.64 77.31 78.94 48.25 80.60 79.51 80.05 44.96
CMB2 80.88 77.60 79.21 49.02 80.69 79.64 80.16 46.24
Table 1: Performance of different edge scoring methods on the test data. For each measure we report precision,
recall, F-score, and average Spearman?s correlation (?). The highest results for each measure are marked in bold. The
underlined F-scores are significantly better compared to the baseline.
allow the unlexicalised parser to treat these nodes as
atomic unseen words during POS tagging and pars-
ing. However, we use the last lemma in each multi-
word phrase for calculating the edge score statistics.
In order to initialise our parse reranking frame-
work, we also need a background corpus that closely
matches the evaluation domain. The annotated sen-
tences in GENIA-GR were chosen from abstracts
that are labelled with the MeSH term ?NF-kappa B?.
Following this method, we created our background
corpus by extracting 7,100 full-text articles (1.6M
sentences) from the PubMed Central Open Access
collection, containing any of the following terms
with any capitalisation: ?nf-kappa b?, ?nf-kappab?,
?nf kappa b?, ?nf-kappa b?, ?nf-kb?, ?nf-?b?. Since
we retain all texts from matching documents, this
keyword search acts as a broad indicator that the sen-
tences contain topics which correspond to the evalu-
ation dataset. This focussed corpus was then parsed
with the unlexicalised parser and used to create a
statistical model for the reranking system, following
the same methods as described in Sections 3 and 4.2.
Table 1 also contains the results for experiments
in the biomedical domain. The first thing to notice
is that while the upper bound for the unlexicalised
parser is similar to that for the DepBank experiments
in Section 4.2, the baseline results are considerably
higher. This is largely due to the nature of the dataset
? since many complicated multi-word phrases are
treated as single nodes, the parser is not evaluated on
edges within these nodes. In addition, treating these
nodes as unseen words eliminates many incorrect
derivations that would otherwise split the phrases.
This results in a naturally higher baseline of 79.38%,
and also makes it more difficult to further improve
the performance.
The edge scoring methods I, CES1 and ECES1
deliver F-scores lower than the baseline in this ex-
periment. RES, CES2 and ECES2 yield a modest
improvement in both F-score and Spearman?s cor-
relation. Finally, the combination methods again
give the best performance, with CMB2 delivering an
F-score of 80.16%, an absolute increase of 0.78%,
which is statistically significant (p < 0.05). The
experiment shows that our self-learning framework
works on very different domains, and it can be used
to significantly increase the accuracy of an unlexi-
calised parser without requiring any annotated data.
5 Conclusion
We developed a new self-learning framework for de-
pendency graph reranking that requires only a plain-
text corpus from a suitable domain. We automati-
cally parse this corpus and use the highest ranked
analyses to estimate maximum likelihood probabili-
ties for bilexical relations. Every dependency graph
is first modified to incorporate additional edges that
model selected higher-order dependency path rela-
tionships. Each edge in the graph is then assigned a
confidence score based on statistics from the back-
ground corpus and ranking preferences from the un-
398
lexicalised parser. We also described a novel method
for smoothing these scores using directional dis-
tributional similarity measures. Finally, the edge
scores are combined into an overall graph score by
first averaging them over individual nodes.
As the method requires no annotated data, it can
be easily adapted to different domains and genres.
Our experiments showed that the reranking process
significantly improved performance on both WSJ
and biomedical data.
References
Shilpa Arora, Elijah Mayfield, Carolyn Penstein-Rose?,
and Eric Nyberg. 2010. Sentiment Classification us-
ing Automatically Extracted Subgraph Features. In
Proceedings of the NAACL HLT 2010 Workshop on
Computational Approaches to Analysis and Genera-
tion of Emotion in Text.
Mohit Bansal and Dan Klein. 2011. Web-scale fea-
tures for full-scale parsing. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics, pages 693?702.
Ted Briscoe and John Carroll. 2006. Evaluating the
accuracy of an unlexicalized statistical parser on the
PARC DepBank. In Proceedings of the COLING/ACL
on Main conference poster sessions, number July,
pages 41?48, Morristown, NJ, USA. Association for
Computational Linguistics.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proceed-
ings of the COLING/ACL 2006 Interactive Presenta-
tion Sessions, number July, pages 77?80, Sydney, Aus-
tralia. Association for Computational Linguistics.
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL, vol-
ume 7, pages 957?961.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. Proceedings of the 43rd Annual Meeting on As-
sociation for Computational Linguistics - ACL ?05,
1(June):173?180.
Stephen Clark and James R. Curran. 2007. Formalism-
independent parser evaluation with CCG and Dep-
Bank. In Proceedings of the 45th Annual Meeting
of the Association of Computational Linguistics, vol-
ume 45, pages 248?255.
Paul R Cohen. 1995. Empirical Methods for Artificial
Intelligence. The MIT Press, Cambridge, MA.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In The 17th International Con-
ference on Machine Learning (ICML).
Daniel Gildea. 2001. Corpus variation and parser per-
formance. In Proceedings of the 2001 Conference on
Empirical Methods in Natural Language Processing,
pages 167?202.
Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, and Jun?ichi
Tsujii. 2003. GENIA corpus - a semantically an-
notated corpus for bio-textmining. Bioinformatics,
19(1):180?182.
Tracy H. King, Richard Crouch, Stefan Riezler, Mary
Dalrymple, and Ronald M. Kaplan. 2003. The PARC
700 dependency bank. In Proceedings of the EACL03:
4th International Workshop on Linguistically Inter-
preted Corpora (LINC-03), pages 1?8.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics, number July, pages 423?430. Association for
Computational Linguistics Morristown, NJ, USA.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
linguistics, pages 1?22.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of the Human Language Technology Conference
of the North American Chapter of the Association of
Computational Linguistics, number June, pages 152?
159, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Dominick Ng, Matthew Honnibal, and James R. Curran.
2010. Reranking a wide-coverage CCG parser. In
Australasian Language Technology Association Work-
shop 2010, page 90.
Eric W. Noreen. 1989. Computer Intensive Methods
for Testing Hypotheses: An Introduction. Wiley, New
York.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th annual meeting of the ACL (ACL ?06),
pages 433?440, Morristown, NJ, USA. Association for
Computational Linguistics.
Barbara Plank and Gertjan van Noord. 2008. Explor-
ing an auxiliary distribution based approach to domain
adaptation of a syntactic disambiguation model. In
Coling 2008: Proceedings of the Workshop on Cross-
Framework and Cross- Domain Parser Evaluation,
pages 9?16, Manchester, UK. Association for Com-
putational Linguistics.
399
Marek Rei. 2013. Minimally supervised dependency-
based methods for natural language processing. Ph.D.
thesis, University of Cambridge.
Satoshi Sekine. 1997. The domain dependence of pars-
ing. In Proceedings of the fifth conference on Applied
natural language processing, volume 1, pages 96?102,
Morristown, NJ, USA. Association for Computational
Linguistics.
Yuka Tateisi, Yusuke Miyao, Kenji Sagae, and Jun?ichi
Tsujii. 2008. GENIA-GR: a Grammatical Relation
Corpus for Parser Evaluation in the Biomedical Do-
main. In Proceedings of LREC, pages 1942?1948.
Gertjan van Noord. 2007. Using self-trained bilexical
preferences to improve disambiguation accuracy. In
Proceedings of the 10th International Conference on
Parsing Technologies, number June, pages 1?10, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Rebecca Watson, Ted Briscoe, and John Carroll. 2007.
Semi-supervised training of a statistical parser from
unlabeled partially-bracketed data. Proceedings of the
10th International Conference on Parsing Technolo-
gies - IWPT ?07, (June):23?32.
Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai. 2011.
Exploiting Web-Derived Selectional Preference to Im-
prove Statistical Dependency Parsing. In 49th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1556?1565.
400
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 56?63,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Combining Manual Rules and Supervised Learning for Hedge Cue and
Scope Detection
Marek Rei
Computer Laboratory
University of Cambridge
United Kingdom
Marek.Rei@cl.cam.ac.uk
Ted Briscoe
Computer Laboratory
University of Cambridge
United Kingdom
Ted.Briscoe@cl.cam.ac.uk
Abstract
Hedge cues were detected using a super-
vised Conditional Random Field (CRF)
classifier exploiting features from the
RASP parser. The CRF?s predictions were
filtered using known cues and unseen in-
stances were removed, increasing preci-
sion while retaining recall. Rules for scope
detection, based on the grammatical re-
lations of the sentence and the part-of-
speech tag of the cue, were manually-
developed. However, another supervised
CRF classifier was used to refine these pre-
dictions. As a final step, scopes were con-
structed from the classifier output using a
small set of post-processing rules. Devel-
opment of the system revealed a number of
issues with the annotation scheme adopted
by the organisers.
1 Introduction
Speculative or, more generally, ?hedged? language
is a way of weakening the strength of a statement.
It is usually signalled by a word or phrase, called a
hedge cue, which weakens some clauses or propo-
sitions. These weakened portions of a sentence
form the scope of the hedge cues.
Hedging is an important tool in scientific lan-
guage allowing scientists to guide research be-
yond the evidence without overstating what fol-
lows from their work. Vincze et al (2008) show
that 19.44% of all sentences in the full papers
of the BioScope corpus contain hedge cues. De-
tecting these cues is potentially valuable for tasks
such as scientific information extraction or liter-
ature curation, as typically only definite informa-
tion should be extracted and curated. Most work
so far has been done on classifying entire text
sentences as hedged or not, but this risks los-
ing valuable information in (semi-)automated sys-
tems. More recent approaches attempt to find the
specific parts of a text sentence that are hedged.
Here we describe a system that is designed to
find hedge cues and their scopes in biomedical re-
search papers. It works in three stages:
1. Detecting the cues using a token-level super-
vised classifier.
2. Finding the scopes with a combination of
manual rules and a second supervised token-
level classifier.
3. Applying postprocessing rules to convert the
token-level annotation into predictions about
scope.
Parts of the system are similar to that of Morante
and Daelemans (2009) ? both make use of ma-
chine learning to tag tokens as being in a cue or
a scope. The most important differences are the
use of manually defined rules and the inclusion of
grammatical relations from a parser as critical fea-
tures.
2 Data
A revised version of the BioScope corpus (Vincze
et al, 2008), containing annotation of cues and
scopes, was provided as training data for the
CoNLL-2010 shared task (Farkas et al, 2010).
This includes 9 full papers and 1273 abstracts
from biomedical research fields. A separate new
set of full papers was released for evaluation as
part of the task. Table 1 contains an overview of
the training corpus statistics.
(1) provides an example sentence from the cor-
pus illustrating the annotation provided for train-
ing.
(2) shows the same sentence, representing cues
with angle brackets and scopes with round brack-
ets.
56
Papers Abstracts
Documents 9 1273
Sentences 2670 11871
Cues 682 2694
Scopes 668 2659
Unique cues 100 112
Cues with multiple words 10.70% 12.25%
Scopes start with cue 72.00% 80.59%
Scopes with multiple cues 2.10% 1.28%
Table 1: Training data statistics.
(1) <sentence id=?S1.166?>We <xcope
id=?X1.166.2?><cue ref=?X1.166.2?
type=?speculation?>expect</cue> that this cluster
<xcope id=?X1.166.1?><cue ref=?X1.166.1?
type=?speculation?>may</cue> represent a novel
selenoprotein
family</xcope></xcope>.</sentence>
(2) We (<expect> that this cluster (<may> represent a
novel selenoprotein family)).
There are a number of conditions on the anno-
tation that are imposed:
? Every cue has one scope.
? Every scope has one or more cues.
? The cue must be contained within its scope.
? Cues and scopes have to be continuous.
For development of the system, before the eval-
uation data were released, we used 60% of the
available corpus for training and 40% for testing.
The results we give below measure the system per-
formance on the evaluation data while using all
of the training data to build the supervised clas-
sifiers. The manually-developed rules are based
on the 60% of the development data we originally
reserved for training.
All of the training and test data sentences
were tokenised and parsed using the RASP sys-
tem (Briscoe et al, 2006). Multiple part-of-
speech (POS) tag outputs were passed to the parser
(to compensate for the high number of unseen
words in biomedical text), retaining just the high-
est ranked directed graph of grammatical relations
(GRs). Each node in the graph represents a word
token annotated with POS, lemma, and positional
order information. In the case of parse failure the
set of unconnected graphs returned by the highest-
ranked spanning subanalyses for each sentence
were retained.
3 Speculation cues
The hedge cues are found using a Conditional
Random Field (CRF) (Lafferty et al, 2001) classi-
fier, implemented using CRF++ 1. We chose the
CRF model because we framed the task as one
of token-level sequential tagging and CRFs are
known to achieve state-of-the-art performance on
related text classification tasks. Each word token
is assigned one of the following tags: F (first word
of a cue), I (inside a cue), L (last word of a cue),
O (outside, not a cue), hereafter referred to as the
FILO scheme.
The feature types used for classification are de-
fined in terms of the grammatical relations output
provided by the RASP system. We use binary fea-
tures that indicate whether a word token is a head
or a dependent in specific types of grammatical
relation (GR). This distinguishes between differ-
ent functions of the same word (when used as a
subject, object, modifier, etc.). These features are
combined with POS and lemma of the word to dis-
tinguish between uses of different cues and cue
types. We also utilise features for the lemma and
POS of the 3 words before and after the current
word.
The list of feature types for training the classi-
fier is:
? string
? lemma
? part-of-speech
? broad part-of-speech
? incoming GRs + POS
? outgoing GRs + POS
? incoming GRs + POS + lemma
? outgoing GRs + POS + lemma
? lemma + POS + POS of next word
? lemma + POS + POS of previous word
? 3 previous lemma + POS combinations
? 3 following lemma + POS combinations.
Outgoing GRs are grammatical relations where
the current word is the head, incoming GRs where
it is the dependent.
The predictions from the classifier are com-
pared to the list of known cues extracted from the
training data; the longest possible match is marked
as a cue. For example, the classifier could output
the following tag sequence:
(3) This[O] indicates[F] that[O] these[O] two[O]
lethal[O] mutations[O] . . .
1http://crfpp.sourceforge.net
57
indicates is classified as a cue but that is not.The list of known cues contains ?indicates that?which matches this sentence, therefore the systemprediction is:
(4) This <indicates that> these two lethal mutations . . .
Experiments in section 5.1 show that our sys-
tem is not good at finding previously unseen cues.
Lemma is the most important feature type for cue
detection and when it is not available, there is not
enough evidence to make good predictions. There-
fore, we compare all system predictions to the list
of known cues and if there is no match, they are
removed. The detection of unseen hedge cues is a
potential topic for future research.
4 Speculation scopes
We find a scope for each cue predicted in the pre-
vious step. Each word token in the sentence is
tagged with either F (first word of a scope), I (in-
side a scope), L (last word of a scope) or O (out-
side, not in a scope). Using our example sentence
(2) the correct tagging is:
expect may
We O O
expect F O
that I O
this I O
cluster I O
may I F
represent I I
a I I
novel I I
selenoprotein I I
family L L
. O O
Table 2: Example of scope tagging.
If a cue contains multiple words, they are each
processed separately and the predictions are later
combined by postprocessing rules.
As the first step, manually written rules are ap-
plied that find the scope based on GRs and POS
tags. We refine these predictions using a second
CRF classifier and further feature types extracted
from the RASP system output. Finally, postpro-
cessing rules are applied to convert the tagging se-
quence into scopes. By default, the minimal scope
returned is the cue itself.
4.1 Manual rules
Manual rules were constructed based on the de-
velopment data and annotation guidelines. In the
following rules and examples:
? ?below? refers to nodes that are in the sub-
graph of GRs rooted in the current node.
? ?parent? refers to the node that is the head
of the current node in the directed, connected
GR graph.
? ?before? and ?after? refer to word positions
in the text centered on the current node.
? ?mark everything below? means mark all
nodes in the subgraph as being in the scope
(i.e. tag as F/I/L as appropriate). However,
the traversal of the graph is terminated when
a text adjunct (TA) GR boundary or a word
POS-tagged as a clause separator is found,
since they often indicate the end of the scope.
The rules for finding the scope of a cue are trig-
gered based on the generalised POS tag of the cue:
? Auxiliary ? VM
Mark everything that is below the parent and
after the cue.
If the parent verb is passive, mark everything
below its subject (i.e. the dependent of the
subj GR) before the cue.
? Verb ? VV
Mark everything that is below the cue and af-
ter the cue.
If cue is appear or seem, mark everything be-
low subject before the cue.
If cue is passive, mark everything below sub-
ject before the cue.
? Adjective ? JJ
Find parent of cue. If there is no parent, the
cue is used instead.
Mark everything that is below the parent and
after the cue.
If parent is passive, mark everything below
subject before the cue.
If cue is (un)likely and the next word is to,
mark everything below subject before the
cue.
? Adverb ? RR
Mark everything that is below the parent and
after the cue.
58
? Noun ? NN
Find parent of cue. If there is no parent, the
cue is used instead.
Mark everything that is below the parent and
after the cue.
If parent is passive, mark everything below
subject before the cue.
? Conjunction ? CC
Mark everything below the conjunction.
If the cue is or and there is another cue either
before, combine them together.
? ?Whether? as a conjunction ? CSW
Mark everything that is below the cue and af-
ter the cue.
? Default ? anything else
Mark everything that is below the parent and
after the cue.
If parent verb is passive, mark everything be-
low subject before the cue.
Either . . . or . . . is a frequent exception contain-
ing two separate cues that form a single scope. An
additional rule combines these cues when they are
found in the same sentence.
The partial GR graph for (5) is given in Figure
1 (with positional numbering suppressed for read-
ability).
(5) Lobanov et al thus developed a sensitive search
method to deal with this problem, but they also
admitted that it (<would> fail to identify highly
unusual tRNAs).
Following the rules, would is identified as a cue
word with the part-of-speech VM; this triggers the
first rule in the list. The parent of would is fail
since they are connected with a GR where fail is
the head. Everything that is below fail in the GR
graph and positioned after would is marked as be-
ing in the scope. Since fail is not passive, the sub-
ject it is left out. The final scope returned by the
rule is then would fail to identify highly unusual
tRNAs.
4.2 Machine learning
The tagging sequence from the manual rules is
used as input to a second CRF classifier, along
with other feature types from RASP. The output
of the classifier is a modified sequence of FILO
tags.
The list of features for each token, used both
alone and as sequences of 5-grams before and after
the token, is:
Figure 1: Partial GR graph for sample sentence (5)
? tag from manual rules
? lemma
? POS
? is the token also the cue
? distance from the cue
? absolute distance from the cue
? relative position to the cue
? are there brackets between the word and the
cue
? is there any other punctuation between the
word and the cue
? are there any special (clause ending) words
between the word and cue
? is the word in the GR subtree of the cue
? is the word in the GR subtree of the main verb
? is the word in the GR subject subtree of the
main verb
Features of the current word, used in combina-
tion with the POS sequence of the cue:
? POS
? distance from the cue
? absolute distance from the cue
? relative position to the cue
? is the word in the GR subtree of the cue
? is the word in the GR subtree of the main verb
? is the word in the GR subject subtree of the
main verb
59
Additional features:
? GR paths between the word and the cue: full
path plus subpaths with up to 5 nodes
? GR paths in combination with the lemma se-
quence of the cue
The scope of the hedge cue can often be found
by tracking the sequence of grammatical relations
in the GR graph of a sentence, as described by
the manual rules. To allow the classifier to learn
such regularities, we introduce the concept of a
GR path.
Given that the sentence has a full parse and con-
nected graph, we can find the shortest connected
path between any two words. We take the con-
nected path between the word and the cue and con-
vert it into a string representation to use it as a fea-
ture value in the classifier. Path sections of differ-
ent lengths allow the system to find both general
and more specific patterns. POS tags are used as
node values to abstract away from word tokens.
An example for the word unusual, using the
graph from Figure 1, is given below. Five fea-
tures representing paths with increasing lengths
plus one feature containing the full path are ex-
tracted.
(6) 1: VM
2: VM<?aux?VV0
3: VM<?aux?VV0?xcomp?>VV0
4: VM<?aux?VV0?xcomp?>VV0?dobj?>NP2
5: VM<?aux?VV0?xcomp?>VV0?dobj?>NP2?
ncmod?>JJ
6: VM<?aux?VV0?xcomp?>VV0?dobj?>NP2?
ncmod?>JJ
Line 1 shows the POS of the cue would (VM).
On line 2, this node is connected to fail (VV0) by
an auxiliary GR type. More links are added until
we reach unusual (JJ).
The presence of potential clause ending words,
used by Morante and Daelemans (2009), is in-
cluded as a feature type with values: whereas,
but, although, nevertheless, notwithstanding, how-
ever, consequently, hence, therefore, thus, instead,
otherwise, alternatively, furthermore, moreover,
since.
4.3 Post-processing
If the cue contains multiple words, the tag se-
quences have to be combined. This is done by
overlapping the sequences and choosing the pre-
ferred tag for each word, according to the hierar-
chy F > L > I > O.
Next, scopes are constructed from tag se-
quences using the following rules:
? Scope start point is the first token tagged as
F before the cue. If none are found, the first
word of the cue is used as the start point.
? Scope end point is the last token tagged as L
after the cue. If none are found, look for tags
I and F. If none are found, the last word of the
cue is used as end point.
The scopes are further modified until none of
the rules below return any updates:
? If the last token of the scope is punctuation,
move the endpoint before the token.
? If the last token is a closing bracket, move the
scope endpoint before the opening bracket.
? If the last token is a number and it is not pre-
ceded by a capitalised word (e.g. Table 16),
move the scope endpoint before the token.
This is a heuristic rule to handle trailing ci-
tations which are frequent in the training data
and often misattached by the parser.
Finally, scopes are checked for partial overlap
and any instances are corrected. For example, the
system might return a faulty version (7) of the sen-
tence (2) in which one scope is only partially con-
tained within the other.
(7) We [<expect> that this cluster (<may> represent a
novel] selenoprotein family).
This prediction cannot be represented within the
format specified for the shared task and we were
unable to find cases where such annotation would
be needed. These scopes are modified by moving
the end of the first scope to the end of the second
scope. The example above would become:
(8) We [<expect> that this cluster (<may> represent a
novel selenoprotein family)].
5 Results
5.1 Hedge cues
In evaluation a predicted cue is correct if it con-
tains the correct substring of the sentence. Token-
level evaluation would not give accurate results
because of varying tokenisation rules. A sentence
is classified as hedged if it contains one or more
cues.
60
The results below are obtained using the scorers
implemented by the organisers of the shared task.
As our baseline system, we use simple string
matching. The list of known cues is collected from
the training data and compared to the evaluation
sentences. The longest possible match is always
marked as a cue. ML1 to ML3 are variations of
the system described in section 3. All available
data, from papers and abstracts, were used to train
the CRF classifier. ML1 uses the results of the
classifier directly. The longest sequence of tokens
tagged as being part of a cue is used to form the fi-
nal prediction. ML2 incorporates the list of known
cues, constructing a cue over the longest sequence
of matching tokens where at least one token has
been tagged as belonging to a cue. ML3 uses the
list of known cues and also removes any predicted
cues not seen in the training data.
Baseline ML1 ML2 ML3
Total cues 1047 1047 1047 1047
Predicted cues 3062 995 1006 995
Correctly
predicted cues
1018 785 810 810
Cue precision 0.332 0.789 0.805 0.814
Cue recall 0.972 0.750 0.774 0.774
Cue F-measure 0.495 0.769 0.789 0.793
Sentence
precision
0.413 0.831 0.831 0.838
Sentence recall 0.995 0.843 0.843 0.842
Sentence
F-measure
0.584 0.837 0.837 0.840
Table 3: Cue detection results.
The baseline system returns nearly all cues but
since it matches every string, it also returns many
false positives, resulting in low precision. ML1
delivers more realistic predictions and increases
precision to 0.79. This illustrates how the use of a
word as a hedge cue depends on its context and not
only on the word itself. ML2 incorporates known
cues and increases both precision and recall. ML3
removes any unseen cue predictions further im-
proving precision. This shows the system is un-
able to accurately predict cues that have not been
included in the training data.
Table 4 lists the ten most common cues in the
test data and the number of cues found by the ML3
system.
In the cases of may and suggest, which are also
the most common cues in the development data,
the system finds all the correct instances. Can
and or are not detected as accurately because they
are both common words that in most cases are
TP FP Gold
may 161 5 161
suggest 124 0 124
can 2 1 61
or 9 12 52
indicate that 49 2 50
whether 42 6 42
might 42 1 42
could 30 17 41
would 37 14 37
appear 31 14 31
Table 4: True and false positives of the ten most
common cues in the evaluation data, using ML3
system.
not functioning as hedges. For example, there are
1215 occurrences of or in the training data and
only 146 of them are hedge cues; can is a cue in 64
out of 506 instances. We have not found any ex-
tractable features that reliably distinguish between
the different uses of these words.
5.2 Hedge scopes
A scope is counted as correct if it has the correct
beginning and end points in the sentence and is
associated with the correct cues. Scope prediction
systems take cues as input, therefore we present
two separate evaluations ? one with gold standard
cues and the other with cues predicted by the ML3
system from section 4.
The baseline system looks at each cue and
marks a scope from the beginning of the cue to the
end of the sentence, excluding the full stop. The
system using manual rules applies a rule for each
cue to find its scope, as described in section 4.1.
The POS tag of the cue is used to decide which
rule should be used and the GRs determine the
scope.
The final system uses the result from the manual
rules to derive features, adds various further fea-
tures from the parser and trains a CRF classifier to
refine the predictions.
We hypothesized that the speculative sentences
in abstracts may differ from the ones in full papers
and a 10-fold cross-validation of the development
data supported this intuition. Therefore, the orig-
inal system (CRF1) only used data from the full
papers to train the scope detection classifier. We
present here also the system trained on all of the
available data (CRF2).
Post-processing rules are applied equally to all
of these systems.
The baseline system performs remarkably well.
61
Baseline Manual
rules
Manual
rules +
CRF1
Manual
rules +
CRF2
Total scopes 1033 1033 1033 1033
Predicted 1047 1035 1035 1035
Correctly
predicted
596 661 686 683
Precision 0.569 0.639 0.663 0.660
Recall 0.577 0.640 0.664 0.661
F-measure 0.573 0.639 0.663 0.661
Table 5: Scope detection results using gold stan-
dard cues.
Baseline Manual
rules
Manual
rules +
CRF1
Manual
rules +
CRF2
Total scopes 1033 1033 1033 1033
Predicted 995 994 994 994
Correctly
predicted
507 532 564 567
Precision 0.510 0.535 0.567 0.570
Recall 0.491 0.515 0.546 0.549
F-measure 0.500 0.525 0.556 0.559
Table 6: Scope detection results using predicted
cues.
It does not use any grammatical or lexical know-
ledge apart from the cue and yet it delivers an F-
score of 0.50 with predicted and 0.57 with gold
standard cues.
Manual rules are essentially a more fine-grained
version of the baseline. Instead of a single rule,
one of 8 possible rules is selected based on the
POS tag of the cue. This improves the results,
increasing the F-score to 0.53 with predicted and
0.64 with gold standard cues. The improvement
suggests that the POS tag of a cue is a good indi-
cator of how it behaves in the sentence.
Error analysis showed that 35% of faulty scopes
were due to incorrect or unconnected GR graphs
output by the parser, and 65% due to exceptions
that the rules do not cover. An example of an ex-
ception, the braces { } showing the scopes pre-
dicted by the rules, is given in (9).
(9) Contamination is {(<probably> below 1%)}, which
is {(<likely> lower than the contamination rate of the
positive dataset) as discussed in 47}.
as discussed in 47 is a modifier of the clause
which is usually included in the scope but in this
case should be left out.
Finally, the last system combines features from
the rule-based system with features from RASP to
train a second classifier and improves our results
further, reaching 0.56 with predicted cues.
Inclusion of the abstracts as training data gave
a small improvement with predicted cues but not
with gold standard cues. It is part of future work
to determine if and how the use of hedges differs
across text sources.
6 Annotation scheme
During analysis of the data, several examples were
found that could not be correctly annotated due to
the restrictions of the markup. This leads us to
believe that the current rules for annotation might
not be best suited to handle complex constructions
containing hedged text.
Most importantly, the requirement for the hedge
scope to be continuous over the surface form of
text sentence does not work for some examples
drawn from the development data. In (10) below
it is uncertain whether fat body disintegration is
independent of the AdoR. In contrast, it is stated
with certainty that fat body disintegration is pro-
moted by action of the hemocytes, yet the latter
assertion is included in the scope to keep it contin-
uous.
(10) (The block of pupariation <appears> to involve
signaling through the adenosine receptor ( AdoR )) ,
but (fat body disintegration , which is promoted by
action of the hemocytes , <seems> to be independent
of the AdoR) .
Similarly, according to the guidelines, the sub-
ject of be likely should be included in its scope,
as shown in example (11). In sentence (12), how-
ever, the subject this phenomenon is separated by
two non-speculative clauses and is therefore left
out of the scope.
(11) Some predictors make use of the observation that
(neighboring genes whose relative location is
conserved across several prokaryotic organisms are
<likely> to interact).
(12) This phenomenon, which is independent of tumour
necrosis factor, is associated with HIV replication, and
(is thus <likely> to explain at least in part the
perpetuation of HIV infection in monocytes).
In (13), arguably, there is no hedging as the sen-
tence precisely describes a statistical technique for
predicting interaction given an assumption.
(13) More recently, other groups have come up with
sophisticated statistical methods to estimate
(<putatively> interacting domain pairs), based on the
(<assumption> of domain reusability).
Ultimately, dealing effectively with these and
related examples would involve representing
62
hedge scope in terms of sets of semantic proposi-
tions recovered from a logical semantic represen-
tation of the text, in which anaphora, word sense,
and entailments had been resolved.
7 Related work
Most of the previous work has been done on classi-
fying sentences as hedged or not, rather than find-
ing the scope of the hedge.
The first linguistically and computationally mo-
tivated study of hedging in biomedical texts is
Light et al (2004). They present an analysis of the
problem based on Medline abstracts and construct
an initial experiment for automated classification.
Medlock and Briscoe (2007) propose a weakly
supervised machine learning approach to the
hedge classification problem. They construct a
classifier with single words as features and use
a small amount of seed data to bootstrap the
system, achieving the precision/recall break-even
point (BEP) of 0.76. Szarvas (2008) extends this
work by introducing bigrams and trigrams as fea-
ture types, improving feature selection and us-
ing external data sources to construct lists of cue
words, achieving a BEP of 0.85.
Kilicoglu and Bergler (2008) apply a combina-
tion of lexical and syntactic methods, improving
on previous results and showing that quantifying
the strength of a hedge can be beneficial for clas-
sification of speculative sentences.
Vincze et al (2008) created a publicly available
annotated corpus of biomedical papers, abstracts
and clinical data called BioScope, parts of which
were also used as training data for the CoNLL10
shared task, building on the dataset and annota-
tion scheme used for evaluation by Medlock and
Briscoe (2007).
Morante and Daelemans (2009) use the Bio-
Scope corpus to approach the problem of identify-
ing cues and scopes via supervised machine learn-
ing. They train a selection of classifiers to tag each
word and combine the results with a final classi-
fier, finding 65.6% of the scopes in abstracts and
35.9% of the scopes in papers.
8 Conclusions
We have shown that the GRs output by the RASP
system can be effectively used as features for de-
tecting cues in a supervised classifier and also as
the basis for manual rules and features for scope
detection. We demonstrated that a small num-
ber of manual rules can provide competitive re-
sults, but that these can be further improved using
machine learning techniques and post-processing
rules. The generally low ceiling for the scope de-
tection results demonstrates the difficulty of both
annotating and detecting the hedge scopes in terms
of surface sentential forms.
Future work could usefully be directed at im-
proving performance on unseen cue detection and
on learning rules of the same form as those de-
veloped manually from annotated training data.
However, perhaps the most pressing issue is that of
establishing the best possible annotation and con-
sequent definition of the scope detection task.
References
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Pro-
ceedings of the COLING/ACL 2006 on Interactive
Presentation Sessions.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos
Csirik, and Gyo?rgy Szarvas. 2010. The CoNLL-
2010 Shared Task: Learning to Detect Hedges and
their Scope in Natural Language Text. In Proceed-
ings of the Fourteenth Conference on Computational
Natural Language Learning (CoNLL-2010): Shared
Task, pages 1?12.
Halil Kilicoglu and Sabine Bergler. 2008. Recogniz-
ing speculative language in biomedical research ar-
ticles: a linguistically motivated perspective. BMC
Bioinformatics, 9 Suppl 11.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of ICML 2001.
Marc Light, Xin Y. Qiu, and Padmini Srinivasan. 2004.
The language of bioscience: Facts, speculations, and
statements in between. In Proceedings of BioLink
2004.
Ben Medlock and Ted Briscoe. 2007. Weakly super-
vised learning for hedge classification in scientific
literature. In Proceedings of ACL 2007.
Roser Morante and Walter Daelemans. 2009. Learn-
ing the scope of hedge cues in biomedical texts. In
Proceedings of the Workshop on BioNLP.
Gyo?rgy Szarvas. 2008. Hedge classification in
biomedical texts with a weakly supervised selection
of keywords. In Proceedings of ACL 2008.
Veronika Vincze, Gyo?rgy Szarvas, Richa?rd Farkas,
Gyo?rgy Mo?ra, and Ja?nos Csirik. 2008. The Bio-
Scope corpus: biomedical texts annotated for uncer-
tainty, negation and their scopes. BMC Bioinformat-
ics, 9 Suppl 11.
63
Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 10?18,
Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational Linguistics
Unsupervised Entailment Detection between Dependency Graph Fragments
Marek Rei
Computer Laboratory
University of Cambridge
United Kingdom
Marek.Rei@cl.cam.ac.uk
Ted Briscoe
Computer Laboratory
University of Cambridge
United Kingdom
Ted.Briscoe@cl.cam.ac.uk
Abstract
Entailment detection systems are generally
designed to work either on single words, re-
lations or full sentences. We propose a new
task ? detecting entailment between depen-
dency graph fragments of any type ? which
relaxes these restrictions and leads to much
wider entailment discovery. An unsupervised
framework is described that uses intrinsic sim-
ilarity, multi-level extrinsic similarity and the
detection of negation and hedged language to
assign a confidence score to entailment rela-
tions between two fragments. The final system
achieves 84.1% average precision on a data set
of entailment examples from the biomedical
domain.
1 Introduction
Understanding that two different texts are semanti-
cally similar has benefits for nearly all NLP tasks,
including IR, IE, QA and Summarisation. Similar-
ity detection is usually performed either on single
words (synonymy) or full sentences and paragraphs
(paraphrasing). A symmetric similarity relation im-
plies that both elements can be interchanged (syn-
onymy and paraphrasing), while directional similar-
ity suggests that one fragment can be substituted for
the other but not the opposite (hyponymy and entail-
ment).
All of these language phenomena can be ex-
pressed using a single entailment relation. For para-
phrases and synonyms the relation holds in both di-
rections (observe? notice), whereas entailment and
hyponymy are modelled as a unidirectional relation
(overexpress ? express). Such relations, however,
can be defined between text fragments of any size
and shape, ranging from a single word to a complete
text segment. For example (argues against? does
not support; the protein has been implicated? the
protein has been shown to be involved).
We propose a new task ? detecting entailment
relations between any kinds of text fragments. A
unified approach is not expected to perform better
when compared to systems optimised only for a spe-
cific task (e.g. recognising entailment between sen-
tences), but constructing a common theory to cover
all text fragments has important benefits. A broader
approach will allow for entailment discovery among
a much wider range of fragment types for which no
specialised systems exist. In addition, entailment re-
lations can be found between different types of frag-
ments (e.g. a predicate entailing an adjunct). Finally,
a common system is much easier to develop and in-
tegrate with potential applications compared to hav-
ing a separate system for each type of fragment.
In this paper we present a unified framework that
can be used to detect entailment relations between
fragments of various types and sizes. The system
is designed to work with anything that can be rep-
resented as a dependency graph, including single
words, constituents of various sizes, text adjuncts,
predicates, relations and full sentences. The ap-
proach is completely unsupervised and requires only
a large plain text corpus to gather information for
calculating distributional similarity. This makes it
ideal for the biomedical domain where the availabil-
ity of annotated training data is limited. We ap-
ply these methods by using a background corpus
10
of biomedical papers and evaluate on a manually
constructed dataset of entailing fragment pairs, ex-
tracted from biomedical texts.
2 Applications
Entailment detection between fragments is a vital
step towards entailment generation ? given text T ,
the system will have to generate all texts that ei-
ther entail T or are entailed by T . This is motivated
by applications in Relation Extraction, Information
Retrieval and Information Extraction. For example,
if we wish to find all genes that are synthesised in
the larval tissue, the following IE query can be con-
structed (with x and y marking unknown variables):
(1) x is synthesised in the larval tissue
Known entailment relations can be used to mod-
ify the query: (overexpression? synthesis), (larval
fat body ? larval tissue) and (the synthesis of x in
y ? x is synthesised in y). Pattern (2) entails pat-
tern (1) and would also return results matching the
information need.
(2) the overexpression of x in the larval fat body
A system for entailment detection can automati-
cally extract a database of entailing fragments from
a large corpus and use them to modify any query
given by the user. Recent studies have also inves-
tigated how complex sentence-level entailment re-
lations can be broken down into smaller consecu-
tive steps involving fragment-level entailment (Sam-
mons et al, 2010; Bentivogli et al, 2010). For ex-
ample:
(3) Text: The mitogenic effects of the B beta chain of
fibrinogen are mediated through cell surface
calreticulin.
Hypothesis: Fibrinogen beta chain interacts with
CRP55.
To recognise that the hypothesis is entailed by the
text, it can be decomposed into five separate steps
involving text fragments:
1. B beta chain of fibrinogen? Fibrinogen beta chain
2. calreticulin? CRP55
3. the mitogenic effects of x are mediated through y?
y mediates the mitogenic effects of x
4. y mediates the mitogenic effects of x ? y interacts
with x
5. y interacts with x? x interacts with y
This illustrates how entailment detection between
various smaller fragments can be used to construct
an entailment decision between more complicated
sentences. However, only the presence of these con-
structions has been investigated and, to the best of
our knowledge, no models currently exist for auto-
mated detection and composition of such entailment
relations.
3 Modelling entailment between graph
fragments
In order to cover a wide variety of language phe-
nomena, a fragment is defined in the following way:
Definition 1. A fragment is any connected subgraph
of a directed dependency graph containing one or
more words and the grammatical relations between
them.
This definition is intended to allow extraction of
a wide variety of fragments from a dependency tree
or graph representation of sentences found using any
appropriate parser capable of returning such output
(e.g. Ku?bler et al, 2009). The definition covers
single- or multi-word constituents functioning as de-
pendents (e.g. sites, putative binding sites), text ad-
juncts (in the cell wall), single- or multi-word pred-
icates (* binds to receptors in the airways) and rela-
tions (* binds and activates *) including ones with
?internal? dependent slots (* inhibits * at *), some of
which may be fixed in the fragment (* induces au-
tophosphorylation of * in * cells), and also full sen-
tences1. An example dependency graph and some
selected fragments can be seen in Figure 1.
Our aim is to detect semantically similar frag-
ments which can be substituted for each other in text,
resulting in more general or more specific versions
of the same proposition. This kind of similarity can
be thought of as an entailment relation and we define
entailment between two fragments as follows:
1The asterisks (*) are used to indicate missing dependents
in order to increase the readability of the fragment when repre-
sented textually. The actual fragments are kept in graph form
and have no need for them.
11
induce inB61recombinant
autophosphorylation of ECK
cell intactmod mod
subj iobj
iobjdobj dobjdobj
Figure 1: Dependency graph for the sentence: Recombinant B61 induces autophosphorylation of ECK in intact cells.
Some interesting fragments are marked by dotted lines.
Definition 2. Fragment A entails fragment B (A?
B) if A can be replaced by B in sentence S and the re-
sulting sentence S? can be entailed from the original
one (S? S?).
This also requires estimating entailment relations
between sentences, for which we use the definition
established by Bar-Haim et al (2006):
Definition 3. Text T entails hypothesis H (T ? H)
if, typically, a human reading T would infer that H
is most likely true.
We model the semantic similarity of fragments as
a combination of two separate directional similarity
scores:
1. Intrinsic similarity: how similar are the com-
ponents of the fragments.
2. Extrinsic similarity: how similar are the con-
texts of the fragments.
To find the overall score, these two similarity
measures are combined linearly using a weighting
parameter ?:
Score(A? B) = ?? IntSim(A? B)
+(1? ?)? ExtSim(A? B)
In this paper f(A ? B) designates an asym-
metric function between A and B. When referring
only to single words, lowercase letters (a,b) are used;
when referring to fragments of any size, including
single words, then uppercase letters are used (A,B).
Score(A? B) is the confidence score that frag-
ment A entails fragment B ? higher score indi-
cates higher confidence and 0 means no entailment.
IntSim(A? B) is the intrinsic similarity between
two fragments. It can be any function that compares
them, for example by matching the structure of one
fragment to another, and outputs a similarity score in
the range of [0, 1]. ExtSim(A ? B) is a measure
of extrinsic similarity that compares the contexts of
the two fragments. ? is set to 0.5 for an unsuper-
vised approach but the effects of tuning this param-
eter are further analysed in Section 5.
The directional similarity score is first found be-
tween words in each fragment, which are then used
to calculate the score between the two fragments.
3.1 Intrinsic similarity
IntSim(A? B) is the intrinsic similarity between
the two words or fragments. In order to best capture
entailment, the measure should be non-symmetrical.
We use the following simple formula for word-level
score calculation:
IntSim(a? b) =
length(c)
length(b)
where c is the longest common substring for a and
b. This measure will show the ratio of b that is also
contained in a. For example:
IntSim(overexpress? expression) = 0.70
IntSim(expression? overexpress) = 0.64
The intrinsic similarity function for fragments is
defined using an injective function between compo-
nents of A and components of B:
IntSim(A? B) =
Mapping(A? B)
|B|
where Mapping(A ? B) is a function that goes
through all the possible word pairs {(a, b)|a ?
A, b ? B} and at each iteration connects the one
12
with the highest entailment score, returning the sum
of those scores. Figure 2 contains pseudocode
for the mapping process. Dividing the value of
Mapping(A ? B) by the number of components
in B gives an asymmetric score that indicates how
well B is covered by A. It returns a lower score
if B contains more elements than A as some words
cannot be matched to anything. While there are ex-
ceptions, it is common that if B is larger than A,
then it cannot be entailed by A as it contains more
information.
while unused elements in A and B do
bestScore = 0
for a ? A, b ? B, a and b are unused do
if Score(a? b) > bestScore then
bestScore = Score(a? b)
end if
end for
total+ = bestScore
end while
return total
Figure 2: Pseudocode for mapping between two frag-
ments
The word-level entailment score Score(a ? b)
is directly used to estimate the entailment score be-
tween fragments, Score(A ? B). In this case we
are working with two levels ? fragments which in
turn consist of words. However, this can be extended
to a truly recursive method where fragments consist
of smaller fragments.
3.2 Extrinsic similarity
The extrinsic similarity between two fragments or
words is modelled using measures of directional dis-
tributional similarity. We define a context relation as
a tuple (a, d, r, a?) where a is the main word, a? is a
word connected to it through a dependency relation,
r is the label of that relation and d shows the direc-
tion of the relation. The tuple f : (d, r, a?) is referred
to as a feature of a.
To calculate the distributional similarity between
two fragments, we adopt an approach similar to
Weeds et al (2005). Using the previous notation,
(d, r, a?) is a feature of fragment A if (d, r, a?) is a
feature for a word which is contained inA. The gen-
eral algorithm for feature collection is as follows:
1. Find the next instance of a fragment in the
background corpus.
2. For each word in the fragment, find dependency
relations which connect to words not contained
in the fragment.
3. Count these dependency relations as distribu-
tional features for the fragment.
For example, in Figure 1 the fragment (* induces
* in *) has three features: (1, subj, B61), (1, dobj,
autophosphorylation) and (1, dobj, cell).
The BioMed Central2 corpus of full papers was
used to collect distributional similarity features for
each fragment. 1000 papers were randomly selected
and separated for constructing the test set, leaving
70821 biomedical full papers. These were tokenised
and parsed using the RASP system (Briscoe et al,
2006) in order to extract dependency relations.
We experimented with various schemes for fea-
ture weighting and found the best one to be a varia-
tion of Dice?s coefficient (Dice, 1945), described by
Curran (2003):
wA(f) =
2P (A, f)
P (A, ?) + P (?, f)
where wA(f) is the weight of feature f for fragment
A, P (?, f) is the probability of the feature appear-
ing in the corpus with any fragment, P (A, ?) is the
probability of the fragment appearing with any fea-
ture, and P (A, f) is the probability of the fragment
and the feature appearing together.
Different measures of distributional similarity,
both symmetrical and directonal, were also tested
and ClarkeDE (Clarke, 2009) was used for the fi-
nal system as it achieved the highest performance on
graph fragments:
ClarkeDE(A? B) =
?
f?FA?FB
min(wA(f), wB(f))
?
f?FA
wA(f)
where FA is the set of features for fragmentA and
wA(f) is the weight of feature f for fragment A. It
quantifies the weighted coverage of the features ofA
by the features of B by finding the sum of minimum
weights.
2http://www.biomedcentral.com/info/about/datamining/
13
The ClarkeDE similarity measure is designed to
detect whether the features of A are a proper subset
of the features of B. This works well for finding
more general versions of fragments, but not when
comparing fragments which are roughly equal para-
phrases. As a solution we constructed a new mea-
sure based on the symmetrical Lin measure (Lin,
1998).
LinD(A? B) =
?
f?FA?FB
[wA(f) + wB(f)]
?
f?FA
wA(f) +
?
f?FA?FB
wB(f)
Compared to the original, the features ofB which
are not found in A are excluded from the score
calculation, making the score non-symmetrical but
more balanced compared to ClarkeDE. We ap-
plied this for word-level distributional similarity and
achieved better results than with other common sim-
ilarity measures.
The LinD similarity is also calculated between
fragment levels to help detect possible paraphrases.
If the similarity is very high (greater than 85%), then
a symmetric relationship between the fragments is
assumed and the value of LinD is used as ExtSim.
Otherwise, the system reverts to the ClarkeDE
measure for handling unidirectional relations.
3.3 Hedging and negation
Language constructions such as hedging and nega-
tion typically invert the normal direction of an en-
tailment relation. For example, (biological discov-
ery? discovery) becomes (no biological discovery
? no discovery) and (is repressed by? is affected
by) becomes (may be repressed by? is affected by).
Such cases are handled by inverting the direction
of the score calculation if a fragment is found to
contain a special cue word that commonly indicates
hedged language or negation. In order to find the
list of indicative hedge cues, we analysed the train-
ing corpus of CoNLL 2010 Shared Task (Farkas et
al., 2010) which is annotated for speculation cues
and scopes. Any cues that occurred less than 5 times
or occurred more often as normal text than as cue
words were filtered out, resulting in the following
list:
(4) suggest, may, might, indicate that, appear,
likely, could, possible, whether, would, think,
seem, probably, assume, putative, unclear,
propose, imply, possibly
For negation cues we used the list collected by
Morante (2009):
(5) absence, absent, cannot, could not, either,
except, exclude, fail, failure, favor over,
impossible, instead of, lack, loss, miss,
negative, neither, nor, never, no, no longer,
none, not, rather than, rule out, unable, with
the exception of, without
This is a fast and basic method for estimating
the presence of hedging and negation in a fragment.
When dealing with longer texts, the exact scope of
the cue word should be detected, but for relatively
short fragments the presence of a keyword acts as a
good indication of hedging and negation.
4 Evaluation
A ?pilot? dataset was created to evaluate different
entailment detection methods between fragments3.
In order to look for valid entailment examples, 1000
biomedical papers from the BioMed Central full-text
corpus were randomly chosen and analysed. We
hypothesised that two very similar sentences orig-
inating from the same paper are likely to be more
and less general versions of the same proposition.
First, the similarities between all sentences in a sin-
gle paper were calculated using a bag-of-words ap-
proach. Then, ten of the most similar but non-
identical sentence pairs from each paper were pre-
sented for manual review and 150 fragment pairs
were created based on these sentences, 100 of which
were selected for the final set.
When applied to sentence-level entailment extrac-
tion, similar methods can suffer from high lexical
overlap as sentences need to contain many match-
ing words to pass the initial filter. However, for the
extraction of entailing fragments most of the match-
ing words are discarded and only the non-identical
fragments are stored, greatly reducing the overlap
problem. Experiments in Section 5 demonstrate
that a simple bag-of-words approach performs rather
poorly on the task, confirming that the extraction
method produces a diverse selection of fragments.
3http://www.cl.cam.ac.uk/~mr472/entailment/
14
Two annotators assigned a relation type to can-
didate pairs based on how well one fragment can
be substituted for the other in text while preserving
meaning (A ? B, A ? B, A ? B or A 6= B).
Cohen?s Kappa between the annotators was 0.88, in-
dicating very high agreement. Instances with dis-
agreement were then reviewed and replaced for the
final dataset.
Each fragment pair has two binary entailment de-
cisions (one in either direction) and the set is evenly
balanced, containing 100 entailment and 100 non-
entailment relations. An example sentence with the
first fragment is also included. Fragment sizes range
from 1 to 20 words, with the average of 2.86 words.
The system assigns a score to each entailment re-
lation, with higher values indicating higher confi-
dence in entailment. All the relations are ranked
based on their score, and average precision (AP) is
used to evaluate the performance:
AP =
1
R
N?
i=1
E(i)? CorrectUpTo(i)
i
where R is the number of correct entailment re-
lations, N is the number of possible relations in
the test set, E(i) is 1 if the i-th relation is en-
tailment in the gold standard and 0 otherwise, and
CorrectUpTo(i) is the number of correctly re-
turned entailment relations up to rank i. Average
precision assigns a higher score to systems which
rank correct entailment examples higher in the list.
As a secondary measure we also report the Break-
Even Point (BEP) which is defined as precision at
the rank where precision is equal to recall. Using
the previous annotation, this can also be calculated
as precision at rank R:
BEP =
CorrectUpTo(R)
R
BEP is a much more strict measure, treating the task
as binary classification and ignoring changes to the
ranks within the classes.
5 Results
The test set is balanced, therefore random guessing
would be expected to achieve an AP and BEP of
0.5 which can be regarded as the simplest (random)
baseline. Table 1 contains results for two more basic
approaches to the task. For the bag-of-words (BOW)
system, the score of A entailing B is the proportion
of words in B that are also contained in A.
Scorebow(A? B) =
|{b|b ? A,B}|
|{b|b ? B}|
We also tested entailment detection when using
only the directional distributional similarity between
fragments as it is commonly done for words. While
both of the systems perform better than random, the
results are much lower than those for more informed
methods. This indicates that even though there is
some lexical overlap between the fragments, it is not
enough to make good decisions about the entailment
relations.
System type AP BEP
Random baseline 0.500 0.500
BOW 0.657 0.610
Distributional similarity 0.645 0.480
Table 1: Results for basic approaches.
Table 2 contains the results for the system de-
scribed in Section 3. We start with the most basic
version and gradually add components. Using only
the intrinsic similarity, the system performs better
than any of the basic approaches, delivering 0.71 AP.
System type AP BEP
Intrinsic similarity only 0.710 0.680
+ Word ExtSim 0.754 0.710
+ Fragment ExtSim 0.801 0.710
+ Negation & hedging 0.831 0.720
+ Paraphrase check 0.841 0.720
Table 2: Results for the system described in Section 3.
Components are added incrementally.
Next, the extrinsic similarity between words is in-
cluded, raising the AP to 0.754. When the string-
level similarity fails, the added directional distri-
butional similarity helps in mapping semantically
equivalent words to each other.
The inclusion of extrinsic similarity between frag-
ments gives a further increase, with AP of 0.801.
The 4.5% increase shows that while fragments are
15
larger and occur less often in a corpus, their distribu-
tional similarity can still be used as a valuable com-
ponent to detect semantic similarity and entailment.
Checking for negation and hedge cues raises the
AP to 0.831. The performance is already high and
a 3% improvement shows that hedging and negation
affect fragment-level entailment and other compo-
nents do not manage to successfully capture this in-
formation.
Finally, applying the fragment-level check for
paraphrases with a more appropriate distributional
similarity measure, as described in Section 3.2, re-
turns an AP of 0.841. The results of this final con-
figuration are significantly different compared to the
initial system using only intrinsic similarity, accord-
ing to the Wilcoxon signed rank test at the level of
0.05.
The formula in Section 3 contains parameter ?
which can be tuned to adjust the balance of intrinsic
and extrinsic similarity. This can be done heuristi-
cally or through machine learning methods and dif-
ferent values can be used for fragments and words.
In order to investigate the effects of tuning on the
system, we tried all possible combinations of ?word
and ?fragment with values between 0 and 1 at incre-
ments of 0.05. Table 3 contains results for some of
these experiments.
?word ?fragment AP BEP
0.5 0.5 0.841 0.720
* 0.0 0.656 0.480
0.0 1.0 0.813 0.720
1.0 1.0 0.765 0.690
0.45 0.65 0.847 0.740
Table 3: Results of tuning the weights for intrinsic and
distributional similarity.
The best results were obtained with ?word = 0.45
and ?fragment = 0.65, resulting in 0.847 AP and
0.74 BEP. This shows that parameter tuning can im-
prove the results, but the 0.6% increase is modest
and a completely unsupervised approach can give
competitive results. In addition, the optimal values
of ? are close to 0.5, indicating that all four com-
ponents (intrinsic and distributional similarities be-
tween words and fragments) are all contributing to
the performance of the final system.
6 Previous work
Most work on entailment has focused on compar-
ing sentences or paragraphs. For example, Haghighi
et al (2005) represent sentences as directed depen-
dency graphs and use graph matching to measure se-
mantic overlap. This method also compares the de-
pendencies when calculating similarity, which sup-
ports incorporation of extra syntactic information.
Hickl et al (2006) combine lexico-syntactic features
and automatically acquired paraphrases to classify
entailing sentences. Lintean and Rus (2009) make
use of weighted dependencies and word semantics
to detect paraphrases. In addition to similarity they
look at dissimilarity between two sentences and use
their ratio as the confidence score for paraphrasing.
Lin and Pantel (2001) were one of the first to
extend the distributional hypothesis to dependency
paths to detect entailment between relations. Szpek-
tor et al (2004) describe the TEASE method for ex-
tracting entailing relation templates from the Web.
Szpektor and Dagan (2008) use the distributional
similarity of arguments to detect unary template en-
tailment, whilst Berant et al (2010) apply it to bi-
nary relations in focused entailment graphs.
Snow et al (2005) described a basic method of
syntactic pattern matching to automatically discover
word-level hypernym relations from text. The use of
directional distributional similarity measures to find
inference relations between single words is explored
by Kotlerman et al (2010). They propose new mea-
sures based on feature ranks and compare them with
existing ones for the tasks of lexical expansion and
text categorisation.
In contrast to current work, each of the ap-
proaches described above only focuses on detecting
entailment between specific subtypes of fragments
(either sentences, relations or words) and optimis-
ing the system for a single scenario. This means
only limited types of entailment relations are found
and they cannot be used for entailment generation
or compositional entailment detection as described
in Section 2.
MacCartney and Manning (2008) approach
sentence-level entailment detection by breaking the
problem into a sequence of atomic edits linking the
premise to the hypothesis. Entailment relations are
then predicted for each edit, propagated up through
16
a syntax tree and then used to compose the result-
ing entailment decision. However, their system fo-
cuses more on natural logic and uses a predefined set
of compositional rules to capture a subset of valid
inferences with high precision but low recall. It
also relies on a supervised classifier and information
from WordNet to reach the final entailment decision.
Androutsopoulos and Malakasiotis (2010) have
assembled a survey of different tasks and approaches
related to paraphrasing and entailment. They de-
scribe three different goals (paraphrase recogni-
tion, generation and extraction) and analyse various
methods for solving them.
7 Conclusion
Entailment detection systems are generally devel-
oped to work on specific text units ? either single
words, relations, or full sentences. While this re-
duces the complexity of the problem, it can also
lead to important information being disregarded. In
this paper we proposed a new task ? detecting en-
tailment relations between any kind of dependency
graph fragments. The definition of a fragment cov-
ers the language structures mentioned above and
also extends to others that have not been fully in-
vestigated in the context of entailment recognition
(such as multi-word constituents, predicates and ad-
juncts).
To perform entailment detection between various
types of dependency graph fragments, a new sys-
tem was built that combines the directional intrin-
sic and extrinsic similarities of two fragments to
reach a final score. Fragments which contain hedg-
ing or negation are identified and their score cal-
culation is inverted to better model the effect on
entailment. The extrinsic similarity is found with
two different distributional similarity measures, first
checking for symmetric similarity and then for di-
rectional containment of distributional features. The
system was evaluated on a manually constructed
dataset of fragment-level entailment relations from
the biomedical domain and each of the added meth-
ods improved the results.
Traditionally, the method for entailment recogni-
tion is chosen based on what appears optimal for
the task ? either structure matching or distributional
similarity. Our experiments show that the combina-
tion of both gives the best performance for all frag-
ment types. It is to be expected that single words will
benefit more from distributional measures while full
sentences get matched by their components. How-
ever, this separation is not strict and evidence from
both methods can be used to strengthen the decision.
The experiments confirmed that entailment be-
tween dependency graph fragments of various types
can be detected in a completely unsupervised set-
ting, without the need for specific resources or an-
notated training data. As our method can be applied
equally to any domain and requires only a large plain
text corpus, we believe it is a promising direction
for research in entailment detection. This can lead
to useful applications in biomedical information ex-
traction where manually annotated datasets are in
short supply.
We have shown that a unified approach can be
used to detect entailment relations between depen-
dency graph fragments. This allows for entail-
ment discovery among a wide range of fragment
types, including ones for which no specialised sys-
tems currently exist. The framework for fragment-
level entailment detection can be integrated into var-
ious applications that require identifying and rewrit-
ing semantically equivalent phrases - for example,
query expansion in IE and IR, text mining, sentence-
level entailment composition, relation extraction and
protein-protein interaction extraction.
References
Ion Androutsopoulos and Prodromos Malakasiotis.
2010. A survey of paraphrasing and textual entailment
methods. Journal of Artificial Intelligence Research,
38(7):135?187.
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo
Giampiccolo, Bernardo Magnini, and Idan Szpektor.
2006. The second pascal recognising textual entail-
ment challenge. In Proceedings of the Second PAS-
CAL Challenges Workshop on Recognising Textual
Entailment, pages 1?9. Citeseer.
Luisa Bentivogli, Elena Cabrio, Ido Dagan, Danilo Gi-
ampiccolo, Medea Lo Leggio, and Bernardo Magnini.
2010. Building Textual Entailment Specialized Data
Sets: a Methodology for Isolating Linguistic Phenom-
ena Relevant to Inference. In Proceedings of the Sev-
enth conference on International Language Resources
and Evaluation (LREC?10).
17
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2010. Global learning of focused entailment graphs.
In Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, number Section
6, pages 1220?1229. Association for Computational
Linguistics.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proceed-
ings of the COLING/ACL 2006 Interactive Presenta-
tion Sessions, number July, pages 77?80, Sydney, Aus-
tralia. Association for Computational Linguistics.
Daoud Clarke. 2009. Context-theoretic semantics for
natural language: an overview. In Proceedings of
the Workshop on Geometrical Models of Natural Lan-
guage Semantics, number March, pages 112?119. As-
sociation for Computational Linguistics.
James Richard Curran. 2003. From distributional to se-
mantic similarity. Ph.D. thesis, University of Edin-
burgh.
Lee R. Dice. 1945. Measures of the amount of ecologic
association between species. Ecology, 26(3):297?302.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja`nos
Csirik, and Gyo?rgy Szarvas. 2010. The CoNLL-2010
Shared Task: Learning to Detect Hedges and their
Scope in Natural Language Text. In Proceedings of
the Fourteenth Conference on Computational Natural
Language Learning ? Shared Task, pages 1?12. As-
sociation for Computational Linguistics.
Aria D. Haghighi, Andrew Y. Ng, and Christopher D.
Manning. 2005. Robust textual inference via graph
matching. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods in
Natural Language Processing, Morristown, NJ, USA.
Association for Computational Linguistics.
Andrew Hickl, Jeremy Bensley, John Williams, Kirk
Roberts, Bryan Rink, and Ying Shi. 2006. Recog-
nizing textual entailment with LCC?s GROUNDHOG
system. In Proceedings of the Second PASCAL Chal-
lenges Workshop.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distributional
similarity for lexical inference. Natural Language En-
gineering, 16(04):359?389.
Sandra Ku?bler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Synthesis Lectures on Hu-
man Language Technologies, 2:1?127.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question-answering. Natural Language
Engineering, 7(04):343?360.
Dekang Lin. 1998. Automatic retrieval and cluster-
ing of similar words. In Proceedings of the 17th in-
ternational conference on Computational linguistics-
Volume 2, pages 768?774. Association for Computa-
tional Linguistics.
Mihain C. Lintean and Vasile Rus. 2009. Para-
phrase Identification Using Weighted Dependencies
and Word Semantics. In Proceedings of the FLAIRS-
22, volume 22, pages 19?28.
Bill MacCartney and Christopher D. Manning. 2008.
Modeling semantic containment and exclusion in natu-
ral language inference. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics-
Volume 1, pages 521?528. Association for Computa-
tional Linguistics.
Roser Morante. 2009. Descriptive analysis of negation
cues in biomedical texts. In Proceedings of the Sev-
enth International Language Resources and Evalua-
tion (LREC10), pages 1429?1436.
Mark Sammons, V.G. Vinod Vydiswaran, and Dan Roth.
2010. Ask not what textual entailment can do for
you... In Proceedings of the Annual Meeting of the As-
sociation of Computational Linguistics (ACL), pages
1199?1208. Association for Computational Linguis-
tics.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In Advances in Neural Information Pro-
cessing Systems.
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary templates. In Proceedings of the
22nd International Conference on Computational Lin-
guistics - COLING ?08, pages 849?856, Morristown,
NJ, USA. Association for Computational Linguistics.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition of
entailment relations. In Proceedings of EMNLP, vol-
ume 4, pages 41?48.
Julie Weeds, David Weir, and Bill Keller. 2005. The dis-
tributional similarity of sub-parses. In Proceedings of
the ACL Workshop on Empirical Modeling of Semantic
Equivalence and Entailment, pages 7?12, Morristown,
NJ, USA. Association for Computational Linguistics.
18
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 68?77,
Baltimore, Maryland USA, June 26-27 2014.
c?2014 Association for Computational Linguistics
Looking for Hyponyms in Vector Space
Marek Rei
SwiftKey
95 Southwark Bridge Rd
London, UK
marek@swiftkey.net
Ted Briscoe
Computer Laboratory
University of Cambridge
Cambridge, UK
ted.briscoe@cl.cam.ac.uk
Abstract
The task of detecting and generating hy-
ponyms is at the core of semantic under-
standing of language, and has numerous
practical applications. We investigate how
neural network embeddings perform on
this task, compared to dependency-based
vector space models, and evaluate a range
of similarity measures on hyponym gener-
ation. A new asymmetric similarity mea-
sure and a combination approach are de-
scribed, both of which significantly im-
prove precision. We release three new
datasets of lexical vector representations
trained on the BNC and our evaluation
dataset for hyponym generation.
1 Introduction
Hyponymy is a relation between two word senses,
indicating that the meaning of one word is also
contained in the other. It can be thought of as a
type-of relation; for example car, ship and train
are all hyponyms of vehicle. We denote a hy-
ponymy relation between words a and b as (a? b),
showing that a is a hyponym of b, and b is a hyper-
nym of a. Hyponymy relations are closely related
to the concept of entailment, and this notation is
consistent with indicating the direction of infer-
ence ? if a is true, b must be true as well.
Automatic detection and generation of hy-
ponyms has many practical applications in nearly
all natural language processing tasks. Information
retrieval, information extraction and question an-
swering can be improved by performing appropri-
ate query expansions. For example, a user search-
ing for arthritis treatment is most likely also inter-
ested in results containing the hyponyms of treat-
ment, such as arthritis therapy, arthritis medica-
tion, and arthritis rehabilitation. Summarisation
systems can increase coherence and reduce repe-
tition by correctly handling hyponymous words in
the input text. Entailment and inference systems
can improve sentence-level entailment resolution
by detecting the presence and direction of word-
level hyponymy relations. Distributionally simi-
lar words have been used for smoothing language
models and word co-occurrence probabilities (Da-
gan et al., 1999; Weeds and Weir, 2005), and hy-
ponyms can be more suitable for this application.
We distinguish between three different tasks
related to hyponyms. Given a directional word
pair, the goal of hyponym detection is to deter-
mine whether one word is a hyponym of the other
(Zhitomirsky-Geffet and Dagan, 2009; Kotlerman
et al., 2010; Baroni and Lenci, 2011). In con-
trast, hyponym acquisition is the task of extract-
ing all possible hyponym relations from a given
text (Hearst, 1992; Caraballo, 1999; Pantel and
Ravichandran, 2004; Snow et al., 2005). Such sys-
tems often make use of heuristic rules and patterns
for extracting relations from surface text, and pop-
ulate a database with hyponymous word pairs. Fi-
nally, the task of hyponym generation is to re-
turn a list of all possible hyponyms, given only
a single word as input. This is most relevant to
practical applications, as many systems require a
set of appropriate substitutes for a specific term.
Automated ontology creation (Biemann, 2005) is
a related field that also makes use of distributional
similarity measures. However, it is mostly focused
on building prototype-based ontologies through
clustering (Ushioda, 1996; Bisson et al., 2000;
Wagner, 2000; Paa? et al., 2004; Cimiano and
Staab, 2005), and is not directly applicable to hy-
ponym generation.
While most work has been done on hyponym
detection (and the related task of lexical substitu-
tion), barely any evaluation has been done for hy-
ponym generation. We have found that systems for
hyponym detection often perform poorly on hy-
ponym generation, as the latter requires returning
results from a much less restricted candidate set,
68
and therefore a task-specific evaluation is required.
In this paper we focus on hyponym generation
and approach it by scoring a very large candidate
set of potential hyponyms. Distributional similar-
ity methods are especially interesting for this task,
as they can be easily applied to different domains,
genres and languages without requiring annotated
training data or manual pattern construction. We
perform a systematic comparison of different vec-
tor space models and similarity measures, in order
to better understand the properties of a successful
method for hyponym generation.
The main contributions of this paper are:
1. Systematic evaluation of different vector
space models and similarity measures on the
task of hyponym generation.
2. Proposal of new properties for modelling the
directional hyponymy relation.
3. Release of three lexical vector datasets,
trained using neural network, window-based,
and dependency-based features.
2 Vector space models
In order to use similarity measures for hyponym
detection, every word needs to be mapped to a
point in vector space. The method of choosing
appropriate features for these vectors is crucial to
achieving the optimal performance. We compare
five different approaches:
Window: As a simple baseline, we created vec-
tors by counting word co-occurrences in a fixed
context window. Every word that occurs within a
window of three words before or after is counted
as a feature for the target word. Pointwise mutual
information is then used for weighting.
CW: Collobert and Weston (2008) constructed
a neural network language model that is trained to
predict the next word in the sequence, and simul-
taneously learns vector representations for each
word. The vectors for context words are concate-
nated and used as input for the neural network,
which uses a sample of possible outputs for gra-
dient calculation to speed up the training process.
Turian et al. (2010) recreated their experiments
and made the vectors available online.
1
HLBL: Mnih and Hinton (2007) created word
representations using the hierarchical log-bilinear
1
http://metaoptimize.com/projects/wordreprs/
model ? a neural network that takes the concate-
nated vectors of context words as input, and is
trained to predict the vector representation of the
next word, which is then transformed into a prob-
ability distribution over possible words. To speed
up training and testing, they use a hierarchical data
structure for filtering down the list of candidates.
Both CW and HLBL vectors were trained using
37M words from RCV1.
Word2vec: We created word representations
using the word2vec
2
toolkit. The tool is based
on a feedforward neural network language model,
with modifications to make representation learn-
ing more efficient (Mikolov et al., 2013a). We
make use of the skip-gram model, which takes
each word in a sequence as an input to a log-linear
classifier with a continuous projection layer, and
predicts words within a certain range before and
after the input word. The window size was set to
5 and vectors were trained with both 100 and 500
dimensions.
Dependencies: Finally, we created vector rep-
resentations for words by using dependency rela-
tions from a parser as features. Every incoming
and outgoing dependency relation is counted as a
feature, together with the connected term. For ex-
ample, given the dependency relation (play, dobj,
guitar), the tuple (>dobj, guitar) is extracted as a
feature for play, and (<dobj, play) as a feature for
guitar. We use only features that occur more than
once in the dataset, and weight them using point-
wise mutual information to construct feature vec-
tors for every term. Features with negative weights
were retained, as they proved to be beneficial for
some similarity measures.
The window-based, dependency-based and
word2vec vector sets were all trained on 112M
words from the British National Corpus, with pre-
processing steps for lowercasing and lemmatis-
ing. Any numbers were grouped and substituted
by more generic tokens. For constructing the
dependency-based vector representations, we used
the parsed version of the BNC created by Ander-
sen et al. (2008) with the RASP toolkit (Briscoe
et al., 2006). When saved as plain text, the 500-
dimensional word2vec vectors and dependency-
based vectors are comparable in size (602MB and
549MB), whereas the window-based vectors are
twice as large (1,004MB). We make these vector
2
https://code.google.com/p/word2vec/
69
sets publically available for download.
3
Recently, Mikolov et al. (2013b) published in-
teresting results about linguistic regularities in
vector space models. They proposed that the rela-
tionship between two words can be characterised
by their vector offset, for example, we could find
the vector for word ?queen? by performing the op-
eration ?king - man + woman? on corresponding
vectors. They also applied this approach to hy-
ponym relations such as (shirt ? clothing) and
(bowl ? dish). We evaluate how well this method
applies to hyponym generation with each of the
vector space models mentioned above. Using the
training data, we learn a vector for the hyponymy
relation by averaging over all the offset vectors
for hyponym-hypernym pairs. This vector is then
added to the hypernym during query time, and
the result is compared to hyponym candidates us-
ing cosine similarity. For sparse high-dimensional
vector space models it was not feasible to use the
full offset vector during experiments, therefore we
retain only the top 1,000 highest-weighted fea-
tures.
3 Similarity measures
We compare the performance of a range of simi-
larity measures, both directional and symmetrical,
on the task of hyponym generation.
Cosine similarity is defined as the angle be-
tween two feature vectors and has become a stan-
dard measure of similarity between weighted vec-
tors in information retrieval (IR).
Lin similarity, created by Lin (1998), uses the
ratio of shared feature weights compared to all fea-
ture weights. It measures the weighted proportion
of features that are shared by both words.
DiceGen2 is one possible method for generalis-
ing the Dice measure to real-valued weights (Cur-
ran, 2003; Grefenstette, 1994). The dot product of
the weight vectors is normalised by the total sum
of all weights. The same formula can also be con-
sidered as a possible generalisation for the Jaccard
measure.
WeedsPrec and WeedsRec were proposed by
Weeds et al. (2004) who suggested using precision
and recall as directional measures of word simi-
larity. In this framework, the features are treated
similarly to retrieved documents in information re-
trieval ? the vector of the broader term b is used as
the gold standard, and the vector of the narrower
3
http://www.marekrei.com/projects/vectorsets/
term a is in the role of retrieval results. Precision
is then calculated by comparing the intersection
(items correctly returned) to the values of the nar-
rower term only (all items returned). In contrast,
WeedsRec quantifies how well the features of the
breader term are covered by the narrower term.
Balprec is a measure created by Szpektor and
Dagan (2008). They proposed combining Weed-
sPrec together with the Lin measure by taking
their geometric average. This aims to balance the
WeedsPrec score, as the Lin measure will penalise
cases where one vector contains very few features.
ClarkeDE, proposed by Clarke (2009), is an
asymmetric degree of entailment measure, based
on the concept of distributional generality (Weeds
et al., 2004). It quantifies the weighted coverage of
the features of the narrower term a by the features
of the broader term b.
BalAPInc, a measure described by Kotlerman
et al. (2010), combines the APInc score with Lin
similarity by taking their geometric average. The
APInc measure finds the proportion of shared fea-
tures relative to the features for the narrower term,
but this can lead to unreliable results when the
number of features is very small. The motivation
behind combining these measures is that the sym-
metric Lin measure will decrease the final score
for such word pairs, thereby balancing the results.
4 Properties of a directional measure
Finding similar words in a vector space, given
a symmetric similarity measure, is a relatively
straightforward task. However finding hyponyms
is arguably more difficult, as the relation is asym-
metric, and looking at the distance or angle be-
tween the two words may not be enough.
Kotlerman et al. (2010) investigate the related
problem of detecting directional lexical entail-
ment, and they propose three desirable properties
that a directional distributional similarity measure
should capture:
1. The relevance of the shared features to the
narrower term.
2. The relevance of the shared features to the
broader term.
3. That relevance is less reliable if the num-
ber of features of either the narrower or the
broader term is small.
70
Given a term pair (a ? b) we refer to a as the
narrower term and b as the broader term. The fea-
tures of a that are also found in b (have non-zero
weights for both a and b) are referred to as shared
features.
They show that existing measures which cor-
respond to these criteria perform better and con-
struct the BalAPInc measure based on the princi-
ples. However, it is interesting to note that these
properties do not explicitly specify any directional
aspects of the measure, and symmetric similarity
scores can also fulfil the requirements.
Based on investigating hyponym distributions
in our training data, we suggest two additions to
this list of desired properties, one of which specif-
ically targets the asymmetric properties of the de-
sired similarity measures:
4. The shared features are more important to
the directional score calculation, compared to
non-shared features.
5. Highly weighted features of the broader term
are more important to the score calculation,
compared to features of the narrower term.
Most existing directional similarity scores mea-
sure how many features of the narrower term are
present for the broader term. If a entails b, then
it is assumed that the possible contexts of a are a
subset of contexts for b, but b occurs in a wider
range of contexts compared to a. This intuition is
used by directional measures such as ClarkeDE,
WeedsPrec and BalAPInc. In contrast, we found
that many features of the narrower term are often
highly specific to that term and do not generalise
even to hypernyms. Since these features have a
very high weight for the narrower term, their ab-
sence with the broader term will have a big nega-
tive impact on the similarity score.
We hypothesise that many terms have certain
individual features that are common to them but
not to other related words. Since most weighting
schemes reward high relative co-occurrence, these
features are also likely to receive high weights.
Therefore, we suggest that features which are not
found for both terms should have a decreased im-
pact on the score calculation, as many of them are
not expected to be shared between hyponyms and
hypernyms. However, removing them completely
is also not advisable, as they allow the measure
to estimate the overall relative importance of the
shared features to the specific term.
We also propose that among the shared features,
those ranked higher for the broader term are more
important to the directional measure. In the hy-
ponymy relation (a ? b), the term b is more gen-
eral and covers a wider range of semantic con-
cepts. This also means it is more likely to be
used in contexts that apply to different hyponyms
of b. For example, some of the high-ranking fea-
tures for food are blandly-flavoured, high-calorie
and uneaten. These are properties that co-occur
often with the term food, but can also be applied
to most hyponyms of food. Therefore, we hypoth-
esise that the presence of these features for the nar-
rower term is a good indication of a hyponymy re-
lation. This is somewhat in contrast to most previ-
ous work, where the weights of the narrower term
have been used as the main guideline for similarity
calculation.
5 Weighted cosine
We now aim to construct a similarity measure that
follows all five of the properties mentioned above.
Cosine similarity is one of the symmetric similar-
ity measures which corresponds to the first three
desired properties, and our experiments showed
that it performs remarkably well at the task of hy-
ponym generation. Therefore, we decided to mod-
ify cosine similarity to also reflect the final two
properties and produce a more appropriate asym-
metric score.
The standard feature vectors for each word con-
tain weights indicating how important this feature
is to the word. We specify additional weights that
measure how important the feature is to that spe-
cific directional relation between the two terms.
Weighted cosine similarity, shown in Table 1, can
then be used to calculate a modified similarity
score. F
a
denotes the set of weighted features for
word a, w
a
(f) is the weight of feature f for word
a, and z(f) is the additional weight for feature f ,
given the directional word pair (a, b).
Based on the new desired properties we want
to downweight the importance of features that are
not present for both terms. For this, we choose
the simple solution of scaling them with a small
constant C ? [0, 1]. Next, we also want to assign
higher z(f) values to the shared features that have
high weights for the broader term b. We use the
relative rank of feature f in F
b
, r
b
(f), as the indi-
cator of its importance and scale this value to the
range from C to 1. This results in the importance
71
WeightedCosine(F
a
, F
b
) =
?
f?F
a
?F
b
(z(f)?w
a
(f))?(z(f)?w
b
(f))
??
f?F
a
(z(f)?w
a
(f))
2
?
??
f?F
b
(z(f)?w
b
(f))
2
z(f) =
{
(1?
r
b
(f)
|F
b
|+1
)? (1? C) + C if f ? F
a
? F
b
C otherwise
Table 1: Weighted cosine similarity measure
function decreasing linearly as the rank number
increases, but the weights for the shared features
always remain higher compared to the non-shared
features. Tied feature values are handled by as-
signing them the average rank value. Adding 1
to the denominator of the relative rank calculation
avoids exceptions with empty vectors, and also en-
sures that the value will always be strictly greater
than C. While the basic function is still the sym-
metric cosine, the z(f) values will be different de-
pending on the order of the arguments.
The parameter C controls the relative impor-
tance of the ?unimportant? features to the direc-
tional relation. Setting it to 0 will ignore these
features completely, while setting it to 1 will result
in the traditional cosine measure. Experiments on
the development data showed that the exact value
of this parameter is not very important, as long as
it is not too close to the extreme values of 0 or 1.
We use the value C = 0.5 for reporting our results,
meaning that the non-shared features are half as
important, compared to the shared features.
6 Dataset
As WordNet (Miller, 1995) contains numerous
manually annotated hyponymy relations, we can
use it to construct suitable datasets for evaluat-
ing hyponym generation. While WordNet terms
are annotated with only the closest hyponyms, we
are considering all indirect/inherited hyponyms
to be relevant ? for example, given relations
(genomics ? genetics) and (genetics ? biology),
then genomics is also regarded as a hyponym of
biology. WordNet relations are defined between
synsets, but we refrain from the task of word sense
disambiguation and count word a as a valid hy-
ponym for word b if it is valid for any sense of b.
Synonymy can be thought of as a symmetric is-
a relation, and most real-world applications would
require synonyms to also be returned, together
with hyponyms. Therefore, in our dataset we con-
sider synonyms as hyponyms in both directions.
We also performed experiments without synonyms
and found that this had limited effect on the re-
sults ? while the accuracy of all similarity mea-
sures slightly decreased (due to fewer numbers of
correct answers), the relative ranking remained the
same. As shown in the next section, the number of
synonyms is typically small compared to the num-
ber of all inherited hyponyms.
To construct the dataset, we first found all
single-word nouns in WordNet that are contained
at least 10 times in the British National Corpus
(BNC). Next, we retained only words that have
at least 10 hyponyms, such that they occur 10 or
more times in the BNC. This selection process
aims to discard WordNet hypernyms that are very
rare in practical use, and would not have enough
examples for learning informative vector represen-
tations. The final dataset contains the remaining
terms, together with all of their hyponyms, includ-
ing the rare/unseen hyponyms. As expected, some
general terms, such as group or location, have a
large number of inherited hyponyms. On average,
each hypernym in the dataset has 233 hyponyms,
but the distribution is roughly exponential, and the
median is only 36.
In order to better facilitate future experiments
with supervised methods, such as described by Ba-
roni et al. (2012), we randomly separated the data
into training (1230 hypernyms), validation (922),
and test (922) sets, and we make these datasets
publically available online.
4
7 Experiments
We evaluate how well different vector space mod-
els and similarity measures perform on the task of
hyponym generation. Given a single word as in-
put, the system needs to return a ranked list of
words with correct hyponyms at the top. As the
list of candidates for scoring we use all words in
the BNC that occur at least 10 times (a total of
86,496 words). All the experiments are performed
using tokenised and lemmatised words.
As the main evaluation measure, we report
4
http://www.marekrei.com/projects/hypgen/
72
Cosine Cosine+offset
MAP P@1 P@5 MAP P@1 P@5
Window 2.18 19.76 12.20 2.19 19.76 12.25
CW-100 0.66 3.80 3.21 0.59 3.91 2.89
HLBL-100 1.01 10.31 6.04 1.01 10.31 6.06
Word2vec-100 1.78 15.96 10.12 1.50 12.38 8.71
Word2vec-500 2.06 19.76 11.92 1.77 17.05 10.71
Dependencies 2.73 25.41 14.90 2.73 25.52 14.92
Table 2: Experiments using different vector space models for hyponym generation on the test set. We
report results using regular cosine similarity and the vector offset method described in Section 2.
Mean Average Precision (MAP), which averages
precision values at various recall points in the re-
turned list. It combines both precision and recall,
as well as the quality of the ranking, into a sin-
gle measure, and is therefore well-suited for com-
paring different methods. The reported MAP val-
ues are very low ? this is due to many rare Word-
Net hyponyms not occurring in the candidate set,
for which all systems are automatically penalised.
However, this allows us to evaluate recall, making
the results comparable between different systems
and background datasets. We also report precision
at top-1 and top-5 returned hyponyms.
As a baseline we report the results of a tra-
ditional hyponym acquisition system. For this,
we implemented the pattern-based matching pro-
cess described by Hearst (1992), and also used by
Snow et al. (2005). These patterns look for ex-
plicit examples of hyponym relations mentioned
in the text, for example:
X such as {Y
1
, Y
2
, ... , (and|or)} Y
n
where X will be extracted as the hypernym, and Y
1
to Y
n
as hyponyms. We ran the patterns over the
BNC and extracted 21,704 hyponym pairs, which
were then ranked according to the number of times
they were found.
7.1 Evaluation of vector spaces
Table 2 contains experiments with different vector
space models. We report here results using cosine,
as it is an established measure and a competitive
baseline. For our task, the HLBL vectors perform
better than CW vectors, even though they were
trained on the same data. Both of them are out-
performed by word2vec-100 vectors, which have
the same dimensionality but are trained on much
more text. Increasing the dimensionality with
word2vec-500 gives a further improvement. In-
terestingly, the simple window-based vectors per-
form just as well as the ones trained with neural
networks. However, the advantage of word2vec-
500 is that the representations are more compact
and require only about half the space. Finally,
the dependency-based vectors outperform all other
vector types, giving 2.73% MAP and 25.41% pre-
cision at the top-ranked result. While the other
models are built by using neighbouring words as
context, this model looks at dependency relations,
thereby taking both semantic and syntactic roles
into account. The results indicate that word2vec
and window-based models are more suitable when
the general topic of words needs to be captured,
whereas dependency-based vectors are preferred
when the task requires both topical and functional
similarity between words. Our experiments also
included the evaluation of other similarity mea-
sures on different vector space models, and we we
found these results to be representative.
Contrary to previous work, the vector offset
method, described in Section 2, did not pro-
vide substantial improvements on the hyponym
generation task. For the neural network-based
vectors this approach generally decreased perfor-
mance, compared to using direct cosine similar-
ity. There are some marginal improvements for
window and dependency-based models. Unfortu-
nately, the original work did not include baseline
performance using cosine similarity, without ap-
plying vector modifications. It is possible that this
method does not generalise to all word relations
equally well. As part of future work, it is worth
exploring if a hypernym-specific strategy of se-
lecting training examples could improve the per-
formance.
73
Validation Test
MAP P@1 P@5 MAP P@1 P@5
Pattern-based 0.53 7.06 4.58 0.51 8.14 4.45
Cosine 2.48 21.06 12.96 2.73 25.41 14.90
Lin 1.87 16.50 10.75 2.01 21.17 12.23
DiceGen2 2.27 18.57 12.62 2.44 21.82 14.55
WeedsPrec 0.13 0.00 0.09 0.12 0.11 0.04
WeedsRec 0.72 0.33 2.45 0.69 0.54 2.41
BalPrec 1.78 15.31 10.55 1.88 17.48 11.34
ClarkeDE 0.23 0.00 0.02 0.24 0.00 0.09
BalAPInc 1.64 14.22 9.12 1.68 15.85 9.66
WeightedCosine 2.59 21.39 13.59 2.85 25.84 15.46
Combined 3.27 23.02 16.09 3.51 27.69 18.02
Table 3: Evaluation of different vector similarity measures on the validation and test set of hyponym
generation. We report Mean Average Precision (MAP), precision at rank 1 (P@1), and precision at rank
5 (P@5).
7.2 Evaluation of similarity measures
Table 3 contains experiments with different sim-
ilarity measures, using the dependency-based
model, and Table 4 contains sample output from
the best system. The results show that the pattern-
based baseline does rather poorly on this task.
MAP is low due to the system having very lim-
ited recall, but higher precision at top ranks would
have been expected. Analysis showed that this
system was unable to find any hyponyms for more
than half (513/922) of the hypernyms in the vali-
dation set, leading to such poor recall that it also
affects Precision@1. While the pattern-based sys-
tem did extract a relatively large number of hy-
ponyms from the corpus (21,704 pairs), these are
largely concentrated on a small number of hyper-
nyms (e.g., area, company, material, country) that
are more likely to be mentioned in matching con-
texts.
Cosine, DiceGen2 and Lin ? all symmetric
similarity measures ? perform relatively well on
this task, whereas established directional measures
perform unexpectedly poorly. This can perhaps be
explained by considering the distribution of hy-
ponyms. Given a word, the most likely candi-
dates for a high cosine similarity are synonyms,
antonyms, hypernyms and hyponyms of that word
? these are words that are likely to be used in simi-
lar topics, contexts, and syntactic roles. By def-
inition, there are an equal number of hyponym
and hypernym relations in WordNet, but this ra-
tio changes rapidly as we remove lower-frequency
words. Figure 1 shows the number of relations ex-
tracted from WordNet, as we restrict the minimum
frequency of the main word. It can be seen that the
number of hyponyms increases much faster com-
pared to the other three relations. This also applies
to real-world data ? when averaging over word in-
stances found in the BNC, hyponyms cover 85% of
these relations. Therefore, the high performance
of cosine can be explained by distributionally sim-
ilar words having a relatively high likelihood of
being hyponyms.
0 10 20 30 40 50 60 70 80 90 100
0
20
40
60
80
100
hyponyms hypernyms
synonyms antonyms
min freq
avg
 re
late
d w
ord
s
Figure 1: Average number of different relations
per word in WordNet, as we restrict the minimum
word frequency.
One possible reason for the poor performance
of directional measures is that most of them quan-
tify how well the features of the narrower term are
included in the broader term. In contrast, we found
that for hyponym generation it is more important
to measure how well the features of the broader
term are included in the narrower term. This
74
scientist researcher, biologist, psychologist, economist, observer, physicist, sociologist
sport football, golf, club, tennis, athletics, rugby, cricket, game, recreation, entertainment
treatment therapy, medication, patient, procedure, surgery, remedy, regimen, medicine
Table 4: Examples of top results using the combined system. WordNet hyponyms are marked in bold.
is supported by WeedsRec outperforming Weed-
sPrec, although the opposite was intended by their
design.
Another explanation for the low performance
is that these directional measures are often devel-
oped in an artificial context. For example, Kotler-
man et al. (2010) evaluated lexical entailment de-
tection on a dataset where the symmetric Lin sim-
ilarity measure was used to select word pairs for
manual annotation. This creates a different task,
as correct terms that do not have a high symmetric
similarity will be excluded from evaluation. The
BalAPInc measure performed best in that setting,
but does not do as well for hyponym generation,
where candidates are filtered only based on mini-
mum frequency.
The weighted cosine measure, proposed in Sec-
tion 5, outperformed all other similarity measures
on both hyponym generation datasets. The im-
provement over cosine is relatively small; how-
ever, it is consistent and the improvement in MAP
is statistically significant on both datasets (p <
0.05), using the Approximate Randomisation Test
(Noreen, 1989; Cohen, 1995) with 10
6
iterations.
This further supports the properties of a directional
similarity measure described in Section 4.
Finally, we created a new system by combining
together two separate approaches: the weighted
cosine measure using the dependency-based vec-
tor space, and the normal cosine similarity using
word2vec-500 vectors. We found that the former
is good at modelling the grammatical roles and di-
rectional containment, whereas the latter can pro-
vide useful information about the topic and seman-
tics of the word. Turney (2012) also demonstrated
the importance of both topical (domain) and func-
tional vector space models when working with se-
mantic relations. We combined these approaches
by calculating both scores for each word pair and
taking their geometric average, or 0 if it could not
be calculated. This final system gives considerable
improvements across all evaluation metrics, and is
significantly (p < 0.05) better compared to cosine
or weighted cosine methods individually. Table 4
contains some example output from this system.
8 Conclusion
Hyponym generation has a wide range of pos-
sible applications in NLP, such as query expan-
sion, entailment detection, and language model
smoothing. Pattern-based hyponym acquisition
can be used to find relevant hyponyms, but these
approaches rely on both words being mentioned
together in a specific context, leading to very low
recall. Vector similarity methods are interesting
for this task, as they can be easily applied to differ-
ent domains and languages without any supervised
learning or manual pattern construction. We cre-
ated a dataset for evaluating hyponym generation
systems and experimented with a range of vector
space models and similarity measures.
Our results show that choosing an appropriate
vector space model is equally important to using a
suitable similarity measure. We achieved the high-
est performance using dependency-based vector
representations, which outperformed neural net-
work and window-based models. Symmetric sim-
ilarity measures, especially cosine similarity, per-
formed surprisingly well on this task. This can
be attributed to an unbalanced distribution of hy-
ponyms, compared to other high-similarity words.
The choice of vector space can be highly depen-
dent on the specific task, and we have made avail-
able our vector datasets created from the same
source using three different methods.
We proposed two new properties for detecting
hyponyms, and used them to construct a new di-
rectional similarity measure. This weighted co-
sine measure significantly outperformed all others,
showing that a theoretically-motivated directional
measure is still the most accurate method for mod-
elling hyponymy relations. Finally, we combined
together two different methods, achieving further
substantial improvements on all evaluation met-
rics.
References
?istein E. Andersen, Julien Nioche, Edward J. Briscoe,
and John Carroll. 2008. The BNC parsed with
RASP4UIMA. In Proceedings of the Sixth Interna-
75
tional Language Resources and Evaluation Confer-
ence (LREC08), Marrakech, Morocco.
Marco Baroni and Alessandro Lenci. 2011. How
we BLESSed distributional semantic evaluation. In
Proceedings of the GEMS 2011 Workshop on GE-
ometrical Models of Natural Language Semantics,
Edinburgh.
Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do,
and Chung-chieh Shan. 2012. Entailment above the
word level in distributional semantics. In Proceed-
ings of the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 23?32.
Chris Biemann. 2005. Ontology learning from text: A
survey of methods. LDV Forum, 20(2002):75?93.
Gilles Bisson, Claire N?edellec, and Dolores Ca?namero.
2000. Designing clustering methods for ontology
building-The Mo?K workbench. In ECAI Ontology
Learning Workshop.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Pro-
ceedings of the COLING/ACL 2006 Interactive Pre-
sentation Sessions, number July, pages 77?80, Syd-
ney, Australia. Association for Computational Lin-
guistics.
Sharon A. Caraballo. 1999. Automatic construction of
a hypernym-labeled noun hierarchy from text. Pro-
ceedings of the 37th annual meeting of the Asso-
ciation for Computational Linguistics on Computa-
tional Linguistics, pages 120?126.
Philipp Cimiano and Steffen Staab. 2005. Learning
concept hierarchies from text with a guided hierar-
chical clustering algorithm. In ICML-Workshop on
Learning and Extending Lexical Ontologies by using
Machine Learning Methods.
Daoud Clarke. 2009. Context-theoretic semantics for
natural language: an overview. In Proceedings of
the Workshop on Geometrical Models of Natural
Language Semantics, number March, pages 112?
119. Association for Computational Linguistics.
Paul R Cohen. 1995. Empirical Methods for Artificial
Intelligence. The MIT Press, Cambridge, MA.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. Proceed-
ings of the 25th international conference on Ma-
chine learning.
James R. Curran. 2003. From distributional to seman-
tic similarity. Ph.D. thesis, University of Edinburgh.
Ido Dagan, Lillian Lee, and Fernando C. N. Pereira.
1999. Similarity-based models of word cooccur-
rence probabilities. Machine Learning, 31:1?31.
Gregory Grefenstette. 1994. Explorations in Auto-
matic Thesaurus Discovery. Kluwer Academic Pub-
lishers, Norwell, MA, USA.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings
of the 14th conference on Computational linguistics
(COLING ?92), number July, page 539, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribu-
tional similarity for lexical inference. Natural Lan-
guage Engineering, 16(04):359?389.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 17th inter-
national conference on Computational linguistics-
Volume 2, pages 768?774. Association for Compu-
tational Linguistics.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient Estimation of Word Repre-
sentations in Vector Space. ICLR Workshop, pages
1?12.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic Regularities in Continuous Space
Word Representations. (June):746?751.
George A. Miller. 1995. WordNet: a lexical
database for English. Communications of the ACM,
38(11):39?41.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
Proceedings of the 24th international conference on
Machine learning - ICML ?07, pages 641?648.
Eric W. Noreen. 1989. Computer Intensive Methods
for Testing Hypotheses: An Introduction. Wiley,
New York.
Gerhard Paa?, J?org Kindermann, and Edda Leopold.
2004. Learning prototype ontologies by hierachical
latent semantic analysis.
Patrick Pantel and Deepak Ravichandran. 2004. Auto-
matically labeling semantic classes. In Proceedings
of HLT/NAACL.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In Advances in Neural Information Pro-
cessing Systems.
Idan Szpektor and Ido Dagan. 2008. Learning en-
tailment rules for unary templates. In Proceedings
of the 22nd International Conference on Computa-
tional Linguistics (COLING ?08), pages 849?856,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
76
Joseph Turian, Lev Ratinov, and Y Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics.
Peter D. Turney. 2012. Domain and function: A dual-
space model of semantic relations and compositions.
Journal of Artificial Intelligence Research, 44:533?
585.
Akira Ushioda. 1996. Hierarchical clustering of words
and application to NLP tasks. In Fourth Workshop
on Very Large Corpora, pages 28?41.
Andreas Wagner. 2000. Enriching a lexical semantic
net with selectional preferences by means of statisti-
cal corpus analysis. In ECAI Workshop on Ontology
Learning.
Julie Weeds and David Weir. 2005. Co-occurrence
retrieval: A flexible framework for lexical distribu-
tional similarity. Computational Linguistics.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional
similarity. Proceedings of the 20th international
conference on Computational Linguistics - COLING
?04.
Maayan Zhitomirsky-Geffet and Ido Dagan. 2009.
Bootstrapping Distributional Feature Vector Quality.
Computational Linguistics, 35(3):435?461, Septem-
ber.
77
