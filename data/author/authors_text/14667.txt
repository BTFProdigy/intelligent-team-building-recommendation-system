Proceedings of the Third ACL-SIGSEM Workshop on Prepositions, pages 51?56,
Trento, Italy, April 2006. c?2006 Association for Computational Linguistics
A Multilingual Analysis of the Notion of Instrumentality
Asanee Kawtrakul, Mukda Suktarachan (Kasetsart univ. Bangkok, Thailand),
Bali Ranaivo-Malancon, Pek Kuan Ng, (Univ. Sains Malaysia, Penang, Malaysia),
Achla Raina (IIT Kanpur, India),
Sudeshna Sarkar (IIT Kharagpur, India),
Alda Mari (Enst-Cnrs, Paris, France),
Sina Zarriess (Universita?t Potsdam, Germany),
Elixabete Murguia (Univ. Deusto, Bilbao, Spain),
Patrick Saint-Dizier (Irit-Cnrs, Toulouse, France)
Abstract
Instruments are expressed in language by
various means: prepositions, postposi-
tions, affixes including case marks, nonfi-
nite verbs, etc. We consider here 12 lan-
guages from five families in order to be
able to identify the different meaning com-
ponents that structure instrumentality.
1 Credits
This work has been made possible partly via the
STIC-Asia cooperation framework.
2 Introduction
It is difficult to give a comprehensive definition of
what instrumentality is. In WordNet it is defined
as ?an artifact, or a set of artifacts, that are instru-
mental (i.e. behave as instruments) in accomplish-
ing some end?, i.e. reaching a certain goal. In
this definition, the triple relation agent-instrument-
goal (as in: John cuts the bread with a knife, where
John is agent, knife is instrument that does the cut-
ting, and bread cut is the goal), is left vague in
what concerns the exact involvement of the agent
and the instrument in the action, and the control
the agent has on the instrument and on the action
(Mari and St-Dizier 01).
If almost anything can be an instrument, we can
nevertheless formulate a few criteria that we will
try to elaborate in this paper. First, an instrument
is basically non volitional. When humans play the
role of instruments, they are obviously volitional,
but the action is controlled by another agent who
acts as an ?initiator agent?, taking the initiative of
the action. Instruments cannot be easily associated
with traditional thematic roles, in fact this is not
of much interest, because this is too superficial a
notion and also because instruments are generally
modifiers, not arguments.
In this paper, we address instrumentality as con-
veyed by prepositions or equivalent means (e.g.
postpositions, affixes). Our aim in this study
is twofold: (1) to identify the conceptual facets
of instrumentality so that a conceptual seman-
tics can be defined in the spirit of (Talmy, 01,
03), (Wierzbicka 92, 96) and (2) to elaborate an
accurate enough model for answering questions
about instruments within a cooperative question-
answering system. Instead of focusing on a spe-
cific language to elaborate all possible forms of
instrumentality, we found it more adequate to de-
velop a multilingual approach, considering lan-
guages from various families.
We consider for Europe: German, Spanish,
French, Italian; for India: Kashmiri, Urdu, Hindi
and Bengali; for the far-east: Thai and Malay;
for Northern Africa and the Middle East: Ara-
bic, and Berber dialects (in the group of Amazigh
languages). In this paper, we use the first let-
ter of each language to identify it: Thai, Malay,
Hindi, Urdu, Kashmiri, Bengali, German, Span-
ish, French, Italian, Arabic, BeRber). We have
also more or less adequately transcribed characters
into latin characters. The upper case A is equiva-
lent to aa.
3 An overview of preposition structures
3.1 European languages
German, Spanish, Italian and French, like most
European languages have prepositions that in-
troduce instrumental PPs. The most current
prepositions are:
- German: mit, mit Hilfe von,
mittels, durch, anhand, kraft,
51
dank, per.
- French: avec, par, au moyen de,
gra?ce a`, a` l?aide de, a` travers.
- Spanish: con, en, por, a trave?s de,
mediante, por medio de, a base
de, con la ayuda de, gracias a.
- Italian: con, per mezzo di, tramite,
per, grazie a, con l?aiuto di.
3.2 Arabic and Berber
Arabic essentially has the preposition bi, used as
a prefix of the noun it heads:
Aktoub bi al kalami (I write with a pen).
bi can also be associated with a specific noun
or deverbal form (e.g. ?by applying?) to char-
acterize in more depth the instrumental relation.
This entails generic forms such as bi-tarika,
bi-istemali (by means of, the first form rein-
forces the importance of the instrument, while the
latter is more formal) and bi-fadli (thanks to).
For example, we have a kind of nominal form is-
tikhdam (= using or with the use of, constructed
from the root ?use?) in:
Fasser el massala bi istikhdam mithel (explain a
problem ?with the use of? example). Arabic makes
explicit the metonymy we have in European lan-
guages.
There are a few other prepositions such as min
khilal (through) and min a to express the du-
ality source + instrument of the argument (as in
drink with a bottle, litterally). In the spatial con-
text, au can be used (to reach the top by this trail),
and ala is used to express a channel of commu-
nication.
Berber is composed of a large number of di-
alects, some just spoken in small ?tribes?, others
in larger communities. It is basically dialectal, but
a number of common elements can be identified.
We consider here Berber from the Moroccan Rif,
and the Algerian Kabyle. The main instrumental
preposition is e`g, but prefixes are also used s-,
ge-, th-, kh- which affect the morphology
of the noun they are attached to. The prefix s-
is widely used, with some variants (si, sei,
so in Kabyle), e`g (also realized as g? or ge in
some places), is appropriate only when the action
is under full control of the agent. It is not em-
ployed when the instrument is abstract. In the Rif
area, s- focusses on the instrument contributing to
the action, it makes a kind of fusion between the
action verb and the instrumental noun. ge is only
used in spatial contexts involving means of trans-
portation (travelling by plane), kh is used only in
spatial contexts involving paths.
3.3 The Indic language family
The four languages considered, Kashmiri (Raina,
02), Urdu, Hindi and Bengali, share some simil-
itudes due to their common origins but also
contrasts that result from independent evolutions
which are useful to our analysis. They all have suf-
fixes and postpositions. Case suffixes (vibakhti)
are added to nouns or pronouns, but case suf-
fixes do not correspond strictly to thematic roles
(kAraka). Postpositions may also be used instead
of vibhaktis or in conjunction with them, as a kind
or re-inforcement. Basically, but with some nu-
ances, Hindi, Urdu, Bengali and Kashmiri lan-
guages have the following kernels:
? direct instrument: se (H/U), sity (K), -e/-te,
diye after null, dwArA after -r (B). Example:
H: raam ne chaabi se taala khola (ram - erg.
key with lock open-past= Ram openened the
lock with a key).
? means instrument: me (H/U), ke zariye (U),
manz (K), -e/-te (B). Example: U: raam
gaadi ke zariye daftar gayaa (ram car by
means of office go-past = Ram went to work
by car).
? causal instrument: ke kaaran (H), ki vajah se
(U), kiny (K), kArANe, kripAya (B). Exam-
ple: K: dil chi sayaahat kiny amir (Delhi be-
present tourism with rich = Delhi is rich with
tourism),
B: dillI paryatan-er khAtire samriddha or dillI
paryatan-er kripAya samriddha (Delhi be-
rich-er thanks to tourism)
? agentive instrument: ke dwaraa (H) ke zariye
(U) zariy, desi (K), diye (B) after -ke when
the nominal form is animate and specific,
diye after null when the nominal form is an-
imate and general (B). Example: H: raam ne
shyaam dwara apna kaam karvaaya (ram erg.
shyam by self?s work do-cause-past = Ram
got his work done by Shyam)
? action instrumentalised: kar (H/U), kerith
(K). Example: U: raam kuud kar ghar ke an-
dar daakhil huaa (ram jump participle house
52
into enter-past = Ram entered the house by
jumping in it)
There are less important cases that capture no-
tions like containment, which will be detailed in
section 5. Postpositions may vary also depending
on the semantic type of the NP they head.
In Bengali, case is indicated in several ways.
Vibhaktis are suffixes that are added to the stems
to form surface forms of words. In Bengali the
nominal stems take one of the following suffixes:
null or shunya vibhakti, -e, -te, -r, etc. Case is also
indicated in Bengali by the use of postpositions.
We have, for example: Cut bread with a knife, re-
alized as either:
1. chhuri diye ruti kATa (knife diye bread cut), or:
2. chhuri-te ruti kAta (knife-te bread cut)
Postpositions in Bengali are derived from certain
inflected forms of nouns, and also certain verbs
in participle form. When these words are used
as postpositions they are often not considered in
their original sense but define a specific type of re-
lationship of the noun phrase with the finite verb
phrase in the sentence. These words are appear
in a fixed form (indeclinables) as postpositions.
When a postposition is used to denote the case,
the nominal word preceding the postposition takes
on a vibhakti that is determined by the particu-
lar postposition. ?diye? is used after null vibhakti,
?dwArA? after -r vibhakti, etc.
3.4 Thai and Malay
Thai and Malay, although spoken in neighbour
countries, are substantially different.
Thai, from the Thai-Kadai family, has 6 prepo-
sitions (Silapasarn, 98) to denote instruments, the
most common being doi and duai which are
used for concrete instruments, means of trans-
portation, instruments close to manners, etc. kap
is used when the instrument is a part of the body,
while thang is used for means of transportation
only. tam characterizes control of the agent, and
chak is restricted to the instruments that convey
an idea of source. Examples:
khian - duai - din so (Write - with - pencil)
pai - pa ris - doi - khrueang bin (Go - Paris - by -
plane)
These semantic distinctions are, however, often vi-
olated in colloquial Thai.
Malay, from the Malayo-polynesian family, has
three ways to introduce instruments: preposition +
NP, affixes and compounding. Affixed words are
built from stems which are instrumental nouns,
this allows for the construction of the equivalent
of PPs, based on the prototypical use of the instru-
mental noun. The most common being: prefixes:
beR- (from kuda, horse, berkuda, on horseback),
meN- (from kunci, key, mengunci, lock with key),
prefix + suffix: meN- + -kan (from paku, nail,
memakukan, to fasten with nails), and with suffix
-i (from ubat, medicine, mengubati, by means of
medicine). Prepositions occur as the head of PPs,
and in verb particle constructions. PPs may also
be subject complements, avoiding the use of verbs
(dia di rumah, she at home). Besides affixes,
Malay has 6 prepositions that denote instrumen-
tality: dengan, melalui, mengikut,
menerusi, dengan menggunakan,
secara.
A simple example is:
berhubung - melalui - telefon (communicate - by -
telephone).
4 The meaning components of
instrumentality
Let us now consider the different meaning compo-
nents that emerge from our multilingual analysis.
The results presented below are still exploratory
due to the complexity of the notion. The distinc-
tions made (e.g. between concrete and abstract in-
struments) may seem arbitrary: they are just meant
to structure the presentation.
4.1 Concrete instruments
All languages studied have at least one basic in-
strumental mark operating over concrete objects
(T: duai, M: dengan, H: se, U: se, K: sity, B: diye,
-e, -te, G: mit, S: con, F: avec, A: bi, BR: e`g). Sev-
eral refinements are identified, for specific types of
NPs, or to denote a specific intention:
? the instrument is a recipient (S: en) or, more
generally, conveys an idea of container (e.g.
spoon) (B: -e kare), the idea behing is that
the container is used to carry the object along
a certain trajectory,
? the instrument is a part of the body (e.g.
hand): T: kap. In this case, the instrument
is not strictly artifactual.
? the goal is difficult to reach, it requires some
efforts from the agent (S: a base de),
53
? the focus can be emphasized by using ded-
icated marks (G: mit Hilfe (von), Mittels
(more fomal: Das Gericht hat mittels einst-
weiliger Verfu?gung den Drogenhandel unter-
sagt (the court has with provisional ordinance
the drug traffic prohibited))).
The second major difficulty is prototypicality
(Rosch, 78). When the instrument used is not very
prototypical of the action, several languages re-
inforce the instrumental prepositions to, sort of,
coerce the type of the noun so that it can become
an acceptable instrument. We have examples in S:
por medio de, B: sAhAjye, sahojoge,
I: per mezzo di, F: au moyen de, par
le biais de (biais= bias which directly ex-
presses this idea), as in:
F: Il a ouvert la porte au moyen d?un cric (he
opened the door by means of a jack).
At a conceptual level, it is quite difficult to
characterize what is a prototypical instrument
for a given action (characterized by subject-verb-
object: John opens the door). Each event has its
own prototypical instrument, making corpus stud-
ies extremely large, probably unfeasable. When
searching on the web, we find an incredible variety
of instruments to open a door, almost impossible
to classify. Next, prototypicality is not a boolean
notion: instruments are more or less prototypical.
Since the instrument is very much dependent on
the verb and on the object, we cannot foresee any
form of incorporation in the verb that would give
us indications. A direction could be to assume
Qualia structures (Pustejovsky 91) associated with
each potential instrument that describes the func-
tion of the object in the telic role. For example,
key(X) would have open(X, door), with door be-
ing quite generic. This approach could work via a
large lexical development for concrete nouns, it is
much more risky when terms are abstract.
4.2 Abstract instruments
Abstract instruments (theorems, regulations, ex-
amples, etc.) are realized identically to concrete
instruments, but with some typical marks such
as: T: tam, H: dwAra, K: zariyi, B: dwArA,
M. mengikut. At this stage, it is difficult to ex-
plain why marks are different from concrete in-
struments. An hypothesis could be that abstract in-
struments are closer to causes (see 5.5), or to more
formal situations for which specific terms were de-
veloped (e.g. for G: kraft).
There are additional marks dedicated to partic-
ular fields: B: sahajoge, and A: min khilal
when instruments are of type ?example? (explain
with an example). U: -ke zariye, S: por
medio de and G: Anhand, Kraft are more
formal, stronger for Kraft and apply particularly
to areas like juridical or psychological domains.
People and organizations can be seen as appro-
priate intermediaries for reaching a goal. They
may be conceived as metaphorical instruments.
Investigations show that people can get controlled
much in the same way as concrete objects:
F: Elle a informe? Paul de son de?part par Pauline
(She informed Paul of her leaving ?by? Pauline).
If we now consider: S: Juan env??o este paquete
por correo (John sent this parcel ?by? post)
Since post is the by-default medium to send pack-
ages, por is the only choice. Using more precise
services, like FedEx, is considered to be an alter-
native way, in that case F: par, avec S: por,
con are both acceptable.
4.3 Metaphorical instruments
Both concrete and abstract objects can be used
metaphorically as instruments. Examples abound
in the literature and on the Web. In 5.6 we ex-
amine the path metaphor which is very produc-
tive. Besides this case, we have a number of
metaphors, such as: write with your heart, fight
with your head, etc. These are not essentially dif-
ferent from metaphors observed in other situations
(Lakoff and Johnson 99).
4.4 The overlap instrument-manner
In a number of cases, it is not very easy to make
a distinction between instrument and manner. It
seems there is a continuum between these two no-
tions or even some form of overlap, where the ob-
ject is both an instrument and a manner at various
degrees, which may depend on context. A vari-
ety of marks contribute to characterize this over-
lap, manners at stake being quite diverse, but we
will not go into the study of manners. Specific
marks dealing with the manner/instrument am-
biguity are: T: doi, G: durch (which is also
used for metaphorical spatial uses), M: dengan
menggunakan, S: en, con, a as in S: es-
cribir en/con rojo (write in red),
T: khian - duai - muek - daeng (write - with - red -
ink)
BR: te`te s-e?fe`ssen (She-eats with-hands).
54
4.5 Causality
It is clear that, a priori, instruments can be viewed
at various degrees as causes of an event. There is
a kind of overlap between these two notions. In-
struments are not volitional, so they are under the
partial or full control of an agent (humans playing
the role of instruments are also controlled by an
agent). Typically I: a causa di, F: a cause
de, S: a causa de signal that the instrument
has brought about an event:
I : Il castello e distrutto a causa di un violento in-
cendio. (The castle has been destroyed ?because
of? a violent fire.)
Causality (e.g. Talmy, 01) being a complex no-
tion, it is not surprising that instruments, viewed
as intermediaries at various degrees, share some
features with causes. For example in cut the bread
with a knife, the cause of the bread being cut is
the action of the agent, but also the use of a pro-
totypical property of the knife: the knife does the
cutting. In (Talmy 01), the instrument is embed-
ded into the causing event:
(caused event) RESULTS FROM (causing event)
where the causing event has the structure:
Instrument ACTON object, where object is bound
or related in some way to the object in the caused
event.
As analyzed in (Mari and Saint-Dizier, 01), in-
strumentality is the convergence of several factors:
? the degree of involvement of the instrument
in the action, therefore, the fact that the in-
strument causes the action or is just a means
managed by the agent who is the main cause,
? the type of control the agent has on the instru-
ment for the action at stake, from full control
to lack of control,
? the control the agent has over the action as a
whole.
Indic languages and Thai are particular explicit on
these matters. They have specific marks for two
major cases:
1. agentive instrument, action not controlled by
the agent: H: ke dwAra, U: ke zariye
K: zariy, desi, T: doi,
2. causal instrument that does most of the ac-
tion, under the control of the agent: H ke
kAran, U: ki vajah se, K. kiny, T:
duai,
Berber allows e`g only when the agent controls the
instrument. The other cases are expressed by non
prepositional forms.
4.6 Instruments and paths
Another productive situation is the use of spatial
metaphors to express instrumentality. The use of
F: par and other marks (e.g. in B., U.), show
that there is a close link between instrumentality
and path descriptions (spatial as well as temporal
paths). This is a kind of metaphorical use of paths
viewed as instruments (as can be seen in (Lakoff
et al 99): ?action is motion, goals are paths, actors
are travellers?). Using an instrument parallels the
use of paths in the domain of space.
Marks denoting paths or sources are of much
interest. Some have really restricted uses, whereas
others are more flexible. We observe the following
main components:
? paths: T: tam, A: min khilal, S: por, a trave?s
de(por correo, by post), de, G: durch, F:
a` travers, note the distinctions, e.g. in M:
melalui (metaphorical paths: M : berhubung
melalui telefon (communicate by telephone)),
menerusi (channel of transmission), H, U:
me, se, T: thang. In B, -e and -te denote
paths where the agent that does the action has
no control, whereas diye and dhare involve at
least a partial control from the agent. In M,
metaphorical passages require melalui.
? sources: F: a`, A: min a, T: chak (for con-
crete and abstract sources). Example: A:
Achroubou mina Karoura (I am drinking with
bottle), which is also a kind of manner.
The duality path/instrument is particularly visi-
ble in, e.g.:
K: raam vot tshochi vati kiny gari (ram reach-past
short route via home = Ram reached home by the
short route).
Another interesting phenomenon occurs when
an argument is both an instrument and a path,
as in look at the moon in a telescope. Tele-
scope is indeed the instrument used and also
the path through which one looks, or which the
light traverses. This double facet of the argu-
ment is visible in surface realizations, where the
preposition used is ambiguous between instrument
(first preposition) and path (second one) readings:
G: mit, durch, S: con, por, M: dengan,
melalui, F: avec, dans. When one wants
55
to strongly stress the path interpretation, then a
more path-oriented preposition is used, e.g. S: a
trave`s de.
4.7 Means of transportation as instruments
Means of transportation (trains, spoons, boxes, en-
velopes, etc.), sometimes viewed as containers,
and mediums of transportation (by air) receive a
special treatment in a number of languages: T:
doi, thang, M: menerusi, melalui (for metaphorical
mediums and passages), H: me, U: ke zariye, K:
manz, zariy, B: kare, -e kare, -ya kare, A: ala, BR:
ge-, kh-, G: per, S: por, en, F: par. We have, for ex-
ample: U: raam gaadi ke zariye daftar gayaa (ram
car by means of office go-past = ram went to post
office by car)
T: pai - pa ris - doi - khrueang bin (Go to - Paris -
by - plane)
B: Nouko-ya kare phuketa jAo or Nouko-ya
phuketa jAo (boat-e kare phuket go or boat-e
phuket go = go by boat to Phuket)
A distinction is made between the medium and
the means as for: M: secara, which is used for
means of communication such as email or letters.
If the agent has effective control over the means,
then, for example, S uses con.
4.8 Language levels
Some marks are proper to formal discourse: G:
Mittels, Kraft, Anhand, H: dwAra.
4.9 Positive or negative orientation
The languages we studied also abound in positive-
oriented marks that express in a certain way the
idea of ?thanks to?: T: khop khun (+kah for fem-
inime and krup for masculine), H: ke kAran, U:
ki vajah, K: kiny, B: -er khAtire, (-er) kripAya, G:
dank, S: gracias a, F: gra?ce a`.
There are also several negative-oriented marks
such as the following prepositional compounds:
F: de la faute de, I: per colpa di, S:
por colpa de (by the fault of), where the term
?fault? conveys a negative orientation.
4.10 Metonymies
In most languages, the prototypical action denoted
by the instrument is implicit, it is analyzed as a
metonymy: object for action. Action is inferred
from the instrument and the verb in the given con-
text. In a number of situations, A and M need
to make explicit the action. For particular cases,
gerundive forms may be prefered to PPs (but not
to be confused with manners, e.g. ?by swim-
ming?), so that the verb that lexicalizes the action
is present.
For example, in M, ?by the trail? in to reach the
top of a mountain by the trail requires to make ex-
plicit how the trail is used: dengan mengikuti de-
nai itu (litt.: with follow trail DET (same cases
in Arabic and German)). Another case is: G:
Mit Flugzeugen la?sst sich Geld verdienen, (With
planes you can money earn), where a concrete ob-
ject replaces the whole procedure.
The metonymy could be reconstructed, for sim-
ple cases, by the Generative Lexicon (Pustejovsky
86), whose role is precisely to make explicit pro-
totypical functions of objects via their telic role, as
advocated above.
References
Dowty, D., 1989, On the Semantic Content of the No-
tion of Thematic Role, in G. Cherchia, B. Partee,
R. Turner (eds), Properties, Types and meaning,
Kluwer Academic.
Dowty, D., 1991, Thematic Proto-roles and Argument
Selection, Language, vol. 67-3.
Lakoff G., JohnsonM., 1999. Philosophy in the Flesh.
Basic books, NY, USA.
Mari, A., Saint-Dizier, P., 2001, A Conceptual Se-
mantics for Prepositions Denoting Instrumentality,
in proc. 1st workshop on prepositions, Toulouse, and
in Syntax and semantics of prepositions, P. Saint-
Dizier (ed), Kluwer academic, 2006.
Pustejovsky, J., 1991, The Generative Lexicon, Com-
putational Linguistics, vol. 17, MIT Press.
Raina, Achla M., 2002. The Verb Second Phe-
nomenon, O.N. Koul and K Wali (eds.), Topics in
Kashmiri Linguistics. Creative Books, New Delhi,
India.
Rosch, E., 1978. Principles of Categorization. In E.
Rosch and B.B. Lloyd (eds.), Cognition and Catego-
rization. Hillsdale : Lawrence Erlbaum Associates
Publishers.
Sinlapasarn, Upakitt. 1998. Thai Grammar. Thai
Watthana Panich, Bangkok, Thailand.
Talmy L., 2001, 2003. Towards a Cognitive Seman-
tics, vol. 1 and 2. MIT Press.
Wiezbicka, A.,1996. Semantics primes and universals.
Oxford: Oxford University Press.
Wierzbicka, A., 1992, Semantic Primitives and Se-
mantic Fields, in A. Lehrer and E.F. Kittay (eds.),
Frames, Fields and Contrasts. Hillsdale: Lawrence
Erlbaum Associates, pp. 208-227.
56
Proceedings of the 2009 Workshop on Grammar Engineering Across Frameworks, ACL-IJCNLP 2009, pages 10?18,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
Developing German Semantics on the basis of Parallel LFG
Grammars
Sina Zarrie?
Department of Linguistics
University of Potsdam, Germany
sina@ling.uni-potsdam.de
Abstract
This paper reports on the develop-
ment of a core semantics for German
which was implemented on the ba-
sis of an English semantics that con-
verts LFG f-structures to flat meaning
representations in a Neo-Davidsonian
style. Thanks to the parallel design
of the broad-coverage LFG grammars
written in the context of the ParGram
project (Butt et al, 2002) and the
general surface independence of LFG
f-structure analyses, the development
process was substantially facilitated.
We also discuss the overall architec-
ture of the semantic conversion sys-
tem from a crosslinguistic, theoretical
perspective.
1 Introduction
This paper reports on the development of a
core semantics for German which was imple-
mented on the basis of an English semantics
that converts LFG f-structures to flat mean-
ing representations in a Neo-Davidsonian
style. The development strategy relies on the
parallel design of the broad-coverage LFG
grammars written in the context of the Par-
Gram project (Butt et al, 2002). We will
first describe the overall architecture of the
semantic conversion system as well as the
basic properties of the semantic representa-
tion. Section 3 discusses the development
strategy and the core semantic phenomena
covered by the German semantics. In sec-
tion 3.4, we will discuss the benefits and the
limitations of the presented architecture for
crosslingual semantics by means of an ex-
ample phenomenon, the semantics of clause-
embedding verbs. The rest of this introduc-
tion will be devoted to the broader theoreti-
cal context of this work.
Recently, the state of the art in wide-
coverage parsing has made wide-coverage se-
mantic processing come into the reach of re-
search in computational semantics (Bos et
al., 2004). This shift from the theoret-
ical conception of semantic formalisms to
wide-coverage semantic analysis raises many
questions about appropriate meaning repre-
sentations as well as engineering problems
concerning the development and evaluation
strategies of semantic processing systems.
The general aim of this work is to explore
wide-coverage LFG syntax as a backbone for
linguistically motivated semantic processing.
Research in the framework of LFG has tra-
ditionally adopted a crosslingual perspective
on linguistic theory (Bresnan, 2000). In the
context of the ParGram project, a number
of high quality, broad-coverage grammars for
several languages have been produced over
the years (Butt et al, 2002; Butt and King,
2007).1 The project?s research methodology
particularly focusses on parallelism which
means that the researchers rely on a com-
mon syntactic theory as well as development
tools, but which also concerns parallelism on
the level of syntactic analyses. As the LFG
formalism assumes a two-level syntax that di-
1Also see the webpage for a nice project overview:
http://www2.parc.com/isl/groups/nltt/pargram/
10
vides the analysis into a more language and
surface dependent constituent structure and
a functional structure which basically repre-
sents the surface independent grammatical
relations of a sentence, it constitutes a partic-
ularly appropriate basis for large-scale, mul-
tilingual syntax.
Parallel grammar development bears the
practical advantage that the resources de-
velopped for a particular language can of-
ten easily be ported to related languages.
Kim et al (2003) report that the Korean
ParGram grammar was constructed in two
months by adapting the Japanese grammar
for Korean. Moreover, parallel grammars
have a straightforward application in multi-
lingual NLP tasks like machine translation
(Frank, 1999).
A general motivation for multilingual, deep
grammars are higher-level NLP tasks which
involve some kind of semantic or meaning-
sensititive processing (Butt and King, 2007).
The work presented in this paper shows that
parallel grammar development not only fa-
cilitates porting of grammars, but substan-
tially facilitates the development of resources
and applications that involve such a par-
allel grammar. We rely on the semantic
conversion system presented in (Crouch and
King, 2006) to implement a system that de-
rives semantic representations from LFG f-
structures for German. Due to the paral-
lelism of syntactic f-structure input, the Ger-
man core semantics could be implemented
within a single month.
2 F-Structure Rewriting as an
LFG Semantics
Since the early days of LFG, there has
been research on interfacing LFG syntax
with various semantic formalisms (Dalrym-
ple, 1999). For the English and Japanese
ParGram grammar, a broad-coverage, glue
semantic construction has been implemented
by (Crouch, 1995; Umemoto, 2006). In con-
trast to these approaches, the semantic con-
version described in (Crouch and King, 2006)
is not driven by a specific semantic theory
about meaning representation, nor by a the-
oretically motivated apparatus of meaning
construction. Therefore, we will talk about
?semantic conversion? instead of ?construc-
tion? in this paper.
The main idea of the system is to convert
the surface-independent, syntactic relations
and features encoded in an f-structure to nor-
malized semantic relations. The representa-
tion simplifies many phenomena usually dis-
cussed in the formal semantic literature (see
the next section), but is tailored for use in
Question Answering (Bobrow et al, 2007a)
or Textual Entailment (Bobrow et al, 2007b)
applications.
The semantic conversion was implemented
by means of the XLE platform, used
for grammar development in the ParGram
project. It makes use of the built-in trans-
fer module to convert LFG f-structures to
semantic representations. The idea to use
transfer rules to model a semantic concstruc-
tion has also been pursued by (Spreyer and
Frank, 2005) who use the transfer module to
model a RMRS semantic construction for the
German treebank TIGER .
2.1 The Semantic Representation
As a first example, a simplified f-structure
analysis for the following sentence and the
corresponding semantic representation are
given in figure 1.
(1) In the afternoon, John was seen in the park.
The basic idea of the representation exem-
plified in figure 1 is to represent the syntactic
arguments and adjuncts of the main predi-
cate in terms of semantic roles of the context
introduced by the main predicate or some
higher semantic operator. Thus, the gram-
matical roles of the main verb in sentence
(1) are semantically normalized such that the
subject of the passive becomes a theme and
an unspecified agent is introduced, see fig-
ure 1. The role of the modifiers are speci-
11
fied in terms of their head preposition. This
type of semantic representation is inspired by
Neo-Davidsonian event semantics (Parsons,
1990). Other semantic properties of the event
introduced by the main verb such as tense or
nominal properties such as quantification and
cardinality are explicitely encoded as conven-
tionalized predications.
The contexts can be tought of as propo-
sitions or possible worlds. They are headed
by an operator that can recursively embed
further contexts. Context embeddings can
be induced by lexical items or syntactic con-
structions and include the following opera-
tors: (i) negation (ii) sentential modifiers
(possibly) (iii) coordination with or (iv) con-
ditionals (v) some subordinating conjunc-
tions (without) (vi) clause-embedding verbs
(doubt).
The representation avoids many formal se-
mantic complexities typically discussed in
the literature, for instance the interpreta-
tion of quantifiers by encoding them as con-
ventionalized semantic predications. Given
this skolemized first-order language, the
task of textual entailment can be conceived
as matching the hypothesis representation
against the semantic representation of the
text where higher-order reasonning is ap-
proximated by explicit entailment rules (e.g.
all entails some, past does not entail present),
see (Bobrow et al, 2007b) for a presentation
of an RTE system based on this semantic rep-
resentation.
2.2 The Semantic Conversion
The XLE transfer module, which we use for
the implementation of the conversion of f-
structures to semantic representations, is a
term rewrite system that applies an ordered
list of rewrite rules to a given f-structure
input and yields, depending on the rewrite
rules, new f-structures (e.g. translated f-
structures) or semantic representations. The
technical features of the XLE transfer mod-
ule are described in (Crouch et al, 2006).
An important feature for large-scale develop-
+VTYPE(%V, %%), +PASSIVE(%V,+),
OBL-AG(%V, %LogicalSUBJ), PTYPE(%LogicalSUBJ,%%),
OBJ(%LogicalSUBJ,%P)
==> SUBJ(%V, %P), arg(%V,%N,%P).
Figure 2: Example rewrite rule for passive
normalization
ment is for instance the mechanism of packed
rewriting that allows for an efficient represen-
tation and processing of ambigous f-structure
analyses.
The semantic conversion, as described in
(Crouch and King, 2006), is not a priori con-
strained by a formal apparatus of meaning
assembly. The main intuition of the con-
version is that the embeddings encoded in
the syntactic analysis have to be normalized
or reencoded in a way such that they cor-
respond to a semantic embedding. An ex-
ample rewrite rule which applies to passive
f-structure analyses and converts them to an
active analysis is given in figure refpassive-
fig.
In order to be maintainable and extensible,
the set of transfer rules producing the seman-
tic representations are organized in a modu-
lar way. The main steps of the semantic con-
version are given in the following: (i) Flat-
tening of syntax specific f-structure embed-
dings that don?t correspond to semantic em-
beddings (ii) Canonicalization of grammati-
cal relations (e.g. depassivization) (iii) Mark-
ing of items that induce a semantic embed-
ding (which is not encoded in the f-structure)
(iv) Linking of f-structure scopes and context
of the semantic representation. (v) Remov-
ing of f-structure specific features.
An explicitely modular conception of the
transfer procedure also facilitates its port-
ing to other languages. Thus, steps 1 and
2 (and partly 3) may be dependent on the
language specific f-structure encoding, while
the general steps from 3 and 5 don?t have to
be changed at all when porting the transfer
rules to another language.
12
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
pred ?see
?
NULL,
(
? SUBJ
)
?
?
subj
?
?
?
?
pred ?John?
case nom
gend-sem male
... ...
?
?
?
?
adjunct
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
pred ?in
?
(
? OBJ
)
?
?
obj
[
pred ?afternoon?
case obl
]
?
?
?
?
?
?
?
?
pred ?in
?
(
? OBJ
)
?
?
obj
[
pred ?park?
case obl
]
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
ctx_head(t,see),
in_ctx(t, past(see)),
in_ctx(t, cardinality(John,sg)),
in_ctx(t, cardinality(afternoon,sg)),
in_ctx(t, cardinality(park,sg)),
in_ctx(t, specifier(afternoon,the)),
in_ctx(t, specifier(park,the)),
in_ctx(t, time_expr(afternoon:8,+)),
in_ctx(t, proper_name(John,name,John)),
in_ctx(t, role(Agent,see,agent_pro)),
in_ctx(t, role(Theme,see,John)),
in_ctx(t, role(prep(in),see,afternoon)),
in_ctx(t, role(prep(in),see,park))
Figure 1: LFG f-structure analysis and corresponding semantic representation
3 From English to German
Semantics
3.1 Semantic Grammar Development
In contrast to the various gold standard tree-
banks available for the development and eval-
uation of parsers, gold standards for seman-
tic representations are hardly available. This
has a number of methodological implications
for ?semantic grammar? development. For
instance, the authors in (de Paiva and King,
2008) argue for large-scale development of a
semantics that is based on an application-
oriented testsuite of entailment pairs instead
of sentences and their theoretically correct
representations. However, in the context of
this work, we didn?t focus on a semantic ap-
plication, but we wanted to assess the porta-
bility of the semantic representations to other
languages directly. Adopting such a theory-
driven perspective on semantic grammar de-
velopment, the only possibility to account for
the accuracy of the semantic construction is
to manually inspect the output of the system
for a necessarily small set of input sentences.
Moreover, the transfer scenario compli-
cates the assessment of the system?s cover-
age. While in (Bos et al, 2004), the coverage
of the meaning construction can be quanti-
fied by the number of syntactic analysis that
the construction algorithm can process, the
transfer conversion will never fail on a given
syntactic input. Since the transfer rules just
try to match the input, the unmatched fea-
tures just pass unchanged to the output and
will be probably deleted by some of the catch-
all rules which remove remaining syntactic
features in the final step of the conversion.
Therefore, manual inspection is necessary to
see whether the conversion has processed all
the input it was supposed to process.
This limited evaluation scenario entails
that the semantics developer has to think
hard about defining the set of phenomena he
wants to cover and document precisely which
type of syntactic phenomena his semantics
intends to assign an interpretation to. There-
fore, in the rest of this section, we will try
to give a concrete overview of the type of
phenomena that is covered by the English-
German semantics.
3.2 A Parallel Testsuite
In consequence to these considerations on
evaluation, a central aspect of our develop-
ment metholodogy is a testsuite of German
sentences which represents the ?core seman-
tics? that our systems covers. The multi-
13
lingual perspective provided a major orien-
tation for the composition of this testsuite.
As our base English semantics implicitely de-
fines a set of core phenomena interpreted by
the syntax-semantic interface, we dispose of a
set of grammatical f-structure relations that
receive a particular semantic representation.
Fortunately, the developers of the English se-
mantics had documented many ?core? trans-
fer rules (assuring the normalization and con-
text embedding) with example phrases or
sentences such that one could easily recon-
struct the type of phenomenon each transfer
rule was intended to analyze.
On the basis of this system documenta-
tion, we first conceived an English testsuite
where each sentence contained a construc-
tion related to the application of a specific
transfer rule. For each of the sentences we
selected a German sentence which exhibited
the German counterpart of the phenomenon
targeted in the English sentence. For in-
stance, if a transfer rule for relative clauses
fired on a given English sentence we trans-
lated the German sentence such that it con-
tained a relative clause. As most of the test
sentences target fairly general phenomena at
the syntax-semantic interface (see the next
section), there was a parallel German realiza-
tion of the construction in most of the cases.
In cases where no straightforward parallel
realization could be found, we recur to a se-
mantically parallel translation. For instance,
the English cleft construction exemplified by
the following sentence of our testsuite, does
not have a syntactically parallel realization
in German. In this case, the sentence was
translated by a ?semantic? equivalent that
emphasizes the oblique argument.
(2) a. It is to the store that they went.
b. Zum Markt sind sie gegangen.
During the development process, the test-
set was further extended. These extensions
were due to cases where the English gram-
mar assigns a uniform analysis to some con-
structions that the German gramamr dis-
tinguishes. For instance, while the En-
glish grammar encodes oblique arguments
the same way it encodes direct objects, the
German grammar has a formally slightly dif-
ferent analysis such that rules which fire on
obliques in English, don?t fire for German in-
put. Now, the final parallel testsuite com-
prises 200 sentence pairs.
The following enumeration lists the basic
morpho-syntactic phenomena covered by our
core semantics testsuite.
1. Sentence types (declaratives, interroga-
tives, quotations etc.)
2. Coordination (of various phrase types)
3. Argument - semantic role mapping, in-
cluding argument realization normaliza-
tion (depassivization etc.)
4. Sentential and verbal modification (dis-
cursive, propositional, temporal, etc.)
5. Nominal modification (measures, quan-
tifiers, comparatives, etc.)
6. Tense and aspect
7. Appositions and titles
8. Clause-embeddings, relative clauses,
gerunds, etc.
9. Predicative and copula constructions
10. Topicalization
It turns out that the abstract conception
of LFG f-structure analysis already assumes
a major step towards semantic interpreta-
tion. Many global syntactic properties are
explicitely represented as feature-value pairs,
e.g. features for sentence type, mood, tense
and aspect. Moreover, the f-structure al-
ready contains many information about e.g.
the type of nominal phrases (proper names,
quantified phrases etc.) or types of modifiers
(e.g. adverb types). Finally, this also jus-
tifies our testsuite approach since the range
of syntactic variation on this abstract level is
much smaller than on the level of word-order.
14
3.3 Parallel Core Semantics
The English core semantics developped by
(Crouch and King, 2006) comprises 798 (or-
dered!) rewrite rules. As we hypothesized
that a major part of the English rules will
also apply to German f-structure input, we
first copied all English transfer rules to the
German semantics and then proceeded by
manual error correction: For each German
test sentence, we manually checked whether
the transfer semantics produce an interpre-
tation of the sentence which is parallel to the
English analysis. In case a mismatch was de-
tected, the respective rules where changed or
added in the German transfer rule set.
To cover the 200 sentences in our parallel
testsuite, 47 rewrite rules had to be changed
out of the 798 rules which constitute the core
English semantics. Out of these 47 rules, 23
rules relate to real structural differences in
the f-structure encoding for German and En-
glish. The rest of the modifications is mainly
due to renamings of the features or lexical
items that are hard-coded in the transfer
grammar.
While in a more surface-oriented syntax,
it would be hardly possible to design largely
parallel syntax-semantic interfaces for the
range of phenomena listed in the last section,
the surface-independence (and the resulting
relative crosslingual generality) of LFG f-
structures ensures that a major part of the
English core semantics straightforward-ly ap-
plies to the German input.
An impressive illustration of the language
independence of LFG f-structure analyses in
the ParGram grammars is the pair of anal-
yses presented in figure 3, produced by the
semantic conversion for the example pair in
(3).
(3) a. Wo hat Tom gestern geschlafen?
b. Where did Tom sleep yesterday?
The representation for the German sen-
tence was produced by running the English
transfer semantics on German syntactic in-
put. Although the word-order of English
and German questions is governed by dis-
tinct syntactic principles, the semantic rep-
resentation of the German sentence is almost
entirely correct since the f-structure analy-
ses abstract from the word-order differences.
The only fault in the German representation
in 3 is the interpretation of the temporal ad-
verb yesterday - gestern. The transfer rule
for temporal verb modification didn?t fire be-
cause the adverb type features for English
and German differ.
3.4 Discussion: Clause-embeddings
and Semantic Fine-graininess
The crosslinguistic parallelism of the seman-
tics presented in this paper is also due to the
relative coarse-grained level of representation
that interprets many phenomena prone to
subtle crosslingual divergences (e.g. the in-
terpretation of quantifiers or tense and as-
pect) in terms of conventionalized predica-
tions, e.g. the interpretation of tense as
past(see) in figure 1. Thus, the real se-
mantic interpretation of these phenomena is
deferred to later representation or processing
layers, as in this framework, to the defini-
tion of entailment relations (Bobrow et al,
2007b). A meaning representation that de-
fers much of the semantic interpretation to
the formulation of entailment rules runs the
obvious risk of making to few theoretical gen-
eralizations which results in very complex en-
tailment rules. This section will briefly illus-
trate this problem by discussing the represen-
tation of clause-embeddings in our semantics.
The various semantic operators defined by
the semantic conversion to induce an em-
bedding (see section 2.1) embed a seman-
tic entity of the type context which can be
roughly considered as the common semantic
type of ?proposition?. An example for the se-
mantic representation of a clause-embedding
verb is given in figure 4.
For many semantic applications, such em-
bedded contexts are of particular interest
since they often express propositions to
15
ctx_head(ctx(s),schlafen),
ctx_index(t,schlafen),
in_ctx(t,interrogative(ctx(s))),
in_ctx(ctx(s),perf(schlafen)),
in_ctx(ctx(s),pres(schlafen)),
in_ctx(ctx(s),query_term(wo)),
in_ctx(ctx(s),cardinality(?Tom?,sg)),
in_ctx(ctx(s),proper_name(?Tom?,name,?Tom?)),
in_ctx(ctx(s),role(?Agent?,schlafen,?Tom?)),
in_ctx(ctx(s),role(adeg,gestern,normal)),
in_ctx(ctx(s),role(adeg,wo,normal)),
in_ctx(ctx(s),role(amod,schlafen,gestern)),
in_ctx(ctx(s),role(amod,schlafen,wo))
ctx_head(ctx(s),sleep),
ctx_index(t,sleep),
in_ctx(t,interrogative(ctx(s))),
in_ctx(ctx(s),past(sleep)),
in_ctx(ctx(s),query_term(where)),
in_ctx(ctx(s),cardinality(?Tom?,sg)),
in_ctx(ctx(s),time_expr(yesterday,?+?)),
in_ctx(ctx(s),proper_name(?Tom?,name,?Tom?)),
in_ctx(ctx(s),role(?Agent?,sleep,?Tom?)),
in_ctx(ctx(s),role(occurs_during,sleep,yesterday)),
in_ctx(ctx(s),role(prep(where),sleep,where))
Figure 3: Parallel semantic analyses for the sentence pair given in example (3)
whom the speaker is not committed to, i.e.
which aren?t veridical. In our system, the
veridicality inferences that these embeddings
exhibit are computed by further knowledge
representation modules that explicitely rep-
resent the speaker commitment of a context
(Bobrow et al, 2007b). Concerning the com-
plements of clause-embedding verbs, these
inferences are modelled via a lexical verb
classification that basically distinguishes im-
plicatives (manage to TRUE - don?t manage
to FALSE ) and factives (know that TRUE -
don?t know that TRUE ) (Nairn et al, 2006).
Veridicality entailments of sentential comple-
ments are treated as a interaction of the lex-
ical class of the subordinating verb and the
polarity of the context.
(4) Tom glaubt, dass der Nachbar ihn nicht
erkannt hat.
?Tom believes that the neighbour didn?t rec-
ognize him.?
This account of clause-embeddings - a uni-
fied semantic representation and a lexical en-
tailment classification - generalizes and prob-
ably simplifies too much the various theoret-
ical insights into the semantics of comple-
mentation. In the formal semantics litera-
ture, various theories opt for a semantic rep-
resentation that assumes several types of ab-
stract semantic entities (e.g. events (Parsons,
1990), situations (Barwise and Perry, 1999)
or other, very fine-grained categories (Asher,
1993) ). In terms of entailment, the typologi-
cal literature reports crosslingually relatively
stable distinctions of types of complements
according to the semantic relations the ma-
trix verbs have to their complement (Givon,
1990). For instance, while in example (5),
the infinite complement has causal, tempo-
ral and spatial relations to the matrix event,
there is no such inferential relation between
matrix and complement in example (4) .
(5) Seine Freundin brachte ihn dazu, ein Haus zu
bauen.
His girlfriend made him build a house.
Moreover, the semantics of clause-
embedding verbs shows subtle distinctions
with resepct to other linguistic features
(apart from the polarity of the context) that
can trigger a particular speaker commit-
ment. For instance, in languages that have a
morphological aspect marking (like Frensh,
in the following example), the following
aspectually motivated entailments can be
observed (see (Bhatt, 2006)):
(6) Jean pouvait soulever cette table, mais il ne
l?a pas fait.
?Jean was able.IMP to lift this table, but he
didn?t do it.?
(7) Jean a pu soulever cette table, #mais il ne l?a
pas fait.
?Jean was able.PERF to lift this table,#but
he didn?t do it.?
In sentence (6), the imperfect aspect
16
causes the modality of the complement such
that it is not necessarily true, while in sen-
tence (7), the embedded clause is neces-
sarily true due to the perfective aspect of
the clause-embedding verb. This aspectual
matrix-com-plement relation is however only
observable for certain types of modality or
clause-embedding verbs and has no clear se-
mantic parallel in other languages that don?t
have aspectual marking.
For another type of clause-embedding
verbs, called epistemic verbs, the recent for-
mal semantics literature discusses many ex-
amples where the lexical neutral entailment
class is overriden by pragmatic interpretation
constraints that cause the embedded com-
plement to be interpreted as true although
the embedding operator does not entail the
veridicality of its complement (Simons, 2006;
von Fintel and Gillies, 2007). As an exam-
ple, consider the following text - hypothe-
sis pair annotated as a valid entailment in
the Pascal RTE 3 set alough the hypothesis
clearly refers to an embedded proposition in
the given text.
(8) Between March and June, scientific observers
say, up to 300,000 seals are killed. In Canada,
seal-hunting means jobs, but opponents say
it is vicious and endangers the species, also
threatened by global warming.
(9) Hunting endangers seal species. FOLLOWS
(RTE3 ID:225)
Such examples suggest that entailments
concern various aspects of the meaning of
a sentence or proposition, thus, not only its
veridicality but also its temporal properties,
informations about involved agents, space
and time. These properties are clearly re-
lated to the semantic type of the embedded
clause.
Purely lexical entailment rules for clause-
em-bedding operators will be very hard to
formulate in the light of the complex in-
teraction of the various linguistic parame-
ters. These considerations reveal a general
trade-off between a representation that gen-
ctx_head(t,glauben),
ctx_head(ctx(kennen),kennen)),
ctx_head(ctx(nicht,nicht),
in_ctx(t,role(sem_comp,glauben,ctx(nicht))),
in_ctx(t,role(sem_subj,glauben,?Andreas?)),
in_ctx(ctx(kennen),role(sem_obj,kennen,pro)),
in_ctx(ctx(nicht),role(adeg,nicht,normal)),
in_ctx(ctx(nicht),role(amod,ctx(kennen),nicht)
Figure 4: Example representation for context
embeddings, sentence (4)
eralizes over many (purely) theoretical and
crosslingual subtleties and a representation
that does not capture certain generalizations
which would lead to a more linguistically in-
formed account of entailment relations. Fu-
ture work on the semantics presented in this
paper will have to take such tensions into ac-
count and think about the general goals and
applications of the semantic representation.
4 Conclusion
This work amply illustrates the positive
implications of crosslinguistic, parallely de-
signed resources for large-scale linguistic en-
gineering. Due to the abstract f-structure
layer in LFG syntax and its parallel imple-
mentation in the ParGram project, further
resources that build on f-structure represen-
tations can be very easily ported to other lan-
guages. Future research will have to investi-
gate to what extent this also applies to more
distant languages, like Urdu and English for
instance.
The paper also discussed some problematic
aspects of the development of a large-scale
semantic system. The crosslingual develop-
ment perspective allowed us to define a set
of core semantic phenomena covered by the
representation. However, from a formal se-
mantic view point, the simplifying represen-
tation obstructs potential crosslingual differ-
ences in semantic interpretation. Future re-
search still has to be conducted to develop
a more general development and evaluation
methodology for the representation of mean-
ing.
17
References
Nicholas Asher. 1993. Reference to Abstract Objects
in Discourse. Studies in Linguistics and Philoso-
phy. Kluwer Academic Publishers.
Jon Barwise and John Perry. 1999. Situations and
Attitudes. CSLI Publications.
Rajesh Bhatt, 2006. Covert Modality in Non-finite
Contexts, volume 8 of Interface Explorations, chap-
ter Ability modals and their actuality entailments.
Mouton de Gruyter.
Daniel G. Bobrow, Bob Cheslow, Cleo Condoravdi,
Lauri Karttunen, Tracy Holloway King, Rowan
Nairn, Valeria de Paiva, Charlotte Price, and
Annie Zaenen. 2007a. PARC?s Bridge ques-
tion answering system. In Tracy Holloway King
and Emily M. Bender, editors, Proceedings of
the GEAF (Grammar Engineering Across Frame-
works) 2007 Workshop, pages 13?15.
Daniel G. Bobrow, Bob Cheslow, Cleo Condoravdi,
Lauri Karttunen, Tracy Holloway King, Rowan
Nairn, Valeria de Paiva, Charlotte Price, and An-
nie Zaenen. 2007b. Precision-focused textual in-
ference. In ACL-PASCAL Workshop on Textual
Entailment and Paraphrasing, pages 28 ? 29.
Johan Bos, Stephen Clark, Mark Steedman, James R.
Curran, and Julia Hockenmaier. 2004. Wide-
coverage semantic representations from a CCG
parser. In COLING ?04: Proceedings of the 20th
international conference on Computational Lin-
guistics, page 1240, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Joan Bresnan. 2000. Lexical-Functional Syntax.
Blackwell, Oxford.
Miriam Butt and Tracy Holloway King. 2007. XLE
and XFR: A Grammar Development Platform
with a Parser/Generator and Rewrite System.
In International Conference on Natural Language
Processing (ICON) Tutorial.
Miriam Butt, Helge Dyvik, Tracy Holloway King, Hi-
roshi Masuichi, and Christian Rohrer. 2002. The
Parallel Grammar Project.
Richard Crouch and Tracy Holloway King. 2006.
Semantics via F-Structure Rewriting. In Miriam
Butt and Tracy Holloway King, editors, Proceed-
ings of the LFG06 Conference.
Dick Crouch, Mary Dalrymple, Tracy King, John
Maxwell, and Paula Newman, 2006. XLE Doc-
umentation.
Dick Crouch. 1995. Packed Rewriting for Mapping
Semantics to KR. In Proceedings of the Interna-
tional Workshop on Computational Semantics.
Mary Dalrymple. 1999. Semantics and Syntax in
Lexical Functional Grammar: The Resource Logic
Approach. MIT Press, Cambridge, Mass.
Valeria de Paiva and Tracy Holloway King. 2008.
Designing testsuites for grammar-based systems
in applications. In Proc. of the COLING GEAF
Workshop 2008.
Anette Frank. 1999. From Parallel Grammar Devel-
opment towards Machine Translation (shortened
version). In Miriam Butt and Tracy Holloway
King, editors, Proceedings of the LFG-99 Con-
ference, CSLI Online Publications, University of
Manchester. Section 4 of: Miriam Butt and Ste-
fanie Dipper and Anette Frank and Tracy Hol-
loway King.
Talmy Givon. 1990. Syntax, volume 2. Benjamins.
Roger Kim, Mary Dalrymple, Ronald M. Kaplan,
Tracy Holloway King, Hiroshi Masuichi, and
Tomoko Ohkuma. 2003. Multilingual Grammar
Development via Grammar Porting. . In ESSLLI
2003 Workshop on Ideas and Strategies for Multi-
lingual Grammar Development.
Rowan Nairn, Cleo Condoravdi, and Lauri Kart-
tunen. 2006. Computing relative polarity for tex-
tual inference. In Inference in Computational Se-
mantics (ICoS-5).
Terence Parsons. 1990. Events in the semantics of
English, volume 19 of Current studies in linguistics
series ; 19. MIT Pr., Cambridge, Mass. [u.a.].
Mandy Simons. 2006. Observations on embedding
verbs, evidentiality, and presupposition. Lingua.
Kathrin Spreyer and Anette Frank. 2005. The
TIGER 700 RMRS Bank: RMRS Construction
from Dependencies. In Proceedings of LINC 2005,
pages 1?10.
Hiroshi Umemoto. 2006. Implementing a Japanese
Semantic Parser Based on Glue Approach. In Pro-
ceedings of The 20th Pacific Asia Conference on
Language, Information and Computation.
Kai von Fintel and Anthony S. Gillies. 2007.
An opinionated guide to epistemic modality. In
Tamar Gendler Szabo and John Hawthorne, edi-
tors, Oxford Studies in Epistomology, Vol. 2. Ox-
ford University Press.
18
Proceedings of the 2009 Workshop on Multiword Expressions, ACL-IJCNLP 2009, pages 23?30,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
Exploiting Translational Correspondences for Pattern-Independent MWE
Identification
Sina Zarrie?
Department of Linguistics
University of Potsdam, Germany
sina@ling.uni-potsdam.de
Jonas Kuhn
Department of Linguistics
University of Potsdam, Germany
kuhn@ling.uni-potsdam.de
Abstract
Based on a study of verb translations in
the Europarl corpus, we argue that a wide
range of MWE patterns can be identified in
translations that exhibit a correspondence
between a single lexical item in the source
language and a group of lexical items in
the target language. We show that these
correspondences can be reliably detected
on dependency-parsed, word-aligned sen-
tences. We propose an extraction method
that combines word alignment with syn-
tactic filters and is independent of the
structural pattern of the translation.
1 Introduction
Parallel corpora have proved to be a valuable re-
source not only for statistical machine translation,
but also for crosslingual induction of morphologi-
cal, syntactic and semantic analyses (Yarowsky et
al., 2001; Dyvik, 2004). In this paper, we propose
an approach to the identification of multiword ex-
pressions (MWEs) that exploits translational cor-
respondences in a parallel corpus. We will con-
sider in translations of the following type:
(1) Der
The
Rat
Council
sollte
should
unsere
our
Position
position
beru?cksichtigen.
consider.
(2) The Council should take account of our position.
This sentence pair has been taken from the
German - English section of the Europarl corpus
(Koehn, 2005). It exemplifies a translational cor-
respondence between an English MWE take ac-
count of and a German simplex verb beru?cksichti-
gen. In the following, we refer to such correspon-
dences as one-to-many translations. Based on a
study of verb translations in Europarl, we will ex-
plore to what extent one-to-many translations pro-
vide evidence for MWE realization in the target
language. It will turn out that crosslingual corre-
spondences realize a wide range of different lin-
guistic patterns that are relevant for MWE iden-
tification, but that they pose problems to auto-
matic word alignment. We propose an extraction
method that combines distributional word align-
ment with syntactic filters. We will show that
these correspondences can be reliably detected
on dependency-parsed, wordaligned sentences and
are able to identify various MWE patterns.
In a monolingual setting, the task of MWE ex-
traction is usually conceived of as a lexical as-
sociation problem where distributional measures
model the syntactic and semantic idiosyncracy ex-
hibited by MWEs, e.g. (Pecina, 2008). This ap-
proach generally involves two main steps: 1) the
extraction of a candidate list of potential MWEs,
often constrained by a particular target pattern
of the detection method, like verb particle con-
structions (Baldwin and Villavicencio, 2002) or
verb PP combinations (Villada Moiro?n and Tiede-
mann, 2006), 2) the ranking of this candidate list
by an appropriate assocation measure.
The crosslingual MWE identification we
present in this paper is, a priori, independent
of any specific association measure or syntactic
pattern. The translation scenario allows us to
adopt a completely data-driven definition of what
constitutes an MWE: Given a parallel corpus,
we propose to consider those tokens in a target
language as MWEs which correspond to a single
lexical item in the source language. The intuition
is that if a group of lexical items in one lan-
guage can be realized as a single item in another
language, it can be considered as some kind of
lexically fixed entity. By this means, we will
not approach the MWE identification problem
by asking for a given list of candidates whether
these are MWEs or not. Instead, we will ask for
a given list of lexical items in a source language
whether there exists a one-to-many translation for
this item in a target language (and whether these
23
one-to-many translations correspond to MWEs).
This strategy offers a straightforward solution to
the interpretation problem: As the translation can
be related to the meaning of the source item and
to its other translations in the target language, the
interpretation is independent of the expression?s
transparency. This solution has its limitations
compared to other approaches that need to auto-
matically establish the degree of compositionality
of a given MWE candidate. However, for many
NLP applications, coarse-grained knowledge
about the semantic relation between a wide
range of MWEs and their corresponding atomic
realization is already very useful.
In this work, we therefore focus on a general
method of MWE identification that captures the
various patterns of translational correspondences
that can be found in parallel corpora. Our exper-
iments described in section 3 show that one-to-
many translations should be extracted from syn-
tactic configurations rather than from unstructured
sets of aligned words. This syntax-driven method
is less dependent on frequency distributions in a
given corpus, but is based on the intuition that
monolingual idiosyncracies like MWE realization
of an entity are not likely to be mirrored in another
language (see section 4 for discussion).
Our goal in this paper is twofold: First, we want
to investigate to what extent one-to-many transla-
tional correspondences can serve as an empirical
basis for MWE identification. To this end, Sec-
tion 2 presents a corpus-based study of the rela-
tion between one-to-many translations and MWEs
that we carried out on a translation gold standard.
Second, we investigate methods for the automatic
detection of complex lexical correspondences for
a given parallel corpus. Therefore, Section 3 eval-
uates automatic word alignments against our gold
standard and gives a method for high-precision
one-to-many translation detection that relies on
syntactic filters, in addition to word-alignments.
2 Multiword Translations as MWEs
The idea to exploit one-to-many translations for
the identification of MWE candidates has not re-
ceived much attention in the literature. Thus, it is
not a priori clear what can be expected from trans-
lational correspondences with respect to MWE
identification. To corroborate the intuitions intro-
duced in the last section, we carried out a corpus-
based study that aims to discover linguistic pat-
Verb 1-1 1-n n-1 n-n No
anheben (v1) 53.5 21.2 9.2 16 325
bezwecken (v2) 16.7 51.3 0.6 31.3 150
riskieren (v3) 46.7 35.7 0.5 17 182
verschlimmern (v4) 30.2 21.5 28.6 44.5 275
Table 1: Proportions of types of translational cor-
respondences (token-level) in our gold standard.
terns exhibited by one-to-many translations.
We constructed a gold standard covering all En-
glish translations of four German verb lemmas ex-
tracted from the Europarl Corpus. These verbs
subcategorize for a nominative subject and an ac-
cusative object and are in the middle frequency
layer (around 200 occurrences). We extracted all
sentences in Europarl with occurences of these
lemmas and their automatic word alignments pro-
duced by GIZA++ (Och and Ney, 2003). These
alignments were manually corrected on the basis
of the crosslingual word alignment guidelines de-
velopped by (Grac?a et al, 2008).
For each of the German source lemmas, our
gold standard records four translation categories:
one-to-one, one-to-many, many-to-one, many-to-
many translations. Table 1 shows the distribution
of these categories for each verb. Strikingly, the
four verbs show very different proportions con-
cerning the types of their translational correspon-
dences. Thus, while the German verb anheben
(en. increase) seems to have a frequent parallel
realization, the verbs bezwecken (en. intend to)
or verschlimmern (en. aggravate) tend to be real-
ized by more complex phrasal translations. In any
case, the percentage of one-to-many translations is
relatively high which corroborates our hypothesis
that parallel corpora constitute a very interesting
resource for data-driven MWE discovery.
A closer look at the one-to-many translations re-
veals that these cover a wide spectrum of MWE
phenomena traditionally considered in the liter-
ature, as well as constructions that one would
usually not regard as an MWE. Below, we will
shortly illustrate the different classes of one-to-
many translations we found in our gold standard.
Morphological variations: This type of one-to-
many translations is mainly due to non-parallel re-
alization of tense. It?s rather irrelevant from an
MWE perspective, but easy to discover and filter
automatically.
24
(3) Sie
They
verschlimmern
aggravate
die
the
?Ubel.
misfortunes.
(4) Their action is aggravating the misfortunes.
Verb particle combinations: A typical MWE
pattern, treated for instance in (Baldwin and
Villavicencio, 2002). It further divides into trans-
parent and non-transparent combinations, the lat-
ter is illustrated below.
(5) Der
The
Ausschuss
committe
bezweckt,
intends,
den
the
Institutionen
institutions
ein
a
politisches
political
Instrument
instrument
an
at
die
the
Hand
hand
zu
to
geben.
give.
(6) The committee set out to equip the institutions with a
political instrument.
Verb preposition combinations: While this
class isn?t discussed very often in the MWE lit-
erature, it can nevertheless be considered as an id-
iosyncratic combination of lexical items. Sag et al
(2002) propose an analysis within an MWE frame-
work.
(7) Sie
They
werden
will
den
the
Treibhauseffekt
green house effect
verschlimmern.
aggravate.
(8) They will add to the green house effect.
Light verb constructions (LVCs): This is the
most frequent pattern in our gold standard. It ac-
tually subsumes various subpatterns depending on
whether the light verbs complement is realized as a
noun, adjective or PP. Generally, LVCs are syntac-
tically and semantically more flexible than other
MWE types, such that our gold standard contains
variants of LVCs with similar, potentially mod-
ified adjectives or nouns, as in the example be-
low. However, it can be considered an idiosyn-
cratic combination since the LVCs exhibit specific
lexical restrictions (Sag et al, 2002).
(9) Ich
Ich
werde
will
die
the
Sache
thing
nur
only
noch
just
verschlimmern.
aggravate.
(10) I am just making things more difficult.
Idioms: This MWE type is probably the most
discussed in the literature due to its semantic and
syntactic idiosyncracy. It?s not very frequent in
our gold standard which may be mainly due to its
limited size and the source items we chose.
(11) Sie
They
bezwecken
intend
die
the
Umgestaltung
conversion
in
into
eine
a
zivile
civil
Nation.
nation.
(12) They have in mind the conversion into a civil nation.
v1 v2 v3 v4
Ntype 22 (26) 41 (47) 26 (35) 17 (24)
V Part 22.7 4.9 0.0 0.0
V Prep 36.4 41.5 3.9 5.9
LVC 18.2 29.3 88.5 88.2
Idiom 0.0 2.4 0.0 0.0
Para 36.4 24.3 11.5 23.5
Table 2: Proportions of MWE types per lemma
Paraphrases: From an MWE perspective, para-
phrases are the most problematic and challenging
type of translational correspondence in our gold
standard. While the MWE literature typically dis-
cusses the distinction between collocations and
MWEs, the boarderline between paraphrases and
MWEs is not really clear. On the hand, para-
phrases, as we classified them here, are transparent
combinations of lexical items, like in the exam-
ple below ensure that something increases. How-
ever, semantically, these transparent combinations
can also be rendered by an atomic expression in-
crease. A further problem raised by paraphrases is
that they often involve translational shifts (Cyrus,
2006). These shifts are hard to identify automat-
ically and present a general challenge for seman-
tic processing of parallel corpora. An example is
given below.
(13) Wir
We
brauchen
need
bessere
better
Zusammenarbeit,
cooperation
um
to
die
the
Ru?ckzahlungen
repayments.OBJ
anzuheben .
increase.
(14) We need greater cooperation in this respect to ensure
that repayments increase .
Table 2 displays the proportions of the MWE
categories for the number of types of one-to-many
correspondences in our gold standard. We filtered
the types due to morphological variations only (the
overall number of types is indicated in brackets).
Note that some types in our gold standard fall into
several categories, e.g. they combine a verb prepo-
sition with a verb particle construction. For all
of the verbs, the number of types belonging to
core MWE categories largely outweighs the pro-
portion of paraphrases. As we already observed
in our analysis of general translation categories,
here again, the different verb lemmas show strik-
ing differences with respect to their realization in
English translations. For instance, anheben (en.
increase) or bezwecken (en. intend) are frequently
25
translated with verb particle or preposition combi-
nations, while the other verbs are much more of-
ten translated by means of LVCs. Also, the more
specific LVC patterns differ largely among the
verbs. While verschlimmern (en. aggravate) has
many different adjectival LVC correspondences,
the translations of riskieren (en. risk) are predomi-
nantly nominal LVCs. The fact that we found very
few idioms in our gold standard may be simply
related to our arbitrary choice of German source
verbs that do not have an English idiom realiza-
tion (see our experiment on a random set of verbs
in Section 3.3).
In general, one-to-many translational corre-
spondences seem to provide a very fruitful ground
for the large-scale study of MWE phenomena.
However, their reliable detection in parallel cor-
pora is far from trivial, as we will show in the
next section. Therefore, we will not further in-
vestigate the classification of MWE patterns in
the rest of the paper, but concentrate on the high-
precision detection of one-to-many translations.
Such a pattern-independent identification method
is crucial for the further data-driven study of one-
to-many translations in parallel corpora.
3 Multiword Translation Detection
This section is devoted to the problem of high-
precision detection of one-to-many translations.
Section 3.1 describes an evaluation of automatic
word alignments against our gold standard. In
section 3.2, we describe a method that extracts
loosely aligned syntactic configurations which
yields much more promising results.
3.1 One-to-many Alignments
To illustrate the problem of purely distributional
one-to-many alignment, table 3 presents an eval-
uation of the automatic one-to-many word align-
ments produced by GIZA++ that uses the stan-
dard heuristics for bidirectional word alignment
from phrase-based MT (Och and Ney, 2003). We
evaluate the rate of translational correspondences
on the type-level that the system discovers against
the one-to-many translations in our gold standard.
By type we mean the set of lemmatized English
tokens that makes up the translation of the Ger-
man source lemma. Generally, automatic word
alignment yields a very high FPR if no frequency
threshold is used. Increasing the threshold may
help in some cases, however the frequency of the
verb n > 0 n > 1 n > 3
FPR FNR FPR FNR FPR FNR
v1 0.97 0.93 1.0 1.0 1.0 1.0
v2 0.93 0.9 0.5 0.96 0.0 0.98
v3 0.88 0.83 0.8 0.97 0.67 0.97
v4 0.98 0.92 0.8 0.92 0.34 0.92
Table 3: False positive rate and False negative rate
of GIZA++ one-to-many alignments
translation types is so low, that already at a thresh-
old of 3, almost all types get filtered. This does not
mean that the automatic word alignment does not
discover any correct correspondences at all, but it
means that the detection of the exact set of tokens
that correspond to the source token is rare.
This low precision of one-to-many alignments
isn?t very surprising. Many types of MWEs con-
sist of items that contribute most of the lexical se-
mantic content, while the other items belong to the
class of semantically almost ?empty? items (e.g.
particles, light verbs). These semantically ?light?
items have a distribution that doesn?t necessarily
correlate with the source item. For instance, in
the following sentence pair taken from Europarl,
GIZA++ was not able to capture the correspon-
dence between the German main verb behindern
(en. impede) and the LVC constitute an obstacle
to, but only finds an alignment link between the
verb and the noun obstacle.
(15) Die
The
Korruption
corruption
behindert
impedes
die
the
Entwicklung.
development.
(16) Corruption constitutes an obstacle to development.
Another limitation of the word-alignment mod-
els is that are independent of whether the sen-
tences are largely parallel or rather free transla-
tions. However, parallel corpora like Europarl are
know to contain a very large number of free trans-
lations. In these cases, direct lexical correspon-
dences are much more unlikely to be found.
3.2 Aligning Syntactic Configurations
High-precision extraction of one-to-many trans-
lation detection thus involves two major prob-
lems: 1) How to identify sentences or configura-
tions where reliable lexical correspondences can
be found? 2) How to align target items that have a
low occurrence correlation?
We argue that both of these problems can be
adressed by taking syntactic information into ac-
26
count. As an example, consider the pair of paral-
lel configurations in Figure 1 for the sentence pair
given in (15) and (16). Although there is no strict
one-to-one alignment for the German verb, the ba-
sic predicate-argument structure is parallel: The
verbs arguments directly correspond to each other
and are all dominated by a verbal root node.
Based on these intuitions, we propose a
generate-and-filter strategy for our one-to-many
translation detection which extracts partial, largely
parallel dependency configurations. By admitting
target dependency paths to be aligned to source
single dependency relations, we admit configura-
tions where the source item is translated by more
than one word. For instance, given the configura-
tion in Figure 1, we allow the German verb to be
aligned to the path connecting constitute and the
argument Y2.
Our one-to-many translation detection consists
of the following steps: a) candidate generation
of aligned syntactic configurations, b) filtering the
configurations c) alignment post-editing, i.e. as-
sembling the target tokens corresponding to the
source item. The following paragraphs will briefly
caracterize these steps.
behindert
X 1 Y 1
Y 2
an to
X 2 obstacle
create
Figure 1: Example of a typical syntactic MWE
configuration
Data We word-aligned the German and English
portion of the Europarl corpus by means of the
GIZA++ tool. Both portions where assigned flat
syntactic dependency analyses by means of the
MaltParser (Nivre et al, 2006) such that we ob-
tain a parallel resource of word-aligned depen-
dency parses. Each sentence in our resource can
be represented by the triple (DG, DE , AG,E). DG
is the set of dependency triples (s1, rel, s2) such
that s2 is a dependent of s1 of type rel and s1, s2
are words of the source language. DE is the set
of dependency triples of the target sentence. AG,E
corresponds to the set of pairs (s1, t1) such that
s1, t1 are aligned.
Candidate Generation This step generates a
list of source configurations by searching for oc-
curences of the source lexical verb where it is
linked to some syntactic dependents (e.g. its argu-
ments). An example input would be the configura-
tion ( (verb,SB,%), (verb,OA,%)) for
our German verbs.
Filtering Given our source candidates, a valid
parallel configuration (DG, DE , AG,E) is then de-
fined by the following conditions:
1. The source configuration DG is the set of tu-
ples (s1, rel, sn) where s1 is our source item and
sn some dependent.
2. For each sn ? DG, there is a tuple (sn, tn) ?
AG,E , i.e. every dependent has an alignment.
3. There is a target item t1 ? DE such that
for each tn, there is a p ? DE such that p is
a path (t1, rel, tx), (tx, rel, ty)...(tz, rel, tn) that
connects t1 and tn. Thus, the target dependents
have a common root.
To filter noise due to parsing or alignment er-
rors, we further introduce a filter on the length of
the path that connects the target root and its de-
pendents and w exclude paths cross contain sen-
tence boundaries. Moreover, the above candi-
date filtering doesn?t exclude configurations which
exhibit paraphrases involving head-switching or
complex coordination. Head-switching can be de-
tected with the help of alignment information: if
there is a item in our target configuration that has
an reliable alignment with an item not contained in
our source configuration, our target configuration
is likely to contain such a structural paraphrases
and is excluded from our candidate set. Coordina-
tion can be discarded by imposing the condition on
the configuration not to contain a coordination re-
lation. This Generate-and-Filter strategy now ex-
tracts a set of sentences where we are likely to find
a good one-to-one or one-to-many translation for
the source verb.
Alignment Post-editing In the final alignment
step, one now needs to figure out which lexical
material in the aligned syntactic configurations ac-
tually corresponds to the translation of the source
item. The intuition discussed in 3.2 was that all
27
the items lying on a path between the root item
and the terminals belong to the translation of the
source item. However, these items may have other
syntactic dependents that may also be part of the
one-to-many translation. As an example, consider
the configuration in figure 1 where the article an
which is part of the LVC create an obstacle to has
to be aligned to the German source verb.
Thus, for a set of items ti for which there is a de-
pendency relation (tx, rel, ti) ? DE such that tx is
an element of our target configuration, we need to
decide whether (s1, ti) ? AG,E . This translation
problem now largely parallels collocation trans-
lation problems discussed in the literature, as in
(Smadja and McKeown, 1994). But, crucially, our
syntactic filtering strategy has substantially nar-
rowed down the number of items that are possi-
ble parts of the one-to-many translation. Thus, a
straightforward way to assemble the translational
correspondence is to compute the correlation or
association of the possibly missing items with the
given translation pair as proposed in (Smadja and
McKeown, 1994). Therefore, we propose the fol-
lowing alignment post-editing algorithm:
Given the source item s1 and the set of target items
T , where each ti ? T is an element of our target
configuration,
1. Compute corr(s1, T ), the correlation be-
tween s1 and T .
2. For each ti, tx such that there is
a (ti, rel, tx) ? DE , compute
corr(s1, T + {tx})
3. if corr(s1, T + {tx}) ? corr(s1, T ), add tx
to T .
As the Dice coefficient is often to give the best
results, e.g. in (Smadja and McKeown, 1994), we
also chose Dice as our correlation measure. In fu-
ture work, we will experiment with other associa-
tion measures. Our correlation scores are thus de-
fined by the formula:
corr(s1, T ) =
2(freq(s1 ? T ))
freq(s1) + freq(T )
We define freq(T ) as the number of sentence
pairs whose target sentence contains occurrences
of all ti ? T , and freq(s1) accordingly. The ob-
servation frequency freq(s1?T ) is the number of
sentence pairs that where s1 occurs in the source
sentence, and T in the target sentence.
The output translation can then be rep-
resented as a dependency configuration
of the following kind :((of,PMOD,%),
(risk,NMOD,of),(risk,NMOD,the), (run,OBJ,risk),
(run,SBJ,%)) which is the syntactic representation
for the English MWE run the risk of.
3.3 Evaluation
Our translational approach to MWE extraction
bears the advantage that evaluation is not exclu-
sively bound to the manual judgement of candi-
date lists. Instead, we can first evaluate the system
output against translation gold standards which are
easier to obtain. The linguistic classification of the
candidates according to their compositionality can
then be treated as a separate problem.
We present two experiments in this evaluation
section: We will first evaluate the translation de-
tection on our gold standard to assess the gen-
eral quality of the extraction method. Since this
gold standard is to small to draw conclusions about
the quality of MWE patterns that the system de-
tects, we further evaluate the translational corre-
spondences for a larger set of verbs.
Translation evaluation: In the first experiment,
we extracted all types of translational correspon-
dences for the verbs we annotated in the gold stan-
dard. We converted the output dependency con-
figurations to the lemmatized bag-of-word form
we already applied for the alignment evaluation
and calculated the FPR and FNR of the trans-
lation types. The evaluation is displayed in ta-
ble 4. Nearly all translation types that our sys-
tem detected are correct. This confirms our hy-
pothesis that syntactic filtering yields more reli-
able translations that just coocurrence-based align-
ments. However, the false negative rate is also
very high. This low recall is due to the fact that
our syntactic filters are very restrictive such that a
major part of the occurrences of the source lemma
don?t figure in the prototypical syntactic configu-
ration. Column two and three of the evaluation ta-
ble present the FPR and FNR for experiments with
a relaxed syntactic filter that doesn?t constrain the
syntactic type of the parallel argument relations.
While not decreasing the FNR, the FPR decreases
significantly. This means that the syntactic filters
mainly fire on noisy configurations and don?t de-
crease the recall. A manual error analysis has also
shown that the relatively flat annotation scheme of
our dependency parses significantly narrows down
28
the number of candidate configurations that our al-
gorithm detects. As the dependency parses don?t
provide deep analyses for tense or control phe-
nomena, very often, a verb?s arguments don?t fig-
ure as its syntactic dependents and no configura-
tion is found. Future work will explore the im-
pact of deep syntactic analysis for the detection of
translational correspondences.
MWE evaluation: In a second experiment, we
evaluated the patterns of correspondences found
by our extraction method for use in an MWE con-
text. Therefore, we selected 50 random verbs oc-
curring in the Europarl corpus and extracted their
respective translational correspondences. This set
of 50 verbs yields a set of 1592 one-to-many types
of translational correspondences. We filtered the
types wich display only morphological variation,
such that the set of potential MWE types com-
prises 1302 types. Out of these, we evaluated a
random sample of 300 types by labelling the types
with the MWE categories we established for the
analysis of our gold standard. During the clas-
sification, we encountered a further category of
oneto- many correspondence which cannot be con-
sidered an MWE, the category of alternation. For
instance, we found a translational correspondence
between the active realization of the German verb
begru??en (en. appreciate) and the English passive
be pleased by.
The classification is displayed in table 5. Al-
most 83% of the translational correspondences
that our system extracted are perfect translation
types. Almost 60% of the extracted types can be
considered MWEs that exhibit some kind of se-
mantic idiosyncrasy. The other translations could
be classified as paraphrases or alternations. In our
random sample, the portions of idioms is signifi-
cantly higher than in our gold standard which con-
firms our intuition that the MWE pattern of the
one-to-many translations for a given verb are re-
lated to language-specific, semantic properties of
the verbs and the lexical concepts they realize.
4 Related Work
The problem sketched in this paper has clear con-
ncetions to statistical MT. So-called phrase-based
translation models generally target whole sentence
alignment and do not necessarily recur to linguis-
tically motivated phrase correspondences (Koehn
et al, 2003). Syntax-based translation that speci-
fies formal relations between bilingual parses was
Strict Filter Relaxed Filter
FPR FNR FPR FNR
v1 0.0 0.96 0.5 0.96
v2 0.25 0.88 0.47 0.79
v3 0.25 0.74 0.56 0.63
v4 0.0 0.875 0.56 0.84
Table 4: False positive and false negative rate of
one-to-many translations.
Trans. type Proportion
MWE type Proportion
MWEs 57.5%
V Part 8.2%
V Prep 51.8%
LVC 32.4%
Idiom 10.6%
Paraphrases 24.4%
Alternations 1.0%
Noise 17.1%
Table 5: Classification of 300 types sampled from
the set of one-to-many translations for 50 verbs
established by (Wu, 1997). Our way to use syn-
tactic configurations can be seen as a heuristic to
check relaxed structural parallelism.
Work on MWEs in a crosslingual context has
almost exclusively focussed on MWE translation
(Smadja and McKeown, 1994; Anastasiou, 2008).
In (Villada Moiro?n and Tiedemann, 2006), the au-
thors make use of alignment information in a par-
allel corpus to rank MWE candidates. These ap-
proaches don?t rely on the lexical semantic knowl-
edge about MWEs in form of one-to-many trans-
lations.
By contrast, previous approaches to paraphrase
extraction made more explicit use of crosslingual
semantic information. In (Bannard and Callison-
Burch, 2005), the authors use the target language
as a pivot providing contextual features for iden-
tifying semantically similar expressions. Para-
phrasing is however only partially comparable to
the crosslingual MWE detection we propose in
this paper. Recently, the very pronounced context
dependence of monolingual pairs of semantically
similar expressions has been recognized as a ma-
jor challenge in modelling word meaning (Erk and
Pado, 2009).
The idea that parallel corpora can be used as
a linguistic resource that provides empirical evi-
dence for monolingual idiosyncrasies has already
29
been exploited in, e.g. morphology projection
(Yarowsky et al, 2001) or word sense disambigua-
tion (Dyvik, 2004). While in a monolingual set-
ting, it is quite tricky to come up with theoretical
or empirical definitions of sense discriminations,
the crosslingual scenario offers a theory-neutral,
data-driven solution: Since ambiguity is an id-
iosyncratic property of a lexical item in a given
language, it is not likely to be mirrored in a tar-
get language. Similarly, our approach can also be
seen as a projection idea: we project the semantic
information of simplex realization in a source lan-
guage to an idiosyncratic, multiword realization in
the target language.
5 Conclusion
We have explored the phenomenon of one-to-
many translations in parallel corpora from the
perspective of MWE identification. Our man-
ual study on a translation gold standard as well
as our experiments in automatic translation ex-
traction have shown that one-to-many correspon-
dences provide a rich resource and fruitful basis
of study for data-driven MWE identification. The
crosslingual perspective raises new research ques-
tions about the identification and interpretation of
MWEs. It challenges the distinction between para-
phrases and MWEs, a problem that does not arise
at all in the context of monolingual MWE ex-
traction. It also allows for the study of the rela-
tion between the semantics of lexical concepts and
their MWE realization. Further research in this di-
rection should investigate translational correspon-
dences on a larger scale and further explore these
for monolingual interpretation of MWEs.
Our extraction method that is based on syn-
tactic filters identifies MWE types with a much
higher precision than purely cooccurence-based
word alignment and captures the various patterns
we found in our gold standard. Future work on the
extraction method will have to focus on the gener-
alization of these filters and the generalization to
other items than verbs. The experiments presented
in this paper also suggest that the MWE realiza-
tion of certain lexical items in a target language
is subject to certain linguistic patterns. Moreover,
the method we propose is completely languagein-
dependent such that further research has to study
the impact of the relatedness of the considered
languages on the patterns of one-to-many transla-
tional correspondences.
References
Dimitra Anastasiou. 2008. Identification of idioms by mt?s
hybrid research system vs. three commercial system. In
Proceedings of the EAMT, pp. 12?20.
Timothy Baldwin and Aline Villavicencio. 2002. Extract-
ing the unextractable: a case study on verb-particles. In
Proceedings of the COLING-02, pp. 1?7.
Colin Bannard and Chris Callison-Burch. 2005. Paraphras-
ing with bilingual parallel corpora. In Proceedings of the
43rd Annual Meeting of the ACL, pp. 597?604 .
Lea Cyrus. 2006. Building a resource for studying transla-
tion shifts. In Proceedings of the 5th LREC, pp. 1240?
1245.
Helge Dyvik. 2004. Translations as semantic mirrors. From
parallel corpus to WordNet. Language and Computers,
1:311 ? 326.
Katrin Erk and Sebastian Pado. 2009. Paraphrase assess-
ment in structured vector space: Exploring parameters and
datasets. In Proc. of the EACL GEMS Workshop, pp. 57?
65.
Joa?o de Almeida Varelas Grac?a, Joana Paulo Pardal, Lu??sa
Coheur, and Diamantino Anto?nio Caseiro. 2008. Multi-
language word alignments annotation guidelines. Techni-
cal report, Tech. Rep. 38 / 2008 INESC-ID Lisboa.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of the
NAACL ?03, pp. 48?54.
Philipp Koehn. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In MT Summit 2005, pp. 79?86.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
parser: A data driven parser-generator for dependency
parsing. In Proc. of LREC-2006, pp. 2216?2219.
Franz Josef Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51.
Pavel Pecina. 2008. A machine learning approach to multi-
word expression extraction. In Proceedings of the LREC
MWE 2008 Workshop, pp. 54?57.
Ivan A. Sag, Timothy Baldwin, Francis Bond, and Ann
Copestake. 2002. Multiword expressions: A pain in the
neck for NLP. In Proc. of the CICLing-2002, pp. 1?15.
Frank Smadja and Kathleen McKeown. 1994. Translating
collocations for use in bilingual lexicons. In Proceedings
of the HLT ?94 workshop, pp. 152?156.
Begon?a Villada Moiro?n and Jo?rg Tiedemann. 2006.
Identifying idiomatic expressions using automatic word-
alignment. In Proc. of the EACL MWE 2006 Workshop,
pp. 33?40.
Dekai Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Comput.
Linguist., 23(3):377?403.
David Yarowsky, Grace Ngai, and Richard Wicentowski.
2001. Inducing multilingual text analysis tools via robust
projection across aligned corpora. In Proceedings of HLT
2001, pp. 1?8.
30
Coling 2010: Poster Volume, pages 1426?1434,
Beijing, August 2010
Cross-Lingual Induction for Deep Broad-Coverage Syntax: A Case
Study on German Participles
Sina Zarrie? Aoife Cahill Jonas Kuhn Christian Rohrer
Institut fu?r Maschinelle Sprachverarbeitung (IMS), University of Stuttgart
{zarriesa,cahillae,jonas.kuhn,rohrer}@ims.uni-stuttgart.de
Abstract
This paper is a case study on cross-lingual
induction of lexical resources for deep,
broad-coverage syntactic analysis of Ger-
man. We use a parallel corpus to in-
duce a classifier for German participles
which can predict their syntactic category.
By means of this classifier, we induce a
resource of adverbial participles from a
huge monolingual corpus of German. We
integrate the resource into a German LFG
grammar and show that it improves pars-
ing coverage while maintaining accuracy.
1 Introduction
Parallel corpora are currently exploited in a wide
range of induction scenarios, including projection
of morphologic (Yarowsky et al, 2001), syntactic
(Hwa et al, 2005) and semantic (Pado? and Lap-
ata, 2009) resources. In this paper, we use cross-
lingual data to learn to predict whether a lexi-
cal item belongs to a specific syntactic category
that cannot easily be learned from monolingual re-
sources. In an application test scenario, we show
that this prediction method can be used to obtain
a lexical resource that improves deep, grammar-
based parsing.
The general idea of cross-lingual induction is
that linguistic annotations or structures, which are
not available or explicit in a given language, can
be inferred from another language where these an-
notations or structures are explicit or easy to ob-
tain. Thus, this technique is very attractive for
cheap acquisition of broad-coverage resources, as
is proven by the approaches cited above. More-
over, this induction process can be attractive for
the induction of deep (and perhaps specific) lin-
guistic knowledge that is hard to obtain in a mono-
lingual context. However, this latter perspective
has been less prominent in the NLP community
so far.
This paper investigates a cross-lingual induc-
tion method based on an exemplary problem aris-
ing in the deep syntactic analysis of German. This
showcase is the syntactic flexibility of German
participles, being morphologically ambiguous be-
tween verbal, adjectival and adverbial readings,
and it is instructive for several reasons: first, the
phenomenon is a notorious problem for linguistic
analysis and annotation of German, such that stan-
dard German resources do not represent the under-
lying analysis. Second, in Zarrie? et al (2010),
we showed that integrating the phenomenon of
adverbial participles in a naive way into a broad-
coverage grammar of German leads to significant
parsing problems, due to spurious ambiguities.
Third, it is completely straightforward to detect
adverbial participles in cross-lingual data since in
other languages, e.g. English or French, adverbs
are often morphologically marked.
In this paper, we use instances of adverbially
translated participles in a parallel corpus to boot-
strap a classifier that is able to identify an ad-
verbially used participle based on its monolingual
syntactic context. In contrast to what is commonly
assumed, we show that it is possible to detect ad-
verbial participles using only a relatively narrow
context window. This classifier enables us to iden-
tify an occurence of an adverbial participle inde-
pendently of its translation in a parallel corpus,
going far beyond the induction methodology in
Zarrie? et al (2010). By means of the participle
classifier, we can extract new types of adverbial
participles from a larger corpus of German news-
paper text and substantially augment the size of
the resource extracted only on Europarl data. Fi-
nally, we integrate this new resource into the Ger-
man LFG grammar and show that it improves cov-
erage without negatively affecting performance.
1426
The paper is structured as follows: in Sec-
tion 2, we describe the linguistic and computa-
tional problems related to the parsing of adver-
bial participles in German. Section 3 introduces
the general idea of using the translation data to
find instances of different participle categories. In
Section 4, we illustrate the training of the clas-
sifier, evaluating the impact of the context win-
dow and the quality of the training data obtained
from cross-lingual text. In Section 5, we apply the
classifier to new, monolingual data and describe
the extension of the resource for adverbial partici-
ples. Section 6 evaluates the extended resource by
means of parsing experiments using the German
LFG grammar.
2 The Problem
In German, past perfect participles are ambiguous
with respect to their morphosyntactic category. As
in other languages, they can be used as part of
the verbal complex (Example (1-a)) or as adjec-
tives (Example (1-b)). Since German adjectives
can generally undergo conversion into adverbs,
participles can also be used adverbially (Example
(1-c)). The verbal and adverbial participle forms
are morphologically identical.
(1) a. Sie haben das Experiment wiederholt.
?They have repeated the experiment.?
b. Das wiederholte Experiment war erfolgreich.
?The repeated experiment was succesful.?
c. Sie haben das Experiment wiederholt abge-
brochen.
?They cancelled the experiment repeatedly.?
Moreover, German adjectival modifiers can be
generally used as predicatives that can be either
selected by a verb (Example (2-a)) or that can oc-
cur as free predicatives (Example (2-b)).
(2) a. Er scheint begeistert von dem Experiment.
?He seems enthusiastic about the experiment.?
b. Er hat begeistert experimentiert.
?He has experimented enthusiastic.?
Since predicative adjectives are not inflected,
the surface form of a German participle is ambigu-
ous between a verbal, predicative or adverbial use.
2.1 Participles in the German LFG
In order to account for sentences like (1-c), an in-
tuitive approach would be to generally allow for
adverb conversion of participles in the grammar.
However, in Zarrie? et al (2010), we show that
such a rule can have a strong negative effect on
the overall performance of the parsing system, de-
spite the fact that it produces the desired syntac-
tic and semantic analysis for specific sentences.
This problem was illustrated using a German LFG
grammar (Rohrer and Forst, 2006) constructed as
part of the ParGram project (Butt et al, 2002).
The grammar is implemented in the XLE, a gram-
mar development environment which includes a
very efficient LFG parser and a stochastic dis-
ambiguation component which is based on a log-
linear probability model (Riezler et al, 2002).
In Zarrie? et al (2010), we found that the
naive implementation of adverbial participles in
the German LFG, i.e. in terms of a general gram-
mar rule that allows for participles-adverb conver-
sion, leads to spurious ambiguities that mislead
the disambiguation component of the grammar.
Moreover, the rule increases the number of time-
outs, i.e. sentences that cannot be parsed in a pre-
defined amount of time (20 seconds). Therefore,
we observe a drop in parsing accuracy although
grammar coverage is improved. As a solution, we
induced a lexical resource of adverbial participles
based on their adverbial translations in a paral-
lel corpus. This resource, comprising 46 partici-
ple types, restricts the adverb conversion such that
most of the spurious ambiguities are eliminated.
To assess the impact of specific rules in a broad-
coverage grammar, possibly targeting medium-to-
low frequency phenomena, we have established a
fine-grained evaluation methodology. The chal-
lenge posed by these low-frequent phenomena is
typically two-fold: on the one hand, if one takes
into account the disambiguation component of the
grammar and pursues an evaluation of the most
probable parses on a general test set, the new
grammr rule cannot be expected to show a positive
effect since the phenomenon is not likely to occur
very often in the test set. On the other hand, if one
is interested in a linguistically precise grammar,
it is very unsatisfactory to reduce grammar cov-
erage to statistically frequent phenomena. There-
fore, we combined a coverage-oriented evaluation
on specialised testsuites with a quantitative evalu-
ation including disambiguation, making sure that
1427
the increased coverage does not lead to an overall
drop in accuracy. The evaluation methodolgy will
also be applied to evaluate the impact of the new
participle resource, see Section 6.
2.2 The Standard Flat Analysis of Modifiers
The fact that German adjectival modifiers can gen-
erally undergo conversion into adverbs without
overt morphological marking is a notorious prob-
lem for the syntactic analysis of German: there
are no theoretically established tests to distinguish
predicative adjectives and adverbials, see Geuder
(2004). For this reason, the standard German tag
set assigns a uniform tag (?ADJD?) to modifiers
that are morphologically ambiguous between an
adjectival and adverbial reading. Moreover, in
the German treebank TIGER (Brants et al, 2002)
the resulting syntactic differences between the two
readings are annotated by the same flat structure
that does not disambiguate the sentence.
Despite certain theoretical problems related to
the analysis of German modifiers, their interpre-
tation in real corpus sentences is often unambigu-
ous for native speakers. As an example, consider
example (3) from the TIGER treebank. In the
sentence, the participle unterschrieben (signed)
clearly functions as a predicative modifier of the
sentence?s subject. The other, theoretically possi-
ble reading where the participle would modify the
verb send is semantically not acceptable. How-
ever, in TIGER, the participle is analysed as an
ADJD modifier attached under the VP node which
is the general analysis for adjectival and adverbial
modifiers.
(3) Die
It
sollte
should
unterschrieben
signed
an
to
die
the
Leitung
administration
zuru?ckgesandt
sent back
werden.
be.
?It should be sent back signed to the administation.?
Sentence (4) (also taken from TIGER) illus-
trates the case of an adverbial participle. In this
example, the reading where angemessen (ade-
quately) modifies the main verb is the only one
that is semantically plausible. In the treebank, the
participle is tagged as ADJD and analysed as a
modifier in the VP.
(4) Der
The
menschliche
human
Geist
mind
la??t
lets
sich
itself
rechnerisch
computationally
nicht
not
angemessen
adequately
simulieren.
simulate.
?The human mind cannot be adequately simulated in a
computational way.?
The flat annotation strategy adopted for modi-
fiers in the standard German tag set and in the tree-
bank TIGER entails that instances of adverbs (and
adverbial participles) cannot be extracted from au-
tomatically tagged, or parsed, text. Therefore,
it would be very hard to obtain training mate-
rial from German resources to train a system that
automatically identifies adverbially used partici-
ples. However, the intuition corroborated by the
examples presented in this section is that the struc-
tures can actually be disambiguated in many cor-
pus sentences.
In the following sections, we show how we ex-
ploit parallel text to obtain training material for
learning to predict occurences of adverbial par-
ticiples, without any manual effort. Moreover, by
means of this technique, we can substantially ex-
tend the grammatical resource for adverbial par-
ticiples compared to the resource that can be di-
rectly extracted from the parallel text.
3 Participles in the Parallel Corpus
The intuition of the cross-lingual induction ap-
proach is that adverbial participles can easily be
extracted from parallel corpora since in other lan-
guages (such as English or French) adverbs are
often morphologically marked and easily labelled
by statistical PoS taggers. As an example, con-
sider sentence (5) extracted from Europarl, where
the German participle versta?rkt is translated by an
English adverb (increasingly).
(5) a. Nicht
Not
ohne
without
Grund
reason
sprechen
speak
wir
we
versta?rkt
increasingly
vom
of a
Europa
Europe
der
of the
Regionen.
Regions.
b. It is not without reason that we increasingly speak
in terms of a Europe of the Regions.
The idea is to project specific morphological
information about adverbs which is overt in lan-
guages like English onto German where adverbs
cannot be directly extracted from tagged data.
While this idea might seem intuitively straightfor-
1428
ward, we also know that translation pairs in paral-
lel data are not always lingusitically parallel, and
as a consequence, word-alignment is not always
reliable. To assess the impact of non-parallelism
in adverbial translations of German participles,
we manually annotated a sample of 300 transla-
tions. This data also constitutes the basis for the
experiments reported in Section 4.
3.1 Data
Our experiments are based on the same data as in
(Zarrie? et al, 2010). For convenience, we pro-
vide a short description here.
We limit our investigations to non-lexicalised
participles occuring in the Europarl corpus and
not yet recorded as adverbs in the lexicon of the
German LFG grammar (5054 participle types in
total). Given the participle candidates, we ex-
tract the set of sentences that exhibit a word align-
ment between a German participle and an English,
French or Dutch adverb. The word alignments
have been obtained with GIZA++. The extrac-
tion yields 27784 German-English sentence pairs
considering all alignment links, and 5191 sen-
tence pairs considering only bidirectional align-
ments between a participle and an English adverb.
3.2 Systematic Non-Parallelism
For data exploration and evaluation, we anno-
tated 300 participle alignments out of the 5191
German-English sentences (with a bidirectional
participle-adverb alignment). We distinguish the
following annotation categories: (i) parallel trans-
lation, adverb information can be projected, (ii)
incorrect alignment, (iii) correct alignment, but
translation is a multi-word expression, (iv) correct
alignment, but translation is a paraphrase (possi-
bly involving a translation shift).
Parallel Cases In our annotated sample of En-
glish adverb - German participle pairs, 43%1 of
the translation instances are parallel in the sense
that the overt adverb information from the English
side can be projected onto the German participle.
This means that if we base the induction technique
1The diverging figures we report in Zarrie? et al (2010)
were due to a small bug in the script and it does not affect the
overall interpretation of the data.
on word-alignments alone, its precision would be
relatively low.
Non-Parallel Cases Taking a closer look at the
non-parallel cases in our sample (57% of the
translation pairs), we find that 47% of this set are
due to incorrect word alignments. The remain-
ing 53% thus reflect regular cases of non-parallel
translations. A typical configuration which makes
up 30% of the the non-parallel cases is exempli-
fied in (6) where the German main verb vorlegen
is translated by the English multiword expression
put forward.
(6) a. Wir haben eine Reihe von Vorschla?gen vorgelegt.
b. We have put forward a number of proposals.
An example for the general paraphrase or trans-
lation shift category is given in Sentence (7).
Here, the translational correspondence between
gekommen (arrived) and the adverb now is due
to language-specific, idiomatic realisations of an
identical underlying semantic concept. The para-
phrase translations make up 23% of the non-
parallel cases in the annotated sample.
(7) a. Die
That
Zeit
time
ist
is
noch
yet
nicht
not
gekommen
arrived.
.
b. That time is not now .
Furthermore, it is noticeable that the cross-
lingual approach seems to inherently factor out
the ambiguity between predicative and adverbial
participles. In our annotated sample, there are no
predicative participles that have been translated by
an English adverb.
3.3 Filtering Mechanisms
The data analysis in the previous section, show-
ing only 43% of parallel cases in English adverb
translations for German participles, mainly con-
firms other studies in annotation projection which
find that translational correspondences only allow
for projection of linguistic analyses in a more or
less limited proportion (Yarowsky et al, 2001;
Hwa et al, 2005; Mihalcea et al, 2007).
In previous studies on annotation projection,
quite distinct filtering methods have been pro-
posed: in Yarowsky et al (2001), projection er-
rors are mainly attributed to word alignment er-
rors and filtered based on translation probabilities.
1429
Hwa et al (2005) find that errors in the projec-
tion of syntactic relations are also due to system-
atic grammatical divergences between languages
and propose correcting these errors by means of
specific, manually designed filters. Bouma et al
(2008) make similar observations to Hwa et al
(2005), but try to replace manual correction rules
by filters from additional languages.
In Zarrie? et al (2010), we compared a num-
ber of filtering techniques on our participle data.
The 300 annotated translation instances are used
as a test set for evaluation. In particular, we
have established that a combination of syntactic
dependency-based filters and multilingual filters
can very accurately separate non-parallel transla-
tions from parallel ones where the adverb infor-
mation can be projected. In Section 4, we show
that these filtering techniques are also very useful
for removing noise from the training material that
we use to build a classifier.
4 Bootstrapping a German Participle
Classifier from Crosslingual Data
In the previous section, we have seen that German
adverbial participles can be easily found in cross-
lingual text by looking at their translations in a
language that morphologically marks adverbials.
In previous work, we exploited this observation
by directly extracting types of adverbial partici-
ples based on word alignment links and the filter-
ing mechanisms mentioned in Section 3. How-
ever, this method is very closely tied to data in
the parallel corpus, which only comprises around
5000 participle-adverb translations in total, which
results in 46 types of adverbial participles after fil-
tering. Thus, we have no means of telling whether
we would discover new types of adverbial partici-
ples in other corpora, from different domains to
Europarl. As this corpus is rather small and genre
specific, it even seems very likely that one could
find additional adverbial participles in a bigger
corpus. Moreover, we cannot be sure that certain
adverbial participles have systematically diverg-
ing translations in other languages, due to cross-
lingual lexicalisation differences. Generally, it is
not clear whether we have learned something gen-
eral about the syntactic phenomenon of adverbial
participles in German or whether we have just ex-
tracted a small, corpus-dependent subset of the
class of adverbial participles.
In this section, we use instances of adverbially
translated participles as training material for a
classifier that learns to predict adverbial partici-
ples based on their monolingual syntactic context.
Thus, we exploit the translations in the parallel
corpus as a means of obtaining ?annotated? or dis-
ambiguated training data without any manual ef-
fort. During training, we only consider the mono-
lingual context of the participle, such that the fi-
nal application of the classifier is not dependent
on cross-lingual data anymore.
4.1 Context-based Identification of
Adverbial Participles
Given the general linguistic problems related to
adverbial participles (see Section 2), one could
assume that it is very difficult to identify them
in a given context. To assess the general dif-
ficulty of this syntactic problem, we run a first
experiment comparing a grammar-based identifi-
cation method against a classifier that only con-
siders relatively narrow morpho-syntactic context.
For evaluation, we use the 300 annotated partici-
ple instances described in Section 3. This test
set divides into 172 negative instances, i.e. non-
adverbial participles, and 128 positive instances.
We report accuracy of the identification method,
as well as precision and recall relating to the num-
ber of correctly predicted adverbial participles.
For the grammar-based identification, we use
the German LFG which integrates the lexical
resource for adverbial participles established in
(Zarrie? et al, 2010). We parse the 300 Europarl
sentences and check whether the most probable
parse proposed by the grammar analyses the re-
spective participle as an adverb or not. The gram-
mar obtains a complete parse for 199 sentences
out of the test set and we only consider these in
the evaluation. The results are given in Table 1.
The high precision and accuracy of the
grammar-based identification of adverbial partici-
ples suggests that in a lot of sentences, the adver-
bial analysis is the only possible reading, i.e. the
only analysis that makes the sentence grammati-
cal. But of course, we have substantially restricted
the adverb participle-conversion in the grammar,
1430
Training Data Precision Recall Accuracy
Grammar 97.3 90.12 94.97
Classifier Unigram 87.10 84.38 87.92
Classifier Bigram 88.28 88.28 89.93
Classifier Trigram 89.60 87.5 90.27
Table 1: Evaluation on 300 participle instances
from Europarl
so that it does not propose adverbial analyses for
participles that are very unlikely to function as
modifiers of verbs.
For the classifier-based identification, we use
the adverbially translated participle tokens in our
Europarl data (5191 tokens in total) as training
material. We remove the 300 test instances from
this training set, and then divide it into a set of
positive and negative instances. To do this, we
use the filtering mechanisms already proposed in
Zarrie? et al (2010). These filters apply on the
type level, such that we first identify the positive
types (46 total) and then use all instances of these
types in the 4891 sentences as positive instances
of adverbial participles (1978 instances). The re-
maining sentences are used as negative instances.
For the training of the classifier, we use
maximum-entropy classification, which is also
commonly used for the general task of tagging
(Ratnaparkhi, 1996). In particular, we use the
open source TADM tool for parameter estimation
(Malouf, 2002). The tags of the words surround-
ing the participles are used as features in the clas-
sification task. We explore different sizes of the
context window, where the trigram window is the
most succesful (see Table 1). Beyond the trigram
window, the results of the classifier start decreas-
ing again, probably because of too many mislead-
ing features. Generally, this experiment shows
that the grammar-based identification is more pre-
cise, but that the classifier still performs surpris-
ingly well. Compared to the results from the
grammar-based identification, the high accuracy
of the classifier suggests that even the narrow syn-
tactic contexts of adverbial vs. non-adverbial par-
ticiples are quite distinct.
4.2 Designing Training Data for Participle
Classification
There are several questions related to the design
of the training data that we use to build our clas-
sifier. First, it is not clear how many negative
instances are helpful for learning the adverbial -
non-adverbial distinction. In the above experi-
ment, we simply use the instances that do not pass
the cross-lingual filters. In this section, we exper-
iment with an augmented set of negative instances
that was also obtained by extracting German par-
ticiple that are bi-directionally aligned to an En-
glish participle in Europarl. This is based on the
assumption that these participles are very likely
to be verbal. Second, it is not clear whether we
really need the filtering mechanisms proposed in
Zarrie? et al (2010) and whether we could im-
prove the classifier by training it on a larger set
of positive instances. Therefore, we also experi-
ment with two further sets of positive instances:
one where we used all participles (not necessarily
bidirectionally) aligned to an adverb, one where
we only use the bidirectional alignments. The re-
sults obtained for the different sizes of positive
and negative instance sets are given in Table 2.
The picture that emerges from the results in Ta-
ble 2 is very clear: the stricter the filtering of the
training material (i.e. the positive instances) is,
the better the performance of the classifier. The
fact that we (potentially) loose certain positive in-
stances in the filtering does not negatively impact
on the classifier which substantially benefits from
the fact that noise gets removed. Moreover, we
find that if the training material is appropriately
filtered, adding further negative instances does not
help improving the accuracy. By contrast, if we
train on a noisy set of positive instances, the clas-
sifier benefits from a larger set of negative in-
stances. However, the positive effect that we get
from augmenting the non-filtered training data is
still weaker than the positive effect we get from
the filtering.
5 Induction of Adverbial Participles on
Monolingual Data
Given the classifier from Section 4 that predicts
the syntactic category of a participle instance
1431
Training Data Pos. Instances Neg. Instances Precision Recall Accuracy
Non-Filtered Instances (all alignments) 27.184 10.000 43.10 100 43.10
Non-Filtered Instances (all alignments) 27.184 50.000 74.38 92.97 83.22
Non-Filtered Instances (symm. alignments) 4891 10.000 78.08 89.06 84.56
Non-Filtered Instances (symm. alignments) 4891 50.000 82.31 83.59 85.23
Filtered Instances 1978 10.000 91.60 85.16 90.27
Filtered Instances 1978 50.000 90.83 77.34 86.91
Table 2: Evaluation on 300 participle instances from Europarl
based on its monolingual syntactic context, we
can now detect new instances or types of adver-
bial participles in any PoS-tagged German corpus.
In this section, we investigate whether the classi-
fier can be used to augment the resource of ad-
verbial participles directly induced from Europarl
with new types.
5.1 Data Extraction
We run our extraction experiment on the Huge
German Corpus (HGC), a corpus of 200 million
words of newspaper and other text. This corpus
has been tagged with TreeTagger (Schmid, 1994).
For each of the 5054 participle candidates, we ex-
tract all instances from the HGC which have not
been tagged as finite verbs (at most 2000 tokens
per participle). For each participle token, we also
extract its syntactic context in terms of the 3 pre-
ceding and the 3 following tags. For classification,
we use only those participles that have more than
50 instances in the corpus (2953 types).
In contrast to the cross-lingual filtering mech-
anisms developed in Zarrie? et al (2010) which
operate on the type-level, the classifier makes a
prediction for every token of a given participle
candidate. Thus, for each of the participle can-
didates, we obtain a percentage of instances that
have been classified as adverbs. As we would ex-
pect, the percentage of adverbial instances is very
low for most of the participles in our candidate set:
for 75% of the 2953 types, the percentage is below
5%. This result confirms our initial intuition that
the property of being used as an adverb is strongly
lexically restricted to a certain class of participles.
5.2 Evaluation
Since we know that the classifier has an accu-
racy of 90% on the Europarl data, we only con-
sider participles as candidates for adverbs where
the classifier predicted more than 14% adverbial
instances. This leaves us with a set of 210 partici-
ples, which comprises 13 of the original 46 par-
ticiples extracted from Europarl, meaning we have
discovered 197 new adverbial participle types.
We performed a manual evaluation of 50 ran-
domly selected types out of the set of 197 new
participle types. Therefore, we looked at the in-
stances and their context which the classifier pre-
dicted to be adverbial. If there was at least one ad-
verbial instance among these, the participle type
was evaluated as correctly annotated by the clas-
sifier. By this means, we find that 76% of the par-
ticiples were correctly classified.
This evaluation suggests that the accuracy of
our classifier which we trained and tested on Eu-
roparl data is lower on the HGC data. The rea-
son for this drop in performance will be explained
in the following Section 5.3. However, assuming
an accuracy of 76%, we have discovered 150 new
types of adverbial participles. We argue that this is
a very satisfactory result given that we have not in-
vested any manual effort into the annotation or ex-
traction of adverbial participles. This results also
makes clear that the previous resource we induced
on Europarl data, comprising only 46 participle
types, was a very limited one.
5.3 Error Analysis
Taking a closer look at the 12 participle candi-
dates that the classifier incorrectly labels as adver-
bial, we observe that their adverbially classified
instances are mostly instances of a predicative use.
This means that our Europarl training data does
not contain enough evidence to learn the distinc-
tion between adverbial and predicative participles.
This is not surprising since the set of negative
instances used for training the classifier mainly
comprises verbal instances of participles. More-
over, the syntactic contexts and constructions in
which some predicatives and adverbials are used
1432
Grammar Prec. Rec. F-Sc. Time
in sec
46 Part-Adv 84.12 78.2 81.05 665
243 Part-Adv 84.12 77.67 80.76 665
Table 3: Evaluation on 371 TIGER sentences
are very similar. Thus, in future work, we will
have to include more data on predicatives (which
is more difficult to obtain) and analyse the syntac-
tic contexts in more detail.
6 Assessing the Impact of Resource
Coverage on Grammar-based Parsing
In this section, we evaluate the classifier-based in-
duction of adverbial participles from a grammar-
based perspective. We integrate the entire set of
induced adverbial participles (46 from Europarl
and 197 from the HGC) into the German LFG
grammar. As a consequence, the grammar al-
lows the adverb conversion for 243 lexical par-
ticiple types. We use the evaluation methodolgy
explained in Section 2.
First, we conduct an accuracy-oriented evalua-
tion on the standard TIGER test set. We compare
against the German LFG that only integrates the
small participle resource from Europarl. The re-
sults are given in Table 3. The difference between
the 46 Part-Adv and 243 Part-Adv resource is not
statistically signficant. Thus, the larger participle
resource has no overall negative effect on the pars-
ing performance. As established by an automatic
upperbound evaluation in Zarrie? et al (2010),
we cannot not expect to find a positive effect in
this evaluation because the phenomenon does not
occur in the standard test set.
To show that the augmented resource indeed
improves the coverage of the grammar, we built
a specialised testsuite of 1044 TIGER sentences
that contain an instance of a participle from the
resource. Since this testsuite comprises sen-
tences from the training set, we can only report
a coverage-oriented evaluation here, see Table 4.
The 243 Part-Adv increases the coverage by 8%
on the specialised testsuite.
Moreover, we manually evaluated 20 sentences
covered by the 243-Part-Adv grammar and not
by 46-Part-Adv as to whether they contain a cor-
rectly analysed adverbial participle. In two sen-
Grammar Parsed
Sent.
Starred
Sent.
Time-
outs
Time
in sec
No Part-Adv 665 315 64 3033
46 Part-Adv 710 269 65 3118
243 Part-Adv 767 208 69 3151
Table 4: Performance on the specialised TIGER
test set (1044 sentences)
tences, the grammar obtained an adverbial analy-
sis for clearly predicative modifiers, based on the
enlarged resource. In three different sentences, it
was difficult to decide whether the participle acts
as an adverb or a predicative. In the remaining 15
sentences, the grammar established the the correct
analysis of a clearly adverbially used participle.
7 Conclusion
We have proposed a cross-lingual induction
method to automatically obtain data on adverbial
participles in German. We exploited this cross-
lingual data as training material for a classifier that
learns to predict the syntactic category of a partici-
ple from its monolingual syntactic context. Since
this category is usually not annotated in German
resources and hard to describe in theory, the find-
ing that adverbial participles can be predicted rel-
atively precisely is of general interest for theo-
retic and computational approaches to the syntac-
tic analysis of German.
We showed that, in order to obtain an accurate
participle classifier, the quality of the training ma-
terial induced from the parallel corpus is of crucial
importance. By applying the filtering techniques
from Zarrie? et al (2010), the accuracy of the
classifier increases between 5% and 7%. In future
work, we plan to include more data on predicative
participles to learn a more accurate distinction be-
tween predicative and adverbial participles.
Finally, we used the participle classifier to ex-
tract a lexical resource of adverbial participles for
the German LFG grammar. In comparison to the
relatively small resource of 46 types that can be
directly induced from Europarl, we discovered a
large number of new participle types (197 types
in total). In a parsing experiment, we showed that
this much bigger resource does not negatively im-
pact on parsing performance and improves gram-
mar coverage.
1433
References
Bouma, Gerlof, Jonas Kuhn, Bettina Schrader, and
Kathrin Spreyer. 2008. Parallel LFG Grammars
on Parallel Corpora: A Base for Practical Trian-
gulation. In Butt, Miriam and Tracy Holloway
King, editors, Proceedings of the LFG08 Confer-
ence, pages 169?189, Sydney, Australia. CSLI Pub-
lications, Stanford.
Brants, Sabine, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
Treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories.
Butt, Miriam, Helge Dyvik, Tracy Holloway King, Hi-
roshi Masuichi, and Christian Rohrer. 2002. The
Parallel Grammar Project.
Geuder, Wilhelm. 2004. Depictives and transpar-
ent adverbs. In Austin, J. R., S. Engelbrecht,
and G. Rauh, editors, Adverbials. The Interplay of
Meaning, Context, and Syntactic Structure, pages
131?166. Benjamins.
Hwa, Rebecca, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Nat. Lang. Eng., 11(3):311?325.
Malouf, Robert. 2002. A comparison of algorithms
for maximum entropy parameter estimation. In Pro-
ceedings of the Sixth Conference on Natural Lan-
guage Learning (CoNLL-2002), pages 49?55.
Mihalcea, Rada, Carmen Banea, and Jan Wiebe.
2007. Learning multilingual subjective language
via cross-lingual projections. In Proceedings of
the Association for Computational Linguistics (ACL
2007), pages 976?983, Prague.
Pado?, Sebastian and Mirella Lapata. 2009. Cross-
lingual annotation projection of semantic roles.
Journal of Artificial Intelligence Research, 36:307?
340.
Ratnaparkhi, Adwait. 1996. A maximum entropy
model for part-of-speech tagging. In Proceedings
of EMNLP 96, pages 133?142.
Riezler, Stefan, Tracy Holloway King, Ronald M. Ka-
plan, Richard Crouch, John T. Maxwell, and Mark
Johnson. 2002. Parsing the Wall Street Journal
using a Lexical-Functional Grammar and Discrim-
inative Estimation Techniques . In Proceedings of
ACL 2002.
Rohrer, Christian and Martin Forst. 2006. Improving
coverage and parsing quality of a large-scale LFG
for German. In Proceedings of LREC-2006.
Schmid, Helmut. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of In-
ternational Conference on New Methods in Lan-
guage Processing.
Yarowsky, David, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analy-
sis tools via robust projection across aligned cor-
pora. In Proceedings of HLT 2001, First Interna-
tional Conference on Human Language Technology
Research.
Zarrie?, Sina, Aoife Cahill, Jonas Kuhn, and Christian
Rohrer. 2010. A Cross-Lingual Induction Tech-
nique for German Adverbial Participles. In Pro-
ceedings of the 2010 Workshop on NLP and Lin-
guistics: Finding the Common Ground, ACL 2010,
pages 34?42, Uppsala, Sweden.
1434
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 767?776,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
To what extent does sentence-internal realisation reflect discourse
context? A study on word order
Sina Zarrie? Jonas Kuhn
Institut fu?r maschinelle Sprachverarbeitung
University of Stuttgart, Germany
zarriesa,jonas@ims.uni-stuttgart.de
Aoife Cahill
Educational Testing Service
Princeton, NJ 08541, USA
acahill@ets.org
Abstract
We compare the impact of sentence-
internal vs. sentence-external features on
word order prediction in two generation
settings: starting out from a discrimina-
tive surface realisation ranking model for
an LFG grammar of German, we enrich
the feature set with lexical chain features
from the discourse context which can be
robustly detected and reflect rough gram-
matical correlates of notions from theoreti-
cal approaches to discourse coherence. In a
more controlled setting, we develop a con-
stituent ordering classifier that is trained
on a German treebank with gold corefer-
ence annotation. Surprisingly, in both set-
tings, the sentence-external features per-
form poorly compared to the sentence-
internal ones, and do not improve over
a baseline model capturing the syntactic
functions of the constituents.
1 Introduction
The task of surface realization, especially in a rel-
atively free word order language like German, is
only partially determined by hard syntactic con-
straints. The space of alternative realizations that
are strictly speaking grammatical is typically con-
siderable. Nevertheless, for any given choice of
lexical items and prior discourse context, only a
few realizations will come across as natural and
will contribute to a coherent text. Hence, any NLP
application involving a non-trivial generation step
is confronted with the issue of soft constraints on
grammatical alternatives in one way or another.
There are countless approaches to modelling
these soft constraints, taking into account their
interaction with various aspects of the discourse
context (givenness or salience of particular refer-
ents, prior mentioning of particular concepts).
Since so many factors are involved and there is
further interaction with subtle semantic and prag-
matic differentiations, lexical choice, stylistics
and presumably processing factors, theoretical ac-
counts making reliable predictions for real cor-
pus examples have for a long time proven elusive.
As for German, only quite recently, a number of
corpus-based studies (Filippova and Strube, 2007;
Speyer, 2005; Dipper and Zinsmeister, 2009) have
made some good progress towards a coherence-
oriented account of at least the left edge of the
German clause structure, the Vorfeld constituent.
What makes the technological application of
theoretical insights even harder is that for most
relevant factors, automatic recognition cannot be
performed with high accuracy (e.g., a coreference
accuracy in the 70?s means there is a good deal
of noise) and for the higher-level notions such
as the information-structural focus, interannotator
agreement on real corpus data tends to be much
lower than for core-grammatical notions (Poesio
and Artstein, 2005; Ritz et al 2008).
On the other hand, many of the relevant dis-
course factors are reflected indirectly in proper-
ties of the sentence-internal material. Most no-
tably, knowing the shape of referring expressions
narrows down many aspects of givenness and
salience of its referent; pronominal realizations
indicate givenness, and in German there are even
two variants of the personal pronoun (er and der)
for distinguishing salience. So, if the genera-
tion task is set in such a way that the actual lex-
ical choice, including functional categories such
as determiners, is fully fixed (which is of course
not always the case), one can take advantage of
767
these reflexes. This explains in part the fairly high
baseline performance of n-gram language mod-
els in the surface realization task. And the effect
can indeed be taken much further: the discrimi-
native training experiments of Cahill and Riester
(2009) show how effective it is to systematically
take advantage of asymmetry patterns in the mor-
phosyntactic reflexes of the discourse notion of
information status (i.e., using a feature set with
well-chosen purely sentence-bound features).
These observations give rise to the question: in
the light of the difficulty in obtaining reliable dis-
course information on the one hand and the effec-
tiveness of exploiting the reflexes of discourse in
the sentence-internal material on the other ? can
we nevertheless expect to gain something from
adding sentence-external feature information?
We propose two scenarios for adressing this
question: first, we choose an approximative ac-
cess to context information and relations between
discourse referents ? lexical reiteration of head
words, combined with information about their
grammatical relation and topological positioning
in prior sentences. We apply these features in a
rich sentence-internal surface realisation ranking
model for German. Secondly, we choose a more
controlled scenario: we train a constituent order-
ing classifier based on a feature model that cap-
tures properties of discourse referents in terms of
manually annotated coreference relations. As we
get the same effect in both setups ? the sentence-
external features do not improve over a baseline
that captures basic morphosyntactic properties of
the constituents ? we conclude that sentence-
internal realisation is actually a relatively accurate
predictor of discourse context, even more accurate
than information that can be obtained from coref-
erence and lexical chain relations.
2 Related Work
In the generation literature, most works on ex-
ploiting sentence-external discourse information
are set in a summarisation or content ordering
framework. Barzilay and Lee (2004) propose an
account for constraints on topic selection based on
probabilistic content models. Barzilay and Lapata
(2008) propose an entity grid model which repre-
sents the distribution of referents in a discourse
for sentence ordering. Karamanis et al(2009)
use Centering-based metrics to assess coherence
in an information ordering system. Clarke and La-
pata (2010) have improved a sentence compres-
sion system by capturing prominence of phrases
or referents in terms of lexical chain information
inspired by Morris and Hirst (1991) and Center-
ing (Grosz et al 1995). In their system, discourse
context is represented in terms of hard constraints
modelling whether a certain constituent can be
deleted or not.
In the linearisation or surface realisation do-
main, there is a considerable body of work ap-
proximating information structure in terms of
sentence-internal realisation (Ringger et al 2004;
Filippova and Strube, 2009; Velldal and Oepen,
2005; Cahill et al 2007). Cahill and Riester
(2009) improve realisation ranking for German ?
which mainly deals with word order variation ? by
representing precedence patterns of constituents
in terms of asymmetries in their morphosyntac-
tic properties. As a simple example, a pattern ex-
ploited by Cahill and Riester (2009) is the ten-
dency of definite elements tend to precede indef-
inites, which, on a discourse level, reflects that
given entities in a sentence tend to precede new
entities.
Other work on German surface realisation has
highlighted the role of the initial position in the
German sentence, the so-called Vorfeld (or ?pre-
field?). Filippova and Strube (2007) show that
once the Vorfeld (i.e. the constituent that precedes
the finite verb) is correctly determined, the pre-
diction of the order in the Mittelfeld (i.e. the con-
stituents that follow the finite verb) is very easy.
Cheung and Penn (2010) extend the approach
of Filippova and Strube (2007) and augment a
sentence-internal constituent ordering model with
sentence-external features inspired from the en-
tity grid model proposed by Barzilay and Lapata
(2008).
3 Motivation
While there would be many ways to construe
or represent discourse context (e.g. in terms of
the global discourse or information structure), we
concentrate on capturing local coherence through
the distribution of discourse referents in a text.
These discourse referents basically correspond to
the constituents that our surface realisation model
has to put in the right order. As the order of refer-
ents or constituents is arguably influenced by the
information structure of a sentence given the pre-
vious text, our main assumption was that infor-
768
(1) a. Kurze Zeit spa?ter erkla?rte ein Anrufer bei Nachrichtenagenturen in Pakistan , die Gruppe Gamaa bekenne sich.
Shortly after, a caller declared at the news agencies in Pakistan, that the group Gamaa avowes itself.
b. Diese Gruppe wird fu?r einen Gro?teil der Gewalttaten verantwortlich gemacht , die seit dreieinhalb Jahren in
A?gypten veru?bt worden sind .
This group is made responsible for most of the violent acts that have been committed in Egypt in the last three and
a half years.
(2) a. Belgien wu?nscht, dass sich WEU und NATO daru?ber einigen.
Belgium wants that WEU and NATO agree on that.
b. Belgien sieht in der NATO die beste milita?rische Struktur in Europa .
Belgium sees the best military structure of Europe in the NATO.
(3) a. Frauen vom Land ka?mpften aktiv darum , ein Staudammprojekt zu verhindern.
Women from the countryside fighted actively to block the dam project.
b. Auch in den Sta?dten fa?nden sich immer mehr Frauen in Selbsthilfeorganisationen zusammen.
Also in the cities, more and more women team up in self-help organisations.
mation about the prior mentioning of a referent
would be helpful for predicting the position of this
referent in a sentence.
The idea that the occurence of discourse refer-
ents in a text is a central aspect of discourse struc-
ture has been systematically pursued by Centering
Theory (Grosz et al 1995). Its most important
notions are related to the realisation of discourse
referents (i.e. described as ?centers?) and the way
the centers are arranged in a sequence of utter-
ances to make this sequence a coherent discourse.
Another important concept is the ?ranking? of dis-
course referents which basically determines the
prominence of a referent in a certain sentence and
is driven by several factors (e.g. their grammati-
cal function). For free word order languages like
German, word order has been proposed as one of
the factors that account for the ranking (Poesio et
al., 2004). In a similar spirit, Morris and Hirst
(1991) have proposed that chains of (related) lex-
ical items in a text are an important indicator of
text structure.
Our main hypothesis was that it is possible to
exploit these intuitions from Centering Theory
and the idea of lexical chains for word order pre-
diction. Thus, we expected that it would be easier
to predict the position of a referent in a sentence
if we have not only given its realisation in the cur-
rent utterance but also its prominence in the previ-
ous discourse. Especially, we expected this intu-
ition to hold for cases where the morpho-syntactic
realisation of a constituent does not provide many
clues. This is illustrated in Examples (1) and (2)
which both exemplify the reiteration of a lexical
item in two subsequent sentences, (reiteration is
one type of lexical chain discussed in Morris and
Hirst (1991)). In Example (1), the second instance
of the noun ?group? is modified by a demonstra-
tive pronoun such that its ?known? and prominent
discourse status is overt in the morpho-syntactic
realisation. In Example (2), both instances of
?Belgium? are realised as bare proper nouns with-
out an overt morphosyntactic clue indicating their
discourse status.
Beyond the simple presence of reitered items in
sequences of sentences, we expected that it would
be useful to look at the position and syntactic
function of the previous mentions of a discourse
referent. In Example (1), the reiterated item is first
introduced in an embedded sentence and realised
in the Vorfeld in the second utterance. In terms
of centering, this transition would correspond to
a topic shift. In Example (2), both instances are
realised in the Vorfeld, such that the topic of the
first sentence is carried over to the next.
In Example (3), we illustrate a further type of
lexical reiteration. In this case, two identical head
nouns are realised in subsequent sentences, even
though they refer to two different discourse refer-
ents. While this type of lexical chain is described
as ?reiteration without identity of referents? by
Morris and Hirst (1991), it would not be captured
in Centering since this is not a case of strict coref-
erence. On the other hand, lexical chains do not
capture types of reiterated discourse referents that
have distinct morpho-syntactic realisations, e.g.
nouns and pronouns.
Originally, we had the hypothesis that strict
corefence information is more useful and accurate
for word order prediction than rather loose lexi-
cal chains which conflate several types of referen-
tial and lexical relations. However, the advantage
of chains, especially chains of reiteration, is that
they can be easily detected in any corpus text and
769
that they might capture ?topics? of sentences be-
yond the identity of referents. Thus, we started
out from the idea of lexical chains and added cor-
responding features in a statistical ranking model
for surface realisation of German (Section 4). As
this strategy did not work out, we wanted to assess
whether an ideal coreference annotation would be
helpful at all for predicting word order. In a sec-
ond experiment, we use a corpus which is manu-
ally annotated for coreference (Section 5).
4 Experiment 1: Realisation Ranking
with Lexical Chains
In this Section, we present an experiment that in-
vestigates sentence-external context in a surface
realisation task. The sentence-external context is
represented in terms of lexical chain features and
compared to sentence-internal models which are
based on morphosyntactic features. The experi-
ment thus targets a generation scenario where no
coreference information is available and aims at
assessing whether relatively naive context infor-
mation is also useful.
4.1 System Description
We carry out our first experiment in a regener-
ation set-up with two components: a) a large-
scale hand-crafted Lexical Functional Grammar
(LFG) for German (Rohrer and Forst, 2006), used
to parse and regenerate a corpus sentence, b)
a stochastic ranker that selects the most appro-
priate regenerated sentence in context according
to an underlying, linguistically motivated feature
model. In contrast to fully statistical linearisation
methods, our system first generates the full set
of sentences that correspond to the grammatically
well-formed realisations of the intermediate syn-
tactic representation.1 This representation is an
f-structure, which underspecifies the order of con-
stituents and, to some extent, their morphological
realisation, such that the output sentences contain
all possible combinations of word order permu-
tations and morphological variants. Depending
on the length and structure of the original corpus
sentence, the set of regenerated sentences can be
huge (see Cahill et al(2007) for details on regen-
erating the German treebank TIGER).
1There are occasional mistakes in the grammar which
sometimes lead to ungrammatical strings being generated,
but this is rare.
The realisation ranking component is an SVM
ranking model implemented with SVMrank,
a Support Vector Machine-based learning tool
(Joachims, 2006). During training, each sentence
is annotated with a rank and a set of features ex-
tracted from the F-structure, its surface string and
external resources (e.g. a language model). If
the sentence matches the original corpus string,
its rank will be highest, the assumption being that
the original sentence corresponds to the optimal
realisation in context. The output of generation,
the top-ranked sentence, is evaluated against the
original corpus sentence.
4.2 The Feature Models
As the aim of this experiment is to better un-
derstand the nature of sentence-internal features
reflecting discourse context and compare them
to sentence-external ones, we build several fea-
ture models which capture different aspects of the
constituents in a given sentence. The sentence-
internal features describe the morphosyntacic re-
alisation of constituents, for instance their func-
tion (?subject?, ?object?), and can be straightfor-
wardly extracted from the f-structure. These fea-
tures are then combined into discriminative prece-
dence features, for instance ?subject-precedes-
object?. We implement the following types of
morphosyntactic features:
? syntactic function (arguments and adjuncts)
? modification (e.g. nouns modified by relative
clauses, genitive etc.)
? syntactic category (e.g. adverbs, proper
nouns, phrasal arguments)
? definiteness for nouns
? number and person for nominal elements
? types of pronouns (e.g. demonstrative, re-
flexive)
? constituent span and number of embedded
nodes in the tree
In addition, we also include language model
scores in our ranking model. In Section 4.4,
we report on results for several subsets of these
features where ?BaseSyn? refers to a model that
only includes the syntactic function features and
?FullMorphSyn? includes all features mentioned
above.
For extracting the lexical chains, we check for
any overlapping nouns in the n sentences previ-
ous to the current one being generated. We check
770
Rank Sentence and Features
% Diese Gruppe wird fu?r einen Gro?teil der Gewalttaten verantwortlich gemacht.
% This group is for a major part of the violent acts responsible made.
1 subject-<-pp-object, demonstrative-<-indefinite, overlap-<-no-overlap, overlap-in-vorfeld, lm:-7.89
% Fu?r einen Gro?teil der Gewalttaten wird diese Gruppe verantwortlich gemacht.
% For a major part of the violent acts is this group responsible made.
3 pp-object-<-subject, indefinite-<-demonstrative, no-overlap-<-overlap, no-overlap-in-vorfeld, lm:-10.33
% Verantwortlich gemacht wird diese Gruppe fu?r einen Gro?teil der Gewalttaten.
% Responsible made is this group for a major part of the violent acts.
3 subject-<-pp-object, demonstrative-<-indefinite, overlap-<-no-overlap, lm:-9.41
Figure 1: Made-up training example for realisation ranking with precedence features
proper and common nouns, considering full and
partial overlaps as shown in Examples (1) and
(2), where the (a) example is the previous sen-
tence in the corpus. For each overlap, we record
the following properties: (i) function in the previ-
ous sentence, (ii) position in the previous sentence
(e.g. Vorfeld), (iii) distance between sentences,
(iv) total number of overlaps.
These overlap features are then also
combined in terms of precedence, e.g.
?has subject overlap:3-precedes-no overlap?,
meaning that in the current sentence a noun
that was previously mentioned in a subject 3
sentences ago precedes a noun that was not
mentioned before.
In Figure 1, we give an example of a set of gen-
eration alternatives and their (partial) feature rep-
resentation for the sentence (1-b). Precedence is
indicated by ?<?.
Basically, our sentence-external feature model
is built on the intuition that lexical chains or over-
laps approximate discourse status in a way which
is similar to sentence-internal morphosyntactic
properties. Thus, we would expect that overlaps
indicate givenness, salience or prominence and
that asymmetries between overlapping and non-
overlapping entities are helpful in the ranking.
4.3 Data
All our models are trained on 7,039 sentences
(subdivided into 1259 texts) from the TIGER
Treebank of German newspaper text (Brants et al
2002). We tune the parameters of our SVM model
on a development set of 55 sentences and report
the final results for our unseen test set of 240 sen-
tences. Table 1 shows how many sentences in our
training, development and test sets have at least
one textually overlapping phrase in the previous
1?10 sentences.
We choose the TIGER treebank, which has no
# Sentences % Sentences with overlap
in context Training Dev Test
1 20.96 23.64 20.42
2 35.42 40.74 35.00
3 45.58 50.00 53.33
4 52.66 53.70 58.75
5 57.45 58.18 64.58
6 61.42 57.41 68.75
7 64.58 61.11 70.83
8 67.05 62.96 72.08
9 69.20 64.81 74.17
10 71.16 70.37 75.83
Table 1: The percentage of sentences that have at least
one overlapping entity in the previous n sentences
coreference annotation, since we already have a
number of resources available to match the syn-
tactic analyses produced by our grammar against
the analyses in the treebank. Thus, in our regen-
eration system, we parse the sentences with the
grammar, and choose the parsed f-structures that
are compatible with the manual annotation in the
TIGER treebank as is done in Cahill et al(2007).
This compatibility check eliminates noise which
would be introduced by generating from incorrect
parses (e.g. incorrect PP-attachments typically re-
sult in unnatural and non-equivalent surface reali-
sations).
For comparing the string chosen by the mod-
els against the original corpus sentence, we use
BLEU, NIST and exact match. Exact match is
a strict measure that only credits the system if it
chooses the exact same string as the original cor-
pus string. BLEU and NIST are more relaxed
measures that compare the strings on the n-gram
level. Finally, we report accuracy scores for the
Vorfeld position (VF) corresponding to the per-
centage of sentences generated with a correct Vor-
feld.
771
Sc BLEU NIST Exact VF
0 0.766 11.885 50.19 64.0
1 0.765 11.756 49.78 64.0
2 0.765 11.886 50.01 64.1
3 0.765 11.885 50.08 63.8
4 0.761 11.723 49.43 63.2
5 0.765 11.884 49.71 64.2
6 0.768 11.892 50.42 64.6
7 0.765 11.885 50.01 64.5
8 0.764 11.884 49.78 64.3
9 0.765 11.888 49.82 63.6
10 0.764 11.889 49.7 63.5
Table 2: Tenfold-crossvalidation for feature model
FullMorphSyn and different context windows (Sc)
Model BLEU VF
Language Model 0.702 51.2
Language Model + Context Sc = 5 0.715 54.3
BaseSyn 0.757 62.0
BaseSyn + Context Sc = 5 0.760 63.0
FullMorphSyn 0.766 64.0
FullMorphSyn + Context Sc = 5 0.763 64.2
Table 3: Evaluation for different feature models; ?Lan-
guage Model?: ranking based on language model
scores, ?BaseSyn?: precedence between constituent
functions, ?FullMorphSyn?: entire set of sentence-
internal features.
4.4 Results
In Table 2, we report the performance of the full
sentence-internal feature model combined with
context windows from zero to ten. The scores
have been obtained from tenfold-crossvalidation.
For none of the context windows, the model out-
performs the baseline with a zero context which
has no sentence-external features. In Table 3,
we compare the performance of several feature
models corresponding to subsets of the features
used so far which are combined with sentence-
external features respectively. We note that the
function precedence features (i.e. the ?BaseSyn?
model) are very powerful, leading to a major im-
provement compared to a language model. The
sentence-external features lead to an improvement
when combined with the language-model based
ranking. However, this improvement is leveled
out in the BaseSyn model.
On the one hand, the fact that the lexical chain
features improve a language-model based ranking
suggests these features are, to some extent, pre-
dictive for certain patterns of German word order.
On the other hand, the fact that they don?t improve
over an informed sentence-internal baseline sug-
gests that these patterns are equally well captured
by morphosyntactic features. However, we cannot
exclude the possibility that the chain features are
too noisy as they conflate several types of lexical
and coreferential relations. This will be adressed
in the following experiment.
5 Experiment 2: Constituent Ordering
with Centering-inspired Features
We now look at a simpler generation setup where
we concentrate on the ordering of constituents in
the German Vorfeld and Mittelfeld. This strat-
egy has also been adopted in previous investiga-
tions of German word order: Filippova and Strube
(2007) show that once the German Vorfeld is cor-
rectly chosen, the prediction accuracy for the Mit-
telfeld (the constituents following the finite verb)
is in the 90s.
In order to eliminate noise introduced from po-
tentially heterogeneous chain features, we look at
coreference features and, again, compare them to
sentence-internal morphosyntactic features. We
target a generation scenario where coreference in-
formation is available. The aim is to establish an
upper bound concerning the quality improvement
for word order prediction by recurring to manual
corefence annotation.
5.1 Data and Setup
We carry out the constituent ordering experiment
on the Tu?ba-D/Z treebank (v5) of German news-
paper articles (Telljohann et al 2006). It com-
prises about 800k tokens in 45k sentences. We
choose this corpus because it is not only annotated
with syntactic analyses but also with coreference
relations (Naumann, 2006). The syntactic annota-
tion format differs from the TIGER treebank used
in the previous experiment, for instance, it ex-
plicitely represents the Vorfeld and Mittelfeld as
phrasal nodes in the tree. This format is very con-
venient for the extraction of constituents in the re-
spective positions.
The Tu?ba-D/Z coreference annotation distin-
guishes several relations between discourse ref-
erents, most importantly ?coreferential relation?
and ?anaphoric relation? where the first denotes
a relation between noun phrases that refer to the
same entity, and the latter refers to a link between
a pronoun and a contextual antecedent, see Nau-
mann (2006) for further detail. We expected the
coreferential relation to be particularly useful, as
772
it cannot always be read off the morphosyntac-
tic realisation of a noun phrase, whereas pronouns
are almost always used in an anaphoric relation.
The constituent ordering model is implemented
as a classifier that is given a set of constituents
and predicts the constituent that is most likely to
be realised in the Vorfeld.
The set of candidate constituents is determined
from the tree of the original corpus sentence. We
will assume that all constituents under a Vorfeld
and Mittelfeld node can be freely reordered. Thus,
we do not check whether the word order variants
we look at are actually grammatical assuming that
most of them are. In this sense, this experiment
is close to fully statistical generation approaches.
As a further simplification, we do not look at mor-
phological generation variants of the constituents
or their head verb.
The classifier is implemented with SVMrank
again. In contrast to the previous experiment
where we learned to rank sentences, the classi-
fier now learns to rank constituents. The con-
stituents have been extracted using the tool de-
scribed in Bouma (2010). The final data set com-
prises 48.513 candidate sets of freely orderable
constituents.
5.2 Centering-inspired Feature Model
To compare the discourse context model against a
sentence-based model, we implemented a number
of sentence-internal features that are very similar
to the features used in the previous experiment.
Since we extract them from the syntactic annota-
tion instead of f-structures, some labels and fea-
ture names will be different, however, the design
of the sentence-internal model is identical to the
previous one in Section 4.
The sentence-external features differ in some
aspects from Section 4, since we extract coref-
erence relations of several types (see (Naumann,
2006) for the anaphoric relations annotated in the
Tueba-D/Z). For each type of coreference link,
we extract the following properties: (i) function
of the antecedent, (ii) position of the antecedent,
(iii) distance between sentences, (iv) type of rela-
tion. We also distinguish coreference links anno-
tated for the whole phrase (?head link?) and links
that are annotated for an element embedded by the
constituent (?contained link?). The two types are
illustrated in Examples (4) and (5). Note that both
cases would not have been captured in the lexical
# VF # MF
Backward Center 3.5% 5.1%
Forward Center 6.8% 6.8%
Coref Link 30.5% 23.4%
Table 4: Backward and forward centers and their posi-
tions
chain model since there is no lexical overlap be-
tween the realisations of the discourse referents.
These types of coreference features implicitly
carry the information that would also be consid-
ered in a Centering formalisation of discourse
context. In addition to these, we designed features
that explicitly describe centers as these might
have a higher weight. In line with Clarke and
Lapata (2010), we compute backward (CB) and
forward centers (CF ) in the following way:
1. Extract all entities from the current sentence
and the previous sentence.
2. Rank the entities of the previous sentence ac-
cording to their function (subject < direct
object < indirect object ...).
3. Find the highest ranked entity in the previous
sentence that has a link to an entity in the
current sentence, this entity is the CB of the
sentence.
In the same way, we mark entities as forward
centers that are ranked highest in the current sen-
tence and have a link to an entity in the following
sentence.2 In Table 4, we report the percentage of
sentences that have backward and forward centers
in the Vorfeld or Mittelfeld. While the percentage
of sentences that realise a backward center is quite
low, the overall proportion of sentences contain-
ing some type of coreference link is in a dimen-
sion such that the learner could definitely pick up
some predictive patterns. Going by the relative
frequencies, coreferential constituents have a bias
towards appearing in the Vorfeld rather than in the
Mittelfeld.
5.3 Results
First, we build three coreference-based con-
stituent classifiers on their entire training set and
compare them to their sentence-internal baseline.
The most simple baseline records the category of
2In Centering, all entities in a given utterance can be seen
as forward centers, however we thought that this implemen-
tation would be more useful.
773
(4) a. Die Rechnung geht an die AWO.
The bill goes to the AWO.
b. [Hintergrund der gegenseitigen Vorwu?rfe in der Arbeiterwohlfahrt] sind offenbar scharfe Konkurrenzen zwischen
Bremern und Bremerhavenern.
Apparently, [the background of the mutual accusations at the labour welfare] are rivalries between people from
Bremen and Bremerhaven.
(5) a. Dies ist die Behauptung, mit der Bremens Ha?fensenator die Skeptiker davon u?berzeugt hat, [...].
This is the claim, which Bremen?s harbour senator used to convince doubters, [...].
b. Fu?r diese Behauptung hat Beckmeyer bisher keinen Nachweis geliefert. So far, Beckmeyer has not given a prove of
this claim.
Model VF
ConstituentLength + HeadPos 47.48%
ConstituentLength + HeadPos + Coref 51.30%
BaseSyn 54.82%
BaseSyn + Coref 56.21%
FullMorphSyn 57.24%
FullMorphSyn + Coref 57.40%
Table 5: Results from Vorfeld classification, training
and evaluation on entire treebank
the constituent head and the number of words that
the constituent spans. Additionally, in parallel to
the experiment in Section 4, we build a ?BaseSyn?
model which has the syntactic function features,
and a ?FullMorphSyn? model which comprises
the entire set of sentence-internal features. To
each of these baseline, we add the coreference
features. The results are reported in Table 5.
In this experiment, we find an effect of
the sentence-external features over the simple
sentence-internal baselines. However, in the fully
spelled-out, sentence-internal model, the effect
is, again, minimal. Moreover, for each base-
line, we obtain higher improvements by adding
further sentence-internal features than by adding
sentence-external ones the accuracy of the sim-
ple baseline (47.48%) improves by 7.34 points
through adding function features (the accuracy
of BaseSyn is 54.82%) and by only 3.48 points
through adding coreference features.
We run a second experiment in order to so see
whether the better performance of the sentence-
internal features is related to their coverage. We
build and evaluate the same set of classifiers on
the subset of sentences that contain at least one
coreference link for one of its constituents (see
Table 4 for the distribution of coreference links
in our data). The results are given in Table 6. In
this experiment, the coreference features improve
over all sentence-internal baselines including the
?FullMorphSyn? model.
Model VF
ConstituentLength + HeadPos 46.61%
ConstituentLength + HeadPos + Coref 52.23%
BaseSyn 54.63%
BaseSyn + Coref 56.67%
FullMorphSyn 55.36%
FullMorphSyn + Coref 57.93%
Table 6: Results from Vorfeld classification, training
and evaluation on sentences that contain a coreference
link
5.4 Discussion
The results presented in this Section consis-
tently complete the picture that emerged from
the experiments in Section 4. Even if we have
high quality information about discourse con-
text in terms of relations between referents, a
non-trivial sentence-internal model for word or-
der prediction can be hardly improved. This
suggests that sentence-internal approximations of
discourse context provide a fairly good way of
dealing with local coherence in a linearisation
task. It is also interesting that the sentence-
external features improve over simple baselines,
but get leveled out in rich sentence-internal fea-
ture models. From this, we conclude that the
sentence-external features we implemented are to
some extent predictive for word order, but that
they can be covered by sentence-internal features
as well.
Our second evaluation concentrating on the
sentences that have coreference information
shows that the better performance of the sentence-
internal features is also related to their cover-
age. These results confirm our initial intuition
that coreference information can add to the pre-
dictive power of the morpho-syntactic features in
certain contexts. This positive effect disappears
when sentences with and without coreferential
constituents are taken together. For future work,
it would be promising to investigate whether the
774
positive impact of coreference features can be
strengthened if the coreference annotation scheme
is more exhaustive, including, e.g., bridging and
event anaphora.
6 Conclusion
We have carried out a number of experiments that
show that sentence-internal models for word order
are hardly improved by features which explicitely
represent the preceding context of a sentence in
terms of lexical and referential relations between
discourse entities. This suggests that sentence-
internal realisation implicitly carries a lot of im-
formation about discourse context. On average,
the morphosyntactic properties of constituents in
a text are better approximates of their discourse
status than actual coreference relations.
This result feeds into a number of research
questions concerning the representation of dis-
course and its application in generation systems.
Although we should certainly not expect a com-
putational model to achieve a perfect accuracy in
the constituent ordering task ? even humans only
agree to a certain extent in rating word order vari-
ants (Belz and Reiter, 2006; Cahill, 2009) ? the
average accuracy in the 60?s for prediction of Vor-
feld occupance is still moderate. An obvious di-
rection would be to further investigate more com-
plex representations of discourse that take into ac-
count the relations between utterances, such as
topic shifts. Moreover, it is not clear whether the
effects we find for linearisation in this paper carry
over to other levels of generation such as tacti-
cal generation where syntactic functions are not
fully specified. In a broader perspective, our re-
sults underline the need for better formalisations
of discourse that can be translated into features for
large-scale applications such as generation.
Acknowledgments
This work was funded by the Collaborative Re-
search Centre (SFB 732) at the University of
Stuttgart.
References
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Com-
putational Linguistics, 34:1?34.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models with applications
to generation and summarization. In Proceedings of
HLT-NAACL 2004, Boston,MA.
Anja Belz and Ehud Reiter. 2006. Comparing auto-
matic and human evaluation of NLG systems. In
Proceedings of EACL 2006, pages 313?320, Trento,
Italy.
Gerlof Bouma. 2010. Syntactic tree queries in prolog.
In Proceedings of the Fourth Linguistic Annotation
Workshop, ACL 2010.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
Treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories.
Aoife Cahill and Arndt Riester. 2009. Incorporat-
ing information status into generation ranking. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, pages 817?825, Suntec, Singapore,
August. Association for Computational Linguistics.
Aoife Cahill, Martin Forst, and Christian Rohrer.
2007. Stochastic Realisation Ranking for a Free
Word Order Language. In Proceedings of the
Eleventh European Workshop on Natural Language
Generation, pages 17?24, Saarbru?cken, Germany.
DFKI GmbH.
Aoife Cahill. 2009. Correlating human and automatic
evaluation of a german surface realiser. In Proceed-
ings of the ACL-IJCNLP 2009 Conference Short Pa-
pers, pages 97?100, Suntec, Singapore, August. As-
sociation for Computational Linguistics.
Jackie C.K. Cheung and Gerald Penn. 2010. Entity-
based local coherence modelling using topological
fields. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics
(ACL 2010). Association for Computational Lin-
guistics.
James Clarke and Mirella Lapata. 2010. Discourse
constraints for document compression. Computa-
tional Linguistics, 36(3):411?441.
Stefanie Dipper and Heike Zinsmeister. 2009. The
role of the German Vorfeld for local coherence. In
Christian Chiarcos, Richard Eckart de Castilho, and
Manfred Stede, editors, Von der Form zur Bedeu-
tung: Texte automatisch verarbeiten/From Form to
Meaning: Processing Texts Automatically, pages
69?79. Narr, Tu?bingen.
Katja Filippova and Michael Strube. 2007. The ger-
man vorfeld and local coherence. Journal of Logic,
Language and Information, 16:465?485.
Katja Filippova and Michael Strube. 2009. Tree Lin-
earization in English: Improving Language Model
Based Approaches. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, Companion Volume:
Short Papers, pages 225?228, Boulder, Colorado,
June. Association for Computational Linguistics.
775
Barbara J. Grosz, Aravind Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the
local coherence of discourse. Computational Lin-
guistics, 21(2):203?225.
Thorsten Joachims. 2006. Training linear SVMs in
linear time. In Proceedings of the ACM Conference
on Knowledge Discovery and Data Mining (KDD),
pages 217?226.
Nikiforos Karamanis, Massimo Poesioand Chris Mel-
lish, and Jon Oberlander. 2009. Evaluating center-
ing for information ordering using corpora. Com-
putational Linguistics, 35(1).
Jane Morris and Graeme Hirst. 1991. Lexical cohe-
sion, the thesaurus, and the structure of text. Com-
putational Linguistics, 17(1):21?225.
Karin Naumann. 2006. Manual for the annotation of
in-document referential relations. Technical report,
Seminar fu?r Sprachwissenschaft, Abt. Computerlin-
guistik, Universita?t Tu?bingen.
Massimo Poesio and Ron Artstein. 2005. The relia-
bility of anaphoric annotation, reconsidered: Taking
ambiguity into account. In Proc. of ACL Workshop
on Frontiers in Corpus Annotation.
Massimo Poesio, Rosemary Stevenson, Barbara di Eu-
genio, and Janet Hitzeman. 2004. Centering: A
parametric theory and its instantiations. Computa-
tional Linguistics, 30(3):309?363.
Eric K. Ringger, Michael Gamon, Robert C. Moore,
David Rojas, Martine Smets, and Simon Corston-
Oliver. 2004. Linguistically Informed Statisti-
cal Models of Constituent Structure for Ordering
in Sentence Realization. In Proceedings of the
2004 International Conference on Computational
Linguistics, Geneva, Switzerland.
Julia Ritz, Stefanie Dipper, and Michael Go?tze. 2008.
Annotation of information structure: An evaluation
across different types of texts. In Proceedings of the
the 6th LREC conference.
Christian Rohrer and Martin Forst. 2006. Improv-
ing Coverage and Parsing Quality of a Large-Scale
LFG for German. In Proceedings of the Fifth In-
ternational Conference on Language Resources and
Evaluation (LREC), Genoa, Italy.
Augustin Speyer. 2005. Competing constraints on
vorfeldbesetzung in german. In Proceedings of the
Constraints in Discourse Workshop, pages 79?87.
Heike Telljohann, Erhard Hinrichs, Sandra Ku?bler,
and Heike Zinsmeister. 2006. Stylebook for the
tu?bingen treebank of written german (tu?ba-d/z).
revised version. Technical report, Seminar fu?r
Sprachwissenschaft, Universita?t Tu?bingen.
Erik Velldal and Stephan Oepen. 2005. Maximum
entropy models for realization ranking. In Proceed-
ings of the 10th Machine Translation Summit, pages
109?116, Thailand.
776
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1007?1017,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Underspecifying and Predicting Voice for Surface Realisation Ranking
Sina Zarrie?, Aoife Cahill and Jonas Kuhn
Institut fu?r maschinelle Sprachverarbeitung
Universita?t Stuttgart, Germany
{sina.zarriess,aoife.cahill,jonas.kuhn}@ims.uni-stuttgart.de
Abstract
This paper addresses a data-driven surface
realisation model based on a large-scale re-
versible grammar of German. We investigate
the relationship between the surface realisa-
tion performance and the character of the in-
put to generation, i.e. its degree of underspec-
ification. We extend a syntactic surface reali-
sation system, which can be trained to choose
among word order variants, such that the can-
didate set includes active and passive variants.
This allows us to study the interaction of voice
and word order alternations in realistic Ger-
man corpus data. We show that with an ap-
propriately underspecified input, a linguisti-
cally informed realisation model trained to re-
generate strings from the underlying semantic
representation achieves 91.5% accuracy (over
a baseline of 82.5%) in the prediction of the
original voice.
1 Introduction
This paper1 presents work on modelling the usage
of voice and word order alternations in a free word
order language. Given a set of meaning-equivalent
candidate sentences, such as in the simplified En-
glish Example (1), our model makes predictions
about which candidate sentence is most appropriate
or natural given the context.
(1) Context: The Parliament started the debate about the state
budget in April.
a. It wasn?t until June that the Parliament approved it.
b. It wasn?t until June that it was approved by the Parliament.
c. It wasn?t until June that it was approved.
We address the problem of predicting the usage of
linguistic alternations in the framework of a surface
1This work has been supported by the Deutsche Forschungs-
gemeinschaft (DFG; German Research Foundation) in SFB 732
Incremental specification in context, project D2 (PIs: Jonas
Kuhn and Christian Rohrer).
realisation ranking system. Such ranking systems
are practically relevant for the real-world applica-
tion of grammar-based generators that usually gen-
erate several grammatical surface sentences from a
given abstract input, e.g. (Velldal and Oepen, 2006).
Moreover, this framework allows for detailed exper-
imental studies of the interaction of specific linguis-
tic features. Thus it has been demonstrated that for
free word order languages like German, word or-
der prediction quality can be improved with care-
fully designed, linguistically informed models cap-
turing information-structural strategies (Filippova
and Strube, 2007; Cahill and Riester, 2009).
This paper is situated in the same framework, us-
ing rich linguistic representations over corpus data
for machine learning of realisation ranking. How-
ever, we go beyond the task of finding the correct or-
dering for an almost fixed set of word forms. Quite
obviously, word order is only one of the means at
a speaker?s disposal for expressing some content in
a contextually appropriate form; we add systematic
alternations like the voice alternation (active vs. pas-
sive) to the picture. As an alternative way of pro-
moting or demoting the prominence of a syntactic
argument, its interaction with word ordering strate-
gies in real corpus data is of high theoretical interest
(Aissen, 1999; Aissen, 2003; Bresnan et al, 2001).
Our main goals are (i) to establish a corpus-based
surface realisation framework for empirically inves-
tigating interactions of voice and word order in Ger-
man, (ii) to design an input representation for gen-
eration capturing voice alternations in a variety of
contexts, (iii) to better understand the relationship
between the performance of a generation ranking
model and the type of realisation candidates avail-
able in its input. In working towards these goals,
this paper addresses the question of evaluation. We
conduct a pilot human evaluation on the voice al-
1007
ternation data and relate our findings to our results
established in the automatic ranking experiments.
Addressing interactions among a range of gram-
matical and discourse phenomena on realistic corpus
data turns out to be a major methodological chal-
lenge for data-driven surface realisation. The set of
candidate realisations available for ranking will in-
fluence the findings, and here, existing surface re-
alisers vary considerably. Belz et al (2010) point
out the differences across approaches in the type of
syntactic and semantic information present and ab-
sent in the input representation; and it is the type of
underspecification that determines the number (and
character) of available candidate realisations and,
hence, the complexity of the realisation task.
We study the effect of varying degrees of under-
specification explicitly, extending a syntactic gen-
eration system by a semantic component capturing
voice alternations. In regeneration studies involving
underspecified underlying representations, corpus-
oriented work reveals an additional methodological
challenge. When using standard semantic represen-
tations, as common in broad-coverage work in se-
mantic parsing (i.e., from the point of view of analy-
sis), alternative variants for sentence realisation will
often receive slightly different representations: In
the context of (1), the continuation (1-c) is presum-
ably more natural than (1-b), but with a standard
sentence-bounded semantic analysis, only (1-a) and
(1-b) would receive equivalent representations.
Rather than waiting for the availability of robust
and reliable techniques for detecting the reference of
implicit arguments in analysis (or for contextually
aware reasoning components), we adopt a relatively
simple heuristic approach (see Section 3.1) that ap-
proximates the desired equivalences by augmented
representations for examples like (1-c). This way
we can overcome an extremely skewed distribution
in the naturally occurring meaning-equivalent active
vs. passive sentences, a factor which we believe jus-
tifies taking the risk of occasional overgeneration.
The paper is structured as follows: Section 2 situ-
ates our methodology with respect to other work on
surface realisation and briefly summarises the rele-
vant theoretical linguistic background. In Section 3,
we present our generation architecture and the de-
sign of the input representation. Section 4 describes
the setup for the experiments in Section 5. In Section
6, we present the results from the human evaluation.
2 Related Work
2.1 Generation Background
The first widely known data-driven approach to
surface realisation, or tactical generation, (Langk-
ilde and Knight, 1998) used language-model n-
gram statistics on a word lattice of candidate re-
alisations to guide a ranker. Subsequent work ex-
plored ways of exploiting linguistically annotated
data for trainable generation models (Ratnaparkhi,
2000; Marciniak and Strube, 2005; Belz, 2005, a.o.).
Work on data-driven approaches has led to insights
into the importance of linguistic features for sen-
tence linearisation decisions (Ringger et al, 2004;
Filippova and Strube, 2009). The availability of dis-
criminative learning techniques for the ranking of
candidate analyses output by broad-coverage gram-
mars with rich linguistic representations, originally
in parsing (Riezler et al, 2000; Riezler et al, 2002),
has also led to a revival of interest in linguistically
sophisticated reversible grammars as the basis for
surface realisation (Velldal and Oepen, 2006; Cahill
et al, 2007). The grammar generates candidate
analyses for an underlying representation and the
ranker?s task is to predict the contextually appropri-
ate realisation.
The work that is most closely related to ours is
Velldal (2008). He uses an MRS representation
derived by an HPSG grammar that can be under-
specified for information status. In his case, the
underspecification is encoded in the grammar and
not directly controlled. In multilingually oriented
linearisation work, Bohnet et al (2010) generate
from semantic corpus annotations included in the
CoNLL?09 shared task data. However, they note that
these annotations are not suitable for full generation
since they are often incomplete. Thus, it is not clear
to which degree these annotations are actually un-
derspecified for certain paraphrases.
2.2 Linguistic Background
In competition-based linguistic theories (Optimal-
ity Theory and related frameworks), the use of
argument alternations is construed as an effect
of markedness hierarchies (Aissen, 1999; Aissen,
2003). Argument functions (subject, object, . . . ) on
1008
the one hand and the various properties that argu-
ment phrases can bear (person, animacy, definite-
ness) on the other are organised in markedness hi-
erarchies. Wherever possible, there is a tendency to
align the hierarchies, i.e., use prominent functions to
realise prominently marked argument phrases. For
instance, Bresnan et al (2001) find that there is a sta-
tistical tendency in English to passivise a verb if the
patient is higher on the person scale than the agent,
but an active is grammatically possible.
Bresnan et al (2007) correlate the use of the En-
glish dative alternation to a number of features such
as givenness, pronominalisation, definiteness, con-
stituent length, animacy of the involved verb argu-
ments. These features are assumed to reflect the dis-
course acessibility of the arguments.
Interestingly, the properties that have been used
to model argument alternations in strict word or-
der languages like English have been identified as
factors that influence word order in free word or-
der languages like German, see Filippova and Strube
(2007) for a number of pointers. Cahill and Riester
(2009) implement a model for German word or-
der variation that approximates the information sta-
tus of constituents through morphological features
like definiteness, pronominalisation etc. We are not
aware of any corpus-based generation studies inves-
tigating how these properties relate to argument al-
ternations in free word order languages.
3 Generation Architecture
Our data-driven methodology for investigating fac-
tors relevant to surface realisation uses a regen-
eration set-up2 with two main components: a) a
grammar-based component used to parse a corpus
sentence and map it to all its meaning-equivalent
surface realisations, b) a statistical ranking compo-
nent used to select the correct, i.e. contextually most
appropriate surface realisation. Two variants of this
set-up that we use are sketched in Figure 1.
We generally use a hand-crafted, broad-coverage
LFG for German (Rohrer and Forst, 2006) to parse
a corpus sentence into a f(unctional) structure3
and generate all surface realisations from a given
2Compare the bidirectional competition set-up in some
Optimality-Theoretic work, e.g., (Kuhn, 2003).
3The choice among alternative f-structures is done with a
discriminative model (Forst, 2007).
Sntx
SVM Ranker
Snta1 Snta2 ... Sntam
LFG grammar
FSa
LFG grammar
Snti
Snty
SVM Ranker
Sntb1 Snta1 Snta2 ... Sntbn
LFG Grammar
FSa FSb
Reverse Sem. Rules
SEM
Sem. Rules
FS1
LFG Grammar
Snti
Figure 1: Generation pipelines
f-structure, following the generation approach of
Cahill et al (2007). F-structures are attribute-
value matrices representing grammatical functions
and morphosyntactic features; their theoretical mo-
tivation lies in the abstraction over details of sur-
face realisation. The grammar is implemented in the
XLE framework (Crouch et al, 2006), which allows
for reversible use of the same declarative grammar
in the parsing and generation direction.
To obtain a more abstract underlying representa-
tion (in the pipeline on the right-hand side of Fig-
ure 1), the present work uses an additional seman-
tic construction component (Crouch and King, 2006;
Zarrie?, 2009) to map LFG f-structures to meaning
representations. For the reverse direction, the mean-
ing representations are mapped to f-structures which
can then be mapped to surface strings by the XLE
generator (Zarrie? and Kuhn, 2010).
For the final realisation ranking step in both
pipelines, we used SVMrank, a Support Vector
Machine-based learning tool (Joachims, 1996). The
ranking step is thus technically independent from the
LFG-based component. However, the grammar is
used to produce the training data, pairs of corpus
sentences and the possible alternations.
The two pipelines allow us to vary the degree to
which the generation input is underspecified. An f-
structure abstracts away from word order, i.e. the
candidate set will contain just word order alterna-
tions. In the semantic input, syntactic function and
voice are underspecified, so a larger set of surface
realisation candidates is generated. Figure 2 illus-
trates the two representation levels for an active and
1009
a passive sentence. The subject of the passive and
the object of the active f-structure are mapped to the
same role (patient) in the meaning representation.
3.1 Issues with ?naive? underspecification
In order to create an underspecified voice represen-
tation that does indeed leave open the realisation op-
tions available to the speaker/writer, it is often not
sufficient to remove just the syntactic function in-
formation. For instance, the subject of the active
sentence (2) is an arbitrary reference pronoun man
?one? which cannot be used as an oblique agent in
a passive, sentence (2-b) is ungrammatical.
(2) a. Man
One
hat
has
den
the
Kanzler
chancellor
gesehen.
seen.
b. *Der
The
Kanzler
chancellor
wurde
was
von
by
man
one
gesehen.
seen.
So, when combined with the grammar, the mean-
ing representation for (2) in Figure 2 contains im-
plicit information about the voice of the original cor-
pus sentence; the candidate set will not include any
passive realisations. However, a passive realisation
without the oblique agent in the by-phrase, as in Ex-
ample (3), is a very natural variant.
(3) Der
The
Kanzler
chancellor
wurde
was
gesehen.
seen.
The reverse situation arises frequently too: pas-
sive sentences where the agent role is not overtly
realised. Given the standard, ?analysis-oriented?
meaning representation for Sentence (4) in Figure
2, the realiser will not generate an active realisation
since the agent role cannot be instantiated by any
phrase in the grammar. However, depending on the
exact context there are typically options for realis-
ing the subject phrase in an active with very little
descriptive content.
Ideally, one would like to account for these phe-
nomena in a meaning representation that under-
specifies the lexicalisation of discourse referents,
and also captures the reference of implicit argu-
ments. Especially the latter task has hardly been
addressed in NLP applications (but see Gerber and
Chai (2010)). In order to work around that problem,
we implemented some simple heuristics which un-
derspecify the realisation of certain verb arguments.
These rules define: 1. a set of pronouns (generic and
neutral pronouns, universal quantifiers) that corre-
spond to ?trivial? agents in active and implicit agents
Active Passive
2-role trans. 71% (82%) 10% (2%)
1-role trans. 11% (0%) 8% (16%)
Table 1: Distribution of voices in SEMh (SEMn)
in passive sentences; 2. a set of prepositional ad-
juncts in passive sentences that correspond to sub-
jects in active sentence (e.g. causative and instru-
mental prepositions like durch ?by means of?); 3.
certain syntactic contexts where special underspec-
ification devices are needed, e.g. coordinations or
embeddings, see Zarrie? and Kuhn (2010) for ex-
amples. In the following, we will distinguish 1-role
transitives where the agent is ?trivial? or implicit
from 2-role transitives with a non-implicit agent.
By means of the extended underspecification rules
for voice, the sentences in (2) and (3) receive an
identical meaning representation. As a result, our
surface realiser can produce an active alternation for
(3) and a passive alternation for (2). In the follow-
ing, we will refer to the extended representations as
SEMh (?heuristic semantics?), and to the original
representations as SEMn (?naive semantics?).
We are aware of the fact that these approximations
introduce some noise into the data and do not always
represent the underlying referents correctly. For in-
stance, the implicit agent in a passive need not be
?trivial? but can correspond to an actual discourse
referent. However, we consider these heuristics as
a first step towards capturing an important discourse
function of the passive alternation, namely the dele-
tion of the agent role. If we did not treat the passives
with an implicit agent on a par with certain actives,
we would have to ignore a major portion of the pas-
sives occurring in corpus data.
Table 1 summarises the distribution of the voices
for the heuristic meaning representation SEMh on
the data-set we will introduce in Section 4, with
the distribution for the naive representation SEMn
in parentheses.
4 Experimental Set-up
Data To obtain a sizable set of realistic corpus ex-
amples for our experiments on voice alternations, we
created our own dataset of input sentences and rep-
resentations, instead of building on treebank exam-
ples as Cahill et al (2007) do. We extracted 19,905
sentences, all containing at least one transitive verb,
1010
f-structure
Example (2)
2
6
6
6
6
4
PRED ?see < (? SUBJ)(? OBJ) >?
SUBJ
?
PRED ?one?
?
OBJ
?
PRED ?chancellor?
?
TOPIC
?
?one?
?
PASS ?
3
7
7
7
7
5
f-structure
Example (3)
2
6
6
4
PRED ?see < NULL (? SUBJ) >?
SUBJ
?
PRED ?chancellor?
?
TOPIC
?
?chancellor?
?
PASS +
3
7
7
5
semantics
Example (2)
HEAD (see)
PAST (see)
ROLE (agent,see,one)
ROLE (patient,see,chancellor)
semantics
Example (3)
HEAD (see)
PAST (see)
ROLE (agent,see,implicit)
ROLE (patient,see,chancellor)
Figure 2: F-structure pair for passive-active alternation
from the HGC, a huge German corpus of newspa-
per text (204.5 million tokens). The sentences are
automatically parsed with the German LFG gram-
mar. The resulting f-structure parses are transferred
to meaning representations and mapped back to f-
structure charts. For our generation experiments,
we only use those f-structure charts that the XLE
generator can map back to a set of surface realisa-
tions. This results in a total of 1236 test sentences
and 8044 sentences in our training set. The data loss
is mostly due to the fact the XLE generator often
fails on incomplete parses, and on very long sen-
tences. Nevertheless, the average sentence length
(17.28) and number of surface realisations (see Ta-
ble 2) are higher than in Cahill et al (2007).
Labelling For the training of our ranking model,
we have to tell the learner how closely each surface
realisation candidate resembles the original corpus
sentence. We distinguish the rank categories: ?1?
identical to the corpus string, ?2? identical to the
corpus string ignoring punctuation, ?3? small edit
distance (< 4) to the corpus string ignoring punc-
tuation, ?4? different from the corpus sentence. In
one of our experiments (Section 5.1), we used the
rank category ?5? to explicitly label the surface real-
isations derived from the alternation f-structure that
does not correspond to the parse of the original cor-
pus sentence. The intermediate rank categories ?2?
and ?3? are useful since the grammar does not al-
ways regenerate the exact corpus string, see Cahill
et al (2007) for explanation.
Features The linguistic theories sketched in Sec-
tion 2.2 correlate morphological, syntactic and se-
mantic properties of constituents (or discourse ref-
erents) with their order and argument realisation. In
our system, this correlation is modelled by a combi-
nation of linguistic properties that can be extracted
from the f-structure or meaning representation and
of the surface order that is read off the sentence
string. Standard n-gram features are also used as
features.4 The feature model is built as follows:
for every lemma in the f-structure, we extract a set
of morphological properties (definiteness, person,
pronominal status etc.), the voice of the verbal head,
its syntactic and semantic role, and a set of infor-
mations status features following Cahill and Riester
(2009). These properties are combined in two ways:
a) Precedence features: relative order of properties
in the surface string, e.g. ?theme < agent in pas-
sive?, ?1st person < 3rd person?; b) ?scale align-
ment? features (ScalAl.): combinations of voice and
role properties with morphological properties, e.g.
?subject is singular?, ?agent is 3rd person in active
voice? (these are surface-independent, identical for
each alternation candidate).
The model for which we present our results is
based on sentence-internal features only; as Cahill
and Riester (2009) showed, these feature carry a
considerable amount of implicit information about
the discourse context (e.g. in the shape of referring
expressions). We also implemented a set of explic-
itly inter-sentential features, inspired by Centering
Theory (Grosz et al, 1995). This model did not im-
prove over the intra-sentential model.
Evaluation Measures In order to assess the gen-
eral quality of our generation ranking models, we
4The language model is trained on the German data release
for the 2009 ACL Workshop on Machine Translation shared
task, 11,991,277 total sentences.
1011
FS SEMn SEMh
Avg. # strings 36.7 68.2 75.8
Random Match 16.98 10.72 7.28
LM
Match 15.45 15.04 11.89
BLEU 0.68 0.68 0.65
NIST 13.01 12.95 12.69
Ling. Model
Match 27.91 27.66 26.38
BLEU 0.764 0.759 0.747
NIST 13.18 13.14 13.01
Table 2: Evaluation of Experiment 1
use several standard measures: a) exact match:
how often does the model select the original cor-
pus sentence, b) BLEU: n-gram overlap between
top-ranked and original sentence, c) NIST: modifi-
cation of BLEU giving more weight to less frequent
n-grams. Second, we are interested in the model?s
performance wrt. specific linguistic criteria. We re-
port the following accuracies: d) Voice: how often
does the model select a sentence realising the correct
voice, e) Precedence: how often does the model gen-
erate the right order of the verb arguments (agent and
patient), and f) Vorfeld: how often does the model
correctly predict the verb arguments to appear in the
sentence initial position before the finite verb, the
so-called Vorfeld. See Sections 5.3 and 6 for a dis-
cussion of these measures.
5 Experiments
5.1 Exp. 1: Effect of Underspecified Input
We investigate the effect of the input?s underspecifi-
cation on a state-of-the-art surface realisation rank-
ing model. This model implements the entire fea-
ture set described in Section 4 (it is further analysed
in the subsequent experiments). We built 3 datasets
from our alternation data: FS - candidates generated
from the f-structure; SEMn - realisations from the
naive meaning representations; SEMh - candidates
from the heuristically underspecified meaning rep-
resentation. Thus, we keep the set of original cor-
pus sentences (=the target realisations) constant, but
train and test the model on different candidate sets.
In Table 2, we compare the performance of the
linguistically informed model described in Section 4
on the candidates sets against a random choice and
a language model (LM) baseline. The differences in
BLEU between the candidate sets and models are
FS SEMn SEMh SEMn?
A
ll
Tr
an
s. Voice Acc. 100 98.06 91.05 97.59
Voice Spec. 100 22.8 0 0
Majority BL 82.4 98.1
2-
ro
le
Tr
an
s. Voice Acc. 100 97.7 91.8 97.59
Voice Spec. 100 8.33 0 0
Majority BL 88.5 98.1
1-
ro
le
Tr
an
s. Voice Acc. 100 100 90.0 -
Voice Spec. 100 100 0 -
Majority BL 53.9 -
Table 3: Accuracy of Voice Prediction by Ling. Model in
Experiment 1
statistically significant.5 In general, the linguistic
model largely outperforms the LM and is less sen-
sitive to the additional confusion introduced by the
SEMh input. Its BLEU score and match accuracy
decrease only slightly (though statistically signifi-
cantly).
In Table 3, we report the performance of the lin-
guistic model on the different candidate sets with re-
spect to voice accuracy. Since the candidate sets dif-
fer in the proportion of items that underspecify the
voice (see ?Voice Spec.? in Table 3), we also report
the accuracy on the SEMn? test set, which is a sub-
set of SEMn excluding the items where the voice is
specified. Table 3 shows that the proportion of active
realisations for the SEMn? input is very high, and
the model does not outperform the majority baseline
(which always selects active). In contrast, the SEMh
model clearly outperforms the majority baseline.
Example (4) is a case from our development set
where the SEMn model incorrectly predicts an ac-
tive (4-a), and the SEMh correctly predicts a passive
(4-b).
(4) a. 26
26
kostspielige
expensive
Studien
studies
erwa?hnten
mentioned
die
the
Finanzierung.
funding.
b. Die
The
Finanzierung
funding
wurde
was
von
by
26
26
kostspieligen
expensive
Studien
studies
erwa?hnt.
mentioned.
This prediction is according to the markedness hier-
archy: the patient is singular and definite, the agent
5According to a bootstrap resampling test, p < 0.05
1012
Features Match BLEU Voice Prec. VF
Prec. 16.3 0.70 88.43 64.1 59.1
ScalAl. 10.4 0.64 90.37 58.9 56.3
Union 26.4 0.75 91.50 80.2 70.9
Table 4: Evaluation of Experiment 2
is plural and indefinite. Counterexamples are possi-
ble, but there is a clear statistical preference ? which
the model was able to pick up.
On the one hand, the rankers can cope surpris-
ingly well with the additional realisations obtained
from the meaning representations. According to the
global sentence overlap measures, their quality is
not seriously impaired. On the other hand, the de-
sign of the representations has a substantial effect
on the prediction of the alternations. The SEMn
does not seem to learn certain preferences because
of the extremely imbalanced distribution in the in-
put data. This confirms the hypothesis sketched in
Section 3.1, according to which the degree of the
input?s underspecification can crucially change the
behaviour of the ranking model.
5.2 Exp. 2: Word Order and Voice
We examine the impact of certain feature types on
the prediction of the variation types in our data. We
are particularly interested in the interaction of voice
and word order (precedence) since linguistic theo-
ries (see Section 2.2) predict similar information-
structural factors guiding their use, but usually do
not consider them in conjunction.
In Table 4, we report the performance of ranking
models trained on the different feature subsets intro-
duced in Section 4. The union of the features corre-
sponds to the model trained on SEMh in Experiment
1. At a very broad level, the results suggest that the
precedence and the scale alignment features interact
both in the prediction of voice and word order.
The most pronounced effect on voice accuracy
can be seen when comparing the precedence model
to the union model. Adding the surface-independent
scale alignment features to the precedence features
leads to a big improvement in the prediction of word
order. This is not a trivial observation since a) the
surface-independent features do not discriminate be-
tween the word orders and b) the precedence fea-
tures are built from the same properties (see Sec-
tion 4). Thus, the SVM learner discovers depen-
dencies between relative precedence preferences and
abstract properties of a verb argument which cannot
be encoded in the precedence alone.
It is worth noting that the precedence features im-
prove the voice prediction. This indicates that wher-
ever the application context allows it, voice should
not be specified at a stage prior to word order. Ex-
ample (5) is taken from our development set, illus-
trating a case where the union model predicted the
correct voice and word order (5-a), and the scale
alignment model top-ranked the incorrect voice and
word order. The active verb arguments in (5-b) are
both case-ambigous and placed in the non-canonical
order (object < subject), so the semantic relation can
be easily misunderstood. The passive in (5-a) is un-
ambiguous since the agent is realised in a PP (and
placed in the Vorfeld).
(5) a. Von
By
den
the
deutschen
German
Medien
media
wurden
were
die
the
Ausla?nder
foreigners
nur
only
erwa?hnt,
mentioned,
wenn
when
es
there
Zoff
trouble
gab.
was.
b. Wenn
When
es
there
Zoff
trouble
gab,
was,
erwa?hnten
mentioned
die
the
Ausla?nder
foreigners
nur
only
die
the
deutschen
German
Medien.
media.
Moreover, our results confirm Filippova and
Strube (2007) who find that it is harder to predict
the correct Vorfeld occupant in a German sentence,
than to predict the relative order of the constituents.
5.3 Exp. 3: Capturing Flexible Variation
The previous experiment has shown that there is a
certain inter-dependence between word order and
voice. This experiment addresses this interaction
by varying the way the training data for the ranker
is labelled. We contrast two ways of labelling the
sentences (see Section 4): a) all sentences that are
not (nearly) identical to the reference sentence have
the rank category ?4?, irrespective of their voice (re-
ferred to as unlabelled model), b) the sentences that
do not realise the correct voice are ranked lower than
sentences with the correct voice (?4? vs. ?5?), re-
ferred to as labelled model. Intuitively, the latter
way of labelling tells the ranker that all sentences
in the incorrect voice are worse than all sentences
in the correct voice, independent of the word order.
Given the first labelling strategy, the ranker can de-
cide in an unsupervised way which combinations of
word order and voice are to be preferred.
1013
Top 1 Top 1 Top 1 Top 2 Top 3
Model Match BLEU NIST Voice Prec. Prec.+Voice Prec.+Voice Prec.+Voice
Labelled, no LM 21.52 0.73 12.93 91.9 76.25 71.01 78.35 82.31
Unlabelled, no LM 26.83 0.75 13.01 91.5 80.19 74.51 84.28 88.59
Unlabeled + LM 27.35 0.75 13.08 91.5 79.6 73.92 79.74 82.89
Table 5: Evaluation of Experiment 3
In Table 5, it can be seen that the unlabelled model
improves over the labelled on all the sentence over-
lap measures. The improvements are statistically
significant. Moreover, we compare the n-best ac-
curacies achieved by the models for the joint pre-
diction of voice and argument order. The unla-
belled model is very flexible with respect to the word
order-voice interaction: the accuracy dramatically
improves when looking at the top 3 sentences. Ta-
ble 5 also reports the performance of an unlabelled
model that additionally integrates LM scores. Sur-
prisingly, these scores have a very small positive ef-
fect on the sentence overlap features and no positive
effect on the voice and precedence accuracy. The
n-best evaluations even suggest that the LM scores
negatively impact the ranker: the accuracy for the
top 3 sentences increases much less as compared to
the model that does not integrate LM scores.6
The n-best performance of a realisation ranker is
practically relevant for re-ranking applications such
as Velldal (2008). We think that it is also concep-
tually interesting. Previous evaluation studies sug-
gest that the original corpus sentence is not always
the only optimal realisation of a given linguistic in-
put (Cahill and Forst, 2010; Belz and Kow, 2010).
Humans seem to have varying preferences for word
order contrasts in certain contexts. The n-best evalu-
ation could reflect the behaviour of a ranking model
with respect to the range of variations encountered
in real discourse. The pilot human evaluation in the
next Section deals with this question.
6 Human Evaluation
Our experiment in Section 5.3 has shown that the ac-
curacy of our linguistically informed ranking model
dramatically increases when we consider the three
6(Nakanishi et al, 2005) also note a negative effect of in-
cluding LM scores in their model, pointing out that the LM was
not trained on enough data. The corpus used for training our
LM might also have been too small or distinct in genre.
best sentences rather than only the top-ranked sen-
tence. This means that the model sometimes predicts
almost equal naturalness for different voice realisa-
tions. Moreover, in the case of word order, we know
from previous evaluation studies, that humans some-
times prefer different realisations than the original
corpus sentences. This Section investigates agree-
ment in human judgements of voice realisation.
Whereas previous studies in generation mainly
used human evaluation to compare different sys-
tems, or to correlate human and automatic evalua-
tions, our primary interest is the agreement or cor-
relation between human rankings. In particular, we
explore the hypothesis that this agreement is higher
in certain contexts than in others. In order to select
these contexts, we use the predictions made by our
ranking model.
The questionnaire for our experiment comprised
24 items falling into 3 classes: a) items where the
3 best sentences predicted by the model have the
same voice as the original sentence (?Correct?), b)
items where the 3 top-ranked sentences realise dif-
ferent voices (?Mixed?), c) items where the model
predicted the incorrect voice in all 3 top sentences
(?False?). Each item is composed of the original
sentence, the 3 top-ranked sentences (if not identical
to the corpus sentence) and 2 further sentences such
that each item contains different voices. For each
item, we presented the previous context sentence.
The experiment was completed by 8 participants,
all native speakers of German, 5 had a linguistic
background. The participants were asked to rank
each sentence on a scale from 1-6 according to its
naturalness and plausibility in the given context. The
participants were explicitly allowed to use the same
rank for sentences they find equally natural. The par-
ticipants made heavy use of this option: out of the
192 annotated items, only 8 are ranked such that no
two sentences have the same rank.
We compare the human judgements by correlat-
1014
ing them with Spearman?s ?. This measure is con-
sidered appropriate for graded annotation tasks in
general (Erk and McCarthy, 2009), and has also
been used for analysing human realisation rankings
(Velldal, 2008; Cahill and Forst, 2010). We nor-
malise the ranks according to the procedure in Vell-
dal (2008). In Table 6, we report the correlations
obtained from averaging over all pairwise correla-
tions between the participants and the correlations
restricted to the item and sentence classes. We used
bootstrap re-sampling on the pairwise correlations to
test that the correlations on the different item classes
significantly differ from each other.
The correlations in Table 6 suggest that the agree-
ment between annotators is highest on the false
items, and lowest on the mixed items. Humans
tended to give the best rank to the original sentence
more often on the false items (91%) than on the oth-
ers. Moreover, the agreement is generally higher on
the sentences realising the correct voice.
These results seem to confirm our hypothesis that
the general level of agreement between humans dif-
fers depending on the context. However, one has to
be careful in relating the effects in our data solely to
voice preferences. Since the sentences were chosen
automatically, some examples contain very unnatu-
ral word orders that probably guided the annotators?
decisions more than the voice. This is illustrated
by Example (6) showing two passive sentences from
our questionnaire which differ only in the position of
the adverb besser ?better?. Sentence (6-a) is com-
pletely implausible for a native speaker of German,
whereas Sentence (6-b) sounds very natural.
(6) a. Durch
By
das
the
neue
new
Gesetz
law
sollen
should
besser
better
Eigenheimbesitzer
house owners
geschu?tzt
protected
werden.
be.
b. Durch
By
das
the
neue
new
Gesetz
law
sollen
should
Eigenheimbesitzer
house owners
besser
better
geschu?tzt
protected
werden.
be.
This observation brings us back to our initial point
that the surface realisation task is especially chal-
lenging due to the interaction of a range of semantic
and discourse phenomena. Obviously, this interac-
tion makes it difficult to single out preferences for a
specific alternation type. Future work will have to
establish how this problem should be dealt with in
Items
All Correct Mixed False
?All? sent. 0.58 0.6 0.54 0.62
?Correct? sent. 0.64 0.63 0.56 0.72
?False? sent. 0.47 0.57 0.48 0.44
Top-ranked
corpus sent.
84% 78% 83% 91%
Table 6: Human Evaluation
the design of human evaluation experiments.
7 Conclusion
We have presented a grammar-based generation ar-
chitecture which implements the surface realisation
of meaning representations abstracting from voice
and word order. In order to be able to study voice
alternations in a variety of contexts, we designed
heuristic underspecification rules which establish,
for instance, the alternation relation between an ac-
tive with a generic agent and a passive that does
not overtly realise the agent. This strategy leads
to a better balanced distribution of the alternations
in the training data, such that our linguistically
informed generation ranking model achieves high
BLEU scores and accurately predicts active and pas-
sive. In future work, we will extend our experiments
to a wider range of alternations and try to capture
inter-sentential context more explicitly. Moreover, it
would be interesting to carry over our methodology
to a purely statistical linearisation system where the
relation between an input representation and a set of
candidate realisations is not so clearly defined as in
a grammar-based system.
Our study also addressed the interaction of dif-
ferent linguistic variation types, i.e. word order
and voice, by looking at different types of linguis-
tic features and exploring different ways of labelling
the training data. However, our SVM-based learn-
ing framework is not well-suited to directly assess
the correlation between a certain feature (or fea-
ture combination) and the occurrence of an alterna-
tion. Therefore, it would be interesting to relate our
work to the techniques used in theoretical papers,
e.g. (Bresnan et al, 2007), where these correlations
are analysed more directly.
1015
References
Judith Aissen. 1999. Markedness and subject choice in
optimality theory. Natural Language and Linguistic
Theory, 17(4):673?711.
Judith Aissen. 2003. Differential Object Marking:
Iconicity vs. Economy. Natural Language and Lin-
guistic Theory, 21:435?483.
Anja Belz and Eric Kow. 2010. Comparing rating
scales and preference judgements in language evalu-
ation. In Proceedings of the 6th International Natural
Language Generation Conference (INLG?10).
Anja Belz, Mike White, Josef van Genabith, Deirdre
Hogan, and Amanda Stent. 2010. Finding common
ground: Towards a surface realisation shared task.
In Proceedings of the 6th International Natural Lan-
guage Generation Conference (INLG?10).
Anja Belz. 2005. Statistical generation: Three meth-
ods compared and evaluated. In Proceedings of Tenth
European Workshop on Natural Language Generation
(ENLG-05), pages 15?23.
Bernd Bohnet, Leo Wanner, Simon Mill, and Alicia
Burga. 2010. Broad coverage multilingual deep sen-
tence generation with a stochastic multi-level realizer.
In Proceedings of the 23rd International Conference
on Computational Linguistics (COLING 2010), Bei-
jing, China.
Joan Bresnan, Shipra Dingare, and Christopher D. Man-
ning. 2001. Soft Constraints Mirror Hard Constraints:
Voice and Person in English and Lummi. In Proceed-
ings of the LFG ?01 Conference.
Joan Bresnan, Anna Cueni, Tatiana Nikitina, and Harald
Baayen. 2007. Predicting the Dative Alternation. In
G. Boume, I. Kraemer, and J. Zwarts, editors, Cogni-
tive Foundations of Interpretation. Amsterdam: Royal
Netherlands Academy of Science.
Aoife Cahill and Martin Forst. 2010. Human Evaluation
of a German Surface Realisation Ranker. In Proceed-
ings of the 12th Conference of the European Chapter
of the ACL (EACL 2009), pages 112 ? 120, Athens,
Greece. Association for Computational Linguistics.
Aoife Cahill and Arndt Riester. 2009. Incorporating
Information Status into Generation Ranking. In Pro-
ceedings of the 47th Annual Meeting of the ACL, pages
817?825, Suntec, Singapore, August. Association for
Computational Linguistics.
Aoife Cahill, Martin Forst, and Christian Rohrer. 2007.
Stochastic realisation ranking for a free word order
language. In Proceedings of the Eleventh European
Workshop on Natural Language Generation, pages
17?24, Saarbru?cken, Germany, June. DFKI GmbH.
Document D-07-01.
Dick Crouch and Tracy Holloway King. 2006. Se-
mantics via F-Structure Rewriting. In Miriam Butt
and Tracy Holloway King, editors, Proceedings of the
LFG06 Conference.
Dick Crouch, Mary Dalrymple, Ron Kaplan, Tracy King,
John Maxwell, and Paula Newman. 2006. XLE Docu-
mentation. Technical report, Palo Alto Research Cen-
ter, CA.
Katrin Erk and Diana McCarthy. 2009. Graded Word
Sense Assignment. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 440 ? 449, Singapore.
Katja Filippova and Michael Strube. 2007. Generat-
ing constituent order in German clauses. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics (ACL 07), Prague, Czech
Republic.
Katja Filippova and Michael Strube. 2009. Tree lin-
earization in English: Improving language model
based approaches. In Companion Volume to the Pro-
ceedings of Human Language Technologies Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics (NAACL-HLT 09,
short)., Boulder, Colorado.
Martin Forst. 2007. Filling Statistics with Linguistics
? Property Design for the Disambiguation of German
LFG Parses. In ACL 2007 Workshop on Deep Linguis-
tic Processing, pages 17?24, Prague, Czech Republic,
June. Association for Computational Linguistics.
Matthew Gerber and Joyce Chai. 2010. Beyond nom-
bank: A study of implicit argumentation for nominal
predicates. In Proceedings of the ACM Conference on
Knowledge Discovery and Data Mining (KDD).
Barbara J. Grosz, Aravind Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203?225.
Thorsten Joachims. 1996. Training linear svms in linear
time. In M. Butt and T. H. King, editors, Proceedings
of the ACM Conference on Knowledge Discovery and
Data Mining (KDD), CSLI Proceedings Online.
Jonas Kuhn. 2003. Optimality-Theoretic Syntax?A
Declarative Approach. CSLI Publications, Stanford,
CA.
Irene Langkilde and Kevin Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
Proceedings of the ACL/COLING-98, pages 704?710,
Montreal, Quebec.
Tomasz Marciniak and Michael Strube. 2005. Using
an annotated corpus as a knowledge source for lan-
guage generation. In Proceedings of Workshop on Us-
ing Corpora for Natural Language Generation, pages
19?24, Birmingham, UK.
Hiroko Nakanishi, Yusuke Miyao, and Junichi Tsujii.
2005. Probabilistic models for disambiguation of an
1016
HPSG-based chart generator. In Proceedings of IWPT
2005.
Adwait Ratnaparkhi. 2000. Trainable methods for sur-
face natural language generation. In Proceedings of
NAACL 2000, pages 194?201, Seattle, WA.
Stefan Riezler, Detlef Prescher, Jonas Kuhn, and Mark
Johnson. 2000. Lexicalized stochastic modeling of
constraint-based grammars using log-linear measures
and EM training. In Proceedings of the 38th Annual
Meeting of the Association for Computational Linguis-
tics (ACL?00), Hong Kong, pages 480?487.
Stefan Riezler, Dick Crouch, Ron Kaplan, Tracy King,
John Maxwell, and Mark Johnson. 2002. Parsing the
Wall Street Journal using a Lexical-Functional Gram-
mar and discriminative estimation techniques. In Pro-
ceedings of the 40th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL?02), Pennsyl-
vania, Philadelphia.
Eric K. Ringger, Michael Gamon, Robert C. Moore,
David Rojas, Martine Smets, and Simon Corston-
Oliver. 2004. Linguistically Informed Statistical
Models of Constituent Structure for Ordering in Sen-
tence Realization. In Proceedings of the 2004 In-
ternational Conference on Computational Linguistics,
Geneva, Switzerland.
Christian Rohrer and Martin Forst. 2006. Improving
coverage and parsing quality of a large-scale LFG for
German. In Proceedings of LREC-2006.
Erik Velldal and Stephan Oepen. 2006. Statistical rank-
ing in tactical generation. In Proceedings of the 2006
Conference on Empirical Methods in Natural Lan-
guage Processing, Sydney, Australia.
Erik Velldal. 2008. Empirical Realization Ranking.
Ph.D. thesis, University of Oslo, Department of Infor-
matics.
Sina Zarrie? and Jonas Kuhn. 2010. Reversing F-
structure Rewriting for Generation from Meaning Rep-
resentations. In Proceedings of the LFG10 Confer-
ence, Ottawa.
Sina Zarrie?. 2009. Developing German Semantics on
the basis of Parallel LFG Grammars. In Proceed-
ings of the 2009 Workshop on Grammar Engineering
Across Frameworks (GEAF 2009), pages 10?18, Sun-
tec, Singapore, August. Association for Computational
Linguistics.
1017
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1547?1557,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Combining Referring Expression Generation and Surface Realization:
A Corpus-Based Investigation of Architectures
Sina Zarrie? Jonas Kuhn
Institut fu?r maschinelle Sprachverarbeitung
University of Stuttgart, Germany
sina.zarriess,jonas.kuhn@ims.uni-stuttgart.de
Abstract
We suggest a generation task that inte-
grates discourse-level referring expression
generation and sentence-level surface re-
alization. We present a data set of Ger-
man articles annotated with deep syntax
and referents, including some types of im-
plicit referents. Our experiments compare
several architectures varying the order of a
set of trainable modules. The results sug-
gest that a revision-based pipeline, with in-
termediate linearization, significantly out-
performs standard pipelines or a parallel
architecture.
1 Introduction
Generating well-formed linguistic utterances from
an abstract non-linguistic input involves making
a multitude of conceptual, discourse-level as well
as sentence-level, lexical and syntactic decisions.
Work on rule-based natural language generation
(NLG) has explored a number of ways to com-
bine these decisions in an architecture, ranging
from integrated systems where all decisions hap-
pen jointly (Appelt, 1982) to strictly sequential
pipelines (Reiter and Dale, 1997). While inte-
grated or interactive systems typically face issues
with efficiency and scalability, they can directly
account for interactions between discourse-level
planning and linguistic realization. For instance,
Rubinoff (1992) mentions Example (1) where the
sentence planning component needs to have ac-
cess to the lexical knowledge that ?order? and not
?home? can be realized as a verb in English.
(1) a. *John homed him with an order.
b. John ordered him home.
In recent data-driven generation research, the
focus has somewhat shifted from full data-to-text
systems to approaches that isolate well-defined
subproblems from the NLG pipeline. In particular,
the tasks of surface realization and referring ex-
pression generation (REG) have received increas-
ing attention using a number of available anno-
tated data sets (Belz and Kow, 2010; Belz et al,
2011). While these single-task approaches have
given rise to many insights about algorithms and
corpus-based modelling for specific phenomena,
they can hardly deal with aspects of the architec-
ture and interaction between generation levels.
This paper suggests a middle ground between
full data-to-text and single-task generation, com-
bining two well-studied NLG problems. We in-
tegrate a discourse-level approach to REG with
sentence-level surface realization in a data-driven
framework. We address this integrated task with a
set of components that can be trained on flexible
inputs which allows us to systematically explore
different ways of arranging the components in a
generation architecture. Our main goal is to inves-
tigate how different architectural set-ups account
for interactions between generation decisions at
the level of referring expressions (REs), syntax
and word order.
Our basic set-up is inspired from the Generating
Referring Expressions in Context (GREC) tasks,
where candidate REs have to be assigned to in-
stances of a referent in a Wikipedia article (Belz
and Kow, 2010). We have created a dataset of Ger-
man texts with annotations that extend this stan-
dard in three substantial ways: (i) our domain con-
sists of articles about robbery events that mainly
involve two main referents, a victim and a per-
petrator (perp), (ii) annotations include deep and
shallow syntactic relations similar to the repre-
sentations used in (Belz et al, 2011) (iii) anno-
tations include empty referents, as e.g. in passives
and nominalizations directing attention to the phe-
nomenon of implicit reference, which is largely
understudied in NLG. Figure 1 presents an exam-
ple for a deep syntax tree with underspecified RE
1547
(Tree) be
agent
perp
mod
on
pobj
trial
mod
because
sub
attack
agent
perp
theme
victim
perp italians
two
men
the two
they <empty>
victim man
a young
victim
the
he <empty>
Figure 1: Underspecified tree with RE candidates
slots and lists of candidates REs for each referent.
Applying a strictly sequential pipeline on our
data, we observe incoherent system output that
is related to an interaction of generation levels,
very similar to the interleaving between sentence
planning and lexicalization in Example (1). A
pipeline that first inserts REs into the underspec-
ified tree in Figure 1, then generates syntax and fi-
nally linearizes, produces inappropriate sentences
like (2-a).
(2) a. *[The two men]p are on trial because of an attack
by [two italians]p on [a young man]v .
b. [Two italians]p are on trial because of an attack on
[a young man]v .
Sentence (2-a) is incoherent because the syntac-
tic surface obscurs the intended meaning that ?two
italians? and ?the two men? refer to the same ref-
erent. In order to generate the natural Sentence
(2-b), the RE component needs information about
linear precedence of the two perp instances and the
nominalization of ?attack?. These types of inter-
actions between referential and syntactic realiza-
tion have been thoroughly discussed in theoretical
accounts of textual coherence, as e.g. Centering
Theory (Grosz et al, 1995).
The integrated modelling of REG and surface
realization leads to a considerable expansion of
the choice space. In a sentence with 3 referents
that each have 10 RE candidates and can be freely
ordered, the number of surface realizations in-
creases from 6 to 6?103, assuming that the remain-
ing words can not be syntactically varied. Thus,
even when the generation problem is restricted to
these tasks, a fully integrated architecture faces
scalability issues on realistic corpus data.
In this work, we assume a modular set-up of
the generation system that allows for a flexible
ordering of the single components. Our experi-
ments vary 3 parameters of the generation archi-
tecture: 1) the sequential order of the modules,
2) parallelization of modules, 3) joint vs. sepa-
rate modelling of implicit referents. Our results
suggest that the interactions between RE and syn-
tax can be modelled in sequential generation ar-
chitecture where the RE component has access
to information about syntactic realization and an
approximative, intermediate linearization. Such
a system is reminiscent of earlier work in rule-
based generation that implements an interactive or
revision-based feedback between discourse-level
planning and linguistic realisation (Hovy, 1988;
Robin, 1993).
2 Related Work
Despite the common view of NLG as a pipeline
process, it is a well-known problem that high-
level, conceptual knowledge and low-level lin-
guistic knowledge are tightly interleaved (Danlos,
1984; Mellish et al, 2000). In rule-based, strictly
sequential generators these interactions can lead
to a so-called generation gap, where a down-
stream module cannot realize a text or sentence
plan generated by the preceding modules (Meteer,
1991; Wanner, 1994). For this reason, a num-
ber of other architectures has been proposed, see
De Smedt et al (1996) for an overview. For rea-
sons of tractability and scalability, many prac-
tical NLG systems still have been designed as
sequential pipelines that follow the basic layout
of macroplanning-microplanning-linguistic real-
ization (Reiter, 1994; Cahill et al, 1999; Bateman
and Zock, 2003).
In recent data-driven research on NLG, many
single tasks have been addressed with corpus-
based methods. For surface realization, the stan-
dard set-up is to regenerate from syntactic rep-
resentations that have been produced for realis-
tic corpus sentences. The first widely known sta-
tistical approach by Langkilde and Knight (1998)
used language-model n-gram statistics on a word
lattice of candidate realisations to guide a ranker.
Subsequent work explored ways of exploiting lin-
guistically annotated data for trainable generation
models (Ratnaparkhi, 2000; Belz, 2005). Work on
data-driven approaches has led to insights about
the importance of linguistic features for sentence
1548
linearization decisions (Ringger et al, 2004; Filip-
pova and Strube, 2007; Cahill and Riester, 2009).
(Zarrie? et al, 2012) have recently argued that
the good performance of these linguistically mo-
tivated word order models, which exploit morpho-
syntactic features of noun phrases (i.e. refer-
ents), is related to the fact that these morpho-
syntactic features implicitly encode a lot of knowl-
edge about the underlying discourse or informa-
tion structure.
A considerable body of REG research has been
done in the paradigm established by Dale (1989;
1995). More closely related to our work are ap-
proaches in the line of Siddarthan and Copes-
take (2004) or Belz and Varges (2007) who gener-
ate contextually appropriate REs for instances of a
referent in a text. Belz and Varges (2007)?s GREC
data set includes annotations of implicit subjects
in coordinations. Zarrie? et al (2011) deal with
implicit subjects in passives, proposing a set of
heuristics for adding these agents to the genera-
tion input. Roth and Frank (2012) acquire au-
tomatic annotations of implicit roles for the pur-
pose of studying coherence patterns in texts. Im-
plicit referents have also received attention for the
analysis of semantic roles (Gerber and Chai, 2010;
Ruppenhofer et al, 2010).
Statistical methods for data-to-text generation
have been explored only recently. Belz (2008)
trains a probabilistic CFG to generate weather
forecasts, Chen et al (2010) induce a synchronous
grammar to generate sportcaster text. Both ad-
dress a restricted domain where a direct align-
ment between units in the non-linguistic represen-
tation and the linguistic utterance can be learned.
Marciniak and Strube (2005) propose an ILP
model for global optimization in a generation task
that is decomposed into a set of classifiers. Bohnet
et al (2011) deal with multi-level generation in a
statistical framework and in a less restricted do-
main. They adopt a standard sequential pipeline
approach.
Recent corpus-based generation approaches
faced the problem that existing standard treebank
representations for parsing or other analysis tasks
do not necessarily fit the needs of generation
(Bohnet et al, 2010; Wanner et al, 2012). Zarrie?
et al (2011) discuss the problem of an input rep-
resentation that is appropriately underspecified for
the realistic generation of voice alternations.
3 The Data Set
The data set for our generation experiments con-
sists of 200 newspaper articles about robbery
events. The articles were extracted from a large
German newspaper corpus. A complete example
text with RE annotations is given in Figure 2, Ta-
ble 1 summarizes some data set statistics.
3.1 RE annotation
The RE annotations mark explicit and implicit
mentions of referents involved in the robbery event
described in an article. Explicit mentions are
marked as spans on the surface sentence, labeled
with the referent?s role and an ID. We annotate the
following referential roles: (i) perpetrator (perp),
(ii) victim, (iii) source, according to the core roles
of the Robbery frame in English FrameNet. We
include source since some texts do not mention a
particular victim, but rather the location of the rob-
bery (e.g. a bank, a service station). The ID distin-
guishes referents that have the same role, e.g. ?the
husband? and the ?young family? in Sentences
(3-a) and (3-d) in Figure 2. Each RE is linked to
its syntactic head. This complies with the GREC
data sets, and is also useful for further annotation
of the deep syntax level (see Section 3.2).
The RE implicit mentions of victim, perp, and
source are annotated as attributes of their syntac-
tic heads in the surface sentence. We consider the
following types of implicit referents: (i) agents in
passives (e.g. ?robbed? in (3-a)), (ii) arguments of
nominalizations (e.g. ?resistance? in (3-e)), (iii)
possessives (e.g. ?watch? in (3-f)), (iv) missing
subjects in coordinations. (e.g. ?flee? in (3-f))
The brat tool (Stenetorp et al, 2012) was used
for annotation. We had 2 annotators with a compu-
tational linguistic background, provided with an-
notation guidelines. They were trained on a set of
20 texts. We measure a good agreement on another
set of 15 texts: the simple pairwise agreement for
explicit mentions is 95.14%-96.53% and 78.94%-
76.92% for implicit mentions.1
3.2 Syntax annotation
The syntactic annotation of our data includes two
layers: shallow and deep, labeled dependencies,
similar to the representation used in surface real-
ization shared tasks (Belz et al, 2011). We use
1Standard measures for the ?above chance annotator
agreement? are only defined for task where the set of anno-
tated items is pre-defined.
1549
(3) a.
 Junge Familie v:0 Young family aufon demthe Heimwegposs:vway homeposs:v ausgeraubtag:probbedag:p
b. Die
The
Polizei
police
sucht
looks
nach
for
zwei ungepflegt wirkenden jungen Ma?nnern im Alter von etwa 25 Jahren p:0.
two shabby-looking young men of about 25 years .
c. Sie p:0
They
sollen
are said to
am
on
Montag
Monday
gegen
around
20
20
Uhr
o?clock
 eine junge Familie mit ihrem sieben Monate alten Baby v:0 a young family with their seven month old baby aufondem
the
Heimwegposs:v
way homeposs:v
von
from
einem
a
Einkaufsbummel
shopping tour
u?berfallen
attacked
und
and
ausgeraubt
robbed
haben.
have.
d. Wie
As
die
the
Polizei
police
berichtet,
reports,
drohten
threatened
die zwei Ma?nner p:0
the two men
  dem Ehemann v:1,  the husband
  ihn v:1  him zusammenzuschlagen.beat up.
e.
  Er v:1  He gabgave deshalbtherefore
  seine v:1  his Brieftaschewallet ohnewithout Gegenwehrag:v,the:presistanceag:v,the:p heraus.out.
f. Anschlie?end
Afterwards
nahmen
took
  ihm v:1  him die Ra?uber p:0the robbers nochalso diethe Armbanduhrposs:vwatchposs:v aboff undand flu?chtetenag:p.fleedag:p.
Figure 2: Example text with RE annotations, oval boxes mark victim mentions, square boxes mark perp
mentions, heads of implicit arguments are underlined
the Bohnet (2010) dependency parser to obtain an
automatic annotation of shallow or surface depen-
dencies for the corpus sentences.
The deep syntactic dependencies are derived
from the shallow layer by a set of hand-written
transformation rules. The goal is to link referents
to their main predicate in a uniform way, indepen-
dently of the surface-syntactic realization of the
verb. We address passives, nominalizations and
possessives corresponding to the contexts where
we annotated implicit referents (see above). The
transformations are defined as follows:
1. remove auxiliary nodes, verb morphology and finite-
ness, a tense feature distinguishes past and present, e.g.
?haben:AUX u?berfallen:VVINF? (have attacked) maps
to ?u?berfallen:VV:PAST? (attack:PAST)
2. map subjects in actives and oblique agents in passives
to ?agents?; objects in actives and subjects in passive to
?themes?, e.g. victim/subj was attacked by perp/obl-ag
maps to perp/agent attack victim/theme
3. attach particles to verb lemma, e.g. ?gab? ... ?heraus?
in (3-e) is mapped to ?herausgeben? (give to)
4. map nominalized to verbal lemmas, their prepositional
and genitive arguments to semantic subjects and ob-
jects, e.g. attack on victim is mapped to attack vic-
tim/theme
5. normalize prenominal and genitive postnominal poses-
sives, e.g. ?seine Brieftasche? (his wallet) and ?die
Brieftasche des Opfers? (the wallet of the victim) map
to ?die Brieftasche POSS victim? (the wallet of victim),
only applies if possessive is an annotated RE
Nominalizations are mapped to their verbal
base forms on the basis of lexicalized rules for the
nominalized lemmas observed in the corpus. The
other transformations are defined on the shallow
dependency annotation.
# sentences 2030
# explicit REs 3208
# implicit REs 1778
# passives 383
# nominalizations 393
# possessives 1150
Table 1: Basic annotation statistics
3.3 Multi-level Representation
In the final representation of our data set, we inte-
grate the RE and deep syntax annotation by replac-
ing subtrees corresponding to an RE span. The RE
slot in the tree of the sentence is labeled with its
referential role and its ID. All RE subtrees for a
referent in a text are collected in a candidate list
which is initialized with three default REs: (i) a
pronoun, (ii) a default nominal (e.g. ?the victim?),
(iii) the empty RE. In contrast to the GREC data
sets, our RE candidates are not represented as the
original surface strings, but as non-linearized sub-
trees. The resulting multi-layer representation for
each text is structured as follows:
1. unordered deep trees with RE slots (deepSyn?re)
2. unorderd shallow trees with RE slots
(shallowSyn?re)
3. unordered RE subtrees
4. linearized, fully specified surface trees (linSyn+re)
5. alignments between nodes in 1., 2., 4.
The generation components in Section 4 also
use intermediate layers where REs are inserted
into the deep trees (deepSyn+re) or shallow trees
(shallowSyn+re).
Nodes in unordered trees are deterministically
sorted by their : 1. distance to the root, 2. label,
1550
3. PoS tag, 4. lemma. The generation components
traverse the nodes in this the order.
4 Generation Systems
Our main goal is to investigate different architec-
tures for combined surface realization and refer-
ring expression generation. We assume that this
task is split into three main modules: a syntax gen-
erator, an REG component, and a linearizer. The
components are implemented in a way that they
can be trained and applied on varying inputs, de-
pending on the pipeline. Section 4.1 describes the
basic set-up of our components. Section 4.2 de-
fines the architectures that we will compare in our
experiments (Section 5). Section 4.3 presents the
implementation of the underlying feature models.
4.1 Components
4.1.1 SYN: Deep to Shallow Syntax
For mapping deep to shallow dependency trees,
the syntax generator induces a probabilistic tree
transformation. The transformations are restricted
to verb nodes in the deep tree (possessives are
handled in the RE module) and extracted from
the alignments between the deep and shallow
layer in the training input. As an example, the
deep node ?attack:VV? aligns to ?have:AUX at-
tacked:VVINF?, ?attacks:VVFIN?, ?the:ART at-
tack:NN on:PRP?. The learner is implemented
as a ranking component, trained with SVMrank
(Joachims, 2006). During training, each instance
of a verb node has one optimal shallow depen-
dency alignment and a set of distractor candidates.
During testing, the module has to pick the best
shallow candidate according to its feature model.
In our crossvalidation set-up (see Section 5),
we extract, on average, 374 transformations from
the training sets. This set subdivides into non-
lexicalized and lexicalized transformations. The
mapping rule in (4-a) that simply rewrites the verb
underspecified PoS tag to the finite verb tag in the
shallow tree illustrates the non-lexicalized case.
Most transformation rules (335 out of 374 on aver-
age) are lexicalized for a specific verb lemma and
mostly transform nominalizations as in rule (4-b)
and particles (see Section 3.2).
(4) a. (x,lemma,VV,y)? (x,lemma,VVFIN,y)
b. (x,u?berfallen/attack,VV,y) ? (x,bei/at,PREP,y),
(z,U?berfall/attack,NN,x),(q,der/the,ART,z)
The baseline for the verb transformation com-
ponent is a two-step procedure: 1) pick a lexical-
ized rule if available for that verb lemma, 2) pick
the most frequent transformation.
4.1.2 REG: Realizing Referring Expressions
Similar to the syntax component, the REG mod-
ule is implemented as a ranker that selects surface
RE subtrees for a given referential slot in a deep
or shallow dependency tree. The candidates for
the ranking correspond to the entire set of REs
used for that referential role in the original text
(see Section 3.1). The basic RE module is a joint
model of all RE types, i.e. nominal, pronominal
and empty realizations of the referent. For the ex-
periment in Section 5.4, we use an additional sep-
arate classifier for implicit referents, also trained
with SVMrank. It uses the same feature model
as the full ranking component, but learns a binary
distinction for implicit or explicit mentions of a
referent. The explicit mentions will be passed to
the RE ranking component.
The baseline for the REG component is defined
as follows: if the preceding and the current RE
slot are instances of the same referent, realize a
pronoun, else realize the longest nominal RE can-
didate that has not been used in the preceding text.
4.1.3 LIN: Linearization
For linearization, we use the state-of-the-art
dependency linearizer described in Bohnet et
al. (2012). We train the linearizer on an auto-
matically parsed version of the German TIGER
treebank (Brants et al, 2002). This version
was produced with the dependency parser by
Bohnet (2010), trained on the dependency conver-
sion of TIGER by Seeker and Kuhn (2012).
4.2 Architectures
Depending on the way the generation components
are combined in an architecture, they will have ac-
cess to different layers of the input representation.
The following definitions of architectures recur to
the layers introduced in Section 3.3.
4.2.1 First Pipeline
The first pipeline corresponds most closely to a
standard generation pipeline in the sense of (Reiter
and Dale, 1997). REG is carried out prior to sur-
face realization such that the RE component does
not have access to surface syntax or word order
whereas the SYN component has access to fully
specified RE slots.
? training
1551
1. train REG: (deepSyn?re, deepSyn+re)
2. train SYN: (deepSyn+re, shallowSyn+re)
? prediction
1. apply REG: deepSyn?re ? deepSyn+re
2. apply SYN: deepSyn+re ? shallowSyn+re
3. linearize: shallowSyn+re ? linSyn+re
4.2.2 Second Pipeline
In the second pipeline, the order of the RE and
SYN component is switched. In this case, REG
has access to surface syntax without word order
but the surface realization is trained and applied
on trees with underspecified RE slots.
? training
1. train SYN: (deepSyn?re, shallowSyn?re)
2. train REG: (shallowSyn?re, shallowSyn+re)
? prediction
1. apply SYN: deepSyn?re ? shallowSyn?re
2. apply REG: shallowSyn?re ?
shallowSyn+re
3. linearize: shallowSyn+re ? linSyn+re
4.2.3 Parallel System
A well-known problem with pipeline architectures
is the effect of error propagation. In our parallel
system, the components are trained independently
of each other and applied in parallel on the deep
syntactic input with underspecified REs.
? training
1. train SYN: (deepSyn?re, shallowSyn?re)
2. train REG: (deepSyn?re, deepSyn+re)
? prediction
1. apply REG and SYN:
deepSyn?re ? shallowSyn+re
2. linearize: shallowSyn+re ? linSyn+re
4.2.4 Revision-based System
In the revision-based system, the RE component
has access to surface syntax and a preliminary lin-
earization, called prelinSyn. In this set-up, we ap-
ply the linearizer first on trees with underspeci-
fied RE slots. For this step, we insert the default
REs for the referent into the respective slots. After
REG, the tree is linearized once again.
? training
1. train SYN on gold pairs of
(deepSyn?re, shallowSyn?re)
2. train REG on gold pairs of
(prelinSyn?re, prelinSyn+re)
? prediction
1. apply SYN: deepSyn?re ? shallowSyn?re
2. linearize: shallowSyn?re ? prelinSyn?re
3. apply REG: prelinSyn?re ? prelinSyn+re
4. linearize: prelinSyn+re ? linSyn+re
4.3 Feature Models
The implementation of the feature models is based
on a general set of templates for the SYN and REG
component. The exact form of the models depends
on the input layer of a component in a given ar-
chitecture. For instance, when SYN is trained on
deepSyn?re, the properties of the children nodes
are less specific for verbs that have RE slots as
their dependents. When the SYN component is
trained on deepSyn+re, lemma and POS of the
children nodes are always specified.
The feature templates for SYN combine prop-
erties of the shallow candidate nodes (label, PoS
and lemma for top node and its children) with the
properties of the instance in the tree: (i) lemma,
tense, (ii) sentence is a header, (iii) label, PoS,
lemma of mother node, children and grandchil-
dren nodes (iv) number, lemmas of other verbs in
the sentence.
The feature templates for REG combine proper-
ties of the candidate RE (PoS and lemma for top
node and its children, length) with properties of
the RE slot in the tree: lemma, PoS and labels for
the (i) mother node, (ii) grandmother node, (iii)
uncle and sibling nodes. Additionally, we imple-
ment a small set of global properties of a referent
in a text: (i) identity is known, (ii) plural or sin-
gular referent, (iii) age is known, and a number of
contextual properties capturing the previous refer-
ents and their predicted REs: (i) role and realiza-
tion of the preceding referent, (ii) last mention of
the current referent, (iii) realization of the referent
in the header.
5 Experiments
In this experimental section, we provide a corpus-
based evaluation of the generation components
and architectures introduced in Section 4. In the
following, Section 5.1 presents the details of our
evaluation methodology. In Section 5.2, we dis-
cuss the first experiment that evaluates the pipeline
architectures and the single components on oracle
inputs. Section 5.3 describes an experiment which
compares the parallel and the revision-based ar-
chitecture against the pipeline. In Section 5.4, we
compare two methods for dealing with the implicit
referents in our data. Section 5.5 provides some
general discussion of the results.
1552
Sentence overlap SYN Accuracy RE Accuracy
Input System BLEU NIST BLEUr String Type String Type Impl
deepSyn?re Baseline 42.38 9.9 47.94 35.66 44.81 33.3 36.03 50.43
deepSyn?re 1st pipeline 54.65 11.30 59.95 57.09 68.15 54.61 71.51 84.72
deepSyn?re 2nd pipeline 54.28 11.25 59.62 59.14 68.58 52.24 68.2 82
gold deepSyn+re SYN?LIN 63.9 12.7 62.86 60.83 69.74 100 100 100
gold shallowSyn?re REG?LIN 60.57 11.87 68.06 100 100 60.53 75.86 88.86
gold shallowSyn+re LIN 79.17 13.91 72.7 100 100 100 100 100
Table 2: Evaluating pipeline architectures against the baseline and upper bounds
5.1 Evaluation Measures
We split our data set into 10 splits of 20 articles.
We use one split as the development set, and cross-
validate on the remaining splits. In each case,
the downstream modules of the pipeline will be
trained on the jackknifed training set.
Text normalization: We carry out automatic
evaluation calculated on lemmatized text with-
out punctuation, excluding additional effects that
would be introduced from a morphology genera-
tion component.
Measures: First, we use a number of evalua-
tion measures familiar from previous generation
shared tasks:
1. BLEU, sentence-level geometric mean of 1- to 4-gram
precision, as in (Belz et al, 2011)
2. NIST, sentence-level n-gram overlap weighted in
favour of less frequent n-grams, as in (Belz et al, 2011)
3. RE Accuracy on String, proportion of REs selected by
the system with a string identical to the RE string in the
original corpus, as in (Belz and Kow, 2010)
4. RE Accuracy on Type, proportion of REs selected by
the system with an RE type identical to the RE type in
the original corpus, as in (Belz and Kow, 2010)
Second, we define a number of measures moti-
vated by our specific set-up of the task:
1. BLEUr , sentence-level BLEU computed on post-
processed output where predicted referring expressions
for victim and perp are replaced in the sentences (both
gold and predicted) by their original role label, this
score does not penalize lexical mismatches between
corpus and system REs
2. RE Accuracy on Impl, proportion of REs predicted cor-
rectly as implicit/non-implicit
3. SYN Accuracy on String, proportion of shallow verb
candidates selected by the system with a string identical
to the verb string in the original corpus
4. SYN Accuracy on Type, proportion of shallow verb
candidates selected by the system with a syntactic cat-
egory identical to the category in the original corpus
5.2 Pipelines and Upper Bounds
The first experiment addresses the first and sec-
ond pipeline introduced in Section 4.2.1 and 4.2.2.
The baseline combines the baseline version of
the SYN component (Section 4.1.1) and the REG
component (Section 4.1.2) respectively. As we re-
port in Table 2, both pipelines largely outperform
the baseline. Otherwise, they obtain very similar
scores in all measures with a small, weakly signif-
icant tendency for the first pipeline. The only re-
markable difference is that the accuracy of the in-
dividual components is, in each case, lower when
they are applied as the second step in the pipeline.
Thus, the RE accuracy suffers from mistakes from
the predicted syntax in the same way that the qual-
ity of syntax suffers from predicted REs.
The three bottom rows in Table 2 report the per-
formance of the individual components and lin-
earization when they are applied to inputs with an
REG and SYN oracle, providing upper bounds for
the pipelines applied on deepSyn?re. When REG
and linearization are applied on shallowSyn?re
with gold shallow trees, the BLEU score is
lower (60.57) as compared to the system that ap-
plies syntax and linearization on deepSyn+re,
deep trees with gold REs (BLEU score of 63.9).
However, the BLEUr score, which generalizes
over lexical RE mismatches, is higher for the
REG?LIN components than for SYN?LIN.
Moreover, the BLEUr score for the REG?LIN
system comes close to the upper bound that ap-
plies linearization on linSyn+re, gold shallow
trees with gold REs (BLEUr of 72.4), whereas
the difference in standard BLEU and NIST is
high. This effect indicates that the RE predic-
tion mostly decreases BLEU due to lexical mis-
matches, whereas the syntax prediction is more
likely to have a negative impact on final lineariza-
tion.
The error propagation effects that we find in the
first and second pipeline architecture clearly show
that decisions at the levels of syntax, reference
and word order interact, otherwise their predic-
1553
Input System BLEU NIST BLEUr
deepSyn?re 1st pipeline 54.65 11.30 59.95
deepSyn?re Parallel 54.78 11.30 60.05
deepSyn?re Revision 56.31 11.42 61.30
Table 3: Architecture evaluation
tion would not affect each other. In particular, the
REG module seems to be affected more seriously,
the String Accuracy decreases from 60.53 on gold
shallow trees to 52.24 on predicted shallow trees
whereas the Verb String Accuracy decreases from
60.83 on gold REs to 57.04 on predicted REs.
5.3 Revision or parallelism?
The second experiment compares the first pipeline
against the parallel and the revision-based ar-
chitecture introduced in Section 4.2.3 and 4.2.4.
The evaluation in Table 3 shows that the paral-
lel architecture improves only marginally over the
pipeline. By contrast, we obtain a clearly signifi-
cant improvement for the revision-based architec-
ture on all measures. The fact that this architec-
ture significantly improves the BLEU, NIST and
the BLEUr score of the parallel system indicates
that the REG benefits from the predicted syntax
when it is approximatively linearized. The fact
that also the BLEUr score improves shows that a
higher lexical quality of the REs leads to better fi-
nal linearizations.
Table 4 shows the performance of the REG
module on varying input layers, providing a more
detailed analysis of the interaction between RE,
syntax and word order. In order to produce the
deeplinSyn?re layer, deep syntax trees with ap-
proximative linearizations, we preprocessed the
deep trees by inserting a default surface trans-
formation for the verb nodes. We compare this
input for REG against the prelinSyn?re layer
used in the revision-based architecture, and the
deepSyn?re layer used in the pipeline and the par-
allel architecture. The REG module benefits from
the linearization in the case of deeplinSyn?re
and prelinSyn?re, outperforming the compo-
nent trained applied on the non-linearized deep
syntax trees. However, the REG module ap-
plied on prelinSyn?re, predicted shallow and lin-
earized trees, clearly outperforms the module ap-
plied on deeplinSyn?re. This shows that the
RE prediction can actually benefit from the pre-
dicted shallow syntax, but only when the predicted
trees are approximatively linearized. As an up-
per bound, we report the performance obtained on
RE Accuracy
Input System String Type Impl
deepSyn?re RE 54.61 71.51 84.72
deeplinSyn?re RE 56.78 72.23 84.71
prelinSyn?re RE 58.81 74.34 86.37
gold linSyn?re RE 68.63 83.63 94.74
Table 4: RE generation from different input layers
linSyn?re, gold shallow trees with gold lineariza-
tions. This set-up corresponds to the GREC tasks.
The gold syntax leads to a huge increase in perfor-
mance.
These results strengthen the evidence from the
previous experiment that decisions at the level of
syntax, reference and word order are interleaved.
A parallel architecture that simply ?circumvents?
error propagation effects by making decisions in-
dependent of each other is not optimal. Instead,
the automatic prediction of shallow syntax can
positively impact on RE generation if these shal-
low trees are additionally processed with an ap-
proximative linearization step.
5.4 A joint treatment of implicit referents?
The previous experiments have pursued a joint
approach for modeling implicit referents. The
hypothesis for this experiment is that the SYN
component and the intermediate linearization in
a revision-based architecture could benefit from a
separate treatment of implicit referents since verb
alternations like passive or nominalization often
involve referent deletions.
The evaluation in Table 5 provides contradic-
tory results depending on the evaluation measure.
For the first pipeline, the system with a separate
treatment of implicit referents significantly outper-
forms the joint system in terms of BLEU. How-
ever, the BLEUr score does not improve. In the
revision-based architecture, we do not find a clear
result for or against a joint modelling approach.
The revision-based system with disjoint modelling
of implicits shows a slight, non-significant in-
crease in BLEU score. By contrast, the BLEUr
score is signficantly better for the joint approach.
We experimented with parallelization of syntax
generation and prediction of implicit referents in
a revision-based system. This has a small positive
effect on the BLEUr score and a small negative
effect on the plain BLEU and NIST score. These
contradictory scores might indicate that the auto-
matic evaluation measures cannot capture all as-
pects of text quality, an issue that we discuss in
the following.
1554
(5) Generated by sequential system:
a. Deshalb
Therefore
gab
gave
dem Ta?ter
to the robber
  seine  his Brieftaschewallet ohnewithout da?that
 das Opfer  the victim Widerstandresistance leistetshows heraus.out.
b. Er
He
nahm
takes
anschlie?end
afterwards
 dem Opfer  the victim diethe Armbanduhrwatch aboff undand der Ta?terthe robber flu?chtete.fleed.
(6) Generated by revision-based system:
a.
 Das Opfer  The victim gibtgave deshalbtherefore
  seine  his Brieftaschewallet ohnewithout Widerstandresistance zuto leistenshow heraus.out.
b. Anschlie?end
Afterwards
nahm
took
der Ta?ter
the robber
 dem Opfer  the victim diethe Armbanduhrwatch aboff undand flu?chtete.fleed.
Figure 3: Two automatically generated outputs for the Sentences (3e-f) in Figure 2.
Joint System BLEU NIST BLEUr
+ 1st pipeline 54.65 11.30 59.95
- 1st pipeline 55.38 11.48 59.52
+ Revision 56.31 11.42 61.30
- Revision 56.42 11.54 60.52
- Parallel+Revision 56.29 11.51 60.63
Table 5: Implicit reference and architectures
5.5 Discussion
The results presented in the preceding evaluations
consistenly show the tight connections between
decisions at the level of reference, syntax and word
order. These interactions entail highly interde-
pendent modelling steps: Although there is a di-
rect error propagation effect from predicted verb
transformation on RE accuracy, predicted syntax
still leads to informative intermediate lineariza-
tions that improve the RE prediction. Our optimal
generation architecture thus has a sequential set-
up, where the first linearization step can be seen
as an intermediate feedback that is revised in the
final linearization. This connects to work in, e.g.
(Hovy, 1988; Robin, 1993).
In Figure 3, we compare two system outputs for
the last two sentences of the text in Figure 2. The
output of the sequential system is severely inco-
herent and would probably be rejected by a hu-
man reader: In sentence (5a) the victim subject of
an active verb is deleted, and the relation between
the possessive and the embedded victim RE is not
clear. In sentence (5b) the first conjunct realizes
a pronominal perp RE and the second conjunct a
nominal perp RE. The output of the revision-based
system reads much more natural. This example
shows that the extension of the REG problem to
texts with more than one main referent (as in the
GREC data set) yields interesting inter-sentential
interactions that affect textual coherence.
We are aware of the fact that our automatic eval-
uation might only partially render certain effects,
especially with respect to textual coherence. It
is likely that the BLEU scores do not capture the
magnitude of the differences in text quality illus-
trated by the Examples (5-6). Ultimately, a hu-
man evaluation for this task is highly desirable.
We leave this for future work since our integrated
set-up rises a number of questions with respect to
evaluation design. In a preliminary analysis, we
noticed the problem that human readers find it dif-
ficult to judge discourse-level properties of a text
like coherence or naturalness when the generation
output is not perfectly grammatical or fluent at the
sentence level.
6 Conclusion
We have presented a data-driven approach for in-
vestigating generation architectures that address
discourse-level reference and sentence-level syn-
tax and word order. The data set we created for our
experiments basically integrates standards from
previous research on REG and surface realization
and extends the annotations to further types of im-
plicit referents. Our results show that interactions
between the different generation levels are best
captured in a sequential, revision-based pipeline
where the REG component has access to predic-
tions from the syntax and the linearization mod-
ule. These empirical findings obtained from ex-
periments with generation architectures have clear
connections to theoretical accounts of textual co-
herence.
Acknowledgements
This work was supported by the Deutsche
Forschungsgemeinschaft (German Research
Foundation) in SFB 732 Incremental Specification
in Context, project D2.
1555
References
Douglas Edmund Appelt. 1982. Planning natural lan-
guage utterances to satisfy multiple goals. Ph.D.
thesis, Stanford, CA, USA.
John Bateman and Michael Zock. 2003. Natural
Language Generation. In Ruslan Mitkov, editor,
The Oxford Handbook of Computational Linguis-
tics. Oxford University Press.
Anja Belz and Eric Kow. 2010. The GREC Challenges
2010: overview and evaluation results. In Proc. of
the 6th International Natural Language Generation
Conference, INLG ?10, pages 219?229, Strouds-
burg, PA, USA.
Anja Belz and Sebastian Varges. 2007. Generation of
repeated references to discourse entities. In Proc. of
the 11th European Workshop on Natural Language
Generation, ENLG ?07, pages 9?16, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Anja Belz, Mike White, Dominic Espinosa, Eric Kow,
Deirdre Hogan, and Amanda Stent. 2011. The first
surface realisation shared task: Overview and evalu-
ation results. In Proc. of the Generation Challenges
Session at the 13th European Workshop on Natu-
ral Language Generation, pages 217?226, Nancy,
France, September. Association for Computational
Linguistics.
Anja Belz. 2005. Statistical generation: Three meth-
ods compared and evaluated. In Proc. of the 10th
European Workshop on Natural Language Genera-
tion, pages 15?23.
Anja Belz. 2008. Automatic generation of
weather forecast texts using comprehensive proba-
bilistic generation-space models. Nat. Lang. Eng.,
14(4):431?455, October.
Bernd Bohnet, Leo Wanner, Simon Milles, and Ali-
cia Burga. 2010. Broad coverage multilingual deep
sentence generation with a stochastic multi-level re-
alizer. In Proc. of the 23rd International Conference
on Computational Linguistics, Beijing, China.
Bernd Bohnet, Simon Mille, Beno??t Favre, and Leo
Wanner. 2011. <stumaba >: From deep repre-
sentation to surface. In Proc. of the Generation
Challenges Session at the 13th European Workshop
on Natural Language Generation, pages 232?235,
Nancy, France, September.
Bernd Bohnet, Anders Bjo?rkelund, Jonas Kuhn, Wolf-
gang Seeker, and Sina Zarriess. 2012. Generating
non-projective word order in statistical linearization.
In Proc. of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 928?
939, Jeju Island, Korea, July.
Bernd Bohnet. 2010. Top accuracy and fast de-
pendency parsing is not a contradiction. In Proc.
of the 23rd International Conference on Computa-
tional Linguistics, pages 89?97, Beijing, China, Au-
gust.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
Treebank. In Proc. of the Workshop on Treebanks
and Linguistic Theories.
Aoife Cahill and Arndt Riester. 2009. Incorporat-
ing Information Status into Generation Ranking. In
Proc. of the 47th Annual Meeting of the ACL, pages
817?825, Suntec, Singapore, August.
Lynne Cahill, Christy Doran, Roger Evans, Chris Mel-
lish, Daniel Paiva, Mike Reape, Donia Scott, and
Neil Tipper. 1999. In search of a reference architec-
ture for nlg systems. In Proc. of the European Work-
shop on Natural Language Generation (EWNLG),
pages 77?85.
David L. Chen, Joohyun Kim, and Raymond J.
Mooney. 2010. Training a multilingual sportscaster:
Using perceptual context to learn language. Journal
of Artificial Intelligence Research, 37:397?435.
Robert Dale and Ehud Reiter. 1995. Computational
interpretations of the gricean maxims in the gener-
ation of referring expressions. Cognitive Science,
19(2):233?263.
Robert Dale. 1989. Cooking up referring expressions.
In Proc. of the 27th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 68?75,
Vancouver, British Columbia, Canada, June.
Laurence Danlos. 1984. Conceptual and linguistic de-
cisions in generation. In Proc. of the 10th Interna-
tional Conference on Computational Linguistics and
22nd Annual Meeting of the Association for Compu-
tational Linguistics, pages 501?504, Stanford, Cali-
fornia, USA, July.
Koenraad De Smedt, Helmut Horacek, and Michael
Zock. 1996. Architectures for natural language gen-
eration: Problems and perspectives. In Trends In
Natural Language Generation: An Artifical Intelli-
gence Perspective, pages 17?46. Springer-Verlag.
Katja Filippova and Michael Strube. 2007. Generating
constituent order in german clauses. In Proc. of the
45th Annual Meeting of the Association for Compu-
tational Linguistics, Prague, Czech Republic.
Matthew Gerber and Joyce Chai. 2010. Beyond nom-
bank: A study of implicit arguments for nominal
predicates. In Proc. of the 48th Annual Meeting
of the Association for Computational Linguistics,
pages 1583?1592, Uppsala, Sweden, July.
Barbara J. Grosz, Aravind Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203?225.
1556
Eduard H. Hovy. 1988. Planning coherent multisen-
tential text. In Proc. of the 26th Annual Meeting
of the Association for Computational Linguistics,
pages 163?169, Buffalo, New York, USA, June.
Thorsten Joachims. 2006. Training linear SVMs
in linear time. In Proc. of the ACM Conference
on Knowledge Discovery and Data Mining (KDD),
pages 217?226.
Irene Langkilde and Kevin Knight. 1998. Gener-
ation that exploits corpus-based statistical knowl-
edge. In Proc. of the 36th Annual Meeting of
the Association for Computational Linguistics and
17th International Conference on Computational
Linguistics, Volume 1, pages 704?710, Montreal,
Quebec, Canada, August. Association for Compu-
tational Linguistics.
Tomasz Marciniak and Michael Strube. 2005. Be-
yond the pipeline: discrete optimization in nlp. In
Proc. of the 9th Conference on Computational Nat-
ural Language Learning, CONLL ?05, pages 136?
143, Stroudsburg, PA, USA.
Chris Mellish, Roger Evans, Lynne Cahill, Christy Do-
ran, Daniel Paiva, Mike Reape, Donia Scott, and
Neil Tipper. 2000. A representation for complex
and evolving data dependencies in generation. In
Proc. of the 6th Conference on Applied Natural Lan-
guage Processing, pages 119?126, Seattle, Wash-
ington, USA, April.
Marie Meteer. 1991. Bridging the generation gap be-
tween text planning and linguistic realization. In
Computational Intelligence, volume 7 (4).
Adwait Ratnaparkhi. 2000. Trainable methods for
surface natural language generation. In Proc. of
the 1st North American chapter of the Association
for Computational Linguistics conference, NAACL
2000, pages 194?201, Stroudsburg, PA, USA.
Ehud Reiter and Robert Dale. 1997. Building applied
natural language generation systems. Nat. Lang.
Eng., 3(1):57?87, March.
Ehud Reiter. 1994. Has a Consensus NL Genera-
tion Architecture Appeared, and is it Psycholinguis-
tically Plausible? pages 163?170.
Eric K. Ringger, Michael Gamon, Robert C. Moore,
David Rojas, Martine Smets, and Simon Corston-
Oliver. 2004. Linguistically Informed Statisti-
cal Models of Constituent Structure for Ordering in
Sentence Realization. In Proc. of the 2004 Inter-
national Conference on Computational Linguistics,
Geneva, Switzerland.
Jacques Robin. 1993. A revision-based generation ar-
chitecture for reporting facts in their historical con-
text. In New Concepts in Natural Language Gener-
ation: Planning, Realization and Systems. Frances
Pinter, London and, pages 238?265. Pinter Publish-
ers.
Michael Roth and Anette Frank. 2012. Aligning predi-
cate argument structures in monolingual comparable
texts: A new corpus for a new task. In Proc. of the
1st Joint Conference on Lexical and Computational
Semantics (*SEM), Montreal, Canada.
Robert Rubinoff. 1992. Integrating text planning and
linguistic choice by annotating linguistic structures.
In Robert Dale, Eduard H. Hovy, Dietmar Ro?sner,
and Oliviero Stock, editors, NLG, volume 587 of
Lecture Notes in Computer Science, pages 45?56.
Springer.
Josef Ruppenhofer, Caroline Sporleder, Roser
Morante, Collin Baker, and Martha Palmer. 2010.
Semeval-2010 task 10: Linking events and their
participants in discourse. In Proc. of the 5th
International Workshop on Semantic Evaluation,
pages 45?50, Uppsala, Sweden, July.
Wolfgang Seeker and Jonas Kuhn. 2012. Making El-
lipses Explicit in Dependency Conversion for a Ger-
man Treebank. In Proc. of the 8th conference on
International Language Resources and Evaluation,
Istanbul, Turkey, May.
Advaith Siddharthan and Ann Copestake. 2004. Gen-
erating referring expressions in open domains. In
Proceedings of the 42nd Meeting of the Association
for Computational Linguistics (ACL?04), Main Vol-
ume, pages 407?414, Barcelona, Spain, July.
Pontus Stenetorp, Sampo Pyysalo, Goran Topic?,
Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 2012. brat: a web-based tool for nlp-assisted text
annotation. In Proc. of the Demonstrations at the
13th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 102?
107, Avignon, France, April.
Leo Wanner, Simon Mille, and Bernd Bohnet. 2012.
Towards a surface realization-oriented corpus anno-
tation. In Proc. of the 7th International Natural Lan-
guage Generation Conference, pages 22?30, Utica,
IL, May.
Leo Wanner. 1994. Building another bridge over the
generation gap. In Proc. of the 7th International
Workshop on Natural Language Generation, INLG
?94, pages 137?144, Stroudsburg, PA, USA.
Sina Zarrie?, Aoife Cahill, and Jonas Kuhn. 2011. Un-
derspecifying and predicting voice for surface real-
isation ranking. In Proc. of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1007?
1017, Portland, Oregon, USA, June.
Sina Zarrie?, Aoife Cahill, and Jonas Kuhn. 2012.
To what extent does sentence-internal realisation re-
flect discourse context? a study on word order. In
Proc. of the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 767?776, Avignon, France, April.
1557
Proceedings of the 2010 Workshop on NLP and Linguistics: Finding the Common Ground, ACL 2010, pages 34?42,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
A Cross-Lingual Induction Technique for German Adverbial Participles
Sina Zarrie? Aoife Cahill Jonas Kuhn Christian Rohrer
Institut fu?r Maschinelle Sprachverarbeitung (IMS)
University of Stuttgart
Stuttgart, Germany
{zarriesa,cahillae,jonas.kuhn,rohrer}@ims.uni-stuttgart.de
Abstract
We provide a detailed comparison of
strategies for implementing medium-to-
low frequency phenomena such as Ger-
man adverbial participles in a broad-
coverage, rule-based parsing system. We
show that allowing for general adverb con-
version of participles in the German LFG
grammar seriously affects its overall per-
formance, due to increased spurious am-
biguity. As a solution, we present a
corpus-based cross-lingual induction tech-
nique that detects adverbially used par-
ticiples in parallel text. In a grammar-
based evaluation, we show that the auto-
matically induced resource appropriately
restricts the adverb conversion to a limited
class of participles, and improves parsing
quantitatively as well as qualitatively.
1 Introduction
In German, past perfect participles are ambigu-
ous with respect to their morphosyntactic cate-
gory. As in other languages, they can be used
as part of the verbal complex (example (1-a)) or
as adjectives (example (1-b)). Since German ad-
jectives can generally undergo conversion into ad-
verbs, participles can also be used adverbially (ex-
ample (1-c)). All three participle forms in (1) are
morphologically identical.
(1) a. Das Experiment hat ihn begeistert.
?The experiment has enthused him.?
b. Er scheint von dem Experiment begeistert.
?He seems enthusiastic about the experiment.?
c. Er hat begeistert experimentiert.
?He has experimented in an enthusiastic way? or:
?He was enthusiastic when he experimented.?
This paper adresses the question of how to deal
with medium-to-low frequency phenomena such
as adverbial participles in a broad-coverage, rule-
based parsing system. In order to account for sen-
tences like (1-c), an intuitive approach would be to
generally allow for adverb conversion of partici-
ples in the grammar. However, on the basis of the
German LFG grammar (Rohrer and Forst, 2006),
we show that such a rule can have a strong negative
on the overall performance of the parsing system,
despite the fact that it produces the desired syntac-
tic and semantic analysis for specific sentences.
This trade-off between large-scale, statistical
and theoretically precise coverage is often en-
countered in engineering broad-coverage and, at
the same time, linguistically motivated parsing
systems: adding the analysis for a specific phe-
nomenon does not necessarily improve the overall
quality of the system since the rule might overgen-
erate and interact with completely different phe-
nomena in unpredicted ways.
In principle, there are two ways of dealing with
such an overgeneration problem in a grammar-
based framework: First, one could hand-craft
word lists or other linguistic constraints that re-
strict the adverb conversion to a certain set of par-
ticiples. Second, one could try to mine corpora for
this particular type of adverbs and integrate this
automatically induced knowledge into the gram-
mar (i.e. by means of pre-tagged input, word lists,
etc.). In the case of adverbial participles, both
ways are prone with difficulties. To our knowl-
edge, there has not been much theoretical work on
the linguistic properties of the participle adverb
conversion. Moreover, since the distinction be-
tween (predicative) adjectives and adverbs is the-
oretically hard to establish, the standard tag set
for German and, in consequence, annotated cor-
pora for German do not explicitly capture this phe-
nomenon. Thus, available statistical taggers and
parsers for German usually conflate the syntactic
structures underlying (1-b) and (1-c).
In this paper, we present a corpus-based ap-
proach to restricting the overgenerating adverb
conversion for participles in German, exploiting
34
parallel corpora and cross-lingual NLP induc-
tion techniques. Since adverbs are often overtly
marked in other languages (i.e. the ly-suffix in
English), adverbial participles can be straightfor-
wadly detected on word-aligned parallel text. We
describe the ingretation of the automatically in-
duced resource of adverbial participles into the
German LFG, and provide a detailed evaluation of
its effect on the grammar, see Section 5.
While the use of parallel resources is rather
familiar in a wide range of NLP domains, such
as statistical machine translation (Koehn, 2005)
or annotation projection (Yarowsky et al, 2001),
our work shows that they can be exploited for
very specific problems that arise in deep linguis-
tic analysis (see Section 4). In this way, high-
precision, data-oriented induction techniques can
clearly improve rule-based system development
through combining the benefits of high empirical
accuracy and little manual effort.
2 A Broad-Coverage LFG for German
Lexical Functional Grammar (LFG) (Bresnan,
2000) is a constraint-based theory of grammar. It
posits two levels of representation, c(onstituent)-
structure and f(unctional)- structure. C-structure
is represented by contextfree phrase-structure
trees, and captures surface grammatical configu-
rations. F-structures approximate basic predicate-
argument and adjunct structures.
The experiments reported in this paper use the
German LFG grammar constructed as part of the
ParGram project (Butt et al, 2002). The grammar
is implemented in the XLE, a grammar develop-
ment environment which includes a very efficient
LFG parser. Within the spectrum of appraoches
to natural language parsing, XLE can be consid-
ered a hybrid system combining a hand-crafted
grammar with a number of automatic ambiguity
management techniques: (i) c-structure pruning
where, based on information from statstically ob-
tained parses, some trees are ruled out before f-
structure unification (Cahill et al, 2007), (ii) an
Optimaly Theory-style constraint mechanism for
filtering and ranking competing analyses (Frank
et al, 2001), and (iii) a stochastic disambiguation
component which is based on a log-linear proba-
bility model (Riezler et al, 2002) and works on
the packed representations.
The German LFG grammar integrates a mor-
phological component which is a variant of
DMOR1 (Becker, 2001). This means that the (in-
ternal) lexicon does not comprise entries for sur-
face word forms, but entries for specific morpho-
logical tags, see (Dipper, 2003).
3 Participles in the German LFG
3.1 Analysis
The morphosyntactic ambiguity of German par-
ticiples presents a notorious difficulty for theoreti-
cal and computational analysis. The reason is that
adjectives (i.e. adjectival participles) do not only
occur as attributive modifiers (shown in (1-a)), but
can also be used as predicatives (see (2-b)). These
predicatives have exactly the same form as ver-
bal or adverbial participles (compare the three sen-
tences in (2)). Predicatives do appear either as ar-
guments of verbs like seem or as free adjuncts such
that they are not even syntactically distinguishable
from adverbs. The sentence in (2-c) is thus am-
biguous as to whether the participle is an adverb
modifying the main verb, or a predicative which
modifies the subject. Especially in the case of
modifiers refering to a psychological state, the two
underlying readings are hard to tell apart (Geuder,
2004). It is due to the lack of reliable semantic
tests that the standard German tag set (Schiller et
al., 1995) assigns the tag ?ADJD? to predicative
adjectives as well as adverbs.
(2) a. Das Experiment hat ihn begeistert.
?The experiment has enthused him.?
b. Er scheint von dem Experiment begeistert.
?He seems enthusiastic about the experiment.?
c. Er hat begeistert experimentiert.
?He has experimented in an enthusiastic way? or:
?He was enthusiastic when he experimented.?
For performance reasons, the German LFG does
not cover free predicatives at the moment. In the
context of our crosslingual induction approach,
the distinction between predicatives and adverbs
is rather straigtforward since we base our experi-
ments on languages that have morphologically dis-
tinct forms for these categories. In the follow-
ing, we will thus limit the discussion to adverbial
participles and ignore the complexities related to
predicative participles.
In the German LFG, the treatment of a given
participle form is closely tight to the morphologi-
cal analysis encoded in DMOR. In particular, ad-
verbial participles can have different degrees of
lexicalisation. For bestimmt (probably) in (3-a),
which is completely lexicalised, the morphology
35
proposes two analyses: (i) a participle tag of the
verbal lemma bestimmen (determine) and (ii) an
adverb tag for the lemma bestimmt. In this case,
the LFG parsing algorithm will figure out which
morphological analysis yields a syntactically well-
formed analysis. For gezielt (purposeful) in (3-b),
DMOR outputs, besides the participle analysis, an
adjective tag for the lemma. However, the gram-
mar can turn it into an adverb by a general ad-
verb conversion rule for adjectives. The difficult
case for the German LFG grammar is illustrated in
(3-c) by means of the adverbial participle wieder-
holt (repeatedly). This participle is neither lexi-
calised as an adverb nor as an adjective, but it still
can be used as an adverb.
(3) a. Bestimmt
Probably
ist
is
dieser
the
Mann
man
sehr
very
traurig.
sad.
b. Der
The
Mann
man
hat
has
gezielt
acted
gehandelt.
purposefully.
c. Der
The
Mann
man
hat
has
wiederholt
repeatedly
geweint.
cried.
To cover sentences like (3-c), the grammar
needs to include a rule that allows adverb conver-
sion for participles. Unfortunately, this rule is very
costly in terms of the overall performance of the
grammar, as is shown in the following section.
3.2 Assessing the Effect of Participle
Ambiguity on the German LFG
In this section, we want to illustrate the effect of
one specific grammar rule, i.e. the rule that gener-
ally allows for conversion of participles into ad-
verbs. We perform a contrastive evaluation of
two versions of the grammar: (i) the No-Part-Adv
version which does not allow for adverb conver-
sion (except for the lexicalised participles from
DMOR), (ii) the All-Part-Adv version which al-
lows every participle to be analysed as adverb.
Otherwise, the two versions of the grammar are
completely identical.
The comparison between the All-Part-Adv and
No-Part-Adv grammar version pursues two major
goals: On the one hand, we want to assess their
overall quantitative performance on representative
gold standard data, as it is common practice for
statistical parsing systems. On the other hand, we
are interested in getting a detailed picture of the
quality of the grammar for parsing adverbial par-
ticiples. These two goals do not necessarily go to-
gether since we know that the phenomenon is not
very frequent in the data which we use for evalu-
ation. Therefore, we do not only report accuracy
on gold standard data in the following, but also fo-
cus on error analysis and describe ways of qualti-
tatively assessing the grammar performance.
For evaluation, we use the TIGER treebank
(Brants et al, 2002). We report grammar per-
formance on the development set which consists
of the first 5000 TIGER sentences, and statistical
accuracy on the standard heldout set which com-
prises 371 sentences.
Quantitative Evaluation We first want to assess
the quantitative impact of the phenomenon of ad-
verbial participles in our evaluation data. We parse
the heldout set storing all possible analyses ob-
tained by both grammars, in order to compare the
upperbound score that the both versions can op-
timally achieve (i.e. independently of the disam-
biguation quality). Then, we run the XLE eval-
uation in the ?oracle? mode which means that the
disambiguation compares all system analyses for a
given sentence to its gold analysis, and chooses the
best system analysis for computing accuracy. The
upperbound f-score for both grammar versions is
almost identical (at about 83.6%). This suggests
that the phenomenon of adverbial participles does
not occur in the heldout set.
If we run the grammar versions on a larger
set of sentences, the difference in coverage be-
comes more obvious. In Table 1, we report the
absolute number of parsed sentences, starred sen-
tences (only receiving a partial or fragment parse),
and the timeouts 1 on our standard TIGER devel-
opment set. Not very surprisingly, the coverage
of the All-Part-Adv version seems to be broader.
However, this does not necessarily mean that the
40 additionally covered sentences all exhibit ad-
verbial participles (see below). Moreover, Table 2
gives a first indication of the fact that the extended
coverage comes at a price: the All-Part-Adv ver-
sion massively increases the number of ambigui-
ties per sentence. Related to this, in the All-Part-
Adv version, the number of timeouts increases by
16% and parsing speed goes down by 6% com-
pared to the No-Part-Adv version.
To assess the effect of the massively increased
ambiguity rate and the bigger proportion of time-
outs in All-Part-Adv, we perform a statistical eval-
uation of the two versions of the grammar against
the heldout set, i.e. we compute f-score based
1Sentences whose parsing can not be finished in prede-
fined amount of time, the maximally allowed parse time is
set to 20 seconds.
36
Grammar Parsed
Sent.
Starred
Sent.
Time-
outs
Time
in sec
No-Part-Adv 4301 608 90 6853
All-Part-Adv 4339 555 105 7265
Table 1: Coverage-based evaluation on the TIGER
development set (sentences 1-5000), 4999 sen-
tences total
Sent. Av. ambiguities per sent. Av.
length No-Part-Adv All-Part-Adv Incr.
1-10 2.95 3.3 11%
11-20 24.99 36.09 44%
21-30 250.4 343.76 37%
31-40 1929.06 2972.847 54%
41-50 173970.0 663310.4 429%
Table 2: Average number of ambiguities per sen-
tence
on the parses that the XLE disambiguation selects
as the most probable parse. Both versions use
the same disambiguation model which results in
a slightly biased comparison but still reflects the
effect of increased ambiguity on the disambigua-
tion component. In Table 3, we can see that the
All-Part-Adv version performs significantly worse
than the grammar version which does not cap-
ture adverbial participles. The spurious ambigu-
ities and timeouts produced in All-Part-Adv have
such a strong negative impact on the disambigua-
tion component that it can not be outweighed by
the extended coverage of the grammar.
Qualitative Evaluation The fact that the All-
Part-Adv version generally increases parse ambi-
guity suggests that it produces a lot of undesired
analyses for constructions not related to adverbial
participles. To assess this assumption, we drew a
random sample of 20 sentences out of the addi-
tionally covered 41 sentences and checked manu-
ally whether these contained an adverbial partici-
ple: Only 40% of these sentences are actually cor-
rectly analysed. In all other cases, the grammar
lacks an analysis for a completely different phe-
Grammar Prec. Rec. F-Sc. Time
in sec
All-Part-Adv 83.80 76.71 80.1 666.55
No-Part-Adv 84.25 78.3 81.17 632.21
Table 3: Evaluation on the TIGER heldout set, 371
sentences total
nomenon (mostly related to coordination), but ob-
tains an (incorrect) analysis on the basis of the ad-
verb conversion rule.
As an example, Figure 1 presents two c-
structure analyses for the sentence in (4) in the
All-Part-Adv grammar. In the second c-structure
(CS2), the participle kritisiert (criticised) is anal-
ysed as adverb modifing the main verb haben
(have). This results in a very strange underlying f-
structure, meaning something like the Greens pos-
sess the SPD in a criticising manner.
(4) Die
The
Gru?nen
Greens
haben
have
die
the
SPD
SPD
kritisiert.
criticised.
?The Greens have criticised the SPD?
3.3 Interim Conclusion
This section has illustrated an exemplary dilemma
for parsing systems that aim broad-coverage and
linguisitically motivated analyses at the same time.
Since these systems need to explicitly address and
represent ambiguities that purely statistical sys-
tems are able to conflate or ignore, their perfor-
mance is not automatically improved by adding
a specific rule for a specific phenomenon. Inter-
estingly, the negative consequences affecting the
quantitative (statistical) as well as the qualitative
(linguistic) dimension of the grammar seem to be
closely related: The overgenerating adverb con-
version rule empirically leads to linguistically un-
motivated analyses which causes problems for the
disambiguation component. In the rest of the pa-
per, we show how the adverbial analysis of partici-
ples can be reasonably constrained on the basis of
a lexical resource induced from a parallel corpus.
4 Cross-Lingual Induction of Adverbial
Participles
The intuition of the cross-lingual induction ap-
proach is that adverbial participles can be easily
extracted from parallel corpora since in other lan-
guages (such as English or French) adverbs are
often morphologically marked and easily labelled
by statistical PoS taggers. As an example, con-
sider the sentence in (5), extracted from Europarl,
where the German participle versta?rkt is translated
by unambiguous adverbs in English and French
(increasingly and davantage).
(5) a. Nach der Osterweiterung stehen die Zeichen
versta?rkt auf Liberalisierung.
b. Following enlargement towards the east, the emphasis
is increasingly on liberalisation.
37
CS 1: ROOT:2543
CProot[std]:2536
DP[std]:984
DPx[std]:981
D[std]:616
die:34
NP:773
N[comm]:717
NAdj:714
Gr?nen:85
Cbar:2506
Vaux[haben,fin]:1054
haben:159
VP[v,part]:2080
DP[std]:1856
DPx[std]:2321
D[std]:1180
die:204
NP:1720
N[comm]:284
SPD:257
VC[v,part]:2009
V[v,part]:1593
Vx[v,part]:1590
kritisiert:348
PERIOD:418
.:410
CS 2: ROOT:2543
CProot[std]:2536
DP[std]:984
DPx[std]:981
D[std]:616
die:34
NP:773
N[comm]:717
NAdj:714
Gr?nen:85
Cbar:2506
V[v,fin]:2494
Vx[v,fin]:2491
haben:159
DP[std]:1856
DPx[std]:2321
D[std]:1180
die:204
NP:1720
N[comm]:284
SPD:257
ADVP[std]:1493
V[v,-infl]:1491
Vx[v,-infl]:1488
kritisiert:348
PERIOD:418
.:410
Figure 1: Two c-structures for sentence (4), obtained by the grammar All-Part-Adv - CS1 is correct, CS2
is semantically very strange
c. Apre`s l? e?largissement a` l? Est, la tendance sera da-
vantage a` la libe?ralisation.
In the following, we describe experiments on
Europarl where we automatically extract and fil-
ter adverbially translated German participles.
4.1 Data
We base our experiments on the German, En-
glish, French and Dutch part of the Europarl cor-
pus. We automatically word-aligned the German
part to each of the others with the GIZA++ tool
(Och and Ney, 2003). Note that, due to diver-
gences in sentence alignment and tokenisation,
the three word-alignments are not completely syn-
chronised. Moreover, each of the 4 languages has
been automatically PoS tagged using the TreeTag-
ger (Schmid, 1994). In addition, the German and
English parts have been parsed with MaltParser
(Nivre et al, 2006).
Since we want to limit our investigation to those
participles that are not already recorded as lexi-
calised adjective or adverb in the DMOR morphol-
ogy, we first have to generate the set of participle
candidates from the tagged Europarl data. We ex-
tract all distinct words (types) from the German
part that have been either tagged as ADJD (pred-
icative or adverbial modifier), 6089 types in total,
or as VVPP (past perfect participle), 5469 types
in total. We intersect this set of potential partici-
ples with the set of DMOR participles that only
have a verbal lemma. The resulting intersection
(5054 types in total) constitutes the set of all Ger-
man participles in Europarl that are not recorded
as lexicalised in the DMOR morphology .
Given the participle candidates, we now ex-
tract the set of sentences that exhibit a word
alignment between a German participle and an
English, French or Dutch adverb. The extrac-
tion yields 5191 German-English sentence pairs,
2570 German-French, and 4129 German-Dutch
sentence pairs. The German-English pairs com-
prise 1070 types of potentially adverbial partici-
ples. The types found in the German-French and
German-Dutch part form a proper subset of the
types extracted from the German-English pairs.
Thus, the additional languages will not increase
the recall of the induction. However, we will show
that they are extremely useful for filtering incor-
rect or uninteresting participle alignments.
For data exploration and evaluation, we anno-
tated 300 participle alignments out of the 5191
German-English sentences as to whether the En-
glish adverbial really points to an adverbial par-
ticiple on the German side (and/or the word-
alignment was correct). Throughout the entire set
of annotated sentences, this ratio between the par-
allel cases (where an English adverbial correctly
indicates a German adverbial) and all adverbially
translated participles is at about 30%. This means
that if we base the induction on word-alignments
alone, its precision would be relatively low.
The remaining 60% translation pairs do not only
reflect word alignment errors, but also cases where
we find a proper participle in the German sentence
that has a correct adverbial translation for other
reasons. A typical configuration is exemplified in
(6) where the German main verb vorlegen is trans-
lated as the verb-adverb combination put forward.
(6) a. Wir haben eine Reihe von Vorschla?gen vorgelegt.
b. We have put forward a number of proposals.
These sentence pairs are cases of free or para-
38
Figure 2: Type/token ratio for adverbial participles
phrasing translations. Ideally, we want our induc-
tion method to filter such type of configurations.
The 300 annotated sentences comprise 121 to-
ken instances of German adverbially used partici-
ples that have an adverbial translation in English.
However, these 121 tokens reduce to 24 partici-
ple types. The graph in Figure 2 displays the
type/token-ratio for an increasing number of in-
stances in our gold standard. The curve exponen-
tially decays from about 10 tokens onward and
suggests that from about 30 tokens onward, the
number of unseen types is relatively low. This can
be interpreted as evidence in favour of the hypoth-
esis that the number of adverbially used participles
is actually fairly limited and can be integrated into
the grammar in terms of a hard-coded resource.
4.2 Filtering
The data analysis in the previous section has
shown that approximately one third of the English
adverb alignments actually point to an adverbial
participle on the German side. This means that we
have to rigorously filter the data that we extract on
the basis of word-alignments in order to obtain a
high quality resource for our grammar. In this sec-
tion, we will investigate several filtering methods
and evaluate them on our annotated sentence pairs.
Frequency-based filtering As a first attempt,
we filtered the non-parallel cases in our set of
participle-adverb translations by means of the rel-
ative frequency of the adverb translations. For
each participle candidate, we counted the number
of tokens that exhibit an adverbial alignment on
the English side, and divided this number by its
total number of occurrences in the German Eu-
roparl. The best f-score of the ADV-FREQ filter
(see Table 4) is achieved by the 0.05 threshold, but
generally, the precision of the frequency filters is
too low for high-quality resource induction. The
reason for the poor performance of the frequency-
based filters seems to be that some German verbs
are systematically translated as verb - adverb com-
binations as in (6). For these participles, the rel-
ative frequency of adverbial alignments is not a
good indicator for their adverbial use in German.
Multilingual Filtering Similar to filters used
in annotation projection where noisy word-
alignments are ?cleaned? with the help of addi-
tional languages (Bouma et al, 2008), we have
implemented a filter that only selects those par-
ticiples as adverbials which also exhibit a certain
amount of adverbial translations in the French and
Dutch Europarl. We count the total number of
adverbial translations of a given participle on the
French side and divide it by the number of English
adverbial translations. For French, the best f-score
is achieved at a threshold of >0.1 (filter FR). For
Dutch, the best f-score is achieved at a threshold
of >0.05 (filter NL). The exact precision and re-
call values are given in Table 4.
Syntax-based Filtering The intuition behind
the filters presented in this section is that adver-
bial translations which are due to cross-lingual di-
vergences can be identified on the basis of their
syntactic contexts. Information about these con-
texts can be extracted from the dependency anal-
yses produced by MaltParser for the German and
English data. On the German side, we want to ex-
clude those participle instances for which the Ger-
man parser has found an auxiliary head, since this
configuration points to a normal partciple context
in German. The filter is called G-HEAD in Table
4. It filters all types which have an auxiliary head
in more than 40% of their adverbial translation
configurations. On the English side, we exclude
all translations where the adverb has a verbal head
which is also aligned to the German partciple. The
filter is called E-HEAD in Table 4. It excludes all
participle types which exhibit the E-HEAD con-
figuration in more than 50% of the cases.
39
filter prec. rec. f-sc.
ADV-FREQ 0.38 0.75 0.51
FR 0.48 0.76 0.58
NL 0.33 0.73 0.45
G-HEAD 0.65 0.8 0.71
E-HEAD 0.4 0.8 0.53
COMBINED-1 0.61 0.8 0.69
COMBINED-2 0.86 0.76 0.81
Table 4: Performance of filters on the set of gold
adverbial participle types
Combined Token-level Filtering So far, we
have shown that multilingual and syntactic in-
formation is useful to filter non-parallel partici-
ple translations. We have found that the pre-
cision of the syntactic filters can still be in-
creased by combining it with the multilingual fil-
ters. COMBINED-1 in Table 4 refers to the filter
which only includes those participle types which
have at least one adverbial translation on the En-
glish target side such that (i) the adverbial trans-
lation is paralleled on the French or Dutch target
side for the same German participle token and (ii)
the German participle token does not have an aux-
iliary head. If we combine this token-level filter-
ing with the syntactic type-level filtering G-HEAD
and E-HEAD (the filter called COMBINED-2 in
Table 4), the precision increases by about 25%
with little loss in recall.
4.3 Analysis
Based on the filtering techniques described in the
previous section, we can finally induce a list of 46
German adverbial participles from Europarl. The
fact that this participle class seems fairly delimited
in our data raises the theoretical question whether
the adverb conversion is licensed by any linguistic,
i.e. lexical-semantic, properties of these partici-
ples. However, we observe that the automatically
induced list comprises very diverse types of ad-
verbs, as well as very distinct types of underlying
verbs. Thus, besides adverbs that clearly modify
events (see sentence (5)), we also found adverbs
that are more likely to modify adjectives (sentence
(7-a)), or propositions (sentence (7-b)).
(7) a. Es ist eine verdammt gefa?hrliche Situation.
?It is a damned dangerous situation.?
b. Wir machen einen Bericht u?ber den Bericht des Rech-
nungshofes , zugegeben.
?We are drafting a report about the report of the Court
of Auditors , admittedly.?
A more fine-grained classification and analysis
of adverbial participles is left for future research.
5 Grammar-based Evaluation
The resource of participles licensing adverbial use,
whose induction was described in the previous
section, can be straightforwardly integrated into
the German LFG. By explicitly enumerating the
participles in the adverb lexicon, the grammar can
apply the standard adverb macros to them. To as-
sess the effect of the filtering, we built two new
versions of the grammar: (i) Euro-Part-Adv, its ad-
verb lexicon comprises all adverbially translated
participles found in Europarl (1091 types) and (ii)
Filt-Part-Adv, its adverb lexicon comprises only
the syntactically and multilingually filtered par-
ticiples found in Europarl (46 types).
Although we have seen in section 3.2 that adver-
bial participles do not seem to occur in the TIGER
heldout set, we also know that it is important to
assess the effect of ambiguity rate on the overall
grammar performance. Therefore, we computed
the accuracy of the most probable parses produced
by the Euro-Part-Adv and Filt-Part-Adv on the
heldout set. As is shown in Table 5, the Euro-Part-
Adv performs significantly worse than Filt-Part-
Adv. This suggests that the non-filtered participle
resource is not constrained enough and still pro-
duces a lot of spurious ambiguites that mislead the
disambiguation component. The coverage values
in Table 6 further corroborate the observation that
the unfiltered participle resource behaves similar
to the unrestricted adverb conversion in All-Part-
Adv (see Section 3.2). The coverage of the filtered
vs. the unfiltered version on the development set is
identical, however the timeouts in Euro-Part-Adv
increase by 17% and parsing time by 8%.
By contrast, there is no significant difference
in f-score between the No-Part-Adv version pre-
sented in Section 3.2 and the Filt-Part-Adv ver-
sion. Thus, we can, at least, assume that the fil-
tered participles resources has restricted the mas-
sive overgeneration caused by the general adverb
conversion rule such that the overall performance
of the original grammar is not negatively affected.
To evaluate the participle resource as to whether
it could have a positive qualtitative effect on pars-
ing TIGER at all, we built a specialised test-
suite which comprises only sentences containing
a non-lexicalised participle, which has an adver-
bial translation in Europarl and is tagged as ADJD
40
Grammar Prec. Rec. F-Sc. Time
in sec
Euro-Part-Adv 82.32 75.78 78.91 701
Filt-Part-Adv 84.12 78.2 81.05 665
Table 5: Evaluation on the TIGER heldout set, 371
sentences total
Grammar Parsed
Sent.
Starred
Sent.
Time-
outs
Time
in sec
Euro-Part-Adv 4304 588 107 7359
Filt-Part-Adv 4304 604 91 6791
Table 6: Performance on the TIGER development
set (sentences 1-5000), 4999 sentences total
in TIGER. The sentences were extracted from the
whole TIGER corpus yielding a set of 139 sen-
tences. In this quality-oriented evaluation, we
only contrast the No-Part-Adv version with the
filtered Filt-Part-Adv version since the unfiltered
version leads to worse overall performance. As
can be seen in Table 7, the No-Part-Adv can only
completely cover 36% of the specialised testsuite
which is much lower than its average complete
coverage on the development set (86%). This sug-
gests that a substantial number of the extracted
ADJD participles are actually used as adverbial in
the specialised testsuite.
Similar to the qualitative evaluation procedure
in 3.2, we manually evaluated a random sample of
20 sentences covered by Filt-Part-Adv and not by
No-Part-Adv as to whether they contain an adver-
bial participle that has been correctly recognised.
This was the case for 90% of the sentences, the
remaining 2 sentences were cases of secondary
predications. An example of a relatively simple
TIGER sentence that the grammar could not cover
in the No-Part-Adv version is given in (8).
(8) Die Anti-Baby-Pillen stehen im Verdacht , vermehrt
Thrombosen auszulo?sen.
?The birth control pill is suspected to increasingly cause
thromboses.?
We also manually checked a random sample of
Grammar Parsed
Sent.
Starred
Sent.
Time-
outs
Time
in sec
No-Part-Adv 50 77 12 427
Filt-Part-Adv 92 39 8 366
Table 7: Performance on the specialised TIGER
test set, 139 sentences total
20 sentences that the Filt-Part-Adv grammar could
not cover, in order to see whether the grammar sys-
tematically misses certain cases of adverbial par-
ticiples. In this second random sample, the per-
centage of sentences containing a true adverbial
participle was again 90%. The grammar could
not correctly analyse these because of their spe-
cial syntax that is not covered by the general ad-
verb macro (or, of course, because of difficult con-
structions not related to adverbial participles). An
example for such a case is given in (9).
(9) Transitreisen junger Ma?nner vom Gaza-Streifen ins
Westjordanland und umgekehrt sind nicht gestattet.
?Transit travels from the Gaza Strip to the West Bank and
vice versa are not allowed for young men.?
The high proportion of true adverbial participle
instances in our specific testsuite suggests that the
data we induced from Europarl largely carries over
to TIGER (despite genre differences, for instance)
and constitutes a generally useful resource. Thus,
we can not only say that the filtered participle re-
source has no negative effect on the overall per-
formance of the German LFG, but also extends its
coverage for a less frequent phenomenon in a lin-
guistically precise way.
6 Conclusion
We have proposed an empirical account for detect-
ing adverbial participles in German. Since this
category is usually not annotated in German re-
sources and hard to describe in theory, we based
our method on multilingual parallel data. This
data suggests that only a fairly limited class of par-
ticiples actually undergo the conversion to adverbs
in free text. We have described a set of linguisti-
cally motivated filters which are necessary to in-
duce a high-precision resource for adverbial par-
ticiples from parallel data. This resource has been
integrated into the German LFG grammar. In con-
trast to the version of the grammar which does not
restrict the participle - adverb conversion, the re-
stricted version produces less spurious ambigui-
ties which leads to better f-score on gold standard
data. Moreover, by manually evaluating a spe-
cialised data set, we have established that the re-
stricted version also extends the coverage and pro-
duces the correct analyses which can be used for
further linguistic study.
41
References
Tanja Becker. 2001. DMOR: Handbuch. Technical
report, IMS, University of Stuttgart.
Gerlof Bouma, Jonas Kuhn, Bettina Schrader, and
Kathrin Spreyer. 2008. Parallel LFG Grammars
on Parallel Corpora: A Base for Practical Trian-
gulation. In Miriam Butt and Tracy Holloway
King, editors, Proceedings of the LFG08 Confer-
ence, pages 169?189, Sydney, Australia. CSLI Pub-
lications, Stanford.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The tiger
treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories.
Joan Bresnan. 2000. Lexical-Functional Syntax.
Blackwell, Oxford.
Miriam Butt, Helge Dyvik, Tracy Holloway King, Hi-
roshi Masuichi, and Christian Rohrer. 2002. The
Parallel Grammar Project.
Aoife Cahill, John T. Maxwell III, Paul Meurer, Chris-
tian Rohrer, and Victoria Rose?n. 2007. Speeding
up LFG Parsing using C-Structure Pruning . In Col-
ing 2008: Proceedings of the workshop on Grammar
Engineering Across Frameworks, pages 33 ? 40.
Stefanie Dipper. 2003. Implementing and Document-
ing Large-Scale Grammars ? German LFG. Ph.D.
thesis, Universita?t Stuttgart, IMS.
Anette Frank, Tracy Holloway King, Jonas Kuhn, and
John T. Maxwell. 2001. Optimality Theory Style
Constraint Ranking in Large-Scale LFG Grammars
. In Peter Sells, editor, Formal and Empirical Issues
in Optimality Theoretic Syntax, page 367?397. CSLI
Publications.
Wilhelm Geuder. 2004. Depictives and transparent ad-
verbs. In J. R. Austin, S. Engelbrecht, and G. Rauh,
editors, Adverbials. The Interplay of Meaning, Con-
text, and Syntactic Structure, pages 131?166. Ben-
jamins.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit 2005.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
Maltparser: A data driven parser-generator for de-
pendency parsing. In Proc. of LREC-2006.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Stefan Riezler, Tracy Holloway King, Ronald M. Ka-
plan, Richard Crouch, John T. Maxwell, and Mark
Johnson. 2002. Parsing the Wall Street Journal us-
ing a Lexical-Functional Grammar and Discrimina-
tive Estimation Techniques . In Proceedings of ACL
2002.
Christian Rohrer and Martin Forst. 2006. Improving
coverage and parsing quality of a large-scale LFG
for German. In Proceedings of LREC-2006.
Anne Schiller, Simone Teufel, and Christine Thielen.
1995. Guidelines fuer das Tagging deutscher Tex-
tkorpora mit STTS. Technical report, IMS, Univer-
sity of Stuttgart.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
International Conference on New Methods in Lan-
guage Processing.
David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analysis
tools via robust projection across aligned corpora. In
Proceedings of HLT 2001, First International Con-
ference on Human Language Technology Research.
42
Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 62?67,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
LFG-based Features for Noun Number and Article Grammatical Errors
Ga?bor Berend1, Veronika Vincze2, Sina Zarriess3, Richa?rd Farkas1
1University of Szeged
Department of Informatics
{berendg,rfarkas}@inf.u-szeged.hu
2Research Group on Artificial Intelligence
Hungarian Academy of Sciences
vinczev@inf.u-szeged.hu
3University of Stuttgart
Institute for Natural Language Processing
zarriesa@ims.uni-stuttgart.de
Abstract
We introduce here a participating system
of the CoNLL-2013 Shared Task ?Gram-
matical Error Correction?. We focused on
the noun number and article error cate-
gories and constructed a supervised learn-
ing system for solving these tasks. We car-
ried out feature engineering and we found
that (among others) the f-structure of an
LFG parser can provide very informative
features for the machine learning system.
1 Introduction
The CoNLL-2013 Shared Task aimed at identify-
ing and correcting grammatical errors in the NU-
CLE learner corpus of English (Dahlmeier et al,
2013). This task has become popular in the natural
language processing (NLP) community in the last
few years (Dale and Kilgariff, 2010), which mani-
fested in the organization of shared tasks. In 2011,
the task Helping Our Own (HOO 2011) was held
(Dale and Kilgariff, 2011), which targeted the pro-
motion of NLP tools and techniques in improving
the textual quality of papers written by non-native
speakers of English within the field of NLP. The
next year, HOO 2012 (Dale et al, 2012) specifi-
cally focused on the correction of determiner and
preposition errors in a collection of essays writ-
ten by candidates sitting for the Cambridge ESOL
First Certificate in English (FCE) examination. In
2013, the CoNLL-2013 Shared Task has continued
this direction of research.
The CoNLL-2013 Shared Task is based on the
NUCLE corpus, which consists of about 1,400
student essays from undergraduate university stu-
dents at The National University of Singapore
(Dahlmeier et al, 2013). The corpus contains over
one million words and it is completely annotated
with grammatical errors and corrections. Among
the 28 error categories, this year?s shared task fo-
cused on the automatic detection and correction of
five specific error categories.
In this paper, we introduce our contribution of
the CoNLL-2013 Shared Task. We propose a su-
pervised learning-based approach. The main con-
tribution of this work is the exploration of several
feature templates for grammatical error categories.
We focused on the two ?nominal? error categories:
1.1 Article and Determiner Errors
This error type involved all kinds of errors
which were related to determiners and articles
(ArtOrDet). It required multiple correction
strategies. On the one hand, superfluous articles
or determiners should be deleted from the text.
On the other hand, missing articles or determin-
ers should be inserted and at the same time it was
sometimes also necessary to replace a certain type
of article or determiner to an other type. Here is
an example:
For nations like Iran and North Ko-
rea, the development of nuclear power
is mainly determined by the political
forces. ? For nations like Iran and
North Korea, the development of nu-
clear power is mainly determined by po-
litical forces.
62
1.2 Wrong Number of the Noun
The wrong number of nouns (Nn) meant that either
a singular noun should occur in the plural form or
a plural noun should occur in the singular form.
A special case of such errors was that sometimes
uncountable nouns were used in the plural, which
is ungrammatical. The correction involved here
the change of the number. Below we provide an
example:
All these measures are implemented to
meet the safety expectation of the op-
eration of nuclear power plant. ? All
these measures are implemented to meet
the safety expectation of the operation
of nuclear power plants.
2 System Description
Our approach for grammatical error detection was
to construct supervised classifiers for each candi-
date of grammatical error locations. In general,
our candidate extraction and features are based
on the output of the preprocessing step provided
by the organizers which contained both the POS-
tag sequences and the constituency phrase struc-
ture outputs for every sentence in the training and
test sets determined by Stanford libraries. We em-
ployed the Maximum Entropy based supervised
classification model using the MALLET API (Mc-
Callum, 2002), which was responsible for suggest-
ing the various corrections.
The most closely related approach to ours is
probably the work of De Felice and Pulman
(2008). We also employ a Maximum Entropy clas-
sifier and a syntax-motivated feature set. However,
we investigate deeper linguistic features (based on
the f-structure of an LFG parser).
In the following subsections we introduce our
correction candidate recognition procedure and
the features used for training and prediction of
the machine learning classifier. We employed the
same feature set for each classification task.
2.1 Candidate Locations
We used the following heuristics for the recogni-
tion of the possible locations of grammatical er-
rors. We also describe the task of various classi-
fiers at these candidate locations.
Article and Determiner Error category We
handled the beginning of each noun phrase
(NP) as a possible location for errors related
to articles or determiners. The NP was
checked if it started with any definite or
indefinite article. If it did, we asked our
three-class classifier whether to leave it
unmodified, change its type (i.e. an indefinite
to a definite one or vice versa) or simply
delete it. However, when there was no article
at all at the beginning of a noun phrase,
the decision made by a different three-class
classifier was whether to leave that position
empty or to put a definite or indefinite article
in that place.
Wrong Number of the Noun Error category
Every token tagged as a noun (either in plural
or singular) was taken into consideration at
this subtask. We constructed two ? i.e. one
for the word forms originally written in plu-
ral and singular ? binary classifiers whether
the number (i.e. plural or singular) of the
noun should be changed or left unchanged.
2.2 LFG parse-based features
We looked for the minimal governing NP for each
candidate location. We reparsed this NP with-
out context by a Lexical Functional Grammar
(LFG) parser and we acquired features from its
f-structure. In the following paragraph, LFG is
introduced briefly while Table 1 summarizes the
features extracted from the LFG parse.
Lexical Functional Grammar (LFG) (Bresnan,
2000) is a constraint-based theory of grammar. It
posits two levels of representation, c(onstituent)-
structure and f(unctional)-structure.
C-structure is represented by context free
phrase-structure trees, and captures surface gram-
matical configurations. F-structures approximate
basic predicate-argument and adjunct structures.
The experiments reported in this paper use the
English LFG grammar constructed as part of the
ParGram project (Butt et al, 2002). The gram-
mar is implemented in XLE, a grammar develop-
ment environment, which includes a very efficient
LFG parser. Within the spectrum of approaches to
natural language parsing, XLE can be considered
a hybrid system combining a hand-crafted gram-
mar with a number of automatic ambiguity man-
agement techniques:
(i) c-structure pruning where, based on informa-
tion from statistically obtained parses, some trees
are ruled out before f-structure unification (Cahill
et al, 2007)
63
COORD NP/PP is coordinated +/-
COORD-LEVEL syntactic category of coordi-
nated phrase
DEG-DIM dimension for comparitive NPs,
(?equative?/?pos?/?neg?)
DEGREE semantic type of adjec-
tival modifier (?posi-
tive?/?comparative?/?superlative?)
DET-TYPE type of determiner
(?def?/?indef?/?demon?)
LOCATION-TYPE marks locative NPs
NAME-TYPE ?first name?/?last name?
NSYN syntactic noun type (?com-
mon?/?proper?/?pronoun?)
PRON-TYPE syntactic pronoun type (e.g.
?pers?, ?refl?, ?poss?)
PROPER-TYPE type of proper noun (e.g. ?com-
pany?, ?location?, ?name?)
Table 1: Short characterization of the LFG fea-
tures incorporated in our models designed to cor-
rect noun phrase-related grammatical errors
(ii) an Optimality Theory-style constraint mecha-
nism for filtering and ranking competing analyses
(Frank et al, 2001),
and (iii) a stochastic disambiguation component
which is based on a log-linear probability model
(Riezler et al, 2002) and works on the packed rep-
resentations.
Although we use a deep, hand-crafted LFG
grammar for processing the data, our approach is
substantially different from other grammar-based
approaches to CALL. For instance, Fortmann and
Forst (2004) supplement a German LFG devel-
oped for newspaper text with so-called malrules
that accept marked or ungrammatical input of
some predefined types. In our work, we apply an
LFG parser developed for standard texts to get a
rich feature representation that can be exploited
by a classifier. While malrules would certainly be
useful for finding other error types, such as agree-
ment errors, the NP- and PP-errors are often ana-
lyzed as grammatical by the parser (e.g. ?the po-
litical forces? vs. ?political forces?). Thus, the
grammaticality of a phrase predicted by the gram-
mar is not necessarily a good indicator for correc-
tion in our case.
2.3 Phrase-based contextual features
Besides the LFG features describing the internal
structure of the minimal NP that dominates a can-
didate location, we defined features describing its
context as well. Phrase-based contextual features
searched for those minimal prepositional and noun
phrases that governed a token at a certain can-
Final results Corrected output
P 0.0552 0.1260
R 0.0316 0.0292
F 0.0402 0.0474
Table 2: Overall results aggregated over the five
error types
didate location of the sentence where a decision
was about to be taken. Then features encoding the
types of the phrases that preceded and succeeded
a given minimal governing noun or prepositional
phrase were extracted.
The length of those minimal governing noun
and prepositional phrases as well as those of the
preceding and succeeding ones were taken into
account as numeric features. The motivation be-
hind using the span size of the minimal governing
and neighboring noun and prepositional phrases
is that it was assumed that grammatical errors in
the sentence result in unusual constituency subtree
patterns that could manifest in minimal governing
phrases having too long spans for instance. The
relative position of the candidate position inside
the smallest dominating noun and prepositional
phrases was also incorporated as a feature since
this information might carry some information for
noun errors.
2.4 Token-based contextual features
A third group of features described the context of
the candidate location at the token level. Here, two
sets of binary features were introduced to mark the
fact if the token was present in the four token-sized
window to its left or right. Dedicated nominal fea-
tures were introduced to store the word form of
the token immediately preceding a decision point
within a sentence and the POS-tags at the preced-
ing and actual token positions.
Two lists were manually created which con-
sisted of entirely uncountable nouns (e.g. blood)
and nouns that are uncountable most of the times
(e.g. aid or dessert). When generating fea-
tures for those classifiers that can modify the plu-
rality of a noun, we marked the fact in a binary
feature if they were present in any of these lists.
Another binary feature indicated if the actual noun
to be classified could be found at an earlier point
of the document.
64
Only erroneous All sentences
P 0.1260 0.1061
R 0.0292 0.0085
F 0.0474 0.0158
Table 3: Overall results aggregated over the five
error types
Only erroneous All sentences
P 0.2500 0.0167
R 0.0006 0.0006
F 0.0012 0.0012
Table 4: Overall results aggregated over the five
error types, not using the LFG parser based fea-
tures
3 Results
It is important to note that our officially submit-
ted architecture included an unintended step which
meant that whenever our system predicted that at
a certain point an indefinite article should be in-
serted or (re-)written, the indefinite article an was
put at that place erroneously when the succeeding
token started with a consonant (e.g. outputting an
serious instead of a serious).
Since the output that contained this kind of error
served as the basis of the official ranking we in-
clude in Table 2 the results achieved with the out-
put affected by this unintended behavior, however,
in the following we present our results in such a
manner where this kind of error is eliminated from
the output of our system.
Upon training our systems we followed two
strategies. For the first approach we used all the
sentences regardless if they had any error in them
at all. However, in an alternative approach we uti-
lized only those sentences from the training corpus
that had at least one error in them from the five er-
ror categories to be dealt with in the shared task.
The different results achieved on the test set ac-
cording to the two approaches are detailed in Ta-
ble 3. Turning off the LFG features ended up in
the results detailed in Table 4.
Since our framework in its present state only
aims at the correction of errors explicitly re-
lated to noun phrases, no error categories besides
ArtOrDet and Nn (for more details see Sections
1.1 and 1.2, respectively) could be possibly cor-
rected by our system. Note that these two error
categories covered 66.1% of the corrections on the
test set, so with our approach this was the highest
possibly achievable score in recall.
In order to get a clearer picture on the effective-
ness of our proposed methodology on the two error
types that we aimed at, we present results focusing
on those two error classes.
Nn ArtOrDet
P 0.4783 (44/92) 0.0151 (4/263)
R 0.1111 (44/396) 0.0058 (4/690)
F 0.1803 0.0084
Table 5: The scores achieved and the number of
true positive, suggestions, real errors for the Noun
Number (Nn) and Article and Determiner Errors
(ArtOrDet) categories.
4 Error Analysis
In order to analyze the performance of our system
in more detail, we carried out an error analysis.
As our system was optimized for errors related to
nouns (i.e. Nn and ArtOrDet errors), we focus
on these error categories in our discussion and ne-
glect verbal and prepositional errors.
Some errors in our system?s output were due
to pronouns, which are conventionally tagged as
nouns (e.g. something), but were incorrectly put
in the plural, resulting in the erroneous correc-
tion somethings. These errors would have been
avoided by including a list of pronouns which
could not be used in the plural (even if they are
tagged as nouns).
Another common source of errors was that
countable and uncountable uses of nouns which
can have both features in different senses or
metonymic usage (e.g. coffee as a substance is un-
countable but coffee meaning ?a cup of coffee? is
countable) were hard to separate. Performance on
this class of nouns could be ameliorated by apply-
ing word sense disambiguation/discrimination or
a metonymy detector would also prove useful for
e.g. mass nouns.
A great number of nominal errors involved
cases where a singular noun occurred in the text
without any article or determiner. In English, this
is only grammatical in the case of uncountable
nouns which occur in generic sentences, for in-
stance:
Radio-frequency identification is a
technology which uses a wireless non-
contact system to scan and transfer the
data [...]
65
The above sentence offers a definition of radio-
frequency identification, hence it is a generic state-
ment and should be left as it is. In other cases,
two possible strategies are available for correc-
tion. First, the noun gets an article or a determiner.
The actual choice among the articles or determin-
ers depends on the context: if the noun has been
mentioned previously and thus is already known
(definite) in the context, it usually gets a definite
article (or a possessive determiner). If it is men-
tioned for the first time, it gets an indefinite arti-
cle (unless it is a unique thing such as the sun).
The difficulty of the problem lies in the fact that
in order to adequately assign an article or deter-
miner to the noun, it is not sufficient to rely only
on the sentence. Thus, is also necessary to go be-
yond the sentence and move on the level of text
or discourse, which requires natural language pro-
cessing techniques that we currently lack but are
highly needed. With the application of such tech-
niques, we would have probably achieved better
results but this remains now for future work.
Second, the noun could be put in the plural.
This strategy is usually applied when either there
are more than one of the thing mentioned or it is a
generic sentence (i.e. things are discussed in gen-
eral and no specific instances of things are spo-
ken of). In this case, the detection of generic sen-
tences/events would be helpful, which again re-
quires deep semantic processing of the discourse
and is also a possible direction for future work.
To conclude, the successful identification of
noun number and article errors would require a
much deeper semantic (and even pragmatic) anal-
ysis and representation of the texts in question.
5 Discussion and further work
Comparing the columns of Table 3 we can con-
clude that restricting the training sentences to only
those which had some kind of grammatical error
in them had a useful effect on the overall effec-
tiveness of our system.
In a similar way, it can be stated based on the
results in Table 4 that composing features from the
output of an LFG parser is essentially beneficial
for the determination of Nn-type errors. Table 5
reveals, however, that those features which work
relatively well on the correction of Nn type errors
are less useful on ArtOrDet-type errors without
any modification.
As our only target at this point was to suggest
error corrections related to noun phrases, our ob-
vious future plans include the extension of our sys-
tem to deal with error categories of different types.
Simultaneously, we are planning to utilize large
scale corpus statistics, such as the Google N-gram
Corpus to build a more effective system.
Acknowledgements
This work was supported in part by the European
Union and the European Social Fund through the
project FuturICT.hu (grant no.: TA?MOP-4.2.2.C-
11/1/KONV-2012-0013).
References
Joan Bresnan. 2000. Lexical-Functional Syntax.
Blackwell, Oxford.
Miriam Butt, Helge Dyvik, Tracy Holloway King,
Hiroshi Masuichi, and Christian Rohrer. 2002.
The Parallel Grammar Project. In Proceedings of
COLING-2002 Workshop on Grammar Engineering
and Evaluation, Taipei, Taiwan.
Aoife Cahill, John T. Maxwell III, Paul Meurer, Chris-
tian Rohrer, and Victoria Rose?n. 2007. Speeding
up LFG Parsing using C-Structure Pruning. In Col-
ing 2008: Proceedings of the workshop on Grammar
Engineering Across Frameworks, pages 33 ? 40.
Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.
2013. Building a Large Annotated Corpus of
Learner English: The NUS Corpus of Learner En-
glish. In Proceedings of the 8th Workshop on Inno-
vative Use of NLP for Building Educational Appli-
cations (BEA 2013), Atlanta, Georgia, USA. Asso-
ciation for Computational Linguistics.
Robert Dale and Adam Kilgariff. 2010. Helping Our
Own: Text massaging for computational linguistics
as a new shared task. In Proceedings of the 6th Inter-
national Natural Language Generation Conference,
pages 261?265, Dublin, Ireland.
Robert Dale and Adam Kilgariff. 2011. Helping Our
Own: The HOO 2011 Pilot Shared Task. In Pro-
ceedings of the 13th European Workshop on Natural
Language Generation, Nancy, France.
Robert Dale, Ilya Anisimoff, and George Narroway.
2012. HOO 2012: A Report on the Preposition
and Determiner Error Correction Shared Task. In
Proceedings of the Seventh Workshop on Building
Educational Applications Using NLP, pages 54?62,
Montre?al, Canada, June. Association for Computa-
tional Linguistics.
Rachele De Felice and Stephen G. Pulman. 2008. A
Classifier-Based Approach to Preposition and Deter-
miner Error Correction in L2 English. In Proceed-
ings of the 22nd International Conference on Com-
66
putational Linguistics (Coling 2008), pages 169?
176.
Christian Fortmann and Martin Forst. 2004. An LFG
Grammar Checker for CALL. In Proceedings of
ICALL 2004.
Anette Frank, Tracy Holloway King, Jonas Kuhn, and
John T. Maxwell. 2001. Optimality Theory Style
Constraint Ranking in Large-Scale LFG Grammars.
In Peter Sells, editor, Formal and Empirical Issues in
Optimality Theoretic Syntax, pages 367?397. CSLI
Publications.
Andrew Kachites McCallum. 2002. MAL-
LET: A Machine Learning for Language Toolkit.
http://mallet.cs.umass.edu.
Stefan Riezler, Tracy Holloway King, Ronald M. Ka-
plan, Richard Crouch, John T. Maxwell, and Mark
Johnson. 2002. Parsing the Wall Street Journal us-
ing a Lexical-Functional Grammar and Discrimina-
tive Estimation Techniques. In Proceedings of ACL
2002.
67
