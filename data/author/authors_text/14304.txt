Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 725?735,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
A New Approach to Lexical Disambiguation of Arabic Text
Rushin Shah
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213, USA
rnshah@cs.cmu.edu
Paramveer S. Dhillon, Mark Liberman,
Dean Foster, Mohamed Maamouri
and Lyle Ungar
University of Pennsylvania
3451 Walnut Street
Philadelphia, PA 19104, USA
{dhillon|myl|ungar}@cis.upenn.edu,
foster@wharton.upenn.edu,
maamouri@ldc.upenn.edu
Abstract
We describe a model for the lexical analy-
sis of Arabic text, using the lists of alterna-
tives supplied by a broad-coverage morpho-
logical analyzer, SAMA, which include sta-
ble lemma IDs that correspond to combina-
tions of broad word sense categories and POS
tags. We break down each of the hundreds
of thousands of possible lexical labels into
its constituent elements, including lemma ID
and part-of-speech. Features are computed
for each lexical token based on its local and
document-level context and used in a novel,
simple, and highly efficient two-stage super-
vised machine learning algorithm that over-
comes the extreme sparsity of label distribu-
tion in the training data. The resulting system
achieves accuracy of 90.6% for its first choice,
and 96.2% for its top two choices, in selecting
among the alternatives provided by the SAMA
lexical analyzer. We have successfully used
this system in applications such as an online
reading helper for intermediate learners of the
Arabic language, and a tool for improving the
productivity of Arabic Treebank annotators.
1 Background and Motivation
This paper presents a methodology for generating
high quality lexical analysis of highly inflected lan-
guages, and demonstrates excellent performance ap-
plying our approach to Arabic. Lexical analysis of
the written form of a language involves resolving,
explicitly or implicitly, several different kinds of am-
biguities. Unfortunately, the usual ways of talking
about this process are also ambiguous, and our gen-
eral approach to the problem, though not unprece-
dented, has uncommon aspects. Therefore, in order
to avoid confusion, we begin by describing how we
define the problem.
In an inflected language with an alphabetic writ-
ing system, a central issue is how to interpret strings
of characters as forms of words. For example, the
English letter-string ?winds? will normally be in-
terpreted in one of four different ways, all four
of which involve the sequence of two formatives
wind+s. The stem ?wind? might be analyzed as (1) a
noun meaning something like ?air in motion?, pro-
nounced [wInd] , which we can associate with an ar-
bitrary but stable identifier like wind n1; (2) a verb
wind v1 derived from that noun, and pronounced the
same way; (3) a verb wind v2 meaning something
like ?(cause to) twist?, pronounced [waInd]; or (4)
a noun wind n2 derived from that verb, and pro-
nounced the same way. Each of these ?lemmas?, or
dictionary entries, will have several distinguishable
senses, which we may also wish to associate with
stable identifiers. The affix ?-s? might be analyzed
as the plural inflection, if the stem is a noun; or as
the third-person singular inflection, if the stem is a
verb.
We see this analysis as conceptually divided into
four parts: 1) Morphological analysis, which rec-
ognizes that the letter-string ?winds? might be (per-
haps among other things) wind/N + s/PLURAL or
wind/V + s/3SING; 2) Morphological disambigua-
tion, which involves deciding, for example, that in
the phrase ?the four winds?, ?winds? is probably a
plural noun, i.e. wind/N + s/PLURAL; 3) Lemma
analysis, which involves recognizing that the stem
wind in ?winds? might be any of the four lem-
mas listed above ? perhaps with a further listing of
senses or other sub-entries for each of them; and 4)
Lemma disambiguation, deciding, for example, that
725
the phrase ?the four winds? probably involves the
lemma wind n1.
Confusingly, the standard word-analysis tasks in
computational linguistics involve various combina-
tions of pieces of these logically-distinguished op-
erations. Thus, ?part of speech (POS) tagging? is
mainly what we?ve called ?morphological disam-
biguation?, except that it doesn?t necessarily require
identifying the specific stems and affixes involved.
In some cases, it also may require a small amount of
?lemma disambiguation?, for example to distinguish
a proper noun from a common noun. ?Sense disam-
biguation? is basically a form of what we?ve called
?lemma disambiguation?, except that the sense dis-
ambiguation task may assume that the part of speech
is known, and may break down lexical identity more
finely than our system happens to do. ?Lemmatiza-
tion? generally refers to a radically simplified form
of ?lemma analysis? and ?lemma disambiguation?,
where the goal is simply to collapse different in-
flected forms of any similarly-spelled stems, so that
the strings ?wind?, ?winds?, ?winded?, ?winding? will
all be treated as instances of the same thing, without
in fact making any attempt to determine the identity
of ?lemmas? in the traditional sense of dictionary
entries.
Linguists use the term morphology to include all
aspects of lexical analysis under discussion here.
But in most computational applications, ?morpho-
logical analysis? does not include the disambigua-
tion of lemmas, because most morphological ana-
lyzers do not reference a set of stable lemma IDs.
So for the purposes of this paper, we will continue to
discuss lemma analysis and disambiguation as con-
ceptually distinct from morphological analysis and
disambiguation, although, in fact, our system dis-
ambiguates both of these aspects of lexical analysis
at the same time.
The lexical analysis of textual character-strings
is a more complex and consequential problem in
Arabic than it is in English, for several reasons.
First, Arabic inflectional morphology is more com-
plex than English inflectional morphology is. Where
an English verb has five basic forms, for example,
an Arabic verb in principle may have dozens. Sec-
ond, the Arabic orthographic system writes elements
such as prepositions, articles, and possessive pro-
nouns without setting them off by spaces, roughly
as if the English phrase ?in a way? were written ?in-
away?. This leads to an enormous increase in the
number of distinct ?orthographic words?, and a sub-
stantial increase in ambiguity. Third, short vowels
are normally omitted in Arabic text, roughly as if
English ?in a way? were written ?nway?.
As a result, a whitespace/punctuation-delimited
letter-string in Arabic text typically has many more
alternative analyses than a comparable English
letter-string does, and these analyses have many
more parts, drawn from a much larger vocabulary of
form-classes. While an English ?tagger? can spec-
ify the morphosyntactic status of a word by choos-
ing from a few dozen tags, an equivalent level of
detail in Arabic would require thousands of alterna-
tives. Similarly, the number of lemmas that might
play a role in a given letter-sequence is generally
much larger in Arabic than in English.
We start our labeling of Arabic text with the alter-
native analyses provided by SAMA v. 3.1, the Stan-
dard Arabic Morphological Analyzer (Maamouri et
al., 2009). SAMA is an updated version of the ear-
lier Buckwalter analyzers (Buckwalter, 2004), with
a number of significant differences in analysis to
make it compatible with the LDC Arabic Treebank
3-v3.2 (Maamouri et al, 2004). The input to SAMA
is an Arabic orthographic word (a string of letters
delimited by whitespace or punctuation), and the
output of SAMA is a set of alternative analyses, as
shown in Table 1. For a typical word, SAMA pro-
duces approximately a dozen alternative analyses,
but for certain highly ambiguous words it can pro-
duce hundreds of alternatives.
The SAMA analyzer has good coverage; for typ-
ical texts, the correct analysis of an orthographic
word can be found somewhere in SAMA?s list of
alternatives about 95% of the time. However, this
broad coverage comes at a cost; the list of analytic
alternatives must include a long Zipfian tail of rare
or contextually-implausible analyses, which collec-
tively are correct often enough to make a large con-
tribution to the coverage statistics. Furthermore,
SAMA?s long lists of alternative analyses are not
evaluated or ordered in terms of overall or contex-
tual plausibility. This makes the results less useful
in most practical applications.
Our goal is to rank these alternative analyses so
that the correct answer is as near to the top of the list
726
Token Lemma Vocalization Segmentation Morphology Gloss
yHlm Halam-u 1 yaHolumu ya + Holum +
u
IV3MS + IV + IV-
SUFF MOOD:I
he / it + dream + [ind.]
yHlm Halam-u 1 yaHoluma ya + Holum +
a
IV3MS + IV + IV-
SUFF MOOD:S
he / it + dream + [sub.]
yHlm Halum-u 1 yaHolumo ya + Holum +
o
IV3MS + IV + IV-
SUFF MOOD:J
he / it + be gentle + [jus.]
qbl qabil-a 1 qabila qabil + a PV + PV-
SUFF SUBJ:3MS
accept/receive/approve +
he/it [verb]
qbl qabol 1 qabol qabol NOUN Before
Table 1: Partial output of SAMA for yHlm and qbl. On average, every token produces more than 10 such analyses
as possible. Despite some risk of confusion, we?ll
refer to SAMA?s list of alternative analyses for an
orthographic word as potential labels for that word.
And despite a greater risk of confusion, we?ll refer to
the assignment of probabilities to the set of SAMA
labels for a particular Arabic word in a particular
textual context as tagging, by analogy to the oper-
ation of a stochastic part-of-speech tagger, which
similarly assigns probabilities to the set of labels
available for a word in textual context.
Although our algorithms have been developed for
the particular case of Arabic and the particular set
of lexical-analysis labels produced by SAMA, they
should be applicable without modification to the sets
of labels produced by any broad-coverage lexical
analyzer for the orthographic words of any highly-
inflected language.
In choosing our approach, we have been moti-
vated by two specific applications. One applica-
tion aims to help learners of Arabic in reading text,
by offering a choice of English glosses with asso-
ciated Arabic morphological analyses and vocaliza-
tions. SAMA?s excellent coverage is an important
basis for this help; but SAMA?s long, unranked list
of alternative analyses for a particular letter-string,
where many analyses may involve rare words or al-
ternatives that are completely implausible in the con-
text, will be confusing at best for a learner. It is
much more helpful for the list to be ranked so that
the correct answer is almost always near the top, and
is usually one of the top two or three alternatives.
In our second application, this same sort of rank-
ing is also helpful for the linguistically expert native
speakers who do Arabic Treebank analysis. These
annotators understand the text without difficulty, but
find it time-consuming and fatiguing to scan a long
list of rare or contextually-implausible alternatives
for the correct SAMA output. Their work is faster
and more accurate if they start with a list that is
ranked accurately in order of contextual plausibility.
Other applications are also possible, such as vo-
calization of Arabic text for text-to-speech synthe-
sis, or lexical analysis for Arabic parsing. However,
our initial goals have been to rank the list of SAMA
outputs for human users.
We note in passing that the existence of set of sta-
ble ?lemma IDs? is an unusual feature of SAMA,
which in our opinion ought to be emulated by ap-
proaches to lexical analysis in other languages. The
lack of such stable lemma IDs has helped to disguise
the fact that without lemma analysis and disam-
biguation, morphological analyses and disambigua-
tion is only a partial solution to the problem of lexi-
cal analysis.
In principle, it is obvious that lemma disambigua-
tion and morphological disambiguation are mutually
beneficial. If we know the answer to one of the ques-
tions, the other one is easier to answer. However,
these two tasks require rather different sets of con-
textual features. Lemma disambiguation is similar
to the problem of word-sense disambiguation ? on
some definitions, they are identical ? and as a re-
sult, it benefits from paragraph-level and document-
level bag-of-words attributes that help to character-
ize what the text is ?about? and therefore which lem-
mas are more likely to play a role in it. In contrast,
morphological disambiguation mainly depends on
features of nearby words, which help to character-
727
ize how inflected forms of these lemmas might fit
into local phrasal structures.
2 Problem and Methodology
Consider a collection of tokens (observations), ti, re-
ferred to by index i ? {1, . . . , n}, where each token
is associated with a set of p features, xij , for the jth
feature, and a label, li, which is a combination of
a lemma and a morphological analysis. We use in-
dicator functions yik to indicate whether or not the
kth label for the ith token is present. We represent
the complete set of features and labels for the en-
tire training data using matrix notation as X and Y ,
respectively. Our goal is to predict the label l (or
equivalently, the vector y for a given feature vector
x.
A standard linear regression model of this prob-
lem would be
y = x? +  (1)
The standard linear regression estimate of ? (ig-
noring, for simplicity the fact that the ys are 0/1) is:
?? = (XTtrainXtrain)
?1XTtrainYtrain (2)
where Ytrain is an n?h matrix containing 0s and
1s indicating whether or not each of the h possible
labels is the correct label (li) for each of the n tokens
ti, Xtrain is an n ? p matrix of context features for
each of the n tokens, the coefficients ?? are p? h.
However, this is a large, sparse, multiple label
problem, and the above formulation is neither statis-
tically nor computationally efficient. Each observa-
tion (x,y) consists of thousands of features associ-
ated with thousands of potential labels, almost all of
which are zero. Worse, the matrix of coefficients ?,
to be estimated is large (p? h) and one should thus
use some sort of transfer learning to share strength
across the different labels.
We present a novel principled and highly compu-
tationally efficient method of estimating this multi-
label model. We use a two stage procedure, first
using a subset (Xtrain1, Ytrain1) of training data
to give a fast approximate estimate of ?; we then
use a second smaller subset of the training data
(Xtrain2, Ytrain2,) to ?correct? these estimates in a
way that we will show can be viewed as a spe-
cialized shrinkage. Our first stage estimation ap-
proximates ?, but avoids the expensive computa-
tion of (XTtrainXtrain)
?1. Our second stage corrects
(shrinks) these initial estimates in a manner special-
ized to this problem. The second stage takes ad-
vantage of the fact that we only need to consider
those candidate labels produced by SAMA. Thus,
only dozens of the thousands of possible labels are
considered for each token.
We now present our algorithm. We start with a
corpus D of documents d of labeled Arabic text. As
described above, each token, ti is associated with a
set of features characterizing its context, computed
from the other words in the same document, and a la-
bel, li = (lemmai,morphologyi), which is a combi-
nation of a lemma and a morphological analysis. As
described below, we introduce a novel factorization
of the morphology into 15 different components.
Our estimation algorithm, shown in Algorithm 1,
has two stages. We partition the training corpus into
two subsets, one of which (Xtrain1) is used to es-
timate the coefficients ?s and the other of which
(Xtrain2) is used to optimally ?shrink? these coeffi-
cient estimates to reduce variance and prevent over-
fitting due to data sparsity.
For the first stage of our estimation procedure, we
simplify the estimate of the (?) matrix (Equation 2)
to avoid the inversion of the very high dimensional
(p?p) matrix (XTX) by approximating (XTX) by
its diagonal, Var(X), the inverse of which is trivial
to compute; i.e. we estimate ? using
?? = Var(Xtrain1)
?1XTtrain1Ytrain1 (3)
For the second stage, we assume that the coeffi-
cients for each feature can be shrunk differently, but
that coefficients for each feature should be shrunk
the same regardless of what label they are predict-
ing. Thus, for a given observation we predict:
g?ik =
p?
j=1
wj ??jkxij (4)
where the weightswj indicate how much to shrink
each of the p features.
In practice, we fold the variance of each of the j
features into the weight, giving a slightly modified
equation:
g?ik =
p?
j=1
?j?
?
jkxij (5)
728
where ?? = XTtrain1Ytrain1 is just a matrix of the
counts of how often each context feature shows up
with each label in the first training set. The vec-
tor ?, which we will estimate by regression, is just
the shrinkage weightsw rescaled by the feature vari-
ance.
Note that the formation here is different from the
first stage. Instead of having each observation be
a token, we now let each observation be a (token,
label) pair, but only include those labels that were
output by SAMA. For a given token ti and poten-
tial label lk, our goal is to approximate the indica-
tor function g(i, k), which is 1 if the kth label of
token ti is present, and 0 otherwise. We find candi-
date labels using a morphological analyzer (namely
SAMA), which returns a set of possible candidate
labels, say C(t), for each Arabic token t. Our pre-
dicted label for ti is then argmaxk?C(ti)g(i, k).
The regression model for learning the weights ?j
in the second stage thus has a row for each label
g(i, k) associated with a SAMA candidate for each
token i = ntrain1+1 . . . ntrain2 in the second train-
ing set. The value of g(i, k) is predicted as a func-
tion of the feature vector zijk = ??jkxij .
The shrinkage coefficients, ?j , could be estimated
from theory, using a version of James-Stein shrink-
age (James and Stein, 1961), but in practice, superior
results are obtained by estimating them empirically.
Since there are only p of them (unlike the p ? h ?s),
a relatively small training set is sufficient. We found
that regression-SVMs work slightly better than lin-
ear regression and significantly better than standard
classification SVMs for this problem.
Prediction is then done in the obvious way by tak-
ing the tokens in a test corpusDtest, generating con-
text features and candidate SAMA labels for each
token ti, and selected the candidate label with the
highest score g?(i, k) that we set out to learn. More
formally, The model parameters ?? and ? produced
by the algorithm allow one to estimate the most
likely label for a new token ti out of a set of can-
didate labels C(ti) using
kpred = argmaxk?C(ti)
p?
j=1
?j?
?
jkxij (6)
The most expensive part of the procedure is es-
timating ??, which requires for each token in cor-
Algorithm 1 Training algorithm.
Input: A training corpusDtrain of n observations
(Xtrain, Ytrain)
PartitionDtrain into two sets,D1 andD2, of sizes
ntrain1 and ntrain2 = n? ntrain1 observations
// Using D1, estimate ??
??jk =
?ntrain1
i=1 xijyik for the j
th feature and kth
label
// Using D2, estimate ?j
// Generate new ?features? Z and the true labels
g(i, k) for each of the SAMA candidate labels for
each of the tokens in D2
zijk = ??jkxij for i in i = ntrain1 + 1 . . . ntrain2
Estimate ?j for the above (feature,label) pairs
(zijk, g(i, k)) using Regression SVMs
Output: ? and ??
pus D1, (a subset of D), finding the co-occurrence
frequencies of each label element (a lemma, or a
part of the morphological segmentation) with the
target token and jointly with the token and with
other tokens or characters in the context of the to-
ken of interest. For example, given an Arabic to-
ken, ?yHlm?, we count what fraction of the time
it is associated with each lemma (e.g. Halam-
u 1), count(lemma=Halam-u 1, token=yHlm) and
each segment (e.g. ?ya?), count(segment=ya, to-
ken=yHlm). (Of course, most tokens never show up
with most lemmas or segments; this is not a prob-
lem.) We also find the base rates of the components
of the labels (e.g., count(lemma=Halam-u 1), and
what fraction of the time the label shows up in vari-
ous contexts, e.g. count(lemma=Halam-u 1, previ-
ous token = yHlm). We describe these features in
more detail below.
3 Features and Labels used for Training
Our approach to tagging Arabic differs from conven-
tional approaches in the two-part shrinkage-based
method used, and in the choice of both features and
labels used in our model. For features, we study
both local context variables, as described above, and
document-level word frequencies. For the labels, the
key question is what labels are included and how
they are factored. Standard ?taggers? work by doing
an n-way classification of all the alternatives, which
is not feasible here due to the thousands of possi-
729
ble labels. Standard approaches such as Conditional
Random Fields (CRFs) are intractable with so many
labels. Moreover, few if any taggers do any lemma
disambiguation; that is partly because one must start
with some standard inventory of lemmas, which are
not available for most languages, perhaps because
the importance of lemma disambiguation has been
underestimated.
We make a couple of innovations to deal with
these issues. First, we perform lemma disambigua-
tion in addition to ?tagging?. As mentioned above,
lemmas and morphological information are not in-
dependent; the choice of lemma often influences
morphology and vice versa. For example, Table 1
contains two analyses for the word qbl. For the first
analysis, where the lemma is qabil-a 1 and the gloss
is accept/receive/approve + he/it [verb], the word is
a verb. However, for the second analysis, where the
lemma is qabol 1 and the gloss is before, the word
is a noun.
Simultaneous lemma disambiguation and tagging
introduces additional complexity: An analysis of
ATB and SAMA shows that there are approximately
2,200 possible morphological analyses (?tags?) and
40,000 possible lemmas; even accounting for the
fact that most combinations of lemmas and morpho-
logical analyses don?t occur, the size of the label
space is still in the order of tens of thousands. To
deal with data sparsity, our second innovation is to
factor the labels. We factor each label l into a set of
16 label elements (LEs). These include lemmas, as
well as morphological elements such as basic part-
of-speech, suffix, gender, number, mood, etc. These
are explained in detail below. Thus, since each la-
bel l is a set of 15 categorical variables, each y in
the first learning stage is actually a vector with 16
nonzero components and thousands of zeros. Since
we do simultaneous estimation of the entire set of
label elements, the value g(i, k) being predicted in
the second learning phase is 1 if the entire label set
is correct, and zero otherwise. We do not learn sep-
arate models for each label.
3.1 Label Elements (LEs)
The fact that there are tens of thousands of possible
labels presents the problem of extreme sparsity of
label distribution in the training data. We find that a
model that estimates coefficients ?? to predict a sin-
LE Description
lemma Lemma
pre1 Closer prefix
pre2 Farther prefix
det Determiner
pos Basic POS
dpos Additional data on basic pos
suf Suffix
perpos Person (basic pos)
numpos Number (basic pos)
genpos Gender (basic pos)
persuf Person (suffix)
numsuf Number (suffix)
gensuf Gender (suffix)
mood Mood of verb
pron Pronoun suffix
Table 2: Label Elements (LEs). Examples of additional
data on basic POS include whether a noun is proper or
common, whether a verb is transitive or not, etc. Both
the basic POS and its suffix may have person, gender and
number data.
gle label (a label being in the Cartesian product of
the set of label elements) yields poor performance.
Therefore, as just mentioned, we factor each label
l into a set of label elements (LEs), and learn the
correlations ?? between features and label elements,
rather than features and entire label sets. This re-
duces, but does not come close to eliminating, the
problem sparsity. A complete list of these LEs and
their possible values is detailed in Table 2.
3.2 Features
3.2.1 Local Context Features
We take (t, l) pairs from D2, and for each such
pair generate features Z based on co-occurrence
statistics ?? in D1, as mentioned in Algorithm 2.
These statistics include unigram co-occurrence fre-
quencies of each label with the target token and bi-
gram co-occurrence of the label with the token and
with other tokens or characters in the context of the
target token. We define them formally in Table 3.
Let Zbaseline denote the set of all such basic features
based on the local context statistics of the target to-
ken, namely the words and letters preceding and fol-
lowing it. We will use this set to create a baseline
model.
730
Statistic Description
Freq countD1(t, l)
PrevWord countD1(t, l, t?1)
NextWord countD1(t, l, t+1)
PreviLetter countD1(t, l, first letter(t?1))
NextiLetter countD1(t, l, first letter(t+1)
PrevfLetter countD1(t, l, last letter(t?1)
NextfLetter countD1(t, l, last letter(t+1)
Table 3: Co-occurrence statistics ??. We use these to
generate feature sets for our regression SVMs.
For each label element (LE) e, we define a set of
features Ze similar to Zbaseline; these features are
based on co-occurrence frequencies of the particular
LE e, not the entire label l.
Finally, we define an aggregate feature set Zaggr
as follows:
Zaggr = Zbaseline
?
{Ze} (7)
where e ? {lemma, pre1, pre2, det, pos, dpos,
suf, perpos, numpos, genpos, persuf, numsuf, gensuf,
mood, pron}.
3.2.2 Document Level Features
When trying to predict the lemma, it is useful to
include not just the words and characters immedi-
ately adjacent to the target token, but also the all the
words in the document. These words capture the
?topic? of the document, and help to disambiguate
different lemmas, which tend to be used or not used
based on the topic being discussed, similarly to the
way that word sense disambiguation systems in En-
glish sometimes use the ?bag of words? the docu-
ment to disambiguate, for example a ?bank? for de-
positing money from a ?bank? of a river. More pre-
cisely, we augment the features for each target token
with the counts of each word in the document (the
?term frequency? tf) in which the token occurs with
a given label.
Zfull = Zaggr
?
Ztf (8)
This setZfull is our final feature set. We useZfull
to train an SVM model Mfull; this is our final pre-
dictive model.
3.3 Corpora used for Training and Testing
We use three modules of the Penn Arabic Tree-
bank (ATB) (Maamouri et al, 2004), namely ATB1,
ATB2 and ATB3 as our corpus of labeled Ara-
bic text, D. Each ATB module is a collection
of newswire data from a particular agency. ATB1
uses the Associated Press as a source, ATB2 uses
Ummah, and ATB3 uses Annahar. D contains a total
of 1,835 documents, accounting for approximately
350,000 words. We construct the training and test-
ing setsDtrain andDtest fromD using 10-fold cross
validation, and we constructD1 andD2 fromDtrain
by randomly performing a 9:1 split.
As mentioned earlier, we use the SAMA mor-
phological analyzer to obtain candidate labels C(t)
for each token t while training and testing an SVM
model on D2 and Dtest respectively. A sample out-
put of SAMA is shown in Table 1. To improve cov-
erage, we also add to C(t) all the labels l seen for t
in D1. We find that doing so improves coverage to
98%. This is an upper bound on the accuracy of our
model.
C(t) = SAMA(t)
?
{l|(t, l) ? D1} (9)
4 Results
We use two metrics of accuracy: A1, which mea-
sures the percentage of tokens for which the model
assigns the highest score to the correct label or LE
value (or E1= 100?A1, the corresponding percent-
age error), and A2, which measures the percentage
of tokens for which the correct label or LE value
is one of the two highest ranked choices returned
by the model (or E2 = 100 ? A2). We test our
modelMfull onDtest and achieve A1 and A2 scores
of 90.6% and 96.2% respectively. The accuracy
achieved by our Mfull model is, to the best of our
knowledge, higher than prior approaches have been
able to achieve so far for the problem of combined
morphological and lemma disambiguation. This is
all the more impressive considering that the upper
bound on accuracy for our model is 98% because,
as described above, our set of candidate labels is in-
complete.
In order to analyze how well different LEs can be
predicted, we train an SVM model Me for each LE
e using the feature set Ze, and test all such models
731
on Dtest. The results for all the LEs are reported in
the form of error percentages E1 and E2 in Table 4.
Model E1 E2 Model E1 E2
Mlemma 11.1 4.9 Mpre1 1.9 1.4
Mpre2 0.2 0 Mdet 0.7 0.1
Mpos 23.4 4.0 Mdpos 10.3 1.9
Msuf 7.6 2.5 Mperpos 3.0 0.1
Mnumpos 3.2 0.2 Mgenpos 1.8 0.1
Mpersuf 3.2 0.1 Mnumsuf 8.2 0.5
Mgensuf 11.6 0.4 Mmood 1.6 1.4
Mpron 1.8 0.6 Mcase 14.7 5.9
Mfull 9.4 3.8 - - -
Table 4: Results of Me for each LE e. Note: The results
reported are 10 fold cross validation test accuracies and
no parameters have been tuned on them.
A comparison of the results for Mfull with the
results for Mlemma and Mpos is particularly infor-
mative. We see that Mfull is able to achieve a sub-
stantially lower E1 error score (9.4%) than Mlemma
(11.1%) and Mpos (23.4%); in other words, we find
that our full model is able to predict lemmas and ba-
sic parts-of-speech more accurately than the individ-
ual models for each of these elements.
We examine the effect of varying the size of D2,
i.e. the number of SVM training instances, on the
performance of Mfull on Dtest, and find that with
increasing sizes of D2, E1 reduces only slightly
from 9.5% to 9.4%, and shows no improvement
thereafter. We also find that the use of document-
level features in Mlemma reduces E1 and E2 per-
centages for Mlemma by 5.7% and 3.2% respec-
tively.
4.1 Comparison to Alternate Approaches
4.1.1 Structured Prediction Models
Preliminary experiments showed that knowing the
predicted labels (lemma + morphology) of the sur-
rounding words can slightly improve the predic-
tive accuracy of our model. To further investi-
gate this effect, we tried running experiments us-
ing different structured models, namely CRF (Con-
ditional Random Fields) (Lafferty et al, 2001),
(Structured) MIRA (Margin Infused Relaxation Al-
gorithm) (Crammer et al, 2006) and Structured
Perceptron (Collins, 2002). We used linear chain
CRFs as implemented in MALLET Toolbox (Mc-
Callum, 2001) and for Structured MIRA and Per-
ceptron we used their implementations from EDLIN
Toolbox (Ganchev and Georgiev, 2009). However,
given the vast label space of our problem, running
these methods proved infeasible. The time complex-
ity of these methods scales badly with the number of
labels; It took a week to train a linear chain CRF
for only ? 50 labels and though MIRA and Per-
ceptron are online algorithms, they also become in-
tractable beyond a few hundred labels. Since our
label space contains combinations of lemmas and
morphologies, so even after factoring, the dimension
of the label space is in the order of thousands.
We also tried a na??ve version (two-pass approxi-
mation) of these structured models. In addition to
the features in Zfull, we include the predicted la-
bels for the tokens preceding and following the tar-
get token as features. This new model is not only
slow to train, but also achieves only slightly lower
error rates (1.2% lower E1 and 1.0% lower E2) than
Mfull. This provides an upper bound on the bene-
fit of using the more complex structured models, and
suggests that given their computational demands our
(unstructured) model Mfull is a better choice.
4.1.2 MADA
(Habash and Rambow, 2005) perform morpho-
logical disambiguation using a morphological ana-
lyzer. (Roth et al, 2008) augment this with lemma
disambiguation; they call their system MADA. Our
work differs from theirs in a number of respects.
Firstly, they don?t use the two step regression proce-
dure that we use. Secondly, they use only ?unigram?
features. Also, they do not learn a single model from
a feature set based on labels and LEs; instead, they
combine models for individual elements by using
weighted agreement. We trained and tested MADA
v2.32 using its full feature set on the same Dtrain
andDtest. We should point out that this is not an ex-
act comparison, since MADA uses the older Buck-
walter morphological analyzer.1
4.1.3 Other Alternatives
Unfactored Labels: To illustrate the benefit ob-
tained by breaking down each label l into
1A new version of MADA was released very close to the
submission deadline for this conference.
732
LEs, we contrast the performance of our Mfull
model to an SVM model Mbaseline trained us-
ing only the feature set Zbaseline, which only
contains features based on entire labels, those
based on individual LEs.
Independent lemma and morphology prediction:
Another alternative approach is to pre-
dict lemmas and morphological analyses
separately. We construct a feature set
Zlemma? = Zfull ? Zlemma and train an SVM
model Mlemma? using this feature set. Labels
are then predicted by simply combining the
results predicted independently by Mlemma
and Mlemma? . Let Mind denote this approach.
Unigram Features: Finally, we also consider a
context-less approach, i.e. using only ?uni-
gram? features for labels as well as LEs. We
call this feature set Zuni, and the correspond-
ing SVM model Muni.
The results of these various models, along with
those of Mfull are summarized in Table 5. We see
thatMfull has roughly half the error rate of the state-
of-the-art MADA system.
Model E1 E2
Mbaseline 13.6 9.1
Mind 18.7 6.0
Muni 11.6 6.4
Mcheat 8.2 2.8
MADA 16.9 12.6
Mfull 9.4 3.8
Table 5: Percent error rates of alternative approaches.
Note: The results reported are 10 fold cross validation
test accuracies and no parameters have been tuned on
them. We used same train-test splits for all the datasets.
5 Related Work
(Hajic, 2000) show that for highly inflectional
languages, the use of a morphological analyzer
improves accuracy of disambiguation. (Diab et
al., 2004) perform tokenization, POS tagging
and base phrase chunking using an SVM based
learner. (Ahmed and Nu?rnberger, 2008) perform
word-sense disambiguation using a Naive Bayesian
model and rely on parallel corpora and match-
ing schemes instead of a morphological ana-
lyzer. (Kulick, 2010) perform simultaneous tok-
enization and part-of-speech tagging for Arabic by
separating closed and open-class items and focus-
ing on the likelihood of possible stems of open-
class words. (Mohamed and Ku?bler, 2010) present
a hybrid method between word-based and segment-
based POS tagging for Arabic and report good re-
sults. (Toutanova and Cherry, 2009) perform joint
lemmatization and part-of-speech tagging for En-
glish, Bulgarian, Czech and Slovene, but they do
not use the two step estimation-shrinkage model de-
scribed in this paper; nor do they factor labels. The
idea of joint lemmatization and part-of-speech tag-
ging has also been discussed in the context of Hun-
garian in (Kornai, 1994).
A substantial amount of relevant work has been
done previously for Hebrew. (Adler and Elhadad,
2006) perform Hebrew morphological disambigua-
tion using an unsupervised morpheme-based HMM,
but they report lower scores than those achieved by
our model. Moreover, their analysis doesn?t include
lemma IDs, which is a novelty of our model. (Gold-
berg et al, 2008) extend the work of (Adler and El-
hadad, 2006) by using an EM algorithm, and achieve
an accuracy of 88% for full morphological analy-
sis, but again, this does not include lemma IDs. To
the best of our knowledge, there is no existing re-
search for Hebrew that does what we did for Arabic,
namely to use simultaneous lemma and morpholog-
ical disambiguation to improve both. (Dinur et al,
2009) show that prepositions and function words can
be accurately segmented using unsupervised meth-
ods. However, by using this method as a preprocess-
ing step, we would lose the power of a simultaneous
solution for these problems. Our method is closer in
style to a CRF, giving much of the accuracy gains of
simultaneous solution, while being about 4 orders of
magnitude easier to train.
We believe that our use of factored labels is novel
for the problem of simultaneous lemma and mor-
phological disambiguation; however, (Smith et al,
2005) and (Hatori et al, 2008) have previously
made use of features based on parts of labels in
CRF models for morphological disambiguation and
word-sense disambiguation respectively. Also, we
note that there is a similarity between our two-stage
733
machine learning approach and log-linear models in
machine translation that break the data in two parts,
estimating log-probabilities of generative models
from one part, and discriminatively re-weighting the
models using the second part.
6 Conclusions
We introduced a new approach to accurately predict
labels consisting of both lemmas and morphologi-
cal analyses for Arabic text. We obtained an accu-
racy of over 90% ? substantially higher than current
state-of-the-art systems. Key to our success is the
factoring of labels into lemma and a large set of mor-
phosyntactic elements, and the use of an algorithm
that computes a simple initial estimate of the coef-
ficient relating each contextual feature to each la-
bel element (simply by counting co-occurrence) and
then regularizes these features by shrinking each of
the coefficients for each feature by an amount deter-
mined by supervised learning using only the candi-
date label sets produced by SAMA.
We also showed that using features of word n-
grams is preferable to using features of only individ-
ual tokens of data. Finally, we showed that a model
using a full feature set based on labels as well as
factored components of labels, which we call label
elements (LEs) works better than a model created
by combining individual models for each LE. We
believe that the approach we have used to create our
model can be successfully applied not just to Arabic
but also to other languages such as Turkish, Hungar-
ian and Finnish that have highly inflectional mor-
phology. The current accuracy of of our model, get-
ting the correct answer among the top two choices
96.2% of the time is high enough to be highly use-
ful for tasks such as aiding the manual annotation
of Arabic text; a more complete automation would
require that accuracy for the single top choice.
Acknowledgments
We woud like to thank everyone at the Linguis-
tic Data Consortium, especially Christopher Cieri,
David Graff, Seth Kulick, Ann Bies, Wajdi Za-
ghouani and Basma Bouziri for their help. We also
wish to thank the anonymous reviewers for their
comments and suggestions.
References
Meni Adler and Michael Elhadad. 2006. An Unsuper-
vised Morpheme-Based HMM for Hebrew Morpho-
logical Disambiguation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th annual meeting of the Association for
Computational Linguistics.
Farag Ahmed and Andreas Nu?rnberger. 2008. Ara-
bic/English Word Translation Disambiguation using
Parallel Corpora and Matching Schemes. In Proceed-
ings of EAMT?08, Hamburg, Germany.
Tim Buckwalter. 2004. Buckwalter Arabic Morphologi-
cal Analyzer version 2.0.
Michael Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Experi-
ments with Perceptron Algorithms. In Proceedings of
EMNLP?02.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online Passive-
Aggressive Algorithms. Journal of Machine Learning
Research, 7:551?585.
Mona Diab, Kadri Hacioglu, and Daniel Jurafsky. 2004.
Automatic Tagging of Arabic text: From Raw Text to
Base Phrase Chunks. In Proceedings of the 5th Meet-
ing of the North American Chapter of the Associa-
tion for Computational Linguistics/Human Language
Technologies Conference (HLT-NAACL?04).
Elad Dinur, Dmitry Davidov, and Ari Rappoport. 2009.
Unsupervised Concept Discovery in Hebrew Using
Simple Unsupervised Word Prefix Segmentation for
Hebrew and Arabic. In Proceedings of the EACL 2009
Workshop on Computational Approaches to Semitic
Languages.
Kuzman Ganchev and Georgi Georgiev. 2009. Edlin:
An Easy to Read Linear Learning Framework. In Pro-
ceedings of RANLP?09.
Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008.
EM Can Find Pretty Good HMM POS-Taggers (When
Given a Good Start)*. In Proceedings of ACL?08.
Nizar Habash and Owen Rambow. 2005. Arabic Tok-
enization, Part-of-Speech Tagging and Morphological
Disambiguation in One Fell Swoop. In Proceedings of
ACL?05, Ann Arbor, MI, USA.
Jan Hajic. 2000. Morphological Tagging: Data vs. Dic-
tionaries. In Proceedings of the 1st Meeting of the
North American Chapter of the Association for Com-
putational Linguistics (NAACL?00).
Jun Hatori, Yusuke Miyao, and Jun?ichi Tsujii. 2008.
Word Sense Disambiguation for All Words using Tree-
Structured Conditional Random Fields. In Proceed-
ings of COLing?08.
W. James and Charles Stein. 1961. Estimation with
Quadratic Loss. In Proceedings of the Fourth Berkeley
734
Symposium on Mathematical Statistics and Probabil-
ity, Volume 1.
Andra?s Kornai. 1994. On Hungarian morphology (Lin-
guistica, Series A: Studia et Dissertationes 14). Lin-
guistics Institute of Hungarian Academy of Sciences,
Budapest.
Seth Kulick. 2010. Simultaneous Tokenization and Part-
of-Speech Tagging for Arabic without a Morphologi-
cal Analyzer. In Proceedings of ACL?10.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional Random Fields: Proba-
bilistic Models for Segmenting and Labeling Sequence
Data. In Proceedings of ICML?01, pages 282?289.
Mohamed Maamouri, Ann Bies, and Tim Buckwalter.
2004. The Penn Arabic Treebank: Building a Large
Scale Annotated Arabic Corpus. In Proceedings of
NEMLAR Conference on Arabic Language Resources
and Tools.
Mohamed Maamouri, David Graff, Basma Bouziri, Son-
dos Krouna, and Seth Kulick. 2009. LDC Standard
Arabic Morphological Analyzer (SAMA) v. 3.0.
Andrew McCallum, 2001. MALLET: A Machine Learn-
ing for Language Toolkit. Software available at
http://mallet.cs.umass.edu.
Emad Mohamed and Sandra Ku?bler. 2010. Arabic Part
of Speech Tagging. In Proceedings of LREC?10.
Ryan Roth, Owen Rambow, Nizar Habash, Mona Diab,
and Cynthia Rudin. 2008. Arabic Morphological Tag-
ging, Diacritization, and Lemmatization Using Lex-
eme Models and Feature Ranking. In Proceedings of
ACL?08, Columbus, Ohio, USA.
Noah A. Smith, David A. Smith, and Roy W. Tromble.
2005. Context-Based Morphological Disambiguation
with Random Fields*. In Proceedings of Human
Language Technology Conference and Conference on
Empirical Methods in Natural Language Processing
(HLT/EMNLP).
Kristina Toutanova and Colin Cherry. 2009. A Global
Model for Joint Lemmatization and Part-of-Speech
Prediction. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th Inter-
national Joint Conference on Natural Language Pro-
cessing, pages 486?494.
735
Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 136?144,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
CONE: Metrics for Automatic Evaluation of Named Entity              
Co-reference Resolution  
 
 
Bo Lin, Rushin Shah, Robert Frederking, Anatole Gershman 
Language Technologies Institute, School of Computer Science 
Carnegie Mellon University 
5000 Forbes Ave., PA 15213, USA 
 {bolin,rnshah,ref,anatoleg}@cs.cmu.edu 
 
 
Abstract 
Human annotation for Co-reference Resolu-
tion (CRR) is labor intensive and costly, and 
only a handful of annotated corpora are cur-
rently available. However, corpora with 
Named Entity (NE) annotations are widely 
available. Also, unlike current CRR systems, 
state-of-the-art NER systems have very high 
accuracy and can generate NE labels that are 
very close to the gold standard for unlabeled 
corpora.  We propose a new set of metrics col-
lectively called CONE for Named Entity Co-
reference Resolution (NE-CRR) that use a 
subset of gold standard annotations, with the 
advantage that this subset can be easily ap-
proximated using NE labels when gold stan-
dard CRR annotations are absent. We define 
CONE B3 and CONE CEAF metrics based on 
the traditional B3 and CEAF metrics and show 
that CONE B3 and CONE CEAF scores of any 
CRR system on any dataset are highly corre-
lated with its B3 and CEAF scores respectively. 
We obtain correlation factors greater than 0.6 
for all CRR systems across all datasets, and a 
best-case correlation factor of 0.8. We also 
present a baseline method to estimate the gold 
standard required by CONE metrics, and show 
that CONE B3 and CONE CEAF scores using 
this estimated gold standard are also correlated 
with B3 and CEAF scores respectively. We 
thus demonstrate the suitability of CONE 
B3and CONE CEAF for automatic evaluation 
of NE-CRR. 
1 Introduction 
Co-reference resolution (CRR) is the problem of 
determining whether two entity mentions in a 
text refer to the same entity in real world or not. 
Noun Phrase CRR (NP-CRR) considers all noun 
phrases as entities, while Named Entity CRR 
restricts itself to noun phrases that describe a 
Named Entity. In this paper, we consider the task 
of Named Entity CRR (NE-CRR) only. Most, if 
not all, recent efforts in the field of CRR have 
concentrated on machine-learning based ap-
proaches. Many of them formulate the problem 
as a pair-wise binary classification task, in which 
possible co-reference between every pair of men-
tions is considered, and produce chains of co-
referring mentions for each entity as their output. 
One of the most important problems in CRR is 
the evaluation of CRR results. Different evalua-
tion metrics have been proposed for this task. B-
cubed (Bagga and Baldwin, 1998) and CEAF 
(Luo, 2005) are the two most popular metrics; 
they compute Precision, Recall and F1 measure 
between matched equivalent classes and use 
weighted sums of Precision, Recall and F1 to 
produce a global score. Like all metrics, B3 and 
CEAF require gold standard annotations; howev-
er, gold standard CRR annotations are scarce, 
because producing such annotations involves a 
substantial amount of human effort since it re-
quires an in-depth knowledge of linguistics and a 
high level of understanding of the particular text. 
Consequently, very few corpora with gold stan-
dard CRR annotations are available (NIST, 2003; 
MUC-6, 1995; Agirre, 2007). By contrast, gold 
standard Named Entity (NE) annotations are easy 
to produce; indeed, there are many NE annotated 
corpora of different sizes and genres. Similarly, 
there are few CRR systems and even the best 
scores obtained by them are only in the region of 
F1 = 0.5 - 0.6. There are only four such CRR 
systems freely available, to the best of our know-
ledge (Bengston and Roth, 2007; Versley et al, 
2008; Baldridge and Torton, 2004; Baldwin and 
Carpenter, 2003). In comparison, there are nu-
merous Named Entity recognition (NER) sys-
tems, both general-purpose and specialized, and 
many of them achieve scores better than F1 = 
0.95 (Ratinov and Roth, 2009; Finkel et al, 
136
2005). Although these facts can be partly attri-
buted to the ?hardness? of CRR compared to 
NER, they also reflect the substantial gap be-
tween NER and CRR research. In this paper, we 
present a set of metrics, collectively called 
CONE, that leverage widely available NER sys-
tems and resources and tools for the task of eva-
luating co-reference resolution systems. The ba-
sic idea behind CONE is to predict a CRR sys-
tem?s performance for the task of full NE-CRR 
on some dataset using its performance for the 
subtask of named mentions extraction and group-
ing (NMEG) on that dataset. The advantage of 
doing so is that measuring NE-CRR performance 
requires the co-reference information of all men-
tions of a Named Entity, including named men-
tions, nominal and pronominal references, while 
measuring the NMEG performance only requires 
co-reference information of named mentions of a 
NE, and this information is relatively easy to ob-
tain automatically even in the absence of gold 
standard annotations. We compute correlation 
between CONE B3, B3, CONE CEAF and CEAF 
scores for various CRR systems on various gold-
standard annotated datasets and show that the 
CONE B3 and B3 scores are highly correlated for 
all such combinations of CRR systems and data-
sets, as are CONE CEAF and CEAF scores, with 
a best-case correlation of 0.8. We produce esti-
mated gold standard annotations for the Enron 
email corpus, since no actual gold standard CRR 
annotations exist for it, and then use CONE B3 
and CONE CEAF with these estimated gold 
standard annotations to compare the performance 
of various NE-CRR systems on this corpus. No 
such comparison has been previously performed 
for the Enron corpus. 
We adopt the same terminology as in (Luo, 
2005): a mention refers to each individual phrase 
and an entity refers to the equivalence class or 
co-reference chain with several mentions. This 
allows us to note some differences between NE-
CRR and NP-CRR. NE-CRR involves indentify-
ing named entities and extracting their co-
referring mentions; equivalences classes without 
any NEs are not considered. NE-CRR is thus 
clearly a subset of NP-CRR, where all co-
referring mentions and equivalence classes are 
considered. However, we focus on NE-CRR be-
cause it is currently a more active research area 
than NP-CRR and a better fit for target applica-
tions such as text forensics and web mining, and 
also because it is more amenable to the automatic 
evaluation approach that we propose. 
The research questions that motivate our work 
are:  
(1) Is it possible to use only NER resources to 
evaluate NE-CRR systems? If so, how is this 
problem formulated?  
(2) How does one perform evaluation in a way 
that is accurate and automatic with least hu-
man intervention?  
(3) How does one perform evaluation on large 
unlabeled datasets?  
We show that our CONE metrics achieve good 
results and represent a promising first step to-
ward answering these questions.  
 
The rest of the paper is organized as follows. We 
present related work in the field of automatic 
evaluation methods for natural language 
processing tasks in Section 2. In Section 3, we 
give an overview of the standard metrics current-
ly used for evaluating co-reference resolution. 
We define our new metrics CONE B3 and CONE 
CEAF in Section 4. In section 5, we provide ex-
perimental results that illustrate the performance 
of CONE B3 and CONE CEAF compared to B3 
and CEAF respectively. In Section 6, we give an 
example of the application of CONE metrics by 
evaluating NE-CRR systems on an unlabeled 
dataset, and discuss possible drawbacks and ex-
tensions of these metrics. Finally, in section 7 we 
present our conclusions and ideas for future 
work.  
2 Related Work 
There has been a substantial amount of research 
devoted to automatic evaluation for natural lan-
guage processing, especially tasks involving lan-
guage generation. The BLEU score (Papineni et 
al., 2002) proposed for evaluating machine trans-
lation results is the best known example of this. 
It uses n-gram statistics between machine gener-
ated results and references. It inspired the 
ROUGE metric (Lin and Hovy, 2003) and other 
methods (Louis and Nenkova, 2009) to perform 
automatic evaluation of text summarization. Both 
these metrics have show strong correlation be-
tween automatic evaluation results and human 
judgments. The two metrics successfully reduce 
the need for human judgment and help speed up 
research by allowing large-scale evaluation. 
Another example is the alignment entropy (Per-
vouchine et al, 2009) for evaluating translitera-
tion alignment. It reduces the need for alignment 
gold standard and highly correlates with transli-
teration system performance. Thus it is able to 
137
serve as a good metric for transliteration align-
ment. We contrast our work with (Stoyanov et al, 
2009), who show that the co-reference resolution 
problem can be separated into different parts ac-
cording to the type of the mention. Some parts 
are relatively easy to solve. The resolver per-
forms equally well in each part across datasets. 
They use the statistics of mentions in different 
parts with test results on other datasets as a pre-
dictor for unseen datasets, and obtain promising 
results with good correlations. We approach the 
problem from a different perspective. In our 
work, we show the correlation between the 
scores on traditional metrics and scores on our 
CONE metrics, and show how to automatically 
estimate the gold standard required by CONE 
metrics. Thus our method is able to predict the 
co-reference resolution performance without 
gold standard at all. We base our new metrics on 
the standard B3 and CEAF metrics used for com-
puting CRR scores. (Vilian et al, 1995; Bagga 
and Baldwin, 1998; Luo, 2005). B3 and CEAF 
are believed to be more discriminative and inter-
pretable than earlier metrics and are widely 
adopted especially for machine-learning based 
approaches.  
 
3 Standard Metrics: B3 and CEAF 
We now provide an overview of the standard B3 
and CEAF metrics used to evaluate CRR sys-
tems. Both metrics assume that a CRR system 
produces a set of equivalence classes {O} and 
assigns each mention to only one class. Let Oi be 
the class to which the ith mention was assigned 
by the system. We also assume that we have a set 
of correct equivalence classes {G} (the gold 
standard). Let Gi be the gold standard class to 
which the ith mention should belong. Let Ni de-
note the number of mentions in Oi which are also 
in Gi ? the correct mentions. B
3 computes the 
presence rate of correct mentions in the same 
equivalent classes. The individual precision and 
recall score is defined as follows: 
|| i
i
i O
NP ?
 
|| i
i
i G
NR ?
 
Here |Oi| and |Gi| are the cardinalities of sets Oi 
and Gi.   
The final precision and recall scores are: 
?
?
?
n
i
ii PwP
1
 ?
?
?
n
i
ii RwR
1
 
Here, in the simplest case the weight wi is set to 
1/n, equal for all mentions. 
CEAF (Luo, 2005) produces the optimal 
matching between output classes and true classes 
first, with the constraint that one true class, Gi, 
can be mapped to at most one output class, say 
Of(i) and vice versa. This can be solved by the 
KM algorithm (Kuhn, 1955; Munkres, 1957) for 
maximum matching in a bipartite graph. CEAF 
then computes the precision and recall score as 
follows: 
?
?
?
i
i
i
ifi
O
M
P
)(,      
?
?
?
i
i
i
ifi
G
M
R
)(,  
jiji GOM ??,
 
We use the terms Mi,j from CEAF to re-write B
3, 
its formulas then reduce to: 
???? i j i
ji
i
i O
M
O
P
2
,1  
???? i j i
ji
i
i G
M
G
R
2
,1  
We can see that B3 simply iterates through all 
pairs of matchings instead of considering the one 
to one mappings as CEAF does. Thus, B3 com-
putes the weighted sum of the F-measures for 
each individual mention which helps alleviate the 
bias in the pure link-based F-measure, while 
CEAF computes the same as B3 but enforces at 
most one matched equivalence class for every 
class in the system output and gold standard out-
put. 
4 CONE B3 and CONE CEAF Metrics:  
We now formally define the new CONE B3 and 
CONE CEAF metrics that we propose for 
automatic evaluation of NE-CRR systems. 
      Let G denote the set of gold standard 
annotations and O denote the output of an NE-
CRR system. Let Gi denote the equivalent class 
of entity i in the gold standard and Oj denote the 
equivalence class for entity j in the system output.  
Also let Gij denote the j
th mention in the 
equivalence class of entity i in the gold standard 
and Oij denote the j
th mention in the system 
output. 
As described earlier, the standard B3 and CEAF 
metrics evaluate scores using G and O and can 
be thought of as functions of the form B3(G, O). 
and CEAF(G, O) respectively. Let us use 
Score(G, O) to collectively refer to both these 
138
functions. An equivalence class Gi in G may 
contain three types of mentions: named mentions 
gNMij, nominal mentions g
NO
ij, and pronominal 
mentions gPRij. Similarly, we can define o
NM
ij, 
oNOij and o
PR
ij for a class Oi in O. Now for each 
gold standard equivalence class Gi and system 
output equivalence class Oi, we define the 
following sets GNMi  and  O
NM
i: 
iijNMijNMiNM GggGi ??? },{,  
iijNMijNMiNM OooOi ??? },{,  
In other words, GNMi and O
NM
i are the subsets of 
Gi and Oi containing all named mentions and no 
mentions of any other type.  
Let GNM denote the set of all such equivalance 
classes GNMi and O
NM denote the set of all 
equivalence classes ONMi. It is clear that G
NM and 
ONM are pruned versions of the gold standard 
annotations and system output respectively. 
We now define CONE B3 and CONE CEAF as 
follows: 
CONE B3 = B3(GNM, ONM) 
CONE CEAF = CEAF(GNM, ONM) 
 
Following our previous notation, we denote 
CONE B3 and CONE CEAF collectively as 
Score(GNM, ONM). We observe that Score(GNM, 
ONM) measures a NE-CRR system?s  
performance for the NE-CRR subtask of named 
mentions extraction and grouping (NMEG). We 
find that Score(GNM, ONM) is highly correlated 
with Score(G, O) for all the freely available NE-
CRR systems over various datasets. This 
provides the neccessary  justification for the use 
of Score(GNM, ONM).  
We use SYNERGY (Shah et al, 2010), an 
ensemble NER system that combines the UIUC 
NER (Ritanov and Roth, 2009) and Stanford 
NER (Finkel et al, 2005) systems, to produce 
GNM and ONM from G and O by  selecting named 
mentions. However, any other good NER system 
would serve the same purpose. 
We see that while standard evaluation metrics 
require the use of G, i.e. the full set of NE-CRR 
gold standard annotations including named, 
nominal and pronimal mentions, CONE metrics 
require only GNM, i.e. gold standard annotations 
consisting of named mentions only. The key 
advantage of using CONE metrics is that GNM 
can be automatically approximated using an 
NER system with a good degree of accuracy. 
This is because state-of-the-art NER systems 
achieve near-optimal performance, exceeding F1 
= 0.95 in many cases, and after obtaining their 
output, the task of estimating GNM reduces to 
simply clustering it to seperate mentions of 
diffrerent real-world entities. This clustering can 
be thought of as a form of named entity matching, 
which is not a very hard problem. There exist 
systems that perform such matching in a 
sophisticated manner with a high degree of 
accuracy. We use simple heuristics such as exact 
matching, word matches, matches between in-
itials, etc. to design such a matching system 
ourselves and use it to obtain estimates of GNM, 
say GNM-approx. We then calculate CONE B3 and 
CONE CEAF scores using GNM-approx instead of 
GNM; in other words, we perform fully automatic 
evaluation of NE-CRR systems by using 
Score(GNM-approx, ONM) instead of Score(GNM, 
ONM). In order to show the validity of this 
evaluation, we calculate the correlation between 
the Score(GNM-approx, ONM) and Score(G, O) for  
different NE-CRR systems across different 
datasets and find that they are indeed correlated. 
CONE thus makes automatic evaluation of NE-
CRR systems possible. By leveraging the widely 
available named entity resources, it reduces the 
need for gold standard annotations in the 
evaluation process. 
4.1 Analysis 
There are two major kinds of errors that affect 
the performance of NE-CRR systems for the full 
NE-CRR task: 
? Missing Named Entity (MNE): If a named 
mention is missing from the system output, 
it is very likely that its nearby nominal and 
anaphoric mentions will be lost, too 
? Incorrectly grouped Named Entity (IGNE): 
Even if the named mention is correctly iden-
tified with its nearby nominal and anaphoric 
mentions to form a chain, it is still possible 
to misclassify the named mentions and its 
co-reference chain 
Consider the following example of these two 
types of errors. Here, the alphabets represent the 
named mentions and numbers represent other 
type of mentions: 
 
Gold standard, G: (A, B, C, 1, 2, 3, 4) 
Output from System 1, O1: (A, B, 1, 2, 3) 
Output from System 2, O2: (A, C, 1, 2, 4), (B, 3) 
O1 shows an example of an MNE error, while 
O2 shows an example of an IGNE error.  
 
Both these types of errors are in fact rooted in 
named mention extraction and grouping 
(NMEG). Therefore, we hypothesize that they 
must be preserved in a NE-CRR system?s output 
139
for the subtask of named mentions extraction and 
grouping (NMEG) and will be reflected in the 
CONE B3 and CONE CEAF metrics that eva-
luate scores for this subtask. Consider the follow-
ing extension of the previous example:  
 
GNM: (A, B, C) 
O1NM: (A, B) 
O2NM: (A, C), (B) 
 
We observe that the MNE error in O1 is pre-
served in O1NM, and the IGNE error in O2 is pre-
served in O2NM. Empirically we sample several 
output files in our experiments and observe the 
same phenomena. Therefore, we argue that it is 
possible to capture the two major kinds of errors 
described by considering only GNM and ONM in-
stead of G and O.  
 
We now provide a more detailed theoretical 
analysis of the CONE metrics. For a given NE-
CRR system and dataset, consider the system 
output O and gold standard annotation G. Let P 
and R indicate precision and recall scores ob-
tained by evaluating O against G, using CEAF. If 
we replace both G and O with their subsets GNM 
and ONM respectively, such that GNM and ONM 
contain only named mentions, we can modify the 
equations for precision and recall for CEAF to 
derive the following equations for precision PNM 
and recall RNM for CONE CEAF: 
??
i
iNMOOSum NM }{
     
??
i
iNMNM GGSum }{
 
??
i
NM
ifi
NM
NM
OSum
M
P }{
)(,     
??
i
NM
ifi
NM
NM
GSum
M
R }{
)(, 
 
The corresponding equations for CONE B3 Pre-
cision are: 
?
?
?
?
i
NM
i
NM
j
ji
NM
NM
OSumO
M
P
}{
2
,
?
?
?
?
i
NM
i
NM
j
ji
NM
RSumR
M
R
}{
'
2
,
 
 
In order to support the hypothesis that CONE 
metrics evaluated using (GNM, ONM) represent an 
effective substitute for standard metrics that use 
(G, O), we compute entity level correlation be-
tween the corresponding CONE and standard 
metrics. For example, in the case of CEAF / 
CONE CEAF Precision, we calculate correlation 
between the following quantities: 
??? }{
)(,
NM
ifi
NM
NM
SSum
M
P?
 and 
??? }{
)(,
SSum
M
P ifi?  
We perform this experiment with the LBJ and 
BART CRR systems on the ACE Phase 2 corpus. 
We illustrate the correlation results in Figure 1.  
 
Figure 1. Correlation between NMP? andP?  - 
Entity Level CEAF Precision 
From Figure 1, we can see that the two 
measures are highly correlated. In fact, we find 
that the Pearson?s correlation coefficient (Soper 
et al, 1917; Cohen, 1988) is 0.73. The points 
lining up on the x-axis and y=1.0 represent very 
small equivalence classes and are a form of noise; 
their removal doesn?t affect this coefficient. To 
show that this strong correlation is not a 
statistical anomaly, we also compute entity-level 
correlation using (Gi - G
NM
i, Oj - O
NM
j) and (Gi, 
Oj) instead of (G
NM
i, O
NM
j) and (Gi, Oj) and find 
that the coefficient drops to 0.03, which is 
obviously not correlated at all.  
We now know NMP? andP?  are highly correlated. 
Assume the correlation is linear, with the 
following equation: 
?? ?? iNMi PP  
where ? and ? are the linear regression 
parameters. 
Thus 
? ? ???? nPnPPP NM
i
iNM
i i
????? ??
   
Here, n is the number of equivalence classes.    
We conclude that the overall CEAF Precision 
and CONE CEAF Precision should be highly 
140
correlated too. We repeat this experiment with 
CEAF / CONE CEAF Recall, B3 / CONE B3 
Precision and B3 / CONE B3 Recall and obtain 
similar results, allowing us to conclude that these 
sets of measures should also be highly correlated. 
We note here some generally accepted 
terminology regarding correlation: If two 
quantities have a Pearson?s correlation 
coefficient greater than 0.7, they are considered  
"strongly correlated", if their correlation is 
between 0.5 and 0.7, they are considered "highly 
correlated", if it is between 0.3 and 0.5, they are 
considered "correlated", and otherwise they are 
considered "not correlated".  
It is important to note that like all automatic 
evaluation metrics, CONE B3 and CONE CEAF 
too can be easily ?cheated?, e.g. a NE-CRR sys-
tem that performs NER and named entity match-
ing well but does not even detect and classify 
anaphora or nominal mentions would nonethe-
less score highly on these metrics. A possible 
solution to this problem would be to create gold 
standard annotations for a small subset of the 
data, call these annotations G?, and report two 
scores: B3 / CEAF (G?), and CONE B3 / CONE 
CEAF (GNM-approx). Discrepancies between these 
two scores would enable the detection of such 
?cheating?. A related point is that designers of 
NE-CRR systems should not optimize for CONE 
metrics alone, since by using GNM-approx (or GNM 
where gold standard annotations are available), 
these metrics are obviously biased towards 
named mentions. This issue can also be ad-
dressed by having gold standard annotations G? 
for a small subset. One could then train a system 
by optimizing both B3 / CEAF (G?) and CONE 
B3 / CONE CEAF (GNM-approx). This can be 
thought of as a form of semi-supervised learning, 
and may be useful in areas such as domain adap-
tation, where we could use some annotated test-
set in a standard domain, e.g. newswire as the 
smaller set and an unlabeled large testset from 
some other domain, such as e-mail or biomedical 
documents. An interesting future direction is to 
monitor the effectiveness of our metrics over 
time. As co-reference resolution systems evolve 
in strength, our metrics might be less effective, 
however this could be a good indicator to discri-
minate on different subtasks the improvements 
gained by the co-reference resolution systems. 
5 Experimental Results 
We present experimental results in support of the 
validity and effectiveness of CONE metrics. As 
mentioned earlier, we used the following four 
publicly available CRR systems: UIUC?s LBJ 
system (L), BART from JHU Summer Workshop 
(B), LingPipe from Alias-i (LP), and OpenNLP 
(OP) (Bengston and Roth, 2007; Versley et al, 
2008; Baldridge and Torton, 2004; Baldwin and 
Carpenter, 2003). All these CRR systems per-
form Noun Phrase co-reference resolution (NP-
CRR), not NE-CRR. So, we must first eliminate 
all equivalences classes that do not contain any 
named mentions. We do so using the SYNERGY 
NER system to separate named mentions from 
unnamed ones. Note that this must not be con-
fused with the use of SYNERGY to produce GNM 
and ONM from G and O respectively. For that task, 
all equivalence classes in G and O already con-
tain at least one named mention and we remove 
all unnamed mentions from each class. This 
process effectively converts the NP-CRR results 
of these systems into NE-CRR ones. We use the 
ACE Phase 2 NWIRE and ACE 2005 English 
datasets. We avoid using the ACE 2004 and 
MUC6 datasets because the UIUC LBJ system 
was trained on ACE 2004 (Bengston and Roth, 
2008), while BART and LingPipe were trained 
on MUC6. There are 29 files in the test set of 
ACE Phrase 2 and 81 files in ACE 2005, sum-
ming up to 120 files with around 50,000 tokens 
with 5000 valid co-reference mentions. Tables 1 
and 2 show the Pearson?s correlation coefficients  
between CONE metric scores of the type 
Score(GNM, ONM) and standard metric scores of 
the type Score(G, O) for combinations of various 
CRR systems and datasets.  
  B3/CONE B3  CEAF/CONE CEAF 
  P R F1 P R F1 
L 0.82 0.71 0.7 0.81 0.71 0.77 
B 0.85 0.5 0.66 0.71 0.61 0.68 
LP 0.84 0.66 0.67 0.74 0.71 0.73 
OP 0.31 0.57 0.61 0.79 0.72 0.79 
Table 1. GNM: Correlation on ACE Phase 2 
  B3/CONE B3  CEAF/CONE CEAF 
  P R F1 P R F1 
L 0.6 0.62 0.62 0.75 0.61 0.68 
B 0.74 0.82 0.84 0.72 0.68 0.67 
LP 0.91 0.65 0.73 0.44 0.57 0.53 
OP 0.48 0.77 0.8 0.54 0.67 0.65 
Table 2. GNM: Correlation on ACE 2005 
 
We observe from Tables 1 and 2 that CONE B3 
and CONE CEAF scores are highly correlated 
141
with B3 and CEAF scores respectively, and this 
holds true for Precision, Recall and F1 scores, for 
all combinations of CRR systems and datasets. 
This justifies our assumption that a system?s per-
formance for the subtask of NMEG is a good 
predictor of its performance for the full task of 
NE-CRR. These correlation coefficients are 
graphically illustrated in Figures 2 and 3. 
We now use our baseline named entity matching 
method to automatically generate estimated gold 
standard annotations GNM-approx and recalculate 
CONE CEAF and CONE B3 scores using GNM-
approx instead of GNM. Tables 3 and 4 show the 
correlation coefficients between the new CONE 
scores and the standard metric scores. 
 
  B3/CONE B3  CEAF/CONE CEAF 
  P R F1 P R F1 
L 0.31 0.23 0.22 0.33 0.55 0.56 
B 0.71 0.44 0.43 0.61 0.63 0.71 
LP 0.57 0.43 0.49 0.36 0.25 0.31 
OP 0.1 0.6 0.64 0.35 0.53 0.53 
Table 3. GNM-approx: Correlation on ACE Phase 2  
  B3/CONE B3  CEAF/CONE CEAF 
  P R F1 P R F1 
L 0.33 0.32 0.42 0.22 0.34 0.36 
B 0.25 0.66 0.65 0.2 0.45 0.37 
LP 0.19 0.33 0.34 0.77 0.68 0.72 
OP 0.26 0.66 0.67 0.28 0.42 0.38 
Table 4. GNM-approx: Correlation on ACE Phase 2 
We observe from Tables 3 and 4 that these corre-
lation factors are encouraging, but not as good as 
those in Tables 1 and 2. All the corresponding 
CONE B3 and CONE CEAF scores are corre-
lated, but very few are highly correlated. We 
should note however that our baseline system to 
create GNM-approx uses relatively simple clustering 
methods and heuristics. It is easy to observe that 
a sophisticated named entity matching system 
would produce a GNM-approx that better approx-
imates GNM than our baseline method, and CONE 
B3 and CONE CEAF scores calculated using this 
GNM-approx would be more correlated with stan-
dard B3 and CEAF scores.  
We note from the above results that correlations 
scores are very similar across different systems 
and datasets. In order to formalize this assertion, 
we calculate correlation scores in a system-
independent and data-independent manner. We 
combine all the data points across all four differ-
ent systems and plot them in Figure 2 and 3 for 
ACE Phase 2 NWIRE corpus and in Figure 4 and 
5 for ACE 2005 corpus respectively. We illu-
strate only F1 scores; the results for precision 
and recall are similar. 
 
Figure 2. Correlation between B3 F1 and CONE 
B3 F1 for all systems on ACE 2 
 
Figure 3. Correlation between CEAF F1 and 
CONE CEAF F1 for all systems on ACE 2 
 
Figure 2 reflects a Pearson?s correlation coeffi-
cient of 0.70, suggesting that all the B3 F1 and 
CONE B3 F1 scores for different systems are 
highly correlated and that CONE B3 F1 does not 
bias towards any particular system. Figure 3 re-
flects a Pearson?s correlation coefficient of 0.83, 
providing similar evidence for the system-
independence of correlation between CEAF F1 
and CONE CEAF F1 scores. Figures 4 and 5 
corresponding to ACE 2005 reflect similar corre-
lation coefficients of 0.89 and 0.82, and thus 
support the idea that the correlations between B3 
F1 and CONE B3 F1, as well as between CEAF 
F1and CONE CEAF F1, are dataset-independent 
in addition to being system-independent.  
 
142
 
Figure 4. Correlation between B3 F1 and CONE 
B3 F1 for all systems on ACE 2005 
 
 
Figure 5. Correlation between CEAF F1 and 
CONE CEAF F1 for all systems on ACE 2005 
6 Application and Discussion  
To illustrate the applicability of CONE metrics, 
we consider the Enron e-mail corpus. It is of a 
different genre than the newswire corpora that 
CRR systems are usually trained on, and no CRR 
gold standard annotations exist for it. Conse-
quently, no CRR systems have been evaluated on 
it so far. We used CONE B3 and CONE CEAF to 
evaluate and compare the NE-CRR performance 
of various CRR systems on a subset of the Enron 
e-mail corpus (Klimt and Yang, 2004) that was 
cleaned and stripped of spam messages. We re-
port the results in Table 5. 
 
  CONE B3  CONE CEAF 
  P R F1 P R F1 
L 0.43 0.21 0.23 0.31 0.17 0.21 
B 0.26 0.18 0.2 0.26 0.16 0.2 
LP 0.61 0.51 0.53 0.58 0.53 0.54 
OP 0.19 0.03 0.05 0.11 0.02 0.04 
Table 5. GNM-approx Scores on Enron corpus 
 
We find that LingPipe is the best of all the sys-
tems we considered, and LBJ is slightly ahead of 
BART in all measures. We suspect that since 
LingPipe is a commercial system, it may have 
extra training resources in the form of non-
traditional corpora. Nevertheless, we believe our 
method is robust and scalable for large corpora 
without NE-CRR gold standard annotations. 
 
7 Conclusion and Future Work 
We propose the CONE B3 and CONE CEAF me-
trics for automatic evaluation of Named Entity 
Co-reference Resolution (NE-CRR). These me-
trics measures a NE-CRR system?s performance 
on the subtask of named mentions extraction and 
grouping (NMEG) and use it to estimate the sys-
tem?s performance on the full task of NE-CRR. 
We show that CONE B3 and CONE CEAF 
scores of various systems across different data-
sets are strongly correlated with their standard B3 
and CEAF scores respectively. The advantage of 
CONE metrics compared to standard ones is that 
instead of the full gold standard data G, they only 
require a subset GNM of named mentions which 
even if not available can be closely approximated 
by using a state-of-the-art NER system and clus-
tering its results. Although we use a simple base-
line algorithm for producing the approximate 
gold standard GNM-approx, CONE B3 and CONE 
CEAF scores of various systems obtained using 
this GNM-approx still prove to be correlated with 
their standard B3 and CEAF scores obtained us-
ing the full gold standard G. CONE metrics thus 
reduce the need of expensive labeled corpora. 
We use CONE B3 and CONE CEAF to evaluate 
the NE-CRR performance of various CRR sys-
tems on a subset of the Enron email corpus, for 
which no gold standard annotations exist and no 
such evaluations have been performed so far. In 
the future, we intend to use more sophisticated 
named entity matching schemes to produce better 
approximate gold standards GNM-approx. We also 
intend to use the CONE metrics to evaluate NE-
CRR systems on new datasets in domains such as 
chat, email, biomedical literature, etc. where very 
few corpora with gold standard annotations exist. 
 
Acknowledgments 
We would like to thank Prof. Ani Nenkova from 
the University of Pennsylvania for her talk about 
automatic evaluation for text summarization at 
the spring 2010 CMU LTI Colloquium and ano-
nymous reviewers for insightful comments.  
143
References  
E. Agirre, L. M?rquez and R. Wicentowski, Eds. 
2007. Proceedings of the Fourth International 
Workshop on Semantic Evaluations (SemEval).   
A. Bagga and B. Baldwin. 1998. Algorithms for Scor-
ing Coreference Chains. Proceedings of LREC 
Workshop on Linguistic Coreference. 
J. Baldridge and T. Morton. 2004. OpenNLP. 
http://opennlp.sourceforge.net/. 
B. Baldwin and B. Carpenter. 2003. LingPipe. Alias-i. 
E. Bengtson and D. Roth. 2008. Understanding the 
Value of Features for Coreference Resolution. Pro-
ceedings of EMNLP. 
J. Cohen. 1988. Statistical power analysis for the be-
havioral sciences. (2nd ed.) 
A.K. Elmagarmid, P.G. Ipeirotis and V.S. Verykios. 
2007. Duplicate Record Detection: A Survey. IEEE 
Transactions on Knowledge and Data Engineering, 
v.19 n.1, 2007.   
J.R. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating Non-local Information into Informa-
tion Extraction Systems by Gibbs Sampling. Pro-
ceedings of ACL. 
B. Klimt and Y. Yang. 2004. The Enron corpus: A 
new dataset for email classification research. Pro-
ceedings of ECML. 
H.W. Kuhn. 1955. The Hungarian method for the 
assignment problem. Naval Research Logistics 
Quarterly, 2(83). 
C. Lin and E. Hovy. 2003. Automatic evaluation of 
summaries using n-gram co-occurrence statistics. 
Proceedings of HLT-NAACL.    
C. Lin and F.J. Och. 2004. Automatic evaluation of 
machine translation quality using longest common 
subsequence and skip-bigram statistics. Proceed-
ings of ACL.  
A. Louis and A. Nenkova. 2009. Automatically Eva-
luating Content Selection in Summarization with-
out Human Models. Proceedings of EMNLP, pages 
306?314, Singapore, 6-7 August 2009. 
X. Luo. 2005. On coreference resolution performance 
metrics. Proceedings of EMNLP. 
MUC-6. 1995. Proceedings of the Sixth Understand-
ing Conference (MUC-6). 
J. Munkres. 1957. Algorithms for the assignment and 
transportation problems. Journal of SIAM, 5:32-38. 
NIST. 2003. The ACE evaluation plan. 
www.nist.gov/speech/tests/ace/index.htm. 
K Papineni, S Roukos, T Ward and W.J. Zhu. 2002. 
BLEU: a method for automatic evaluation of ma-
chine translation. Proceedings of ACL. 
V. Pervouchine, H. Li and B. Lin. 2009. Translitera-
tion alignment. Proceedings of ACL. 
L. Ratinov and D. Roth. 2009. Design Challenges and 
Misconceptions in Named Entity Recognition. 
Proceedings of CoNLL. 
R. Shah, B. Lin, A. Gershman and R. Frederking. 
2010. SYNERGY: a named entity recognition sys-
tem for resource-scarce languages such as Swahili 
using online machine translation. Proceedings of 
LREC Workshop on African Language Technology. 
H.E. Soper, A.W. Young, B.M. Cave, A. Lee and K. 
Pearson. 1917. On the distribution of the correla-
tion coefficient in small samples. Appendix II to 
the papers of "Student" and R. A. Fisher. A co-
operative study. Biometrika, 11, 328-413. 
V. Stoyanov, N. Gilbert, C. Cardie and E. Riloff. 
2009. Conundrums in Noun Phrase Coreference 
Resolution: Making Sense of the State-of-the-Art. 
Proceedings of ACL. 
Y. Versley, S.P. Ponzetto, M. Poesio, V. Eidelman, A. 
Jern, J. Smith, X. Yang and A. Moschitti. 2008. 
BART: A Modular Toolkit for Coreference Reso-
lution. Proceedings of EMNLP. 
M. Vilain, J. Burger, J. Aberdeen, D. Connolly and L. 
Hirschman. 1995. A model-theoretic coreference 
scoring scheme. Proceedings of MUC 6. 
 
 
144
