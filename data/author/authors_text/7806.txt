First Story Detection using a Composite Document 
Representation. 
Nicola Stokes, Joe Carthy, 
Department of Computer Science, 
University College Dublin, 
Ireland. 
{nicola.stokes,joe.carthy}@ucd.ie 
 
 
 
ABSTRACT 
In this paper, we explore the effects of data fusion on First Story 
Detection [1] in a broadcast news domain. The data fusion 
element of this experiment involves the combination of evidence 
derived from two distinct representations of document content in a 
single cluster run. Our composite document representation 
consists of a concept representation (based on the lexical chains 
derived from a text) and free text representation (using traditional 
keyword index terms). Using the TDT1 evaluation methodology 
we evaluate a number of document representation strategies and 
propose reasons why our data fusion experiment shows 
performance improvements in the TDT domain.  
Keywords 
Lexical Chaining, Data Fusion, First Story Detection. 
 
1. INTRODUCTION  
The goal of TDT is to monitor and reorganize a stream of 
broadcast news stories in such a way as to help a user 
recognize and explore different news events that have 
occurred in the data set. First story detection (or online new 
event detection [1]) is one aspect of the detection problem 
which constitutes one of the three technical tasks defined 
by the TDT initiative (the other two being segmentation 
and tracking). Given a stream of news stories arriving in 
chronological order, a detection system must group or 
cluster articles that discuss distinct news events in the data 
stream. The TDT initiative has further clarified the notion 
of topic detection by differentiating between classification 
in a retrospective (Event Clustering) and an online 
environment (First Story Detection). In FSD the system 
must identify all stories in the data stream that discuss 
novel news events. This classification decision is made by 
considering only those documents that have arrived prior to 
the current document being evaluated, forcing the system to 
adhere to the temporal constraints of a real-time news 
stream.  
 
 
 
 
 
 
 
 
 
In other words the system must make an irrevocable 
classification decision (i.e. either the document discusses a 
new event or previously detected event) as soon as the 
document arrives on the input stream. The goal of event 
clustering on the other hand is to partition the data stream 
into clusters of related documents that discuss distinct 
events. This decision can be made after the system has 
considered all the stories in the input stream. 
In addition to defining three research problems 
associated with broadcast news, the TDT initiative also 
attempted to formally define an event with respect to how it 
differs from the traditional IR notion of a subject or a topic 
as defined by the TREC community. An event is defined as 
?something that happens at some specific time and place 
(e.g. an assassination attempt, or a volcanic eruption in 
Greece)?. A topic on the other hand is a ?seminal event or 
activity along with all directly related events and activities 
(e.g. an investigation or a political campaign)? [1]. Initial 
TDT research into event tracking and detection focused on 
developing a classification algorithm to address this subtle 
distinction between an event and a topic. For example 
successful attempts were made to address the temporal 
nature of news stories1 by exploiting the time between 
stories when determining their similarity in the detection 
process [1]. However current research is now focusing on 
the use of NLP techniques such as language modeling [2, 
3], or other forms of feature selection like the identification 
of events based on the domain dependencies between 
words [4], or the extraction of certain word classes from 
stories i.e. noun phrases, noun phrases heads [5]. All these 
techniques offer a means of determining the most 
informative features about an event as opposed to 
classifying documents based on all the words in the 
document. The aim of our research is also based on this 
notion of feature selection. In this paper we investigate if 
the use of lexical chains to classify documents can better 
encapsulate this notion of an event. In particular we look at 
the effect on FSD when a composite document 
representation (using a lexical chain representation and free 
text representation) is used to represent events in the TDT 
domain.  
                                                           
1
 Stories closer together on the input stream are more likely to 
discuss the same event than stories further apart on this stream. 
In sections 2 and 3 we describe the first component of our 
composite document representation derived from lexical 
chains, with a subsequent description of FSD classification 
based on our data fusion strategy in Section 4. The 
remaining sections of this paper give a detailed account of 
our experimental results, concluding with a discussion of 
their significance in terms of two general criteria for 
successful data fusion. 
 
2. LEXICAL CHAINING 
A lexical chain is a set of semantically related words in a 
text. For example in a document concerning cars a typical 
chain might consist of the following words {vehicle, 
engine, wheel, car, automobile, steering wheel}, where 
each word in the chain is directly or indirectly related to 
another word by a semantic relationship such as holonymy, 
hyponymy, meronymy and hypernymy.  
When reading any text it is obvious that it is not 
merely made up of a set of unrelated sentences, but that 
these sentences are in fact connected to each other in one of 
two ways cohesion and coherence.   As Morris and Hirst 
[6] point out cohesion relates to the fact that the elements 
of a text ?tend to hang together?.   Whilst coherence refers 
to the fact that ?there is sense in the text?.  Obviously 
coherence is a semantic relationship and needs 
computationally expensive processing for identification, 
however cohesion is a surface relationship and is hence 
more accessible. As indicated by Halliday and Hasan [7] 
cohesion can be roughly classified into three distinct 
classes, reference, conjunction and lexical cohesion.  
Conjunction is the only class, which explicitly shows the 
relationship between two sentences, ?I have a cat and his 
name is Felix?. Reference and lexical cohesion on the other 
hand indicate sentence relationships in terms of two 
semantically same or related words. In the case of 
reference, pronouns are the most likely means of conveying 
referential meaning. For example in the following 
sentences, ? ?Get inside now!? shouted the teacher. When 
nobody moved, he was furious?. In order for the reader to 
understand that ?the teacher? is being referred to by the 
pronoun ?he? in the second sentence, they must refer back 
to the first sentence. Lexical cohesion on the other hand 
arises from the selection of vocabulary items and the 
semantic relationships between them. For example,  ?I 
parked outside the library, and then went inside the 
building to return my books?, where cohesion is represented 
by the semantic relationship between the lexical items 
?library?, ?building? and ?books?. For automatic 
identification of these relationships it is far easier to work 
with lexical cohesion than reference because less 
underlying implicit information is needed to discover the 
relationship between the above pronoun and the word it 
references.  Hence lexical cohesion is used as a linguistic 
device for investigating the discourse structure of texts and 
lexical chains have been found to be an adequate means of 
exposing this discourse structure.  These lexical chains 
have many practical applications in IR and computational 
linguistics such as hypertext construction [8], automatic 
document summarization [9], the detection of 
malapropisms within text [10], as a term weighting 
technique capturing the lexical cohesion in a text [11], as a 
means of segmenting text into distinct blocks of self 
contained text [12]. For the purpose of this project we 
exploit three such applications: 
1. We use lexical chains as a means of exploring and 
presenting the most prevalent topics discussed in news 
stories. 
2. A valuable side effect of lexical chain creation is that 
the words of a text are automatically disambiguated. 
3. Because lexical chains disambiguate words based on 
the context in which they occur, lexical chains also 
address two linguistic problems synonymy and 
polysemy, which hinder the effectiveness of traditional 
IR systems such as the vector space model. 
 
3. CHAIN FORMATION ALGORITHM 
In general the first task of an IR system is to execute a set 
of text operations (e.g. stemming, removal of stopwords) to 
reduce the complexity of a full text representation of a 
document into a more manageable set of index terms. 
Although these index terms are a subset of the original 
representation, their purpose is to adequately represent the 
semantic content of the original document in a more 
concise manner. This is a difficult NLP task, as natural 
language frequently does not obey the principle of 
compositionality where the meaning of the whole can be 
strictly determined from its parts. So in order to derive the 
correct representation of a text, we need to determine the 
interpretation of a word or phase in the context in which it 
occurs i.e. before the original text is manipulated into a set 
of index terms. The creation of lexical chains which is 
described below, aims to capture this additional textual 
information while still maintaining a manageable 
representation size.   
Firstly each term contained in a particular document 
is dealt with in chronological order. Then each subsequent 
word is added to an existing lexical chain or becomes the 
seed of a new chain, in much the same manner as the 
clustering of documents. A stronger criterion than simple 
semantic similarity is imposed on the addition of a term to 
a chain, where terms must be added to the most recently 
updated (semantically related) chain. This favors the 
creation of lexical chains containing words that are in close 
proximity within the text, prompting the correct 
disambiguation of a word based on the context in which it 
was used. We use WordNet to determine the semantic 
relatedness between a candidate word and the words of a 
chain. If we view WordNet as a large semantic network of 
nodes (meanings) inter-related by semantic relations 
(meronymy, hyponymy, etc.), then finding a relationship 
between two words in the chaining process involves 
activating the network of one node and observing the 
activity of the other in this activated network.  
                                          
 
 
 
 
                                             
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: Shows expanded document terms ?car? and ?trunk? 
and their semantic relatedness. 
So far we have talked abstractly about how to determine if 
a word is semantically related to a chain. To explain this 
fully it is first necessary to discuss the structure of the 
WordNet thesaurus, which is used to determine this 
semantic connection or closeness between words in a text. 
In WordNet, nouns, verbs, adjectives, and adverbs are 
arranged into synsets (group of synonymous words e.g. cat, 
feline, tabby), which are further organized into a set of 
lexical source files by syntactic category. In our case we 
are only interested in the noun index and data files, because 
the verb file in WordNet has no relation with the three 
other files (noun, adverb and adjective files), and the 
adverb file has only unidirectional relations with the 
adjective file.  So each word in a particular document is 
searched for in the noun index file, if it is not found then 
we make the assumption that this word is not a noun and 
hence will play no further part in the chaining process. If 
the word is found then it will be represented by a unique set 
of synset numbers, where each synset number represents a 
particular sense associated with that word. Each synset 
number points to the position in the noun data file where 
words related to this sense of the word are stored with a 
gloss, and sample sentence using this word. Words related 
to a particular sense are associated with it by several 
different semantic relations, such as hyponymy (kind-of, 
lorry/vehicle), hypernymy (is-a, vehicle/car), holonymy 
(has-part, tree/branch) and meronymy (part-of, engine/car). 
As shown in Figure 1, each sense associated with a word is 
expanded using WordNet (in reality these senses and senses 
related to them are represented by synset numbers). This 
example of the chain formation process shows us that the 
word ?car? is related to the word ?trunk? by the fact that ?car 
trunk?, one of the senses of ?trunk?, is a meronymy of 
?automobile? which is a possible sense of  ?car?. In this way 
both words have been successfully disambiguated so all 
redundant senses belonging to each word are eliminated 
and ?car? is added to the chain containing ?trunk?. This 
chain may also contain other semantically related words 
pertaining to the topic of an automobile e.g. {car, trunk, 
engine, vehicle?}. The chain formation process is 
continued in this way until all the words in a particular 
document (in our case nouns) have been chained.  Any 
words that remain unchained or ambiguous after this 
chaining process are eliminated from our chain word 
representation based on the following hypothesis: 
?The occurrence of words in a text which fail to participate 
in the overall cohesive structure of a text (i.e. remain 
unchained) is purely coincidental. Consequently these 
words are considered irrelevant in describing the general 
topic of a document.? 
This implies that our lexical chaining strategy also provides 
us with an automatic means of selecting the most salient 
features of a particular news story. So when all redundant 
words have been removed in this manner, all remaining 
chains are then merged into a single chain containing all the 
synset numbers from each individual chain involved in this 
process. This representation is a semantic representation as 
opposed to a syntactic representation (in the case of a ?bag 
of words? representation) because it contains concepts (i.e. 
synset numbers) rather than simple terms to represent the 
content of a document.   
 
 
 
SENSE OF WORD 
KIND-OF (HYPONYMY) 
HAS PART (HOLONYMY) 
PART OF (MERONYMY) 
TRUNK 
CAR TRUNK 
AUTOMOBILE 
TREE 
TRUNK 
ELEPHANT 
TRUNK 
BARK 
VEHICLE 
CAR 
TRAIN CAR AUTOMOBILE 
CABLE CAR 
The final stage of our combined document 
representation strategy involves collecting all free text 
words for each document and storing them in a set of index 
files. So effectively our composite document representation 
used in the detection process (described in the next section) 
consists of two weighted vectors, a chain vector and an 
ordinary term vector, where both chain words and free text 
words are weighted simply in terms of the frequency in 
which they occur in a document. 
 
4. DETECTION ALGORITHM USING THE   
FUSION METHOD 
Online Detection or First Story Detection is in essence a 
classification problem where documents arriving in 
chronological order on the input stream are tagged with a 
?YES? flag if they discuss a previously unseen news event, 
or a ?NO? flag when they discuss an old news topic. 
However unlike detection in a retrospective environment a 
story must be identified as novel before subsequent stories 
can be considered. The single-pass clustering algorithm 
bases its clustering methodology on the same assumption, 
the general structure of which is summarised as follows.  
1. Convert the current document into a weighted chain 
word vector and a weighted free text vector. 
2. The first document on the input stream will become the 
first cluster. 
3. All subsequent incoming documents are compared 
with all previously created clusters up to the current 
point in time. A comparison strategy is used here to 
determine the extent of the similarity between a 
document and a cluster. In our IR model we use sub-
vectors to describe our two distinct document 
representations. This involves calculating the closeness 
or similarity between the chain word vectors and free 
text vectors for each document/cluster comparison 
using the standard cosine similarity measure (used in 
this variation of the vector space model to compute the 
cosine of the angle between two weighted vectors). 
The data fusion element of this experiment involves 
the combination of two distinct representations of 
document content in a single cluster run i.e. j equals 2 
in equation (1). So the overall similarity between a 
document D and a cluster C is a linear combination of 
the similarities for each sub-vector formally defined as: 
 
where Sim(X, Y) is the cosine similarity measure for 
two vectors X and Y,  and w is a coefficient that biases 
the weight of evidence each document representation j, 
contributes to the similarity measure.  
 
 
4. When the most similar cluster is found a thresholding 
strategy [13] is used to discover if this similarity 
measure is high enough to warrant the addition of that 
document to the cluster and the classification of the 
current document as an old event. If this document 
does not satisfy the similarity condition set out by the 
thresholding methodology then the document is 
declared as discussing a new event, and this document 
will form the seed of a new cluster. 
5. This clustering process will continue until all 
documents in the input stream have been classified. 
 
5. EXPERIMENTAL RESULTS  
A number of experiments were conducted on the TDT-1 
broadcast news collection [1]. The results of these 
experiments were used to observe the effects on first story 
detection when lexical chains are used in conjunction with 
free text as a combined document classifier. The main aim 
of the experiments was to determine if lexical chains are a 
suitable document representation when classifying news 
stories in the TDT domain. The official TDT evaluation 
requires that the system output is a declaration (a YES or 
NO flag) for each story processed. These declarations are 
then used to calculate two system errors percentage misses 
and false alarms. Misses occur when the system fails to 
detect the first story discussing a new event and false 
alarms occur when a document discussing a previously 
detected event is classified as a new event. 
5.1 System Descriptions 
Three distinct detection systems TRAD, CHAIN and 
LexDetect are examined in the following set of 
experiments. The TRAD system [13], our benchmark 
system in these experiments is a basic FSD system that 
classifies news stories based on the syntactic similarity 
between documents and clusters. The design of this system 
is based on a traditional vector space model which 
represents documents as a vector, each component of which 
corresponds to a particular word and who?s value reflects 
the frequency of that word in the document. Classification 
of a new event occurs in a similar manner to that described 
in Section 4, the most important difference between the two 
methods is that a single free text representation is used to 
express document content, rather than a combined 
representation. A Time Window [13] of length 30 is 
employed in the TRAD, CHAIN and LexDetect systems.  
The design of our second system LexDetect has 
been described in detail in sections 3 and 4. The 
dimensionality of LexDetect (80 words) remains static 
through out these experiments. Using the current method of 
lexical chain creation, just under 72% of documents 
contained greater than or equal to 30 chained words. We 
therefore normalized the length of chain word 
representations by imposing a chain dimensionality value 
?
=
?=
k
j
jjj )CDSimwCDSim
1
)1(,(),(
of 30 on all LexDetect schemes2. In theory it is possible to 
vary the length of the free text representation in our 
combined representation however in these experiments all 
schemes contain free text representations of length 50, 
since optimal performance is achieved for TRAD when 
dimensionality 50 is used. The final system parameter to be 
varied in these experiments is the weighting coefficient wj 
used in
 
equation (1). The design of our third system 
CHAIN like TRAD, involves the use of a singular 
document representation. However this document 
representation contains chain words only rather than free 
text terms, and so the dimensionality of the system must be 
30. 
5.2 The Data Fusion Experiment 
 From the results shown in Figure 2 (a Detection 
Error Tradeoff Graph where points closer to the origin 
indicate better overall performance), we deduce that a 
marginal increase in system effectiveness can be achieved 
when lexical chain representations are used in conjunction 
with free text representations in the detection process. In 
particular, we see that the miss rate of our FSD system 
LexDetect decreased with little or no impact to the false 
alarm rate of the system.  
 
DET graph showing %Misses and %False Alarms for 
TRAD_50, LexDetect and CHAIN systems 
0
10
20
30
40
50
60
70
80
90
100
0 20 40 60 80 100
%Misses
%
Fa
ls
e
 A
la
rm
s
Lex_Detect
TRAD_50
CHAIN
 
Figure 2: The effect on performance when a weighted 
combined document representation is used. 
 
 
                                                           
2
 An IR ?system? and an IR ?scheme? are used in this context to 
describe two different concepts. An IR system refers to the 
physical implementation of an IR algorithm, which can have 
various operational modes or various parameter settings. The 
same IR system may be used to execute different IR schemes by 
adjusting these parameters [20]. 
Optimal performance for the LexDetect system (as 
shown in Figure 2) was found when a weighted 
combination of evidence was used. This involved treating 
our free text representation as weaker evidence during the 
detection process. Results shown in Figure 3 contrast the 
effect on LexDetect performance when both the chain and 
free text representations are given equal weight (Lex) and 
when the weight of the free text representation is halved 
(LexDetect). This is an interesting result as similar 
experiments using composite document representations to 
improve search system performance based on ranking, only 
experienced optimal effectiveness when they allowed free 
text evidence to bias the retrieval process [14, 15]. This 
prompted us to question the necessity of the free text 
component of our composite representation, however 
results show that system performance degrades when this 
element of document content is excluded. This is due to the 
inability of WordNet to correlate the relationship between 
proper nouns and other semantically related concepts i.e. 
{Bill Clinton, US president}, which are often crucial in 
representing journalistic event identity because they reflect 
the ?who, what, where, when and how? of a news story. 
Our final experiment involves plotting TRAD_80 
against LexDetect shown in Figure 4. The aim of this 
experiment is to prove that the increase in system 
effectiveness observed when a composite document 
representation is used can be attributed solely to the 
combination of evidence derived from our free text and 
chain representations rather than as a consequence of 
increasing the dimensionality of the system to 80 features. 
As the DET graph in Figure 4 shows, our LexDetect system 
still outperforms our TRAD system under conditions of 
equal dimensionality. 
DET graph showing % Misses and %False Alarms for 
LexDetect and Lex 
0
10
20
30
40
50
60
70
80
90
100
0 20 40 60 80 100
% Misses
%
 F
al
se
 A
la
rm
s
Lex
LexDetect
 
Figure 3: The effect on performance when equal weight is 
given to both representations (Lex) in contrast to a weighted 
combined document representation (LexDetect). 
DET graph showing % Misses and %False Alarms for LexDetect 
and TRAD_80
0
10
20
30
40
50
60
70
80
90
100
0 20 40 60 80 100
% Misses
%
 F
al
se
 A
la
rm
s
TRAD_80
LexDetect
 
Figure 4: The effect on performance when equal 
dimensionality of 80 is given to both the LexDetect and TRAD 
systems.  
 
6. CRITERIA FOR SUCCESSFUL DATA 
FUSION 
In the previous section our results showed that when a 
chain word representation is used in conjunction with a free 
text representation of a document, improvements in FSD 
effectiveness are observed. However these results fail to 
provide any concrete reasoning as to why data fusion under 
these particular conditions work. There are many papers in 
the data fusion literature, which attempt to explain why 
certain data fusion experiments succeed where others have 
failed. Many of these papers look at the effects of 
combining specific sources of evidence such as the 
combination of rank retrieval lists, multiple searches or 
multiple queries. However Ng and Kantor [16] have tried to 
formulate some general preconditions for successful data 
fusion involving non-specific sources of evidence. 
The first of these criteria is based on the 
dissimilarity between two sources of evidence. 
1. Dissimilarity: Data fusion between operationally very 
similar IR systems may not give better performance. 
To calculate the level of dissimilarity between our FSD 
systems described in Section 5, we now define two ratios 
based on the number of common relevant and common 
non-relevant tagged documents between two distinct 
systems. The number of relevant tagged documents, 
|r1?r2| is defined as the number of documents that were 
correctly classified (as a new or old event) by both systems. 
The total number of relevant documents, r1+r2 is the sum 
of the number of correctly classified documents for each 
system. |n1?n2| and n1+n2 are similarly defined in terms 
of  the number of incorrectly classified documents returned 
by both systems (i.e. missed events or wrongly detected 
new events) as shown in equation 3.  
 
)3(2
)2(2
21
21
21
21
nn
nn
N
rr
rr
R
 overlap
 overlap
+
?
+
?
?
=
?
=
 
 
The results for this experiment are shown in tables 1 
and 2 below. We can see that in general the relevant 
document overlap Roverlap between the pair-wise similarities 
of all four systems is between 85% and 92%, the most 
similar systems being not surprisingly our two TRAD 
schema which differ only in the length of their classifiers. 
The pair-wise similarities Noverlap of all four systems 
regarding non-relevant document classifications exhibit a 
similar trend of high similarity between the TRAD and 
LexDetect systems. However the most important point to 
be taken from these sets of results regards the fact that our 
CHAIN and TRAD systems exhibit the lowest relevant and 
non-relevant document overlap of all our pair-wise 
comparisons. This is an important and encouraging result 
as it shows that our chain word representations (used in 
CHAIN) is sufficiently dissimilar to our simple ?bag of 
words? representation (used in TRAD) to contribute 
additional evidence to a combination experiment involving 
both these representations. In particular this satisfaction of 
Ng and Kantor?s dissimilarity criteria explains why 
marginal improvements in system performance were 
observed in our data fusion experiment. 
 
Table 1: Relevant document overlap between FSD systems. 
ROVERLAP LexDetect TRAD_50 TRAD_80  CHAIN 
LexDetect 1    
TRAD_50 0.85 1   
TRAD_80 0.85 0.92 1  
CHAIN 0.56 0.52 0.53 1 
 
Table 2: Non-relevant document overlap between FSD 
systems. 
NOVERLAP LexDetect TRAD_50 TRAD_80  CHAIN 
LexDetect 1    
TRAD_50 0.67 1   
TRAD_80 0.68 0.82 1  
CHAIN 0.58 0.51 0.53 1 
 
 
The second criteria defined for successful data 
fusion regards efficacy or the quality of the individual 
sources of evidence before they are combined in the data 
fusion process. 
2. Efficacy: Data fusion between a capable IR system and 
a very incapable IR system may not give better 
performance. 
In our data fusion experiment in Section 5 we observed that 
our CHAIN system was our worst performing FSD system. 
So as the efficacy criteria suggests a better performing 
chain word representation is needed before further 
improvements are observed in our combination system 
LexDetect. 
  
7. FUTURE WORK 
There are many factors which can affect the final chain 
word representation of a document, ranging from the 
greedy nature of the chaining algorithm, to the effects 
caused when varying degrees of freedom are used in this 
algorithm (i.e. system parameters such as the amount of 
activation used in WordNet). However the single biggest 
influence on the quality of the resultant lexical chains is the 
knowledge source used to create them. In other words the 
quality of our lexical chain formation is directly dependent 
on the comprehensiveness/complexity of the thesaurus used 
to create them. In the case of WordNet, there are a number 
of structural inadequacies that degrade the effectiveness of 
our chain representation: 
1. Missing semantic links between related words. 
2. Inconsistent semantic distances between different 
concepts. 
3. Overloaded synsets such as ?being? which are 
connected to a large number of synsets. These types of 
synsets cause spurious chaining, where an unrelated 
word is added to a chain based on a weak yet 
semantically close relationship with one of these 
overloaded synsets  (a special case of 2.).  
4. No means of correlating the relationship between 
proper nouns and other noun phrases (see Section 5.2). 
5. The level of sense granularity used to define word 
meanings in WordNet is often too fine for the chain 
formation process. 
All of these factors play a part in reducing the 
effectiveness of the disambiguation process and the 
comprehensiveness and accuracy of the final chain 
representation. A number of these weaknesses are 
discussed in previous work on lexical chaining [8, 12]. 
However the last two cases are particularly important when 
considering the similarity between documents and clusters 
in the detection process. As explained in Section 6.2 
lexical chains are an incomplete means of representing 
events in a topic detection application since they fail to 
contain information on the proper nouns involved in the 
discourse structure of the text.  
The last case is more a comment on the unsuitability of 
WordNet as a knowledge source in this application rather 
than as a reference to any specific weakness in its design. 
For example consider two distinct documents which both 
contain the word ?city? in their respective chain 
representations. WordNet defines three distinct meanings 
or senses of this word: 
 
? An incorporated administrative district establish by a 
state charter. 
? A large densely populated municipality. 
? An urban center.  
 
When disambiguating a word like ?city? in the chain 
formation process this level of sense distinction is 
unnecessary. In fact if our aforementioned documents have 
chosen two different yet closely related definitions of this 
word (i.e. different synset numbers) then these documents 
will be considered less related than they actually are.  Other 
research efforts in the lexical chaining area have suggested 
?cleaning? WordNet [8] of rare senses or using some 
additional knowledge source in the chaining process that 
could biases the suitability of certain senses in particular 
contexts3.  In future work we hope to address this problem 
by considering the use of collocation information like noun 
pairs such as ?physician/hospital? or ?Gates/Microsoft? in 
the chain formation process. Using such information will 
help to smooth out the discrepancies in semantic distances 
between concepts and help detect missing semantic 
relationships between these concepts. This occurrence 
information could also reduce the sensitivity of the 
detection process to fine levels of sense granularity if such 
information was used when determining the similarity 
between two document representations. So effectively this 
technique would eliminate the need for a composite 
representation in the identification of novel events in a 
news stream. Instead the data fusion element of our system 
would involve supplementing our knowledge source 
WordNet with word co-occurrence information in the chain 
formation process. 
 
8. CONCLUSIONS 
A variety of techniques for data fusion have been proposed 
in IR literature. Results from data fusion research have 
suggested that significant improvements in system 
effectiveness can be obtained by combining multiple 
sources of evidence of relevancy such as document 
representations, query formulations and search strategies. 
                                                           
3
 Recent editions of WordNet now contain information on the 
probability of use of a word based on polysemy. WordNet 
researchers noted the direct relationship between the increase in 
the frequency of occurrence of a word and the number of 
distinct meanings it has. This frequency value could also be 
used in the ?cleaning? process.  
In this paper we investigated the impact on FSD 
performance when a composite document representation is 
used in this TDT task. Our results showed that a marginal 
increase in system effectiveness could be achieved when 
lexical chain representations were used in conjunction with 
free text representations. In particular, we saw that the miss 
rate of our FSD system LexDetect, decreased with little or 
no impact to the false alarm rate of the system. When a 
weighted combination of evidence was used on the same 
system this improvement was even more apparent. From 
these results we deduced that using our chain word 
representation as stronger evidence in the classification 
process could lead to improved performance. Based on Ng 
and Kantor?s dissimilarity criteria for successful data fusion 
we attributed the success of our composite document 
representation to the fact that a chain word classifier is 
sufficiently dissimilar to a simple ?bag of words? classifier 
to contribute additional evidence to a combination 
experiment involving both these representations. In future 
experiments, we expect an even greater improvement in 
FSD effectiveness as we continue to refine our lexical 
chain representation.  
 
9. ACKNOWLEDGMENT 
This project is funded by an Enterprise Ireland research 
grant [SC/1999/083]. 
 
10. REFERENCES 
[1] R. Papka, J. Allan, Topic Detection and Tracking: Event 
Clustering as a basis for first story detection, Kluwer 
Academic Publishers, pp. 97-126, 2000. 
[2] Y. Yang, T. Ault, T. Pierce, Combining multiple learning 
strategies for effective cross validation, the Proceedings of 
the 17th International Conference on Machine Learning 
(ICML), pp. 1167-1182, 2000. 
[3] F. Walls, H. Jin, S.Sista, R. Schwartz, Topic Detection in 
broadcast news, In the proceedings of the DARPA Broadcast 
News Workshop, pp. 193-198, San Francisco, CA: Morgan 
Kaufman Publishers Inc, 1999. 
[4] F. Fukumoto, Y. Suzuki, Event Tracing based on Domain 
Dependency, In the proceedings of the 23rd ACM SIGIR 
Conference, Athens, pp. 57-63, 2000. 
 
 
 
 
 
 
 
[5] V. Hatzivassiloglou, L. Gravano, A. Maganti, An 
Investigation of Linguistic Features and Clustering 
Algorithms for Topical Document Clustering, In the 
proceedings of the 23rd ACM SIGIR Conference, Athens, pp. 
224-231, 2000. 
[6] J. Morris, G. Hirst, Lexical Cohesion by Thesaural Relations 
as an Indicator of the Structure of Text, Computational 
Linguistics 17(1), March 1991.  
[7] M. Halliday, R. Hasan, Cohesion in English, Longman: 
1976. 
[8] S. J. Green, Automatically Generating Hypertext By 
Comparing Semantic Similarity, University of Toronto, 
Technical Report number 366, October 1997. 
[9] R. Barzilay, M. Elhadad, Using Lexical Chains for Text 
Summarization, In Proceedings of the Intelligent Scalable 
Text Summarization Workshop (ISTS?97), ACL, Madrid, 
1997. 
[10] D. St-Onge, Detection and Correcting Malapropisms with 
Lexical Chains, Dept. of Computer Science, University of 
Toronto, M.Sc Thesis, March 1995. 
[11] M. A. Stairmand, W. J. Black, Conceptual and Contextual 
Indexing using WordNet-derived Lexical Chains, In the 
Proceedings of BCS IRSG Colloquium, pp. 47-65, 1997.    
[12] M. Okumura, T. Honda, Word sense disambiguation and text 
segmentation based on lexical cohesion, In Proceedings of 
the Fifteen Conference on Computational Linguistics 
(COLING-94), volume 2, pp. 755-761, 1994. 
[13] N. Stokes, P. Hatch, J. Carthy, Topic Detection, a new 
application for lexical chaining?, In the Proceedings of the 
22nd BCS IRSG Colloquium on Information Retrieval, pp. 
94-103, 2000.  
[14] E. Fox, G. Nunn, W. Lee, Coefficients for combining concept 
classes in a collection, In the proceedings of the 11th ACM 
SIGIR Conference, pp. 291-308, 1988. 
[15] J. Katzer, M. McGill, J. Tessier, W. Frakes, P. DasGupta, A 
study of the overlap among document representations, 
Information Technology: Research and Development, 
1(4):261-274, 1982.  
[16] K. Ng, P. Kantor, An Investigation of the preconditions for 
effective data fusion in IR: A pilot study, In the Proceedings 
of the 61th Annual Meeting of the American Society for 
Information Science 1998. 
 
 
 
 
 
 
 
 
 
 
Spoken and Written News Story Segmentation using Lexical Chains 
 
Nicola Stokes. 
Department of Computer Science, 
University College Dublin, Ireland. 
Nicola.Stokes@ucd.ie 
 
 
 
Abstract 
In this paper we describe a novel approach to 
lexical chain based segmentation of broadcast 
news stories. Our segmentation system        
SeLeCT is evaluated with respect to two other 
lexical cohesion based segmenters TextTiling 
and C99. Using the Pk and WindowDiff 
evaluation metrics we show that SeLeCT   
outperforms both systems on spoken news 
transcripts (CNN) while the C99 algorithm 
performs best on the written newswire        
collection (Reuters). We also examine the   
differences between spoken and written news 
styles and how these differences can affect 
segmentation accuracy. 
1 Introduction 
Text segmentation can be defined as the automatic iden-
tification of boundaries between distinct textual units 
(segments) in a textual document. The aim of early 
segmentation research was to model the discourse struc-
ture of a text, thus focusing on the detection of fine-
grained topic shifts, at a clausal, sentence or pas-
sage/subtopic level (Hearst 1997). More recently with 
the introduction of the TDT initiative (Allan et al 1998) 
segmentation research has concentrated on the detection 
of coarse-grained topic shifts resulting in the identifica-
tion of story boundaries in news feeds. In particular, un-
segmented broadcast news streams represent a challeng-
ing real-world application for text segmentation ap-
proaches, since the success of other tasks such as topic 
tracking or first story detection depend heavily on the 
correct identification of distinct and non-overlapping 
news stories. Most approaches to story segmentation use 
either Information Extraction techniques (cue phrase 
extraction), techniques based on lexical cohesion analy-
sis or a combination of both (Reynar 1998; Beeferman 
et al 1999). More recently promising results have also 
been achieved though the use of Hidden Markov model-
ing techniques, which are commonly used in speech 
recognition applications (Mulbregt et al 1999). 
      In this paper we focus on lexical cohesion based 
approaches to story segmentation. Lexical cohesion is 
one element of a broader linguistic device called cohe-
sion which is describe as the textual quality responsible 
for making the elements of a text appear unified or con-
nected. More specifically, lexical cohesion ?is the cohe-
sion that arises from semantic relationships between 
words? (Morris, Hirst 1991).  With respect to segmenta-
tion, an analysis of lexical cohesion can be used to indi-
cate portions of text that represent single topical units or 
segments i.e. they contain a high number of semanti-
cally related words. Almost all approaches to lexical 
cohesion based segmentation examine patterns of syn-
tactic repetition in the text e.g. (Reynar 1998; Hearst 
1997; Choi 2000). However, there are four additional 
types of lexical cohesion present in text: synonymy 
(car, automobile), specialization/generalization (horse, 
stallion), part-whole/whole-part (politicians, govern-
ment) and statistical co-occurrences (Osama bin Laden, 
World Trade Center). Lexical chaining based ap-
proaches to text segmentation, on the other hand, ana-
lyse all aspects of lexical cohesion in text. Lexical 
chains are defined as groups of semantically related 
words that represent the lexical cohesive structure of a 
text e.g. {flower, petal, rose, garden, tree}. In our lexi-
cal chaining implementation, words are clustered based 
on the existence of statistical relationships and lexico-
graphical associations (provided by the WordNet online 
thesaurus) between terms in a text. 
      There have been three previous attempts to tackle 
text segmentation using lexical chains. The first by 
Okumara and Honda (1994) involved an evaluation 
based on five Japanese texts, the second by Stairmand 
(1997) used twelve general interest magazine articles 
and the third by Kan et al (1998) used fifteen Wall 
Street Journal and five Economist articles. All of these 
attempts focus on sub-topic rather than story segmenta-
tion. In contrast, this paper investigates the usefulness of 
lexical chains as a technique for determining story seg-
ments in spoken and written broadcast news streams. In 
Section 2, we explain how this technique can be refined 
                                                               Edmonton, May-June 2003
                                                 Student Research Workshop , pp. 49-54
                                                         Proceedings of HLT-NAACL 2003
to address story segmentation. In Section 3, we compare 
the segmentation performance of our lexical chaining 
algorithm with two other well known lexical cohesion 
based approaches to segmentation; namely TextTiling 
(Hearst 1997) and C99 (Choi 2000). Finally we examine 
the grammatical differences between written and spoken 
news media and show how these differences can be util-
ized to improve spoken transcript segmentation accu-
racy.  
2 SeLeCT: Segmentation using Lexical 
Chains on Text  
In this section we present our topic segmenter SeLeCT. 
This system takes a concatenated stream of text and 
returns a segmented stream of distinct news reports. The 
system consists of three components a ?Tokeniser?, a 
?Chainer? which creates lexical chains, and a ?Detector? 
that uses these chains to determine news story bounda-
ries. More detailed descriptions of the ?Tokeniser? and 
?Chainer? components are reported in Stokes et al 
(2003).  
2.1  The Tokeniser 
The objective of the chain formation process is to build 
a set of lexical chains that capture the cohesive structure 
of the input stream. Before work can begin on lexical 
chain identification, each sample text is processed by a 
part-of-speech tagger. Morphological analysis is then 
performed on these tagged texts; all plural nouns are 
transformed into their singular form, adjectives pertain-
ing to nouns are nominalized and all sequences of words 
that match grammatical structures of compound noun 
phrases are extracted. This idea is based on a simple 
heuristic proposed by Justeson and Katz (Justeson, Katz 
1995), which involves scanning part-of-speech tagged 
texts for patterns of adjacent tags that commonly match 
proper noun phrases like ?White House aid?, ?PLO 
leader Yasir Arafat?, and WordNet noun phrases like 
?red wine? or  ?act of god?. Since the likelihood of find-
ing exact syntactic matches of these phrases elsewhere 
in a story is low, we include a fuzzy string matching 
function in the lexical chainer to identify related phrases 
like George_Bush  President_Bush.  
2.2 The Lexical Chainer 
The aim of the Chainer is to find relationships between 
tokens (nouns, proper nouns, compound nouns, nomi-
nalized adjectives) in the data set using the WordNet 
thesaurus and a set of statistical word associations, and 
to then create lexical chains from these relationships 
with respect to a set of chain membership rules. The 
chaining procedure is based on a single-pass clustering 
algorithm, where the first token in the input stream be-
comes the head of the first lexical chain. Each subse-
quent token is then added to the most recently updated 
chain that it shares the strongest semantic relationship1 
with. This process is continued until all tokens in the 
text have been chained. Our chaining algorithm is simi-
lar to one proposed by St Onge (1995) for the detection 
of malapropisms in text, however statistical word asso-
ciations and proper nouns were not considered in his 
original implementation. 
2.3  Boundary Detection 
The final step in the segmentation process is to partition 
the text into its individual news stories based on the 
patterns of lexical cohesion identified by the Chainer in 
the previous step. Our boundary detection algorithm is a 
variation on one devised by Okumara and Honda 
(Okumara, Honda 1994) and is based on the following 
observation: 
?Since lexical chain spans (i.e. start and end points) 
represent semantically related units in a text, a high 
concentration of chain begin and end points between 
two adjacent textual units is a good indication of a 
boundary point between two distinct news stories? 
We define boundary strength w(n, n+1) between each 
pair of adjacent textual unit in our test set, as the sum of 
the number of lexical chains whose span ends at para-
graph n and the number of chains that begin their span 
at paragraph n+1. When all boundary strengths between 
adjacent paragraphs have been calculated we then get the 
mean of all the non-zero cohesive strength scores. This 
mean value then acts as the minimum allowable boundary 
strength that must be exceeded if the end of textual unit n 
is to be classified as the boundary point between two news 
stories.  
       Finally these boundary strength scores are ?cleaned? 
using an error reduction filter which removes all bound-
ary points which are separated by less than x number of 
textual units from a higher scoring boundary, where x is 
too small to be a ?reasonable? story length. This filter 
has the effect of smoothing out local maxima in the 
boundary score distribution, thus increasing segmenta-
tion precision. Different occurrences of this error are 
illustrated in Figure 1. Regions A and C represent clusters 
of adjacent boundary points. In this situation only the 
boundary with the highest score in the cluster is retained 
as the true story boundary. Therefore the boundary which 
scores 6 is retained in region A while in region C both 
points have the same score so in this case we consider the 
last point in region C to be the correct boundary position. 
Finally, the story boundary in region B is also eliminated 
because it is situated too close to the boundary points in 
                                                 
1
 Repetition is the strongest cohesive relationship, followed by 
synonymy, and then statistical associations, generaliza-
tion/specialization and part-whole/whole-part relationships. 
C B A 
region C and it has a lower score than either of those 
boundaries.  
0 0
4
6 5
0 0 0 0 0 0 0
3
0 0 0
5 5
0 0
 
Figure 1. Diagram shows different types of segmentation 
error; numbers greater than zero are possible boundary 
positions, while zero scores represent no story boundary 
point between these two textual units. 
3 Segmentation Evaluation 
In this section we give details of two news story seg-
mentation test sets, some evaluation metrics used to 
determine segmentation accuracy, and the performance 
results of the SeLeCT, C99 and TextTiling algorithms. 
3.1 News Segmentation Test Collections  
Both the CNN and Reuters test collections referred to in 
this paper contain 1000 randomly selected news stories 
taken from the TDT1 corpus. These test collections 
were then reorganized into 40 files each consisting of 25 
concatenated news stories. Consequently, all 
experimental results in Section 3.3 are averaged scores 
generated from the individual results calculated for each 
of the 40 samples. By definition a segment in this 
context refers to a distinct news story, thus eliminating 
the need for a set of human-judged topic shifts for 
assessing system accuracy. 
3.2 Evaluation Metrics 
There has been much debate in the segmentation litera-
ture regarding appropriate evaluation metrics for esti-
mating segmentation accuracy.  Earlier experiments 
favored an IR style evaluation that measures perform-
ance in terms of recall and precision. However these 
metrics were deemed insufficiently sensitive when try-
ing to determine system parameters that yield optimal 
performance. The most widely used evaluation metric is 
Beeferman et al?s (1999) probabilistic error metric 
Pk, which calculates segmentation accuracy with respect 
to three different types of segmentation error: false posi-
tives (falsely detected segments), false negatives 
(missed segments) and near-misses (very close but not 
exact boundaries). However, in a recent publication 
Pevzner and Hearst (2002) highlight several faults with 
the Pk metric. Most notable they criticize Pk for its un-
fair penalization of false negatives over false positives 
and its over-penalization of near-misses. In their paper, 
the authors proposed an alternative error metric called 
WindowDiff which rectifies these problems. 
3.3 Story Segmentation Results 
In this section we present performance results for each 
segmenter on both the CNN and Reuters test sets with 
respect to the aforementioned evaluation metrics. As 
explained in Section 3, we determine the effectiveness 
of our SeLeCT system with respect to two other lexical 
cohesion based approaches to segmentation, namely the 
TextTiling (Hearst 1997) and C99 algorithms (Choi 
2000)2. We also include average results from a random 
segmenter that returned 25 random boundary positions 
for each of the 40 files in both test sets. These results 
represent a lower bound on segmentation performance. 
All results in this section are calculated using para-
graphs as the basic unit of text. Since both our test sets 
are in SGML format, we consider the beginning of a 
paragraph in this context to be indicated by a speaker 
change tag in the CNN transcripts and a paragraph tag 
in the case of the Reuters news stories.  
 Table 1: Pk and WD (WindowDiff) values for segmenta-
tion systems on CNN and Reuters Collections. 
Table 1 summarizes the results of the CNN data set 
for each segmentation system evaluated with respect to 
the four metrics. All values for these metrics range from 
0 to 1 inclusively, where 0 represents the lowest possi-
ble measure of system error. From these results we ob-
serve that the accuracy of our SeLeCT segmentation 
algorithm is greater than the accuracy of C99, 
TextTiling or the Random segmenter for both evalua-
tion metrics on the CNN ?spoken? data set. As for the 
Reuters segmentation performance, the C99 algorithm 
significantly outperforms both the SeLeCT and 
TextTiling systems. We also observe that the Win-
dowDiff metric penalizes systems more than Pk, how-
ever the overall ranking of the systems with respect to 
these error metrics remains the same. With regard to the 
SeLeCT system, optimal performance was achieved 
when only patterns of lexical repetition were examined 
during the boundary detection phase, thus eliminating 
the need for an examination of lexicographical and sta-
tistical relationships between tokens in the text.            
                                                 
2
 We use Choi?s java implementations of TextTiling and C99 
available for free download at www.cs.man.ac.uk/~choif. In 
(Choi 2000) boundaries are hypothesized using sentences as 
the basic unit of text; however both C99 and TextTiling can 
take advantage of paragraph information when the input con-
sists of one paragraph per line.  
CNN  Reuters  System 
Pk WD Pk WD 
SeLeCT 0.25 0.253 0.191 0.207 
TextTiling 0.259 0.299 0.221 0.244 
C99 0.294 0.351 0.128 0.148 
Random 0.421 0.48 0.490 0.514 
A similar conclusion was reported by Hearst (1997) and 
Min-Yen et al (1998); however neither of these ap-
proaches included statistical word associations in their 
chaining process. 
4 Written and Spoken Text Segmentation 
It is evident from the results of our segmentation ex-
periments on the CNN and Reuters test collections that 
system performance is dependant on the type of news 
source being segmented i.e. spoken texts are more diffi-
cult to segment. This disagreement between result sets 
is a largely unsurprising outcome as it is well docu-
mented by the linguistic community that written and 
spoken language modes differ greatly in the way in 
which they convey information. At a first glance, it is 
obvious that written texts tend to use more formal and 
verbose language than their spoken equivalents. How-
ever, although CNN transcripts share certain spoken text 
characteristics (see Section 4.1), they lie somewhere 
nearer written documents on a spectrum of linguistic 
forms of expression, since they contain a mixture of 
speech styles ranging from formal prepared speeches 
from anchor people, politicians, and correspondents, to 
informal interviews/comments from ordinary members 
of the public. Furthermore, spoken language is also 
characterized by false starts, hesitations, back-trackings, 
and interjections; however information regarding pro-
sodic features and these characteristics are not repre-
sented in CNN transcripts. In the next section we look at 
some grammatical differences between spoken and writ-
ten text that are actually evident in CNN transcripts. In 
particular, we look at the effect that these differences 
have on parts of speech distributions and how these im-
pact segmentation performance. 
4.1 Lexical Density    
One method of measuring the grammatical intricacy of 
speech compared to written text, is to calculate the lexi-
cal density of the language being used. The simplest 
measure of lexical density, as defined by Halliday 
(1995), is the ?the number of lexical items (content 
words) as a portion of the number of running words 
(grammatical words)?. Halliday states that written texts 
are more lexically dense while spoken texts are more 
lexically sparse. In accordance with this, we observe 
based on part-of-speech tag information that the CNN 
test set contains 8.58% less lexical items than the 
Reuters news collection.3 
                                                 
3
 Lexical items included all nouns, adjectives and verbs, ex-
cept for function verbs like modals and auxiliary verbs. In-
stead these verbs form part of the grammatical item lexicon 
with all remaining parts of speech. Our CNN and Reuters data 
sets consisted of 43.68% and 52.26% lexical items respec-
tively.   
Halliday explains that this difference in lexical den-
sity between the two modes of expression can be attrib-
uted to the following observation: 
?Written language represents phenomena as products, 
while spoken language represents phenomena as proc-
esses.? 
In real terms this means that written text tends to con-
veys most of its meaning though nouns (NN) and adjec-
tives (ADJ), while spoken text conveys it though ad-
verbs (ADV) and verbs (VB). To illustrate this point 
consider the following written and spoken paraphrase of 
the same information:  
Written: Improvements/NN in American zoos 
have resulted in better living/ADJ conditions 
for their animal residents/NN. 
Spoken: Since/RB American zoos have been 
improved/VB the animals residing/VB in them 
are now/RB living/VB in better conditions. 
Although this example is a little contrived, it shows 
that in spite of changes to the grammar, by and large the 
vocabulary has remained the same. More specifically, 
these paraphrases illustrate how the products in the writ-
ten version, improvements, resident, and living, are con-
veyed as processes in spoken language though the use 
of verbs. The spoken variant also contains more ad-
verbs; a grammatical necessity that provides cohesion to 
text when processes are being described in verb clauses.  
As explained in Section 2.2 the SeLeCT lexical 
chainer only looks at cohesive relationships between 
nouns and nominalized adjectives in a text. This ac-
counts partly for SeLeCT?s lower performance on the 
CNN test set, since the extra information conveyed 
though verbs in spoken texts is ignored by the lexical 
chainer. However since C99 and TextTiling use all parts 
of speech in their analysis of the text, the replacement of 
products with processes is not the reason for a similar 
deterioration in their performance. More specifically, 
both C99 and TextTiling rely on stopword lists to iden-
tifying spurious inter-segment links between function 
words that by their nature do not indicate common topi-
cality. For the purpose of their original implementation 
their stopwords lists contained mostly pronouns, deter-
miners, adverbs, and function verbs such as auxiliary 
and modal verbs. However, we have observed that the 
standard set of textual function verbs is not enough for 
speech text processing tasks and that their lists should 
be extended to include other common ?low information? 
verbs. These types of verbs are not necessarily charac-
terized by large frequency counts in the spoken news 
collection like the domain specific phrases to report or 
to comment. Instead these verbs tend to have no 
?equivalent? nominal form, like the verbs ?to let? ?to 
hear? ?to look? or ?to try?. 
To test this observation we re-ran C99 and 
TextTiling experiments on the Reuters and CNN      
C B A 
collections, using only nouns, adjectives, nominalized 
verbs (provided by the NOMLEX (Meyers et al 1998)), 
and nominalized adjectives as input. Our results show 
that there is a significant decrease in WindowDiff error 
for the C99 system on both the CNN collection (a de-
crease from 0.351 to 0.268) and the Reuters collection 
(a decrease from 0.148 to 0.121). Similarly, we observe 
an improvement in the WindowDiff based performance 
of the TextTiling system on the CNN data set (a de-
crease from 0.299 to 0.274). However, we observe a 
marginal fall in performance on the Reuters data set (an 
increase from 0.244 to 0.247). These results illustrate 
the increased dominance of verbs in spoken text and the 
importance of function verb removal by our verb nomi-
nalization process for CNN segmentation performance.   
4.2 Reference and Conjunction in Spoken Text 
A picture paints a thousand words, they say, and since 
news programme transcripts are accompanied by visual 
and audio cues in the news stream, there will always be 
a loss in communicative value when transcripts are in-
terpreted independently. As stated in Section 4.1, it is 
well known that conversational speech is accompanied 
by prosodic and paralinguistic contributions, facial ex-
pressions, gestures, intonation etc., which are rarely 
conveyed in spoken transcripts. However there are also 
explicit (exophoric) references in the transcript to events 
occurring outside the lexical system itself. These exo-
phoric references in CNN transcripts relate specifically 
to audio references like speaker change, musical inter-
ludes, background noise; and visual references like 
event, location and people shots in the video stream.  
We believe that this property of transcribed news is an-
other reason for the deterioration in segmentation per-
formance on the CNN test collection. 
Solving endophoric (anaphora and cataphora) and 
exophoric reference has long been recognized as a very 
difficult problem, which requires pragmatic, semantic 
and syntactic knowledge in order to be solved. However 
there are simple heuristics commonly used by text seg-
mentation algorithms that in our case can be used to 
take advantage of the increased presence of reference in 
spoken text. One such heuristic is based on the observa-
tion that when common referents like personal and pos-
sessive pronouns, and possessive determiners appear at 
the beginning of a sentence, this indicates that these 
referents are linked in some way to the previous textual 
unit (in our case the previous paragraph). The resolution 
of these references is not of interest to our algorithm but 
the fact that two textual units are linked in this way 
gives the boundary detection process an added advan-
tage when determining story segments in the text. An 
analysis of conjunction (another form of textual cohe-
sion) can also be used to provide the detection process 
with useful evidence of related paragraphs, since para-
graphs that begin with conjunctions (because, and, or, 
however, nevertheless) and conjunctive phrases (in the 
mean time, in addition, on the other hand) are particu-
larly useful in identify cohesive links between units in 
conversational/interview sequences in the transcript. 
4.3 Refining SeLeCT Boundary Detection  
In Section 2.3 we describe in detail how the boundary 
detection phrase uses lexical chaining information to 
determine story segments in a text. One approach to 
integrating referential and conjunctive information with 
the lexical cohesion analysis provided by the chains is 
to remove all paragraphs from the system output that 
contain a reference or conjunctive relationship with the 
paragraph immediately following it in the text. The 
problem with this approach is that Pk and WindowDiff 
errors will increase if ?incorrect? segment end points are 
removed that represented near system misses rather than 
?pure? false positives. Hence, we take a more measured 
approach to integration that uses conjunctive and refer-
ential evidence in the final filtering step of the detection 
phrase, to eliminate boundaries in boundary clusters 
(Section 2.3) that cannot be story end points in the news 
stream.  Figure 2 illustrates how this technique can be 
used to refine the filtering step. Originally, the boundary 
with score six in region A would have been considered 
the correct boundary point. However since a conjunctive 
phrase links the adjacent paragraphs at this boundary 
position in the text, the boundary which scores five is 
deemed the correct boundary point by the algorithm. 
0 0
4
6 5
0 0 0 0 0 0 0
3
0 0 0
5 5
0 0
 
Figure 2 Illustrates how cohesion information can help 
SeLeCT?s boundary detector resolve clusters of possible 
story boundaries. 
Using this technique and the verb nominalization proc-
ess described in section 4.1 on both news media collec-
tions, we observed an improvement in SeLeCT system 
performance on the CNN data set (a decrease in error 
from 0.253 to 0.225), but no such improvement on the 
Reuters collection. Again the ineffectiveness of this 
technique on the Reuters results can be attributed to 
differences between the two modes of language expres-
sion, where conjunctive and referential relationships 
resolve 51.66% of the total possible set of boundary 
points between stories in the CNN collection and only 
22.04% in the Reuters collection. In addition, these ref-
erences in the Reuters articles mostly occur between 
sentences in a paragraph rather than between paragraphs 
in the text thus provide no additional cohesive          
information. A summary of the improved results dis-
cussed in this section is shown in Table 2. 
CNN WD Score Reuters WD Score System 
Before After Before After 
 SeLeCT 0.253 0.225 0.207 0.209 
 C99 0.351 0.268 0.148 0.121 
 TextTiling 0.299 0.274 0.244 0.247 
Table 2: Improvements in system performance as a result 
of system modifications discuss in Sections 4.1 and 4.3. 
5 Conclusions 
In this paper we have presented a lexical chaining based 
approach to coarse-grained segmentation of CNN news 
transcripts and concatenated Reuters newswire articles. 
We have shown that the performance of our SeLeCT 
system exceeds that of the TextTiling and C99 systems 
when detecting topic shifts in CNN transcripts. How-
ever the results of a similar experiment on Reuters news 
stories showed that the C99 system outperformed all 
other systems on a written news collection. Overall, 
lower CNN segmentation results were attributed to the 
information loss caused by prosodic and paralinguistic 
characteristics of speech and grammatical differences 
between written and spoken modes of expression. Fur-
ther experiments showed that by limiting the input of all 
the segmentation systems to nouns, adjectives, and 
nominalized verbs and adjectives, the effect of these 
grammatical differences on CNN segmentation per-
formance was significantly reduced. Additional SeLeCT 
performance improvements were also achieved by using 
referential and conjunctive relationships as additional 
evidence of cohesion in the boundary detection step. In 
future experiments we plan to compare SeLeCT?s per-
formance on written and spoken news texts with two 
recently proposed systems, U00 (Utiyama 2001) and 
CWM (Choi 2001), which have marginally outper-
formed the C99 algorithm on Choi?s (2000) test corpus. 
Acknowledgements  
The support of Enterprise Ireland is gratefully acknowledged. 
Also I wish to thank Marti Hearst for providing us with a ver-
sion of the WindowDiff evaluation software and Joe Carthy for 
invaluable comments. 
References 
Allan J., J. Carbonell, G. Doddington, J. Yamron, Y. 
Yang. Topic Detection and Tracking Pilot Study Fi-
nal Report. In the proceedings of the DARPA Broad-
casting News Workshop, pp. 194-218, 1998.   
Beeferman D., A. Berger, and J. Lafferty. Statistical 
models for text segmentation. Machine Learning, 
(34):177-210. 1999. 
Choi F., Advances in domain independent linear text 
segmentation. In Proceedings of NAACL?00. 2000. 
Choi F., P. Wiemer-Hastings, J. Moore. Latent semantic 
analysis for Text Segmentation. In proceedings 
EMNLP 2001, pp.109-117, 2001.  
Halliday M.A.K., Spoken and Written Language.Oxford 
University Press, 1985. 
Hearst M., TextTiling: Segmenting Text into Multi-
Paragraph Subtopic Passages, Computational Lin-
guistics, 23 (1):33-64, 1997.  
Justeson, J. S., S.M. Katz., Technical terminology: some 
linguistic properties and an algorithm for identifica-
tion in text. Natural Language Engineering (11): 9-
27, 1995. 
Kan Min-Yen, J. L. Klavans, K. R. McKeown. Linear 
Segmentation and Segment Relevance. In the pro-
ceedings of WVLC-6, pp. 197-205, 1998. 
Kozima H., Text segmentation based on similarity be-
tween words. In Proceedings of ACL-93, pp. 286-
288, 1993. 
Meyers A., et al Using NOMLEX to produce nominali-
zation patterns for information extraction. In Pro-
ceedings of the COLING-ACL Workshop on Com-
putational Treatment of Nominals, 1998. 
Morris J., G. Hirst, Lexical Cohesion by Thesaural 
Relations as an Indicator of the Structure of Text, 
Computational Linguistics 17(1), 1991. 
Okumura M., T. Honda, Word sense disambiguation 
and text segmentation based on lexical cohesion. In 
proceedings of COLING-94, pp. 755-761, 1994. 
Pevzner, L., and M. Hearst, A Critique and Improve-
ment of an Evaluation Metric for Text Segmentation, 
Computational Linguistics, 28 (1):19-36, 2002.  
Reynar J., Topic Segmentation: Algorithms and 
Applications, Ph.D. thesis, Dept. Computer and 
Information Science, UPenn, 1998.  
Stairmand M.A, A Computational Analysis of Lexical 
Cohesion with Applications in IR, PhD Thesis, Dept. 
of Language Engineering, UMIST. 1996. 
St-Onge D., Detecting and Correcting Malapropisms 
with Lexical Chains, Dept. of Computer Science, 
University of Toronto, M.Sc. Thesis, 1995.  
Stokes N., J. Carthy, A.F. Smeaton. SeLeCT: A Lexical 
Cohesion Based News Story Segmentation System. 
Technical Report CS02-03, Dept. of Computer Sci-
ence, University College Dublin, 2003. 
Utiyama M., H. Isahara. A statistical model for domain-
independent text segmentation. In proceedings of 
ACL-2001, pp.491-498, 2001.  
van Mulbregt P., I. Carp, L. Gillick, S. A. Lowe, J. P. 
Yamron. Segmentation of Automatically Transcribed 
Broadcast News Text, In Proceedings of the DARPA 
Broadcast News Workshop, 1999. 
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 617?624
Manchester, August 2008
Investigating Statistical Techniques for Sentence-Level Event
Classification
Martina Naughton
School of Computer Science,
University College Dublin,
Belfield, Dublin 4, Ireland
Nicola Stokes
NICTA Victoria Laboratory,
University of Melbourne,
Victoria, Australia
Joe Carthy
School of Computer Science
University College Dublin
Belfield, Dublin 4, Ireland
Abstract
The ability to correctly classify sentences
that describe events is an important task for
many natural language applications such
as Question Answering (QA) and Sum-
marisation. In this paper, we treat event
detection as a sentence level text classifi-
cation problem. We compare the perfor-
mance of two approaches to this task: a
Support Vector Machine (SVM) classifier
and a Language Modeling (LM) approach.
We also investigate a rule based method
that uses hand crafted lists of terms derived
from WordNet. These terms are strongly
associated with a given event type, and can
be used to identify sentences describing in-
stances of that type. We use two datasets in
our experiments, and evaluate each tech-
nique on six distinct event types. Our re-
sults indicate that the SVM consistently
outperform the LM technique for this task.
More interestingly, we discover that the
manual rule based classification system is
a very powerful baseline that outperforms
the SVM on three of the six event types.
1 Introduction
Event detection is a core Natural Language Pro-
cessing (NLP) task that focuses on the automatic
identification and classification of various event
types in text. This task has applications in au-
tomatic Text Summararisation and Question An-
swering (QA). For example, event recognition is
a core task in QA since the majority of web user
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
questions have been found to relate to events and
situations in the world (Saur?? et al, 2005). For
complex questions such as How many people were
killed in Baghdad in March?, QA systems often
rely on event detection systems to identify all rel-
evant events in a set of documents before formu-
lating an answer. More recently, much research in
summarisation has focused on the use of phrasal
concepts such as events to represent sentences in
extractive summarisation systems. Specifically,
(Filatova and Hatzivassiloglou, 2004) use event-
based features to represent sentences and shows
that this approach improves the quality of the final
summaries when compared with a baseline bag-of-
words approach.
In this paper, we investigate the use of statistical
methods for identifying the sentences in a docu-
ment that describe one or more instances of a spec-
ified event type. We treat this task as a text classi-
fication problem where each sentence in a given
document is either classified as containing an in-
stance of the target event or not. We view this
task as a filtering step in a larger pipeline NLP ar-
chitecture (e.g. a QA system) which helps speed
up subsequent processing by removing irrelevant,
non-event sentences.
Two event detection approaches are explored in
this paper. More specifically, we train a Support
Vector Machine (SVM) using a variety of term,
lexical and additional event based features to en-
code each train/test instance. We also adopt a prob-
abilistic language modeling approach that captures
how text within sentences that describe event in-
stances is likely to be generated. We estimate a
series of models using three well-known smooth-
ing approaches, including LaPlace, Jelinek-Mercer
and Absolute Discounting Smoothing. Their over-
all behavior on classification performance is ex-
617
amined. One advantage of language modeling for
text classification is that instead of explicitly pre-
computing features and selecting a subset based
on arbitrary decisions (as is often the case with
standard classification learning approaches such as
an SVM), the language modeling approach simply
considers all terms occurring in the text as candi-
date features, and implicitly considers the contri-
bution of every feature in the final model. Thus,
language modeling approaches avoids a potentially
error-prone feature selection process.
Event classification at a sentence level is a chal-
lenging task. For example, if the target event is
?Die?, we want our system to extract sentences
like ?5 people were killed in the explosion.? and
?A young boy and his mother were found dead on
Wednesday evening.?. However, it also needs to
detect complex cases like: ?An ambulance rushed
the soldier to hospital, but efforts to save him
failed.? and reject instances like ?Fragmentation
mines have a killing range of 100 feet.?. It seems
intuitive that a na??ve system that selects only sen-
tences that contain terms with senses connected
with death like ?kill?, ?die? or ?execute? as pos-
itive instances would catch many positive cases.
However, there are instances where this approach
would fail. In this work we evaluate the effective-
ness of such a shallow NLP approach, by devel-
oping a manual rule based system that finds sen-
tences connected to a target event type using a
hand crafted list of terms created with senses found
in WordNet.
We use two datasets in our experiments. The
first is the ACE 2005 Multilingual Training Corpus
(Walker et al, 2006) that was annotated for 33 dif-
ferent event types. However, within the ACE data
the number of instances referring to each event
type is somewhat limited. For this reason, we se-
lect the six types with the highest frequency in the
data. These include ?Die?, ?Attack?, ?Transport?,
?Meet?, ?Injure? and ?Charge-indict? types. The
second corpus is a collection of articles from the
Iraq Body Count (IBC) database
1
annotated for
the ?Die? event. This dataset arose from a larger
humanitarian project that focuses on the collec-
tion of fatalities statistics from unstructured news
data. We use this additional corpus to augment the
amount of data used for training and testing the
?Die? event type, and to investigate the use of extra
training data on overall classification performance.
1
http://www.iraqbodycount.org/
Overall, our results demonstrate that the trained
SVM proves to be more effective than the LM
based approach for this task across all event types.
We also show that our baseline system, a hand
crafted rule-based system, performs surprisingly
well. The remainder of this paper is organised as
follows. Section 2 covers related work. We con-
tinue with details of the datasets used in the exper-
iments in Section 3. Section 4 describes our event
classification approaches while Section 5 presents
their results. We conclude with a discussion of ex-
perimental observations and opportunities for fu-
ture work in Section 6.
2 Background and Related Work
Event detection, in the context of news stories, has
been an active area of research for the best part of
ten years. For example, the NIST sponsored Topic
Detection and Tracking (TDT) project, which be-
gan in 1998 investigated the development of tech-
nologies that could detect novel events in seg-
mented or unsegmented news streams, and track
the progression of these event over time (Allan et
al., 1998). Although this project ended in 2004,
event detection is still investigated by more re-
cently established projects such as the Automatic
Content Extraction (ACE) program, and in do-
mains outside of news text such as Biomedical
Text Processing (Murff et al, 2003).
The aim of the TDT First Story Detection (FSD)
or New Event Detection (NED) task was to flag
documents that discuss breaking news stories as
they arrive on a news stream. Dragon Systems
adopted a LM approach to this task (Allan et al,
1998; Yamron et al, 2002) building discriminator
topic models from the collection and representing
documents using unigram term frequencies. Then
they used a single-pass clustering algorithm to de-
termine the documents that describe new events.
The overall goal of the TDT Event Tracking task
was to track the development of specific events
over time. However, these TDT tasks were some-
what restrictive in the sense that detection is car-
ried out at document level. Our work differs from
TDT research since event detection is performed
at a sentence level where the amount of data to
build discriminate models for recognising event in-
stances is far more limited.
The goal of the ACE Event Detection and
Recognition task is to identify all event instances
(as well as the attributes and participants of each
618
Table 1: ACE Corpus Statistics
Die Injure Attack Meet Transport Charge-Indict
Number of Documents 154 50 235 84 181 43
Avg. document length 29.19 29.74 29.62 31.41 32.78 14.81
Avg. event instances per document 2.31 1.64 3.55 1.55 2.55 1.72
Avg. event instances per sentence 1.13 1.11 1.12 1.02 1.08 1.03
Table 2: IBC Corpus Statistics
IBC Corpus
Number of Documents 332
Number of Sources 77
Avg. document length 25.98
Avg. events per document 4.6
Avg. events per sentence 1.14
instance) of a pre-specified set of event types. An
ACE event is defined as a specific occurence in-
volving zero or more ACE entities
2
, values and
time expressions. Two spans of text are used to
identify each event: the event trigger and the event
mention. An event trigger or anchor is the word
that most clearly expresses its occurrence. In many
cases, this will be the main verb in the event men-
tion. It can also appear as a noun (?The meeting
lasted 5 hours.?) or an adjective (?the dead men
. . . ?). The event mention is the sentence that de-
scribes the event. Even though the task of iden-
tifying event mentions is not directly evaluated in
ACE, systems still need to identify them so that
the various attributes and participants within the
mention can be extracted. The algorithms evalu-
ated in this paper can also be applied to the detec-
tion of event mentions that contain the ACE events.
Overall five sites participated in this task in 2005.
The most similar work to that describe in this pa-
per is detailed in (Ahn, 2006), who treats the task
of finding all event triggers (used to identify each
event) as a word classification task where the task
is to classify every term in a document with a la-
bel defined by 34 classes. Features used included
various lexical, WordNet, dependency and related
entity features.
3 Corpora
The ACE 2005 Multilingual Corpus was annotated
for Entities, Relations and Events. It consists of
articles originating from six difference sources in-
cluding Newswire (20%), Broadcast News (20%),
Broadcast Conversation (15%), Weblog (15%),
2
An ACE Entity is an entity identified using guidelines
outlined by ACE Entity Detection and Recognition task.
Usenet Newsgroups (15%) and Conversational
Telephone Speech (15%). Statistics on the docu-
ments in this collection are presented in Table 1.
We evaluate our methods on the following event
types which have a high number of instances in the
collection: ?Die?, ?Attack?, ?Transport?, ?Meet?,
?Injure? and ?Charge-indict?.
The data we use from the IBC database con-
sists of Newswire articles gathered from 77 differ-
ent news sources. Statistics describing this dataset
are contained in Table 2. To obtain a gold standard
set of annotations for articles in the IBC corpus,
we asked ten volunteers to mark up all the ?Die?
event instances. To maintain consistency across
both datasets, events in the IBC corpus were iden-
tified in a manner that conforms to the ACE an-
notation guidelines. In order to approximate the
level of inter-annotation agreement achieved for
the IBC corpus, two annotators were asked to an-
notate a disjoint set of 250 documents. Inter-rater
agreements were calculated using the kappa statis-
tic that was first proposed by (Cohen, 1960). Using
the annotated data, a kappa score of 0.67 was ob-
tained, indicating that while the task is difficult for
humans the data is still useful for our training and
test purposes. Discrepancies were adjudicated and
resolved by an independent volunteer.
4 Event Detection as Classification
We treat the task of determining whether a given
sentence describes an instance of the target event
as a binary text classification task where it is as-
signed one of the following classes:
? On-Event Sentence: a sentence that contains
one or more instances of the target event type.
? Off-Event Sentence: a sentence that does not
contain any instances of the target event type.
4.1 A Machine Learning Approach
In an attempt to develop a gold standard ap-
proach for this task we use Support Vector Ma-
chines (SVM) to automatically classify each in-
stance as either an ?on-event? or ?off-event? sen-
tence. SVMs have been shown to be robust in
619
classification tasks involving text where the dimen-
sionality is high (Joachims, 1998). Each sentence
forms a train/test instance for our classifier and is
encoded using the following set of features.
Terms: Stemmed terms with a frequency in the
training data greater than 2 were used as a term
feature. Stopwords were not used as term features.
Noun Chunks: All noun chunks (e.g. ?ameri-
can soldier?) with a frequency greater than 2 in the
training data were also used as a feature.
Lexical Information: The presence or absence of
each part of speech (POS) tag and chunk tag was
used as a feature. We use the Maximum Entropy
POS tagger and Chunker that are available with the
C&C Toolkit (Curran et al, 2007). The POS Tag-
ger uses the standard set of grammatical categories
from the Penn Treebank and the chunker recog-
nises the standard set of grammatical chunk tags:
NP, VP, PP, ADJP, ADVP and so on.
Additional Features: We added the following
additional features to the feature vector: sen-
tence length, sentence position, presence/absence
of negative terms (e.g. no, not, didn?t, don?t, isn?t,
hasn?t), presence/absence of a modal terms (e.g.
may, might, shall, should, must, will), a looka-
head feature that indicates whether the next sen-
tence is an event sentence, a look-back feature in-
dicating whether or not the previous sentence is
an event sentence and the presence/absence of a
time-stamp. Time-stamps were identified using in-
house software developed by the Language Tech-
nology Group at the University of Melbourne
3
.
In the past, feature selection methods have been
found to have a positive effect on classification ac-
curacy of text classification tasks. To examine the
effects of such techniques on this task, we use In-
formation Gain (IG) to reduce the number of fea-
tures used by the classifier by a factor of 2.
4.2 Language Modeling Approaches
The Language modeling approach presented here
is based on Bayesian decision theory. Consider the
situation where we wish to classify a sentence s
k
into a category c ? C = {C
1
. . . . . . C
|C|
}. One
approach is to choose the category that has the
largest posterior probability given the training text:
c
?
= argmax
c?C
{Pr(c|s
k
)} (1)
Specifically, we construct a language model
LM(c
i
) for each class c
i
. All models built
3
http://www.cs.mu.oz.au/research/lt/
are unigram models that use a maximum like-
lihood estimator to approximate term probabili-
ties. According to this model (built from sentences
{s
1
. . . s
m
} belonging to class c
i
in the training
data) we can calculate the probability that term w
was generated from class c
i
as:
P (w|LM(c
i
)) =
tf(w, c
i
)
|c
i
|
(2)
where tf(w, c
i
) is the term frequency of term w in
c
i
(that is, {s
1
. . . s
m
}) and |c
i
| is the total number
of terms in class c
i
. We make the usual assump-
tions that word co-occurences are independent. As
a result, the probability of a sentence is the product
of the probabilities of its terms. We calculate the
probability that a given test sentence s
k
belongs to
class c
i
as follows:
P (s
k
|LM(c
i
)) =
?
w?s
k
P (w|LM(c
i
)) (3)
However, this model will generally under-
estimate the probability of any unseen word in the
sentence, that is terms that do not appear in the
training data used to build the language model. To
combat this, smoothing techniques are used to as-
sign a non-zero probability to the unseen words,
which improves the accuracy of the overall term
probability estimation. Many smoothing meth-
ods have been proposed over the years, and in
general, they work by discounting the probabili-
ties of seen terms and assign this extra probability
mass to unseen words. In IR, it has been found
that the choice of smoothing method significantly
affects retrieval performance (Zhai and Lafferty,
2001; Kraaij and Spitters, 2003). For this reason,
we experiment with the Laplace, Jelinek-Mercer
and Absolute Discounting Smoothing methods,
and compare their effects on classification perfor-
mance in Section 5.
For this classification task, we normalise all
numeric references, locations, person names and
organisations to ?DIGIT?, ?LOC?, ?PER?, and
?ORG? respectively. This helps to reduce the di-
mensionality of our models, and improve their
classification accuracy, particular in cases where
unseen instances of these entities occur in the test
data.
4.3 Baseline Measures
We compare the performance our ML and LM ap-
proaches to the following plausible baseline sys-
tems: Random assigns each instance (sentence)
620
randomly to one of the possible classes. While
Majority Class Baseline assigns each instance to
the class that is most frequent in the training data.
In our case, this is the ?off-event? class.
According to the ACE annotation guidelines
4
event instances are identified in the text by find-
ing event triggers that explicitly mark the occur-
rence of the event. As a result, each event instance
tagged in our datasets have a corresponding trigger
that the annotators used to identify it. For exam-
ple, terms like ?killing?, ?death? and ?murder? are
common triggers used to identify the ?Die? event
type. Therefore, we expect that a system that se-
lects sentences containing one or more candidate
trigger terms as positive ?on-event? sentences for
a given event type, would be a suitable baseline
for this task. To investigate this further we add the
following baseline system:
Manual Trigger-Based Classification: For each
event type, we use WordNet to manually create
a list of terms that are synonyms or hyponyms
(is a type of) of the event type. For example, in
the case of the ?Meet? and ?Die? events common
trigger terms include {?encounter?, ?visit?, ?re-
unite?} and {?die?, ?suicide?, ?assassination?} re-
spectively. We classify each sentence for a given
event type as follows: if a sentence contains one
or more terms in the trigger list for that event type
then it is assigned to the ?on-event? class for that
type. Otherwise it is assigned to the ?off-event?
class. Table 3 contains the number of trigger terms
used for each event
5
.
Table 3: Trigger term lists for the six event types used in the
experiments.
Event Type Number of triggers terms
Die 29
Transport 14
Meet 12
Injure 10
Charge-Indict 8
Attack 8
5 Evaluation Methodology & Results
A standard measure for classification performance
is classification accuracy. However for corpora
where the class distribution is skewed (as is the
case in our datasets where approx. 90% of the in-
4
Available at http://projects.ldc.upenn.
edu/ace/annotation/
5
The lists of the trigger terms used for each event type are
available at http://inismor.ucd.ie/
?
martina/
stances belong to the ?off-event? class) this mea-
sure can be misleading. So instead we have used
precision, recall and F1 to evaluate each technique.
If a is the number of sentences correctly classi-
fied by a system to class i, b is the total num-
ber of sentences classified to class i by a system,
and c is the total number of human-annotated sen-
tences in class i. Then the precision and recall
for class i can be defined as follows: Prec
i
=
a
b
,
Recall
i
=
a
c
. Finally, F1 (the harmonic mean be-
tween precision and recall) for class i is defined as
F1
i
=
2?Prec
i
?Recall
i
Prec
i
+Recall
i
. In the results presented in
this section we present the precision recall and F1
for each class as well as the overall accuracy score.
Results: In our experiments we use a relatively
efficient implementation of an SVM called the Se-
quential Minimal Optimisation (SMO) algorithm
(Platt, 1999) which is provided by the Weka frame-
work (Witten and Frank, 2000). Results presented
in this section are divided into two parts. In the
first part, all results were obtained using the IBC
dataset where the target event type is ?Die?. We
provide a more detailed comparison of the perfor-
mance of each algorithm using this type as more
data was available for it. In the second section,
we examine the effectiveness of each approach for
all six event types (listed in Section 3) using the
ACE data. All reported scores were generated us-
ing 50:50 randomly selected train/test splits aver-
aged over 5 runs.
As part of the first set of results Table 4
shows the precision, recall and F1 achieved for
the ?on-event? and ?off-event? classes as well
as the overall classification accuracy obtained by
each approach. Two variations of the SVM were
built. The first version (denoted in the table
by SVM(All Features IG)) was built using all
terms, nouns chunks, lexical and additional fea-
tures to encode each train/test instance where the
features were reduced using IG. In the second
version, the same features were used but no fea-
ture reduction was carried out (denoted in the
table by SVM(All Features)). LangModel(JM),
LangModel(DS) and LangModel(LP) represent
language models smoothed using Jelinek-Mercer,
Discount Smoothing and LaPlace techniques re-
spectively. Overall these results suggest that the
SVM using IG for feature selection is the most ef-
fective method for correctly classifying both ?on-
event? and ?off-event? sentences. Specifically, it
achieves 90.23% and 96.70% F1 score for these
621
Table 4: % Precision, Recall and F1 for both classes as well as the classification accuracy achieved by all algorithms using a
50:50 train/test split where the target event type is ?Die?.
Algorithm
On-Event Class Off-Event Class
Accuracy
Precision Recall F1 Precision Recall F1
SVM(All Features IG) 90.61 89.87 90.23 96.15 97.26 96.70 94.60
SVM(All Features) 89.63 88.52 89.06 96.08 96.49 96.28 94.45
Trigger-Based Classification 83.10 93.34 87.92 97.25 93.24 95.20 93.09
LangModel(DS) 63.11 82.4 71.46 93.13 83.16 87.86 82.98
LangModel(JM) 59.46 86.01 70.31 94.22 79.53 86.25 81.22
LangModel(LP) 59.22 79.56 67.89 91.89 80.88 86.03 80.54
Majority Class (?off-event?) 0.0 0.0 0.0 74.50 100.00 85.38 74.17
Random 26.57 51.73 35.10 75.58 50.0 60.18 50.34
Table 5: % F1 for both classes achieved by the SVM using
different combinations of features.
Features F1(On-Event) F1(Off-Event)
terms 89.52 96.43
terms + nc 89.58 96.31
terms + nc + lex 89.62 96.44
All Features 90.23 96.70
classes respectively. When IG is not used we see a
marginal decrease of approx. 1% in these scores.
The fact that both versions of the SVM obtain
approx. 90% F1 scores for the ?on-event? class
is extremely encouraging when you consider the
large skew in class distribution that is present here
(i.e., the majority of training instances belong to
the?off-event? class).
To examine the effects of the various features on
overall performance, we evaluated the SVM using
different feature combinations. These results are
shown in Table 5 where ?terms?, ?nc?, and ?lex?
denote the terms, noun chunks and lexical feature
sets respectively. ?All Features? includes these fea-
tures and the ?Additional Features? described in
Section 4.1. One obvious conclusion from this ta-
ble is that terms alone prove to be the most valu-
able features for this task. Only a little increase in
performance is achieved by adding the other fea-
ture sets.
The graphs in Figure 1 shows the % F1 of both
classes achieved by all methods using varying lev-
els of training data. From these graphs we see
that the SVM obtains over 80% F1 for the ?on-
event? class and over 90% F1 for the ?off-event?
class when only 10% of the training data is used.
These results increase gradually when the amount
of training data increases. For levels of train-
ing data greater than 30% the SVM consistency
achieves higher F1 scores for both classes than all
other methods for this task.
In general, the language modeling based tech-
niques are not as effective as the SVM approach
for this classification task. However, from Ta-
ble 4 we see that all language models achieve ap-
prox. 70% F1 for the ?on-event? class and ap-
prox. 86% F1 for the ?off-event? class when only
50% of the IBC data is used to build the mod-
els. This is encouraging since they require little
or no feature engineering and less time to train.
Models smoothed with the Laplace method tend to
have the least impact out of the three model vari-
ations. This is due to the fact that this method
assigns the same probability to all unseen terms.
Thus, a term like ?professor? that may only occur
once in the dataset has the same likelihood of oc-
curring in an ?on-event? sentence as a term like
?kill? that has a very high frequency in the dataset.
In contrast, the Jelinek-Mercer and Absolute Dis-
counting smoothing methods estimate the proba-
bility of unseen terms according to a background
model built using the entire collection. Therefore,
the probabilities assigned to unseen words is pro-
portional to their global distribution in the entire
corpus. Consequently, the probabilities assigned
to unseen terms tend to be more reliable approxi-
mations of true term probabilities.
Overall, the trigger-based classification base-
line approach performs very well achieving simi-
lar scores to the SVM. This suggests that select-
ing sentences with terms associated with the target
event is an effective way of solving this problem.
That said, it still makes mistakes that the SVM and
language models have the ability to correct. For
example, many sentences that contain terms like
?suicide? and ?killing? as part of a noun phase (e.g.
?suicide driver? or ?killing range?) do not report a
death. The trigger classification baseline will clas-
sify these as an ?on-event? instances whereas the
SVM correctly places them in the ?off-event? cat-
egory. More interesting are the cases missed by
the trigger classification baseline and SVM that are
622
20
30
40
50
60
70
80
90
100
10 20 30 40 50 60 70 80 90
F
1
(
O
n
-
E
v
e
n
t
)
Percentage Training
SVM
Trigger-Based Classificaton
LangModel(LP)
LangModel(JM)
LangModel(DS)
Random
20
30
40
50
60
70
80
90
100
10 20 30 40 50 60 70 80 90
F
1
(
O
f
f
-
E
v
e
n
t
)
Percentage Training
SVM
Trigger-Based Classificaton
LangModel(LP)
LangModel(JM)
LangModel(DS)
Random
Figure 1: % F1 for the on-event (top) and off-event (bottom)
classes for all methods using varying levels of training data
where the target event is ?Die?.
corrected by the language models. These include
sentences like ?Three bodies were found yesterday
in central Baghdad.? and ?If the Americans have
killed them, then why dont they show the tape.?. In
fact, it turns out that over 50% of the errors pro-
duced by the SVM and manual trigger-based ap-
proach when the target event is ?Die? are classified
correctly by the language models. Although this is
encouraging, the overall error rate of the language
modeling approach is too high to rely on it alone.
However, this evidence suggests that it may prove
useful in the future to somehow combine the pre-
dictions of all three approaches in a way that im-
proves overall classification performance.
We now move on to the second part of the exper-
iments. Here, we present the results for six event
types (as listed in Section 3) using only data from
ACE corpus. Figure 2 shows the % F1 of the ?on-
event? class achieved by all approaches for each
event type. We have omitted the % F1 scores for
the ?off-event? class as they do not vary signifi-
cantly across event types and are similar to those
reported in Table 4. The SVM, language mod-
els, trigger-based and random baselines achieve
0
20
40
60
80
100
Die Charge-Indict Meet Attack Injure Transport
F
1
(
O
n
-
E
v
e
n
t
)
ACE Event Types
SVM
Trigger-Based Classificaton
LangModel(LP)
LangModel(JM)
LangModel(DS)
Random
Figure 2: % F1 of the ?on-event? class achieved by
all methods for the six ACE event types.
approx. 96%, 95%, 86% and 60% ?off-event? F1
scores respectively across all event types.
On the other hand, Figure 2 demonstrates that
the performance of each approach for the ?on-
event? class varies considerably across the event
types. For instance, the trigger-based classification
baseline out-performs all other approaches achiev-
ing over 60% F1 score for the ?Meet?, ?Die? and
?Charge-Indict? types. However for events like
?Attack? and ?Transport? this baselines F1 score
drops to approx. 20% thus achieving scores that
are only marginally above the random baseline. In-
terestingly, we notice that although it performs bet-
ter for events like ?Meet? and ?Charge-Indict?, the
number of trigger terms used to detect these types
is much smaller than the number used for the ?At-
tack? and ?Transport? types (see Table 3). This in-
dicates that event types where this simple baseline
performs well are those where the vocabulary used
to describe them is small. Event types where it
achieves poor results are broader types like ?Trans-
port? and ?Attack? that cover a larger spectrum of
event instances from heterogeneous contexts and
situations. However, we see from Figure 2 that the
SVM performs well on such event types and as a
result out-performs the trigger-based selection pro-
cess by approximately a factor of 4 for the ?Attack?
event and a factor of 2 for the ?Transport? event.
When we compare both datasets we find that the
ACE data is made up of newswire articles, Broad-
cast news, broadcast conversational texts, weblogs,
usenet newsgroup texts and conversational tele-
phone speech that has been transcribed, whereas
the IBC corpus consists mainly of newswire arti-
cles reporting fatalities during the Iraqi War. As
623
a result, event instances in the ACE data describ-
ing the ?Die? event type are likely to report fatali-
ties not only from Iraq but also from more diverse
contexts and situations. To investigate how perfor-
mance differs for the ?Die? event type across these
datasets, we compare the IBC results in Table 4
with the ACE results in Figure 2. We find that the
F1 scores of the ?off-event? class are not affected
much. However, the F1 scores for the ?on-event?
class for the SVM and trigger-based baseline are
reduced by margins of approx. 12% and 5% re-
spectively. We also notice that the performance of
the unigram language models are reduced signifi-
cantly by a factor of 2 indicating that they struggle
to approximate accurate term probabilities when
the vocabulary is more diverse and the amount of
training data is limited.
6 Discussion
Sentence level event classification is an important
first step for many NLP applications such as QA
and summarisation systems. For each event type
used in our experiments we treated this as a binary
classification task and compared a variety of ap-
proaches for identifying sentences that described
instances of that type. The results showed that the
trained SVM was more effective than the language
modeling approaches across all event types. An-
other interesting contribution of this paper is that
the trigger-based classification baseline performed
better than expected. Specifically, for three of the
six event types it out-performed the trained SVM.
This suggests that although there are cases where
such terms appear in sentences that do not describe
instances of a given type (for instance, ?The boy
was nearly killed.?), these cases are in the minor-
ity. However, the success of this baseline is some-
what dependent on the nature of the event in ques-
tion. For broader events like ?Transport? and ?At-
tack? where the trigger terms can be harder to pre-
dict, it performs quiet poorly. Therefore, as part
of future work, we hope to investigate ways of au-
tomating the creation of these term lists for a spec-
ified event type as this proved to be an effective
approach to this task.
Acknowledgements. This research was sup-
ported by the Irish Research Council for Science,
Engineering & Technology (IRCSET) and IBM
under grant RS/2004/IBM/1. The authors also
wishes to thank the members of the Language
Technology Research Group at the University of
Melbourne and NICTA for their helpful discus-
sions regarding this research.
References
Ahn, David. 2006. The stages of event extraction. In Pro-
ceedings of the ACL Workshop on Annotating and Reason-
ing about Time and Events, pages 1?8, Sydney, Australia,
July.
Allan, James, Jaime Carbonell, George Doddington, Jonathon
Yamron, and Yiming Yang. 1998. Topic detection and
tracking pilot study. final report.
Cohen, Jacob. 1960. A coeficient of agreement for nomi-
nal scales. Educational and Psychological Measurement,
20(1):37?46.
Curran, James, Stephen Clark, and Johan Bos. 2007. Lin-
guistically motivated large-scale nlp with c & c and boxer.
In Proceedings of the ACL 2007 Demonstrations Session
(ACL-07 demo), pages 29?32.
Filatova, Elena and Vasileios Hatzivassiloglou. 2004. Event-
based extractive summarization. In In Proceedings of ACL
Workshop on Summarization, pages 104 ? 111.
Joachims, Thorsten. 1998. Text categorization with support
vector machines: learning with many relevant features. In
N?edellec, Claire and C?eline Rouveirol, editors, Proceed-
ings of the 10th ECML, pages 137?142, Chemnitz, DE.
Springer Verlag, Heidelberg, DE.
Kraaij, Wessel and Martijn Spitters. 2003. Language models
for topic tracking. In Croft, Bruce and John Lafferty, edi-
tors, Language Models for Information Retrieval. Kluwer
Academic Publishers.
Murff, Harvey, Vimla Patel, George Hripcsak, and David
Bates. 2003. Detecting adverse events for patient safety
research: a review of current methodologies. Journal of
Biomedical Informatics, 36(1/2):131?143.
Platt, John. 1999. Fast training of support vector machines
using sequential minimal optimization. Advances in kernel
methods: support vector learning, pages 185?208.
Saur??, Roser, Robert Knippen, Marc Verhagen, and James
Pustejovsky. 2005. Evita: a robust event recognizer for
qa systems. In HLT, pages 700?707.
Walker, Christopher., Stephanie. Strassel, Julie Medero, and
Linguistic Data Consortium. 2006. ACE 2005 Multilin-
gual Training Corpus. Linguistic Data Consortium, Uni-
versity of Pennsylvania.
Witten, Ian and Eibe Frank. 2000. Data mining: practical
machine learning tools and techniques with Java imple-
mentations. Morgan Kaufmann Publishers Inc.
Yamron, JP, L. Gillick, P. van Mulbregt, and S. Knecht. 2002.
Statistical models of topical content. The Kluwer Interna-
tional Series on Information Retrieval, pages 115?134.
Zhai, Chengxiang and John Lafferty. 2001. A study of
smoothing methods for language models applied to ad hoc
information retrieval. In Research and Development in In-
formation Retrieval, pages 334?342.
624
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 38?46,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Analyzing Patient Records to Establish If and When a Patient Suffered from
a Medical Condition
James Cogley, Nicola Stokes, Joe Carthy and John Dunnion
School of Computer Science and Informatics
University College Dublin
Dublin, Ireland
James.Cogley@ucdconnect.ie {Nicola.Stokes, Joe.Carthy, John.Dunnion}@ucd.ie
Abstract
The growth of digital clinical data has raised
questions as to how best to leverage this data
to aid the world of healthcare. Promising ap-
plication areas include Information Retrieval
and Question-Answering systems. Such sys-
tems require an in-depth understanding of the
texts that are processed. One aspect of this
understanding is knowing if a medical con-
dition outlined in a patient record is recent,
or if it occurred in the past. As well as this,
patient records often discuss other individu-
als such as family members. This presents
a second problem - determining if a medi-
cal condition is experienced by the patient de-
scribed in the report or some other individ-
ual. In this paper, we investigate the suitabil-
ity of a machine learning (ML) based system
for resolving these tasks on a previously unex-
plored collection of Patient History and Phys-
ical Examination reports. Our results show
that our novel Score-based feature approach
outperforms the standard Linguistic and Con-
textual features described in the related litera-
ture. Specifically, near-perfect performance is
achieved in resolving if a patient experienced
a condition. While for the task of establish-
ing when a patient experienced a condition,
our ML system significantly outperforms the
ConText system (87% versus 69% f-score, re-
spectively).
1 Introduction
The growth of the digitization of clinical docu-
ments has fostered interest in how to best lever-
age this data in providing assistance in the world
of healthcare, including novel information re-
trieval (Voorhees and Tong, 2010), question an-
swering (Demner-Fushman and Lin, 2007; Patrick
and Li, 2011) and clinical summarization sys-
tems (Feblowitz et al, 2011).
Given the richness of the language found in clin-
ical reports, novel systems require a deeper under-
standing of this textual data. One aspect of this un-
derstanding is the assertion status of medical condi-
tions (Demner-Fushman et al, 2011). The assertion
status of a medical condition may refer to Negation
Resolution, Temporal Grounding (deciding if a con-
dition is currently or historical, and Condition Attri-
bution (deciding if a condition is experienced by the
patient described in the report or some other individ-
ual). The focus of this paper rests on the latter two
tasks of Temporal Grounding and Condition Attribu-
tion as Negation has been satisfactorily addressed in
Chapman et al (2007).
Several approaches, ranging in complexity, have
been proposed for resolving temporal information.
Hripcsak et al (2005) modeled the task as a con-
straint satisfaction problem. Another rule-based ap-
proach that achieved moderate results uses regular
expressions matching occurrences of trigger terms
(Chapman et al 2007). A trigger term in this context
refers to a term or phrase that provides strong evi-
dence supporting the attribution (e.g., patient, fam-
ily member) or temporality (e.g., current, past) of
a condition. Given the limitations of solely us-
ing pre-composed trigger term lists, recent focus
has been placed on the use of rule-based learning
systems with different feature sets (Mowery et al,
2009). Section headers, tense and aspect are investi-
gated as features, with promising results for the tem-
porality task achieving an accuracy score of 89%.
However, the authors? acknowledge that conclusions
drawn must be tentative as a majority class classifier
achieved an accuracy of 86.9% (only 13% of condi-
tions in the dataset are historical).
38
This paper extends current work in the domain in
the following ways. The dataset used in these exper-
iments is generated from a collection of previously
unannotated History & Physical (H&P) Examina-
tion reports. Prior work has focused on other report
types such as discharge summaries and emergency
department reports. In these cases the distribution
of historical and recent conditions is often heavily
skewed in favour of descriptions of recent conditions
experienced by the patient. As H&P reports aim to
provide a comprehensive picture of a patient?s past
and present state, a more uniform distribution of his-
torical and recent conditions is present in this report
type. This work extends previous work by exploring
the use of machine learning (ML) as an alternative to
hand-crafted rule based systems and rule-based ML
approaches to resolving these tasks.
In this work, a comparative analysis of several
ML algorithms from different paradigms are eval-
uated, in order to determine the best approach for
our tasks. Building on this, the performance of four
automatically extracted feature sets are evaluated,
identifying their contributions and also their interac-
tions. This work also extends existing work by au-
tomatically extracting features that were previously
extracted manually as well as the proposal of a set
of novel score-based features. The performance of
the ML algorithms are compared to the rule-based
system - ConText. Our results show that the ML
approaches significantly outperform this rule-based
system on the Temporal Grounding task.
2 Related Work
Natural Language Processing techniques have
been shown to have many different uses in Clinical
Text Analysis, such as in the representation (Sager
et al, 1994) and understanding (Christensen, 2002)
of clinical narratives, and frequently now in the con-
text of more elaborate large-scale systems, such as a
clinical decision support system (Demner-Fushman
et al, 2009).
Given the sensitive nature of clinical narratives
and the difficulty in obtaining data collections for
experimental purposes, evaluation and comparison
of NLP systems in this domain is difficult. However,
recently anonymised data provided by the Biomedi-
cal Language Understanding (BLU) Lab at the Uni-
versity of Pittsburgh as well as datasets provided
as part of the i2b2/VA 2010 challenge (Uzuner et
al., 2011), has greatly aided NLP research into the
processing of clinical narratives. The dataset pro-
vided by BLU Lab and used in this work con-
sists of 101,711 reports of several different report
types ranging from discharge summaries to surgical
pathology reports. The report types differ in con-
tent, technical language and structure. For example,
surgical pathology reports are very technical and ex-
plicit in the information that they convey, e.g. results
of a biopsy, blood cell counts etc. In contrast, dis-
charge summaries and consultation reports are more
expressive, and aim to create a more complete pa-
tient profile, e.g. including work and personal cir-
cumstances. The BLU Lab have published a num-
ber of papers on the topic of resolving the assertion
status of medical conditions (Chapman et al, 2007;
Harkema et al, 2009; Mowery et al, 2009). Their
ConText algorithm (Chapman et al, 2007) uses
handcrafted regular expressions, along with trigger
terms and termination terms to determine character-
istics of a condition mention in a text. The condition
characteristics investigated included negation, tem-
porality (recent, historical, hypothetical) and experi-
encer (patient, other). Their approach worked very
well on the negation and hypothetical temporality,
achieving an f-score of 97% in determining nega-
tion and an f-score of 88% in resolving hypothetical
conditions. However, the approach was less success-
ful when determining historical conditions and their
experiencer, with f-scores of 71% and 67%, respec-
tively. These results were generated on emergency
room reports only.
In more recent work, their algorithm was ap-
plied to 5 other types of clinical document, namely:
surgical pathology, operative procedure, radiol-
ogy, echocardiogram and discharge summaries
(Harkema et al, 2009). Results achieved on these
new datasets were largely the same, with f-scores
for negation ranging between 75% and 95%, and for
hypothetical conditions ranging between 76% and
96%. Again, a marked drop-off was seen for histor-
ical conditions, with few occurrences of conditions
for other experiencers annotated in the datasets (i.e.
relatives) making it difficult to draw definitive con-
clusions from this work.
Although this manual rule based approach has
performed well and is advocated due to its ease of
implementation (Meystre et al, 2008), Harkema et
al. (2009) note its limitations in resolving historical
conditions, and encourage the possibility of statisti-
cal classifiers in which information other than lexi-
cal items, are considered as features. Further work
investigating the use of Machine Learning (Uzuner
et al, 2009; Mowery et al, 2009) has seen posi-
39
tive breakthroughs in resolving the assertion status
of medical conditions.
The 2010 i2b2 challenge (Uzuner et al, 2011)
provided a rigid and standardized platform for eval-
uating systems in resolving the assertion status of
medical conditions found in Discharge Summaries.
The challenge consisted of three subtasks:Concept
Extraction, Assertion and Relation Identification.
The second subtask of Assertion involved the devel-
opment of systems that resolved the assertion sta-
tus of medical conditions. As part of the asser-
tion task there were six possible assertion statuses:
present, absent, uncertain, conditional, or not associ-
ated with the patient. Systems submitted to this chal-
lenge ranged from more simplistic pattern matching
techniques to more complex supervised and semi-
supervised approaches (de Bruijn et al, 2011; Clark
et al, 2011). The datasets used in the 2010 i2b2
challenge were not available to non-participants at
the time the experiments presented in this work were
conducted. We plan to explore these datasets in
future work. This research investigates patient vs.
non-patient conditions as well as past vs. present
conditions in order to fine tune feature-sets that may
be generalized to further assertion statuses.
In the context of this paper, while the BLU Lab
clinical report collection is available, the medical
condition annotations are not. As already stated, our
intention is to explore a machine learning approach
to these tasks. For this purpose we annotated a por-
tion of the consultation report section of the collec-
tion. There were two reasons for this - firstly, the
BLU Lab have not reported results on this report
type so there is no duplication of annotation effort
and secondly, it turns out that the consultation re-
ports are a much richer source of historical condi-
tions and condition attribution than any of the report
types annotated previously.
3 Method
3.1 Corpus
For this task, 120 H&P reports were randomly
extracted from the BluLab?s NLP repository. As
already stated, this report type?s fuller descriptions
make it richer than previous datasets in instances
of condition attribution and temporal grounding. A
breakdown in the distributions of these annotations
can be seen in Tables 1 and 2.
H&P reports may vary in the organization of con-
tent, but the content is mostly uniform, expressing
the same information about patients (Sager et al,
1987). As well as this, many reports feature head-
ings for different sections of the report (past medical
history, impression), information which can be used
as features in a classification task. Before annotat-
ing conditions found in the text, preprocessing was
required in order to retain such information.
Table 1: Annotated Condition Attribution Occurrences
Class Count
Patient 872
Other 93
Total 965
Table 2: Annotated Temporal Grounding Occurrences
Class Count
Historical 448
Recent 424
Total 872
3.1.1 Preprocessing
Preprocessing of the data consisted of a simple
Java program that extended Lingpipe1 tools in or-
der to correctly split sentences on this dataset, and
extract the heading for the section in which the sen-
tence is contained.
The preprocessing outputs the sentence number,
followed by a separator, the sentence?s contents and
the heading under which the sentence features. Sen-
tences were split for ease of annotation and also
to allow parsing and part-of-speech tagging by the
C&C2 parsing tools. C&C was chosen for its scala-
bilty, speed and the accuracy of its biomedical lan-
guage models. A cursory analysis of its output in-
dicates that its performance is acceptable. As C&C
does not provide a sentence splitter, Lingpipe?s split-
ter was availed of for this task.
3.1.2 Annotation
Annotation of the dataset was performed by two
annotators over a 60 hour period. The inter-
annotator agreement was measured using the kappa
statistic (Carletta, 1996). A kappa statistic of 0.78
was achieved. The annotators were presented with
the collection, to annotate with an XML like tag
?CONDITION?. This tag must have two attributes,
?EXP? representing condition attribution and ?HIST?
1http://alias-i.com/lingpipe/
2http://svn.ask.it.usyd.edu.au/trac/
candc
40
representing the temporal grounding of the condi-
tion.
? HIST: A value of 1 indicates the occurrence of a
historical condition, where 0 describes a current
or recent condition. e.g. ?The patient presented
with <CONDITION NUM=?1? HIST=?0?> re-
nal failure </CONDITION>? would indicate the
condition ?renal failure? is current.
? EXP: A value of 1 implies the expe-
riencer is the patient with 0 signifying
?other?. e.g. ?The patient has a fam-
ily history of <CONDITION NUM=?1?
EXP=?0?>hypertension </CONDITION>?
signifies the condition ?hypertension? is not
experienced by the patient.
3.2 Machine Learning Algorithms
Early work in the resolution of assertion status
primarily focused on the use of manually created
rule-based systems, with more recent work focusing
on statistical and ML methods. However, the do-
main of ML contains many sub-paradigms and vary-
ing approaches to classification. In this paper, three
ML methods that have not been previously applied
to this task are explored. These three classifiers,
namely Naive Bayes, k-Nearest Neighbour and Ran-
dom Forest represent the paradigms of probabilistic,
lazy and ensemble learning, respectively.
Naive Bayes is a probabilistic classifier imple-
menting Bayes Theorem. As a result, features im-
plemented using this classifier are deemed to be in-
dependent. Despite this strong assumption it has
been shown to be more than successful in text classi-
fication tasks such as spam filtering (Provost, 1999).
k-Nearest Neighbour (kNN) (Cover and Hart,
1967) is a simple pattern recognition algorithm that
classifies an instance according to its distance to the
k closest training instances. This algorithm has been
chosen to represent the paradigm of lazy learning,
i.e. there is no training phase as all computation
is performed at the classification stage. Despite its
simplicity, k-NN has often produce high accuracy
results in comparison to other approaches (Caruana,
2006).
The final classifier chosen for this task represents
the state-of-the-art in machine learning, namely the
Random Forest algorithm (Breiman, 2001). A Ran-
dom Forest consists of many different decision trees,
combining bagging (Breiman, 1996), and random
feature selection.
3.3 Features
In this section, a list of features contributing to
this task are presented. All features are automati-
cally extracted using a set of tools developed by the
authors. Section 3.3.1 presents score-based features
that are unique to this work. Section 3.3.2 describes
the implementation of features outlined in Chapman
et al(2007). Section 3.3.3 and Section 3.3.4 present
features similar to those investigated in Mowery et
al. (2009).
3.3.1 Score based features
Scored based features used in this system extend
and reinforce Trigger List features by computing a
normalized score for the number of occurrences of
Trigger List terms3. This feature aims to add fur-
ther granularity to the decision making of the ML al-
gorithms, presenting a floating point number rather
than a binary one.
The equation for computing these scores is de-
fined as follows.
s =
Nt
(Nw ? Sw)
(1)
Nt represents the number of trigger terms found in
the sentence that contains the condition, Nw is the
total number of words in the sentence, with Sw being
the number of stopwords4. These scores are calcu-
lated for each type of trigger term. For example, for
trigger type relative mention, a score is calculated
using mentions of relatives in the sentence.
3.3.2 Trigger List Features
? precededByHistTerm: This feature performs
a look-up for trigger terms from the historical
word list, checking if it directly precedes the
condition. An example historical trigger term
would be ?history of? as in ?a history of dia-
betes?. If a condition, such as diabetes, is mod-
ified by a historical trigger term, it will return 1,
otherwise 0.
? containsHistMention: This is a weaker
form of precededByHistTerm, checking sim-
ply if a trigger term from the historical list oc-
curs in the same sentence as the condition. If
one does, it will return 1 otherwise 0.
? hasRelativeMention: If the sentence which
contains the condition also contains a trigger
3These trigger lists may be downloaded at http:
//csserver.ucd.ie/?jcogley/downloads/
wordlists.tar.gz
4The list of stopwords may be downloaded at
http://csserver.ucd.ie/?jcogley/downloads/
stopwords.txt
41
term from the experiencer list such as ?mother?,
?father? or ?uncle? it will return 1, otherwise 0.
? hasPatientMention: 1 if the sentence men-
tions the patient, otherwise 0.
? containsDeath: 1 if it contains the terms ?de-
ceased?, ?died? from the death trigger terms list
otherwise 0. A sentence describing death is more
likely to refer to a relative, rather than the pa-
tient.
? mentionsCommunity: 1 if one of ?area?,
?community? from the geographical trigger list
is mentioned, otherwise 0. If a sentence de-
scribes a community, as in ?there has been a re-
cent outbreak of flu in the area?, it is not refer-
ring to the patient, therefore the condition should
not be attributed to the patient.
? precededByWith: 1 if the condition is directly
preceded by ?with?, otherwise 0. ?with? was
found to have higher frequency when describ-
ing patients rather than individuals other than the
patient. e.g. ?Patient presented with high blood
pressure and fever.?
? containsPseudoTerms: Pseudo-historical
terms or phrases may mention a term that is
found in the Historical list, but do not indicate
that a condition mention in the same sentence is
being used in a historical context. For example,
?poor history? is a pseudo-historical trigger
term. It uses a historical trigger term (?history?);
however ?poor history? refers to the incomplete
nature of the patient?s medical history, not the
historical nature of their condition. This feature
returns 1 on the occurrence of a pseudo trigger
term, otherwise 0.
3.3.3 Contextual features
In resolving the textual context of conditions, it
is important to look at what surrounds the condition
beyond the lexical items. With these contextual fea-
tures, we can capture that section in which a sen-
tence occurs, and how many conditions occur in the
sentence.
? isInFamHist: The importance of header infor-
mation is motivated by the assumption that con-
ditions that fall under explicit headings, are more
than likely to have a greater affinity to the head-
ing. This feature returns 1 if it is under Family
History, 0 otherwise.
? isInList: A binary feature denoting whether
a condition occurs as part of a list of conditions,
with one condition per line. Returns 1 if it is a
member of such a list, otherwise 0.
? numOfConditions: This feature represents the
number of conditions present in a given sen-
tence. A higher number of conditions indicates
that the condition may be part of a list. Sentences
that contain a list of conditions tend to list past
conditions rather than recently suffered illnesses.
3.3.4 Linguistically motivated features
Three features were designed to monitor the ef-
fect of the verb tense on a condition. This feature
has already been shown to aid the classification pro-
cess (Mowery et al, 2009). For this task, linguistic
features were extracted from the output of the C&C
parsing tool, using both part-of-speech tags along
with dependency information.
? hasPastTense: A binary feature with 1 indi-
cating the sentence contains a past tense verb, 0
otherwise. e.g. ?The patient previously suffered
renal failure? would return 1. If a condition is
modified by a past tense verb, it has occurred in
the past.
? hasPresentTense: A binary feature with 1
indicating the sentence contains a present tense
verb, 0 otherwise. If a condition is modified by a
present tense verb, the condition is current. e.g.
?the patient presents coughing?.
? containsModalVerb: A binary feature with 1
indicating the sentence contains a modal verb,
0 otherwise. e.g. ?palpitations may have been
caused by anxiety?. The presence of the modal
?may? following the condition indicates the con-
dition is currently being examined and is there-
fore recent.
? tenseInClause: Analyzes the tense found in
the same syntactic clause as the condition being
examined. For example, in ?abdominal pain has
ceased, but patient now complains of lower ex-
tremity pain?, ?abdominal pain? has a past tense
within its clausal boundary, where the clause
which contains ?lower extremity pain? has a
present tense verb.
? tenseChange: Determines whether the verb
tense used in the clause that contains the con-
dition differs with the verb in another clause in
the sentence. e.g. ?Migraines persist yet palpi-
tations resolved?. This feature allows finer gran-
ularity in resolving the tense surrounding condi-
tions, such as the description of current condi-
tions in the context of the patient?s history.
42
4 Experiment Setup & Evaluation
There are two aims of the experiments reported
in this section: firstly, to evaluate the performance
of ML algorithms in resolving the assertion status of
medical conditions. Secondly, to assess the perfor-
mance of individual feature sets in order to discover
the most contributory and informatory features or
sets of features. To evaluate the latter, classifications
using all possible combinations of feature sets (listed
in Table 3) were performed.
Table 3: Feature-set Combinations
ID Feature-Sets
TrigLingConScore trigger, linguistic, score-based, contextual
TrigLingScore trigger, linguistic, score-based
TrigLingCon trigger, linguistic, contextual
TrigConScore trigger, score-based, contextual
LingConScore linguistic, score-based, contextual
TrigLing trigger, linguistic
TrigScore trigger, score-based
TrigCon trigger, contextual
LingScore linguistic, score-based
LingCon linguistic, contextual
ConScore score-based, contextual
Trigger trigger
Ling linguistic
Score score-based
Con contextual
4.1 Evaluation
The systems are evaluated by the metrics preci-
sion, recall and f-score:
precision =
TP
TP + FP
recall =
TP
TP + FN
f =
2? Precision?Recall
Precision + Recall
where TP is the number of true positives, FP is the num-
ber of false positives, FN is the number of false negatives.
Systems are evaluated using both cross-validation
and hold-out methods. In the hold-out method there
are two data sets, one used for training the classifier
and a second for testing it on a blind sub-set of test
material. 10-fold cross-validation is performed on
the training sets and final results are reported in this
paper on the held-out blind test set. Three hold-out
classification splits were experimented with (i.e.,
train/test splits: 30%/70%; 50%/50%; 70%/30%).
We found that results for each of the data splits and
cross-validation experiments were largely uniform.
To avoid repetition of results, Section 5 focuses on
experiments using a held-out method training/test
split of 70%/30%. All hold-out experiments were
implemented using Weka?s (Hall et al, 2009) Ex-
perimenter interface. Cross-Validation experiments
were performed using a script developed by the au-
thors in conjunction with Weka?s API. This allowed
the ML approaches and the ConText algorithm to be
evaluated against the same test-folds.
4.1.1 Comparison with a rule-based system
ConText (Chapman et al, 2007) is a simple yet
effective rule-based system designed to resolve the
assertion status of medical conditions. Comparative
analysis is performed between an implementation of
ConText5 and the ML approaches described in 3.2.
The ML systems were trained on 70% of the dataset
(610 conditions). The remaining 30% (262 condi-
tions) was used as a test set for both ConText and
the Machine Learning systems. For cross-validation
experiments, ConText was evaluated against each
test set fold. For the Condition Attribution exper-
iments training was performed on 675 conditions
with testing performed on 290 conditions. In eval-
uating Temporal Grounding the training set com-
prised of 610 conditions with the test-set containing
262 conditions.
5 Experimental Results
This section reports results of the experiments
outlined in Section 4.
5.1 Condition Attribution
In a system that resolves the assertion status of
medical conditions, it is of benefit to know who is
experiencing the medical condition before resolving
more complex information such as temporality. In
this section, preliminary results on Condition Attri-
bution are presented. The dataset used in evaluat-
ing the effectiveness of Condition Attribution was
highly skewed, as shown in Table 1. This is a natural
skew caused simply by the fact that reports discuss
the patient more than other individuals (e.g., blood
relatives). As a result the baseline score using a Ma-
jority Class classifier achieved an f-score of 95%
(Table 4). Given these results, the contextual fea-
ture set contributes most, as shown by the removal
of the contextual feature set in TrigLingScore coin-
ciding with a drop in performance. However, the
skewed dataset resulted in no statistical significance
5http://code.google.com/p/negex/
downloads/detail?name=GeneralConText.Java.
v.1.0_10272010.zip
43
between classifiers and feature-sets.
Table 4: Selected feature-sets (f-score) using Cross-
Validation for the Condition Attribution task
ID RFor kNN NB Maj.
TrigLingConScore 100% 100% 100% 95%
TrigLingScore 96% 96% 96% 95%
TrigConScore 100% 100% 100% 95%
Con 100% 100% 100% 95%
In this task, ConText achieved an f-score of 99%.
As there is little difference in scores achieved be-
tween ConText and the approaches in Table 4 - a
manual rule-based system can be considered ade-
quate for this task.
Taking a closer look at the performance of the fea-
ture sets, we see that the contextual feature set con-
tributes most to the task with the removal of contex-
tual features coinciding with a drop in performance
e.g., TrigLingScore in Table 4. The strength of this
feature set lies with the feature isInFamHist. This
feature simply checks if the condition occurs under
the heading ?Family History?. Its highly influen-
tial performance would indicate that its quite rare
for the mention of another individual anywhere else
in a clinical report. The Con run, which is solely
composed of contextual features achieves near per-
fect performance, an indication that the contribution
of other features to the task of Condition Attribu-
tion is minimal. Although this work used only H&P
reports, this finding may be generalized to other re-
port types such as Discharge Summaries which also
explicitly mark sections pertaining to other individ-
uals.
5.2 Temporal Grounding
The distribution of past and recent medical con-
ditions is not skewed (as in the Condition Attribu-
tion task), and hence it presents a more challeng-
ing classification task. Despite the varying per-
formance of individual classifiers and feature sets
the results obtained are again largely similar across
cross-validation and hold-out methods, with the per-
formance of each training set fitting the distribu-
tion in Figure 1. Initial experiments investigated the
use of another state-of-the-art classifier, the Support
Vector Machine using a polykernel, however due to
its relatively poor performance (70% f-score, with
its precision soundly beaten by other approaches) it
will not be discussed in further detail.
Random Forest proved to be the most effective
classifier across almost all feature sets. However,
kNN was a very near second place - Random Forest
only significantly6 outperformed kNN on two occa-
sions (TrigLingConScore, LingConScore). In con-
strast, Naive Bayes performed poorly - despite hav-
ing outperformed all other systems on the precision
metric, it failed to outperform the baseline majority
classifier on the recall.
Although the precision of ConText matches that
of the Random Forest and kNN (Table 5), it is also
let down by its recall performance. As a result, there
is a statistical significant difference between its f-
score and that of the Random Forest and kNN.
Table 5: Temporal Grounding ConText Comparison
System Precision Recall F-score
kNN 80% 80% 80%
RandomForest 82% 84% 83%
NaiveBayes 91% 61% 72%
ConText 80% 61% 69%
Majority 55% 100% 71%
6 Discussion
The distribution of recent and historical condi-
tions for the task of Temporal Grounding is more
evenly distributed than that used in Condition Attri-
bution, allowing for a more interesting comparison
of the approaches and features employed.
Figure 1 shows the performance of each ML for
each feature-set combination. Random Forest was
expectedly the best performing algorithm. However,
more surprising was the comparative performance
of the often overlooked kNN algorithm. Both ap-
proaches statistically significantly outperformed the
rule-based system ConText. Though the rule based
system matched the high performing ML systems in
terms of precision, it was significantly outperformed
with respect to recall.
The most contributory feature set in the ML runs
was the novel score-based feature set. This feature
creates a normalized score for the occurrence of trig-
ger terms in the same sentence as the medical con-
dition in question. It was designed to reinforce the
importance of trigger terms, by providing a numeric
score to support the binary Trigger List feature. The
addition of score-based features to any of the fea-
ture combinations coincided with a statistical signif-
icant boost in performance, with Score (composed
solely of score-based features) outperforming half of
all other feature combinations as seen in Figure 1,.
On the contrary, the effect of contextual features
on the performance of the algorithms for Temporal
6Significance calculated by Paired T-Test with 95% confi-
dence.
44
30%	 ?
40%	 ?
50%	 ?
60%	 ?
70%	 ?
80%	 ?
90%	 ?
100%	 ?
TrigLin
gConSc
ore 
TrigLin
gScore
 
TrigLin
gCon 
TrigCo
nScore
 
LingCo
nScore
 
TrigLin
g 
TrigSco
re TrigCo
n 
LingSco
re LingCo
n 
ConSco
re Trig Ling Score Con 
Ran.	 ?Forest	 ? kNN	 ? Naive	 ?Bayes	 ? Maj.	 ?Class	 ?
Figure 1: Temporal Grounding f-score performance with 70% Training Data
Grounding is minimal, or even detrimental to the
task. For example, in Figure 1, the performance
of the kNN algorithm increases from TrigLingCon-
Score to TrigLingScore with the removal of contex-
tual features. The performance of the Random For-
est is unaffected by such detrimental features as it
performs its own feature selection prior to classifi-
cation. Though there are several feature set com-
binations reaching a high level of performance, the
most effective approach combines trigger list terms,
linguistic and score based features with the Random
Forest algorithm.
These experiments extend previous work by pro-
viding a systematic, automated approach to feature
extraction for the purpose of ML approaches to Tem-
poral Grounding. They also indicate the high per-
formance and contribution of our novel score-based
features. These features are not designed to solely
classify instances found in H&P reports and can
be applied to other clinical reports such as Dis-
charge Summaries and Emergency Department re-
ports. Previous work has involved the use of the
latter mentioned report types. Unfortunately, given
the terms-of-use of these datasets they could not be
made available to authors to facilitate comparative
experiments.
7 Conclusion
In this paper, we proposed the use of machine
learning (ML) in resolving if and when a patient
experienced a medical condition. The implemented
ML algorithms made use of features comprising of
trigger terms, linguistic and contextual information,
and novel score-based features. In an evaluation of
these feature sets it was found that score-based fea-
tures contributed to the task of resolving when a pa-
tient experienced a medical condition.
The ML approaches were also evaluated against
the rule-based system ConText on a new annotated
dataset of History & Physical (H&P) Examination
Reports. In this evaluation it was discovered that the
task of resolving if a condition was experienced by
the patient was adequately solved by the ConText
system, achieving an f-score of 99%. Although, the
ML approaches proposed achieved perfect perfor-
mance, there is no statistical significance between
the result sets. However, the more challenging task
of deciding when a patient experienced a medical
condition is deemed to be best suited to a ML ap-
proach, with the top performing classifier Random
Forest achieving an f-score of 87%, significantly
outperforming ConText which achieved 69% on the
same dataset .
The results achieved in these tasks have paved the
way for several avenues of future work. We be-
lieve that the performance of these tasks is now suffi-
ciently accurate to justify their inclusion in an Infor-
mation Retrieval (IR) application. It is our intention
to use our medical condition analysis techniques to
annotate clinical documents and build an advanced
IR system capable of taking advantage of this mark
up in the context of the TREC Medical Records
Track 20127. With the availability of datasets such
as that of the i2b2 Shared Task 2010 data, further
work will include experimentation on these datasets
as well as an investigation into further assertion sta-
tuses.
8 Acknowledgments
We are grateful to Dr Martina Naughton for her
advice on many aspects of this paper. We also wish
to acknowledge the support of Science Foundation
Ireland, who fund this research under grant number
10/RFP/CMS2836.
7http://groups.google.com/group/trec-med
45
References
L. Breiman. 1996. Bagging predictors. Machine Learn-
ing, 24:123?140.
L. Breiman. 2001. Random forests. Machine Learning,
45:5?32.
J. Carletta. 1996. Assessing agreement on classification
tasks: the kappa statistic. Computational Linguistics,
22(2):249 ? 254.
R. Caruana. 2006. An empirical comparison of super-
vised learning algorithms. In Proceedings of 23rd In-
ternational Conference on Machine Learning.
W. W. Chapman, D. Chu, and J. N. Dowling. 2007. Con-
text: An algorithm for identifying contextual features
from clinical text. In BioNLP 2007: Biological, trans-
lational, and clinical language processing, pages 81?
88, June.
L. M. Christensen. 2002. Mplus: A probabilistic med-
ical language understanding system. In Proceedings
of Workshop on Natural Language Processing in the
Biomedical Domain, pages 29?36.
C. Clark, J. Aberdeen, M. Coarr, D. Tresner-Kirsch,
B. Wellner, A. Yeh, and L. Hirschman. 2011. Mitre
system for clinical assertion status classification. Jour-
nal of the American Medical Informatics Association.
T. Cover and P. Hart. 1967. Nearest neighbor pattern
classification. Transactions on Information Theory.
B. de Bruijn, C. Cherry, S. Kiritchenko, J. Martin, and
X. Zhu. 2011. Machine-learned solutions for three
stages of clinical information extraction: the state of
the art at i2b2 2010. Journal of the American Medical
Informatics Association.
D. Demner-Fushman and J. Lin. 2007. Answering
clinical questions with knowledge-based and statisti-
cal techniques. In Computational Linguistics, pages
63?103.
D Demner-Fushman, W W. Chapman, and C J. McDon-
ald. 2009. What can natural language processing do
for clinical decision support? Journal of Biomedical
Informatics, 42:760?772.
D. Demner-Fushman, S. Abhyankar, A. Jimeno-Yepes,
R. Loane, B. Rance, F. Lang, N. Ide, E. Apostolova,
and A. R. Aronson. 2011. A knowledge-based ap-
proach to medical records retrieval. In TREC 2011
Working Notes.
J. Feblowitz, A. Wright, H. Singh, L. Samal, and D. Sit-
tig. 2011. Summarization of clinical information: A
conceptual model. Biomedical Informatics.
M. Hall, F. Eibe, G. Holmes, B. Pfahringer, P. Reute-
mann, and I. H. Witten. 2009. The weka data mining
software: An update. SIGKDD Explorations.
H. Harkema, J. N. Dowling, T. Thornblade, and W. W.
Chapman. 2009. Context: An algorithm for identify-
ing contextual features from clinical text. Journal of
Biomedical Informatics, 42(5):839?851.
G. Hripcsak, L. Zhou, S. Parsons, A. K. Das, and S. B.
Johnson. 2005. Modeling electronic discharge sum-
maries as a simple temporal constraint satisfaction
problem. Journal of the American Medical Informat-
ics Association, 12(1):55?63, January.
S. M. Meystre, G. K. Savova, K. C. Kipper-Schuler, and
J. F. Hurdle. 2008. Extracting information from tex-
tual documents in the electronic health record: a re-
view of recent research. IMIA Yearbook of Medical
Informatics, pages 128?144.
D. L. Mowery, H Harkema, J. N. Dowling, J. L. Lust-
garten, and W. W. Chapman. 2009. Distinguishing
historical from current problems in clinical reports-
which textual features help? In Proceedings of the
Workshop on Current Trends in Biomedical Natural
Language Processin, pages 10?18. Association for
Computational Linguistics.
J. Patrick and M. Li. 2011. An ontolotgy for clinical
questions about the contents of patients notes. Journal
of Biomedical Informatics.
J. Provost. 1999. Naive-bayes vs. rule-learning in classi-
fication of email. Technical report, The University of
Texas at Austin.
N. Sager, C. Friedman, and M.S. Lyman. 1987. Medi-
cal Language Processing: Computer Management of
Narrative Data. Addison-Wesley.
N. Sager, M. Lyman, C. Bucknall, N. Nhan, and L. J.
Tick. 1994. Natural language processing and the rep-
resentation of clinical data. Journal of the American
Medical Informatics Association, 1:142?160.
O. Uzuner, X. Zhang, and T. Sibanda. 2009. Machine
learning and rule-based approaches to assertion classi-
fication. Journal of the American Medical Informatics
Association, 16(1):109?115.
O?. Uzuner, BR. South, S. Shen, and SL. DuVall. 2011.
2010 i2b2/va challenge on concepts, assertions, and re-
lations in clinical text. Journal of the American Medi-
cal Informatics Association.
E. Voorhees and R. Tong. 2010. Overview of the trec
2011 medical records track. preprint.
46
