Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 51?60, Prague, June 2007. c?2007 Association for Computational Linguistics
Getting the structure right for word alignment: LEAF
Alexander Fraser
ISI / University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
fraser@isi.edu
Daniel Marcu
ISI / University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
marcu@isi.edu
Abstract
Word alignment is the problem of annotating
parallel text with translational correspon-
dence. Previous generative word alignment
models have made structural assumptions
such as the 1-to-1, 1-to-N, or phrase-based
consecutive word assumptions, while previ-
ous discriminative models have either made
such an assumption directly or used features
derived from a generative model making one
of these assumptions. We present a new gen-
erative alignment model which avoids these
structural limitations, and show that it is
effective when trained using both unsuper-
vised and semi-supervised training methods.
1 Introduction
Several generative models and a large number of
discriminatively trained models have been proposed
in the literature to solve the problem of automatic
word alignment of bitexts. The generative propos-
als have required unrealistic assumptions about the
structure of the word alignments. Two assumptions
are particularly common. The first is the 1-to-N as-
sumption, meaning that each source word generates
zero or more target words, which requires heuristic
techniques in order to obtain alignments suitable for
training a SMT system. The second is the consec-
utive word-based ?phrasal SMT? assumption. This
does not allow gaps, which can be used to particular
advantage by SMT models which model hierarchi-
cal structure. Previous discriminative models have
either made such assumptions directly or used fea-
tures from a generative model making such an as-
sumption. Our objective is to automatically produce
alignments which can be used to build high quality
machine translation systems. These are presumably
close to the alignments that trained bilingual speak-
ers produce. Human annotated alignments often
contain M-to-N alignments, where several source
words are aligned to several target words and the re-
sulting unit can not be further decomposed. Source
or target words in a single unit are sometimes non-
consecutive.
In this paper, we describe a new generative model
which directly models M-to-N non-consecutive
word alignments. The rest of the paper is organized
as follows. The generative story is presented, fol-
lowed by the mathematical formulation. Details of
the unsupervised training procedure are described.
The generative model is then decomposed into fea-
ture functions used in a log-linear model which is
trained using a semi-supervised algorithm. Experi-
ments show improvements in word alignment accu-
racy and usage of the generated alignments in hier-
archical and phrasal SMT systems results in an in-
creased BLEU score. Previous work is discussed
and this is followed by the conclusion.
2 LEAF: a generative word alignment
model
2.1 Generative story
We introduce a new generative story which enables
the capture of non-consecutive M-to-N alignment
structure. We have attempted to use the same la-
bels as the generative story for Model 4 (Brown et
51
al., 1993), which we are extending.
Our generative story describes the stochastic gen-
eration of a target string f (sometimes referred to
as the French string, or foreign string) from a source
string e (sometimes referred to as the English string),
consisting of l words. The variable m is the length
of f . We generally use the index i to refer to source
words (ei is the English word at position i), and j to
refer to target words.
Our generative story makes the distinction be-
tween different types of source words. There are
head words, non-head words, and deleted words.
Similarly, for target words, there are head words,
non-head words, and spurious words. A head word
is linked to zero or more non-head words; each non-
head word is linked to from exactly one head word.
The purpose of head words is to try to provide a ro-
bust representation of the semantic features neces-
sary to determine translational correspondence. This
is similar to the use of syntactic head words in sta-
tistical parsers to provide a robust representation of
the syntactic features of a parse sub-tree.
A minimal translational correspondence consists
of a linkage between a source head word and a target
head word (and by implication, the non-head words
linked to them). Deleted source words are not in-
volved in a minimal translational correspondence, as
they were ?deleted? by the translation process. Spu-
rious target words are also not involved in a min-
imal translational correspondence, as they sponta-
neously appeared during the generation of other tar-
get words.
Figure 1 shows a simple example of the stochas-
tic generation of a French sentence from an English
sentence, annotated with the step number in the gen-
erative story.
1. Choose the source word type.
for each i = 1, 2, ..., l choose a word type
?i = ?1 (non-head word), ?i = 0 (deleted
word) or ?i = 1 (head word) according to the
distribution g(?i|ei)
let ?0 = 1
2. Choose the identity of the head word for each
non-head word.
for each i = 1, 2, ..., l if ?i = ?1 choose a
?linked from head word? value ?i (the position
of the head word which ei is linked to) accord-
ing to the distribution w?1(?i ? i|classe(ei))
for each i = 1, 2, ..., l if ?i = 1 let ?i = i
for each i = 1, 2, ..., l if ?i = 0 let ?i = 0
for each i = 1, 2, ..., l if ??i 6= 1 return ?fail-
ure?
3. Choose the identity of the generated target head
word for each source head word.
for each i = 1, 2, ..., l if ?i = 1 choose ?i1
according to the distribution t1(?i1|ei)
4. Choose the number of words in a target cept
conditioned on the identity of the source head
word and the source cept size (?i is 1 if the cept
size is 1, and 2 if the cept size is greater).
for each i = 1, 2, ..., l if ?i = 1 choose a For-
eign cept size ?i according to the distribution
s(?i|ei, ?i)
for each i = 1, 2, ..., l if ?i < 1 let ?i = 0
5. Choose the number of spurious words.
choose ?0 according to the distribution
s0(?0|
?
i ?i)
let m = ?0 +
?l
i=1 ?i
6. Choose the identity of the spurious words.
for each k = 1, 2, ..., ?0 choose ?0k according
to the distribution t0(?0k)
7. Choose the identity of the target non-head
words linked to each target head word.
for each i = 1, 2, ..., l and for each k =
2, 3, ..., ?i choose ?ik according to the distribu-
tion t>1(?ik|ei, classh(?i1))
8. Choose the position of the target head and non-
head words.
for each i = 1, 2, ..., l and for each k =
1, 2, ..., ?i choose a position piik as follows:
? if k = 1 choose pii1 accord-
ing to the distribution d1(pii1 ?
c?i |classe(e?i), classf (?i1))
? if k = 2 choose pii2 according to the dis-
tribution d2(pii2 ? pii1|classf (?i1))
52
source absolutely [comma] they do not want to spend that money
word type (1) DEL. DEL. HEAD non-head HEAD HEAD non-head HEAD HEAD HEAD
linked from (2) THEY do NOT|| WANT to SPEND{{ THAT MONEY
head(3) ILS PAS DESIRENT DEPENSER CET ARGENT
cept size(4) 1 2 1 1 1 1
num spurious(5) 1
spurious(6) aujourd?hui
non-head(7) ILS PAS "" ne DESIRENT DEPENSER CET ARGENT
placement(8) aujourd?hui ILS ne DESIRENT PASww DEPENSER CET ARGENT
spur. placement(9) ILS ne DESIRENT PASww DEPENSER CET ARGENT aujourd?hui
Figure 1: Generative story example, (number) indicates step number
? if k > 2 choose piik according to the dis-
tribution d>2(piik ? piik?1|classf (?i1))
if any position was chosen twice, return ?fail-
ure?
9. Choose the position of the spuriously generated
words.
for each k = 1, 2, ..., ?0 choose a position pi0k
from ?0 ? k + 1 remaining vacant positions in
1, 2, ...,m according to the uniform distribution
let f be the string fpiik = ?ik
We note that the steps which return ?failure? are
required because the model is deficient. Deficiency
means that a portion of the probability mass in the
model is allocated towards generative stories which
would result in infeasible alignment structures. Our
model has deficiency in the non-spurious target word
placement, just as Model 4 does. It has addi-
tional deficiency in the source word linking deci-
sions. (Och and Ney, 2003) presented results sug-
gesting that the additional parameters required to en-
sure that a model is not deficient result in inferior
performance, but we plan to study whether this is
the case for our generative model in future work.
Given e, f and a candidate alignment a, which
represents both the links between source and tar-
get head-words and the head-word connections of
the non-head words, we would like to calculate
p(f, a|e). The formula for this is:
p(f, a|e) =[
l?
i=1
g(?i|ei)]
[
l?
i=1
?(?i,?1)w?1(?i ? i|classe(ei))]
[
l?
i=1
?(?i, 1)t1(?i1|ei)]
[
l?
i=1
?(?i, 1)s(?i|ei, ?i)]
[s0(?0|
l?
i=1
?i)]
[
?0?
k=1
t0(?0k)]
[
l?
i=1
?i?
k=2
t>1(?ik|ei, classh(?i1))]
[
l?
i=1
?i?
k=1
Dik(piik)]
where:
?(i, i?) is the Kronecker delta function which is
equal to 1 if i = i? and 0 otherwise.
?i is the position of the closest English head word
to the left of the word at i or 0 if there is no such
word.
53
classe(ei) is the word class of the English word at
position i, classf (fj) is the word class of the French
word at position j, classh(fj) is the word class of
the French head word at position j.
p0 and p1 are parameters describing the proba-
bility of not generating and of generating a target
spurious word from each non-spurious target word,
p0 + p1 = 1.
m? =
l?
i=1
?i (1)
s0(?0|m?) =
(m?
?0
)
pm???00 p?01 (2)
Dik(j) =
?
???????
???????
d1(j ? c?i |classe(e?i), classf (?ik))
if k = 1
d2(j ? pii1|classf (?ik))
if k = 2
d>2(j ? piik?1|classf (?ik))
if k > 2
(3)
?i = min(2,
l?
i?=1
?(?i? , i)) (4)
ci =
{ ceiling(??ik=1 piik/?i) if ?i 6= 0
0 if ?i = 0 (5)
The alignment structure used in many other mod-
els can be modeled using special cases of this frame-
work. We can express the 1-to-N structure of mod-
els like Model 4 by disallowing ?i = ?1, while for
1-to-1 structure we both disallow ?i = ?1 and de-
terministically set ?i = ?i. We can also specialize
our generative story to the consecutive word M-to-N
alignments used in ?phrase-based? models, though
in this case the conditioning of the generation deci-
sions would be quite different. This involves adding
checks on source and target connection geometry to
the generative story which, if violated, would return
?failure?; naturally this is at the cost of additional
deficiency.
2.2 Unsupervised Parameter Estimation
We can perform maximum likelihood estimation of
the parameters of this model in a similar fashion
to that of Model 4 (Brown et al, 1993), described
thoroughly in (Och and Ney, 2003). We use Viterbi
training (Brown et al, 1993) but neighborhood es-
timation (Al-Onaizan et al, 1999; Och and Ney,
2003) or ?pegging? (Brown et al, 1993) could also
be used.
To initialize the parameters of the generative
model for the first iteration, we use bootstrapping
from a 1-to-N and a M-to-1 alignment. We use the
intersection of the 1-to-N and M-to-1 alignments
to establish the head word relationship, the 1-to-N
alignment to delineate the target word cepts, and the
M-to-1 alignment to delineate the source word cepts.
In bootstrapping, a problem arises when we en-
counter infeasible alignment structure where, for in-
stance, a source word generates target words but no
link between any of the target words and the source
word appears in the intersection, so it is not clear
which target word is the target head word. To ad-
dress this, we consider each of the N generated tar-
get words as the target head word in turn and assign
this configuration 1/N of the counts.
For each iteration of training we search for the
Viterbi solution for millions of sentences. Evidence
that inference over the space of all possible align-
ments is intractable has been presented, for a sim-
ilar problem, in (Knight, 1999). Unlike phrase-
based SMT, left-to-right hypothesis extension using
a beam decoder is unlikely to be effective because in
word alignment reordering is not limited to a small
local window and so the necessary beam would be
very large. We are not aware of admissible or inad-
missible search heuristics which have been shown to
be effective when used in conjunction with a search
algorithm similar to A* search for a model predict-
ing over a structure like ours. Therefore we use
a simple local search algorithm which operates on
complete hypotheses.
(Brown et al, 1993) defined two local search op-
erations for their 1-to-N alignment models 3, 4 and
5. All alignments which are reachable via these
operations from the starting alignment are consid-
ered. One operation is to change the generation de-
cision for a French word to a different English word
(move), and the other is to swap the generation de-
cision for two French words (swap). All possible
operations are tried and the best is chosen. This is
repeated. The search is terminated when no opera-
54
tion results in an improvement. (Och and Ney, 2003)
discussed efficient implementation.
In our model, because the alignment structure is
richer, we define the following operations: move
French non-head word to new head, move English
non-head word to new head, swap heads of two
French non-head words, swap heads of two English
non-head words, swap English head word links of
two French head words, link English word to French
word making new head words, unlink English and
French head words. We use multiple restarts to try to
reduce search errors. (Germann et al, 2004; Marcu
and Wong, 2002) have some similar operations with-
out the head word distinction.
3 Semi-supervised parameter estimation
Equation 6 defines a log-linear model. Each feature
function hm has an associated weight ?m. Given
a vector of these weights ?, the alignment search
problem, i.e. the search to return the best alignment
a? of the sentences e and f according to the model, is
specified by Equation 7.
p?(f, a|e) = exp(
?
m ?mhm(a, e, f))?
a?,f ? exp(
?
m ?mhm(a?, e, f ?))
(6)
a? = argmax
a
?
m
?mhm(f, a, e) (7)
We decompose the new generative model pre-
sented in Section 2 in both translation directions
to provide the initial feature functions for our log-
linear model, features 1 to 10 and 16 to 25 in Table
1.
We use backoffs for the translation decisions (fea-
tures 11 and 26 and the HMM translation tables
which are features 12 and 27) and the target cept size
distributions (features 13, 14, 28 and 29 in Table 1),
as well as heuristics which directly control the num-
ber of unaligned words we generate (features 15 and
30 in Table 1).
We use the semi-supervised EMD algorithm
(Fraser and Marcu, 2006b) to train the model. The
initial M-step bootstraps parameters as described in
Section 2.2 from a M-to-1 and a 1-to-N alignment.
We then perform the D-step following (Fraser and
A B C
D
nnnnnnnnnnnnnn E
@@@@@@@
~~~~~~~
A B C
D
nnnnnnnnnnnnnn E
@@@@@@@
~~~~~~~
Figure 2: Two alignments with the same transla-
tional correspondence
Marcu, 2006b). Given the feature function param-
eters estimated in the M-step and the feature func-
tion weights ? determined in the D-step, the E-step
searches for the Viterbi alignment for the full train-
ing corpus.
We use 1 ? F-Measure as our error criterion.
(Fraser and Marcu, 2006a) established that it is im-
portant to tune ? (the trade-off between Precision
and Recall) to maximize performance. In working
with LEAF, we discovered a methodological prob-
lem with our baseline systems, which is that two
alignments which have the same translational cor-
respondence can have different F-Measures. An ex-
ample is shown in Figure 2.
To overcome this problem we fully interlinked the
transitive closure of the undirected bigraph formed
by each alignment hypothesized by our baseline
alignment systems1. This operation maps the align-
ment shown to the left in Figure 2 to the alignment
shown to the right. This operation does not change
the collection of phrases or rules extracted from a
hypothesized alignment, see, for instance, (Koehn et
al., 2003). Working with this fully interlinked rep-
resentation we found that the best settings of ? were
? = 0.1 for the Arabic/English task and ? = 0.4 for
the French/English task.
4 Experiments
4.1 Data Sets
We perform experiments on two large alignments
tasks, for Arabic/English and French/English data
sets. Statistics for these sets are shown in Table 2.
All of the data used is available from the Linguis-
tic Data Consortium except for the French/English
1All of the gold standard alignments were fully interlinked
as distributed. We did not modify the gold standard alignments.
55
1 chi(?i|ei) source word type 9 d2(4j|classf (fj)) movement for left-most target
non-head word
2 ?(4i|classe(ei)) choosing a head word 10 d>2(4j|classf (fj)) movement for subsequent target
non-head words
3 t1(fj |ei) head word translation 11 t(fj |ei) translation without dependency on word-type
4 s(?i|ei, ?i) ?i is number of words in target cept 12 t(fj |ei) translation table from final HMM iteration
5 s0(?0|
P
i ?i) number of unaligned target words 13 s(?i|?i) target cept size without dependency onsource head word e
6 t0(fj) identity of unaligned target words 14 s(?i|ei) target cept size without dependency on ?i
7 t>1(fj |ei, classh(?i1)) non-head word translation 15 target spurious word penalty
8 d1(4j|classe(e?), classf (fj)) movement for target
head words
16-30 (same features, other direction)
Table 1: Feature functions
gold standard alignments which are available from
the authors.
4.2 Experiments
To build all alignment systems, we start with 5 iter-
ations of Model 1 followed by 4 iterations of HMM
(Vogel et al, 1996), as implemented in GIZA++
(Och and Ney, 2003).
For all non-LEAF systems, we take the best per-
forming of the ?union?, ?refined? and ?intersection?
symmetrization heuristics (Och and Ney, 2003) to
combine the 1-to-N and M-to-1 directions resulting
in a M-to-N alignment. Because these systems do
not output fully linked alignments, we fully link the
resulting alignments as described at the end of Sec-
tion 3. The reader should recall that this does not
change the set of rules or phrases that can be ex-
tracted using the alignment.
We perform one main comparison, which is of
semi-supervised systems, which is what we will use
to produce alignments for SMT. We compare semi-
supervised LEAF with a previous state of the art
semi-supervised system (Fraser and Marcu, 2006b).
We performed translation experiments on the align-
ments generated using semi-supervised training to
verify that the improvements in F-Measure result in
increases in BLEU.
We also compare the unsupervised LEAF sys-
tem with GIZA++ Model 4 to give some idea of
the performance of the unsupervised model. We
made an effort to optimize the free parameters of
GIZA++, while for unsupervised LEAF there are
no free parameters to optimize. A single iteration
of unsupervised LEAF2 is compared with heuristic
2Unsupervised LEAF is equivalent to using the log-linear
model and setting ?m = 1 for m = 1 to 10 and m = 16 to 25,
symmetrization of GIZA++?s extension of Model 4
(which was run for four iterations). LEAF was boot-
strapped as described in Section 2.2 from the HMM
Viterbi alignments.
Results for the experiments on the French/English
data set are shown in Table 3. We ran GIZA++
for four iterations of Model 4 and used the ?re-
fined? heuristic (line 1). We ran the baseline semi-
supervised system for two iterations (line 2), and in
contrast with (Fraser and Marcu, 2006b) we found
that the best symmetrization heuristic for this sys-
tem was ?union?, which is most likely due to our
use of fully linked alignments which was discussed
at the end of Section 3. We observe that LEAF
unsupervised (line 3) is competitive with GIZA++
(line 1), and is in fact competitive with the baseline
semi-supervised result (line 2). We ran the LEAF
semi-supervised system for two iterations (line 4).
The best result is the LEAF semi-supervised system,
with a gain of 1.8 F-Measure over the LEAF unsu-
pervised system.
For French/English translation we use a state of
the art phrase-based MT system similar to (Och and
Ney, 2004; Koehn et al, 2003). The translation test
data is described in Table 2. We use two trigram lan-
guage models, one built using the English portion of
the training data and the other built using additional
English news data. The BLEU scores reported in
this work are calculated using lowercased and tok-
enized data. For semi-supervised LEAF the gain of
0.46 BLEU over the semi-supervised baseline is not
statistically significant (a gain of 0.78 BLEU would
be required), but LEAF semi-supervised compared
with GIZA++ is significant, with a gain of 1.23
BLEU. We note that this shows a large gain in trans-
while setting ?m = 0 for other values of m.
56
ARABIC/ENGLISH FRENCH/ENGLISH
A E F E
TRAINING
SENTS 6,609,162 2,842,184
WORDS 147,165,003 168,301,299 75,794,254 67,366,819
VOCAB 642,518 352,357 149,568 114,907
SINGLETONS 256,778 158,544 60,651 47,765
ALIGN DISCR.
SENTS 1,000 110
WORDS 26,882 37,635 1,888 1,726
LINKS 39,931 2,292
ALIGN TEST
SENTS 83 110
WORDS 1,510 2,030 1,899 1,716
LINKS 2,131 2,176
TRANS. DEV SENTS 728 (4 REFERENCES) 833 (1 REFERENCE)WORDS 18,255 22.0K TO 24.6K 20,562 17,454
TRANS. TEST SENTS 1,056 (4 REFERENCES) 2,380 (1 REFERENCE)WORDS 28,505 35.8K TO 38.1K 58,990 49,182
Table 2: Data sets
lation quality over that obtained using GIZA++ be-
cause BLEU is calculated using only a single refer-
ence for the French/English task.
Results for the Arabic/English data set are also
shown in Table 3. We used a large gold standard
word alignment set available from the LDC. We ran
GIZA++ for four iterations of Model 4 and used the
?union? heuristic. We compare GIZA++ (line 1)
with one iteration of the unsupervised LEAF model
(line 2). The unsupervised LEAF system is worse
than four iterations of GIZA++ Model 4. We be-
lieve that the features in LEAF are too high dimen-
sional to use for the Arabic/English task without the
backoffs available in the semi-supervised models.
The baseline semi-supervised system (line 3) was
run for three iterations and the resulting alignments
were combined with the ?union? heuristic. We ran
the LEAF semi-supervised system for two iterations.
The best result is the LEAF semi-supervised system
(line 4), with a gain of 5.4 F-Measure over the base-
line semi-supervised system.
For Arabic/English translation we train a state of
the art hierarchical model similar to (Chiang, 2005)
using our Viterbi alignments. The translation test
data used is described in Table 2. We use two tri-
gram language models, one built using the English
portion of the training data and the other built using
additional English news data. The test set is from the
NIST 2005 translation task. LEAF had the best per-
formance scoring 1.43 BLEU better than the base-
line semi-supervised system, which is statistically
significant.
5 Previous Work
The LEAF model is inspired by the literature on gen-
erative modeling for statistical word alignment and
particularly by Model 4 (Brown et al, 1993). Much
of the additional work on generative modeling of 1-
to-N word alignments is based on the HMM model
(Vogel et al, 1996). (Toutanova et al, 2002) and
(Lopez and Resnik, 2005) presented a variety of re-
finements of the HMM model particularly effective
for low data conditions. (Deng and Byrne, 2005)
described work on extending the HMM model us-
ing a bigram formulation to generate 1-to-N align-
ment structure. The common thread connecting
these works is their reliance on the 1-to-N approx-
imation, while we have defined a generative model
which does not require use of this approximation, at
the cost of having to rely on local search.
There has also been work on generative models
for other alignment structures. (Wang and Waibel,
1998) introduced a generative story based on ex-
tension of the generative story of Model 4. The
alignment structure modeled was ?consecutive M
to non-consecutive N?. (Marcu and Wong, 2002)
defined the Joint model, which modeled consec-
utive word M-to-N alignments. (Matusov et al,
2004) presented a model capable of modeling 1-to-
N and M-to-1 alignments (but not arbitrary M-to-
N alignments) which was bootstrapped from Model
4. LEAF directly models non-consecutive M-to-N
alignments.
One important aspect of LEAF is its symmetry.
(Och and Ney, 2003) invented heuristic symmetriza-
57
FRENCH/ENGLISH ARABIC/ENGLISH
SYSTEM F-MEASURE (? = 0.4) BLEU F-MEASURE (? = 0.1) BLEU
GIZA++ 73.5 30.63 75.8 51.55
(FRASER AND MARCU, 2006B) 74.1 31.40 79.1 52.89
LEAF UNSUPERVISED 74.5 72.3
LEAF SEMI-SUPERVISED 76.3 31.86 84.5 54.34
Table 3: Experimental Results
tion of the output of a 1-to-N model and a M-to-1
model resulting in a M-to-N alignment, this was ex-
tended in (Koehn et al, 2003). We have used in-
sights from these works to help determine the struc-
ture of our generative model. (Zens et al, 2004)
introduced a model featuring a symmetrized lexi-
con. (Liang et al, 2006) showed how to train two
HMM models, a 1-to-N model and a M-to-1 model,
to agree in predicting all of the links generated, re-
sulting in a 1-to-1 alignment with occasional rare 1-
to-N or M-to-1 links. We improve on these works by
choosing a new structure for our generative model,
the head word link structure, which is both sym-
metric and a robust structure for modeling of non-
consecutive M-to-N alignments.
In designing LEAF, we were also inspired by
dependency-based alignment models (Wu, 1997;
Alshawi et al, 2000; Yamada and Knight, 2001;
Cherry and Lin, 2003; Zhang and Gildea, 2004). In
contrast with their approaches, we have a very flat,
one-level notion of dependency, which is bilingually
motivated and learned automatically from the paral-
lel corpus. This idea of dependency has some sim-
ilarity with hierarchical SMT models such as (Chi-
ang, 2005).
The discriminative component of our work is
based on a plethora of recent literature. This lit-
erature generally views the discriminative modeling
problem as a supervised problem involving the com-
bination of heuristically derived feature functions.
These feature functions generally include the predic-
tion of some type of generative model, such as the
HMM model or Model 4. A discriminatively trained
1-to-N model with feature functions specifically de-
signed for Arabic was presented in (Ittycheriah and
Roukos, 2005). (Lacoste-Julien et al, 2006) created
a discriminative model able to model 1-to-1, 1-to-
2 and 2-to-1 alignments for which the best results
were obtained using features based on symmetric
HMMs trained to agree, (Liang et al, 2006), and
intersected Model 4. (Ayan and Dorr, 2006) de-
fined a discriminative model which learns how to
combine the predictions of several alignment algo-
rithms. The experiments performed included Model
4 and the HMM extensions of (Lopez and Resnik,
2005). (Moore et al, 2006) introduced a discrimi-
native model of 1-to-N and M-to-1 alignments, and
similarly to (Lacoste-Julien et al, 2006) the best re-
sults were obtained using HMMs trained to agree
and intersected Model 4. LEAF is not bound by
the structural restrictions present either directly in
these models, or in the features derived from the
generative models used. We also iterate the gener-
ative/discriminative process, which allows the dis-
criminative predictions to influence the generative
model.
Our work is most similar to work using discrim-
inative log-linear models for alignment, which is
similar to discriminative log-linear models used for
the SMT decoding (translation) problem (Och and
Ney, 2002; Och, 2003). (Liu et al, 2005) presented
a log-linear model combining IBM Model 3 trained
in both directions with heuristic features which re-
sulted in a 1-to-1 alignment. (Fraser and Marcu,
2006b) described symmetrized training of a 1-to-
N log-linear model and a M-to-1 log-linear model.
These models took advantage of features derived
from both training directions, similar to the sym-
metrized lexicons of (Zens et al, 2004), including
features derived from the HMM model and Model
4. However, despite the symmetric lexicons, these
models were only able to optimize the performance
of the 1-to-N model and the M-to-1 model sepa-
rately, and the predictions of the two models re-
quired combination with symmetrization heuristics.
We have overcome the limitations of that work by
defining new feature functions, based on the LEAF
generative model, which score non-consecutive M-
to-N alignments so that the final performance crite-
rion can be optimized directly.
58
6 Conclusion
We have found a new structure over which we can
robustly predict which directly models translational
correspondence commensurate with how it is used
in hierarchical SMT systems. Our new generative
model, LEAF, is able to model alignments which
consist of M-to-N non-consecutive translational cor-
respondences. Unsupervised LEAF is comparable
with a strong baseline. When coupled with a dis-
criminative training procedure, the model leads to
increases between 3 and 9 F-score points in align-
ment accuracy and 1.2 and 2.8 BLEU points in trans-
lation accuracy over strong French/English and Ara-
bic/English baselines.
7 Acknowledgments
This work was partially supported under the GALE
program of the Defense Advanced Research Projects
Agency, Contract No. HR0011-06-C-0022. We
would like to thank the USC Center for High Per-
formance Computing and Communications.
References
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin
Knight, John D. Lafferty, I. Dan Melamed, David
Purdy, Franz J. Och, Noah A. Smith, and David
Yarowsky. 1999. Statistical machine translation, final
report, JHU workshop.
Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas.
2000. Learning dependency translation models as col-
lections of finite state head transducers. Computa-
tional Linguistics, 26(1):45?60.
Necip Fazil Ayan and Bonnie J. Dorr. 2006. A maxi-
mum entropy approach to combining word alignments.
In Proceedings of HLT-NAACL, pages 96?103, New
York.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and R. L. Mercer. 1993. The mathematics of
statistical machine translation: Parameter estimation.
Computational Linguistics, 19(2):263?311.
Colin Cherry and Dekang Lin. 2003. A probability
model to improve word alignment. In Proceedings of
ACL, pages 88?95, Sapporo, Japan.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL, pages 263?270, Ann Arbor, MI.
Yonggang Deng and William Byrne. 2005. Hmm word
and phrase alignment for statistical machine trans-
lation. In Proceedings of HLT-EMNLP, Vancouver,
Canada.
Alexander Fraser and Daniel Marcu. 2006a. Measuring
word alignment quality for statistical machine transla-
tion. In Technical Report ISI-TR-616, ISI/University
of Southern California.
Alexander Fraser and Daniel Marcu. 2006b. Semi-
supervised training for statistical word alignment. In
Proceedings of COLING-ACL, pages 769?776, Syd-
ney, Australia.
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel
Marcu, and Kenji Yamada. 2004. Fast decoding and
optimal decoding for machine translation. Artificial
Intelligence, 154(1-2):127?143.
Abraham Ittycheriah and Salim Roukos. 2005. A max-
imum entropy word aligner for Arabic-English ma-
chine translation. In Proceedings of HLT-EMNLP,
pages 89?96, Vancouver, Canada.
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Computational Lin-
guistics, 25(4):607?615.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
HLT-NAACL, pages 127?133, Edmonton, Canada.
Simon Lacoste-Julien, Dan Klein, Ben Taskar, and
Michael Jordan. 2006. Word alignment via quadratic
assignment. In Proceedings of HLT-NAACL, pages
112?119, New York, NY.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of HLT-NAACL,
New York.
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-linear
models for word alignment. In Proceedings of ACL,
pages 459?466, Ann Arbor, MI.
Adam Lopez and Philip Resnik. 2005. Improved hmm
alignment models for languages with scarce resources.
In Proceedings of the ACL Workshop on Building and
Using Parallel Texts, pages 83?86, Ann Arbor, MI.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine trans-
lation. In Proceedings of EMNLP, pages 133?139,
Philadelphia, PA.
Evgeny Matusov, Richard Zens, and Hermann Ney.
2004. Symmetric word alignments for statistical
machine translation. In Proceedings of COLING,
Geneva, Switzerland.
59
Robert C. Moore, Wen-Tau Yih, and Andreas Bode.
2006. Improved discriminative bilingual word align-
ment. In Proceedings of COLING-ACL, pages 513?
520, Sydney, Australia.
Franz J. Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proceedings of ACL, pages
295?302, Philadelphia, PA.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(1):417?449.
Franz J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of ACL,
pages 160?167, Sapporo, Japan.
Kristina Toutanova, H. Tolga Ilhan, and Christopher D.
Manning. 2002. Extensions to hmm-based statistical
word alignment models. In Proceedings of EMNLP,
Philadelphia, PA.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of COLING, pages 836?841,
Copenhagen, Denmark.
Ye-Yi Wang and Alex Waibel. 1998. Modeling with
structures in statistical machine translation. In Pro-
ceedings of COLING-ACL, volume 2, pages 1357?
1363, Montreal, Canada.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of ACL,
pages 523?530, Toulouse, France.
Richard Zens, Evgeny Matusov, and Hermann Ney.
2004. Improved word alignment using a symmetric
lexicon model. In Proceedings of COLING, Geneva,
Switzerland.
Hao Zhang and Daniel Gildea. 2004. Syntax-based
alignment: Supervised or unsupervised? In Proceed-
ings of COLING, Geneva, Switzerland.
60
A Smorgasbord of Features for Statistical Machine Translation
Franz Josef Och
USC/ISI
Daniel Gildea
U. of Rochester
Sanjeev Khudanpur
Johns Hopkins U.
Anoop Sarkar
Simon Fraser U.
Kenji Yamada
Xerox/XRCE
Alex Fraser
USC/ISI
Shankar Kumar
Johns Hopkins U.
Libin Shen
U. of Pennsylvania
David Smith
Johns Hopkins U.
Katherine Eng
Stanford U.
Viren Jain
U. of Pennsylvania
Zhen Jin
Mt. Holyoke
Dragomir Radev
U. of Michigan
Abstract
We describe a methodology for rapid exper-
imentation in statistical machine translation
which we use to add a large number of features
to a baseline system exploiting features from a
wide range of levels of syntactic representation.
Feature values were combined in a log-linear
model to select the highest scoring candidate
translation from an n-best list. Feature weights
were optimized directly against the BLEU eval-
uation metric on held-out data. We present re-
sults for a small selection of features at each
level of syntactic representation.
1 Introduction
Despite the enormous progress in machine translation
(MT) due to the use of statistical techniques in recent
years, state-of-the-art statistical systems often produce
translations with obvious errors. Grammatical errors in-
clude lack of a main verb, wrong word order, and wrong
choice of function words. Frequent problems of a less
grammatical nature include missing content words and
incorrect punctuation.
In this paper, we attempt to address these problems by
exploring a variety of new features for scoring candidate
translations. A high-quality statistical translation system
is our baseline, and we add new features to the exist-
ing set, which are then combined in a log-linear model.
To allow an easy integration of new features, the base-
line system provides an n-best list of candidate transla-
tions which is then reranked using the new features. This
framework allows us to incorporate different types of fea-
tures, including features based on syntactic analyses of
the source and target sentences, which we hope will ad-
dress the grammaticality of the translations, as well as
lower-level features. As we work on n-best lists, we can
easily use global sentence-level features.
We begin by describing our baseline system and the
n-best rescoring framework within which we conducted
our experiments. We then present a selection of new fea-
tures, progressing from word-level features to those based
to part-of-speech tags and syntactic chunks, and then to
features based on Treebank-based syntactic parses of the
source and target sentences.
2 Log-linear Models for Statistical MT
The goal is the translation of a text given in some source
language into a target language. We are given a source
(?Chinese?) sentence f = fJ1 = f1, . . . , fj , . . . , fJ ,
which is to be translated into a target (?English?) sentence
e = eI1 = e1, . . . , ei, . . . , eI Among all possible target
sentences, we will choose the sentence with the highest
probability:
e?I1 = argmax
eI1
{Pr(eI1|f
J
1 )} (1)
As an alternative to the often used source-channel ap-
proach (Brown et al, 1993), we directly model the pos-
terior probability Pr(eI1|fJ1 ) (Och and Ney, 2002) us-
ing a log-linear combination of feature functions. In
this framework, we have a set of M feature functions
hm(eI1, f
J
1 ),m = 1, . . . ,M . For each feature function,
there exists a model parameter ?m,m = 1, . . . ,M . The
direct translation probability is given by:
Pr(eI1|f
J
1 ) =
exp[
?M
m=1 ?mhm(e
I
1, f
J
1 )]
?
e?I1
exp[
?M
m=1 ?mhm(e
?I
1, f
J
1 )]
(2)
We obtain the following decision rule:
e?I1 = argmax
eI1
{ M?
m=1
?mhm(e
I
1, f
J
1 )
}
(3)
The standard criterion for training such a log-linear
model is to maximize the probability of the parallel train-
ing corpus consisting of S sentence pairs {(fs, es) : s =
1, . . . , S}. However, this does not guarantee optimal per-
formance on the metric of translation quality by which
our system will ultimately be evaluated. For this reason,
we optimize the parameters directly against the BLEU
metric on held-out data. This is a more difficult optimiza-
tion problem, as the search space is no longer convex.
Figure 1: Example segmentation of Chinese sentence and
its English translation into alignment templates.
However, certain properties of the BLEU metric can be
exploited to speed up search, as described in detail by
Och (2003). We use this method of optimizing feature
weights throughout this paper.
2.1 Baseline MT System: Alignment Templates
Our baseline MT system is the alignment template system
described in detail by Och, Tillmann, and Ney (1999)
and Och and Ney (2004). In the following, we give a
short description of this baseline model.
The probability model of the alignment template sys-
tem for translating a sentence can be thought of in distinct
stages. First, the source sentence words fJ1 are grouped to
phrases f?K1 . For each phrase f? an alignment template z is
chosen and the sequence of chosen alignment templates
is reordered (according to piK1 ). Then, every phrase f?
produces its translation e? (using the corresponding align-
ment template z). Finally, the sequence of phrases e?K1
constitutes the sequence of words eI1.
Our baseline system incorporated the following feature
functions:
Alignment Template Selection Each alignment
template is chosen with probability p(z|f?), estimated by
relative frequency. The corresponding feature function in
our log-linear model is the log probability of the product
of p(z|f?) for all used alignment templates used.
Word Selection This feature is based on the lexical
translation probabilities p(e|f), estimated using relative
frequencies according to the highest-probability word-
level alignment for each training sentence. A translation
probability conditioned on the source and target position
within the alignment template p(e|f, i, j) is interpolated
with the position-independent probability p(e|f).
Phrase Alignment This feature favors monotonic
alignment at the phrase level. It measures the ?amount
of non-monotonicity? by summing over the distance (in
the source language) of alignment templates which are
consecutive in the target language.
Language Model Features As a language model
feature, we use a standard backing off word-based tri-
gram language model (Ney, Generet, and Wessel, 1995).
The baseline system actually includes four different lan-
guage model features trained on four different corpora:
the news part of the bilingual training data, a large Xin-
hua news corpus, a large AFP news corpus, and a set of
Chinese news texts downloaded from the web.
Word/Phrase Penalty This word penalty feature
counts the length in words of the target sentence. Without
this feature, the sentences produced tend to be too short.
The phrase penalty feature counts the number of phrases
produced, and can allow the model to prefer either short
or long phrases.
Phrases from Conventional Lexicon The baseline
alignment template system makes use of the Chinese-
English lexicon provided by LDC. Each lexicon entry is
a potential phrase translation pair in the alignment tem-
plate system. To score the use of these lexicon entries
(which have no normal translation probability), this fea-
ture function counts the number of times such a lexicon
entry is used.
Additional Features A major advantage of the log-
linear modeling approach is that it is easy to add new
features. In this paper, we explore a variety of features
based on successively deeper syntactic representations of
the source and target sentences, and their alignment. For
each of the new features discussed below, we added the
feature value to the set of baseline features, re-estimated
feature weights on development data, and obtained re-
sults on test data.
3 Experimental Framework
We worked with the Chinese-English data from the recent
evaluations, as both large amounts of sentence-aligned
training corpora and multiple gold standard reference
translations are available. This is a standard data set,
making it possible to compare results with other systems.
In addition, working on Chinese allows us to use the ex-
isting Chinese syntactic treebank and parsers based on it.
For the baseline MT system, we distinguish the fol-
lowing three different sentence- or chunk-aligned parallel
training corpora:
? training corpus (train): This is the basic training
corpus used to train the alignment template transla-
tion model (word lexicon and phrase lexicon). This
corpus consists of about 170M English words. Large
parts of this corpus are aligned on a sub-sentence
level to avoid the existence of very long sentences
which would be filtered out in the training process
to allow a manageable word alignment training.
? development corpus (dev): This is the training cor-
pus used in discriminative training of the model-
parameters of the log-linear translation model. In
most experiments described in this report this cor-
pus consists of 993 sentences (about 25K words) in
both languages.
? test corpus (test): This is the test corpus used to
assess the quality of the newly developed feature
functions. It consists of 878 sentences (about 25K
words).
For development and test data, we have four English (ref-
erence) translations for each Chinese sentence.
3.1 Reranking, n-best lists, and oracles
For each sentence in the development, test, and the blind
test corpus a set of 16,384 different alternative transla-
tions has been produced using the baseline system. For
extracting the n-best candidate translations, an A* search
is used. These n-best candidate translations are the basis
for discriminative training of the model parameters and
for re-ranking.
We used n-best reranking rather than implementing
new search algorithms. The development of efficient
search algorithms for long-range dependencies is very
complicated and a research topic in itself. The rerank-
ing strategy enabled us to quickly try out a lot of new
dependencies, which would not have been be possible if
the search algorithm had to be changed for each new de-
pendency.
On the other hand, the use of n-best list rescoring lim-
its the possibility of improvements to what is available
in the n-best list. Hence, it is important to analyze the
quality of the n-best lists by determining how much of an
improvement would be possible given a perfect reranking
algorithm. We computed the oracle translations, that is,
the set of translations from our n-best list that yields the
best BLEU score.1
We use the following two methods to compute the
BLEU score of an oracle translation:
1. optimal oracle (opt): We select the oracle sentences
which give the highest BLEU score compared to the
set of 4 reference translations. Then, we compute
BLEU score of oracle sentences using the same set
of reference translations.
2. round-robin oracle (rr): We select four differ-
ent sets of oracle sentences which give the highest
BLEU score compared to each of the 4 references
translations. Then, we compute for each set of or-
acle sentences a BLEU score using always those
three references to score that have not been cho-
sen to select the oracle. Then, these 4 3-reference
BLEU scores are averaged.
1Note that due to the corpus-level holistic nature of the
BLEU score it is not trivial to compute the optimal set of oracle
translations. We use a greedy search algorithm for the oracle
translations that might find only a local optimum. Empirically,
we do not observe a dependence on the starting point, hence we
believe that this does not pose a significant problem.
Table 1: Oracle BLEU scores for different sizes of the
n-best list. The avBLEUr3 scores are computed with
respect to three reference translations averaged over the
four different choices of holding out one reference.
avBLEUr3[%] BLEUr4
n rr opt opt
human 35.8 -
1 28.3 28.3 31.6
4 29.1 30.8 34.5
16 29.9 33.2 37.3
64 30.6 35.6 38.7
256 31.3 37.8 42.8
1024 31.7 40.0 45.3
4096 32.0 41.8 47.3
The first method provides the theoretical upper bound of
what BLEU score can be obtained by rescoring a given n-
best list. Using this method with a 1000-best list, we ob-
tain oracle translations that outperform the BLEU score
of the human translations. The oracle translations achieve
113% against the human BLEU score on the test data
(Table 1), while the first best translations obtain 79.2%
against the human BLEU score. The second method uses
a different references for selection and scoring. Here, us-
ing an 1000-best list, we obtain oracle translations with a
relative human BLEU score of 88.5%.
Based on the results of the oracle experiment, and
in order to make rescoring computationally feasible for
features requiring significant computation for each hy-
pothesis, we used the top 1000 translation candidates for
our experiments. The baseline system?s BLEU score is
31.6% on the test set (equivalent to the 1-best oracle in
Table 1). This is the benchmark against which the contri-
butions of the additional features described in the remain-
der of this paper are to be judged.
3.2 Preprocessing
As a precursor to developing the various syntactic fea-
tures described in this report, the syntactic represen-
tations on which they are based needed to be com-
puted. This involved part-of-speech tagging, chunking,
and parsing both the Chinese and English side of our
training, development, and test sets.
Applying the part-of-speech tagger to the often un-
grammatical MT output from our n-best lists sometimes
led to unexpected results. Often the tagger tries to ?fix
up? ungrammatical sentences, for example by looking for
a verb when none is present:
China NNP 14 CD open JJ border NN
cities NNS achievements VBZ remarkable JJ
Here, although achievements has never been seen as a
verb in the tagger?s training data, the prior for a verb
in this position is high enough to cause a present tense
verb tag to be produced. In addition to the inaccura-
cies of the MT system, the difference in genre from the
tagger?s training text can cause problems. For example,
while our MT data include news article headlines with no
verb, headlines are not included in the Wall Street Journal
text on which the tagger is trained. Similarly, the tagger
is trained on full sentences with normalized punctuation,
leading it to expect punctuation at the end of every sen-
tence, and produce a punctuation tag even when the evi-
dence does not support it:
China NNP ?s POS economic JJ
development NN and CC opening VBG
up RP 14 CD border NN cities NNS
remarkable JJ achievements .
The same issues affect the parser. For example the
parser can create verb phrases where none exist, as in the
following example in which the tagger correctly did not
identify a verb in the sentence:
These effects have serious implications for designing
syntactic feature functions. Features such ?is there a verb
phrase? may not do what you expect. One solution would
be features that involve the probability of a parse subtree
or tag sequence, allowing us to ask ?how good a verb
phrase is it?? Another solution is more detailed features
examining more of the structure, such as ?is there a verb
phrase with a verb??
4 Word-Level Feature Functions
These features, directly based on the source and target
strings of words, are intended to address such problems as
translation choice, missing content words, and incorrect
punctuation.
4.1 Model 1 Score
We used IBM Model 1 (Brown et al, 1993) as one of the
feature functions. Since Model 1 is a bag-of-word trans-
lation model and it gives the sum of all possible alignment
probabilities, a lexical co-occurrence effect, or triggering
effect, is expected. This captures a sort of topic or seman-
tic coherence in translations.
As defined by Brown et al (1993), Model 1 gives a
probability of any given translation pair, which is
p(f |e; M1) =

(l + 1)m
m?
j=1
l?
i=0
t(fj |ei).
We used GIZA++ to train the model. The training data is
a subset (30 million words on the English side) of the en-
tire corpus that was used to train the baseline MT system.
For a missing translation word pair or unknown words,
where t(fj |ei) = 0 according to the model, a constant
t(fj |ei) = 10?40 was used as a smoothing value.
The average %BLEU score (average of the best four
among different 20 search initial points) is 32.5. We also
tried p(e|f ; M1) as feature function, but did not obtain
improvements which might be due to an overlap with the
word selection feature in the baseline system.
The Model 1 score is one of the best performing fea-
tures. It seems to ?fix? the tendency of our baseline sys-
tem to delete content words and it improves word selec-
tion coherence by the triggering effect. It is also possible
that the triggering effect might work on selecting a proper
verb-noun combination, or a verb-preposition combina-
tion.
4.2 Lexical Re-ordering of Alignment Templates
As shown in Figure 1 the alignment templates (ATs)
used in the baseline system can appear in various con-
figurations which we will call left/right-monotone and
left/right-continuous. We built 2 out of these 4 models to
distinguish two types of lexicalized re-ordering of these
ATs:
The left-monotone model computes the total proba-
bility of all ATs being left monotone: where the lower
left corner of the AT touches the upper right corner of the
previous AT. Note that the first word in the current AT
may or may not immediately follow the last word in the
previous AT. The total probability is the product over all
alignment templates i, either P (ATi is left-monotone) or
1 ? P (ATi is left-monotone).
The right-continuous model computes the total prob-
ability of all ATs being right continuous: where the
lower left corner of the AT touches the upper right cor-
ner of the previous AT and the first word in the cur-
rent AT immediately follows the last word in the pre-
vious AT. The total probability is the product over all
alignment templates i, either P (ATi is right-continuous)
or 1 ? P (ATi is right-continuous).
In both models, the probabilities P have been esti-
mated from the full training data (train).
5 Shallow Syntactic Feature Functions
By shallow syntax, we mean the output of the part-of-
speech tagger and chunkers. We hope that such features
can combine the strengths of tag- and chunk-based trans-
lation systems (Schafer and Yarowsky, 2003) with our
baseline system.
5.1 Projected POS Language Model
This feature uses Chinese POS tag sequences as surro-
gates for Chinese words to model movement. Chinese
words are too sparse to model movement, but an attempt
to model movement using Chinese POS may be more
successful. We hope that this feature will compensate for
a weak model of word movement in the baseline system.
Chinese POS sequences are projected to English us-
ing the word alignment. Relative positions are indicated
for each Chinese tag. The feature function was also tried
without the relative positions:
CD +0 M +1 NN +3 NN -1 NN +2 NN +3
14 (measure) open border cities
The table shows an example tagging of an English hy-
pothesis showing how it was generated from the Chinese
sentence. The feature function is the log probability out-
put by a trigram language model over this sequence. This
is similar to the HMM Alignment model (Vogel, Ney, and
Tillmann, 1996) but in this case movement is calculated
on the basis of parts of speech.
The Projected POS feature function was one of the
strongest performing shallow syntactic feature functions,
with a %BLEU score of 31.8. This feature function can
be thought of as a trade-off between purely word-based
models, and full generative models based upon shallow
syntax.
6 Tree-Based Feature Functions
Syntax-based MT has shown promise in the
work of, among others, Wu and Wong (1998) and
Alshawi, Bangalore, and Douglas (2000). We hope that
adding features based on Treebank-based syntactic
analyses of the source and target sentences will address
grammatical errors in the output of the baseline system.
6.1 Parse Tree Probability
The most straightforward way to integrate a statistical
parser in the system would be the use of the (log of the)
parser probability as a feature function. Unfortunately,
this feature function did not help to obtain better results
(it actually seems to significantly hurt performance).
To analyze the reason for this, we performed an ex-
periment to test if the used statistical parser assigns a
higher probability to presumably grammatical sentences.
The following table shows the average log probability as-
signed by the Collins parser to the 1-best (produced), or-
acle and the reference translations:
Hypothesis 1-best Oracle Reference
log(parseProb) -147.2 -148.5 -154.9
We observe that the average parser log-probability of
the 1-best translation is higher than the average parse
log probability of the oracle or the reference translations.
Hence, it turns out that the parser is actually assigning
higher probabilities to the ungrammatical MT output than
to the presumably grammatical human translations. One
reason for that is that the MT output uses fewer unseen
words and typically more frequent words which lead to
a higher language model probability. We also performed
experiments to balance this effect by dividing the parser
probability by the word unigram probability and using
this ?normalized parser probability? as a feature function,
but also this did not yield improvements.
6.2 Tree-to-String Alignment
A tree-to-string model is one of several syntax-
based translation models used. The model is a
conditional probability p(f |T (e)). Here, we used
a model defined by Yamada and Knight (2001) and
Yamada and Knight (2002).
Internally, the model performs three types of opera-
tions on each node of a parse tree. First, it reorders the
child nodes, such as changing VP ? VB NP PP into
VP ? NP PP VB. Second, it inserts an optional word at
each node. Third, it translates the leaf English words into
Chinese words. These operations are stochastic and their
probabilities are assumed to depend only on the node, and
are independent of other operations on the node, or other
nodes. The probability of each operation is automatically
obtained by a training algorithm, using about 780,000 En-
glish parse tree-Chinese sentence pairs. The probability
of these operations ?(eki,j) is assumed to depend on the
edge of the tree being modified, eki,j , but independent of
everything else, giving the following equation,
p(f |T (e)) =
?
?
?
?(eki,j)
p(?(eki,j)|e
k
i,j) (4)
where ? varies over the possible alignments between the
f and e and ?(eki,j) is the particular operations (in ?) for
the edge eki,j .
The model is further extended to incorporate phrasal
translations performed at each node of the input parse
tree (Yamada and Knight, 2002). An English phrase cov-
ered by a node can be directly translated into a Chinese
phrase without regular reorderings, insertions, and leaf-
word translations.
The model was trained using about 780,000 English
parse tree-Chinese sentence pairs. There are about 3 mil-
lion words on the English side, and they were parsed by
Collins? parser.
Since the model is computationally expensive, we
added some limitations on the model operations. As the
base MT system does not produce a translation with a
big word jump, we restrict the model not to reorder child
nodes when the node covers more than seven words. For
a node that has more than four children, the reordering
probability is set to be uniform. We also introduced prun-
ing, which discards partial (subtree-substring) alignments
if the probability is lower than a threshold.
The model gives a sum of all possible alignment prob-
abilities for a pair of a Chinese sentence and an English
parse tree. We also calculate the probability of the best
alignment according to the model. Thus, we have the fol-
lowing two feature functions:
hTreeToStringSum(e, f) = log(
?
?
?
?(eki,j)
p(?(eki,j)|e
k
i,j))
hTreeToStringViterbi(e, f) = log(max
?
?
?(eki,j)
p(?(eki,j)|e
k
i,j))
As the model is computationally expensive, we sorted the
n-best list by the sentence length, and processed them
from the shorter ones to the longer ones. We used 10
CPUs for about five days, and 273/997 development sen-
tences and 237/878 test sentences were processed.
The average %BLEU score (average of the best four
among different 20 search initial points) was 31.7 for
both hTreeToStringSum and hTreeToStringViterbi. Among the pro-
cessed development sentences, the model preferred the
oracle sentences over the produced sentence in 61% of
the cases.
The biggest problem of this model is that it is compu-
tationally very expensive. It processed less than 30% of
the n-best lists in long CPU hours. In addition, we pro-
cessed short sentences only. For long sentences, it is not
practical to use this model as it is.
6.3 Tree-to-Tree Alignment
A tree-to-tree translation model makes use of syntac-
tic tree for both the source and target language. As in
the tree-to-string model, a set of operations apply, each
with some probability, to transform one tree into another.
However, when training the model, trees for both the
source and target languages are provided, in our case
from the Chinese and English parsers.
We began with the tree-to-tree alignment model pre-
sented by Gildea (2003). The model was extended to han-
dle dependency trees, and to make use of the word-level
alignments produced by the baseline MT system. The
probability assigned by the tree-to-tree alignment model,
given the word-level alignment with which the candidate
translation was generated, was used as a feature in our
rescoring system.
We trained the parameters of the tree transformation
operations on 42,000 sentence pairs of parallel Chinese-
English data from the Foreign Broadcast Information Ser-
vice (FBIS) corpus. The lexical translation probabili-
ties Pt were trained using IBM Model 1 on the 30 mil-
lion word training corpus. This was done to overcome
the sparseness of the lexical translation probabilities es-
timated while training the tree-to-tree model, which was
not able to make use of as much training data.
As a test of the tree-to-tree model?s discrimination, we
performed an oracle experiment, comparing the model
scores on the first sentence in the n-best list with candi-
date giving highest BLEU score. On the 1000-best list for
the 993-sentence development set, restricting ourselves
to sentences with no more than 60 words and a branching
factor of no more than five in either the Chinese or En-
glish tree, we achieved results for 480, or 48% of the 993
sentences. Of these 480, the model preferred the pro-
duced over the oracle 52% of the time, indicating that
it does not in fact seem likely to significantly improve
BLEU scores when used for reranking. Using the prob-
ability of the source Chinese dependency parse aligning
with the n-best hypothesis dependency parse as a feature
function, making use of the word-level alignments, yields
a 31.6 %BLEU score ? identical to our baseline.
6.4 Markov Assumption for Tree Alignments
The tree-based feature functions described so far have the
following limitations: full parse tree models are expen-
sive to compute for long sentences and for trees with flat
constituents and there is limited reordering observed in
the n-best lists that form the basis of our experiments. In
addition to this, higher levels of parse tree are rarely ob-
served to be reordered between source and target parse
trees.
In this section we attack these problems using a simple
Markov model for tree-based alignments. It guarantees
tractability: compared to a coverage of approximately
30% of the n-best list by the unconstrained tree-based
models, using the Markov model approach provides 98%
coverage of the n-best list. In addition, this approach is
robust to inaccurate parse trees.
The algorithm works as follows: we start with word
alignments and two parameters: n for maximum number
of words in tree fragment and k for maximum height of
tree fragment. We proceed from left to right in the Chi-
nese sentence and incrementally grow a pair of subtrees,
one subtree in Chinese and the other in English, such that
each word in the Chinese subtree is aligned to a word in
the English subtree. We grow this pair of subtrees un-
til we can no longer grow either subtree without violat-
ing the two parameter values n and k. Note that these
aligned subtree pairs have properties similar to alignment
templates. They can rearrange in complex ways between
source and target. Figure 2 shows how subtree-pairs for
parameters n = 3 and k = 3 can be drawn for this
sentence pair. In our experiments, we use substantially
bigger tree fragments with parameters set to n = 8 and
k = 9.
Once these subtree-pairs have been obtained, we can
easily assert a Markov assumption for the tree-to-tree and
tree-to-string translation models that exploits these pair-
ings. Let consider a sentence pair in which we have dis-
covered n subtree-pairs which we can call Frag0, . . .,
Fragn. We can then compute a feature function for the
sentence pair using the tree-to-string translation model as
follows:
hMarkovTreeToString =
logPtree-to-string(Frag0) + . . . + logPtree-to-string(Fragn)
Using this Markov assumption on tree alignments with
Figure 2: Markov assumption on tree alignments.
the Tree to String model described in Section 6.2 we ob-
tain a coverage improvement to 98% coverage from the
original 30%. The accuracy of the tree to string model
also improved with a %BLEU score of 32.0 which is the
best performing single syntactic feature.
6.5 Using TAG elementary trees for scoring word
alignments
In this section, we consider another method for carving
up the full parse tree. However, in this method, instead of
subtree-pairs we consider a decomposition of parse trees
that provides each word with a fragment of the original
parse tree as shown in Figure 3. The formalism of Tree-
Adjoining Grammar (TAG) provides the definition what
each tree fragment should be and in addition how to de-
compose the original parse trees to provide the fragments.
Each fragment is a TAG elementary tree and the compo-
sition of these TAG elementary trees in a TAG deriva-
tion tree provides the decomposition of the parse trees.
The decomposition into TAG elementary trees is done by
augmenting the parse tree for source and target sentence
with head-word and argument (or complement) informa-
tion using heuristics that are common to most contempo-
rary statistical parsers and easily available for both En-
glish and Chinese. Note that we do not use the word
alignment information for the decomposition into TAG
elementary trees.
Once we have a TAG elementary tree per word,
we can create several models that score word align-
ments by exploiting the alignments between TAG ele-
mentary trees between source and target. Let tfi and
tei be the TAG elementary trees associated with the
aligned words fi and ei respectively. We experimented
with two models over alignments: unigram model over
alignments:
?
i P (fi, tfi , ei, tei) and conditional model:?
i P (ei, tei | fi, tfi) ? P (fi+1, tfi+1 | fi, tfi)
We trained both of these models using the SRI Lan-
guage Modeling Toolkit using 60K aligned parse trees.
We extracted 1300 TAG elementary trees each for Chi-
Figure 3: Word alignments with TAG elementary trees.
nese and for English. The unigram model gets a %BLEU
score of 31.7 and the conditional model gets a %BLEU
score of 31.9.
%BLEU
Baseline 31.6
IBM Model 1 p(f |e) 32.5
Tree-to-String Markov fragments 32.0
Right-continuous alignment template 32.0
TAG conditional bigrams 31.9
Left-monotone alignment template 31.9
Projected POS LM 31.8
Tree-to-String 31.7
TAG unigram 31.7
Tree-to-Tree 31.6
combination 32.9
Table 2: Results for the baseline features, each new fea-
ture added to the baseline features on its own, and a com-
bination of new features.
7 Conclusions
The use of discriminative reranking of an n-best list pro-
duced with a state-of-the-art statistical MT system al-
lowed us to rapidly evaluate the benefits of off-the-shelf
parsers, chunkers, and POS taggers for improving syntac-
tic well-formedness of the MT output. Results are sum-
marized in Table 2; the best single new feature improved
the %BLEU score from 31.6 to 32.5. The 95% confi-
dence intervals computed with the bootstrap resampling
method are about 0.8%. In addition to experiments with
single features we also integrated multiple features using
a greedy approach where we integrated at each step the
feature that most improves the BLEU score. This feature
integration produced a statistically significant improve-
ment of absolute 1.3% to 32.9 %BLEU score.
Our single best feature, and in fact the only single fea-
ture to produce a truly significant improvement, was the
IBM Model 1 score. We attribute its success that it ad-
dresses the weakness of the baseline system to omit con-
tent words and that it improves word selection by em-
ploying a triggering effect. We hypothesize that this al-
lows for better use of context in, for example, choosing
among senses of the source language word.
A major goal of this work was to find out if we can ex-
ploit annotated data such as treebanks for Chinese and
English and make use of state-of-the-art deep or shal-
low parsers to improve MT quality. Unfortunately, none
of the implemented syntactic features achieved a statisti-
cally significant improvement in the BLEU score. Poten-
tial reasons for this might be:
? As described in Section 3.2, the use of off-the-shelf
taggers and parsers has various problems due to vari-
ous mismatches between the parser training data and
our application domain. This might explain that the
use of the parser probability as feature function was
not successful. A potential improvement might be to
adapt the parser by retraining it on the full training
data that has been used by the baseline system.
? The use of a 1000-best list limits the potential im-
provements. It is possible that more improvements
could be obtained using a larger n-best list or a word
graph representation of the candidates.
? The BLEU score is possibly not sufficiently sensi-
tive to the grammaticality of MT output. This could
not only make it difficult to see an improvement in
the system?s output, but also potentially mislead the
BLEU-based optimization of the feature weights. A
significantly larger corpus for discriminative train-
ing and for evaluation would yield much smaller
confidence intervals.
? Our discriminative training technique, which di-
rectly optimizes the BLEU score on a development
corpus, seems to have overfitting problems with
large number of features. One could use a larger de-
velopment corpus for discriminative training or in-
vestigate alternative discriminative training criteria.
? The amount of annotated data that has been used
to train the taggers and parsers is two orders of
magnitude smaller than the parallel training data
that has been used to train the baseline system (or
the word-based features). Possibly, a comparable
amount of annotated data (e.g. a treebank with 100
million words) is needed to obtain significant im-
provements.
This is the first large scale integration of syntactic analy-
sis operating on many different levels with a state-of-the-
art phrase-based MT system. The methodology of using
a log-linear feature combination approach, discriminative
reranking of n-best lists computed with a state-of-the-art
baseline system allowed members of a large team to si-
multaneously experiment with hundreds of syntactic fea-
ture functions on a common platform.
Acknowledgments
This material is based upon work supported by the Na-
tional Science Foundation under Grant No. 0121285.
References
Alshawi, Hiyan, Srinivas Bangalore, and Shona Douglas. 2000.
Learning dependency translation models as collections of
finite state head transducers. Computational Linguistics,
26(1):45?60.
Brown, Peter F., Stephen A. Della Pietra, Vincent J. Della
Pietra, and R. L. Mercer. 1993. The mathematics of sta-
tistical machine translation: Parameter estimation. Compu-
tational Linguistics, 19(2):263?311.
Gildea, Daniel. 2003. Loosely tree-based alignment for ma-
chine translation. In Proc. of the 41st Annual Meeting of the
Association for Computational Linguistics (ACL), Sapporo,
Japan.
Ney, Hermann, M. Generet, and Frank Wessel. 1995. Ex-
tensions of absolute discounting for language modeling. In
Proc. of the Fourth European Conf. on Speech Communica-
tion and Technology, Madrid, Spain.
Och, Franz Josef. 2003. Minimum error rate training in statisti-
cal machine translation. In Proc. of the 41st Annual Meeting
of the Association for Computational Linguistics (ACL), Sap-
poro, Japan.
Och, Franz Josef and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical ma-
chine translation. In Proc. of the 40th Annual Meeting of the
Association for Computational Linguistics (ACL), Philadel-
phia, PA.
Och, Franz Josef and Hermann Ney. 2004. The alignment tem-
plate approach to statistical machine translation. Computa-
tional Linguistics. Accepted for Publication.
Och, Franz Josef, Christoph Tillmann, and Hermann Ney.
1999. Improved alignment models for statistical machine
translation. In Proc. of the Joint SIGDAT Conf. on Empiri-
cal Methods in Natural Language Processing and Very Large
Corpora, College Park, MD.
Schafer, Charles and David Yarowsky. 2003. Statistical ma-
chine translation using coercive two-level syntactic transduc-
tion. In Proc. of the 2003 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP), Philadel-
phia, PA.
Vogel, Stephan, Hermann Ney, and Christoph Tillmann. 1996.
HMM-based word alignment in statistical translation. In
COLING ?96: The 16th Int. Conf. on Computational Lin-
guistics, Copenhagen, Denmark.
Wu, Dekai and H. Wong. 1998. Machine translation with a
stochastic grammatical channel. In COLING-ACL ?98: 36th
Annual Meeting of the Association for Computational Lin-
guistics and 17th Int. Conf. on Computational Linguistics,
Montreal, Canada.
Yamada, Kenji and Kevin Knight. 2001. A syntax-based sta-
tistical translation model. In Proc. of the 39th Annual Meet-
ing of the Association for Computational Linguistics (ACL),
Toulouse, France.
Yamada, Kenji and Kevin Knight. 2002. A decoder for syntax-
based MT. In Proc. of the 40th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), Philadelphia,
PA.
 
	 ffProceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 769?776,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Semi-Supervised Training for Statistical Word Alignment
Alexander Fraser
ISI / University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
fraser@isi.edu
Daniel Marcu
ISI / University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
marcu@isi.edu
Abstract
We introduce a semi-supervised approach
to training for statistical machine transla-
tion that alternates the traditional Expecta-
tion Maximization step that is applied on a
large training corpus with a discriminative
step aimed at increasing word-alignment
quality on a small, manually word-aligned
sub-corpus. We show that our algorithm
leads not only to improved alignments
but also to machine translation outputs of
higher quality.
1 Introduction
The most widely applied training procedure for
statistical machine translation ? IBM model 4
(Brown et al, 1993) unsupervised training fol-
lowed by post-processing with symmetrization
heuristics (Och and Ney, 2003) ? yields low
quality word alignments. When compared with
gold standard parallel data which was manually
aligned using a high-recall/precision methodology
(Melamed, 1998), the word-level alignments pro-
duced automatically have an F-measure accuracy
of 64.6 and 76.4% (see Section 2 for details).
In this paper, we improve word alignment and,
subsequently, MT accuracy by developing a range
of increasingly sophisticated methods:
1. We first recast the problem of estimating the
IBM models (Brown et al, 1993) in a dis-
criminative framework, which leads to an ini-
tial increase in word-alignment accuracy.
2. We extend the IBM models with new
(sub)models, which leads to additional in-
creases in word-alignment accuracy. In the
process, we also show that these improve-
ments are explained not only by the power
of the new models, but also by a novel search
procedure for the alignment of highest prob-
ability.
3. Finally, we propose a training procedure that
interleaves discriminative training with max-
imum likelihood training.
These steps lead to word alignments of higher
accuracy which, in our case, correlate with higher
MT accuracy.
The rest of the paper is organized as follows.
In Section 2, we review the data sets we use to
validate experimentally our algorithms and the as-
sociated baselines. In Section 3, we present itera-
tively our contributions that eventually lead to ab-
solute increases in alignment quality of 4.8% for
French/English and 4.8% for Arabic/English, as
measured using F-measure for large word align-
ment tasks. These contributions pertain to the
casting of the training procedure in the discrim-
inative framework (Section 3.1); the IBM model
extensions and modified search procedure for the
Viterbi alignments (Section 3.2); and the in-
terleaved, minimum error/maximum likelihood,
training algorithm (Section 4). In Section 5, we as-
sess the impact that our improved alignments have
on MT quality. We conclude with a comparison of
our work with previous research on discriminative
training for word alignment and a short discussion
of semi-supervised learning.
2 Data Sets and Baseline
We conduct experiments on alignment and
translation tasks using Arabic/English and
French/English data sets (see Table 1 for details).
Both sets have training data and two gold stan-
dard word alignments for small samples of the
training data, which we use as the alignment
769
ARABIC/ENGLISH FRENCH/ENGLISH
A E F E
TRAINING
SENTS 3,713,753 2,842,184
WORDS 102,473,086 119,994,972 75,794,254 67,366,819
VOCAB 489,534 231,255 149,568 114,907
SINGLETONS 199,749 104,155 60,651 47,765
ALIGN DISCR.
SENTS 100 110
WORDS 1,712 2,010 1,888 1,726
LINKS 2,129 2,292
ALIGN TEST
SENTS 55 110
WORDS 1,004 1,210 1,899 1,716
LINKS 1,368 2,176
MAX BLEU SENTS 728 (4 REFERENCES) 833 (1 REFERENCE)WORDS 17664 22.0K TO 24.5K 20,562 17,454
TRANS. TEST SENTS 663 (4 REFERENCES) 2,380 (1 REFERENCE)WORDS 16,075 19.0K TO 21.6K 58,990 49,182
Table 1: Datasets
SYSTEM F-MEASURE F TO E F-MEASURE E TO F F-MEASURE BEST SYMM.
A/E MODEL 4: ITERATION 4 65.6 / 60.5 53.6 / 50.2 69.1 / 64.6 (UNION)
F/E MODEL 4: ITERATION 4 73.8 / 75.1 74.2 / 73.5 76.5 / 76.4 (REFINED)
Table 2: Baseline Results. F-measures are presented on both the alignment discriminative training set
and the alignment test set sub-corpora, separated by /.
discriminative training set and alignment test set.
Translation quality is evaluated by translating
a held-out translation test set. An additional
translation set called the Maximum BLEU set is
employed by the SMT system to train the weights
associated with the components of its log-linear
model (Och, 2003).
The training corpora are publicly avail-
able: both the Arabic/English data and the
French/English Hansards were released by
LDC. We created the manual word alignments
ourselves, following the Blinker guidelines
(Melamed, 1998).
To train our baseline systems we follow a stan-
dard procedure. The models were trained two
times, first using French or Arabic as the source
language and then using English as the source
language. For each training direction, we run
GIZA++ (Och and Ney, 2003), specifying 5 iter-
ations of Model 1, 4 iterations of the HMM model
(Vogel et al, 1996), and 4 iterations of Model 4.
We quantify the quality of the resulting hypothe-
sized alignments with F-measure using the manu-
ally aligned sets.
We present the results for three different con-
ditions in Table 2. For the ?F to E? direction the
models assign non-zero probability to alignments
consisting of links from one Foreign word to zero
or more English words, while for ?E to F? the
models assign non-zero probability to alignments
consisting of links from one English word to zero
or more Foreign words. It is standard practice to
improve the final alignments by combining the ?F
to E? and ?E to F? directions using symmetriza-
tion heuristics. We use the ?union?, ?refined? and
?intersection? heuristics defined in (Och and Ney,
2003) which are used in conjunction with IBM
Model 4 as the baseline in virtually all recent work
on word alignment. In Table 2, we report the best
symmetrized results.
The low F-measure scores of the baselines mo-
tivate our work.
3 Improving Word Alignments
3.1 Discriminative Reranking of the IBM
Models
We reinterpret the five groups of parameters of
Model 4 listed in the first five lines of Table 3 as
sub-models of a log-linear model (see Equation 1).
Each sub-model hm has an associated weight ?m.
Given a vector of these weights ?, the alignment
search problem, i.e. the search to return the best
alignment a? of the sentences e and f according to
the model, is specified by Equation 2.
p?(f, a|e) =
exp(?i ?ihi(a, e, f))
?
a?,f ? exp(
?
i ?ihi(a?, e, f ?))
(1)
a? = argmax
a
?
i
?ihi(f, a, e) (2)
770
m Model 4 Description m Description
1 t(f |e) translation probs, f and e are words 9 translation table using approx. stems
2 n(?|e) fertility probs, ? is number of words generated by e 10 backoff fertility (fertility estimated
over all e)
3 null parameters used in generating Foreign words which
are unaligned
11 backoff fertility for words with count
<= 5
4 d1(4j) movement probs of leftmost Foreign word translated
from a particular e
12 translation table from HMM iteration 4
5 d>1(4j) movement probs of other Foreign words translated
from a particular e
13 zero fertility English word penalty
6 translation table from refined combination of both
alignments
14 non-zero fertility English word penalty
7 translation table from union of both alignments 15 NULL Foreign word penalty
8 translation table from intersection of both alignments 16 non-NULL Foreign word penalty
Table 3: Sub-Models. Note that sub-models 1 to 5 are IBM Model 4, sub-models 6 to 16 are new.
Log-linear models are often trained to maxi-
mize entropy, but we will train our model di-
rectly on the final performance criterion. We use
1?F-measure as our error function, comparing hy-
pothesized word alignments for the discriminative
training set with the gold standard.
Och (2003) has described an efficient exact
one-dimensional error minimization technique for
a similar search problem in machine translation.
The technique involves calculating a piecewise
constant function fm(x) which evaluates the er-
ror of the hypotheses which would be picked by
equation 2 from a set of hypotheses if we hold all
weights constant, except for the weight ?m (which
is set to x).
The discriminative reranking algorithm is ini-
tialized with the parameters of the sub-models ?,
an initial choice of the ? vector, gold standard
word alignments (labels) for the alignment dis-
criminative training set, the constant N specifying
the N-best list size used1, and an empty master set
of hypothesized alignments. The algorithm is a
three step loop:
1. Enrich the master set of hypothesized align-
ments by producing an N-best list using ?.
If all of the hypotheses in the N-best list are
already in the master set, the algorithm has
converged, so terminate the loop.
2. Consider the current ? vector and 999 addi-
tional randomly generated vectors, setting ?
to the vector with lowest error on the master
set.
3. Repeatedly run Och?s one-dimensional error
minimization step until there is no further er-
ror reduction (this results in a new vector ?).
1N = 128 for our experiments
3.2 Improvements to the Model and Search
3.2.1 New Sources of Knowledge
We define new sub-models to model factors not
captured by Model 4. These are lines 6 to 16
of Table 3, where we use the ?E to F? align-
ment direction as an example. We use word-level
translation tables informed by both the ?E to F?
and the ?F to E? translation directions derived us-
ing the three symmetrization heuristics, the ?E to
F? translation table from the final iteration of the
HMM model and an ?E to F? translation table de-
rived using approximative stemming. The approx-
imative stemming sub-model (sub-model 9) uses
the first 4 letters of each vocabulary item as the
stem for English and French while for Arabic we
use the full word as the stem. We also use sub-
models for backed off fertility, and direct penal-
ization of unaligned English words (?zero fertil-
ity?) and aligned English words, and unaligned
Foreign words (?NULL-generated? words) and
aligned Foreign words. This is a small sampling
of the kinds of knowledge sources we can use in
this framework; many others have been proposed
in the literature.
Table 4 shows an evaluation of discriminative
reranking. We observe:
1. The first line is the starting point, which is
the Viterbi alignment of the 4th iteration of
HMM training.
2. The 1-to-many alignments generated by dis-
criminatively reranking Model 4 are better
than the 1-to-many alignments of four itera-
tions of Model 4.
3. The 1-to-many alignments of the discrimina-
tively reranked extended model are much bet-
ter than four iterations of Model 4.
771
SYSTEM F-MEASURE F TO E F-MEASURE E TO F F-MEASURE BEST SYMM.
A/E LAST ITERATION HMM 58.6 / 54.4 47.7 / 39.9 62.1 / 57.0 (UNION)
A/E MODEL 4 RERANKING 65.3 / 59.5 55.7 / 51.4 69.7 / 64.6 (UNION)
A/E EXTENDED MODEL RERANKING 68.4 / 62.2 61.6 / 57.7 72.0 / 66.4 (UNION)
A/E MODEL 4: ITERATION 4 65.6 / 60.5 53.6 / 50.2 69.1 / 64.6 (UNION)
F/E LAST ITERATION HMM 72.4 / 73.9 71.5 / 71.8 76.4 / 77.3 (REFINED)
F/E MODEL 4 RERANKING 77.9 / 77.9 78.4 / 77.7 79.2 / 79.4 (REFINED)
F/E EXTENDED MODEL RERANKING 78.7 / 80.2 79.3 / 79.6 79.6 / 80.4 (REFINED)
F/E MODEL 4: ITERATION 4 73.8 / 75.1 74.2 / 73.5 76.5 / 76.4 (REFINED)
Table 4: Discriminative Reranking with Improved Search. F-measures are presented on both the align-
ment discriminative training set and the alignment test set sub-corpora, separated by /.
4. The discriminatively reranked extended
model outperforms four iterations of Model
4 in both cases with the best heuristic
symmetrization, but some of the gain is
lost as we are optimizing the F-measure of
the 1-to-many alignments rather than the
F-measure of the many-to-many alignments
directly.
Overall, the results show our approach is better
than or competitive with running four iterations of
unsupervised Model 4 training.
3.2.2 New Alignment Search Algorithm
Brown et al (1993) introduced operations defin-
ing a hillclimbing search appropriate for Model 4.
Their search starts with a complete hypothesis and
exhaustively applies two operations to it, selecting
the best improved hypothesis it can find (or termi-
nating if no improved hypothesis is found). This
search makes many search errors2. We developed
a new alignment algorithm to reduce search errors:
? We perform an initial hillclimbing search (as
in the baseline algorithm) but construct a pri-
ority queue of possible other candidate align-
ments to consider.
? Alignments which are expanded are marked
so that they will not be returned to at a future
point in the search.
? The alignment search operates by consider-
ing complete hypotheses so it is an ?anytime?
algorithm (meaning that it always has a cur-
rent best guess). Timers can therefore be
used to terminate the processing of the pri-
ority queue of candidate alignments.
The first two improvements are related to the
well-known Tabu local search algorithm (Glover,
2A search error in a word aligner is a failure to find the
best alignment according to the model, i.e. in our case a fail-
ure to maximize Equation 2.
1986). The third improvement is important for
restricting total time used when producing align-
ments for large training corpora.
We performed two experiments. The first evalu-
ates the number of search errors. For each corpus
we sampled 1000 sentence pairs randomly, with
no sentence length restriction. Model 4 parameters
are estimated from the final HMM Viterbi align-
ment of these sentence pairs. We then search to
try to find the Model 4 Viterbi alignment with both
the new and old algorithms, allowing them both
to process for the same amount of time. The per-
centage of known search errors is the percentage
of sentences from our sample in which we were
able to find a more probable candidate by apply-
ing our new algorithm using 24 hours of compu-
tation for just the 1000 sample sentences. Table
5 presents the results, showing that our new algo-
rithm reduced search errors in all cases, but fur-
ther reduction could be obtained. The second ex-
periment shows the impact of the new search on
discriminative reranking of Model 4 (see Table 6).
Reduced search errors lead to a better fit of the dis-
criminative training corpus.
4 Semi-Supervised Training for Word
Alignments
Intuitively, in approximate EM training for Model
4 (Brown et al, 1993), the E-step corresponds to
calculating the probability of all alignments ac-
cording to the current model estimate, while the
M-step is the creation of a new model estimate
given a probability distribution over alignments
(calculated in the E-step).
In the E-step ideally all possible alignments
should be enumerated and labeled with p(a|e, f),
but this is intractable. For the M-step, we would
like to count over all possible alignments for each
sentence pair, weighted by their probability ac-
cording to the model estimated at the previous
772
SYSTEM F TO E ERRORS % E TO F ERRORS %
A/E OLD 19.4 22.3
A/E NEW 8.5 15.3
F/E OLD 32.5 25.9
F/E NEW 13.7 10.4
Table 5: Comparison of New Search Algorithm with Old Search Algorithm
SYSTEM F-MEASURE F TO E F-MEASURE E TO F F-MEASURE BEST SYMM.
A/E MODEL 4 RERANKING OLD 64.1 / 58.1 54.0 / 48.8 67.9 / 63.0 (UNION)
A/E MODEL 4 RERANKING NEW 65.3 / 59.5 55.7 / 51.4 69.7 / 64.6 (UNION)
F/E MODEL 4 RERANKING OLD 77.3 / 77.8 78.3 / 77.2 79.2 / 79.1 (REFINED)
F/E MODEL 4 RERANKING NEW 77.9 / 77.9 78.4 / 77.7 79.2 / 79.4 (REFINED)
Table 6: Impact of Improved Search on Discriminative Reranking of Model 4
step. Because this is not tractable, we make the
assumption that the single assumed Viterbi align-
ment can be used to update our estimate in the M-
step. This approximation is called Viterbi training.
Neal and Hinton (1998) analyze approximate EM
training and motivate this type of variant.
We extend approximate EM training to perform
a new type of training which we call Minimum Er-
ror / Maximum Likelihood Training. The intuition
behind this approach to semi-supervised training
is that we wish to obtain the advantages of both
discriminative training (error minimization) and
approximate EM (which allows us to estimate a
large numbers of parameters even though we have
very few gold standard word alignments). We in-
troduce the EMD algorithm, in which discrimina-
tive training is used to control the contributions
of sub-models (thereby minimizing error), while a
procedure similar to one step of approximate EM
is used to estimate the large number of sub-model
parameters.
A brief sketch of the EMD algorithm applied
to our extended model is presented in Figure 1.
Parameters have a superscript t representing their
value at iteration t. We initialize the algorithm
with the gold standard word alignments (labels) of
the word alignment discriminative training set, an
initial ?, N, and the starting alignments (the iter-
ation 4 HMM Viterbi alignment). In line 2, we
make iteration 0 estimates of the 5 sub-models of
Model 4 and the 6 heuristic sub-models which are
iteration dependent. In line 3, we run discrimi-
native training using the algorithm from Section
3.1. In line 4, we measure the error of the result-
ing ? vector. In the main loop in line 7 we align
the full training set (similar to the E-step of EM),
while in line 8 we estimate the iteration-dependent
sub-models (similar to the M-step of EM). Then
1: Algorithm EMD(labels, ??, N, starting alignments)
2: estimate ?0m for m = 1 to 11
3: ?0 = Discrim(?0, ??, labels, N)
4: e0 = E(?0, labels)
5: t = 1
6: loop
7: align full training set using ?t?1 and ?t?1m
8: estimate ?tm for m = 1 to 11
9: ?t = Discrim(?t, ???, labels, N)
10: et = E(?t, labels)
11: if et >= et?1 then
12: terminate loop
13: end if
14: t = t + 1
15: end loop
16: return hypothesized alignments of full training set
Figure 1: Sketch of the EMD algorithm
we perform discriminative reranking in line 9 and
check for convergence in lines 10 and 11 (conver-
gence means that error was not decreased from the
previous iteration). The output of the algorithm is
new hypothesized alignments of the training cor-
pus.
Table 7 evaluates the EMD semi-supervised
training algorithm. We observe:
1. In both cases there is improved F-measure
on the second iteration of semi-supervised
training, indicating that the EMD algorithm
performs better than one step discriminative
reranking.
2. The French/English data set has converged3
after the second iteration.
3. The Arabic/English data set converged after
improvement for the first, second and third
iterations.
We also performed an additional experiment for
French/English aimed at understanding the poten-
tial contribution of the word aligned data without
3Convergence is achieved because error on the word
alignment discriminative training set does not improve.
773
SYSTEM F-MEASURE F TO E F-MEASURE E TO F BEST SYMM.
A/E STARTING POINT 58.6 / 54.4 47.7 / 39.9 62.1 / 57.0 (UNION)
A/E EMD: ITERATION 1 68.4 / 62.2 61.6 / 57.7 72.0 / 66.4 (UNION)
A/E EMD: ITERATION 2 69.8 / 63.1 64.1 / 59.5 74.1 / 68.1 (UNION)
A/E EMD: ITERATION 3 70.6 / 65.4 64.3 / 59.2 74.7 / 69.4 (UNION)
F/E STARTING POINT 72.4 / 73.9 71.5 / 71.8 76.4 / 77.3 (REFINED)
F/E EMD: ITERATION 1 78.7 / 80.2 79.3 / 79.6 79.6 / 80.4 (REFINED)
F/E EMD: ITERATION 2 79.4 / 80.5 79.8 / 80.5 79.9 / 81.2 (REFINED)
Table 7: Semi-Supervised Training Task F-measure
the new algorithm4. Like Ittycheriah and Roukos
(2005), we converted the alignment discrimina-
tive training corpus links into a special corpus
consisting of parallel sentences where each sen-
tence consists only of a single word involved in
the link. We found that the information in the
links was ?washed out? by the rest of the data and
resulted in no change in the alignment test set?s
F-Measure. Callison-Burch et al (2004) showed
in their work on combining alignments of lower
and higher quality that the alignments of higher
quality should be given a much higher weight than
the lower quality alignments. Using this insight,
we found that adding 10,000 copies of the special
corpus to our training data resulted in the highest
alignment test set gain, which was a small gain
of 0.6 F-Measure. This result suggests that while
the link information is useful for improving F-
Measure, our improved methods for training are
producing much larger improvements.
5 Improvement of MT Quality
The symmetrized alignments from the last iter-
ation of EMD were used to build phrasal SMT
systems, as were the symmetrized Model 4 align-
ments (the baseline). Aside from the final align-
ment, all other resources were held constant be-
tween the baseline and contrastive SMT systems,
including those based on lower level alignments
models such as IBM Model 1. For all of our ex-
periments, we use two language models, one built
using the English portion of the training data and
the other built using additional English news data.
We run Maximum BLEU (Och, 2003) for 25 iter-
ations individually for each system.
Table 8 shows our results. We report BLEU (Pa-
pineni et al, 2001) multiplied by 100. We also
show the F-measure after heuristic symmetrization
of the alignment test sets. The table shows that
4We would like to thank an anonymous reviewer for sug-
gesting that this experiment would be useful even when using
a small discriminative training corpus.
our algorithm produces heuristically symmetrized
final alignments of improved F-measure. Us-
ing these alignments in our phrasal SMT system,
we produced a statistically significant BLEU im-
provement (at a 95% confidence interval a gain of
0.78 is necessary) on the French/English task and
a statistically significant BLEU improvement on
the Arabic/English task (at a 95% confidence in-
terval a gain of 1.2 is necessary).
5.1 Error Criterion
The error criterion we used for all experiments
is 1 ? F-measure. The formula for F-measure is
shown in Equation 3. (Fraser and Marcu, 2006) es-
tablished that tuning the trade-off between Preci-
sion and Recall in the F-Measure formula will lead
to the best BLEU results. We tuned ? by build-
ing a collection of alignments using our baseline
system, measuring Precision and Recall against
the alignment discriminative training set, build-
ing SMT systems and measuring resulting BLEU
scores, and then searching for an appropriate ?
setting. We searched ? = 0.1, 0.2, ..., 0.9 and set
? so that the resulting F-measure tracks BLEU to
the best extent possible. The best settings were
? = 0.2 for Arabic/English and ? = 0.7 for
French/English, and these settings of ? were used
for every result reported in this paper. See (Fraser
and Marcu, 2006) for further details.
F (A, S, ?) = 1
?Precision(A,S) +
(1??)
Recall(A,S)
(3)
6 Previous Research
Previous work on discriminative training for word-
alignment differed most strongly from our ap-
proach in that it generally views word-alignment
as a supervised task. Examples of this perspective
include (Liu et al, 2005; Ittycheriah and Roukos,
2005; Moore, 2005; Taskar et al, 2005). All
of these also used knowledge from one of the
IBM Models in order to obtain competitive results
774
SYSTEM BLEU F-MEASURE
A/E UNSUP. MODEL 4 UNION 49.16 64.6
A/E EMD 3 UNION 50.84 69.4
F/E UNSUP. MODEL 4 REFINED 30.63 76.4
F/E EMD 2 REFINED 31.56 81.2
Table 8: Evaluation of Translation Quality
with the baseline (with the exception of (Moore,
2005)). We interleave discriminative training with
EM and are therefore performing semi-supervised
training. We show that semi-supervised training
leads to better word alignments than running unsu-
pervised training followed by discriminative train-
ing.
Another important difference with previous
work is that we are concerned with generating
many-to-many word alignments. Cherry and Lin
(2003) and Taskar et al (2005) compared their re-
sults with Model 4 using ?intersection? by look-
ing at AER (with the ?Sure? versus ?Possible? link
distinction), and restricted themselves to consider-
ing 1-to-1 alignments. However, ?union? and ?re-
fined? alignments, which are many-to-many, are
what are used to build competitive phrasal SMT
systems, because ?intersection? performs poorly,
despite having been shown to have the best AER
scores for the French/English corpus we are using
(Och and Ney, 2003). (Fraser and Marcu, 2006)
recently found serious problems with AER both
empirically and analytically, which explains why
optimizing AER frequently results in poor ma-
chine translation performance.
Finally, we show better MT results by using F-
measure with a tuned ? value. The only previous
discriminative approach which has been shown to
produce translations of similar or better quality to
those produced by the symmetrized baseline was
(Ittycheriah and Roukos, 2005). They had access
to 5000 gold standard word alignments, consider-
ably more than the 100 or 110 gold standard word
alignments used here. They also invested signif-
icant effort in sub-model engineering (producing
both sub-models specific to Arabic/English align-
ment and sub-models which would be useful for
other language pairs), while we use sub-models
which are simple extensions of Model 4 and lan-
guage independent.
The problem of semi-supervised learning is of-
ten defined as ?using unlabeled data to help su-
pervised learning? (Seeger, 2000). Most work on
semi-supervised learning uses underlying distribu-
tions with a relatively small number of parame-
ters. An initial model is estimated in a supervised
fashion using the labeled data, and this supervised
model is used to attach labels (or a probability dis-
tribution over labels) to the unlabeled data, then a
new supervised model is estimated, and this is it-
erated. If these techniques are applied when there
are a small number of labels in relation to the num-
ber of parameters used, they will suffer from the
?overconfident pseudo-labeling problem? (Seeger,
2000), where the initial labels of poor quality as-
signed to the unlabeled data will dominate the
model estimated in the M-step. However, there
are tasks with large numbers of parameters where
there are sufficient labels. Nigam et al (2000) ad-
dressed a text classification task. They estimate
a Naive Bayes classifier over the labeled data and
use it to provide initial MAP estimates for unla-
beled documents, followed by EM to further re-
fine the model. Callison-Burch et al (2004) exam-
ined the issue of semi-supervised training for word
alignment, but under a scenario where they simu-
lated sufficient gold standard word alignments to
follow an approach similar to Nigam et al (2000).
We do not have enough labels for this approach.
We are aware of two approaches to semi-
supervised learning which are more similar in
spirit to ours. Ivanov et al (2001) used discrimi-
native training in a reinforcement learning context
in a similar way to our adding of a discriminative
training step to an unsupervised context. A large
body of work uses semi-supervised learning for
clustering by imposing constraints on clusters. For
instance, in (Basu et al, 2004), the clustering sys-
tem was supplied with pairs of instances labeled
as belonging to the same or different clusters.
7 Conclusion
We presented a semi-supervised algorithm based
on IBM Model 4, with modeling and search ex-
tensions, which produces alignments of improved
F-measure over unsupervised Model 4 training.
We used these alignments to produce transla-
tions of higher quality.
775
The semi-supervised learning literature gen-
erally addresses augmenting supervised learning
tasks with unlabeled data (Seeger, 2000). In con-
trast, we augmented an unsupervised learning task
with labeled data. We hope that Minimum Error /
Maximum Likelihood training using the EMD al-
gorithm can be used for a wide diversity of tasks
where there is not enough labeled data to allow
supervised estimation of an initial model of rea-
sonable quality.
8 Acknowledgments
This work was partially supported under the
GALE program of the Defense Advanced Re-
search Projects Agency, Contract No. HR0011-
06-C-0022. We would like to thank the USC Cen-
ter for High Performance Computing and Commu-
nications.
References
Sugato Basu, Mikhail Bilenko, and Raymond J.
Mooney. 2004. A probabilistic framework for semi-
supervised clustering. In KDD ?04: Proc. of the
ACM SIGKDD international conference on knowl-
edge discovery and data mining, pages 59?68, New
York. ACM Press.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and R. L. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter
estimation. Computational Linguistics, 19(2):263?
311.
Chris Callison-Burch, David Talbot, and Miles Os-
borne. 2004. Statistical machine translation with
word- and sentence-aligned parallel corpora. In
Proc. of the 42nd Annual Meeting of the Association
for Computational Linguistics, Barcelona, Spain,
July.
Colin Cherry and Dekang Lin. 2003. A probability
model to improve word alignment. In Proc. of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, Sapporo, Japan, July.
Alexander Fraser and Daniel Marcu. 2006. Measur-
ing word alignment quality for statistical machine
translation. In Technical Report ISI-TR-616. Avail-
able at http://www.isi.edu/ fraser/research.html,
ISI/University of Southern California, May.
Fred Glover. 1986. Future paths for integer program-
ming and links to artificial intelligence. Computers
and Operations Research, 13(5):533?549.
Abraham Ittycheriah and Salim Roukos. 2005. A
maximum entropy word aligner for Arabic-English
machine translation. In Proc. of Human Language
Technology Conf. and Conf. on Empirical Methods
in Natural Language Processing, Vancouver, BC.
Yuri A. Ivanov, Bruce Blumberg, and Alex Pentland.
2001. Expectation maximization for weakly labeled
data. In ICML ?01: Proc. of the Eighteenth Interna-
tional Conf. on Machine Learning, pages 218?225.
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-
linear models for word alignment. In Proc. of the
43rd Annual Meeting of the Association for Compu-
tational Linguistics, pages 459?466, Ann Arbor, MI.
I. Dan Melamed. 1998. Manual annotation of trans-
lational equivalence: The blinker project. Techni-
cal Report 98-07, Institute for Research in Cognitive
Science, Philadelphia, PA.
Robert C. Moore. 2005. A discriminative framework
for bilingual word alignment. In Proc. of Human
Language Technology Conf. and Conf. on Empirical
Methods in Natural Language Processing, Vancou-
ver, BC, October.
Radford M. Neal and Geoffrey E. Hinton. 1998. A
view of the EM algorithm that justifies incremental,
sparse, and other variants. In M. I. Jordan, editor,
Learning in Graphical Models. Kluwer.
Kamal Nigam, Andrew K. McCallum, Sebastian
Thrun, and Tom M. Mitchell. 2000. Text classifi-
cation from labeled and unlabeled documents using
EM. Machine Learning, 39(2/3):103?134.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of the 41st
Annual Meeting of the Association for Computa-
tional Linguistics, pages 160?167, Sapporo, Japan.
Kishore A. Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2001. BLEU: a method for auto-
matic evaluation of machine translation. Technical
Report RC22176 (W0109-022), IBM Research Di-
vision, Thomas J. Watson Research Center, York-
town Heights, NY, September.
Matthias Seeger. 2000. Learning with labeled and un-
labeled data. In Technical report, 2000. Available at
http://www.dai.ed.ac.uk/ seeger/papers.html.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein.
2005. A discriminative matching approach to word
alignment. In Proc. of Human Language Technol-
ogy Conf. and Conf. on Empirical Methods in Natu-
ral Language Processing, Vancouver, BC, October.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical
translation. In COLING ?96: The 16th Int. Conf. on
Computational Linguistics, pages 836?841, Copen-
hagen, Denmark, August.
776
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 91?94,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
ISI?s Participation in the Romanian-English Alignment Task
Alexander Fraser
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292-6601
fraser@isi.edu
Daniel Marcu
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292-6601
marcu@isi.edu
Abstract
We discuss results on the shared task of
Romanian-English word alignment. The
baseline technique is that of symmetrizing
two word alignments automatically gener-
ated using IBM Model 4. A simple vo-
cabulary reduction technique results in an
improvement in performance. We also
report on a new alignment model and a
new training algorithm based on alternat-
ing maximization of likelihood with mini-
mization of error rate.
1 Introduction
ISI participated in the WPT05 Romanian-English
word alignment task. The system used for baseline
experiments is two runs of IBM Model 4 (Brown et
al., 1993) in the GIZA++ (Och and Ney, 2003) im-
plementation, which includes smoothing extensions
to Model 4. For symmetrization, we found that Och
and Ney?s ?refined? technique described in (Och and
Ney, 2003) produced the best AER for this data set
under all experimental conditions.
We experimented with a statistical model for in-
ducing a stemmer cross-lingually, but found that the
best performance was obtained by simply lower-
casing both the English and Romanian text and re-
moving all but the first four characters of each word.
We also tried a new model and a new training
criterion based on alternating the maximization of
likelihood and minimization of the alignment error
rate. For these experiments, we have implemented
an alignment package for IBM Model 4 using a hill-
climbing search and Viterbi training as described in
(Brown et al, 1993), and extended this to use new
submodels. The starting point is the final alignment
generated using GIZA++?s implementation of IBM
Model 1 and the Aachen HMM model (Vogel et al,
1996).
Paper organization: Section 2 is on the baseline,
Section 3 discusses vocabulary reduction, Section 4
introduces our new model and training method, Sec-
tion 5 describes experiments, Section 6 concludes.
We use the following notation: e refers to an En-
glish sentence composed of English words labeled
ei. f refers to a Romanian sentence composed of
Romanian words labeled fj . a is an alignment of e
to f . We use the term ?Viterbi alignment? to denote
the most probable alignment we can find, rather than
the true Viterbi alignment.
2 Baseline
To train our systems, Model 4 was trained two times,
first using Romanian as the source language and
then using English as the source language. For each
training, we ran 5 iterations of Model 1, 5 iterations
of the HMM model and 3 iterations of Model 4.
For the distortion calculations of Model 4, we re-
moved the dependencies on Romanian and English
word classes. We applied the ?union?, ?intersection?
and ?refined? symmetrization metrics (Och and Ney,
2003) to the final alignments output from training, as
well as evaluating the two final alignments directly.
We tried to have a strong baseline. GIZA++ has
many free parameters which can not be estimated us-
ing Maximum Likelihood training. We did not use
91
the defaults, but instead used settings which produce
good AER results on French/English bitext. We
also optimized p0 on the 2003 test set (using AER),
rather than using likelihood training. Turning off the
extensions to GIZA++ and training p0 as in (Brown
et al, 1993) produces a substantial increase in AER.
3 Vocabulary Size Reduction
Romanian is a Romance language which has a sys-
tem of suffixes for inflection which is richer than
English. Given the small amount of training data,
we decided that vocabulary size reduction was de-
sirable. As a baseline for vocabulary reduction, we
tried reducing words to prefixes of varying sizes
for both English and Romanian after lowercasing
the corpora. We also tried Porter stemming (Porter,
1997) for English.
(Rogati et al, 2003) extended Model 1 with an ad-
ditional hidden variable to represent the split points
in Arabic between the prefix, the stem and the suf-
fix to generate a stemming for use in Cross-Lingual
Information Retrieval. As in (Rogati et al, 2003),
we can find the most probable stemming given the
model, apply this stemming, and retrain our word
alignment system. However, we can also use the
modified model directly to find the best word align-
ment without converting the text to its stemmed
form.
We introduce a variable rj for the Romanian stem
and a variable sj for the Romanian suffix (which
when concatenated together give us the Romanian
word fj) into the formula for the probability of gen-
erating a Romanian word fj using an alignment aj
given only an English sentence e. We use the index
z to denote a particular stemming possibility. For a
given Romanian word the stemming possibilities are
simply every possible split point where the stem is at
least one character (this includes the null suffix).
p(fj , aj |e) =
?
z
p(rj,z, sj,z, aj |e) (1)
If the assumption is made that the stem and the
suffix are generated independently from e, we can
assume conditional independence.
p(fj , aj |e) =
?
z
p(rj,z, aj |e)p(sj,z, aj |e) (2)
We performed two sets of experiments, one set
where the English was stemmed using the Porter
stemmer and one set where each English word was
stemmed down to its first four characters. We
tried the best performing scoring heuristic for Ara-
bic from (Rogati et al, 2003) where p(sj,z, aj |e) is
modeled using the heuristic p(sj,z|lj) where sj,z is
the Romanian suffix, and lj is the last letter of the
Romanian word fj ; these adjustments are updated
during EM training. We also tried several other ap-
proximations of p(sj,z, aj |e) with and without up-
dates in EM training. We were unable to produce
better results and elected to use the baseline vocab-
ulary reduction technique for the shared task.
4 New Model and Training Algorithm
Our motivation for a new model and a new training
approach which combines likelihood maximization
with error rate minimization is threefold:
? Maximum Likelihood training of Model 4 is
not sufficient to find good alignments
? We would like to model factors not captured by
IBM Model 4
? Using labeled data could help us produce better
alignments, but we have very few labels
We create a new model and train it using an al-
gorithm which has a step which increases likelihood
(like one iteration in the EM algorithm), alternating
with a step which decreases error. We accomplish
this by:
? grouping the parameters of Model 4 into 5 sub-
models
? implementing 6 new submodels
? combining these into a single log-linear model
with 11 weights, ?1 to ?11, which we group
into the vector ?
? defining a search algorithm for finding the
alignment of highest probability given the sub-
models and ?
? devising a method for finding a ? which min-
imizes alignment error given fixed submodels
and a set of gold standard alignments
? inventing a training method for alternating
steps which estimate the submodels by increas-
ing likelihood with steps which set ? to de-
crease alignment error
The submodels in our new alignment model are
listed in table 1, where for ease of exposition we
92
Table 1: Submodels used for alignment
1 t(fj |ei) TRANSLATION PROBABILITIES
2 n(?i|ei) FERTILITY PROBABILITIES, ?i IS THE NUMBER OF WORDS GENERATED BY THE ENGLISH WORD ei
3 null PARAMETERS USED IN GENERATING ROMANIAN WORDS FROM ENGLISH NULL WORD (INCLUDING p0, p1)
4 d1(4j) MOVEMENT (DISTORTION) PROBABILITIES OF FIRST ROMANIAN WORD GENERATED FROM ENGLISH WORD
5 d>1(4j) MOVEMENT (DISTORTION) PROBABILITIES OF OTHER ROMANIAN WORDS GENERATED FROM ENGLISH WORD
6 TTABLE ESTIMATED FROM INTERSECTION OF TWO STARTING ALIGNMENTS FOR THIS ITERATION
7 TRANSLATION TABLE FROM ENGLISH TO ROMANIAN MODEL 1 ITERATION 5
8 TRANSLATION TABLE FROM ROMANIAN TO ENGLISH MODEL 1 ITERATION 5
9 BACKOFF FERTILITY (FERTILITY ESTIMATED OVER ALL ENGLISH WORDS)
10 ZERO FERTILITY ENGLISH WORD PENALTY
11 NON-ZERO FERTILITY ENGLISH WORD PENALTY
consider English to be the source language and Ro-
manian the target language.
The log-linear alignment model is specified by
equation 3. The model assigns non-zero proba-
bilities only to 1-to-many alignments, like Model
4. (Cettolo and Federico, 2004) used a log-linear
model trained using error minimization for the trans-
lation task, 3 of the submodels were taken from
Model 4 in a similar way to our first 5 submodels.
p?(a, f |e) =
exp(?m ?mhm(f, a, e))
?
f,e,a exp(
?
m ?mhm(f, a, e))
(3)
Given ?, the alignment search problem is to find
the alignment a of highest probability according to
equation 3. We solve this using the local search de-
fined in (Brown et al, 1993).
We set ? as follows. Given a sequence A of align-
ments we can calculate an error function, E(A). For
these experiments average sentence AER was used.
We wish to minimize this error function, so we se-
lect ? accordingly:
argmin
?
?
a?
E(a?)?(a?, (argmax
a
p?(a, f |e))) (4)
Maximizing performance for all of the weights
at once is not computationally tractable, but (Och,
2003) has described an efficient one-dimensional
search for a similar problem. We search over each
?m (holding the others constant) using this tech-
nique to find the best ?m to update and the best value
to update it to. We repeat the process until no further
gain can be found.
Our new training method is:
REPEAT
? Start with submodels and lambda from previ-
ous iteration
? Find Viterbi alignments on entire training cor-
pus using new model (similar to E-step of
Model 4 training)
? Reestimate submodel parameters from Viterbi
alignments (similar to M-step of Model 4
Viterbi training)
? Find a setting for ? that reduces AER on dis-
criminative training set (new D-step)
We use the first 148 sentences of the 2003 test set
for the discriminative training set. 10 settings for ?
are found, the hypothesis list is augmented using the
results of 10 searches using these settings, and then
another 10 settings for ? are found. We then select
the best ?. The discriminative training regimen is
otherwise similar to (Och, 2003).
5 Experiments
Table 2 provides a comparison of our baseline sys-
tems using the ?refined? symmetrization metric with
the best limited resources track system from WPT03
(Dejean et al, 2003) on the 2003 test set. The best
results are obtained by stemming both English and
Romanian words to the first four letters, as described
in section 2.
Table 3 provides details on our shared task sub-
mission. RUN1 is the word-based baseline system.
RUN2 is the stem-based baseline system. RUN4
uses only the first 6 submodels, while RUN5 uses
all 11 submodels. RUN3 had errors in processing,
so we omit it.
Results:
? Our new 1-to-many alignment model and train-
ing method are successful, producing decreases
of 0.03 AER when the source is Romanian, and
0.01 AER when the source is English.
93
Table 2: Summary of results for 2003 test set
SYSTEM STEM SIZES AER
XEROX ?NOLEM-ER-56K? 0.289
BASELINE NO PROCESSING 0.284
BASELINE ENG PORTER / ROM 4 0.251
BASELINE ENG 4 / ROM 4 0.248
Table 3: Full results on shared task submissions (blind test 2005)
RUN NAMES STEM SIZES SOURCE ROM SOURCE ENG UNION INTERSECTION REFINED
ISI.RUN1 NO PROCESSING 0.3834 0.3589 0.3590 0.3891 0.3165
ISI.RUN2 ENG 4 / ROM 4 0.3056 0.2880 0.2912 0.3041 0.2675
ISI.RUN4 ENG 4 / ROM 4 0.2798 0.2833 0.2773 0.2862 0.2663
ISI.RUN5 ENG 4 / ROM 4 0.2761 0.2778 0.2736 0.2807 0.2655
? These decreases do not translate to a large im-
provement in the end-to-end task of producing
many-to-many alignments with a balanced pre-
cision and recall. We had a very small decrease
of 0.002 AER using the ?refined? heuristic.
? The many-to-many alignments produced using
?union? and the 1-to-1 alignments produced us-
ing ?intersection? were also improved.
? It may be a problem that we trained p0 using
likelihood (it is in submodel 3) rather than op-
timizing p0 discriminatively as we did for the
baseline.
6 Conclusion
? Considering multiple stemming possibilities
for each word seems important.
? Alternating between increasing likelihood and
decreasing error rate is a useful training ap-
proach which can be used for many problems.
? Our model and training method improve upon a
strong baseline for producing 1-to-many align-
ments.
? Our model and training method can be used
with the ?intersection? heuristic to produce
higher quality 1-to-1 alignments
? Models which can directly model many-to-
many alignments and do not require heuristic
symmetrization are needed to produce higher
quality many-to-many alignments. Our train-
ing method can be used to train them.
7 Acknowledgments
This work was supported by DARPA-ITO grant
NN66001-00-1-9814 and NSF grant IIS-0326276.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra,
and R. L. Mercer. 1993. The mathematics of statistical ma-
chine translation: Parameter estimation. Computational Lin-
guistics, 19(2):263?311.
Mauro Cettolo and Marcello Federico. 2004. Minimum er-
ror training of log-linear translation models. In Proc. of the
International Workshop on Spoken Language Translation,
pages 103?106, Kyoto, Japan.
Herve Dejean, Eric Gaussier, Cyril Goutte, and Kenji Yamada.
2003. Reducing parameter space for word alignment. In
HLT-NAACL 2003 Workshop on Building and Using Paral-
lel Texts: Data Driven Machine Translation and Beyond, Ed-
monton, Alberta, July.
Franz Josef Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proc. of the 41st Annual Meet-
ing of the Association for Computational Linguistics (ACL),
pages 160?167, Sapporo, Japan, July.
M. F. Porter. 1997. An algorithm for suffix stripping. In Read-
ings in information retrieval, pages 313?316, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Monica Rogati, Scott McCarley, and Yiming Yang. 2003. Un-
supervised learning of arabic stemming using a parallel cor-
pus. In Proc. of the 41st Annual Meeting of the Association
for Computational Linguistics (ACL), Sapporo, Japan, July.
Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996.
HMM-based word alignment in statistical translation. In
COLING ?96: The 16th Int. Conf. on Computational Lin-
guistics, pages 836?841, Copenhagen, Denmark, August.
94
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 282?290,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Rich bitext projection features for parse reranking
Alexander Fraser Renjing Wang
Institute for Natural Language Processing
University of Stuttgart
{fraser,wangrg}@ims.uni-stuttgart.de
Hinrich Schu?tze
Abstract
Many different types of features have
been shown to improve accuracy in parse
reranking. A class of features that thus far
has not been considered is based on a pro-
jection of the syntactic structure of a trans-
lation of the text to be parsed. The intu-
ition for using this type of bitext projec-
tion feature is that ambiguous structures
in one language often correspond to un-
ambiguous structures in another. We show
that reranking based on bitext projection
features increases parsing accuracy signif-
icantly.
1 Introduction
Parallel text or bitext is an important knowledge
source for solving many problems such as ma-
chine translation, cross-language information re-
trieval, and the projection of linguistic resources
from one language to another. In this paper, we
show that bitext-based features are effective in ad-
dressing another NLP problem, increasing the ac-
curacy of statistical parsing. We pursue this ap-
proach for a number of reasons. First, one lim-
iting factor for syntactic approaches to statistical
machine translation is parse quality (Quirk and
Corston-Oliver, 2006). Improved parses of bi-
text should result in improved machine translation.
Second, as more and more texts are available in
several languages, it will be increasingly the case
that a text to be parsed is itself part of a bitext.
Third, we hope that the improved parses of bitext
will serve as higher quality training data for im-
proving monolingual parsing using a process sim-
ilar to self-training (McClosky et al, 2006).
It is well known that different languages encode
different types of grammatical information (agree-
ment, case, tense etc.) and that what can be left
unspecified in one language must be made explicit
NP
NP
NP
DT
a
NN
baby
CC
and
NP
DT
a
NN
woman
SBAR
who had gray hair
Figure 1: English parse with high attachment
in another. This information can be used for syn-
tactic disambiguation. However, it is surprisingly
hard to do this well. We use parses and alignments
that are automatically generated and hence imper-
fect. German parse quality is considered to be
worse than English parse quality, and the annota-
tion style is different, e.g., NP structure in German
is flatter.
We conduct our research in the framework of
N-best parse reranking, but apply it to bitext and
add only features based on syntactic projection
from German to English. We test the idea that,
generally, English parses with more isomorphism
with respect to the projected German parse are bet-
ter. The system takes as input (i) English sen-
tences with a list of automatically generated syn-
tactic parses, (ii) a translation of the English sen-
tences into German, (iii) an automatically gen-
erated parse of the German translation, and (iv)
an automatically generated word alignment. We
achieve a significant improvement of 0.66 F1 (ab-
solute) on test data.
The paper is organized as follows. Section 2
outlines our approach and section 3 introduces the
model. Section 4 describes training and section 5
presents the data and experimental results. In sec-
tion 6, we discuss previous work. Section 7 ana-
lyzes our results and section 8 concludes.
282
NP
NP
DT
a
NN
baby
CC
and
NP
NP
DT
a
NN
woman
SBAR
who had gray hair
Figure 2: English parse with low attachment
CNP
NP
ART
ein
NN
Baby
KON
und
NP
ART
eine
NN
Frau
,
,
S
die...
Figure 3: German parse with low attachment
2 Approach
Consider the English sentence ?He saw a baby and
a woman who had gray hair?. Suppose that the
baseline parser generates two parses, containing
the NPs shown in figures 1 and 2, respectively, and
that the semantically more plausible second parse
in figure 2 is correct. How can we determine that
the second parse should be favored? Since we are
parsing bitext, we can observe the German trans-
lation which is ?Er sah ein Baby und eine Frau,
die graue Haare hatte? (glossed: ?he saw a baby
and a woman, who gray hair had?). The singular
verb in the subordinate clause (?hatte?: ?had?) in-
dicates that the subordinate S must be attached low
to ?woman? (?Frau?) as shown in figure 3.
We follow Collins? (2000) approach to discrim-
inative reranking (see also (Riezler et al, 2002)).
Given a new sentence to parse, we first select the
best N parse trees according to a generative model.
Then we use new features to learn discriminatively
how to rerank the parses in this N-best list. We
use features derived using projections of the 1-best
German parse onto the hypothesized English parse
under consideration.
In more detail, we take the 100 best English
parses from the BitPar parser (Schmid, 2004) and
rerank them. We have a good chance of finding the
optimal parse among the 100-best1. An automati-
cally generated word alignment determines trans-
lational correspondence between German and En-
glish. We use features which measure syntactic di-
1Using an oracle to select the best parse results in an F1
of 95.90, an improvement of 8.01 absolute over the baseline.
vergence between the German and English trees to
try to rank the English trees which have less diver-
gence higher. Our test set is 3718 sentences from
the English Penn treebank (Marcus et al, 1993)
which were translated into German. We hold out
these sentences, and train BitPar on the remain-
ing Penn treebank training sentences. The average
F1 parsing accuracy of BitPar on this test set is
87.89%, which is our baseline2. We implement
features based on projecting the German parse to
each of the English 100-best parses in turn via the
word alignment. By performing cross-validation
and measuring test performance within each fold,
we compare our new system with the baseline on
the 3718 sentence set. The overall test accuracy
we reach is 88.55%, a statistically significant im-
provement over baseline of 0.66.
Given a word alignment of the bitext, the sys-
tem performs the following steps for each English
sentence to be parsed:
(i) run BitPar trained on English to generate 100-
best parses for the English sentence
(ii) run BitPar trained on German to generate the
1-best parse for the German sentence
(iii) calculate feature function values which mea-
sure different kinds of syntactic divergence
(iv) apply a model that combines the feature func-
tion values to score each of the 100-best parses
(v) pick the best parse according to the model
3 Model
We use a log-linear model to choose the best En-
glish parse. The feature functions are functions
on the hypothesized English parse e, the German
parse g, and the word alignment a, and they as-
sign a score (varying between 0 and infinity) that
measures syntactic divergence. The alignment of
a sentence pair is a function that, for each English
word, returns a set of German words that the En-
glish word is aligned with as shown here for the
sentence pair from section 2:
Er sah ein Baby und eine Frau , die graue Haare
hatte
He{1} saw{2} a{3} baby{4} and{5} a{6}
woman{7} who{9} had{12} gray{10} hair{11}
Feature function values are calculated either by
taking the negative log of a probability, or by using
a heuristic function which scales in a similar fash-
2The test set is very challenging, containing English sen-
tences of up to 99 tokens.
283
ion3. The form of the log-linear model is shown in
eq. 1. There are M feature functions h1, . . . , hM .
The vector ? is used to control the contribution of
each feature function.
p?(e|g, a) =
exp(??i ?ihi(e, g, a))
?
e? exp(?
?
i ?ihi(e?, g, a))
(1)
Given a vector of weights ?, the best English
parse e? can be found by solving eq. 2. The model
is trained by finding the weight vector ? which
maximizes accuracy (see section 4).
e? = argmax
e
p?(e|g, a)
= argmin
e
exp(
?
i
?ihi(e, g, a)) (2)
3.1 Feature Functions
The basic idea behind our feature functions is that
any constituent in a sentence should play approx-
imately the same syntactic role and have a similar
span as the corresponding constituent in a trans-
lation. If there is an obvious disagreement, it
is probably caused by wrong attachment or other
syntactic mistakes in parsing. Sometimes in trans-
lation the syntactic role of a given semantic consti-
tutent changes; we assume that our model penal-
izes all hypothesized parses equally in this case.
For the initial experiments, we used a set of 34
probabilistic and heuristic feature functions.
BitParLogProb (the only monolingual feature)
is the negative log probability assigned by BitPar
to the English parse. If we set ?1 = 1 and ?i = 0
for all i 6= 1 and evaluate eq. 2, we will select the
parse ranked best by BitPar.
In order to define our feature functions, we first
introduce auxiliary functions operating on indi-
vidual word positions or sets of word positions.
Alignment functions take an alignment a as an ar-
gument. In the descriptions of these functions we
omit a as it is held constant for a sentence pair (i.e.,
an English sentence and its German translation).
f(i) returns the set of word positions of German
words aligned with an English word at position i.
f ?(i) returns the leftmost word position of the
German words aligned with an English word at po-
sition i, or zero if the English word is unaligned.
f?1(i) returns the set of positions of English
3For example, a probability of 1 is a feature value of 0,
while a low probability is a feature value which is ? 0.
words aligned with a German word at position i.
f ??1(i) returns the leftmost word position of the
English words aligned with a German word at po-
sition i, or zero if the German word is unaligned.
We overload the above functions to allow the ar-
gument i to be a set, in which case union is used,
for example, f(i) = ?j?if(j). Positions in a
tree are denoted with integers. First, the POS tags
are numbered from 1 to the length of the sentence
(i.e., the same as the word positions). Constituents
higher in the tree are also indexed using consecu-
tive integers. We refer to the constituent that has
been assigned index i in the tree t as ?constituent i
in tree t? or simply as ?constituent i?. The follow-
ing functions have the English and German trees
as an implicit argument; it should be obvious from
the argument to the function whether the index
i refers to the German tree or the English tree.
When we say ?constituents?, we include nodes
on the POS level of the tree. Our syntactic trees
are annotated with a syntactic head for each con-
stituent. Finally, the tag at position 0 is NULL.
mid2sib(i) returns 0 if i is 0, returns 1 if i has
exactly two siblings, one on the left of i and one
on the right, and otherwise returns 0.
head(i) returns the index of the head of i. The
head of a POS tag is its own position.
tag(i) returns the tag of i.
left(i) returns the index of the leftmost sibling of
i.
right(i) returns the index of the rightmost sibling.
up(i) returns the index of i?s parent.
?(i) returns the set of word positions covered by
i. If i is a set, ? returns all word positions between
the leftmost position covered by any constituent in
the set and the rightmost position covered by any
constituent in the set (inclusive).
n(A) returns the size of the set A.
c(A) returns the number of characters (including
punctuation and excluding spaces) covered by the
constituents in set A.
JpiK is 1 if pi is true, and 0 otherwise.
l and m are the lengths in words of the English and
German sentences, respectively.
3.1.1 Count Feature Functions
Feature CrdBin counts binary events involving
the heads of coordinated phrases. If in the English
parse we have a coordination where the English
CC is aligned only with a German KON, and both
have two siblings, then the value contributed to
CrdBin is 1 (indicating a constraint violation) un-
284
less the head of the English left conjunct is aligned
with the head of the German left conjunct and like-
wise the right conjuncts are aligned. Eq. 3 calcu-
lates the value of CrdBin.
l
?
i=1
J(tag(i) = CCKJ(n(f(i)) = 1K mid2sib(i)
mid2sib(f ?(i)) Jtag(f ?(i)) = KON-CDK
J[head(left(f ?(i))) 6= f ?(head(left(i)))] OR
[head(right(f ?(i))) 6= f ?(head(right(i)))]K (3)
Feature Q simply captures a mismatch between
questions and statements. If an English sentence is
parsed as a question but the parallel German sen-
tence is not, or vice versa, the feature value is 1;
otherwise the value is 0.
3.1.2 Span Projection Feature Functions
Span projection features calculate the percentage
difference between a constituent?s span and the
span of its projection. Span size is measured in
characters or words. To project a constituent in
a parse, we use the word alignment to project all
word positions covered by the constituent and then
look for the smallest covering constituent in the
parse of the parallel sentence.
CrdPrj is a feature that measures the diver-
gence in the size of coordination constituents and
their projections. If we have a constituent (XP1
CC XP2) in English that is projected to a German
coordination, we expect the English and German
left conjuncts to span a similar percentage of their
respective sentences, as should the right conjuncts.
The feature computes a character-based percent-
age difference as shown in eq. 4.
l
?
i=1
Jtag(i) = CCKJn(f(i)) = 1K (4)
Jtag(f ?(i)) = KON-CDK
mid2sib(i)mid2sib(f ?(i))
(|c(?(left(i)))r ?
c(?(left(f ?(i))))
s |
+|c(?(right(i)))r ?
c(?(right(f ?(i))))
s |)
r and s are the lengths in characters of the En-
glish and German sentences, respectively. In the
English parse in figure 1, the left conjunct has 5
characters and the right conjunct has 6, while in
figure 2 the left conjunct has 5 characters and the
right conjunct has 20. In the German parse (fig-
ure 3) the left conjunct has 7 characters and the
right conjunct has 27. Finally, r = 33 and s = 42.
Thus, the value of CrdPrj is 0.48 for the first hy-
pothesized parse and 0.05 for the second, which
captures the higher divergence of the first English
parse from the German parse.
POSParentPrj is based on computing the span
difference between all the parent constituents of
POS tags in a German parse and their respective
coverage in the corresponding hypothesized parse.
The feature value is the sum of all the differences.
POSPar(i) is true if i immediately dominates a
POS tag. The projection direction is from German
to English, and the feature computes a percentage
difference which is character-based. The value of
the feature is calculated in eq. 5, where M is the
number of constituents (including POS tags) in the
German tree.
M
?
i=1
JPOSPar(i)K|c(?(i))s ?
c(?(f?1(?(i))))
r |
(5)
The right conjunct in figure 3 is a POSParent
that corresponds to the coordination NP in fig-
ure 1, contributing a score of 0.21, and to the right
conjunct in figure 2, contributing a score of 0.04.
For the two parses of the full sentences contain-
ing the NPs in figure 1 and figure 2, we sum over
7 POSParents and get a value of 0.27 for parse 1
and 0.11 for parse 2. The lower value for parse
2 correctly captures the fact that the first English
parse has higher divergence than the second due to
incorrect high attachment.
AbovePOSPrj is similar to POSParentPrj, but
it is word-based and the projection direction is
from English to German. Unlike POSParentPrj
the feature value is calculated over all constituents
above the POS level in the English tree.
Another span projection feature function is
DTNNPrj, which projects English constituents of
the form (NP(DT)(NN)). DTNN(i) is true if i
is an NP immediately dominating only DT and
NN. The feature computes a percentage difference
which is word-based, shown in eq. 6.
L
?
i=1
JDTNN(i)K|n(?(i))l ?
n(?(f(?(i))))
m | (6)
L is the number of constituents in the English
tree. This feature is designed to disprefer parses
285
where constituents starting with ?DT NN?, e.g.,
(NP (DT NN NN NN)), are incorrectly split into
two NPs, e.g., (NP (DT NN)) and (NP (NN NN)).
This feature fires in this case, and projects the (NP
(DT NN)) into German. If the German projection
is a surprisingly large number of words (as should
be the case if the German also consists of a deter-
miner followed by several nouns) then the penalty
paid by this feature is large. This feature is impor-
tant as (NP (DT NN)) is a very common construc-
tion.
3.1.3 Probabilistic Feature Functions
We use Europarl (Koehn, 2005), from which we
extract a parallel corpus of approximately 1.22
million sentence pairs, to estimate the probabilis-
tic feature functions described in this section.
For the PDepth feature, we estimate English
parse depth probability conditioned on German
parse depth from Europarl by calculating a sim-
ple probability distribution over the 1-best parse
pairs for each parallel sentence. A very deep Ger-
man parse is unlikely to correspond to a flat En-
glish parse and we can penalize such a parse using
PDepth. The index i refers to a sentence pair in
Europarl, as does j. Let li and mi be the depths
of the top BitPar ranked parses of the English and
German sentences, respectively. We calculate the
probability of observing an English tree of depth
l? given German tree of depth m? as the maxi-
mum likelihood estimate, shown in eq. 7, where
?(z, z?) = 1 if z = z? and 0 otherwise. To avoid
noisy feature values due to outliers and parse er-
rors, we bound the value of PDepth at 5 as shown
in eq. 84.
p(l?|m?) =
?
i ?(l?, li)?(m?,mi)
?
j ?(m?,mj)
(7)
min(5,? log10(p(l?|m?))) (8)
The full parse of the sentence containing the En-
glish high attachment has a parse depth of 8 while
the full parse of the sentence containing the En-
glish low attachment has a depth of 9. Their fea-
ture values given the German parse depth of 6 are
? log10(0.12) = 0.93 and ? log10(0.14) = 0.84.
The wrong parse is assigned a higher feature value
indicating its higher divergence.
The feature PTagEParentGPOSGParent mea-
sures tagging inconsistency based on estimating
4Throughout this paper, assume log(0) = ??.
the probability that for an English word at posi-
tion i, the parent of its POS tag has a particular
label. The feature value is calculated in eq. 10.
q(i, j) = p(tag(up(i))|tag(j), tag(up(j))) (9)
l
?
i=1
min(5,
?
j?f(i) ? log10(q(i, j))
n(f(i)) ) (10)
Consider (S(NP(NN fruit))(VP(V flies))) and
(NP(NN fruit)(NNS flies)) with the translation
(NP(NNS Fruchtfliegen)). Assume that ?fruit?
and ?flies? are aligned with the German com-
pound noun ?Fruchtfliegen?. In the incorrect En-
glish parse the parent of the POS of ?fruit? is
NP and the parent of the POS of ?flies? is VP,
while in the correct parse the parent of the POS of
?fruit? is NP and the parent of the POS of ?flies?
is NP. In the German parse the compound noun
is POS-tagged as an NNS and the parent is an
NP. The probabilities considered for the two En-
glish parses are p(NP|NNS,NP) for ?fruit? in both
parses, p(VP|NNS,NP) for ?flies? in the incorrect
parse, and p(NP|NNS,NP) for ?flies? in the cor-
rect parse. A German NNS in an NP has a higher
probability of being aligned with a word in an En-
glish NP than with a word in an English VP, so the
second parse will be preferred.
As with the PDepth feature, we use relative
frequency to estimate this feature. When an En-
glish word is aligned with two words, estimation is
more complex. We heuristically give each English
and German pair one count. The value calculated
by the feature function is the geometric mean5 of
the pairwise probabilities, see eq. 10.
3.1.4 Other Features
Our best system uses the nine features we have
described in detail so far. In addition, we imple-
mented the following 25 other features, which did
not improve performance (see section 7): (i) 7
?ptag? features similar to PTagEParentGPOSG-
Parent but predicting and conditioning on differ-
ent combinations of tags (POS tag, parent of POS,
grandparent of POS)
(ii) 10 ?prj? features similar to POSParentPrj
measuring different combinations of character and
word percentage differences at the POS parent and
5Each English word has the same weight regardless of
whether it was aligned with one or with more German words.
286
POS grandparent levels, projecting from both En-
glish and German
(iii) 3 variants of the DTNN feature function
(iv) A NPPP feature function, similar to the
DTNN feature function but trying to counteract a
bias towards (NP (NP) (PP)) units
(v) A feature function which penalizes aligning
clausal units to non-clausal units
(vi) The BitPar rank
4 Training
Log-linear models are often trained using the
Maximum Entropy criterion, but we train our
model directly to maximize F1. We score F1 by
comparing hypothesized parses for the discrimi-
native training set with the gold standard. To try
to find the optimal ? vector, we perform direct ac-
curacy maximization, meaning that we search for
the ? vector which directly optimizes F1 on the
training set.
Och (2003) has described an efficient exact one-
dimensional accuracy maximization technique for
a similar search problem in machine translation.
The technique involves calculating an explicit
representation of the piecewise constant function
gm(x) which evaluates the accuracy of the hy-
potheses which would be picked by eq. 2 from a
set of hypotheses if we hold all weights constant,
except for the weight ?m, which is set to x. This
is calculated in one pass over the data.
The algorithm for training is initialized with a
choice for ? and is described in figure 4. The func-
tion F1(?) returns F1 of the parses selected using
?. Due to space we do not describe step 8 in detail
(see (Och, 2003)). In step 9 the algorithm per-
forms approximate normalization, where feature
weights are forced towards zero. The implemen-
tation of step 9 is straight-forward given the M
explicit functions gm(x) created in step 8.
5 Data and Experiments
We used the subset of the Wall Street Journal
investigated in (Atterer and Schu?tze, 2007) for
our experiments, which consists of all sentences
that have at least one prepositional phrase attach-
ment ambiguity. This difficult subset of sentences
seems particularly interesting when investigating
the potential of information in bitext for improv-
ing parsing performance. The first 500 sentences
of this set were translated from English to German
by a graduate student and an additional 3218 sen-
1: Algorithm TRAIN(?)
2: repeat
3: add ? to the set s
4: let t be a set of 1000 randomly generated vectors
5: let ? = argmax??(s?t) F1(?)
6: let ?? = ?
7: repeat
8: repeatedly run one-dimensional error minimiza-
tion step (updating a single scalar of the vector ?)
until no further error reduction
9: adjust each scalar of ? in turn towards 0 such that
there is no increase in error (if possible)
10: until no scalar in ? changes in last two steps (8 and
9)
11: until ? = ??
12: return ?
Figure 4: Sketch of the training algorithm
tences by a translation bureau. We withheld these
3718 English sentences (and an additional 1000
reserved sentences) when we trained BitPar on the
Penn treebank.
Parses. We use the BitPar parser (Schmid,
2004) which is based on a bit-vector im-
plementation (cf. (Graham et al, 1980)) of
the Cocke-Younger-Kasami algorithm (Kasami,
1965; Younger, 1967). It computes a compact
parse forest for all possible analyses. As all pos-
sible analyses are computed, any number of best
parses can be extracted. In contrast, other treebank
parsers use sophisticated search strategies to find
the most probable analysis without examining the
set of all possible analyses (Charniak et al, 1998;
Klein and Manning, 2003). BitPar is particularly
useful for N-best parsing as the N-best parses can
be computed efficiently.
For the 3718 sentences in the translated set, we
created 100-best English parses and 1-best Ger-
man parses. The German parser was trained on
the TIGER treebank. For the Europarl corpus, we
created 1-best parses for both languages.
Word Alignment. We use a word alignment
of the translated sentences from the Penn tree-
bank, as well as a word alignment of the Europarl
corpus. We align these two data sets together
with data from the JRC Acquis (Steinberger et al,
2006) to try to obtain better quality alignments (it
is well known that alignment quality improves as
the amount of data increases (Fraser and Marcu,
2007)). We aligned approximately 3.08 million
sentence pairs. We tried to obtain better alignment
quality as alignment quality is a problem in many
cases where syntactic projection would otherwise
work well (Fossum and Knight, 2008).
287
System Train +base Test +base
1 Baseline 87.89 87.89
2 Contrastive 88.70 0.82 88.45 0.56
(5 trials/fold)
3 Contrastive 88.82 0.93 88.55 0.66
(greedy selection)
Table 1: Average F1 of 7-way cross-validation
To generate the alignments, we used Model 4
(Brown et al, 1993), as implemented in GIZA++
(Och and Ney, 2003). As is standard practice, we
trained Model 4 with English as the source lan-
guage, and then trained Model 4 with German as
the source language, resulting in two Viterbi align-
ments. These were combined using the Grow Diag
Final And symmetrization heuristic (Koehn et al,
2003).
Experiments. We perform 7-way cross-
validation on 3718 sentences. In each fold of the
cross-validation, the training set is 3186 sentences,
while the test set is 532 sentences. Our results are
shown in table 1. In row 1, we take the hypothesis
ranked best by BitPar. In row 2, we train using the
algorithm outlined in section 4. To cancel out any
effect caused by a particularly effective or ineffec-
tive starting ? value, we perform 5 trials each time.
Columns 3 and 5 report the improvement over the
baseline on train and test respectively. We reach
an improvement of 0.56 over the baseline using
the algorithm as described in section 4.
Our initial experiments used many highly cor-
related features. For our next experiment we use
greedy feature selection. We start with a ? vector
that is zero for all features, and then run the error
minimization without the random generation of
vectors (figure 4, line 4). This means that we add
one feature at a time. This greedy algorithm winds
up producing a vector with many zero weights. In
row 3 of table 1, we used the greedy feature selec-
tion algorithm and trained using F1, resulting in
a performance of 0.66 over the baseline which is
our best result. We performed a planned one-tailed
paired t-test on the F1 scores of the parses selected
by the baseline and this system for the 3718 sen-
tences (parses were taken from the test portion
of each fold). We found that there is a signifi-
cant difference with the baseline (t(3717) = 6.42,
p < .01). We believe that using the full set of 34
features (many of which are very similar to one
another) made the training problem harder with-
out improving the fit to the training data, and that
greedy feature selection helps with this (see also
section 7).
6 Previous Work
As we mentioned in section 2, work on parse
reranking is relevant, but a vital difference is that
we use features based only on syntactic projection
of the two languages in a bitext. For an overview
of different types of features that have been used in
parse reranking see Charniak and Johnson (2005).
Like Collins (2000) we use cross-validation to
train our model, but we have access to much less
data (3718 sentences total, which is less than 1/10
of the data Collins used). We use rich feature func-
tions which were designed by hand to specifically
address problems in English parses which can be
disambiguated using the German translation.
Syntactic projection has been used to bootstrap
treebanks in resource poor languages. Some ex-
amples of projection of syntactic parses from En-
glish to a resource poor language for which no
parser is available are the works of Yarowsky and
Ngai (2001), Hwa et al (2005) and Goyal and
Chatterjee (2006). Our work differs from theirs
in that we are performing a parse reranking task
in English using knowledge gained from German
parses, and parsing accuracy is generally thought
to be worse in German than in English.
Hopkins and Kuhn (2006) conducted research
with goals similar to ours. They showed how to
build a powerful generative model which flexibly
incorporates features from parallel text in four lan-
guages, but were not able to show an improvement
in parsing performance. After the submission of
our paper for review, two papers outlining relevant
work were published. Burkett and Klein (2008)
describe a system for simultaneously improving
Chinese and English parses of a Chinese/English
bitext. This work is complementary to ours. The
system is trained using gold standard trees in both
Chinese and English, in contrast with our system
which only has access to gold standard trees in En-
glish. Their system uses a tree alignment which
varies within training, but this does not appear to
make a large difference in performance. They use
coarsely defined features which are language in-
dependent. We use several features similar to their
two best performing sets of features, but in con-
trast with their work, we also define features which
are specifically aimed at English disambiguation
problems that we have observed can be resolved
288
using German parses. They use an in-domain
Chinese parser and out-of-domain English parser,
while for us the English parser is in-domain and
the German parser is out-of-domain, both of which
make improving the English parse more difficult.
Their Maximum Entropy training is more appro-
priate for their numerous coarse features, while
we use Minimum Error Rate Training, which is
much faster. Finally, we are projecting from a sin-
gle German parse which is a more difficult prob-
lem. Fossum and Knight (2008) outline a system
for using Chinese/English word alignments to de-
termine ambiguous English PP-attachments. They
first use an oracle to choose PP-attachment deci-
sions which are ambiguous in the English side of a
Chinese/English bitext, and then build a classifier
which uses information from a word alignment to
make PP-attachment decisions. No Chinese syn-
tactic information is required. We use automati-
cally generated German parses to improve English
syntactic parsing, and have not been able to find a
similar phenomenon for which only a word align-
ment would suffice.
7 Analysis
We looked at the weights assigned during the
cross-validation performed to obtain our best re-
sult. The weights of many of the 34 features we
defined were frequently set to zero. We sorted
the features by the number of times the relevant
? scalar was zero (i.e., the number of folds of
the cross-validation for which they were zero; the
greedy feature selection is deterministic and so we
do not run multiple trials). We then reran the same
greedy feature selection algorithm as was used in
table 1, row 3, but this time using only the top
9 feature values, which were the features which
were active on 4 or more folds6. The result was an
improvement on train of 0.84 and an improvement
on test of 0.73. This test result may be slightly
overfit, but the result supports the inference that
these 9 feature functions are the most important.
We chose these feature functions to be described
in detail in section 3. We observed that the variants
of the similar features POSParentPrj and Above-
POSPrj projected in opposite directions and mea-
sured character and word differences, respectively,
and this complementarity seems to help.
6We saw that many features canceled one another out on
different folds. For instance either the word-based or the
character-based version of DTNN was active in each fold,
but never at the same time as one another.
We also tried to see if our results depended
strongly on the log-linear model and training algo-
rithm, by using the SVM-Light ranker (Joachims,
2002). In order to make the experiment tractable,
we limited ourselves to the 8-best parses (rather
than 100-best). Our training algorithm and model
was 0.74 better than the baseline on train and 0.47
better on test, while SVM-Light was 0.54 better
than baseline on train and 0.49 better on test (us-
ing linear kernels). We believe that the results are
not unduly influenced by the training algorithm.
8 Conclusion
We have shown that rich bitext projection features
can improve parsing accuracy. This confirms the
hypothesis that the divergence in what information
different languages encode grammatically can be
exploited for syntactic disambiguation. Improved
parsing due to bitext projection features should be
helpful in syntactic analysis of bitexts (by way of
mutual syntactic disambiguation) and in comput-
ing syntactic analyses of texts that have transla-
tions in other languages available.
Acknowledgments
This work was supported in part by Deutsche
Forschungsgemeinschaft Grant SFB 732. We
would like to thank Helmut Schmid for support of
BitPar and for his many helpful comments on our
work. We would also like to thank the anonymous
reviewers.
References
Michaela Atterer and Hinrich Schu?tze. 2007. Preposi-
tional phrase attachment without oracles. Computa-
tional Linguistics, 33(4).
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and R. L. Mercer. 1993. The mathe-
matics of statistical machine translation: parameter
estimation. Computational Linguistics, 19(2).
David Burkett and Dan Klein. 2008. Two lan-
guages are better than one (for syntactic parsing). In
EMNLP.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In ACL.
Eugene Charniak, Sharon Goldwater, and Mark John-
son. 1998. Edge-based best-first chart parsing. In
Proceedings of the Sixth Workshop on Very Large
Corpora.
289
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In ICML.
Victoria Fossum and Kevin Knight. 2008. Using bilin-
gual Chinese-English word alignments to resolve
PP-attachment ambiguity in English. In AMTA.
Alexander Fraser and Daniel Marcu. 2007. Measuring
word alignment quality for statistical machine trans-
lation. Computational Linguistics, 33(3).
Shailly Goyal and Niladri Chatterjee. 2006. Parsing
aligned parallel corpus by projecting syntactic re-
lations from annotated source corpus. In Proceed-
ings of the COLING/ACL main conference poster
sessions.
Susan L. Graham, Michael A. Harrison, and Walter L.
Ruzzo. 1980. An improved context-free recognizer.
ACM Transactions on Programming Languages and
Systems, 2(3).
Mark Hopkins and Jonas Kuhn. 2006. A framework
for incorporating alignment information in parsing.
In Proceedings of the EACL 2006 Workshop on
Cross-Language Knowledge Induction.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Nat. Lang. Eng., 11(3).
Thorsten Joachims. 2002. Optimizing search en-
gines using clickthrough data. In Proceedings of the
Eighth ACM SIGKDD.
Takao Kasami. 1965. An efficient recognition and syn-
tax analysis algorithm for context-free languages.
Technical Report AFCRL-65-7558, Air Force Cam-
bridge Research Laboratory.
Dan Klein and Christopher Manning. 2003. A* pars-
ing: fast exact viterbi parse selection. In HLT-
NAACL.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In HLT-NAACL.
Philipp Koehn. 2005. Europarl: a parallel corpus for
statistical machine translation. In MT Summit X.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn treebank. Computa-
tional Linguistics, 19(2).
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
HLT-NAACL.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1).
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In ACL.
Chris Quirk and Simon Corston-Oliver. 2006. The im-
pact of parse quality on syntactically-informed sta-
tistical machine translation. In EMNLP.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard S. Crouch, John T. Maxwell III, and Mark
Johnson. 2002. Parsing the Wall Street Journal us-
ing a lexical-functional grammar and discriminative
estimation techniques. In ACL.
Helmut Schmid. 2004. Efficient parsing of highly am-
biguous context-free grammars with bit vectors. In
COLING.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Tomaz Erjavec, Dan Tufis, and
Daniel Varga. 2006. The JRC-Acquis: a multilin-
gual aligned parallel corpus with 20+ languages. In
LREC.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual POS taggers and NP bracketers via robust
projection across aligned corpora. In NAACL.
Daniel H. Younger. 1967. Recognition of context-free
languages in time n3. Information and Control, 10.
290
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 115?119,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Experiments in morphosyntactic processing for translating to and from
German
Alexander Fraser
Institute for Natural Language Processing
University of Stuttgart
fraser@ims.uni-stuttgart.de
Abstract
We describe two shared task systems and
associated experiments. The German to
English system used reordering rules ap-
plied to parses and morphological split-
ting and stemming. The English to Ger-
man system used an additional translation
step which recreated compound words and
generated morphological inflection.
1 Introduction
The Institute for Natural Language Processing
(IfNLP), Stuttgart, participated in the WMT-2009
shared tasks for German to English and English
to German translation with constrained systems
which employed morphological and syntactic pro-
cessing techniques. The systems were based on
the open source Moses docoder (Koehn et al,
2007). We combined IfNLP tools for syntactic and
morphological analysis (which are publicly avail-
able and widely used) with preprocessing tech-
niques that were successfully used by other groups
in WMT-2008, and extended these. For English to
German translation, we additionally performed a
step which recreated compound words and gener-
ated morphological inflection.
1.1 Baseline
The baseline is the standard system supplied for
the shared task. We used the default parameters
of the Moses toolkit, except for a small difference
in the generation of the word alignments, see sec-
tion 3.
2 Improvements
2.1 Character Normalization
We normalize both the English and German by
converting all characters to their nearest equivalent
in Latin-1 (ISO 8859-1) encoding1, except for the
euro sign, which is handled specially. We did not
modify the SGML files used for calculating BLEU
and METEOR scores in any way.
2.2 German Writing Reform
German underwent a writing reform from the alte
Rechtschreibung (old spelling rules/orthography)
to the neue Rechtschreibung (gloss: new spelling
rules/orthography) recently. Early Europarl
data are written using the alte Rechtschreibung
and hence need to be converted to the neue
Rechtschreibung in order to match the news data,
which is in the new form.
We began the process by mapping all cased vari-
ants of a particular word to a single class (such
as by mapping two words which are written with
ue and u?, but are otherwise identical, to a single
class). We then tried to automatically identify the
correct variant under the writing reform for each
class. Initially we tried the linux tool aspell but
found that its coverage (the recall of its lexicon)
was poor.
We used a simple technique for finding the best
variant. We separated the Europarl corpus into
portions written using the old and new forms. We
used the incidence of the word dass (the comple-
mentizer meaning that) and its old rules variant
da?. We used a chunk size of 70 sentences to
segment Europarl into old and new by counting
whether there were more instances of da? or dass,
respectively, in each chunk. We added the news
corpora to the new portion. For each variant we
counted the number of times it occurred in the
new data and subtracted the number of times it oc-
curred in the old data; the variant with the highest
adjusted count was selected.
1Latin-1 is an 8-bit encoding which has the common ac-
cented characters used in Western European languages. A
reviewer pointed out that ISO 8859-15 has superseded ISO
8859-1.
115
2.3 Reordering German
German word order differs from English substan-
tially. Preprocessing approaches involving the
use of a syntactic parse of the source sentence to
change the word order to more closely match the
word order of the target language have been stud-
ied by Niessen and Ney (2004), Xia and McCord
(2004), Dra?bek and Yarowsky (2004), Collins et
al. (2005), Popovic? and Ney (2006), Wang et al
(2007) and many others.
To obtain a parse of each German sentence in
the training, dev and test corpora, we employed the
IfNLP BitPar probabilistic parser (Schmid, 2004),
using models learned from the Tiger Treebank for
German.
Dealing with morphological productivity is im-
portant in the syntactic parsing of German. Bit-
Par has been designed with this in mind. IfNLP?s
SMOR analyzer is used for morphological analy-
sis (Schmid et al, 2004). SMOR is run over a list
of types in each German sentence, and outputs a
list of analyses for each type, each of which corre-
sponds to a POS tag. BitPar is limited to choosing
one of these POS tags for this type. Words which
SMOR fails to analyze are allowed to occur with
any POS tag.
We reimplemented the syntactic preprocessing
approach of Collins et al (2005), with modifica-
tions. Reordering rules are applied to a German
parse tree (generated by BitPar), and focus on re-
ordering the words in the German clause structure
to more closely resemble English clause structure.
The rules are applied to both the training data for
the SMT system, and the input (the dev and test
sets). We previously performed an error analysis
of this approach and for the work described here
we addressed some of the shortcomings identified
through the analysis. The analysis was performed
on the Europarl dev2006 set.
The first error that we noticed occurring fre-
quently was that some large clausal units which
were labeled as subjects were being moved for-
ward in the sentence. We modified the rule moving
subjects forward to not apply to the constituents S,
CS, VP and CVP. See the first part of table 3 for an
example. The phrase ?dass der Balkan ist kein Ge-
biet? is moved under the original rules, and with
the modification is no longer moved2.
2Note that there is an unrelated reordering error at the end
of the sentence for both BEFORE and AFTER, gibt (gloss:
gives) should have moved to follow das (gloss: that).
System BLEU METEOR LR
no processing 18.91 49.50 1.0097
c+w 19.37 49.69 1.0067
c+w, s/s 19.18 51.13 1.0035
c+w, old reordering 19.61 50.44 1.0092
c+w, new reordering 19.91 50.84 1.0059
c+w, new reordering, s/s
(submitted, bug)
19.65 51.57 1.0093
* c+w, new reordering, s/s 19.73 51.59 1.0062
as * IRSTLM quantized 19.52 51.33 1.0003
as * IRSTLM 19.75 51.61 1.0013
as * IRSTLM 21.2 quan-
tized
19.52 51.51 1.0095
as * RANDLM 19.67 51.73 1.0067
as * RANDLM 21.2 21.03 51.96 1.0111
Table 1: German to English, dev-2009b (case
sensitive), c+w = char+word normalization, s/s =
splitting/stemming, 21.2 = larger LM
System BLEU METEOR LR
no processing 13.55 38.31 0.9910
c+w (no second step) 14.11 38.27 0.9991
c+w, s/s, second step
(submitted, bug)
12.34 37.89 1.0338
c+w, s/s, second step 13.05 37.94 1.0157
Table 2: English to German, dev-2009b (case
sensitive), c+w = char+word normalization, s/s =
splitting/stemming
The second error that we handled was that S-RC
constituents which do not have a complementizer
are reordered incorrectly. We modified the orig-
inal verb 2nd rule, so that if there is no comple-
mentizer in a S-RC constituent, then the head is
moved to the second position, see the second part
of table 3 for an example. Using the original rules,
the verb 2nd rule fails to fire, incorrectly leaving
haben (gloss: have) at the end of the clause.
2.4 Morphological Decomposition
We implemented the frequency-based word split-
ting approach of Koehn and Knight (2003), and
made modifications, including some similar to
those described by Stymne et al (2008). This
well-known technique splits compound words. In
addition, we performed simple suffix elimination,
aimed at removing inflection marking features
such as gender and case that are not necessary for
translation to English. We took the stem combi-
nation with the highest geometric mean of the fre-
quencies of the stems, but following Stymne et al
(2008), we restricted stems to minimum length 4,
and we allowed an extended list of infixes: s, n,
en, nen, es, er and ien. For suffixes, we allowed:
e, en, n, es, s, em and er, which is more aggressive
116
INPUT Mir ist bewusst , dass der Balkan kein
Gebiet ist , das Anlass zu Optimismus
gibt .
gloss me is clear , that the Balkans not area is
, that opportunity for optimism gives .
BEFORE Mir dass der Balkan ist kein Gebiet ist
bewusst , , das Anlass zu Optimismus
gibt .
gloss me that the Balkans is not area is clear ,
that opportunity for optimism gives .
AFTER Mir ist bewusst , dass der Balkan ist kein
Gebiet , das Anlass zu Optimismus gibt
.
gloss me is clear , that the Balkans is not area
, that opportunity for optimism gives .
REF I am aware that the Balkans are not the
most promising area for optimism .
INPUT Am 23. November 1999 hat ein Partner-
schaftstag stattgefunden , an dem viele
von uns teilgenommen haben .
gloss on 23 November 1999 have a
partnership-day took-place , in which
many of us participated have .
BEFORE Am 23. November 1999 ein Partner-
schaftstag hat stattgefunden , an dem
teilgenommen viele von uns haben .
gloss on 23 November 1999 a partnership-day
have took-place , in which participated
many of us have .
AFTER Am 23. November 1999 ein Partner-
schaftstag hat stattgefunden , an dem
viele von uns haben teilgenommen .
gloss on 23 November 1999 a partnership-day
have took-place , in which many of us
have participated .
REF A partnership day was held on 23
November 1999 , in which many of us
participated .
Table 3: Differences in reordering: BEFORE is re-
ordering using rules in (Collins et al, 2005), AF-
TER is our modified reordering
than used in previous work (and therefore gener-
alizes more but at the same time causes some er-
roneous conflation). We stripped e, en and n from
all stems (but remembered the most frequent vari-
ant, so that applying the procedure to Kirchturm
results in Kirche Turm (gloss: church tower)). We
store an alignment from the original German to the
simplified German which we will use in the next
section.
2.5 Morphological Generation
For translation from English to German, we first
translated from English to the simplified German
presented in the previous section, and then per-
formed an independent translation step from sim-
plified German to fully inflected German.
Two processes are handled by this step. First,
series of stems corresponding to compound words
are recomposed (along with infixes which are not
present in the simplified German form) into com-
pound words. Second, inflection is added (e.g.,
case and gender agreement is handled). Both of
these processes are implemented using a Moses
system trained on a parallel corpus where the
source language is simplified German and the tar-
get language is fully inflected German. The align-
ment is error-free as it was generated as a side
effect of the splitting and stemming process de-
scribed in the previous section. In translation, re-
ordering is not allowed, but we otherwise use stan-
dard Moses settings.
3 Experiments
3.1 German to English
We trained our German to English system on the
constrained parallel data. The English data was
processed using character normalization. The Ger-
man data was first processed using character and
word (writing reform) normalization. We then
parsed the German data using BitPar and applied
the modified reordering rules. After this the split-
ting and stemming process was applied. Finally,
we lowercased the data.
Word alignments were generated using Model
4 (Brown et al, 1993) using the multi-threaded
implementation of GIZA++ (Och and Ney, 2003;
Gao and Vogel, 2008). We first trained Model 4
with English as the source language, and then with
German as the source language, resulting in two
Viterbi alignments3. The resulting Viterbi align-
ments were combined using the Grow Diag Final
And symmetrization heuristic (Koehn et al, 2003).
We estimated a standard Moses system using de-
fault settings. MERT was run until convergence
using dev-2009a (separately for each experiment).
One limitation of our German to English system
is that we were unable to scale to the full language
modeling data using SRILM (Stolcke, 2002), 5-
grams and modified Kneser-Ney with no single-
ton deletion4. The language model in our sub-
mitted system is based on all of the available En-
glish data, but news-train08 is truncated to the first
10193376 lines, meaning that we did not train on
the remaining 11038787 lines, so we used a little
less than half of the data. We converted the lan-
3We used 5 iterations of Model 1, 4 iterations of HMM
(Vogel et al, 1996) and 4 iterations of Model 4.
4SRILM failed when trained on the full data, even when a
machine with 32 GB RAM and 48 GB swap was used.
117
guage model trained using SRILM to the binary
format using IRSTLM.
Experiments are presented in table 1, using
BLEU (Papineni et al, 2001) and METEOR5
(Banerjee and Lavie, 2005), and we also show
the length ratio (ratio of hypothesized tokens to
reference tokens). For translation into English
METEOR had superior correlation with human
rankings to BLEU at WMT 2008 (Callison-Burch
et al, 2008). Our submitted system had a bug
where the environment variable LC ALL was set
to en US when creating the binarized filtered lex-
icalized reordering table for the test set (and for
the blindtest set, but not for the dev set used for
MERT). This caused minor degradation, see the
system marked (*) for the system with the bug cor-
rected.
Each system increases in both BLEU and ME-
TEOR as improvements are added. An exception
is that splitting/stemming decreases BLEU some-
what. However, we trust the METEOR results
more due to their better correlation with human
judgements.
We also compared using a different language
model instead of the SRILM model (the bottom
half of table 1). These used either the reduced
English language modeling data or the full data
(21.2 M segments, marked 21.2 in the results).
RANDLM (Talbot and Osborne, 2007) performs
well and scaled to the full data with improvement
(resulting in our best overall system). IRSTLM
(Federico and Cettolo, 2007) also performs well,
but the quantized model on the 21.2 data did
not improve over the smaller quantized model6.
IRSTLM uses an approximation of Witten-Bell
smoothing, our results support that this is compet-
itive.
3.2 English to German
We trained our English to German system on the
constrained parallel data. The first SMT system
translates from lowercased English to lowercased
simplified German, which is then recased. The
syntactic reordering process is not used, but other-
wise the German data is processed identically. The
alignment from simplified German to English is
generated as described in the previous section. We
used all of the German data to train the language
5METEOR used default weights, stemming and Wordnet
synsets.
6After speaking with the authors, we plan to try IRSTLM
on the full data using memory mapping for binarization.
model on simplified German. The second SMT
system translates mixed case simplified German to
mixed case unsimplified German. The translation
model is built only on the simplified German from
the parallel text, and the language model is trained
on all German data.
We present the results in table 2. METEOR7 did
not correlate as well as BLEU for translation out of
English in WMT 2008. The BLEU score of our fi-
nal system is worse than the baseline. We had cho-
sen to submit this system as we found it more in-
teresting than submitting a vanilla system. In addi-
tion, the system of Stymne et al (2008) received a
good human evaluation despite having a relatively
low BLEU score, and we hoped we were perform-
ing similar morphological generalization. We ex-
pect to be able to improve this system through er-
ror analysis. In an initial inspection we found case
mismatching problems between step one and step
two.
4 Conclusion
We presented our German to English system
which employed character normalization, com-
pensated for problems caused by the German writ-
ing reform, used modified syntactic reordering
rules (in combination with morphologically aware
parsing), and employed substring-based morpho-
logical analysis. Our best system improves by
2.46 METEOR and 1.12 BLEU over a standard
Moses system. Our English to German sys-
tem used the same two normalizations and the
substring-based morphological analysis, and addi-
tionally implemented a second translation step for
recreating compound words and generating case
and gender inflection. We will improve this sys-
tem in future work.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with im-
proved correlation with human judgments. In Pro-
ceedings of Workshop on Intrinsic and Extrinsic
Evaluation Measures for MT and/or Summarization
at the 43th Annual Meeting of the Association of
Computational Linguistics (ACL-2005), Ann Arbor,
Michigan.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and R. L. Mercer. 1993. The mathe-
matics of statistical machine translation: parameter
7METEOR for this task is calculated using default
weights but no Wordnet synsets.
118
estimation. Computational Linguistics, 19(2):263?
311.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
ACL Third Workshop on Statistical Machine Trans-
lation, Columbus, Ohio.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In ACL, pages 531?540, Ann Arbor, MI.
Elliott F. Dra?bek and David Yarowsky. 2004. Improv-
ing bitext word alignments via syntax-based reorder-
ing of English. In The Companion Volume to the
Proceedings of the 42nd Annual Meeting of the As-
sociation for Computational Linguistics, pages 146?
149, Barcelona, Spain.
Marcello Federico and Mauro Cettolo. 2007. Efficient
handling of n-gram language models for statistical
machine translation. In Proceedings of the Second
Workshop on Statistical Machine Translation, pages
88?95, Prague, Czech Republic.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, pages 49?57.
Philipp Koehn and Kevin Knight. 2003. Empirical
methods for compound splitting. In EACL, pages
187?193, Morristown, NJ.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In HLT-NAACL,
pages 127?133, Edmonton, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL, Demonstration Program, Prague, Czech Re-
public.
Sonja Niessen and Hermann Ney. 2004. Statisti-
cal machine translation with scarce resources using
morpho-syntactic information. Computational Lin-
guistics, 30(2):181?204.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Kishore A. Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2001. BLEU: a method for auto-
matic evaluation of machine translation. Technical
Report RC22176 (W0109-022), IBM Research Di-
vision, Thomas J. Watson Research Center, York-
town Heights, NY.
Maja Popovic? and Hermann Ney. 2006. POS-based
word reorderings for statistical machine translation.
In LREC, pages 1278?1283, Genoa, Italy.
Helmut Schmid, Arne Fitschen, and Ulrich Heid.
2004. SMOR: a German computational morphology
covering derivation, composition, and inflection. In
LREC, pages 1263?1266, Lisbon, Portugal.
Helmut Schmid. 2004. Efficient parsing of highly am-
biguous context-free grammars with bit vectors. In
COLING, Geneva, Switzerland.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Intl. Conf. Spoken Lan-
guage Processing, Denver, Colorado.
Sara Stymne, Maria Holmqvist, and Lars Ahrenberg.
2008. Effects of morphological analysis in transla-
tion between German and English. In Proceedings
of the Third Workshop on Statistical Machine Trans-
lation, pages 135?138, Columbus, Ohio.
David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine transla-
tion. In ACL, pages 512?519, Prague, Czech Re-
public.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical
translation. In COLING, pages 836?841, Copen-
hagen, Denmark.
Chao Wang, Michael Collins, and Philipp Koehn.
2007. Chinese syntactic reordering for statistical
machine translation. In EMNLP-CONLL, pages
737?745.
Fei Xia and Michael McCord. 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. In COLING.
119
Language Weaver Arabic->English MT 
Daniel MARCU, Alex FRASER, William WONG, Kevin KNIGHT 
Language Weaver, Inc.  
4640 Admiralty Way, Suite 1210 
Marina del Rey, CA, USA, 90292 
{marcu,afraser,wong,knight}@languageweaver.com 
 
Abstract 
This presentation is primarily a demonstration 
of a working statistical machine translation 
system which translates Modern Standard 
Arabic into English.  
1 Overview 
Language Weaver has produced a high-
performance statistical Arabic-to-English machine 
translation system, based on research work 
conducted at the University of Southern California, 
Information Sciences Institute (USC/ISI).  Getting 
resource-unlimited laboratory systems to run in 
real time, on a typical desktop Windows machine, 
is among Language Weaver?s contributions.  The 
system is designed to provide broad general 
coverage of Arabic news, and is currently used at 
various sites within the U.S. Government.  
 
 
 
 
 
The Arabic->English translation system to be 
demonstrated has been prepared in versions that 
require 1 or 2 GB of RAM, and run on a 1.5GHz or 
faster processor and translates at a minimum rate 
of 500 words per minute.  The system includes an 
option to trade off speed for quality in the 
translation process allowing users to select the 
fastest possible gisting-quality output, or the best 
possible translation quality for each sentence. 
2 Demonstration 
The translation system will be demonstrated on 
current news, and possibly other postings from 
Internet, or other files: 
Coling 2010: Poster Volume, pages 81?89,
Beijing, August 2010
Improved Unsupervised Sentence Alignment for Symmetrical and
Asymmetrical Parallel Corpora
Fabienne Braune Alexander Fraser
Institute for Natural Language Processing
Universita?t Stuttgart
{braunefe,fraser}@ims.uni-stuttgart.de
Abstract
We address the problem of unsupervised
and language-pair independent alignment
of symmetrical and asymmetrical parallel
corpora. Asymmetrical parallel corpora
contain a large proportion of 1-to-0/0-to-1
and 1-to-many/many-to-1 sentence corre-
spondences. We have developed a novel
approach which is fast and allows us to
achieve high accuracy in terms of F1 for
the alignment of both asymmetrical and
symmetrical parallel corpora. The source
code of our aligner and the test sets are
freely available.
1 Introduction
Sentence alignment is the problem of, given a par-
allel text, finding a bipartite graph matching min-
imal groups of sentences in one language to their
translated counterparts. Because sentences do not
always align 1-to-1, the sentence alignment task is
non-trivial.
The achievement of high accuracy with mini-
mal consumption of computational resources is a
common requirement for sentence alignment ap-
proaches. However, in order to be applicable to
parallel corpora in any language without requir-
ing a separate training set, a method for sentence-
alignment should also work in an unsupervised
fashion and be language pair independent. By
?unsupervised?, we denote methods that infer the
alignment model directly from the data set to be
aligned. Language pair independence refers to ap-
proaches that require no specific knowledge about
the languages of the parallel texts to align.
We have developed an approach to unsuper-
vised and language-pair independent sentence
alignment which allows us to achieve high accu-
racy in terms of F1 for the alignment of both sym-
metrical and asymmetrical parallel corpora. Due
to the incorporation of a novel two-pass search
procedure with pruning, our approach is accept-
ably fast. Compared with Moore?s bilingual sen-
tence aligner (Moore, 2002), we obtain an average
F1 of 98.38 on symmetrical parallel documents,
while Moore?s aligner achieves 94.06. On asym-
metrical documents, our approach achieves 97.67
F1 while Moore?s aligner obtains 88.70. On av-
erage, our sentence aligner is only about 4 times
slower than Moore?s aligner.
This paper is organized as follows: previous
work is described in section 2. In section 3, we
present our approach. Finally, in section 4, we
conduct an extensive evaluation, including a brief
insight into the impact of our aligner on the over-
all performance of an MT system.
2 Related Work
Among approaches that are unsupervised and lan-
guage independent, (Brown et al, 1991) and (Gale
and Church, 1993) use sentence-length statistics
in order to model the relationship between groups
of sentences that are translations of each other. As
shown in (Chen, 1993) the accuracy of sentence-
length based methods decreases drastically when
aligning texts containing small deletions or free
translations. In contrast, our approach augments a
sentence-length based model with lexical statistics
and hence constantly provides high quality align-
ments.
(Moore, 2002) proposes a multi-pass search
81
procedure where sentence-length based statistics
are used in order to extract the training data for
the IBM Model-1 translation tables. The ac-
quired lexical statistics are then combined with
the sentence-length based model in order to ex-
tract 1-to-1 correspondences with high accuracy1.
Moore?s approach constantly achieves high preci-
sion, is robust to sequences of inserted and deleted
text, and is fast. However, the obtained recall is
at most equal to the proportion of 1-to-1 corre-
spondences contained in the parallel text to align.
This point is especially problematic when align-
ing asymmetrical parallel corpora. In contrast,
our approach allows to extract 1-to-many/many-
to-1 correspondences. Hence, we achieve high
accuracy in terms of precision and recall on both
symmetrical and asymmetrical documents. More-
over, because we use, in the last pass of our multi-
pass method, a novel two-stage search procedure,
our aligner also requires acceptably low computa-
tional resources.
(Deng et al, 2006) have developed a multi-
pass method similar to (Moore, 2002) but where
the last pass is composed of two alignment pro-
cedures: a standard dynamic programming (DP)
search that allows one to find many-to-many
alignments containing a large amount of sentences
in each language and a divisive clustering al-
gorithm that optimally refines those alignments
through iterative binary splitting. This alignment
method allows one to find, in addition to 1-to-
1 correspondences, high quality 1-to-many/many-
to-1 alignments. However, 1-to-0 and 0-to-1 cor-
respondences are not modeled in this approach2.
This leads to poor performance on parallel texts
containing that type of correspondence. Further-
more performing an exhaustive DP search in or-
der to find large size many-to-many alignments
involves high computational costs. In comparison
to (Deng et al, 2006), our approach works in the
opposite way. Our two-step search procedure first
1The used search heuristic is a forward-backward compu-
tation with a pruned dynamic programming procedure as the
forward pass.
2In (Deng et al, 2006), p. 5, the p(ak) = p(x, y) which
determines the prior probability of having an alignment con-
taining x source and y target sentences, is equal to 0 if x < 1
or y < 1. As p(ak) is a multiplicative factor of the model,
the probability of having an insertion or a deletion is always
equal to 0.
finds a model-optimal alignment composed of the
smallest possible correspondences, namely 1-to-
0/0-to-1 and 1-to-1, and then merges those cor-
respondences into larger alignments. This allows
the finding of 1-to-0/0-to-1 alignments as well
as high quality 1-to-many/many-to-1 alignments,
leading to high accuracy on parallel texts but also
on corpora containing large blocs of inserted or
deleted text. Furthermore, our approach keeps the
computational costs of the alignment procedure
low: our aligner is, on average, about 550 times
faster than our implementation3 of (Deng et al,
2006).
Many other approaches to sentence-alignment
are either supervised or language dependent. The
approaches by (Chen, 1993), (Ceausu et al, 2006)
or (Fattah et al, 2007) need manually aligned
pairs of sentences in order to train the used align-
ment models. The approaches by (Wu, 1994),
(Haruno and Yamazaki, 1996), (Ma, 2006) and
(Gautam and Sinha, 2007) require an externally
supplied bilingual lexicon. Similarly, the ap-
proaches by (Simard and Plamondon, 1998) or
(Melamed, 2000) are language pair dependent in-
sofar as they are based on cognates.
3 Two-Step Clustering Approach
We present here our two-step clustering approach
to sentence alignment4 which is the main contri-
bution of this paper. We begin by giving the main
ideas of our approach using an introductory exam-
ple (section 3.1). Then we show to which extent
computational costs are reduced in comparison to
a standard DP search (section 3.2) before present-
ing the theoretical background of our approach
(section 3.3). We further discuss a novel prun-
ing strategy used within our approach (section
3.4). This pruning technique is another important
contribution of this paper. Next, we present the
alignment model (section 3.5) which is a slightly
modified version of the alignment model used in
(Moore, 2002). Finally, we describe the overall
3In order to provide a precise comparison between our
aligner and (Deng et al, 2006), we have implemented their
model into our optimized framework.
4Note that our approach does not aim to find many-to-
many alignments. None of the unsupervised sentence align-
ment approaches discussed in section 2 are able to correctly
find that type of correspondence.
82
procedure required to align a parallel text with our
method (section 3.6).
3.1 Sketch of Approach
Consider a parallel text composed of six source
language sentences Fi and four target language
sentences Ej . Further assume that the correct
alignment between the given texts is composed of
four correspondences: three 1-to-1 alignments be-
tween F1, E1; F2, E2 and F6, E4 as well as a 3-to-
1 alignment between F3, F4, F5 and E3. Figure 1
illustrates this alignment.
F1 E1
F2 E2
F3
F4
F5
F6 E4
E3
Figure 1: Correct Alignment between Fi and Ej
In the perspective of a statistical approach to
sentence alignment, the alignment in figure 1 is
found by computing the model-optimal alignment
A? for the bitext considered:
A? = argmax
A
?
ak?A
SCORE(ak) (1)
where SCORE(ak) denotes the score attributed
by the alignment model5 to a minimal alignment
ak composing A?. The optimization given in
equation 1 relies on two commonly made assump-
tions: (c1) a model-optimal alignment A? can
be decomposed into k minimal and independent
alignments ak; (c2) each alignment ak depends
only on local portions of text in both languages.
The search for A? is generally performed us-
ing a dynamic programming (DP) procedure over
the space formed by the l source and m target
sentences. The computation of A? using a DP
search relies on the assumption (c3) that sentence
alignment is a monotonic and continuous process.
The DP procedure recursively computes the opti-
mal score D(l,m)? for a sequence of alignments
covering the whole parallel corpus. The optimal
score D(l,m)? is given by the following recur-
5The alignment model will be presented in section 3.5.
sion:
D(l,m)? = min
0?x,y?R , x=1?y=1D(l ? x,m? y)
?
? logSCORE(ak)
(2)
where x denotes the number of sentences on the
source language side of ak and y the number of
sentences on the target language side of ak.
The constant R constitutes an upper bound to
the number of sentences that are allowed on each
side of a minimal alignment ak. This constant has
an important impact on the computational costs
of the DP procedure insofar as it determines the
number of minimal alignments that have to be
compared and scored at each step of the recursion
given in equation 2. As will be shown in section
3.2, the number of comparisons increases depend-
ing on R.
The solution we propose to the combinatorial
growth of the number of performed operations
consists of dividing the search for A? into two
steps. First, a model-optimal alignment A?1, in
which the value of R is fixed to 1, is found. Sec-
ond, the alignments a?k composing A?1 are merged
into clusters mr containing up to R sentences on
either the source or target language side. The
alignment composed of these clusters is A?R.
The search for the first alignment A?1 is per-
formed using a standard DP procedure as given in
equation 2 but withR = 1. This first alignment is,
hence, only composed of 0-to-1, 1-to-0 and 1-to-1
correspondences. Using our example, we show, in
figure 2, the alignment A?1 found in the first step
of our approach. The neighbors of F4, that is F3
and F5, are aligned as 1-to-0 correspondences.
F1 E1
F2 E2
F3
F4
F5
F6 E4
E3
Figure 2: A?1 in our Approach (first step)
The search for A?R is performed using a DP
search over the alignments a?k composingA?1. The
score D(AR)? obtained when all alignments a?k ?
A?1 have been optimally clustered can be written
83
recursively as:
D(AR)? = min0?r?RD(AR ? r)
?
? logSCORE(mr)
(3)
whereD(AR?r)? denotes the best score obtained
for the prefix covering all minimal alignments in
A?1 except the last r minimal alignments consid-
ered for composing the last cluster mr.
The application of the second step of our ap-
proach is illustrated in figure 3. The first align-
ment, between F1 and E1, cannot be merged to be
part of a 1-to-many or many-to-1 cluster because
the following alignment in A?1 is also 1-to-1. So
it must be retained as given in A?1. The five last
alignments are, however, candidates for compos-
ing clusters. For instance, the alignment F2-E2
and F3-, where  denotes the empty string, could
be merged in order to compose the 2-to-1 cluster
F2,F3-E2. However, in our example, the align-
ment model chooses to merge the alignments F3-
, F4-E3 and F5- in order to compose the 3-to-1
cluster F3,F4,F5-E3.
F1 E1
F2 E2
F3
F4
F5
F6 E4
E3
Figure 3: A?R in our Approach (second step)
3.2 Computational Gains
The aim of this section is to give an idea about
why our method is faster than the standard DP
approach. Let C denote the number of compar-
isons performed at each step of the recursion of
the standard DP procedure, as given in equation
2. This amount is equivalent to the number of
possible combinations of x source sentences with
y target sentences. Hence, for an approach find-
ing all types of correspondences except many-to-
many, we have:
C = 2R+ 1 (4)
In terms of lookups in the word-correspondence
tables of a model including lexical statistics, the
number of operations Cl performed at each step
of the recursion is given by:
Cl = R? ? w2 (5)
where R? denotes the number of scored sen-
tences6. w denotes the average length of each
sentence in terms of words. The total number of
lookups performed in order to align a parallel text
containing l source and m target sentences using
a standard DP procedure is hence given by:
L = R? ? w2 ? l ?m (6)
In the perspective of our two-step search proce-
dure, the computational costs of the search for the
initial alignment A?1 is given by:
L?1 = w2 ? l ?m (7)
For the second step of our approach, because A?R
is a cluster of A?1, the dynamic programming pro-
cedure used to find this alignment is no longer
over the l ? m space formed by the source and
target sentences but instead over the space formed
by the minimal alignments a?k in A?1. The aver-
age number of those alignments is approximately
l+m
2 .7 The number of lookups performed at eachstep of our DP procedure is given by:
L?2 = R? ? w2 ?
l +m
2 (8)
where R? and w are defined as in equation 6.
The total number of lookups for our clustering ap-
proach is hence given by:
L?1+2 = (w2 ? l ?m) + (R? ? w2 ?
l +m
2 ) (9)
In order to compare the costs of our approach and
a standard DP search over the l ?m space formed
by the source and target sentences, we re-write
equation 6 as:
L = (w2 ? l ?m) + ((R?? 1) ?w2 ? l ?m) (10)
The comparison of equation 9 with equation 10
shows that the computational gains obtained using
our two-step approach reside in the reduction of
the search space from l ?m to l+m2 .8
6In a framework where no caching of scores is performed,
we have R? = R2 +R+1 compared sentences while score-
caching allows one to reduce R? to R.
7Note that this amount tends to l +m when A?1 contains
a large number of 0-to-1/1-to-0 correspondences.
8It should be noted that through efficient pruning, the
search space of the standard (DP) procedure can be further
reduced, see section 3.4.
84
3.3 Theoretical Background
We now present the theoretical foundation of our
approach. First, we rewrite equation 1 in a more
detailed fashion as:
A?R = argmax
A
?
ak(xk,yk)?AR
P (ak(xk, yk), sqi , trj)
(11)
with 0 ? xk, yk ? R, where R denotes the max-
imal amounts x and y of source and target lan-
guage sentences composing a minimal alignment
ak(xk, yk). The distribution P (ak(xk, yk), sqi , trj)
specifies the alignment model presented in section
3.5.
As seen in section 3.1, the formulation of the
alignment problem as given in equation 11 and the
use of a DP search in order to solve this equation
rely on the assumptions (c1) to (c3). Following
these assumptions, a model-optimal alignmentA?1
can be defined as an ordered set of minimal align-
ments a?k(xk, yk), with 0 ? xk, yk ? 1, where the
aligned portions of text are sequential. In other
words, if the k ? th alignment a?k(xk, yk) con-
tains the sequences sqi and trj of source and tar-
get language sentences, then the next alignment
a?k+1(xk+1, yk+1) is composed of the sequences
suq+1 and tvr+1. Hence, each alignment composing
AR, with R > 1, can be obtained through sequen-
tial merging of a series of alignments a?k(xk, yk) ?
A?1.9 Accordingly, the sequences of sentences su1
and tv1 are obtained by merging sq1 and tr1 with
suq+1 and tvr+1. It can then be assumed that (c4) the
ordered set of minimal alignments composing A?R
under equation 11 is equivalent to the set of clus-
ters obtained by sequentially merging the minimal
alignments composing A?1. Following assump-
tion (c4), the optimization over ak(xk, yk) ? AR
is equivalent to an optimization over the merged
alignmentsmr(xr, yr) ? AR. Hence, equation 11
is equivalent to:
A?R = argmax
AR
?
mr(xr,yr)?AR
P (mr(xr, yr), sui , tvj )
(12)
where each mr(xr, yr) is obtained by merging r
minimal alignments a?k(xk, yk) ? A?1.
9Alignments of type 1-to-0/0-to-1 and 1-to-1 are assumed
to be clusters where a minimal alignment a?k(xk, yk) ? A?1has been merged with the empty alignment e0(0, 0)(, ).
The computation of A?R is done in two
steps. First, a model-optimal alignment A?1 is
found using a standard DP procedure as de-
fined in equation 2 but with R = 1 and where
SCORE(ak) is given by the alignment model
? logP (ak, sll?x+1, tmm?y+1). In the second step,
the search procedure used to find the optimal
clusters is defined as in equation 3 but where
SCORE(mr) is given by the alignment model
? logP (mr, sui , tvj ).
3.4 Search Space Pruning
In order to further reduce the costs of finding A?1,
we initially pruned the search space in the same
fashion as (Moore, 2002). We explored a nar-
row band around the main diagonal of the bi-
text to align. Each time the approximated align-
ment came close to the boundaries of the band,
the search was reiterated with a larger band size.
However, the computational costs for alignments
that were not along the diagonal quickly increased
with this pruning strategy. A high loss of effi-
ciency was hence observed when aligning asym-
metrical documents with this technique. Inciden-
tally, Moore reports, in his experiments, that for
the alignment of a parallel text containing 300
deleted sentences, the computational costs of his
pruned DP procedure is 40 times higher than for a
corpus containing no deletions.
In order to overcome this problem, we devel-
oped a pruning strategy that allows us to avoid the
loss of efficiency occurring when aligning asym-
metrical documents. Instead of exploring a nar-
row band around the main diagonal of the text to
align, we use sentence-length statistics in order to
compute an approximate path through the consid-
ered bitext. Our search procedure then explores
the groups of sentences that are around this path.
If the approximated alignment comes close to the
boundaries of the band, the search is re-iterated.
The path initially provided using a sentence-
length model10 and then iteratively refined is
closer to the correct alignment than the main di-
agonal of the bitext to align. Hence, the approxi-
mated alignment does not come close to the band
10The used model is the sentence-length based component
of (Moore, 2002), which is able to find 1-to-0/0-to-1 corre-
spondences.
85
as often as when searching around the main di-
agonal. This results in relatively high computa-
tional gains, especially for asymmetrical parallel
texts (see section 4).
3.5 Moore?s Alignment Model
The model we use is basically the same as in
(Moore, 2002) but minor modifications have been
made in order to integrate this model in our two-
step clustering approach. The three component
distributions of the model are given by11:
P (ak, sqi , trj) = P (ak)P (s
q
i |ak)P (trj |ak, s
q
i )
(13)
The first component, P (ak), specifies the gen-
eration of a minimal alignment ak. The second
component, P (sqi |ak), specifies the generation of
a sequence sqi of source language sentences in
a minimal alignment ak. The last component,
i.e. P (trj |ak, sqi ), specifies the generation of a se-
quence of target language sentences depending on
a sequence of generated source sentences.
Our first modification to Moore?s model con-
cerns the component distribution P (ak). In the
second pass of our two-step approach, which is
the computation of the model-optimal clustered
alignment A?R, we estimate P (ak) by computing
the relative frequency of sequences of alignments
a?k in the initial alignment A?1 that are candidates
for composing a cluster mr of specific size.12 A
second minimal modification to Moore?s model
concerns the lexical constituent of P (trj |ak, sqi ),
which we denote here by P (fb|en, ak). In contrast
with Moore, we use the best alignment (Viterbi
alignment) of each target word fb with all source
words en, according to IBM Model-1:
P (fb|en, ak) =
argmaxlen=1 Pt(fb|en)
le + 1
(14)
where le denotes the number of words in the
source sentence(s) of ak. Our experimental results
have shown that this variant performed slightly
better than Moore?s summing over all alignments.
11In order to simplify the presentation of the model, we
use the short notation ak for denoting ak(xk, yk)
12For the computation ofA?1, the distribution P (ak) is de-
fined as in Moore?s work.
3.6 Alignment Procedure
In order to align a parallel text (sl1, tm1 ) we use
a multi-pass procedure similar to (Moore, 2002)
but where the last pass is replaced by our two-
step clustering approach. In the first pass, an ap-
proximate alignment is computed using sentence-
length based statistics and the one-to-one corre-
spondences with likelihood higher than a given
threshold are selected for the training of the IBM
Model-1 translation tables13. Furthermore, each
found alignment is cached in order to be used as
the initial diagonal determining the search space
for the next pass. In the second pass, the corpus is
re-aligned according to our two-step approach: (i)
a model-optimal14 alignment containing at most
one sentence on each side of the minimal align-
ments ak(xk, yk) is found; (ii) those alignments
are model-optimally merged in order to obtain an
alignment containing up to R sentences on each
side of the clusters mr(xr, yr). In our experi-
ments, a maximum number of 4 sentences is al-
lowed on each side of a cluster.
4 Experiments
We evaluate our approach (CA) using three base-
lines against which we compare alignment qual-
ity and computational costs.15 The first (Mo) is
the method by (Moore, 2002). As a second base-
line (Std), we have implemented an aligner that
finds the same type of correspondences as our ap-
proach but performs a standard DP search instead
of our two-pass clustering procedure and imple-
ments Moore?s pruning strategy. Our third base-
line (Std P.) is similar to (Std) but integrates our
pruning technique.16 We also evaluate the impact
13Words with frequency < 3 in the corpus have been
dropped.
14This is optimal according to the alignment model which
will be presented in section 3.5.
15We do not evaluate sentence-length based methods in
our experiments because these methods obtain an F1 which
is generally about 10% lower than for our approach on
symmetrical documents. For asymmetrical documents the
performance is even worse. For example, when using
Gale&Church F1 sinks to 13.8 on documents which are not
aligned at paragraph level and contain small deletions.
16We do not include (Deng et al, 2006) in our exper-
iments because our implementation of this aligner is 550
times slower than our proposed method and the inability to
find 1-to-0/0-to-1 correspondences makes it inappropriate for
asymmetrical documents.
86
S 1-1 1-N/N-1 0-1/1-0 Oth. Tot.
1 88.2% 10.9 % 0.005% 0.85% 3,877
2 91.9% 7.5% 0.007% 0.53% 2,646
3 91.6% 2.7% 4.3% 1.4% 23,715
4 44.8% 6.2% 49% 0.01% 2,606
Table 1: Test Set for Evaluation with 2 ? N ? 4
of our aligner on the overall performance of an
MT system.
Evaluation. We evaluate the alignment accu-
racy of our approach using four test sets annotated
at sentence-level. The two first are composed
of hand aligned documents from the Europarl
corpus for the language-pairs German-to-English
and French-to-English. The third is composed
of an asymmetric document from the German-to-
English part of the Europarl corpus. Our fourth
test set is a version of the BAF corpus (Simard,
1998), where we corrected the tokenization. BAF
is an interesting heterogeneous French-to-English
test set composed of 11 texts belonging to four
different genres. The types of correspondences
composing our test sets are given in table 1. The
metrics used are precision, recall and F117. Only
alignments that correspond exactly to reference
alignments count as correct. The computational
costs required for each approach are measured in
seconds. The time required to train IBM Model-1
is not included in our calculations18.
Summary of Results. Regarding alignment ac-
curacy, the results in table 2 show that (CA) ob-
tains, on average, an F1 that is 4.30 better than
for (Mo) on symmetrical documents. The results
in table 3 show that, on asymmetrical texts, (CA)
achieves an F1 which is 8.97 better than (Mo).
The accuracy obtained using (CA), (Std) and (Std
P.) is approximately the same. We have further
compared the accuracy of (CA) with (Std) for
finding 1-to-many/many-to-1 alignments. The ob-
tained results show that (CA) achieves an F1 that
is 5.0 better than (Std).
Regarding computational costs, the time re-
quired by (CA) is on average 4 times larger than
17We measure precision, recall and F1 on the 1-to-N/N-to-
1 alignments,N >= 1, which means that we view insertions
and deletions as ?negative? decisions, like Moore.
18The reason for this decision is that our optimized frame-
work trains the Model-1 translation tables far faster than
Moore?s bilingual sentence aligner.
for (Mo) when aligning symmetrical documents.
On asymmetrical documents, (Mo) is, however,
only 1.5 times faster than (CA). Compared to
(Std), (CA) is approximately 6 times faster on
symmetrical and 80 times faster on asymmetrical
documents. The time of (Std P.) is 3 times higher
than for (CA) on symmetrical documents and 22
times higher on asymmetrical documents. This
shows that, first, our pruning technique is more
efficient than Moore?s and, second, that the main
increase in speed is due to the two step clustering
approach.
Discussion. On the two first test sets, (Mo)
achieves high precision while the obtained recall
is limited by the number of correspondences that
are not 1-to-1 (see table 1). Regarding (Std), (Std
P.) and (CA), all aligners achieve high precision
as well as high recall, leading to an F1 which is
over 98% for both documents. The computational
costs of (CA) for the alignment of symmetrical
documents are, on average, 4 times higher than
(Mo), 6 times lower than (Std) and 3.5 times
lower than (Std P.). On our third test set (Mo)
achieves, with an F1 of 88.70, relatively poor
recall while the other aligners reach precision
and recall values that are over 98%. Regarding
the computational costs, (CA) is only 1.5 times
slower than (Mo) on asymmetrical documents
while it is 80 times faster than (Std) and about 22
times faster than (Std P.). On our fourth test set
all evaluated aligners perform approximately the
same than on Europarl. While (Mo) obtains, with
94.46, an F1 which is the same as for Europarl,
(CA) performs, with an F1 of 97.67, about
1% worse than on Europarl. A slightly larger
decrease of 1.6% is observed for (Std) which
obtains 96.81 F1. Note, however, that (CA), (Std)
and (Std P.) still perform about 3% better than
(Mo). Regarding computational costs, (CA) is
4 times slower than (Mo) and 40 times faster
than (Std). The high difference in speed between
our approach and (Std) is due to the fact that the
BAF corpus contains texts of variable symmetry
while (Std) shows a great speed decrease when
aligning asymmetrical documents. Finally, we
have compared the accuracy of (Std) and (CA) for
the finding of 1-to-many/many-to-1 alignments
containing at least 3 sentences on the ?many?
87
Appr. Lang. Prec. Rec. F1 Speed
Mo D-E 98.75 87.88 92.99 935s
Mo F-E 98.97 91.56 95.12 1,661s
Std D-E 98.42 98.57 98.49 24,152s
Std F-E 98.45 98.83 98.64 35,041s
Std P. D-E 98.37 98.49 98.43 13,387s
Std P. F-E 98.41 98.78 98.60 21,848s
CA D-E 98.25 98.70 98.47 3,461s
CA F-E 98.00 98.60 98.30 6,978s
Table 2: Performance on Europarl
Appr. Prec. Rec. F1 Speed
Mo 97.90 81.08 88.70 552s
Std 97.66 97.74 97.70 71,475s
Std P. 97.74 97.81 97.77 17,502s
CA 97.38 97.97 97.67 800s
Table 3: Performance on asym. documents
Appr. Prec. Rec. F1 Speed
Mo 96.58 92.43 94.46 563s
Std 96.82 96.80 96.81 84,988s
CA 97.05 97.63 97.34 2,137s
Table 4: Performance on BAF
side. This experiment has shown that (Std)
finds a larger amount of those alignments while
making numerous wrong conjectures. On the
other hand, (CA) finds less 1-to-many/many-to-1
correspondences but makes only few incorrect
hypotheses. Hence, F1 is about 5% better for
(CA).
MT evaluation We also measured the impact
of 1-to-N/N-to-1 alignments (which are not ex-
tracted by Moore) on MT. We used standard set-
tings of the Moses toolkit, and the Europarl de-
vtest2006 set as our test set. We ran MERT sep-
arately for each system. System (s1) was trained
just on the 1-to-1 alignments extracted from the
Europarl v3 corpus by our system while system
(s2) was trained with all correspondences found.
(s1) obtains a BLEU score of 0.2670 while (s2)
obtains a BLEU score of 0.2703. Application of
the pairwise bootstrap test (Koehn, 2004) shows
that (s2) is significantly better than (s1).
5 Conclusion
We have addressed the problem of unsupervised
and language-pair independent alignment of sym-
metrical and asymmetrical parallel corpora. We
have developed a novel approach which is fast
and allows us to achieve high accuracy in terms
of F1 for the alignment of bilingual corpora.
Our method achieved high accuracy on symmet-
rical and asymmetrical parallel corpora, and we
have shown that the 1-to-N/N-to-1 alignments ex-
tracted by our approach are useful. The source
code of the aligner and the test sets are available
at http://sourceforge.net/projects/gargantua .
6 Acknowledgements
The first author was partially supported by the
Hasler Stiftung19. Support for both authors was
provided by Deutsche Forschungsgemeinschaft
grants Models of Morphosyntax for Statistical
Machine Translation and SFB 732.
References
Brown, Peter F., Jennifer C. Lai, and Robert L. Mercer.
1991. Aligning sentences in parallel corpora. In In
Proceedings of 29th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 169?176.
Ceausu, Alexandru, Dan Stefanescu, and Dan Tufis.
2006. Acquis communautaire sentence alignment
using support vector machines. In LREC 2006:
Fifth International Conference on Language Re-
sources and Evaluation.
Chen, Stanley F. 1993. Aligning sentences in bilingual
corpora using lexical information. In Proceedings
of the 31st Annual Meeting of the Association for
Computational Linguistics, pages 9?16.
Deng, Yoggang, Shankar Kumar, and William Byrne.
2006. Segmentation and alignment of parallel text
for statistical machine translation. Natural Lan-
guage Engineering, 12:1?26.
Fattah, Mohamed Abdel, David B. Bracewell, Fuji
Ren, and Shingo Kuroiwa. 2007. Sentence align-
ment using p-nnt and gmm. Computer Speech and
Language, (21):594?608.
Gale, William A. and Kenneth Ward Church. 1993. A
program for aligning sentences in bilingual corpora.
Computational Linguistics, 19(1):75?102.
Gautam, Mrityunjay and R. M. K. Sinha. 2007. A
program for aligning sentences in bilingual cor-
pora. Proceedings of the International Conference
19http://www.haslerstiftung.ch/.
88
on Computing: Theory and Applications, ICCTA
?07, (1):480?484.
Haruno, M. and T. Yamazaki. 1996. High-
performance bilingual text alignment using statisti-
cal and dictionary information. In Proceedings of
ACL ?96, pages 131?138.
Koehn, Philipp. 2004. Statistical significance tests for
machine translation evaluation. In Lin, Dekang and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July. Association
for Computational Linguistics.
Ma, Xiaoyi. 2006. Champollion: A robust paral-
lel text sentence aligner. In LREC 2006: Fifth In-
ternational Conference on Language Resources and
Evaluation.
Melamed, I. Dan. 2000. Models of translational
equivalence among words. Computational Linguis-
tics, 26:221?249.
Moore, Robert. 2002. Fast and accurate sentence
alignment of bilingual corpora. In In Proceedings
of 5th Conference of the Association for Machine
Translation in the Americas, pages 135?244.
Simard, Michel and Pierre Plamondon. 1998. Bilin-
gual sentence alignment: Balancing robustness and
accuracy. Machine Translation, 13(1):59?80.
Simard, Michel. 1998. The baf: A corpus of english-
french bitext. In Proceedings of LREC 98, Granada,
Spain.
Wu, Dekai. 1994. Aligning a parallel English-Chinese
corpus statistically with lexical criteria. In In Pro-
ceedings of the 32nd Annual Conference of the
Association for Computational Linguistics, 80?87,
Las, pages 80?87.
89
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 421?432, Dublin, Ireland, August 23-29 2014.
Investigating the Usefulness of Generalized Word Representations in SMT
Nadir Durrani
University of Edinburgh
dnadir@inf.ed.ac.uk
Helmut Schmid Alexander Fraser
Ludwig Maximilian University Munich
fraser,schmid@cis.uni-muenchen.de
Philipp Koehn
University of Edinburgh
pkoehn@inf.ed.ac.uk
Abstract
We investigate the use of generalized representations (POS, morphological analysis and word
clusters) in phrase-based models and the N-gram-based Operation Sequence Model (OSM). Our
integration enables these models to learn richer lexical and reordering patterns, consider wider
contextual information and generalize better in sparse data conditions. When interpolating gen-
eralized OSM models on the standard IWSLT and WMT tasks we observed improvements of up
to +1.35 on the English-to-German task and +0.63 for the German-to-English task. Using auto-
matically generated word classes in standard phrase-based models and the OSM models yields
an average improvement of +0.80 across 8 language pairs on the IWSLT shared task.
1 Introduction
The increasing availability of digital text has galvanized the use of empirical methods in many fields
including Machine Translation. Given bilingual text, it is now possible to automatically learn translation
rules that required years of effort previously. Bilingual data, however, is abundantly available for only a
handful of language pairs. The problem of reliably estimating statistical models for translation becomes
more of a challenge under sparse data conditions especially when translating into morphologically rich
or syntactically divergent languages. The former becomes challenging due to lexical sparsity and the
latter suffers from sparsity in learning underlying reordering patterns. The last decade of research in
Statistical Machine Translation has witnessed many attempts to integrate linguistic analysis into SMT
models, to address the challenges of (i) translating into morphologically rich language languages, (ii)
modeling syntactic divergence across languages for better generalization in sparse data conditions.
The integration of the Operation Sequence Model into phrase-based paradigm (Durrani et al., 2013a;
Durrani et al., 2013b) improved the reordering capability and addressed the problem of the phrasal inde-
pendence assumption in the phrase-based models. The OSM model integrates translation and reordering
into a single generative story. By jointly considering translation and reordering context across phrasal
boundaries, the OSM model considers much richer conditioning than phrasal translation and lexicalized
reordering models. However, due to data sparsity the model often falls back to very small context sizes.
We address this problem by learning operation sequences over generalized representations such as POS
and Morph tags. This enables us to learn richer translation and reordering patterns that can general-
ize better in sparse data conditions. The model benefits from wider contextual information as we show
empirically in our results.
We investigate two methods to combine generalized OSM models with the lexically driven OSM
model and experimented on German-English translation tasks. Our best system that uses a linear combi-
nation of different OSM models gives significant improvements over a competitive baseline system. An
improvement of up to +1.35 was observed on the English-to-German and up to +0.63 BLEU points on
the German-to-English task over a factored augmented baseline system (Koehn and Hoang, 2007).
POS taggers and morphological analyzers, however, are not available for many resource poor lan-
guages. In the second half of the paper we investigate whether annotating the data with automatic word
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
421
clusters helps improve the performance. Word clustering is similar to POS-tagging/Morphological anno-
tation except that it also captures interesting syntactic and lexical semantics, for example countries and
languages are grouped in separate clusters, animate objects are differentiated from inanimate objects,
colors are grouped in a separate cluster etc. Word clusters, however, deterministically map each word
type to a unique
1
cluster, unlike POS/Morph tagging, and therefore might be less useful for disambigua-
tion. We use the mkcls utility in GIZA (Och and Ney, 2003) to cluster source and target vocabularies
into classes and will therefore refer to automatic classes as Och clusters/classes in this paper.
We first use Och classes as an additional factor in phrase-based translation model, along with a target
LM model over cluster-ids to improve the baseline system. We then additionally use the OSM model
over cluster-ids. Our experiments include translation from English to Dutch, French, Italian, Polish,
Portuguese, Russian, Spanish, Slovenian and Turkish on IWSLT shared task data. Our results show an
average improvement of +0.80, ranging from +0.41 to +2.02. Compared to the improved baseline system
obtained by using Och classes as a factor in phrase-based translation models, adding an OSM model over
cluster-ids improved performance in four (French, Spanish, Dutch and Slovenian) out of eight cases. In
other cases performance stayed constant or dropped slightly. We also used POS annotations for three
tasks, namely translating from English into French, Spanish and Dutch to compare the performance of
the two different kinds of generalizations. Surprisingly, using Och classes always performed better than
using POS annotations. The rest of the paper is organized as follows. Section 2 gives an account on
related work. Section 3 discusses the factor-based OSM model. Section 4 presents the experimental
setup and the results. Section 5 concludes the paper.
2 Related Work
Previous work on integrating linguistic knowledge into SMT models can be broken into two groups. The
first group focuses on using linguistic knowledge to improve reordering between syntactically different
languages. A second group focuses on translating into morphologically rich languages.
Initial efforts to use linguistic annotation focused on rearranging source sentences to be in the target
order. Xia and McCord (2004) proposed a method to automatically learn rewrite rules to preorder source
sentences. Collins et al. (2005) and Popovi?c and Ney (2006) proposed methods for reordering the source
using a small set of handcrafted rules. Crego and Mari?no (2007) use syntactic trees to derive rewrite
rules. Hoang and Koehn (2009) used POS tags to create templates for surface word translation to create
longer phrase translation. A whole new paradigm of using syntactic annotation to address long range
reorderings has emerged following Galley et al. (2006), Zollmann and Venugopal (2006), Chiang (2007)
etc. Crego and Yvon (2010) and Niehues et al. (2011) used a Tuple Sequence Model (TSM) over POS
tags in an N-gram-based search to improve mid-range reorderings. Our work is similar to them except
that OSM model is substantially different from the TSM model as it integrates both the translation and
reordering mechanisms into a combined model. Therefore both translation and reordering decisions can
benefit from richer generalized representations.
A second group of work addresses the problem of translating into morphologically richer languages.
The idea of translating to stems and then inflecting the stems in a separate step has been studied by
Toutanova et al. (2008), de Gispert and Mari?no (2008), Fraser et al. (2012), Chahuneau et al. (2013) and
others. Koehn and Hoang (2007) proposed to integrate different levels of linguistic information as factors
into the phrase-based translation model. Yeniterzi and Oflazer (2010) used source syntactic structures as
additional complex tag factors for English-to-Turkish phrase-based machine translation. Green and DeN-
ero (2012) proposed a target-side, class-based agreement model to handle morpho-syntactic agreement
errors when translating from English-to-Arabic. El Kholy and Habash (2012) tested three models to find
out which features are best handled by modeling them as a part of translation, and which ones are better
predicted through generation, also in the English-to-Arabic task. Several researchers attempted to use
word lattices to handle generalized representation (Dyer et al., 2008; Hardmeier et al., 2010; Wuebker
and Ney, 2012). Automatically clustering the training data into word classes in order to obtain smoother
1
We are referring to hard clustering here. Soft clustering is intractable as it requires a marginalization over all possible
classes when calculating the n-gram probabilities.
422
Figure 1: Operation Sequence Model ? Training Sentence with Generation and Test Sentences
distributions and better generalizations has been a widely known and applied technique in natural lan-
guage processing. Training based on word classes has been previously explored by various researchers.
Cherry (2013) addressed data sparsity in lexicalized reordering models by using sparse features based
on word classes. Other parallel attempts on using word-class models include Wuebker et al. (2013),
Chahuneau et al. (2013) and Bisazza and Monz (2014).
More recent research has started to set apart from the conventional maximum likelihood estimates
toward neural network-based models that use continuous space representation (Schwenk, 2012; Le et al.,
2012; Hu et al., 2014; Gao et al., 2014). Although these methods have achieved impressive improve-
ments, traditional models continue to dominate the field due to their simplicity and low computational
complexity. How much of the improvement will be retained when scaling these models to all available
data instead of a limited amount will be interesting.
3 Operation Sequence Model
The Operation Sequence Model (Durrani et al., 2011) is an instance of the N-gram based SMT framework
(Casacuberta and Vidal, 2004; Mari?no et al., 2006). It represents the translation process through a
sequence of operations. An operation can be to simultaneously generate source or target words or to
perform reordering. Reordering is carried out through jump and gap operations. The model is different
from its ancestors in that it strongly integrates translation and reordering into a single generative story in
which translation decisions can influence and get impacted by the reordering decisions and vice versa.
Given a bilingual sentence pair < F,E > and its alignment A, a sequence of operations o
1
, o
2
. . . , o
J
is generated deterministically through a conversion algorithm. The model is learned by learning Markov
chains over these sequences and is formally defined as:
p
osm
(F,E,A) =
J
?
j=1
p(o
j
|o
j?n+1
, ..., o
j?1
)
Figure 1 shows an example of an aligned bilingual sentence pair and the corresponding operation se-
quence used to generate it. There is a 1-1 correspondence between a sentence pair and its operation
sequence. We thus get a unique sequence for every bilingual sentence pair given the alignment.
3.1 Motivation
Due to data sparsity it is impossible to observe all possible reordering patterns with all possible lexical
choices in translation operations. The lexically driven OSM model therefore often backs off to very
small context sizes. Coming back to the training example in Figure 1. The useful reordering pattern
423
learned through this example is:
Ich kann umstellen? I can rearrange
which is memorized through the operation sequence:
Generate(Ich, I) ? Generate(kann, can) ? Insert Gap ? Generate(umstellen, rearrange)
It can generalize to the test sentence shown in Figure 1(a). However, it fails to generalize to the sentences
in Figure 1(b) and (c) although the underlying reordering pattern is the same. The second part of the
German verb complex usually appears at the end of a clause or a sentence and needs to be moved in order
to produce the correct English word order. However, due to data sparsity such a combination of lexical
decisions and reordering decisions may not be observed during training. The model would therefore fail
to generalize in such circumstances. This problem can be addressed by learning a generalized form of
the same reordering rule. By annotating the corpus with word classes such as POS tags, we obtain the
reordering pattern:
PPER VMFIN VVINF? PP MD VB
memorized through the operation sequence:
Generate (PPER,PP) ? Generate (VMFIN,MD) ? Insert Gap ? Generate (VVINF,VB)
This rule generalizes to all the test sentences in Figure 1. Since the OSM model strongly couples
translation and reordering, the probability of each translation or reordering operation depends on the
n previous translation/reordering decisions. The generalization of the model by replacing words with
POS tags allows the model to consider a wider syntactic context, thus improving lexical decisions and
the reordering capability of the model. Using different kinds of word classes, we can also control the
type of abstraction. Using lemmas for example, we can map different forms of the verb ?k?onnen ? can?
(kann, kannst, konnte) to a single class. Och clusters can provide different levels of granularity.
3.2 Models
Given that we can learn OSM models over different word representations, the question then is how
to combine the lexically driven OSM model with an OSM model based on a generalized word repre-
sentation. The simplest approach is to treat each OSM model as a separate feature in the log-linear
framework, thus summing up the weighted log probabilities. The effect of this is similar to an And
operation. A translation is considered good if both, the word-based OSM and the POS-based OSM
models indicate that it is a good translation. However, an Or operation might be more desirable in
some scenarios. The operation Generate (trotz, in spite of) should be ranked high although the POS-
based operation Generate(APPR, IN IN IN) is improbable. Similarly, the generalized operation sequence:
Insert Gap ? Generate (ADJ, JJ) ? Jump Back ? Generate (NOM, NN)
that captures the swapping of noun and adjective in French-English, should be ranked higher
even though noir (black) never appeared after cheval (horse) during training and the sequence:
Insert Gap ? Generate (noir, black) ? Jump Back ? Generate (cheval , horse)
is never observed. Instead of using both the models, a single model that could switch between
different generalized OSMs during translation and choose the one which gives the best prediction
in each situation, can be used. In order to achieve this effect, we formulated a second model that
interpolates the lexically driven OSM model with its generalized variants. However, we can only
424
interpolate two models that predict the same representation. The lexically driven OSM predicts the
surface forms whereas the POS-based OSM predicts POS translations. To make the two comparable,
we multiply the POS-based OSM probability with the probability of the lexical operation given the POS
operation. More specifically the probability of the generalized model gm can be defined as:
p
gm
(o
j
|o
j?1
j?n+1
) = p
osm
pos
(o
?
j
|o
?
j?1
j?n+1
) p(o
j
|o
?
j
) (1)
where p
osm
pos
is the operation sequence model learned over POS tags and p(o
j
|o
?
j
) is the probability of
the lexical operation given the POS-based operation. It is 1 for all reordering operations. We assume here
that for each lexical operation o
j
a corresponding POS-based operation o
?
j
is uniquely determined. With
p
osm
sur
= p
osm
sur
(o
j
|o
j?1
j?n+1
) (lexically driven OSM model) and p
gm
= p
gm
(o
j
|o
j?1
j?n+1
) (generalized
OSM model as described above), the overall probability of the new model p
osm
is defined as:
p
osm
= ?p
osm
sur
+ (1? ?)p
gm
(2)
Such an interpolation is expensive in the discriminative training. It would require a sub-tuning routine
inside of tuning, a main loop to train all the features including the OSM model and an inner loop to
distribute the weight assigned to OSM model among lexically driven and POS-based OSM models. We
therefore just take the larger one of the two model values and add a POS-based translation penalty ?. The
value of this penalty is the number of times that the POS-based operation was chosen when translating
a sentence. This penalty acts similarly as the prior ? above. Using this formulation, the model could
therefore be redefined as:
p
osm
=
{
p
osm
sur
if p
osm
sur
? e
?
p
gm
e
?
p
gm
otherwise
(3)
where ? is the weight for the POS driven translation penalty ?. This allows the optimizer to control
whether it prefers the lexically driven or the POS-driven OSM model. By setting a very low weight ?
the optimizer can force the translator to always choose lexically driven OSM. This formulation can be
extended to multiple generalized OSM models based on e.g. POS tags, morphological tags, or word
clusters. Equation 2 can be rewritten as follows:
p
osm
= ?
1
p
osm
sur
+
n
?
i=2
?
i
p
gm
i
(4)
with
?
n
i=1
?
i
= 1 and p
gm
i
defined analogous to Equation 1.
Setting p
gm
1
= p
osm
sur
and ?
1
= 0, we can again simplify Equation 4 by taking the maximum to:
p
osm
=
n
max
i=1
e
?
i
p
gm
i
(5)
We use a translation penalty ?
i
for each generalized model and tune its weight ?
i
along with the weights
of other features. We will refer to this model as Model
or
in this paper and the commonly used log-
linear interpolation of the features as Model
and
. The intuition behind Model
or
is that we back-off
to generalized representations only when the lexically driven model doesn?t provide enough contextual
evidence. The downside of this approach, however, is that unlike Model
and
, it cannot distribute weights
over multiple features and solely relies on a single model.
4 Evaluation
Data: We ran experiments with data made available for the translation task of the IWSLT-13 (Cettolo et
al., 2013): International Workshop on Spoken Language Translation
2
and WMT-13 (Bojar et al., 2013):
Eighth Workshop on Statistical Machine Translation.
3
The sizes of bitext used for the estimation of
translation and monolingual language models are reported in Table 1.
We used LoPar (Schmid, 2000) to obtain morphological analysis and POS annotation of German and
MXPOST (Ratnaparkhi, 1998), a maximum entropy model for English POS tags. For other language
pairs we used TreeTagger (Schmid, 1994).
2
http://www.iwslt2013.org/
3
http://www.statmt.org/wmt13/
425
Pair Parallel Monolingual Pair Parallel Monolingual Pair Parallel Monolingual
de?en ?4.6 M ?287.3 M en?de ?4.6 M ?59.5 M en?fr ?5.5 M ?69 M
en?es ?4.1 M ?59.6 M en?nl ?2.1 M ?21.7 M en?ru ?1.15 M ?21 M
en?pt ?1.0 M ?2.3 M en?pl ?0.77 M ?0.8 M en?sl ?0.63 M ?0.65 M
en?tr ?0.13 M ?0.14 M
Table 1: Number of Sentences (in Millions) used for Training
Model iwslt
10
wmt
13
iwslt
10
wmt
13
English-to-German German-to-English
Baseline 23.56 20.38 31.46 27.27
M
and
(pos,pos)
23.93?+0.37 20.61 ?+0.23 31.91?+0.45 27.55 ?+0.28
M
and
(pos,morph)
24.62?+1.06 20.88?+0.50 32.09?+0.63 27.62?+0.35
M
and
(all)
24.91?+1.35 20.93?+0.55 32.00?+0.54 27.71?+0.44
M
or
(pos,pos)
23.61 ?+0.05 20.24 ?-0.14 31.55 ?+0.09 27.32 ?+0.05
M
or
(pos,morph)
23.83 ?+0.27 20.44 ?+0.08 31.58 ?+0.12 27.20 ?-0.07
M
or
(all)
23.88 ?+0.32 20.55 ?+0.17 31.40 ?-0.06 27.15 ?-0.12
Table 2: Evaluating Generalized OSM Models for German-English pairs ? Bold: Statistically Significant
(Koehn, 2004) w.r.t Baseline
Baseline System: We trained a Moses system (Koehn et al., 2007), replicating the settings described
in (Birch et al., 2013) developed for the 2013 Workshop on Spoken Language Translation. The features
included: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ align-
ments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011)
used at runtime, a lexically-driven 5-gram operation sequence model (Durrani et al., 2013b) with 4
additional supportive features: 2 gap-based penalties, 1 distance-based feature and 1 deletion penalty,
lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6,
100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), Cube Prun-
ing (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the
no-reordering-over-punctuation heuristic. We used the compact phrase table representation by Junczys-
Dowmunt (2012). For our German-to-English experiments, we used compound splitting (Koehn and
Knight, 2003). German-to-English and English-to-German baseline systems also used POS and mor-
phological target sequence models built on the in-domain subset of the parallel corpus using Kneser-Ney
smoothed 7-gram models and as additional factors in phrase translation models (Koehn and Hoang,
2007). We used an unsupervised transliteration model (Durrani et al., 2014) to transliterate OOV words
when translating into Russian.
Tuning and Test: The systems were tuned on the dev2010 dataset and evaluated on the test2010-2013
datasets made available for the IWSLT-13 workshop. We performed a secondary set of experiments
for German-English pairs using tuning and test sets made available for the WMT-13 workshop. We
concatenated the news-test sets 2008 and 2009 to obtain a large dev-set of 4576 sentences. Evaluation
was performed on the news-test set 2013 which contains 3000 sentences. Tuning was performed using
the k-best batch MIRA algorithm (Cherry and Foster, 2012) with at most 25 iterations. We use BLEU
(Papineni et al., 2002) as a metric to evaluate our results.
Results I ? Using Linguistic Annotation: We trained 5-gram OSM models over different representa-
tions and added these to the baseline system. First we evaluated Model
and
(M
and
) which uses a MIRA
tuned linear combination of different OSM models versus Model
or
(M
or
) which computes only one
OSM model but allows the generator to switch between different OSM models built on various gener-
alized forms. Table 2 shows results from running experiments on German-English pairs. We found that
the simpler model Model
and
outperforms Model
or
in all the experiments. Model
or
does not give
significant improvements over the baseline system and shows an occasional drop. This result is contrary
to the expectation formulated in Section 3.2. We speculate that the optimizer faces problems to train this
kind of model, because it cannot take into account that the selected OSM model can change when the
weight parameter is modified. It assumes that the feature stays constant. In our formulation the same
426
derivation can occur with different feature scores in different decoding runs and the optimizer is unable
to handle this. Our speculation is based on the observation of ?
?
, the weight of feature ? which allows
the translator to switch between different OSM models. The value of ?
?
was not stable across different
iterations and different experiments.
Model
and
consistently improves the baseline. Adding an OSM model over [pos, morph] (source:pos,
target:morph) combination gave the best results, giving a statistically significant gain of +1.06 on the
iwslt
10
test-set and +0.50 on the wmt
13
test-set. Using an OSM model over a [pos,pos] combination
also showed improvements, however, not as much as using morphological tags. Morphological tags pro-
vide richer information for disambiguation when translating into German. Note that the baseline system
also used a target sequence model over morphological tags. Nevertheless using an OSM [pos,morph]
model still gives significant improvements which shows that learning a joint model over source and tar-
get units is more fruitful than only considering target-side information. Using both the models together
gave best results for English-to-German giving a further improvement of +0.29 on the iwslt
10
task but
no real gain on the wmt
13
task. Using morphological tags also produced the best results for the German-
to-English pair, giving a statistically significant gain of +0.63 on iwslt
10
and +0.35 on wmt
13
. Using
both the models together did not give any further significant improvements. The results changed by
+0.10 and -0.09 on the wmt
13
and iwslt
10
test-sets respectively.
Results-II ? Using Och Classes: In our secondary experiments we tested the effect of using Och
clusters. The overall goal was to study whether using unsupervised word classes can serve the same
purpose as POS tags and to compare the two methods of annotating the data. We obtained Och clusters
using the mkcls utility (Och, 1999) in GIZA++ (Och and Ney, 2003). This is generally run during
the alignment process where data is divided into 50 classes to estimate IBM Model-4. Chahuneau et
al. (2013) found mapping data to 600 Och clusters useful, so we used this as well. We additionally
experimented with using 200 and 1000 classes. We integrated Och clusters as additional factors
4
when
training the phrase-translation models and used a monolingual n-gram model over cluster-ids built on the
target-side of the in-domain corpus. Then we added a 5-gram OSM model over cluster-ids. We replace
surface forms with their cluster-ids in source and target corpus and convert it to operation sequences,
that jointly generate source and target cluster-ids. We only used Model
and
for these experiments when
adding an OSM model over cluster-ids.
B
0
50 200 600 1000 POS 50 200 600 1000 POS
Target Sequence Model over Word Clusters Operation Sequence Model over Word Clusters
en? fr 33.17 33.30 33.40 33.05 33.05 33.14 33.76 33.74 33.58 33.75 33.03
en? es 34.14 34.33 34.58 34.46 33.96 33.91 34.73 34.62 34.60 34.55 34.35
en? nl 26.51 26.67 26.15 26.31 26.47 26.55 26.91 26.52 26.61 26.49 26.62
en? ru 13.12 13.34 13.51 13.53 13.97 ? 13.61 13.66 13.80 13.63 ?
en? sl 17.98 18.67 18.55 17.67 17.97 ? 18.64 18.91 18.17 17.98 ?
en? pt 30.80 31.62 32.21 32.40 32.44 ? 31.77 32.44 32.34 31.90 ?
en? pl 9.74 9.90 10.11 10.05 10.43 ? 10.06 10.19 10.24 10.14 ?
en? tr 7.18 7.43 7.45 7.50 7.50 ? 7.26 7.28 7.51 7.54 ?
Table 3: Evaluating Phrase-based and N-gram-based Translation Models over Och Clusters
Table 3 shows results from using models based on cluster-ids. The left side of the table evaluate the
use of adding a target sequence model over cluster-ids using a factored-based translation model. Results
improved consistently in all resource poor languages (pt, pl, tr) giving significant improvements in most
of the cases. Mixed results were obtained for the pairs with a reasonable amount of parallel data (fr,
es, nl), showing an occasional drop in performance. However, improvements can be found for all the
language pairs.
4
Note that adding cluster-ids in factored models alone has no impact in this scenario, as we are using hard clustering (each
word deterministically maps onto a unique cluster-id). In a joint source-target factored model which is what we are using, it
will result in an identical distribution as the baseline system.
427
In the right half of the table we tested whether additionally using an OSM model built over cluster-ids,
on top of a phrase-based system that uses cluster-ids as factor and target language model, improves the
performance any further. Consistent improvements were seen in Spanish and French. Better systems
were produced in the case of French, Spanish, Dutch and Slovenian. No improvements were observed
for Turkish and Portuguese whereas the performance got worse in Polish and Russian.
Using 50 classes consistently improved the baseline. Different numbers of clusters provide different
levels of abstraction and granularity. We also tried using OSM models over different numbers of clus-
ters simultaneously for English-to-Spanish, English-to-French and English-to-Dutch pairs in an effort to
explore whether using different numbers of clusters to classify data provides different information. A
slight gain was observed for EN-ES as the best system improved from 34.73 to 34.95. No further gains
were observed for the other two pairs.
We also used POS annotation as a factor instead of Och clusters in French, Spanish and Dutch. See
the POS columns of Table 3. Using POS as an additional factor, did not improve over the baseline
performance. A significant drop was seen in the case of English-to-Spanish. Using a POS-based OSM
on top of the POS-based phrase-model did not help either except for Spanish where results got improved
by +0.44 over its phrase-based variant that used a POS factor. However, using Och clusters produced
better results in all three cases. We speculate that the reason for this result is that Och clusters are
more evenly distributed as compared to POS tags where the distribution is biased toward noun class
and secondly Och clusters are optimized for language modeling. Also each word is deterministically
mapped to a single class but can have multiple POS tags. The latter thus causes a sparser translation
model. Finally Table 4 shows the comparison of results on iwslt
11?13
by running baseline B
0
and best
systems B
x
in Tables 3.
iwslt
11
iwslt
12
iwslt
13
Avg
B
0
B
x
B
0
B
x
B
0
B
x
B
0
B
x
?
en? fr 39.84 40.63 40.50 41.24 ? ? 40.24 40.94 +0.70
en? es 32.89 33.24 26.45 26.81 34.01 34.73 31.12 31.60 +0.48
en? nl 30.01 30.31 26.40 26.72 24.96 25.57 27.12 27.53 +0.41
en? ru 14.93 15.91 13.01 13.53 15.65 16.4 14.53 15.28 +0.75
en? sl ? ? 11.34 12.40 12.85 13.73 12.09 13.10 +1.01
en? pt 31.61 33.62 33.24 34.91 30.83 33.24 31.89 33.92 +2.02
en? pl 12.73 13.13 9.52 10.50 11.30 11.54 11.18 11.72 +0.53
en? tr 7.01 7.42 6.99 7.43 6.21 6.84 6.74 7.23 +0.49
Avg 24.15 24.89 20.93 21.69 19.40 20.29 21.49 22.29 +0.80
Table 4: Evaluating on Test Sets iwslt
11?13
? B
0
= Baseline System, B
x
= Best Systems in Tables 2
Analysis: In a post-evaluation analysis we confirmed whether using generalized OSM models actually
consider a wider contextual window than its lexically driven variant. The graph shown in Figure 2 shows
average context size considered (on top of each set of bars) and percentages of 1-5 gram matches by
different OSM models. The results show that the probability of an operation is conditioned on less than a
trigram in the OSM model over surface forms. In comparison OSM models over POS, morph or cluster-
ids consider a window of roughly 4 previous operations thus considering more contextual information.
The percentage of 5-gram matches increases from 15.5% to 59.2% using POS-based OSM model and
up to 45.6% in morph-based OSM model, the number of unigram matches are decreased from 8.30% to
less than 1% in both the models. Similar observation is made for the OSM models over clusters where
5-gram matches improve from 12% to 30% on average, showing the ability of the generalized models to
use richer conditioning thus improving the translation quality.
We also analyzed what kind of words are clustered together using Och classes and found that clusters
capture both syntax and lexical semantics. Figure 2 (b) shows several useful clusters to exhibit this. We
also saw negative examples where words from different classes are clustered together. ?Boy?, ?Girl? and
?Man? for example were clustered into a single class but ?Woman? in another. Similarly ?Grey? and
?Orange? were grouped together with animated objects.
428
Figure 2: (a) Average Size of N-grams Used in Different OSM Models and Percentages of 1-5 Gram
Matches in Three Language Pairs (b) Different Word Clusters using 50 Classes
5 Conclusion
In this paper we investigated the usefulness of integrating word classes in phrase-based models and
Operation Sequence N-gram models. We explored two models of interpolating generalized OSM models
and tested variations on the standard IWSLT and WMT tasks. Our results showed that the simpler more
commonly used method of integrating the models in the log-linear framework worked best. We showed
that by learning OSM models over generalized POS and morphological representations, we were able
to build richer models that outperformed state-of-the-art baseline systems. Statistically significant gains
of up to +1.35 and +0.63 were observed in English-to-German and German-to-English tasks. We also
made use of Och classes as additional factors in phrase translation and language models. These were
tested translating from English to 8 different languages which includes a mixture of morphologically
rich (French, Spanish and Russian, Dutch, and Turkish) and sparse data (Portuguese, Polish, Slovenian
and Turkish) languages. Our results show that using clusters was helpful in all of the cases. Using
the OSM model over word-clusters additionally improved the performance further. Our results show an
average improvement of +0.80, ranging from +0.41 to +2.02. Our EN-FR systems were ranked third (on
tst2013) and second (on tst2011-tst2012) in IWSLT-13 translation task following EU-Bridge (Freitag et
al., 2013) which used our output for system combination. The code to train class-based models has been
made available to the research community via the Moses toolkit. See Advanced Features
5
in the Moses
Decoder for details.
Acknowledgements
We would like to thank the anonymous reviewers for their helpful feedback and suggestions. The re-
search leading to these results has received funding from the European Union Seventh Framework Pro-
gramme (FP7/2007-2013) under grant agreements n
?
287658 (EU-Bridge) and n
?
287688 (MateCat).
Alexander Fraser was funded by Deutsche Forschungsgemeinschaft grant Models of Morphosyntax for
Statistical Machine Translation. Helmut Schmid was supported by Deutsche Forschungsgemeinschaft
grant SFB 732. This publication only reflects the authors? views.
References
Alexandra Birch, Nadir Durrani, and Philipp Koehn. 2013. Edinburgh SLT and MT System Description for the
IWSLT 2013 Evaluation. In Proceedings of the 10th International Workshop on Spoken Language Translation,
5
http://www.statmt.org/moses/?n=Moses.AdvancedFeatures
429
pages 40?48, Heidelberg, Germany, December.
Arianna Bisazza and Christof Monz. 2014. Class-Based Language Modeling for Translating into Morphologically
Rich Languages. In Proceedings of the 25th Annual Conference on Computational Linguistics (COLING),
Dublin, Ireland, August.
Ondrej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof
Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine
Translation. In Eighth Workshop on Statistical Machine Translation, WMT-2013, pages 1?44, Sofia, Bulgaria.
Francisco Casacuberta and Enrique Vidal. 2004. Machine Translation with Inferred Stochastic Finite-State Trans-
ducers. Computational Linguistics, 30:205?225.
Mauro Cettolo, Jan Niehues, Sebastian St?uker, Luisa Bentivogli, and Marcello Federico. 2013. Report on the 10th
IWSLT Evaluation Campaign. Proceedings of the International Workshop on Spoken Language Translation,
Heidelberg, Germany.
Victor Chahuneau, Eva Schlinger, Noah A. Smith, and Chris Dyer. 2013. Translating into Morphologically Rich
Languages with Synthetic Phrases. In Proceedings of the 2013 Conference on Empirical Methods in Natural
Language Processing.
Colin Cherry and George Foster. 2012. Batch Tuning Strategies for Statistical Machine Translation. In Proceed-
ings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, pages 427?436, Montr?eal, Canada, June. Association for Computational Lin-
guistics.
Colin Cherry. 2013. Improved Reordering for Phrase-Based Translation using Sparse Features. In Proceedings of
the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, pages 22?31, Atlanta, Georgia, June. Association for Computational Linguistics.
David Chiang. 2007. Hierarchical Phrase-Based Translation. Computational Linguistics, 33(2):201?228.
Michael Collins, Philipp Koehn, and Ivona Kucerova. 2005. Clause Restructuring for Statistical Machine Trans-
lation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL?05),
pages 531?540, Ann Arbor, MI.
Josep M. Crego and Jos?e B. Mari?no. 2007. Syntax-Enhanced N-gram-Based SMT. In Proceedings of the 11th
Machine Translation Summit, MT Summit XI, pages 111?118.
Josep M. Crego and Franc?ois Yvon. 2010. Improving Reordering with Linguistically Informed Bilingual N-
Grams. In Coling 2010: Posters, pages 197?205, Beijing, China, August. Coling 2010 Organizing Committee.
Adri`a de Gispert and Jos?e B. Mari?no. 2008. On the Impact of Morphology in English to Spanish statistical MT.
Speech Communication, 50(11-12):1034?1046.
Nadir Durrani, Helmut Schmid, and Alexander Fraser. 2011. A Joint Sequence Translation Model with Integrated
Reordering. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:
Human Language Technologies, pages 1045?1054, Portland, Oregon, USA, June.
Nadir Durrani, Alexander Fraser, and Helmut Schmid. 2013a. Model With Minimal Translation Units, But Decode
With Phrases. In The 2013 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, Atlanta, Georgia, USA, June. Association for Computational Lin-
guistics.
Nadir Durrani, Alexander Fraser, Helmut Schmid, Hieu Hoang, and Philipp Koehn. 2013b. Can Markov Models
Over Minimal Translation Units Help Phrase-Based SMT? In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics, Sofia, Bulgaria, August. Association for Computational Linguistics.
Nadir Durrani, Hassan Sajjad, Hieu Hoang, and Philipp Koehn. 2014. Integrating an Unsupervised Transliteration
Model into Statistical Machine Translation. In Proceedings of the 15th Conference of the European Chapter of
the ACL (EACL 2014), Gothenburg, Sweden, April. Association for Computational Linguistics.
Christopher Dyer, Smaranda Muresan, and Philip Resnik. 2008. Generalizing Word Lattice Translation. In
Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1012?
1020, Columbus, OH, USA. The Association for Computer Linguistics.
Ahmed El Kholy and Nizar Habash. 2012. Translate, Predict or Generate: Modeling Rich Morphology in Statis-
tical Machine Translation. volume 12.
430
Alexander Fraser, Marion Weller, Aoife Cahill, and Fabienne Cap. 2012. Modeling Inflection and Word-
Formation in SMT. In Proceedings of the 13th Conference of the European Chapter of the Association for
Computational Linguistics, pages 664?674, Avignon, France, April. Association for Computational Linguistics.
Markus Freitag, Stephan Peitz, Joern Wuebker, Hermann Ney, Nadir Durrani, Matthias Huck, Philipp Koehn,
Thanh-Le Ha, Jan Niehues, Mohammed Mediani, Teresa Herrmann, Alex Waibel, Nicola Bertoldi, Mauro Cet-
tolo, and Marcello Federico. 2013. EU-BRIDGE MT: Text Translation of Talks in the EU-BRIDGE Project. In
International Workshop on Spoken Language Translation, Heidelberg, Germany, December.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer.
2006. Scalable Inference and Training of Context-Rich Syntactic Translation Models. In Proceedings of
COLING-ACL, pages 961?968, Sydney, Australia. Association for Computational Linguistics.
Jianfeng Gao, Xiaodong He, Scott Wen-tau Yih, and Li Deng. 2014. Learning Continuous Phrase Representations
for Translation Modeling. In Proceedings of the Association for Computational Linguistics, Baltimore, MD,
USA, June.
Spence Green and John DeNero. 2012. A Class-Based Agreement Model for Generating Accurately Inflected
Translations. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 146?155, Jeju Island, Korea, July. Association for Computational Linguistics.
Christian Hardmeier, Arianna Bisazza, and Marcello Federico. 2010. FBK at WMT 2010: Word Lattices for Mor-
phological Reduction and Chunk-Based Reordering. In Proceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 88?92, Uppsala, Sweden, July. Association for Computational
Linguistics.
Eva Hasler, Barry Haddow, and Philipp Koehn. 2012. Sparse Lexicalised features and Topic Adaptation for
SMT. In Proceedings of the seventh International Workshop on Spoken Language Translation (IWSLT), pages
268?275.
Kenneth Heafield. 2011. KenLM: Faster and Smaller Language Model Queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages 187?197, Edinburgh, Scotland, United Kingdom, July.
Hieu Hoang and Philipp Koehn. 2009. Improving Mid-Range Re-Ordering Using Templates of Factors. In
Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), pages 372?379, Athens,
Greece, March. Association for Computational Linguistics.
Yuening Hu, Michael Auli, Qin Gao, and Jianfeng Gao. 2014. Minimum Translation Modeling with Recurrent
Neural Networks. In Proceedings of the 14th Conference of the European Chapter of the Association for Com-
putational Linguistics, pages 20?29, Gothenburg, Sweden, April. Association for Computational Linguistics.
Liang Huang and David Chiang. 2007. Forest Rescoring: Faster Decoding with Integrated Language Models.
In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 144?151,
Prague, Czech Republic, June. Association for Computational Linguistics.
Marcin Junczys-Dowmunt. 2012. Phrasal Rank-Encoding: Exploiting Phrase Redundancy and Translational
Relations for Phrase Table Compression. The Prague Bulletin of Mathematical Linguistics, 98:63?74.
Philipp Koehn and Hieu Hoang. 2007. Factored Translation Models. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Processing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 868?876, Prague, Czech Republic, June. Association for Computational Linguistics.
Philipp Koehn and Kevin Knight. 2003. Empirical methods for compound splitting. In Proceedings of the 10th
Conference of the European Chapter of the Association for Computational Linguistics (EACL), pages 187?193,
Morristown, NJ.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan
Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In ACL 2007 Demonstrations,
Prague, Czech Republic.
Philipp Koehn. 2004. Statistical Significance Tests for Machine Translation Evaluation. In Dekang Lin and Dekai
Wu, editors, Proceedings of EMNLP 2004, pages 388?395, Barcelona, Spain, July.
Shankar Kumar and William J. Byrne. 2004. Minimum Bayes-Risk Decoding for Statistical Machine Translation.
In HLT-NAACL, pages 169?176.
431
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon. 2012. Continuous Space Translation Models with Neural
Networks. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Com-
putational Linguistics: Human Language Technologies, pages 39?48, Montr?eal, Canada, June. Association for
Computational Linguistics.
Jos?e B. Mari?no, Rafael E. Banchs, Josep M. Crego, Adri`a de Gispert, Patrik Lambert, Jos?e A. R. Fonollosa, and
Marta R. Costa-juss`a. 2006. N-gram-Based Machine Translation. Computational Linguistics, 32(4):527?549.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and Alex Waibel. 2011. Wider Context by Using Bilingual Lan-
guage Models in Machine Translation. In Proceedings of the Sixth Workshop on Statistical Machine Translation,
pages 198?206, Edinburgh, Scotland, July. Association for Computational Linguistics.
Franz J. Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Franz J. Och. 1999. An Efficient Method for Determining Bilingual Word Classes. In Processings of EACL, pages
71?76, Bergen, Norway.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: A Method for Automatic Eval-
uation of Machine Translation. In Proceedings of the 40th Annual Meeting on Association for Computational
Linguistics, ACL ?02, pages 311?318, Morristown, NJ, USA.
Maja Popovi?c and Hermann Ney. 2006. POS-based Word Reorderings for Statistical Machine Translation. In
International Conference on Language Resources and Evaluation, pages 1278?1283, Genoa, Italy.
Adwait Ratnaparkhi. 1998. Maximum Entropy Models for Natural Language Ambiguity Resolution. Ph.D. thesis,
University of Pennsylvania, Philadelphia, PA.
Helmut Schmid. 1994. Probabilistic Part-of-Speech Tagging Using Decision Trees. In International Conference
on New Methods in Language Processing, pages 44?49, Manchester, UK.
Helmut Schmid. 2000. Lopar: Design and implementation. Bericht des sonderforschungsbereiches ?sprachtheo-
retische grundlagen fr die computerlinguistik?, Institute for Computational Linguistics, University of Stuttgart.
Holger Schwenk. 2012. Continuous Space Translation Models for Phrase-Based Statistical Machine Translation.
In Proceedings of COLING 2012: Posters, pages 1071?1080, Mumbai, India, December. The COLING 2012
Organizing Committee.
Kristina Toutanova, Hisami Suzuki, and Achim Ruopp. 2008. Applying Morphology Generation Models to
Machine Translation. In Proceedings of ACL-08: HLT, pages 514?522, Columbus, Ohio, June. Association for
Computational Linguistics.
Joern Wuebker and Hermann Ney. 2012. Phrase Model Training for Statistical Machine Translation with Word
Lattices of Preprocessing Alternatives. In NAACL 2012 Seventh Workshop on Statistical Machine Translation,
pages 450?459, Montreal, Canada, June. Association for Computational Linguistics.
Joern Wuebker, Stephan Peitz, Felix Rietig, and Hermann Ney. 2013. Improving Statistical Machine Translation
with Word Class Models. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language
Processing, pages 1377?1381, Seattle, Washington, USA, October. Association for Computational Linguistics.
Fei Xia and Michael McCord. 2004. Improving a Statistical MT System with Automatically Learned Rewrite
Patterns. In Proceedings of Coling 2004, pages 508?514, Geneva, Switzerland, Aug 23?Aug 27. COLING.
Reyyan Yeniterzi and Kemal Oflazer. 2010. Syntax-to-Morphology Mapping in Factored Phrase-Based Statistical
Machine Translation from English to Turkish. In Proceedings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 454?464, Uppsala, Sweden, July. Association for Computational Linguistics.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax Augmented Machine Translation via Chart Parsing.
In Proceedings on the Workshop on Statistical Machine Translation, pages 138?141, New York City, June.
Association for Computational Linguistics.
432
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 664?674,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Modeling Inflection and Word-Formation in SMT
Alexander Fraser? Marion Weller? Aoife Cahill? Fabienne Cap?
?Institut fu?r Maschinelle Sprachverarbeitung ?Educational Testing Service
Universita?t Stuttgart Princeton, NJ 08541
D?70174 Stuttgart, Germany USA
{fraser,wellermn,cap}@ims.uni-stuttgart.de acahill@ets.org
Abstract
The current state-of-the-art in statistical
machine translation (SMT) suffers from is-
sues of sparsity and inadequate modeling
power when translating into morphologi-
cally rich languages. We model both in-
flection and word-formation for the task
of translating into German. We translate
from English words to an underspecified
German representation and then use linear-
chain CRFs to predict the fully specified
German representation. We show that im-
proved modeling of inflection and word-
formation leads to improved SMT.
1 Introduction
Phrase-based statistical machine translation
(SMT) suffers from problems of data sparsity
with respect to inflection and word-formation
which are particularly strong when translating to
a morphologically rich target language, such as
German. We address the problem of inflection
by first translating to a stem-based representation,
and then using a second process to inflect these
stems. We study several models for doing
this, including: strongly lexicalized models,
unlexicalized models using linguistic features,
and models combining the strengths of both of
these approaches. We address the problem of
word-formation for compounds in German, by
translating from English into German word parts,
and then determining whether to merge these
parts to form compounds.
We make the following new contributions: (i)
we introduce the first SMT system combining
inflection prediction with synthesis of portman-
teaus and compounds. (ii) For inflection, we com-
pare the mostly unlexicalized prediction of lin-
guistic features (with a subsequent surface form
generation step) versus the direct prediction of
surface forms, and show that both approaches
have complementary strengths. (iii) We com-
bine the advantages of the prediction of linguis-
tic features with the prediction of surface forms.
We implement this in a CRF framework which
improves on a standard phrase-based SMT base-
line. (iv) We develop separate (but related) pro-
cedures for inflection prediction and dealing with
word-formation (compounds and portmanteaus),
in contrast with most previous work which usu-
ally either approaches both problems as inflec-
tional problems, or approaches both problems as
word-formation problems.
We evaluate on the end-to-end SMT task of
translating from English to German of the 2009
ACL workshop on SMT. We achieve BLEU score
increases on both the test set and the blind test set.
2 Overview of the translation process for
inflection prediction
The work we describe is focused on generaliz-
ing phrase-based statistical machine translation to
better model German NPs and PPs. We particu-
larly want to ensure that we can generate novel
German NPs, where what we mean by novel is
that the (inflected) realization is not present in the
parallel German training data used to build the
SMT system, and hence cannot be produced by
our baseline (a standard phrase-based SMT sys-
tem). We first present our system for dealing with
the difficult problem of inflection in German, in-
cluding the inflection-dependent phenomenon of
portmanteaus. Later, after performing an exten-
sive analysis of this system, we will extend it
664
to model compounds, a highly productive phe-
nomenon in German (see Section 8).
The key linguistic knowledge sources that we
use are morphological analysis and generation of
German based on SMOR, a morphological ana-
lyzer/generator of German (Schmid et al 2004)
and the BitPar parser, which is a state-of-the-art
parser of German (Schmid, 2004).
2.1 Issues of inflection prediction
In order to ensure coherent German NPs, we
model linguistic features of each word in an NP.
We model case, gender, and number agreement
and whether or not the word is in the scope of
a determiner (such as a definite article), which
we label in-weak-context (this linguistic feature
is necessary to determine the type of inflection of
adjectives and other words: strong, weak, mixed).
This is a diverse group of features. The number
of a German noun can often be determined given
only the English source word. The gender of a
German noun is innate and often difficult to deter-
mine given only the English source word. Case
is a function of the slot in the subcategorization
frame of the verb (or preposition). There is agree-
ment in all of these features in an NP. For instance
the number of an article or adjective is determined
by the head noun, while the type of inflection of an
adjective is determined by the choice of article.
We can have a large number of surface forms.
For instance, English blue can be translated as
German blau, blaue, blauer, blaues, blauen. We
predict which form is correct given the context.
Our system can generate forms not seen in the
training data. We follow a two-step process: in
step-1 we translate to blau (the stem), in step-2 we
predict features and generate the inflected form.1
2.2 Procedure
We begin building an SMT system by parsing the
German training data with BitPar. We then extract
morphological features from the parse. Next, we
lookup the surface forms in the SMOR morpholog-
ical analyzer. We use the morphological features
in the parse to disambiguate the set of possible
SMOR analyses. Finally, we output the ?stems?
of the German text, with the addition of markup
taken from the parse (discussed in Section 2.3).
1E.g., case=nominative, gender=masculine, num-
ber=singular, in-weak-context=true; inflected: blaue.
We then build a standard Moses system trans-
lating from English to German stems. We obtain
a sequence of stems and POS2 from this system,
and then predict the correct inflection using a se-
quence model. Finally we generate surface forms.
2.3 German Stem Markup
The translation process consists of two major
steps. The first step is translation of English
words to German stems, which are enriched with
some inflectional markup. The second step is
the full inflection of these stems (plus markup)
to obtain the final sequence of inflected words.
The purpose of the additional German inflectional
markup is to strongly improve prediction of in-
flection in the second step through the addition of
markup to the stems in the first step.
In general, all features to be predicted are
stripped from the stemmed representation because
they are subject to agreement restrictions of a
noun or prepositional phrase (such as case of
nouns or all features of adjectives). However, we
need to keep all morphological features that are
not dependent on, and thus not predictable from,
the (German) context. They will serve as known
input for the inflection prediction model. We now
describe this markup in detail.
Nouns are marked with gender and number: we
consider the gender of a noun as part of its stem,
whereas number is a feature which we can obtain
from English nouns.
Personal pronouns have number and gender an-
notation, and are additionally marked with nom-
inative and not-nominative, because English pro-
nouns are marked for this (except for you).
Prepositions are marked with the case their ob-
ject takes: this moves some of the difficulty in pre-
dicting case from the inflection prediction step to
the stem translation step. Since the choice of case
in a PP is often determined by the PP?s meaning
(and there are often different meanings possible
given different case choices), it seems reasonable
to make this decision during stem translation.
Verbs are represented using their inflected surface
form. Having access to inflected verb forms has a
positive influence on case prediction in the second
2We use an additional target factor to obtain the coarse
POS for each stem, applying a 7-gram POS model. Koehn
and Hoang (2007) showed that the use of a POS factor only
results in negligible BLEU improvements, but we need ac-
cess to the POS in our inflection prediction models.
665
input decoder output inflected merged
in
in<APPR><Dat> in
im
die<+ART><Def> dem
contrast Gegensatz<+NN><Masc><Sg>Gegensatz Gegensatz
to zu<APPR><Dat> zu
zur
the die<+ART><Def> der
animated lebhaft<+ADJ><Pos> lebhaften lebhaften
debate Debatte<+NN><Fem><Sg> Debatte Debatte
Table 1: Re-merging of prepositions and articles after
inflection to form portmanteaus, in dem means in the.
step through subject-verb agreement.
Articles are reduced to their stems (the stem itself
makes clear the definite or indefinite distinction,
but lemmatizing involves removing markings of
case, gender and number features).
Other words are also represented by their stems
(except for words not covered by SMOR, where
surface forms are used instead).
3 Portmanteaus
Portmanteaus are a word-formation phenomenon
dependent on inflection. As we have discussed,
standard phrase-based systems have problems
with picking a definite article with the correct
case, gender and number (typically due to spar-
sity in the language model, e.g., a noun which
was never before seen in dative case will often
not receive the correct article). In German, port-
manteaus increase this sparsity further, as they
are compounds of prepositions and articles which
must agree with a noun.
We adopt the linguistically strict definition of
the term portmanteau: the merging of two func-
tion words.3 We treat this phenomena by split-
ting the component parts during training and re-
merging during generation. Specifically for
German, this requires splitting the words which
have German POS tag APPRART into an APPR
(preposition) and an ART (article). Merging is re-
stricted, the article must be definite, singular4 and
the preposition can only take accusative or dative
case. Some prepositions allow for merging with
an article only for certain noun genders, for exam-
ple the preposition inDative is only merged with
the following article if the following noun is of
masculine or neuter gender. The definite article
3Some examples are: zum (to the) = zu (to) + dem (the)
[German], du (from the) = de (from) + le (the) [French] or al
(to the) = a (to) + el (the) [Spanish].
4This is the reason for which the preposition + article in
Table 2 remain unmerged.
must be inflected before making a decision about
whether to merge a preposition and the article into
a portmanteau. See Table 1 for examples.
4 Models for Inflection Prediction
We present 5 procedures for inflectional predic-
tion using supervised sequence models. The first
two procedures use simple N-gram models over
fully inflected surface forms.
1. Surface with no features is presented with an
underspecified input (a sequence of stems), and
returns the most likely inflected sequence.
2. Surface with case, number, gender is a hybrid
system giving the surface model access to linguis-
tic features. In this system prepositions have addi-
tionally been labeled with the case they mark (in
both the underspecified input and the fully spec-
ified output the sequence model is built on) and
gender and number markup is also available.
The rest of the procedures predict morpholog-
ical features (which are input to a morphological
generator) rather than surface words. We have de-
veloped a two-stage process for predicting fully
inflected surface forms. The first stage takes a
stem and predicts morphological features for that
stem, based on the surrounding context. The aim
of the first stage is to take a stem and predict
four morphological features: case, gender, num-
ber and type of inflection. We experiment with
a number of models for doing this. The sec-
ond stage takes the stems marked with morpho-
logical features (predicted in the first stage) and
uses a morphological generator to generate the
full surface form. For the second stage, a modified
version of SMOR (Schmid et al 2004) is used,
which, given a stem annotated with morphologi-
cal features, generates exactly one surface form.
We now introduce our first linguistic feature
prediction systems, which we call joint sequence
models (JSMs). These are standard language
models, where the ?word? tokens are not repre-
sented as surface forms, but instead using POS
and features. In testing, we supply the input as a
sequence in underspecified form, where some of
the features are specified in the stem markup (for
instance, POS=Noun, gender=masculine, num-
ber=plural), and then use Viterbi search to find the
most probable fully specified form (for instance,
POS=Noun, gender=masculine, number=plural,
666
output decoder input prediction output prediction inflected forms gloss
haben<VAFIN> haben-V haben-V haben have
Zugang<+NN><Masc><Sg> NN-Sg-Masc NN-Masc.Acc.Sg.in-weak-context=false Zugang access
zu<APPR><Dat> APPR-zu-Dat APPR-zu-Dat zu to
die<+ART><Def> ART-in-weak-context=true ART-Neut.Dat.Pl.in-weak-context=true den the
betreffend<+ADJ><Pos> ADJA ADJA-Neut.Dat.Pl.in-weak-context=true betreffenden respective
Land<+NN><Neut><Pl> NN-Pl-Neut NN-Neut.Dat.Pl.in-weak-context=true La?ndern countries
Table 2: Overview: inflection prediction steps using a single joint sequence model. All words except verbs and
prepositions are replaced by their POS tags in the input. Verbs are inflected in the input (?haben?, meaning
?have? as in ?they have?, in the example). Prepositions are lexicalized (?zu? in the example) and indicate which
case value they mark (?Dat?, i.e., Dative in the example).
case=nominative, in-weak-context=true).5
3. Single joint sequence model on features. We
illustrate the different stages of the inflection pre-
diction when using a joint sequence model. The
stemmed input sequence (cf. Section 2.3) contains
several features that will be part of the input to
the inflection prediction. With the exception of
verbs and prepositions, the representation for fea-
ture prediction is based on POS-tags.
As gender and number are given by the heads
of noun phrases and prepositional phrases, and
the expected type of inflection is set by articles,
the model has sufficient information to compute
values for these features and there is no need to
know the actual words. In contrast, the prediction
of case is more difficult as it largely depends on
the content of the sentence (e.g. which phrase is
object, which phrase is subject). Assuming that
verbs and prepositions indicate subcategorization
frames, the model is provided crucial information
for the prediction of case by keeping verbs (recall
that verbs are produced by the stem translation
system in their inflected form) and prepositions
(the prepositions also have case markup) instead
of replacing them with their tags.
After having predicted a single label with val-
ues for all features, an inflected word form for the
stem and the features is generated. The prediction
steps are illustrated in Table 2.
4. Using four joint sequence models (one for
each linguistic feature). Here the four linguistic
feature values are predicted separately. The as-
sumption that the different linguistic features can
be predicted independently of one another is a rea-
5Joint sequence models are a particularly simple HMM.
Unlike the HMMs used for POS-tagging, an HMM as used
here only has a single emission possibility for each state,
with probability 1. The states in the HMM are the fully
specified representation. The emissions of the HMM are the
stems+markup (the underspecified representation).
sonable linguistic assumption to make given the
additional German markup that we use. By split-
ting the inflection prediction problem into 4 com-
ponent parts, we end up with 4 simpler models
which are less sensitive to data sparseness.
Each linguistic feature is modeled indepen-
dently (by a JSM) and has a different input rep-
resentation based on the previously described
markup. The input consists of a sequence of
coarse POS tags, and for those stems that are
marked up with the relevant feature, this feature
value. Finally, we combine the predicted fea-
tures together to produce the same final output as
the single joint sequence model, and then generate
each surface form using SMOR.
5. Using four CRFs (one for each linguistic fea-
ture). The sequence models already presented are
limited to the n-gram feature space, and those that
predict linguistic features are not strongly lexi-
calized. Toutanova et al(2008) uses an MEMM
which allows the integration of a wide variety of
feature functions. We also wanted to experiment
with additional feature functions, and so we train
4 separate linear chain CRF6 models on our data
(one for each linguistic feature we want to pre-
dict). We chose CRFs over MEMMs to avoid the
label bias problem (Lafferty et al 2001).
The CRF feature functions, for each German
word wi, are in Table 3. The common feature
functions are used in all models, while each of the
4 separate models (one for each linguistic feature)
includes the context of only that linguistic feature.
We use L1 regularization to eliminate irrelevant
feature functions, the regularization parameter is
optimized on held out data.
6We use the Wapiti Toolkit (Lavergne et al 2010) on 4
x 12-Core Opteron 6176 2.3 GHz with 256GB RAM to train
our CRF models. Training a single CRF model on our data
was not tractable, so we use one for each linguistic feature.
667
Common lemmawi?5...wi+5 , tagwi?7...wi+7
Case casewi?5...wi+5
Gender genderwi?5...wi+5
Number numberwi?5...wi+5
in-weak-context in-weak-contextwi?5...wi+5
Table 3: Feature functions used in CRF models (fea-
ture functions are binary indicators of the pattern).
5 Experimental Setup
To evaluate our end-to-end system, we perform
the well-studied task of news translation, us-
ing the Moses SMT package. We use the En-
glish/German data released for the 2009 ACL
Workshop on Machine Translation shared task on
translation.7 There are 82,740 parallel sentences
from news-commentary09.de-en and 1,418,115
parallel sentences from europarl-v4.de-en. The
monolingual data contains 9.8 M sentences.8
To build the baseline, the data was tokenized
using the Moses tokenizer and lowercased. We
use GIZA++ to generate alignments, by running
5 iterations of Model 1, 5 iterations of the HMM
Model, and 4 iterations of Model 4. We sym-
metrize using the ?grow-diag-final-and? heuris-
tic. Our Moses systems use default settings. The
LM uses the monolingual data and is trained as
a five-gram9 using the SRILM-Toolkit (Stolcke,
2002). We run MERT separately for each sys-
tem. The recaser used is the same for all systems.
It is the standard recaser supplied with Moses,
trained on all German training data. The dev set
is wmt-2009-a and the test set is wmt-2009-b, and
we report end-to-end case sensitive BLEU scores
against the unmodified reference SGML file. The
blind test set used is wmt-2009-blind (all lines).
In developing our inflection prediction sys-
tems (and making such decisions as n-gram order
used), we worked on the so-called ?clean data?
task, predicting the inflection on stemmed refer-
ence sentences (rather than MT output). We used
the 2000 sentence dev-2006 corpus for this task.
Our contrastive systems consist of two steps,
the first is a translation step using a similar
Moses system (except that the German side is
stemmed, with the markup indicated in Sec-
7http://www.statmt.org/wmt09/translation-task.html
8However, we reduced the monolingual data (only) by
retaining only one copy of each unique line, which resulted
in 7.55 M sentences.
9Add-1 smoothing for unigrams and Kneser-Ney
smoothing for higher order n-grams, pruning defaults.
tion 2.3), and the second is inflection prediction
as described previously in the paper. To derive
the stem+markup representation we first parse
the German training data and then produce the
stemmed representation. We then build a sys-
tem for translating from English words to Ger-
man stems (the stem+markup representation), on
the same data (so the German side of the parallel
data, and the German language modeling uses the
stem+markup representation). Likewise, MERT
is performed using references which are in the
stem+markup representation.
To train the inflection prediction systems, we
use the monolingual data. The basic surface form
model is trained on lowercased surface forms,
the hybrid surface form model with features is
trained on lowercased surface forms annotated
with markup. The linguistic feature prediction
systems are trained on the monolingual data pro-
cessed as described previously (see Table 2).
Our JSMs are trained using the SRILM Toolkit.
We use the SRILM disambig tool for predicting
inflection, which takes a ?map? that specifies the
set of fully specified representations that each un-
derspecified stem can map to. For surface form
models, it specifies the mapping from stems to
lowercased surface forms (or surface forms with
markup for the hybrid surface model).
6 Results for Inflection Prediction
We build two different kinds of translation sys-
tem, the baseline and the stem translation system
(where MERT is used to train the system to pro-
duce a stem+markup sequence which agrees with
the stemmed reference of the dev set). In this sec-
tion we present the end-to-end translation results
for the different inflection prediction models de-
fined in Section 4, see Table 4.
If we translate from English into a stemmed
German representation and then apply a unigram
stem-to-surface-form model to predict the surface
form, we achieve a BLEU score of 9.97 (line 2).
This is only presented for comparison.
The baseline10 is 14.16, line 1. We compare
this with a 5-gram sequence model11 that predicts
10This is a better case-sensitive score than the baselines
on wmt-2009-b in experiments by top-performers Edinburgh
and Karlsruhe at the shared task. We use Moses with default
settings.
11Note that we use a different set, the ?clean data? set, to
determine the choice of n-gram order, see Section 7. We use
668
surface forms without access to morphological
features, resulting in a BLEU score of 14.26. In-
troducing morphological features (case on prepo-
sitions, number and gender on nouns) increases
the BLEU score to 14.58, which is in the same
range as the single JSM system predicting all lin-
guistic features at once.
This result shows that the mostly unlexicalized
single JSM can produce competitive results with
direct surface form prediction, despite not having
access to a model of inflected forms, which is the
desired final output. This strongly suggests that
the prediction of morphological features can be
used to achieve additional generalization over di-
rect surface form prediction. When comparing the
simple direct surface form prediction (line 3) with
the hybrid system enriched with number, gender
and case (line 4), it becomes evident that feature
markup can also aid surface form prediction.
Since the single JSM has no access to lexical
information, we used a language model to score
different feature predictions: for each sentence of
the development set, the 100 best feature predic-
tions were inflected and scored with a language
model. We then optimized weights for the two
scores LM (language model on surface forms)
and FP (feature prediction, the score assigned by
the JSM). This method disprefers feature predic-
tions with a top FP-score if the inflected sen-
tence obtains a bad LM score and likewise dis-
favors low-ranked feature prediction with a high
LM score. The prediction of case is the most
difficult given no lexical information, thus scor-
ing different prediction possibilities on inflected
words is helpful. An example is when the case of
a noun phrase leads to an inflected phrase which
never occurs in the (inflected) language model
(e.g., case=genitive vs. case=other). Applying
this method to the single JSM leads to a negligible
improvement (14.53 vs. 14.56). Using the n-best
output of the stem translation system did not lead
to any improvement.
The comparison between different feature pre-
diction models is also illustrative. Performance
decreases somewhat when using individual joint
sequence models (one for each linguistic feature)
compared to one single model (14.29, line 6).
The framework using the individual CRFs for
a 5-gram for surface forms and a 4-gram for JSMs, and the
same smoothing (Kneser-Ney, add-1 for unigrams, default
pruning).
1 baseline 14.16
2 unigram surface (no features) 9.97
3 surface (no features) 14.26
4 surface (with case, number, gender features) 14.58
5 1 JSM morphological features 14.53
6 4 JSMs morphological features 14.29
7 4 CRFs morphological features, lexical information 14.72
Table 4: BLEU scores (detokenized, case sensitive) on
the development test set wmt-2009-b
each linguistic feature performs best (14.72, line
7). The CRF framework combines the advantages
of surface form prediction and linguistic feature
prediction by using feature functions that effec-
tively cover the feature function spaces used by
both forms of prediction. The performance of the
CRF models results in a statistically significant
improvement12 (p < 0.05) over the baseline. We
also tried CRFs with bilingual features (projected
from English parses via the alignment output by
Moses), but obtained only a small improvement of
0.03, probably because the required information
is transferred in our stem markup (also a poor im-
provement beyond monolingual features is con-
sistent with previous work, see Section 8.3). De-
tails are omitted due to space.
We further validated our results by translating
the blind test set from wmt-2009, which we have
never looked at in any way. Here we also had
a statistically significant difference between the
baseline and the CRF-based prediction, the scores
were 13.68 and 14.18.
7 Analysis of Inflection-based System
Stem Markup. The first step of translating
from English to German stems (with the markup
we previously discussed) is substantially easier
than translating directly to inflected German (we
see BLEU scores on stems+markup that are over
2.0 BLEU higher than the BLEU scores on in-
flected forms when running MERT). The addition
of case to prepositions only lowered the BLEU
score reached by MERT by about 0.2, but is very
helpful for prediction of the case feature.
Inflection Prediction Task. Clean data task re-
sults13 are given in Table 5. The 4 CRFs outper-
form the 4 JSMs by more than 2%.
12We used Kevin Gimpel?s implementation of pairwise
bootstrap resampling with 1000 samples.
1326,061 of 55,057 tokens in our test set are ambiguous.
We report % surface form matches for ambiguous tokens.
669
Model Accuracy
unigram surface (no features) 55.98
surface (no features) 86.65
surface (with case, number, gender features) 91.24
1 JSM morphological features 92.45
4 JSMs morphological features 92.01
4 CRFs morphological features, lexical information 94.29
Table 5: Comparing predicting surface forms directly
with predicting morphological features.
training data 1 model 4 models
7.3 M sentences 92.41 91.88
1.5 M sentences 92.45 92.01
100000 sentences 90.20 90.64
1000 sentences 83.72 86.94
Table 6: Accuracy for different training data sizes of
the single and the four separate joint sequence models.
As we mentioned in Section 4, there is a spar-
sity issue at small training data sizes for the sin-
gle joint sequence model. This is shown in Ta-
ble 6. At the largest training data sizes, model-
ing all 4 features together results in the best pre-
dictions of inflection. However using 4 separate
models is worth this minimal decrease in perfor-
mance, since it facilitates experimentation with
the CRF framework for which the training of a
single model is not currently tractable.
Overall, the inflection prediction works well for
gender, number and type of inflection, which are
local features to the NP that normally agree with
the explicit markup output by the stem transla-
tion system (for example, the gender of a com-
mon noun, which is marked in the stem markup,
is usually successfully propagated to the rest of
the NP). Prediction of case does not always work
well, and could maybe be improved through hier-
archical labeled-syntax stem translation.
Portmanteaus. An example of where the sys-
tem is improved because of the new handling of
portmanteaus can be seen in the dative phrase
im internationalen Rampenlicht (in the interna-
tional spotlight), which does not occur in the par-
allel data. The accusative phrase in das interna-
tionale Rampenlicht does occur, however in this
case there is no portmanteau, but a one-to-one
mapping between in the and in das. For a given
context, only one of accusative or dative case is
valid, and a strongly disfluent sentence results
from the incorrect choice. In our system, these
two cases are handled in the same way (def-article
international Rampenlicht). This allows us to
generalize from the accusative example with no
portmanteau and take advantage of longer phrase
pairs, even when translating to something that will
be inflected as dative and should be realized as a
portmanteau. The baseline does not have this ca-
pability. It should be noted that the portmanteau
merging method described in Section 3 remerges
all occurrences of APPR and ART that can techni-
cally form a portmanteau. There are a few cases
where merging, despite being grammatical, does
not lead to a good result. Such exceptions require
semantic interpretation and are difficult to capture
with a fixed set of rules.
8 Adding Compounds to the System
Compounds are highly productive in German and
lead to data sparsity. We split the German com-
pounds in the training data, so that our stem trans-
lation system can now work with the individual
words in the compounds. After we have trans-
lated to a split/stemmed representation, we deter-
mine whether to merge words together to form a
compound. Then we merge them to create stems
in the same representation as before and we per-
form inflection and portmanteau merging exactly
as previously discussed.
8.1 Details of Splitting Process
We prepare the training data by splitting com-
pounds in two steps, following the technique of
Fritzinger and Fraser (2010). First, possible split
points are extracted using SMOR, and second, the
best split points are selected using the geometric
mean of word part frequencies.
compound word parts gloss
Inflationsrate Inflation Rate inflation rate
auszubrechen aus zu brechen out to break (to break out)
Training data is then stemmed as described in
Section 2.3. The formerly modifying words of the
compound (in our example the words to the left
of the rightmost word) do not have a stem markup
assigned, except for two cases: i) they are nouns
themselves or ii) they are particles separated from
a verb. In these cases, former modifiers are rep-
resented identically to their individual occurring
counterparts, which helps generalization.
8.2 Model for Compound Merging
After translation, compound parts have to be
resynthesized into compounds before inflection.
Two decisions have to be taken: i) where to
670
merge and ii) how to merge. Following the work
of Stymne and Cancedda (2011), we implement
a linear-chain CRF merging system using the
following features: stemmed (separated) surface
form, part-of-speech14 and frequencies from the
training corpus for bigrams/merging of word and
word+1, word as true prefix, word+1 as true suf-
fix, plus frequency comparisons of these. The
CRF is trained on the split monolingual data. It
only proposes merging decisions, merging itself
uses a list extracted from the monolingual data
(Popovic et al 2006).
8.3 Experiments
We evaluated the end-to-end inflection system
with the addition of compounds.15 As in the in-
flection experiments described in Section 5, we
use a 5-gram surface LM and a 7-gram POS
LM, but for this experiment, they are trained on
stemmed, split data. The POS LM helps com-
pound parts and heads appear in correct order.
The results are in Table 7. The BLEU score of the
CRF on test is 14.04, which is low. However the
system produces 19 compound types which are
in the reference but not in the parallel data, and
therefore not accessible to other systems. We also
observe many more compounds in general. The
100-best inflection rescoring technique previously
discussed reached 14.07 on the test set. Blind
test results with CRF prediction are much better,
14.08, which is a statistically significant improve-
ment over the baseline (13.68) and approaches the
result we obtained without compounds (14.18).
Correctly generated compounds are single words
which usually carry the same information as mul-
tiple words in English, and are hence likely un-
derweighted by BLEU. We again see many in-
teresting generalizations. For instance, take the
case of translating English miniature cameras to
the German compound Miniaturkameras. minia-
ture camera or miniature cameras does not occur
in the training data, and so there is no appropri-
ate phrase pair in any system (baseline, inflec-
tion, or inflection&compound-splitting). How-
ever, our system with compound splitting has
learned from split composita that English minia-
14Compound modifiers get assigned a special tag based on
the POS of their former heads, e.g., Inflation in the example
is marked as a non-head of a noun.
15We found it most effective to merge word parts during
MERT (so MERT uses the same stem references as before).
1 1 JSM morphological features 13.94
2 4 CRFs morphological features, lexical information 14.04
Table 7: Results with Compounds on the test set
ture can be translated as German Miniatur- and
gets the correct output.
9 Related Work
There has been a large amount of work on trans-
lating from a morphologically rich language to
English, we omit a literature review here due to
space considerations. Our work is in the opposite
direction, which primarily involves problems of
generation, rather than problems of analysis.
The idea of translating to stems and then in-
flecting is not novel. We adapted the work of
Toutanova et al(2008), which is effective but lim-
ited by the conflation of two separate issues: word
formation and inflection.
Given a stem such as brother, Toutanova et. al?s
system might generate the ?stem and inflection?
corresponding to and his brother. Viewing and
and his as inflection is problematic since a map-
ping from the English phrase and his brother to
the Arabic stem for brother is required. The situ-
ation is worse if there are English words (e.g., ad-
jectives) separating his and brother. This required
mapping is a significant problem for generaliza-
tion. We view this issue as a different sort of prob-
lem entirely, one of word-formation (rather than
inflection). We apply a ?split in preprocessing and
resynthesize in postprocessing? approach to these
phenomena, combined with inflection prediction
that is similar to that of Toutanova et. al. The
only work that we are aware of which deals with
both issues is the work of de Gispert and Marin?o
(2008), which deals with verbal morphology and
attached pronouns. There has been other work
on solving inflection. Koehn and Hoang (2007)
introduced factored SMT. We use more complex
context features. Fraser (2009) tried to solve the
inflection prediction problem by simply building
an SMT system for translating from stems to in-
flected forms. Bojar and Kos (2010) improved on
this by marking prepositions with the case they
mark (one of the most important markups in our
system). Both efforts were ineffective on large
data sets. Williams and Koehn (2011) used uni-
fication in an SMT system to model some of the
671
agreement phenomena that we model. Our CRF
framework allows us to use more complex con-
text features.
We have directly addressed the question as to
whether inflection should be predicted using sur-
face forms as the target of the prediction, or
whether linguistic features should be predicted,
along with the use of a subsequent generation
step. The direct prediction of surface forms is
limited to those forms observed in the training
data, which is a significant limitation. How-
ever, it is reasonable to expect that the use of
features (and morphological generation) could
also be problematic as this requires the use of
morphologically-aware syntactic parsers to anno-
tate the training data with such features, and addi-
tionally depends on the coverage of morpholog-
ical analysis and generation. Despite this, our
research clearly shows that the feature-based ap-
proach is superior for English-to-German SMT.
This is a striking result considering state-of-the-
art performance of German parsing is poor com-
pared with the best performance on English pars-
ing. As parsing performance improves, the per-
formance of linguistic-feature-based approaches
will increase.
Virpioja et al(2007), Badr et al(2008), Luong
et al(2010), Clifton and Sarkar (2011), and oth-
ers are primarily concerned with using morpheme
segmentation in SMT, which is a useful approach
for dealing with issues of word-formation. How-
ever, this does not deal directly with linguistic fea-
tures marked by inflection. In German these lin-
guistic features are marked very irregularly and
there is widespread syncretism, making it difficult
to split off morphemes specifying these features.
So it is questionable as to whether morpheme seg-
mentation techniques are sufficient to solve the in-
flectional problem we are addressing.
Much previous work looks at the impact of us-
ing source side information (i.e., feature func-
tions on the aligned English), such as those
of Avramidis and Koehn (2008), Yeniterzi and
Oflazer (2010) and others. Toutanova et. al.?s
work showed that it is most important to model
target side coherence and our stem markup also
allows us to access source side information. Us-
ing additional source side information beyond the
markup did not produce a gain in performance.
For compound splitting, we follow Fritzinger
and Fraser (2010), using linguistic knowledge en-
coded in a rule-based morphological analyser and
then selecting the best analysis based on the ge-
ometric mean of word part frequencies. Other
approaches use less deep linguistic resources
(e.g., POS-tags Stymne (2008)) or are (almost)
knowledge-free (e.g., Koehn and Knight (2003)).
Compound merging is less well studied. Popovic
et al(2006) used a simple, list-based merging ap-
proach, merging all consecutive words included
in a merging list. This approach resulted in too
many compounds. We follow Stymne and Can-
cedda (2011), for compound merging. We trained
a CRF using (nearly all) of the features they used
and found their approach to be effective (when
combined with inflection and portmanteau merg-
ing) on one of our two test sets.
10 Conclusion
We have shown that both the prediction of sur-
face forms and the prediction of linguistic features
are of interest for improving SMT. We have ob-
tained the advantages of both in our CRF frame-
work, and also integrated handling of compounds,
and an inflection-dependent word formation phe-
nomenon, portmanteaus. We validated our work
on a well-studied large corpora translation task.
Acknowledgments
The authors wish to thank the anonymous review-
ers for their comments. Aoife Cahill was partly
supported by Deutsche Forschungsgemeinschaft
grant SFB 732. Alexander Fraser, Marion Weller
and Fabienne Cap were funded by Deutsche
Forschungsgemeinschaft grant Models of Mor-
phosyntax for Statistical Machine Translation.
The research leading to these results has received
funding from the European Community?s Seventh
Framework Programme (FP7/2007-2013) under
grant agreement Nr. 248005. This work was sup-
ported in part by the IST Programme of the Euro-
pean Community, under the PASCAL2 Network
of Excellence, IST-2007-216886. This publica-
tion only reflects the authors? views. We thank
Thomas Lavergne and Helmut Schmid.
References
Eleftherios Avramidis and Philipp Koehn. 2008. En-
riching Morphologically Poor Languages for Statis-
tical Machine Translation. In Proceedings of ACL-
672
08: HLT, pages 763?770, Columbus, Ohio, June.
Association for Computational Linguistics.
Ibrahim Badr, Rabih Zbib, and James Glass. 2008.
Segmentation for English-to-Arabic statistical ma-
chine translation. In Proceedings of ACL-08: HLT,
Short Papers, pages 153?156, Columbus, Ohio,
June. Association for Computational Linguistics.
Ondr?ej Bojar and Kamil Kos. 2010. 2010 Failures in
English-Czech Phrase-Based MT. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 60?66, Upp-
sala, Sweden, July. Association for Computational
Linguistics.
Ann Clifton and Anoop Sarkar. 2011. Combin-
ing morpheme-based machine translation with post-
processing morpheme prediction. In Proceed-
ings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Lan-
guage Technologies, pages 32?42, Portland, Ore-
gon, USA, June. Association for Computational
Linguistics.
Adria` de Gispert and Jose? B. Marin?o. 2008. On the
impact of morphology in English to Spanish statisti-
cal MT. Speech Communication, 50(11-12):1034?
1046.
Alexander Fraser. 2009. Experiments in Morphosyn-
tactic Processing for Translating to and from Ger-
man. In Proceedings of the Fourth Workshop on
Statistical Machine Translation, pages 115?119,
Athens, Greece, March. Association for Computa-
tional Linguistics.
Fabienne Fritzinger and Alexander Fraser. 2010. How
to Avoid Burning Ducks: Combining Linguistic
Analysis and Corpus Statistics for German Com-
pound Processing. In Proceedings of the Fifth
Workshop on Statistical Machine Translation, pages
224?234. Association for Computational Linguis-
tics.
Philipp Koehn and Hieu Hoang. 2007. Factored
Translation Models. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 868?
876, Prague, Czech Republic, June. Association for
Computational Linguistics.
Philipp Koehn and Kevin Knight. 2003. Empirical
methods for compound splitting. In EACL ?03:
Proceedings of the 10th conference of the European
chapter of the Association for Computational Lin-
guistics, pages 187?193, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the International
Conference on Machine Learning, pages 282?289.
Morgan Kaufmann, San Francisco, CA.
Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.
2010. Practical very large scale CRFs. In Proceed-
ings the 48th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 504?513.
Association for Computational Linguistics, July.
Minh-Thang Luong, Preslav Nakov, and Min-Yen
Kan. 2010. A Hybrid Morpheme-Word Represen-
tation for Machine Translation of Morphologically
Rich Languages. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 148?157, Cambridge, MA, Octo-
ber. Association for Computational Linguistics.
Maja Popovic, Daniel Stein, and Hermann Ney. 2006.
Statistical Machine Translation of German Com-
pound Words. In Proceedings of FINTAL-06, pages
616?624, Turku, Finland. Springer Verlag, LNCS.
Helmut Schmid, Arne Fitschen, and Ulrich Heid.
2004. SMOR: A German Computational Morphol-
ogy Covering Derivation, Composition, and Inflec-
tion. In 4th International Conference on Language
Resources and Evaluation.
Helmut Schmid. 2004. Efficient Parsing of Highly
Ambiguous Context-Free Grammars with Bit Vec-
tors. In Proceedings of Coling 2004, pages 162?
168, Geneva, Switzerland, Aug 23?Aug 27. COL-
ING.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In International Confer-
ence on Spoken Language Processing.
Sara Stymne and Nicola Cancedda. 2011. Produc-
tive Generation of Compound Words in Statistical
Machine Translation. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
250?260, Edinburgh, Scotland UK, July. Associa-
tion for Computational Linguistics.
Sara Stymne. 2008. German Compounds in Factored
Statistical Machine Translation. In Proceedings of
GOTAL-08, pages 464?475, Gothenburg, Sweden.
Springer Verlag, LNCS/LNAI.
Kristina Toutanova, Hisami Suzuki, and Achim
Ruopp. 2008. Applying Morphology Generation
Models to Machine Translation. In Proceedings of
ACL-08: HLT, pages 514?522, Columbus, Ohio,
June. Association for Computational Linguistics.
Sami Virpioja, Jaakko J. Va?yrynen, Mathias Creutz,
and Markus Sadeniemi. 2007. Morphology-aware
statistical machine translation based on morphs in-
duced in an unsupervised manner. In PROC. OF
MT SUMMIT XI, pages 491?498.
Philip Williams and Philipp Koehn. 2011. Agree-
ment constraints for statistical machine translation
into German. In Proceedings of the Sixth Workshop
on Statistical Machine Translation, pages 217?226,
Edinburgh, Scotland, July. Association for Compu-
tational Linguistics.
Reyyan Yeniterzi and Kemal Oflazer. 2010. Syntax-
to-Morphology Mapping in Factored Phrase-Based
673
Statistical Machine Translation from English to
Turkish. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 454?464, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
674
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 726?735,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Determining the placement of German verbs in English?to?German
SMT
Anita Gojun Alexander Fraser
Institute for Natural Language Processing
University of Stuttgart, Germany
{gojunaa, fraser}@ims.uni-stuttgart.de
Abstract
When translating English to German, exist-
ing reordering models often cannot model
the long-range reorderings needed to gen-
erate German translations with verbs in the
correct position. We reorder English as a
preprocessing step for English-to-German
SMT. We use a sequence of hand-crafted
reordering rules applied to English parse
trees. The reordering rules place English
verbal elements in the positions within the
clause they will have in the German transla-
tion. This is a difficult problem, as German
verbal elements can appear in different po-
sitions within a clause (in contrast with En-
glish verbal elements, whose positions do
not vary as much). We obtain a significant
improvement in translation performance.
1 Introduction
Phrase-based SMT (PSMT) systems translate
word sequences (phrases) from a source language
into a target language, performing reordering of
target phrases in order to generate a fluent target
language output. The reordering models, such as,
for example, the models implemented in Moses
(Koehn et al 2007), are often limited to a cer-
tain reordering range since reordering beyond this
distance cannot be performed accurately. This re-
sults in problems of fluency for language pairs
with large differences in constituent order, such
as English and German. When translating from
English to German, verbs in the German output
are often incorrectly left near their position in En-
glish, creating problems of fluency. Verbs are also
often omitted since the distortion model cannot
move verbs to positions which are licensed by the
German language model, making the translations
difficult to understand.
A common approach for handling the long-
range reordering problem within PSMT is per-
forming syntax-based or part-of-speech-based
(POS-based) reordering of the input as a prepro-
cessing step before translation (e.g., Collins et al
(2005), Gupta et al(2007), Habash (2007), Xu
et al(2009), Niehues and Kolss (2009), Katz-
Brown et al(2011), Genzel (2010)).
We reorder English to improve the translation
to German. The verb reordering process is im-
plemented using deterministic reordering rules on
English parse trees. The sequence of reorderings
is derived from the clause type and the composi-
tion of a given verbal complex (a (possibly dis-
contiguous) sequence of verbal elements in a sin-
gle clause). Only one rule can be applied in a
given context and for each word to be reordered,
there is a unique reordered position. We train a
standard PSMT system on the reordered English
training and tuning data and use it to translate the
reordered English test set into German.
This paper is structured as follows: in section
2, we outline related work. In section 3, English
and German verb positioning is described. The
reordering rules are given in section 4. In sec-
tion 5, we show the relevance of the reordering,
present the experiments and present an extensive
error analysis. We discuss some problems ob-
served in section 7 and conclude in section 8.
2 Related work
There have been a number of attempts to handle
the long-range reordering problem within PSMT.
Many of them are based on the reordering of a
source language sentence as a preprocessing step
726
before translation. Our approach is related to the
work of Collins et al(2005). They reordered
German sentences as a preprocessing step for
German-to-English SMT. Hand-crafted reorder-
ing rules are applied on German parse trees in
order to move the German verbs into the posi-
tions corresponding to the positions of the English
verbs. Subsequently, the reordered German sen-
tences are translated into English leading to better
translation performance when compared with the
translation of the original German sentences.
We apply this method on the opposite trans-
lation direction, thus having English as a source
language and German as a target language. How-
ever, we cannot simply invert the reordering rules
which are applied on German as a source lan-
guage in order to reorder the English input. While
the reordering of German implies movement of
the German verbs into a single position, when re-
ordering English, we need to split the English ver-
bal complexes and, where required, move their
parts into different positions. Therefore, we need
to identify exactly which parts of a verbal com-
plex must be moved and their possible positions
in a German sentence.
Reordering rules can also be extracted automat-
ically. For example, Niehues and Kolss (2009)
automatically extracted discontiguous reordering
rules (allowing gaps between POS tags which
can include an arbitrary number of words) from
a word-aligned parallel corpus with POS tagged
source side. Since many different rules can be ap-
plied on a given sentence, a number of reordered
sentence alternatives are created which are en-
coded as a word lattice (Dyer et al 2008). They
dealt with the translation directions German-to-
English and English-to-German, but translation
improvement was obtained only for the German-
to-English direction. This may be due to miss-
ing information about clause boundaries since En-
glish verbs often have to be moved to the clause
end. Our reordering has access to this kind of
knowledge since we are working with a full syn-
tactic parser of English.
Genzel (2010) proposed a language-
independent method for learning reordering
rules where the rules are extracted from parsed
source language sentences. For each node, all
possible reorderings (permutations) of a limited
number of the child nodes are considered. The
candidate reordering rules are applied on the
dev set which is then translated and evaluated.
Only those rule sequences are extracted which
maximize the translation performance of the
reordered dev set.
For the extraction of reordering rules, Gen-
zel (2010) uses shallow constituent parse trees
which are obtained from dependency parse trees.
The trees are annotated using both Penn Tree-
bank POS tags and using Stanford dependency
types. However, the constraints on possible re-
orderings are too restrictive in order to model all
word movements required for English-to-German
translation. In particular, the reordering rules in-
volve only the permutation of direct child nodes
and do not allow changing of child-parent rela-
tionships (deleting of a child or attaching a node
to a new father node). In our implementation, a
verb can be moved to any position in a parse tree
(according to the reordering rules): the reordering
can be a simple permutation of child nodes, or at-
tachment of these nodes to a new father node (cf.
movement of bought and read in figure 11).
Thus, in contrast to Genzel (2010), our ap-
proach does not have any constraints with respect
to the position of nodes marking a verb within the
tree. Only the syntactic structure of the sentence
restricts the distance of the linguistically moti-
vated verb movements.
3 Verb positions in English and German
3.1 Syntax of German sentences
Since in this work, we concentrate on verbs, we
use the notion verbal complex for a sequence con-
sisting of verbs, verbal particles and negation.
The verb positions in the German sentences de-
pend on clause type and the tense as shown in ta-
ble 1. Verbs can be placed in 1st, 2nd or clause-
final position. Additionally, if a composed tense
is given, the parts of a verbal complex can be
interrupted by the middle field (MF) which con-
tains arbitrary sentence constituents, e.g., sub-
jects and objects (noun phrases), adjuncts (prepo-
sitional phrases), adverbs, etc. We assume that the
German sentences are SVO (analogously to En-
glish); topicalization is beyond the scope of our
work.
In this work, we consider two possible posi-
tions of the negation in German: (1) directly in
1The verb movements shown in figure 1 will be explained
in detail in section 4.
727
1st 2nd MF clause-
final
decl
subject finV any ?
subject finV any mainV
int/perif
finV subject any ?
finV subject any mainV
sub/inf
relCon subject any finV
relCon subject any VC
Table 1: Position of the German subjects and verbs
in declarative clauses (decl), interrogative clauses and
clauses with a peripheral clause (int/perif ), subordi-
nate/infinitival (sub/inf ) clauses. mainV = main verb,
finV = finite verb, VC = verbal complex, any = arbi-
trary words, relCon = relative pronoun or conjunction.
We consider extraponed consituents in perif, as well as
optional interrogatives in int to be in position 0.
front of the main verb, and (2) directly after the
finite verb. The two negation positions are illus-
trated in the following examples:
(1) Ich
I
behaupte,
claim
dass
that
ich
I
es
it
nicht
not
gesagt
say
habe.
did.
(2) Ich
I
denke
think
nicht,
not
dass
that
er
he
das
that
gesagt
said
hat.
has.
It should, however, be noted that in German, the
negative particle nicht can have several positions
in a sentence depending on the context (verb argu-
ments, emphasis). Thus, more analysis is ideally
needed (e.g., discourse, etc.).
3.2 Comparison of verb positions
English and German verbal complexes differ both
in their construction and their position. The Ger-
man verbal complex can be discontiguous, i.e., its
parts can be placed in different positions which
implies that a (large) number of other words can
be placed between the verbs (situated in the MF).
In English, the verbal complex can only be inter-
rupted by adverbials and subjects (in interrogative
clauses). Furthermore, in German, the finite verb
can sometimes be the last element of the verbal
complex, while in English, the finite verb is al-
ways the first verb in the verbal complex.
In terms of positions, the verbs in English and
German can differ significantly. As previously
noted, the German verbal complex can be discon-
tiguous, simultaneously occupying 1st/2nd and
clause-final position (cf. rows decl and int/perif in
table 1), which is not the case in English. While in
English, the verbal complex is placed in the 2nd
position in declarative, or in the 1st position in in-
terrogative clauses, in German, the entire verbal
complex can additionally be placed at the clause
end in subordinate or infinitival clauses (cf. row
sub/inf in table 1).
Because of these differences, for nearly all
types of English clauses, reordering is needed in
order to place the English verbs in the positions
which correspond to the correct verb positions in
German. Only English declarative clauses with
simple present and simple past tense have the
same verb position as their German counterparts.
We give statistics on clause types and their rele-
vance for the verb reordering in section 5.1.
4 Reordering of the English input
The reordering is carried out on English parse
trees. We first enrich the parse trees with clause
type labels, as described below. Then, for each
node marking a clause (S nodes), the correspond-
ing sequence of reordering rules is carried out.
The appropriate reordering is derived from the
clause type label and the composition of the given
verbal complex. The reordering rules are deter-
ministic. Only one rule can be applied in a given
context and for each verb to be reordered, there is
a unique reordered position.
The reordering procedure is the same for the
training and the testing data. It is carried out
on English parse trees resulting in modified parse
trees which are read out in order to generate the
reordered English sentences. These are input for
training a PSMT system or input to the decoder.
The processing steps are shown in figure 1.
For the development of the reordering rules, we
used a small sample of the training data. In par-
ticular, by observing the English parse trees ex-
tracted randomly from the training data, we de-
veloped a set of rules which transform the origi-
nal trees in such a way that the English verbs are
moved to the positions which correspond to the
placement of verbs in German.
4.1 Labeling clauses with their type
As shown in section 3.1, the verb positions in Ger-
man depend on the clause type. Since we use En-
glish parse trees produced by the generative parser
of Charniak and Johnson (2005) which do not
have any function labels, we implemented a sim-
ple rule-based clause type labeling script which
728
..
.
.
WHNP
which
NP
DT NN
a book
Yesterday
RB
ADVP ,
,
S?EXTR
I
PRP
NP
NP
JJ NN
last week
NP
PRP
I
Yesterday
RB
ADVP ,
,
I
PRP
NP
S?EXTR
NP
JJ NN
last week
NP
PRP
I
NP
DT NN
a book
NP
S?SUB
S
VP
VBD
read
VBD
bought
NP
WHNP
which
S?SUB
S
VP
VBD
bought
VBD
read
reor
dering
read out and translate
VP
VP1
1
Figure 1: Processing steps: Clause type labeling an-
notates the given original tree with clause type labels
(in figure, S-EXTR and S-SUB). Subsequently, the re-
ordering is performed (cf. movement of the verbs read
and bought). The reordered sentence is finally read out
and given to the decoder.
enriches every clause starting node with the corre-
sponding clause type label. The label depends on
the context (father, child nodes) of a given clause
node. If, for example, the first child node of a
given S node is WH* (wh-word) or IN (subordi-
nating conjunction), then the clause type label is
SUB (subordinate clause, cf. figure 1).
We defined five clause type labels which indi-
cate main clauses (MAIN), main clauses with a
peripheral clause in the prefield (EXTR), subor-
dinate (SUB), infinitival (XCOMP) and interroga-
tive clauses (INT).
4.2 Clause boundary identification
The German verbs are often placed at the clause
end (cf. rows decl, int/perif and sub/inf in ta-
ble 1), making it necessary to move their En-
glish counterparts into the corresponding posi-
tions within an English tree. For this reason, we
identify the clause ends (the right boundaries).
The search for the clause end is implemented as
a breadth-first search for the next S node or sen-
tence end. The starting node is the node which
marks the verbal phrase in which the verbs are
enclosed. When the next node marking a clause
is identified, the search stops and returns the posi-
tion in front of the identified clause marking node.
When, for example, searching for the clause
boundary of S-EXTR in figure 1, we search re-
cursively for the first clause marking node within
VP1 , which is S-SUB. The position in front of S-
SUB is marked as clause-final position of S-EXTR.
4.3 Basic verb reordering rules
The reordering procedure takes into account the
following word categories: verbs, verb particles,
the infinitival particle to and the negative parti-
cle not, as well as its abbreviated form ?t. The
reordering rules are based on POS labels in the
parse tree.
The reordering procedure is a sequence of ap-
plications of the reordering rules. For each el-
ement of an English verbal complex, its proper-
ties are derived (tense, main verb/auxiliary, finite-
ness). The reordering is then carried out corre-
sponding to the clause type and verbal properties
of a verb to be processed.
In the following, the reordering rules are pre-
sented. Examples of reordered sentences are
given in table 2, and are discussed further here.
Main clause (S-MAIN)
(i) simple tense: no reordering required
(cf. appearsfinV in input 1);
(ii) composed tense: the main verb is moved to
the clause end. If a negative particle exists, it
is moved in front of the reordered main verb,
while the optional verb particle is moved af-
ter the reordered main verb (cf. [has]finV
[been developing]mainV in input 2).
Main clause with peripheral clause (S-EXTR)
(i) simple tense: the finite verb is moved to-
gether with an optional particle to the 1st po-
sition (i.e. in front of the subject);
(ii) composed tense: the main verb, as well
as optional negative and verb particles are
moved to the clause end. The finite verb is
moved in the 1st position, i.e. in front of the
subject (cf. havefinV [gone up]mainV in in-
put 3).
729
Subordinate clause (S-SUB)
(i) simple tense: the finite verb is moved to the
clause end (cf. boastsfinV in input 3);
(ii) composed tense: the main verb, as well
as optional negative and verb particles are
moved to the clause end, the finite verb is
placed after the reordered main verb (cf.
havefinV [been executed]mainV in input 5).
Infinitival clause (S-XCOMP)
The entire English verbal complex is moved from
the 2nd position to the clause-final position (cf.
[to discuss]VC in input 4).
Interrogative clause (S-INT)
(i) simple tense: no reordering required;
(ii) composed tense: the main verb, as well
as optional negative and verb particles are
moved to the clause end (cf. [did]finV
knowmainV in input 5).
4.4 Reordering rules for other phenomena
4.4.1 Multiple auxiliaries in English
Some English tenses require a sequence of aux-
iliaries, not all of which have a German coun-
terpart. In the reordering process, non-finite
auxiliaries are considered to be a part of the
main verb complex and are moved together with
the main verb (cf. movement of hasfinV [been
developing]mainV in input 2).
4.4.2 Simple vs. composed tenses
In English, there are some tenses composed of
an auxiliary and a main verb which correspond
to a German tense composed of only one verb,
e.g., am reading? lese and does John read? ?
liest John? Splitting such English verbal com-
plexes and only moving the main verbs would
lead to constructions which do not exist in Ger-
man. Therefore, in the reordering process, the
English verbal complex in present continuous, as
well as interrogative phrases composed of do and
a main verb, are not split. They are handled as
one main verb complex and reordered as a sin-
gle unit using the rules for main verbs (e.g. [be-
cause I am reading a book]SUB ? because I a
book am reading? weil ich ein Buch lese.2
2We only consider present continuous and verbs in com-
bination with do for this kind of reordering. There are also
4.4.3 Flexible position of German verbs
We stated that the English verbs are never moved
outside the subclause they were originally in. In
German there are, however, some constructions
(infinitival and relative clauses), in which the
main verb can be placed after a subsequent clause.
Consider two German translations of the English
sentence He has promised to come:
(3a) Er
he
hat
has
[zu
to
kommen]S
come
versprochen.
promised.
(3b) Er
he
hat
has
versprochen,
promised,
[zu
to
kommen]S .
come.
In (3a), the German main verb versprochen is
placed after the infinitival clause zu kommen (to
come), while in (3b), the same verb is placed in
front of it. Both alternatives are grammatically
correct.
If a German verb should come after an em-
bedded clause as in example (3a) or precede it
(cf. example (3b)), depends not only on syntac-
tic but also on stylistic factors. Regarding the
verb reordering problem, we would therefore have
to examine the given sentence in order to derive
the correct (or more probable) new verb position
which is beyond the scope of this work. There-
fore, we allow only for reorderings which do not
cross clause boundaries as shown in example (3b).
5 Experiments
In order to evaluate the translation of the re-
ordered English sentences, we built two SMT sys-
tems with Moses (Koehn et al 2007). As train-
ing data, we used the Europarl corpus which con-
sists of 1,204,062 English/German sentence pairs.
The baseline system was trained on the original
English training data while the contrastive system
was trained on the reordered English training data.
In both systems, the same original German sen-
tences were used. We used WMT 2009 dev and
test sets to tune and test the systems. The baseline
system was tuned and tested on the original data
while for the contrastive system, we used the re-
ordered English side of the dev and test sets. The
German 5-gram language model used in both sys-
tems was trained on the WMT 2009 German lan-
guage modeling data, a large German newspaper
corpus consisting of 10,193,376 sentences.
other tenses which could (or should) be treated in the same
way (cf. has been developing on input 2, table 2). We do not
do this to keep the reordering rules simple and general.
730
Input 1 The programme appears to be successful for published data shows that MRSA is on the decline in the UK.
Reordered The programme appears successful to be for published data shows that MRSA on the decline in the UK is.
Input 2 The real estate market in Bulgaria has been developing at an unbelievable rate - all of Europe has its eyes
on this heretofore rarely heard-of Balkan nation.
Reordered The real estate market in Bulgaria has at an unbelievable rate been developing - all of Europe has its eyes
on this heretofore rarely heard-of Balkan nation.
Input 3 While Bulgaria boasts the European Union?s lowest real estate prices, they have still gone up by 21 percent
in the past five years.
Reordered While Bulgaria the European Union?s lowest real estate prices boasts, have they still by 21 percent in the
past five years gone up.
Input 4 Professionals and politicians from 192 countries are slated to discuss the Bali Roadmap that focuses on
efforts to cut greenhouse gas emissions after 2012, when the Kyoto Protocol expires.
Reordered Professionals and politicians from 192 countries are slated the Bali Roadmap to discuss that on efforts
focuses greenhouse gas emissions after 2012 to cut, when the Kyoto Protocol expires.
Input 5 Did you know that in that same country, since 1976, 34 mentally-retarded offenders have been executed?
Reordered Did you know that in that same country, since 1976, 34 mentally-retarded offenders been executed have?
Table 2: Examples of reordered English sentences
5.1 Applied rules
In order to see how many English clauses are rel-
evant for reordering, we derived statistics about
clause types and the number of reordering rules
applied on the training data.
In table 3, the number of the English clauses
with all considered clause type/tense combination
are shown. The bold numbers indicate combina-
tions which are relevant to the reordering. Over-
all, 62% of all EN clauses from our training data
(2,706,117 clauses) are relevant for the verb re-
ordering. Note that there is an additional category
rest which indicates incorrect clause type/tense
combinations and might thus not be correctly re-
ordered. These are mostly due to parsing and/or
tagging errors.
The performance of the systems was measured
by BLEU (Papineni et al 2002). The evaluation
results are shown in table 4. The contrastive sys-
tem outperforms the baseline. Its BLEU score is
13.63 which is a gain of 0.61 BLEU points over
the baseline. This is a statistically significant im-
provement at p<0.05 (computed with Gimpel?s
implementation of the pairwise bootstrap resam-
pling method (Koehn, 2004)).
Manual examination of the translations pro-
duced by both systems confirms the result of
the automatic evaluation. Many translations pro-
duced by the contrastive system now have verbs in
the correct positions. If we compare the generated
translations for input sentence 1 in table 5, we
see that the contrastive system generates a trans-
tense MAIN EXTR SUB INT XCOMP
simple 675,095 170,806 449,631 8,739 -
composed 343,178 116,729 277,733 8,817 314,573
rest 98,464 5,158 90,139 306 146,746
Table 3: Counts of English clause types and used
tenses. Bold numbers indicate clause type/tense com-
binations where reordering is required.
Baseline Reordered
BLEU 13.02 13.63
Table 4: Scores of baseline and contrastive systems
lation in which all verbs are placed correctly. In
the baseline translation, only the translation of the
finite verb was, namely war, is placed correctly,
while the translation of the main verb (diagnosed
? festgestellt) should be placed at the clause end
as in the translation produced by our system.
5.2 Evaluation
Often, the English verbal complex is translated
only partially by the baseline system. For exam-
ple, the English verbal complexes in sentence 2 in
table 5 will climb and will drop are only partially
translated (will climb? wird (will), will drop?
fallen (fall)). Moreover, the generated verbs are
placed incorrectly. In our translation, all verbs are
translated and placed correctly.
Another problem which was often observed in
the baseline is the omission of the verbs in the
German translations. The baseline translation of
the example sentence 3 in table 5 illustrates such
731
a case. There is no translation of the English in-
finitival verbal complex to have. In the transla-
tion generated by the contrastive system, the ver-
bal complex does get translated (zu haben) and
is also placed correctly. We think this is because
the reordering model is not able to identify the
position for the verb which is licensed by the lan-
guage model, causing a hypothesis with no verb
to be scored higher than the hypotheses with in-
correctly placed verbs.
6 Error analysis
6.1 Erroneous reordering in our system
In some cases, the reordering of the English parse
trees fails. Most erroneous reorderings are due to
a number of different parsing and tagging errors.
Coordinated verbs are also problematic due to
their complexity. Their composition can vary, and
thus it would require a large number of different
reordering rules to fully capture this. In our re-
ordering script, the movement of complex struc-
tures such as verbal phrases consisting of a se-
quence of child nodes is not implemented (only
nodes with one child, namely the verb, verbal par-
ticle or negative particle are moved).
6.2 Splitting of the English verbal complex
Since in many cases, the German verbal complex
is discontiguous, we need to split the English ver-
bal complex and move its parts into different posi-
tions. This ensures the correct placement of Ger-
man verbs. However, this does not ensure that the
German verb forms are correct because of highly
ambiguous English verbs. In some cases, we can
lose contextual information which would be use-
ful for disambiguating ambiguous verbs and gen-
erating the appropriate German verb forms.
6.2.1 Subject?verb agreement
Let us consider the English clause in (4a) and its
reordered version in (4b):
(4a) ... because they have said it to me yesterday.
(4b) ... because they it to me yesterday said have.
In (4b), the English verbs said have are separated
from the subject they. The English said have can
be translated in several ways into German. With-
out any information about the subject (the dis-
tance between the verbs and the subject can be
very large), it is relatively likely that an erroneous
German translation is generated.
On the other hand, in the baseline SMT system,
the subject they is likely to be a part of a trans-
lation phrase with the correct German equivalent
(they have said? sie haben gesagt). They is then
used as a disambiguating context which is missing
in the reordered sentence (but the order is wrong).
6.2.2 Verb dependency
A similar problem occurs in a verbal complex:
(5a) They have said it to me yesterday.
(5b) They have it to me yesterday said.
In sentence (5a), the English consecutive verbs
have said are a sequence consisting of a finite
auxiliary have and the past participle said. They
should be translated into the corresponding Ger-
man verbal complex haben gesagt. But, if the
verbs are split, we will probably get translations
which are completely independent. Even if the
German auxiliary is correctly inflected, it is hard
to predict how said is going to be translated. If
the distance between the auxiliary habe and the
hypothesized translation of said is large, the lan-
guage model will not be able to help select the
correct translation. Here, the baseline SMT sys-
tem again has an advantage as the verbs are con-
secutive. It is likely they will be found in the train-
ing data and extracted with the correct German
phrase (but the German order is again incorrect).
6.3 Collocations
Collocations (verb?object pairs) are another case
which can lead to a problem:
(6a) I think that the discussion would take place
later this evening.
(6b) I think that the discussion place later this
evening take would.
The English collocation in (6a) consisting of the
verb take and the object place corresponds to the
German verb stattfinden. Without this specific ob-
ject, the verb take is likely to be translated liter-
ally. In the reordered sentence, the verbal com-
plex take would is indeed separated from the ob-
ject place which would probably lead to the literal
translation of both parts of the mentioned collo-
cation. So, as already described in the preceding
paragraphs, an important source of contextual in-
formation is lost which could ensure the correct
translation of the given phrase.
This problem is not specific to English?to?
German. For instance, the same problem occurs
when translating German into English. If, for ex-
732
Input 1 An MRSA - an antibiotic resistant staphylococcus - infection was recently diagnosed in the trauma-
tology ward of Ja?nos hospital.
Reordered
input
An MRSA - an antibiotic resistant staphylococcus - infection was recently in the traumatology ward
of Ja?nos hospital diagnosed.
Baseline
translation
Ein
A
MRSA
MRSA
-
-
ein
an
Antibiotikum
antibiotic
resistenter
resistant
Staphylococcus
Staphylococcus
-
-
war
was
vor
before
kurzem
recent
in
in
der
the
festgestellt
diagnosed
traumatology
traumatology
Ward
ward
von
of
Ja?nos
Ja?nos
Krankenhaus.
hospital.
Reordered
translation
Ein
A
MRSA
MRSA
-
-
ein
an
Antibiotikum
antibiotic
resistenter
resistant
Staphylococcus
Staphylococcus
-
-
Infektion
infection
wurde
was
vor
before
kurzem
recent
in
in
den
the
traumatology
traumatology
Station
ward
der
of
Ja?nos
Ja?nos
Krankenhaus
hospital
diagnostiziert.
diagnosed.
Input 2 The ECB predicts that 2008 inflation will climb to 2.5 percent from the earlier 2.1, but will drop
back to 1.9 percent in 2009.
Reordered
input
The ECB predicts that 2008 inflation to 2.5 percent from the earlier 2.1 will climb, but back to 1.9
percent in 2009 will drop.
Baseline
translation
Die
The
EZB
ECB
sagt,
says,
dass
that
2008
2008
die
the
Inflationsrate
inflation rate
wird
will
auf
to
2,5
2.5
Prozent
percent
aus
from
der
the
fru?heren
earlier
2,1,
2.1,
sondern
but
fallen
fall
zuru?ck
back
auf
to
1,9
1.9
Prozent
percent
im
in the
Jahr
year
2009.
2009.
Reordered
translation
Die
The
EZB
ECB
prophezeit,
predicts,
dass
that
2008
2008
die
the
Inflation
inflation rate
zu
to
2,5
2.5
Prozent
percent
aus
from
der
the
fru?heren
earlier
2,1
2.1
ansteigen
climb
wird,
will,
aber
but
auf
to
1,9
1.9
Prozent
percent
in
in
2009
2009
sinken
fall
wird.
will.
Input 3 Labour Minister Mo?nika Lamperth appears not to have a sensitive side.
R. input Labour Minister Mo?nika Lamperth appears a sensitive side not to have .
Baseline
translation
Arbeitsminister
Labour Minister
Mo?nika
Mo?nika
Lamperth
Lamperth
scheint
appears
nicht
not
eine
a
sensible
sensitive
Seite.
side.
Reordered
translation
Arbeitsminister
Labour Minister
Mo?nika
Mo?nika
Lamperth
Lamperth
scheint
appears
eine
a
sensible
sensitive
Seite
side
nicht
not
zu
to
haben.
have.
Table 5: Example translations, the baseline has problems with verbal elements, reordered is correct
ample, the object Kauf (buying) of the colloca-
tion nehmen + in Kauf (accept) is separated from
the verb nehmen (take), they are very likely to be
translated literally (rather than as the idiom mean-
ing ?to accept?), thus leading to an erroneous En-
glish translation.
6.4 Error statistics
We manually checked 100 randomly chosen En-
glish sentences to see how often the problems de-
scribed in the previous sections occur. From a
total of 276 clauses, 29 were not reordered cor-
rectly. 20 errors were caused by incorrect parsing
and/or POS tags, while the remaining 9 are mostly
due to different kinds of coordination. Table 6
shows correctly reordered clauses which might
pose a problem for translation (see sections 6.2?
6.3). Although the positions of the verbs in the
translations are now correct, the distance between
subjects and verbs, or between verbs in a single
VP might lead to the generation of erroneously
inflected verbs. The separate generation of Ger-
man verbal morphology is an interesting area of
future work, see (de Gispert and Marin?o, 2008).
We also found 2 problematic collocations but note
that this only gives a rough idea of the problem,
further study is needed.
6.5 POS-based disambiguation of the
English verbs
With respect to the problems described in 6.2.1
and 6.2.2, we carried out an experiment in which
733
total d ? 5 tokens
subject?verb 40 19
verb dependency 32 14
collocations 8 2
Table 6: total is the number of clauses found for the
respective phenomenon. d? 5 tokens is the number of
clauses where the distance between relevant tokens is
at least 5, which is problematic.
Baseline + POS Reordered + POS
BLEU 13.11 13.68
Table 7: BLEU scores of the baseline and the con-
trastive SMT system using verbal POS tags
we used POS tags in order to disambiguate the
English verbs. For example, the English verb said
corresponds to the German participle gesagt, as
well as to the finite verb in simple past, e.g. sagte.
We attached the POS tags to the English verbs in
order to simulate a disambiguating suffix of a verb
(e.g. said? said VBN, said VBD). The idea be-
hind this was to extract the correct verbal trans-
lation phrases and score them with appropriate
translation probabilities (e.g. p(said VBN, gesagt)
> p(said VBN, sagte).
We built and tested two PSMT systems using
the data enriched with verbal POS tags. The
first system is trained and tested on the original
English sentences, while the contrastive one was
trained and tested on the reordered English sen-
tences. Evaluation results are shown in table 7.
The baseline obtains a gain of 0.09 and the con-
trastive system of 0.05 BLEU points over the cor-
responding PSMT system without POS tags. Al-
though there are verbs which are now generated
correctly, the overall translation improvement lies
under our expectation. We will directly model the
inflection of German verbs in future work.
7 Discussion and future work
We implemented reordering rules for English ver-
bal complexes because their placement differs
significantly from German placement. The imple-
mentation required dealing with three important
problems: (i) definition of the clause boundaries,
(ii) identification of the new verb positions and
(iii) correct splitting of the verbal complexes.
We showed some phenomena for which a
stochastic reordering would be more appropriate.
For example, since in German, the auxiliary and
the main verb of a verbal complex can occupy
different positions in a clause, we had to define
the English counterparts of the two components
of the German verbal complex. We defined non-
finite English verbal elements as a part of the main
verb complex which are then moved together with
the main verb. This rigid definition could be re-
laxed by considering multiple different splittings
and movements of the English verbs.
Furthermore, the reordering rules are applied
on a clause not allowing for movements across the
clause boundaries. However, we also showed that
in some cases, the main verbs may be moved after
the succeeding subclause. Stochastic rules could
allow for both placements or carry out the more
probable reordering given a specific context. We
will address these issues in future work.
Unfortunately, some important contextual in-
formation is lost when splitting and moving En-
glish verbs. When English verbs are highly am-
biguous, erroneous German verbs can be gener-
ated. The experiment described in section 6.5
shows that more effort should be made in order to
overcome this problem. The incorporation of sep-
arate morphological generation of inflected Ger-
man verbs would improve translation.
8 Conclusion
We presented a method for reordering English as a
preprocessing step for English?to?German SMT.
To our knowledge, this is one of the first papers
which reports on experiments regarding the re-
ordering problem for English?to?German SMT.
We showed that the reordering rules specified in
this work lead to improved translation quality. We
observed that verbs are placed correctly more of-
ten than in the baseline, and that verbs which were
omitted in the baseline are now often generated.
We carried out a thorough analysis of the rules
applied and discussed problems which are related
to highly ambiguous English verbs. Finally we
presented ideas for future work.
Acknowledgments
This work was funded by Deutsche Forschungs-
gemeinschaft grant Models of Morphosyntax for
Statistical Machine Translation.
734
References
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In ACL.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In ACL.
Adria` de Gispert and Jose? B. Marin?o. 2008. On the
impact of morphology in English to Spanish statis-
tical MT. Speech Communication, 50(11-12).
Chris Dyer, Smaranda Muresan, and Philip Resnik.
2008. Generalizing word lattice translation. In
ACL-HLT.
Dmitriy Genzel. 2010. Automatically learning
source-side reordering rules for large scale machine
translation. In COLING.
Deepa Gupta, Mauro Cettolo, and Marcello Federico.
2007. POS-based reordering models for statistical
machine translation. In Proceedings of the Machine
Translation Summit (MT-Summit).
Nizar Habash. 2007. Syntactic preprocessing for sta-
tistical machine translation. In Proceedings of the
Machine Translation Summit (MT-Summit).
Jason Katz-Brown, Slav Petrov, Ryan McDon-
ald, Franz Och, David Talbot, Hiroshi Ichikawa,
Masakazu Seno, and Hideto Kazawa. 2011. Train-
ing a parser for machine translation reordering. In
EMNLP.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In ACL, Demonstration Program.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In EMNLP.
Jan Niehues and Muntsin Kolss. 2009. A POS-based
model for long-range reorderings in SMT. In EACL
Workshop on Statistical Machine Translation.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a method for auto-
matic evaluation of machine translation. In ACL.
Peng Xu, Jaecho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
SMT for subject-object-verb languages. In NAACL.
735
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 579?587,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
How to Produce Unseen Teddy Bears:
Improved Morphological Processing of Compounds in SMT
Fabienne Cap, Alexander Fraser
CIS, University of Munich
{cap|fraser}@cis.uni-muenchen.de
Marion Weller
IMS, University of Stuttgart
wellermn@ims.uni-stuttgart.de
Aoife Cahill
Educational Testing Service
acahill@ets.org
Abstract
Compounding in morphologically rich
languages is a highly productive process
which often causes SMT approaches to
fail because of unseen words. We present
an approach for translation into a com-
pounding language that splits compounds
into simple words for training and, due
to an underspecified representation, allows
for free merging of simple words into
compounds after translation. In contrast to
previous approaches, we use features pro-
jected from the source language to predict
compound mergings. We integrate our ap-
proach into end-to-end SMT and show that
many compounds matching the reference
translation are produced which did not ap-
pear in the training data. Additional man-
ual evaluations support the usefulness of
generalizing compound formation in SMT.
1 Introduction
Productive processes like compounding or inflec-
tion are problematic for traditional phrase-based
statistical machine translation (SMT) approaches,
because words can only be translated as they have
occurred in the parallel training data. As paral-
lel training data is limited, it is desirable to ex-
tract as much information from it as possible. We
present an approach for compound processing in
SMT, translating from English to German, that
splits compounds prior to training (in order to ac-
cess the individual words which together form the
compound) and recombines them after translation.
While compound splitting is a well-studied task,
compound merging has not received as much at-
tention in the past. We start from Stymne and Can-
cedda (2011), who used sequence models to pre-
dict compound merging and Fraser et al. (2012)
who, in addition, generalise over German inflec-
tion. Our new contributions are: (i) We project
features from the source language to support com-
pound merging predictions. As the source lan-
guage input is fluent, these features are more re-
liable than features derived from target language
SMT output. (ii) We reduce compound parts to
an underspecified representation which allows for
maximal generalisation. (iii) We present a detailed
manual evaluation methodology which shows that
we obtain improved compound translations.
We evaluated compound processing both on
held-out split data and in end-to-end SMT. We
show that using source language features increases
the accuracy of compound generation. Moreover,
we find more correct compounds than the base-
lines, and a considerable number of these com-
pounds are unseen in the training data. This is
largely due to the underspecified representation we
are using. Finally, we show that our approach im-
proves upon the previous work.
We discuss compound processing in SMT in
Section 2, and summarise related work in Sec-
tion 3. In Section 4 we present our method for
splitting compounds and reducing the component
words to an underspecified representation. The
merging to obtain German compounds is the sub-
ject of Section 5. We evaluate the accuracy of
compound prediction on held-out data in Section 6
and in end-to-end SMT experiments in Section 7.
We conclude in Section 8.
2 Dealing with Compounds in SMT
In German, two (or more) single words (usually
nouns or adjectives) are combined to form a
compound which is considered a semantic unit.
The rightmost part is referred to as the head while
all other parts are called modifiers. EXAMPLE (1)
lists different ways of joining simple words into
compounds: mostly, no modification is required
(A) or a filler letter is introduced (B). More rarely,
a letter is deleted (C), or transformed (D).
579
Werkzeug
Handel
Obst
Kiste
fruit
box
trading
tool Handelswerkzeug
ObstkisteWerkzeugkiste
Obsthandel fruittrading
tool
boxKiste
Werkzeug
Obst
Handel
splitting training
splitting training
testing re?combination
testing re?combination
Figure 1: Compound processing in SMT allows the synthesis of compounds unseen in the training data.
EXAMPLE (1)
(A) Haus+Boot = Hausboot (?house boat?)
(B) Ort+s+Zeit = Ortszeit (?local time?)
(C) Kirche-e+Turm = Kirchturm (?church tower?)
(D) Kriterium+Liste = Kriterienliste (?criteria list?)
German compounds are highly productive,
1
and
traditional SMT approaches often fail in the face
of such productivity. Therefore, special process-
ing of compounds is required for translation into
German, as many compounds will not (e.g. Haus-
boot, ?house boat?) or only rarely have been seen
in the training data.
2
In contrast, most compounds
consist of two (or more) simple words that occur
more frequently in the data than the compound as
a whole (e.g. Haus (7,975) and Boot (162)) and of-
ten, these compound parts can be translated 1-to-
1 into simple English words. Figure 1 illustrates
the basic idea of compound processing in SMT:
imagine, ?Werkzeug? (?tool?) occurred only as a
modifier of e.g. ?Kiste? (?box?) in the training
data, but the test set contains ?tool? as a simple
word or as the head of a compound. Splitting com-
pounds prior to translation model training enables
better access to the component translations and al-
lows for a high degree of generalisation. At test-
ing time, the English text is translated into the split
German representation, and only afterwards, some
sequences of simple words are (re-)combined into
(possibly unseen) compounds where appropriate.
This merging of compounds is much more chal-
lenging than the splitting, as it has to be applied
to disfluent MT output: i.e., compound parts may
not occur in the correct word order and even if they
do, not all sequences of German words that could
form a compound should be merged.
3 Related Work
Compound processing for translation into a com-
pounding language includes both compound split-
1
Most newly appearing words in German are compounds.
2
~30% of the word types and ~77% of the compound
types we identified in our training data occurred ? 3 times.
ting and merging, we thus report on previous ap-
proaches for both of these tasks.
In the past, there have been numerous attempts
to split compounds, all improving translation qual-
ity when translating from a compounding to a non-
compounding language. Several compound split-
ting approaches make use of substring corpus fre-
quencies in order to find the optimal split points of
a compound (e.g. Koehn and Knight (2003), who
allowed only ?(e)s? as filler letters). Stymne et al.
(2008) use Koehn and Knight?s technique, include
a larger list of possible modifier transformations
and apply POS restrictions on the substrings, while
Fritzinger and Fraser (2010) use a morphological
analyser to find only linguistically motivated sub-
strings. In contrast, Dyer (2010) presents a lattice-
based approach to encode different segmentations
of words (instead of finding the one-best split).
More recently, Macherey et al. (2011) presented
a language-independent unsupervised approach in
which filler letters and a list of words not to be split
(e.g., named entities) are learned using phrase ta-
bles and Levenshtein distance.
In contrast to splitting, the merging of com-
pounds has received much less attention in the
past. An early approach by Popovi?c et al. (2006)
recombines compounds using a list of compounds
and their parts. It thus never creates invalid Ger-
man compounds, but on the other hand it is limited
to the coverage of the list. Moreover, in some con-
texts a merging in the list may still be wrong, cf.
EXAMPLE (3) in Section 5 below. The approach
of Stymne (2009) makes use of a factored model,
with a special POS-markup for compound mod-
ifiers, derived from the POS of the whole com-
pound. This markup enables sound mergings of
compound parts after translation if the POS of the
candidate modifier (X-Part) matches the POS of
the candidate compound head (X): Inflations|N-
Part + Rate|N = Inflationsrate|N (?inflation rate?).
In Stymne and Cancedda (2011) the factored ap-
580
Gas|Traum      8.34Gastraum        3.74Gast|Raum    8.59
4) Disambiguation
...
...
...
Amerikanische Medien ...
Tim Baumeister besiegt ...
Der Gastraum des ...
0) Original Text
...
(S(NP(ADJA Amerikanische) (NN Medien)...))
...(S(NP(PN(NE Tim)(NE Baumeister))(VV besiegt)...))
...
(S(NP(ART Der) (NN Gastraum) (ART des)...))
1) Bitpar Parsed Text
> amerikanische
> Gastraum
> BaumeisterBau<NN>Meister<+NN>Baumeister<+NPROP>
Gast<NN>Raum<+NN>Gas<NN>Traum<+NN>
amerikanisch<+ADJ>
3) SMOR Analysis
amerikanische Medien ...ADJA NN
NETim Baumeister besiegt ...NE VV
ARTNNARTder Gastraum des ...
...
...
...
2) True Casing
Figure 2: Compound splitting pipeline 1) The original text is parsed with BITPAR to get unambiguous POS tags,
2) The original text is then true-cased using the most frequent casing for each word and BITPAR tags are added,
3) All words are analysed with SMOR, analyses are filtered using BITPAR tags (only bold-faced analyses are kept),
4) If several splitting options remain, the geometric mean of the word (part) frequencies is used to disambiguate them.
proach was extended to make use of a CRF se-
quence labeller (Lafferty et al., 2001) in order
to find reasonable merging points. Besides the
words and their POS, many different target lan-
guage frequency features were defined to train the
CRF. This approach can even produce new com-
pounds unseen in the training data, provided that
the modifiers occurred in modifier position of a
compound and heads occurred as heads or even as
simple words with the same inflectional endings.
However, as former compound modifiers were left
with their filler letters (cf. ?Inflations?), they can
not be generalised to compound heads or simple
words, nor can inflectional variants of compound
heads or simple words be created (e.g. if ?Rate?
had only been observed in nominative form in the
training data, the genitive ?Raten? could not be
produced). The underspecified representation we
are using allows for maximal generalisation over
word parts independent of their position of oc-
currence or inflectional realisations. Moreover,
their experiments were limited to predicting com-
pounds on held-out data; no results were reported
for using their approach in translation. In Fraser
et al. (2012) we re-implemented the approach of
Stymne and Cancedda (2011), combined it with
inflection prediction and applied it to a transla-
tion task. However, compound merging was re-
stricted to a list of compounds and parts. Our
present work facilitates more independent com-
bination. Toutanova et al. (2008) and Weller et
al. (2013) used source language features for target
language inflection, but to our knowledge, none of
these works applied source language features for
compound merging.
4 Step 1: Underspecified Representation
In order to enhance translation model accuracy,
it is reasonable to have similar degrees of mor-
phological richness between source and target lan-
guage. We thus reduce the German target lan-
guage training data to an underspecified represen-
tation: we split compounds, and lemmatise all
words (except verbs). All occurrences of simple
words, former compound modifiers or heads have
the same representation and can thus be freely
merged into ?old? and ?new? compounds after
translation, cf. Figure 1 above. So that we can later
predict the merging of simple words into com-
pounds and the inflection of the words, we store
all of the morphological information stripped from
the underspecified representation.
Note that erroneous over-splitting might make
the correct merging of compounds difficult
3
(or even impossible), due to the number of
correct decisions required. For example, it
requires only 1 correct prediction to recom-
bine ?Niederschlag|Menge? into ?Niederschlags-
menge? (?amount of precipitation?) but 3 for
the wrong split into ?nie|der|Schlag|Menge?
(?never|the|hit|amount?). We use the compound
splitter of Fritzinger and Fraser (2010), who have
shown that using a rule-based morphological anal-
yser (SMOR, Schmid et al. (2004)) drastically re-
duced the number of erroneous splits when com-
pared to the frequency-based approach of Koehn
and Knight (2003). However, we adapted it to
work on tokens: some words can, depending on
their context, either be interpreted as named enti-
ties or common nouns, e.g., ?Dinkelacker? (a Ger-
man beer brand or ?spelt|field?).
4
We parsed the
training data and use the parser?s decisions to iden-
tify proper names, see ?Baumeister? in Figure 2.
After splitting, we use SMOR to reduce words to
lemmas, keeping morphological features like gen-
der or number, and stripping features like case, as
illustrated for ?
?
Olexporteure? (?oil exporters?):
3
In contrast, they may not hurt translation quality in the
other direction, where phrase-based SMT is likely to learn
the split words as a phrase and thus recover from that error.
4
Note that Macherey et al. (2011) blocked splitting of
words which can be used as named entities, independent of
context, which is less general than our solution.
581
No. Feature Description Example
Experiment
SC T TR
1SC surface form of the word string: Arbeit<+NN><Fem><Sg> X X
2SC main part of speech of the word (from the parser) string: +NN X X
3SC word occurs in a bigram with the next word frequency: 0 X X
4SC word combined to a compound with the next word frequency: 10,000 X X X
5SC word occurs in modifier position of a compound frequency: 100,000 X X
6SC word occurs in a head position of a compound frequency: 10,000 X X
7SC word occurs in modifier position vs. simplex string: P>W (P= 5SC, W= 100,000) X
8SC word occurs in head position vs. simplex string: S<W (S= 6SC, W= 100,000) X
7SC+ word occurs in modifier position vs. simplex ratio: 10 (10**ceil(log10(5SC/W))) X X
8SC+ word occurs in head position vs. simplex ratio: 1 (10**ceil(log10(6SC/W))) X X
9N different head types the word can combine with number: 10,000 X X
Table 1: Target language CRF features for compound merging. SC = features taken from Stymne and Cancedda
(2011), SC+ = improved versions, N = new feature. Experiments: SC = re-implementation of Stymne and Cancedda (2011),
T= use full Target feature set, TR = use Target features, but only a Reduced set.
EXAMPLE (2)
?l<+NN><Neut><Sg> Exporteur<+NN> <Masc><Pl>
?l<NN>Exporteur<+NN><Masc><Nom><Pl>compound
headmodifier
While the former compound head (?Exporteure?)
automatically inherits all morphological features
of the compound as a whole, the features of the
modifier need to be derived from SMOR in an ad-
ditional step. We need to ensure that the repre-
sentation of the modifier is identical to the same
word when it occurs independently in order to ob-
tain full generalisation over compound parts.
5 Step 2: Compound Merging
After translation from English into the underspec-
ified German representation, post-processing is re-
quired to transform the output back into fluent,
morphologically fully specified German. First,
compounds need to be merged where appropriate,
e.g., ?Hausboote? (?house boats?):
Haus<+NN><Neut><Sg> + Boot<+NN><Neut><Pl>
? Haus<NN>Boot<+NN><Neut><Pl> (merged)
and second, all words need to be inflected:
Haus<NN>Boot<+NN><Neut><Acc><Pl>
? Hausbooten (inflected)
5.1 Target Language Features
To decide which words should be combined, we
follow Stymne and Cancedda (2011) who used
CRFs for this task. The features we derived from
the target language to train CRF models are listed
in Table 1. We adapted features No. 1-8 from
Stymne and Cancedda (2011). Then, we modi-
fied two features (7+8) and created a new feature
indicating the productivity of a modifier (9N).
5.2 Projecting Source Language Features
We also use new features derived from the English
source language input, which is coherent and flu-
ent. This makes features derived from it more reli-
able than the target language features derived from
disfluent SMT output. Moreover, source language
features might support or block merging decisions
in unclear cases, i.e., where target language fre-
quencies are not helpful, either because they are
very low or they have roughly equal frequency dis-
tributions when occurring in a compound (as mod-
ifier or head) vs. as a simple word.
In Table 2, we list three types of features:
1. Syntactic features: different English noun
phrase patterns that are aligned to German
compound candidate words (cf. 10E-13E)
2. The POS tag of the English word (cf. 14E)
3. Alignment features, derived from word
alignments (cf. 15E-18E)
The examples given in Table 2 (10E-13E) show
that English compounds often have 1-to-1 corre-
spondences to the parts of a German compound.
Knowing that two consecutive German simple
words are aligned to two English words of the
same noun phrase is a strong indicator that the
German words should be merged:
EXAMPLE (3)
should be merged:
ein erh?ohtes verkehrs aufkommen sorgt f?ur chaos
?an increased traffic volume causes chaos?
(S...(NP(DT An)(VN increased)(NN traffic)(NN volume))..)))
should not be merged:
f?ur die finanzierung des verkehrs aufkommen
?pay for the financing of transport?
(VP(V pay)(PP(IN for)(NP(NP(DT the)(NN financing))
(PP(IN of)(NP(NN transport)..))
In the compound reading of ?verkehr + aufkom-
men?, the English parse structure indicates that
the words aligned to ?verkehr? (?traffic?) and
582
No. Feature Description Type
10E
word and next word are aligned from a noun phrase in the English source sentence:
(NP(NN traffic)(NN accident))? Verkehr (?traffic?) + Unfall (?accident?)
true/false
11E
word and next word are aligned from a gerund construction in the English source sentence:
(NP(VBG developing)(NNS nations))? Entwicklung (?development?) + L?ander (?countries?)
true/false
12E
word and next word are aligned from a genitive construction in the English source sentence:
(NP(NP(DT the)(NN end))(PP(IN of)(NP(DT the)(NN year))? Jahr (?year?) + Ende(?end?)
true/false
13E
word and next word are aligned from an adjective noun construction in the English source sentence:
(NP (ADJ protective)(NNS measures))? Schutz (?protection?) + Ma?nahmen (?measures?)
true/false
14E print the POS of the corresponding aligned English word string
15E
word and next word are aligned 1-to-1 from the same word in the English source sentence, e.g.,
beef
?
?
Rind(?cow?)
Fleisch(?meat?)
true/false
16E like 15E, but the English word contains a dash, e.g., Nobel ? Prize
?
?
Nobel(?Nobel?)
Preis(?prize?)
true/false
17E like 15E, but also considering 1-to-n and n-to-1 links true/false
18E like 16E, but also considering 1-to-n and n-to-1 links true/false
Table 2: List of new source language CRF features for compound merging.
?aufkommen? (?volume?), are both nouns and
part of one common noun phrase, which is a strong
indicator that the two words should be merged
in German. In contrast, the syntactic relation-
ship between ?pay? (aligned to ?aufkommen?)
and ?transport? (aligned to ?verkehr?) is more dis-
tant
5
: merging is not indicated.
We also use the POS of the English words to
learn (un)usual combinations of POS, indepen-
dent of their exact syntactic structure (14E). Re-
consider EXAMPLE (3): NN+NN is a more com-
mon POS pair for compounds than V+NN.
Finally, the alignment features (15E-18E) pro-
mote the merging into compounds whose align-
ments indicate that they should not have been split
in the first place (e.g., Rindfleisch, 15E).
5.3 Compound Generation and Inflection
So far, we reported on how to decide which sim-
ple words are to be merged into compounds, but
not how to recombine them. Recall from EXAM-
PLE (1) that the modifier of a compound some-
times needs to be transformed, before it can be
combined with the head word (or next modifier),
e.g., ?Ort?+?Zeit? = ?Ortszeit? (?local time?).
We use SMOR to generate compounds from a
combination of simple words. This allows us to
create compounds with modifiers that never oc-
curred as such in the training data. Imagine that
?Ort? occurred only as compound head or as a
single word in the training data. Using SMOR, we
are still able to create the correct form of the mod-
ifier, including the required filler letter: ?Orts?.
This ability distinguishes our approach from pre-
5
Note that ?f?ur etwas aufkommen? (lit. ?for sth. arise?,
idiom.: ?to pay for sth.?) is an idiomatic expression.
vious approaches: Stymne and Cancedda (2011)
do not reduce modifiers to their base forms
6
(they
can only create new compounds when the modifier
occurred as such in the training data) and Fraser et
al. (2012) use a list for merging.
Finally, we use the system described in Fraser
et al. (2012) to inflect the entire text.
6 Accuracy of Compound Prediction
We trained CRF models on the parallel training
data (~40 million words)
7
of the EACL 2009
workshop on statistical machine translation
8
us-
ing different feature (sub)sets, cf. the ?Exper-
iment? column in Table 1 above. We exam-
ined the reliability of the CRF compound predic-
tion models by applying them to held-out data:
1. split the German wmt2009 tuning data set
2. remember compound split points
3. predict merging with CRF models
4. combine predicted words into compounds
5. calculate f-scores on how properly the
compounds were merged
Table 3 lists the CRF models we trained, together
with their compound merging accuracies on held-
out data. It can be seen that using more features
(SC?T?ST) is favourable in terms of precision
and overall accuracy and the positive impact of us-
ing source language features is clearer when only
reduced feature sets are used (TR vs. STR).
However, these accuracies only somewhat cor-
relate with SMT performance: while being trained
and tested on clean, fluent German language, the
6
They account for modifier transformations by using char-
acter n-gram features (cf.EXAMPLE (1)).
7
However, target language feature frequencies are derived
from the monolingual training data, ~146 million words.
8
http://www.statmt.org/wmt09
583
exp to be all correct wrong wrong not merging
precision recall f-score
merged merged merged merged merged wrong
SC 1,047 997 921 73 121 3 92.38% 88.13% 90.21%
T 1,047 979 916 59 128 4 93.56% 87.40% 90.38%
ST 1,047 976 917 55 126 4 93.95% 87.58% 90.66%
TR 1,047 893 836 52 204 5 93.62% 80.00% 86.27%
STR 1,047 930 866 58 172 6 93.12% 82.95% 87.74%
Table 3: Compound production accuracies of CRF models on held-out data: SC: re-implementation of Stymne
and Cancedda (2011); T: all target language features, including a new one (cf. Table 1); ST = all Source and Target language
features; TR: only a reduced set of target language features; STR: TR, plus all source language features given in Table 2.
exp BLEU SCORES #compounds found
mert.log BLEU RTS all ref new new*
RAW 14.88 14.25 1.0054 646 175 n.a. n.a.
UNSPLIT 15.86 14.74 0.9964 661 185 n.a. n.a.
SC 15.44 14.45 0.9870 882 241 47 8
T 15.56 14.32 0.9634 845 251 47 8
ST 15.33 14.51 0.9760 820 248 46 9
TR 15.24 14.26 0.9710 753 234 44 5
STR 15.37 14.61 0.9884 758 239 43 7
#compounds in reference text: 1,105 1,105 396 193
Table 4: SMT results. Tuning scores (mert.log) are on merged but uninflected data (except RAW).
RTS: length ratio; all: #compounds produced; ref: reference matches; new: unknown to parallel data; new*: unknown to
target language data. bold face indicates statistical significance wrt. the RAW baseline, SC, T and TR.
models will later be applied to disfluent SMT out-
put and might thus lead to different results there.
Stymne and Cancedda (2011) dealt with this by
noisifying the CRF training data: they translated
the whole data set using an SMT system that was
trained on the same data set. This way, the train-
ing data was less fluent than in its original format,
but still of higher quality than SMT output of un-
seen data. In contrast, we left the training data as
it was, but strongly reduced the feature set for CRF
model training (e.g., no more use of surface words
and POS tags, cf. TR and STR in Table 3) instead.
7 Translation Performance
We integrated our compound processing pipeline
into an end-to-end SMT system. Models were
trained with the default settings of the Moses SMT
toolkit, v1.0 (Koehn et al., 2007) using the data
from the EACL 2009 workshop on statistical ma-
chine translation. All compound processing sys-
tems are trained and tuned identically, except us-
ing different CRF models for compound predic-
tion. All training data was split and reduced
to the underspecified representation described in
Section 4. We used KenLM (Heafield, 2011) with
SRILM (Stolcke, 2002) to train a 5-gram language
model based on all available target language train-
ing data. For tuning, we used batch-mira with ?-
safe-hope? (Cherry and Foster, 2012) and ran it
separately for every experiment. We integrated the
CRF-based merging of compounds into each itera-
tion of tuning and scored each output with respect
to an unsplit and lemmatised version of the tuning
reference. Testing consists of:
1. translation into the split, underspecified
German representation
2. compound merging using CRF models
to predict recombination points
3. inflection of all words
7.1 SMT Results
We use 1,025 sentences for tuning and 1,026 sen-
tences for testing. The results are given in Table 4.
We calculate BLEU scores (Papineni et al., 2002)
and compare our systems to a RAW baseline (built
following the instructions of the shared task) and a
baseline very similar to Fraser et al. (2012), using
a lemmatised representation of words for decod-
ing, re-inflecting them after translation, but with-
out compound processing (UNSPLIT). Table 4
shows that only UNSPLIT and STR (source lan-
guage and a reduced set of target language fea-
tures) are significantly
9
improving over the RAW
baseline. They also significantly outperform all
other systems, except ST (full source and target
language feature set). The difference between STR
(14.61) and the UNSPLIT baseline (14.74) is not
statistically significant.
9
We used pair-wise bootstrap resampling with sample size
1000 and p-value 0.05, from: http://www.ark.cs.cmu.edu/MT
584
group ID example reference english UNSPLIT STR
lexically 1a: perfect match Inflationsrate Inflationsrate inflation rate 185 239
matches 1b: inflection wrong Rohstoffpreisen Rohstoffpreise raw material prices 40 44
the 2a: merging wrong Anwaltsbewegung Anw?altebewegung lawyers movement 5 9
reference 2b: no merging Polizei Chef Polizeichef police chief 101 54
correct 3a: compound Zentralbanken Notenbank central banks 92 171
translation 3b: no compound pflanzliche
?
Ole Speise?ol vegetable oils 345 291
wrong 4a: compound Haushaltsdefizite Staatshaushalts state budget 12 42
translation 4b: no compound Ansporn Linien Nebenlinien spur lines 325 255
Total number of compounds in reference text: 1,105 1,105
Table 5: Groups for detailed manual compound evaluation and results for UNSPLIT and STR.
reference English source UNSPLIT baseline STR
Teddyb?aren teddy bear 4b
Teddy tragen
1a
Teddyb?aren
(Teddy, to bear) (teddy bear)
Emissionsreduktion emissions reduction 3b
Emissionen Reduzierung
3a
Emissionsverringerung
(emissions, reducing) (emission decrease)
Geldstrafe fine 4b
sch?onen
3a
Bu?geld
(fine/nice) (monetary fine)
Tischtennis table tennis 2b
Tisch Tennis
4a
Spieltischtennis
(table, tennis) (play table tennis)
Kreditkartenmarkt credit-card market 2b
Kreditkarte Markt
4a
Kreditmarkt
(credit-card, market) (credit market)
Rotationstempo rotation rate 2b
Tempo Rotation
4a
Temporotation
(rate, rotation) (rate rotation)
Table 6: Examples of the detailed manual compound analysis for UNSPLIT and STR.
Compound processing leads to improvements at
the level of unigrams and as BLEU is dominated
by four-gram precision and length penalty, it does
not adequately reflect compound related improve-
ments. We thus calculated the number of com-
pounds matching the reference for each experi-
ment and verified whether these were known to
the training data. The numbers in Table 4 show
that all compound processing systems outperform
both baselines in terms of finding more exact refer-
ence matches and also more compounds unknown
to the training data. Note that STR finds less ref-
erence matches than e.g. T or ST, but it also pro-
duces less compounds overall, i.e. it is more pre-
cise when producing compounds.
However, as compounds that are correctly com-
bined but poorly inflected are not counted, this is
only a lower bound on true compounding perfor-
mance. We thus performed two additional manual
evaluations and show that the quality of the com-
pounds (Section 7.2), and the human perception of
translation quality is improving (Section 7.3).
7.2 Detailed Evaluation of Compounds
This evaluation focuses on how compounds in the
the reference text have been translated.
10
We:
10
In another evaluation, we investigated the 519 com-
pounds that our system produced but which did not match
the reference: 367 were correct translations of the English,
1. manually identify compounds in German
reference text (1,105 found)
2. manually perform word alignment of these
compounds to the English source text
3. project these English counterparts of com-
pounds in the reference text to the decoded
text using the ??print-alignment-info? flag
4. manually annotate the resulting tuples, us-
ing the categories given in Table 5
The results are given in the two rightmost columns
of Table 5: besides a higher number of reference
matches (cf. row 1a), STR overall produces more
compounds than the UNSPLIT baseline, cf. rows
2a, 3a and 4a. Indirectly, this can also be seen from
the low numbers of STR in category 2b), where
the UNSPLIT baseline produces much more (101
vs. 54) translations that lexically match the refer-
ence without being a compound. While the 171
compounds of STR of category 3a) show that our
system produces many compounds that are correct
translations of the English, even though not match-
ing the reference (and thus not credited by BLEU),
the compounds of categories 2a) and 4a) contain
examples where we either fail to reproduce the
correct compound or over-generate compounds.
We give some examples in Table 6: for ?teddy
bear?, the correct German word ?Teddyb?aren? is
87 contained erroneous lexemes and 65 were over-mergings.
585
missing in the parallel training data and instead
of ?B?ar? (?bear?), the baseline selected ?tragen?
(?to bear?). Extracting all words containing the
substring ?b?ar? (?bear?) from the original parallel
training data and from its underspecified split
version demonstrates that our approach is able
to access all occurrences of the word. This leads
to higher frequency counts and thus enhances
the probabilities for correct translations. We can
generalise over 18 different word types containing
?bear? (e.g. ?polar bears?, ?brown bears?, ?bear
skin?, ?bear fur?) to obtain only 2:
occurrences in raw training data: B?ar (19), B?aren
(26), B?arendienst (42), B?arenfarmen (1), B?arenfell (2),
B?arengalle(1), B?arenhaut (1), B?arenmarkt (1), Braunb?ar
(1), Braunb?aren (3), Braunb?arengebiete (1), Braunb?ar-
Population (1), Eisb?aren(18), Eisb?arenpopulation (2),
Eisb?arenpopulationen (1), Schwarzb?ar (1), Schwarzb?aren (1)
?b?ar? occurring in underspecified split data:
B?ar<+NN><Masc><Sg> (94)
B?ar<+NN><Masc><Pl> (29)
?Emissionsverringerung? (cf. Table 6) is a typ-
ical example of group 3a): a correctly translated
compound that does not lexically match the ref-
erence, but which is semantically very similar to
the reference. The same applies for ?Bu?geld?,
a synonym of ?Geldstrafe?, for which the UN-
SPLIT baseline selected ?sch?onen? (?fine, nice?)
instead. Consider also the wrong compound pro-
ductions, e.g. ?Tischtennis? is combined with
the verb ?spielen? (?to play?) into ?Spieltischten-
nis?. In contrast, ?Kreditmarkt? dropped the mid-
dle part ?Karte? (?card?), and in the case of ?Tem-
porotation?, the head and modifier of the com-
pound are switched.
7.3 Human perception of translation quality
We presented sentences of the UNSPLIT baseline
and of STR in random order to two native speak-
ers of German and asked them to rank the sen-
tences according to preference. In order to pre-
vent them from being biased towards compound-
bearing sentences, we asked them to select sen-
tences based on their native intuition, without re-
vealing our focus on compound processing.
Sentences were selected based on source lan-
guage sentence length: 10-15 words (178 sen-
tences), of which either the reference or our
system had to contain a compound (95 sen-
tences). After removing duplicates, we ended up
with 84 sentences to be annotated in two subse-
(a) Fluency: without reference sentence
? = 0.3631
person 1
STR UNSPLIT equal
p
e
r
s
o
n
2
STR 24 6 7 37
UNSPLIT 5 16 9 30
equal 6 2 9 17
35 24 25 84
(b) Adequacy: with reference sentence
? = 0.4948
person 1
STR UNSPLIT equal
p
e
r
s
o
n
2
STR 23 4 5 32
UNSPLIT 4 21 7 32
equal 5 3 12 20
32 28 24 84
Table 7: Human perception of translation quality.
quent passes: first, without being given the refer-
ence sentence (approximating fluency), then, with
the reference sentence (approximating adequacy).
The results are given in Table 7. Both annotators
preferred more sentences of our system overall,
but the difference is clearer for the fluency task.
8 Conclusion
Compounds require special attention in SMT, es-
pecially when translating into a compounding lan-
guage. Compared with the baselines, all of our ex-
periments that included compound processing pro-
duced not only many more compounds matching
the reference exactly, but also many compounds
that did not occur in the training data. Taking
a closer look, we found that some of these new
compounds could only be produced due to the un-
derspecified representation we are using, which al-
lows us to generalise over occurrences of simple
words, compound modifiers and heads. Moreover,
we demonstrated that features derived from the
source language are a valuable source of informa-
tion for compound prediction: experiments were
significantly better compared with contrastive ex-
periments without these features. Additional man-
ual evaluations showed that compound processing
leads to improved translations where the improve-
ment is not captured by BLEU.
Acknowledgements
This work was supported by Deutsche For-
schungsgemeinschaft grants Models of Mor-
phosyntax for Statistical Machine Translation
(Phase 2) and Distributional Approaches to Se-
mantic Relatedness. We thank the anonymous re-
viewers for their comments and the annotators.
586
References
Colin Cherry and George Foster. 2012. Batch Tun-
ing Strategies for Statistical Machine Translation. In
HLT-NAACL?12: Proceedings of the Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, volume 12, pages 34?35. Association
for Computational Linguistics.
Chris Dyer. 2010. A Formal Model of Ambiguity and
its Applications in Machine Translation. Phd disser-
tation, University of Maryland, USA.
Alexander Fraser, Marion Weller, Aoife Cahill, and Fa-
bienne Cap. 2012. Modeling Inflection and Word
Formation in SMT. In EACL?12: Proceedings of the
13th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 664?
674. Association for Computational Linguistics.
Fabienne Fritzinger and Alexander Fraser. 2010. How
to Avoid Burning Ducks: Combining Linguistic
Analysis and Corpus Statistics for German Com-
pound Processing. In Proceedings of the Fifth Work-
shop on Statistical Machine Translation, pages 224?
234. Association for Computational Linguistics.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
Edinburgh, UK, July. Association for Computational
Linguistics.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In EACL ?03:
Proceedings of the 10th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 187?193, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In ACL?07: Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics, Demonstration Session, pages 177?180. Asso-
ciation for Computational Linguistics.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Fields: Prob-
abilistic Models for Segmenting and Labeling Se-
quence Data. In ICML?01: Proceedings of the 18th
International Conference on Machine Learning.
Klaus Macherey, Andrew M. Dai, David Talbot,
Ashok C. Popat, and Franz Och. 2011. Language-
independent Compound Splitting with Morpholog-
ical Operations. In ACL ?11: Proceedings of the
49th annual meeting of the Association for Compu-
tational Linguistics, pages 1395?1404. Association
for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A Method for Automatic
Evaluation of Machine Translation. In ACL?02:
Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics, pages 311?
318. Association for Computational Linguistics.
Maja Popovi?c, Daniel Stein, and Hermann Ney. 2006.
Statistical Machine Translation of German Com-
pound Words. In FinTAL?06: Proceedings of the
5th International Conference on Natural Language
Processing, pages 616?624. Springer Verlag.
Helmut Schmid, Arne Fitschen, and Ulrich Heid.
2004. SMOR: A German Computational Morphol-
ogy Covering Derivation, Composition and Inflec-
tion. In LREC ?04: Proceedings of the 4th Confer-
ence on Language Resources and Evaluation, pages
1263?1266.
Andreas Stolcke. 2002. SRILM ? an Extensible Lan-
guage Modelling Toolkit. In ICSLN?02: Proceed-
ings of the international conference on spoken lan-
guage processing, pages 901?904.
Sara Stymne and Nicola Cancedda. 2011. Productive
Generation of Compound Words in Statistical Ma-
chine Translation. In EMNLP?11: Proceedings of
the 6th Workshop on Statistical Machine Transla-
tion and Metrics MATR of the conference on Em-
pirical Methods in Natural Language Processing,
pages 250?260. Association for Computational Lin-
guistics.
Sara Stymne, Maria Holmqvist, and Lars Ahrenberg.
2008. Effects of Morphological Analysis in Transla-
tion between German and English. In ACL?08: Pro-
ceedings of the 3rd workshop on statistical machine
translation of the 46th annual meeting of the Associ-
ation for Compuational Linguistics, pages 135?138.
Association for Computational Linguistics,.
Sara Stymne. 2009. A Comparison of Merging Strate-
gies for Translation of German Compounds. In
EACL ?09: Proceedings of the Student Research
Workshop of the 12th conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 61?69. Association for Computa-
tional Linguistics.
Kristina Toutanova, Hisami Suzuki, and Achim Ruopp.
2008. Applying Morphology Generation Models to
Machine Translation. In ACL?08: Proceedings of
the 46th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 514?522. Association for Computational
Linguistics.
Marion Weller, Alexander Fraser, and Sabine
Schulte im Walde. 2013. Using Subcatego-
rization Knowledge to Improve Case Prediction for
Translation to German. In ACL?13: Proceedings
of the 51st Annual Meeting of the Association
for Computational Linguistics, pages 593?603.
Association for Computational Linguistics.
587
Squibs and Discussions
Measuring Word Alignment Quality for Statistical
Machine Translation
Alexander Fraser?
University of Southern California
Daniel Marcu?
University of Southern California
Automatic word alignment plays a critical role in statistical machine translation. Unfortu-
nately, the relationship between alignment quality and statistical machine translation perform-
ance has not been well understood. In the recent literature, the alignment task has frequently
been decoupled from the translation task and assumptions have been made about measuring
alignment quality for machine translation which, it turns out, are not justified. In particular,
none of the tens of papers published over the last five years has shown that significant decreases
in alignment error rate (AER) result in significant increases in translation performance. This
paper explains this state of affairs and presents steps towards measuring alignment quality in a
way which is predictive of statistical machine translation performance.
1. Introduction
Automatic word alignment (Brown et al 1993) is a vital component of all statistical
machine translation (SMT) approaches. There were a number of research papers pre-
sented from 2000 to 2005 at ACL, NAACL, HLT, COLING, WPT03, WPT05, and so
forth, outlining techniques for attempting to increase word alignment quality. Despite
this high level of interest, none of these techniques has been shown to result in a large
gain in translation performance as measured by BLEU (Papineni et al 2001) or any
other metric. We find this lack of correlation between previous word alignment quality
metrics and BLEU counterintuitive, because we and other researchers have measured
this correlation in the context of building SMT systems that have benefited from using
the BLEU metric in improving performance in open evaluations such as the NIST
evaluations.1
We confirm experimentally that previous metrics do not predict BLEU well and
develop a methodology for measuring alignment quality that is predictive of BLEU. We
? USC/ISI - Natural Language Group, 4676 Admiralty Way, Suite 1001, Marina del Rey, CA 90292-6601.
E-mail: fraser@isi.edu, marcu@isi.edu.
1 Because in our experiments we use BLEU to compare the performance of systems built using a common
framework where the only difference is the word alignment, we make no claims about the utility of BLEU
for measuring translation quality in absolute terms, nor its utility for comparing two completely different
MT systems.
? 2007 Association for Computational Linguistics
Computational Linguistics Volume 33, Number 3
also show that alignment error rate (AER) is not correctly derived from F-Measure and
is therefore unlikely to be useful as a metric.
2. Experimental Methodology
2.1 Data
To build an SMT system we require a bitext and a word alignment of that bitext, as well
as language models built from target language data. In all of our experiments, we will
hold the bitext and target language resources constant, and only vary how we construct
the word alignment.
The gold standard word alignment sets we use have been manually annotated
using links between words showing translational correspondence. Links which must
be present in a hypothesized alignment are called Sure links. Some of the alignment
sets also have links which are not Sure links but are Possible links (Och and Ney 2003).
Possible links which are not Sure2 may be present but need not be present.
We evaluate the translation performance of SMT systems by translating a held-
out translation test set and measuring the BLEU score of our hypothesized translations
against one or more reference translations. We also have an additional held-out transla-
tion set, the development set, which is employed by the MT system to train the weights
of its log-linear model to maximize BLEU (Och 2003). We work with data sets for three
different language pairs, examining French to English, Arabic to English, and Romanian
to English translation tasks.
The training data for the French/English data set is taken from the LDC Canadian
Hansard data set, from which the word aligned data (presented in Och and Ney 2003)
was also taken. The English side of the bitext is 67.4 million words. We used a separate
Canadian Hansard data set (released by ISI) as the source of the translation test set and
development set. We evaluate two different tasks using this data, a medium task where
1/8 of the data (8.4 million English words) is used as the fixed bitext, and a large task
where all of the data is used as the fixed bitext. The 484 sentences in the gold standard
word alignments have 4,376 Sure links and 19,222 Possible links.
The Arabic/English training corpus is the data used for the NIST 2004 machine
translation evaluation.3 The English side of the bitext is 99.3 million words. The trans-
lation development set is the ?NIST 2002 Dry Run,? and the test set is the ?NIST 2003
evaluation set.? We have annotated gold standard alignments for 100 parallel sentences
using Sure links, following the Blinker guidelines (Melamed 1998), which call for Sure
links only (there were 2,154 Sure links). Here we also examine a medium task using 1/8
of the data (12.4 million English words) and a large task using all of the data.
The Romanian/English training data was used for the tasks on Romanian/English
alignment at WPT03 (Mihalcea and Pederson 2003) and WPT05 (Martin, Mihalcea,
and Pedersen 2005). We carefully removed two sections of news bitext to use as the
translation development and test sets. The English side of the training corpus is 964,000
words. The alignment set is the first 148 annotated sentences used for the 2003 task
(there were 3,181 Sure links).
2 Sure links are by definition also Possible.
3 http://www.nist.gov/speech/tests/summaries/2004/mt04.htm.
294
Fraser and Marcu Measuring Word Alignment Quality for Statistical Machine Translation
2.2 Measuring Translation Performance Changes Caused By Alignment
In phrased-based SMT (Koehn, Och, and Marcu 2003) the knowledge sources which
vary with the word alignment are the phrase translation lexicon (which maps source
phrases to target phrases using counts from the word alignment) and some of the word
level translation parameters (sometimes called lexical smoothing). However, many
knowledge sources do not vary with the final word alignment, such as rescoring with
IBM Model 1, n-gram language models, and the length penalty. In our experiments,
we use a state-of-the-art phrase-based system, similar to Koehn, Och, and Marcu.
The weights of the different knowledge sources in the log-linear model used by our
system are trained using Maximum BLEU (Och 2003), which we run for 25 iterations
individually for each system. Two language models are used, one built using the target
language training data and the other built using additional news data.
2.3 Generating Alignments of Varying Quality
We have observed in the past that generative models used for statistical word alignment
create alignments of increasing quality as they are exposed to more data. The intuition
behind this is simple; as more co-occurrences of source and target words are observed,
the word alignments are better. If we wish to increase the quality of a word alignment,
we allow the alignment process access to extra data, which is used only during the
alignment process and then removed. If we wish to decrease the quality of a word
alignment, we divide the bitext into pieces and align the pieces independently of one
another, finally concatenating the results together.
To generate word alignments we use GIZA++ (Och and Ney 2003), which im-
plements both the IBM Models of Brown et al (1993) and the HMM model (Vogel,
Ney, and Tillmann 1996). We use Model 1, HMM, and Model 4, in that order. The
output of these models is an alignment of the bitext which projects one language to
another. GIZA++ is run end-to-end twice. In one case we project the source language
to the target language, and in the other we project the target language to the source
language. The output of GIZA++ is then post-processed using the three ?symmetriza-
tion heuristics? described in Och and Ney (2003). We evaluate our approaches using
these heuristics because we would like to account for alignments generated in different
fashions. These three heuristics were used as the baselines in virtually all recent work
on automatic word alignment, and most of the best SMT systems use these techniques
as well.
When applying the union symmetrization heuristic, we take the transitive closure
of the bipartite graph created, which results in fully connected components indicating
translational correspondence.4 Each of the presented alignments are equivalent from a
translational correspondence perspective and the first two will be mapped to the third
4 We have no need to do this for the ?refined? and ?intersection? heuristics, because they only produce
alignments in which the components are fully connected.
295
Computational Linguistics Volume 33, Number 3
in order to ensure consistency between the number of links an alignment has and the
translational equivalences licensed by that alignment.
A B C
D















E







A B C
D















E














A B C
D















E














3. Word Alignment Quality Metrics
3.1 Alignment Error Rate is Not a Useful Measure
We begin our study of metrics for word alignment quality by testing AER (Och and Ney
2003). AER requires a gold standard manually annotated set of Sure links and Possible
links (referred to as S and P). Given a hypothesized alignment consisting of the link set
A, three measures are defined:
Precision(A, P) =
|P ? A|
|A| (1)
Recall(A, S) =
|S ? A|
|S| (2)
AER(A, P, S) = 1 ? |P ? A|+ |S ? A||S|+ |A| (3)
In our graphs, we will present 1 ? AER so that we have an accuracy measure.
We created alignments of varying quality for the medium French/English training
set. We broke the data into separate pieces corresponding to 1/16, 1/8, 1/4, and 1/2
of the original data to generate degraded alignments, and we used 2, 4, and 8 times
the original data to generate enhanced alignments. For the ?fractional? alignments we
report the average AER of the pieces.5
The graph in Figure 1 shows the correlation of 1 ? AER with BLEU. High correlation
would look like a line from the bottom left corner to the top right corner. As can be seen
by looking at the graph, there is low correlation between 1 ? AER and the BLEU score. A
concise mathematical description of correlation is the coefficient of determination (r2),
5 For example, for 1/16, we perform 16 pairs of alignments, each of which includes the full gold standard
text, and another 16 pairs of alignments without the gold standard text. We then apply the
symmetrization heuristics to these pairs. We use the symmetrized alignments including the text from the
gold standard set to measure AER and we concatenate those not including the gold standard text to build
SMT systems for measuring BLEU.
296
Fraser and Marcu Measuring Word Alignment Quality for Statistical Machine Translation
Figure 1
French 1 ? AER versus BLEU, r2 = 0.16.
which is the square of the Pearson product-moment correlation coefficient (r). Here,
r2 = 0.16, which is low.
The correlation is low because of a significant shortcoming in the mathematical
formulation of AER, which to our knowledge has not been previously reported. Och
and Ney (2003) state that AER is derived from F-Measure. But AER does not share a
very important property of F-Measure, which is that unbalanced precision and recall
are penalized, where S ? P (i.e., when we make the Sure versus Possible distinction).6
We will show this using an example.
We first define the measure ?F-Measure with Sure and Possible? using Och and
Ney?s Precision and Recall formulae together with the standard F-Measure formula
(van Rijsbergen 1979). In the F-Measure formula (4) there is a parameter ? which sets
the trade-off between Precision and Recall. When an equal trade-off is desired, ? is set
to 0.5.
F-Measure with Sure and Possible(A, P, S, ?) = 1
?
Precision(A,P) +
(1??)
Recall(A,S)
(4)
We compare two hypothesized alignments where |A|, the number of hypothesized
alignment links, is the same, for instance, |A| = 100. Let |S| = 100. In the first case, let
|P ? A| = 50 and |S ? A| = 50. Precision is 0.50 and Recall is 0.50. In the second case, let
|P ? A| = 75 and |S ? A| = 25. Precision is 0.75 and Recall is 0.25.
The basic property of F-Measure, if we set ? equal to 0.5, is that unbalanced
precision and recall should be penalized. The first hypothesized alignment has an
F-Measure with Sure and Possible score of 0.50, whereas the second has a worse score,
0.375.
However, if we substitute the relevant values into the formula for AER (Equa-
tion (3)), we see that 1 ? AER for both of the hypothesized alignments is 0.5. Therefore
AER does not share the property of F-Measure (with ? = 0.5) that unbalanced precision
and recall are penalized. Because of this, it is possible to maximize AER by favoring
precision over recall, which can be done by simply guessing very few alignment links.
Unfortunately, when S ? P, this leads to strong biases, which makes AER not useful as
a metric.
6 Note that if S = P then AER reduces to balanced F-Measure.
297
Computational Linguistics Volume 33, Number 3
Figure 2
French F-Measure with Sure and Possible ? = 0.5 versus BLEU, r2 = 0.20.
Goutte, Yamada, and Gaussier (2004) previously observed that AER could be un-
fairly optimized by using a bias toward precision which was unlikely to improve the
usefulness of the alignments. Possible problems with AER were discussed at WPT 2003
and WPT 2005.
Examining the graph in Figure 2, we see that F-Measure with Sure and Possible has
some predictive power for the data points generated using a single heuristic, but the
overall correlation is still low, r2 = 0.20. We need a measure that predicts BLEU without
having a dependency on the way the alignments are generated.
3.2 Balanced F-Measure is Better, but Still Inadequate
We wondered whether the low correlation was caused by the Sure and Possible dis-
tinction. We reannotated the first 110 sentences of the French test set using the Blinker
guidelines (there were 2,292 Sure links). We define F-Measure without the Sure versus
Possible distinction (i.e., all links are Sure) in Equation (5), and set ? = 0.5. This measure
has been extensively used with other word alignment test sets. Figure 3 shows the
results. Correlation is higher: r2 = 0.67.
F-Measure(A, S, ?) = 1
?
Precision(A,S) +
(1??)
Recall(A,S)
(5)
3.3 Varying the Trade-Off Between Precision and Recall Works Well
We then hypothesized that the trade-off between precision and recall is important. This
is controlled in both formulae by the constant ?. We search ? = 0.1, 0.2, ..., 0.9. The best
results are: ? = 0.1 for the original annotation annotated with Sure and Possible (see
Figure 4), and ? = 0.4 for the first 110 sentences as annotated by us (see Figure 5).7 The
relevant r2 scores were 0.80 and 0.85, respectively. With a good ? setting, we are able
7 We also checked the first 110 sentences using the original annotation to ensure that the differences
observed were not an effect of restricting our annotation to these sentences.
298
Fraser and Marcu Measuring Word Alignment Quality for Statistical Machine Translation
Figure 3
French F-Measure ? = 0.5 versus BLEU, r2 = 0.67.
Figure 4
French F-Measure with Sure and Possible ? = 0.1 versus BLEU, r2 = 0.80.
Figure 5
French F-Measure ? = 0.4 versus BLEU, r2 = 0.85.
to predict the machine translation results reasonably well. For the original annotation,
recall is very highly weighted, whereas for our annotation, recall is still more important
than precision.8 Our results also suggest that better correlation will be achieved when
using Sure-only annotation than with Sure and Possible annotation.
8 ? less than 0.5 weights recall higher, whereas ? greater than 0.5 weights precision higher; see the
F-Measure formulae.
299
Computational Linguistics Volume 33, Number 3
Figure 6
Arabic F-Measure ? = 0.1 versus BLEU, r2 = 0.93.
Figure 7
Large French F-Measure ? = 0.4 (110 sentences) versus BLEU, r2 = 0.64.
We then tried the medium Arabic training set. Results are shown in Figure 6, the
best setting of ? = 0.1, and r2 = 0.93. F-Measure is effective in predicting machine
translation performance for this set.
We also tried the larger tasks, where we can only decrease alignment quality, as
we have no additional data. For the large French/English corpus the best results are
with ? = 0.2 for the original annotation of 484 sentences and ? = 0.4 for the new
Figure 8
Large Arabic F-Measure ? = 0.1 (100 sentences) versus BLEU, r2 = 0.90.
300
Fraser and Marcu Measuring Word Alignment Quality for Statistical Machine Translation
annotation of 110 sentences with only Sure links (see Figure 7). Relevant r2 scores were
0.62 and 0.64, respectively. Disappointingly, our measures are not able to fully explain
MT performance for the large French/English task.
For the large Arabic/English corpus, the results were better: the best correlation
was at ? = 0.1, for which r2 = 0.90 (see Figure 8). We can predict MT performance for
this set. It is worth noting that the Arabic/English translation task and data set has
been tested in conjunction with our translation system over a long period, but the
French/English translation task and data has not. As a result, there may be hidden
factors that affect the performance of our MT system, which only appear in conjunction
with the large French/English task.
One well-studied task on a smaller data set is the Romanian/English shared word
alignment task from the Workshop on Parallel Text at ACL 2005 (Martin, Mihalcea, and
Pedersen 2005). We only decreased alignment quality and used 5 data points for each
symmetrization heuristic due to the small bitext. The best setting of ? was ? = 0.2, for
which r2 = 0.94, showing that F-Measure is again effective in predicting BLEU.
4. Conclusion
We have presented an empirical study of the use of simple evaluation metrics based on
gold standard alignment of a small number of sentences to predict machine translation
performance. Based on our experiments we can now draw the following conclusions:
1. When S ? P, AER does not share the important property of F-Measure
that unequal precision and recall are penalized, making it easy to obtain
good AER scores by simply guessing fewer alignment links. As a result
AER is a misleading metric that should no longer be used.
2. Good correlation was obtained for the medium French and Arabic data
sets, the large Arabic data set, and the small Romanian data set. We have
explained most of the effect of alignment quality on these sets, and if we
are given the F-Measure of a hypothesized word alignment for the bitext,
we can make a reasonable prediction as to what the resulting BLEU score
will be.
3. We have only partially explained the effect of alignment quality on BLEU
for the large French data set, and further investigation is warranted.
4. We recommend using the Blinker guidelines as a starting point for new
alignment annotation efforts, and that Sure-only annotation be used. If a
larger gold standard is available and was already annotated using the Sure
versus Possible distinction, this is likely to have only slightly worse results.
Although we have addressed measuring alignment quality for phrasal SMT, similar
work is now required to see how to measure alignment quality for other settings of
machine translation and for other tasks. For an evaluation campaign the organizers
should pick a specific task, such as improving phrasal SMT, and calculate an appropriate
? to be used. Individual researchers working on the same phrasal SMT tasks as those
reported here (or on very similar tasks) could use the values of ? we calculated.
Our work invalidates some of the conclusions of recent alignment work which
presented only evaluations based on metrics like AER or balanced F-Measure, and
explains the lack of correlation in the few works which presented both such a metric
301
Computational Linguistics Volume 33, Number 3
and final MT results. A good example of the former are our own results (Fraser and
Marcu 2005). The work presented there had the highest balanced F-Measure scores for
the Romanian/English WPT05 shared task, but based on the findings here it is possible
that a different algorithm tuned for the correct criterion would have had better MT
performance. Other work includes many papers working on alignment models where
words are allowed to participate in a maximum of one link. These models generally
have higher precision and lower recall than IBM Model 4 symmetrized using the ?Re-
fined? or ?Union? heuristics. Recall that in Section 3.1 we showed that AER is broken in
a way that favors precision. It is therefore likely that the results reported in these papers
are affected by the AER bias and that the corresponding improvements in AER score do
not correlate with increases in phrasal SMT performance.
We suggest comparing alignment algorithms by measuring performance in an
identified final task such as machine translation. F-Measure with an appropriate setting
of ? will be useful during the development process of new alignment models, or as a
maximization criterion for discriminative training of alignment models (Cherry and Lin
2003; Ayan, Dorr, and Monz 2005; Ittycheriah and Roukos 2005; Liu, Liu, and Lin 2005;
Fraser and Marcu 2006; Lacoste-Julien et al 2006; Moore, Yih, and Bode 2006).
Acknowledgments
This work was supported by DARPA-ITO
grant NN66001-00-1-9814, NSF grant
IIS-0326276, and the DARPA GALE Program.
We thank USC High Performance
Computing & Communications.
References
Ayan, Necip Fazil, Bonnie J. Dorr, and
Christof Monz. 2005. Neuralign:
Combining word alignments using neural
networks. In Proceedings of HLT-EMNLP,
pages 65?72, Vancouver, Canada.
Brown, Peter F., Stephen A. Della Pietra,
Vincent J. Della Pietra, and R. L. Mercer.
1993. The mathematics of statistical
machine translation: Parameter
estimation. Computational Linguistics,
19(2):263?311.
Cherry, Colin and Dekang Lin. 2003. A
probability model to improve word
alignment. In Proceedings of ACL,
pages 88?95, Sapporo, Japan.
Fraser, Alexander and Daniel Marcu. 2005.
Isi?s participation in the Romanian-English
alignment task. In Proceedings of the ACL
Workshop on Building and Using Parallel
Texts, pages 91?94, Ann Arbor, MI.
Fraser, Alexander and Daniel Marcu. 2006.
Semi-supervised training for word
alignment. In Proceedings of COLING-ACL,
pages 769?776, Sydney, Australia.
Goutte, Cyril, Kenji Yamada, and Eric
Gaussier. 2004. Aligning words using
matrix factorisation. In Proceedings of ACL,
pages 502?509, Barcelona, Spain.
Ittycheriah, Abraham and Salim Roukos.
2005. A maximum entropy word aligner
for Arabic-English machine translation. In
Proceedings of HLT-EMNLP, pages 89?96,
Vancouver, Canada.
Koehn, Philipp, Franz J. Och, and Daniel
Marcu. 2003. Statistical phrase-based
translation. In Proceedings of HLT-NAACL,
pages 127?133, Edmonton, Canada.
Lacoste-Julien, Simon, Dan Klein,
Ben Taskar, and Michael Jordan.
2006. Word alignment via quadratic
assignment. In Proceedings
of HLT-NAACL, pages 112?119,
New York, NY.
Liu, Yang, Qun Liu, and Shouxun Lin.
2005. Log-linear models for word
alignment. In Proceedings of ACL,
pages 459?466, Ann Arbor, MI.
Martin, Joel, Rada Mihalcea, and
Ted Pedersen. 2005. Word
alignment for languages with
scarce resources. In Proceedings of the
ACL Workshop on Building and
Using Parallel Texts, pages 65?74,
Ann Arbor, MI.
Melamed, I. Dan. 1998. Manual annotation
of translational equivalence: The
Blinker project. Technical Report 98?07,
Institute for Research in Cognitive
Science, University of Pennsylvania,
Philadelphia, PA.
Mihalcea, Rada and Ted Pederson. 2003.
An evaluation exercise for word
alignment. In Proceedings of the
HLT-NAACL Workshop on Building
and Using Parallel Texts, pages 1?10,
Edmonton, Canada.
302
Fraser and Marcu Measuring Word Alignment Quality for Statistical Machine Translation
Moore, Robert C., Wen-Tau Yih, and Andreas
Bode. 2006. Improved discriminative
bilingual word alignment. In Proceedings
of COLING-ACL, pages 513?520,
Sydney, Australia.
Och, Franz J. 2003. Minimum error rate
training in statistical machine translation.
In Proceedings of ACL, pages 160?167,
Sapporo, Japan.
Och, Franz J. and Hermann Ney. 2003.
A systematic comparison of various
statistical alignment models.
Computational Linguistics, 29(1):19?51.
Papineni, Kishore A., Salim Roukos,
Todd Ward, and Wei-Jing Zhu.
2001. BLEU: A method for
automatic evaluation of machine
translation. Technical Report
RC22176 (W0109-022), IBM Research
Division, Thomas J. Watson
Research Center, Yorktown
Heights, NY.
van Rijsbergen, Keith. 1979. Information
Retrieval. Butterworth-Heinemann,
Newton, MA.
Vogel, Stephan, Hermann Ney, and
Christoph Tillmann. 1996. HMM-based
word alignment in statistical translation.
In Proceedings of COLING, pages 836?841,
Copenhagen, Denmark.
303

Knowledge Sources for Constituent Parsing
of German, a Morphologically Rich and
Less-Configurational Language
Alexander Fraser?
Institute for NLP, University of Stuttgart
Helmut Schmid??
Institute for NLP, University of Stuttgart
Richa?rd Farkas?
Institute for NLP, University of Stuttgart
Renjing Wang?
Institute for NLP, University of Stuttgart
Hinrich Schu?tze?
Institute for NLP, University of Stuttgart
We study constituent parsing of German, a morphologically rich and less-configurational
language. We use a probabilistic context-free grammar treebank grammar that has been adapted
to the morphologically rich properties of German by markovization and special features added
to its productions. We evaluate the impact of adding lexical knowledge. Then we examine both
monolingual and bilingual approaches to parse reranking. Our reranking parser is the new state
of the art in constituency parsing of the TIGER Treebank. We perform an analysis, concluding
with lessons learned, which apply to parsing other morphologically rich and less-configurational
languages.
? Institute for Natural Language Processing, University of Stuttgart, Pfaffenwaldring 5b, 70569 Stuttgart,
Germany. E-mail: fraser@ims.uni-stuttgart.de.
?? Institute for Natural Language Processing, University of Stuttgart, Pfaffenwaldring 5b, 70569 Stuttgart,
Germany. E-mail: schmid@ims.uni-stuttgart.de.
? Institute for Natural Language Processing, University of Stuttgart, Pfaffenwaldring 5b, 70569 Stuttgart,
Germany. E-mail: farkas@ims.uni-stuttgart.de.
? Institute for Natural Language Processing, University of Stuttgart, Pfaffenwaldring 5b, 70569 Stuttgart,
Germany.
? Institute for Natural Language Processing, University of Stuttgart, Pfaffenwaldring 5b, 70569 Stuttgart,
Germany.
Submission received: October 1, 2011; revised submission received: May 30, 2012; accepted for publication:
August 3, 2012
? 2013 Association for Computational Linguistics
Computational Linguistics Volume 39, Number 1
1. Introduction
A large part of the methodology for parsing in natural language processing has been
developed for English and a majority of publications on parsing are about parsing
of English. English is a strongly configurational language. Nearly all of the syntactic
information needed by anyNLP application can be obtained by configurational analysis
(e.g., by having a correct constituent parse).
Many other languages of the world are fundamentally different from English in this
respect. At the other end of the configurational?nonconfigurational spectrum we find a
language like Hungarian that has very little fixed structure on the level of the sentence.
Leaving aside the issue of the internal structure of NPs, most sentence-level syntactic
information in Hungarian is conveyed by morphology, not by configuration.
In this paper, we address German, a third type of language that is intermediate
between English and Hungarian. German has strong configurational constraints (e.g.,
main clauses are verb-second) as well as rich derivational and inflectional morphology,
all of which must be modeled for high-quality parsing. German?s intermediate status
raises a number of interesting issues in parsing that are of particular prominence for a
mixed configurational/morphological language, but are?as we will argue?of general
relevance for morphologically rich languages. Partly this is the case because there are
few (if any) languages archetypical of being purely configurational and purely noncon-
figurational (e.g., morphology is also important for English and even Hungarian has
configurational constraints). For lack of a better termwe refer to intermediate languages
as typified by German as MR&LC for morphologically rich and less-configurational.
Part of the motivation for this special issue is that most work on parsing to date
has been done on English, a morphologically simple language. As computational lin-
guistics broadens its focus beyond English it becomes important to take a more general
approach to parsing that can handle languages that are typologically very different from
English. Rich morphology (RM) is one very salient characteristic of a language that
affects the design of parsing methods. We argue that there are two other properties
of languages that are relevant in a discussion of parsing RM languages: syncretism
and configurationality. These two properties are correlated typologically with RM and
should therefore be taken into account when we address parsing RM languages.1
We first define the three properties and explain their relevance for parsing. The
large number of languages for which this correlation holds can be ordered along a
single dimension that can be interpreted as degree of morphological complexity. We
give examples for a number of languages that are positioned at different points on this
scale. Finally, we argue that just as languages that are at the opposite end of the spectrum
from English (prototypical examples of morphological richness like Hungarian) require
parsing methods that can be quite different from those optimal for English, the same
is true for a language like German that is in the middle of the spectrum?and what is
required is in some respects different from what is optimal for one extreme (English) or
the other (Hungarian).
The three correlated properties are rich morphology, syncretism, and configura-
tionality. Morphological richness can be roughly measured by the number of different
morphological forms a word of a particular syntactic category can have; for example,
1 We note, however, that this relationship is not a language universal. It is instead a frequently observed
correlation; for Chinese, for instance, the correlation does not seem to hold as strongly.
58
Fraser et al Knowledge Sources for Parsing German
a typical English noun has two forms (singular and plural), a typical German noun
has eight forms (singular and plural in four different cases), and a typical Hungarian
noun has several hundreds of forms. Syncretism refers to the fact that different mor-
phological forms have identical surface realization; for example, the formMann (?man?
in German) can be the nominative, dative, or accusative singular of Mann depending
on context. Configurationality refers to the degree to which the arrangement of words
and phrases of a particular syntactic function in a sentence is fixed. English is highly
configurational: it has limited flexibility in how the major phrases in a sentence (subject,
verb, direct object, indirect object, etc.) can be ordered. Hungarian and Latin are highly
flexible: Even though there are pragmatic constraints, in principle a large number of
possible orderings are grammatical. German is less configurational. It has some strict
constraints (verb second in main clauses, verb final in subordinate clauses), but also
some properties of a nonconfigurational language; for example, ordering of phrases
in the mittelfeld (the part of the main clause enclosed by the two parts of the verbal
complex) is very flexible.
It is obvious why configurationality and rich morphology are typologically (neg-
atively) correlated. Rich morphology specifies the syntactic role of a phrase in the
sentence, so fixing a position is not required, and many morphologically rich languages
therefore do not fix the position. Conversely, simple morphology gives little specific
information about the role of words and phrases in the sentence. One device often used
by morphologically simple languages to address this problem and reduce widespread
ambiguity is to fix the order of words and phrases in the sentence.
Syncretism has an effect that is similar to simplification of complex morphology.
Simple morphology is unspecific about grammatical function because it uses a small
number of morphological categories. Syncretism is unspecific about grammatical func-
tion because it suffers from a high degree of ambiguity. Even though the number of
different morphological categories is potentially large, syncretic forms conflate many of
these categories, so that these forms are much less helpful in determining grammatical
function than forms in a nonsyncretic language with the same number of categories.
Again, to counteract the communicative difficulties that lack of morphological speci-
ficity would create, stricter constraints on ordering and configuration are often used by
syncretic languages.
We have used English and Hungarian as examples for the extremes and German
for the middle of the spectrum. We now give examples of other languages and their
positions on the scale. Dutch is similar to German in that it also is verb second in main
clauses and verb final in subordinate clauses. The order of arguments in the mittelfeld is
much more restricted than in German, however. At the same time, Dutch morphology
has been much more simplified in the last centuries than German morphology. This
nicely confirms the correlation between RM and configurationality. Thus, Dutch is
positioned between English and German on the scale.
Classical Arabic is somewhat similar to German: The number of different morpho-
logical forms is roughly comparable to German and it allows a number of different
word orders. Modern Standard Arabic speakers rarely mark case, however, at least not
in spontaneous speech. At the same time, Modern Standard Arabic speakers use SVO
order much more frequently and consistently than is the case in Classical Arabic. Thus,
Classical Arabic is roughly at the same position as German on the scale whereas spoken
Modern Standard Arabic may be more comparable to Dutch.
Finally, Modern Greek is a language that is intermediate between German and
Hungarian. It has richer morphology than German, but it has a fair amount of syn-
cretism and therefore more morphological ambiguity than Hungarian. SVO is the
59
Computational Linguistics Volume 39, Number 1
predominant word order in modern Greek, but other word orders can be used. The
order within the noun phrase is more flexible than in German: Adjectives can precede
or follow the noun.
In the examples we have given, the amount of information conveyed by a mor-
phological form is negatively correlated with the amount of information conveyed by
configuration. If morphology conveys a lot of information (due to a large number of
distinctions and the lack of syncretism), then word order is freer and conveys less
information. If morphology conveys less information (due to fewer distinctions or more
syncretism), then configuration is fixed and provides more information to the speaker.
This suggests that RM and configuration are important variables that should be taken
into account in the design of parsing methods. In addition to looking at the extremes of
the spectrum that are exemplified by English andHungarian, we should also investigate
the middle: morphologically (somewhat) rich languages that are less configurational. In
this article, we look at the example of German.
One key question for MR&LC parsing is which type of parsing formalism to adopt,
constituency or dependency. It is a widely held belief that dependency structures are
better suited to represent syntactic analyses for morphologically rich languages because
they allow non-projective structures (the equivalent of discontinuous constituents in
constituency parsing). As Tsarfaty et al (2010) point out, however, this is not the same
as proving that dependency parsers function better than constituency parsers for pars-
ing morphologically rich languages. In fact, most state-of-the-art dependency parsers
(McDonald and Pereira 2006; Hall and Nivre 2008; Seeker et al 2010a) generate purely
projective dependency structures that are optionally transformed into non-projective
structures in a post-processing step. Comparable post-processing techniques have been
used in English constituency parsing (Gabbard, Marcus, and Kulick 2006; Schmid 2006;
Cai, Chiang, and Goldberg 2011) to identify discontinuous constituents andmight work
for other languages, as well.
The overview paper of the Parsing German Shared Task (Ku?bler 2008) reports
higher accuracies for detecting grammatical functions with dependency parsers than
with constituent parsers, but the direct comparison is not fair as it required phrase
boundaries to be correct on the constituent side while the tokens were the unit of
evaluation on the dependency side.2 How to carry out an absolutely fair comparison
of the two representations is still an open research question.3
Constituent parses often provide more information than dependency parses. An
example is the coordination ambiguity in old men and women versus old men and children.
The correct constituent parse for the first expression contains a coordination at the
noun level whereas the parse for the second expression coordinates at the level of
NPs. The dependency structures of both expressions, on the other hand, are usually
identical and thus unable to reflect the fact that oldmodifies women but not children. It is
possible, in principle, to encode the difference in dependency trees (cf. Rambow 2010),
2 This is due to how the evalb tool used to calculate PARSEVAL works. If a constituent is not perfectly
matched, the grammatical function is considered to be wrong, even if there was a partial match (at the
token level). This is not a problem with dependency-based evaluation. For further discussion of the
PARSEVAL metric and dependency-based evaluation see, for example, Rehbein and van Genabith (2007)
and Tsarfaty, Nivre, and Andersson (2012).
3 Two possible solutions are to use TedEval (Tsarfaty, Nivre, and Andersson 2012), or to conduct an analysis
of grammatical functions at the token level in a consistent fashion for both dependency and constituent
parsers. In our case, the latter would require a high quality conversion from the Tiger constituency
representation to a dependency representation, which we hope to implement in future work.
60
Fraser et al Knowledge Sources for Parsing German
for example, by enriching the edge labels, but the constituent representation is simpler
for this phenomenon.
Finally, there are some applications that need constituent parses rather than depen-
dency parses. For instance, many hierarchical statistical machine translation systems
use constituency parses, requiring the output of a dependency parser to be transformed
into a constituent parse.4 We conclude that there is no clear evidence for preferring
dependency parsing over constituency parsing in analyzing languages with RM and
instead argue that research in both frameworks is important.
We view the detailed description of a constituency parsing system for a mor-
phologically rich language, a system that addresses the major problems that arise in
constituency parsing for MR&LC, as one of our main contributions in this paper.
The first problem we address is the proliferation of phrase structure rules in
MR&LC languages. For example, there are a large number of possible orderings of the
phrases in the German mittelfeld, and many orderings are exceedingly rare. A standard
constituency parser cannot estimate probabilities for the corresponding rules reliably.
The solution we adopt here is markovization?complex rules are decomposed
into small unidirectional rules that can be modeled and estimated more reliably than
complex rules. Although markovization in itself is not new, we stress its importance for
MR&LC languages here and present a detailed, reproducible account of how we use it
for German. Markovization combines the best of both worlds for MR&LC languages:
Preferential configurational information can be formalized and exploited by the parser
without incurring too large of a performance penalty due to sparse data problems.
The second problem that needs to be addressed in parsingmanyMR&LC languages
is widespread syncretism. We mainly address syncretism by using a high performance
finite-state automata-based morphological analyzer. Such an analyzer is of obvious
importance for any morphologically rich language because the productivity of mor-
phologically rich languages significantly increases the unknown-word rate in new text
versus morphologically poor languages. So the parser cannot simply memorize the
grammatical properties of words in the Treebank used for training. Instead we incorpo-
rate a complex guesser into our parser that, based on the input from the morphological
analyzer, predicts the grammatical properties of new words and (equally important)
unobserved grammatical properties of known words. With prevailing syncretism, this
task is muchmore complex than in a language where case, gender, number, and so forth,
can be deterministically deduced from morphology.
The morphological analyzer is based on (i) a finite state formalization of German
morphology and (ii) a large lexicon of morphologically analyzed German words. We
refer to these two components together as lexical knowledge. We show that lexical
knowledge is beneficial for parsing performance for an MR&LC language like German.
In addition to lexical knowledge, there is a second important aspect of syncretism
that needs to be addressed in MR&LC languages. Syntactic disambiguation in these
languages must always involve both systems of grammatical encoding, morphology
and configuration, acting together. The most natural way of doing this in a language
like German is to perform this integration of the two knowledge sources directly as part
of parsing.We do this by annotating constituent labels with grammatical functionwhere
appropriate. In contrast with syntactic parses of strongly configurational languages
like English, syntactic parses of German are not useful for most tasks without having
4 We do note, however, that there are a few translation systems which use a dependency representation
directly (e.g., Quirk, Menezes, and Cherry 2005; Shen, Xu, and Weischedel 2008; Tu et al 2010).
61
Computational Linguistics Volume 39, Number 1
grammatical functions indicated. It is not even possible to access the basic subcatego-
rization of the verb (such as determining the subject) without grammatical functions.
We argue that MR&LC languages like German should always be evaluated on labels-
cum-grammatical-function.
Our last main contribution in this paper concerns the fact that we believe that
MR&LC languages give rise to more ambiguity than languages that are predominantly
configurational or morphological. As an example consider the German sentence ?Die
[the] Katze [cat] jagt [hunts] die [the] Schlange [snake].? In German either the cat or the
snake can be the hunter. This type of ambiguity neither occurs in a strongly configu-
rational language like English (where configuration determines grammatical function)
nor in a morphologically rich language like Hungarian that has no or little syncretism
(where morphology determines grammatical function). Although morphology and
configuration in MR&LC languages often work hand in hand for complete disambigua-
tion, there are also many sentences where neither of the two provides the necessary
information for disambiguation. We believe that this distinguishing characteristic of
MR&LC languages makes it necessary to tap additional knowledge sources. In this
paper, we look at two such knowledge sources: monolingual reranking (which captures
global properties of well-formed parses for additional disambiguation) and bilingual
reranking (which exploits parallel text in a different language for disambiguation).
For monolingual reranking, we define a novel set of rich features based on sub-
categorization frames. We compare our compact feature set with a sparse feature set
designed for German previously by Versley and Rehbein (2009). We show that the
richer subcategorization-based framework for monolingual reranking is effective; it has
comparable performance to the sparse feature set?moreover, they complement each
other.
For bilingual reranking, we present our approach to bitext parsing, where a German
parse is found that minimizes syntactic divergence with an automatically generated
parse of its English translation. We pursue this approach for a number of reasons. First,
one limiting factor for syntactic approaches to statistical machine translation is parse
quality (Quirk and Corston-Oliver 2006). Improved parses of bitext should result in
improved machine translation. Second, as more and more texts are available in several
languages, it will be increasingly the case that a text to be parsed is itself part of a
bitext. Third, we hope that the improved parses of bitext can serve as higher quality
training data for improving monolingual parsing using a process similar to self-training
(McClosky, Charniak, and Johnson 2006a).
We show that the three different knowledge sources we use in this paper (lexical
knowledge, monolingual features, and bilingual features) are valuable separately. We
also show that the gain of the two sets of reranking features (monolingual and bilingual)
is additive, suggesting that they capture different types of information.
The resulting parser is currently the best constituent parser for German (with or
without bilingual features). In particular, we show that the baseline parser without
reranking is competitive with the previous state of the art (the Berkeley parser) and
that the re-ranking can add an important gain.
2. Previous Work
Constituent parsing for English is well studied. The best generative constituent parsers
are currently the Brown reranking parser (Charniak and Johnson 2005), the exten-
sion of this parser with self training by McClosky, Charniak, and Johnson (2006b),
and the parser of Petrov and Klein (2007), which is an unlexicalized probabilistic
62
Fraser et al Knowledge Sources for Parsing German
context-free grammar (PCFG) parser with latent feature annotations. Charniak and
Johnson (2005) and Huang (2008) have introduced a significant improvement by
feature-rich discriminative reranking as well.
The number of treebank constituent parsers for German is smaller. Dubey and
Keller (2003) adapted Collins?s (1997) lexicalized parser to German. An unlexicalized
PCFG parser similar to our generative parser was presented by Schiehlen (2004). The
best constituent parser participating in the ACL-08 Workshop on Parsing German
(Ku?bler 2008) was the Berkeley parser (Petrov and Klein 2008). The Stanford parser
was also adapted to German (Rafferty andManning 2008). German dependency parsers
have been developed by Menzel and Schro?der (1998), Duchier and Debusmann (2001),
Hall and Nivre (2008), Henderson et al (2008), and Seeker et al (2010a), to name
a few.
There is also some previous work on German parse reranking. Forst (2007) pre-
sented a reranker for German LFG parsing, and Dreyer, Smith, and Smith (2006) applied
reranking to German dependency parsing. Versley and Rehbein (2009) developed a
reranking method for German constituent parsers. The work by Versley and Rehbein
and by Schiehlen (2004) is closest to ours. Like them, we rerank the unlexicalized BitPar
parser. We also refine treebank labels to increase parsing performance, but add more
information and achieve a larger improvement. We use the monolingual feature set of
Versley and Rehbein in our reranker, but add further monolingual features as well as
bilingual features.
3. Generative Parsing Framework
Our generative parser is an unlexicalized PCFG parser which is based on the BitPar
parser (Schmid 2004). BitPar uses a fast bitvector-based implementation of the well-
known Cocke-Younger-Kasami algorithm and stores the chart as a large bit vector.
This representation is memory efficient and allows full parsing (without search space
pruning) with large treebank grammars. BitPar is also quite fast because the basic
parsing operations are parallelized by means of (single-instruction) and-operations on
bitvectors. BitPar can either be used to compute the most likely parse (Viterbi parse), or
the full set of parses in the form of a parse forest, or the n-best parse trees.
3.1 Grammar
The grammar and lexicon used by our generative parser are extracted from the Tiger2
Treebank (Brants et al 2002). Similar to Johnson (1998) andKlein andManning (2003) we
improve the accuracy of the unlexicalized parser by refining the non-terminal symbols
of the grammar to encode relevant contextual information. This refinement weakens
the strong independence assumptions of PCFGs and improves parsing accuracy. The
extraction of the grammar and lexicon involves the following steps:
1. Discontinuous constituents are eliminated (Section 3.2).
2. Treebank annotations are transformed (Section 3.4) and augmented
(Section 3.5).
3. Grammar rules, lexical rules, and their frequencies are extracted from the
annotated parse trees.
4. The grammar is markovized (Section 3.6).
63
Computational Linguistics Volume 39, Number 1
.
S-TOP
PROPAV-OP-1
Daraus
This-from
VMFIN-HD
kann
can
VP-OC
VP-OC
PROAV-OP
*T*-1
VVPP-HD
gefolgert
concluded
VAINF-HD
werden
be
Figure 1
Projectivized parse tree for the sentence: Daraus kann gefolgert werden [From this can be
concluded].
3.2 Raising for Non-Projectivity
The Tiger2 Treebank that we used in our experiments contains discontinuous con-
stituents. As in other work on German parsing using the Tiger2 Treebank (Dubey
and Keller 2003; Schiehlen 2004; Ku?bler, Hinrichs, and Maier 2006), we eliminated
discontinuous constituents by raising those parts of the discontinuous constituent that
do not contain the head to the child position of an ancestor node of the discontinuous
constituent. Hsu (2010) compared three different Tiger2 conversion schemes and found
raising to be the most effective. The projective parse tree in Figure 1, for instance, is
obtained from a Tiger parse tree where the pronominal adverb Daraus was a dis-
continuous child of the lower VP-OC node.
The parse tree in Figure 1 shows a trace node and coreference indices (similar to
the Penn Treebank annotation style for discontinuous constituents). If slash features
are added to the nodes on the path between the PROAV node and its trace within the
VP, it is possible to restore discontinuous constituents (Schmid 2006). Due to sparse
data problems caused by the added slash features, however, the parsing accuracy
drops by 1.5% compared with the version without slash features (when evaluated on
projectivized parse trees). Traces are recognized with a precision of 53% and a recall of
33%. The correct antecedents are identified with a precision of 48% and a recall of 30%.
These figures indicate that the identification of discontinuous constituents in Tiger parse
trees is a harder task than in English Penn Treebank parses, considering the 84% F-score
for the recognition of empty constituents and the 77% F-score for the identification of
antecedents reported in Schmid (2006) for an analogous approach.
As the example in Figure 1 shows, the precise attachment point of constituents
is often not required: We can simply assume that all constituents appearing at the S
level are dependents of the main verb of the clause. Only for modifiers with scope
ambiguities (e.g., negation particles) is it relevant whether they are attached at the S
or VP level. These considerations suggest that it is better to recognize discontinuous
constituents in a post-processing step as in Johnson (2001), Campbell (2004), and Levy
and Manning (2004). In the rest of the paper, we will only work with parse trees from
which coreference indices and trace nodes have been removed.
3.3 Morphological Features and Grammatical Functions
The Tiger2 Treebank annotates non-terminals not only with syntactic categories but
also with grammatical function labels such as SB (subject), OA (accusative object), or
64
Fraser et al Knowledge Sources for Parsing German
MO (modifier). These labels provide important information that is necessary in order to
derive a semantic representation from a parse. It is not possible to infer the grammatical
role of a constituent from its position in the parse tree alone (as can be done in English,
for instance). Case information is needed in addition in order to help determine the
correct grammatical role. The Tiger2 Treebank provides case, number, degree (positive,
comparative, superlative), and gender information at the part-of-speech (POS) level.
Our parser concatenates the grammatical function labels as well as the case infor-
mation of the POS tags to the base labels similarly to Dubey (2004) and Versley (2005).
Our earlier experiments showed that adding case information increases F-score by 2.1%
absolute. Further enriching the grammar with morphological features, however, hurts
performance. Adding number features decreased F-score by 0.5%. Adding number,
gender, and degree decreased F-score by 1.6%. When grammatical functions are taken
into account in the evaluation, the performance drops by 1.5% when number, gender,
and degree features are incorporated. It seems that the additional information supplied
by the agreement features is not useful enough to outweigh sparse data problems
caused by the more fine-grained label set. Therefore we only use case, but designing
a smoothing procedure allowing us to use number, gender, and degree is interesting
future work.
3.4 Tree Transformations
Similarly to Schiehlen (2004), we automatically augment the Tiger2 annotation with ad-
ditional feature annotations. Our feature annotation set is larger than that of Schiehlen.
In addition to making feature annotations, we also perform some tree transformations
that reduce the complexity of the grammar. In all evaluations, we use the original
(projectivized) Tiger parse trees as gold standard and convert the parse trees generated
by our parser to the same format by undoing the transformations and removing the
additional features. In the rest of this section, we explain the tree transformations that
we used. The following section describes the feature annotations.5
Unary branching rules. The Tiger Treebank avoids unary branching nodes. NPs
and other phrasal categories which dominate just a single node are usually omitted.
The sentence Sie zo?gern [They hesitate], for instance, is analyzed as (S-TOP (PPER-SB
Sie) (VVFIN-HD zo?gern)) without an explicit NP or VP. The lack of unary branching
nodes increases the number of rules because now a rule S-TOP? PPER-SB VVFIN-HD
is needed in addition to the rule S-TOP? NP-SB VVFIN-HD, for instance.
In order to reduce sparse-data problems, we insert unary branching nodes and
transform this parse to (S-TOP (NP-SB (PPER-HD Sie)) (VVFIN-HD zo?gern)) by adding
an NP node with the grammatical function (GF) of the pronoun. The GF of the pronoun,
in turn, is replaced by HD (head). Such unary branching NPs are added on top of nouns
(NN), pronouns (PPER, PDS, PIS, PRELS), cardinals (CARD), and complex proper
names (PN) that are dominated by S, VP, TOP, or DL6 nodes.7 The transformation is
reversible, which allows the original annotation to be restored.
5 Descriptions of the different symbols used in the Tiger annotation scheme are available at
http://www.ims.uni-stuttgart.de/tcl/RESOURCES/CL.html.
6 DL is a discourse level constituent.
7 If a single proper name (NE) forms a noun phrase, we first add a PN node and then an NP node on top.
If a simple noun (NN) with a GF other than NK appears inside of an NP, PP, CNP, CO, or AP, we also add
an NP node on top of it. Similarly, we add a PN node on top of proper names (NE) in the same context.
65
Computational Linguistics Volume 39, Number 1
.
CPP-MO
KON-CD
weder
neither
PP-CJ
APPR-AC
in
in
NE-NK
Berlin
Berlin
KON-CD
noch
nor
PP-CJ
APPR-AC
in
in
NE-NK
Frankfurt
Frankfurt
.
CPP-MO
KON-CD/weder
weder
neither
PP-MO
APPR-AC/in
in
in
NE-HD
Berlin
Berlin
KON-CD/noch
noch
nor
PP-MO
APPR-AC/in
in
in
NE-HD
Frankfurt
Frankfurt
Figure 2
Parse of the phrase weder in Berlin noch in Frankfurt [neither in Berlin nor in Frankfurt] before
and after selective lexicalization of prepositions and conjunctions. This example also shows the
replacement of the grammatical function features CJ and NK discussed in the previous section.
The modified parts are printed in boldface.
By adding a unary-branching NP-SB node, for instance, we introduce an additional
independence assumption, namely, we assume that the expansion of the subject NP is
independent of the other arguments and adjuncts of the verb (a plausible assumption
that is confirmed by a performance improvement).
Elimination of NK. Tiger normally uses the grammatical function HD to mark the
head of a phrase. In case of NPs and PPs, however, the GF of the head is NK (noun
kernel). The same GF is also assigned to the adjectives and determiners of the noun
phrase. We replace NK by HD in order to reduce the set of symbols.8
Elimination of CJ. Tiger annotates each conjunct in a coordination with the spe-
cial grammatical function label CJ. We replace CJ by the grammatical function of the
coordinated phrase. This transformation is also reversible.
3.5 Additional Feature Annotations
Selective lexicalization. We mark the POS tags of the frequent prepositions in [in], von
[from, of], auf [on], durch [through, by means of], unter [under], um [around, at] and
their variants regarding capitalization (e.g., Unter) and incorporation of articles (e.g.,
unters, unterm) with a feature which identifies the preposition. This can be seen as a
restricted form of lexicalization. In the same way, we also ?lexicalize? the coordinating
conjunctions (KON-CD) sowohl [as well], als [as], weder [neither], noch [nor], entweder
[either], and oder [or] if preceded by entweder. Figure 2 shows an example.
Sentence punctuation. If a clause node (S) has a sibling node labeled with POS tag
?$.? that dominates a question mark or exclamation mark, then the clause node and the
POS tag are annotated with quest or excl, so the grammar models different clause types.
8 The original annotation can be restored: HD never occurs in NP or PP children in original Tiger parses.
66
Fraser et al Knowledge Sources for Parsing German
.
CS-RC
S-RC
NP-SB/rel
PRELS-HD-Nom
die
who
NP-OA
NN-HD-ACC
Surfen
surfing
VVFIN-HD
sagen
say
KON-CD
und
and
S-RC/norel/nosubj
NP-OA
NN-HD-Acc
Freiheit
freedom
VVFIN-HD
meinen
mean
Figure 3
Parse of the phrase die Surfen sagen und Freiheit meinen [who say surfing and mean freedom] before
and after annotation with relative clause features. This example also shows the nosubj feature,
which will be discussed later.
Adjunct attachment. Adjuncts often differ with respect to their preferred attach-
ment sites. Therefore, we annotate PPs and adverbials (AVP, ADV, ADJD) with one of
the features N, V, or 0 which indicate a nominal parent (NP or PP), a verbal parent
(VP, S), or anything else, respectively. In case of adverbial phrases (AVP), the label is
propagated to the head child.
Relative clause features. In many relative clauses (S-RC), the relative pronoun
(PRELS, PRELAT, PWAV, PWS) is embedded inside of another constituent. In this case,
all nodes on the path between the pronoun and the clause node are marked with the
feature rel. Furthermore, we add a feature norel to relative clauses if no relative pronoun
is found. Figure 3 shows an example.
Wh features. Similar to the feature rel assigned to phrases that dominate a relative
pronoun, we use a feature wh which is assigned to all NPs and PPs which immediately
dominate a wh-pronoun (PWAT, PWS, PWAV). This feature better restricts the positions
where such NPs and PPs can occur.
Noun sequence feature. If two nouns occur together within a GermanNP (as in drei
Liter Milch [three liters (of) milk] or Ende Januar [end (of) January]), then the first noun
is usually a kind of measure noun. We mark it with the feature seq.
Proper name chunks. Some noun phrases such as Frankfurter Rundschau, Junge
Union, Die Zeit are used as proper names. In this case, the grammatical function of the
NP is PNC. In order to restrict the nouns and adjectives that can occur inside of such
proper name chunks, we mark their POS tags with the feature name.
Predicative APs. Complex adjectival phrases (AP) are either attributively used as
noun modifiers inside of an NP or PP, or predicatively elsewhere. In order to better
model the two types of APs, we mark APs that dominate a predicative adjective (ADJD)
with the feature pred.9
Nominal heads of APs. Sometimes the head of an AP is a noun as in (AP drei
Millionen) Mark [three million Marks] or ein (AP politisch Verfolgter) [a politically
persecuted-person]. We mark these APs with the feature nom.
Year numbers.Years such as 1998 can appear in places where other numbers cannot.
Therefore POS tags of numbers between 1900 and 2019 are marked with year.10
Clause type feature for conjunctions. The type of a subordinate clause and the
subordinating conjunction are highly correlated. German object clauses (S-OC) usually
9 We also mark an AP parent of a node with the label AP-HD/pred in the same way.
10 For some texts, it might be advantageous to use a broader definition of year numbers.
67
Computational Linguistics Volume 39, Number 1
start with dass [that] or ob [whether]; modifier clauses (S-MO) often start with wenn
[if], weil [because], or als [when]. We mark subordinating conjunctions of argument
clauses (S-OC), modifier clauses (S-MO), subject clauses (S-SB), and dislocated clauses
(S-RE) with a feature (OC,MO, SB, or RE) identifying clause type. Without this feature,
argument clauses of nouns, for instance, are often misanalyzed as modifiers of the main
clause.
VP features. VPs that are headed by finite verbs, infinitives, past participles, imper-
atives, and zu infinitives are all used in different contexts. Therefore we mark object VPs
(VP-OC) with a corresponding feature. When parsing the sentence Alle Ra?ume mu?ssen
mehrfach gesa?ubert und desinfiziert werden [all rooms must multiply cleaned and disin-
fected be; all rooms must be ...], this feature allows the parser to correctly coordinate the
two past participle VPs mehrfach gesa?ubert and desinfiziert instead of the past participle
VP mehrfach gesa?ubert and the infinitival VP desinfiziert werden.
Phrases without a head. Some phrases in the Tiger corpus lack a head. This is
frequent in coordinations. All phrases that do not have a child node with one of the
grammatical functions HD, PNC, AC, AVC, NMC, PH, PD, ADC, UC, or DH aremarked
with the feature nohead.
Clauses without a subject. We also mark conjunct clauses with the feature nosubj
if they are neither headed by an imperative nor contain a child node with the gram-
matical function SB (subject) or EP (expletive). This is useful in order to correctly parse
coordinations where the subject is dropped in the second conjunct.
3.6 Markovization
The Tiger Treebank uses rather flat structures where nodes have up to 25 child nodes.
This causes sparse data problems because only some of the possible rules of that length
actually appear in the training corpus. The sparse data problem is solved bymarkoviza-
tion (Collins 1997; Klein and Manning 2003), which splits long rules into a set of shorter
rules. The shorter rules generate the child nodes of the original rule one by one. First,
the left siblings of the head child of the rule are generated from left to right, then the
right siblings are generated from right to left. Finally, the head is generated. Figure 4
shows the markovization of the rule NP? NMNN PP PP.
The auxiliary symbols that are used here encode information about the parent cat-
egory, the head child, and previously generated children. Because all auxiliary symbols
encode the head category, the head is already selected by the first rule, but only later
actually generated by the last rule.
.
NP
NM ?L:NP[NN]NN|NM?
?M:NP[NN]?
?R:NP[NN]PP|PP?
?R:NP[NN]NN|PP?
NN
PP
PP
Figure 4
Markovization of the rule NP? NMNN PP PP.
68
Fraser et al Knowledge Sources for Parsing German
The general form of the auxiliary symbols is ?direction:parent[head]next|previous?
where direction is either L, M, or R, parent is the symbol on the left hand side of the
rule, head is the head on the right hand side of the rule, next is the symbol which will
be generated next, and previous is the symbol that was generated before. Auxiliaries
starting with L generate the children to the left of the head. Auxiliaries starting with
R similarly generate the children to the right of the head and the head itself. The
auxiliary starting with M is used to switch from generating left children to generating
right children. Each rule contains information about the parent, the head, and (usually)
three child symbols (which may include an imaginary boundary symbol). The first rule
encodes the trigram left-boundary NM NN. The second rule is an exception which only
encodes the bigram NM NN. The third rule encodes the trigram PP PP right-boundary.
The last rule is an exception, again, and only encodes NN PP. There is no rule which
covers the trigram consisting of the head and its two immediate neighbors.
Our markovization strategy only transforms rules that occur less than 10 times in
the training data. If one of the auxiliary symbols introduced by the markovization (such
as ?L:NP[NN]NN?NM?) is used less than 20 times (the values of the two thresholds
were optimized on part of the development data) overall, it is replaced by a simpler
symbol ?L:NP[NN]NN? that encodes less context. In this way, we switch from a trigram
model (where the next child depends on the two preceding children) to a bigrammodel
(where it only depends on the preceding child) in order to avoid sparse data problems.
Themethod is similar to the markovization strategy of Klein andManning (2003) except
that they markovize all rules. We simulated their strategy by raising the rule frequency
threshold to a larger value, but obtained worse results. We also tried an alternative
markovization strategy that generates all children left to right (the auxiliary symbols
now lack the direction flag, and the rules cover all possible trigrams), but again obtained
worse results. A disadvantage of our markovization method are spurious ambiguities.
They arise because some of the rules which are not markovized are also covered by
markovization rules.
3.7 Dealing with Unknown Words and Unseen POS Tags
BitPar includes a sophisticated POS guesser that uses several strategies to deal with
unknown words and unseen POS tags of known words. Unknown words are divided
into eight classes11 based on regular expressions that are manually defined. These
classes distinguish between lower-case words, capitalized words, all upper-case words,
hyphenated words, numbers, and so forth. For each word class, BitPar builds a suffix
tree (Weischedel et al 1993; Schmid 1995; Brants 2000) from the suffixes of all words in
the lexicon up to a length of 7. At each node of the suffix tree, it sums up the conditional
POS probabilities (given the word) over all known words with that suffix. By summing
POS probabilities rather than frequencies, all words have the same weight, which is
appropriate here because we need to model the POS probabilities of infrequent words.
BitPar computes POS probability estimates for each node using the sum of probabilities
as a pseudo-frequency for each tag. The estimates are recursively smoothed with the
Witten-Bell method using the smoothed POS probabilities of the parent node as a
backoff probability distribution.12 The suffix trees are pruned by recursively removing
11 We also experimented with more complex classifications, but they failed to improve the results.
12 The number of ?observed? POS tags, which is needed by Witten-Bell smoothing, is defined as the
number of POS tags with a pseudo-frequency larger than 0.5.
69
Computational Linguistics Volume 39, Number 1
leaf nodes whose pseudo-frequency is below 5 or whose weighted information gain13
is below a threshold of 1.
Whenever an unknown word is encountered during parsing, BitPar determines the
word class and obtains the tag probability distribution from the corresponding suffix
tree. BitPar assumes that function words are completely covered by the lexicon and
never guesses function word POS classes for unknown words.
BitPar uses information from the unknown word POS guesser and (if available)
information from an external lexicon (generated by a computational morphology, for
instance, as we will discuss in Section 5.1) in order to predict unobserved POS tags
for known words. First the external lexicon and the lexicon extracted from the training
corpus are merged. Then smoothed probabilities are estimated using Witten-Bell
smoothing with a backoff distribution. The backoff distribution is the average of:
(1) the probability distribution returned by the unknown word POS guesser
if at least one possible POS tag of the word according to the lexicon is an
open-class POS tag,
(2) the average POS probability distribution of all words with exactly the same
set of possible POS tags as the given word14 if at least one of the possible
tags is unseen, and
(3) the prior POS probability distribution if no other word in the lexicon has
the same set of possible POS tags and at least one of the word?s possible
POS tags is unseen.
4. Evaluation of the Generative Parser
As we present each knowledge source, we would like to evaluate it against manually
annotated Treebanks. Our first evaluation shows that our generative parser introduced
in the previous section is comparable with the Berkeley generative parser. Before we
present this comparison in Section 4.1 we discuss evaluating parse accuracy.
In our evaluations, we use the Tiger Treebank (Brants et al 2002) and a small
Europarl Treebank (Pado? and Lapata 2009). We take the first 40,474 sentences of the
Tiger Treebank as training data (Tiger train), the next 5,000 sentences as development
data (Tiger dev), and the last 5,000 sentences as test data (Tiger test). The Europarl
data consists of 662 sentences15 and are either completely used as test data and not di-
vided up or we carried out seven-fold cross-validation experiments with our reranking
models.
All parsers are evaluated on projectivized parse trees. This means that we apply
step 1 of the grammar extraction process described in Section 3.1 to the test parses
and use the result as the gold standard (except for the Pado? set, which is already
projectivized). The test sentences are parsed and the resulting parse trees are converted
13 The weighted information gain is the difference between the entropy of the parent node and the entropy
of the current node, multiplied by the total frequency of the current node and divided by the number of
?observed? POS tags of the current node.
14 A similar pooling of lexicon entries was previously used in the POS tagger of Cutting et al (1992).
15 We use only the sentences in this set which had a single sentence as a translation, so that they could
be used in bilingual reranking, which will be discussed later.
70
Fraser et al Knowledge Sources for Parsing German
to the same format as the gold standard trees by undoing Steps 2, 3, and 4 of Section 3.1.
This conversion involves four steps:
1. Demarkovization removes all the auxiliary nodes introduced by
markovization and raises their children to the next non-auxiliary node.
2. The added unary-branching nodes are eliminated.
3. The original grammatical function labels NK inside of NPs and PPs,
and CJ inside of coordinated phrases, are restored.
4. All feature annotations are deleted.
We use PARSEVAL scores (Black et al 1991) and the standard evaluation tool evalb16
to compare the converted parse trees with the gold standard parse trees using labeled
F-score. We report accuracies for all test sentences and not just sentences of length up to
40. We do not evaluate parsers with gold standard POS tags, but instead automatically
infer them. These considerations make our evaluation setting as close to the real-world
setting as possible.
We report results for evaluations with and without grammatical functions. We
report PARSEVAL scores with grammatical functions inside parentheses after the
results using only basic constituent categories. We believe that grammatical functions
are an important part of the syntactic analysis for any downstream applications in less-
configurational languages such as German because crucial distinctions (e.g., the distinc-
tion between subject and object) are not feasible without them. We should mention that
our results are not directly comparable to previously published results on the Tiger2
corpus (Ku?bler 2008; Versley and Rehbein 2009; Seeker et al 2010b), because each of
the previous studies used different portions of the corpus and there are differences in
the evaluation metric as well. The transformed corpus (in our train, development, and
test split format) and the evaluation scripts we used are available,17 which we hope will
enable direct comparison with our results.
4.1 Comparison of BitPar and Berkeley
The best constituent parser participating in the Parsing German Shared Task (Ku?bler
2008) was the Berkeley parser (Petrov and Klein 2008) and to the best of our knowledge
it has achieved the best published accuracy for German constituency parsing so far.
The Berkeley parser takes an automated approach, in which each constituent symbol is
split into subsymbols applying an expectation-maximization method. We compare our
manually enriched grammar to this automatic approach.
We trained the Berkeley parser on Tiger train using the basic constituent categories
concatenated to the grammatical function labels as starting symbols. We found that it
achieved the best PARSEVAL scores on Tiger dev after the fourth iteration. This model
was used for parsing Tiger dev, Tiger test, and the Europarl corpus.
BitPar achieved 82.51 (72.46), 76.67 (65.61), and 77.13 (66.06), and the Berkeley
parser achieved 82.76 (73.20), 76.37 (65.66), and 75.51 (63.3) on the three corpora,
respectively. In general, these results indicate that these two parsers are competitive.
On the other hand, the fact that the results of the Berkeley parser are much worse than
16 http://nlp.cs.nyu.edu/evalb/, 2008 version.
17 See http://www.ims.uni-stuttgart.de/tcl/RESOURCES/CL.html.
71
Computational Linguistics Volume 39, Number 1
BitPar on the out-of-domain Europarl corpus indicates that it overfits to the domain
of the training corpus (Tiger2). Following a reviewer suggestion, we looked at the
sentences containing many words not occurring in the training data, and observed that
our lexical resource is strongly helpful for these sentences. Another disadvantage of
the automatic approach of the Berkeley parser is that the resulting subsymbols are not
easily interpretable, which can hinder defining features for parse reranking using them.
Based on these considerations, we decided to use BitPar in our reranking experiments.
The combination of the two radically different approaches (linguistically motivated
grammar extensions and automatic symbol splitting) is a rather promising area of
research for improving parsing accuracy, which we plan to address in future work.
5. Impact of Our Lexical Resource
5.1 Integration of SMOR with BitPar
There are a large number of inflectedword forms formanyGerman lemmas. This causes
sparse data problems if some forms are not observed in the training data. BitPar applies
the heuristics described in Section 3.7 to obtain POS probabilities for unseen words.
Although these heuristics seem to work quite well, we expect better results if the parser
has access to information from a morphological analyzer.
We use the German finite-state morphology SMOR (Schmid, Fitschen, and Heid
2004) to provide sets of possible POS tags for all words. SMOR covers inflection, deriva-
tion, and compounding and achieves good coverage in combination with the stem
lexicon IMSLex (Lezius, Dipper, and Fitschen 2000). SMOR is integrated into the parser
in the following way. We create a combined word list from the training and testing
data18 and analyze it with SMOR. The SMOR analyses are then mapped to the POS
tag set used by the parser, and supplied to BitPar as an external lexicon (see Section 3.7).
Consider the example word erlischt [goes out], which did not appear in the train-
ing corpus. SMOR produces the analysis erlo?schen.V.3.Sg.Pres.Ind, which is mapped
to VVFIN-HD and added to the lexicon. Using this entry, BitPar correctly parsed
the sentence Die Anzeige erlischt [The display goes out]. Without using SMOR, the
parser analysed erlischt as a past participle because scht is a frequent past participle
ending.
5.2 Effect on In-Domain and Out-of-Domain Parsing
In order to measure the effect of the integration of a German morphology on parsing
accuracy (see Section 5.1), we tested the BitPar parser on the Tiger data and on Europarl
data. The results are summarized in Table 1. They show that the morphology helps on
out-of-domain data (Europarl), but not so much on in-domain data (Tiger). The POS
tagging accuracy, however, also increases on Tiger data by 0.13%. When grammatical
functions are included in the evaluation, the performance improvement more than
doubles on Europarl data. As a result, we decided to use the finite-state morphology
in the rest of the experiments we conducted.
Table 1 also shows that the Tiger test data is harder to parse than the dev data. We
examined the two subcorpora and found that the test data contains longer sentences
18 Because we are only using the words here, and not their POS labels, this approach is methodologically
sound and could be applied to any unparsed data in the same way.
72
Fraser et al Knowledge Sources for Parsing German
Table 1
Effect of using finite-state morphology on parsing accuracy. The values in parentheses are
labeled F-scores from the evaluation with grammatical functions.
morphology Tiger dev Tiger test Europarl
without 82.51 (72.46) 76.67 (65.61) 76.81 (65.31)
with 82.42 (72.36) 76.84 (65.91) 77.13 (66.06)
difference ?0.09 (?0.10) +0.17 (+0.30) +0.32 (+0.75)
(18.4 vs. 15.3 words on average) and that the ratio of unknown words is higher (10.0%
vs. 7.6%).
6. Parse Reranking
The most successful supervised phrase-structure parsers are feature-rich discriminative
parsers that heavily depend on an underlying PCFG grammar (Charniak and Johnson
2005; Huang 2008). These approaches consist of two stages. At the first stage they apply
a PCFG grammar to extract possible parses. The full set of possible parses cannot be
iterated through in practice, and is usually pruned as a consequence. The n-best list
parsers keep just the 50?100 best parses according to the PCFG. Other methods remove
nodes and edges from the packed parse forest whose posterior probability is under a
pre-defined threshold (Charniak and Johnson 2005).
The task of the second stage is to select the best parse from the set of possible
parses (i.e., rerank this set). These methods use a large feature set (usually a few
million features) (Collins 2000; Charniak and Johnson 2005). The n-best list approaches
can straightforwardly use local and non-local features as well because they decide at
the sentence-level (Charniak and Johnson 2005). Involving non-local features is more
complicated in the forest-based approaches. The conditional random field methods
usually use only local features (Yusuke and Jun?ichi 2002; Finkel, Kleeman, and
Manning 2008). Huang (2008) introduced a beam-search and average perceptron-based
procedure incorporating non-local features in a forest-based approach. His empirical
results show only a minor improvement from incorporating non-local features,
however.
In this study, we experiment with n-best list reranking using a maximum entropy
machine learning model for (re)ranking along with local and non-local features. Our
reranking framework follows Charniak and Johnson (2005). At the first-stage of parsing,
we extract the 100 best parses for a sentence according to BitPar?s probability model.
At parsing time, a weight vector w is given for the feature vectors (which numerically
represent one possible parse) and we select the parse with the highest inner product
of these two vectors. The goal of training is to adjust w. In the maximum entropy
framework, this is achieved by solving the optimization problem of maximizing the
posterior probability of the oracle parse?the parse with the highest F-score.19 Our
method aims to select the oracle, as the gold standard parse is often not present in
the 100-best parses.20 Our preliminary experiments showed that parse candidates close
19 Ties are broken using the PCFG probabilities of the parses.
20 The oracle F-score (i.e., the upper limit of 100-best reranking on the Tiger development corpus) is 90.17.
73
Computational Linguistics Volume 39, Number 1
to the oracle confuse training. Hence during training, we removed all parses whose
F-score is closer than 1.0 to the score of the oracle.21
As we discussed in Section 1, the parsing output of morphologically rich languages
is useful only when it is additionally annotated with grammatical functions. The oracle
parses often change if the grammatical function labels are also taken into consideration
at the PARSEVAL score calculation. Hence slightly different objective functions are used
in the two cases. We will report results achieved by reranking models where the oracle
selection for training agrees with the evaluation metric utilized?that is, we trained
different models (which differ in the oracle selection) for the basic constituent label
evaluation and for the evaluation on grammatical functions.
During training we followed an eight-fold cross validation technique for candidate
extraction (Collins 2000). Here, one-eighth of the training corpus was parsed with a
PCFG extracted from seven-eighths of the data set. This provides realistic training
examples for the reranker as these parses were not seen during grammar extraction. We
used the ranking MaxEnt implementation of MALLET (McCallum 2002) with default
parameters.
7. Monolingual Reranking
7.1 Subcategorization-Based Monolingual Reranking Features
We introduce here several novel subcategorization-based features for monolingual
reranking. For this, we first describe our algorithm for extracting subcategorization
(subcat) information. We use our enriched version of the Tiger2 training set. In order
to extract verbal subcat frames we find all nodes labeled with the category S (clause)
or VP-MO (modifying VP) and extract their arguments. Arguments22 are nodes of the
categories shown in Table 2. The arguments of nouns are obtained by looking for NN
nodes which are either dominated by an NP or a PP, and which take a following node
of category PP, VP-OC, or S-OC as argument.
The feature functions we present are mostly lexicalized. This means we need access
to the head words of the arguments. The argument heads are extracted as follows: As
NP headwe take the last nodewhose function label is either HD,NK, or PH. If this node
is of category NP or PN, we recursively select the head of that constituent. Similarly, the
head of an AP is the last node with functional label HD. If it is an AP, the head is
searched inside of it. In the case of PPs, we extract two heads, namely, the preposition
(or postposition) as well as the nominal head of the PP, which is found using similar
rules as for NPs. We also extract the case of the nominal head.
The extraction of verbal heads is somewhat more complicated. In order to obtain
the correct verbal head of a clause irrespective of the verb position (verb-first, verb-
second, verb-final), we extract all verbs that are dominated by the clause and a possibly
empty sequence of VP-OC or VP-PD (statal passive) nodes and an optional VZ-HD
node. Then we take the first non-finite verb, or alternatively the first finite verb if all
verbs were finite. In order to avoid sparse data problems caused by the many different
inflections of German verbs, we lemmatize the verbs.
21 In Fraser, Wang, and Schu?tze (2009) we used Minimum Error Rate Training. Once we made this change
to maximum entropy the results on small feature sets became similar (details omitted).
22 An exception to this is that if a PP argument dominates a node of category PROAV-PH, it is considered
a PROAV-PH argument. An example is the sentence Er [he] wartet [waits] (PP-OP (PROAV-PH darauf
[for this]), (S-RE dass [that] sie [she] kommt [comes])).
74
Fraser et al Knowledge Sources for Parsing German
Table 2
Arguments used in extracted subcategorization frames.
NP-SB, PN-SB, CNP-SB, S-SB, VP-SB subjects
NP-OA, PN-OA, CNP-OA direct objects
NP-DA, PN-DA, CNP-DA indirect objects
PRF-OA reflexive direct objects
PRF-DA reflexive indirect objects
NP-PD, CNP-PD predicative NPs
ADJD-PD, AP-PD, CAP-PD predicative adjectives
S-OC, CS-OC argument clauses
PP-OP, CPP-OP PP arguments
VP-OC/zu infinitival complement clauses
PROAV-OP pronominal adverbs serving as PP proxies such as
daraus [out of this]
NP-EP expletive subjects
VP-RE, NP-RE VP/NP appearing in expletive constructions
In the case of coordinated phrases, we take the head of the first conjunct. Arguments
are sorted to put them in a well-defined order. An example is that given the correct
parse of the sentence Statt [instead of] Details [details] zu [to] nennen [name], hat [has]
er [he] unverdrossen [assiduously] die [the] ?Erfolgsformel? [formula of success] wiederholt
[repeated], meaning ?instead of naming the details, he assiduously repeated the formula
of success,? we extract the two subcat frames:
VP-MO OBJ:Details VZ-HD:zu:nennen
S-TOP VP-MO SUBJ:er OBJ:Erfolgsformel VVPP-HD:wiederholt
We can now describe our features. The features focus on subcat frames taken from
S nodes (VP-MO is treated as S), and on attachment of prepositions and conjunctions to
nouns. We define conditional probability and mutual information (MI) features.
The two conditional probability features are ProbPrepAttach and ProbAdverb-
Attach, which calculate the probability for each preposition or adverb to be attached
to its governor, given the label of the governor. We estimate this from the training
data as follows, for the example of the PP feature. In the feature scoring, we give
each preposition attachment a score which is the negative log10 of the probabil-
ity p(lex prep|label governor) = f (lex prep, label governor)/f (label governor) (with a
cutoff of 5).
For all of our other monolingual features, we use (negative) pointwise mutual
information: ?log10(p(a, b)/p(a)p(b)) (here we use cutoffs of 5 and ?5).
MI NounP and MI NounConj give an assessment of a preposition or a conjunction
being attached to a noun (given the lexicalized preposition and the lexicalized noun).
For the MI VSubcat feature, we use as a the frame (without lexicalization), and as
b the head verb. p(a) is estimated as the relative frequency of this frame over all frames
extracted from Tiger2 train. MI VSimpleSubcat is a simpler version of MI VSubcat.
PP is excluded from frames because PP is often an adjunct rather than an argument.
For the MI VArg feature, we use as a the argument function and the head word
of the argument (e.g., OBJ:Buch, which is ?book? used as an object). As b we again
use the head verb. The estimate of p(a) is frequency(OBJ:Buch)/(total number of
extracted frames).23 In addition, this feature is refined into individual features for
23 We make the assumption that every frame has an object, but that this object can be NULL.
75
Computational Linguistics Volume 39, Number 1
different kinds of arguments: MI VSubj, MI VObj, MI VIobj, MI VPP, MI VPRF,
MI VS OC, MI VVP, and MI VerbPROAV. As an example, the MI of ?lesen, OBJ:Buch?
(reading, object:Book) would be used for the MI VArg features and for the MI VObj
feature. For functions such as MI VPP which are headed by both a function word (here,
a preposition) and a content word, only the function word is used (and no case).
The last MI feature is MI VParticle. Some German verbs contain a separable parti-
cle, which can also be analyzed as an adverb but will then have a different meaning. For
the sentence ?Und [and] Frau [Mrs.] Ku?nast [(proper name)] bringt [brings] das [that] auch
[also] nicht [not] ru?ber [across],? if ?ru?ber? is analyzed as an adverb, the verb means to
carry/take/bring over [to another physical location], but if it is viewed as a particle, the
sentence means Frau Ku?nast is not able to explain this. The feature MI VParticle helps
with this kind of disambiguation.
7.2 The Versley and Rehbein Feature Set
We also carried out experiments with the feature set of Versley and Rehbein (2009),
which is specially designed for German. It consists of features constructed from the
lexicalized parse tree along with features based on external statistical information.
The features here are local in the sense that their values can be computed at the
constituent in question, its daughters, and its spanning words. All features except
the external statistical information are binary and indicate that a lexicalized pattern is
present in the parse. They were originally designed for forest-based reranking (Versley
and Rehbein 2009). Following Charniak and Johnson (2005) we sum up these local
feature values in the parse tree. Thus our versions count the number of times that a
particular pattern occurs in the entire parse tree.
The patterns used can be further subcategorized into three groups. The wordform-
based patterns are token?POS (e.g., one pattern is ?lesen-VVINF?) and the word class
of the token in question (word class comes from an automatic clustering of words based
on contextual features). The constituent-based patterns are the size of the constituent,
the constituent label, and the right-hand side of the derivational rule applied at the node
in question. The last and biggest group of the pattern features is formed by the bilexical
dependencies. They are based on the head word of the constituent node in question
and its daughters. Versley and Rehbein (2009) have also introduced features that exploit
statistical information gathered from an external data set and aim to resolve PP attach-
ment ambiguity. Mutual information values were gathered on the association between
nouns and immediately following prepositions, as well as between prepositions and
closely following verbs on the DE-WaC corpus (Baroni and Kilgarriff 2006). These
feature values were then used at NP?PP and VP?PP daughter attachments.
A total of 2.7 million features fired in the Tiger train. We ignored features firing
in less than five sentences for computational efficiency, resulting in 117,000 extremely
sparse features.
7.3 Monolingual Reranking Experiments
We rerank 100-best lists from BitPar (Schmid 2004), which uses the grammar extraction
procedure and lexical resources introduced in Section 3. In each of the experiments we
extracted the grammar from the Tiger train and used it to obtain the 100-best parses for
the sentences of the evaluation corpus.
We trained reranking models on the Tiger train as described in Section 6 using our
subcategorization-based features, the Versley09 feature set, and the union of these two
76
Fraser et al Knowledge Sources for Parsing German
Table 3
The PARSEVAL score of monolingual features to rerank the parses of Europarl (seven-way
cross-validation on 662 sentences) and Tiger2 (development and test sets).
Tiger dev Tiger test Europarl CROSS Europarl IN
Baseline 82.42 (72.36) 76.84 (65.91) 77.13 (66.06)
subcat 83.19 (73.63) 77.65 (67.21) 77.23 (66.13) 77.73 (66.95)
Versley09 83.56 (73.89) 78.57 (68.42) 77.82 (66.87) 77.62 (66.05)
subcat+Versley09 84.19 (74.96) 78.86 (69.04) 77.76 (66.84) 77.93 (66.75)
sets. We evaluated the models on Tiger dev, Tiger test, and Europarl. As the domains
of Tiger and Europarl are quite different, besides this cross-domain parser evaluation
(CROSS) we carried out an in-domain (IN) evaluation as well. In the latter we followed
the seven-fold cross-validation approach, that is, the reranking models were trained on
six-sevenths of Europarl. The results are presented in Table 3.
The results presented in Table 3 show that the reranking models achieve an im-
provement over the baseline parser using both our and the Versley09 feature sets. The
Versley09 feature set achieved better results than our monolingual features when a
training dataset with sufficient size is given (Tiger). On the other hand using our 16
rich features (compared with 117,000 sparse features) is more suitable for the settings
where only a limited amount of training instances are available (the training sets consist
of 567 sentences of Europarl in seven-fold cross-validation). The rerankingmodels using
the union of the feature sets obtain close to the sum of the improvements of the two in-
dividual feature sets. The subcategorization features model rich non-local information,
and the fine-grained features capture local distinctions well and the features based on
the Web corpus access additional knowledge.
We performed an experiment adding one feature at a time, and found that the
most effective features were ProbAdverbAttach, MI VPP, MI VPRF, MI VSubj, and
MI VArg. After this the variation caused by numeric instability was too high to see a
consistent incremental gain from the rest of the features. We conclude that these features
can be robustly estimated and have more discriminative power than the others, but we
emphasize that we used all features in our experiments.
Figure 5 shows a parse tree produced by the BitPar parser in which the noun phrase
diese Finanzierung is incorrectly classified as an accusative object. The monolingual
subcategorization features MI VSubcat, MI VSimpleSubcat, and MI VArg enable the
reranker to correctly analyze the noun phrase as a subject and to move it from the VP
level to the S level.
.
S-TOP
PWAV-MO
Woher
where-from
VMFIN-HD
soll
should
VP-OC
NP-OA
PDAT-HD
diese
this
NN-HD
Finanzierung
financing
VVINF-HD
kommen
come
Figure 5
Erroneous parse produced by BitPar that is corrected by monolingual features.
77
Computational Linguistics Volume 39, Number 1
8. Bilingual Reranking
We now present our bilingual reranking framework. This follows our previous work
(Fraser, Wang, and Schu?tze 2009), which defined feature functions for reranking
English parses, but now we will use these same feature functions (and three additional
feature functions introduced to capture phenomena higher in the syntactic tree) to
rerank German parses. The intuition for using this type of bitext projection feature is
that ambiguous structures in one language often correspond to unambiguous structures
in another. Our feature functions are functions on the hypothesized English parse e,
the German parse g, and the word alignment a, and they assign a score (varying
between 0 and infinity) that measures syntactic divergence. The alignment of a sentence
pair is a function that, for each English word, returns a set of German words with
which the English word is aligned. Feature function values are calculated either by
taking the negative log of a probability, or by using a heuristic function which scales
similarly.24
The bilingual feature functions we define are functions that measure differ-
ent types of syntactic divergence between an English parse and a German parse.
Charniak and Johnson (2005) defined the state of the art in discriminative n-best
constituency parsing of English syntax (without the use of self-training). The n-best
output of their generative parser is reranked discriminatively by a reranker. We call
this CJRERANK. We will use an array of feature functions measuring the syntactic
divergence of candidate German parses with the projection of the English parse
obtained from CJRERANK.
In our experiments we use the English text of the parallel Treebank extracted from
the Europarl corpus and annotated by Pado? and Lapata (2009). There are 662 German
sentences that are aligned to single English sentences; this is the set that we use. Due to
the limited number of trees, we perform cross-validation to measure performance.
The basic idea behind our feature functions is that any constituent in a sentence
should play approximately the same syntactic role and have a similar span as the corre-
sponding constituent in a translation. If there is an obvious disagreement, it is probably
caused by wrong attachment or other syntactic mistakes in parsing. Sometimes in
translation the syntactic role of a given semantic constituent changes; we assume that
our model penalizes all hypothesized parses equally in this case.
To determine which features to describe here we conducted a greedy feature addi-
tion experiment (adding one feature at a time), on top of our best monolingual system
(combining both subcat and Versley09 feature sets). All bilingual experiments use all of
the features (not just the features we describe here). Definitions are available.25
BitParLogProb (the only monolingual feature used in the bilingual-only experi-
ment) is the negative log probability assigned by BitPar to the German parse.
8.1 Count Feature Functions
Count feature functions count projection constraint violations.
Feature CrdBin counts binary events involving the heads of coordinated phrases. If
we have a coordination where the English CC is aligned only with a German KON, and
24 A probability of 1 is a feature value of 0, whereas a low probability is a feature value which is
 0.
25 See http://www.ims.uni-stuttgart.de/tcl/RESOURCES/CL.html.
78
Fraser et al Knowledge Sources for Parsing German
Table 4
Other projection features selected; see the previously mentioned Web page25 for precise
definitions.
POSParentPrjWordPerG2E Computes the span difference between all the parent constituents
of POS tags in a German parse and their respective coverage
in the corresponding English parse, measured using percentage
coverage of the sentence in words. The feature value is the sum
of all the differences. The projection direction is from German to
English.
AbovePOSPrjPer Projection direction is from English to German, and measured in
percentage sentence coverage using characters, not words. The
feature value is calculated over all constituents above the POS
level in the English tree.
AbovePOSPrjWord Calculates a length-based difference using words.
POSPar2Prj Only applies when the POS tag?s parent has two children (the
POS tag has only one sibling). Projects from English to German
and calculates a length-based difference in characters.
POSPar2PrjPer Calculates a percentage-based difference based on characters.
POSPar2PrjG2E Like POSPar2Prj except projects from German to English.
POSPar2PrjWordG2E Like POSPar2PrjG2E except uses word-based differences.
both have two siblings, then the value contributed toCrdBin is 1 (indicating a constraint
violation) unless the head of the English left conjunct is aligned with the head of the
German left conjunct and likewise the right conjuncts are aligned.
Feature Q simply captures a mismatch between questions and statements. If a
German sentence is parsed as a question but the parallel English sentence is not, or
vice versa, the feature value is 1; otherwise the value is 0.
Feature S-OC considers that a clausal object (OC) in a German parse should be
projected to a simple declarative clause in English. This feature counts violations.
EngPPinSVP checks whether a PP inside of a S or VP in English attaches to the
same (projected) constituent in German. If an English PP follows immediately a VP or
a single verb, and the whole constituent is labeled ?S? or ?VP,? then the PP should be
identified as governed by the VP. In this case the corresponding German PP should
attach as well to the German VP to which the English VP is projected (attachment in
German can be to the left or to the right). If the governor in German does not turn out to
be a VP or have a tag starting with ?V,? a value of 1 will be added to the feature for this
German parse.
EngLeftSVP checks whether the left sibling of S or VP in English attaches to the
same (projected) constituent in German (where attachment can be left or right). This
feature counts violations.
Span Projection Feature Functions. Span projection features calculate an absolute or
percentage difference between a constituent?s span and the span of its projection. Span
size is measured in characters or words. To project a constituent in a parse, we use the
word alignment to project all word positions covered by the constituent and then look
for the smallest covering constituent in the parse of the parallel sentence.
PPParentPrjWord checks the correctness of PP attachment. It projects all the parents
of PP constituents in an English parse to German, and sums all the span differences. It is
measured in words. In addition to PPParentPrjWord we implement two bonus features,
NonPPWord and NonPPPer. The former simply calculates the number of words that
79
Computational Linguistics Volume 39, Number 1
do not belong to PP phrases in the sentence, and the latter computes the non-PP
proportion in a character-based fashion. These can be thought of as tunable parameters
which adjust PPParentPrjWord to not disfavor large PPs. The other selected projection
features are described in Table 4.
Probabilistic Feature Functions. We use Europarl (Koehn 2005), from which we
extract a parallel corpus of approximately 1.22 million sentence pairs, to estimate
the probabilistic feature functions described in this section.
We describe the feature PTag, despite the fact that it was not selected by the feature
analysis, because several variations (described next) were selected. PTag measures
tagging inconsistency based on estimating the probability for each English word that
it has a particular POS tag, given the aligned German word?s POS tag. To avoid noisy
feature values due to outliers and parse errors, we bound the value of PTag at 5.26 We
use relative frequency to estimate this feature. When an English word is aligned with
two words, estimation is more complex. We heuristically give each English and German
pair one count. The value calculated by the feature function is the geometric mean27 of
the pairwise probabilities.
The feature PTagEParent measures tagging inconsistency based on estimating the
probability that the parent of the English word at position i has a particular tag, given
the aligned German word?s POS label. PTagBiGLeft measures tagging inconsistency
based on estimating the probability for each English word that it has a particular POS
tag, given the aligned German word?s label and the word to the left of the aligned
German word?s label. PTagBiGParent measures tagging inconsistency based on esti-
mating the probability for each English word that it has a particular POS tag, given the
aligned German word?s label and the German word?s parent?s label.
8.2 Bilingual Reranking Experiments
We performed experiments looking at bilingual reranking performance. To train the
parameters of the probabilistic feature functions, we use 1-best parses of the large
Europarl parallel corpus (from CJRERANK and BitPar). We work on the same 100-best
list (of the German sentences in the small Pado? set) as was used in the previous section.
We parse the English sentences of the small Europarl set with CJRERANK; this parse is
used as our bilingual knowledge source. Finally we rerank using the bilingual features
(results in the first row of Table 5).
We then combine the monolingual features with the bilingual features. We rerank
using both the monolingual and the bilingual features together, and the results are
presented in Table 5. The bilingual feature-based reranker achieved 1 percentage point
improvement over the baseline. This advantage was just slightly decreasedwhenmono-
lingual features are also present. This indicates again that themonolingual and bilingual
features can capture different linguistic phenomena and their information content is
rather different. As in the Europarl IN setting, using the large sparse Versley09 feature
set the reranker could not learn a meaningful model from a moderate-sized training
data set.
26 Throughout this paper, assume log(0) = ??.
27 Each English word has the same weight regardless of whether it was aligned with one or with more
German words.
80
Fraser et al Knowledge Sources for Parsing German
Table 5
PARSEVAL scores of bi+monolingual features to rerank the parses of Europarl (seven-way
cross-validation) and the added value of bilingual features over the results achieved by the
corresponding monolingual feature set.
Mono features without bilingual with bilingual added value
NONE 77.13 (66.06) 78.10 (67.12) +0.97 (+1.06)
subcat 77.73 (66.95) 78.54 (67.95) +0.78 (+1.00)
Versley09 77.62 (66.05) 77.71 (66.06) +0.09 (+0.01)
subcat+Versley09 77.93 (66.75) 78.70 (67.45) +0.78 (+0.70)
The parse tree in Figure 6 demonstrates the value of bilingual features. It was
produced by the monolingual reranker and it incorrectly combines the two adverbs aber
and ebenso into an adverbial phrase and places this under the VP. The bilingual reranker
instead attaches the two adverbs separately at the S level. The attachment to the S node
indicates that the two adverbs modify the modal verb kann and not the full verb sagen.
This is triggered by the feature POSPar2Prj.
8.3 Previous Work on Bitext Parsing
Bitext parsing was also addressed by Burkett and Klein (2008). In that work, they use
feature functions defined on triples of (English parse tree, Chinese parse tree, alignment)
which are combined in a log-linear model, much as we do. In later work (Burkett,
Blitzer, and Klein 2010), they developed a unified joint model for solving the same
problem using a weakly synchronized grammar. To train these models they use a small
parallel Treebank that contains gold standard trees for parallel sentences in Chinese
and English, whereas we only require gold standard trees for the language we are
reranking. Another important difference is that Burkett and Klein (2008) use a large
number of automatically generated features (defined in terms of feature generation
templates) whereas we use a small number of carefully designed features that we found
by linguistic analysis of parallel corpora. Burkett, Blitzer, and Klein (2010) use a subset
of the features of Burkett and Klein (2008) for synchronization, along with monolin-
gual parsing and alignment based features. Finally, self-training (McClosky, Charniak,
and Johnson 2006b) is another differentiator of our work. We use probabilities esti-
mated from aligned English CJRERANK parses and German BitPar parses of the large
Europarl corpus in our bilingual feature functions. These feature functions are used to
.
S-TOP
PIS-SB
Man
one
VMFIN-HD
kann
can
VP-OC
AVP-MO
ADV-MO
aber
but
ADV-HD
ebenso
just-as-well
VVINF-HD
sagen
say
,
,
S-OC
KOUS-CP
dass
that
PPER-SB
sie
they
ADJD-PD
anspruchsvoll
demanding
VAFIN-HD
sind
are
Figure 6
Erroneous parse produced by the reranker using only monolingual features, which is corrected
by bilingual features. The sentence means One can, however, just as well say that they are demanding.
81
Computational Linguistics Volume 39, Number 1
improve ranking of German BitPar parses in the held-out test sets, which is a form of
self-training.
Two other interesting studies in this area are those of Fossum and Knight (2008)
and of Huang, Jiang, and Liu (2009). They improve English prepositional phrase at-
tachment using features from a Chinese sentence. Unlike our approach, however, they
do not require a Chinese syntactic parse as the word order in Chinese is sufficient to
unambiguously determine the correct attachment point of the prepositional phrase in
the English sentence without using a Chinese syntactic parse.
We know of no other work that has investigated to what extent monolingual and
bilingual features in parse reranking are complementary. In particular, the work on bi-
text parsing by Burkett and Klein (2008) does not address the question as to whether the
effect of monolingual and bilingual features in parse reranking is (partially) additive.
We demonstrate bilingual improvement for a strong parser of German. Previously,
we showed bilingual improvement for parsing English with an unlexicalized parser
(Fraser, Wang, and Schu?tze 2009), using 34 of the 37 bilingual feature functions we use
in this work.
9. Conclusion
In this paper, we have focused on MR&LC languages like German?languages that
are morphologically rich, but also have a strong configurational component. We have
argued that constituency parsing is, perhaps contrary to conventional wisdom, an ap-
propriate parsing formalism for MR&LC because constituents capture configurational
constraints in a transparent way and because for many applications constituency pars-
ing is preferable to dependency parsing. Our detailed description of a constituency
parsing system for a morphologically rich language, a system that addresses the major
problems that arise in constituency parsing for MR&LC, is one main contribution of this
paper. Two of these problems are rule proliferation and syncretism. We have addressed
rule proliferation bymarkovization and syncretism by (i) deploying a high performance
finite-state-based morphological analyzer that is based on rich lexical knowledge and
(ii) encoding grammatical functions directly as part of the phrase labels. This direct
encoding allows us to directly combine morphological and configurational informa-
tion in parsing and arrive at a maximally disambiguated parse. We argued that this
is the right setup for MR&LC languages because applications must have access to
grammatical functions.
A large part of this paper was concerned with making available and evaluating
additional knowledge sources for improved parsing of the MR&LC language German.
Our motivation was that (as we argued) MR&LC languages have in general higher am-
biguity than purely configurational and purely morphological languages, in particular
with respect to grammatical functions. Apart from the lexical knowledge embedded
in the morphological analyzer, we presented work on two other knowledge sources to
address this type of additional ambiguity: monolingual reranking (which looks at global
sentence-wide constraints for disambiguation) and bitext reranking (which exploits
parallel text for disambiguation). We were able to improve the performance of a strong
baseline parser using these three knowledge sources and we showed that they are
largely complementary: Performance improvements were additive when we used them
together. The resulting parser is currently the best constituent parser for German (with
or without bilingual features).
New languages and even new domains can require new treebanks. To create such
a treebank for a MR&LC language, we would first annotate a small number of gold
82
Fraser et al Knowledge Sources for Parsing German
standard trees, using parallel text with English or another language if such text is
available. Next, wewould consider how to quickly differentiate constituents of the same
type using constituent labels plus grammatical functions, as we outlined in Section 3.
Following this, we would use BitPar to build a parser in the same way as we presented
here, and to determine the optimal level of markovization, which we assume would be
very high with a small number of gold standard training trees. Next, as more trees are
annotated in an active learning framework, we would begin to develop morphological
analysis. We would implement the bilingual framework following this (if we have
access to bitext). Then we would implement basic subcategorization extraction and add
monolingual features. Finally, as more gold standard trees are annotated, the reranking
framework should be constantly retrained. In particular, we expect that the effect of the
knowledge sources we have presented will be much stronger when starting with less
training data.
Our work in this paper will be of use to developers of German syntactic parsers
as we have state-of-the-art performance using linguistically motivated features that are
easy to understand. We also hope that our work can serve as a cookbook of ideas to try
for others working on parsers for other morphologically rich languages.
Acknowledgments
We would like to thank Sandra Ku?bler and
Yannick Versley. We gratefully acknowledge
Deutsche Forschungsgemeinschaft (DFG)
for funding this work (grants SCHU 2246/
6-1Morphosyntax for MT and SFB 732
D4Modular lexicalization of PCFGs). This
work was supported in part by the IST
Programme of the European Community,
under the PASCAL2 Network of Excellence,
IST-2007-216886. This publication only
reflects the authors? views.
References
Baroni, Marco and Adam Kilgarriff. 2006.
Large linguistically processed Web
corpora for multiple languages.
In EACL: Posters & Demonstrations,
pages 87?90, Trento.
Black, E., S. Abney, S. Flickenger,
C. Gdaniec, C. Grishman, P. Harrison,
D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus,
S. Roukos, B. Santorini, and
T. Strzalkowski. 1991. Procedure for
quantitatively comparing the syntactic
coverage of English grammars. In
Proceedings of the Workshop on Speech
and Natural Language, HLT ?91,
pages 306?311, Pacific Grove, CA.
Brants, Sabine, Stefanie Dipper, Silvia
Hansen, Wolfgang Lezius, and George
Smith. 2002. The TIGER Treebank.
In Proceedings of the Workshop on
Treebanks and Linguistic Theories,
pages 24?41, Sozopol.
Brants, Thorsten. 2000. TnT?a statistical
part-of-speech tagger. In ANLP,
pages 224?231, Seattle, WA.
Burkett, David, John Blitzer, and Dan Klein.
2010. Joint parsing and alignment
with weakly synchronized grammars.
In HLT-NAACL, pages 127?135,
Los Angeles, CA.
Burkett, David and Dan Klein. 2008. Two
languages are better than one (for syntactic
parsing). In EMNLP, pages 877?886,
Honolulu, HI.
Cai, Shu, David Chiang, and Yoav Goldberg.
2011. Language-independent parsing with
empty elements. In ACL, pages 212?216,
Portland, OR.
Campbell, Richard. 2004. Using linguistic
principles to recover empty categories.
In ACL, pages 645?652, Barcelona.
Charniak, Eugene and Mark Johnson. 2005.
Coarse-to-fine n-best parsing and MaxEnt
discriminative reranking. In ACL,
pages 173?180, Ann Arbor, MI.
Collins, Michael. 1997. Three generative,
lexicalized models for statistical parsing.
In ACL, pages 16?23, Madrid.
Collins, Michael. 2000. Discriminative
reranking for natural language parsing.
In ICML, pages 25?70, Stanford, CA.
Cutting, Doug, Julian Kupiec, Jan Pedersen,
and Penelope Sibun. 1992. A practical
part-of-speech tagger. In ANLP,
pages 133?140, Trento.
Dreyer, Markus, David A. Smith, and
Noah A. Smith. 2006. Vine parsing and
minimum risk reranking for speed and
precision. In CoNLL, pages 201?205,
New York, NY.
83
Computational Linguistics Volume 39, Number 1
Dubey, Amit. 2004. Statistical Parsing for
German: Modeling Syntactic Properties
and Annotation Differences. Ph.D. thesis,
Saarland University.
Dubey, Amit and Frank Keller. 2003.
Probabilistic parsing for German using
sister-head dependencies. In ACL,
pages 96?103, Sapporo.
Duchier, Denys and Ralph Debusmann.
2001. Topological dependency trees:
a constraint-based account of linear
precedence. In ACL, pages 180?187,
Toulouse.
Finkel, Jenny Rose, Alex Kleeman, and
Christopher D. Manning. 2008. Efficient,
feature-based, conditional random
field parsing. In ACL, pages 959?967,
Columbus, OH.
Forst, Martin. 2007. Filling statistics
with linguistics?property design
for the disambiguation of German
LFG parses. In Proceedings of the ACL
Workshop on Deep Linguistic Processing,
pages 17?24, Prague.
Fossum, Victoria and Kevin Knight. 2008.
Using bilingual Chinese?English word
alignments to resolve PP-attachment
ambiguity in English. In AMTA,
pages 48?53, Honolulu, HI.
Fraser, Alexander, Renjing Wang,
and Hinrich Schu?tze. 2009. Rich
bitext projection features for parse
reranking. In EACL, pages 282?290,
Athens.
Gabbard, Ryan, Mitchell Marcus, and Seth
Kulick. 2006. Fully parsing the Penn
Treebank. In HLT-NAACL, pages 184?191,
Morristown, NJ.
Hall, Johan and Joakim Nivre. 2008.
A dependency-driven parser for
German dependency and constituency
representations. In Proceedings of the
Workshop on Parsing German, pages 47?54,
Columbus, OH.
Henderson, James, Paola Merlo, Gabriele
Musillo, and Ivan Titov. 2008. A latent
variable model of synchronous parsing
for syntactic and semantic dependencies.
In CoNLL, pages 178?182, Manchester.
Hsu, Yu-Yin. 2010. Comparing conversions
of discontinuity in PCFG parsing. In TLT,
pages 103?113, Tartu.
Huang, Liang. 2008. Forest reranking:
Discriminative parsing with non-local
features. In ACL, pages 586?594,
Columbus, OH.
Huang, Liang, Wenbin Jiang, and
Qun Liu. 2009. Bilingually constrained
(monolingual) shift-reduce parsing.
In EMNLP, pages 1,222?1,231,
Singapore.
Johnson, Mark. 1998. PCFG models
of linguistic tree representations.
Computational Linguistics, 24(4):613?632.
Johnson, Mark. 2001. A simple pattern-
matching algorithm for recovering empty
nodes and their antecedents. In ACL,
pages 136?143, Philadelphia, PA.
Klein, Dan and Christopher D. Manning.
2003. Accurate unlexicalized parsing.
In ACL, pages 423?430, Sapporo.
Koehn, Philipp. 2005. Europarl: a parallel
corpus for statistical machine translation.
InMT Summit X, pages 79?86, Phuket.
Ku?bler, Sandra. 2008. The PaGe 2008 shared
task on parsing German. In Proceedings
of the Workshop on Parsing German,
pages 55?63, Columbus, OH.
Ku?bler, Sandra, Erhard W. Hinrichs, and
Wolfgang Maier. 2006. Is it really that
difficult to parse German? In EMNLP,
pages 111?119, Sydney.
Levy, Roger and Christopher D. Manning.
2004. Deep dependencies from context-free
statistical parsers: Correcting the surface
dependency approximation. In ACL,
pages 327?334, Barcelona.
Lezius, Wolfgang, Stefanie Dipper, and Arne
Fitschen. 2000. IMSLex?representing
morphological and syntactical information
in a relational database. In EURALEX,
pages 133?139, Stuttgart.
McCallum, Andrew Kachites. 2002. Mallet:
A machine learning for language toolkit.
http://mallet.cs.umass.edu.
McClosky, David, Eugene Charniak,
and Mark Johnson. 2006a. Effective
self-training for parsing. In HLT-NAACL,
pages 152?159, Morristown, NJ.
McClosky, David, Eugene Charniak,
and Mark Johnson. 2006b. Reranking
and self-training for parser adaptation.
In COLING-ACL, pages 337?344,
Sydney.
McDonald, Ryan and Fernando Pereira.
2006. Online learning of approximate
dependency parsing algorithms.
In EACL, pages 81?88, Trento.
Menzel, Wolfgang and Ingo Schro?der.
1998. Decision procedures for dependency
parsing using graded constraints.
In COLING-ACL Workshop on Processing
of Dependency-Based Grammars,
pages 78?87, Montreal.
Pado?, Sebastian and Mirella Lapata. 2009.
Cross-lingual annotation projection of
semantic roles. Journal of Artificial
Intelligence Research, 36:307?340.
84
Fraser et al Knowledge Sources for Parsing German
Petrov, Slav and Dan Klein. 2007. Improved
inference for unlexicalized parsing.
In HLT-NAACL, pages 404?411,
Rochester, NY.
Petrov, Slav and Dan Klein. 2008. Parsing
German with latent variable grammars.
In Proceedings of the Workshop on Parsing
German, pages 33?39, Columbus, OH.
Quirk, Chris and Simon Corston-Oliver.
2006. The impact of parse quality on
syntactically-informed statistical
machine translation. In EMNLP,
pages 62?69, Sydney.
Quirk, Chris, Arul Menezes, and
Colin Cherry. 2005. Dependency treelet
translation: Syntactically informed
phrasal SMT. In ACL, pages 271?279,
Oxford.
Rafferty, Anna and Christopher D. Manning.
2008. Parsing three German Treebanks:
Lexicalized and unlexicalized baselines.
In Proceedings of the Workshop on Parsing
German, pages 40?46, Columbus, OH.
Rambow, Owen. 2010. The simple truth
about dependency and phrase structure
representations: an opinion piece.
In HLT-NAACL, pages 337?340,
Los Angeles, CA.
Rehbein, Ines and Josef van Genabith.
2007. Evaluating evaluation measures.
In NODALIDA, pages 372?379, Tartu.
Schiehlen, Michael. 2004. Annotation
strategies for probabilistic parsing in
German. In COLING, pages 390?396,
Geneva.
Schmid, Helmut. 1995. Improvements in
part-of-speech tagging with an application
to German. In Proceedings of the ACL
SIGDAT-Workshop, pages 47?50, Dublin.
Schmid, Helmut. 2004. Efficient parsing
of highly ambiguous context-free
grammars with bit vectors. In COLING,
pages 162?168, Geneva.
Schmid, Helmut. 2006. Trace prediction
and recovery with unlexicalized PCFGs
and slash features. In COLING-ACL,
pages 177?184, Sydney.
Schmid, Helmut, Arne Fitschen, and
Ulrich Heid. 2004. SMOR: A German
computational morphology covering
derivation, composition and inflection.
In LREC, pages 1,263?1,266, Lisbon.
Seeker, Wolfgang, Bernd Bohnet, Lilja
?vrelid, and Jonas Kuhn. 2010a.
Informed ways of improving data-driven
dependency parsing for German. In
COLING: Posters, pages 1,122?1,130,
Beijing.
Seeker, Wolfgang, Ines Rehbein, Jonas Kuhn,
and Josef Van Genabith. 2010b. Hard
constraints for grammatical function
labelling. In ACL, pages 1,087?1,097,
Uppsala.
Shen, Libin, Jinxi Xu, and Ralph Weischedel.
2008. A new string-to-dependency
machine translation algorithm with a
target dependency language model. In
ACL-HLT, pages 577?585, Columbus, OH.
Tsarfaty, Reut, Joakim Nivre, and Evelina
Andersson. 2012. Cross-framework
evaluation for statistical parsing.
In EACL, pages 44?54, Avignon.
Tsarfaty, Reut, Djame? Seddah, Yoav
Goldberg, Sandra Kuebler, Yannick
Versley, Marie Candito, Jennifer Foster,
Ines Rehbein, and Lamia Tounsi. 2010.
Statistical parsing of morphologically
rich languages (SPMRL) what, how and
whither. In Proceedings of the NAACL HLT
2010 First Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 1?12,
Los Angeles, CA.
Tu, Zhaopeng, Yang Liu, Young-Sook
Hwang, Qun Liu, and Shouxun Lin. 2010.
Dependency forest for statistical machine
translation. In COLING, pages 1,092?1,100,
Beijing.
Versley, Yannick. 2005. Parser evaluation
across text types. In Fourth Workshop on
Treebanks and Linguistic Theories (TLT),
pages 209?220, Barcelona.
Versley, Yannick and Ines Rehbein. 2009.
Scalable discriminative parsing for
German. In IWPT, pages 134?137, Paris.
Weischedel, Ralph, Marie Meteer, Richard
Schwartz, Lance Ramshaw, and Jeff
Palmucci. 1993. Coping with ambiguity
and unknown words through probabilistic
models. Computational Linguistics,
19(2):359?382.
Yusuke, Miyao and Tsujii Jun?ichi. 2002.
Maximum entropy estimation for
feature forests. In HLT, pages 292?297,
San Diego, CA.
85

Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 737?740,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Bitext-Based Resolution of German Subject-Object Ambiguities
Florian Schwarck Alexander Fraser
Institute for Natural Language Processing
University of Stuttgart
{koehlefn,fraser}@ims.uni-stuttgart.de
Hinrich Schu?tze
Abstract
We present a method for disambiguating syn-
tactic subjects from syntactic objects (a fre-
quent ambiguity) in German sentences taken
from an English-German bitext. We exploit
the fact that subject and object are usually eas-
ily determined in English. We show that a
simple method disambiguates some subject-
object ambiguities in German, while making
few errors. We view this procedure as the first
step in automatically acquiring (mostly) cor-
rect labeled data. We also evaluate using it to
improve a state of the art statistical parser.
1 Introduction
Ambiguity of grammatical role is a problem when
parsing a number of natural languages. In German,
subject-object ambiguities are frequent. The sen-
tence ?Die Maus jagt die Katze? ?the ? mouse ?
chases ? the ? cat? exhibits such an ambiguity. Be-
cause word order is freer in German than in English,
the sentence has two possible meanings: (i) The cat
is chasing the mouse and (ii) the mouse is chasing
the cat. We exploit the fact that such ambiguities are
much less frequent in languages that possess a less
flexible syntax than German. In English, the trans-
lation of the sentence ?Die Maus jagt die Katze? is
not ambiguous. If we have access to this translation,
we can use this information to disambiguate the Ger-
man sentence. The English translation is viewed as
a surrogate for both contextual knowledge from the
text and for world knowledge.
We present a method for disambiguating the sub-
ject and object roles in German sentences. We use
an English-German bitext and exploit the fact that
subject and object roles are rarely ambiguous in En-
glish. Using a new gold standard we created we
show that our method disambiguates a significant
proportion of subject-object ambiguities in German
with high precision. We view this procedure as the
first step in automatically acquiring (mostly) correct
labeled data for training a statistical disambiguator
that can be used on German text (even when no
translation is available). In addition to measuring
algorithm performance directly, we present experi-
ments on improving the disambiguation of BitPar, a
state of the art statistical parser.
2 Algorithm
Data and Word Alignment. We use the aligned
English and German sentences in Europarl (Koehn,
2005) for our experiments. The corpus contains long
and complex sentences. To establish translational
correspondence between parallel sentences we use
GIZA++ (Och and Ney, 2003). Its input is a tok-
enized parallel corpus. We lemmatized the text prior
to aligning it.
Procedure. Figure 1 shows the architecture of
our system. The boxes signify data sets, while the
lines are processes applied to the data sets. The pa-
per presents two applications. The first is the cre-
ation of a set of disambiguated German sentences
(which involves word alignments in the upper right
corner, and the use of parsers in the middle of the
graphic). We also present a reranking of the N -best
parses produced by BitPar (Schmid, 2004), a state of
the art statistical parser (bottom of the graphic).
For processing of German we chose FSPar
737
Figure 1: System Architecture
(Schiehlen, 2003), a fast shallow dependency parser.
FSPar has extensive lexical knowledge which helps
it to find subject-object ambiguities with high accu-
racy, but it does not try to resolve such ambiguities.
The key to our approach is to project syntactic
roles from English text. For English parsing we used
MINIPAR (Lin, 1998).
Based on FSPar?s analysis, all German sentences
with a subject-object ambiguity (about a third) were
selected from EuroParl. The parallel English sen-
tences were parsed with MINIPAR.
Words marked as ambiguous by FSPar were then
processed using our algorithm. If an ambiguous
German word was aligned to an English word that
MINIPAR had (unambiguously) assigned the gram-
matical role of subject or object, then the syntactic
role of the German word was defined by this infor-
mation, see Figure 2.
Figure 2: Disambiguation Algorithm
We used standard heuristics for improving word
alignment (Och and Ney, 2003; Koehn et al, 2003),
but there were many misalignments of ambiguous
German words. In order for the procedure to work,
we require that the German word to be disam-
biguated be aligned to the English subject or object.
For this reason, we implemented second guessing
based on a dictionary that lists for every German
word the 10 most frequently aligned English words
(found using the word alignment of all of Europarl).
If an ambiguous German word was either unaligned
or not aligned to the English subject or object, it was
checked whether a dictionary translation was part of
the parallel sentence and marked as subject or ob-
ject by MINIPAR. If so, this dictionary word was
used for disambiguation.
3 Evaluation
Gold Standard. We had access to a small set of
gold standard parses (Pado? and Lapata, 2009), but
decided to create a larger corpus. We found that FS-
Par had acceptable performance for finding subject-
object ambiguities1. The syntactic roles of words
marked as ambiguous by FSPar were annotated.
Four annotators annotated the syntactic roles in 4000
sentences using a graphical user interface (GUI).
The GUI showed the ambiguous words in context
and gave the annotator four different subject-object
labels to choose from for each ambiguous word:
subject, object, expletive es and none. Because the
syntactic expletive ?es? (English gloss: ?it?) is fre-
quent in German, as in ?es scheint zu regnen? ?it
appears to be raining,? we created a separate label
for expletive ?es?, which is not treated as a subject.2
The statistics are shown in table 1.
1000 sentences were annotated by all four an-
notators. Inter-annotator agreement was sufficient
(? = 0.77 on average (Carletta, 1996)).
Evaluation Measures. The output of our algo-
rithm labels each word that FSPar classified as am-
biguous with one of the three possible labels subject,
1FSPar has a very high precision in detecting subject-object
ambiguities, as can be seen in Table 1 (approximately 0.955,
the sum of two left columns divided by sum of all cells). We
tried to get an idea of recall using the smaller gold standard.
We made conservative assumptions about recall errors which
we manually checked on a small sample, details are omitted.
Using these assumptions led to an estimate for recall of 0.733,
but true recall is likely higher.
2German ?es? is also frequently used as a non-expletive,
where it can take a syntactic role.
738
subj obj expl es none
Annotator1 4152 3210 115 150
Annotator2 4472 3359 92 226
Annotator3 4444 3584 42 155
Annotator4 4027 3595 9 650
Table 1: Annotator decisions on the full gold standard
DE2EN Refined GDFA Intersection
nosg
P 0.8412 0.8381 0.8353 0.8551
R 0.4436 0.3856 0.3932 0.3380
F1 0.5809 0.5282 0.5347 0.4845
sg
P 0.7404 0.7307 0.7310 0.7240
R 0.5564 0.4873 0.4946 0.4528
F1 0.6353 0.5847 0.5900 0.5571
filter-nosg
P 0.9239 0.9203 0.9192 0.9277
R 0.3940 0.3397 0.3461 0.2984
F1 0.5524 0.4962 0.5028 0.4515
filter-sg
P 0.8458 0.8358 0.8369 0.8290
R 0.4839 0.4213 0.4279 0.3898
F1 0.6156 0.5602 0.5662 0.5302
Table 2: Precision, Recall and F1 of the algorithm.
object and no decision3. We use the standard evalua-
tion metrics Precision (P , the percentage of subject
and object labelings in our hypothesis that are cor-
rect), Recall (R, the percentage of subject and ob-
ject labelings in the gold standard that are correctly
labeled in the hypothesis), and balanced F (F1).
4 Experiments
Algorithm Performance. Table 2 shows the perfor-
mance of our algorithm when evaluated against the
manual annotation4. The lines nosg, sg, filter-nosg
and filter-sg denote different configurations of the al-
gorithm: Second guessing (section 2) was (?sg?) or
was not (?nosg?) applied and filtering was (?filter?)
or was not applied. The filter increases precision by
only keeping labels of subjects and objects that oc-
cur in the default order (e.g., the subject is to the
left of the object in the main clause). As an aid to
the user, FSPar presents such a determination of de-
fault order depending on its classification of clause
type5. The columns indicate the heuristic postpro-
3If expletive es or none was annotated, the system is correct
if it does not make a decision.
4Because of problems with BitPar caused by preprocessing
for FSPar, we use 11,279 sentences of the 13,000 annotated.
5Using this determination alone results in P 0.7728 R 0.8206
F 0.7960, very high recall but low precision.
configuration P R F1
1 top-1 (no change) 0.8088 0.8033 0.8060
2 relabeling nosg 0.7998 0.8176 0.8086
3 relabeling filter-nosg 0.8229 0.8344 0.8286
4 reranking nosg 0.8082 0.8123 0.8102
5 reranking filter-nosg 0.8145 0.8143 0.8144
Table 3: Precision, Recall and F1 of changing BitPar de-
cisions, DE2EN alignment
cessing we applied to GIZA++?s alignment. DE2EN
is the 1-to-N alignment calculated using German as
the source language and English as the target lan-
guage (i.e., each English word is linked to exactly
zero or one German words).
As we see in table 2, with the most strict setup,
filter-nosg, the algorithm resolves subject-object
ambiguities with a precision of more than 92%
but the best recall is only 39.4%, obtained using
DE2EN. Second guessing increases recall but leads
to losses in precision. The best precision result with-
out the filter is 85.5%.
Improving BitPar?s Subject-Object Decisions.
For improving BitPar (which always tries to disam-
biguate subject-object), our baseline is the accuracy
of the most probable parse shown in table 3, row 1.
Using the most probable parse from BitPar, we
relabel a word ?subject? or ?object? if our system
indicates to do so. With the algorithm alone we are
able to improve recall (table 3, row 2). When we add
the filter both precision and recall are improved (row
3). This experiment measures the improvement pos-
sible if our syntactic role information were directly
integrated as a hard constraint into a parser (see sec-
tion 5).
We now perform a simple reranking experiment,
using BitPar?s 100-best parses. For each sentence
we choose the parse which agrees with as many of
the subject/object decisions of the algorithm as pos-
sible (once again ignoring words where the algo-
rithm chooses no decision). In case of ties in the
number of agreements, we take the most probable
parse. The results are in rows 4?5. Reranking in-
creases F1 by about 0.8%.
5 Related Work
Syntactic projection has been used to bootstrap tree-
banks in resource poor languages (Yarowsky and
739
Ngai, 2001; Hwa et al, 2005). In contrast with such
work, we are addressing subject-object ambiguity in
German. German parsers have no access to the con-
textual and world knowledge necessary to resolve
this ambiguity.
Work on projecting semantic roles (Pado? and La-
pata, 2009; Fung et al, 2007) requires both syntac-
tic parsing and semantic role labeling and is con-
cerned with filling in the complete information in a
semantic frame. Our approach is simpler and con-
cerned only with syntactic disambiguation, not se-
mantic projection. We focus only on difficult cases
of subject-object ambiguity and although we do not
always make a prediction, we obtain levels of pre-
cision that projection approaches making no use of
knowledge of German syntax cannot achieve.
In bitext parsing, Burkett and Klein (2008) and
Fraser et al (2009) used feature functions defined on
triples of (parse tree in language 1, parse tree in lan-
guage 2, word alignment), combined in a log-linear
model trained to maximize parse accuracy, requir-
ing translated treebanks. We focus only on subject-
object disambiguation in German, and annotated a
new gold standard. We work on sentences that a
partial parser has determined to be ambiguous. Fos-
sum and Knight (2008) and Huang et al (2009) im-
prove English prepositional phrase attachment using
features from an unparsed Chinese sentence. The
latter work integrated the PP-attachment constraint
(detected from the Chinese translation) directly into
an English shift-reduce parser. As we have shown
in the labeling experiment, integrating our subject-
object disambiguation into BitPar could result in fur-
ther increases beyond 100-best reranking.
6 Conclusion
We demonstrated the utility of bitext-based disam-
biguation of grammatical roles. We automatically
created a large corpus of 164,874 disambiguated
subject-object decisions with a precision of over
92%. This corpus will be of use in future research
on syntactic role preferences and for the training
of monolingual subject-object disambiguators. We
presented a prototype application of subject-object
disambiguation through a simple reranking of the
100-best list output by BitPar, and showed a possible
further improvement if integrated in the parser. The
new gold standard, which is publicly available, will
hopefully be useful for work on both monolingual
and bitext-based disambiguation.
Acknowledgments
This work was supported by Deutsche Forschungs-
gemeinschaft grants SFB 732 and MorphoSynt.
References
David Burkett and Dan Klein. 2008. Two languages are
better than one (for syntactic parsing). In EMNLP.
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: The kappa statistic. Computational Lin-
guistics, 22.
Victoria Fossum and Kevin Knight. 2008. Using bilin-
gual Chinese-English word alignments to resolve PP-
attachment ambiguity in English. In AMTA.
Alexander Fraser, Renjing Wang, and Hinrich Schu?tze.
2009. Rich bitext projection features for parse rerank-
ing. In EACL.
Pascale Fung, Zhaojun Wu, Yongsheng Yang, and Dekai
Wu. 2007. Learning bilingual semantic frames: Shal-
low semantic parsing vs. semantic role projection. In
TMI.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In EMNLP.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Nat. Lang. Eng., 11(3).
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In HLT-NAACL.
Philipp Koehn. 2005. Europarl: a parallel corpus for
statistical machine translation. In MT Summit X.
Dekang Lin. 1998. Dependency-based evaluation of
MINIPAR. In Workshop on Eval of Parsing Systems.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1).
Sebastian Pado? and Mirella Lapata. 2009. Cross-lingual
annotation projection of semantic roles. Journal of Ar-
tificial Intelligence Research, 36:307?340.
Michael Schiehlen. 2003. A cascaded finite-state parser
for German. In Research Notes (EACL).
Helmut Schmid. 2004. Efficient parsing of highly am-
biguous context-free grammars with bit vectors. In
COLING.
David Yarowsky and Grace Ngai. 2001. Inducing multi-
lingual POS taggers and NP bracketers via robust pro-
jection across aligned corpora. In NAACL.
740
Proceedings of NAACL-HLT 2013, pages 1?11,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Model With Minimal Translation Units, But Decode With Phrases
Nadir Durrani?
University of Edinburgh
dnadir@inf.ed.ac.uk
Alexander Fraser Helmut Schmid
University of Stuttgart
fraser,schmid@ims.uni-stuttgart.de
Abstract
N-gram-based models co-exist with their
phrase-based counterparts as an alternative
SMT framework. Both techniques have pros
and cons. While the N-gram-based frame-
work provides a better model that captures
both source and target contexts and avoids
spurious phrasal segmentation, the ability to
memorize and produce larger translation units
gives an edge to the phrase-based systems dur-
ing decoding, in terms of better search per-
formance and superior selection of transla-
tion units. In this paper we combine N-gram-
based modeling with phrase-based decoding,
and obtain the benefits of both approaches.
Our experiments show that using this combi-
nation not only improves the search accuracy
of the N-gram model but that it also improves
the BLEU scores. Our system outperforms
state-of-the-art phrase-based systems (Moses
and Phrasal) and N-gram-based systems by
a significant margin on German, French and
Spanish to English translation tasks.
1 Introduction
Statistical Machine Translation advanced from
word-based models (Brown et al, 1993) towards
more sophisticated models that take contextual in-
formation into account. Phrase-based (Och and
Ney, 2004; Koehn et al, 2003) and N-gram-based
(Marin?o et al, 2006) models are two instances of
such frameworks. While the two models have some
common properties, they are substantially different.
?Much of the work presented here was carried out while the
first author was at the University of Stuttgart.
Phrase-based systems employ a simple and effec-
tive machinery by learning larger chunks of trans-
lation called phrases1. Memorizing larger units en-
ables the phrase-based model to learn local depen-
dencies such as short reorderings, idioms, insertions
and deletions, etc. The model however, has the fol-
lowing drawbacks: i) it makes independence as-
sumptions over phrases ignoring the contextual in-
formation outside of phrases ii) it has issues han-
dling long-distance reordering iii) it has the spurious
phrasal segmentation problem which allows multi-
ple derivations of a bilingual sentence pair having
different model scores for each segmentation.
Modeling with minimal translation units helps ad-
dress some of these issues. The N-gram-based SMT
framework is based on tuples. Tuples are mini-
mal translation units composed of source and target
cepts2. N-gram-based models are Markov models
over sequences of tuples (Marin?o et al, 2006; Crego
and Marin?o, 2006) or operations encapsulating tu-
ples (Durrani et al, 2011). This mechanism has sev-
eral useful properties. Firstly, no phrasal indepen-
dence assumption is made. The model has access
to both source and target context outside of phrases.
Secondly the model learns a unique derivation of a
bilingual sentence given its alignment, thus avoiding
the spurious segmentation problem.
Using minimal translation units, however, results
in a higher number of search errors because of i)
1A phrase-pair in PBSMT is a sequence of source and target
words that is translation of each other, and is not necessarily a
linguistic constituent. Phrases are built by combining minimal
translation units and ordering information.
2A cept is a group of words in one language that is translated
as a minimal unit in one specific context (Brown et al, 1993).
1
poor translation selection, ii) inaccurate future-cost
estimates and iii) incorrect early pruning of hypothe-
ses that would produce better model scores if al-
lowed to continue. In order to deal with these
problems, search is carried out only on a graph
of pre-calculated orderings, and ad-hoc reordering
limits are imposed to constrain the search space
(Crego et al, 2005; Crego and Marin?o, 2006), or
a higher beam size is used in decoding (Durrani
et al, 2011). The ability to memorize and pro-
duce larger translation chunks during decoding, on
the other hand, gives a distinct advantage to the
phrase-based system during search. Phrase-based
systems i) have access to uncommon translations,
ii) do not require higher beam sizes, iii) have more
accurate future-cost estimates because of the avail-
ability of phrase-internal language model context
before search is started. To illustrate this consider
the German-English phrase-pair ?scho? ein Tor ?
scored a goal?, composed from the tuples (cept-
pairs) ?scho? ? scored?, ?ein ? a? and ?Tor ? goal?.
It is likely that the N-gram system does not have
the tuple ?scho? ? scored? in its n-best translation
options because ?scored? is an uncommon transla-
tion for ?scho?? outside the sports domain. Even if
?scho? ? scored? is hypothesized, it will be ranked
quite low in the stack until ?ein? and ?Tor? are gen-
erated in the next steps. A higher beam is required
to prevent it from getting pruned. Phrase-based sys-
tems, on the other hand, are likely to have access to
the phrasal unit ?scho? ein Tor ? scored a goal? and
can generate it in a single step. Moreover, a more ac-
curate future-cost estimate can be computed because
of the available context internal to the phrase.
In this work, we extend the N-gram model, based
on operation sequences (Durrani et al, 2011), to
use phrases during decoding. The main idea is to
study whether a combination of modeling with min-
imal translation units and using phrasal information
during decoding helps to solve the above-mentioned
problems.
The remainder of this paper is organized as fol-
lows. The next two sections review phrase-based
and N-gram-based SMT. Section 2 provides a com-
parison of phrase-based and N-gram-based SMT.
Section 3 summarizes the operation sequence model
(OSM), the main baseline for this work. Section
4 analyzes the search problem when decoding with
Figure 1: Different Segmentations of a Bilingual Sen-
tence Pair
minimal units. Section 5 discusses how information
available in phrases can be used to improve search
performance. Section 6 presents the results of this
work. We conducted experiments on the German-to-
English and French-to-English translation tasks and
found that using phrases in decoding improves both
search accuracy and BLEU scores. Finally we com-
pare our system with two state-of-the-art phrase-
based systems (Moses and Phrasal) and two state-
of-the-art N-gram-based systems (Ncode and OSM)
on standard translation tasks.
2 Previous Work
Phrase-based and N-gram-based SMT are alter-
native frameworks for string-to-string translation.
Phrase-based SMT segments a bilingual sentence
pair into phrases that are continuous sequences of
words (Och and Ney, 2004; Koehn et al, 2003)
or discontinuous sequences of words (Galley and
Manning, 2010). These phrases are then reordered
through a lexicalized reordering model that takes
into account the orientation of a phrase with respect
to its previous phrase (Tillmann and Zhang, 2005)
or block of phrases (Galley and Manning, 2008).
There are several drawbacks of the phrase-based
model. Firstly it makes an independence assump-
tion over phrases, according to which phrases are
translated independently of each other, thus ignor-
ing the contextual information outside of the phrasal
boundary. This problem is corrected by the monolin-
gual language model that takes context into account.
But often the language model cannot compensate for
the dispreference of the translation model for non-
local dependencies. The second problem is that the
model is unaware of the actual phrasal segmentation
of a sentence during training. It therefore learns all
possible ways of segmenting a bilingual sentence.
Different segmentations of a bilingual sentence re-
2
sult in different probability scores for the translation
and reordering models, causing spurious ambiguity
in the model. See Figure 1. In the first segmentation,
the model learns the lexical and reordering proba-
bilities of the phrases ?sie wu?rden ? they would?
and ?gegen ihre kampagne abstimmen ? vote against
your campaign?. In the second segmentation, the
model learns the lexical and reordering probabilities
of the phrases ?sie ? they? ?wu?rden ? would?, ?ab-
stimmen ? vote?, ?gegen ihre kampagne ? against
your campaign?. Both segmentations result in dif-
ferent translation and reordering scores. This kind
of ambiguity in the model subsequently results in
the presence of many different equivalent segmen-
tations in the search space. Also note that the two
segmentations contain different information. From
the first segmentation the model learns the depen-
dency between the verb ?abstimmen ? vote? and the
phrase ?gegen ihre kampagne ? against your cam-
paign?. The second segmentation allows the model
to capture the reordering of the complex verb pred-
icate ?wu?rden ? would? and ?abstimmen ? vote? by
learning that the verb ?abstimmen ? vote? is discon-
tinuous with respect to the auxiliary. This informa-
tion cannot be captured in the first segmentation be-
cause of the phrasal independence assumption and
stiff phrasal boundaries. The model loses one of the
dependencies depending upon which segmentation
it chooses during decoding.
N-gram-based SMT is an instance of a joint
model that generates source and target strings to-
gether in bilingual translation units called tuples.
Tuples are essentially phrases but they are atomic
units that cannot be decomposed any further. This
condition of atomicity results in a unique segmen-
tation of the bilingual sentence pair given its align-
ments. The model does not make any phrasal inde-
pendence assumption and generates a tuple by look-
ing at a context of n ? 1 previous tuples (or opera-
tions). This allows the N-gram model to model all
the dependencies through a single derivation.
The main drawback of N-gram-based SMT is its
poor search mechanism which is inherent from us-
ing minimal translation units during search. Decod-
ing with tuples has problems with a high number
of search errors caused by lower translation cover-
age, inaccurate future-cost estimation and pruning
of correct hypotheses (see Section 4.2 for details).
Crego and Marin?o (2006) proposed a way to couple
reordering and search through POS-based rewrite
rules. These rules are learned during training when
units with crossing alignments are unfolded through
source linearization to form minimal tuples. For ex-
ample, in Figure 1, the N-gram-based MT will lin-
earize the word sequence ?gegen ihre kampagne ab-
stimmen? to ?abstimmen gegen ihre kampagne?, so
that it is in the same order as the English words.
It also learns a POS-rule ?IN PRP NN VB ? VB
IN PRP NN?. The POS-based rewrite rules serve
to precompute the orderings that are hypothesized
during decoding. Coupling reordering and search
allows the N-gram model to arrange hypotheses in
2m stacks (for an m word source sentence), each
containing hypotheses that cover exactly the same
foreign words. This removes the need for future-
cost estimation3. Secondly, memorizing POS-based
rules enables phrase-based like reordering, however
without lexical selection. There are three drawbacks
of this approach. Firstly, lexical generation and re-
ordering are decoupled. Search is only performed on
a small number of reorderings, pre-calculated using
the source side and completely ignoring the target-
side. And lastly, the POS-based rules face data spar-
sity problems especially in the case of long distance
reorderings.
Durrani et al (2011) recently addressed these
problems by proposing an operation sequence N-
gram model which strongly couples translation and
reordering, hypothesizes all possible reorderings
and does not require POS-based rules. Represent-
ing bilingual sentences as a sequence of operations
enables them to memorize phrases and lexical re-
ordering triggers like PBSMT. However, using min-
imal units during decoding and searching over all
possible reorderings means that hypotheses can no
longer be arranged in 2m stacks. The problem of
inaccurate future-cost estimates resurfaces resulting
in more search errors. A higher beam size of 500 is
therefore used to produce translation units in com-
parison to phrase-based systems. This, however,
still does not eliminate all search errors. This pa-
per shows that using phrases instead of cepts in de-
3Using m stacks with future-cost estimation is a more effi-
cient solution but is not used ?due to the complexity of accu-
rately computing these estimations in the N-gram architecture?
(Crego et al, 2011).
3
coding improves the search accuracy and translation
quality. It also shows that using some phrasal in-
formation in cept-based decoding captures some of
these improvements.
3 Operation Sequence Model
The N-gram model with integrated reordering mod-
els a sequence of operations obtained through the
transformation of a bilingual sentence pair. An op-
eration can either be to i) generate a sequence of
source and target words, ii) to insert a gap as a place-
holder for skipped words, iii) or to jump forward and
backward in a sentence to translate words discon-
tinuously. The translate operation Generate(X,Y)
encapsulates the translation tuple (X,Y). It gener-
ates source and target translations simultaneously4.
This is similar to N-gram-based SMT except that
the tuples in the N-gram-based model are generated
monotonically, whereas in this case lexical genera-
tion and reordering information is strongly coupled
in an operation sequence.
Consider the phrase pair:
The model memorizes it
through the sequence:
Generate(Wie, What is)? Gap? Generate (Sie,
your)? Jump Back (1)?Generate (heissen, name)
Let O = o1, . . . , oj?1 be a sequence of opera-
tions as hypothesized by the translator to generate
the bilingual sentence pair ?F,E? with an alignment
function A. The translation model is defined as:
p(F,E,A) = p(oJ1 ) =
J?
j=1
p(oj |oj?n+1, ..., oj?1)
where n indicates the amount of context used. The
translation model is implemented as an N-gram
model of operations using SRILM-Toolkit (Stol-
cke, 2002) with Kneser-Ney smoothing. A 9-gram
model is used. Several count-based features such as
gap and open gap penalties and distance-based fea-
tures such as gap-width and reordering distance are
added to the model, along with the lexical weighting
and length penalty features in a standard log-linear
framework (Durrani et al, 2011).
4The generation is carried out in the order of the target lan-
guage E.
4 Search
4.1 Overview of Decoding Framework
The decoding framework used in the operation se-
quence model is based on Pharaoh (Koehn, 2004a).
The decoder uses beam search to build up the trans-
lation from left to right. The hypotheses are ar-
ranged in m stacks such that stack i maintains hy-
potheses that have already translated i many foreign
words. The ultimate goal is to find the best scor-
ing hypothesis, that has translated all the words in
the foreign sentence. The overall process can be
roughly divided into the following steps: i) extrac-
tion of translation units ii) future-cost estimation, iii)
hypothesis extension iv) recombination and pruning.
During the hypothesis extension each extracted
phrase is translated into a sequence of operations.
The reordering operations (gaps and jumps) are gen-
erated by looking at the position of the translator,
the last foreign word generated etc. (Refer to Algo-
rithm 1 in Durrani et al (2011)). The probability of
an operation depends on the n ? 1 previous opera-
tions. The model backs-off to the smaller n-grams
of operations if the full history is unknown. We use
Kneser-Ney smoothing to handle back-off5.
4.2 Drawbacks of Cept-based Decoding
One of the main drawbacks of the operation se-
quence model is that it has a more difficult search
problem than the phrase-based model. The opera-
tion model, although based on minimal translation
units, can learn larger translation chunks by mem-
orizing a sequence of operations. However, using
cepts during decoding has the following drawbacks:
i) the cept-based decoder does not have access to
all the translation units that a phrase-based decoder
uses as part of a larger phrase. ii) it requires a higher
beam size to prevent early pruning of better hypothe-
ses that lead toward higher model scores when al-
lowed to continue and iii) it uses worse future-cost
estimates than the phrase-based decoder.
Recall the example from the last section. For
the cept-based decoder to generate the same phrasal
translation, it requires three separate tuple transla-
tions ?Wie ? what is?, ?Sie ? your? and ?hei?en ?
name?. Here we are faced with three challenges.
5We also tried Witten-Bell and Good Turing methods of dis-
counting and found Kneser-Ney smoothing to produce the best
results.
4
Translation Coverage: The first problem is that
the N-gram model does not have the same cov-
erage of translation options. The English cepts
?what is?, ?your? and ?name? are not good candi-
date translations for the German cepts ?Wie?, ?Sie?
and ?hei?en?, respectively. When extracting tuple
translations for these cepts from the Europarl data
for our system, the tuple ?Wie ? what is? is ranked
124th, ?hei?en ? name? is ranked 56th, and ?Sie ?
your? is ranked 9th in the list of n-best translation
candidates. Typically only the 20 best translation
options are used, to reduce the decoding time, and
such phrasal units with less frequent cept transla-
tions are never hypothesized in the N-gram-based
systems. The phrase-based system on the other hand
can extract the phrase ?Wie hei?en Sie ? what is
your name? even if it is observed only once dur-
ing training. A similar problem is also reported in
Costa-jussa` et al (2007). When trying to repro-
duce the sentences in the n-best translation output
of the phrase-based system, the N-gram-based sys-
tem was only able to produce 37.5% of the sen-
tences in the Spanish-to-English and 37.2% in the
English-to-Spanish translation tasks. In compar-
ison the phrase-based system was able to repro-
duce 57.5% and 48.6% of the sentences in the n-
best translation output of the Spanish-to-English and
English-to-Spanish N-gram-based systems.
Larger Beam Size: A related problem is that a
higher beam size is required in cept-based decod-
ing to prevent uncommon translations from getting
pruned. The phrase-based system can generate the
phrase-pair ?Wie hei?en Sie ? what is your name?
in a single step placing it directly into the stack three
words to the right. The cept-based decoder generates
this phrase in three stacks with the tuple translations
?Wie ? What is?, ?Sie ? your? and ?hei?en ? name?.
A very large stack size is required during decoding
to prevent the pruning of ?Wie ? What is? which is
ranked quite low in the stack until the tuple ?Sie ?
your? is hypothesized in the next stack. Costa-jussa`
et al (2007) reports a significant drop in the perfor-
mance of N-gram-based SMT when a beam size of
10 is used instead of 50 in their experiments. For the
(cept-based) operation sequence model, Durrani et
al. (2011) required a stack size of 500. In compari-
son, the translation quality achieved by phrase-based
SMT remains the same when varying the beam size
between 5 and 50.
Future-Cost Estimation: A third problem is
caused by inaccurate future-cost estimation. Using
phrases helps phrase-based SMT to better estimate
the future language model cost because of the larger
context available, and allows the decoder to capture
local (phrase-internal) reorderings in the future cost.
In comparison the future cost for tuples is mostly un-
igram probabilities. The future-cost estimate for the
phrase pair ?Wie hei?en Sie ? What is your name?
is estimated by calculating the cost of each feature.
The language model cost, for example, is estimated
in the phrase-based system as follows:
plm = p(What)? p(is|What)? p(your|What is)
? p(name|What is your)
The cost of the direct phrase translation probabil-
ity, one of the features used in the phrase translation
model, is estimated as:
ptm = p(What is your name|Wie hei?en Sie)
Phrase-based SMT is aware during the prepro-
cessing step that the words ?Wie hei?en Sie? may
be translated as a phrase. This is helpful for estimat-
ing a more accurate future cost because the phrase-
internal context is already available. The same is not
true for the operation sequence model, to which only
minimal units are available. The operation model
does not have the information that ?Wie hei?en Sie?
may be translated as a phrase during decoding. The
future-cost estimate available to the operation model
for the span covering ?Wie hei?en Sie? will have un-
igram probabilities for both the translation and lan-
guage model:
plm = p(What)? p(is|What)? p(your)? p(name)
ptm = p(Generate(Wie, What is))? p(Generate
(hei?en,name))? p(Generate(Sie, your))
Thus the future-cost estimate in the operation
model is much worse than that of the phrase-based
model. The poor future-cost estimation leads to
search errors, causing a drop in the translation qual-
ity. A more accurate future-cost estimate for the
translation model cost would be:
5
ptm = p(Generate(Wie,What is))? p(Insert Gap|C)
? p(Generate(Sie,your)|C)? p(Jump Back(1)|C)
p(Generate(hei?en,name)|C)
where C is the context, i.e., the n?1 previously gen-
erated operations. The future-cost estimates com-
puted in this manner are much more accurate be-
cause they not only consider context, but also take
the reordering operations into account.
5 N-gram Model with Phrase-based
Decoding
In the last section we discussed the disadvantages of
using cepts during search in a left-to-right decoding
framework. We now define a method to empirically
study the mentioned drawbacks and whether using
information available in phrase-pairs during decod-
ing can help improve search accuracy and translation
quality.
5.1 Training
We extended the training steps in Durrani et al
(2011) to extract a phrase lexicon from the paral-
lel data. We extract all phrase pairs of length 6 and
below, that are consistent (Och et al, 1999) with
the word alignments. Only continuous phrases as
used in a traditional phrase-based system are ex-
tracted thus allowing only inside-out (Wu, 1997)
type of alignments. The future cost of each fea-
ture component used in the log-linear model is cal-
culated. The operation sequence required to hypoth-
esize each phrase is generated and its future cost is
calculated. The future costs of other features such
as language models, lexicalized probability features,
etc. are also estimated. The estimates of the count-
based reordering penalties (gap penalty and open
gap penalty) and the distance-based features (gap-
width and reordering distance) could not be esti-
mated previously with cepts but are available when
using phrases.
5.2 Decoding
We extended the decoder developed by Durrani et al
(2011) and tried three ideas. In our primary experi-
ments we enabled the decoder to use phrases instead
of cepts. This allows the decoder to i) use phrase-
internal context when computing the future-cost es-
timates, ii) hypothesize translation options not avail-
able to the cept-based decoder iii) cover multiple
source words in a single step subsequently improv-
ing translation coverage and search. Note that us-
ing phrases instead of cepts during decoding, does
not reintroduce the spurious phrasal segmentation
problem as is present in the phrase-based system,
because the model is built on minimal units which
avoids segmentation ambiguity. Different compo-
sitions of the same phrasal unit lead to exactly the
same model score. We therefore do not create any
alternative compositions of the same phrasal unit
during decoding. This option is not available in
phrase-based decoding, because an alternative com-
position may lead towards a better model score.
In our secondary set of experiments, we used
cept-based decoding but modified the decoder to
use information available from the phrases extracted
for the test sentences. Firstly, we used future-cost
estimates from the extracted phrases (see system
cept.500.fc in Table1). This however, leads to in-
consistency in the cases where the future cost is es-
timated from some phrasal unit that cannot be gen-
erated through the available cept translations. For
example, say the best cost to cover the sequence
?Wie hei?en Sie? is given by the phrase ?What is
your name?. The 20-best translation options in cept-
based system, however, do not have tuples ?Wie ?
What? and ?hei?en ? name?. To remove this dis-
crepancy, we add all such tuples that are used in
the extracted phrases, to the list of extracted cepts
(system cept.500.fc.t). We also studied how much
gain we obtain by only adding tuples from phrases
and using cept-based future-cost estimates (system
cept.500.t).
5.3 Evaluation Method
To evaluate our modifications we apply a simple
strategy. We hold the model constant and change
the search to use the baseline decoder, which uses
minimal translation units, or the modified decoders
that use phrasal information during decoding. The
model parameters are optimized by running MERT
(minimum error rate training) for the baseline de-
coder on the dev set. After we have the optimized
weights, we run the baseline decoder and our mod-
ifications on the test. Note that because all the de-
coding runs use the same feature vector, the model
6
stays constant, only search changes. This allows us
to compare different decoding runs, obtained using
the same parameters, but different search strategies,
in terms of model scores. We compute a search ac-
curacy and translation quality for each run.
Search accuracy is computed by comparing trans-
lation hypotheses from the different decoding runs.
We form a collection of the best scoring hypotheses
by traversing through all the runs and selecting the
sentences with highest model score. For each input
sentence we select a single best scoring hypothesis.
The best scoring hypothesis can be contributed from
several runs. In this case all these runs will be given
a credit for that particular sentence when computing
the search accuracy. The search accuracy of a decod-
ing run is defined as the percentage of hypotheses
that were contributed from this run, when forming a
list of best scoring hypotheses. For example, for a
test set of 1000 sentences, the accuracy of a decod-
ing run would be 30% if it was able to produce the
best scoring hypothesis for 300 sentences in the test
set. Translation quality is measured through BLEU
(Papineni et al, 2002).
6 Experimental Setup
We initially experimented with two language pairs:
German-to-English (G-E) and French-to-English (F-
E). We trained our system and the baseline sys-
tems on most of the data6 made available for the
translation task of the Fourth Workshop on Statis-
tical Machine Translation.7 We used 1M bilin-
gual sentences, for the estimation of the transla-
tion model and 2M sentences from the monolingual
corpus (news commentary) which also contains the
English part of the bilingual corpus. Word align-
ments are obtained by running GIZA++ (Och and
Ney, 2003) with the grow-diag-final-and (Koehn et
al., 2005) symmetrization heuristic. We follow the
training steps described in Durrani et al (2011), con-
sisting of i) post-processing the alignments to re-
move discontinuous and unaligned target cepts, ii)
conversion of bilingual alignments into operation
sequences, iii) estimation of the n-gram language
models.
6We did not use all the available data due to scalability is-
sues. The scores reported are therefore well below those ob-
tained by the systems submitted to the WMT evaluation series.
7http://www.statmt.org/wmt09/translation-task.html
6.1 Search Accuracy Results
We divided our evaluation into two halves. In
the first half we carried out experiments to mea-
sure search accuracy and translation quality of
our decoders against the baseline cept-based OSM
(cept.500) that uses minimal translation units with a
stack size of 500. We used the version of the cept-
based OSM system that does not allow discontinu-
ous8 source cepts. To increase the speed of the sys-
tem we used a hard reordering limit of 159, in the
baseline decoder and our modifications, disallowing
jumps that are beyond 15 words from the first open
gap. For each extracted cept or phrase 10-best trans-
lation options are extracted.
Using phrases in search reduces the decoding
speed. In order to make a fair comparison, both the
phrase-based and the baseline cept-based decoders
should be allowed to run for the same amount of
time. We therefore reduced the stack size in the
phrase-based decoder so that it runs in the same
amount of time as the cept-based decoder. We found
that using a stack size of 20010 for the phrase-based
decoder was comparable in speed to using a stack-
size of 500 in the cept-based decoding.
We first tuned the baseline on dev11 to obtain an
optimized weight vector. We then ran the baseline
and our decoders as discussed in Section 5.2 on the
dev-test. Then we repeated this experiment by tun-
ing the weights with our phrase-based decoder (us-
ing a stack size of 100) and ran all the decoders again
using the new weights.
Table 1 shows the average search accuracies and
BLEU scores of the two experiments. Using phrases
during decoding in the G-E experiments resulted
in a statistically significant12 0.69 BLEU points
gain comparing our best system phrase.200 with the
baseline system cept.500. We mark a result as sig-
8Discontinuous source-side units did not lead to any im-
provements in (Durrani et al, 2011) and increased the decoding
times by multiple folds. We also found these to be less useful.
9Imposing a hard reordering limit significantly reduced the
decoding time and also slightly increased the BLEU scores.
10Higher stack sizes leads to improvement in model scores
for both German-English and French-English and slight im-
provement of BLEU in the case of the former.
11We used news-dev2009a as dev and news-dev2009b as dev-
test and tuned the weights with Z-MERT (Zaidan, 2009).
12We use bootstrap resampling (Koehn, 2004b) to test our
results against the baseline result.
7
System German French
Acc. BLEU Acc. BLEU
Baseline System cept.stack-size
cept.50 25.95% 19.50 42.10% 21.44
cept.100 30.04% 19.79 47.32% 21.70
cept.200 35.17% 19.98 51.47% 21.82
cept.500 41.56% 20.14 54.93% 21.87
Our Cept-based Decoders
cept.500.fc 48.44% 20.52* 54.73% 21.86
cept.500.t 52.24% 20.34 67.95% 22.00
cept.500.fc.t 61.81% 20.53* 67.76% 21.96
Our Phrase-based Decoders
phrase.50 58.88% 20.58* 80.83% 22.04
phrase.100 69.85% 20.73* 88.34% 22.13
phrase.200 79.71% 20.83* 92.93% 22.17*
Table 1: Search Accuracies (Acc.) and BLEU scores of
the Baseline and Our Decoders with different Stack Sizes
(fc = Future Cost Estimated from Phrases, t = Cept Trans-
lation Options enriched from Phrases)
nificant if the improvement shown by our decoder
over the baseline decoder (cept.500) is significant at
the p ? 0.05 level, in both the runs. All the out-
puts that show statistically significant improvements
over the baseline decoder (cept.500) in Table 1 are
marked with an asterisk.
The search accuracy of our best system
(phrase.200), in G-E experiments is roughly
80%, which means that 80% of the times the
phrase-based decoder (using stack size 200) was
able to produce the same model score or a better
model score than the cept-based decoders (using
a stack size of 500). Our F-E experiments also
showed improvements in BLEU and model scores.
The search accuracy of our best system phrase.200
is roughly 93% as compared with 55% in the
baseline decoder (cept.500) giving a BLEU point
gain of +0.30 over the baseline.
Our modifications to the cept-based decoder also
showed improvements. We found that extending
the cept translation table (cept.500.t) using phrases
helps both in G-E and F-E experiments by extend-
ing the list of n-best translation options by 18% and
18.30% respectively. Using future costs estimated
from phrases (cept.500.fc) improved both search ac-
curacy and BLEU scores in G-E experiments, but
does not lead to any improvements in the F-E ex-
periments, as both BLEU and model scores drop
slightly. We looked at a few examples where the
model score dropped and found that in these cases,
the best scoring hypotheses are ranked very low ear-
lier in the decoding and make their way to the top
gradually in subsequent steps. A slight difference in
the future-cost estimate prunes these hypotheses in
one or the other decoder. We found future cost to
be more critical in G-E than F-E experiments. This
can be explained by the fact that more reordering is
required in G-E and it is necessary to account for the
reordering operations and jump-based features (gap-
based penalties, reordering distance and gap-width)
in the future-cost estimation. F-E on the other hand
is largely monotonic except for a few short distance
reorderings such as flipping noun and adjective.
6.2 Comparison with other Baseline Systems
In the second half of our evaluation we compared
our best system phrase.200 with the baseline sys-
tem cept.500, and other state-of-the-art phrase-based
and N-gram-based systems on German-to-English,
French-to-English, and Spanish-to-English tasks13.
We used the official evaluation data (news-test sets)
from the Statistical Machine Translation Workshops
2009-2011 for all three language pairs (German,
Spanish and French). The feature weights for all the
systems are tuned using the dev set news-dev2009a.
We separately tune the baseline system (cept.500)
and the phrase-based system (phrase.200) and do not
hold the lambda vector constant like before.
Baseline Systems: We also compared our system
with i) Moses (Koehn et al, 2007), ii) Phrasal14 (Cer
et al, 2010), and iii) Ncode (Crego et al, 2011).
We used the default stack sizes of 100 for
Moses15, 200 for Phrasal, 25 for Ncode (with 2m
stacks). A 5-gram English language model is used.
Both phrase-based systems use 20-best phrases for
translation, Ncode uses 25-best tuple translations.
The training and test data for Ncode was tagged us-
ing TreeTagger (Schmid, 1994). All the baseline
systems used lexicalized reordering model. A hard
reordering limit16 of 6 words is used as a default in
13We did not include the results of Spanish in the previous
section due to space limitations but these are similar to those of
the French-to-English translation task.
14Phrasal provides two extensions to Moses: i) hierarchical
reordering model (Galley and Manning, 2008) and ii) discon-
tinuous phrases (Galley and Manning, 2010).
15Using stacks sizes from 200?1000 did not improve results.
16We tried to increase the distortion limit in the baseline sys-
8
both the baseline phrase-based systems. Amongst
the other defaults we retained the hard source gap
penalty of 15 and a target gap penalty of 7 in Phrasal.
We provide Moses and Ncode with the same post-
edited alignments17 from which we removed target-
side discontinuities. We feed the original alignments
to Phrasal because of its ability to learn discontinu-
ous source and target phrases. All the systems use
MERT for the optimization of the weight vector.
Ms Pd Nc C500 P200
German-to-English
MT09 18.73* 19.00* 18.37* 19.06* 19.66
MT10 18.58* 18.96* 18.64* 19.12* 19.70
MT11 17.38* 17.58* 17.49* 17.87* 18.19
French-to-English
MT09 24.61* 24.73* 24.28* 24.94* 25.27
MT10 23.69* 23.09* 23.96 23.90* 24.25
MT11 25.17* 25.55* 24.92* 25.40* 25.92
Spanish-to-English
MT09 24.38* 24.63 24.72 24.48* 24.72
MT10 25.55* 25.66* 25.87 25.68* 26.10
MT11 25.72* 26.17* 26.36* 26.48 26.67
Table 2: Comparison on 3-Test Sets ? Ms = Moses, Pd
= Phrasal (Discontinuous Phrases), Nc = Ncode, C500 =
Cept.500, P200 = Phrase.200
Table 2 compares the performance of our phrase-
based decoder against the baselines. Our system
shows an improvement over all the baseline systems
for the G-E pair, in 11 out of 12 cases in the F-E
pair and in 8 out of 12 cases in the S-E language
pair. We mark a baseline with ?*? to indicate that
our decoder shows an improvement over this base-
line result which is significant at the p ? 0.05 level.
7 Conclusion and Future Work
We proposed a combination of using a model based
on minimal units and decoding with phrases. Mod-
eling with minimal units enables us to learn local
and non-local dependencies in a unified manner and
avoid spurious segmentation ambiguities. However,
using minimal units also in the search presents a
significant challenge because of the poor transla-
tion coverage, inaccurate future-cost estimates and
tems to 15 (in G-E experiments) as used in our systems but the
results dropped significantly in case of Moses and slightly for
Phrasal so we used the default limits for both decoders.
17Using post-processed alignments gave slightly better re-
sults than the original alignments for these baseline systems.
Details are omitted due to space limitation.
the pruning of the correct hypotheses. Phrase-based
SMT on the other hand overcomes these drawbacks
by using larger translation chunks during search.
However, the drawback of the phrase-based model is
the phrasal independence assumption, spurious am-
biguity in segmentation and a weak mechanism to
handle non-local reorderings. We showed that com-
bining a model based on minimal units with phrase-
based decoding can improve both search accuracy
and translation quality. We also showed that the
phrasal information can be indirectly used in cept-
based decoding with improved results. We tested
our system against the state-of-the-art phrase-based
and N-gram-based systems, for German-to-English,
French-to-English, and Spanish-to-English for three
standard test sets. Our system showed statistically
significant improvements over all the baseline sys-
tems in most of the cases. We have shown the bene-
fits of using phrase-based search with a model based
on minimal units. In future work, we would like to
study whether a phrase-based system like Moses or
Phrasal can profit from an OSM-style or N-gram-
style feature. Feng et al (2010) previously showed
that adding a linearized source-side language model
in a phrase-based system helped. It would also
be interesting to study whether the insight of us-
ing minimal units for modeling and phrase-based
search would hold for hierarchical SMT. Vaswani et
al. (2011) recently showed that a Markov model over
the derivation history of minimal rules can obtain the
same translation quality as using grammars formed
with composed rules.
Acknowledgments
We would like to thank the anonymous reviewers
for their helpful feedback and suggestions. Nadir
Durrani and Alexander Fraser were funded by
Deutsche Forschungsgemeinschaft grant Models of
Morphosyntax for Statistical Machine Translation.
Nadir Durrani was partially funded by the European
Union Seventh Framework Programme (FP7/2007-
2013) under grant agreement n ? 287658. Helmut
Schmid was supported by Deutsche Forschungsge-
meinschaft grant SFB 732. This work was sup-
ported in part by the IST Programme of the Eu-
ropean Community, under the PASCAL2 Network
of Excellence, IST-2007-216886. This publication
only reflects the authors? views.
9
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and R. L. Mercer. 1993. The Mathematics
of Statistical Machine Translation: Parameter Estima-
tion. Computational Linguistics, 19(2):263?311.
Daniel Cer, Michel Galley, Daniel Jurafsky, and Christo-
pher D. Manning. 2010. Phrasal: A Statistical Ma-
chine Translation Toolkit for Exploring New model
Features. In Proceedings of the NAACL HLT 2010
Demonstration Session, pages 9?12, Los Angeles,
California, June.
Marta R. Costa-jussa`, Josep M. Crego, David Vilar,
Jose? A.R. Fonollosa, Jose? B. Marin?o, and Hermann
Ney. 2007. Analysis and System Combination of
Phrase- and N-Gram-Based Statistical Machine Trans-
lation Systems. In Human Language Technologies
2007: The Conference of the North American Chap-
ter of the Association for Computational Linguistics;
Companion Volume, Short Papers, pages 137?140,
Rochester, New York, April.
Josep M. Crego and Jose? B. Marin?o. 2006. Improving
Statistical MT by Coupling Reordering and Decoding.
Machine Translation, 20(3):199?215.
Josep M. Crego, Jose? B. Marin?o, and Adria` de Gispert.
2005. Reordered Search and Unfolding Tuples for N-
Gram-Based SMT. In Proceedings of the 10th Ma-
chine Translation Summit (MT Summit X), pages 283?
289, Phuket, Thailand.
Josep M. Crego, Franc?ois Yvon, and Jose? B. Marin?o.
2011. Ncode: an Open Source Bilingual N-gram SMT
Toolkit. The Prague Bulletin of Mathematical Lin-
guistics, (96):49?58.
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A Joint Sequence Translation Model with Inte-
grated Reordering. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies, pages 1045?
1054, Portland, Oregon, USA, June.
Minwei Feng, Arne Mauser, and Hermann Ney. 2010.
A Source-side Decoding Sequence Model for Statisti-
cal Machine Translation. In Conference of the Associ-
ation for Machine Translation in the Americas 2010,
Denver, Colorado, USA, October.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reordering
Model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 848?856, Honolulu, Hawaii, October.
Michel Galley and Christopher D. Manning. 2010. Ac-
curate Non-Hierarchical Phrase-Based Translation. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 966?
974, Los Angeles, California, June. Association for
Computational Linguistics.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proceedings
of HLT-NAACL, pages 127?133, Edmonton, Canada.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh System Description
for the 2005 IWSLT Speech Translation Evaluation. In
International Workshop on Spoken Language Transla-
tion 2005.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL 2007
Demonstrations, Prague, Czech Republic.
Philipp Koehn. 2004a. Pharaoh: A Beam Search De-
coder for Phrase-Based Statistical Machine Transla-
tion Models. In AMTA, pages 115?124.
Philipp Koehn. 2004b. Statistical Significance Tests
for Machine Translation Evaluation. In Dekang Lin
and Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July.
Jose? B. Marin?o, Rafael E. Banchs, Josep M. Crego, Adria`
de Gispert, Patrik Lambert, Jose? A. R. Fonollosa, and
Marta R. Costa-jussa`. 2006. N-gram-Based Machine
Translation. Computational Linguistics, 32(4):527?
549.
Franz J. Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Franz J. Och and Hermann Ney. 2004. The Alignment
Template Approach to Statistical Machine Translation.
Computational Linguistics, 30(1):417?449.
Franz J. Och, Christoph Tillmann, and Hermann Ney.
1999. Improved alignment models for statistical ma-
chine translation. In Joint SIGDAT Conf. on Empiri-
cal Methods in Natural Language Processing and Very
Large Corpora, pages 20?28, University of Maryland,
College Park, MD.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 311?318, Morris-
town, NJ, USA.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In International Conference
on New Methods in Language Processing, pages 44?
49, Manchester, UK.
10
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Intl. Conf. Spoken Lan-
guage Processing, Denver, Colorado.
Christoph Tillmann and Tong Zhang. 2005. A Local-
ized Prediction Model for Statistical Machine Transla-
tion. In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL?05),
pages 557?564, Ann Arbor, Michigan, June.
Ashish Vaswani, Haitao Mi, Liang Huang, and David
Chiang. 2011. Rule Markov Models for Fast Tree-to-
String Translation. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 856?864,
Portland, Oregon, USA, June.
Dekai Wu. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Corpora.
Computational Linguistics, 23(3):377?403.
Omar F. Zaidan. 2009. Z-MERT: A Fully Configurable
Open Source Tool for Minimum Error Rate Training
of Machine Translation Systems. The Prague Bulletin
of Mathematical Linguistics, 91:79?88.
11
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 465?474,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Hindi-to-Urdu Machine Translation Through Transliteration
Nadir Durrani Hassan Sajjad Alexander Fraser Helmut Schmid
Institute for Natural Language Processing
University of Stuttgart
{durrani,sajjad,fraser,schmid}@ims.uni-stuttgart.de
Abstract
We present a novel approach to integrate
transliteration into Hindi-to-Urdu statisti-
cal machine translation. We propose two
probabilistic models, based on conditional
and joint probability formulations, that are
novel solutions to the problem. Our mod-
els consider both transliteration and trans-
lation when translating a particular Hindi
word given the context whereas in pre-
vious work transliteration is only used
for translating OOV (out-of-vocabulary)
words. We use transliteration as a tool
for disambiguation of Hindi homonyms
which can be both translated or translit-
erated or transliterated differently based
on different contexts. We obtain final
BLEU scores of 19.35 (conditional prob-
ability model) and 19.00 (joint probability
model) as compared to 14.30 for a base-
line phrase-based system and 16.25 for a
system which transliterates OOV words in
the baseline system. This indicates that
transliteration is useful for more than only
translating OOV words for language pairs
like Hindi-Urdu.
1 Introduction
Hindi is an official language of India and is writ-
ten in Devanagari script. Urdu is the national lan-
guage of Pakistan, and also one of the state lan-
guages in India, and is written in Perso-Arabic
script. Hindi inherits its vocabulary from Sanskrit
while Urdu descends from several languages in-
cluding Arabic, Farsi (Persian), Turkish and San-
skrit. Hindi and Urdu share grammatical structure
and a large proportion of vocabulary that they both
inherited from Sanskrit. Most of the verbs and
closed-class words (pronouns, auxiliaries, case-
markers, etc) are the same. Because both lan-
guages have lived together for centuries, some
Urdu words which originally came from Arabic
and Farsi have also mixed into Hindi and are now
part of the Hindi vocabulary. The spoken form of
the two languages is very similar.
The extent of overlap between Hindi and Urdu
vocabulary depends upon the domain of the text.
Text coming from the literary domain like novels
or history tend to have more Sanskrit (for Hindi)
and Persian/Arabic (for Urdu) vocabulary. How-
ever, news wire that contains text related to me-
dia, sports and politics, etc., is more likely to have
common vocabulary.
In an initial study on a small news corpus of
5000 words, randomly selected from BBC1 News,
we found that approximately 62% of the Hindi
types are also part of Urdu vocabulary and thus
can be transliterated while only 38% have to be
translated. This provides a strong motivation to
implement an end-to-end translation system which
strongly relies on high quality transliteration from
Hindi to Urdu.
Hindi and Urdu have similar sound systems but
transliteration from Hindi to Urdu is still very hard
because some phonemes in Hindi have several or-
thographic equivalents in Urdu. For example the
?z? sound2 can only be written as whenever it
occurs in a Hindi word but can be written as ,
, and in an Urdu word. Transliteration
becomes non-trivial in cases where the multiple
orthographic equivalents for a Hindi word are all
valid Urdu words. Context is required to resolve
ambiguity in such cases. Our transliterator (de-
scribed in sections 3.1.2 and 4.1.3) gives an accu-
racy of 81.6% and a 25-best accuracy of 92.3%.
Transliteration has been previously used only as
a back-off measure to translate NEs (Name Enti-
ties) and OOV words in a pre- or post-processing
step. The problem we are solving is more difficult
than techniques aimed at handling OOV words,
1http://www.bbc.co.uk/hindi/index.shtml
2All sounds are represented using SAMPA notation.
465
Hindi Urdu SAMPA Gloss
/ Am Mango/Ordinary
/ d ZAli Fake/Net
/ Ser Lion/Verse
Table 1: Hindi Words That Can Be Transliterated
Differently in Different Contexts
Hindi Urdu SAMPA Gloss
/ simA Border/Seema
/ Amb@r Sky/Ambar
/ vId Ze Victory/Vijay
Table 2: Hindi Words That Can Be Translated or
Transliterated in Different Contexts
which focus primarily on name transliteration, be-
cause we need different transliterations in differ-
ent contexts; in their case context is irrelevant. For
example: consider the problem of transliterating
the English word ?read? to a phoneme represen-
tation in the context ?I will read? versus the con-
text ?I have read?. An example of this for Hindi
to Urdu transliteration: the two Urdu words
(face/condition) and (chapter of the Koran)
are both written as (sur@t d) in Hindi. The
two are pronounced identically in Urdu but writ-
ten differently. In such cases we hope to choose
the correct transliteration by using context. Some
other examples are shown in Table 1.
Sometimes there is also an ambiguity of
whether to translate or transliterate a particular
word. The Hindi word , for example, will
be translated to (peace, s@kun) when it is a
common noun but transliterated to (Shanti,
SAnt di) when it is a proper name. We try to
model whether to translate or transliterate in a
given situation. Some other examples are shown
in Table 2.
The remainder of this paper is organized as fol-
lows. Section 2 provides a review of previous
work. Section 3 introduces two probabilistic mod-
els for integrating translations and transliterations
into a translation model which are based on condi-
tional and joint probability distributions. Section 4
discusses the training data, parameter optimization
and the initial set of experiments that compare our
two models with a baseline Hindi-Urdu phrase-
based system and with two transliteration-aided
phrase-based systems in terms of BLEU scores
(Papineni et al, 2001). Section 5 performs an er-
ror analysis showing interesting weaknesses in the
initial formulations. We remedy the problems by
adding some heuristics and modifications to our
models which show improvements in the results as
discussed in section 6. Section 7 gives two exam-
ples illustrating how our model decides whether
to translate or transliterate and how it is able to
choose among different valid transliterations given
the context. Section 8 concludes the paper.
2 Previous Work
There has been a significant amount of work on
transliteration. We can break down previous work
into three groups. The first group is generic
transliteration work, which is evaluated outside of
the context of translation. This work uses either
grapheme or phoneme based models to translit-
erate words lists (Knight and Graehl, 1998; Li
et al, 2004; Ekbal et al, 2006; Malik et al,
2008). The work by Malik et al addresses Hindi to
Urdu transliteration using hand-crafted rules and
a phonemic representation; it ignores translation
context.
A second group deals with out-of-vocabulary
words for SMT systems built on large parallel cor-
pora, and therefore focuses on name translitera-
tion, which is largely independent of context. Al-
Onaizan and Knight (2002) transliterate Arabic
NEs into English and score them against their re-
spective translations using a modified IBM Model
1. The options are further re-ranked based on dif-
ferent measures such as web counts and using co-
reference to resolve ambiguity. These re-ranking
methodologies can not be performed in SMT at
the decoding time. An efficient way to compute
and re-rank the transliterations of NEs and inte-
grate them on the fly might be possible. However,
this is not practical in our case as our model con-
siders transliterations of all input words and not
just NEs. A log-linear block transliteration model
is applied to OOV NEs in Arabic to English SMT
by Zhao et al (2007). This work is also translit-
erating only NEs and not doing any disambigua-
tion. The best method proposed by Kashani et
al. (2007) integrates translations provided by ex-
ternal sources such as transliteration or rule-base
translation of numbers and dates, for an arbitrary
number of entries within the input text. Our work
is different from Kashani et al (2007) in that our
model compares transliterations with translations
466
on the fly whereas transliterations in Kashani et al
do not compete with internal phrase tables. They
only compete amongst themselves during a sec-
ond pass of decoding. Hermjakob et al (2008) use
a tagger to identify good candidates for translit-
eration (which are mostly NEs) in input text and
add transliterations to the SMT phrase table dy-
namically such that they can directly compete with
translations during decoding. This is closer to
our approach except that we use transliteration as
an alternative to translation for all Hindi words.
Our focus is disambiguation of Hindi homonyms
whereas they are concentrating only on translit-
erating NE?s. Moreover, they are working with
a large bitext so they can rely on their transla-
tion model and only need to transliterate NEs and
OOVs. Our translation model is based on data
which is both sparse and noisy. Therefore we pit
transliterations against translations for every input
word. Sinha (2009) presents a rule-based MT sys-
tem that uses Hindi as a pivot to translate from En-
glish to Urdu. This work also uses transliteration
only for the translation of unknown words. Their
work can not be used for direct translation from
Hindi to Urdu (independently of English) ?due to
various ambiguous mappings that have to be re-
solved?.
The third group uses transliteration models in-
side of a cross-lingual IR system (AbdulJaleel and
Larkey, 2003; Virga and Khudanpur, 2003; Pirkola
et al, 2003). Picking a single best transliteration
or translation in context is not important in an IR
system. Instead, all the options are used by giv-
ing them weights and context is typically not taken
into account.
3 Our Approach
Both of our models combine a character-based
transliteration model with a word-based transla-
tion model. Our models look for the most probable
Urdu token sequence un1 for a given Hindi token
sequence hn1 . We assume that each Hindi token is
mapped to exactly one Urdu token and that there is
no reordering. The assumption of no reordering is
reasonable given the fact that Hindi and Urdu have
identical grammar structure and the same word or-
der. An Urdu token might consist of more than one
Urdu word3. The following sections give a math-
3This occurs frequently in case markers with nouns,
derivational affixes and compounds etc. These are written
as single words in Hindi as opposed to Urdu where they are
ematical formulation of our two models, Model-1
and Model-2.
3.1 Model-1 : Conditional Probability Model
Applying a noisy channel model to compute the
most probable translation u?n1 , we get:
argmax
un1
p(un1 |h
n
1 ) = argmax
un1
p(un1 )p(h
n
1 |u
n
1 )
(1)
3.1.1 Language Model
The language model (LM) p(un1 ) is implemented
as an n-gram model using the SRILM-Toolkit
(Stolcke, 2002) with Kneser-Ney smoothing. The
parameters of the language model are learned from
a monolingual Urdu corpus. The language model
is defined as:
p(un1 ) =
n?
i=1
pLM (ui|u
i?1
i?k) (2)
where k is a parameter indicating the amount of
context used (e.g., k = 4 means 5-gram model).
ui can be a single or a multi-word token. A
multi-word token consists of two or more Urdu
words. For a multi-word ui we do multiple lan-
guage model look-ups, one for each uix in ui =
ui1 , . . . , uim and take their product to obtain the
value pLM (ui|u
i?1
i?k).
Language Model for Unknown Words: Our
model generates transliterations that can be known
or unknown to the language model and the trans-
lation model. We refer to the words known to
the language model and to the translation model
as LM-known and TM-known words respectively
and to words that are unknown as LM-unknown
and TM-unknown respectively.
We assign a special value ? to the LM-unknown
words. If one or more uix in a multi-word ui are
LM-unknown we assign a language model score
pLM (ui|u
i?1
i?k) = ? for the entire ui, meaning
that we consider partially known transliterations
to be as bad as fully unknown transliterations. The
parameter ? controls the trade-off between LM-
known and LM-unknown transliterations. It does
not influence translation options because they are
always LM-known in our case. This is because our
monolingual corpus also contains the Urdu part of
translation corpus. The optimization of ? is de-
scribed in section 4.2.1.
written as two words. For example (beautiful ; xub-
sur@t d) and (your?s ; ApkA) are written as
and respectively in Urdu.
467
3.1.2 Translation Model
The translation model (TM) p(hn1 |u
n
1 ) is approx-
imated with a context-independent model:
p(hn1 |u
n
1 ) =
n?
i=1
p(hi|ui) (3)
where hi and ui are Hindi and Urdu tokens re-
spectively. Our model estimates the conditional
probability p(hi|ui) by interpolating a word-
based model and a character-based (translitera-
tion) model.
p(hi|ui) = ?pw(hi|ui) + (1? ?)pc(hi|ui) (4)
The parameters of the word-based translation
model pw(h|u) are estimated from the word align-
ments of a small parallel corpus. We only retain
1-1/1-N (1 Hindi word, 1 or more Urdu words)
alignments and throw away N-1 and M-N align-
ments for our models. This is further discussed in
section 4.1.1.
The character-based transliteration model
pc(h|u) is computed in terms of pc(h, u), a joint
character model, which is also used for Chinese-
English back-transliteration (Li et al, 2004) and
Bengali-English name transliteration (Ekbal et al,
2006). The character-based transliteration proba-
bility is defined as follows:
pc(h, u) =
?
an1?align(h,u)
p(an1 )
=
?
an1?align(h,u)
n?
i=1
p(ai|a
i?1
i?k) (5)
where ai is a pair consisting of the i-th Hindi char-
acter hi and the sequence of 0 or more Urdu char-
acters that it is aligned with. A sample alignment
is shown in Table 3(b) in section 4.1.3. Our best
results are obtained with a 5-gram model. The
parameters p(ai|a
i?1
i?k) are estimated from a small
transliteration corpus which we automatically ex-
tracted from the translation corpus. The extrac-
tion details are also discussed in section 4.1.3. Be-
cause our overall model is a conditional probabil-
ity model, joint-probabilities are marginalized us-
ing character-based prior probabilities:
pc(h|u) =
pc(h, u)
pc(u)
(6)
The prior probability pc(u) of the character se-
quence u = cm1 is defined with a character-based
language model:
pc(u) =
m?
i=1
p(ci|c
i?1
i?k) (7)
The parameters p(ci|c
i?1
i?k) are estimated from
the Urdu part of the character-aligned translitera-
tion corpus. Replacing (6) in (4) we get:
p(hi|ui) = ?pw(hi|ui) + (1? ?)
pc(hi, ui)
pc(ui)
(8)
Having all the components of our model defined
we insert (8) and (2) in (1) to obtain the final equa-
tion:
u?n1 = argmax
un1
n?
i=1
pLM (ui|u
i?1
i?k)[?pw(hi|ui)
+ (1? ?)
pc(hi, ui)
pc(ui)
] (9)
The optimization of the interpolating factor ? is
discussed in section 4.2.1.
3.2 Model-2 : Joint Probability Model
This section briefly defines a variant of our model
where we interpolate joint probabilities instead of
conditional probabilities. Again, the translation
model p(hn1 |u
n
1 ) is approximated with a context-
independent model:
p(hn1 |u
n
1 ) =
n?
i=1
p(hi|ui) =
n?
i=1
p(hi, ui)
p(ui)
(10)
The joint probability p(hi, ui) of a Hindi and an
Urdu word is estimated by interpolating a word-
based model and a character-based model.
p(hi, ui) = ?pw(hi, ui)+(1??)pc(hi, ui) (11)
and the prior probability p(ui) is estimated as:
p(ui) = ?pw(ui) + (1? ?)pc(ui) (12)
The parameters of the translation model pw(hi, ui)
and the word-based prior probabilities pw(ui) are
estimated from the 1-1/1-N word-aligned corpus
(the one that we also used to estimate translation
probabilities pw(hi|ui) previously).
The character-based transliteration probability
pc(hi, ui) and the character-based prior probabil-
ity pc(ui) are defined by (5) and (7) respectively in
468
the previous section. Putting (11) and (12) in (10)
we get
p(hn1 |u
n
1 ) =
n?
i=1
?pw(hi, ui) + (1? ?)pc(hi, ui)
?pw(ui) + (1? ?)pc(ui)
(13)
The idea is to interpolate joint probabilities and di-
vide them by the interpolated marginals. The final
equation for Model-2 is given as:
u?n1 = argmax
un1
n?
i=1
pLM (ui|u
i?1
i?k)?
?pw(hi, ui) + (1? ?)pc(hi, ui)
?pw(ui) + (1? ?)pc(ui)
(14)
3.3 Search
The decoder performs a stack-based search using
a beam-search algorithm similar to the one used
in Pharoah (Koehn, 2004a). It searches for an
Urdu string that maximizes the product of trans-
lation probability and the language model proba-
bility (equation 1) by translating one Hindi word
at a time. It is implemented as a two-level pro-
cess. At the lower level, it computes n-best
transliterations for each Hindi word hi accord-
ing to pc(h, u). The joint probabilities given by
pc(h, u) are marginalized for each Urdu transliter-
ation to give pc(h|u). At the higher level, translit-
eration probabilities are interpolated with pw(h|u)
and then multiplied with language model probabil-
ities to give the probability of a hypothesis. We use
20-best translations and 25-best transliterations for
pw(h|u) and pc(h|u) respectively and a 5-gram
language model.
To keep the search space manageable and time
complexity polynomial we apply pruning and re-
combination. Since our model uses monotonic de-
coding we only need to recombine hypotheses that
have the same context (last n-1 words). Next we
do histogram-based pruning, maintaining the 100-
best hypotheses for each stack.
4 Evaluation
4.1 Training
This section discusses the training of the different
model components.
4.1.1 Translation Corpus
We used the freely available EMILLE Corpus
as our bilingual resource which contains roughly
13,000 Urdu and 12,300 Hindi sentences. From
these we were able to sentence-align 7000 sen-
tence pairs using the sentence alignment algorithm
given by Moore (2002).
The word alignments for this task were ex-
tracted by using GIZA++ (Och and Ney, 2003) in
both directions. We extracted a total of 107323
alignment pairs (5743 N-1 alignments, 8404 M-
N alignments and 93176 1-1/1-N alignments). Of
these alignments M-N and N-1 alignment pairs
were ignored. We manually inspected a sample of
1000 instances of M-N/N-1 alignments and found
that more than 70% of these were (totally or par-
tially) wrong. Of the 30% correct alignments,
roughly one-third constitute N-1 alignments. Most
of these are cases where the Urdu part of the align-
ment actually consists of two (or three) words
but was written without space because of lack of
standard writing convention in Urdu. For exam-
ple (can go ; d ZA s@kt de) is alterna-
tively written as (can go ; d ZAs@kt de)
i.e. without space. We learned that these N-1
translations could be safely dropped because we
can generate a separate Urdu word for each Hindi
word. For valid M-N alignments we observed that
these could be broken into 1-1/1-N alignments in
most of the cases. We also observed that we usu-
ally have coverage of the resulting 1-1 and 1-N
alignments in our translation corpus. Looking at
the noise in the incorrect alignments we decided
to drop N-1 and M-N cases. We do not model
deletions and insertions so we ignored null align-
ments. Also 1-N alignments with gaps were ig-
nored. Only the alignments with contiguous words
were kept.
4.1.2 Monolingual Corpus
Our monolingual Urdu corpus consists of roughly
114K sentences. This comprises 108K sentences
from the data made available by the University of
Leipzig4 + 5600 sentences from the training data
of each fold during cross validation.
4.1.3 Transliteration Corpus
The training corpus for transliteration is extracted
from the 1-1/1-N word-alignments of the EMILLE
corpus discussed in section 4.1.1. We use an edit
distance algorithm to align this training corpus at
the character level and we eliminate translation
pairs with high edit distance which are unlikely to
be transliterations.
4http://corpora.informatik.uni-leipzig.de/
469
We used our knowledge of the Hindi and Urdu
scripts to define the initial character mapping. The
mapping was further extended by looking into
available Hindi-Urdu transliteration systems[5,6]
and other resources (Gupta, 2004; Malik et al,
2008; Jawaid and Ahmed, 2009). Each pair in the
character map is assigned a cost. A Hindi charac-
ter that always map to only one Urdu character is
assigned a cost of 0 whereas the Hindi characters
that map to different Urdu characters are assigned
a cost of 0.2. The edit distance metric allows
insert, delete and replace operations. The hand-
crafted pairs define the cost of replace operations.
We set a cost of 0.6 for deletions and insertions.
These costs were optimized on held out data. The
details of optimization are not mentioned due to
limited space. Using this metric we filter out the
word pairs with high edit-distance to extract our
transliteration corpus. We were able to extract
roughly 2100 unique pairs along with their align-
ments. The resulting alignments are modified by
merging unaligned ? ? 1 (no character on source
side, 1 character on target side) or ? ? N align-
ments with the preceding alignment pair. If there
is no preceding alignment pair then it is merged
with the following pair. Table 3 gives an example
showing initial alignment (a) and the final align-
ment (b) after applying the merge operation. Our
model retains 1 ? ? and N ? ? alignments as
deletion operations.
a) Hindi ? b c ? e f
Urdu A XY C D ? F
b) Hindi b c e f
Urdu AXY CD ? F
Table 3: Alignment (a) Before (b) After Merge
The parameters pc(h, u) and pc(u) are trained
on the aligned corpus using the SRILM toolkit.
We use Add-1 smoothing for unigrams and
Kneser-Ney smoothing for higher n-grams.
4.1.4 Diacritic Removal and Normalization
In Urdu, short vowels are represented with diacrit-
ics but these are rarely written in practice. In or-
der to keep the data consistent, all diacritics are
removed. This loss of information is not harm-
ful when transliterating/translating from Hindi to
Urdu because undiacritized text is equally read-
5CRULP: http://www.crulp.org/software/langproc.htm
6Malerkotla.org: http://translate.malerkotla.co.in
able to native speakers as its diacritized counter
part. However leaving occasional diacritics in the
corpus can worsen the problem of data sparsity by
creating spurious ambiguity7.
There are a few Urdu characters that have mul-
tiple equivalent Unicodes. All such forms are nor-
malized to have only one representation8.
4.2 Experimental Setup
We perform a 5-fold cross validation taking 4/5 of
the data as training and 1/5 as test data. Each fold
comprises roughly 1400 test sentences and 5600
training sentences.
4.2.1 Parameter Optimization
Our model contains two parameters ? (the inter-
polating factor between translation and transliter-
ation modules) and ? (the factor that controls the
trade-off between LM-known and LM-unknown
transliterations). The interpolating factor ? is ini-
tialized, inspired by Written-Bell smoothing, with
a value of NN+B
9. We chose a very low value
1e?40 for the factor ? initially, favoring LM-
known transliterations very strongly. Both of these
parameters are optimized as described below.
Because our training data is very sparse we do
not use held-out data for parameter optimization.
Instead we optimize these parameters by perform-
ing a 2-fold optimization for each of the 5 folds.
Each fold is divided into two halves. The param-
eters ? and ? are optimized on the first half and
the other half is used for testing, then optimiza-
tion is done on the second half and the first half is
used for testing. The optimal value for parameter
? occurs between 0.7-0.84 and for the parameter
? between 1e?5 and 1e?10.
4.2.2 Results
Baseline Pb0: We ran Moses (Koehn et al, 2007)
using Koehn?s training scripts10, doing a 5-fold
cross validation with no reordering11. For the
other parameters we use the default values i.e.
5-gram language model and maximum phrase-
length= 6. Again, the language model is imple-
7It should be noted though that diacritics play a very im-
portant role when transliterating in the reverse direction be-
cause these are virtually always written in Hindi as dependent
vowels.
8www.crulp.org/software/langproc/urdunormalization.htm
9N is the number of aligned word pairs (tokens) and B is
the number of different aligned word pairs (types).
10http://statmt.org/wmt08/baseline.html
11Results are worse with reordering enabled.
470
M Pb0 Pb1 Pb2 M1 M2
BLEU 14.3 16.25 16.13 18.6 17.05
Table 4: Comparing Model-1 and Model-2 with
Phrase-based Systems
mented as an n-gram model using the SRILM-
Toolkit with Kneser-Ney smoothing. Each fold
comprises roughly 1400 test sentences, 5000 in
training and 600 in dev12. We also used two meth-
ods to incorporate transliterations in the phrase-
based system:
Post-process Pb1: All the OOV words in the
phrase-based output are replaced with their top-
candidate transliteration as given by our translit-
eration system.
Pre-process Pb2: Instead of adding translit-
erations as a post process we do a second pass
by adding the unknown words with their top-
candidate transliteration to the training corpus and
rerun Koehn?s training script with the new training
corpus. Table 4 shows results (taking arithmetic
average over 5 folds) from Model-1 and Model-
2 in comparison with three baselines discussed
above.
Both our systems (Model-1 and Model-2) beat
the baseline phrase-based system with a BLEU
point difference of 4.30 and 2.75 respectively. The
transliteration aided phrase-based systems Pb1
and Pb2 are closer to our Model-2 results but are
way below Model-1 results. The difference of
2.35 BLEU points between M1 and Pb1 indicates
that transliteration is useful for more than only
translating OOV words for language pairs like
Hindi-Urdu. Our models choose between trans-
lations and transliterations based on context un-
like the phrase-based systems Pb1 and Pb2 which
use transliteration only as a tool to translate OOV
words.
5 Error Analysis
Based on preliminary experiments we found three
major flaws in our initial formulations. This sec-
tion discusses each one of them and provides some
heuristics and modifications that we employ to try
to correct deficiencies we found in the two models
described in section 3.1 and 3.2.
12After having the MERT parameters, we add the 600 dev
sentences back into the training corpus, retrain GIZA, and
then estimate a new phrase table on all 5600 sentences. We
then use the MERT parameters obtained before together with
the newer (larger) phrase-table set.
5.1 Heuristic-1
A lot of errors occur because our translation model
is built on very sparse and noisy data. The moti-
vation for this heuristic is to counter wrong align-
ments at least in the case of verbs and functional
words (which are often transliterations). This
heuristic favors translations that also appear in the
n-best transliteration list over only-translation and
only-transliteration options. We modify the trans-
lation model for both the conditional and the joint
model by adding another factor which strongly
weighs translation+transliteration options by tak-
ing the square-root of the product of the translation
and transliteration probabilities. Thus modifying
equations (8) and (11) in Model-1 and Model-2
we obtain equations (15) and (16) respectively:
p(hi|ui) = ?1pw(hi|ui) + ?2
pc(hi, ui)
pc(ui)
+ ?3
?
pw(hi|ui)
pc(hi, ui)
pc(ui)
(15)
p(hi, ui) = ?1pw(hi, ui) + ?2pc(hi, ui)
+ ?3
?
pw(hi, ui)pc(hi, ui) (16)
For the optimization of lambda parameters we
hold the value of the translation coefficient ?113
and the transliteration coefficient ?2 constant (us-
ing the optimized values as discussed in section
4.2.1) and optimize ?3 again using 2-fold opti-
mization on all the folds as described above14.
5.2 Heuristic-2
When an unknown Hindi word occurs for which
all transliteration options are LM-unknown then
the best transliteration should be selected. The
problem in our original models is that a fixed LM
probability ? is used for LM-unknown transliter-
ations. Hence our model selects the translitera-
tion that has the best pc(hi,ui)pc(ui) score i.e. we max-
imize pc(hi|ui) instead of pc(ui|hi) (or equiva-
lently pc(hi, ui)). The reason is an inconsistency
in our models. The language model probabil-
ity of unknown words is uniform (and equal to
?) whereas the translation model uses the non-
uniform prior probability pc(ui) for these words.
There is another reason why we can not use the
13The translation coefficient ?1 is same as ? used in previ-
ous models and the transliteration coefficient ?2 = 1? ?
14After optimization we normalize the lambdas to make
their sum equal to 1.
471
value ? in this case. Our transliterator model also
produces space inserted words. The value of ? is
very small because of which transliterations that
are actually LM-unknown, but are mistakenly bro-
ken into constituents that are LM-known, will al-
ways be preferred over their counter parts. An ex-
ample of this is (America) for which two
possible transliterations as given by our model are
(AmerIkA, without space) and (AmerI
kA, with space). The latter version is LM-known
as its constituents are LM-known. Our models al-
ways favor the latter version. Space insertion is an
important feature of our transliteration model. We
want our transliterator to tackle compound words,
derivational affixes, case-markers with nouns that
are written as one word in Hindi but as two or more
words in Urdu. Examples were already shown in
section 3?s footnote.
We eliminate the inconsistency by using pc(ui)
as the 0-gram back-off probability distribution in
the language model. For an LM-unknown translit-
erations we now get in Model-1:
p(ui|u
i?1
i?k)[?pw(hi|ui) + (1? ?)
pc(hi, ui)
pc(ui)
]
= p(ui|u
i?1
i?k)[(1? ?)
pc(hi, ui)
pc(ui)
]
=
k?
j=0
?(ui?1i?j )pc(ui)[(1? ?)
pc(hi, ui)
pc(ui)
]
=
k?
j=0
?(ui?1i?j )[(1? ?)pc(hi, ui)]
where
?k
j=0 ?(u
i?1
i?j ) is just the constant that
SRILM returns for unknown words. The last
line of the calculation shows that we simply drop
pc(ui) if ui is LM-unknown and use the constant?k
j=0 ?(u
i?1
i?j ) instead of ?. A similar calculation
for Model-2 gives
?k
j=0 ?(u
i?1
i?j )pc(hi, ui).
5.3 Heuristic-3
This heuristic discusses a flaw in Model-2. For
transliteration options that are TM-unknown, the
pw(h, u) and pw(u) factors becomes zero and the
translation model probability as given by equation
(13) becomes:
(1? ?)pc(hi, ui)
(1? ?)pc(ui)
=
pc(hi, ui)
pc(ui)
In such cases the ? factor cancels out and no
weighting of word translation vs. transliteration
H1 H2 H12
M1 18.86 18.97 19.35
M2 17.56 17.85 18.34
Table 5: Applying Heuristics 1 and 2 and their
Combinations to Model-1 and Model-2
H3 H13 H23 H123
M2 18.52 18.93 18.55 19.00
Table 6: Applying Heuristic 3 and its Combina-
tions with other Heuristics to Model-2
occurs anymore. As a result of this, translitera-
tions are sometimes incorrectly favored over their
translation alternatives.
In order to remedy this problem we assign a
minimal probability ? to the word-based prior
pw(ui) in case of TM-unknown transliterations,
which prevents it from ever being zero. Because
of this addition the translation model probability
for LM-unknown words becomes:
(1? ?)pc(hi, ui)
?? + (1? ?)pc(ui)
where ? =
1
Urdu Types in TM
6 Final Results
This section shows the improvement in BLEU
score by applying heuristics and combinations of
heuristics in both the models. Tables 5 and 6 show
the improvements achieved by using the differ-
ent heuristics and modifications discussed in sec-
tion 5. We refer to the results as MxHy where x
denotes the model number, 1 for the conditional
probability model and 2 for the joint probability
model and y denotes a heuristic or a combination
of heuristics applied to that model15.
Both heuristics (H1 and H2) show improve-
ments over their base models M1 and M2.
Heuristic-1 shows notable improvement for both
models in parts of test data which has high num-
ber of common vocabulary words. Using heuris-
tic 2 we were able to properly score LM-unknown
transliterations against each other. Using these
heuristics together we obtain a gain of 0.75 over
M-1 and a gain of 1.29 over M-2.
Heuristic-3 remedies the flaw in M2 by assign-
ing a special value to the word-based prior pw(ui)
for TM-unknown words which prevents the can-
celation of interpolating parameter ?. M2 com-
bined with heuristic 3 (M2H3) results in a 1.47
15For example M1H1 refers to the results when heuristic-
1 is applied to model-1 whereas M2H12 refers to the results
when heuristics 1 and 2 are together applied to model 2.
472
BLEU point improvement and combined with all
the heuristics (M2H123) gives an overall gain of
1.95 BLEU points and is close to our best results
(M1H12). We also performed significance test
by concatenating all the fold results. Both our best
systems M1H12 and M2H123 are statistically sig-
nificant (p < 0.05)16 over all the baselines dis-
cussed in section 4.2.2.
One important issue that has not been investi-
gated yet is that BLEU has not yet been shown
to have good performance in morphologically rich
target languages like Urdu, but there is no metric
known to work better. We observed that some-
times on data where the translators preferred to
translate rather than doing transliteration our sys-
tem is penalized by BLEU even though our out-
put string is a valid translation. For other parts of
the data where the translators have heavily used
transliteration, the system may receive a higher
BLEU score. We feel that this is an interesting
area of research for automatic metric developers,
and that a large scale task of translation to Urdu
which would involve a human evaluation cam-
paign would be very interesting.
7 Sample Output
This section gives two examples showing how our
model (M1H2) performs disambiguation. Given
below are some test sentences that have Hindi
homonyms (underlined in the examples) along
with Urdu output given by our system. In the first
example (given in Figure 1) Hindi word can be
transliterated to ( Lion) or (Verse) depend-
ing upon the context. Our model correctly identi-
fies which transliteration to choose given the con-
text.
In the second example (shown in Figure 2)
Hindi word can be translated to (peace,
s@kun) when it is a common noun but transliter-
ated to (Shanti, SAnt di) when it is a proper
name. Our model successfully decides whether to
translate or transliterate given the context.
8 Conclusion
We have presented a novel way to integrate
transliterations into machine translation. In
closely related language pairs such as Hindi-Urdu
with a significant amount of vocabulary overlap,
16We used Kevin Gimpel?s tester
(http://www.ark.cs.cmu.edu/MT/) which uses bootstrap
resampling (Koehn, 2004b), with 1000 samples.
Ser d Z@ngl kA rAd ZA he
?Lion is the king of jungle?
AIqbAl kA Aek xub sur@t d Ser he
?There is a beautiful verse from Iqbal?
Figure 1: Different Transliterations in Different
Contexts
p hIr b hi vh s@kun se n@her?h s@kt dA
?Even then he can?t live peacefully?
Aom SAnt di Aom frhA xAn ki d dusri fIl@m he
?Om Shanti Om is Farah Khan?s second film?
Figure 2: Translation or Transliteration
transliteration can be very effective in machine
translation for more than just translating OOV
words. We have addressed two problems. First,
transliteration helps overcome the problem of data
sparsity and noisy alignments. We are able to gen-
erate word translations that are unseen in the trans-
lation corpus but known to the language model.
Additionally, we can generate novel translitera-
tions (that are LM-Unknown). Second, generat-
ing multiple transliterations for homograph Hindi
words and using language model context helps us
solve the problem of disambiguation. We found
that the joint probability model performs almost as
well as the conditional probability model but that
it was more complex to make it work well.
Acknowledgments
The first two authors were funded by the Higher
Education Commission (HEC) of Pakistan. The
third author was funded by Deutsche Forschungs-
gemeinschaft grants SFB 732 and MorphoSynt.
The fourth author was funded by Deutsche
Forschungsgemeinschaft grant SFB 732.
473
References
Nasreen AbdulJaleel and Leah S. Larkey. 2003. Sta-
tistical transliteration for English-Arabic cross lan-
guage information retrieval. In CIKM 03: Proceed-
ings of the twelfth international conference on In-
formation and knowledge management, pages 139?
146.
Yaser Al-Onaizan and Kevin Knight. 2002. Translat-
ing named entities using monolingual and bilingual
resources. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 400?408.
Asif Ekbal, Sudip Kumar Naskar, and Sivaji Bandy-
opadhyay. 2006. A modified joint source-channel
model for transliteration. In Proceedings of the
COLING/ACL poster sessions, pages 191?198, Syd-
ney, Australia. Association for Computational Lin-
guistics.
Swati Gupta. 2004. Aligning Hindi and Urdu bilin-
gual corpora for robust projection. Masters project
dissertation, Department of Computer Science, Uni-
versity of Sheffield.
Ulf Hermjakob, Kevin Knight, and Hal Daume? III.
2008. Name translation in statistical machine trans-
lation - learning when to transliterate. In Proceed-
ings of ACL-08: HLT, pages 389?397, Columbus,
Ohio. Association for Computational Linguistics.
Bushra Jawaid and Tafseer Ahmed. 2009. Hindi to
Urdu conversion: beyond simple transliteration. In
Conference on Language and Technology 2009, La-
hore, Pakistan.
Mehdi M. Kashani, Eric Joanis, Roland Kuhn, George
Foster, and Fred Popowich. 2007. Integration of an
Arabic transliteration module into a statistical ma-
chine translation system. In Proceedings of the Sec-
ond Workshop on Statistical Machine Translation,
pages 17?24, Prague, Czech Republic. Association
for Computational Linguistics.
Kevin Knight and Jonathan Graehl. 1998. Ma-
chine transliteration. Computational Linguistics,
24(4):599?612.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation for Computational Linguistics, Demonstra-
tion Program, Prague, Czech Republic.
Philipp Koehn. 2004a. Pharaoh: A beam search de-
coder for phrase-based statistical machine transla-
tion models. In AMTA, pages 115?124.
Philipp Koehn. 2004b. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July. Association
for Computational Linguistics.
Haizhou Li, Zhang Min, and Su Jian. 2004. A joint
source-channel model for machine transliteration.
In ACL ?04: Proceedings of the 42nd Annual Meet-
ing on Association for Computational Linguistics,
pages 159?166, Barcelona, Spain. Association for
Computational Linguistics.
M G Abbas Malik, Christian Boitet, and Pushpak Bhat-
tacharyya. 2008. Hindi Urdu machine translitera-
tion using finite-state transducers. In Proceedings
of the 22nd International Conference on Computa-
tional Linguistics, Manchester, UK.
Robert C. Moore. 2002. Fast and accurate sentence
alignment of bilingual corpora. In Conference of the
Association for Machine Translation in the Ameri-
cas (AMTA).
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Kishore A. Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2001. BLEU: a method for auto-
matic evaluation of machine translation. Technical
Report RC22176 (W0109-022), IBM Research Di-
vision, Thomas J. Watson Research Center, York-
town Heights, NY.
Ari Pirkola, Jarmo Toivonen, Heikki Keskustalo, Kari
Visala, and Kalervo Ja?rvelin. 2003. Fuzzy trans-
lation of cross-lingual spelling variants. In SIGIR
?03: Proceedings of the 26th annual international
ACM SIGIR conference on Research and develop-
ment in informaion retrieval, pages 345?352, New
York, NY, USA. ACM.
R. Mahesh K. Sinha. 2009. Developing English-Urdu
machine translation via Hindi. In Third Workshop
on Computational Approaches to Arabic Script-
based Languages (CAASL3), MT Summit XII, Ot-
tawa, Canada.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Intl. Conf. Spoken Lan-
guage Processing, Denver, Colorado.
Paola Virga and Sanjeev Khudanpur. 2003. Translit-
eration of proper names in cross-lingual information
retrieval. In Proceedings of the ACL 2003 workshop
on Multilingual and mixed-language named entity
recognition, pages 57?64, Morristown, NJ, USA.
Association for Computational Linguistics.
Bing Zhao, Nguyen Bach, Ian Lane, and Stephan Vo-
gel. 2007. A log-linear block transliteration model
based on bi-stream HMMs. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 364?371, Rochester, New York. Associ-
ation for Computational Linguistics.
474
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 430?439,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
An Algorithm for Unsupervised Transliteration Mining with an Application
to Word Alignment
Hassan Sajjad Alexander Fraser Helmut Schmid
Institute for Natural Language Processing
University of Stuttgart
{sajjad,fraser,schmid}@ims.uni-stuttgart.de
Abstract
We propose a language-independent method
for the automatic extraction of transliteration
pairs from parallel corpora. In contrast to
previous work, our method uses no form of
supervision, and does not require linguisti-
cally informed preprocessing. We conduct
experiments on data sets from the NEWS
2010 shared task on transliteration mining and
achieve an F-measure of up to 92%, out-
performing most of the semi-supervised sys-
tems that were submitted. We also apply our
method to English/Hindi and English/Arabic
parallel corpora and compare the results with
manually built gold standards which mark
transliterated word pairs. Finally, we integrate
the transliteration module into the GIZA++
word aligner and evaluate it on two word
alignment tasks achieving improvements in
both precision and recall measured against
gold standard word alignments.
1 Introduction
Most previous methods for building transliteration
systems were supervised, requiring either hand-
crafted rules or a clean list of transliteration pairs,
both of which are expensive to create. Such re-
sources are also not applicable to other language
pairs.
In this paper, we show that it is possible to ex-
tract transliteration pairs from a parallel corpus us-
ing an unsupervised method. We first align a bilin-
gual corpus at the word level using GIZA++ and
create a list of word pairs containing a mix of non-
transliterations and transliterations. We train a sta-
tistical transliterator on the list of word pairs. We
then filter out a few word pairs (those which have
the lowest transliteration probabilities according to
the trained transliteration system) which are likely
to be non-transliterations. We retrain the translitera-
tor on the filtered data set. This process is iterated,
filtering out more and more non-transliteration pairs
until a nearly clean list of transliteration word pairs
is left. The optimal number of iterations is automat-
ically determined by a novel stopping criterion.
We compare our unsupervised transliteration min-
ing method with the semi-supervised systems pre-
sented at the NEWS 2010 shared task on translit-
eration mining (Kumaran et al, 2010) using four
language pairs. We refer to this task as NEWS10.
These systems used a manually labelled set of data
for initial supervised training, which means that
they are semi-supervised systems. In contrast, our
system is fully unsupervised. We achieve an F-
measure of up to 92% outperforming most of the
semi-supervised systems.
The NEWS10 data sets are extracted Wikipedia
InterLanguage Links (WIL) which consist of par-
allel phrases, whereas a parallel corpus consists of
parallel sentences. Transliteration mining on the
WIL data sets is easier due to a higher percentage
of transliterations than in parallel corpora. We also
do experiments on parallel corpora for two language
pairs. To this end, we created gold standards in
which sampled word pairs are annotated as either
transliterations or non-transliterations. These gold
standards have been submitted with the paper as sup-
plementary material as they are available to the re-
search community.
430
Finally we integrate a transliteration module into
the GIZA++ word aligner and show that it improves
word alignment quality. The transliteration mod-
ule is trained on the transliteration pairs which our
mining method extracts from the parallel corpora.
We evaluate our word alignment system on two lan-
guage pairs using gold standard word alignments
and achieve improvements of 10% and 13.5% in pre-
cision and 3.5% and 13.5% in recall.
The rest of the paper is organized as follows. In
section 2, we describe the filtering model and the
transliteration model. In section 3, we present our
iterative transliteration mining algorithm and an al-
gorithm which computes a stopping criterion for the
mining algorithm. Section 4 describes the evaluation
of our mining method through both gold standard
evaluation and through using it to improve word
alignment quality. In section 5, we present previous
work and we conclude in section 6.
2 Models
Our algorithms use two different models. The first
model is a joint character sequence model which
we apply to transliteration mining. We use the
grapheme-to-phoneme converter g2p to implement
this model. The other model is a standard phrase-
based MT model which we apply to transliteration
(as opposed to transliteration mining). We build it
using the Moses toolkit.
2.1 Joint Sequence Model Using g2p
Here, we briefly describe g2p using notation from
Bisani and Ney (2008). The details of the model,
its parameters and the utilized smoothing techniques
can be found in Bisani and Ney (2008).
The training data is a list of word pairs (a source
word and its presumed transliteration) extracted
from a word-aligned parallel corpus. g2p builds a
joint sequence model on the character sequences of
the word pairs and infers m-to-n alignments between
source and target characters with Expectation Maxi-
mization (EM) training. The m-to-n character align-
ment units are referred to as ?multigrams?.
The model built on multigrams consisting of
source and target character sequences greater than
one learns too much noise (non-transliteration infor-
mation) from the training data and performs poorly.
In our experiments, we use multigrams with a maxi-
mum of one character on the source and one charac-
ter on the target side (i.e., 0,1-to-0,1 character align-
ment units).
The N-gram approximation of the joint probabil-
ity can be defined in terms of multigrams qi as:
p(qk1 ) ?
k+1?
j=1
p(qj |q
j?1
j?N+1) (1)
where q0, qk+1 are set to a special boundary symbol.
N-gram models of order > 1 did not work well
because these models tended to learn noise (infor-
mation from non-transliteration pairs) in the training
data. For our experiments, we only trained g2p with
the unigram model.
In test mode, we look for the best sequence of
multigrams given a fixed source and target string and
return the probability of this sequence.
For the mining process, we trained g2p on
lists containing both transliteration pairs and non-
transliteration pairs.
2.2 Statistical Machine Transliteration System
We build a phrase-based MT system for translitera-
tion using the Moses toolkit (Koehn et al, 2003). We
also tried using g2p for implementing the translit-
eration decoder but found Moses to perform bet-
ter. Moses has the advantage of using Minimum Er-
ror Rate Training (MERT) which optimizes translit-
eration accuracy rather than the likelihood of the
training data as g2p does. The training data con-
tains more non-transliteration pairs than transliter-
ation pairs. We don?t want to maximize the like-
lihood of the non-transliteration pairs. Instead we
want to optimize the transliteration performance for
test data. Secondly, it is easy to use a large language
model (LM) with Moses. We build the LM on the
target word types in the data to be filtered.
For training Moses as a transliteration system, we
treat each word pair as if it were a parallel sentence,
by putting spaces between the characters of each
word. The model is built with the default settings
of the Moses toolkit. The distortion limit ?d? is set
to zero (no reordering). The LM is implemented as
a five-gram model using the SRILM-Toolkit (Stol-
cke, 2002), with Add-1 smoothing for unigrams and
Kneser-Ney smoothing for higher n-grams.
431
3 Extraction of Transliteration Pairs
Training of a supervised transliteration system re-
quires a list of transliteration pairs which is expen-
sive to create. Such lists are usually either built man-
ually or extracted using a classifier trained on man-
ually labelled data and using other language depen-
dent information. In this section, we present an it-
erative method for the extraction of transliteration
pairs from parallel corpora which is fully unsuper-
vised and language pair independent.
Initially, we extract a list of word pairs from a
word-aligned parallel corpus using GIZA++. The
extracted word pairs are either transliterations, other
kinds of translations, or misalignments. In each it-
eration, we first train g2p on the list of word pairs.
Then we delete those 5% of the (remaining) train-
ing data which are least likely to be transliterations
according to g2p.1 We determine the best iteration
according to our stopping criterion and return the fil-
tered data set from this iteration. The stopping crite-
rion uses unlabelled held-out data to predict the opti-
mal stopping point. The following sections describe
the transliteration mining method in detail.
3.1 Methodology
We will first describe the iterative filtering algorithm
(Algorithm 1) and then the algorithm for the stop-
ping criterion (Algorithm 2). In practice, we first
run Algorithm 2 for 100 iterations to determine the
best number of iterations. Then, we run Algorithm 1
for that many iterations.
Initially, the parallel corpus is word-aligned using
GIZA++ (Och and Ney, 2003), and the alignments
are refined using the grow-diag-final-and heuristic
(Koehn et al, 2003). We extract all word pairs which
occur as 1-to-1 alignments in the word-aligned cor-
pus. We ignore non-1-to-1 alignments because they
are less likely to be transliterations for most lan-
guage pairs. The extracted set of word pairs will be
called ?list of word pairs? later on. We use the list
of word pairs as the training data for Algorithm 1.
Algorithm 1 builds a joint sequence model using
g2p on the training data and computes the joint prob-
ability of all word pairs according to g2p. We nor-
malize the probabilities by taking the nth square root
1Since we delete 5% from the filtered data, the number of
deleted data items decreases in each iteration.
Algorithm 1 Mining of transliteration pairs
1: training data?list of word pairs
2: I? 0
3: repeat
4: Build a joint source channel model on the training
data using g2p and compute the joint probability
of every word pair.
5: Remove the 5% word pairs with the lowest length-
normalized probability from the training data.
{and repeat the process with the filtered training
data}
6: I? I+1
7: until I = Stopping iteration from Algorithm 2
where n is the average length of the source and the
target string. The training data contains mostly non-
transliteration pairs and a few transliteration pairs.
Therefore the training data is initially very noisy and
the joint sequence model is not very accurate. How-
ever it can successfully be used to eliminate a few
word pairs which are very unlikely to be translitera-
tions.
On the filtered training data, we can train a model
which is slightly better than the previous model. Us-
ing this improved model, we can eliminate further
non-transliterations.
Our results show that at the iteration determined
by our stopping criterion, the filtered set mostly
contains transliterations and only a small number
of transliterations have been mistakenly eliminated
(see section 4.2).
Algorithm 2 automatically determines the best
stopping point of the iterative transliteration min-
ing process. It is an extension of Algorithm 1. It
runs the iterative process of Algorithm 1 on half of
the list of word pairs (training data) for 100 itera-
tions. For every iteration, it builds a transliteration
system on the filtered data. The transliteration sys-
tem is tested on the source side of the other half of
the list of word pairs (held-out). The output of the
transliteration system is matched against the target
side of the held-out data. (These target words are ei-
ther transliterations, translations or misalignments.)
We match the target side of the held-out data under
the assumption that all matches are transliterations.
The iteration where the output of the transliteration
system best matches the held-out data is chosen as
the stopping iteration of Algorithm 1.
432
Algorithm 2 Selection of the stopping iteration for
the transliteration mining algorithm
1: Create clusters of word pairs from the list of word
pairs which have a common prefix of length 2 both
on the source and target language side.
2: Randomly add each cluster either to the training data
or to the held-out data.
3: I? 0
4: while I < 100 do
5: Build a joint sequence model on the training
data using g2p and compute the length-normalized
joint probability of every word pair in the training
data.
6: Remove the 5% word pairs with the lowest prob-
ability from the training data. {The training data
will be reduced by 5% of the rest in each iteration}
7: Build a transliteration system on the filtered train-
ing data and test it using the source side of the
held-out and match the output against the target
side of the held-out.
8: I? I+1
9: end while
10: Collect statistics of the matching results and take the
median from 9 consecutive iterations (median9).
11: Choose the iteration with the best median9 score for
the transliteration mining process.
We will now describe Algorithm 2 in detail. Al-
gorithm 2 initially splits the word pairs into training
and held-out data. This could be done randomly, but
it turns out that this does not work well for some
tasks. The reason is that the parallel corpus con-
tains inflectional variants of the same word. If two
variants are distributed over training and held-out
data, then the one in the training data may cause the
transliteration system to produce a correct transla-
tion (but not transliteration) of its variant in the held-
out data. This problem is further discussed in section
4.2.2. Instead of randomly splitting the data, we first
create clusters of word pairs which have a common
prefix of length 2 both on the source and target lan-
guage side. We randomly add each cluster either to
the training data or to the held-out data.
We repeat the mining process (described in Algo-
rithm 1) to eliminate non-transliteration pairs from
the training data. For each iteration of Algorithm 2,
i.e., steps 4 to 9, we build a transliteration system on
the filtered training data and test it on the source side
of the held-out. We collect statistics on how well the
output of the system matches the target side of the
held-out. The matching scores on the held-out data
often make large jumps from iteration to iteration.
We take the median of the results from 9 consecutive
iterations (the 4 iterations before, the current and the
4 iterations after the current iteration) to smooth the
scores. We call this median9. We choose the iter-
ation with the best smoothed score as the stopping
point for the filtering process. In our tests, the me-
dian9 heuristic indicated an iteration close to the op-
timal iteration.
Sometimes several nearby iterations have the
same maximal smoothed score. In that case, we
choose the one with the highest unsmoothed score.
Section 4.2 explains the median9 heuristic in more
detail and presents experimental results showing that
it works well.
4 Experiments
We evaluate our transliteration mining algorithm on
three tasks: transliteration mining from Wikipedia
InterLanguage Links, transliteration mining from
parallel corpora, and word alignment using a word
aligner with a transliteration component. On the
WIL data sets, we compare our fully unsupervised
system with the semi-supervised systems presented
at the NEWS10 (Kumaran et al, 2010). In the eval-
uation on parallel corpora, we compare our min-
ing results with a manually built gold standard in
which each word pair is either marked as a translit-
eration or as a non-transliteration. In the word align-
ment experiment, we integrate a transliteration mod-
ule which is trained on the transliterations pairs ex-
tracted by our method into a word aligner and show
a significant improvement. The following sections
describe the experiments in detail.
4.1 Experiments Using Parallel Phrases of
Wikipedia InterLanguage Links
We conduct transliteration mining experiments on
the English/Arabic, English/Hindi, English/Tamil
and English/Russian Wikipedia InterLanguage
Links (WIL) used in the NEWS10.2 All data sets
2We do not evaluate on the English/Chinese data because
the Chinese data requires word segmentation which is beyond
the scope of our work. Another problem is that our extraction
method was developed for alphabetic languages and probably
needs to be adapted before it is applicable to logographic lan-
guages such as Chinese.
433
Our S-Best S-Worst Systems Rank
EA 87.4 91.5 70.2 16 3
ET 90.1 91.4 57.5 14 3
EH 92.2 94.4 71.4 14 3
Table 1: Summary of results on NEWS10 data sets where
?EA? is English/Arabic, ?ET? is English/Tamil and ?EH?
is English/Hindi. ?Our? shows the F-measure of our fil-
tered data against the gold standard using the supplied
evaluation tool, ?Systems? is the total number of partic-
ipants in the subtask, and ?Rank? is the rank we would
have obtained if our system had participated.
contain training data, seed data and reference data.
We make no use of the seed data since our system is
fully unsupervised. We calculate the F-measure of
our filtered transliteration pairs against the supplied
gold standard using the supplied evaluation tool.
For English/Arabic, English/Hindi and En-
glish/Tamil, our system is better than most of the
semi-supervised systems presented at the NEWS
2010 shared task for transliteration mining. Table 1
summarizes the F-scores on these data sets.
On the English/Russian data set, our system
achieves 76% F-measure which is not good com-
pared with the systems that participated in the shared
task. The English/Russian corpus contains many
cognates which ? according to the NEWS10 defi-
nition ? are not transliterations of each other. Our
system learns the cognates in the training data and
extracts them as transliterations (see Table 2).
The two best teams on the English/Russian task
presented various extraction methods (Jiampoja-
marn et al, 2010; Darwish, 2010). Their sys-
tems behave differently on English/Russian than on
other language pairs. Their best systems for En-
glish/Russian are only trained on the seed data and
the use of unlabelled data does not help the perfor-
mance. Since our system is fully unsupervised, and
the unlabelled data is not useful, we perform badly.
4.2 Experiments Using Parallel Corpora
The Wikipedia InterLanguage Links shared task
data contains a much larger proportion of translitera-
tions than a parallel corpus. In order to examine how
well our method performs on parallel corpora, we
apply it to parallel corpora of English/Hindi and En-
glish/Arabic, and compare the transliteration mining
results with a gold standard.
Table 2: Cognates from English/Russian corpus extracted
by our system as transliteration pairs. None of them are
correct transliteration pairs according to the gold stan-
dard.
We use the English/Hindi corpus from the shared
task on word alignment, organized as part of the
ACL 2005 Workshop on Building and Using Par-
allel Texts (WA05) (Martin et al, 2005). For En-
glish/Arabic, we use a freely available parallel cor-
pus from the United Nations (UN) (Eisele and Chen,
2010). We randomly take 200,000 parallel sentences
from the UN corpus of the year 2000. We cre-
ate gold standards for both language pairs by ran-
domly selecting a few thousand word pairs from the
lists of word pairs extracted from the two corpora.
We manually tag them as either transliterations or
non-transliterations. The English/Hindi gold stan-
dard contains 180 transliteration pairs and 2084
non-transliteration pairs and the English/Arabic gold
standard contains 288 transliteration pairs and 6639
non-transliteration pairs. We have submitted these
gold standards with the paper. They are available to
the research community.
In the following sections, we describe the me-
dian9 heuristic and the splitting method of Algo-
rithm 2. The splitting method is used to avoid early
peaks in the held-out statistics, and the median9
heuristic smooths the held-out statistics in order to
obtain a single peak.3
4.2.1 Motivation for Median9 Heuristic
Algorithm 2 collects statistics from the held-out data
(step 10) and selects the stopping iteration. Due to
the noise in the held-out data, the transliteration ac-
curacy on the held-out data often jumps from itera-
tion to iteration. The dotted line in figure 1 (right)
shows the held-out prediction accuracy for the En-
3We do not use the seed data in our system. However,
to check the correctness of the stopping point, we tested
the transliteration system on the seed data (available with
NEWS10) for every iteration of Algorithm 2. We verified that
the median9 held-out statistics and accuracy on the seed data
have their peaks at the same iteration.
434
glish/Hindi parallel corpus. The curve is very noisy
and has two peaks. It is difficult to see the effect of
the filtering. We take the median of the results from
9 consecutive iterations to smooth the scores. The
solid line in figure 1 (right) shows a smoothed curve
built using the median9 held-out scores. A compari-
son with the gold standard (section 4.2.3) shows that
the stopping point (peak) reached using the median9
heuristic is better than the stopping point obtained
with unsmoothed scores.
4.2.2 Motivation for Splitting Method
Algorithm 2 initially splits the list of word pairs into
training and held-out data. A random split worked
well for the WIL data, but failed on the parallel cor-
pora. The reason is that parallel corpora contain in-
flectional variants of the same word. If these vari-
ants are randomly distributed over training and held-
out data, then a non-transliteration word pair such as
the English-Hindi pair ?change ? badlao? may end
up in the training data and the related pair ?changes
? badlao? in the held-out data. The Moses system
used for transliteration will learn to ?transliterate?
(or actually translate) ?change? to ?badlao?. From
other examples, it will learn that a final ?s? can be
dropped. As a consequence, the Moses transliterator
may produce the non-transliteration ?badlao? for the
English word ?changes? in the held-out data. Such
matching predictions of the transliterator which are
actually translations lead to an overestimate of the
transliteration accuracy and may cause Algorithm 2
to predict a stopping iteration which is too early.
By splitting the list of word pairs in such a way
that inflectional variants of a word are placed either
in the training data, or in the held-out, but not in
both, this problem can be solved.4
The left graph in Figure 1 shows that the median9
held-out statistics obtained after a random data split
of a Hindi/English corpus contains two peaks which
occur too early. These peaks disappear in the right
graph of Figure 1 which shows the results obtained
after a split with the clustering method.
The overall trend of the smoothed curve in fig-
ure 1 (right) is very clear. We start by filtering out
non-transliteration pairs from the data, so the results
4This solution is appropriate for all of the language pairs
used in our experiments, but should be revisited if there is in-
flection realized as prefixes, etc.
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0 10 20 30 40 50 60 70 80 90
accu
racy
iterations
held outmedian9
0
0.1
0.2
0.3
0.4
0.5
0.6
0 10 20 30 40 50 60 70 80 90
accu
racy
iterations
held outmedian 9
Figure 1: Statistics of held-out prediction of En-
glish/Hindi data using modified Algorithm 2 with random
division of the list of word pairs (left) and using Algo-
rithm 2 (right). The dotted line shows unsmoothed held-
out scores and solid line shows median9 held-out scores
of the transliteration system go up. When no more
non-transliteration pairs are left, we start filtering
out transliteration pairs and the results of the system
go down. We use this stopping criterion for all lan-
guage pairs and achieve consistently good results.
4.2.3 Results on Parallel Corpora
According to the gold standard, the English/Hindi
and English/Arabic data sets contain 8% and 4%
transliteration pairs respectively. We repeat the same
mining procedure ? run Algorithm 2 up to 100 itera-
tions and return the stopping iteration. Then, we run
Algorithm 1 up to the stopping iteration returned by
Algorithm 2 and obtain the filtered data.
TP FN TN FP
EH Filtered 170 10 2039 45
EA Filtered 197 91 6580 59
Table 3: Transliteration mining results using the parallel
corpus of English/Hindi (EH) and English/Arabic (EA)
against the gold standard
Table 3 shows the mining results on the En-
glish/Hindi and English/Arabic corpora. The gold
standard is a subset of the data sets. The En-
glish/Hindi gold standard contains 180 translitera-
tion pairs and 2084 non-transliteration pairs. The
English/Arabic gold standard contains 288 translit-
eration pairs and 6639 non-transliteration pairs.
From the English/Hindi data, the mining system has
mined 170 transliteration pairs out of 180 transliter-
ation pairs. The English/Arabic mined data contains
197 transliteration pairs out of 288 transliteration
pairs. The mining system has wrongly identified a
few non-transliteration pairs as transliterations (see
435
table 3, last column). Most of these word pairs are
close transliterations and differ by only one or two
characters from perfect transliteration pairs. The
close transliteration pairs provide many valid multi-
grams which may be helpful for the mining system.
4.3 Integration into Word Alignment Model
In the previous section, we presented a method for
the extraction of transliteration pairs from a parallel
corpus. In this section, we will explain how to build
a transliteration module on the extracted transliter-
ation pairs and how to integrate it into MGIZA++
(Gao and Vogel, 2008) by interpolating it with the t-
table probabilities of the IBM models and the HMM
model. MGIZA++ is an extension of GIZA++. It
has the ability to resume training from any model
rather than starting with Model1.
4.3.1 Modified EM Training of the Word
Alignment Models
GIZA++ applies the IBM models (Brown et al,
1993) and the HMM model (Vogel et al, 1996)
in both directions, i.e., source to target and target
to source. The alignments are refined using the
grow-diag-final-and heuristic (Koehn et al, 2003).
GIZA++ generates a list of translation pairs with
alignment probabilities, which is called the t-table.
In this section, we propose a method to modify the
translation probabilities of the t-table by interpolat-
ing the translation counts with transliteration counts.
The interpolation is done in both directions. In the
following, we will only consider the e-to-f direction.
The transliteration module which is used to calcu-
late the conditional transliteration probability is de-
scribed in Algorithm 3.
We build a transliteration system by training
Moses on the filtered transliteration corpus (using
Algorithm 1) and apply it to the e side of the list
of word pairs. For every source word, we gener-
ate the list of 10-best transliterations nbestTI(e).
Then, we extract every f that cooccurs with e in a
parallel sentence and add it to nbestTI(e) which
gives us the list of candidate transliteration pairs
candidateTI(e). We use the sum of transliteration
probabilities
?
f ??CandidateTI(e) pmoses(f
?, e) as an
approximation for the prior probability pmoses(e) =?
f ? pmoses(f
?, e) which is needed to convert the
joint transliteration probability into a conditional
Algorithm 3 Estimation of transliteration probabili-
ties, e-to-f direction
1: unfiltered data?list of word pairs
2: filtered data ?transliteration pairs extracted using
Algorithm 1
3: Train a transliteration system on the filtered data
4: for all e do
5: nbestTI(e) ? 10 best transliterations for e ac-
cording to the transliteration system
6: cooc(e) ? set of all f that cooccur with e in a
parallel sentence
7: candidateTI(e)? cooc(e) ? nbestTI(e)
8: end for
9: for all f do
10: pmoses(f, e)? joint transliteration probability of
e and f according to the transliterator
11: pti(f |e)?
pmoses(f,e)P
f??CandidateTI(e) pmoses(f
?,e)
12: end for
probability. We use the constraint decoding option
of Moses to compute the joint probability of e and f.
It computes the probability by dividing the transla-
tion score of the best target sentence given a source
sentence by the normalization factor.
We combine the transliteration probabilities with
the translation probabilities of the IBM models and
the HMM model. The normal translation probability
pta(f |e) of the word alignment models is computed
with relative frequency estimates.
We smooth the alignment frequencies by adding
the transliteration probabilities weighted by the fac-
tor ? and get the following modified translation
probabilities
p?(f |e) =
fta(f, e) + ?pti(f |e)
fta(e) + ?
(2)
where fta(f, e) = pta(f |e)f(e). pta(f |e) is ob-
tained from the original t-table of the alignment
model. f(e) is the total corpus frequency of e. ?
is the transliteration weight which is optimized for
every language pair (see section 4.3.2). Apart from
the definition of the weight ?, our smoothing method
is equivalent to Witten-Bell smoothing.
We smooth after every iteration of the IBM mod-
els and the HMM model except the last iteration of
each model. Algorithm 4 shows the smoothing for
IBM Model4. IBM Model1 and the HMM model
are smoothed in the same way. We also apply Algo-
rithm 3 and Algorithm 4 in the alignment direction
436
Algorithm 4 Interpolation with the IBM Model4, e-
to-f direction
1: {We want to run four iterations of Model4}
2: f(e)? total frequency of e in the corpus
3: Run MGIZA++ for one iteration of Model4
4: I ? 1
5: while I < 4 do
6: Look up pta(f |e) in the t-table of Model4
7: fta(f, e)? pta(f |e)f(e) for all (f, e)
8: p?(f |e)? fta(f,e)+?pti(f |e)fta(e)+? for all (f, e)
9: Resume MGIZA++ training for 1 iteration using
the modified t-table probabilities p?(f |e)
10: I ? I + 1
11: end while
f to e. The final alignments are generated using the
grow-diag-final-and heuristic (Koehn et al, 2003).
4.3.2 Evaluation
The English/Hindi corpus available from WA05
consists of training, development and test data. As
development and test data for English/Arabic, we
use manually created gold standard word alignments
for 155 sentences extracted from the Hansards cor-
pus released by LDC. We use 50 sentences for de-
velopment and 105 sentences for test.
Baseline: We align the data sets using GIZA++
(Och and Ney, 2003) and refine the alignments us-
ing the grow-diag-final-and heuristic (Koehn et al,
2003). We obtain the baseline F-measure by com-
paring the alignments of the test corpus with the gold
standard alignments.
Experiments We use GIZA++ with 5 iterations of
Model1, 4 iterations of HMM and 4 iterations of
Model4. We interpolate translation and translitera-
tion probabilities at different iterations (and different
combinations of iterations) of the three models and
always observe an improvement in alignment qual-
ity. For the final experiments, we interpolate at every
iteration of the IBM models and the HMM model
except the last iteration of every model where we
could not interpolate for technical reasons.5 Algo-
5We had problems in resuming MGIZA++ training when
training was supposed to continue from a different model, such
as if we stopped after the 5th iteration of Model1 and then
tried to resume MGIZA++ from the first iteration of the HMM
model. In this case, we ran the 5th iteration of Model1, then the
first iteration of the HMM and only then stopped for interpola-
rithm 4 shows the interpolation of the transliteration
probabilities with IBM Model4. We used the same
procedure with IBM Model1 and the HMM model.
The parameter ? is optimized on development
data for every language pair. The word alignment
system is not very sensitive to ?. Any ? in the
range between 50 and 100 works fine for all lan-
guage pairs. The optimization helps to maximize the
improvement in word alignment quality. For our ex-
periments, we use ? = 80.
On test data, we achieve an improvement of
approximately 10% and 13.5% in precision and
3.5% and 13.5% in recall on English/Hindi and En-
glish/Arabic word alignment, respectively. Table 4
shows the scores of the baseline and our word align-
ment model.
Lang Pb Rb Fb Pti Rti Fti
EH 49.1 48.5 51.2 59.1 52.1 55.4
EA 50.8 49.9 50.4 64.4 63.6 64
Table 4: Word alignment results on the test data of En-
glish/Hindi (EH) and English/Arabic (EA) where Pb is
the precision of baseline GIZA++ and Pti is the precision
of our word alignment system
We compared our word alignment results with the
systems presented at WA05. Three systems, one
limited and two un-limited, participated in the En-
glish/Hindi task. We outperform the limited system
and one un-limited system.
5 Previous Research
Previous work on transliteration mining uses a man-
ually labelled set of training data to extract translit-
eration pairs from a parallel corpus or comparable
corpora. The training data may contain a few hun-
dred randomly selected transliteration pairs from a
transliteration dictionary (Yoon et al, 2007; Sproat
et al, 2006; Lee and Chang, 2003) or just a few
carefully selected transliteration pairs (Sherif and
Kondrak, 2007; Klementiev and Roth, 2006). Our
work is more challenging as we extract translitera-
tion pairs without using transliteration dictionaries
or gold standard transliteration pairs.
Klementiev and Roth (2006) initialize their
transliteration model with a list of 20 transliteration
tion; so we did not interpolate in just those iterations of training
where we were transitioning from one model to the next.
437
pairs. Their model makes use of temporal scoring
to rank the candidate transliterations. A lot of work
has been done on discovering and learning translit-
erations from comparable corpora by using temporal
and phonetic information (Tao et al, 2006; Klemen-
tiev and Roth, 2006; Sproat et al, 2006). We do not
have access to this information.
Sherif and Kondrak (2007) train a probabilistic
transducer on 14 manually constructed translitera-
tion pairs of English/Arabic. They iteratively extract
transliteration pairs from the test data and add them
to the training data. Our method is different from the
method of Sherif and Kondrak (2007) as our method
is fully unsupervised, and because in each iteration,
they add the most probable transliteration pairs to
the training data, while we filter out the least proba-
ble transliteration pairs from the training data.
The transliteration mining systems of the four
NEWS10 participants are either based on discrim-
inative or on generative methods. All systems use
manually labelled (seed) data for the initial training.
The system based on the edit distance method sub-
mitted by Jiampojamarn et al (2010) performs best
for the English/Russian task. Jiampojamarn et al
(2010) submitted another system based on a stan-
dard n-gram kernel which ranked first for the En-
glish/Hindi and English/Tamil tasks.6 For the En-
glish/Arabic task, the transliteration mining system
of Noeman and Madkour (2010) was best. They
normalize the English and Arabic characters in the
training data which increases the recall.7
Our transliteration extraction method differs in
that we extract transliteration pairs from a paral-
lel corpus without supervision. The results of the
NEWS10 experiments (Kumaran et al, 2010) show
that no single system performs well on all language
pairs. Our unsupervised method seems robust as its
performance is similar to or better than many of the
semi-supervised systems on three language pairs.
We are only aware of one previous work which
uses transliteration information for word alignment.
6They use the seed data as positive examples. In order to
obtain also negative examples, they generate all possible word
pairs from the source and target words in the seed data and ex-
tract the ones which are not transliterations but have a common
substring of some minimal length.
7They use the phrase table of Moses to build a mapping table
between source and target characters. The mapping table is then
used to construct a finite state transducer.
Hermjakob (2009) proposed a linguistically focused
word alignment system which uses many features
including hand-crafted transliteration rules for Ara-
bic/English alignment. His evaluation did not ex-
plicitly examine the effect of transliteration (alone)
on word alignment. We show that the integration
of a transliteration system based on unsupervised
transliteration mining increases the word alignment
quality for the two language pairs we tested.
6 Conclusion
We proposed a method to automatically extract
transliteration pairs from parallel corpora without
supervision or linguistic knowledge. We evaluated
it against the semi-supervised systems of NEWS10
and achieved high F-measure and performed bet-
ter than most of the semi-supervised systems. We
also evaluated our method on parallel corpora and
achieved high F-measure. We integrated the translit-
eration extraction module into the GIZA++ word
aligner and showed gains in alignment quality. We
will release our transliteration mining system and
word alignment system in the near future.
Acknowledgments
The authors wish to thank the anonymous re-
viewers for their comments. We would like to
thank Christina Lioma for her valuable feedback
on an earlier draft of this paper. Hassan Sajjad
was funded by the Higher Education Commission
(HEC) of Pakistan. Alexander Fraser was funded
by Deutsche Forschungsgemeinschaft grant Models
of Morphosyntax for Statistical Machine Transla-
tion. Helmut Schmid was supported by Deutsche
Forschungsgemeinschaft grant SFB 732.
References
Maximilian Bisani and Hermann Ney. 2008. Joint-
sequence models for grapheme-to-phoneme conver-
sion. Speech Communication, 50(5).
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and R. L. Mercer. 1993. The mathematics of
statistical machine translation: parameter estimation.
Computational Linguistics, 19(2):263?311.
Kareem Darwish. 2010. Transliteration mining with
phonetic conflation and iterative training. In Proceed-
ings of the 2010 Named Entities Workshop, Uppsala,
Sweden. Association for Computational Linguistics.
438
Andreas Eisele and Yu Chen. 2010. MultiUN: A multi-
lingual corpus from United Nation documents. In Pro-
ceedings of the Seventh conference on International
Language Resources and Evaluation (LREC?10), Val-
letta, Malta.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engineer-
ing, Testing, and Quality Assurance for Natural Lan-
guage Processing, Columbus, Ohio, June. Association
for Computational Linguistics.
Ulf Hermjakob. 2009. Improved word alignment with
statistics and linguistic heuristics. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing: Volume 1 - Volume 1, EMNLP
?09, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Sittichai Jiampojamarn, Kenneth Dwyer, Shane Bergsma,
Aditya Bhargava, Qing Dou, Mi-Young Kim, and
Grzegorz Kondrak. 2010. Transliteration generation
and mining with limited training resources. In Pro-
ceedings of the 2010 Named Entities Workshop, Upp-
sala, Sweden. Association for Computational Linguis-
tics.
Alexandre Klementiev and Dan Roth. 2006. Weakly
supervised named entity transliteration and discovery
from multilingual comparable corpora. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and the 44th annual meeting of the
ACL, Morristown, NJ, USA.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
the Human Language Technology and North Ameri-
can Association for Computational Linguistics Con-
ference, pages 127?133, Edmonton, Canada.
A Kumaran, Mitesh M. Khapra, and Haizhou Li. 2010.
Whitepaper of news 2010 shared task on translitera-
tion mining. In Proceedings of the 2010 Named En-
tities Workshop the 48th Annual Meeting of the ACL,
Uppsala, Sweden.
Chun-Jen Lee and Jason S. Chang. 2003. Acqui-
sition of English-Chinese transliterated word pairs
from parallel-aligned texts using a statistical machine
transliteration model. In Proceedings of the HLT-
NAACL 2003Workshop on Building and using parallel
texts, Morristown, NJ, USA. ACL.
Joel Martin, Rada Mihalcea, and Ted Pedersen. 2005.
Word alignment for languages with scarce resources.
In ParaText ?05: Proceedings of the ACL Workshop
on Building and Using Parallel Texts, Morristown, NJ,
USA. Association for Computational Linguistics.
Sara Noeman and Amgad Madkour. 2010. Language
independent transliteration mining system using finite
state automata framework. In Proceedings of the 2010
Named Entities Workshop, Uppsala, Sweden. Associ-
ation for Computational Linguistics.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Tarek Sherif and Grzegorz Kondrak. 2007. Boot-
strapping a stochastic transducer for Arabic-English
transliteration extraction. In ACL, Prague, Czech Re-
public.
Richard Sproat, Tao Tao, and ChengXiang Zhai. 2006.
Named entity transliteration with comparable corpora.
In ACL.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Intl. Conf. Spoken Language Pro-
cessing, Denver, Colorado.
Tao Tao, Su-Yoon Yoon, Andrew Fister, Richard Sproat,
and ChengXiang Zhai. 2006. Unsupervised named
entity transliteration using temporal and phonetic
correlation. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), Sydney.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In 16th International Conference on Computa-
tional Linguistics, pages 836?841, Copenhagen, Den-
mark.
Su-Youn Yoon, Kyoung-Young Kim, and Richard Sproat.
2007. Multilingual transliteration using feature based
phonetic method. In Proceedings of the 45th Annual
Meeting of the ACL, Prague, Czech Republic.
439
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1045?1054,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
A Joint Sequence Translation Model with Integrated Reordering
Nadir Durrani Helmut Schmid Alexander Fraser
Institute for Natural Language Processing
University of Stuttgart
{durrani,schmid,fraser}@ims.uni-stuttgart.de
Abstract
We present a novel machine translation model
which models translation by a linear sequence
of operations. In contrast to the ?N-gram?
model, this sequence includes not only trans-
lation but also reordering operations. Key
ideas of our model are (i) a new reordering
approach which better restricts the position to
which a word or phrase can be moved, and
is able to handle short and long distance re-
orderings in a unified way, and (ii) a joint
sequence model for the translation and re-
ordering probabilities which is more flexi-
ble than standard phrase-based MT. We ob-
serve statistically significant improvements in
BLEU over Moses for German-to-English and
Spanish-to-English tasks, and comparable re-
sults for a French-to-English task.
1 Introduction
We present a novel generative model that explains
the translation process as a linear sequence of oper-
ations which generate a source and target sentence
in parallel. Possible operations are (i) generation of
a sequence of source and target words (ii) insertion
of gaps as explicit target positions for reordering op-
erations, and (iii) forward and backward jump oper-
ations which do the actual reordering. The probabil-
ity of a sequence of operations is defined according
to an N-gram model, i.e., the probability of an op-
eration depends on the n ? 1 preceding operations.
Since the translation (generation) and reordering op-
erations are coupled in a single generative story,
the reordering decisions may depend on preceding
translation decisions and translation decisions may
depend on preceding reordering decisions. This pro-
vides a natural reordering mechanism which is able
to deal with local and long-distance reorderings in a
consistent way. Our approach can be viewed as an
extension of the N-gram SMT approach (Marin?o et
al., 2006) but our model does reordering as an inte-
gral part of a generative model.
The paper is organized as follows. Section 2 dis-
cusses the relation of our work to phrase-based and
the N-gram SMT. Section 3 describes our genera-
tive story. Section 4 defines the probability model,
which is first presented as a generative model, and
then shifted to a discriminative framework. Section
5 provides details on the search strategy. Section 6
explains the training process. Section 7 describes
the experimental setup and results. Section 8 gives
a few examples illustrating different aspects of our
model and Section 9 concludes the paper.
2 Motivation and Previous Work
2.1 Relation of our work to PBSMT
Phrase-based SMT provides a powerful translation
mechanism which learns local reorderings, transla-
tion of short idioms, and the insertion and deletion of
words sensitive to local context. However, PBSMT
also has some drawbacks. (i) Dependencies across
phrases are not directly represented in the translation
model. (ii) Discontinuous phrases cannot be used.
(iii) The presence of many different equivalent seg-
mentations increases the search space.
Phrase-based SMT models dependencies between
words and their translations inside of a phrase well.
However, dependencies across phrase boundaries
are largely ignored due to the strong phrasal inde-
1045
German English
hat er ein buch gelesen he read a book
hat eine pizza gegessen has eaten a pizza
er he
hat has
ein a
eine a
menge lot of
butterkekse butter cookies
gegessen eaten
buch book
zeitung newspaper
dann then
Table 1: Sample Phrase Table
pendence assumption. A phrase-based system us-
ing the phrase table1 shown in Table 1, for exam-
ple, correctly translates the German sentence ?er
hat eine pizza gegessen? to ?he has eaten a pizza?,
but fails while translating ?er hat eine menge but-
terkekse gegessen? (see Table 1 for a gloss) which
is translated as ?he has a lot of butter cookies eaten?
unless the language model provides strong enough
evidence for a different ordering. The generation of
this sentence in our model starts with generating ?er
? he?, ?hat ? has?. Then a gap is inserted on the Ger-
man side, followed by the generation of ?gegessen ?
eaten?. At this point, the (partial) German and En-
glish sentences look as follows:
er hat gegessen
he has eaten
We jump back to the gap on the German side
and fill it by generating ?eine ? a? and ?pizza ?
pizza?, for the first example and generating ?eine ?
a?, ?menge ? lot of?, ?butterkekse ? butter cookies?
for the second example, thus handling both short
and long distance reordering in a unified manner.
Learning the pattern ?hat gegessen ? has eaten?
helps us to generalize to the second example with
unseen context. Notice how the reordering deci-
sion is triggered by the translation decision in our
model. The probability of a gap insertion operation
after the generation of the auxiliaries ?hat ? has? will
be high because reordering is necessary in order to
move the second part of the German verb complex
(?gegessen?) to its correct position at the end of the
clause. This mechanism better restricts reordering
1The examples given in this section are not taken from the
real data/system, but made-up for the sake of argument.
Figure 1: (a) Known Context (b) Unknown Context
than traditional PBSMT and is able to deal with local
and long-distance reorderings in a consistent way.
Another weakness of the traditional phrase-based
system is that it can only capitalize on continuous
phrases. Given the phrase inventory in Table 1,
phrasal MT is able to generate example in Figure
1(a). The information ?hat...gelesen ? read? is inter-
nal to the phrase pair ?hat er ein buch gelesen ? he
read a book?, and is therefore handled conveniently.
On the other hand, the phrase table does not have
the entry ?hat er eine zeitung gelesen ? he read a
newspaper? (Figure 1(b)). Hence, there is no option
but to translate ?hat...gelesen? separately, translat-
ing ?hat? to ?has? which is a common translation for
?hat? but wrong in the given context. Context-free
hierarchical models (Chiang, 2007; Melamed, 2004)
have rules like ?hat er X gelesen ? he read X? to han-
dle such cases. Galley and Manning (2010) recently
solved this problem for phrasal MT by extracting
phrase pairs with source and target-side gaps. Our
model can also use tuples with source-side discon-
tinuities. The above sentence would be generated
by the following sequence of operations: (i) gener-
ate ?dann ? then? (ii) insert a gap (iii) generate ?er
? he? (iv) backward jump to the gap (v) generate
?hat...[gelesen] ? read? (only ?hat? and ?read? are
added to the sentences yet) (vi) jump forward to the
right-most source word so far generated (vii) insert
a gap (viii) continue the source cept (?gelesen? is in-
serted now) (ix) backward jump to the gap (x) gen-
erate ?ein ? a? (xi) generate ?buch ? book?.
Figure 2: Pattern
From this operation se-
quence, the model learns a
pattern (Figure 2) which al-
lows it to generalize to the
example in Figure 1(b). The open gap represented
by serves a similar purpose as the non-terminal
categories in a hierarchical phrase-based system
such as Hiero. Thus it generalizes to translate ?eine
zeitung? in exactly the same way as ?ein buch?.
1046
Another problem of phrasal MT is spurious
phrasal segmentation. Given a sentence pair and
a corresponding word alignment, phrasal MT can
learn an arbitrary number of source segmentations.
This is problematic during decoding because differ-
ent compositions of the same minimal phrasal units
are allowed to compete with each other.
2.2 Relation of our work to N-gram SMT
N-gram based SMT is an alternative to hierarchi-
cal and non-hierarchical phrase-based systems. The
main difference between phrase-based and N-gram
SMT is the extraction procedure of translation units
and the statistical modeling of translation context
(Crego et al, 2005a). The tuples used in N-gram
systems are much smaller translation units than
phrases and are extracted in such a way that a unique
segmentation of each bilingual sentence pair is pro-
duced. This helps N-gram systems to avoid the
spurious phrasal segmentation problem. Reorder-
ing works by linearization of the source side and tu-
ple unfolding (Crego et al, 2005b). The decoder
uses word lattices which are built with linguistically
motivated re-write rules. This mechanism is further
enhanced with an N-gram model of bilingual units
built using POS tags (Crego and Yvon, 2010). A
drawback of their reordering approach is that search
is only performed on a small number of reorderings
that are pre-calculated on the source side indepen-
dently of the target side. Often, the evidence for
the correct ordering is provided by the target-side
language model (LM). In the N-gram approach, the
LM only plays a role in selecting between the pre-
calculated orderings.
Our model is based on the N-gram SMT model,
but differs from previous N-gram systems in some
important aspects. It uses operation n-grams rather
than tuple n-grams. The reordering approach is en-
tirely different and considers all possible orderings
instead of a small set of pre-calculated orderings.
The standard N-gram model heavily relies on POS
tags for reordering and is unable to use lexical trig-
gers whereas our model exclusively uses lexical trig-
gers and no POS information. Linearization and un-
folding of the source sentence according to the target
sentence enables N-gram systems to handle source-
side gaps. We deal with this phenomenon more di-
rectly by means of tuples with source-side discon-
tinuities. The most notable feature of our work is
that it has a complete generative story of transla-
tion which combines translation and reordering op-
erations into a single operation sequence model.
Like the N-gram model2, our model cannot deal
with target-side discontinuities. These are elimi-
nated from the training data by a post-editing pro-
cess on the alignments (see Section 6). Galley and
Manning (2010) found that target-side gaps were not
useful in their system and not useful in the hierarchi-
cal phrase-based system Joshua (Li et al, 2009).
3 Generative Story
Our generative story is motivated by the complex re-
orderings in the German-to-English translation task.
The German and English sentences are jointly gen-
erated through a sequence of operations. The En-
glish words are generated in linear order3 while
the German words are generated in parallel with
their English translations. Occasionally the trans-
lator jumps back on the German side to insert some
material at an earlier position. After this is done, it
jumps forward again and continues the translation.
The backward jumps always end at designated land-
ing sites (gaps) which were explicitly inserted be-
fore. We use 4 translation and 3 reordering opera-
tions. Each is briefly discussed below.
Generate (X,Y): X and Y are German and English
cepts4 respectively, each with one or more words.
Words in X (German) may be consecutive or discon-
tinuous, but the words in Y (English) must be con-
secutive. This operation causes the words in Y and
the first word in X to be added to the English and
German strings respectively, that were generated so
far. Subsequent words in X are added to a queue to
be generated later. All the English words in Y are
generated immediately because English is generated
in linear order. The generation of the second (and
subsequent) German word in a multi-word cept can
be delayed by gaps, jumps and the Generate Source
Only operation defined below.
Continue Source Cept: The German words added
2However, Crego and Yvon (2009), in their N-gram system,
use split rules to handle target-side gaps and show a slight im-
provement on a Chinese-English translation task.
3Generating the English words in order is also what the de-
coder does when translating from German to English.
4A cept is a group of words in one language translated as a
minimal unit in one specific context (Brown et al, 1993).
1047
to the queue by the Generate (X,Y) operation are
generated by the Continue Source Cept operation.
Each Continue Source Cept operation removes one
German word from the queue and copies it to the
German string. If X contains more than one German
word, say n many, then it requires n translation op-
erations, an initial Generate (X1...Xn, Y ) operation
and n ? 1 Continue Source Cept operations. For
example ?hat...gelesen ? read? is generated by the
operation Generate (hat gelesen, read), which adds
?hat? and ?read? to the German and English strings
and ?gelesen? to a queue. A Continue Source Cept
operation later removes ?gelesen? from the queue
and adds it to the German string.
Generate Source Only (X): The string X is added
at the current position in the German string. This op-
eration is used to generate a German word X with no
corresponding English word. It is performed imme-
diately after its preceding German word is covered.
This is because there is no evidence on the English-
side which indicates when to generate X. Generate
Source Only (X) helps us learn a source word dele-
tion model. It is used during decoding, where a Ger-
man word (X) is either translated to some English
word(s) by a Generate (X,Y) operation or deleted
with a Generate Source Only (X) operation.
Generate Identical: The same word is added at
the current position in both the German and En-
glish strings. The Generate Identical operation is
used during decoding for the translation of unknown
words. The probability of this operation is estimated
from singleton German words that are translated to
an identical string. For example, for a tuple ?Port-
land ? Portland?, where German ?Portland? was ob-
served exactly once during training, we use a Gen-
erate Identical operation rather than Generate (Port-
land, Portland).
We now discuss the set of reordering operations
used by the generative story. Reordering has to be
performed whenever the German word to be gen-
erated next does not immediately follow the previ-
ously generated German word. During the genera-
tion process, the translator maintains an index which
specifies the position after the previously covered
German word (j), an index (Z) which specifies the
index after the right-most German word covered so
far, and an index of the next German word to be cov-
ered (j?). The set of reordering operations used in
Table 2: Step-wise Generation of Example 1(a). The ar-
row indicates position j.
generation depends upon these indexes.
Insert Gap: This operation inserts a gap which acts
as a place-holder for the skipped words. There can
be more than one open gap at a time.
Jump Back (W): This operation lets the translator
jump back to an open gap. It takes a parameter W
specifying which gap to jump to. Jump Back (1)
jumps to the closest gap to Z, Jump Back (2) jumps
to the second closest gap to Z, etc. After the back-
ward jump the target gap is closed.
Jump Forward: This operation makes the transla-
tor jump to Z. It is performed if some already gen-
erated German word is between the previously gen-
erated word and the word to be generated next. A
Jump Back (W) operation is only allowed at position
Z. Therefore, if j 6= Z, a Jump Forward operation
has to be performed prior to a Jump Back operation.
Table 2 shows step by step the generation of a
German/English sentence pair, the corresponding
translation operations, and the respective values of
the index variables. A formal algorithm for convert-
ing a word-aligned bilingual corpus into an opera-
tion sequence is presented in Algorithm 1.
4 Model
Our translation model p(F,E) is based on opera-
tion N-gram model which integrates translation and
reordering operations. Given a source string F , a
sequence of tuples T = (t1, . . . , tn) as hypothe-
sized by the decoder to generate a target string E,
the translation model estimates the probability of a
1048
Algorithm 1 Corpus Conversion Algorithm
i Position of current English cept
j Position of current German word
j? Position of next German word
N Total number of English cepts
fj German word at position j
Ei English cept at position i
Fi Sequence of German words linked to Ei
Li Number of German words linked with Ei
k Number of already generated German words for Ei
aik Position of kth German translation of Ei
Z Position after right-most generated German word
S Position of the first word of a target gap
i := 0; j := 0; k := 0
while fj is an unaligned word do
Generate Source Only (fj)
j := j + 1
Z := j
while i < N do
j? := aik
if j < j? then
if fj was not generated yet then
Insert Gap
if j = Z then
j := j?
else
Jump Forward
if j? < j then
if j < Z and fj was not generated yet then
Insert Gap
W := relative position of target gap
Jump Back (W)
j := S
if j < j? then
Insert Gap
j := j?
if k = 0 then
Generate (Fi, Ei) {or Generate Identical}
else
Continue Source Cept
j := j + 1; k := k + 1
while fj is an unaligned word do
Generate Source Only (fj)
j := j + 1
if Z < j then
Z := j
if k = Li then
i := i + 1; k := 0
Remarks:
We use cept positions for English (not word positions) because
English cepts are composed of consecutive words. German po-
sitions are word-based.
The relative position of the target gap is 1 if it is closest to Z, 2
if it is the second closest gap etc.
The operation Generate Identical is chosen if Fi = Ei and the
overall frequency of the German cept Fi is 1.
generated operation sequence O = (o1, . . . , oJ) as:
p(F,E) ?
J?
j=1
p(oj |oj?m+1...oj?1)
where m indicates the amount of context used. Our
translation model is implemented as an N-gram
model of operations using SRILM-Toolkit (Stolcke,
2002) with Kneser-Ney smoothing. We use a 9-gram
model (m = 8).
Integrating the language model the search is de-
fined as:
E? = argmax
E
pLM (E)p(F,E)
where pLM (E) is the monolingual language model
and p(F,E) is the translation model. But our trans-
lation model is a joint probability model, because of
which E is generated twice in the numerator. We
add a factor, prior probability ppr(E), in the denom-
inator, to negate this effect. It is used to marginalize
the joint-probability model p(F,E). The search is
then redefined as:
E? = argmax
E
pLM (E)
p(F,E)
ppr(E)
Both, the monolingual language and the prior
probability model are implemented as standard
word-based n-gram models:
px(E) ?
J?
j=1
p(wj |wj?m+1, . . . , wj?1)
where m = 4 (5-gram model) for the standard
monolingual model (x = LM ) and m = 8 (same
as the operation model5) for the prior probability
model (x = pr).
In order to improve end-to-end accuracy, we in-
troduce new features for our model and shift from
the generative6 model to the standard log-linear ap-
proach (Och and Ney, 2004) to tune7 them. We
search for a target stringE which maximizes a linear
combination of feature functions:
5In decoding, the amount of context used for the prior prob-
ability is synchronized with the position of back-off in the op-
eration model.
6Our generative model is about 3 BLEU points worse than
the best discriminative results.
7We tune the operation, monolingual and prior probability
models as separate features. We expect the prior probability
model to get a negative weight but we do not force MERT to
assign a negative weight to this feature.
1049
E? = argmax
E
?
?
?
J?
j=1
?jhj(F,E)
?
?
?
where ?j is the weight associated with the feature
hj(F,E). Other than the 3 features discussed above
(log probabilities of the operation model, monolin-
gual language model and prior probability model),
we train 8 additional features discussed below:
Length Bonus The length bonus feature counts the
length of the target sentence in words.
Deletion Penalty Another feature for avoiding too
short translations is the deletion penalty. Deleting a
source word (Generate Source Only (X)) is a com-
mon operation in the generative story. Because there
is no corresponding target-side word, the monolin-
gual language model score tends to favor this op-
eration. The deletion penalty counts the number of
deleted source words.
Gap Bonus and Open Gap Penalty These features
are introduced to guide the reordering decisions. We
observe a large amount of reordering in the automat-
ically word aligned training text. However, given
only the source sentence (and little world knowl-
edge), it is not realistic to try to model the reasons
for all of this reordering. Therefore we can use a
more robust model that reorders less than humans.
The gap bonus feature sums to the total number of
gaps inserted to produce a target sentence. The open
gap penalty feature is a penalty (paid once for each
translation operation performed) whose value is the
number of open gaps. This penalty controls how
quickly gaps are closed.
Distortion and Gap Distance Penalty We have
two additional features to control the reordering de-
cisions. One of them is similar8 to the distance-
based reordering model used by phrasal MT. The
other feature is the gap distance penalty which calcu-
lates the distance between the first word of a source
ceptX and the start of the left-most gap. This cost is
paid once for each Generate, Generate Identical and
Generate Source Only. For a source cept coverd by
indexes X1, . . . , Xn, we get the feature value gj =
X1?S, where S is the index of the left-most source
word where a gap starts.
8Let X1, . . . , Xn and Y1, . . . , Ym represent indexes of the
source words covered by the tuples tj and tj?1 respectively.
The distance between tj and tj?1 is given as dj = min(|Xk ?
Yl| ? 1) ?Xk ? {X1, . . . , Xn} and ? Yl ? {Y1, . . . , Ym}
Lexical Features We also use source-to-target
p(e|f) and target-to-source p(f |e) lexical transla-
tion probabilities. Our lexical features are standard
(Koehn et al, 2003). The estimation is motivated by
IBM Model-1. Given a tuple ti with source words
f = f1, f2, . . . , fn, target words e = e1, e2, . . . , em
and an alignment a between the source word posi-
tions x = 1, . . . , n and the target word positions
y = 1, . . . ,m, the lexical feature pw(f |e) is com-
puted as follows:
pw(f |e, a) =
n?
x=1
1
|{y : (x, y) ? a}|
?
?(x,y)?a
w(fx|ey)
pw(e|f, a) is computed in the same way.
5 Decoding
Our decoder for the new model performs a stack-
based search with a beam-search algorithm similar
to that used in Pharoah (Koehn, 2004a). Given an
input sentence F , it first extracts a set of match-
ing source-side cepts along with their n-best trans-
lations to form a tuple inventory. During hypoth-
esis expansion, the decoder picks a tuple from the
inventory and generates the sequence of operations
required for the translation with this tuple in light
of the previous hypothesis.9 The sequence of op-
erations may include translation (generate, continue
source cept etc.) and reordering (gap insertions,
jumps) operations. The decoder also calculates the
overall cost of the new hypothesis. Recombination
is performed on hypotheses having the same cov-
erage vector, monolingual language model context,
and operation model context. We do histogram-
based pruning, maintaining the 500 best hypotheses
for each stack.10
9A hypothesis maintains the index of the last source word
covered (j), the position of the right-most source word covered
so far (Z), the number of open gaps, the number of gaps so
far inserted, the previously generated operations, the generated
target string, and the accumulated values of all the features dis-
cussed in Section 4.
10We need a higher beam size to produce translation units
similar to the phrase-based systems. For example, the phrase-
based system can learn the phrase pair ?zum Beispiel ? for ex-
ample? and generate it in a single step placing it directly into the
stack two words to the right. Our system generates this example
with two separate tuple translations ?zum ? for? and ?Beispiel
? example? in two adjacent stacks. Because ?zum ? for? is not
a frequent translation unit, it will be ranked quite low in the first
stack until the tuple ?Beispiel ? example? appears in the second
stack. Koehn and his colleagues have repeatedly shown that in-
1050
Figure 3: Post-editing of Alignments (a) Initial (b) No
Target-Discontinuities (c) Final Alignments
6 Training
Training includes: (i) post-editing of the alignments,
(ii) generation of the operation sequence (iii) estima-
tion of the n-gram language models.
Our generative story does not handle target-side
discontinuities and unaligned target words. There-
fore we eliminate them from the training corpus in a
3-step process: If a source word is aligned with mul-
tiple target words which are not consecutive, first
the link to the least frequent target word is iden-
tified, and the group of links containing this word
is retained while the others are deleted. The in-
tuition here is to keep the alignments containing
content words (which are less frequent than func-
tional words). The new alignment has no target-
side discontinuities anymore, but might still contain
unaligned target words. For each unaligned target
word, we determine the (left or right) neighbour that
it appears more frequently with and align it with the
same source word as the neighbour. The result is
an alignment without target-side discontinuities and
unaligned target words. Figure 3 shows an illustra-
tive example of the process. The tuples in Figure 3c
are ?A ? U V?, ?B ? W X Y?, ?C ? NULL?, ?D ? Z?.
We apply Algorithm 1 to convert the preprocessed
aligned corpus into a sequence of translation opera-
tions. The resulting operation corpus contains one
sequence of operations per sentence pair.
In the final training step, the three language mod-
els are trained using the SRILM Toolkit. The oper-
ation model is estimated from the operation corpus.
The prior probability model is estimated from the
target side part of the bilingual corpus. The mono-
lingual language model is estimated from the target
side of the bilingual corpus and additional monolin-
gual data.
creasing the Moses stack size from 200 to 1000 does not have
a significant effect on translation into English, see (Koehn and
Haddow, 2009) and other shared task papers.
7 Experimental Setup
7.1 Data
We evaluated the system on three data sets with
German-to-English, Spanish-to-English and French-
to-English news translations, respectively. We used
data from the 4th version of the Europarl Corpus
and the News Commentary which was made avail-
able for the translation task of the Fourth Workshop
on Statistical Machine Translation.11 We use 200K
bilingual sentences, composed by concatenating the
entire news commentary (? 74K sentences) and Eu-
roparl (? 126K sentence), for the estimation of the
translation model. Word alignments were generated
with GIZA++ (Och and Ney, 2003), using the grow-
diag-final-and heuristic (Koehn et al, 2005). In or-
der to obtain the best alignment quality, the align-
ment task is performed on the entire parallel data and
not just on the training data we use. All data is low-
ercased, and we use the Moses tokenizer and recap-
italizer. Our monolingual language model is trained
on 500K sentences. These comprise 300K sentences
from the monolingual corpus (news commentary)
and 200K sentences from the target-side part of the
bilingual corpus. The latter part is also used to train
the prior probability model. The dev and test sets
are news-dev2009a and news-dev2009b which con-
tain 1025 and 1026 parallel sentences. The feature
weights are tuned with Z-MERT (Zaidan, 2009).
7.2 Results
Baseline: We compare our model to a recent ver-
sion of Moses (Koehn et al, 2007) using Koehn?s
training scripts and evaluate with BLEU (Papineni
et al, 2002). We provide Moses with the same ini-
tial alignments as we are using to train our system.12
We use the default parameters for Moses, and a 5-
gram English language model (the same as in our
system).
We compare two variants of our system. The first
system (Twno?rl) applies no hard reordering limit
and uses the distortion and gap distance penalty fea-
tures as soft constraints, allowing all possible re-
orderings. The second system (Twrl?6) uses no dis-
tortion and gap distance features, but applies a hard
constraint which limits reordering to no more than 6
11http://www.statmt.org/wmt09/translation-task.html
12We tried applying our post-processing to the alignments
provided to Moses and found that this made little difference.
1051
Source German Spanish French
Blno?rl 17.41 19.85 19.39
Blrl?6 18.57 21.67 20.84
Twno?rl 18.97 22.17 20.94
Twrl?6 19.03 21.88 20.72
Table 3: This Work(Tw) vs Moses (Bl), no-rl = No Re-
ordering Limit, rl-6 = Reordering limit 6
positions. Specifically, we do not extend hypotheses
that are more than 6 words apart from the first word
of the left-most gap during decoding. In this exper-
iment, we disallowed tuples which were discontin-
uous on the source side. We compare our systems
with two Moses systems as baseline, one using no
reordering limit (Blno?rl) and one using the default
distortion limit of 6 (Blrl?6).
Both of our systems (see Table 3) outperform
Moses on the German-to-English and Spanish-to-
English tasks and get comparable results for French-
to-English. Our best system (Twno?rl), which uses
no hard reordering limit, gives statistically signifi-
cant (p < 0.05)13 improvements over Moses (both
baselines) for the German-to-English and Spanish-
to-English translation task. The results for Moses
drop by more than a BLEU point without the re-
ordering limit (see Blno?rl in Table 3). All our
results are statistically significant over the baseline
Blno?rl for all the language pairs.
In another experiment, we tested our system also
with tuples which were discontinuous on the source
side. These gappy translation units neither improved
the performance of the system with hard reordering
limit (Twrl?6?asg) nor that of the system without
reordering limit (Twno?rl?asg) as Table 4 shows.
In an analysis of the output we found two reasons
for this result: (i) Using tuples with source gaps in-
creases the list of extracted n-best translation tuples
exponentially which makes the search problem even
more difficult. Table 5 shows the number of tuples
(with and without gaps) extracted when decoding
the test file with 10-best translations. (ii) The fu-
ture cost14 is poorly estimated in case of tuples with
gappy source cepts, causing search errors.
In an experiment, we deleted gappy tuples with
13We used Kevin Gimpel?s implementation of pairwise boot-
strap resampling (Koehn, 2004b), 1000 samples.
14The dynamic programming approach of calculating future
cost for bigger spans gives erroneous results when gappy cepts
can interleave. Details omitted due to space limitations.
Source German Spanish French
Twno?rl?asg 18.61 21.60 20.59
Twrl?6?asg 18.65 21.40 20.47
Twno?rl?hsg 18.91 21.93 20.87
Twrl?6?hsg 19.23 21.79 20.85
Table 4: Our Systems with Gappy Units, asg = All Gappy
Units, hsg = Heuristic for pruning Gappy Units
Source German Spanish French
Gaps 965515 1705156 1473798
No-Gaps 256992 313690 343220
Heuristic (hsg) 281618 346993 385869
Table 5: 10-best Translation Options With & Without
Gaps and using our Heuristic
a score (future cost estimate) lower than the sum of
the best scores of the parts. This heuristic removes
many useless discontinuous tuples. We found that
results improved (Twno?rl?hsg and Twrl?6?hsg in
Table 4) compared to the version using all gaps
(Twno?rl?asg, Twrl?6?asg), and are closer to the
results without discontinuous tuples (Twno?rl and
Twrl?6 in Table 3).
8 Sample Output
In this section we compare the output of our sys-
tems and Moses. Example 1 in Figure 4 shows
the powerful reordering mechanism of our model
which moves the English verb phrase ?do not want
to negotiate? to its correct position between the sub-
ject ?they? and the prepositional phrase ?about con-
crete figures?. Moses failed to produce the correct
word order in this example. Notice that although
our model is using smaller translation units ?nicht
? do not?, ?verhandlen ? negotiate? and ?wollen ?
want to?, it is able to memorize the phrase transla-
tion ?nicht verhandlen wollen ? do not want to ne-
gotiate? as a sequence of translation and reordering
operations. It learns the reordering of ?verhandlen ?
negotiate? and ?wollen ? want to? and also captures
dependencies across phrase boundaries.
Example 2 shows how our system without a re-
ordering limit moves the English translation ?vote?
of the German clause-final verb ?stimmen? across
about 20 English tokens to its correct position be-
hind the auxiliary ?would?.
Example 3 shows how the system with gappy tu-
ples translates a German sentence with the particle
verb ?kehrten...zuru?ck? using a single tuple (dashed
lines). Handling phenomena like particle verbs
1052
Figure 4: Sample Output Sentences
strongly motivates our treatment of source side gaps.
The system without gappy units happens to pro-
duce the same translation by translating ?kehrten? to
?returned? and deleting the particle ?zuru?ck? (solid
lines). This is surprising because the operation for
translating ?kehrten? to ?returned? and for deleting
the particle are too far apart to influence each other
in an n-gram model. Moses run on the same exam-
ple deletes the main verb (?kehrten?), an error that
we frequently observed in the output of Moses.
Our last example (Figure 5) shows that our model
learns idioms like ?meiner Meinung nach ? In my
opinion ,? and short phrases like ?gibt es ? there
are? showing its ability to memorize these ?phrasal?
translations, just like Moses.
9 Conclusion
We have presented a new model for statistical MT
which can be used as an alternative to phrase-
based translation. Similar to N-gram based MT,
it addresses three drawbacks of traditional phrasal
MT by better handling dependencies across phrase
boundaries, using source-side gaps, and solving the
phrasal segmentation problem. In contrast to N-
gram based MT, our model has a generative story
which tightly couples translation and reordering.
Furthermore it considers all possible reorderings un-
like N-gram systems that perform search only on
Figure 5: Learning Idioms
a limited number of pre-calculated orderings. Our
model is able to correctly reorder words across
large distances, and it memorizes frequent phrasal
translations including their reordering as probable
operations sequences. Our system outperformed
Moses on standard Spanish-to-English and German-
to-English tasks and achieved comparable results for
French-to-English. A binary version of the corpus
conversion algorithm and the decoder is available.15
Acknowledgments
The authors thank Fabienne Braune and the re-
viewers for their comments. Nadir Durrani was
funded by the Higher Education Commission (HEC)
of Pakistan. Alexander Fraser was funded by
Deutsche Forschungsgemeinschaft grant Models
of Morphosyntax for Statistical Machine Transla-
tion. Helmut Schmid was supported by Deutsche
Forschungsgemeinschaft grant SFB 732.
15http://www.ims.uni-stuttgart.de/?durrani/resources.html
1053
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and R. L. Mercer. 1993. The mathematics of
statistical machine translation: parameter estimation.
Computational Linguistics, 19(2):263?311.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Josep Maria Crego and Franois Yvon. 2009. Gappy
translation units under left-to-right smt decoding. In
Proceedings of the meeting of the European Associa-
tion for Machine Translation (EAMT), pages 66?73,
Barcelona, Spain.
Josep Maria Crego and Franc?ois Yvon. 2010. Improv-
ing reordering with linguistically informed bilingual
n-grams. In Coling 2010: Posters, pages 197?205,
Beijing, China, August. Coling 2010 Organizing Com-
mittee.
Josep M. Crego, Marta R. Costa-juss, Jos B. Mario,
and Jos A. R. Fonollosa. 2005a. Ngram-based ver-
sus phrasebased statistical machine translation. In In
Proceedings of the International Workshop on Spoken
Language Technology (IWSLT05, pages 177?184.
Josep M. Crego, Jose? B. Marin?o, and Adria` de Gispert.
2005b. Reordered search and unfolding tuples for
ngram-based SMT. In Proceedings of the 10th Ma-
chine Translation Summit (MT Summit X), pages 283?
289, Phuket, Thailand.
Michel Galley and Christopher D. Manning. 2010. Ac-
curate non-hierarchical phrase-based translation. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 966?
974, Los Angeles, California, June. Association for
Computational Linguistics.
Philipp Koehn and Barry Haddow. 2009. Edinburgh?s
submission to all tracks of the WMT 2009 shared task
with reordering and speed improvements to Moses.
In Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 160?164, Athens, Greece,
March. Association for Computational Linguistics.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
the Human Language Technology and North Ameri-
can Association for Computational Linguistics Con-
ference, pages 127?133, Edmonton, Canada.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description for
the 2005 iwslt speech translation evaluation. In Inter-
national Workshop on Spoken Language Translation
2005.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics, Demonstration Program,
Prague, Czech Republic.
Philipp Koehn. 2004a. Pharaoh: A beam search decoder
for phrase-based statistical machine translation mod-
els. In AMTA, pages 115?124.
Philipp Koehn. 2004b. Statistical significance tests
for machine translation evaluation. In Dekang Lin
and Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July. Association
for Computational Linguistics.
Zhifei Li, Chris Callison-burch, Chris Dyer, Juri Ganitke-
vitch, Sanjeev Khudanpur, Lane Schwartz, Wren N. G.
Thornton, Jonathan Weese, and Omar F. Zaidan. 2009.
Joshua: An open source toolkit for parsing-based ma-
chine translation.
J.B. Marin?o, R.E. Banchs, J.M. Crego, A. de Gispert,
P. Lambert, J.A.R. Fonollosa, and M.R. Costa-jussa`.
2006. N-gram-based machine translation. Computa-
tional Linguistics, 32(4):527?549.
I. Dan Melamed. 2004. Statistical machine translation
by parsing. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics,
Barcelona, Spain.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(1):417?449.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 311?318, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Intl. Conf. Spoken Language Pro-
cessing, Denver, Colorado.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
1054
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 469?477,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Statistical Model for Unsupervised and Semi-supervised Transliteration
Mining
Hassan Sajjad Alexander Fraser Helmut Schmid
Institute for Natural Language Processing
University of Stuttgart
{sajjad,fraser,schmid}@ims.uni-stuttgart.de
Abstract
We propose a novel model to automatically
extract transliteration pairs from parallel cor-
pora. Our model is efficient, language pair
independent and mines transliteration pairs in
a consistent fashion in both unsupervised and
semi-supervised settings. We model transliter-
ation mining as an interpolation of translitera-
tion and non-transliteration sub-models. We
evaluate on NEWS 2010 shared task data and
on parallel corpora with competitive results.
1 Introduction
Transliteration mining is the extraction of translit-
eration pairs from unlabelled data. Most transliter-
ation mining systems are built using labelled train-
ing data or using heuristics to extract transliteration
pairs. These systems are language pair dependent or
require labelled information for training. Our sys-
tem extracts transliteration pairs in an unsupervised
fashion. It is also able to utilize labelled information
if available, obtaining improved performance.
We present a novel model of transliteration min-
ing defined as a mixture of a transliteration model
and a non-transliteration model. The transliteration
model is a joint source channel model (Li et al,
2004). The non-transliteration model assumes no
correlation between source and target word charac-
ters, and independently generates a source and a tar-
get word using two fixed unigram character models.
We use Expectation Maximization (EM) to learn pa-
rameters maximizing the likelihood of the interpola-
tion of both sub-models. At test time, we label word
pairs as transliterations if they have a higher proba-
bility assigned by the transliteration sub-model than
by the non-transliteration sub-model.
We extend the unsupervised system to a semi-
supervised system by adding a new S-step to the
EM algorithm. The S-step takes the probability es-
timates from unlabelled data (computed in the M-
step) and uses them as a backoff distribution to
smooth probabilities which were estimated from la-
belled data. The smoothed probabilities are then
used in the next E-step. In this way, the parame-
ters learned by EM are constrained to values which
are close to those estimated from the labelled data.
We evaluate our unsupervised and semi-
supervised transliteration mining system on the
datasets available from the NEWS 2010 shared task
on transliteration mining (Kumaran et al, 2010b).
We call this task NEWS10 later on. Compared with
a baseline unsupervised system our unsupervised
system achieves up to 5% better F-measure. On
the NEWS10 dataset, our unsupervised system
achieves an F-measure of up to 95.7%, and on three
language pairs, it performs better than all systems
which participated in NEWS10. We also evaluate
our semi-supervised system which additionally uses
the NEWS10 labelled data for training. It achieves
an improvement of up to 3.7% F-measure over our
unsupervised system. Additional experiments on
parallel corpora show that we are able to effectively
mine transliteration pairs from very noisy data.
The paper is organized as follows. Section 2 de-
scribes previous work. Sections 3 and 4 define our
unsupervised and semi-supervised models. Section
5 presents the evaluation. Section 6 concludes.
469
2 Previous Work
We first discuss the literature on semi-supervised
and supervised techniques for transliteration min-
ing and then describe a previously defined unsuper-
vised system. Supervised and semi-supervised sys-
tems use a manually labelled set of training data to
learn character mappings between source and tar-
get strings. The labelled training data either con-
sists of a few hundred transliteration pairs or of
just a few carefully selected transliteration pairs.
The NEWS 2010 shared task on transliteration min-
ing (NEWS10) (Kumaran et al, 2010b) is a semi-
supervised task conducted on Wikipedia InterLan-
guage Links (WIL) data. The NEWS10 dataset con-
tains 1000 labelled examples (called the ?seed data?)
for initial training. All systems which participated
in the NEWS10 shared task are either supervised or
semi-supervised. They are described in (Kumaran
et al, 2010a). Our transliteration mining model
can mine transliterations without using any labelled
data. However, if there is some labelled data avail-
able, our system is able to use it effectively.
The transliteration mining systems evaluated on
the NEWS10 dataset generally used heuristic meth-
ods, discriminative models or generative models for
transliteration mining (Kumaran et al, 2010a).
The heuristic-based system of Jiampojamarn et
al. (2010) is based on the edit distance method
which scores the similarity between source and tar-
get words. They presented two discriminative meth-
ods ? an SVM-based classifier and alignment-based
string similarity for transliteration mining. These
methods model the conditional probability distribu-
tion and require supervised/semi-supervised infor-
mation for learning. We propose a flexible genera-
tive model for transliteration mining usable for both
unsupervised and semi-supervised learning.
Previous work on generative approaches uses
Hidden Markov Models (Nabende, 2010; Darwish,
2010; Jiampojamarn et al, 2010), Finite State Au-
tomata (Noeman and Madkour, 2010) and Bayesian
learning (Kahki et al, 2011) to learn transliteration
pairs from labelled data. Our method is different
from theirs as our generative story explains the un-
labelled data using a combination of a transliteration
and a non-transliteration sub-model. The translit-
eration model jointly generates source and target
strings, whereas the non-transliteration system gen-
erates them independently of each other.
Sajjad et al (2011) proposed a heuristic-based un-
supervised transliteration mining system. We later
call it Sajjad11. It is the only unsupervised mining
system that was evaluated on the NEWS10 dataset
up until now, as far as we know. That system is com-
putationally expensive. We show in Section 5 that its
runtime is much higher than that of our system.
In this paper, we propose a novel model-based
approach to transliteration mining. Our approach
is language pair independent ? at least for alpha-
betic languages ? and efficient. Unlike the pre-
vious unsupervised system, and unlike the super-
vised and semi-supervised systems we mentioned,
our model can be used for both unsupervised and
semi-supervised mining in a consistent way.
3 Unsupervised Transliteration Mining
Model
A source word and its corresponding target word can
be character-aligned in many ways. We refer to a
possible alignment sequence which aligns a source
word e and a target word f as ?a?. The function
Align(e, f) returns the set of all valid alignment se-
quences a of a word pair (e, f). The joint transliter-
ation probability p1(e, f) of a word pair is the sum
of the probabilities of all alignment sequences:
p1(e, f) =
?
a?Align(e,f)
p(a) (1)
Transliteration systems are trained on a list of
transliteration pairs. The alignment between the
transliteration pairs is learned with Expectation
Maximization (EM). We use a simple unigram
model, so an alignment sequence from function
Align(e, f) is a combination of 0?1, 1?1, and 1?
0 character alignments between a source word e and
its transliteration f . We refer to a character align-
ment unit as ?multigram? later on and represent it
by the symbol ?q?. A sequence of multigrams forms
an alignment of a source and target word. The prob-
ability of a sequence of multigrams a is the product
of the probabilities of the multigrams it contains.
p(a) = p(q1, q2, ..., q|a|) =
|a|?
j=1
p(qj) (2)
470
While transliteration systems are trained on a
clean list of transliteration pairs, our translitera-
tion mining system has to learn from data con-
taining both transliterations and non-transliterations.
The transliteration model p1(e, f) handles only the
transliteration pairs. We propose a second model
p2(e, f) to deal with non-transliteration pairs (the
?non-transliteration model?). Interpolation with the
non-transliteration model allows the transliteration
model to concentrate on modelling transliterations
during EM training. After EM training, transliter-
ation word pairs are assigned a high probability by
the transliteration submodel and a low probability by
the non-transliteration submodel, and vice versa for
non-transliteration pairs. This property is exploited
to identify transliterations.
In a non-transliteration word pair, the characters
of the source and target words are unrelated. We
model them as randomly seeing a source word and a
target word together. The non-transliteration model
uses random generation of characters from two uni-
gram models. It is defined as follows:
p2(e, f) = pE(e) pF (f) (3)
pE(e) =
?|e|
i=1 pE(ei) and pF (f) =
?|f |
i=1 pF (fi).
The transliteration mining model is an interpo-
lation of the transliteration model p1(e, f) and the
non-transliteration model p2(e, f):
p(e, f) = (1? ?)p1(e, f) + ?p2(e, f) (4)
? is the prior probability of non-transliteration.
3.1 Model Estimation
In this section, we discuss the estimation of the pa-
rameters of the transliteration model p1(e, f) and the
non-transliteration model p2(e, f).
The non-transliteration model consists of two un-
igram character models. Their parameters are esti-
mated from the source and target words of the train-
ing data, respectively, and the parameters do not
change during EM training.
For the transliteration model, we implement a
simplified form of the grapheme-to-phoneme con-
verter, g2p (Bisani and Ney, 2008). In the follow-
ing, we use notations from Bisani and Ney (2008).
g2p learns m-to-n character alignments between a
source and a target word. We restrict ourselves to
0?1,1?1,1?0 character alignments and to a unigram
model.1 The Expectation Maximization (EM) algo-
rithm is used to train the model. It maximizes the
likelihood of the training data. In the E-step the EM
algorithm computes expected counts for the multi-
grams and in the M-step the multigram probabilities
are reestimated from these counts. These two steps
are iterated. For the first EM iteration, the multigram
probabilities are initialized with a uniform distribu-
tion and ? is set to 0.5.
The expected count of a multigram q (E-step) is
computed by multiplying the posterior probability
of each alignment a with the frequency of q in a and
summing these weighted frequencies over all align-
ments of all word pairs.
c(q) =
N?
i=1
?
a?Align(ei,fi)
(1? ?)p1(a, ei, fi)
p(ei, fi)
nq(a)
nq(a) is here the number of times the multigram q
occurs in the sequence a and p(ei, fi) is defined in
Equation 4. The new estimate of the probability of a
multigram is given by:
p(q) =
c(q)
?
q? c(q
?)
(5)
Likewise, we calculate the expected count of non-
transliterations by summing the posterior probabili-
ties of non-transliteration given each word pair:
cntr =
N?
i=1
pntr(ei, fi) =
N?
i=1
?p2(ei, fi)
p(ei, fi)
(6)
? is then reestimated by dividing the expected count
of non-transliterations by N .
3.2 Implementation Details
We use the Forward-Backward algorithm to estimate
the counts of multigrams. The algorithm has a for-
ward variable? and a backward variable ? which are
calculated in the standard way (Deligne and Bimbot,
1995). Consider a node r which is connected with
a node s via an arc labelled with the multigram q.
The expected count of a transition between r and s
is calculated using the forward and backward prob-
abilities as follows:
??rs =
?(r) p(q) ?(s)
?(E)
(7)
1In preliminary experiments, using an n-gram order of
greater than one or more than one character on the source side or
the target side or both sides of the multigram caused the translit-
eration model to incorrectly learn non-transliteration informa-
tion from the training data.
471
where E is the final node of the graph.
We multiply the expected count of a transition
by the posterior probability of transliteration (1 ?
pntr(e, f)) which indicates how likely the string pair
is to be a transliteration. The counts ?rs are then
summed for all multigram types q over all training
pairs to obtain the frequencies c(q) which are used
to reestimate the multigram probabilities according
to Equation 5.
4 Semi-supervised Transliteration Mining
Model
Our unsupervised transliteration mining system can
be applied to language pairs for which no labelled
data is available. However, the unsupervised sys-
tem is focused on high recall and also mines close
transliterations (see Section 5 for details). In a task
dependent scenario, it is difficult for the unsuper-
vised system to mine transliteration pairs according
to the details of a particular definition of what is con-
sidered a transliteration (which may vary somewhat
with the task). In this section, we propose an exten-
sion of our unsupervised model which overcomes
this shortcoming by using labelled data. The idea
is to rely on probabilities from labelled data where
they can be estimated reliably and to use probabili-
ties from unlabelled data where the labelled data is
sparse. This is achieved by smoothing the labelled
data probabilities using the unlabelled data probabil-
ities as a backoff.
4.1 Model Estimation
We calculate the unlabelled data probabilities in the
E-step using Equation 4. For labelled data (contain-
ing only transliterations) we set ? = 0 and get:
p(e, f) =
?
a?Align(e,f)
p1(e, f, a) (8)
In every EM iteration, we smooth the probability
distribution in such a way that the estimates of the
multigrams of the unlabelled data that do not occur
in the labelled data would be penalized. We obtain
this effect by smoothing the probability distribution
of unlabelled and labelled data using a technique
similar to Witten-Bell smoothing (Witten and Bell,
1991), as we describe below.
Figure 1: Semi-supervised training
4.2 Implementation Details
We divide the training process of semi-supervised
mining in two steps as shown in Figure 1. The first
step creates a reasonable alignment of the labelled
data from which multigram counts can be obtained.
The labelled data is a small list of transliteration
pairs. Therefore we use the unlabelled data to help
correctly align it and train our unsupervised min-
ing system on the combined labelled and unlabelled
training data. In the expectation step, the prior prob-
ability of non-transliteration ? is set to zero on the
labelled data since it contains only transliterations.
The first step passes the resulting multigram proba-
bility distribution to the second step.
We start the second step with the probability es-
timates from the first step and run the E-step sepa-
rately on labelled and unlabelled data. The E-step
on the labelled data is done using Equation 8, which
forces the posterior probability of non-transliteration
to zero, while the E-step on the unlabelled data uses
Equation 4. After the two E-steps, we estimate
a probability distribution from the counts obtained
from the unlabelled data (M-step) and use it as a
backoff distribution in computing smoothed proba-
bilities from the labelled data counts (S-step).
The smoothed probability estimate p?(q) is:
p?(q) =
cs(q) + ?sp(q)
Ns + ?s
(9)
where cs(q) is the labelled data count of the multi-
gram q, p(q) is the unlabelled data probability es-
timate, and Ns =
?
q cs(q), and ?s is the number
of different multigram types observed in the Viterbi
alignment of the labelled data.
472
5 Evaluation
We evaluate our unsupervised system and semi-
supervised system on two tasks, NEWS10 and paral-
lel corpora. NEWS10 is a standard task on translit-
eration mining from WIL. On NEWS10, we com-
pare our results with the unsupervised mining sys-
tem of Sajjad et al (2011), the best supervised
and semi-supervised systems presented at NEWS10
(Kumaran et al, 2010b) and the best supervised and
semi-supervised results reported in the literature for
the NEWS10 task. For the challenging task of min-
ing from parallel corpora, we use the English/Hindi
and English/Arabic gold standard provided by Saj-
jad et al (2011) to evaluate our results.
5.1 Experiments using the NEWS10 Dataset
We conduct experiments on four language pairs: En-
glish/Arabic, English/Hindi, English/Tamil and En-
glish/Russian using data provided at NEWS10. Ev-
ery dataset contains training data, seed data and ref-
erence data. The NEWS10 data consists of pairs of
titles of the same Wikipedia pages written in dif-
ferent languages, which may be transliterations or
translations. The seed data is a list of 1000 transliter-
ation pairs provided to semi-supervised systems for
initial training. We use the seed data only in our
semi-supervised system, and not in the unsupervised
system. The reference data is a small subset of the
training data which is manually annotated with pos-
itive and negative examples.
5.1.1 Training
We word-aligned the parallel phrases of the train-
ing data using GIZA++ (Och and Ney, 2003), and
symmetrized the alignments using the grow-diag-
final-and heuristic (Koehn et al, 2003). We extract
all word pairs which occur as 1-to-1 alignments (like
Sajjad et al (2011)) and later refer to them as the
word-aligned list. We compared the word-aligned
list with the NEWS10 reference data and found that
the word-aligned list is missing some transliteration
pairs because of word-alignment errors. We built an-
other list by adding a word pair for every source
word that cooccurs with a target word in a paral-
lel phrase/sentence and call it the cross-product list
later on. The cross-product list is noisier but con-
tains almost all transliteration pairs in the corpus.
Word-aligned Cross-product
P R F P R F
EA 27.8 97.1 43.3 14.3 98.0 25.0
EH 42.5 98.7 59.4 20.5 99.6 34.1
ET 32.0 98.1 48.3 17.2 99.6 29.3
ER 25.5 95.6 40.3 12.8 99.0 22.7
Table 1: Statistics of word-aligned and cross-product
list calculated from the NEWS10 dataset, before min-
ing. EA is English/Arabic, EH is English/Hindi, ET is
English/Tamil and ER is English/Russian
Table 1 shows the statistics of the word-aligned
list and the cross-product list calculated using the
NEWS10 reference data.2 The word-aligned list cal-
culated from the NEWS10 dataset is used to com-
pare our unsupervised system with the unsupervised
system of Sajjad et al (2011) on the same training
data. All the other experiments on NEWS10 use
cross-product lists. We remove numbers from both
lists as they are defined as non-transliterations (Ku-
maran et al, 2010b).
5.1.2 Unsupervised Transliteration Mining
We run our unsupervised transliteration mining
system on the word-aligned list and the cross-
product list. The word pairs with a posterior prob-
ability of transliteration 1 ? pntr(e, f) = 1 ?
?p2(ei, fi)/p(ei, fi) greater than 0.5 are selected as
transliteration pairs.
We compare our unsupervised system with the
unsupervised system of Sajjad11. Our unsupervised
system trained on the word-aligned list shows F-
measures of 91.7%, 95.5%, 92.9% and 77.7% which
is 4.3%, 3.3%, 2.8% and 1.7% better than the sys-
tem of Sajjad11 on English/Arabic, English/Hindi,
English/Tamil and English/Russian respectively.
Sajjad11 is computationally expensive. For in-
stance, a phrase-based statistical MT system is
built once in every iteration of the heuristic proce-
dure. We ran Sajjad11 on the English/Russian word-
aligned list using a 2.4 GHz Dual-Core AMD ma-
chine, which took almost 10 days. On the same ma-
chine, our transliteration mining system only takes
1.5 hours to finish the same experiment.
2Due to inconsistent word definition used in the reference
data, we did not achieve 100% recall in our cross-product list.
For example, the underscore is defined as a word boundary for
English WIL phrases. This assumption is not followed for cer-
tain phrases like ?New York? and ?New Mexico?.
473
Unsupervised Semi-supervised/Supervised
SJD OU OS SBest GR DBN
EA 87.4 92.4 92.7 91.5 94.1 -
EH 92.2 95.7 96.3 94.4 93.2 95.5
ET 90.1 93.2 94.6 91.4 95.5 93.9
ER 76.0 79.4 83.1 87.5 92.3 82.5
Table 2: F-measure results on NEWS10 datasets where
SJD is the unsupervised system of Sajjad11, OU is
our unsupervised system built on the cross-product list,
OS is our semi-supervised system, SBest is the best
NEWS10 system, GR is the supervised system of Kahki
et al (2011) and DBN is the semi-supervised system of
Nabende (2011)
Our unsupervised mining system built on the
cross-product list consistently outperforms the one
built on the word-aligned list. Later, we consider
only the system built on the cross-product list. Ta-
ble 2 shows the results of our unsupervised sys-
tem OU in comparison with the unsupervised sys-
tem of Sajjad11 (SJD), the best semi-supervised sys-
tems presented at NEWS10 (SBEST ) and the best
semi-supervised results reported on the NEWS10
dataset (GR, DBN ). On three language pairs, our
unsupervised system performs better than all semi-
supervised systems which participated in NEWS10.
It has competitive results with the best supervised
results reported on NEWS10 datasets. On En-
glish/Hindi, our unsupervised system outperforms
the state-of-the-art supervised and semi-supervised
systems. Kahki et al (2011) (GR) achieved
the best results on English/Arabic, English/Tamil
and English/Russian. For the English/Arabic task,
they normalized the data using language dependent
heuristics3 and also used a non-standard evaluation
method (discussed in Section 5.1.4).
On the English/Russian dataset, our unsupervised
system faces the problem that it extracts cognates
as transliterations. The same problem was reported
in Sajjad et al (2011). Cognates are close translit-
erations which differ by only one or two characters
from an exact transliteration pair. The unsupervised
system learns to delete the additional one or two
characters with a high probability and incorrectly
mines such word pairs as transliterations.
3They applied an Arabic word segmenter which uses lan-
guage dependent information. Arabic long vowels which have
identical sound but are written differently were merged to one
form. English characters were normalized by dropping accents.
Unsupervised Semi-supervised
P R F P R F
EA 89.2 95.7 92.4 92.9 92.4 92.7
EH 92.6 99.0 95.7 95.5 97.0 96.3
ET 88.3 98.6 93.2 93.4 95.8 94.6
ER 67.2 97.1 79.4 74.0 94.9 83.1
Table 3: Precision(P), Recall(R) and F-measure(F) of our
unsupervised and semi-supervised transliteration mining
systems on NEWS10 datasets
5.1.3 Semi-supervised Transliteration Mining
Our semi-supervised system uses similar initial-
ization of the parameters as used for unsupervised
system. Table 2 shows on three language pairs, our
semi-supervised system OS only achieves a small
gain in F-measure over our unsupervised system
OU . This shows that the unlabelled training data is
already providing most of the transliteration infor-
mation. The seed data is used to help the translit-
eration mining system to learn the right definition
of transliteration. On the English/Russian dataset,
our semi-supervised system achieves almost 7% in-
crease in precision with a 2.2% drop in recall com-
pared to our unsupervised system. This provides a
3.7% gain on F-measure. The increase in precision
shows that the seed data is helping the system in dis-
ambiguating transliteration pairs from cognates.
5.1.4 Discussion
The unsupervised system produces lists with high
recall. The semi-supervised system tends to better
balance out precision and recall. Table 3 compares
the precision, recall and F-measure of our unsuper-
vised and semi-supervised mining systems.
The errors made by our semi-supervised system
can be classified into the following categories:
Pronunciation differences: English proper
names may be pronounced differently in other lan-
guages. Sometimes, English short vowels are con-
verted to long vowels in Hindi such as the English
word ?Lanthanum? which is pronounced ?Laan-
thanum? in Hindi. Our transliteration mining system
wrongly extracts such pairs as transliterations.
In some cases, different vowels are used in two
languages. The English word ?January? is pro-
nounced as ?Janvary? in Hindi. Such word pairs are
non-transliterations according to the gold standard
but our system extracts them as transliterations. Ta-
474
Table 4: Word pairs with pronunciation differences
Table 5: Examples of word pairs which are wrongly an-
notated as transliterations in the gold standard
ble 4 shows a few examples of such word pairs.
Inconsistencies in the gold standard: There are
several inconsistencies in the gold standard where
our transliteration system correctly identifies a word
pair as a transliteration but it is marked as a non-
transliteration or vice versa. Consider the example
of the English word ?George? which is pronounced
as ?Jaarj? in Hindi. Our semi-supervised system
learns this as a non-transliteration but it is wrongly
annotated as a transliteration in the gold standard.
Arabic nouns have an article ?al? attached to them
which is translated in English as ?the?. There are
various cases in the training data where an English
noun such as ?Quran? is matched with an Arabic
noun ?alQuran?. Our mining system classifies such
cases as non-transliterations, but 24 of them are in-
correctly annotated as transliterations in the gold
standard. We did not correct this, and are there-
fore penalized. Kahki et al (2011) preprocessed
such Arabic words and separated ?al? from the noun
?Quran? before mining. They report a match if the
version of the Arabic word with ?al? appears with
the corresponding English word in the gold stan-
dard. Table 5 shows examples of word pairs which
are wrongly annotated as transliterations.
Cognates: Sometimes a word pair differs by only
one or two ending characters from a true translit-
eration. For example in the English/Russian train-
ing data, the Russian nouns are marked with cases
whereas their English counterparts do not mark the
case or translate it as a separate word. Often the
Russian word differs only by the last character from
a correct transliteration of the English word. Due
to the large amount of such word pairs in the En-
glish/Russian data, our mining system learns to
delete the final case marking characters from the
Russian words. It assigns a high transliteration prob-
Table 6: A few examples of English/Russian cognates
ability to these word pairs and extracts them as
transliterations. Table 6 shows some examples.
There are two English/Russian supervised sys-
tems which are better than our semi-supervised sys-
tem. The Kahki et al (2011) system is built on seed
data only. Jiampojamarn et al (2010)?s best sys-
tem on English/Russian is based on the edit distance
method. Both of these systems are focused on high
precision. Our semi-supervised system is focused
on high recall at the cost of lower precision.4
5.2 Transliteration Mining using Parallel
Corpora
The percentage of transliteration pairs in the
NEWS10 datasets is high. We further check the ef-
fectiveness of our unsupervised and semi-supervised
mining systems by evaluating them on parallel cor-
pora with as few as 2% transliteration pairs.
We conduct experiments using two language
pairs, English/Hindi and English/Arabic. The En-
glish/Hindi corpus is from the shared task on word
alignment organized as part of the ACL 2005 Work-
shop on Building and Using Parallel Texts (WA05)
(Martin et al, 2005). For English/Arabic, we use
200,000 parallel sentences from the United Nations
(UN) corpus (Eisele and Chen, 2010). The En-
glish/Hindi and English/Arabic transliteration gold
standards were provided by Sajjad et al (2011).
5.2.1 Experiments
We follow the procedure for creating the training
data described in Section 5.1.1 and build a word-
aligned list and a cross-product list from the parallel
corpus. We first train and test our unsupervised min-
ing system on the word-aligned list and compare our
results with Sajjad et al Table 7 shows the results.
Our unsupervised system achieves 0.6% and 1.8%
higher F-measure than Sajjad et al respectively.
The cross-product list is huge in comparison to
the word-aligned list. It is noisier than the word-
4We implemented a bigram version of our system to learn
the contextual information at the end of the word pairs, but only
achieved a gain of less than 1% F-measure over our unigram
semi-supervised system. Details are omitted due to space.
475
TP FN TN FP P R F
EHSJD 170 10 2039 45 79.1 94.4 86.1
EHO 176 4 2034 50 77.9 97.8 86.7
EASJD 197 91 6580 59 77.0 68.4 72.5
EAO 288 0 6440 199 59.1 100 74.3
Table 7: Transliteration mining results of our unsuper-
vised system and Sajjad11 system trained and tested
on the word-aligned list of English/Hindi and En-
glish/Arabic parallel corpus
TP FN TN FP P R F
EHU 393 19 12279 129 75.3 95.4 84.2
EHS 365 47 12340 68 84.3 88.6 86.4
EAU 277 11 6444 195 58.7 96.2 72.9
EAS 272 16 6497 142 65.7 94.4 77.5
Table 8: Transliteration mining results of our unsuper-
vised and semi-supervised systems trained on the word-
aligned list and tested on the cross-product list of En-
glish/Hindi and English/Arabic parallel corpus
aligned list but has almost 100% recall of transliter-
ation pairs. The English-Hindi cross-product list has
almost 55% more transliteration pairs (412 types)
than the word-aligned list (180 types). We can not
report these numbers on the English/Arabic cross-
product list since the English/Arabic gold standard
is built on the word-aligned list.
In order to keep the experiment computationally
inexpensive, we train our mining systems on the
word-aligned list and test them on the cross-product
list.5 We also perform the first semi-supervised eval-
uation on this task. For our semi-supervised sys-
tem, we additionally use the English/Hindi and En-
glish/Arabic seed data provided by NEWS10.
Table 8 shows the results of our unsupervised
and semi-supervised systems on the English/Hindi
and English/Arabic parallel corpora. Our unsu-
pervised system achieves higher recall than our
semi-supervised system but lower precision. The
semi-supervised system shows an improvement in
F-measure for both language pairs. We looked
into the errors made by our systems. The mined
transliteration pairs of our unsupervised system con-
tains 65 and 111 close transliterations for the En-
glish/Hindi and English/Arabic task respectively.
5There are some multigrams of the cross-product list which
are unknown to the model learned on the word-aligned list. We
define their probability as the inverse of the number of multi-
gram tokens in the Viterbi alignment of the labelled and unla-
belled data together.
The close transliterations only differ by one or two
characters from correct transliterations. We think
these pairs provide transliteration information to
the systems and help them to avoid problems with
data sparseness. Our semi-supervised system uses
the seed data to identify close transliterations as
non-transliterations and decreases the number of
false positives. They are reduced to 35 and 89
for English/Hindi and English/Arabic respectively.
The seed data and the training data used in the
semi-supervised system are from different domains
(Wikipedia and UN). Seed data extracted from the
same domain is likely to work better, resulting in
even higher scores than we have reported.
6 Conclusion and Future Work
We presented a novel model to automatically
mine transliteration pairs. Our approach is ef-
ficient and language pair independent (for alpha-
betic languages). Both the unsupervised and semi-
supervised systems achieve higher accuracy than the
only unsupervised transliteration mining system we
are aware of and are competitive with the state-
of-the-art supervised and semi-supervised systems.
Our semi-supervised system outperformed our un-
supervised system, in particular in the presence of
prevalent cognates in the Russian/English data.
In future work, we plan to adapt our approach
to language pairs where one language is alphabetic
and the other language is non-alphabetic such as En-
glish/Japanese. These language pairs require one-
to-many character mappings to learn transliteration
units, while our current system only learns unigram
character alignments.
Acknowledgments
The authors wish to thank the anonymous review-
ers. We would like to thank Syed Aoun Raza for
discussions of implementation efficiency. Hassan
Sajjad was funded by the Higher Education Com-
mission of Pakistan. Alexander Fraser was funded
by Deutsche Forschungsgemeinschaft grant Models
of Morphosyntax for Statistical Machine Transla-
tion. Helmut Schmid was supported by Deutsche
Forschungsgemeinschaft grant SFB 732. This work
was supported in part by the IST Programme of
the European Community, under the PASCAL2 Net-
work of Excellence, IST-2007-216886. This publi-
cation only reflects the authors? views.
476
References
Maximilian Bisani and Hermann Ney. 2008. Joint-
sequence models for grapheme-to-phoneme conver-
sion. Speech Communication, 50(5).
Kareem Darwish. 2010. Transliteration mining with
phonetic conflation and iterative training. In Proceed-
ings of the 2010 Named Entities Workshop, Uppsala,
Sweden.
Sabine Deligne and Fre?de?ric Bimbot. 1995. Language
modeling by variable length sequences : Theoreti-
cal formulation and evaluation of multigrams. In
Proceedings of the IEEE International Conference on
Acoustics, Speech, and Signal Processing, volume 1,
Los Alamitos, CA, USA.
Andreas Eisele and Yu Chen. 2010. MultiUN: A multi-
lingual corpus from United Nation documents. In Pro-
ceedings of the Seventh conference on International
Language Resources and Evaluation (LREC?10), Val-
letta, Malta.
Sittichai Jiampojamarn, Kenneth Dwyer, Shane Bergsma,
Aditya Bhargava, Qing Dou, Mi-Young Kim, and
Grzegorz Kondrak. 2010. Transliteration generation
and mining with limited training resources. In Pro-
ceedings of the 2010 Named Entities Workshop, Upp-
sala, Sweden.
Ali El Kahki, Kareem Darwish, Ahmed Saad El Din,
Mohamed Abd El-Wahab, Ahmed Hefny, and Waleed
Ammar. 2011. Improved transliteration mining using
graph reinforcement. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP), Edinburgh, UK.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
the Human Language Technology and North Ameri-
can Association for Computational Linguistics Con-
ference, Edmonton, Canada.
A Kumaran, Mitesh M. Khapra, and Haizhou Li. 2010a.
Report of NEWS 2010 transliteration mining shared
task. In Proceedings of the 2010 Named Entities Work-
shop, Uppsala, Sweden.
A Kumaran, Mitesh M. Khapra, and Haizhou Li. 2010b.
Whitepaper of NEWS 2010 shared task on translitera-
tion mining. In Proceedings of the 2010 Named Enti-
ties Workshop, Uppsala, Sweden.
Haizhou Li, Zhang Min, and Su Jian. 2004. A joint
source-channel model for machine transliteration. In
ACL ?04: Proceedings of the 42nd Annual Meeting on
Association for Computational Linguistics, Barcelona,
Spain.
Joel Martin, Rada Mihalcea, and Ted Pedersen. 2005.
Word alignment for languages with scarce resources.
In ParaText ?05: Proceedings of the ACL Workshop
on Building and Using Parallel Texts, Morristown, NJ,
USA.
Peter Nabende. 2010. Mining transliterations from
wikipedia using pair hmms. In Proceedings of the
2010 Named Entities Workshop, Uppsala, Sweden.
Peter Nabende. 2011. Mining transliterations from
Wikipedia using dynamic bayesian networks. In Pro-
ceedings of the International Conference Recent Ad-
vances in Natural Language Processing 2011, Hissar,
Bulgaria.
Sara Noeman and Amgad Madkour. 2010. Language
independent transliteration mining system using finite
state automata framework. In Proceedings of the 2010
Named Entities Workshop, Uppsala, Sweden.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1).
Hassan Sajjad, Alexander Fraser, and Helmut Schmid.
2011. An algorithm for unsupervised transliteration
mining with an application to word alignment. In Pro-
ceedings of the 49th Annual Conference of the Associ-
ation for Computational Linguistics, Portland, USA.
Ian H. Witten and Timothy C. Bell. 1991. The zero-
frequency problem: Estimating the probabilities of
novel events in adaptive text compression. In IEEE
Transactions on Information Theory, volume 37.
477
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 593?603,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Using subcategorization knowledge to improve case prediction
for translation to German
Marion Weller1 Alexander Fraser2 Sabine Schulte im Walde1
1Institut fu?r Maschinelle 2Centrum fu?r Informations-
Sprachverarbeitung und Sprachverarbeitung
Universita?t Stuttgart Ludwig-Maximilians-Universita?t Mu?nchen
{wellermn|schulte}@ims.uni-stuttgart.de fraser@cis.uni-muenchen.de
Abstract
This paper demonstrates the need and im-
pact of subcategorization information for
SMT. We combine (i) features on source-
side syntactic subcategorization and (ii)
an external knowledge base with quantita-
tive, dependency-based information about
target-side subcategorization frames. A
manual evaluation of an English-to-
German translation task shows that the
subcategorization information has a posi-
tive impact on translation quality through
better prediction of case.
1 Introduction
When translating from a morphologically poor
language to a morphologically rich language we
are faced with two major problems: (i) the rich-
ness of the target-language morphology causes
data sparsity problems, and (ii) information about
morphological features on the target side is not
sufficiently contained in the source language mor-
phology.
We address these two problems using a two-
step procedure. We first replace inflected forms
by their stems or lemmas: building a translation
system on a stemmed representation of the target
side leads to a simpler translation task, and the
morphological information contained in the source
and target language parts of the translation model
is more balanced. In the second step, the stemmed
output of the translation is then inflected: the mor-
phological features are predicted, and the inflected
forms are generated using the stem and predicted
morphological features.
In this paper, we focus on improving case pre-
diction for noun phrases (NPs) in German trans-
lations. The NP feature case is extremely dif-
ficult to predict in German: while the NP fea-
tures gender and number are part of the stem or
can be derived from the source-side input, respec-
tively, the prediction of case requires information
about the subcategorization of the entire clause.
This is due to German being a less configurational
language than English, which encodes grammati-
cal relations (e.g. subject-hood, object-hood, etc.)
through the position of constituents. German sen-
tences exhibit a freer constituent order, and thus
case is an important indicator of the grammatical
functions of noun phrases. Correct case predic-
tion is a crucial factor for the adequacy of SMT
output, cf. the example in table 1 providing an
erroneously inflected output (this is taken from a
baseline ?simple inflection prediction? system, cf.
section 5.2). The translation of the English input
sentence in terms of stems is perfectly acceptable;
after the inflection step, however, the translation
of NP4 ongoing military actions represents a geni-
tive modifier of the subject NP2, instead of a direct
object NP of the verb anordnen (to order). The
meaning is thus why the government of the ongo-
ing military actions ordered, which has only one
NP and is completely wrong.
The translation in table 1 needs verb subcatego-
rization information. This is demonstrated by the
invented examples (1) and (2):
(1) [Der Mitarbeiter]NPnom hat [den Bericht]NPacc [dem
Kollegen]NPdat gegeben.
[The employee]NPnom gave [his colleague]NPdat [the
report]NPacc
(2) [Der Mitarbeiter]NPnom hat [dem Bericht]NPdat [des
Kollegen]NPgen zugestimmt.
[The employee]NPnom agreed [on the report]PP [of
his colleague]PP
Both inflected sentences rely on the stem sequence
[d Mitarbeiter] [d Bericht] [d Kollege] ?verb?,
so the case assignment can only be determined by
the verb: While geben ( to give) has a strong pref-
erence for selecting a ditransitive subcategoriza-
tion frame1, including an agentive subject (nomi-
1A ditransitive verb takes a subject and two objects.
593
input [why]1 [the government]2 [ordered]3 [the ongoing military actions]4
output stemmed [warum]1 [d Regierung]2 [d anhaltend milita?risch Aktion]4 [angeordnet]3inflected [warum]1 [die Regierung]2 [der anhaltenden milita?rischen Aktionen]4 [angeordnet]3
Table 1: Example for case confusion in SMT output when using a simple prediction system.
native case), a benefactive (dative case) and a pa-
tient (accusative case), zustimmen (to agree) has
a strong preference for only selecting an agentive
subject (nominative case) and an indirect object
theme (dative case). So in the latter case the NP
[d Kollege] cannot receive case from the verb and
is instead the genitive modifier of the dative NP.
While for examples (1) and (2) knowledge
about the syntactic verb subcategorization func-
tions is sufficient to correctly predict the NP cases,
examples (3) to (6) require subcategorization in-
formation at the syntax-semantic interface.
(3) [Der Mitarbeiter]NPnom hat [dem Kollegen]NPdat
[den Bericht]NPacc gegeben.
(4) [Der Mitarbeiter]NPnom hat [den Bericht]NPacc [dem
Kollegen]NPdat gegeben.
(5) [Dem Kollegen]NPdat hat [der Mitarbeiter]NPnom
[den Bericht]NPacc gegeben.
(6) [Den Bericht]NPacc hat [der Mitarbeiter]NPnom [dem
Kollegen]NPdat gegeben.
In all four examples, the verb and the participat-
ing noun phrases Mitarbeiter (employee), Kollege
(colleague) and Bericht (report) are identical, and
the noun phrases are assigned the same case. How-
ever, given that the stemmed output of the trans-
lation does not tell us anything about case fea-
tures, in order to predict the appropriate cases of
the three noun phrases, we either rely on ordering
heuristics (such that the nominative NP is more
likely to be in the beginning of the sentence (the
German Vorfeld) than the accusative or dative NP,
even though all three of these would be grammati-
cal), or we need fine-grained subcategorization in-
formation beyond pure syntax. For example, both
Mitarbeiter and Kollege would satisfy the agentive
subject role of the verb geben better than Bericht,
and Bericht is more likely to be the patient of
geben.
The contribution of this paper is to improve the
prediction of case in our SMT system by imple-
menting and combining two alternative routes to
integrate subcategorization information from the
syntax-semantic interface: (i) We regard the trans-
lation as a function of the source language in-
put, and project the syntactic functions of the En-
glish nouns to their German translations in the
SMT output. This subcategorization model is nec-
essary when there are several plausible solutions
for the syntactic functions of a noun in combina-
tion with a verb. For example, both Mitarbeiter
and Kollege are plausible subjects and direct ob-
jects of the verb geben, so the information about
these nouns? roles in the input sentence allows
for disambiguation. (ii) The case of an NP is de-
rived from an external knowledge base comprising
quantitative, dependency-based information about
German verb subcategorization frames and noun
modification. The verb subcategorization infor-
mation is not restricted to syntactic noun func-
tions but models association strength for verb?
noun pairs with regard to the entire subcatego-
rization frame plus the syntactic functions of the
nouns. For example, the database can tell us that
while the verb geben is very likely to subcatego-
rize a ditransitive frame, the verb zustimmen is
very likely to subcategorize only a direct object,
next to the obligatory subject (subcat frame pre-
diction). Furthermore, we can retrieve the infor-
mation that the noun Bericht is less likely to ap-
pear as subject of geben than the nouns Mitar-
beiter and Kollege (verb?noun subcat case pre-
diction). And we can look up that the noun Aktion
is very unlikely to be a genitive modification of
Regierung (cf. table 1), while Kollege is a plausi-
ble genitive modification of Bericht (noun?noun
modification case prediction, cf. example (2)).
In summary, model (i) applies when there are no
obvious preferences concerning verb?noun sub-
categorization or noun?noun modification. Model
(ii) predicts case relying on the subcategoriza-
tion and modification preferences. The combina-
tion of our two models approaches a simplified
level of semantic role definition but only relies on
dependency information that is considerably eas-
ier and cheaper to define and obtain than a very
high quality semantic parser and/or a corpus an-
notated with semantic role information. Integrat-
ing semantic role information into SMT has been
demonstrated by various researchers to improve
translation quality (cf. Wu and Fung (2009a), Wu
and Fung (2009b), Liu and Gildea (2008), Liu
and Gildea (2010)). Our approach is in line with
594
Wu and Fung (2009b) who demonstrated that on
the one hand 84% of verb syntactic functions in
a 50-sentence test corpus projected from Chinese
to English, and that on the other hand about 15%
of the subjects were not translated into subjects,
but their semantic roles were preserved across lan-
guage. These two findings correspond to the ex-
pected uses of our models (i) and (ii), respectively.
2 Previous work
Previous work has already introduced the idea of
generating inflected forms as a post-processing
step for a translation system that has been
stripped of (most) target-language-specific fea-
tures. Toutanova et al (2008) and Jeong et al
(2010) built translation systems that predict in-
flected word forms based on a large array of mor-
phological and syntactic features, obtained from
both source and target side. Kholy and Habash
(2012) and Green and DeNero (2012) work on En-
glish to Arabic translation and model gender, num-
ber and definiteness, focusing primarily on im-
proving fluency.
Fraser et al (2012) used a phrase-based system
to transfer stems and generated inflected forms
based on the stems and their morphological fea-
tures. For case prediction, they trained a CRF with
access to lemmas and POS-tags within a given
window. We re-implemented the system by Fraser
et al as a hierarchical machine translation system
using a string-to-tree setup. In contrast to the flat
phrase-based setting of Fraser et al (2012), syn-
tactic trees on the SMT output allow us to work
with verb?noun structures, which are relevant for
case prediction. While the CRF used for case pre-
diction in Fraser et al (2012) has access to lexi-
cal information, it is limited to a certain window
size and has no direct information about the rela-
tion of verb?noun pairs occurring in the sentence.
Using a window of a limited size is particularly
problematic for German, as there can be large gaps
between the verb and its subcategorized nouns; in-
troducing information about the relation of verbs
and nouns helps to bridge such gaps. Furthermore,
that model was not able to make effective use of
source-side features.
One of the objectives of using an inflection
prediction model is morphologically well-formed
output. Kirchhoff et al (2012) evaluated user re-
actions to different error types in machine trans-
lation and came to the result that morphological
well-formedness has only a marginal impact on
the comprehensibility of SMT output in the case
of English-Spanish translation. As already dis-
cussed, German case is essential to the meaning
of the sentence, so this result will not hold for Ger-
man output.
3 Translation pipeline
This section presents an overview of our two-step
translation process. In the first step, English in-
put is translated to German stems. In the sec-
ond step, morphological features are predicted and
inflected forms are generated based on the word
stems and the morphological features. In subsec-
tions 3.1 to 3.4, we present the simple version of
the inflection prediction system; our new features
are described in sections 4.2 and 4.3.
3.1 Stemmed representation/feature markup
We first parse the German side of the parallel
training data with BitPar (Schmid, 2004). This
maps each surface form appearing in normal text
to a stem and morphological features (case, gen-
der, number). We use this representation to create
the stemmed representation for training the trans-
lation model. With the exception of stem-markup
(discussed below), all morphological features are
removed from the stemmed representation. The
stem markup is used as part of the input to the fea-
ture prediction; the basic idea is that the given fea-
ture values are picked up by the prediction model
and then propagated over the phrase.
Nouns, as the head of NPs and PPs, are anno-
tated with gender and number. We consider gen-
der as part of the stem, whereas the value for num-
ber is derived from the source-side: if marked for
number, singular/plural nouns are distinguished
during word alignment and then translated accord-
ingly. Prepositions are also annotated with case;
many prepositions are restricted to only one case,
some are ambiguous and allow for either dative
or accusative. Other words which are subject to
feature prediction (e.g. adjectives, articles) are re-
duced to their stems with no feature markup, as
are all remaining words. As sole exception, we
keep the inflected forms of verbs (verbal inflec-
tion is not modelled). In addition to the transla-
tion model, the target-side language model, as well
as the reference data for parameter tuning use this
representation.
595
3.2 Building a stemmed translation model
We use a hierarchical translation system. Instead
of translating phrases, a hierarchical system ex-
tracts translation rules (Galley et al, 2004) which
allow the decoder to provide a tree spanning over
the translated sentence. In order to avoid sparsity
during rule extraction, we use a string-to-tree
setup, where only the target-side part of the data
is parsed. Translation rules are of the following
form:
[X]1 allows [X]2 ? [NP]1 [NP]2 erlaubt
[X]1 allows [X]2 ? [NP]1 erlaubt [NP]2
This example illustrates how rules can cover the
different word ordering possibilities in German.
PP nodes are annotated with their respective
case, as well as with the lemma of the preposition
they contain. In our experiments, this enriched an-
notation has small improvements over the simpler
setting with only head categories (details omit-
ted). This outcome, in particular that adding the
lemma of the preposition to the PP node helps to
improve translation quality, has been observed be-
fore in tree restructuring work for improving trans-
lation (Huang and Knight, 2006).
3.3 Feature prediction and generation of
inflected forms
In this section we discuss our focus, which is pre-
diction of case, but also the prediction of num-
ber, gender and strong/weak adjectival inflection.
The latter feature is German-specific; its values2
(strong/weak) depend on the combination of the
other features, as well as on the type of determiner
(e.g. definite/indefinite/none).
Morphological features are predicted on four
separate CRF models, one for each feature. The
models for case, number and gender are indepen-
dent of another, whereas the model for adjecti-
val inflection requires information about these fea-
tures, and is thus the last one to be computed, tak-
ing the output of the 3 other models as part of its
input. In contrast, the adjectival inflection model
in Fraser et al (2012) is independent from the
other features. Each model has access to stems,
POS-tags and the feature to be modelled within a
window of four positions to the right and the left
of the current position3.
2Note that the values for strong/weak inflection are not
always the same over the phrase, but follow a certain pattern
depending on the settings of case, number and gender.
3Preliminary experiments showed that larger windows do
not improve translation quality.
Table 2 illustrates the different steps of the in-
flection process: the markup (number and gender
on nouns) in the stemmed output of the SMT sys-
tem is part of the input to the respective feature
prediction. For gender and number, the values
given on the stems of the nouns are then propa-
gated over the phrase. While the case of prepo-
sitional phrases is determined by the case annota-
tion on prepositions, the case of nominal phrases
is computed only based on the respective contexts.
After predicting all morphological features, the in-
formation required to generate inflected forms is
complete: based on the stems and the features, we
use the morphological tool SMOR (Schmid et al,
2004) for the generation of inflected forms.
One general problem with feature-prediction is
that the ill-formed SMT output is not well repre-
sented by the training data which consists of well-
formed sentences. This problem was also men-
tioned by Stymne and Cancedda (2011) and Kholy
and Habash (2012). They deal with this problem
by translating the training data and annotating it
with the respective features, and then adding this
new data set to the original training data. As
this method comes with its own problems, such as
transferring the morphological annotation to not
necessarily isomorphically translated text, we do
not use translated data as part of the training data.
Instead, we limit the power of the CRF model
through experimenting with the removal of fea-
tures, until we had a system that was robust to this
problem.
3.4 Dealing with word formation issues
To reduce data sparsity, we split portmanteau
prepositions. Portmanteaus are compounds of
prepositions and articles, e.g. zur = zu der (to the).
Being components of nominal phrases, they have
to agree in all morphological features with the rest
of the phrase. As only some combinations of arti-
cles and prepositions can form a portmanteau, the
decision of whether to merge prepositions and ar-
ticles is made after feature prediction. Since our
focus is case prediction, we do not do special mod-
elling of German compounds.
4 Using subcategorization information
Within the area of (automatic) lexical acquisition,
the definition of lexical verb information has been
a major focus, because verbs play a central role
for the structure and the meaning of sentences and
596
SMT output predicted features inflected forms gloss
beeinflussen<VVFIN> ? beeinflussen influence
d<ART> Fem.Acc.Sg.St die the
politisch<ADJ> Fem.Acc.Sg.Wk politische political
Stabilita?t<NN><Fem><Sg> Fem.Acc.Sg.Wk Stabilita?t stability
Table 2: Overview of the inflection process: the stem markup is highlighted in the SMT output.
discourse. On the one hand, this has led to a range
of manually or semi-automatically developed lex-
ical resources focusing on verb information, such
as the Levin classes (Levin, 1993), VerbNet (Kip-
per Schuler, 2006), FrameNet4 (Fillmore et al,
2003), and PropBank (Palmer et al, 2005). On the
other hand, we find automatic approaches to the
induction of verb subcategorization information at
the syntax-semantics interface for a large num-
ber of languages, e.g. Briscoe and Carroll (1997)
for English; Sarkar and Zeman (2000) for Czech;
Schulte im Walde (2002a) for German; Messiant
(2008) for French. This basic kind of verb knowl-
edge has been shown to be useful in many NLP
tasks such as information extraction (Surdeanu et
al., 2003; Venturi1 et al, 2009), parsing (Carroll et
al., 1998; Carroll and Fang, 2004) and word sense
disambiguation (Kohomban and Lee, 2005; Mc-
Carthy et al, 2007).
4.1 Extracting subcategorization information
As described in the introductory section, we make
use of two5 major kinds of subcategorization in-
formation. Verb?noun tuples referring to spe-
cific syntactic functions within verb subcatego-
rization (verb?noun subcat case prediction) are
integrated with an associated probability for ac-
cusative (direct object), dative (indirect object)
and nominative (subject).6 Further to the sub-
ject and object noun phrases, the subcategoriza-
tion information provides quantitative triples for
verb?preposition?noun pairs, thus predicting the
case of NPs within prepositional phrases (we do
this only when the prepositions are ambiguious,
i.e., they could subcategorize either a dative or
an accusative NP). In addition to modelling sub-
categorization information, it is also important to
differentiate between subcategorized noun phrases
(such as object or subject), and noun phrases
4Even though the FrameNets approach does not only in-
clude knowledge about verbal predicates, the actual lexicons
are skewed towards verb behaviour.
5The third kind of information, subcat frame prediction
is implicit, since verb?noun tuples rely on specific frames.
6Genitive objects can also occur in German verb subcate-
gorization frames, but this is extremely rare and verb-specific
and thus not considered in our model.
V-SUBJ V-OBJAcc V-OBJDat
EP 454,350 332,847 53,711
HGC 712,717 329,830 160,377
Both 1,089,492 607,541 206,764
Table 3: Number of verb-noun types extracted
from Europarl (EP) and newspaper data (HGC).
that modify nouns (noun?noun modification case
prediction). Typically, these NP modifiers are
genitive NPs. To this end, we integrate noun-
nounGen tuples with their respective frequencies.
These preferences for a certain function (i.e. sub-
ject, object or modifier) are passed on to the sys-
tem at the level of nouns and integrated into the
CRF through the derived probabilities.
The tuples and triples are obtained from
dependency-parsed data by extracting all occur-
rences of the respective relations; table 3 gives an
overview of the number of extracted tuple types.
For the subcategorization information, the verb-
noun tuples (verb-subject, verb-objectAcc, verb-
objectDat) are then grouped as follows:
tuple gloss Acc Dat Nom
SchemaN folgenV pattern follow 0 322 19
We compute the probabilities for the verb-noun tu-
ple to occur in the respective functions based on
the relative frequencies. In the case of SchemaN
folgenV , we find that the function of Schema as da-
tive object is predominant (to follow a pattern), but
it can also occur in the subject position (the pat-
tern follows). The fact that two functions are pos-
sible for this noun are reflected in their probabili-
ties. The probabilities are discretized into 5 buck-
ets (Bp=0, B0<p?0.25, B0.25<p?0.5, B0.5<p?0.75,
B0.75<p?1). In contrast, noun modification in
noun-nounGen construction is represented by co-
occurrence frequencies.7
7The frequencies are bucketed to the powers of ten, i.e.
f = 1, 2 ? f ? 10, 11 ? f ? 100 , etc. and also f = 0:
this representation allows for a more fine-grained distinction
in the low-to-mid frequency range, providing a good basis
for the decision of whether a given noun-noun pair is a true
noun-nounGen structure or just a random co-occurrence of
two nouns.
597
Gloss Stem Tag Acc Dat Nom Verb Gen N1 Gold
1 companies Unternehmen<NN> NN 0.00 0.00 1.00 erhalten ? ? Nom
2 should sollten<VVFIN> VVFIN ? ? ? ? ? ? ?
3 financial finanziell<ADJ> ADJ ? ? ? ? ? ? Acc
4 funding Mittel<NN> NN 1.00 0.00 0.00 erhalten ? ? Acc
5 for fu?r APPR<Acc> PRP ? ? ? ? ? ? ?
6 the d<ART> ART ? ? ? ? ? ? Acc
7 introduction Einfu?hrung<NN> NN ? ? ? ? ? ? Acc
8 new neu<ADJ> ADJ ? ? ? ? ? ? Gen
9 technologies Technologie<NN> NN ? ? ? ? 100 Einfu?hrung<NN> Gen
10 obtain erhalten<VVINF> VVINF ? ? ? ? ? ? ?
Table 4: Adding subcategorization information into SMT output. (EN input: companies should obtain
financial funding for the introduction of new technologies). On the right, the correct labels are given.
4.2 Integrating subcategorization knowledge
There are two possibilities to integrate subcat-
egorization information into the case prediction
model: (i) It can be integrated into the data set
using the tree-structure provided by the decoder.
Here, verb-noun tuples are extracted from VP and
S structures, and then the probabilities for the dif-
ferent functions are looked up. Similarly, for two
adjacent NPs, the occurrence frequencies of the
respective two nouns are looked up in the list of
noun-nounGen constructions. (ii) The subcatego-
rization information can be integrated based on
the verb-noun tuples obtained by using tuples ob-
tained from source-side dependencies.
The classification task of the CRF consists in
predicting a sequence of labels: case values for
NPs/PPs or no value otherwise, cf. table 4. The
model has access to the basic features stem and
tag, as well as the new features based on subcat-
egorizaion information (explained below), using
unigrams within a window of up to four positions
to the right and the left of the current position, as
well as bigrams and trigrams for stems and tags
(current item + left and/or right item).
An example for integrating subcategorization
features is given in table 4. The first word Un-
ternehmen (companies) is annotated as subject of
erhalten (obtain) with probability 1, and Mittel
(funding) is annotated as direct object of erhal-
ten with probability 1. The word Technologie
(technology) has been marked as a candidate for
a genitive in a noun-nounGen construction8; the
co-occurrence frequency of the tuple Einfu?hrung-
Technologie (introduction - technology) lies in the
bucket 11. . . 100.
In addition to the probability/frequency of the
respective functions, we also provide the CRF
with bigrams containing the two parts of the tuple,
8There is no annotation on Einfu?hrung as the preposition
fu?r is always in accusative case.
DE stemmed output
warum<PWAV>die<ART>Regierung<NN><Sg><Fem>die<ART>anhaltend<ADJ>milit?risch<ADJ>Aktion<NN><Pl><Fem>angeordnet<VVFIN>
derived features
SUBJ  V:anordnen
OBJ  V:anordnen
SUBJ
OBJ
EN input
whythe governmentorderedtheongoingmilitaryactions
Figure 1: Deriving features from dependency-
parsed English data via the word alignment.
i.e. verb+noun or the two nouns of possible noun-
nounGen constructions. As can be seen in the ex-
ample in table 4, the subject (line 1) and the verb
(line 10) are far apart from each other. By pro-
viding the parts of the tuple as unigrams, bigrams
or trigrams to the CRF, all relevant information
is available: verb, noun and the probabilities for
the potential functions of the noun in the sentence.
In addition to bridging the long distance between
verbs and subcategorized nouns, a very common
problem for German, this type of precise informa-
tion also helps to close the gap between the well-
formed training data and the broken SMT-output
as it replaces to a certain extent the target-language
context information (n-grams of stems or lemmas
within a small window).
4.3 Integrating source-side features
For predicting case in SMT output, information
about an NP?s function in the input sentence is
essential. Syntax-semantic functions can be iso-
morphic (e.g., English subjects and objects may
have the same function in a German translation),
but this is not necessarily the case. Despite this,
an important advantage of integrating source-side
features is that the well-formed source-side text
can be reliably parsed, whereas SMT output is of-
ten disfluent and cannot be reliably parsed.
The English features are obtained from
dependency-parsed data (Choi and Palmer, 2012).
The relevant annotation of the parser is transferred
598
to the SMT output via word alignment. We focus
on English subjects, direct objects and noun-of-
noun structures (often equivalent to noun-nounGen
phrases on the German side): these structures
are generally likely to correspond to each other
within source and target language. In contrast
to the subcategorization-based information, the
difference between well-formed training data and
disfluent SMT output tends to work to our benefit
here: while the parallel sentences of the training
data were manually translated with the objective
to produce good target-language sentences, the
syntactic structures of the source and target
sentences are often diverging. In contrast, the
SMT system often produces more isomorphic
translations, which is helpful for annotating
source-side features on the target language.
Figure 1 shows the process of integrating
source-side features: for each German noun that
is aligned with an English noun labelled as subject
or direct object, this annotation is transferred to the
target-side. Using the English dependency struc-
tures, the verb subcategorizing the respective noun
is identified, and via the alignment, the equivalent
German verb is obtained. Similarly, candidates for
noun-nounGen structures are identified by extract-
ing and aligning English noun-of-noun phrases.
5 Experiments and evaluation
In this section, we present experiments using dif-
ferent feature combinations. We also present a
manual evaluation of our best system which shows
that the new features improve translation quality.
5.1 Data and experimental setup
We use the hierarchical translation system that
comes with the Moses SMT-package and GIZA++
to compute the word alignment, using the ?grow-
diag-final-and? heuristics. The rule table was
computed with the default parameter setting for
GHKM extraction (Galley et al, 2004) in the im-
plementation by Williams and Koehn (2012).
Our training data contains 1,485,059 parallel
sentences9; the German part of the parallel data
is used as the target-side language model. The dev
and test sets (1025/1026 lines) are wmt-2009-a/b.
For predicting the grammatical features, we
used the Wapiti Toolkit (Lavergne et al, 2010).10
9English/German data released for the 2009 ACL Work-
shop on Machine Translation shared task.
10To eliminate irrelevant features, we use L1 regulariza-
We train four CRFs on data prepared as shown
in section 3. The corpora used for the extrac-
tion of subcategorization tuples were Europarl and
German newspaper data (200 million words). We
choose this particular data combination in order to
provide data that matches the training data, as well
as to add new data of the test set?s domain (news).
The German part of Europarl was dependency-
parsed with Bohnet (2010), and subcategorization
information was extracted as described in Scheible
et al (2013); the newspaper data (HGC - Huge
German Corpus) was parsed with Schmid (2000),
and subcategorization information was extracted
as described in Schulte im Walde (2002b).
5.2 Results
We report results of two types of systems (ta-
ble 5): first, a regular translation system built on
surface forms (i.e., normal text) and second, four
inflection prediction systems. The first inflection
prediction system (1) uses a simple case predic-
tion model, whereas the remaining systems are
enriched with (2) subcategorization information
(cf. section 4.2), (3) source-side features (cf. sec-
tion 4.3), and (4) both source-side features and
subcategorization information. In (2) and (4), the
subcategorization information was included using
tuples obtained from source-side dependencies11.
The simple prediction system corresponds to that
presented in section 3; for all inflection predic-
tion systems, the same SMT output and models for
number, gender and strong/weak inflection were
used; thus the only difference with the simple pre-
diction system is the model for case prediction.
We present three types of evaluation: BLEU
scores (Papineni et al, 2001), prediction accuracy
on clean data and a manual evaluation of the best
system in section 5.3.
Table 5 gives results in case-insensitive BLEU.
While the inflection prediction systems (1-4) are
significantly12 better than the surface-form sys-
tem (0), the different versions of the inflection sys-
tems are not distinguishable in terms of BLEU;
however, our manual evaluation shows that the
new features have a positive impact on translation
quality.
tion; the regularization parameter is optimized on held out
data.
11Using tuples extracted from the target-side parse tree
(produced by the decoder) results in a BLEU score of 14.00.
12We used Kevin Gimpel?s implementation of pairwise
bootstrap resampling with 1000 samples.
599
0 1 2 3 4
surface simple subcat. features source-side source-side
system prediction (tuples from EN side) features + subcat. featues
BLEU 13.43 14.02 14.05 14.10 14.17
Clean ? 85.05 % 85.65 % 85.61 % 85.81 %
Table 5: Results of the simple prediction vs. three systems enriched with extra features.
One problem with using BLEU as an evalua-
tion metric is that it is a precision-oriented met-
ric and tends to reward fluency rather than ade-
quacy (see (Wu and Fung, 2009a; Liu and Gildea,
2010)). As we are working on improving ade-
quacy, this will not be fully reflected by BLEU.
Furthermore, not all components of an NP do nec-
essarily change their inflection with a new case
value; it might happen that the only indicator for
the case of an NP is the determiner: er sieht [den
alten Mann]NPacc (he sees the old man) vs. er
folgt [dem alten Mann]NPdat (he follows the old
man). While the case marking of NPs is essential
for comprehensibility, one changed word per noun
phrase is hardly enough to be reflected by BLEU.
An alternative to study the effectiveness of the
case prediction model is to evaluate the prediction
accuracy on parsed clean data, i.e. not on SMT
output. In this case, we measure (using the dev
set) how often the case of an NP is predicted cor-
rectly13. In all cases, the prediction accuracy is
better for the enriched systems. This shows that
the additional features improve the model, but also
that a gain in prediction accuracy on clean data is
not necessarily related to a gain in BLEU. We ob-
served that the more complex the model, the less
robust it is to differences between the test data
and the training data. Related to this problem,
we observed that high-order n-gram POS/lemma-
based features in the simple prediction (sequences
of lemmas and tags) are given too much weight in
training and thus make it difficult for the new fea-
tures to have a larger impact, so we restricted the
n-gram order of this type of feature to trigrams.
5.3 Manual evaluation of the best system
In order to provide a better understanding of the
impact of the presented features, in particular to
see whether there is an improvement in adequacy,
we carried out a manual evaluation comparing sys-
13The numbers in table 5 are artificially high and downplay
the difference as they also include cases which are very easy
to predict, such as nouns in PPs where only one value for case
is possible. We measure how many case labels were correctly
predicted, not correct inflected forms.
enriched simple equal
preferred preferred
person 1 23 11 12
(a) person 2 21 8 17
person 3 26 11 9
person 1 23 5 18
(b) person 2 21 11 14
person 3 29 8 9
(c) agreement 17 2 6
Table 6: Manual evaluation of 46 sentences: with-
out (a) and with (b) access to EN input, and the
annotators? agreement in the second part (c).
tem (4) with the simple prediction system (1).
From the set of different sentences between the
simple prediction system and the enriched system
(144 of 1026), we evaluated those where the En-
glish input sentence was between 8 and 25 words
long (46 sentences in total). We specifically re-
stricted the test set in order to provide sentences
which are less difficult to annotate, as longer sen-
tences are often very disfluent and too hard to rate.
Most of the sentences in the evaluation set differ
only in the realization of one NP. For comparing
the two systems, the sentences were presented in
random order to 3 native speakers of German.
The evaluation consists of two parts: first, the
participants were asked to decide which sentence
is better without being given the English input
(this measures fluency). In the second part, they
should to mark that sentence which better repro-
duces the content of the English input sentence
(this measures adequacy). The test set is the same
for both tasks, the only difference being that the
English input is given in the second part. The re-
sults are given in table 6. Summarizing we can
say that the participants prefer the enriched sys-
tem over the simple system in both parts; there is a
high agreement (17 cases) in decisions over those
sentences which were rated as enriched better.
When looking at the pairwise inter-annotator
agreement for the task of annotating the test-set
with the 3 possible labels enriched preferred, sim-
ple preferred and no preference, we find that the
annotators P1 and P2 have a substantial agreement
600
input hundreds of policemen were on alert , and [a helicopter]Subj circled the area with searchlights .
1 simple Hunderte von Polizisten auf Trab , und [einen Helikopter]Acc eingekreist das Gebiet mit searchlights .
enriched Hunderte von Polizisten auf Trab , und [ein Helikopter]Nom eingekreist das Gebiet mit searchlights .
input while 38 %percent put [their trust]Obj in viktor orba?n .
2 simple wa?hrend 38 % [ihres Vertrauens]Gen schenken in Viktor Orba?n .
enriched wa?hrend 38 % [ihr Vertrauen]Acc schenken in Viktor Orba?n .
input more than $ 100 billion will enter [the monetary markets]Obj by means of public sales .
3 simple mehr als 100 Milliarden Dollar werden durch o?ffentlichen Verkauf [der Geldma?rkte]Gen treten .
enriched mehr als 100 Milliarden Dollar werden durch o?ffentlichen Verkauf [die Geldma?rkte]Acc treten .
Table 7: Output from the simple system (1) and the enriched system (4).
in terms of Kappa (? = 0.6184), whereas the agree-
ment of P3 with P1/P2 respectively leads to lower
scores (? = 0.4467 and ? = 0.3596). However, the
annotators tend to agree well on sentences with
the label enriched preferred, but largely disagree
on sentences labelled as either simple preferred or
no preference. The number of decisions where all
three annotators agree on a label when given the
English input is listed in table 6(c): for example,
only two sentences were given the label baseline is
better by all three annotators. This outcome shows
how difficult it is to rate disfluent SMT output. For
evaluating the case prediction system, the distinc-
tion between enriched preferred and enriched dis-
preferred is the most important question to answer.
Redefining the annotation task to annotating only
two values by grouping the labels simple preferred
and no preference into one annotation possibility
leads to ? = 0.7391, ? = 0.4048 and ? = 0.5652.
5.4 Examples
Table 7 shows some examples for output from the
simple system and the system using source-side
and subcategorization features. In the first sen-
tence, the subject NP a helicopter was inflected
as a direct object in the simple system, but as a
subject in the enriched system, which was pre-
ferred by all three annotators. In the second sen-
tence, the NP their trust, i.e. a direct object of put,
was incorrectly predicted as genitive-modifier of
38 % (i.e. 38 % of their trust) in the simple sys-
tem. The enriched system made use of the prefer-
ence for accusative for the pair Vertrauen schenken
(place trust), correctly inflecting this NP as di-
rect object. Interestingly, only two annotators pre-
ferred the enriched system, whereas one was unde-
cided. The third sentence illustrates how difficult
it is to rate case marking on disfluent SMT output:
there are two possibilities to translate enter the
money market; the direct equivalent of the English
phrase (den GeldmarktAcc betreten), or via the use
of a prepositional phrase (auf den GeldmarktAcc
treten: ?to step into the money market?). The
SMT-output contains a mix of both, i.e. the verb
treten (instead of betreten), but without the prepo-
sition, which cannot lead to a fully correct inflec-
tion. While the inflection of the simple system (a
genitive construction meaning the public sales of
the money market) is definitely wrong, the inflec-
tion obtained in the enriched system is not use-
ful either, due to the structure of the translation14.
This difficulty is also reflected by the annotators,
who gave twice the label no preference and once
the label enriched better.
6 Conclusion
We illustrated the necessity of using external
knowledge sources like subcategorization infor-
mation for modelling case for English to Ger-
man translation. We presented a translation sys-
tem making use of a subcategorization database
together with source-side features. Our method
is language-independent with regard to the source
language; furthermore, no language-specific high-
quality semantic annotation is needed for the tar-
get language, but the data required to model the
subcategorization preferences can be obtained us-
ing standard NLP techniques. We showed in a
manual evaluation that the proposed features have
a positive impact on translation quality.
Acknowledgements
This work was funded by the DFG Research
Project Distributional Approaches to Semantic Re-
latedness (Marion Weller), the DFG Heisenberg
Fellowship SCHU-2580/1-1 (Sabine Schulte im
Walde), as well as by the Deutsche Forschungsge-
meinschaft grant Models of Morphosyntax for Sta-
tistical Machine Translation (Alexander Fraser).
14Furthermore, with treten being polysemous, die
Geldma?rkte treten can also mean to kick the money markets.
601
References
Bernd Bohnet. 2010. Top Accuracy and Fast Depen-
dency Parsing is not a Contradiction. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics (COLING) 2010, pages 89?
97, Beijing, August.
Ted Briscoe and John Carroll. 1997. Automatic Ex-
traction of Subcategorization from Corpora. In Pro-
ceedings of the 5th ACL Conference on Applied Nat-
ural Language Processing, pages 356?363, Wash-
ington, DC.
John Carroll and Alex C. Fang. 2004. The Auto-
matic Acquisition of Verb Subcategorisations and
their Impact on the Performance of an HPSG Parser.
In Proceedings of the 1st International Joint Confer-
ence on Natural Language Processing, pages 107?
114, Sanya City, China.
John Carroll, Guido Minnen, and Ted Briscoe. 1998.
Can Subcategorisation Probabilities Help a Sta-
tistical Parser? In Proceedings of the 6th
ACL/SIGDAT Workshop on Very Large Corpora,
Montreal, Canada.
Jinho D. Choi and Martha Palmer. 2012. Getting the
Most out of Transition-Based Dependency Parsing.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies.
Charles J. Fillmore, Christopher R. Johnson, and
Miriam R.L. Petruck. 2003. Background to
FrameNet. International Journal of Lexicography,
16:235?250.
Alexander Fraser, Marion Weller, Aoife Cahill, and Fa-
bienne Cap. 2012. Modeling Inflection and Word-
Formation in SMT. In Proceedings of the the Euro-
pean Chapter of the Association for Computational
Linguistics (EACL), Avignon, France.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a Translation Rule?
In Proceedings of the Human Language Technology
and North American Association for Computational
Linguistics Conference (HLT-NAACL).
Spence Green and John DeNero. 2012. A Class-
Based Agreement Model for Generating Accurately
Inflected Translations. pages 146?155.
Bryant Huang and Kevin Knight. 2006. Relabel-
ing Syntax Trees to Improve Syntax-Based Machine
Translation Quality. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the ACL.
Minwoo Jeong, Kristina Toutanova, Hisami Suzuki,
and Chris Quirk. 2010. A Discriminative Lexicon
Model for Complex Morphology. In Proceedings of
the Ninth Conference of the Association for Machine
Translation in the Americas (AMTA 2010).
Ahmed El Kholy and Nizar Habash. 2012. Translate,
Predict or Generate: Modeling Rich Morphology in
Statistical Machine Translation. In European Asso-
ciation for Machine Translation.
Karin Kipper Schuler. 2006. VerbNet: A Broad-
Coverage, Comprehensive Verb Lexicon. Ph.D. the-
sis, University of Pennsylvania, Computer and In-
formation Science.
Katrin Kirchhoff, Daniel Capurro, and Anne Turner.
2012. Evaluating User Preferences in Machine
Translation Using Conjoint Analysis. In European
Association for Machine Translation.
Upali S. Kohomban and Wee Sun Lee. 2005. Learning
Semantic Classes for Word Sense Disambiguation.
In Proceedings of the 43rd Annual Meeting on Asso-
ciation for Computational Linguistics, pages 34?41,
Ann Arbor, MI.
Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.
2010. Practical very large scale CRFs. In Proceed-
ings the 48th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 504?513.
Association for Computational Linguistics, July.
Beth Levin. 1993. English Verb Classes and Alterna-
tions. The University of Chicago Press.
Ding Liu and Daniel Gildea. 2008. Improved Tree-
to-String Transducers for Machine Translation. In
ACL Workshop on Statistical Machine Translation.
Ding Liu and Daniel Gildea. 2010. Semantic Role
Features for Machine Translation. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (COLING) 2010.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2007. Unsupervised Acquisition of Pre-
dominant Word Senses. Computational Linguistics,
33(4):553?590.
Ce?dric Messiant. 2008. A Subcategorization Acqui-
sition System for French Verbs. In Proceedings of
the Student Research Workshop at the 46th Annual
Meeting of the Association for Computational Lin-
guistics, pages 55?60, Columbus, OH.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated Re-
source of Semantic Roles. Computational Linguis-
tics, 31(1):71?106.
Kishore A. Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2001. BLEU: a Method for Auto-
matic Evaluation of Machine Translation. Technical
Report RC22176 (W0109-022), IBM Research Di-
vision, Thomas J. Watson Research Center.
Anoop Sarkar and Daniel Zeman. 2000. Automatic
Extraction of Subcategorization Frames for Czech.
In Proceedings of the 18th International Conference
on Computational Linguistics, Saarbru?cken, Ger-
many.
602
Silke Scheible, Sabine Schulte im Walde, Marion
Weller, and Max Kisselew. 2013. A Compact but
Linguistically Detailed Database for German Verb
Subcategorisation relying on Dependency Parses
from a Web Corpus. In Proceedings of the 8th Web
as Corpus Workshop, Lancaster, UK. To appear.
Helmut Schmid, Arne Fitschen, and Ulrich Heid.
2004. SMOR: a German Computational Morphol-
ogy Covering Derivation, Composition, and Inflec-
tion. In Proceedings of the Fourth International
Conference on Language Resources and Evaluation
(LREC).
Helmut Schmid. 2000. LoPar: Design and Imple-
mentation. Arbeitspapiere des Sonderforschungs-
bereichs 340 ?Linguistic Theory and the Foun-
dations of Computational Linguistics? 149, Insti-
tut fu?r Maschinelle Sprachverarbeitung, Universita?t
Stuttgart.
Helmut Schmid. 2004. Efficient Parsing of Highly
Ambiguous Context-Free Grammars with Bit Vec-
tors.
Sabine Schulte im Walde. 2002a. A Subcategorisa-
tion Lexicon for German Verbs induced from a Lex-
icalised PCFG. In Proceedings of the 3rd Confer-
ence on Language Resources and Evaluation, vol-
ume IV, pages 1351?1357, Las Palmas de Gran Ca-
naria, Spain.
Sabine Schulte im Walde. 2002b. A Subcategorisa-
tion Lexicon for German Verbs induced from a Lex-
icalised PCFG. In Proceedings of the 3rd Confer-
ence on Language Resources and Evaluation, vol-
ume IV, pages 1351?1357, Las Palmas de Gran Ca-
naria, Spain.
Sara Stymne and Nicola Cancedda. 2011. Productive
Generation of Compound Words in Statistical Ma-
chine Translation. In Proceedings of the Sixth Work-
shop on Machine Translation.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using Predicate-Argument
Structures for Information Extraction. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 8?15, Sap-
poro, Japan.
Kristina Toutanova, Hisami Suzuki, and Achim Ruopp.
2008. Applying Morphology Generation Models to
Machine Translation. In Proceedings of the 46th An-
nual Meeting of the Association for Computational
Linguistics (ACL): Human Language Technologies.
Giulia Venturi1, Simonetta Montemagni, Simone
Marchi, Yutaka Sasaki, Paul Thompson, John Mc-
Naught, and Sophia Ananiadou. 2009. Bootstrap-
ping a Verb Lexicon for Biomedical Information
Extraction. In Alexander Gelbukh, editor, Linguis-
tics and Intelligent Text Processing, pages 137?148.
Springer, Heidelberg.
Philip Williams and Phillipp Koehn. 2012. GHKM-
Rule Extraction and Scope-3 Parsing in Moses. In
Proceedings of the 7th Workshop on Statistical Ma-
chine Translation, ACL.
Dekai Wu and Pascale Fung. 2009a. Can Semantic
Role Labeling Improve SMT? In Proceedings of the
13th Annual Conference of the European Associa-
tion for Machine Translation (EAMT).
Dekai Wu and Pascale Fung. 2009b. Semantic Roles
for SMT: A Hybrid two-pass Model. In Proceed-
ings of the North American Chapter of the Associa-
tion for Computational Linguistics and Human Lan-
guage Technologies Conference (NAACL-HLT).
603
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 399?405,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Can Markov Models Over Minimal Translation Units Help Phrase-Based
SMT?
Nadir Durrani
University of Edinburgh
dnadir@inf.ed.ac.uk
Hieu Hoang Philipp Koehn
University of Edinburgh
hieu.hoang,pkoehn@inf.ed.ac.uk
Alexander Fraser Helmut Schmid
Ludwig Maximilian University Munich
fraser,schmid@cis.uni-muenchen.de
Abstract
The phrase-based and N-gram-based
SMT frameworks complement each other.
While the former is better able to memo-
rize, the latter provides a more principled
model that captures dependencies across
phrasal boundaries. Some work has been
done to combine insights from these two
frameworks. A recent successful attempt
showed the advantage of using phrase-
based search on top of an N-gram-based
model. We probe this question in the
reverse direction by investigating whether
integrating N-gram-based translation and
reordering models into a phrase-based
decoder helps overcome the problematic
phrasal independence assumption. A large
scale evaluation over 8 language pairs
shows that performance does significantly
improve.
1 Introduction
Phrase-based models (Koehn et al, 2003; Och
and Ney, 2004) learn local dependencies such as
reorderings, idiomatic collocations, deletions and
insertions by memorization. A fundamental draw-
back is that phrases are translated and reordered
independently of each other and contextual infor-
mation outside of phrasal boundaries is ignored.
The monolingual language model somewhat re-
duces this problem. However i) often the language
model cannot overcome the dispreference of the
translation model for nonlocal dependencies, ii)
source-side contextual dependencies are still ig-
nored and iii) generation of lexical translations and
reordering is separated.
The N-gram-based SMT framework addresses
these problems by learning Markov chains over se-
quences of minimal translation units (MTUs) also
known as tuples (Marin?o et al, 2006) or over op-
erations coupling lexical generation and reorder-
ing (Durrani et al, 2011). Because the mod-
els condition the MTU probabilities on the previ-
ous MTUs, they capture non-local dependencies
and both source and target contextual information
across phrasal boundaries.
In this paper we study the effect of integrating
tuple-based N-gram models (TSM) and operation-
based N-gram models (OSM) into the phrase-
based model in Moses, a state-of-the-art phrase-
based system. Rather than using POS-based
rewrite rules (Crego and Marin?o, 2006) to form
a search graph, we use the ability of the phrase-
based system to memorize larger translation units
to replicate the effect of source linearization as
done in the TSM model.
We also show that using phrase-based search
with MTU N-gram translation models helps to ad-
dress some of the search problems that are non-
trivial to handle when decoding with minimal
translation units. An important limitation of the
OSM N-gram model is that it does not handle un-
aligned or discontinuous target MTUs and requires
post-processing of the alignment to remove these.
Using phrases during search enabled us to make
novel changes to the OSM generative story (also
applicable to the TSM model) to handle unaligned
target words and to use target linearization to deal
with discontinuous target MTUs.
We performed an extensive evaluation, carrying
out translation experiments from French, Spanish,
Czech and Russian to English and in the opposite
direction. Our integration of the OSM model into
Moses and our modification of the OSM model to
deal with unaligned and discontinuous target to-
kens consistently improves BLEU scores over the
399
baseline system, and shows statistically significant
improvements in seven out of eight cases.
2 Previous Work
Several researchers have tried to combine the ideas
of phrase-based and N-gram-based SMT. Costa-
jussa` et al (2007) proposed a method for combin-
ing the two approaches by applying sentence level
reranking. Feng et al (2010) added a linearized
source-side language model in a phrase-based sys-
tem. Crego and Yvon (2010) modified the phrase-
based lexical reordering model of Tillman (2004)
for an N-gram-based system. Niehues et al (2011)
integrated a bilingual language model based on
surface word forms and POS tags into a phrase-
based system. Zhang et al (2013) explored multi-
ple decomposition structures for generating MTUs
in the task of lexical selection, and to rerank the
N-best candidate translations in the output of a
phrase-based. A drawback of the TSM model is
the assumption that source and target information
is generated monotonically. The process of re-
ordering is disconnected from lexical generation
which restricts the search to a small set of precom-
puted reorderings. Durrani et al (2011) addressed
this problem by coupling lexical generation and
reordering information into a single generative
process and enriching the N-gram models to learn
lexical reordering triggers. Durrani et al (2013)
showed that using larger phrasal units during de-
coding is superior to MTU-based decoding in an
N-gram-based system. However, they do not use
phrase-based models in their work, relying only
on the OSM model. This paper combines insights
from these recent pieces of work and show that
phrase-based search combined with N-gram-based
and phrase-based models in decoding is the over-
all best way to go. We integrate the two N-gram-
based models, TSM and OSM, into phrase-based
Moses and show that the translation quality is im-
proved by taking both translation and reordering
context into account. Other approaches that ex-
plored such models in syntax-based systems used
MTUs for sentence level reranking (Khalilov and
Fonollosa, 2009), in dependency translation mod-
els (Quirk and Menezes, 2006) and in target lan-
guage syntax systems (Vaswani et al, 2011).
3 Integration of N-gram Models
We now describe our integration of TSM and
OSM N-gram models into the phrase-based sys-
Figure 1: Example (a) Word Alignments (b) Un-
folded MTU Sequence (c) Operation Sequence (d)
Step-wise Generation
tem. Given a bilingual sentence pair (F,E) and
its alignment (A), we first identify minimal trans-
lation units (MTUs) from it. An MTU is defined
as a translation rule that cannot be broken down
any further. The MTUs extracted from Figure 1(a)
are A ? a,B ? b, C . . .H ? c1 and D ? d.
These units are then generated left-to-right in two
different ways, as we will describe next.
3.1 Tuple Sequence Model (TSM)
The TSM translation model assumes that MTUs
are generated monotonically. To achieve this ef-
fect, we enumerate the MTUs in the target left-
to-right order. This process is also called source
linearization or tuple unfolding. The resulting se-
quence of monotonic MTUs is shown in Figure
1(b). We then define a TSM model over this se-
quence (t1, t2, . . . , tJ ) as:
ptsm(F,E,A) =
J?
j=1
p(tj |tj?n+1, ..., tj?1)
where n indicates the amount of context used. A
4-gram Kneser-Ney smoothed language model is
trained with SRILM (Stolcke, 2002).
Search: In previous work, the search graph in
TSM N-gram SMT was not built dynamically
like in the phrase-based system, but instead con-
structed as a preprocessing step using POS-based
rewrite rules (learned when linearizing the source
side). We do not adopt this framework. We use
1We use . . . to denote discontinuous MTUs.
400
phrase-based search which builds up the decoding
graph dynamically and searches through all pos-
sible reorderings within a fixed window. During
decoding we use the phrase-internal alignments to
perform source linearization. For example, if dur-
ing decoding we would like to apply the phrase
pair ?C D H ? d c?, a combination of t3 and t4 in
Figure 1(b), then we extract the MTUs from this
phrase-pair and linearize the source to be in the
order of the target. We then compute the TSM
probability given the n ? 1 previous MTUs (in-
cluding MTUs occurring in the previous source
phrases). The idea is to replicate rewrite rules
with phrase-pairs to linearize the source. Previ-
ous work on N-gram-based models restricted the
length of the rewrite rules to be 7 or less POS tags.
We use phrases of length 6 and less.
3.2 Operation Sequence Model (OSM)
The OSM model represents a bilingual sentence
pair and its alignment through a sequence of oper-
ations that generate the aligned sentence pair. An
operation either generates source and target words
or it performs reordering by inserting gaps and
jumping forward and backward. The MTUs are
generated in the target left-to-right order just as in
the TSM model. However rather than linearizing
the source-side, reordering operations (gaps and
jumps) are used to handle crossing alignments.
During training, each bilingual sentence pair is de-
terministically converted to a unique sequence of
operations.2 The example in Figure 1(a) is con-
verted to the sequence of operations shown in Fig-
ure 1(c). A step-wise generation of MTUs along
with reordering operations is shown in Figure 1(d).
We learn a Markov model over a sequence of oper-
ations (o1, o2, . . . , oJ ) that encapsulate MTUs and
reordering information which is defined as fol-
lows:
posm(F,E,A) =
J?
j=1
p(oj |oj?n+1, ..., oj?1)
A 9-gram Kneser-Ney smoothed language model
is trained with SRILM.3 By coupling reorder-
ing with lexical generation, each (translation or
reordering) decision conditions on n ? 1 previ-
ous (translation and reordering) decisions span-
ning across phrasal boundaries. The reordering
decisions therefore influence lexical selection and
2Please refer to Durrani et al (2011) for a list of opera-
tions and the conversion algorithm.
3We also tried a 5-gram model, the performance de-
creased slightly in some cases.
vice versa. A heterogeneous mixture of translation
and reordering operations enables the OSM model
to memorize reordering patterns and lexicalized
triggers unlike the TSM model where translation
and reordering are modeled separately.
Search: We integrated the generative story of
the OSM model into the hypothesis extension pro-
cess of the phrase-based decoder. Each hypothesis
maintains the position of the source word covered
by the last generated MTU, the right-most source
word generated so far, the number of open gaps
and their relative indexes, etc. This information
is required to generate the operation sequence for
the MTUs in the hypothesized phrase-pair. After
the operation sequence is generated, we compute
its probability given the previous operations. We
define the main OSM feature, and borrow 4 sup-
portive features, the Gap, Open Gap, Gap-width
and Deletion penalties (Durrani et al, 2011).
3.3 Problem: Target Discontinuity and
Unaligned Words
Two issues that we have ignored so far are the han-
dling of MTUs which have discontinuous targets,
and the handling of unaligned target words. Both
TSM and OSM N-gram models generate MTUs
linearly in left-to-right order. This assumption be-
comes problematic in the cases of MTUs that have
target-side discontinuities (See Figure 2(a)). The
MTU A? g . . . a can not be generated because of
the intervening MTUs B ? b, C . . .H ? c and
D ? d. In the original TSM model, such cases are
dealt with by merging all the intervening MTUs
to form a bigger unit t?1 in Figure 2(c). A solu-
tion that uses split-rules is proposed by Crego and
Yvon (2009) but has not been adopted in Ncode
(Crego et al, 2011), the state-of-the-art TSM N-
gram system. Durrani et al (2011) dealt with
this problem by applying a post-processing (PP)
heuristic that modifies the alignments to remove
such cases. When a source word is aligned to a
discontinuous target-cept, first the link to the least
frequent target word is identified, and the group
of links containing this word is retained while the
others are deleted. The alignment in Figure 2(a),
for example, is transformed to that in Figure 2(b).
This allows OSM to extract the intervening MTUs
t2 . . . t5 (Figure 2(c)). Note that this problem does
not exist when dealing with source-side disconti-
nuities: the TSM model linearizes discontinuous
source-side MTUs such as C . . .H ? c. The
401
Figure 2: Example (a) Original Alignments (b)
Post-Processed Alignments (c) Extracted MTUs ?
t?1 . . . t?3 (from (a)) and t1 . . . t7 (from (b))
OSM model deals with such cases through Insert
Gap and Continue Cept operations.
The second problem is the unaligned target-side
MTUs such as ? ? f in Figure 2(a). Inserting
target-side words ?spuriously? during decoding is
a non-trival problem because there is no evidence
of when to hypothesize such words. These cases
are dealt with in N-gram-based SMT by merging
such MTUs to the MTU on the left or right based
on attachment counts (Durrani et al, 2011), lexical
probabilities obtained from IBM Model 1 (Marin?o
et al, 2006), or POS entropy (Gispert and Marin?o,
2006). Notice how ?? f (Figure 2(a)) is merged
with the neighboring MTU E ? e to form a new
MTU E ? ef (Figure 2 (c)). We initially used the
post-editing heuristic (PP) as defined by Durrani et
al. (2011) for both TSM and OSM N-gram mod-
els, but found that it lowers the translation quality
(See Row 2 in Table 2) in some language pairs.
3.4 Solution: Insertion and Linearization
To deal with these problems, we made novel modi-
fications to the generative story of the OSM model.
Rather than merging the unaligned target MTU
such as ? ? f , to its right or left MTU, we gen-
erate it through a new Generate Target Only (f)
operation. Orthogonal to its counterpart Generate
Source Only (I) operation (as used for MTU t7 in
Figure 2 (c)), this operation is generated as soon
as the MTU containing its previous target word
is generated. In Figure 2(a), ? ? f is generated
immediately after MTU E ? e is generated. In
a sequence of unaligned source and target MTUs,
unaligned source MTUs are generated before the
unaligned target MTUs. We do not modify the de-
coder to arbitrarily generate unaligned MTUs but
hypothesize these only when they appear within
an extracted phrase-pair. The constraint provided
by the phrase-based search makes the Generate
Target Only operation tractable. Using phrase-
based search therefore helps addressing some of
the problems that exist in the decoding framework
of N-gram SMT.
The remaining problem is the discontinuous tar-
get MTUs such as A? g . . . a in Figure 2(a). We
handle this with target linearization similar to the
TSM source linearization. We collapse the target
words g and a in the MTU A ? g . . . a to occur
consecutively when generating the operation se-
quence. The conversion algorithm that generates
the operations thinks that g and a occurred adja-
cently. During decoding we use the phrasal align-
ments to linearize such MTUs within a phrasal
unit. This linearization is done only to compute
the OSM feature. Other features in the phrase-
based system (e.g., language model) work with the
target string in its original order. Notice again how
memorizing larger translation units using phrases
helps us reproduce such patterns. This is achieved
in the tuple N-gram model by using POS-based
split and rewrite rules.
4 Evaluation
Corpus: We ran experiments with data made
available for the translation task of the Eighth
Workshop on Statistical Machine Translation. The
sizes of bitext used for the estimation of translation
and monolingual language models are reported in
Table 1. All data is true-cased.
Pair Parallel Monolingual Lang
fr?en ?39 M ?91 M fr
cs?en ?15.6 M ?43.4 M cs
es?en ?15.2 M ?65.7 M es
ru?en ?2 M ?21.7 M ru
?287.3 M en
Table 1: Number of Sentences (in Millions) used
for Training
We follow the approach of Schwenk and Koehn
(2008) and trained domain-specific language mod-
els separately and then linearly interpolated them
using SRILM with weights optimized on the held-
out dev-set. We concatenated the news-test sets
from four years (2008-2011) to obtain a large dev-
setin order to obtain more stable weights (Koehn
and Haddow, 2012). For Russian-English and
English-Russian language pairs, we divided the
tuning-set news-test 2012 into two halves and used
402
No. System fr-en es-en cs-en ru-en en-fr en-es en-cs en-ru
1. Baseline 31.89 35.07 23.88 33.45 29.89 35.03 16.22 23.88
2. 1+pp 31.87 35.09 23.64 33.04 29.70 35.00 16.17 24.05
3. 1+pp+tsm 31.94 35.25 23.85 32.97 29.98 35.06 16.30 23.96
4. 1+pp+osm 32.17 35.50 24.14 33.21 30.35 35.34 16.49 24.22
5. 1+osm* 32.13 35.65 24.23 33.91 30.54 35.49 16.62 24.25
Table 2: Translating into and from English. Bold: Statistically Significant (Koehn, 2004) w.r.t Baseline
the first half for tuning and second for test. We test
our systems on news-test 2012. We tune with the
k-best batch MIRA algorithm (Cherry and Foster,
2012).
Moses Baseline: We trained a Moses system
(Koehn et al, 2007) with the following settings:
maximum sentence length 80, grow-diag-final-
and symmetrization of GIZA++ alignments, an
interpolated Kneser-Ney smoothed 5-gram lan-
guage model with KenLM (Heafield, 2011) used at
runtime, msd-bidirectional-fe lexicalized reorder-
ing, sparse lexical and domain features (Hasler
et al, 2012), distortion limit of 6, 100-best
translation options, minimum bayes-risk decoding
(Kumar and Byrne, 2004), cube-pruning (Huang
and Chiang, 2007) and the no-reordering-over-
punctuation heuristic.
Results: Table 2 shows uncased BLEU scores
(Papineni et al, 2002) on the test set. Row 2 (+pp)
shows that the post-editing of alignments to re-
move unaligned and discontinuous target MTUs
decreases the performance in the case of ru-en, cs-
en and en-fr. Row 3 (+pp+tsm) shows that our in-
tegration of the TSM model slightly improves the
BLEU scores for en-fr, and es-en. Results drop
in ru-en and en-ru. Row 4 (+pp+osm) shows that
the OSM model consistently improves the BLEU
scores over the Baseline systems (Row 1) giving
significant improvements in half the cases. The
only result that is lower than the baseline system
is that of the ru-en experiment, because OSM is
built with PP alignments which particularly hurt
the performance for ru-en. Finally Row 5 (+osm*)
shows that our modifications to the OSM model
(Section 3.4) give the best result ranging from
[0.24?0.65] with statistically significant improve-
ments in seven out of eight cases. It also shows im-
provements over Row 4 (+pp+osm) even in some
cases where the PP heuristic doesn?t hurt. The
largest gains are obtained in the ru-en translation
task (where the PP heuristic inflicted maximum
damage).
5 Conclusion and Future Work
We have addressed the problem of the indepen-
dence assumption in PBSMT by integrating N-
gram-based models inside a phrase-based system
using a log-linear framework. We try to replicate
the effect of rewrite and split rules as used in the
TSM model through phrasal alignments. We pre-
sented a novel extension of the OSM model to
handle unaligned and discontinuous target MTUs
in the OSM model. Phrase-based search helps us
to address these problems that are non-trivial to
handle in the decoding frameworks of the N-gram-
based models. We tested our extentions and modi-
fications by evaluating against a competitive base-
line system over 8 language pairs. Our integra-
tion of TSM shows small improvements in a few
cases. The OSM model which takes both reorder-
ing and lexical context into consideration consis-
tently improves the performance of the baseline
system. Our modification to the OSM model pro-
duces the best results giving significant improve-
ments in most cases. Although our modifications
to the OSM model enables discontinuous MTUs,
we did not fully utilize these during decoding, as
Moses only uses continous phrases. The discon-
tinuous MTUs that span beyond a phrasal length
of 6 words are therefore never hypothesized. We
would like to explore this further by extending the
search to use discontinuous phrases (Galley and
Manning, 2010).
Acknowledgments
We would like to thank the anonymous reviewers
for their helpful feedback and suggestions. The re-
search leading to these results has received fund-
ing from the European Union Seventh Framework
Programme (FP7/2007-2013) under grant agree-
ment n ? 287658. Alexander Fraser was funded by
Deutsche Forschungsgemeinschaft grant Models
of Morphosyntax for Statistical Machine Transla-
tion. Helmut Schmid was supported by Deutsche
Forschungsgemeinschaft grant SFB 732. This
publication only reflects the authors views.
403
References
Colin Cherry and George Foster. 2012. Batch Tun-
ing Strategies for Statistical Machine Translation. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 427?436, Montre?al, Canada, June. Associa-
tion for Computational Linguistics.
Marta R. Costa-jussa`, Josep M. Crego, David Vilar,
Jose? A.R. Fonollosa, Jose? B. Marin?o, and Her-
mann Ney. 2007. Analysis and System Combina-
tion of Phrase- and N-Gram-Based Statistical Ma-
chine Translation Systems. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Compu-
tational Linguistics; Companion Volume, Short Pa-
pers, pages 137?140, Rochester, New York, April.
Josep M. Crego and Jose? B. Marin?o. 2006. Improving
Statistical MT by Coupling Reordering and Decod-
ing. Machine Translation, 20(3):199?215.
Josep M. Crego and Franc?ois Yvon. 2009. Gappy
Translation Units under Left-to-Right SMT Decod-
ing. In Proceedings of the Meeting of the European
Association for Machine Translation (EAMT), pages
66?73, Barcelona, Spain.
Josep M. Crego and Franc?ois Yvon. 2010. Improv-
ing Reordering with Linguistically Informed Bilin-
gual N-Grams. In Coling 2010: Posters, pages 197?
205, Beijing, China, August. Coling 2010 Organiz-
ing Committee.
Josep M. Crego, Franc?ois Yvon, and Jose? B. Marin?o.
2011. Ncode: an Open Source Bilingual N-gram
SMT Toolkit. The Prague Bulletin of Mathematical
Linguistics, 96:49?58.
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A Joint Sequence Translation Model with In-
tegrated Reordering. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
1045?1054, Portland, Oregon, USA, June.
Nadir Durrani, Alexander Fraser, and Helmut Schmid.
2013. Model With Minimal Translation Units, But
Decode With Phrases. In The 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Atlanta, Georgia, USA, June. Association
for Computational Linguistics.
Minwei Feng, Arne Mauser, and Hermann Ney. 2010.
A Source-side Decoding Sequence Model for Statis-
tical Machine Translation. In Conference of the As-
sociation for Machine Translation in the Americas
2010, Denver, Colorado, USA, October.
Michel Galley and Christopher D. Manning. 2010.
Accurate Non-Hierarchical Phrase-Based Transla-
tion. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 966?974, Los Angeles, California, June. As-
sociation for Computational Linguistics.
Adria` Gispert and Jose? B. Marin?o. 2006. Linguis-
tic Tuple Segmentation in N-Gram-Based Statistical
Machine Translation. In INTERSPEECH.
Eva Hasler, Barry Haddow, and Philipp Koehn. 2012.
Sparse Lexicalised Features and Topic Adaptation
for SMT. In Proceedings of the seventh Interna-
tional Workshop on Spoken Language Translation
(IWSLT), pages 268?275.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
pages 187?197, Edinburgh, Scotland, United King-
dom, 7.
Liang Huang and David Chiang. 2007. Forest Rescor-
ing: Faster Decoding with Integrated Language
Models. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 144?151, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Maxim Khalilov and Jose? A. R. Fonollosa. 2009. N-
Gram-Based Statistical Machine Translation Versus
Syntax Augmented Machine Translation: Compar-
ison and System Combination. In Proceedings of
the 12th Conference of the European Chapter of the
ACL (EACL 2009), pages 424?432, Athens, Greece,
March. Association for Computational Linguistics.
Philipp Koehn and Barry Haddow. 2012. Towards Ef-
fective Use of Training Data in Statistical Machine
Translation. In Proceedings of the Seventh Work-
shop on Statistical Machine Translation, pages 317?
321, Montre?al, Canada, June. Association for Com-
putational Linguistics.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proceed-
ings of HLT-NAACL, pages 127?133, Edmonton,
Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In ACL 2007 Demonstrations, Prague, Czech Re-
public.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July.
Shankar Kumar and William J. Byrne. 2004. Mini-
mum Bayes-Risk Decoding for Statistical Machine
Translation. In HLT-NAACL, pages 169?176.
404
Jose? B. Marin?o, Rafael E. Banchs, Josep M. Crego,
Adria` de Gispert, Patrik Lambert, Jose? A. R. Fonol-
losa, and Marta R. Costa-jussa`. 2006. N-gram-
Based Machine Translation. Computational Lin-
guistics, 32(4):527?549.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and
Alex Waibel. 2011. Wider Context by Using Bilin-
gual Language Models in Machine Translation. In
Proceedings of the Sixth Workshop on Statistical
Machine Translation, pages 198?206, Edinburgh,
Scotland, July. Association for Computational Lin-
guistics.
Franz J. Och and Hermann Ney. 2004. The Alignment
Template Approach to Statistical Machine Transla-
tion. Computational Linguistics, 30(1):417?449.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ?02, pages 311?318,
Morristown, NJ, USA.
Christopher Quirk and Arul Menezes. 2006. Do We
Need Phrases? Challenging the Conventional Wis-
dom in Statistical Machine Translation. In HLT-
NAACL.
Holger Schwenk and Philipp Koehn. 2008. Large and
Diverse Language Models for Statistical Machine
Translation. In International Joint Conference on
Natural Language Processing, pages 661?666, Jan-
uary 2008.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Intl. Conf. Spoken Lan-
guage Processing, Denver, Colorado.
Christoph Tillman. 2004. A Unigram Orienta-
tion Model for Statistical Machine Translation. In
HLT-NAACL 2004: Short Papers, pages 101?104,
Boston, Massachusetts.
Ashish Vaswani, Haitao Mi, Liang Huang, and David
Chiang. 2011. Rule Markov Models for Fast Tree-
to-String Translation. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 856?864, Portland, Oregon, USA, June.
Hui Zhang, Kristina Toutanova, Chris Quirk, and Jian-
feng Gao. 2013. Beyond Left-to-Right: Multi-
ple Decomposition Structures for SMT. In The
2013 Conference of the North American Chapter
of the Association for Computational Linguistics:
Human Language Technologies, Atlanta, Georgia,
USA, June. Association for Computational Linguis-
tics.
405
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 224?234,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
How to Avoid Burning Ducks:
Combining Linguistic Analysis and Corpus Statistics
for German Compound Processing
Fabienne Fritzinger and Alexander Fraser
Institute for Natural Language Processing
University of Stuttgart
{fritzife, fraser}@ims.uni-stuttgart.de
Abstract
Compound splitting is an important prob-
lem in many NLP applications which must
be solved in order to address issues of data
sparsity. Previous work has shown that lin-
guistic approaches for German compound
splitting produce a correct splitting more
often, but corpus-driven approaches work
best for phrase-based statistical machine
translation from German to English, a
worrisome contradiction. We address this
situation by combining linguistic analysis
with corpus-driven statistics and obtain-
ing better results in terms of both produc-
ing splittings according to a gold standard
and statistical machine translation perfor-
mance.
1 Introduction
Compounds are highly productive in German and
cause problems of data sparsity in data-driven sys-
tems. Compound splitting is an important com-
ponent of German to English statistical machine
translation systems. The central result of work by
(Koehn and Knight, 2003) is that corpus-driven
approaches to compound splitting perform better
than approaches based on linguistic analysis, and
this result has since been confirmed by other re-
searchers (Popovic? et al, 2006; Stymne, 2008).
This is despite the fact that linguistic analysis per-
forms better in terms of matching a gold standard
splitting. Our work shows that integrating these
two approaches, by employing high-recall lin-
guistic analysis disambiguated using corpus statis-
tics, effectively combines the benefits of both ap-
proaches. This is important due to the wide us-
age of the Koehn and Knight approach in statisti-
cal machine translation systems.
The splittings we produce are best in terms of
both end-to-end machine translation performance
(resulting in an improvement of 0.59 BLEU and
0.84 METEOR over the corpus-driven approach
of Koehn and Knight on the development test set
used for WMT 20091) and two gold standard eval-
uations (see section 4). We provide an exten-
sive analysis of the improvements of our approach
over the corpus-driven approach. The approach
we have developed may help show how to im-
prove previous approaches to handling compounds
in such applications as speech recognition (e.g.,
(Larson et al, 2000)) or information retrieval (e.g.,
(Braschler and Ripplinger, 2004)).
The organization of the paper is as follows. Sec-
tion 2 discusses previous work on compound split-
ting for statistical machine translation. Section 3
presents approaches for compound splitting and
also presents SMOR, the morphological analyzer
that is a key knowledge source for our approach.
Section 4 presents a comparison of compound
splitting techniques using two gold standard cor-
pora and an error analysis. Section 5 presents
phrase-based statistical machine translation (SMT)
results. Section 6 concludes.
2 Related Work on German Compound
Splitting
Rule-based compound splitting for SMT has been
addressed by Nie?en and Ney (2000), where
GERTWOL was used for morphological analysis
and the GERCG parser for lexical analysis and dis-
ambiguation. Their results showed that morpho-
syntactic analysis could reduce the subjective sen-
tence error rate.
The empirical approach of Koehn and Knight
(2003) splits German compounds into words
found in a training corpus. A minimal amount
of linguistic knowledge is included in that the
filler letters ?s? and ?es? are allowed to be intro-
duced between any two words while ?n? might be
1See Table 6 in section 5 for details.
224
dropped. A scoring function based on the aver-
age log frequency of the resulting words is used
to find the best splitting option, see section 3.2 for
details. SMT experiments with additional knowl-
edge sources (parallel corpus, part-of-speech tag-
ger) for compound splitting performed worse than
using only the simple frequency metric. Stymne
(2008) varies the Koehn and Knight approach by
examining the effect of a number of parameters:
e.g. word length, scoring method, filler letters.
Popovic? et al (2006), compared the approach
of Nie?en and Ney (2000) with the corpus-driven
splitting of Koehn and Knight (2003) in terms of
performance on an SMT task. Both systems yield
similar results for a large training corpus, while
the linguistic-based approach is slightly superior
when the amount of training data is drastically re-
duced.
There has recently been a large amount of in-
terest in the use of input lattices in SMT. One use
of lattices is to defer disambiguation of word-level
phenomena such as inflection and compounds to
decoding. Dyer (2009) applied this to German us-
ing a lattice encoding different segmentations of
German words. The work is evaluated by using the
1-best output of a weak segmenter2 on the training
data and then using a lattice of the N-best output
of the same segmenter on the test set to decode,
which was 0.6 BLEU better than the unsegmented
baseline. It would be of interest to test whether de-
ferral of disambiguation to decoding still produces
an improvement when used in combination with a
high-performance segmenter such as the one we
present, an issue we leave for future work.
3 Compound Processing
Previous work has shown a positive impact of
compound splitting on translation quality of SMT
systems. The splitting reduces data sparsity and
enhances word alignment performance. An exam-
ple is given in Figure 1.
Previous approaches for compound splitting
can be characterized as following two basic ap-
proaches: the use of morphological analyzers to
find split points based on linguistic knowledge
and corpus-driven approaches combining large
2The use of the 1-best output of the segmenter for German
to English decoding results in a degradation of 0.3 BLEU,
showing that it is worse in performance than the corpus-
driven method of Koehn and Knight, which improves perfor-
mance (see the evaluation section). However, this segmenter
is interesting because it is language neutral.
Inflationsraten
English translation
unsplit compound
inflation rates
Inflation Ratensplit compound
1?to?n alignment
1?to?1 alignment
Figure 1: Compound splitting enhances the num-
ber of 1-to-1 word alignments.
amounts of data and scoring metrics.
We briefly introduce the computational mor-
phology SMOR (section 3.1) and the corpus-
driven approach of Koehn and Knight (2003) (sec-
tion 3.2), before we present our hybrid approach
that combines the benefits of both in section 3.3.
3.1 SMOR Morphological Analyzer
SMOR is a finite-state based morphological ana-
lyzer covering the productive word formation pro-
cesses of German, namely inflection, derivation
and compounding (Schmid et al, 2004). Word for-
mation is implemented as a concatenation of mor-
phemes filtered according to selectional restric-
tions. These restrictions are based on feature deco-
rations of stems and affixes encoded in the lexicon.
Inflection is realized using inflection classes.
An abbreviated3 SMOR analysis of the word
Durchschnittsauto (?standard car?)4 is given in
Figure 2 (a). The hierarchical structure of the word
formation process is given in Figure 2 (b). Imple-
mented with finite-state technology, SMOR is not
able to produce this hierarchy: in our example it
outputs two (correct) analyses of different depths
and does not perform disambiguation.
3.2 Corpus-Driven Approach
Koehn and Knight (2003) describe a method re-
quiring no linguistically motivated morphological
analysis to split compounds. Instead, a compound
is broken into parts (words) that are found in a
large German monolingual training corpus.
We re-implemented this approach with an ex-
tended list of filler letters that are allowed to oc-
3We show analyses for nominative, and analyses for the
other cases genitive, ,dative, accusative are left out as they
are identical.
4durch = ?through?, schneiden = ?to cut?, Schnitt = ?(the)
cut?, Durchschnitt = ?average?, Auto = ?car?
part-of-speech: <NN>/<V> (noun/verb)
gender: <Neu> (neutrum)
case: <Nom> (nominative)
number: <Sg> (singular)
suffixation: <SUFF> (suffix)
prefixation: <VPART> (verb particle)
225
analyze> Durchschnittsauto
Durchschnitt<NN>Auto<+NN><Neut><Nom><Sg>
durch<VPART>schneiden<V><NN><SUFF>Auto<+NN><Neut><Nom><Sg>
(a) SMOR output format
Durchschnittsauto
Durchschnitt
schneiden
Auto
durch
<NN>
<NN><NN>durchschneiden<V>
<V><VPART>
(b) Morphological analysis
Figure 2: Morphological analysis of Durchschnittsauto (?standard car?).
cur between any two parts (nen, ien, en, es, er, s,
n) such as s in Inflationsrate (cf. Figure 1) and
deletable letters (e, n), required for compounds
such as Kirchturm = Kirche+Turm (?steeple?,
?church+tower?). Filler letters are dropped only
in cases where the part is more frequent without
the letter than with it (an example is that the fre-
quency of the word Inflation is greater than the
frequency of the word Inflations); the same holds
for deletable letters and hyphens (?-?). The min-
imal part size was set to 3 characters. Word fre-
quencies are derived from the true-cased corpus
using case insensitive matching. In order to reduce
wrong splittings, infrequent words (frequency ?
3) are removed from the training corpus and a stop
list was used5. These are similar choices to those
found to be best in work by Stymne (2008).
The splitting that maximizes the geometric
mean of part frequencies using the following for-
mula6 is chosen:
argmaxS(
?
pi?S
count(pi))
1
n
Figure 3 contains all splitting options of
the corpus-driven approach for Ministerpra?sident
(?prime minister?). As can be seen, the
desired splitting Minister|Pra?sident is among
the options, but in the end Min|ist|Pra?sident
(?Min|is|president?) is picked by the corpus-
driven approach because this splitting maximizes
the geometric mean score (mainly due to the
highly frequent verb ist ?is?). This is linguisti-
cally implausible, and the system we introduce in
the next section splits this correctly.
Even though this corpus-driven approach tends
to oversplit it works well for phrase-based SMT
because adjacent words (or word parts) are likely
5The stop list contains the following units, which occur in
the corpus as separate words (e.g., as names, function words,
etc.), and frequently occur in incorrect splittings: adr, and,
bes, che, chen, den, der, des, eng, ein, fue, ige, igen, iger,
kund, sen, ses, tel, ten, trips, ung, ver.
6Taken from (Koehn and Knight, 2003):
S = split, pi = part, n = number of parts. The original word
is also considered, it has 1 part and a minimal count of 1.
Ministerpr?
Ministerpr?sid
Minister
Mini
Minis
Min isterist
ter
Pr?
Pr?sid
Pr?side
Pr?sident
ent
sid
sident
Figure 3: Corpus-driven splittings of Minis-
terpra?sident (?prime minister?).
to be learned as phrases. We will refer to the
corpus-driven approach using the abbreviation cd.
3.3 Hybrid Approach
We present a novel approach to compound split-
ting: based on linguistically motivated split points
gained from SMOR, we search word frequencies
in a large training corpus (the same corpus as we
will use for the corpus-driven approach) in order
to determine the best splitting option for a word
(or to leave it unsplit). This approach needs no ex-
plicit definition of filler letters or deletable letters,
as this knowledge is encoded in SMOR.
In contrast to the corpus-driven approach de-
scribed in the previous section, the hybrid ap-
proach uses neither a minimal part size constraint,
nor a stop-list. Instead, we make use of the linguis-
tic knowledge encoded in SMOR, i.e. we allow the
hybrid approach to split only into parts that can
appear as free morphemes, such as stems and sep-
aratable particles. An example is auf|gibt (?to give
up?), where the particle auf may occur separated
from the verb, as in Er gibt nicht auf (?he gives
not up?). Bound morphemes, such as prefixes and
suffixes cannot be split from the stem, e.g. verhan-
delbar (?negotiable?) which consists of the prefix
ver-, the stem handeln and the suffix -bar, is left
unsplit by the hybrid approach.
For N-ary compounds (with N>2), we use not
only the split points proposed by SMOR, but we
also search the training corpus for recombinations
of the compound parts: e.g. SMOR provides the
parts A|B|C for the compound ABC, and we addi-
226
(a) SMOR splitting options
Ministerpr?sident
w?hlen Kampf
Wahlkampf
Wahl Kampf
Minister Pr?sident
(b) Part frequencies
word part frequency
Kampf 30,546
Minister 12,742
Ministerpra?sident 22,244
Ministerpra?sidentwahl 111
Ministerpra?sidentwahlkampf 1
Pra?sident 125,747
Pra?sidentenwahl 2,482
Pra?sidentenwahlkampf 25
Wahl 29,255
Wahlkampf 23,335
(c) Log-based geometric mean scores
splitting option score
Ministerpra?sidentenwahlkampf 0
Ministerpra?sident|Wahlkampf 10.04
Ministerpra?sident|Wahl|Kampf 10.21
Ministerpra?sident|wa?hlen|Kampf 9.85
Minister|Pra?sident|Wahlkampf 10.38
Minister|Pra?sident|Wahl|Kampf 10.42
Minister|Pra?sident|wa?hlen|Kampf 10.15
Ministerpra?sidentenwahl|Kampf 7.52
Minister|Pra?sidentenwahl|Kampf 9.19
Minister|Pra?sidentenwahlkampf 6.34
Table 1: Splitting options for Ministerpra?sidentenwahlkampf (?election campaign of the prime minis-
ter?) (a) with part frequencies derived from the corpus (b) and log-based geometric mean scores (c).
tionally search for AB|C and A|BC.
Even though SMOR lemmatizes along with
compound splitting, only the information about
possible split points is used in our splitting ap-
proach. The compound Beitrittsla?nder (?ac-
cession countries?), for example, is reduced to
Beitritt|Land by SMOR, but is retransformed to
Beitritt|La?nder in our approach. This holds also
for adjectives, e.g. firmeninterne ?company-
internal? which is split to firma|interne (interne is
the female form of the adjective intern) and verbs,
such as the participle wasser|gebunden ?water
bound?, where the lemma is Wasser|binden.
Hyphenated words can also be split with SMOR,
as long as the rightmost part of the word is in its
lexicon. However, the word parts which are to the
left of hyphen(s) are left unanalyzed. The SMOR
analyses for NATO-Berichts (?NATO report?) and
the nonsense XYZabc-Berichts (?XYZabc report?)
are given below:
analyze> NATO-Berichts
NATO-<TRUNC>Bericht<+NN><Masc><Gen><Sg>
analyze> XYZabc-Berichts
XYZabc-<TRUNC>Bericht<+NN><Masc><Gen><Sg>
Such Words where the rightmost part is unknown
to SMOR are left completely unanalyzed by
SMOR. Examples include NATO-Berxchts (which
is a typo of NATO-Berichts) or al-Qaeda (a proper
name). If such words occurred less than 5 times in
the training corpus, they were split at the hyphens.
This procedure splits NATO|Berxchts, while it
leaves al-Qaeda unsplit.
Table 1(a) shows the different splittings7 that
SMOR returns for the ambiguous ad-hoc com-
pound Ministerpra?sidentenwahlkampf (?election
campaign of the prime minister?). All of them are
morphologically sound compounds of German.
The corpus frequencies of the parts provided by
SMOR (and their recombinations) are given in Ta-
ble 1 (b). The average natural log frequencies of
the SMOR splittings in Table 1 (c), with the recom-
binations of their parts in the last three rows. We
set the minimal frequency for each part to 1 (which
gives a log frequency of 0) even if it was not seen
in the training corpus.
Even though ?prime? is not a literal transla-
tion of Pra?sident, the best splitting (out of the
given options) is Minister|Pra?sident|Wahl|Kampf
(?minister|president|election|campaign?). It is
scored highest and thus chosen by the hybrid ap-
proach.
For the purpose of SMT, we want to split com-
pounds into parts that have a translational cor-
respondent in the target language. To accom-
plish that, it is often sufficient to consider the
split at the highest linguistic analysis level. For
7Ministerpra?sident = ?prime minister?, Wahlkampf =
?election campaign?, Minister = ?minister?, Pra?sident =
?president?, Wahl = ?election?, wa?hlen = ?to elect?, Kampf
= ?fight?
227
the example Durchschnittsauto (?standard car?)
(cf. Figure 2 above), where the ideal split
is Durchschnitt|Auto (?average|car?). Here, the
deeper analysis of Durchschnitt as a nominalisa-
tion of the particle verb durch|schneiden (?to cut
through?) is not relevant. The same holds for Min-
isterpra?sidentenwahlkampf of Table 1, where in
one of the splittings Wahl is further reduced to the
verb wa?hlen.
In order to prevent such analyses from be-
ing picked, we investigate the use of restricting
SMOR?s splitting options to analyses having a min-
imal number of component parts. On the other
hand, there are many lexicalized compounds in
German, that, besides being analyzed as a com-
pound also appear as a free word stem in SMOR?s
lexicon (e.g. both Gela?ndewagen ?all-terrain vehi-
cle? and Gela?nde|wagen ?terrain vehicle? are re-
turned by SMOR). Therefore, we keep both vari-
ants for our subsequent experiments: the con-
strained version that uses only analyses with a
minimal number of parts (and thus performs a
more conservative splitting) is referred to as smc,
while using all of SMOR?s analyses is named sm.
In addition to these, we use a constraint that splits
only nouns. To do so, the text to be split was POS-
tagged with TreeTagger (Schmid, 1994) to deter-
mine the nouns in the context of the whole sen-
tence. Splitting only nouns will be referred to as
@nn in the remainder of this paper.
Compared to the purely corpus-driven ap-
proach, hybrid compound splitting substantially
reduces the number of false splitting options, be-
cause only splittings that are linguistically moti-
vated are looked up in the training corpus. We
will show that this restriction of splitting options
enhances the number of correct splittings being
picked. The purely corpus-driven approach con-
siders the correct splitting in most cases, but often
does not choose it because there is another higher
scoring splitting option (cf. section 4.3).
The main shortcoming of the hybrid approach
is its dependence on SMOR?s lexical coverage.
SMOR incorporates numerous word formation
rules and thousands of word stems (e.g. over
16,000 noun base stems), but our approach will
leave all words unsplit that cannot be analyzed
with SMOR. However, we will show in both the
gold standard evaluations (section 4) and the SMT
evaluation (section 5) that the recall of SMOR is
sufficient to result in substantial gains over the
corpus-driven approach.
4 Gold Standard Evaluation
The accuracies of the compound splitting ap-
proaches are evaluated against two hand-crafted
gold standards: one that includes linguistically
motivated split points (section 4.1), and one indi-
cating compounds that were translated composi-
tionally by a human translator (section 4.2). We
found that the hybrid approach performs best for
both. In section 5, we will show the impact of the
different splitting approaches on translation per-
formance, with the result that the hybrid approach
outperforms the corpus-driven approach even for
translation quality (in contrast to previous work,
where the best system according to the gold stan-
dard was not the best system for translation qual-
ity). In order to better understand the divergent
results of the splitting approaches, we perform a
detailed error analysis in section 4.3.
The accuracy of compound splitting is mea-
sured using the same terminology and metrics as
described in (Koehn and Knight, 2003):
correct split: should be split and was split correctly
correct not: should not be split and was not
wrong split: should not be split but was split
wrong not: should be split but was not
wrong faulty (fty): should be split, but was split wrongly
precision: correctsplitcorrectsplit+wrongfaulty+wrongsplit
recall: correctsplitcorrectsplit+wrongfaulty+wrongnot
accuracy: correctcorrect+wrong
The results of the following splitting approaches
were investigated:
raw = baseline without splitting
cd = corpus-driven splitting
sm = hybrid approach using all SMOR analyses
smc = hybrid approach using the SMOR analysis
with the minimal number of parts
@nn = split only nouns
The word frequencies required for all splitting ap-
proaches were derived from the German monolin-
gual language model training data (? 225 million
tokens) of the shared task of the 2009 ACL work-
shop on machine translation.
4.1 Linguistically Motivated Gold Standard
In the course of developing the hybrid approach,
we used a hand-crafted gold standard for testing,
which contains 6,187 distinct word types extracted
228
Correct Wrong Metrics
split not split not fty prec. recall acc.
raw 0 5073 0 1114 0 - 0.00% 81.99%
cd 679 4192 883 120 313 36.21% 61.06% 78.73%
sm 912 4534 541 35 165 56.37% 82.01% 88.02%
sm@nn 628 4845 230 337 147 62.49% 56.73% 88.46%
smc 884 4826 249 135 93 72.10% 79.50% 92.29%
smc@nn 648 4981 94 380 84 78.45% 58.27% 90.98%
Table 2: Linguistically motivated gold standard:
6,187 distinct word types. Bold-face font indi-
cates the best result of each column.
from the development set of the 2009 shared MT
task. The most plausible split points were anno-
tated by a native speaker of German, allowing for
splits into word stems or particles, but not into
bound morphemes such as prefixes or suffixes.
Splits were annotated at the highest word
formation level only, see also Durchschnittsauto
in Figure 2 (section 3.1 above), where only the
split point Durchschnitt|Auto would be annotated
in the gold standard. Another example is the
complex derivative Untersuchungsha?ftling
(?person being imprisoned on remand?),
where the inherent word structure looks as
follows: [Untersuchung+Haft]+ling (?[investi-
gation+imprisonment]+being a person?). The
splitting into Untersuchung|Ha?ftling is semanti-
cally not correct and the word is thus left unsplit
in the gold standard. Finally, particles are only
split if these can be used separately from the verb
in a grammatically sound sentence, as is the case
in the example mentioned in section 3.3, auf|gibt:
Er gibt nicht auf (?he gives not up?). In contrast,
the particle cannot be separated in a past participle
construction like aufgegeben: *Er gegeben nicht
auf (?he given not up?), because in this example,
-ge- is an infix introduced between the particle
and the verb in order to form the past participle
form. Constructions of this kind are thus left
unsplit in the gold standard.
We found that 1,114 of the 6,187 types we in-
vestigated were compounds, of which 837 were
nouns. The detailed results are given in Table 2.
Due to the fact that the majority of words should
not be split, the raw method reaches a considerable
accuracy of 81.99%.
As can be seen from Table 2, 679 of the 1,114
compounds are split correctly by the corpus-driven
approach (cd). However, the high number of
wrong splits (883), which is the main shortcoming
of the corpus-driven approach, leads to an accu-
racy below the raw system (78.73% vs. 81.99%).
Out of the variants of the hybrid approach,
the less constrained one, sm achieves the high-
est recall (82.01%), while the most constrained
one smc@nn has the highest precision (78.45%).
The smc variant yields the most accurate splitting
92.29%. The higher precision of the @nn-variants
comes from the fact that most of the compounds
are nouns (837 of 1,114) and that these approaches
(sm@nn, smc@nn) leave more words incorrectly
unsplit than oversplit.
Note that the gold standard we presented in this
section was measured on a few times during devel-
opment of the hybrid approach and there might be
some danger of overfitting. Therefore, we used an-
other gold standard based on human translations to
confirm the high accuracy of the hybrid approach.
We introduce it in the next section.
4.2 One-to-one Correspondence
Gold Standard
The one-to-one correspondence gold standard
(Koehn and Knight, 2003) indicates only com-
pounds that were translated compositionally by a
human translator. Such translations need not al-
ways be consistent: the human translator might
decide to translate a compound compositionally in
one sentence and using a different concept in an-
other sentence. As a consequence, a linguistically
correct split might or might not be considered cor-
rect, depending on how it was translated. This is
therefore a harsh metric.
We used data from the 2009 shared MT task8
for this evaluation. The first 5,000 words of the
test text (news-dev2009b) were annotated manu-
ally with respect to compounds that are translated
compositionally into more than one English word.
This is the same data set as used for the evalu-
ation of SMT performance in section 5, but the
compound annotation was done only after all SMT
experiments were completed, to ensure unbiased
translation results. The use of the same data set fa-
cilitates the comparison of the splitting approaches
in terms of the one-to-one gold standard vs. trans-
lation quality.
The results are given in Table 3. In this set, only
155 compounds with one-to-one correspondences
are found amongst the 5,000 word tokens, which
leads to a very high accuracy of 96.90% with no
splitting (raw).
8http://www.statmt.org/wmt09/
translation-task.html
229
Correct Wrong Metrics
split not split not fty prec. recall acc.
raw 0 4,845 0 155 0 ?? 0.00% 96.90%
cd 81 4,435 404 14 59 14,89% 52.60% 90.32%
sm 112 4,563 283 8 34 26.11% 72.73% 93.50%
sm@nn 107 4,677 169 15 32 34.74% 69.48% 95.68%
smc 128 4,666 180 12 14 39,75% 83,12% 95,88%
smc@nn 123 4,744 102 18 13 51.68% 79.87% 97.34%
Table 3: Evaluation of splitting approaches with
respect to one-to-one correspondences. Bold-face
font indicates the best result of each column.
The corpus-driven approach (cd) splits 81 of the
155 compounds correctly (52.60% recall), but also
splits 404 words that should have been left unsplit,
which leads to a low precision of only 14.89%.
As can be seen from Table 3, all variants of the
hybrid splitting approach, reach higher accuracies
than the corpus-driven approach, and again, the
most restrictive one (smc@nn) performs best: it is
the only one that achieves a slightly higher accu-
racy than raw (97.34% vs. 96.90%). Even though
the number of correct splits of smc@nn (123) is
lower than for e.g. smc (with 128, the highest re-
call 83.12%), the number of correct not splittings
is higher (4,744 vs. 4,666).
Generally speaking, the results of both gold
standards show that linguistic knowledge en-
hances the number of correct splits, while at the
same time it considerably reduces oversplitting,
which is the main shortcoming of the corpus-
driven approach. A detailed error analysis is pro-
vided in the following section 4.3.
4.3 Error Analysis
4.3.1 Errors of the Corpus-Driven Approach
In gold standard evaluation, the purely corpus-
driven approach exhibited a number of erroneous
splits. These splits are not linguistically motivated
and are thus filtered out a priori by the SMOR-
based systems. In the following, we give some
examples for wrong splits that are typical for the
corpus-driven approach.
In Table 4 we divide typical errors into two cat-
egories: frequency-based where wrong splitting is
solely due to higher frequencies of the parts from
the wrong splitting and insertions/deletions where
filler letters or deletions of letters lead to wrong
splittings of which the parts are again more fre-
quent than for the correct splitting.
The adjective lebenstreuen (?true-to-life?) is the
only true compound of Table 4. Its correct split
is Leben|treuen (?life|true?). All other words in
Table 4 should be left unsplit.
error type word splitting
frequency based
lebenstreuen Leben|streuen
true-to-life life|spread
traumatisch Trauma|Tisch
traumatic trauma|table
Themen the|men
themes the|men
insertions/deletions
entbrannte Ente|brannte
broke out duck|burned
Belangen Bela|Gen
aspect Bela|gene
Toynbeesche toy|been|sche
Toynbeean toy|been|*sche
Table 4: Typical errors of the corpus-driven ap-
proach. The only true compound in this table is
Leben|treuen (?life|true?).
The lookup of word frequencies is done case-
insensitively, i.e. the casing variant with the
highest frequency is chosen. This leads to
cases like traumatisch (?traumatic?), where adjec-
tives are split into nominal head words (namely
Trauma|Tisch = ?trauma|table?), which is impos-
sible from a linguistic point of view. If, how-
ever, Traumatisch occurs uppercased and is thus
to be interpreted as a noun, the splitting into
Trauma|Tisch is correct.
The splitting accuracy of the corpus-driven
method is highly dependent on the quality of the
monolingual training corpus from which word
frequencies are derived. The examples Themen
(?themes?) and Toynbeesche (?Toynbeean?) in Ta-
ble 4 show how foreign language material from a
language like English in the training corpus can
lead to severe splitting errors.
In order to account for the lack of linguistic
knowledge, the corpus-driven approach has to al-
low for a high flexibility of filler letters, dele-
tion of letters and combinations of both. The ex-
amples in the lower part of Table 4 show that
this flexibility often leads to erroneous splits that
completely modify the semantic content of the
original word. For example, the verb partici-
ple form of ?to break out?, entbrannte is split
into Ente|brannte (?duck|burned?), because the
corpus-driven approach allows to add an ?e? at the
end of each but the rightmost part. This transfor-
mation is required to cover compounds like Kirch-
turm (?church tower? (or also ?steeple?)) that are
composed of the words Kirche (?church?) and
Turm (?tower?).
Often, one high frequent part of the (possible)
230
compound determines the split of a word, even
though the other part(s) are much less frequent.
This is the case for Belangen (442 occurrences),
where the high frequent Gen (?gene?, 1,397 oc-
currences) leads to a splitting of the word, even
though the proper name Bela is much less frequent
(165 occurrences).
The case of Toynbeesche (which is a proper
noun used as an adjective) shows that the corpus-
driven approach splits everything into parts, as
long as they are more frequent than the unsplit
word. In contrast, all words that are unknown to
SMOR are left unsplit by the hybrid approach.
Finally, the corpus-driven approach often iden-
tifies content-free syllables such as -sche (see last
row of Table 4) as compound parts. These sylla-
bles frequently occur in the training corpus due to
syllabification, making them a prevalent source for
corpus-driven splitting errors. Such wrong split-
tings could be blocked by extending the stopword
list of the corpus-driven approach. See footnote 5
in section 3.2, for the list of stopwords we used in
our implementation.
Previous approaches to corpus-driven com-
pound splitting used a part-of-speech (POS) tagger
to reduce the number of erroneous analyses (e.g.
(Koehn and Knight, 2003), (Stymne, 2008)): the
word class of the rightmost (possible) part of the
compound is restricted to match the word class of
the whole compound, which is coherent to Ger-
man compositional morphology. This constraint
lead to higher accuracies in gold standard evalu-
ations, but it did not improve translation quality
in the experiments of Koehn and Knight (2003)
and Stymne (2008), and therefore, we did not re-
implement the corpus-driven approach with this
POS-constraint. However, some of the errors pre-
sented in this section could have been prevented if
the POS-constraint was used: the erroneous splits
of lebenstreuen and traumatisch were avoided, but
for the splittings of Belangen and entbrannte, the
POS-constraint would not help. A more restrictive
POS-constraint proposed by Stymne (2008), al-
lows splitting only into parts belonging to content-
bearing word classes. This works for Belangen,
but not for entbrannte. In the case of Themen and
Toynbeesche, the output of a POS-tagger for the
last part are not trustworthy, as these are not cor-
rect German words: men belongs to foreign lan-
guage material or it is a content-free syllable, such
as sche.
4.3.2 Errors of the Hybrid Approach
During the development of the hybrid splitting
approach, we did an extensive gold standard eval-
uation along the way, as described in section 4.1
above. The performance of the hybrid approach
is limited by the performance of its constituents,
namely the coverage of SMOR and the quality
of the corpus from which part frequencies are
derived. In the gold standard evaluation, we
distinguished three error categories: wrong split
(should not be split but was), wrong not (should
be split but was not) and wrong faulty (should
be split, and was split, but wrongly). Table 2 (cf.
Section 4.1) contains the results of the gold stan-
dard we used as development set for our approach.
In Table 5, we give a detailed distribution of the
wrong splittings of the less constrained hybrid
approach sm, into the following categories:
frequency-based: SMOR found the correct split, but
a wrong split was scored higher
unknown to SMOR: lexeme or rule missing in SMOR
lexicalized in SMOR: lexeme exists in SMOR, but fully
lexicalized (no splitting possible)
It can be seen from Table 5 that most of the errors
are due to corpus frequencies of the component
parts. An example is Nachteil (?disadvantage?),
which is lexicalized in German, but can also be
correctly divided (even though it is semantically
less plausible) into nach|Teil (?after|part?), and as
both of these parts are high frequent, Nachteil is
split.
As the corpus-driven approach uses the same
disambiguation component, there must be an over-
lap of the frequency-based errors of the two ap-
proaches.
error type
Wrong
split not faulty
frequency-based 538 26 155
unknown to SMOR 3 7 0
lexicalized in SMOR 0 2 10
total number of errors 541 35 165
Table 5: Error analysis of sm with respect to the
gold standard in Table 2 above.
The remaining two categories contain errors
that are attributed to wrong or missing analyses
in SMOR. Compared to the total number of er-
rors, there are very few such errors. Most of the
unknown words are proper names or compounds
with proper names, such as Petrischale (?petri
dish?). Here, the corpus-driven approach is able
231
to correctly the compound into Petri|Schale.
There are a number of compounds in German
that originally consisted of two words, but are
now lexicalized. For some of them SMOR does
not provide any splitting option. An example is
Sackgasse (?dead end street?) which contains the
words Sack (?sack?) and Gasse (?narrow street?),
where SMOR leaves the word unsplit (but not un-
analyzed: it is encoded as one lexeme), while the
corpus-driven approach correctly splits it.
5 Translation Performance
5.1 System Description
The Moses toolkit (Koehn et al, 2007) was used
to construct a baseline PBSMT system (with de-
fault parameters), following the instructions of the
shared task9. The baseline system is Moses built
exactly as described for the shared task baseline.
Contrastive systems are also built identically, ex-
cept for the use of preprocessing on the German
training, tuning and testing data; this ensures that
all measured effects on translation quality are at-
tributable to the preprocessing. We used data from
the EACL 2009 workshop on statistical machine
translation10. The data include ?1.2 million par-
allel sentences for training (EUROPARL and news),
1,025 sentences for tuning and 1,026 sentences
for testing. All data was lowercased and tok-
enized, using the shared task tokenizer. We used
the English side of the parallel data for the lan-
guage model. As specified in the instructions, sen-
tences longer than 40 words were removed from
the bilingual training corpus, but not from the lan-
guage model corpus. The monolingual language
model training data (containing roughly 227 mil-
lion words11) was used to derive corpus frequen-
cies for the splitting approaches.
For tuning of feature weights we ran Mini-
mum Error Rate Training (Och, 2003) until con-
vergence, individually for each system (optimiz-
ing BLEU). The experiments were evaluated using
BLEU (Papineni et al, 2002) and METEOR (Lavie
and Agarwal, 2007)12. Tuning scores are calcu-
lated on lowercased, tokenized text; all test scores
are case sensitive and performed on automatically
9
http://www.statmt.org/wmt09/baseline.html
10
http://www.statmt.org/wmt09/translation-task.
html
11
http://www.statmt.org/wmt09/
training-monolingual.tar
12The version of METEOR used is 0.7, we use ?exact
porter-stem wn-synonmy?, weights are ?0.8 0.83 0.28?.
system
tuning test test
BLEU BLEU METEOR
raw 18.10 15.72 47.65
cd 18.52 16.17 49.29
sm 19.47 16.59 49.98
sm@nn 19.42 16.76 49.77
smc 19.53 16.63 50.13
smc@nn 19.61 16.40 49.64
Table 6: Effects of compound splitting:
raw = without preprocessing, cd = corpus-driven,
sm = hybrid approach using all SMOR analyses,
smc = hybrid approach with minimal SMOR splits
*@nn = split only nouns.
bold-face = significant wrt. raw
underlined = significant wrt. cd
recapitalized, detokenized text.
5.2 Translation Results
The BLEU and METEOR scores of our experi-
ments are summarized in Table 6. Results that
are significantly better than the baseline are bold-
faced13. Underlining indicates that a result is sig-
nificantly better than corpus-driven.
Compared to not-splitting (raw), the corpus-
driven approach (cd) gains 0.45 BLEU points and
+1.64 in METEOR for testing. All variants of the
hybrid approach (sm*) score higher than cd, reach-
ing up to +0.59 BLEU compared to cd and +1.04
BLEU compared to raw for sm@nn. In terms of
METEOR, gains of up to +0.84 compared to cd and
+2.48 compared to raw are observable for smc, all
of them being significant with respect to both, raw
and cd. The smc variant of the hybrid approach
yielded the highest METEOR score and it was also
found to be the most accurate one when evaluated
against the linguistic gold standard in section 4.1.
The restriction to split only nouns (@nn) leads
to a slightly improved performance of sm (+0.17)
BLEU, while METEOR is slightly worse when the
@nn constraint is used: -0.21. Despite the fact that
it had a high precision in the gold standard evalu-
ation of section 4.1 above, smc, when used with
the @nn constraint, decreases in performance ver-
sus smc without the constraint, because the @nn
variant leaves many compounds unsplit (cf. row
?Wrong not?, Table 2), Secion 4.1).
13We used pair-wise bootstrap resampling using sample
size 1,000 and p-value 0.05, code obtained from http:
//www.ark.cs.cmu.edu/MT
232
5.3 Vocabulary Reduction Through
Compound Splitting
One of the main issues in translating from a com-
pounding and/or highly inflected language into
a morphologically less complex language is data
sparsity: many source words occur very rarely,
which makes it difficult to learn the correct transla-
tions. Compound splitting aims at making the vo-
cabulary as small as possible but at the same time
keeping as much of the morphological information
as necessary to ensure translation quality. Table 7
shows the vocabulary sizes of our translation ex-
periments, where ?types? and ?singles? refer to
the training data and ?unknown? refers to the test
set. It can be seen that the vocabulary is smallest
for the corpus-driven approach (cd). However, as
the translation experiments in the previous section
have shown, the cd approach was outperformed by
the hybrid approaches, despite their larger vocab-
ularies.
system types singles unknown
raw 267,392 135,328 1,032
cd 97,378 36,928 506
sm 100,836 37,433 593
sm@nn 130,574 51,799 644
smc 109,837 39,908 608
smc@nn 133,755 52,505 650
Table 7: Measuring Vocabulary Reduction for
Compound Splitting.
6 Conclusion
We combined linguistic analysis with corpus-
based statistics and obtained better results in terms
of both producing splittings and statistical ma-
chine translation performance. We provided an ex-
tensive analysis showing where our approach im-
proves on corpus-driven splitting.
We believe that our work helps to validate the
utility of SMOR. The unsupervised morphology
induction community has already begun to evalu-
ate using SMT (Viripioja et al, 2007). Developers
of high recall hand-crafted morphologies should
also consider statistical machine translation as a
useful extrinsic evaluation.
Acknowledgments
This work was supported by Deutsche
Forschungsgemeinschaft grant ?Models of Mor-
phosyntax for Statistical Machine Translation?.
We would like to thank Helmut Schmid.
References
Martin Braschler and Ba?rbel Ripplinger. 2004. How
effective is stemming and decompounding for Ger-
man text retrieval? Information Retrieval, 7(3-
4):291?316.
Chris Dyer. 2009. Using a maximum entropy
model to build segmentation lattices for MT. In
HLT-NAACL?09: Proceedings of the Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics.
Philipp Koehn and Kevin Knight. 2003. Empirical
methods for compound splitting. In EACL ?03: Pro-
ceedings of the 10th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 187?193, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In ACL?07: Proceedings of the 45th Annual
Meeting of the Association for Computational Lin-
guistics, Demonstration Session, pages 177?180.
Martha Larson, Daniel Willett, Joachim Ko?hler, and
Gerhard Rigoll. 2000. Compound splitting and lexi-
cal unit recombination for improved performance of
a speech recognition system for German parliamen-
tary speeches. In ICSLP?00: Proceedings of the 6th
International Conference on Spoken Language Pro-
cessing, pages 945?948.
Alon Lavie and Abhaya Agarwal. 2007. Meteor: An
automatic metric for MT evaluation with high levels
of correlation with human judgements. In ACL?07:
Proceedings of the 2nd Workshop on Statistical Ma-
chine Translation within the 45th Annual Meeting
of the Association for Computational Linguistics,
pages 228?231.
Sonja Nie?en and Hermann Ney. 2000. Improving
SMT quality with morpho-syntactic analysis. In
COLING?00: Proceedings of the 18th International
Conference on Computational Linguistics, pages
1081?1085. Morgan Kaufmann.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL?03: Proceed-
ings of the 41st Annual Meeting of the Association
for Compuational Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic eval-
uation of machine translation. In ACL?02: Proceed-
ings of the 40th Annual Meeting of the Association
for Computational Linguistics, pages 311?318.
Maja Popovic?, Daniel Stein, and Hermann Ney. 2006.
Statistical machine translation of German compound
233
words. In FinTAL?06: Proceedings of the 5th Inter-
national Conference on Natural Language Process-
ing, pages 616?624. Springer Verlag.
Helmut Schmid, Arne Fitschen, and Ulrich Heid.
2004. Smor: A German computational morphology
covering derivation, composition and inflection. In
LREC ?04: Proceedings of the 4th Conference on
Language Resources and Evaluation, pages 1263?
1266.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, pages 44?49.
Sara Stymne. 2008. German compounds in factored
statistical machine translation. In GoTAL ?08: Pro-
ceedings of the 6th International Conference on Nat-
ural Language Processing, pages 464?475. Springer
Verlag.
Sami Viripioja, Jaakko J. Va?yrynen, Mathias Creutz,
and Markus Sadeniemi. 2007. Morphology-aware
statistical machine translation based on morphs in-
duced in an unsupervised manner. In MT Summit
?07: Proceedings of the 11th Machine Translation
Summit, pages 491?498.
234
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 122?127,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Munich-Edinburgh-Stuttgart Submissions of OSM Systems at WMT13
Nadir Durrani1, Helmut Schmid2, Alexander Fraser2,
Hassan Sajjad3, Richa?rd Farkas4
1University of Edinburgh ? dnadir@inf.ed.ac.uk
2Ludwig Maximilian University Munich ? schmid,fraser@cis.uni-muenchen.de
3Qatar Computing Research Institute ? hsajjad@qf.org.qa
4University of Szeged ? rfarkas@inf.u-szeged.hu
Abstract
This paper describes Munich-Edinburgh-
Stuttgart?s submissions to the Eighth
Workshop on Statistical Machine Transla-
tion. We report results of the translation
tasks from German, Spanish, Czech and
Russian into English and from English to
German, Spanish, Czech, French and Rus-
sian. The systems described in this paper
use OSM (Operation Sequence Model).
We explain different pre-/post-processing
steps that we carried out for different
language pairs. For German-English we
used constituent parsing for reordering
and compound splitting as preprocessing
steps. For Russian-English we transliter-
ated the unknown words. The translitera-
tion system is learned with the help of an
unsupervised transliteration mining algo-
rithm.
1 Introduction
In this paper we describe Munich-Edinburgh-
Stuttgart?s1 joint submissions to the Eighth Work-
shop on Statistical Machine Translation. We use
our in-house OSM decoder which is based on
the operation sequence N-gram model (Durrani
et al, 2011). The N-gram-based SMT frame-
work (Marin?o et al, 2006) memorizes Markov
chains over sequences of minimal translation units
(MTUs or tuples) composed of bilingual transla-
tion units. The OSM model integrates reordering
operations within the tuple sequences to form a
heterogeneous mixture of lexical translation and
1Qatar Computing Research Institute and University of
Szeged were partnered for RU-EN and DE-EN language pairs
respectively.
reordering operations and learns a Markov model
over a sequence of operations.
Our decoder uses the beam search algorithm in
a stack-based decoder like most sequence-based
SMT frameworks. Although the model is based
on minimal translation units, we use phrases dur-
ing search because they improve the search accu-
racy of our system. The earlier decoder (Durrani
et al, 2011) was based on minimal units. But we
recently showed that using phrases during search
gives better coverage of translation, better future
cost estimation and lesser search errors (Durrani
et al, 2013a) than MTU-based decoding. We have
therefore shifted to phrase-based search on top of
the OSM model.
This paper is organized as follows. Section 2
gives a short description of the model and search
as used in the OSM decoder. In Section 3 we
give a description of the POS-based operation se-
quence model that we test for our German-English
and English-German experiments. Section 4 de-
scribes our processing of the German and English
data for German-English and English-German ex-
periments. In Section 5 we describe the unsuper-
vised transliteration mining that has been done for
the Russian-English and English-Russian experi-
ments. In Section 6 we describe the sub-sampling
technique that we have used for several language
pairs. In Section 7 we describe the experimental
setup followed by the results. Finally we summa-
rize the paper in Section 8.
2 System Description
2.1 Model
Our systems are based on the OSM (Operation Se-
quence Model) that simultaneously learns trans-
lation and reordering by representing a bilingual
122
Figure 1: Bilingual Sentence with Alignments
sentence pair and its alignments as a unique se-
quence of operations. An operation either jointly
generates source and target words, or it performs
reordering by inserting gaps or jumping to gaps.
We then learn a Markov model over a sequence of
operations o1, o2, . . . , oJ that encapsulate MTUs
and reordering information as:
posm(o1, ..., oJ) =
J?
j=1
p(oj |oj?n+1, ..., oj?1)
By coupling reordering with lexical generation,
each (translation or reordering) decision depends
on n? 1 previous (translation and reordering) de-
cisions spanning across phrasal boundaries. The
reordering decisions therefore influence lexical se-
lection and vice versa. A heterogeneous mixture
of translation and reordering operations enables us
to memorize reordering patterns and lexicalized
triggers unlike the classic N-gram model where
translation and reordering are modeled separately.
2.2 Training
During training, each bilingual sentence pair is de-
terministically converted to a unique sequence of
operations.2 The example in Figure 1(a) is con-
verted to the following sequence of operations:
Generate(Beide, Both)? Generate(La?nder, coun-
tries)? Generate(haben, have)? Insert Gap?
Generate(investiert, invested)
At this point, the (partial) German and English
sentences look as follows:
Beide La?nder haben investiert
Both countries have invested
The translator then jumps back and covers the
skipped German words through the following se-
quence of operations:
Jump Back(1)?Generate(Millionen, millions)?
Generate(von, of)? Generate(Dollar, dollars)
2Please refer to Durrani et al (2011) for a list of opera-
tions and the conversion algorithm.
The generative story of the OSM model also
supports discontinuous source-side cepts and
source-word deletion. However, it doesn?t provide
a mechanism to deal with unaligned and discon-
tinuous target cepts. These are handled through
a 3-step process3 in which we modify the align-
ments to remove discontinuous and unaligned tar-
get MTUs. Please see Durrani et al (2011) for
details. After modifying the alignments, we con-
vert each bilingual sentence pair and its align-
ments into a sequence of operations as described
above and learn an OSM model. To this end,
a Kneser-Ney (Kneser and Ney, 1995) smoothed
9-gram model is trained with SRILM (Stolcke,
2002) while KenLM (Heafield, 2011) is used at
runtime.
2.3 Feature Functions
We use additional features for our model and em-
ploy the standard log-linear approach (Och and
Ney, 2004) to combine and tune them. We search
for a target string E which maximizes a linear
combination of feature functions:
E? = argmax
E
?
?
?
J?
j=1
?jhj(o1, ..., oJ)
?
?
?
where ?j is the weight associated with the fea-
ture hj(o1, ..., oj). Apart from the main OSM
feature we train 9 additional features: A target-
language model (see Section 7 for details), 2 lex-
ical weighting features, gap and open gap penalty
features, two distance-based distortion models and
2 length-based penalty features. Please refer to
Durrani et al (2011) for details.
2.4 Phrase Extraction
Phrases are extracted in the following way: The
aligned training corpus is first converted to an op-
eration sequence. Each subsequence of operations
that starts and ends with a translation operation, is
considered a ?phrase?. The translation operations
include Generate Source Only (X) operation which
deletes unaligned source word. Such phrases may
be discontinuous if they include reordering opera-
tions. We replace each subsequence of reordering
operations by a discontinuity marker.
3Durrani et al (2013b) recently showed that our post-
processing of alignments hurt the performance of the Moses
Phrase-based system in several language pairs. The solu-
tion they proposed has not been incorporated into the current
OSM decoder yet.
123
During decoding, we match the source tokens
of the phrase with the input. Whenever there is
a discontinuity in the phrase, the next source to-
ken can be matched at any position of the input
string. If there is no discontinuity marker, the next
source token in the phrase must be to the right of
the previous one. Finally we compute the number
of uncovered input tokens within the source span
of the hypothesized phrase and reject the phrase
if the number is above a threshold. We use a
threshold value of 2 which had worked well in
initial experiments. Once the positions of all the
source words of a phrase are known, we can com-
pute the necessary reordering operations (which
may be different from the ones that appeared in
the training corpus). This usage of phrases al-
lows the decoder to generalize from a seen trans-
lation ?scored a goal ? ein Tor schoss? (where
scored/a/goal and schoss/ein/Tor are aligned, re-
spectively) to ?scored a goal ? schoss ein Tor?.
The phrase can even be used to translate ?er schoss
heute ein Tor ? he scored a goal today? although
?heute? appears within the source span of the
phrase ?ein Tor schoss?. Without phrase-based
decoding, the unusual word translations ?schoss?
scored? and ?Tor?goal? (at least outside of the soc-
cer literature) are likely to be pruned.
The phrase tables are further filtered with
threshold pruning. The translation options with
a frequency less than x times the frequency of
the most frequent translation are deleted. We use
x = 0.02. We use additional settings to increase
this threshold for longer phrases. The phrase fil-
tering heuristic was used to speed up decoding. It
did not lower the BLEU score in our small scale
experiments (Durrani et al, 2013a), however we
could not test whether this result holds in a large
scale evaluation.
2.5 Decoder
The decoding framework used in the operation se-
quence model is based on Pharaoh (Koehn, 2004).
The decoder uses beam search to build up the
translation from left to right. The hypotheses are
arranged in m stacks such that stack i maintains
hypotheses that have already translated imany for-
eign words. The ultimate goal is to find the best
scoring hypothesis, that translates all the words
in the foreign sentence. During the hypothesis
extension each extracted phrase is translated into
a sequence of operations. The reordering opera-
tions (gaps and jumps) are generated by looking at
the position of the translator, the last foreign word
generated etc. (Please refer to Algorithm 1 in Dur-
rani et al (2011)). The probability of an opera-
tion depends on the n?1 previous operations. The
model is smoothed with Kneser-Ney smoothing.
3 POS-based OSM Model
Part-of-speech information is often relevant for
translation. The word ?stores? e.g. should be
translated to ?La?den? if it is a noun and to ?spei-
chert? when it is a verb. The sentence ?The small
child cries? might be incorrectly translated to ?Die
kleinen Kind weint? where the first three words
lack number, gender and case agreement.
In order to better learn such constraints which
are best expressed in terms of part of speech, we
add another OSM model as a new feature to the
log-linear model of our decoder, which is identi-
cal to the regular OSM except that all the words
have been replaced by their POS tags. The input
of the decoder consists of the input sentence with
automatically assigned part-of-speech tags. The
source and target part of the training data are also
automatically tagged and phrases with words and
POS tags on both sides are extracted. The POS-
based OSM model is only used in the German-to-
English and English-to-German experiments.4 So
far, we only used coarse POS tags without gender
and case information.
4 Constituent Parse Reordering
Our German-to-English system used constituent
parses for pre-ordering of the input. We parsed all
of the parallel German to English data available,
and the tuning, test and blind-test sets. We then
applied reordering rules to these parses. We used
the rules for reordering German constituent parses
of Collins et al (2005) together with the additional
rules described by Fraser (2009). These are ap-
plied as a preprocess to all German data (training,
tuning and test data). To produce the parses, we
started with the generative BitPar parser trained on
the Tiger treebank with optimizations of the gram-
mar, as described by (Fraser et al, 2013). We then
performed self-training using the high quality Eu-
roparl corpus - we parsed it, and then retrained the
parser on the output.
4This work is ongoing and we will present detailed exper-
iments in the future.
124
Following this, we performed linguistically-
informed compound splitting, using the system of
Fritzinger and Fraser (2010), which disambiguates
competing analyses from the high-recall Stuttgart
Morphological Analyzer SMOR (Schmid et al,
2004) using corpus statistics (Koehn and Knight,
2003). We also split portmanteaus like German
?zum? formed from ?zu dem? meaning ?to the?.
Due to time constraints, we did not address Ger-
man inflection. See Weller et al (2013) for further
details of the linguistic processing involved in our
German-to-English system.
5 Transliteration Mining/Handling
OOVs
The machine translation system fails to translate
out-of-vocabulary words (OOVs) as they are un-
known to the training data. Most of the OOVs
are named entities and simply passing them to
the output often produces correct translations if
source and target language use the same script.
If the scripts are different transliterating them to
the target language script could solve this prob-
lem. However, building a transliteration system
requires a list of transliteration pairs for training.
We do not have such a list and making one is a
cumbersome process. Instead, we use the unsu-
pervised transliteration mining system of Sajjad et
al. (2012) that takes a list of word pairs for train-
ing and extracts transliteration pairs that can be
used for the training of the transliteration system.
The procedure of mining transliteration pairs and
transliterating OOVs is described as follows:
We word-align the parallel corpus using
GIZA++ in both direction and symmetrize the
alignments using the grow-diag-final-and heuris-
tic. We extract all word pairs which occur as 1-
to-1 alignments (like Sajjad et al (2011)) and later
refer to them as the list of word pairs. We train the
unsupervised transliteration mining system on the
list of word pairs and extract transliteration pairs.
We use these mined pairs to build a transliteration
system using the Moses toolkit. The translitera-
tion system is applied in a post-processing step
to transliterate OOVs. Please refer to Sajjad et
al. (2013) for further details on our transliteration
work.
6 Sub-sampling
Because of scalability problems we were not able
to use the entire data made available for build-
ing the translation model in some cases. We used
modified Moore-Lewis sampling (Axelrod et al,
2011) for the language pairs es-en, en-es, en-fr,
and en-cs. In each case we included the News-
Commentary and Europarl corpora in their en-
tirety, and scored the sentences in the remaining
corpora (the selection corpus) using a filtering cri-
terion, adding 10% of the selection corpus to
the training data. We can not say with certainty
whether using the entire data will produce better
results with the OSM decoder. However, we know
that the same data used with the state-of-the-art
Moses produced worse results in some cases. The
experiments in Durrani et al (2013c) showed that
MML filtering decreases the BLEU scores in es-
en (news-test13: Table 19) and en-cs (news-test12:
Table 14). We can therefore speculate that being
able to use all of the data may improve our results
somewhat.
7 Experiments
Parallel Corpus: The amount of bitext used for
the estimation of the translation models is: de?en
? 4.5M and ru?en ? 2M parallel sentences. We
were able to use all the available data for cs-to-en
(? 15.6M sentences). However, sub-sampled data
was used for en-to-cs (? 3M sentences), en-to-fr
(? 7.8M sentences) and es?en (? 3M sentences).
Monolingual Language Model: We used all
the available training data (including LDC Giga-
word data) for the estimation of monolingual lan-
guage models: en? 287.3M sentences, fr? 91M,
es ? 65.7M, cs ? 43.4M and ru ? 21.7M sen-
tences. All data except for ru-en and en-ru was
true-cased. We followed the approach of Schwenk
and Koehn (2008) by training language models
from each sub-corpus separately and then linearly
interpolated them using SRILM with weights op-
timized on the held-out dev-set. We concatenated
the news-test sets from four years (2008-2011) to
obtain a large dev-set5 in order to obtain more sta-
ble weights (Koehn and Haddow, 2012).
Decoder Settings: For each extracted input
phrase only 15-best translation options were used
during decoding.6 We used a hard reordering limit
5For Russian-English and English-Russian language
pairs, we divided the tuning-set news-test 2012 into two
halves and used the first half for tuning and second for test.
6We could not experiment with higher n-best translation
options due to a bug that was not fixed in time and hindered
us from scaling.
125
of 16 words which disallows a jump beyond 16
source words. A stack size of 100 was used during
tuning and 200 for decoding the test set.
Results: Table 1 shows the uncased BLEU
scores along with the rank obtained on the sub-
mission matrix.7 We also show the results from
human evaluation.
Lang Evaluation
Automatic Human
BLEU Rank Win Ratio Rank
de-en 27.6 9/31 0.562 6-8
es-en 30.4 6/12 0.569 3-5
cs-en 26.4 3/11 0.581 2-3
ru-en 24.5 8/22 0.534 7-9
en-de 20.0 6/18
en-es 29.5 3/13 0.544 5-6
en-cs 17.6 14/22 0.517 4-6
en-ru 18.1 6/15 0.456 9-10
en-fr 30.0 7/26 0.541 5-9
Table 1: Translating into and from English
8 Conclusion
In this paper, we described our submissions to
WMT 13 in all the shared-task language pairs
(except for fr-en). We used an OSM-decoder,
which implements a model on n-gram of opera-
tions encapsulating lexical generation and reorder-
ing. For German-to-English we used constituent
parsing and applied linguistically motivated rules
to these parses, followed by compound splitting.
We additionally used a POS-based OSM model for
German-to-English and English-to-German exper-
iments. For Russian-English language pairs we
used unsupervised transliteration mining. Because
of scalability issues we could not use the entire
data in some language pairs and used only sub-
sampled data. Our Czech-to-English system that
was built from the entire data did better in both
automatic and human evaluation compared to the
systems that used sub-sampled data.
Acknowledgments
We would like to thank the anonymous reviewers
for their helpful feedback and suggestions. We
would like to thank Philipp Koehn and Barry Had-
dow for providing data and alignments. Nadir
7http://matrix.statmt.org/
Durrani was funded by the European Union Sev-
enth Framework Programme (FP7/2007-2013) un-
der grant agreement n ? 287658. Alexander
Fraser was funded by Deutsche Forschungsge-
meinschaft grant Models of Morphosyntax for
Statistical Machine Translation. Helmut Schmid
was supported by Deutsche Forschungsgemein-
schaft grant SFB 732. Richa?rd Farkas was
partially funded by the Hungarian National Ex-
cellence Program (TA?MOP 4.2.4.A/2-11-1-2012-
0001). This publication only reflects the authors?
views.
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 355?362, Edinburgh, Scotland, UK., July.
Association for Computational Linguistics.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause Restructuring for Statistical Machine
Translation. In ACL05, pages 531?540, Ann Arbor,
MI.
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A Joint Sequence Translation Model with In-
tegrated Reordering. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
1045?1054, Portland, Oregon, USA, June.
Nadir Durrani, Alexander Fraser, and Helmut Schmid.
2013a. Model With Minimal Translation Units, But
Decode With Phrases. In The 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Atlanta, Georgia, USA, June. Association
for Computational Linguistics.
Nadir Durrani, Alexander Fraser, Helmut Schmid,
Hieu Hoang, and Philipp Koehn. 2013b. Can
Markov Models Over Minimal Translation Units
Help Phrase-Based SMT? In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics, Sofia, Bulgaria, August. Asso-
ciation for Computational Linguistics.
Nadir Durrani, Barry Haddow, Kenneth Heafield, and
Philipp Koehn. 2013c. Edinburgh?s Machine Trans-
lation Systems for European Language Pairs. In
Proceedings of the Eighth Workshop on Statistical
Machine Translation, Sofia, Bulgaria, August. As-
sociation for Computational Linguistics.
Alexander Fraser, Helmut Schmid, Richa?rd Farkas,
Renjing Wang, and Hinrich Schu?tze. 2013. Knowl-
edge sources for constituent parsing of German, a
morphologically rich and less-configurational lan-
guage. Computational Linguistics - to appear.
126
Alexander Fraser. 2009. Experiments in Morphosyn-
tactic Processing for Translating to and from Ger-
man. In Proceedings of the EACL 2009 Fourth
Workshop on Statistical Machine Translation, pages
115?119, Athens, Greece, March.
Fabienne Fritzinger and Alexander Fraser. 2010. How
to Avoid Burning Ducks: Combining Linguistic
Analysis and Corpus Statistics for German Com-
pound Processing. In Proceedings of the ACL 2010
Fifth Workshop on Statistical Machine Translation,
Uppsala, Sweden.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
pages 187?197, Edinburgh, Scotland, United King-
dom, 7.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In In
Proceedings of the IEEE International Conference
on Acoustics, Speech and Signal Processing, vol-
ume I, pages 181?184, Detroit, Michigan, May.
Philipp Koehn and Barry Haddow. 2012. Towards Ef-
fective Use of Training Data in Statistical Machine
Translation. In Proceedings of the Seventh Work-
shop on Statistical Machine Translation, pages 317?
321, Montre?al, Canada, June. Association for Com-
putational Linguistics.
Philipp Koehn and Kevin Knight. 2003. Empirical
methods for compound splitting. In Proceedings of
the 10th Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 187?193, Morristown, NJ.
Philipp Koehn. 2004. Pharaoh: A Beam Search De-
coder for Phrase-Based Statistical Machine Transla-
tion Models. In AMTA, pages 115?124.
Jose? B. Marin?o, Rafael E. Banchs, Josep M. Crego,
Adria` de Gispert, Patrik Lambert, Jose? A. R. Fonol-
losa, and Marta R. Costa-jussa`. 2006. N-gram-
Based Machine Translation. Computational Lin-
guistics, 32(4):527?549.
Franz J. Och and Hermann Ney. 2004. The Alignment
Template Approach to Statistical Machine Transla-
tion. Computational Linguistics, 30(1):417?449.
Hassan Sajjad, Alexander Fraser, and Helmut Schmid.
2011. An algorithm for unsupervised transliteration
mining with an application to word alignment. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies - Volume 1, Portland, USA.
Hassan Sajjad, Alexander Fraser, and Helmut Schmid.
2012. A statistical model for unsupervised and
semi-supervised transliteration mining. In Proceed-
ings of the 50th Annual Meeting of the Association
for Computational Linguistics: Long Papers - Vol-
ume 1, Jeju, Korea.
Hassan Sajjad, Svetlana Smekalova, Nadir Durrani,
Alexander Fraser, and Helmut Schmid. 2013.
QCRI-MES Submission at WMT13: Using Translit-
eration Mining to Improve Statistical Machine
Translation. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Helmut Schmid, Arne Fitschen, and Ulrich Heid.
2004. SMOR: A German Computational Morphol-
ogy Covering Derivation, Composition, and Inflec-
tion. In Proceedings of the Fourth International
Conference on Language Resources and Evaluation
(LREC).
Holger Schwenk and Philipp Koehn. 2008. Large and
Diverse Language Models for Statistical Machine
Translation. In International Joint Conference on
Natural Language Processing, pages 661?666, Jan-
uary 2008.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Intl. Conf. Spoken Lan-
guage Processing, Denver, Colorado.
Marion Weller, Max Kisselew, Svetlana Smekalova,
Alexander Fraser, Helmut Schmid, Nadir Durrani,
Hassan Sajjad, and Richa?rd Farkas. 2013. Munich-
Edinburgh-Stuttgart Submissions at WMT13: Mor-
phological and Syntactic Processing for SMT. In
Proceedings of the Eighth Workshop on Statistical
Machine Translation, Sofia, Bulgaria, August. As-
sociation for Computational Linguistics.
127
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 219?224,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
QCRI-MES Submission at WMT13: Using Transliteration Mining to
Improve Statistical Machine Translation
Hassan Sajjad1, Svetlana Smekalova2, Nadir Durrani3,
Alexander Fraser4, Helmut Schmid4
1Qatar Computing Research Institute ? hsajjad@qf.org.qa
2University of Stuttgart ? smekalsa@ims.uni-stuttgart.de
3University of Edinburgh ? dnadir@inf.ed.ac.uk
4Ludwig-Maximilians University Munich ? (fraser|schmid)@cis.uni-muenchen.de
Abstract
This paper describes QCRI-MES?s sub-
mission on the English-Russian dataset to
the Eighth Workshop on Statistical Ma-
chine Translation. We generate improved
word alignment of the training data by
incorporating an unsupervised translitera-
tion mining module to GIZA++ and build
a phrase-based machine translation sys-
tem. For tuning, we use a variation of PRO
which provides better weights by optimiz-
ing BLEU+1 at corpus-level. We translit-
erate out-of-vocabulary words in a post-
processing step by using a transliteration
system built on the transliteration pairs
extracted using an unsupervised translit-
eration mining system. For the Russian
to English translation direction, we apply
linguistically motivated pre-processing on
the Russian side of the data.
1 Introduction
We describe the QCRI-Munich-Edinburgh-
Stuttgart (QCRI-MES) English to Russian and
Russian to English systems submitted to the
Eighth Workshop on Statistical Machine Trans-
lation. We experimented using the standard
Phrase-based Statistical Machine Translation
System (PSMT) as implemented in the Moses
toolkit (Koehn et al, 2007). The typical pipeline
for translation involves word alignment using
GIZA++ (Och and Ney, 2003), phrase extraction,
tuning and phrase-based decoding. Our system is
different from standard PSMT in three ways:
? We integrate an unsupervised transliteration
mining system (Sajjad et al, 2012) into the
GIZA++ word aligner (Sajjad et al, 2011).
So, the selection of a word pair as a correct
alignment is decided using both translation
probabilities and transliteration probabilities.
? The MT system fails when translating out-of-
vocabulary (OOV) words. We build a statis-
tical transliteration system on the translitera-
tion pairs mined by the unsupervised translit-
eration mining system and transliterate them
in a post-processing step.
? We use a variation of Pairwise Ranking Op-
timization (PRO) for tuning. It optimizes
BLEU at corpus-level and provides better
feature weights that leads to an improvement
in translation quality (Nakov et al, 2012).
We participate in English to Russian and Rus-
sian to English translation tasks. For the Rus-
sian/English system, we present experiments with
two variations of the parallel corpus. One set of
experiments are conducted using the standard par-
allel corpus provided by the workshop. In the sec-
ond set of experiments, we morphologically re-
duce Russian words based on their fine-grained
POS tags and map them to their root form. We
do this on the Russian side of the parallel corpus,
tuning set, development set and test set. This im-
proves word alignment and learns better transla-
tion probabilities by reducing the vocabulary size.
The paper is organized as follows. Section
2 talks about unsupervised transliteration mining
and its incorporation to the GIZA++ word aligner.
In Section 3, we describe the transliteration sys-
tem. Section 4 describes the extension of PRO
that optimizes BLEU+1 at corpus level. Section
5 and Section 6 present English/Russian and Rus-
sian/English machine translation experiments re-
spectively. Section 7 concludes.
219
2 Transliteration Mining
Consider a list of word pairs that consists of either
transliteration pairs or non-transliteration pairs.
A non-transliteration pair is defined as a word
pair where words are not transliteration of each
other. They can be translation, misalignment,
etc. Transliteration mining extracts transliteration
pairs from the list of word pairs. Sajjad et al
(2012) presented an unsupervised transliteration
mining system that trains on the list of word pairs
and filters transliteration pairs from that. It models
the training data as the combination of a translit-
eration sub-model and a non-transliteration sub-
model. The transliteration model is a joint source
channel model. The non-transliteration model as-
sumes no correlation between source and target
word characters, and independently generates a
source and a target word using two fixed uni-
gram character models. The transliteration mining
model is defined as an interpolation of the translit-
eration model and the non-transliteration model.
We apply transliteration mining to the list of
word pairs extracted from English/Russian paral-
lel corpus and mine transliteration pairs. We use
the mined pairs for the training of the translitera-
tion system.
2.1 Transliteration Augmented-GIZA++
GIZA++ aligns parallel sentences at word level. It
applies the IBM models (Brown et al, 1993) and
the HMM model (Vogel et al, 1996) in both direc-
tions i.e. source to target and target to source. It
generates a list of translation pairs with translation
probabilities, which is called the t-table. Sajjad
et al (2011) used a heuristic-based transliteration
mining system and integrated it into the GIZA++
word aligner. We follow a similar procedure but
use the unsupervised transliteration mining system
of Sajjad et al (2012).
We define a transliteration sub-model and train
it on the transliteration pairs mined by the unsuper-
vised transliteration mining system. We integrate
it into the GIZA++ word aligner. The probabil-
ity of a word pair is calculated as an interpolation
of the transliteration probability and the transla-
tion probability stored in the t-table of the differ-
ent alignment models used by the GIZA++ aligner.
This interpolation is done for all iterations of all
alignment models.
2.1.1 Estimating Transliteration Probabilities
We use the algorithm for the estimation of translit-
eration probabilities of Sajjad et al (2011). We
modify it to improve efficiency. In step 6 of Al-
gorithm 1 instead of taking all f that coocur with
e, we take only those that have a word length ra-
tio in range of 0.8-1.2.1 This reduces cooc(e) by
more than half and speeds up step 9 of Algorithm
1. The word pairs that are filtered out from cooc(e)
won?t have transliteration probability pti(f |e). We
do not interpolate in these cases and use the trans-
lation probability as it is.
Algorithm 1 Estimation of transliteration proba-
bilities, e-to-f direction
1: unfiltered data? list of word pairs
2: filtered data?transliteration pairs extracted using unsu-
pervised transliteration mining system
3: Train a transliteration system on the filtered data
4: for all e do
5: nbestTI(e) ? 10 best transliterations for e accord-
ing to the transliteration system
6: cooc(e)? set of all f that cooccur with e in a parallel
sentence with a word length in ratio of 0.8-1.2
7: candidateTI(e)? cooc(e) ? nbestTI(e)
8: for all f do
9: pmoses(f, e) ? joint transliteration probability of e
and f according to the transliterator
10: Calculate conditional transliteration probability
pti(f |e)? pmoses(f,e)?
f??CandidateTI(e) pmoses(f ?,e)
2.1.2 Modified EM Training
Sajjad et al (2011) modified the EM training of
the word alignment models. They combined the
translation probabilities of the IBM models and
the HMM model with the transliteration proba-
bilities. Consider pta(f |e) = fta(f, e)/fta(e) is
the translation probability of the word alignment
models. The interpolated probability is calcu-
lated by adding the smoothed alignment frequency
fta(f, e) to the transliteration probability weight
by the factor ?. The modified translation probabil-
ities is given by:
p?(f |e) = fta(f, e) + ?pti(f |e)fta(e) + ?
(1)
where fta(f, e) = pta(f |e)fta(e). pta(f |e) is ob-
tained from the original t-table of the alignment
model. fta(e) is the total corpus frequency of e.
? is the transliteration weight which is defined as
the number of counts the transliteration model gets
versus the translation model. The model is not
1We assume that the words with very different character
counts are less likely to be transliterations.
220
very sensitive to the value of ?. We use ? = 50
for our experiments. The procedure we described
of estimation of transliteration probabilities and
modification of EM is also followed in the oppo-
site direction f-to-e.
3 Transliteration System
The unsupervised transliteration mining system
(as described in Section 2) outputs a list of translit-
eration pairs. We consider transliteration word
pairs as parallel sentences by putting a space af-
ter every character of the words and train a PSMT
system for transliteration. We apply the transliter-
ation system to OOVs in a post-processing step on
the output of the machine translation system.
Russian is a morphologically rich language.
Different cases of a word are generally represented
by adding suffixes to the root form. For OOVs
that are named entities, transliterating the inflected
forms generates wrong English transliterations as
inflectional suffixes get transliterated too. To han-
dle this, first we need to identify OOV named en-
tities (as there can be other OOVs that are not
named entities) and then transliterate them cor-
rectly. We tackle the first issue as follows: If
an OOV word is starting with an upper case let-
ter, we identify it as a named entity. To correctly
transliterate it to English, we stem the named en-
tity based on a list of suffixes ( , , , , , )
and transliterate the stemmed form. For morpho-
logically reduced Russian (see Section 6.1), we
follow the same procedure as OOVs are unknown
to the POS tagger too and are (incorrectly) not re-
duced to their root forms. For OOVs that are not
identified as named entities, we transliterate them
without any pre-processing.
4 PRO: Corpus-level BLEU
Pairwise Ranking Optimization (PRO) (Hopkins
and May, 2011) is an extension of MERT (Och,
2003) that can scale to thousands of parameters.
It optimizes sentence-level BLEU+1 which is an
add-one smoothed version of BLEU (Lin and Och,
2004). The sentence-level BLEU+1 has a bias
towards producing short translations as add-one
smoothing improves precision but does not change
the brevity penalty. Nakov et al (2012) fixed this
by using several heuristics on brevity penalty, ref-
erence length and grounding the precision length.
In our experiments, we use the improved version
of PRO as provided by Nakov et al (2012). We
call it PROv1 later on.
5 English/Russian Experiments
5.1 Dataset
The amount of bitext used for the estimation of the
translation model is ? 2M parallel sentences. We
use newstest2012a for tuning and newstest2012b
(tst2012) as development set.
The language model is estimated using large
monolingual corpus of Russian ? 21.7M sen-
tences. We follow the approach of Schwenk and
Koehn (2008) by training domain-specific lan-
guage models separately and then linearly inter-
polate them using SRILM with weights optimized
on the held-out development set. We divide the
tuning set newstest2012a into two halves and use
the first half for tuning and second for test in or-
der to obtain stable weights (Koehn and Haddow,
2012).
5.2 Baseline Settings
We word-aligned the parallel corpus using
GIZA++ (Och and Ney, 2003) with 5 iterations
of Model1, 4 iterations of HMM and 4 iterations
of Model4, and symmetrized the alignments us-
ing the grow-diag-final-and heuristic (Koehn et al,
2003). We built a phrase-based machine transla-
tion system using the Moses toolkit. Minimum er-
ror rate training (MERT), margin infused relaxed
algorithm (MIRA) and PRO are used to optimize
the parameters.
5.3 Main System Settings
Our main system involves a pre-processing step
? unsupervised transliteration mining, and a post-
processing step ? transliteration of OOVs. For the
training of the unsupervised transliteration min-
ing system, we take the word alignments from
our baseline settings and extract all word pairs
which occur as 1-to-1 alignments (like Sajjad et
al. (2011)) and later refer to them as a list of
word pairs. The unsupervised transliteration min-
ing system trains on the list of word pairs and
mines transliteration pairs. We use the mined pairs
to build a transliteration system using the Moses
toolkit. The transliteration system is used in Algo-
rithm 1 to generate transliteration probabilities of
candidate word pairs and is also used in the post-
processing step to transliterate OOVs.
We run GIZA++ with identical settings as de-
scribed in Section 5.2. We interpolate for ev-
221
GIZA++ TA-GIZA++ OOV-TI
MERT 23.41 23.51 23.60
MIRA 23.60 23.73 23.85
PRO 23.57 23.68 23.70
PROv1 23.65 23.76 23.87
Table 1: BLEU scores of English to Russian ma-
chine translation system evaluated on tst2012 us-
ing baseline GIZA++ alignment and translitera-
tion augmented-GIZA++. OOV-TI presents the
score of the system trained using TA-GIZA++ af-
ter transliterating OOVs
ery iteration of the IBM Model1 and the HMM
model. We had problem in applying smoothing
for Model4 and did not interpolate transliteration
probabilities for Model4. The alignments are re-
fined using the grow-diag-final-and heuristic. We
build a phrase-based system on the aligned pairs
and tune the parameters using PROv1. OOVs are
transliterated in the post-processing step.
5.4 Results
Table 1 summarizes English/Russian results on
tst2012. Improved word alignment gives up to
0.13 BLEU points improvement. PROv1 improves
translation quality and shows 0.08 BLEU point
increase in BLEU in comparison to the parame-
ters tuned using PRO. The transliteration of OOVs
consistently improve translation quality by at least
0.1 BLEU point for all systems.2 This adds to a
cumulative gain of up to 0.2 BLEU points.
We summarize results of our systems trained on
GIZA++ and transliteration augmented-GIZA++
(TA-GIZA++) and tested on tst2012 and tst2013
in Table 2. Both systems use PROv1 for tuning
and transliteration of OOVs in the post-processing
step. The system trained on TA-GIZA++ per-
formed better than the system trained on the base-
line aligner GIZA++.
6 Russian/English Experiments
In this section, we present translation experiments
in Russian to English direction. We morphologi-
cally reduce the Russian side of the parallel data in
a pre-processing step and train the translation sys-
tem on that. We compare its result with the Rus-
sian to English system trained on the un-processed
parallel data.
2We see similar gain in BLEU when using operation se-
quence model (Durrani et al, 2011) for decoding and translit-
erating OOVs in a post-processing step (Durrani et al, 2013).
SYS tst2012 tst2013
GIZA++ 23.76 18.4
TA-GIZA++ 23.87 18.5*
Table 2: BLEU scores of English to Russian ma-
chine translation system evaluated on tst2012 and
tst2013 using baseline GIZA++ alignment and
transliteration augmented-GIZA++ alignment and
post-processed the output by transliterating OOVs.
Human evaluation in WMT13 is performed on
TA-GIZA++ tested on tst2013 (marked with *)
6.1 Morphological Processing
The linguistic processing of Russian involves POS
tagging and morphological reduction. We first tag
the Russian data using a fine grained tagset. The
tagger identifies lemmas and the set of morpholog-
ical attributes attached to each word. We reduce
the number of these attributes by deleting some
of them, that are not relevant for English (for ex-
ample, gender agreement of verbs). This gener-
ates a morphologically reduced Russian which is
used in parallel with English for the training of
the machine translation system. Further details on
the morphological processing of Russian are de-
scribed in Weller et al (2013).
6.1.1 POS Tagging
We use RFTagger (Schmid and Laws, 2008) for
POS tagging. Despite the good quality of tagging
provided by RFTagger, some errors seem to be un-
avoidable due to the ambiguity of certain gram-
matical forms in Russian. A good example of
this is neuter nouns that have the same form in
all cases, or feminine nouns, which have identi-
cal forms in singular genitive and plural nomina-
tive (Sharoff et al, 2008). Since Russian sentences
have free word order, and the case of nouns can-
not be determined on that basis, this imperfection
can not be corrected during tagging or by post-
processing the tagger output.
6.1.2 Morphological Reduction
English in comparison to Slavic group of lan-
guages is morphologically poor. For example, En-
glish has no morphological attributes for nouns
and adjectives to express gender or case; verbs in
English have no gender either. Russian, on the
contrary, has rich morphology. It suffices to say
that the Russian has 6 cases and 3 grammatical
genders, which manifest themselves in different
222
suffixes for nouns, pronouns, adjectives and some
verb forms.
When translating from Russian into English, a
lot of these attributes become meaningless and ex-
cessive. It makes sense to reduce the number of
morphological attributes before the text is sup-
plied for the training of the MT system. We ap-
ply morphological reduction to nouns, pronouns,
verbs, adjectives, prepositions and conjunctions.
The rest of the POS (adverbs, particles, interjec-
tions and abbreviations) have no morphological at-
tributes and are left unchanged.
We apply morphological reduction to train,
tune, development and test data. We refer to this
data set as morph-reduced later on.
6.2 Dataset
We use two variations of the parallel corpus to
build and test the Russian to English system. One
system is built on the data provided by the work-
shop. For the second system, we preprocess the
Russian side of the data as described in Section
6.1. Both the provided parallel corpus and the
morph-reduced parallel corpus consist of 2M par-
allel sentences each. We use them for the estima-
tion of the translation model. We use large train-
ing data for the estimation of monolingual lan-
guage model ? en? 287.3M sentences. We follow
the identical procedure of interpolated language
model as described in Section 5.1. We use new-
stest2012a for tuning and newstest2012b (tst2012)
for development.
6.3 System Settings
We use identical system settings to those described
in Section 5.3. We trained the systems sepa-
rately on GIZA++ and transliteration augmented-
GIZA++ to compare their results. All systems are
tuned using PROv1. The translation output is post-
processed to transliterate OOVs.
6.4 Results
Table 3 summarizes results of Russian to English
machine translation systems trained on the orig-
inal parallel corpus and on the morph-reduced
corpus and using GIZA++ and transliteration
augmented-GIZA++ for word alignment. The sys-
tem using TA-GIZA++ for alignment shows the
best results for both tst2012 and tst2013. The im-
proved alignment gives a BLEU improvement of
up to 0.4 points.
Original corpus
SYS tst2012 tst2013
GIZA++ 32.51 25.5
TA-GIZA++ 33.40 25.9*
Morph-reduced
SYS tst2012 tst2013
GIZA++ 31.22 24.30
TA-GIZA++ 31.40 24.45
Table 3: Russian to English machine translation
system evaluated on tst2012 and tst2013. Human
evaluation in WMT13 is performed on the system
trained using the original corpus with TA-GIZA++
for alignment (marked with *)
The system built on the morph-reduced data
shows degradation in results by 1.29 BLEU points.
However, the percentage of OOVs reduces for
both test sets when using the morph-reduced data
set compared to the original parallel corpus. We
analyze the output of the system and find that the
morph-reduced system makes mistakes in choos-
ing the right tense of the verb. This might be one
reason for poor performance. This implies that the
morphological reduction is slightly damaging the
data, perhaps for specific parts of speech. In the
future, we would like to investigate this issue in
detail.
7 Conclusion
In this paper, we described the QCRI-Munich-
Edinburgh-Stuttgart machine translation systems
submitted to the Eighth Workshop on Statistical
Machine Translation. We aligned the parallel cor-
pus using transliteration augmented-GIZA++ to
improve the word alignments. We built a phrase-
based system using the Moses toolkit. For tun-
ing the feature weights, we used an improvement
of PRO that optimizes for corpus-level BLEU. We
post-processed the output of the machine transla-
tion system to transliterate OOV words.
For the Russian to English system, we mor-
phologically reduced the Russian data in a pre-
processing step. This reduced the vocabulary size
and helped to generate better word alignments.
However, the performance of the SMT system
dropped by 1.29 BLEU points in decoding. We
will investigate this issue further in the future.
223
Acknowledgments
We would like to thank the anonymous reviewers
for their helpful feedback and suggestions. We
would like to thank Philipp Koehn and Barry Had-
dow for providing data and alignments. Nadir
Durrani was funded by the European Union Sev-
enth Framework Programme (FP7/2007-2013) un-
der grant agreement n ? 287658. Alexander Fraser
was funded by Deutsche Forschungsgemeinschaft
grant Models of Morphosyntax for Statistical Ma-
chine Translation. Helmut Schmid was supported
by Deutsche Forschungsgemeinschaft grant SFB
732. This publication only reflects the authors
views.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and R. L. Mercer. 1993. The mathe-
matics of statistical machine translation: parameter
estimation. Computational Linguistics, 19(2).
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A joint sequence translation model with in-
tegrated reordering. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, Port-
land, USA.
Nadir Durrani, Helmut Schmid, Alexander Fraser, Has-
san Sajjad, and Richa?rd Farkas. 2013. Munich-
Edinburgh-Stuttgart submissions of OSM systems at
WMT13. In Proceedings of the Eighth Workshop on
Statistical Machine Translation, Sofia, Bulgaria.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
Edinburgh, United Kingdom.
Philipp Koehn and Barry Haddow. 2012. Towards
effective use of training data in statistical machine
translation. In Proceedings of the Seventh Work-
shop on Statistical Machine Translation, Montre?al,
Canada.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceed-
ings of the Human Language Technology and North
American Association for Computational Linguis-
tics Conference, Edmonton, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation for Computational Linguistics, Demonstra-
tion Program, Prague, Czech Republic.
Chin-Yew Lin and Franz Josef Och. 2004. OR-
ANGE: a method for evaluating automatic evalua-
tion metrics for machine translation. In Proceed-
ings of the 20th international conference on Compu-
tational Linguistics, Geneva, Switzerland.
Preslav Nakov, Francisco Guzma?n, and Stephan Vo-
gel. 2012. Optimizing for sentence-level BLEU+1
yields short translations. In Proceedings of the
24th International Conference on Computational
Linguistics, Mumbai, India.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1).
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, Sapporo, Japan.
Hassan Sajjad, Alexander Fraser, and Helmut Schmid.
2011. An algorithm for unsupervised translitera-
tion mining with an application to word alignment.
In Proceedings of the 49th Annual Conference of
the Association for Computational Linguistics, Port-
land, USA.
Hassan Sajjad, Alexander Fraser, and Helmut Schmid.
2012. A statistical model for unsupervised and
semi-supervised transliteration mining. In Proceed-
ings of the 50th Annual Conference of the Associa-
tion for Computational Linguistics, Jeju, Korea.
Helmut Schmid and Florian Laws. 2008. Estimation
of conditional probabilities with decision trees and
an application to fine-grained pos tagging. In Pro-
ceedings of the 22nd International Conference on
Computational Linguistics - Volume 1, Manchester,
United Kingdom.
Holger Schwenk and Philipp Koehn. 2008. Large and
Diverse Language Models for Statistical Machine
Translation. In International Joint Conference on
Natural Language Processing, Hyderabad, India.
Serge Sharoff, Mikhail Kopotev, Tomaz Erjavec, Anna
Feldman, and Dagmar Divjak. 2008. Designing
and evaluating a russian tagset. In Proceedings of
the Sixth International Conference on Language Re-
sources and Evaluation.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical
translation. In 16th International Conference on
Computational Linguistics, Copenhagen, Denmark.
Marion Weller, Max Kisselew, Svetlana Smekalova,
Alexander Fraser, Helmut Schmid, Nadir Durrani,
Hassan Sajjad, and Richa?rd Farkas. 2013. Munich-
Edinburgh-Stuttgart submissions at WMT13: Mor-
phological and syntactic processing for SMT. In
Proceedings of the Eighth Workshop on Statistical
Machine Translation, Sofia, Bulgaria.
224
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 232?239,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Munich-Edinburgh-Stuttgart Submissions at WMT13:
Morphological and Syntactic Processing for SMT
Marion Weller1, Max Kisselew1, Svetlana Smekalova1, Alexander Fraser2,
Helmut Schmid2, Nadir Durrani3, Hassan Sajjad4, Richa?rd Farkas5
1University of Stuttgart ? (wellermn|kisselmx|smekalsa)@ims.uni-stuttgart.de
2Ludwig-Maximilian University of Munich ? (schmid|fraser)@cis.uni-muenchen.de
3University of Edinburgh ? dnadir@inf.ed.ac.uk
4Qatar Computing Research Institute ? hsajjad@qf.org.qa
5University of Szeged ? rfarkas@inf.u-szeged.hu
Abstract
We present 5 systems of the Munich-
Edinburgh-Stuttgart1 joint submissions to
the 2013 SMT Shared Task: FR-EN, EN-
FR, RU-EN, DE-EN and EN-DE. The
first three systems employ inflectional gen-
eralization, while the latter two employ
parser-based reordering, and DE-EN per-
forms compound splitting. For our ex-
periments, we use standard phrase-based
Moses systems and operation sequence
models (OSM).
1 Introduction
Morphologically complex languages often lead to
data sparsity problems in statistical machine trans-
lation. For translation pairs with morphologically
rich source languages and English as target lan-
guage, we focus on simplifying the input language
in order to reduce the complexity of the translation
model. The pre-processing of the source-language
is language-specific, requiring morphological anal-
ysis (FR, RU) as well as sentence reordering (DE)
and dealing with compounds (DE). Due to time
constraints we did not deal with inflection for DE-
EN and EN-DE.
The morphological simplification process con-
sists in lemmatizing inflected word forms and deal-
ing with word formation (splitting portmanteau
prepositions or compounds). This needs to take
into account translation-relevant features (e.g. num-
ber) which vary across the different language pairs:
while French only has the features number and
gender, a wider array of features needs to be con-
sidered when modelling Russian (cf. table 6). In
addition to morphological reduction, we also apply
transliteration models learned from automatically
1The language pairs DE-EN and RU-EN were developed
in collaboration with the Qatar Computing Research Institute
and the University of Szeged.
mined transliterations to handle out-of-vocabulary
words (OOVs) when translating from Russian.
Replacing inflected word forms with simpler
variants (lemmas or the components of split com-
pounds) aims not only at reducing the general com-
plexity of the translation model, but also at decreas-
ing the amount of out-of-vocabulary words in the
input data. This is particularly the case with Ger-
man compounds, which are very productive and
thus often lack coverage in the parallel training
data, whereas the individual components can be
translated. Similarly, inflected word forms (e.g. ad-
jectives) benefit from the reduction to lemmas if
the full inflection paradigm does not occur in the
parallel training data.
For EN-FR, a translation pair with a morpho-
logically complex target language, we describe a
two-step translation system built on non-inflected
word stems with a post-processing component for
predicting morphological features and the genera-
tion of inflected forms. In addition to the advantage
of a more general translation model, this method
also allows the generation of inflected word forms
which do not occur in the training data.
2 Experimental setup
The translation experiments in this paper are car-
ried out with either a standard phrase-based Moses
system (DE-EN, EN-DE, EN-FR and FR-EN) or
with an operation sequence model (RU-EN, DE-
EN), cf. Durrani et al (2013b) for more details.
An operation sequence model (OSM) is a state-
of-the-art SMT-system that learns translation and
reordering patterns by representing a sentence pair
and its word alignment as a unique sequence of
operations (see e.g. Durrani et al (2011), Durrani
et al (2013a) for more details). For the Moses sys-
tems we used the old train-model perl scripts rather
than the EMS, so we did not perform Good-Turing
smoothing; parameter tuning was carried out with
batch-mira (Cherry and Foster, 2012).
232
1 Removal of empty lines
2 Conversion of HTML special characters like
&quot; to the corresponding characters
3 Unification of words that were written both
with an ? or with an oe to only one spelling
4 Punctuation normalization and tokenization
5 Putting together clitics and apostrophes like
l ? or d ? to l? and d?
Table 1: Text normalization for FR-EN.
Definite determiners la / l? / les ? le
Indefinite determiners un / une ? un
Adjectives Infl. form ? lemma
Portmanteaus e. g. au ? a` le
Verb participles Reduced to
inflected for gender non-inflected
and number verb participle form
ending in e?e/e?s/e?es ending in e?
Clitics and apostroph- d? ? de,
ized words are converted qu? ? que,
to their lemmas n? ? ne, ...
Table 2: Rules for morphological simplification.
The development data consists of the concate-
nated news-data sets from the years 2008-2011.
Unless otherwise stated, we use all constrained data
(parallel and monolingual). For the target-side lan-
guage models, we follow the approach of Schwenk
and Koehn (2008) and train a separate language
model for each corpus and then interpolate them
using weights optimized on development data.
3 French to English
French has a much richer morphology than English;
for example, adjectives in French are inflected with
respect to gender and number whereas adjectives
in English are not inflected at all. This causes data
sparsity in coverage of French inflected forms. We
try to overcome this problem by simplifying French
inflected forms in a pre-processing step in order to
adapt the French input better to the English output.
Processing of the training and test data The
pre-processing of the French input consists of two
steps: (1) normalizing not well-formed data (cf.
table 1) and (2) morphological simplification.
In the second step, the normalized training data
is annotated with Part-of-Speech tags (PoS-tags)
and word lemmas using RFTagger (Schmid and
Laws, 2008) which was trained on the French tree-
bank (Abeille? et al, 2003). French forms are then
simplified according to the rules given in table 2.
Data and experiments We trained a French to
English Moses system on the preprocessed and
System BLEU (cs) BLEU (ci)
Baseline 29.90 31.02
Simplified French* 29.70 30.83
Table 3: Results of the French to English system
(WMT-2012). The marked system (*) corresponds
to the system submitted for manual evaluation. (cs:
case-sensitive, ci: case-insensitive)
simplified constrained parallel data.
Due to tractability problems with word align-
ment, the 109 French-English corpus and the UN
corpus were filtered to a more manageable size.
The filtering criteria are sentence length (between
15 and 25 words), as well as strings indicating that
a sentence is neither French nor English, or other-
wise not well-formed, aiming to obtain a subset of
good-quality sentences. In total, we use 9M par-
allel sentences. For the English language model
we use large training data with 287.3M true-cased
sentences (including the LDC Giga-word data).
We compare two systems: a baseline with reg-
ular French text, and a system with the described
morphological simplifications. Results for the
WMT-2012 test set are shown in table 3. Even
though the baseline is better than the simplified
system in terms of BLEU, we assume that the trans-
lation model of the simplified system benefits from
the overall generalization ? thus, human annotators
might prefer the output of the simplified system.
For the WMT-2013 set, we obtain BLEU scores
of 29,97 (cs) and 31,05 (ci) with the system built
on simplified French (mes-simplifiedfrench).
4 English to French
Translating into a morphologically rich language
faces two problems: that of asymmetry of mor-
phological information contained in the source and
target language and that of data sparsity.
In this section we describe a two-step system de-
signed to overcome these types of problems: first,
the French data is reduced to non-inflected forms
(stems) with translation-relevant morphological fea-
tures, which is used to built the translation model.
The second step consists of predicting all neces-
sary morphological features for the translation out-
put, which are then used to generate fully inflected
forms. This two-step setup decreases the complex-
ity of the translation task by removing language-
specific features from the translation model. Fur-
thermore, generating inflected forms based on word
stems and morphological features allows to gener-
233
ate forms which do not occur in the parallel training
data ? this is not possible in a standard SMT setup.
The idea of separating the translation into two
steps to deal with complex morphology was in-
troduced by Toutanova et al (2008). Fraser et
al. (2012) applied this method to the language
pair English-German with an additional special
focus on word formation issues such as the split-
ting and merging of portmanteau prepositions and
compounds. The presented inflection prediction
systems focuses on nominal inflection; verbal in-
flection is not addressed.
Morphological analysis and resources The
morphological analysis of the French training data
is obtained using RFTagger, which is designed
for annotating fine-grained morphological tags
(Schmid and Laws, 2008). For generating inflected
forms based on stems and morphological features,
we use an extended version of the finite-state mor-
phology FRMOR (Zhou, 2007). Additionally, we
use a manually compiled list of abbreviations and
named entities (names of countries) and their re-
spective grammatical gender.
Stemming For building the SMT system, the
French data (parallel and monolingual) is trans-
formed into a stemmed representation. Nouns,
i.e. the heads of NPs or PPs, are marked with
inflection-relevant features: gender is considered
as part of the stem, whereas number is determined
by the source-side input: for example, we expect
source-language words in plural to be translated by
translated by stems with plural markup. This stem-
markup is necessary in order to guarantee that the
number information is not lost during translation.
For a better generalization, portmanteaus are split
into separate parts: au? a`+le (meaning, ?to the?).
Predicting morphological features For predict-
ing the morphological features of the SMT output
(number and gender), we use a linear chain CRF
(Lavergne et al, 2010) trained on data annotated
with these features using n-grams of stems and part-
of-speech tags within a window of 4 positions to
each side of the current word. Through the CRF,
the values specified in the stem-markup (number
and gender on nouns) are propagated over the rest
of the linguistic phrase, as shown in column 2 of
table 4. Based on the stems and the morphological
features, inflected forms can be generated using
FRMOR (column 3).
Post-processing As the French data has been
normalized, a post-processing step is needed in or-
der to generate correct French surface forms: split
portmanteaus are merged into their regular forms
based on a simple rule set. Furthermore, apostro-
phes are reintroduced for words like le, la, ne, ... if
they are followed by a vowel. Column 4 in table 4
shows post-processing including portmanteau for-
mation. Since we work on lowercased data, an
additional recasing step is required.
Experiments and evaluation We use the same
set of reduced parallel data as the FR-EN system;
the language model is built on 32M French sen-
tences. Results for the WMT-2012 test set are given
in table 5. Variant 1 shows the results for a small
system trained only on a part of the training data
(Europarl+News Commentary), whereas variant 2
corresponds to the submitted system. A small-scale
analysis indicated that the inflection prediction sys-
tem tends to have problems with subject-verb agree-
ment. We trained a factored system using addi-
tional PoS-tags with number information which
lead to a small improvement on both variants.
While the small model is significantly better than
the baseline2 as it benefits more from the general-
ization, the result for the full system is worse than
the baseline3. Here, given the large amount of
data, the generalization effect has less influence.
However, we assume that the more general model
from the inflection prediction system produces bet-
ter translations than a regular model containing a
large amount of irrelevant inflectional information,
particularly when considering that it can produce
well-formed inflected sequences that are inaccessi-
ble to the baseline. Even though this is not reflected
in terms of BLEU, humans might prefer the inflec-
tion prediction system.
For the WMT-2013 set, we obtain BLEU scores
of 29.6 (ci) and 28.30 (cs) with the inflection pre-
diction system mes-inflection (marked in table 5).
5 Russian-English
The preparation of the Russian data includes the
following stages: (1) tokenization and tagging and
(2) morphological reduction.
Tagging and tagging errors For tagging, we use
a version of RFTagger (Schmid and Laws, 2008)
2Pairwise bootstrap resampling with 1000 samples.
3However, the large inflection-prediction system has a
slightly better NIST score than the baseline (7.63 vs. 7.61).
234
SMT-output predicted generated after post- gloss
with stem-markup in bold print features forms processing
avertissement<Masc><Pl>[N] Masc.Pl avertissements avertissements warnings
sinistre[ADJ] Masc.Pl sinistres sinistres dire
de[P] ? de du from
le[ART] Masc.Sg le the
pentagone<Masc><Sg>[N] Masc.Sg pentagone pentagone pentagon
sur[P] ? sur sur over
de[P] ? de d? of
e?ventuel[ADJ] Fem.Pl e?ventuelles e?ventuelles potential
re?duction<Fem><Pl>[N] Fem.Pl re?ductions re?ductions reductions
de[P] ? de du of
le[ART] Masc.Sg le the
budget<Masc><Sg>[N] Masc.Sg budget budget budget
de[P] ? de de of
le[ART] Fem.Sg la la the
de?fense<Fem><Sg>[N] Fem.Sg de?fense de?fense de?fense
Table 4: Processing steps for the input sentence dire warnings from pentagon over potential defence cuts.
that has been developed based on data tagged with
TreeTagger (Schmid, 1994) using a model from
Sharoff et al (2008). The data processed by Tree-
Tagger contained errors such as wrong definition
of PoS for adverbs, wrong selection of gender for
adjectives in plural and missing features for pro-
nouns and adverbs. In order to train RFTagger, the
output of TreeTagger was corrected with a set of
empirical rules. In particular, the morphological
features of nominal phrases were made consistent
to train RFTagger: in contrast to TreeTagger, where
morphological features are regarded as part of the
PoS-tag, RFTagger allows for a separate handling
of morphological features and POS tags.
Despite a generally good tagging quality, some
errors seem to be unavoidable due to the ambiguity
of certain grammatical forms in Russian. A good
example of this are neuter nouns that have the same
form in all cases, or feminine nouns, which have
identical forms in singular genitive and plural nom-
inative (Sharoff et al, 2008). Since Russian has no
binding word order, and the case of nouns cannot
be determined on that basis, such errors cannot be
corrected with empirical rules implemented as post-
System BLEU (ci) BLEU (cs)
1 Baseline 24.91 23.40
InflPred 25.31 23.81
InflPred-factored 25.53 24.04
2 Baseline 29.32 27.65
InflPred* 29.07 27.40
InflPred-factored 29.17 27.46
Table 5: Results for French inflection prediction
on the WMT-2012 test set. The marked system (*)
corresponds to the system submitted for manual
evaluation.
processing. Similar errors occur when specifying
the case of adjectives, since the suffixes of adjec-
tives are even less varied as compared to the nouns.
In our application, we hope that this type of error
does not affect the result due to the following sup-
pression of a number of morphological attributes
including the case of adjectives.
Morphological reduction In comparison to
Slavic languages, English is morphologically poor.
For example, English has no morphological at-
tributes for nouns and adjectives to express gender
or case; verbs have no gender either. In contrast,
Russian is morphologically very rich ? there are
e.g. 6 cases and 3 grammatical genders, which
manifest themselves in different suffixes for nouns,
pronouns, adjectives and some verb forms. When
translating from Russian into English, many of
these attributes are (hopefully) redundant and are
therefore deleted from the training data. The mor-
phological reduction in our system was applied to
nouns, pronouns, verbs, adjectives, prepositions
and conjunctions. The rest of the POS (adverbs,
particles, interjections and abbreviations) have no
morphological attributes. The list of the original
and the reduced attributes is given in Table 6.
Transliteration mining to handle OOVs The
machine translation system fails to translate out-of-
vocabulary words (OOVs) as they are unknown to
the training data. Most of the OOVs are named en-
tities and transliterating them to the target language
script could solve this problem. The transliteration
system requires a list of transliteration pairs for
training. As we do not have such a list, we use
the unsupervised transliteration mining system of
Sajjad et al (2012) that takes a list of word pairs for
235
Part of Attributes Reduced
Speech RFTagger attributes
Noun Type Type
Gender Gender
Number Number
Case Case
nom,gen,dat,acc,instr,prep gen,notgen
Animate
Case 2
Pronoun Person Person
Gender Gender
Number Number
Case Case
nom,gen,dat,acc,instr,prep nom,notnom
Syntactic type
Animated
Verb Type Type
VForm VForm
Tense Tense
Person Person
Number Number
Gender
Voice Voice
Definiteness
Aspect Aspect
Case
Adjec- Type Type
tive Degree Degree
Gender
Number
Case
Definiteness
Prep- Type
osition Formation
Case
Conjunc- Type Type
tion Formation Formation
Table 6: Rules for simplifying the morphological
complexity for RU.
training and extracts transliteration pairs that can
be used for the training of the transliteration system.
The procedure of mining transliteration pairs and
transliterating OOVs is described as follows: We
word-align the parallel corpus using GIZA++ and
symmetrize the alignments using the grow-diag-
final-and heuristic. We extract all word pairs which
occur as 1-to-1 alignments (Sajjad et al, 2011) and
later refer to them as a list of word pairs. We train
the unsupervised transliteration mining system on
the list of word pairs and extract transliteration
pairs. We use these mined pairs to build a transliter-
ation system using the Moses toolkit. The translit-
eration system is applied as a post-processing step
to transliterate OOVs.
The morphological reduction of Russian (cf. sec-
tion 5) does not process most of the OOVs as they
are also unknown to the POS tagger. So OOVs that
we get are in their original form. When translit-
Original corpus
SYS WMT-2012 WMT-2013
GIZA++ 32.51 25.5
TA-GIZA++ 33.40 25.9*
Morph-reduced
SYS WMT-2012 WMT-2013
GIZA++ 31.22 24.3
TA-GIZA++ 31.40 24.45
Table 7: Russian to English machine translation
system evaluated on WMT-2012 and WMT-2013.
Human evaluation in WMT13 is performed on the
system trained using the original corpus with TA-
GIZA++ for alignment (marked with *).
erating them, the inflected forms generate wrong
English transliterations as inflectional suffixes get
transliterated too, specially OOV named entities.
We solved this problem by stemming the OOVs
based on a list of suffixes ( , , , , , ) and
transliterating the stemmed forms.
Experiments and results We trained the sys-
tems separately on GIZA++ and transliteration
augmented-GIZA++ (TA-GIZA++) to compare
their results; for more details see Sajjad et al
(2013). All systems are tuned using PROv1 (Nakov
et al, 2012). The translation output is post-
processed to transliterate OOVs.
Table 7 summarizes the results of RU-EN trans-
lation systems trained on the original corpus and
on the morph-reduced corpus. Using TA-GIZA++
alignment gives the best results for both WMT-
2012 and WMT-2013, leading to an improvement
of 0.4 BLEU points.
The system built on the morph-reduced data
leads to decreased BLEU results. However, the per-
centage of OOVs is reduced for both test sets when
using the morph-reduced data set compared to the
original data. An analysis of the output showed
that the morph-reduced system makes mistakes in
choosing the right tense of the verb, which might
be one reason for this outcome. In the future, we
would like to investigate this issue in detail.
6 German to English and English to
German
We submitted systems for DE-EN and EN-DE
which used constituent parses for pre-reordering.
For DE-EN we also deal with word formation is-
sues such as compound splitting. We did not per-
form inflectional normalization or generation for
German due to time constraints, instead focusing
236
our efforts on these issues for French and Russian
as previously described.
German to English German has a wider diver-
sity of clausal orderings than English, all of which
need to be mapped to the English SVO order. This
is a difficult problem to solve during inference, as
shown for hierarchical SMT by Fabienne Braune
and Fraser (2012) and for phrase-based SMT by
Bisazza and Federico (2012).
We syntactically parsed all of the source side
sentences of the parallel German to English data
available, and the tuning, test and blindtest sets.
We then applied reordering rules to these parses.
We use the rules for reordering German constituent
parses of Collins et al (2005) together with the
additional rules described by Fraser (2009). These
are applied as a preprocess to all German data.
For parsing the German sentences, we used the
generative phrase-structure parser BitPar with opti-
mizations of the grammar, as described by Fraser
et al (2013). The parser was trained on the Tiger
Treebank (Brants et al, 2002) along with utilizing
the Europarl corpus as unlabeled data. At the train-
ing of Bitpar, we followed the targeted self-training
approach (Katz-Brown et al, 2011) as follows. We
parsed the whole Europarl corpus using a grammar
trained on the Tiger corpus and extracted the 100-
best parse trees for each sentence. We selected the
parse tree among the 100 candidates which got the
highest usefulness scores for the reordering task.
Then we trained a new grammar on the concatena-
tion of the Tiger corpus and the automatic parses
from Europarl.
The usefulness score estimates the value of a
parse tree for the reordering task. We calculated
this score as the similarity between the word order
achieved by applying the parse tree-based reorder-
ing rules of Fraser (2009) and the word order indi-
cated by the automatic word alignment between
the German and English sentences in Europarl.
We used the Kendall?s Tau Distance as the simi-
larity metric of two word orderings (as suggested
by Birch and Osborne (2010)).
Following this, we performed linguistically-
informed compound splitting, using the system of
Fritzinger and Fraser (2010), which disambiguates
competing analyses from the high-recall Stuttgart
Morphological Analyzer SMOR (Schmid et al,
2004) using corpus statistics. We also split German
portmanteaus like zum? zu dem (meaning to the).
system BLEU BLEU system name
(ci) (cs)
DE-EN (OSM) 27.60 26.12 MES
DE-EN (OSM) 27.48 25.99 not submitted
BitPar not self-trained
DE-EN (Moses) 27.14 25.65 MES-Szeged-
reorder-split
DE-EN (Moses) 26.82 25.36 not submitted
BitPar not self-trained
EN-DE (Moses) 19.68 18.97 MES-reorder
Table 8: Results on WMT-2013 (blindtest)
English to German The task of mapping En-
glish SVO order to the different clausal orders in
German is difficult. For our English to German
systems, we solved this by parsing the English and
applying the system of Gojun and Fraser (2012) to
reorder English into the correct German clausal or-
der (depending on the clause type which is detected
using the English parse, see (Gojun and Fraser,
2012) for further details).
We primarily used the Charniak-Johnson gener-
ative parser (Charniak and Johnson, 2005) to parse
the English Europarl data and the test data. How-
ever, due to time constraints we additionally used
Berkeley parses of about 400K Europarl sentences
and the other English parallel training data. We
also left a small amount of the English parallel
training data unparsed, which means that it was
not reordered. For tune, test and blindtest (WMT-
2013), we used the Charniak-Johnson generative
parser.
Experiments and results We used all available
training data for constrained systems; results for
the WMT-2013 set are given in table 8. For the
contrastive BitPar results, we reparsed WMT-2013.
7 Conclusion
We presented 5 systems dealing with complex mor-
phology. For two language pairs with a morpho-
logically rich source language (FR and RU), the
input was reduced to a simplified representation
containing only translation-relevant morphologi-
cal information (e.g. number on nouns). We also
used reordering techniques for DE-EN and EN-DE.
For translating into a language with rich morphol-
ogy (EN-FR), we applied a two-step method that
first translates into a stemmed representation of
the target language and then generates inflected
forms based on morphological features predicted
on monolingual data.
237
Acknowledgments
We would like to thank the anonymous reviewers
for their helpful feedback and suggestions, Daniel
Quernheim for providing Berkeley parses of some
of the English data, Stefan Ru?d for help with the
manual evalution, and Philipp Koehn and Barry
Haddow for providing data and alignments.
Nadir Durrani was funded by the European
Union Seventh Framework Programme (FP7/2007-
2013) under grant agreement n. 287658. Alexan-
der Fraser was funded by Deutsche Forschungs-
gemeinschaft grant Models of Morphosyntax for
Statistical Machine Translation and from the Eu-
ropean Community?s Seventh Framework Pro-
gramme (FP7/2007-2013) under Grant Agreement
n. 248005. Marion Weller was funded from the
European Community?s Seventh Framework Pro-
gramme (FP7/2007-2013) under Grant Agreement
n. 248005. Svetlana Smekalova was funded by
Deutsche Forschungsgemeinschaft grant Models
of Morphosyntax for Statistical Machine Trans-
lation. Helmut Schmid and Max Kisselew were
supported by Deutsche Forschungsgemeinschaft
grant SFB 732. Richa?rd Farkas was supported by
the European Union and the European Social Fund
through project FuturICT.hu (grant n. TA?MOP-
4.2.2.C-11/1/KONV-2012-0013). This publication
only reflects the authors? views.
References
A. Abeille?, L. Cle?ment, and F. Toussenel. 2003. Build-
ing a treebank for french. In A. Abeille?, editor, Tree-
banks. Kluwer, Dordrecht.
Alexandra Birch and Miles Osborne. 2010. Lrscore for
evaluating lexical and reordering quality in mt. In
Proceedings of ACL WMT and MetricsMATR, Upp-
sala, Sweden.
Arianna Bisazza and Marcello Federico. 2012. Mod-
ified distortion matrices for phrase-based statistical
machine translation. In ACL, pages 478?487.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In ACL, pages 173?180, Ann Arbor, MI,
June. Association for Computational Linguistics.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL).
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Porceedings of ACL 2005.
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A Joint Sequence Translation Model with In-
tegrated Reordering. In Proceedings of ACL-HLT
2011, Portland, Oregon, USA.
Nadir Durrani, Alexander Fraser, and Helmut Schmid.
2013a. Model With Minimal Translation Units, But
Decode With Phrases. In Proceedings of NAACL
2013, Atlanta, Georgia, USA.
Nadir Durrani, Helmut Schmid, Alexander Fraser, Has-
san Sajjad, and Richa?rd Farkas. 2013b. Munich-
Edinburgh-Stuttgart Submissions of OSM Systems
at WMT13. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, Sofia, Bulgaria.
Anita Gojun Fabienne Braune and Alexander Fraser.
2012. Long-distance reordering during search for
hierarchical phrase-based SMT. In Proceedings of
EAMT 2012.
Alexander Fraser, Marion Weller, Aoife Cahill, and Fa-
bienne Cap. 2012. Modeling Inflection and Word-
Formation in SMT. In Proceedings of EACL 2012,
Avignon, France.
Alexander Fraser, Helmut Schmid, Richa?rd Farkas,
Renjing Wang, and Hinrich Schu?tze. 2013. Knowl-
edge sources for constituent parsing of German, a
morphologically rich and less-configurational lan-
guage. Computational Linguistics - to appear.
Alexander Fraser. 2009. Experiments in morphosyn-
tactic processing for translating to and from German.
In EACL WMT.
Fabienne Fritzinger and Alexander Fraser. 2010. How
to avoid burning ducks: Combining linguistic analy-
sis and corpus statistics for German compound pro-
cessing. In ACL WMT and Metrics MATR.
Anita Gojun and Alexander Fraser. 2012. Determin-
ing the placement of German verbs in English-to-
German SMT. In Proceedings of EACL 2012.
Jason Katz-Brown, Slav Petrov, Ryan McDon-
ald, Franz Och, David Talbot, Hiroshi Ichikawa,
Masakazu Seno, and Hideto Kazawa. 2011. Train-
ing a parser for machine translation reordering. In
Proceedings of EMNLP 2011, Edinburgh, Scotland.
Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.
2010. Practical very large scale CRFs. In Proceed-
ings of ACL 2010, pages 504?513.
Preslav Nakov, Francisco Guzma?n, and Stephan Vo-
gel. 2012. Optimizing for sentence-level BLEU+1
yields short translations. Mumbai, India.
238
Hassan Sajjad, Alexander Fraser, and Helmut Schmid.
2011. An algorithm for unsupervised transliteration
mining with an application to word alignment. In
Proceedings of ACL 2011, Portland, USA.
Hassan Sajjad, Alexander Fraser, and Helmut Schmid.
2012. A statistical model for unsupervised and semi-
supervised transliteration mining. In Proceedings of
ACL 2012, Jeju, Korea.
Hassan Sajjad, Svetlana Smekalova, Nadir Durrani,
Alexander Fraser, and Helmut Schmid. 2013.
QCRI-MES Submission at WMT13: Using Translit-
eration Mining to Improve Statistical Machine
Translation. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, Sofia, Bulgaria.
Helmut Schmid and Florian Laws. 2008. Estimation
of conditional probabilities with decision trees and
an application to fine-grained pos tagging. In Pro-
ceedings of COLING 2008, Stroudsburg, PA, USA.
Helmut Schmid, Arne Fitschen, and Ulrich Heid. 2004.
SMOR: a German Computational Morphology Cov-
ering Derivation, Composition, and Inflection. In
Proceedings of LREC 2004.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing.
Holger Schwenk and Philipp Koehn. 2008. Large
and diverse language models for statistical machine
translation. In Proceedings of IJCNLP 2008.
Serge Sharoff, Mikhail Kopotev, Tomaz Erjavec, Anna
Feldman, and Dagmar Divjak. 2008. Designing and
evaluating russian tagsets. In Proceedings of LREC
2008.
Kristina Toutanova, Hisami Suzuki, and Achim Ruopp.
2008. Applying Morphology Generation Models to
Machine Translation. In Proceedings of ACL-HLT
2008.
Zhenxia Zhou. 2007. Entwicklung einer franzo?sischen
Finite-State-Morphologie. Diploma Thesis, Insti-
tute for Natural Language Processing, University of
Stuttgart.
239
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 71?78,
Baltimore, Maryland USA, June 26?27, 2014. c?2014 Association for Computational Linguistics
CimS ? The CIS and IMS joint submission to WMT 2014
translating from English into German
Fabienne Cap_, Marion Weller_f, Anita Rammf, Alexander Fraser_
_ CIS, Ludwig-Maximilian University of Munich ? (cap|fraser)@cis.uni-muenchen.de
f IMS, University of Stuttgart ? (wellermn|ramm)@ims.uni-stuttgart.de
Abstract
We present the CimS submissions to the
2014 Shared Task for the language pair
EN?DE. We address the major problems
that arise when translating into German:
complex nominal and verbal morphol-
ogy, productive compounding and flex-
ible word ordering. Our morphology-
aware translation systems handle word
formation issues on different levels of
morpho-syntactic modeling.
1 Introduction
In our shared task submissions, we focus on the
English to German translation direction: we ad-
dress different levels of productivity of the Ger-
man language, i.e., nominal and verbal inflec-
tion and productive word formation, which lead
to data sparsity and thus confuse classical SMT
systems.
Our basic goal is to make the two languages
as morphosyntactically similar as possible. We
use a parser and a morphological analyser to re-
move linguistic features from German that are
not present in English and reorder the English
input to make it more similar to the German sen-
tence structure. Prior to training, all words are
lemmatised and compounds are split into single
words. This is not only beneficial for word align-
ment, but it also allows us to generalise over in-
flectional variants of the same lexemes and over
single words which could occur in one place as a
standalone word and in another place as part of
a compound. Translation happens in two steps:
first, we translate from English into split, lemma-
tised German and then, we perform compound
merging and generation of inflection as a post-
processing step. This way, we are able to cre-
ate German compounds and inflectional vari-
ants that have not been seen in the parallel train-
ing data.
In this paper, we investigate the performance of
well-established source-side reordering, nomi-
nal re-inflection and compound processing sys-
tems on an up-to-date shared task. In addition,
we present experimental results on a verbal in-
flection component and a syntax-based variant
including source-side reordering.
2 Related Work
Re-Inflection The two-step translation ap-
proach we use was described by e.g. Toutanova
et al. (2008) and Jeong et al. (2010), who use
a number of morphological and syntactic
features derived from both source and target
language. More recently, Fraser et al. (2012)
describe a similar approach for German using
different CRF-based feature prediction models,
one for each of the four grammatical features
to be predicted for German words in noun
phrases, namely number, gender, case and
definiteness. This approach also handles word-
formation issues such as portmanteau splitting
and compounding. Weller et al. (2013) added
subcategorization information in combination
with source-side syntactic features in order to
improve the prediction of case.
De Gispert and Mari?o (2008) generate verbal
inflection for translation from English into Span-
ish. They use classifiers trained not only on tar-
get language but also on source language fea-
tures, which is even more crucial for the predic-
tion of verbs than it is for nominal inflection.
More recently, Williams and Koehn (2011)
translate directly into target language surface
forms. Agreement within NPs and PPs, and also
between subject and verb is considered during
the decoding process: they use string-to-tree
translation, where the target language (German)
morphology is expressed as a set of unification
constraints automatically learned from a mor-
phologically annotated German corpus.
71
Compound Processing Compound splitting
for SMT has been addressed by numerous dif-
ferent groups, for translation from German
to English, e.g. using corpus-based frequen-
cies (Koehn and Knight, 2003), using POS-
constraints (Stymne et al., 2008), a lattice-based
approach propagating the splitting decision to
the decoder (Dyer, 2009), a rule-based morpho-
logical analyser (Fritzinger and Fraser, 2010) or
unsupervised, language-independent segmen-
tation (Macherey et al., 2011).
Compound processing in the other translation
direction, however, has been much less investi-
gated. Popovic? et al. (2006) describe a list-based
approach, in which words are only re-combined
if they have been seen as compounds in a huge
corpus. However this approach is limited to
the list?s coverage. The approach of Stymne
(2009) overcomes this coverage issue by mak-
ing use of a POS-markup which distinguishes
former compound modifiers from former heads
and thus allows for their adequate recombina-
tion after translation. An extension of this ap-
proach is reported in Stymne and Cancedda
(2011) where a CRF-model is used for compound
prediction. In Cap et al. (2014) their approach
is extended through using source-language fea-
tures and lemmatisation, allowing for maximal
generalisation over compound parts.
Source-side Reordering One major problem in
English to German translation is the divergent
clausal ordering: in particular, German verbs
tend to occur at the very end of clauses, whereas
English sticks to a rigid SVO order in most cases.
Collins et al. (2005), Fraser (2009) and Gojun
and Fraser (2012) showed that restructuring the
source language so that it corresponds to the ex-
pected structure of the target language is helpful
for SMT.
3 Inflection Prediction
German has a rich morphology, both for nom-
inal and verbal inflection. It requires differ-
ent forms of agreement, e.g., for adjectives and
nouns or verbs and their subjects. Traditional
phrase-based SMT systems often get such agree-
ments wrong. In our systems, we explicitly
model agreement using a two-step approach:
first we translate from English into lemmatised
German and then generate fully inflected forms
in a second step. In this section, we describe our
nominal inflection component and first experi-
mental steps towards verbal re-inflection.
3.1 Noun Phrase Inflection
Prior to training, the German data is re-
duced to a lemmatised representation contain-
ing translation-relevant morphological features.
For nominal inflection, the lemmas are marked
with number and gender: gender is considered
as part of the lemma, whereas number is indi-
rectly determined by the source-side, as we ex-
pect nouns to be translated with their appro-
priate number value. We use a linear chain
CRF (Lafferty et al., 2001) to predict the mor-
phological features (number, gender, case and
strong/weak). The features that are part of the
lemma of nouns (number, gender) are propa-
gated over the rest of the linguistic phrase. In
contrast, case depends on the role of the NP in
the sentence (e.g. subject or direct/indirect ob-
ject) and is thus to be determined entirely from
the respective context in the sentence. The value
for strong/weak depends on the combination of
the other features. Based on the lemma and the
predicted features, inflected forms are then gen-
erated using the rule-based morphological anal-
yser SMOR (Schmid et al., 2004). This system is
described in more detail in Fraser et al. (2012).
3.2 Verbal Inflection
German verbs agree in number and person with
their subjects. We thus have to derive this in-
formation from a noun phrase in nominative
case (= the subject) near the verb. This informa-
tion comes from the nominal inflection predic-
tion described in section 3.1. We predict tense
and mode of the verb using a maximum-entropy
classifier which is trained on English and Ger-
man contextual information. After deriving all
information needed for the generation of the
verbs, the inflected forms are generated with
SMOR.
4 Compound Processing
In English to German translation, compound
processing is more difficult than in the oppo-
site direction. Not only do compounds have to
be split accurately, but they also have to be put
together correctly after decoding. The disflu-
ency of MT output and the difficulty of deciding
which single words should be merged into com-
pounds make this task even more challenging.
72
(split+lem.)
Training
Parallel Training Data
LanguageModel
TranslationModel
English text
....
....
....
Target Training Data
....
....
....
....
....
....
German text
German text
....
....
....
tooltradefruit box Werkzeug KisteHandelObst
Werkzeug KisteObst Handel
Parallel Training Data
....
....
....
....
....
....
ObsthandelWerkzeugkiste
Target Training Data
German text
Pre?Processing
Obsthandel....
....
....
Werkzeugkiste
....
....
....
tool boxfruit trade
English text German text
Post?processing 
....
....ObstkisteObsthandel
German(fluent)
Testing
inputEnglish
....fruit tradefruit box
(split+lem.)
(split+lem.)
lemmatisesplit
splitlemmatise ....
....
RecombineRe?inflect
German
(split+lem.)
....
output
Obst KisteObst Handel
Decoder
Figure 1: Pipeline overview of our primary CimS-CoRI system.
We combine compound processing with in-
flection prediction (see Section 3) and thus ex-
tend the two-step approach respectively: com-
pounds are split and lemmatised simultane-
ously, again using SMOR. This allows for maxi-
mal generalisation over former compound parts
and independently occurring simple words. We
use this split representation for training. Af-
ter decoding, we re-combine words into com-
pounds again, using our extended CRF-based
approach, which is based on Stymne and Can-
cedda (2011), but includes source-language fea-
tures and allows for maximal generalisation
through lemmatisation. More details can be
found in Cap et al. (2014). We then use SMOR
to generate sound German compounds (includ-
ing morphological transformations such as in-
troduction or deletion of filler letters). Finally,
the whole text including the newly-created com-
pounds, is re-inflected using the nominal in-
flection prediction models as described in Sec-
tion 3.1 above. This procedure allows us to create
compounds that have not been seen in the par-
allel training data, and also inflectional variants
of seen compounds. See Figure 1 for an overview
of our compound processing pipeline.
4.1 Portmanteaus
Portmanteaus are a special kind of compound.
They are a fusion of a preposition and a defi-
nite article (thus not productive) and their case
must agree with the case of the noun. For ex-
ample, ?zum? can be split into ?zu? + ?dem? =
to+theDati ve . They introduce additional spar-
sity to the training data: imagine a noun oc-
curred with its definite article in the training
data, but not with the portemanteau required at
testing time. Splitting portemanteaus allows a
phrase-based SMT system to access phrases cov-
ering nouns and their corresponding definite ar-
ticles. In a post-processing step, definite articles
are then re-merged with their preceding prepo-
sitions to restore the original portmanteau, see
(Fraser et al., 2012) for details. This generalisa-
tion effect is even larger as we not only split port-
manteaus, but also lemmatise the articles.
5 System descriptions
Our shared task submissions include different
combinations of the inflection and compound
processing procedures as described in the pre-
vious two sections. We give an overview of all
our systems in Table 1. Note that we did not
re-train the compound processing CRFs on the
new dataset, but used our models trained on the
2009 training data instead. However, this does
not hurt performance, as the CRF we use is not
trained on surface forms, but only frequencies
and source-side features instead. See (Fraser et
al., 2012) and (Cap et al., 2014) for more details
on how we trained the respective CRFs. In con-
trast, the verbal classifier has been trained on
WMT 2014 data.
6 Experimental Settings
In all our systems, we only used data distributed
for the shared task. All available German data
was morphologically analysed with SMOR. For
lemmatisation of the German training data, we
disambiguated SMOR using POS tags we ob-
tained through parsing the German section of
the parallel training data with BitPar (Schmid,
73
No. apprart nominal compound verbal source-sidesplitting inflection processing inflection reordering
CimS-RI X X
CimS-CoRIP X X X
CimS-RIVe X X X
CimS-CoRIVe X X X X
CimS-Syntax-RORI X X X
Table 1: Overview of our submission systems.RI = nominal Re-Inflection, Co = Compound process-
ing, Ve = Verbal inflection, RO = source-side Re-Ordering. Syntax = syntax-based SMT P = primary
submission.
2004) and tagging the big monolingual training
data using RFTagger (Schmid and Laws, 2008)1.
Note that we did not normalise German lan-
guage e.g. with respect to old vs. new writing
convention etc. as we did in previous submis-
sions (e.g. (Fraser, 2009)).
For the compound prediction CRFs using syn-
tactic features derived from the source language,
we parsed the English section of the parallel
data using EGRET, a re-implementation of the
Berkeley-Parser by Hui Zhang2. Before training
our models on the English data, we normalised
all occurrences of British vs. American English
variants to British English. We did so for train-
ing, tuning and testing input.
Language Model We trained 5-gram language
models based on all available German monolin-
gual training data from the shared task (roughly
1.5 billion words) using the SRILM toolkit (Stol-
cke, 2002) with Kneser-Ney smoothing. We then
used KenLM (Heafield, 2011) for faster process-
ing. For each of our experiments, we trained
a separate language model on the whole data
set, corresponding to the different underspeci-
fied representations of German used in our ex-
periments, e.g. lemmatised for CimS-RI, lemma-
tised with split compounds for CimS-CoRI, etc.
Phrase-based Translation model We per-
formed word alignment using the multithreaded
GIZA++ toolkit (Och and Ney, 2003; Gao and
Vogel, 2008). For translation model training and
decoding, we used the Moses toolkit (Koehn
et al., 2007) to build phrase-based statistical
machine translation systems, following the
instructions for the baseline system for the
shared task, using only default settings.
1We could not parse the whole monolingual dataset due
to time-constraints and thus used RFTagger as a substitute.
2available from https://sites.google.com/
site/zhangh1982/egret.
Syntax-based Translation model As a variant
to the phrase-based systems, we applied the in-
flection prediction system to a string-to-tree sys-
tem with GHKM extraction (Galley et al. (2004),
Williams and Koehn (2012)). We used the same
data-sets as for the phrase-based systems, and
applied BitPar (Schmid, 2004) to obtain target-
side trees. For this system, we used source-side
reordering according to Gojun and Fraser (2012)
relying on parses obtained with EGRET3.
Tuning For tuning of feature weights, we used
batch-mira with ??safe-hope? (Cherry and Foster,
2012) until convergence (or maximal 25 runs).
We used the 3,000 sentences of newstest2012 for
tuning. Each experiment was tuned separately,
optimising Bleu scores (Papineni et al., 2002)
against a lemmatised version of the tuning ref-
erence. In the compound processing systems we
integrated the CRF-based prediction and merg-
ing procedure into each tuning iteration and
scored each output against the same unsplit and
lemmatised reference as the other systems.
Testing After decoding, the underspecified
representation has to be retransformed into
fluent German text, i.e., compounds need to
be re-combined and all words have to be re-
inflected. The whole procedure can be divided
into the following steps:
1a) translation into lemmatised German
representation (RI, RIVe)
1b) translation into split and lemmatised
German (CoRi, CoRIVe)
2) compound merging (CoRI, CoRIVe):
3) nominal inflection prediction and gen-
eration of full forms using SMOR (all)
4) verbal re-inflection (RIVe, CoRIVe)
5) merging of portmanteaus (all)
3Note that we observed some data-related issues on the
Syntax-RORI experiments that we hope to resolve in the
near future.
74
Experiment mert.log Bleu ci Bleu cs Bleu ci Bleu csnews2012 news2013 news2013 news2014 news2014
raw 16.52 18.62 17.61 17.80 17.25
CimS-RI 18.51 19.23 18.38 18.33 17.75
CimS-CoRIP 18.36 19.13 18.25 18.51 17.87
CimS-RIVe 19.08 18.89 18.06 17.86 17.31
CimS-CoRIVe 18.69 18.60 17.77 17.38 16.78
CimS-Syntax-RORI 18.26 19.04 18.17 18.15 17.59
Table 2: Bleu scores for all CimS-submissions of the 2014 shared task. ci = case-insensitive, cs = case-
sensitive; P = primary submission.
After these post-processing steps, the text was
automatically recapitalised and detokenised, us-
ing the tools provided by the shared task, which
we trained on the whole German dataset. We cal-
culated Bleu (Papineni et al., 2002) scores using
the NIST script version 13a.
7 Results
We evaluated our systems with the 3,000 sen-
tences of last year?s newstest2013 and also the
2,737 sentences of the 2014 blind test set for the
German-English language pair. The Bleu scores
of our systems are given in Table 2, where raw
denotes our baseline system which we ran with-
out any pre- or postprocessing whatsoever. Note
that the big gap in mert.log scores between raw
and the CimS-systems comes from the fact that
raw is scored against the original (i.e. fully in-
flected) version of the tuning reference, while the
CimS-systems are scored against the stemmed
tuning reference.
As for the Bleu scores of the test sets, we ob-
serve similar improvements for the CimS-RI and
CimS-CoRI systems of +0.5/0.6 with respect to
the raw baseline as we did in previous experi-
ments (Cap et al., 2014)4. In contrast, our sys-
tems incorporating verbal prediction inflection
(CimS-RIVe/CoRIVe) cannot yet catch up with
the performance of the well-investigated nom-
inal inflection and compound processing sys-
tems (CimS-RI/CoRI). We attribute this partly to
the positive influence we assume fully inflected
verbs to have in nominal inflection prediction
models, but as the verb processing systems are
still under development, there might be other is-
sues we have not discovered yet. We plan to re-
4We will have a closer look at the data from a compound
processing view in Section 7.1 below.
visit these systems and improve them.
Finally, the syntax-based reordering system
yields scores that are competitive to those of
CimS-RI/CoRI. While Syntax-RORI so far only in-
corporates source-side reordering and nominal
re-inflection, we plan to investigate further ex-
tensions of this approach in the future.
7.1 Additional Evaluation
We manually screened the filtered 2014 test set
and identified 3,456 German compound tokens,
whereof 862 did not occur in the parallel training
data and thereof, 244 did not even occur in the
monolingual training data. For each of our sys-
tems, we calculated the number of compound
reference matches they produced. The results
are given in Table 3.
system ref new
raw 827 0
CimS-RI . 864 5
CimS-CoRIP 1,064 109
CimS-RIVe 853 5
CimS-CoRIVe 1,070 122
CimS-Syntax-RORI 900 20
Table 3: Numbers of compounds produced by
the systems that matched the reference (ref ) and
did not occur in the parallel training data (new).
The compound processing systems (with Co
in the name) generate many more correct com-
pounds than comparable systems without com-
pound handling. Compared to the raw base-
line, CoRI/CoRIVe did not only produce 237/243
more reference matches, but also 109/122 com-
pounds that matched the reference but did not
occur in the parallel training data. A lookup of
those 109/122 compounds in the monolingual
training data (consisting of roughly 1.5 billion
words) revealed, that 8/6 of them did not oc-
75
cur there either5. These were thus not accessi-
ble to a list-based compound merging approach
either. This result also shows that despite the
fact that CoRIVe does not yield a competitive
translation quality performance yet, the com-
pound processing component seems to bene-
fit from the verbal inflection and it is definitely
worth more investigation in the future.
Moreover, it can be seen from Table 3 that
the re-inflection systems (*RI*) produce more
reference matches than the raw baseline. In-
terestingly, they even produce some reference
matches that have not been seen in the par-
allel training data due to inflectional variation,
and in the case of the syntax-based system due
to a naive list-based compound merging: even
though it has not been trained on a split repre-
sentation of German text, it might occasionally
occur that two German nouns occur next to each
other in the MT output. If so, these two words are
merged into a compound, using a list-based ap-
proach, similar to Popovic? et al. (2006).
8 Reordering
For the system CimS-Syntax-RORI, English data
parsed with EGRET was reordered using scripts
written for parse trees produced by the con-
stituent parser (Charniak and Johnson, 2005),
using a model we trained on the standard Penn
Treebank sections. Unfortunately, the reorder-
ing scripts could not be straightforwardly ap-
plied to EGRET parses and require more signifi-
cant modifications than we first expected.
We thus decided to parse the Europarl data
(v7) with (Charniak and Johnson, 2005) instead
and run our reordering scripts on it (CimS-RO).
For evaluation purposes, we build a baseline sys-
tem raw? which has been trained only on Eu-
roparl. Tuning and testing setup is the same as
for the systems described in Section 6 with the
difference that the weights have been tuned on
newstest2013. The evaluation results are shown
in Table 4. Similarly to previous results reported
in (Gojun and Fraser, 2012), the CimS-RO system
shows an improvement of 0.5 Bleu points when
compared to the raw? baseline .
5Namely: Testflugzeugen (test airplanes), Medientri-
bunal (media tribunal), RBS-Mitarbeiter (RBS worker),
Schulmauersanierung (school wall renovation), Anti-
Terror-Organisationen (anti-terror organisations), and
Tabakimpfstoffe (tobacco-plant-created vaccines) in both
and in CoRI also Hand-gep?ckgeb?hr (hand luggage fee)
and Haftungsstreitigkeiten (liability litigation).
Experiment mert.log Bleu ci Bleu csnews2013 news2014 news2014
raw? 16.87 16.25 15.31
CimS-RO 17.76 16.81 15.81
Table 4: Evaluation of the reordering system
trained on Europarl v7.
9 Summary
We presented the CimS systems, a set of
morphology-aware translation systems cus-
tomised for translation from English to German.
Each system operates on a different level of
morphological description, be it nominal inflec-
tion, verbal inflection, compound processing
or source-side reordering. Some of the systems
are well-established (RI, CoRI and RO), others
are still under developement (RIVe, CoRIVe and
Syntax-RORI). However, all of them, with the ex-
ception of CoRIVe, lead to improved translation
quality when evaluated against a contrastive
baseline without linguistic processing. In an
additional evaluation, we could show that the
compound processing systems are able to create
a considerable number of compounds unseen
in the parallel training data.
In the future, we will investigate further com-
binations and extensions of our morphological
components, including reordering, compound
processing and verbal inflection. There are still
many many interesting challenges to be solved
in all of these areas, and this is especially true for
verbal inflection.
Acknowledgments
This work was supported by Deutsche For-
schungsgemeinschaft grants Models of Mor-
phosyntax for Statistical Machine Translation
(Phase 2) and Distributional Approaches to Se-
mantic Relatedness. We would like to thank
Daniel Quernheim for sharing the workload of
preprocessing the data with us.
Moreover, we thank Edgar Hoch from the IMS
system administration for generously providing
us with disk space and all our colleagues at IMS,
especially Fabienne Braune, Junfei Guo, Nina
Seemann and Jason Utt for postponing their ex-
periments to let us use most of IMS? computing
facilities for a whole week. Thank you each beau-
coup!
76
References
Fabienne Cap, Alexander Fraser, Marion Weller, and
Aoife Cahill. 2014. How to Produce Unseen
Teddy Bears: Improved Morphological Processing
of Compounds in SMT. In Proceedings of EACL
2014.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics
(ACL), Ann Arbor, Michigan.
Colin Cherry and George Foster. 2012. Batch Tun-
ing Strategies for Statistical Machine Translation.
In Proceedings of HLT-NAACL 2012.
Michael Collins, Philipp Koehn, and Ivona Kuc?erov?.
2005. Clause restructuring for statistical machine
translation. In Proceedings ACL 2005.
Chris Dyer. 2009. Using a maximum entropy model
to build segmentation lattices for MT. In Proceed-
ings of HLT-NAACL 2009.
Alexander Fraser, Marion Weller, Aoife Cahill, and Fa-
bienne Cap. 2012. Modeling Inflection and Word
Formation in SMT. In Proceedings of EACL 2012.
Alexander Fraser. 2009. Experiments in Morphosyn-
tactic Processing for Translation to and from Ger-
man. In Proceedings of WMT 2009.
Fabienne Fritzinger and Alexander Fraser. 2010.
How to Avoid Burning Ducks: Combining Lin-
guistic Analysis and Corpus Statistics for Ger-
man Compound Processing. In Proceedings of
WMT@ACL2010.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a Translation Rule?
In Proceedings of HLT-NAACL 2004.
Qin Gao and Stephan Vogel. 2008. Parallel imple-
mentations of word alignment tool. In ACL 2008:
Proceedings of the Workshop on Software Engineer-
ing, Testing, and Quality Assurance for Natural
Language Processing.
Adri? De Gispert and Jos? B. Mari?o. 2008. On the
impact of morphology in English to Spanish statis-
tical MT. Speech Communication.
Anita Gojun and Alexander Fraser. 2012. Determin-
ing the placement of German verbs in English-to-
German SMT. In Proceedings of EACL 2012.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of WMT
2011.
Minwoo Jeong, Kristina Toutanova, Hisami Suzuki,
and Chris Quirk. 2010. A discriminative lexicon
model for complex morphology. In Proceedings of
AMTA 2010.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In Proceedings
of EACL 2003.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open Source Toolkit for Statistical Ma-
chine Translation. In Proceedings of ACL 2007
(Demo Session).
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Fields: Prob-
abilistic Models for Segmenting and Labeling Se-
quence Data. In ICML?01.
Klaus Macherey, Andrew M. Dai, David Talbot,
Ashok C. Popat, and Franz Och. 2011. Language-
independent Compound Splitting with Morpho-
logical Operations. In Proceedings of ACL 2011.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51,.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. Bleu: A Method for Automatic
Evaluation of Machine Translation. In Proceedings
of ACL 2002.
Maja Popovic?, Daniel Stein, and Hermann Ney. 2006.
Statistical Machine Translation of German Com-
pound Words. In Proceedings of FinTAL 2006.
Helmut Schmid and Florian Laws. 2008. Estimation
of conditional probabilities with decision trees and
an application to fine-grained pos tagging. In Pro-
ceedings of COLING 2008.
Helmut Schmid, Arne Fitschen, and Ulrich Heid.
2004. SMOR: A German Computational Morphol-
ogy Covering Derivation, Composition and Inflec-
tion. In Proceedings of LREC 2004.
Helmut Schmid. 2004. Efficient Parsing of Highly
Ambiguous Context-Free Grammars with Bit Vec-
tors. In Proceedings of Coling 2004.
Andreas Stolcke. 2002. SRILM ? an Extensible Lan-
guage Modelling Toolkit. In Proceedings of ICSLN
2002.
Sara Stymne and Nicola Cancedda. 2011. Pro-
ductive Generation of Compound Words in Sta-
tistical Machine Translation. In Proceedings of
WMT@EMNLP?11.
Sara Stymne, Maria Holmqvist, and Lars Ahrenberg.
2008. Effects of Morphological Analysis in Transla-
tion between German and English. In Proceedings
of WMT 2008.
Sara Stymne. 2009. A Comparison of Merging Strate-
gies for Translation of German Compounds. In
Proceedings of EACL 2009 (Student Workshop).
77
Kristina Toutanova, Hisami Suzuki, and Achim
Ruopp. 2008. Applying Morphology Generation
Models to Machine Translation. In Proceedings of
HLT-ACL 2008.
Marion Weller, Alexander Fraser, and Sabine
Schulte im Walde. 2013. Using Subcatego-
rization Knowledge to Improve Case Prediction for
Translation to German. In Proceedings of ACL?13.
Philip Williams and Philipp Koehn. 2011. Agreement
constraints for statistical machine translation into
German. In Proceedings of WMT 2011.
Philip Williams and Phillipp Koehn. 2012. GHKM-
Rule Extraction and Scope-3 Parsing in Moses. In
Proceedings of WMT 2007.
78
Proceedings of the First Workshop on Computational Approaches to Compound Analysis, pages 81?90,
Dublin, Ireland, August 24 2014.
Distinguishing Degrees of Compositionality in Compound Splitting
for Statistical Machine Translation
Marion Weller
1,2
, Fabienne Cap
2
, Stefan M?uller
1
Sabine Schulte im Walde
1
, Alexander Fraser
2
1
IMS, University of Stuttgart
{wellermn;muellesn;schulte}@ims.uni-stuttgart.de
2
CIS, Ludwig-Maximilian University of Munich
{cap;fraser}@cis.uni-muenchen.de
Abstract
The paper presents an approach to morphological compound splitting that takes the degree of
compositionality into account. We apply our approach to German noun compounds and particle
verbs within a German?English SMT system, and study the effect of only splitting compositional
compounds as opposed to an aggressive splitting. A qualitative study explores the translational
behaviour of non-compositional compounds.
1 Introduction
In German, as in many other languages, two (or more) simplex words can be combined to form a com-
pound. This is a productive process, leading to a potentially infinite number of sound German com-
pounds. As a consequence, many NLP applications suffer from coverage issues for compounds which
do not appear or appear only infrequently in language resources. However, while many compounds are
not covered, their component words are often found in lexical resources or training data. Compound
processing allows access to these component words and thus can overcome these sparsity issues.
We use Statistical Machine Translation (SMT) as an example application for compound processing.
Our SMT system translates from German to English, where compounds are usually split in the German
source language prior to training and decoding. The benefits are obvious: vocabulary size is reduced
and the languages are adjusted in terms of granularity, as exemplified by the compound Holzzaun. This
fencewoodenHolzzaun HolzZaun woodenfence
1:1 alignment1:n alignment
results in better alignment quality and model estimation.
Compound splitting also enables the translation of com-
pounds not occurring in the parallel data, if the parts have
been seen and can thus be translated individually. However, these assumptions only hold for compo-
sitional compounds like Holzzaun (?wooden fence?), whose meanings can be derived from the mean-
ings of their constituents, namely Holz (?wood?) and Zaun (?fence?). In contrast, the splitting of non-
compositional compounds may lead to translation errors: e.g. the meaning of J?agerzaun (?lattice fence?)
cannot be represented by the meanings of its constituents J?ager (?hunter?) and Zaun (?fence?). Here, an
erroneous splitting of the compound can lead to wrong generalizations or translation pairs, such as J?ager
? lattice, in the absence of other evidence about how to translate J?ager. When splitting compounds
for SMT, two important factors should thus be considered: (1) whether a compound is compositional
and should be split, and if so (2) how the compound should be split. Most previous approaches mainly
focused on the second task, how to split a compound, e.g. using frequency statistics (Koehn and Knight,
2003) or a rule-based morphology (Fritzinger and Fraser, 2010), and all of them showed improved SMT
quality for compound splitting. The decision about whether the compound is compositional and should
be split at all has not received much attention in the past.
In this work, we examine the effect of only splitting compositional compounds, in contrast to splitting
all compounds. To this end, we combine (A) an approach relying on the distributional similarity be-
tween compounds and their constituents, to predict the degree of compositionality and thus to determine
whether to split the compound with (B) a combination of morphological and frequency-based features
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
81
to determine how to split a compound. We experiment with this novel semantically-informed compound
splitting on the source-side data of a German-English SMT system. As far as we know, we are the first
to study the impact of compositionality-aware compound splitting in SMT. We evaluate our systems on
a standard and on a specifically created test set, both for noun compounds and particle verbs. Our re-
sults show that phrase-based SMT is generally robust with regard to over-splitting non-compositional
compounds, with the exception of low-frequency words. This is in line with corresponding assumptions
from previous work. Furthermore, we present a small-scale study about the translational behaviour of
non-compositional compounds, which can surprisingly often be translated component-wise.
2 Related Work
We combine morphology-based compound splitting with distributional semantics to improve phrase-
based SMT. Here, we discuss relevant work of compound splitting in SMT and distributional semantics.
2.1 Compound Splitting in SMT
Compound splitting in SMT is a well-studied task. There is a wide range of previous work, including
purely string- and frequency-based approaches, but also linguistically-informed approaches. All lines
of research improved translation performance due to compound splitting. In Koehn and Knight (2003),
compounds are split through the identification of substrings from a corpus. The splitting is performed
without linguistic knowledge (except for the insertion of the filler letters ?(e)s?), which necessarily leads
to many erroneous splittings. Multiple possible splitting options are disambiguated using the frequencies
of the substrings. Starting from Koehn and Knight (2003), Stymne (2008) covers more morphological
transformations and imposes POS constraints on the subwords. Nie?en and Ney (2000) and Fritzinger
and Fraser (2010) perform compound splitting by relying on morphological analysers to identify suitable
split points. This has the advantage of returning only linguistically motivated splitting options, but the
analyses are often ambiguous and require disambiguation: Nie?en and Ney (2000) use a parser for
context-sensitive disambiguation, and Fritzinger and Fraser (2010) use corpus frequencies to find the best
split for each compound. Other approaches use a two-step word alignment process: first, word alignment
is performed on a split representation of the compounding language. Then, all former compound parts
for which there is no aligned counterpart in the non-compounding language are merged back to the
compound again. Finally, word alignment is re-run on this representation. See Koehn and Knight (2003)
for experiments on German, DeNeefe et al. (2008) for Arabic and Bai et al. (2008) for Chinese. This
blocks non-compositional compounds from being split if they are translated as one simplex English word
in the training data (e.g. Heckensch?utze, lit. ?hedge|shooter?; ?sniper?) and aligned correctly. However,
cases like J?agerzaun, ?lattice fence? are not covered.
In the present work, we identify compounds with a morphological analyser, disambiguated with corpus
frequencies. Moreover, we restrict splitting to compositional compounds using distributional semantics.
We are not aware of any previous work that takes semantics into account for compound splitting in SMT.
2.2 Distributional Semantics and Compounding
Distributional information has been a steadily increasing, integral part of lexical semantic research over
the past 20 years. Based on the distributional hypothesis (Firth, 1957; Harris, 1968) that ?you shall know
a word by the company it keeps?, distributional semantics exploits the co-occurrence of words in corpora
to explore the meanings and the similarities of the words, phrases, sentences, etc. of interest.
Among many other tasks, distributional semantic information has been utilised to determine the degree
of compositionality (or: semantic transparency) of various types of compounds, most notably regarding
noun compounds (e.g., Zinsmeister and Heid (2004), Reddy et al. (2011), Schulte im Walde et al. (2013),
Salehi et al. (2014)) and particle verbs (e.g., McCarthy et al. (2003), Bannard (2005), Cook and Stevenson
(2006), K?uhner and Schulte im Walde (2010), Bott and Schulte im Walde (2014), Salehi et al. (2014)).
Typically, these approaches rely on co-occurrence information from a corpus (either referring to bags-
of-words, or focusing on target-specific types of features), and compare the distributional features of
the compounds with those of the constituents, in order to predict the degree of compositionality of the
82
OutputInput
Preprocessing 
Step 1:   Identify Component Words Similarity ScoresStep 2:  Predict sitional CompoundsStep 3: Split Compo?
0.311
0.825
0.015
0.725
Holzzaun woodenfence
Holz
Zaun Zaun
Holz
J?gerzaun lattice fenceJ?gerZaun J?gerzaun
SMT
Figure 1: Semantically-informed compound processing in SMT.
compound. The underlying assumption is that a compound which is similar in meaning to a constituent
(as in Holzzaun?Zaun (?wooden fence???fence?) but not in L?owenzahn?Zahn (?lion|tooth (dandelion)??
?tooth?)) is also similar to the constituent with regard to co-occurrence information.
Most related to this work on noun compounds, Reddy et al. (2011) relied on window-based distribu-
tional models to predict the compositionality of English noun compounds, and Schulte im Walde et al.
(2013) compared window-based against syntax-based distributional models to predict the composition-
ality of German noun compounds. Zinsmeister and Heid (2004) used subcategorising verbs to predict
compound?head similarities of German noun compounds. Most recently, Salehi et al. (2014) extended
the previous approaches to take multi-lingual co-occurrence information into account, regarding English
and German noun compounds, and English particle verbs.
3 Methodology
We integrate our semantically-informed compound splitting as a pre-processing step to the German
source language of an SMT system. See Figure 1 for an illustration of our compound processing pipeline.
3.1 Target Compounds
German compounds are combinations of two (or more) simplex words. In some cases, a morphological
transformation is required: for example, when combining the two nouns Ausflug (?excursion?) and Ziel
(?destination?) ? Ausflugsziel (?excursion destination?), a filler letter (here: ?s?) needs to be inserted.
Other such transformations include more filler letters or the deletion/substitution of letters.
Noun compounds are formed of a head noun and a modifier, which can consist of nouns, verbs, adjec-
tives or proper nouns.
Particle verbs are productive compositions of a base verb and a prefix particle, whose part-of-speech
varies between open-class nouns, adjectives, and verbs, and closed-class prepositions and adverbs. In
comparison to noun compounds, the constituents of German particle verbs exhibit a much higher degree
of ambiguity: Verbs in general are more ambiguous than nouns, and the largest sub-class of particles
(those with a preposition particle) is highly ambiguous by itself (e.g. Lechler and Ro?deutscher (2009)
and Springorum (2011)). For example, in anknabbern (?to nibble partially?), the particle an expresses a
partitive meaning , whereas in ankleben (?to glue onto sth.?) an has a topological meaning (to glue sth.
onto an implicit background). In addition, particle verb senses may be transparent or opaque with respect
to their base verbs. For example, abholen ?fetch? is rather transparent with respect to its base verb holen
?fetch?, whereas anfangen ?begin? is more opaque with respect to fangen ?catch?. In contrast, einsetzen
has both transparent (e.g. ?insert?) and opaque (e.g. ?begin?) verb senses with respect to setzen ?put/sit
(down)?. The high degree of ambiguity makes particle verbs a challenge for NLP. Moreover, particle
and base verb can occur separately (er f?angt an: ?he begins?) or in one word (dass er anf?angt: ?that he
begins?), depending on the clausal type. This makes consistent treatment of particle verbs difficult.
3.2 Identification of Component Parts
We use the rule-based morphological analyser SMOR (Schmid et al., 2004) to identify compounds and
their constituents in our parallel training data (cf. Section 4). It relies on a large lexicon of word lemmas
and feature rules for productive morphological processes in German, i.e., compounding, derivation and
83
inflection. In this paper, we will not consider splitting into derivational affixes (as needed for, e.g., Arabic
and Turkish), but instead identify simplex words that may also occur independently. Moreover, we only
keep noun compounds and particle verbs consisting of two constituents. The resulting set consists of
93,299 noun compound types and 3,689 particle verb types.
3.3 Predicting Compositionality based on Distributional Similarity
Starting from this set of compounds as derived from our parallel training data, we collected distributional
co-occurrence information from two large German web corpora and the machine translation training data:
(i) the German COW corpus (Sch?afer and Bildhauer (2012), ?9 billion words), (ii) the SdeWaC (Faa?
and Eckart (2013), ?880 million words), (iii) our MT parallel corpus (?40 million words) and (iv) MT
language model training data (?146 million words). We relied on earlier work and used the 20,000 most
frequent nouns from the SdeWaC as co-occurrence features, looking into a window of 20 words to the left
and to the right of our target compounds and their constituents. We thus obtained a co-occurrence matrix
of all compounds and their constituents with the 20,000 selected nouns. As co-occurrence strength (i.e.,
how strong is a co-occurrence between a target word and a co-occurring noun), we collected frequencies
and transformed them into local mutual information (LMI) values, cf. Evert (2005). Finally, we calcu-
lated the distributional similarity between the compounds and their constituents, relying on the standard
measure cosine. The cosine value is then used to predict the degree of compositionality between the
respective compound?constituent pairs. For example, the cosine value of the pair Baumschule?Baum
1
is
0.38, while the cosine value of the pair Baumschule?Schule is only 0.01.
3.4 Semantically-Informed Compound Splitting
In the two preceding sections, we described how we identified component words and calculated distribu-
tional compositionality scores for all of the compounds found in our training data. Here, we give details
on how we include the semantic information into the compound splitting process. Recall that we only
want to split compositional compounds and keep non-compositional compounds together.
The splitting decision (to split/not split a compound) is based on the compositionality score of the
compound that takes into account either one or both of the compound?constituent cosine values: if the
predicted degree of compositionality is high, the compound is split. We consider and combine four dif-
ferent criteria: i) only the compound?modifier similarity (mod); (ii) only the compound?head similarity
(head); a combination of the compound?modifier and the compound?head similarities, relying on (iii) the
geometric mean (geom) or (iv) on the arithmetic mean (arith). We used different thresholds for each of
these criteria throughout our experiments, with a specific focus on distinguishing the contributions of the
modifiers vs. the heads in the splitting decision, following insights from recent work in psycholinguistic
studies (Gagn?e and Spalding, 2009; Gagn?e and Spalding, 2011) as well as in computational approaches
on noun compounding (Reddy et al., 2011; Schulte im Walde et al., 2013). Furthermore, we compare
the effects of splitting with regard to two types of compounds, noun compounds and particle verbs: Both
types are very productive and can generate a potentially infinite number of new forms.
4 Experimental Setting
This section gives an overview on the technical details of the SMT system and our data sets. Compound
splitting is applied to all source-language data, i.e. the parallel data used to train the model, as well as
the input for parameter tuning and testing.
2
Translation Model Moses is a state-of-the-art toolkit for phrase-based SMT systems (Koehn et al.,
2007). We use it with default settings to train a translation model and we do so separately for each of the
different compound splittings. Word alignment is performed using GIZA++ (Och and Ney, 2003). Fea-
ture weights are tuned using Batch-Mira (Cherry and Foster, 2012) with ?-safe-hope? until convergence.
Training Data Our parallel training data contains the Europarl corpus (version 4, cf. Koehn (2005))
and also newspaper texts, overall ca. 1.5 million sentences
3
(roughly 44 million words). In addition, we
1
Baum|schule: ?tree|school? (tree nursery)
2
Compounds not contained in the parallel data are always split, as they cannot be translated otherwise.
3
Data from the shared task of the EACL 2009 workshop on statistical machine translation: www.statmt.org/wmt09
84
use an English corpus of roughly 227 million words (including the English part of the parallel data) to
build a target-side 5-gram language model with SRILM (Stolcke, 2002) in combination with KENLM
(Heafield, 2011). For parameter tuning, we use 1,025 sentences of news data.
Standard Test set 1,026 sentences of news data (test set from the 2009 WMT Shared Task): this set is
to measure the translation quality on a standard SMT test and make it comparable to other work.
Noun/Verb Test set As our main focus lies on sentences containing compounds, we created a second
test set which is rich in compounds. From the combined 2008-2013 Shared Task test sets, we extracted
all sentences containing at least one noun compound for which we have compound-constituent similarity
scores. Moreover, we excluded sentences containing nouns that are not in the parallel training data: such
compounds can only be translated when split which allows to translate their components. The final test
set consists of 2,574 sentences. Similarly, we also created a set rich in particle verbs (855 sentences).
Opaque Test set As the two first test sets mainly contain compositional compounds, we use a third test
set consisting of sentences with only non-compositional compounds. The underlying compounds were
chosen based on a list containing noun compounds and human ratings for compositionality (von der
Heide and Borgwaldt (2009)). As before, the compounds must have occurred in the parallel data. The
result is a list of 14 compounds, of which 11 have a low modifier-compound similarity and 3 have a low
head-compound similarity. We then extracted sentences containing these compounds (5 per compound =
70 in total) from German newspaper data
4
. In contrast to the other sets, we use this test set in a qualitative
study, to approximate the translation quality by counting the number of correctly translated compounds.
5 SMT Results
In this section, we present and discuss the results of our machine translation experiments. We first report
results for two test sets in terms of a standard evaluation metric (BLEU) and then continue with a small-
scale qualitative study on the translational behaviour of non-compositional compounds.
5.1 Compound Splitting within a Standard SMT Task
BLEU (Papineni et al., 2002) is a common metric to automatically measure the quality of SMT output
by comparing n-gram matches of the SMT output with a human reference translation. Table 1 lists the
results for our SMT-systems: we report on different compound-constituent scores and thresholds, for
noun compounds and particle verbs respectively. Note that BLEU scores are not comparable across dif-
nouns particle verbs
stand. noun stand. verb
baseline 21.00 21.08 21.00 20.29
a
g
g
r
DIST 22.00 22.02 21.02 20.11
FREQ 22.04 21.88 21.11 20.21
0
.
0
5
head 21.77 21.58 ? ?
mod. 22.01 21.74 ? ?
geom. 21.99 21.71 ? ?
arith. 21.95 21.95 ? ?
0
.
1
head 21.91 21.69 21.11 20.24
mod. 22.01 21.63 20.98 20.43
geom. 22.06 21.90 21.12 20.55
arith. 22.05 21.73 21.08 20.34
0
.
1
5
head 21.80 21.67 21.10 20.09
mod. 21.71 21.77 21.00 20.25
geom. 21.78 21.64 20.84 20.30
arith. 22.00 21.77 21.24 20.40
0
.
2
head 21.78 21.51 ? ?
mod. 21.78 21.45 ? ?
geom. 21.76 21.54 ? ?
arith. 22.02 21.79 ? ?
Table 1: BLEU scores for all compound-
constituent variations.
ferent test sets, but only illustrate system differences
within one test set. We compare our systems to the
scores of a baseline system (without compound process-
ing) and an aggressive split system in which all noun
compounds and particle verbs are split. The labels DIST
and FREQ indicate how several possible splittings were
disambiguated: DIST means we chose the splitting option
having the higher geometric mean of the two compound-
constituent scores, assuming that the variant expressing a
higher compositionality score leads to the more probable
splitting analysis. For FREQ, the decision is based on the
geometric mean of corpus frequencies of the respective
components of the compound, as is common practise for
the disambiguation of multiple splitting options in SMT
(Koehn and Knight, 2003; Fritzinger and Fraser, 2010).
In terms of BLEU, there is little difference for these two
variants. For further experiments, we thus decided to al-
ways use FREQ for disambiguation, assuming that com-
ponents chosen by frequency are potentially better repre-
4
www.ims.uni-stuttgart.de/forschung/ressourcen/korpora/hgc.html
85
rating compound gloss mod. head translation
HIGHLY Staats|bankrott nation|bankruptcy 0.4779 0.6527 national bankruptcy
COMP. Staats|gebilde nation|structure 0.6955 0.3431 national structure
MEDIUM Industrie|staat industry|nation 0.0258 0.1488 industrial nation
COMP. Staats|kasse nation|cash box 0.0718 0.2757 public purse, treasury
LOW Staats|spitze nation|top 0.0024 0.0040 top/head of state
COMP. Staats|monotheismus nation|monotheism 0.0071 0.0071 national monotheism
Table 2: Examples for different compound-constitutent score ranges: HIGH: highly compositional,
MEDIUM: cases of doubt, LOW: highly non-compositional, according to their scores.
sented in the training data. Thus, we first use frequencies to determine the best split option in the case
of several possibilities, and then we apply distributional semantics to determine whether to split at all.
The remainder of Table 1 reports on different variants of the semantically-informed splitting criteria we
used. The notation head/mod/geom/arith indicates which (combination of) compound-constituent scores
were applied as criterion, with the threshold indicated by the vertical number. We performed the first
set of experiments with different thresholds for noun compounds, and then applied the medium-range
thresholds to the particle verbs. Generally, there are no considerable differences between the systems
with semantically restricted splitting and the aggressive split systems, even though there seems to be a
slightly positive effect for particle verbs. Having a closer look, we find that for noun compounds on the
standard test set, the best results (threshold: 0.1) are at the same level as the aggressive split systems;
with some small losses in BLEU on some of the other settings.
5.2 Discussion
All settings clearly outperform the baseline system (without compound processing). This indicates that
phrase-based SMT is rather robust with regard to non-semantic splitting as it is can often recover from
over-splitting by translating the word sequence as a phrase. This is in line with previous observations
of Koehn and Knight (2003). The results for the noun test set, which is biased towards containing more
nominal compounds, even suggests that less splitting might harm the system, as the BLEU scores tend
to drop when increasing the threshold. For particle verbs,
5
the picture is slightly different: first, splitting
only particle verbs does not lead to a considerable improvement over the baseline, as in the case of noun
compounds. For the verb test set, it even leads to a drop in BLEU. However, a more restricted splitting
leads to improved BLEU scores, even though not significantly better than the un-split baseline system.
Even though the handling of particle verbs needs to be refined in terms of dealing with their structural
behaviour (split vs. unsplit depending on the sentence structure) or ambiguities of the particle verb,
we consider this an encouraging result indicating that particle verbs can benefit from a semantically-
informed splitting process.
There are several possible reasons why a more restricted splitting might not lead to an improvement,
even though the idea of splitting only compositional compounds is intuitive and straightforward.
Inconsistent Splitting Compositionality is a continuum rather than a binary decision, with the scores of
many (compositional) compounds being in the medium range. Thus, it happens that some compounds
containing a certain constituent are split, whereas others are not: such inconsistent splittings do not
contribute to the generalization compound splitting aims for. Table 2 gives examples for compounds
with different degrees of compositionality, which illustrate this issue: for Industriestaat (?industrial
nation?) and Staatskasse (?public purse?) in the middle part of the table, a splitting decision based on
the head scores for thresholds of 0.15 or 0.2 leads to inconsistent splitting. Only compounds with high
scores, as the examples at the top of Table 2 are always split. The bottom part gives examples with
comparatively low compound-constituent scores that would benefit from splitting, but which will not be
split in any of our systems.
5
Note that there are considerably less particle verbs than noun compounds in the standard test set and the parallel data.
86
compound gloss translation unsplit f split f
Seehunde sea|dogs seals seals 5 seals 5
Flohmarkt flea|market flea market flea market 5 flea market 5
Kopfsalat head|salad lettuce lettuce 5 lettuce 5
Handtuch hand|cloth towel towel 5 towel 5
Kronleuchter crown|candelabra chandelier chandelier 5 crown leuchter 5
G?urteltiere belt|animal armadillo armadillos 5 belt animals 5
Wasserhahn water|rooster tap
water tap 2
tap 5
water supply 3
Meerschweinchen
sea|piglet guinea pig
guinea pig 4
guinea pig 5
sea pig 1
Taschenbuch pocket|book paperback
paper back 3
paperback 5
pocket book 2
Kronkorken crown|cork crown cap *kronkorken 5 crown corks 5
Taschenlampe pocket|lamp flashlight
pocket lamp 4
*taschenlampe 5
bag lamp 1
Fleischwolf meat|wolf meat grinder *fleischwolf 5 meat wolf 5
Marienk?afer Mary|bug ladybug *marienk?afer 5 *marie k?afer 5
Blockfl?oten block|flute recorder
block might 4
*blockfl?oten 5
bloc might 1
Table 3: correct vs. wrong ? Translation of non-compositional compounds (opaque test set) without
being split (unsplit) vs. being split prior to translation. ?*? highlights untranslated compounds.
Coverage of Opaque Compounds Another relevant factor concerns the frequency ranges of compounds
that are most interesting for this approach. High/mid frequency compounds are usually well-covered by
the training data of an SMT system, and in most cases they are translated correctly even if they have
been split erroneously. This is due to the fact that split compounds can be learned and translated as a
phrase if there were enough instances for the system to learn a valid translation. In the case of low-
frequency compounds, the system is less likely to learn a correct translation from the parallel data.
However, low-frequency compounds are not well covered by the system and splitting should thus be
highly beneficial. Newly created, i.e. highly compositional compounds, tend to be of low frequency, as
is illustrated by the example of Staatsmonotheismus (freq=1 in the parallel data) in Table 2. However,
a wrong splitting decision for a non-compositional compound of low frequency is likely to lead to an
incorrect translation as the SMT system has better statistics for the individual parts than for the sequence
of the compounds constituents. We assume that for low-frequency compounds the distributional similar-
ity scores are generally less reliable, even though using LMI helps to minimize this. To a certain extent,
we expect non-compositional compounds ?which are typically considered as lexicalized? to occur with
higher frequencies than novel compositional compounds.
6
Furthermore, there are considerably more
compositional than non-compositional compounds in standard text. Thus, being in favor of splitting in
the case of low-frequency words should be reasonable in most contexts.
6 A Closer Look at Translating Opaque Compounds
In this section, we compare the translations of non-compositional compounds when they are unsplit
and when they are split. We use a small test set containing 70 sentences, 5 for each of the 14 non-
compositional compounds (see Section 4). Then we conduct a small-scale qualitative analysis focusing
on the correct translation of opaque compounds.
Table 3 reports on correct translations for the non-compositional compounds for an experiment where
they have been split or not split (unsplit) prior to translation. Even though all compounds occurred in
the parallel data, five (which are marked with ?*?) cannot be translated by the unsplit system due to not
being aligned correctly. The other compounds are translated correctly (marked with ?+? in Table 3).
In the course of our study, we found that many of the correct translations remain the same (seals, flea
market, lettuce, towel). In the case of guinea pig, paperback and tap there are mixed results of correct and
incorrect translations. Only in the cases of chandelier (?crown leuchter?) and armadillo (?belt animal?),
6
It has to be noted, though, that the model is influenced by the somewhat different domain of the parallel data (European
Parliament proceedings, a standard data set for SMT).
87
compound gloss translation compound gloss translation
B?arlauch bear|leek bear leek Handtasche hand|bag handbag
Baumschule tree|school tree nursery Hirschk?afer stag|beetle stag beetle
L?owenanteil lion|share lion?s share H?uttenk?ase cottage|cheese cottage cheese
Fliegenpilz fly|mushroom fly agaric Kronkorken crown|cork crown cap
Flohmarkt flea|market flea market Teelicht tea|light tea candle
Table 4: Examples for (near) literal translation of non-compositional compounds.
which were translated correctly with the unsplit system, all translations obtained with the split system are
wrong. Somewhat surprisingly, in some cases there even is a benefit from splitting the non-compositional
compounds: Kronkorken, previously not translated at all, is correctly generated as crown cork. For other
previously untranslated words, Fleischwolf and Taschenlampe, literal translations of the constituents
are given: while meat wolf (instead of meat grinder) is probably not understandable, the translation of
Taschenlampe as pocket lamp is certainly preferable to the untranslated compound.
Due to the observed unexpected translational behaviour of 2 of the 14 non-compositional compounds
(Flohmarkt and Kronkorken), which can be translated literally and thus ?in theory? benefit from splitting,
we present a small study illustrating that this phenomenon is not as rare as one would intuitively expect.
This study is not meant to be comprehensive, but rather to point out that the translational behaviour of
non-compositional compounds can correspond to that of compositional compounds; Table 4 lists a few
such examples. We assume that this behaviour is due to the fact that English and German are similar
languages with a similar background. Thus, the ?images? used in non-compositional words often tend to
be similar. For some of the compounds (e.g. Flohmarkt) this is even true for some Romance languages,
too (IT: mercato delle pulci, FR: march?e aux puces) .
Generally, the SMT system should even be able to handle cases where the translation of one part is
not strictly literal (e.g. cap?cork or agaric?mushroom). In comparison to a dictionary, which only lists
few translations, the translation model offers a large choice of translation options that are not always
strictly synonymous, but can cover a large range of related meanings. In combination with the target-
side language model, this could allow to ?guess? good translations of such compounds. However, the
component-wise translation of non-compositional compounds only works if the source- and target lan-
guage compounds contain the same number of constituents. For example, consider translating the word
Faultier (lazy|animal: ?sloth?): even if the SMT system offers the translation faul?sloth, it would also
need to produce a translation for the constituent tier, probably resulting in something like sloth animal.
In conclusion, while phrase-based SMT is often able to recover from over-splitting by translating
a word sequence as a phrase, this is not always necessary for opaque compounds as they can have a
literal or near-literal translation. Thus, for explicitly handling non-compositional compounds in SMT, a
monolingual estimation of compositionality is not the only relevant factor. The translational behaviour
of compounds should also be taken into account.
7 Conclusion and Future Work
We studied the impact of compositionality in German-English SMT by restricting compound splitting to
compositional compounds. The decision about compositionality is based on the distributional similarity
between a compound and its constituents. We experimented with different threshold/score combinations
on a standard and a specifically created test set. Our results indicate that phrase-based SMT is very robust
with regard to over-splitting non-compositional noun compounds, with the exception of low-frequency
compounds. Furthermore, we studied the translational behaviour of non-compositional compounds with
a special focus on the fact that non-compositional compounds can in some cases be translated component-
wise, leading to the conclusion that a monolingual estimation of compositionality is not sufficient for an
optimal explicit handling of compounds in SMT applications.
The relatively low impact of distinguishing the degree of compositionality might also be due to the fact
that the task of translating noun compounds can be considered ?easy?, as the split components always
occur adjacently. In contrast, handling other types of non-compositional structures (e.g. noun-verb or
preposition-noun-verb combinations which are non-compositional) is a challenging task for future work.
88
Acknowledgements
This work was funded by the DFG Research Projects ?Distributional Approaches to Semantic Related-
ness? (Marion Weller, Stefan M?uller) and ?Models of Morphosyntax for Statistical Machine Transla-
tion ? Phase 2? (Fabienne Cap, Alexander Fraser, Marion Weller) and the DFG Heisenberg Fellowship
SCHU-2580/1-1 (Sabine Schulte im Walde).
References
Ming-Hong Bai, Keh-Jiann Chen, and Jason S Chang. 2008. Improving word alignment by adjusting chinese word
segmentation. In IJCNLP?08: Proceedings of the 3rd International Joint Conference on Natural Language
Processing, pages 249?256.
Collin Bannard. 2005. Learning about the Meaning of Verb?Particle Constructions from Corpora. Computer
Speech and Language, 19:467?478.
Stefan Bott and Sabine Schulte im Walde. 2014. Optimizing a Distributional Semantic Model for the Prediction
of German Particle Verb Compositionality. In Proceedings of the 9th Conference on Language Resources and
Evaluation, Reykjavik, Iceland.
Colin Cherry and George Foster. 2012. Batch tuning strategies for statistical machine translation. In HLT-
NAACL?12: Proceedings of the Human Language Technology Conference of the North American Chapter of the
Association for Computational Linguistics, volume 12, pages 34?35.
Paul Cook and Suzanne Stevenson. 2006. Classifying Particle Semantics in English Verb-Particle Constructions.
In Proceedings of the ACL/COLING Workshop on Multiword Expressions: Identifying and Exploiting Underly-
ing Properties, pages 45?53, Sydney, Australia.
Steve DeNeefe, Ulf Hermjakob, and Kevin Knight. 2008. Overcoming vocabulary sparsity in mt using lattices.
In AMTA?08: Proceedings of the 8th Biennial Conference of the Association for Machine Translation in the
Americas.
Stefan Evert. 2005. The Statistics of Word Co-Occurrences: Word Pairs and Collocations. Ph.D. thesis, Institut
f?ur Maschinelle Sprachverarbeitung, Universit?at Stuttgart.
Gertrud Faa? and Kerstin Eckart. 2013. SdeWaC ? a Corpus of Parsable Sentences from the Web. In Proceedings
of the International Conference of the German Society for Computational Linguistics and Language Technology,
pages 61?68, Darmstadt, Germany.
John R. Firth. 1957. Papers in Linguistics 1934-51. Longmans, London, UK.
Fabienne Fritzinger and Alexander Fraser. 2010. How to Avoid Burning Ducks: Combining Linguistic Analysis
and Corpus Statistics for German Compound Processing. In Proceedings of the Fifth Workshop on Statistical
Machine Translation, pages 224?234. Association for Computational Linguistics.
Christina L. Gagn?e and Thomas L. Spalding. 2009. Constituent Integration during the Processing of Compound
Words: Does it involve the Use of Relational Structures? Journal of Memory and Language, 60:20?35.
Christina L. Gagn?e and Thomas L. Spalding. 2011. Inferential Processing and Meta-Knowledge as the Bases for
Property Inclusion in Combined Concepts. Journal of Memory and Language, 65:176?192.
Zellig Harris. 1968. Distributional Structure. In Jerold J. Katz, editor, The Philosophy of Linguistics, Oxford
Readings in Philosophy, pages 26?47. Oxford University Press.
Kenneth Heafield. 2011. Kenlm: faster and smaller language model queries. In EMNLP?11: Proceedings of the
6th workshop on statistical machine translation within the 8th Conference on Empirical Methods in Natural
Language Processing, pages 187?197.
Philipp Koehn and Kevin Knight. 2003. Empirical methods for compound splitting. In EACL ?03: Proceedings of
the 10th Conference of the European Chapter of the Association for Computational Linguistics, pages 187?193,
Morristown, NJ, USA. Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan
Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In ACL?07: Proceedings of the
45th Annual Meeting of the Association for Computational Linguistics, Demonstration Session, pages 177?180.
89
Philipp Koehn. 2005. Europarl: a parallel corpus for statistical machine translation. In MT Summit?05: Proceed-
ings of the 10th machine translation summit, pages 79?86.
Natalie K?uhner and Sabine Schulte im Walde. 2010. Determining the Degree of Compositionality of German Par-
ticle Verbs by Clustering Approaches. In Proceedings of the 10th Conference on Natural Language Processing,
pages 47?56, Saarbr?ucken, Germany.
Andrea Lechler and Antje Ro?deutscher. 2009. German Particle Verbs with auf. Reconstructing their Composition
in a DRT-based Framework. Linguistische Berichte, 220.
Diana McCarthy, Bill Keller, and John Carroll. 2003. Detecting a Continuum of Compositionality in Phrasal
Verbs. In Proceedings of the ACL-SIGLEX Workshop on Multiword Expressions: Analysis, Acquisition and
Treatment, pages 73?80, Sapporo, Japan.
Sonja Nie?en and Hermann Ney. 2000. Improving SMT quality with morpho-syntactic analysis. In COLING?00:
Proceedings of the 18th International Conference on Computational Linguistics, pages 1081?1085. Morgan
Kaufmann.
Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51,.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: A method for automatic evaluation
of machine translation. In ACL?02: Proceedings of the 40th Annual Meeting of the Association for Computa-
tional Linguistics, pages 311?318.
Siva Reddy, Diana McCarthy, and Suresh Manandhar. 2011. An Empirical Study on Compositionality in Com-
pound Nouns. In Proceedings of the 5th International Joint Conference on Natural Language Processing, pages
210?218, Chiang Mai, Thailand.
Bahar Salehi, Paul Cook, and Timothy Baldwin. 2014. Using Distributional Similarity of Multi-Way Translations
to Predict Multiword Expression Compositionality. In Proceedings of EACL 2014.
Roland Sch?afer and Felix Bildhauer. 2012. Building Large Corpora from the Web Using a New Efficient Tool
Chain. In Proceedings of the 8th International Conference on Language Resources and Evaluation, pages
486?493, Istanbul, Turkey.
Helmut Schmid, Arne Fitschen, and Ulrich Heid. 2004. Smor: A German computational morphology covering
derivation, composition and inflection. In LREC ?04: Proceedings of the 4th Conference on Language Resources
and Evaluation, pages 1263?1266.
Sabine Schulte im Walde, Stefan M?uller, and Stephen Roller. 2013. Exploring Vector Space Models to Predict the
Compositionality of German Noun-Noun Compounds. In Proceedings of the 2nd Joint Conference on Lexical
and Computational Semantics, pages 255?265, Atlanta, GA.
Sylvia Springorum. 2011. DRT-based Analysis of the German Verb Particle ?an?. Leuvense Bijdragen, 97:80?
105.
Andreas Stolcke. 2002. SRILM ? an extensible language modelling toolkit. In ICSLN?02: Proceedings of the
international conference on spoken language processing, pages 901?904.
Sara Stymne. 2008. German compounds in factored statistical machine translation. In GoTAL ?08: Proceedings
of the 6th International Conference on Natural Language Processing, pages 464?475. Springer Verlag.
Claudia von der Heide and Susanne Borgwaldt. 2009. Assoziationen zu Unter, Basis und Oberbegriffen. In
Proceedings of the 9th Norddeutsches Linguistisches Kolloquium, pages 51?74.
Heike Zinsmeister and Ulrich Heid. 2004. Collocations of Complex Nouns: Evidence for Lexicalisation. In
Proceedings of Konvens, Vienna, Austria.
90
