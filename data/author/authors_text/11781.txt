Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 85?88,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Discriminative Approach to Predicate-Argument Structure Analysis
with Zero-Anaphora Resolution
Kenji Imamura, Kuniko Saito, and Tomoko Izumi
NTT Cyber Space Laboratories, NTT Corporation
1-1 Hikarinooka, Yokosuka, Kanagawa, 239-0847, Japan
{imamura.kenji,saito.kuniko,izumi.tomoko}@lab.ntt.co.jp
Abstract
This paper presents a predicate-argument
structure analysis that simultaneously con-
ducts zero-anaphora resolution. By adding
noun phrases as candidate arguments that
are not only in the sentence of the target
predicate but also outside of the sentence,
our analyzer identifies arguments regard-
less of whether they appear in the sen-
tence or not. Because we adopt discrimi-
native models based on maximum entropy
for argument identification, we can easily
add new features. We add language model
scores as well as contextual features. We
also use contextual information to restrict
candidate arguments.
1 Introduction
Predicate-argument structure analysis is a type of
semantic role labeling, which is an important mod-
ule to extract event information such as ?who did
what to whom? from a sentence. There are many
arguments called zero pronouns that do not appear
in the surface of a sentence in Japanese. In this
case, predicate-argument structures cannot be con-
structed if we only rely on the syntactic informa-
tion of a single sentence. Similar phenomena also
happen in English noun predicates, in which ar-
guments of noun predicates sometimes do not ex-
ist in the sentence due to things such as ellipses
(Jiang and Ng, 2006). To correctly extract the
structures from such sentences, it is necessary to
resolve what zero pronouns refer to by using other
information such as context.
Although predicate-argument structure analysis
and zero-anaphora resolution are closely related,
it was not until recently that these two tasks were
lumped together. Due to the developments of
large annotated corpora with predicate-argument
and coreference relations (e.g.,(Iida et al, 2007))
and with case frames, several works using statisti-
cal models have been proposed to solve these two
tasks simultaneously (Sasano et al, 2008; Taira et
al., 2008).
In this paper, we present a predicate-argument
structure analysis that simultaneously resolves the
anaphora of zero pronouns in Japanese, based on
supervised learning. The analyzer obtains candi-
date arguments not only from the sentence of the
target predicate but also from the previous sen-
tences. It then identifies the most likely argu-
ments based on discriminative models. To iden-
tify arguments that appear in the sentence and are
represented by zero pronouns without distinction,
the analyzer introduces the following features and
techniques: the language model features of noun
phrases, contextual features, and restrictions of
candidate arguments.
2 Predicate-Argument Structure
Analyzer
2.1 Procedure and Models
The procedure of our predicate-argument structure
analyzer is as follows. The input to the analyzer is
an article (multiple sentences) because our target
is to identify arguments spread across sentences.
1. First, each sentence is individually analyzed
and segmented into base phrases by a morpho-
logical analyzer and a base phrase chunker. In
Japanese, a base phrase is usually constructed
by one or more content words (such as base
noun phrases) and function words (such as case
particles). In addition, dependency relations
among base phrases are parsed by a depen-
dency parser. In this paper, base phrases and
dependency relations are acquired from an an-
notated corpus (i.e., correct parses).
2. Next, predicates are extracted from the base
phrases. In general, a predicate is determined
85
Name Note
Baseline
Features
Predicate Form and POS of the predi-
cate
Noun Form and POS of the head-
word of the candidate phrase
Particle Form and POS of the particle
of the candidate phrase
Path Dependency relation between
the predicate and the candi-
date phrase
Passive Passive auxiliary verbs that
the predicate contains
PhPosit Relative phrase position be-
tween the predicate and the
candidate phrase
SentPosit Relative sentence position be-
tween the predicate and the
candidate phrase
Additional
Features
(c.f.,
Sec. 2.2
and 2.3)
LangModel Language model scores
Used Flag whether the candidate
phrase was used as arguments
of previous predicates
SRLOrder Order in Salient Referent List
Table 1: Features Used in this Paper
based on parts of speech such as verbs and ad-
jectives. In this paper, the predicates are also
provided from an annotated corpus.
3. Concurrently, noun phrases and their head-
words are extracted as candidate arguments
from base phrases. If an argument of a predi-
cate is a zero pronoun, it is likely that the argu-
ment itself has appeared in previous sentences.
Therefore, the analyzer collects not only all
phrases in the sentence but also some phrases
in the previous sentences. We also add the spe-
cial noun phrase NULL, which denotes that the
argument of the predicate is not required or did
not appear in the article (i.e., exophoric).
4. Next, features needed for an argument iden-
tifier are extracted from each pair of a predi-
cate and a candidate argument. Features used
in this paper are shown in Table 1. Base-
line features are roughly those of the predi-
cate, the noun phrase, and their relations (on
the phrasal/sentential sequence and the depen-
dency tree). For binary features, we use all
combinations of these features listed above.
5. Finally, the argument identifier selects the best
phrases for nominative, accusative, and dative
cases from the candidate arguments (Figure 1).
In this paper, we use maximum entropy models
normalized for each predicate to each case. That
is, the identifier directly selects the best phrase that
NULL Phrase 1 Phrase 2 Phrase 3 Phrase 4 ...
Candidate Arguments
Phrase 1 Phrase 3 NULL
Candidate Arguments
in Sentence of Predicate
Candidate Arguments
before Sentences of Predicate
zero-anaphoric(inter-sentential)
exophoric
or no argument
Select
Best
Phrase
Dat.
Model
Select
Best
Phrase
Acc.
Model
Select
Best
Phrase
Nom.
Model
Figure 1: Summary of Argument Identification
satisfies the following equations from the candi-
date arguments:
n? = argmax
n
j
?N
P (d(n
j
) = 1|X
j
;M
c
) (1)
P (d(n
j
) = 1|X
j
;M
c
) =
1
Z
c
(X)
exp
?
k
{?
c
k
f
k
(d(n
j
) = 1, X
j
)}(2)
Z
c
(X) =
?
n
j
?N
exp
?
k
{?
c
k
f
k
(d(n
j
) = 1, X
j
)} (3)
X
j
= ?n
j
, v, A? (4)
where n, c, and v denote a noun phrase of an argu-
ment, the case, and the target predicate, respec-
tively, N denotes a set of candidate arguments,
d(n) is a function that returns 1 iff the phrase n
becomes the argument, and M
c
denotes the model
of the case c. In addition, f
k
(d(n
j
) = 1, X
j
) is a
feature function, ?
c
k
denotes a weight parameter
of the feature function, and A denotes an article in
which all sentences are parsed.
As shown, our analyzer can assign the best noun
phrases to arguments regardless of whether they
appear in the sentence or not by collecting candi-
dates spread across multiple sentences. Further-
more, because the identifier is regarded as a selec-
tor based on the discriminative models, our ana-
lyzer has two properties: 1) New features can be
easily added. 2) The precision can be improved by
restricting the candidate arguments appropriately.
When we analyze predicate-argument struc-
tures and zero-anaphora resolution, syntactic in-
formation sometimes does not help because refer-
ents of zero pronouns do not appear in the sen-
tence of the predicate. To overcome this problem,
86
we introduce additional information, i.e., language
model scores and contextual information.
2.2 Language Models
Even if syntactic information does not help to
identify arguments, we can expect that a certain
noun phrase might be the correct argument of the
predicate when we put it in place of the zero
pronoun and the sentence becomes meaningful.
Therefore, we add language model scores as fea-
tures of the identifier. Because the appearance or-
der of argument phrases is not strongly constricted
in Japanese, we construct generation models that
reflect dependency relations among a predicate, its
case and a noun phrase. That is, we regard gen-
eration probabilities P (n|c, v) acquired from the
dependency tree as the scores of language models.
The language models are built from large plain
texts by using a dependency parser. First, predi-
cates and the base phrases that directly depend on
the predicates are aquired from parsed sentences.
Next, case particles and headwords are extracted
from the base phrases. Finally, generation prob-
abilities are computed using maximum likelihood
estimation. Good-Turing discounting and backoff
smoothing are also applied. Here, it is necessary
to assign generation probabilities to NULLs. Re-
garding the training corpus that will be described
in Section 3, the NULL rates of the nominative,
accusative, and dative cases were 16.7%, 59.9%,
and 81.6%, respectively. We assign these rates to
the backoff term P (NULL|c).
Using the language models, generation proba-
bilities of the noun phrases are computed for ev-
ery case of the predicate, and features that main-
tain the logarithms of language model scores are
added (?LangModel? features in Table 1). Thus,
the values of these feature functions are real.
2.3 Usage of Context
Centering theory claims that noun phrases that
have been used once tend to be used again within
the same context. We adopt this claim and add two
different kinds of features. One is the feature that
indicates whether a candidate has been used as an
argument of predicates in the preceding sentences
(?Used? features). However, the Used features are
affected by the accuracy of the previous analyses.
Thus, we also adopt the Salience Reference List
(Nariyama, 2002), which only uses explicit sur-
face case markers or a topic marker, and added
Training Development Test
# of Articles 1,751 480 695
# of Sentences 24,225 4,833 9,272
# of Predicates 67,145 13,594 25,500
# of Arguments
Nom. 56,132 11,969 21,931
Acc. 26,899 5,566 10,329
Dat. 12,332 3,147 5,944
Table 2: Corpus Statistics
their priority order to the List as another feature
(?SRLOrder? feature).
Another way to adopt contextual information
is to restrict the candidate arguments. When we
analyzed the training corpus from the viewpoint
of zero pronouns, it was found that 102.2 noun
phrases on average were required as candidate ar-
guments if we did not stipulate any restrictions.
When the candidate arguments we had restricted
to those that had been used as arguments of the
predicate appeared in a previous one sentence
(namely, noun phrases appeared in more than one
sentence before have a chance to remain), then the
number of candidate arguments significantly de-
creased to an average of 3.2 but they covered the
62.5% of the referents of zero pronouns.
By using these characteristics, our analyzer re-
stricts the candidate arguments to those that are of
the same sentence, and those that were used as the
arguments of another predicate in a previous sen-
tence.
3 Experiments
3.1 Experimental Settings
Corpora: We used the NAIST Text Corpus ver-
sion 1.4b (Iida et al, 2007) and the Kyoto Text
Corpus 4.0 as the annotated corpora. We could
obtain dependency and predicate-argument struc-
tures because these corpora were annotated to al-
most the same newspaper articles. We divided
them into training, development, and test sets as
shown in Table 2.
Argument Identification Models: Maximum
entropy models were trained using the training set.
In these experiments, we used the Gaussian prior,
and the variance was tuned using the development
set. Candidate argument restrictions were applied
during both training and decoding.
Language Models: Language models were
trained from twelve years of newspaper articles
(Mainichi Shinbun newspaper 1991-2002, about
87
# of
Case Type Args. Prec. Rec. F
Nom. Dep. 14,287 85.2% 88.8% 87.0%
Zero-Intra 4,581 58.8% 43.4% 50.0%
Zero-Inter 3,063 47.5% 7.6% 13.1%
Total 21,931 79.4% 68.0% 73.2%
Acc. Dep. 9,316 95.6% 92.2% 93.9%
Zero-Intra 742 53.7% 21.6% 30.8%
Zero-Inter 271 25.0% 0.4% 0.7%
Total 10,329 94.3% 84.7% 89.2%
Dat. Dep. 5,409 91.1% 72.6% 80.8%
Zero-Intra 396 0.0% 0.0% 0.0%
Zero-Inter 139 0.0% 0.0% 0.0%
Total 5,944 91.1% 66.1% 76.6%
Table 3: Results on the Test Set
5.5M sentences) using the method described in
Section 2.2. However, we eliminated articles that
overlap the NAIST Corpus.
Evaluation: We evaluated the precision and re-
call rates, and F scores, all of which were com-
puted by comparing system output and the correct
answer of each argument. We also evaluated the
rate at which all arguments of a predicate were
completely identified as predicate-argument accu-
racy.
3.2 Results
The results are shown in Table 3. This table
shows accuracies of the argument identification
according to each case and each dependency re-
lation between predicates and arguments. The
predicate-argument accuracy on the test set was
59.4% (15,140/25,500).
First, focusing on the F scores of the Dep. rela-
tions, which denote a predicate and an argument in
the same sentence and directly depend upon each
other, scores of over 80% were obtained for all
cases. Compared with Taira et al (2008), they
were higher in the nominative and accusative cases
but were lower in the dative case. Overall, we ob-
tained F scores between 73.2% and 89.2%.
Next, focusing on the intra-sentential (Zero-
Intra) and inter-sentential (Zero-Intra) zero-
anaphora, the analyzer identified arguments at
some level from the viewpoint of precision. How-
ever, the recall rates and F scores were very
low. The Zero-Inter recall rate for the nominative
case, in which zero pronouns are centered, was
only 7.6%. This is because our method preferred
NULL phrases over unreliable phrases appearing
before the predicate sentence. In fact, the analyzer
output only 488 arguments, although the answer
was 3,063. To control the NULL preference is a
future work for our analyzer.
4 Discussions and Conclusions
We proposed a predicate-argument structure anal-
ysis that simultaneously conducts zero-anaphora
resolution. By adding noun phrases as candidate
arguments that are not only in the sentence of
the target predicate but also outside of the sen-
tence, our analyzer identified arguments regard-
less of whether they appear in the sentence or
not. Because we adopted discriminative models
for argument identification, we can easily add new
features. By using this property, we added lan-
guage model scores as well as contextual features.
We also used contextual information to restrict
candidate arguments. As a result, we achieved
predicate-argument accuracy of 59.4%, and accu-
racies of argument identification were F-scores of
73.2%?89.2%.
Verifying argument structures by language
models evokes selectional preference of case
frames. Sasano et al (2008) has proposed statis-
tical models using case frames built from 1.6 B
sentences. Because the amount of the resources
used in our study is quite different, we cannot di-
rectly compare the methods and results. However,
because our analyzer has scalability that can freely
add new features, for our future work, we hope to
adopt the case frames as new features and compare
their effect.
References
Ryu Iida, Mamoru Komachi, Kentaro Inui, and Yuji
Matsumoto. 2007. Annotating a Japanese text cor-
pus with predicate-argument and coreference rela-
tions. In Proceedings of the Linguistic Annotation
Workshop in ACL-2007, pages 132?139.
Zheng Ping Jiang and Hwee Tou Ng. 2006. Seman-
tic role labeling of nombank: A maximum entropy
approach. In Proceedings of EMNLP-2006, pages
138?145.
Shigeko Nariyama. 2002. Grammar for ellipsis res-
olution in Japanese. In Proceedings of TMI-2002,
pages 135?145.
Ryohei Sasano, Daisuke Kawahara, and Sadao Kuro-
hashi. 2008. A fully-lexicalized probabilistic model
for Japanese zero anaphora resolution. In Proceed-
ings of COLING-2008, pages 769?776.
Hirotoshi Taira, Sanae Fujita, and Masaaki Nagata.
2008. A Japanese predicate argument structure anal-
ysis using decision lists. In Proceedings of EMNLP-
2008, pages 523?532.
88
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 806?815, Dublin, Ireland, August 23-29 2014.
Predicate-Argument Structure Analysis with Zero-Anaphora Resolution
for Dialogue Systems
Kenji Imamura, Ryuichiro Higashinaka, and Tomoko Izumi
NTT Media Intelligence Laboratories, NTT Corporation
1-1 Hikari-no-oka, Yokosuka, 239-0847, Japan
{imamura.kenji,higashinaka.ryuchiro,izumi.tomoko}@lab.ntt.co.jp
Abstract
This paper presents predicate-argument structure analysis (PASA) for dialogue systems in
Japanese. Conventional PASA and semantic role labeling have been applied to newspaper arti-
cles. Because pronominalization and ellipses frequently appear in dialogues, we base our PASA
on a strategy that simultaneously resolves zero-anaphora and adapt it to dialogues. By incor-
porating parameter adaptation and automatically acquiring knowledge from large text corpora,
we achieve a PASA specialized to dialogues that has higher accuracy than that for newspaper
articles.
1 Introduction
Semantic role labeling (SRL) and predicate-argument structure analysis (PASA) are important analysis
techniques for acquiring ?who did what to whom? from sentences
1
. These analyses have been applied to
written texts because most annotated corpora comprise newspaper articles (Carreras and M`arquez, 2004;
Carreras and M`arquez, 2005; Matsubayashi et al., 2014).
Recently, systems for speech dialogue between humans and computers (e.g., Siri of Apple Inc. and
Shabette Concier of NTT DoCoMo) have become familiar with the popularization of smart phones. A
man-machine dialogue system has to interpret human utterances to associate them with system utter-
ances. The predicate-argument structure could be an effective data structure for dialogue management.
However, it is unclear whether we can apply the SRL/PASA for newspaper articles to dialogues because
there are many differences between them, such as the number of speakers, written or spoken language,
and context processing. For example, the following dialogue naturally includes pronouns, and thus
anaphora resolution is necessary for semantic role labeling.
A: [I]
ARG0
want [an iPad Air]
ARG1
.
B: [When]
ARGM
will [you]
ARG0
buy [it(=an iPad Air)]
ARG1
?
Similar phenomena exist in Japanese dialogues. However, most pronouns are omitted (called zero-
pronouns), and zero-anaphora resolution is necessary for Japanese PASA.
A: [iPad Air]
NOM
-ga hoshii-na.
iPad Air NOM. want
?? want an iPad Air.?
B: itsu ?
NOM
?
ACC
kau-no?
when buy?
?When will ? buy ???
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
Recent SRL systems assign labels of predicates and their arguments as semantic roles. Consequently, SRL and PASA are
very similar tasks. We use the term predicate-argument structure analysis in this paper because most Japanese analyzers use
this term.
806
This paper presents predicate-argument structure analysis with zero-anaphora resolution for Japanese
chat dialogues. Here, we regard the task of constructing PASA for dialogues as a kind of domain adap-
tation from newspaper articles to dialogues. M`arquez et al. (2008) and Pradhan et al. (2008) indicated
that the tuning of parameter distribution and reducing the out-of-vocabulary are important for the do-
main adaptation of SRL. We also focus on parameter distribution and out-of-vocabulary to construct a
PASA adapted to dialogues. To the best of our knowledge, this is the first paper to describe a PASA for
dialogues that include many zero-pronouns.
The paper is organized as follows. Section 2 briefly reviews SRL/PASA in English and Japanese.
Section 3 discusses characteristics of chat dialogues by comparing two annotated corpora, newspaper
articles and dialogues. Section 4 describes the basic strategy of our PASA, and Section 5 shows how it
was adapted for dialogues. Experiments are presented in Section 6, and Section 7 concludes the paper.
2 Related Work
2.1 Semantic Role Labeling in English
The advent of the supervised method proposed by Gildea and Jurafsky (2002) has led to the creation of
annotated corpora for semantic role labeling. In the CoNLL-2004 and 2005 shared task (Carreras and
M`arquez, 2004; Carreras and M`arquez, 2005), evaluations were carried out using the Proposition Bank
(Palmer et al., 2005). Because the Proposition Bank was annotated to the Penn Treebank (i.e., the source
texts were from the Wall Street Journal), the shared tasks were evaluated on newspaper articles. M`arquez
et al. (2008) provides a review of SRL.
OntoNotes Corpus (Hovy et al., 2006) contains multiple genres such as newswire, broadcast news,
broadcast conversation. The annotation to OntoNotes includes semantic role labels compliant with the
Proposition Bank. It is currently used for coreference resolution (Pradhan et al., 2012), and is expected
to be applied to dialogue analysis.
A few SRL studies have focused on not only verbal predicates (e.g., ?decide?) but also nominal predi-
cates (e.g., ?decision?) (Jiang and Ng, 2006; Gerber and Chai, 2012; Laparra and Rigau, 2013). Because
the subject and object of nominal predicates are frequently omitted (e.g., the object in the phrase ?the
decision? is omitted), problems similar to the Japanese zero-pronouns have to be resolved in the SRL of
nominal predicates.
2.2 Predicate-Argument Structure Analyses in Japanese
Japanese material includes the NAIST Text Corpus (Iida et al., 2007)
2
, which is an annotated corpus
of predicate-argument structures and coreference information for newspaper articles. Argument noun
phrases of the nominative, accusative, and dative cases are assigned to each predicate. The predicate and
the noun phrases are not limited to the same sentence. If arguments of the predicate are represented as
zero-pronouns, the antecedent noun phrases in other sentences are assigned as the arguments.
Many PASA methods have been studied on the NAIST Text Corpus (Komachi et al., 2007; Taira et al.,
2008; Imamura et al., 2009; Yoshikawa et al., 2011). In Japanese, some of them simultaneously resolve
the zero-anaphora caused by zero-pronouns.
Most English SRL and Japanese PASA currently target newspaper articles, and it is unclear whether
the methods for newspapers can be applied to dialogue conversations.
3 Characteristics of Chat Dialogues
We first collected chat dialogues of two speakers and annotated them with the predicate-argument struc-
ture. The participants chatted via keyboard input. Therefore, fillers and repetitions, which are frequent
in speech dialogues, were rare. The theme was one of 20 topics, such as meals, travel, hobbies, and
TV/radio programs. Annotation of the predicate-argument structure complied with the NAIST Text Cor-
pus. Figure 1 shows a chat dialogue example and its predicate-argument structure annotation.
2
http://cl.naist.jp/nldata/corpus/. We use version 1.5 with our own preprocessing in this paper. NAIST is
an acronym of ?Nara Institute of Science and Technology.?
807
A: natsu-wa (exo2)
NOM
(exog)
DAT
dekake-tari-shimashi-ta-ka?
?Did (you)
NOM
go (anywhere)
DAT
in this summer??
B: 8-gatsu-wa Ito-no [hanabi-taikai]
DAT
-ni (exo1)
NOM
yuki-mashi-ta.
?(I)
NOM
went to
[
the fireworks
?1
]
DAT
at Ito in August.?
A:
[
hanabi
?2
]
ACC
,
[
watashi
?3
]
NOM
-mo mi-takatta-desu.
?
[
Fireworks
?2
]
ACC
,
[
I
?3
]
NOM
also wanted to see (it).?
A: demo, kotoshi-wa (exo1)
NOM
isogashiku-te (exo1)
NOM
(
*
2)
ACC
mi-ni (
*
2)
DAT
ike-masen-deshita.
?But (I)
NOM
couldn?t go (?2)
DAT
to see (it=*2)
ACC
this year because (I)
NOM
was busy.?
Figure 1: Chat Dialogue Example and Its Predicate-Argument Structure Annotation
Lower lines denote glosses of the upper lines. The bold words denote predicates, the square brack-
ets [] denote intra-sentential arguments, and the round brackets () denote inter-sentential or exophoric
arguments.
# of Articles # of Sentences # of Words # of Predicates
Corpus Set /Dialogues /Utterances (per Sentence) (per Sentence)
NAIST Text Corpus Training 1,751 24,283 664,898 (27.4) 68,602 (2.83)
Development 480 4,833 136,585 (28.3) 13,852 (2.87)
Test 696 9,284 255,624 (27.5) 26,309 (2.83)
Chat Dialog Corpus Training 184 6,960 61,872 (8.9) 7,470 (1.07)
Test 101 4,056 38,099 (9.4) 5,333 (1.31)
Table 1: Sizes of Corpora
Zero- Zero- Exophora
Case Corpus # of Arguments Dep Intra Inter exo1 exo2 exog
Nominative NAIST 68,598 54.5% 17.3% 11.4% 2.0% 0.0% 14.7%
Dialogue 7,467 31.8% 7.4% 12.6% 23.9% 5.6% 18.8%
Accusative NAIST 27,986 89.2% 6.9% 3.4% 0.0% 0.0% 0.4%
Dialogue 1,901 46.6% 12.8% 27.5% 0.8% 0.1% 12.2%
Datative NAIST 6,893 84.7% 10.2% 4.3% 0.0% 0.0% 0.8%
Dialogue 2,089 37.6% 7.8% 15.0% 2.5% 1.1% 36.1%
Table 2: Distribution of Arguments in Training Corpora
Table 1 shows the statistics of the NAIST Text Corpus and the Chat Dialogue Corpus we created
3
.
The size of the Dialogue Corpus is about 10% of the NAIST Corpus. The NAIST Corpus is divided into
three parts: training, development, and test. The Dialogue Corpus is divided into training and test.
Table 2 shows distributions of arguments in the training sets of the NAIST/Dialogue corpora. We clas-
sified the arguments into the following six categories because each argument presents different difficulties
for analysis by its position and syntactic relation. The first two categories (Dep and Zero-Intra) are
the ones that in which the predicate and the argument occupy the same sentence.
? Dep: The argument directly depends on the predicate and vice versa on the parse tree.
? Zero-Intra: Intra-sentential zero-pronoun. The predicate and the argument are in the same
sentence, but there is no direct dependency.
? Zero-Inter: Inter-sentential zero-pronoun. The predicate and the argument are in different
sentences.
? exo1/exo2/exog: These are exophoric and denote zero-pronouns of the first person, second per-
son, and the others (general), respectively.
By Table 2, we can see that the ratios of Dep in all cases decreased in the Dialogue Corpus. In the other
categories, the tendencies between the nominative case and the accusative/dative cases were different. In
the nominative case, the Zero-Intra also decreased in the Dialogue Corpus, and the declines were
3
We regard a dialogue and an utterance as an article and a sentence, respectively.
808
exo1 exo2 exogNULL Phrase 1 Phrase 2 Phrase 3 Phrase 4 ?
Special Noun Phrases Candidate Argumentsin Past  Sentences Candidate Argumentsin Current Sentence
Candidate Arguments
SelectorNominativeModel SelectorAccusativeModel SelectorDativeModel
exo1exophoric(first person) zero-anaphoric(inter-sentential)
Phrase 2 NULLno argument
Figure 2: Structure of Argument Identification and Classification
assigned to exo1 and exo2. Namely, the arguments in a sentence were reduced, and zero-pronouns
increased compared with the newspaper articles. Note that many antecedents were the first or second
person. On the other hand, in the accusative and dative cases, the declines of the Dep were assigned to
the Zero-Inter or the exog in the Dialogue Corpus. Namely, anaphora resolution across multiple
sentences is important to dialogue analysis. In contrast, most arguments and the predicate appear in the
same sentence in the accusative/dative cases of newspapers.
4 Basic Strategy for Predicate-Argument Structure Analysis and Zero-Anaphora
Resolution
4.1 Architecture
We use Imamura et al. (2009)?s method developed for newspaper articles as the base PASA in this paper.
It can simultaneously identify arguments of a predicate in the sentence, those in other sentences, and
exophoric arguments. The analyzer receives the entire article (dialogue) and performs the following
steps for each sentence (utterance).
1. The input sentences are tagged and parsed. During parsing, the base phrases and their headwords
are also identified. At this time, the part-of-speech tags and the parse trees of the Dialogue Corpus
are supplied by applying the morphological analyzer MeCab (Kudo et al., 2004) and the dependency
parser CaboCha (Kudo and Matsumoto, 2002). The NAIST Corpus version 1.5 already includes the
part-of-speech tags and the parse trees.
2. Predicate phrases are identified from the sentences. We use the correct predicates in the corpora
for the evaluation. When we build dialogue systems on PASA, predicate phrases will be identified
using part-of-speech patterns that include verbs, adjectives, and copular verbs.
3. For each predicate, candidate arguments are acquired from the sentence that includes the predicate
(called the current sentence) and the past sentences. Concretely, the following base phrases are
regarded as candidates.
? All noun phrases in the current sentence are extracted as intra-sentential candidates regardless
of syntactic relations.
? From the past sentences, noun phrases are contextually extracted as inter-sentential candidates.
Details are described in Section 4.4.
? Exophoric labels (exo1, exo2, and exog) and the NULL (the argument is not required) are
added as special noun phrases.
809
4. The features are generated from the predicate phrase, candidate arguments, and their relations. The
best candidate for each case is independently selected (Figure 2).
4.2 Models
The models for the selector are based on maximum entropy classification. The selector identifies the best
noun phrase n? that satisfies the following equations from the candidate argument set N.
n? = argmax
n
j
?N
P (d(n
j
) = 1|X
j
;M
c
) (1)
P (d(n
j
) = 1|X
j
;M
c
) =
1
Z
c
(X)
exp
?
k
{?
ck
f
k
(d(n
j
) = 1, X
j
)} (2)
Z
c
(X) =
?
n
j
?N
exp
?
k
{?
ck
f
k
(d(n
j
) = 1, X
j
)} (3)
X
j
= ?n
j
, v, A? (4)
where n denotes a candidate argument, N denotes a set of candidate arguments of predicate v, d(n) is
a function that returns 1 iff candidate n becomes the argument, and M
c
denotes the model of case c. In
addition, f
k
(d(n
j
) = 1, X
j
) is a feature function, ?
ck
denotes a weight parameter of the feature function,
and A denotes the article from which all sentences are parsed.
Training phase optimizes the weight parameters in order to maximize the difference in posterior prob-
abilities among the correct noun phrase and the other candidates. Specifically, the model of case M
c
is
learnt by minimizing the following loss function `
c
.
`
c
= ?
?
i
logP (d(n
i
) = 1|X
i
;M
c
) +
1
2C
?
k
||?
ck
||
2
(5)
where n
i
denotes the correct noun phrase of the i-th predicate in the training set, X
i
denotes the i-th
tuple of the correct noun phrase, the predicate, and the article ?n
i
, v
i
, A
i
?. Since the posterior probability
is normalized for each set of candidate arguments of a predicate by Equation (3), the probability of
the correct noun phrase approaches closer to 1.0, and the probabilities of the other candidates approach
closer to 0.0 in Equation (5).
4.3 Features
Similar to other studies (e.g., (Gildea and Jurafsky, 2002)), we use three types of features: 1) predicate
features, 2) noun phrase (NP) features, and 3) the relationship between predicates and noun phrases
(Table 3). We also introduce combined features of the ?Noun? with all other binary features because the
features aim to select the best noun phrase.
The special features in this paper are the dependency language models (three types) and the obligatory
case information (?Frame? feature), which are automatically acquired from large text corpora. We discuss
them in Section 5.2.
4.4 Context Processing
Contexts of dialogues and newspaper articles are different. We should employ context processing spe-
cialized for the dialogues. However, contexts, including system and user utterances, should be managed
collectively by the dialogue manager from the viewpoint of dialogue systems. Thus, this study uses the
same context processing for the newspaper articles and dialogues. Note that the method in this paper
controls the context by selecting the inter-sentential candidates. We can easily alter context management
by providing candidate arguments from an external manager.
Context processing in this paper is as follows.
? From the current sentence, trace back to the past, and find a sentence that contains the other pred-
icate (we call this the prior sentence). This process aims to ignore utterances that do not contain
predicates.
810
Type Name Value Remark
Predicate Pred Binary Lemma of the predicate.
PType Binary Type of predicate. One of ?verb?, ?adjective?, and ?copular verb?.
Voice Binary Declarative or not. If not, the passive/causative auxiliary verb is assigned.
Suffix Binary Sequence of the functional words of the main clause. This feature aims to reflect
the speech act of the utterance.
Frame Binary Obligatory case information. The case requires argument (1) or not (0).
Noun Phrase Noun Binary Headword of the NP
Particle Binary Case particle of the base phrase. If the NP is a special noun phrase, this is NULL.
NType Binary If the substance of the NP is in the article, this is ?NP?; otherwise the same value
of the ?Noun? feature.
Surround Binary POS tags of the surrounding words of the NP. The window size is ?2.
Relation
between
Predicate and
NP
PhPosit Binary Distance between the predicate and the NP. If they are in different sentences, or
the NP is an exophora, this is NULL.
Syn Binary Dependency path between the predicate and the NP. If they are in different sen-
tences, or the NP is an exophora, this is NULL.
Speaker Binary Whether the speakers of the predicate and the NP are the same (SAME) or not
(OTHER).
Dependency
Language
Models
log P (n|c, v) Real Generation probability of NP n given predicate v and case c.
log P (v|c, n) Real Generation probability of predicate v given NP n and case c.
log P (c|n) Real Generation probability of case c given NP n.
Table 3: List of Features
? All noun phrases that lie between the prior to the current sentence are added to the candidate argu-
ments. In addition, noun phrases that are used as arguments of any predicates are also added (called
argument recycling (Imamura et al., 2009)). Argument recycling covers wide contexts because it
can employ distant noun phrases if the past predicates have inter-sentential arguments.
5 Adaptation to Chat Dialogues
The method described in the previous section is common to dialogues and newspaper articles. This
section describes the adaptation made to target dialogues.
5.1 Adaptation of Model Parameters
In order to tune the difference in the argument distribution, model parameters of the selectors are adapted
to the dialogue domain. We use the feature augmentation method (Daum?e, 2007) as the domain adap-
tation technique; it has the same effect as regarding the source domain to be prior knowledge, and the
parameters are optimized to the target domain. Concretely, the models of the selectors are learnt and
applied as follows.
1. First, the feature space is segmented into three parts: common, source, and target.
2. The NAIST Corpus and the Dialogue Corpus are regarded as the source and the target domains,
respectively. The features from the NAIST Corpus are deployed to the common and the source
spaces, and those from the Dialogue Corpus are deployed to the common and the target spaces.
3. The parameters are estimated in the usual way on the above feature space. The weights of the
common features are emphasized if the features are consistent between the source and target. With
regard to domain-dependent features, the weights in the respective space, source or target, are em-
phasized.
4. When the argument is identified, the selectors use only the features in the common and target spaces.
The parameters in the spaces are optimized to the target domain, plus we can utilize the features
that appear only in the source domain data.
5.2 Weak Knowledge Acquisition from Very Large Resources
In this paper, we use two types of knowledge to reduce the harmful effect of out-of-vocabulary in the
training corpus. Both types are constructed by automatically analyzing, summing up, and filtering large
811
text corpora (Kawahara and Kurohashi, 2002; Sasano et al., 2008; Sasano et al., 2013). They provide
information about unknown words with some confidence but they do contain some errors. We use them
as the features of the models, and parameters are optimized by the discriminative learning of the selectors.
5.2.1 Obligatory Case Information (Frame Feature)
Case frames are important clues for SRL and PASA. The obligatory case information (OCI) comprises
subsets of the case frames that only clarify whether the cases of each predicate are necessary or not.
The OCI dictionary is automatically constructed from large text corpora as follows. The process
assumes that 1) most of the cases match the case markers if the noun phrase directly depends on the
predicate, and 2) if the case is obligatory, the occurrence rate on a specific predicate is higher than the
average rate of all predicates.
1. Similar to PASA in this paper (c.f., Section 4.1), predicates and base phrases are identified by
tagging and parsing raw texts.
2. Noun phrases that directly depend on the predicate and accompany a case marker are extracted. We
sum up the frequency of the predicate and cases.
3. Highly frequent predicates are selected according to the final dictionary size. Obligation of the cases
is determined so as to satisfy the following two conditions.
? Co-occurrence of the predicate and the case ?v, c? are higher than the significance level (p ?
0.001; LLR ? 10.83) by the log-likelihood-ratio test.
? The case of the predicate appears at least 10% more frequently than the average of all predi-
cates.
We constructed two OCI dictionaries. The Blog dictionary contains about 480k predicates from one
year of blogs (about 2.3G sentences,). The News dictionary contains about 200k predicates from 12
years of newspaper articles (about 7.7M sentences). The coverage of predicates in the training set of the
Dialogue Corpus was 98.5% by the Blog dictionary and 96.4% by the News dictionary.
5.2.2 Dependency Language Models
Dependency language models (LMs) represent semantic/pragmatic collocations among predicate v, case
c, and noun phrase n. The generation probabilities of v, c, and n are computed by n-gram models. More
concretely, the following real values are computed. The purpose of the biases (probabilities involved
<unk>) is to correct the values to be positive.
? logP (n|c, v) ? logP (<unk>|c, v)
? logP (v|c, n) ? logP (v|c,<unk>)
? logP (c|n) ? logP (c|<unk>)
Each dependency LM is constructed from the tuples of ?v, c, n? extracted in Section 5.2.1 using the
SRILM (Stolcke et al., 2011). Note that since the obligatory case information corresponds to the gener-
ation probability of the case (P (c|v)), we exclude it from the dependency LMs.
Similar to the OCI dictionaries, we constructed two sets of dependency language models from the Blog
and the News sentences. The coverage of triples ?v, c, n? appeared in the training set of the Dialogue
Corpus was 76.4% by the Blog LMs and 38.3% by the News LMs. The Blog LMs cover the Dialogue
Corpus more comprehensively than the News LMs.
6 Experiments
We evaluate the accuracies of the proposed PASA on the Dialogue Corpus (Table 1) from the perspectives
of parameter adaptation and the effect of the automatically acquired knowledge. The evaluation metric
is F-measure of each case (includes exophora identification).
812
a) Adaptation b) NAIST? c) Dialogue? d) Adaptation e) Adaptation
# of OCI:Blog OCI:Blog OCI:Blog OCI:News? OCI:Blog
Case Type Args. LMs:Blog LMs:Blog LMs:Blog LMs:Blog LMs:News?
Nominative Dep 1,811 83.3%?? 77.6% 82.7% 83.0% 82.7%
Zero-Intra 511 37.4% 43.7%? 36.6% 36.5% 38.1%
Zero-Inter 767 8.6% ? 9.1% 9.0% 8.3% 4.5%
exo1 1,193 70.2%? 13.5% 69.9% 70.1% 70.3%
exo2 281 46.8%?? 0.0% 43.1% 47.2% 46.8%
exog 767 46.8%? 32.5% 27.9% 47.2% 47.7%?
Total 5,330 61.5%? 44.4% 61.1% 61.4% 61.4%
Accusative Dep 614 84.2%?? ? 78.6% 81.5% 84.2% 82.4%
Zero-Intra 149 42.9%? ?? 27.1% 45.0% 38.9% 34.3%
Zero-Inter 399 30.4%? ? 0.5% 30.9% 29.4% 24.3%
exo1 19 0.0% 0.0% 0.0% 9.5% 10.0%
exo2 7 0.0% 0.0% 0.0% 0.0% 0.0%
exog 98 25.6%? 0.0% 27.9% 25.2% 25.6%
Total 1,286 59.0%? ? 51.6% 58.9% 58.4% 56.0%
Dative Dep 566 80.5%?? 54.0% 79.0% 80.1% 80.7%
Zero-Intra 70 20.7%? ? 0.0% 20.0% 20.7% 11.8%
Zero-Inter 169 14.6%? 0.0% 14.8% 14.4% 13.4%
exo1 32 0.0% 0.0% 0.0% 0.0% 0.0%
exo2 4 0.0% 0.0% 0.0% 0.0% 0.0%
exog 265 45.4%?? 0.0% 43.1% 44.0% 44.9%
Total 1,106 58.6%?? 32.2% 57.2% 58.2% 58.4%
Table 4: F-measures among Methods/OCI dictionary/Dependency LMs on Dialogue Test Set
The bold values denote the highest F-measures among all methods. The marks ?, ?, ?, ? denote sig-
nificantly better methods by comparing a) with b), c), d), and e), respectively. We used the bootstrap
resampling method (1,000 iterations) as the significance test, in which the significance level was 0.05.
6.1 Experiment 1: Effect of Parameter Adaptation
We compared three methods in order to evaluate parameter adaptation: a) The feature augmentation is
applied to the training (Adaptation). b) Only the NAIST Corpus is used for training (NAIST Training).
c) Only the Dialogue Corpus is used (Dialogue Training). The NAIST Training corresponds to a conven-
tional PASA for newspaper articles. The results on the Dialogue test set are shown in the 4th, 5th, and
6th columns in Table 4.
First, comparing methods a) Adaptation and b) NAIST training, Adaptation was better than the NAIST
training for most types (The ? mark denotes ?significantly better?). In particular, the total F-measures
of all cases were significantly better than NAIST training. Focusing on the types of arguments, the most
characteristic results were exophoras of the first/second persons (exo1 and exo2) of the nominative
case. These two types dominate of the nominative case (about 28%), and exo1 (70.2%) and exo2
(46.8%) became analyzable. Other types such as the Zero-Inter and the exog of the accusative and
dative cases, which could not be analyzed by NAIST training, became analyzable.
Comparing methods a) Adaptation and c) Dialogue training (c.f., ?), the F-measures of Dialogue
training approached those of Adaptation even though the size of the Dialogue Corpus was small. Only
the F-measure of the dative case of Adaptation was significantly better than Dialogue training in total.
This does not imply that the corpus size is sufficient. Rather, we suppose that the Adaptation strategy
could not adequately utilize the advantages of the NAIST Corpus. Adding more dialogue data would
further improve the accuracies on the Dialogue test set.
6.2 Experiment 2: Differences among Automatically Acquired Knowledge
The columns a), d), and e) in Table 4 show the results for the proposed method (Adaptation). Note that
the combination of the OCI dictionary and the dependency language models were changed to a) ?Blog,
Blog?, d) ?News, Blog?, and e) ?Blog, News?.
When the OCI dictionary was changed from a) Blog to d) News (c.f., ?), there were no significant
differences in almost all types except for the Zero-Intra of the accusative case. We suppose that this
813
is because the coverage of the Blog and News dictionaries were almost the same, and obligatory cases of
predicates are general information regardless of the domain.
On the contrary, when the dependency LMs were changed from a) Blog to e) News (c.f., ?), the F-
measures of some types significantly dropped, especially the Zero-Intra and Zero-Inter types,
which are strongly influenced by semantic relation. For example, the Zero-Inter type of the ac-
cusative case was changed from 30.4% to 24.3%, and the F-measure consequently decreased by 3.0
points in total in the accusative case. Zero-anaphora resolution cannot rely on syntax, and the dependency
LMs that measure semantic collocation become relatively important. The Blog LMs yielded greater cov-
erage than the News LMs in this experiment. We can conclude that high-coverage LMs are better for
improving the zero-anaphora resolution.
7 Conclusion
This paper presented predicate-argument structure analysis with zero-anaphora resolution for dialogues.
We regarded this task as a kind of domain adaptation from newspaper articles, which are conventionally
studied, to dialogues. The model parameters were adapted to the dialogues by using a domain adapta-
tion technique. In order to address the out-of-vocabulary issue, the obligatory case information and the
dependency language models were constructed from large text corpora and applied to the selectors.
As a result, arguments that could not be analyzed by PASA for newspaper articles (e.g., zero-pronouns
of the first and second persons in the nominative case) became analyzable by adding only a small number
of dialogues. The parameter adaptation achieved some improvement. Moreover, we confirmed that high-
coverage dependency LMs contribute to improving zero-anaphora resolution and the overall accuracy.
Although we focused on parameter distribution and out-of-vocabulary in this paper, there are the other
differences between dialogues and newspaper articles. For example, we did not discuss the exchange
of turns, which is a special phenomenon of dialogues. To consider further phenomena is our future
work. We are also evaluating the effectiveness of our PASA by incorporating it into a dialogue system
(Higashinaka et al., 2014).
References
Xavier Carreras and Llu??s M`arquez. 2004. Introduction to the CoNLL-2004 shared task: Semantic role labeling.
In Hwee Tou Ng and Ellen Riloff, editors, HLT-NAACL 2004 Workshop: Eighth Conference on Computational
Natural Language Learning (CoNLL-2004), pages 89?97, Boston, Massachusetts, USA, May.
Xavier Carreras and Llu??s M`arquez. 2005. Introduction to the CoNLL-2005 shared task: Semantic role labeling.
In Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL-2005), pages
152?164, Ann Arbor, Michigan, June.
Hal Daum?e, III. 2007. Frustratingly easy domain adaptation. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 256?263, Prague, Czech Republic, June.
Matthew Gerber and Joyce Y. Chai. 2012. Semantic role labeling of implicit arguments for nominal predicates.
Computational Linguistics, 38(4):755?798.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics,
28(3):245?288.
Ryuichiro Higashinaka, Kenji Imamura, Toyomi Meguro, Chiaki Miyazaki, Nozomi Kobayashi, Hiroaki
Sugiyama, Toru Hirano, Toshiro Makino, and Yoshihiro Matsuo. 2014. Towards an open domain conversa-
tional system fully based on natural language processing. In Proceedings of the 25th International Conference
on Computational Linguistics (COLING 2014), Dublin, Ireland, August.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. OntoNotes: The
90% solution. In Proceedings of the Human Language Technology Conference of the NAACL, Companion
Volume: Short Papers, pages 57?60, New York City, USA, June.
Ryu Iida, Mamoru Komachi, Kentaro Inui, and Yuji Matsumoto. 2007. Annotating a Japanese text corpus with
predicate-argument and coreference relations. In Proceedings of the Linguistic Annotation Workshop, pages
132?139, Prague, Czech Republic, June.
814
Kenji Imamura, Kuniko Saito, and Tomoko Izumi. 2009. Discriminative approach to predicate-argument structure
analysis with zero-anaphora resolution. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers,
pages 85?88, Singapore, August.
Zheng Ping Jiang and Hwee Tou Ng. 2006. Semantic role labeling of NomBank: A maximum entropy approach.
In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 138?145,
Sydney, Australia, July.
Daisuke Kawahara and Sadao Kurohashi. 2002. Fertilization of case frame dictionary for robust Japanese case
analysis. In Proceedings of the 19th International Conference on Computational Linguistics (COLING-2002),
pages 425?431, Taipei, Taiwan, August.
Mamoru Komachi, Ryu Iida, Kentaro Inui, and Yuji Matsumoto. 2007. Learning-based argument structure analy-
sis of event-nouns in Japanese. In Proceedings of the Conference of the Pacific Association for Computational
Linguistics (PACLING), pages 208?215, Melbourne, Australia, September.
Taku Kudo and Yuji Matsumoto. 2002. Japanese dependency analysis using cascaded chunking. In CoNLL
2002: Proceedings of the 6th Conference on Natural Language Learning 2002 (COLING 2002 Post-Conference
Workshops), pages 63?69, Taipei, Taiwan.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto. 2004. Applying conditional random fields to Japanese
morphological analysis. In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 230?237,
Barcelona, Spain, July.
Egoitz Laparra and German Rigau. 2013. ImpAr: A deterministic algorithm for implicit semantic role labelling.
In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pages 1180?1189, Sofia, Bulgaria, August.
Llu??s M`arquez, Xavier Carreras, Kenneth C. Litkowski, and Suzanne Stevenson. 2008. Semantic role labeling:
An introduction to the special issue. Computational Linguistics, 34(2):145?159.
Yuichiro Matsubayashi, Ryu Iida, Ryohei Sasano, Hikaru Yokono, Suguru Matsuyoshi, Atsushi Fujita, Yusuke
Miyao, and Kentaro Inui. 2014. Issues on annotation guidelines for Japanese predicate-argument structures.
Journal of Natural Language Processing, 21(2):333?377, April. in Japanese.
Martha Palmer, Daniel Gildia, and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?105.
Sameer S. Pradhan, Wayne Ward, and James H. Martin. 2008. Towards robust semantic role labeling. Computa-
tional Linguistics, 34(2):289?310.
Sameer Pradhan, Alessandro Moschitti, and Nianwen Xue, editors. 2012. Joint Conference on EMNLP and
CoNLL: Proceeding of the Shared Task: Modeling Multilingual Unrestricted Coreference in Onto Notes, Jeju,
Korea, July.
Ryohei Sasano, Daisuke Kawahara, and Sadao Kurohashi. 2008. A fully-lexicalized probabilistic model for
Japanese zero anaphora resolution. In Proceedings of the 22nd International Conference on Computational
Linguistics (Coling 2008), pages 769?776, Manchester, UK, August.
Ryohei Sasano, Daisuke Kawahara, Sadao Kurohashi, and Manabu Okumura. 2013. Automatic knowledge ac-
quisition for case alternation between the passive and active voices in Japanese. In Proceedings of the 2013
Conference on Empirical Methods in Natural Language Processing, pages 1213?1223, Seattle, Washington,
USA, October.
Andreas Stolcke, Jing Zheng, Wen Wang, and Victor Abrash. 2011. SRILM at sixteen: Update and outlook.
In Proceedings of IEEE Automatic Speech Recognition and Understanding Workshop (ASRU 2011), Waikoloa,
Hawaii, December.
Hirotoshi Taira, Sanae Fujita, and Masaaki Nagata. 2008. A Japanese predicate argument structure analysis using
decision lists. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,
pages 523?532, Honolulu, Hawaii, October.
Katsumasa Yoshikawa, Masayuki Asahara, and Yuji Matsumoto. 2011. Jointly extracting Japanese predicate-
argument relation with markov logic. In Proceedings of 5th International Joint Conference on Natural Lan-
guage Processing, pages 1125?1133, Chiang Mai, Thailand, November.
815
Proceedings of the Multiword Expressions: From Theory to Applications (MWE 2010), pages 64?72,
Beijing, August 2010
Standardizing Complex Functional Expressions in Japanese  
Predicates: Applying Theoretically-Based Paraphrasing Rules  
 
Tomoko Izumi?    Kenji Imamura?    Genichiro Kikui?    Satoshi Sato? 
?NTT Cyber Space Laboratories, 
NTT Corporation 
{izumi.tomoko, imamura.kenji, 
kikui.genichiro}@lab.ntt.co.jp
?Graduate School of Engineering,  
Nagoya University 
ssato@nuee.nagoya-u.ac.jp 
 
 
 
Abstract 
In order to accomplish the deep semantic 
understanding of a language, it is essen-
tial to analyze the meaning of predicate 
phrases, a content word plus functional 
expressions. In agglutinating languages 
such as Japanese, however, sentential 
predicates are multi-morpheme expres-
sions and all the functional expressions 
including those unnecessary to the mean-
ing of the predicate are merged into one 
phrase. This triggers an increase in sur-
face forms, which is problematic for 
NLP systems. We solve this by introduc-
ing simplified surface forms of predi-
cates that retain only the crucial meaning 
of the functional expressions. We con-
struct paraphrasing rules based on syn-
tactic and semantic theories in linguistics. 
The results of experiments show that our 
system achieves the high accuracy of 
77% while reducing the differences in 
surface forms by 44%, which is quite 
close to the performance of manually 
simplified predicates. 
 
1 Introduction 
The growing need for text mining systems such 
as opinion mining and sentiment analysis re-
quires the deep semantic understanding of lan-
guages (Inui et al, 2008). In order to accomplish 
this, one needs to not only focus on the meaning 
of a single content word such as buy but also the 
meanings conveyed by function words or func-
tional expressions such as not and would like to. 
In other words, to extract and analyze a predi-
cate, it is critical to consider both the content 
word and the functional expressions (Nasukawa, 
2001). For example, the functional expressions 
would like to as in the predicate ?would like to 
buy? and can?t as in ?can?t install? are key ex-
pressions in detecting the customer?s needs and 
complaints, providing valuable information to 
marketing research applications, consumer opi-
nion analysis etc.  
Although these functional expressions are 
important, there have been very few studies that 
extensively deal with these functional expres-
sions for use in natural language processing 
(NLP) systems (e.g., Tanabe et al, 2001; Mat-
suyoshi and Sato, 2006, 2008). This is due to the 
fact that functional expressions are syntactically 
complicated and semantically abstract and so are 
poorly handled by NLP systems. 
In agglutinating languages such as Japanese, 
functional expressions appear in the form of 
suffixes or auxiliary verbs that follow the 
content word without any space. This sequence 
of a content word (c for short) plus several of 
functional expressions (f for short) forms a 
predicate in Japanese (COMP for completive 
aspect marker, NOM for nominalizer, COP for 
copular verb).   
(1) kat -chai -takat -ta -n -da 
buy -COMP -want -PAST -NOM -COP 
c -f1 -f2 -f3 -f4 -f5 
?(I) wanted to buy (it)? 
The meaning of ?want to? is expressed by -tai 
(f2) and the past tense is expressed by -ta (f3). 
64
The other functional expressions, -chai(f1), -n(f4), 
and -da(f5), only slightly alter the predicative 
meaning of ?wanted to buy,? as there is no direct 
English translation. Therefore, (1) expresses the 
same fact as (2). 
(2)  kai -takat -ta 
  buy -want -PAST 
?(I) wanted to buy (it).? 
As shown, in Japanese, once one extracts a 
predicate phrase, the number of differences in 
surface forms increases drastically regardless of 
their similarities in meaning. This is because 
sentential predicates are multi-word or multi-
morpheme expressions and there are two differ-
ent types of functional expressions, one which is 
crucial for the extraction of predicative meaning 
and the other, which is almost unnecessary for 
NLP applications. This increase in surface forms 
complicates NLP systems including text mining 
because they are unable to recognize that these 
seemingly different predicates actually express 
the same fact. 
In this study, we introduce paraphrasing rules 
to transform a predicate with complex functional 
expressions into a simple predicate. We use the 
term standardize to refer to this procedure. 
Based on syntactic and semantic theories in lin-
guistics, we construct a simple predicate struc-
ture and categorize functional expressions as 
either necessary or unnecessary. We then pa-
raphrase a predicate into one that only retains the 
crucial meaning of the functional expression by 
deleting unnecessary functional expressions 
while adding necessary ones. 
 The paper is organized as follows. In Section 
2, we provide related work on Japanese 
functional expressions in NLP systems as well as 
problems that need to be solved. Section 3 
introduces several linguistic theories and our 
standardizing rules that we constructed based on 
these theories. Section 4 describes the 
experiments conducted on our standardization 
system and the results. Section 5 discusses the 
results and concludes the paper. Throughout this 
paper, we use the term functional expressions to 
indicate not only a single function word but also 
compounds (e.g., would like to). 
 
2 Previous Studies and Problems  
Shudo et al (2004) construct abstract semantic 
rules for functional expressions and use them in 
order to find whether two different predicates 
mean the same. Matsuyoshi and Sato (2006, 
2008) construct an exhaustive dictionary of 
functional expressions, which are hierarchically 
organized, and use it to produce different func-
tional expressions that are semantically equiva-
lent to the original one.  
 Although these studies provide useful in-
sights and resources for NLP systems, if the in-
tention is to extract the meaning of a predicate, 
we find there are still problems that need to be 
solved. There are two problems that we focus on. 
 The first problem is that many functional ex-
pressions are unnecessary, i.e., they do not ac-
tually alter the meaning of a predicate.   
(3) yabure -teshimat -ta -no -dearu 
 rip -COMP -PAST -NOM -COP 
 c -f1 -f2 -f3 -f4  
 ?(something) ripped.? 
(3) can be simply paraphrased as (4) 
(4) yabure -ta 
 rip -PAST 
 c -f1  
In actual NLP applications such as text mining, 
it is essential that the system recognizes that (3) 
and (4) express the same event of something 
?ripped.? In order to achieve this, the system 
needs to recognize -teshimat, -no, and -dearu as 
unnecessary (f1, f3, f4 ??). Previous studies that 
focus on paraphrasing of one functional expres-
sion to another (f ? f?) cannot solve this prob-
lem. 
 The second problem is that we sometimes 
need to add certain functional expressions in 
order to retain the meaning of a predicate (? ?f). 
(5) (Hawai-ni) P1iki, P2nonbirishi -takat -ta 
 (Hawaii-to)  go relax -want -PAST 
   c1 c2 f1 f2 
?I wanted to go to Hawaii and relax.? 
(5) has a coordinate structure, and two verbal 
predicates, iki (P1) ?go? and nonbirishi-takat-ta 
(P2) ?wanted to relax?, are coordinated.  
 As the English translation indicates, the first 
predicate in fact means iki-takat-ta ?wanted to 
65
go,? which implies that the speaker was not able 
to go to Hawaii. If the first predicate was ex-
tracted and analyzed as iku, the base (present) 
form of ?go,? then this would result in a wrong 
extraction of predicate, indicating the erroneous 
fact of going to Hawaii in the future (Present 
tense in Japanese expresses a future event). In 
this case, we need to add the functional expres-
sions takat ?want? and ta, the past tense marker, 
to the first verbal predicate.  
As shown, there are two problems that need 
to be solved in order for a system to extract the 
actual meaning of a predicate. 
i. Several functional expressions are neces-
sary for sustaining the meaning of the event 
expressed by a predicate while others barely 
alter the meaning (f ??). 
ii. Several predicates in coordinate sentences 
lack necessary functional expressions at the 
surface level (? ?f) and this results in a 
wrong extraction of the predicate meaning. 
Based on syntactic and semantic theories in lin-
guistics, we construct paraphrasing rules and 
solve these problems by standardizing complex 
functional expressions. 
 
3 Construction of Paraphrasing Rules  
The overall flow of our standardizing system is 
depicted in Figure 1. The system works as fol-
lows. 
i. Given a parsed sentence as an input, it ex-
tracts a predicate(s) and assigns a semantic 
label to each functional expression based on 
Matsuyoshi and Sato (2006). 
ii. As for an intermediate predicate, necessary 
functional expressions are added if missing 
(? ?f). 
iii. From each predicate, delete unnecessary 
functional expressions that do not alter the 
meaning of the predicate (f ??). 
iv. Conjugate each element and generate a 
simplified predicate.  
There are two fundamental questions that we 
need to answer to accomplish this system. 
A) What are UNNECCESARY functional ex-
pressions (at least for NLP applications), 
i.e., those that do not alter the meaning of 
the event expressed by a predicate? 
B) How do we know which functional expres-
sions are missing and so should be added? 
We answer these questions by combining what is 
needed in NLP applications and what is dis-
cussed in linguistic theories. We first answer 
Question A. 
 
3.1 Categorization of Functional Expressions 
As discussed in Section 1 and in Inui et al 
(2008), what is crucial in the actual NLP appli-
cations is to be able to recognize whether two 
seemingly different predicates express the same 
fact.  
This perspective of factuality is similar to the 
truth-value approach of an event denoted by pre-
dicates as discussed in the field of formal seman-
tics (e.g., Chierchia and Mcconnel-Ginet, 2000; 
Portner, 2005). Although an extensive investiga-
tion of these theories is beyond the scope of this 
paper, one can see that expressions such as tense 
(aspect), negation as well as modality, are often 
discussed in relation to the meaning of an event 
(Partee et al, 1990; Portner, 2005). 
Tense (Aspect): Expresses the time in (at/for) 
which an event occurred. 
Negation: Reverses the truth-value of an event. 
Modality: Provides information such as possi-
bility, obligation, and the speaker?s eagerness 
with regard to an event and relate it to what is 
true in reality. 
The above three categories are indeed useful in 
explaining the examples discussed above. 
(6) kat -chai -takat -ta -n -da 
buy -COMP -want -PAST -NOM -COP 
 aspect modality tense(aspect) 
(7) kai -takat -ta 
 buy -want -PAST 
  modality tense(aspect) 
 ?wanted to buy? 
The predicate ?kat-chai-takat-ta-n-da? in (6) and 
?kai-takat-ta? in (7) express the same event be-
cause they share the same tense (past), negation 
(none), and modality (want). Although (6) has 
the completive aspect marker -chai while (7) 
does not, they still express the same fact. This is 
because the Japanese past tense marker -ta also 
has a function to express the completive aspect. 
The information expressed by -chai in (6) is re-
66
dundant and so unnecessary.  
On the other hand, the predicate ?iku? in (5) 
and ?iki-takat-ta,? which conveys the actual 
meaning of the predicate, express a different fact 
because they establish a different tense (present 
vs. past) and different modality (none vs. want).  
As shown, once we examine the abstract se-
mantic functions of functional expressions, we 
can see the factual information in a predicate is 
influenced by tense (aspect), negation, and mod-
ality. Therefore, the answer to Question A is that 
necessary functional expressions are those that 
belong to tense (aspect), negation, and modality. 
Furthermore, if there are several functional ex-
pressions that have the same semantic function, 
retaining one of them is sufficient. 
 
3.2 Adding Necessary Functional Expressions 
The next question that we need to answer is how 
we find which functional expressions are miss-
ing when standardizing an intermediate predicate 
in a coordinate structure (e.g., (5)). We solve this 
based on a detailed analysis of the syntactic 
structure of predicates. 
Coordinate structures are such that several 
equivalent phrases are coordinated by conjunc-
tions such as and, but, and or. If a predicate is 
coordinated with another predicate, these two 
predicates must share the same syntactic level. 
Therefore, the structure in (5) is indeed depicted 
as follows (What TP and ModP stand for will be 
discussed later). 
[TP[ModP[VP(Hawai-ni) iki][VPnonbirishi]takat]ta ] 
[TP[ModP[VP(Hawaii-to) go][VPrelax] want]PAST] 
This is the reason why the first predicate iki 
should be paraphrased as iki-takat-ta ?wanted to 
go.? It needs to be tagged with the modality ex-
pression tai and the past tense marker ta, which 
seems to attach only to the last predicate.  
 This procedure of adding necessary function-
al expressions to the intermediate predicate is 
not as simple as it seems, however.   
(8) nemutai-mitai-de kaeri -tagatte -tei -ta 
sleepy-seems-COP gohome-want-CONT-PAST 
?He seemed sleepy and wanted to go home.? 
In (8), the first predicate nemutai-mitai-de ?seem 
to be sleepy? should be paraphrased as nemutai-
mitai-dat-ta, ?seemed to be sleepy,? in which 
only the functional expression indicating past is 
required. The other functional expressions such 
as tagatte ?want,? and the aspect marker tei 
(CONTinuation) should not be added (nemutai-
mitai-de-tagat(want)-tei(CONT)-ta(PAST) is 
completely ungrammatical).  
Input  
A parsed Sentence 
Hontoo-wa Hawai-ni iki, nonbirishi takat ta n da kedo
Really-TOP Hawaii-to go relax want PAST NOM COP but 
?I wanted to go to Hawaii and relax if I could.? 
i. Predicate Extraction &
Labeling Semantic Classes 
to Functional Expressions 
ii. ADD necessary 
functional expressions 
(? ? f) 
iii. DELETE unnecessary 
functional expressions 
(f ? ?) 
iv. Conjugate and  
Generate simple predicates 
Output  
Simplified Predicates 
iki 
go 
c 
[[[VP] ?] ?] 
iki tai ta
go want PAST
c [??] [??]
iki takat ta
go want PAST 
nonbirishi takat ta  
relax want PAST  
iki-takat-ta
?wanted to go? 
nonbirishi-takat-ta 
?wanted to relax? 
Figure 1. The flow of Standardization. 
iki tai ta
go want PAST 
c [??] [??] 
nonbirishi takat ta n da kedo 
relax want PAST NOM COP but 
c [??] [??] [??] [??] [????]
nonbirishi takat ta n da kedo 
relax  want PAST NOM COP but 
c [??] [??] [??] [??] [????] 
[[[VP]             ModP]    TP] 
67
 Furthermore, the intermediate predicate in the 
following example does not allow any functional 
expressions to be added. 
(9) (imawa) yasui-ga (mukashiwa) takakat-ta 
(today) inexpensive-but (in old days) expensive-
PAST 
?(They) are inexpensive (today), (but) used to 
be very expensive (in the old days.)? 
In (9), the first predicate yasui ?inexpensive? 
should not be paraphrased as yasukat-ta ?was 
inexpensive? since this would result in the un-
grammatical predicate of ?*(they) were inexpen-
sive (today).?  
 In order to add necessary functional expres-
sions to an intermediate predicate, one needs to 
solve the following two problems. 
i. Find whether the target predicate indeed 
lacks necessary functional expressions.  
ii. If such a shortfall is detected, decide which 
functional expressions should be added to 
the predicate. 
We solve these problems by turning to the in-
completeness of the syntactic structure of a pre-
dicate. 
Studies such as Cinque (2006) and Rizzi 
(1999) propose detailed functional phrases such 
as TopP (Topic Phrase) in order to fully describe 
the syntactic structures of a language. We adopt 
this idea and construct a phrase structure of Jap-
anese predicates which borrows from the func-
tional phrases of TP, ModP, and FocP (Figure 2). 
 ModP stands for a modality phrase and this is 
where modality expressions can appear.1  FocP 
stands for a focus phrase. This is the phrase 
where the copula da appears. This phrase is 
needed because several modality expressions 
syntactically need the copula da in either the 
following or preceding position (Kato, 2007). 
The existence of FocP also indicates that the 
modality expressions within the phrase are com-
plete (no more modality phrase is attached). TP 
stands for a tense phrase and this is where the 
tense marker appears.  
 Note that this structure is constructed for the 
purpose of Standardization and other functional 
                                                 
1 The structure of Figure 2 is recursive. A modality expres-
sion can appear after a TP. Also, more than one ModP can 
appear although ModP and FocP are optional. 
projections such as NegP (negation phrase) will 
not be discussed although we assume there must 
be one. Based on the predicate structure in Fig-
ure 2, we solve the two problems as follows. 
 
The first problem: Detecting whether the target 
predicate lacks necessary functional expressions.  
? If the predicate has the past tense marker ta 
or if the coordinate conjunction following 
the predicate is for combining phrases with 
tense, then consider the predicate as com-
plete and do not add any functional expres-
sions. Otherwise, consider the predicate as 
incomplete and add the appropriate func-
tional expressions. 
The underlying principle of this rule is that if a 
predicate is tensed, then its syntactic structure is 
complete. As often described in syntactic theo-
ries (e.g., Adger, 2003), a sentence can be said to 
be a phrase with tense (i.e., TP). In other words, 
if a predicate is tensed, then it can stand alone as 
a sentence.  
 By adopting this idea, we judge the com-
pleteness of a predicate by the existence of tense. 
Because Japanese marks past tense by the past 
tense marker -ta, if a predicate has -ta, it is com-
plete and no functional expressions need be add-
ed.  
 However, Japanese does not hold an explicit 
present tense marker; the base form of a verb is 
also a present form. We solve this by looking at 
which conjunction follows the predicate. As dis-
cussed in Minami (1993), the finite state and the 
type of conjunction are related; some conjunc-
tions follow tensed phrases while others follow 
infinitival phrases. Following this, we categorize 
all the coordinate conjunctions based on whether 
they can combine with a tensed phrase. These 
conjunctions are listed as tensed in Table 1. If 
TP  
3 
(FocP) T:ta PAST [??] 
3 
(ModP)*   Foc:da COP [??] 
3 
VP   Mod: mitai ?seems? [??] 
4 
iku ?go? 
Figure 2. Structure of a predicate. 
68
the target phrase is followed by one of those 
conjunctions, then we do not add any functional 
expressions to them because they are complete. 
 
The second problem: Finding the appropriate 
functional expressions for incomplete interme-
diate predicates. 
As discussed, we assume that predicates are 
coordinated at one of the functional phrase levels 
in Figure 2. Functional expressions that need to 
be added are, therefore, those of the outer phras-
es of the target phrase.  
 For example, if the target phrase has da, the 
head of FocP, then it only needs the past tense 
marker to be added, which is located above the 
FocP (i.e., TP). This explains the paraphrasing 
pattern of (8). Therefore, by looking at which 
functional expressions held by the target predi-
cate, one can see that functional expressions to 
be added are those that belong to phrases above 
the target phrase. 
 As shown, the answer to Question B is that 
we only add functional expressions to incom-
plete predicates, which are judged based on the 
existence/absence of tense. The appropriate 
functional expressions to be added are those of 
outer phrases of the target phrase. 
 
3.3 Implementing the Standardization 
In this final subsection, we describe how we ac-
tually implement our theoretical observations in 
our standardization system.  
 
CATEGORIZE functional expressions 
First, we selected functional expressions that 
belong to our syntactic and semantic categories 
from those listed in Matsuyoshi and Sato (2006), 
a total of about 17,000 functional expressions 
with 95 different semantic labels. We use ab-
stract semantic labels, such as ?completion,? 
?guess,? and ?desire? for the categorization 
(Table 2).  
 We divided those that did not belong to our 
syntactic and semantic categories into Deletables 
and Undeletables. Deletables are those that do 
not alter the meaning of an event and are, there-
fore, unnecessary. Undeletables are those that 
are a part of content words, and so cannot be 
deleted (e.g., kurai [??] ?about? as in 1-man-
en-kurai-da ?is about one million yen?). Based 
on the categorization of semantic labels as well 
as surface forms of functional expressions, our 
system works as follows; 
 
ADD necessary functional expressions  
A-1: Examine whether the target predicate has 
the tense marker ta or it is followed by the 
conjunctions categorized as tensed. If not, 
then go to Step A-2. 
A-2: Based on the semantic label of the target 
predicate, decide which level of syntactic 
phrase the predicate projects. Add functional 
expressions from the last predicate that be-
longs to outer phrases. 
 
DELETE unnecessary functional expressions 
D-1: Delete all the functional expressions that 
are categorized as Deletables. 
D-2: Leave only one functional expression if 
there is more than one same semantic label. 
For those categorized as Negation, however, 
delete all if the number of negations is even. 
Otherwise, leave one. 
D-3: Delete those categorized as Focus if they 
do not follow or precede a functional expres-
sion categorized as Modality.  
 
GENERATE simple predicates 
Last, conjugate all the elements and generate 
simplified surface forms of predicates. 
 
4 Experiments and Evaluations 
4.1 Constructing Paraphrase Data 
We selected 2,000 sentences from newspaper 
and blog articles in which more than one predi-
cate were coordinated.2 We manually extracted 
predicates (c-f1-f2..fn). Half of them were those in 
which the last predicate had three or more func-
tional expressions (n ? 3). 
                                                 
2 We use Mainichi Newspapers from the year 2000. 
Table 1. Coordinate conjunctions. 
Not tensed Tensed 
gerundive 
form, te  
shi, dakedenaku, ueni, bakarika, 
hoka(ni)(wa), keredo, ga, nonitai-
shi(te),ippou(de),hanmen  
69
We then asked one annotator with a linguistic 
background to paraphrase each predicate into the 
simplest form possible while retaining the mean-
ing of the event.3 We asked another annotator, 
who also has a background in linguistics, to 
check whether the paraphrased predicates made 
by the first annotator followed our criterion, and 
if not, asked the first annotator to make at least 
one paraphrase. 424 out of 4,939 predicates 
(8.5%) were judged as not following the crite-
rion and were re-paraphrased. This means that 
the accuracy of 91.5% is the gold standard of our 
task. 
One of the authors manually assigned a cor-
rect semantic label to each functional expression. 
Procedure i in Figure 1 is, therefore, manually 
implemented in our current study. 
 
4.2 Experiments and Results 
Based on the standardization rules discussed in 
Section 3, our system automatically paraphrased 
functional expressions of test predicates into 
simple forms. We excluded instances that had 
segmentation errors and those that were judged 
as inappropriate as a predicate. 4  A total of 
1,501 intermediate predicates (287 for develop-
ment and 1,214 for test) and 1,958 last predi-
cates (391 for development and 1,567 for test) 
were transformed into simple predicates. 
 The accuracy was measured based on the ex-
act match in surface forms with the manually 
constructed paraphrases. For comparison, we 
                                                 
3 We asked to delete or add functional expressions from 
each predicate when paraphrasing. Only the surface forms 
(and not semantic labels) were used for annotation. 
4 In Japanese, a gerundive form of a verb is sometimes used 
as a postposition. The annotators excluded these examples 
as ?not-paraphrasable.? 
used the following baseline methods. 
? No Add/Delete: Do not add/delete any 
functional expression.  
? Simp Add: Simply add all functional ex-
pressions that the intermediate phrase does 
not have from the last predicate. 
Table 3 indicates the results. Our standardizing 
system achieved high accuracy of around 77% 
and 83 % in open (against the test set) and 
closed tests (against the development set) com-
pared to the baseline methods (No Add/Delete 
(open), 55%; Simp Add (open), 33%). 
We also measured the reduced rate of differ-
ences in surface forms. We counted the number 
of types of functional expressions in the last pre-
dicates (a sequence of f1-f2-f3 is counted as one) 
before and after the standardization. 
For comparison, we also counted the number 
of functional expressions of the manually pa-
raphrased predicates. Table 4 lists the results. As 
shown, our standardizing system succeeded in 
reducing surface differences in predicates from 
the original ones at the rate of 44.0%, which is 
quite close to the rate achieved by the human 
annotators (52.0%). 
 
5 Discussion and Conclusion 
Our standardization system succeeded in gene-
rating simple predicates in which only functional 
expressions crucial for the factual meaning of 
the predicate were retained.  
The predicates produced by our system 
showed fewer variations in their surface forms 
while around 77% of them exactly matched the 
simplified predicates produced by human anno-
tators, which is quite high compared to the base-
line systems.  
Table 2. Syntactic and semantic categorization of semantic labels. 
Syntactic  Semantic  Semantic Labels
T if the 
surface is ta 
Tense 
(Aspect) 
??(completion),??,??,??,??,??,??,??,???, ???, ??,?
?, ??, ?? 
 Negation ??(negation), ??,????,????,???,???,???, ???, ???
Mod Modality ??(guess),  ??(desire),??,??,??,??,??,??, ??, ??, ??, ??
??, ???, ???,???,??,???,???
Foc Focus ??(affirmation), ???,??
 Deletables ??(politeness),?-??,??,??,?-??,????,??, ??, ????, ?
?,??, ????,????,??,??,???
 Undele-
tables 
??(about), ??,??,???,???,??,??,??,??, ??, ????,?
?, ??, ??, ??, ??, ???, ???, ??, ???, ????, ????, ??, 
??, ???, ??,??,??,??,??,??,??,??,?? 
70
This was achieved because we constructed 
solid paraphrasing rules by applying linguistic 
theories in semantics and syntax. The quite low 
accuracy of the baseline method, especially 
SimpAdd, further supports our claim that im-
plementing linguistic theories in actual NLP ap-
plications can greatly improve system perfor-
mance. 
Unlike the study by Inui et al (2008), we did 
not include the meaning of a content word for 
deciding the factuality of the event nor did we 
include it in the paraphrasing rules. This lowers 
the accuracy. Several functional expressions, 
especially those expressing aspect, can be de-
leted or added depending on the meaning of the 
content word. This is because content words in-
herently hold aspectual information, and one 
needs to compare it to the aspectual information 
expressed by functional expressions. Because we 
need a really complicated system to compute the 
abstract semantic relations between a content 
word and functional expressions, we leave this 
problem for future research.  
Regardless of this, our standardizing system 
is useful for a lot of NLP applications let alne 
text mining. As mentioned in Inui et al (2008), 
bag-of-words-based feature extraction is insuffi-
cient for conducting statistically-based deep se-
mantic analysis, such as factual analysis. If stan-
dardized predicates were used instead of a single 
content word, we could expect an improvement 
in those statistically-based methods because each 
predicate holds important information about fact 
while differences in surface forms are quite li-
mited. 
In conclusion, we presented our system for 
standardizing complex functional expressions in 
Japanese predicates. Since our paraphrasing 
rules are based on linguistic theories, we suc-
ceeded in producing simple predicates that have 
only the functional expressions crucial to under-
standing of the meaning of an event. Our future 
research will investigate the relationship be-
tween the meaning of content words and those of 
functional expressions in order to achieve higher 
accuracy. We will also investigate the impact of 
our standardization system on NLP applications.  
 
References 
Adger, David (2003). Core Syntax: A minimalist ap-
proach. New York: Oxford University Press. 
Chierchia, Gennaro, & Sally McConnell-Ginet (2000). 
Meaning and grammar: An introduction to se-
mantics (2nd ed.). Cambridge, MA: The MIT 
press. 
Cinque, Guglielmo (2006). Restructuring and func-
tional heads: The cartography of syntactic struc-
tures, Vol. 4. New York: Oxford University Press. 
Haugh, Michael (2008). Utterance-final conjunctive 
particles and implicature in Japanese conversation. 
Pragmatics, 18 (3), 425-451. 
Inui, Kentaro, Shuya Abe, Kazuo Hara, Hiraku Mori-
ta, Chitose Sao, Megumi Eguchi, Asuka Sumida, 
Koji Murakami, & Suguru Matsuyoshi (2008). 
Experience mining: Building a large-scale data-
base of personal experiences and opinions from 
web documents. Proceedings of the 2008 
IEEE/WIC/ACM International Conference on 
Web Intelligence and Intelligent Agent Technolo-
gy, Vol. 1., 314-321. 
Kato, Shigehiro (2007). Nihongo-no jutsubu-kouzou 
to kyoukaisei [Predicate complex structure and 
morphological boundaries in Japanese]. The an-
nual report on cultural science, Vol. 122(6) (pp. 
97-155). Sapporo, Japan: Hokkaido University, 
Graduate School of Letters. 
Matsuyoshi, Suguru, & Satoshi Sato (2006). Compi-
lation of a dictionary of Japanese functional ex-
pressions with hierarchical organization. Proceed-
ings of the 21st International Conference on 
Computer Processing of Oriental Languages 
 Normalization No Add/Delete Simp Add 
Open (Intermediate) 77.7%(943/1214) 57.8%(702/1214) 32.8%(398/1214) 
Closed (Intermediate) 82.9%(238/287) 62.0%%(178/287) 35.2%(101/287) 
Open (Last) 76.2%(1194/1567) 51.9% (203/391) n.a 
Closed (Last) 83.4%(326/391) 48.1%(188/391) n.a. 
Table 3. Results of our normalization system. 
Original 943 types Reduced Rate
Normalization 530 types 44.0% 
Human Annotation 448 types 52.0% 
Table 4. Reduced rate of surface forms. 
71
(ICCPOL), Lecture Notes in Computer Science, 
Vol. 4285, 395-402.  
Matsuyoshi, Suguru, & Satoshi Sato (2008). Auto-
matic paraphrasing of Japanese functional expres-
sions using a hierarchically organized dictionary. 
Proceedings of the 3rd International Joint Confe-
rence on Natural Language Processing (IJCNLP), 
Vol. 1, 691-696. 
Minami, Fujio (1993). Gendai nihongobunpou-no 
rinkaku [Introduction to modern Japanese gram-
mar]. Tokyo: Taishuukan. 
Nasukawa, Tetsuya (2001). Kooru sentaa-niokeru 
tekisuto mainingu [Text mining application for 
call centers]. Journal of Japanese society for Ar-
tificial Intelligence, 16(2), 219-225. 
Partee, Barbara H., Alice ter Meulen, & Robert E. 
Wall (1990). Mathematical methods in Linguistics. 
Dordrecht, The Netherland: Kluwer. 
Portner, Paul H. (2005). What is meaning?: Funda-
mentals of formal semantics. Malden, MA: 
Blackwell. 
Rizzi, Luigi (1999). On the position ?Int(errogative)? 
in the left periphery of the clause. Ms., Universit? 
di Siena. 
Shudo, Kosho, Toshifumi Tanabe, Masahito Takaha-
shi, & Kenji Yoshimura (2004). MWEs as non-
propositional content indicators. Proceedings of 
second Association for Computational Linguistics 
(ACL) Workshops on Multiword Expressions: In-
tegrating Processing, 32-39. 
Tanabe, Toshifumi, Kenji Yoshimura & Kosho Shu-
do (2001). Modality expressions in Japanese and 
their automatic paraphrasing. Proceedings of the 
6th Natural Language Processing Pacific Rim 
Symposium (NLPRS), 507-512. 
Tsujimura, Natsuko. (2007). An Introduction to Jap-
anese Linguistics (2nd Ed.). Malden, MA: Black-
well. 
72
