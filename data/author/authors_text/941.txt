Proceedings of the 6th Workshop on Statistical Machine Translation, pages 358?364,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Joint WMT Submission of the QUAERO Project
?Markus Freitag, ?Gregor Leusch, ?Joern Wuebker, ?Stephan Peitz, ?Hermann Ney,
?Teresa Herrmann, ?Jan Niehues, ?Alex Waibel,
?Alexandre Allauzen, ?Gilles Adda,?Josep Maria Crego,
?Bianka Buschbeck, ?Tonio Wandmacher, ?Jean Senellart
?RWTH Aachen University, Aachen, Germany
?Karlsruhe Institute of Technology, Karlsruhe, Germany
?LIMSI-CNRS, Orsay, France
?SYSTRAN Software, Inc.
?surname@cs.rwth-aachen.de
?firstname.surname@kit.edu
?firstname.lastname@limsi.fr ?surname@systran.fr
Abstract
This paper describes the joint QUAERO sub-
mission to the WMT 2011 machine transla-
tion evaluation. Four groups (RWTH Aachen
University, Karlsruhe Institute of Technol-
ogy, LIMSI-CNRS, and SYSTRAN) of the
QUAERO project submitted a joint translation
for the WMT German?English task. Each
group translated the data sets with their own
systems. Then RWTH system combination
combines these translations to a better one. In
this paper, we describe the single systems of
each group. Before we present the results of
the system combination, we give a short de-
scription of the RWTH Aachen system com-
bination approach.
1 Overview
QUAERO is a European research and develop-
ment program with the goal of developing multi-
media and multilingual indexing and management
tools for professional and general public applica-
tions (http://www.quaero.org). Research in machine
translation is mainly assigned to the four groups
participating in this joint submission. The aim of
this WMT submission was to show the quality of a
joint translation by combining the knowledge of the
four project partners. Each group develop and main-
tain their own different machine translation system.
These single systems differ not only in their general
approach, but also in the preprocessing of training
and test data. To take the advantage of these dif-
ferences of each translation system, we combined
all hypotheses of the different systems, using the
RWTH system combination approach.
1.1 Data Sets
For WMT 2011 each QUAERO partner trained their
systems on the parallel Europarl and News Com-
mentary corpora. All single systems were tuned on
the newstest2009 dev set. The newstest2008 dev set
was used to train the system combination parame-
ters. Finally the newstest2010 dev set was used to
compare the results of the different system combi-
nation approaches and settings.
2 Translation Systems
2.1 RWTH Aachen Single Systems
For the WMT 2011 evaluation the RWTH utilized
RWTH?s state-of-the-art phrase-based and hierar-
chical translation systems. GIZA++ (Och and Ney,
2003) was employed to train word alignments, lan-
guage models have been created with the SRILM
toolkit (Stolcke, 2002).
2.1.1 Phrase-Based System
The phrase-based translation (PBT) system is
similar to the one described in Zens and Ney (2008).
After phrase pair extraction from the word-aligned
bilingual corpus, the translation probabilities are es-
timated by relative frequencies. The standard feature
set alo includes an n-gram language model, phrase-
level IBM-1 and word-, phrase- and distortion-
penalties, which are combined in log-linear fash-
ion. Parameters are optimized with the Downhill-
Simplex algorithm (Nelder and Mead, 1965) on the
word graph.
358
2.1.2 Hierarchical System
For the hierarchical setups described in this pa-
per, the open source Jane toolkit (Vilar et al, 2010)
is employed. Jane has been developed at RWTH
and implements the hierarchical approach as intro-
duced by Chiang (2007) with some state-of-the-art
extensions. In hierarchical phrase-based translation,
a weighted synchronous context-free grammar is in-
duced from parallel text. In addition to contiguous
lexical phrases, hierarchical phrases with up to two
gaps are extracted. The search is typically carried
out using the cube pruning algorithm (Huang and
Chiang, 2007). The model weights are optimized
with standard MERT (Och, 2003) on 100-best lists.
2.1.3 Phrase Model Training
For some PBT systems a forced alignment pro-
cedure was applied to train the phrase translation
model as described in Wuebker et al (2010). A
modified version of the translation decoder is used
to produce a phrase alignment on the bilingual train-
ing data. The phrase translation probabilities are es-
timated from their relative frequencies in the phrase-
aligned training data. In addition to providing a sta-
tistically well-founded phrase model, this has the
benefit of producing smaller phrase tables and thus
allowing more rapid and less memory consuming
experiments with a better translation quality.
2.1.4 Final Systems
For the German?English task, RWTH conducted
experiments comparing the standard phrase extrac-
tion with the phrase training technique described in
Section 2.1.3. Further experiments included the use
of additional language model training data, rerank-
ing of n-best lists generated by the phrase-based sys-
tem, and different optimization criteria.
A considerable increase in translation quality can
be achieved by application of German compound
splitting (Koehn and Knight, 2003). In comparison
to standard heuristic phrase extraction techniques,
performing force alignment phrase training (FA)
gives an improvement in BLEU on newstest2008
and newstest2009, but a degradation in TER. The
addition of LDC Gigaword corpora (+GW) to the
language model training data shows improvements
in both BLEU and TER. Reranking was done on
1000-best lists generated by the the best available
system (PBT (FA)+GW). Following models were
applied: n-gram posteriors (Zens and Ney, 2006),
sentence length model, a 6-gram LM and IBM-1 lex-
icon models in both normal and inverse direction.
These models are combined in a log-linear fashion
and the scaling factors are tuned in the same man-
ner as the baseline system (using TER?4BLEU on
newstest2009).
The final table includes two identical Jane sys-
tems which are optimized on different criteria. The
one optimized on TER?BLEU yields a much lower
TER.
2.2 Karlsruhe Institute of Technology Single
System
2.2.1 Preprocessing
We preprocess the training data prior to training
the system, first by normalizing symbols such as
quotes, dashes and apostrophes. Then smart-casing
of the first words of each sentence is performed. For
the German part of the training corpus we use the
hunspell1 lexicon to learn a mapping from old Ger-
man spelling to new German spelling to obtain a cor-
pus with homogeneous spelling. In addition, we per-
form compound splitting as described in (Koehn and
Knight, 2003). Finally, we remove very long sen-
tences, empty lines, and sentences that probably are
not parallel due to length mismatch.
2.2.2 System Overview
The KIT system uses an in-house phrase-based
decoder (Vogel, 2003) to perform translation. Op-
timization with regard to the BLEU score is done
using Minimum Error Rate Training as described
by Venugopal et al (2005). The translation model
is trained on the Europarl and News Commentary
Corpus and the phrase table is based on a GIZA++
Word Alignment. We use two 4-gram SRI language
models, one trained on the News Shuffle corpus and
one trained on the Gigaword corpus. Reordering is
performed based on continuous and non-continuous
POS rules to cover short and long-range reorder-
ings. The long-range reordering rules were also ap-
plied to the training corpus and phrase extraction
was performed on the resulting reordering lattices.
Part-of-speech tags are obtained using the TreeTag-
1http://hunspell.sourceforge.net/
359
ger (Schmid, 1994). In addition, the system applies
a bilingual language model to extend the context of
source language words available for translation. The
individual models are described briefly in the fol-
lowing.
2.2.3 POS-based Reordering Model
We use a reordering model that is based on parts-
of-speech (POS) and learn probabilistic rules from
the POS tags of the words in the training corpus and
the alignment information. In addition to continu-
ous reordering rules that model short-range reorder-
ing (Rottmann and Vogel, 2007), we apply non-
continuous rules to address long-range reorderings
as typical for German-English translation (Niehues
and Kolss, 2009). The reordering rules are applied
to the source sentences and the reordered sentence
variants as well as the original sequence are encoded
in a word lattice which is used as input to the de-
coder.
2.2.4 Lattice Phrase Extraction
For the test sentences, the POS-based reordering
allows us to change the word order in the source sen-
tence so that the sentence can be translated more eas-
ily. If we apply this also to the training sentences, we
would be able to extract also phrase pairs for origi-
nally discontinuous phrases and could apply them
during translation of reordered test sentences.
Therefore, we build reordering lattices for all
training sentences and then extract phrase pairs from
the monotone source path as well as from the re-
ordered paths. To limit the number of extracted
phrase pairs, we extract a source phrase only once
per sentence, even if it is found in different paths and
we only use long-range reordering rules to generate
the lattices for the training corpus.
2.2.5 Bilingual Language Model
In phrase-based systems the source sentence is
segmented by the decoder during the search pro-
cess. This segmentation into phrases leads to the
loss of context information at the phrase boundaries.
The language model can make use of more target
side context. To make also source language context
available we use a bilingual language model, an ad-
ditional language model in the phrase-based system
in which each token consist of a target word and all
source words it is aligned to. The bilingual tokens
enter the translation process as an additional target
factor.
2.3 LIMSI-CNRS Single System
2.3.1 System overview
The LIMSI system is built with n-code2, an open
source statistical machine translation system based
on bilingual n-grams.
2.3.2 n-code Overview
In a nutshell, the translation model is im-
plemented as a stochastic finite-state transducer
trained using a n-gram model of (source,target)
pairs (Casacuberta and Vidal, 2004). Training this
model requires to reorder source sentences so as to
match the target word order. This is performed by a
stochastic finite-state reordering model, which uses
part-of-speech information3 to generalize reordering
patterns beyond lexical regularities.
In addition to the translation model, eleven fea-
ture functions are combined: a target-language
model; four lexicon models; two lexicalized reorder-
ing models (Tillmann, 2004) aiming at predicting
the orientation of the next translation unit; a weak
distance-based distortion model; and finally a word-
bonus model and a tuple-bonus model which com-
pensate for the system preference for short transla-
tions. The four lexicon models are similar to the ones
use in a standard phrase based system: two scores
correspond to the relative frequencies of the tuples
and two lexical weights estimated from the automat-
ically generated word alignments. The weights asso-
ciated to feature functions are optimally combined
using a discriminative training framework (Och,
2003), using the newstest2009 data as development
set.
The overall search is based on a beam-search
strategy on top of a dynamic programming algo-
rithm. Reordering hypotheses are computed in a
preprocessing step, making use of reordering rules
built from the word reorderings introduced in the tu-
ple extraction process. The resulting reordering hy-
potheses are passed to the decoder in the form of
word lattices (Crego and Marin?o, 2007).
2http://www.limsi.fr/Individu/jmcrego/n-code
3Part-of-speech information for English and German is com-
puted using the TreeTagger.
360
2.3.3 Data Preprocessing
Based on previous experiments which have
demonstrated that better normalization tools provide
better BLEU scores (K. Papineni and Zhu, 2002),
all the English texts are tokenized and detokenized
with in-house text processing tools (De?chelotte et
al., 2008). For German, the standard tokenizer sup-
plied by evaluation organizers is used.
2.3.4 Target n-gram Language Models
The English language model is trained assuming
that the test set consists in a selection of news texts
dating from the end of 2010 to the beginning of
2011. This assumption is based on what was done
for the 2010 evaluation. Thus, a development cor-
pus is built in order to create a vocabulary and to
optimize the target language model.
Development Set and Vocabulary In order to
cover different period, two development sets are
used. The first one is newstest2008. However, this
corpus is two years older than the targeted time pe-
riod. Thus a second development corpus is gath-
ered by randomly sampling bunches of 5 consecu-
tive sentences from the provided news data of 2010
and 2011.
To estimate a LM, the English vocabulary is first
defined by including all tokens observed in the Eu-
roparl and news-commentary corpora. This vocabu-
lary is then expanded with all words that occur more
that 5 times in the French-English giga-corpus, and
with the most frequent proper names taken from the
monolingual news data of 2010 and 2011. This pro-
cedure results in a vocabulary around 500k words.
Language Model Training All the training data
allowed in the constrained task are divided into 9
sets based on dates on genres. On each set, a
standard 4-gram LM is estimated from the 500k
word vocabulary with in-house tools using abso-
lute discounting interpolated with lower order mod-
els (Kneser and Ney, 1995; Chen and Goodman,
1998).
All LMs except the one trained on the news cor-
pora from 2010-2011 are first linearly interpolated.
The associated coefficients are estimated so as to
minimize the perplexity evaluated on the dev2010-
2011. The resulting LM and the 2010-2011 LM are
finally interpolated with newstest2008 as develop-
ment data. This two steps interpolation aims to avoid
an overestimate of the weight associated to the 2010-
2011 LM.
2.4 SYSTRAN Software, Inc. Single System
The data submitted by SYSTRAN were obtained by
the SYSTRAN baseline system in combination with
a statistical post editing (SPE) component.
The SYSTRAN system is traditionally classi-
fied as a rule-based system. However, over the
decades, its development has always been driven by
pragmatic considerations, progressively integrating
many of the most efficient MT approaches and tech-
niques. Nowadays, the baseline engine can be con-
sidered as a linguistic-oriented system making use of
dependency analysis, general transfer rules as well
as of large manually encoded dictionaries (100k ?
800k entries per language pair).
The basic setup of the SPE component is identi-
cal to the one described in (L. Dugast and Koehn,
2007). A statistical translation model is trained on
the rule-based translation of the source and the tar-
get side of the parallel corpus. This is done sepa-
rately for each parallel corpus. Language models are
trained on each target half of the parallel corpora and
also on additional in-domain corpora. Moreover, the
following measures ? limiting unwanted statistical
effects ? were applied:
? Named entities are replaced by special tokens
on both sides. This usually improves word
alignment, since the vocabulary size is signif-
icantly reduced. In addition, entity translation
is handled more reliably by the rule-based en-
gine.
? The intersection of both vocabularies (i.e. vo-
cabularies of the rule-based output and the ref-
erence translation) is used to produce an addi-
tional parallel corpus (whose target is identical
to the source). This was added to the parallel
text in order to improve word alignment.
? Singleton phrase pairs are deleted from the
phrase table to avoid overfitting.
? Phrase pairs not containing the same number
of entities on the source and the target side are
also discarded.
361
? Phrase pairs appearing less than 2 times were
pruned.
The SPE language model was trained 15M
phrases from the news/europarl corpora, provided
as training data for WMT 2011. Weights for these
separate models were tuned by the MERT algorithm
provided in the Moses toolkit (P. Koehn et al, 2007),
using the provided news development set.
3 RWTH Aachen System Combination
System combination is used to produce consensus
translations from multiple hypotheses produced with
different translation engines that are better in terms
of translation quality than any of the individual hy-
potheses. The basic concept of RWTH?s approach
to machine translation system combination has been
described by Matusov et al (2006; 2008). This ap-
proach includes an enhanced alignment and reorder-
ing framework. A lattice is built from the input hy-
potheses. The translation with the best score within
the lattice according to a couple of statistical mod-
els is selected as consensus translation. A deeper
description will be also given in the WMT11 sys-
tem combination paper of RWTH Aachen Univer-
sity. For this task only the A2L framework has been
used.
4 Experiments
We tried different system combinations with differ-
ent sets of single systems and different optimiza-
tion criteria. As RWTH has two different transla-
tion systems, we put the output of both systems into
system combination. Although both systems have
the same preprocessing, their hypotheses differ. Fi-
nally, we added for both RWTH systems two addi-
tional hypotheses to the system combination. The
two hypotheses of Jane were optimized on differ-
ent criteria. The first hypothesis was optimized on
BLEU and the second one on TER?BLEU. The first
RWTH phrase-based hypothesis was generated with
force alignment, the second RWTH phrase-based
hypothesis is a reranked version of the first one as
described in 2.1.4. Compared to the other systems,
the system by SYSTRAN has a completely different
approach (see section 2.4). It is mainly based on a
rule-based system. For the German?English pair,
SYSTRAN achieves a lower BLEU score in each
test set compared to the other groups. But since the
SYSTRAN system is very different to the others, we
still obtain an improvement when we add it also to
system combination.
We obtain the best result from system combina-
tion of all seven systems, optimizing the parameters
on BLEU. This system was the system we submitted
to the WMT 2011 evaluation.
For each dev set we obtain an improvement com-
pared to the best single systems. For newstest2008
and newstest2009 we get an improvement of 0.5
points in BLEU and 1.8 points in TER compared to
the best single system of Karlsruhe Institute of Tech-
nology. For newstest2010 we get an improvement
of 1.8 points in BLEU and 2.7 points in TER com-
pared to the best single system of RWTH. The sys-
tem combination weights optimized for the best run
are listed in Table 2. We see that although the single
system of SYSTRAN has the lowest BLEU scores,
it gets the second highest system weight. This high
value shows the influence of a completely different
system. On the other hand, all RWTH systems are
very similar, because of their same preprocessing
and their small variations. Therefor the system com-
bination parameter of all four systems by themselves
are relatively small. The summarized ?RWTH ap-
proach? system weight, though, is again on par with
the other systems.
5 Conclusion
The four statistical machine translation systems of
Karlsruhe Institute of Technology, RWTH Aachen
and LIMSI and the very structural approach of SYS-
TRAN produce hypotheses with a huge variability
compared to the others. Finally the RWTH Aachen
system combination combined all single system hy-
potheses to one hypothesis with a higher BLEU
compared to each single system. If the system
combination implementation can handle enough sin-
gle systems we would recommend to add all single
systems to the system combination. Although the
single system of SYSTRAN has the lowest BLEU
scores and the RWTH single systems are similar we
achieved the best result in using all single systems.
362
newstest2008 newstest2009 newstest2010 description
BLEU TER BLEU TER BLEU TER
22.73 60.73 22.50 59.82 25.26 57.37 sc (all systems) BLEU opt
22.61 60.60 22.28 59.39 25.07 56.95 sc (all systems - (1)) TER?BLEU opt
22.50 60.41 22.52 59.61 25.23 57.40 sc (all systems) TER?BLEU opt
22.19 60.09 22.05 59.31 24.74 56.89 sc (all systems - (4)) TER?BLEU opt
22.21 60.71 21.89 59.95 24.72 57.58 sc (all systems - (4,7)) TER?BLEU opt
22.22 60.45 21.79 59.72 24.32 57.59 sc (all systems - (3,4)) TER?BLEU opt
22.27 60.60 21.75 59.92 24.35 57.64 sc (all systems - (3,4)) BLEU opt
22.10 62.59 22.01 61.64 23.34 60.35 (1) Karlsruhe Institute of Technology
21.41 62.77 21.12 61.91 23.44 60.06 (2) RWTH PBT (FA) rerank +GW
21.11 62.96 21.06 62.16 23.29 60.26 (3) RWTH PBT (FA)
21.47 63.89 21.00 63.33 22.93 61.71 (4) RWTH jane + GW BLEU opt
20.89 61.05 20.36 60.47 23.42 58.31 (5) RWTH jane + GW TER?BLEU opt
20.33 64.50 19.79 64.91 21.97 61.44 (6) Limsi-CNRS
17.06 69.48 17.52 67.34 18.68 66.37 (7) SYSTRAN Software
Table 1: All systems for the WMT 2011 German?English translation task (truecase). BLEU and TER results are in
percentage. FA denotes systems with phrase training, +GW the use of LDC data for the language model. sc denotes
system combination.
system weight
Karlsruhe Institute of Technology 0.350
RWTH PBT (FA) rerank +GW 0.001
RWTH PBT (FA) 0.046
RWTH jane + GW BLEU opt 0.023
RWTH jane + GW TER?BLEU opt 0.034
Limsi-CNRS 0.219
SYSTRAN Software 0.328
Table 2: Optimized systems weights for each system of the best system combination result.
Acknowledgments
This work was achieved as part of the QUAERO
Programme, funded by OSEO, French State agency
for innovation.
References
F. Casacuberta and E. Vidal. 2004. Machine translation
with inferred stochastic finite-state transducers. Com-
putational Linguistics, 30(3):205?225.
S.F. Chen and J.T. Goodman. 1998. An empirical
study of smoothing techniques for language modeling.
Technical Report TR-10-98, Computer Science Group,
Harvard University.
D. Chiang. 2007. Hierarchical Phrase-Based Transla-
tion. Computational Linguistics, 33(2):201?228.
J.M. Crego and J.B. Marin?o. 2007. Improving statistical
MT by coupling reordering and decoding. Machine
Translation, 20(3):199?215.
D. De?chelotte, O. Galibert G. Adda, A. Allauzen, J. Gau-
vain, H. Meynard, and F. Yvon. 2008. LIMSI?s statis-
tical translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
L. Huang and D. Chiang. 2007. Forest Rescoring: Faster
Decoding with Integrated Language Models. In Proc.
Annual Meeting of the Association for Computational
Linguistics, pages 144?151, Prague, Czech Republic,
June.
T. Ward K. Papineni, S. Roukos and W. Zhu. 2002. Bleu:
363
a method for automatic evaluation of machine transla-
tion. In ACL ?02: Proc. of the 40th Annual Meeting
on Association for Computational Linguistics, pages
311?318. Association for Computational Linguistics.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Proceedings of the In-
ternational Conference on Acoustics, Speech, and Sig-
nal Processing, ICASSP?95, pages 181?184, Detroit,
MI.
P. Koehn and K. Knight. 2003. Empirical Methods
for Compound Splitting. In Proceedings of European
Chapter of the ACL (EACL 2009), pages 187?194.
J. Senellart L. Dugast and P. Koehn. 2007. Statistical
post-editing on systran?s rule-based translation system.
In Proceedings of the Second Workshop on Statisti-
cal Machine Translation, StatMT ?07, pages 220?223,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
E. Matusov, N. Ueffing, and H. Ney. 2006. Computing
Consensus Translation from Multiple Machine Trans-
lation Systems Using Enhanced Hypotheses Align-
ment. In Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 33?40.
E. Matusov, G. Leusch, R.E. Banchs, N. Bertoldi,
D. Dechelotte, M. Federico, M. Kolss, Y.-S. Lee,
J.B. Mari no, M. Paulik, S. Roukos, H. Schwenk, and
H. Ney. 2008. System Combination for Machine
Translation of Spoken and Written Language. IEEE
Transactions on Audio, Speech and Language Pro-
cessing, 16(7):1222?1237.
J.A. Nelder and R. Mead. 1965. The Downhill Simplex
Method. Computer Journal, 7:308.
J. Niehues and M. Kolss. 2009. A POS-Based Model for
Long-Range Reorderings in SMT. In Fourth Work-
shop on Statistical Machine Translation (WMT 2009),
Athens, Greece.
F.J. Och and H. Ney. 2003. A Systematic Comparison of
Various Statistical Alignment Models. Computational
Linguistics, 29(1):19?51.
F.J. Och. 2003. Minimum Error Rate Training for Statis-
tical Machine Translation. In Proc. Annual Meeting of
the Association for Computational Linguistics, pages
160?167, Sapporo, Japan, July.
A. Birch P. Koehn, H. Hoang, C. Callison-Burch, M. Fed-
erico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,
R. Zens, C. Dyer, O. Bojar, A. Constantin, and
E. Herbst. 2007. Moses: open source toolkit for
statistical machine translation. In Proceedings of the
45th Annual Meeting of the ACL on Interactive Poster
and Demonstration Sessions, ACL ?07, pages 177?
180, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
K. Rottmann and S. Vogel. 2007. Word Reordering in
Statistical Machine Translation with a POS-Based Dis-
tortion Model. In TMI, Sko?vde, Sweden.
H. Schmid. 1994. Probabilistic Part-of-Speech Tagging
Using Decision Trees. In International Conference
on NewMethods in Language Processing, Manchester,
UK.
A. Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proc. Int. Conf. on Spoken Language
Processing, volume 2, pages 901?904, Denver, Col-
orado, USA, September.
C. Tillmann. 2004. A unigram orientation model for sta-
tistical machine translation. In Proceedings of HLT-
NAACL 2004, pages 101?104. Association for Com-
putational Linguistics.
A. Venugopal, A. Zollman, and A. Waibel. 2005. Train-
ing and Evaluation Error Minimization Rules for Sta-
tistical Machine Translation. In Workshop on Data-
drive Machine Translation and Beyond (WPT-05), Ann
Arbor, MI.
D. Vilar, S. Stein, M. Huck, and H. Ney. 2010. Jane:
Open Source Hierarchical Translation, Extended with
Reordering and Lexicon Models. In ACL 2010 Joint
Fifth Workshop on Statistical Machine Translation and
Metrics MATR, pages 262?270, Uppsala, Sweden,
July.
S. Vogel. 2003. SMT Decoder Dissected: Word Re-
ordering. In Int. Conf. on Natural Language Process-
ing and Knowledge Engineering, Beijing, China.
J. Wuebker, A. Mauser, and H. Ney. 2010. Training
Phrase Translation Models with Leaving-One-Out. In
Proceedings of the 48th Annual Meeting of the Assoc.
for Computational Linguistics, pages 475?484, Upp-
sala, Sweden, July.
R. Zens and H. Ney. 2006. N-gram Posterior Proba-
bilities for Statistical Machine Translation. In Human
Language Technology Conf. / North American Chap-
ter of the Assoc. for Computational Linguistics Annual
Meeting (HLT-NAACL), Workshop on Statistical Ma-
chine Translation, pages 72?77, New York City, June.
R. Zens and H. Ney. 2008. Improvements in Dynamic
Programming Beam Search for Phrase-based Statisti-
cal Machine Translation. In Proc. of the Int. Workshop
on Spoken Language Translation (IWSLT), Honolulu,
Hawaii, October.
364
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 322?329,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Joint WMT 2012 Submission of the QUAERO Project
?Markus Freitag, ?Stephan Peitz, ?Matthias Huck, ?Hermann Ney,
?Jan Niehues, ?Teresa Herrmann, ?Alex Waibel,
?Le Hai-son, ?Thomas Lavergne, ?Alexandre Allauzen,
?Bianka Buschbeck, ?Josep Maria Crego, ?Jean Senellart
?RWTH Aachen University, Aachen, Germany
?Karlsruhe Institute of Technology, Karlsruhe, Germany
?LIMSI-CNRS, Orsay, France
?SYSTRAN Software, Inc.
?surname@cs.rwth-aachen.de
?firstname.surname@kit.edu
?firstname.lastname@limsi.fr ?surname@systran.fr
Abstract
This paper describes the joint QUAERO sub-
mission to the WMT 2012 machine transla-
tion evaluation. Four groups (RWTH Aachen
University, Karlsruhe Institute of Technol-
ogy, LIMSI-CNRS, and SYSTRAN) of the
QUAERO project submitted a joint translation
for the WMT German?English task. Each
group translated the data sets with their own
systems and finally the RWTH system combi-
nation combined these translations in our final
submission. Experimental results show im-
provements of up to 1.7 points in BLEU and
3.4 points in TER compared to the best single
system.
1 Introduction
QUAERO is a European research and develop-
ment program with the goal of developing multi-
media and multilingual indexing and management
tools for professional and general public applica-
tions (http://www.quaero.org). Research in machine
translation is mainly assigned to the four groups
participating in this joint submission. The aim of
this WMT submission was to show the quality of a
joint translation by combining the knowledge of the
four project partners. Each group develop and main-
tain their own different machine translation system.
These single systems differ not only in their general
approach, but also in the preprocessing of training
and test data. To take the advantage of these dif-
ferences of each translation system, we combined
all hypotheses of the different systems, using the
RWTH system combination approach.
This paper is structured as follows. In Section
2, the different engines of all four groups are in-
troduced. In Section 3, the RWTH Aachen system
combination approach is presented. Experiments
with different system selections for system combi-
nation are described in Section 4. Finally in Section
5, we discuss the results.
2 Translation Systems
For WMT 2012 each QUAERO partner trained their
systems on the parallel Europarl and News Com-
mentary corpora. All single systems were tuned
on the newstest2009 or newstest2010 development
set. The newstest2011 dev set was used to train
the system combination parameters. Finally, the
newstest2008-newstest2010 dev sets were used to
compare the results of the different system combina-
tion settings. In this Section all four different system
engines are presented.
2.1 RWTH Aachen Single Systems
For the WMT 2012 evaluation the RWTH utilized
RWTH?s state-of-the-art phrase-based and hierar-
chical translation systems. GIZA++ (Och and Ney,
2003) was employed to train word alignments, lan-
guage models have been created with the SRILM
toolkit (Stolcke, 2002).
2.1.1 Phrase-Based System
The phrase-based translation (PBT) system is
similar to the one described in Zens and Ney (2008).
After phrase pair extraction from the word-aligned
parallel corpus, the translation probabilities are esti-
mated by relative frequencies. The standard feature
322
set alo includes an n-gram language model, phrase-
level IBM-1 and word-, phrase- and distortion-
penalties, which are combined in log-linear fash-
ion. The model weights are optimized with standard
Mert (Och, 2003) on 200-best lists. The optimiza-
tion criterium is BLEU.
2.1.2 Hierarchical System
For the hierarchical setups (HPBT) described in
this paper, the open source Jane toolkit (Vilar et
al., 2010) is employed. Jane has been developed at
RWTH and implements the hierarchical approach as
introduced by Chiang (2007) with some state-of-the-
art extensions. In hierarchical phrase-based transla-
tion, a weighted synchronous context-free grammar
is induced from parallel text. In addition to contigu-
ous lexical phrases, hierarchical phrases with up to
two gaps are extracted. The search is typically car-
ried out using the cube pruning algorithm (Huang
and Chiang, 2007). The model weights are opti-
mized with standard Mert (Och, 2003) on 100-best
lists. The optimization criterium is 4BLEU ?TER.
2.1.3 Preprocessing
In order to reduce the source vocabulary size
translation, the German text was preprocessed
by splitting German compound words with the
frequency-based method described in (Koehn and
Knight, 2003a). To further reduce translation com-
plexity for the phrase-based approach, we performed
the long-range part-of-speech based reordering rules
proposed by (Popovic? et al, 2006).
2.1.4 Language Model
For both decoders a 4-gram language model is ap-
plied. The language model is trained on the par-
allel data as well as the provided News crawl, the
109 French-English, UN and LDC Gigaword Fourth
Edition corpora. For the 109 French-English, UN
and LDC Gigaword corpora RWTH applied the data
selection technique described in (Moore and Lewis,
2010).
2.2 Karlsruhe Institute of Technology Single
System
2.2.1 Preprocessing
We preprocess the training data prior to training
the system, first by normalizing symbols such as
quotes, dashes and apostrophes. Then smart-casing
of the first words of each sentence is performed. For
the German part of the training corpus we use the
hunspell1 lexicon to learn a mapping from old Ger-
man spelling to new German spelling to obtain a cor-
pus with homogenous spelling. In addition, we per-
form compound splitting as described in (Koehn and
Knight, 2003b). Finally, we remove very long sen-
tences, empty lines, and sentences that probably are
not parallel due to length mismatch.
2.2.2 System Overview
The KIT system uses an in-house phrase-based
decoder (Vogel, 2003) to perform translation and op-
timization with regard to the BLEU score is done us-
ing Minimum Error Rate Training as described in
Venugopal et al (2005).
2.2.3 Translation Models
The translation model is trained on the Europarl
and News Commentary Corpus and the phrase ta-
ble is based on a discriminative word alignment
(Niehues and Vogel, 2008).
In addition, the system applies a bilingual lan-
guage model (Niehues et al, 2011) to extend the
context of source language words available for trans-
lation.
Furthermore, we use a discriminative word lexi-
con as introduced in (Mauser et al, 2009). The lex-
icon was trained and integrated into our system as
described in (Mediani et al, 2011).
At last, we tried to find translations for
out-of-vocabulary (OOV) words by using quasi-
morphological operations as described in Niehues
and Waibel (2011). For each OOV word, we try to
find a related word that we can translate. We modify
the ending letters of the OOV word and learn quasi-
morphological operations to be performed on the
known translation of the related word to synthesize
a translation for the OOV word. By this approach
we were for example able to translate Kaminen into
chimneys using the known translation Kamin # chim-
ney.
2.2.4 Language Models
We use two 4-gram SRI language models, one
trained on the News Shuffle corpus and one trained
1http://hunspell.sourceforge.net/
323
on the Gigaword corpus. Furthermore, we use a 5-
gram cluster-based language model trained on the
News Shuffle corpus. The word clusters were cre-
ated using the MKCLS algorithm. We used 100
word clusters.
2.2.5 Reordering Model
Reordering is performed based on part-of-speech
tags obtained using the TreeTagger (Schmid, 1994).
Based on these tags we learn probabilistic continu-
ous (Rottmann and Vogel, 2007) and discontinuous
(Niehues and Kolss, 2009) rules to cover short and
long-range reorderings. The rules are learned from
the training corpus and the alignment. In addition,
we learned tree-based reordering rules. Therefore,
the training corpus was parsed by the Stanford parser
(Rafferty and Manning, 2008). The tree-based rules
consist of the head node of a subtree and all its
children as well as the new order and a probability.
These rules were applied recursively. The reordering
rules are applied to the source sentences and the re-
ordered sentence variants as well as the original se-
quence are encoded in a word lattice which is used
as input to the decoder. For the test sentences, the
reordering based on parts-of-speech and trees allows
us to change the word order in the source sentence
so that the sentence can be translated more easily.
In addition, we build reordering lattices for all train-
ing sentences and then extract phrase pairs from the
monotone source path as well as from the reordered
paths.
2.3 LIMSI-CNRS Single System
LIMSI?s system is built with n-code (Crego et al,
2011), an open source statistical machine translation
system based on bilingual n-gram2. In this approach,
the translation model relies on a specific decomposi-
tion of the joint probability of a sentence pair P(s, t)
using the n-gram assumption: a sentence pair is de-
composed into a sequence of bilingual units called
tuples, defining a joint segmentation of the source
and target. In the approach of (Marin?o et al, 2006),
this segmentation is a by-product of source reorder-
ing which ultimately derives from initial word and
phrase alignments.
2http://ncode.limsi.fr/
2.3.1 An Overview of n-code
The baseline translation model is implemented as
a stochastic finite-state transducer trained using a
n-gram model of (source,target) pairs (Casacuberta
and Vidal, 2004). Training this model requires to
reorder source sentences so as to match the target
word order. This is performed by a stochastic finite-
state reordering model, which uses part-of-speech
information3 to generalize reordering patterns be-
yond lexical regularities.
In addition to the translation model, eleven fea-
ture functions are combined: a target-language
model; four lexicon models; two lexicalized reorder-
ing models (Tillmann, 2004) aiming at predicting
the orientation of the next translation unit; a ?weak?
distance-based distortion model; and finally a word-
bonus model and a tuple-bonus model which com-
pensate for the system preference for short transla-
tions. The four lexicon models are similar to the ones
used in a standard phrase based system: two scores
correspond to the relative frequencies of the tuples
and two lexical weights estimated from the automat-
ically generated word alignments. The weights asso-
ciated to feature functions are optimally combined
using a discriminative training framework (Och,
2003), using the newstest2009 development set.
The overall search is based on a beam-search
strategy on top of a dynamic programming algo-
rithm. Reordering hypotheses are computed in a
preprocessing step, making use of reordering rules
built from the word reorderings introduced in the tu-
ple extraction process. The resulting reordering hy-
potheses are passed to the decoder in the form of
word lattices (Crego and Marin?o, 2007).
2.3.2 Continuous Space Translation Models
One critical issue with standard n-gram transla-
tion models is that the elementary units are bilingual
pairs, which means that the underlying vocabulary
can be quite large. Unfortunately, the parallel data
available to train these models are typically smaller
than the corresponding monolingual corpora used to
train target language models. It is very likely then,
that such models should face severe estimation prob-
lems. In such setting, using neural network language
3Part-of-speech labels for English and German are com-
puted using the TreeTagger (Schmid, 1995).
324
model techniques seem all the more appropriate. For
this study, we follow the recommendations of Le et
al. (2012), who propose to factor the joint proba-
bility of a sentence pair by decomposing tuples in
two (source and target) parts, and further each part
in words. This yields a word factored translation
model that can be estimated in a continuous space
using the SOUL architecture (Le et al, 2011).
The design and integration of a SOUL model for
large SMT tasks is far from easy, given the computa-
tional cost of computing n-gram probabilities. The
solution used here was to resort to a two pass ap-
proach: the first pass uses a conventional back-off
n-gram model to produce a k-best list; in the second
pass, the k-best list is reordered using the probabil-
ities of m-gram SOUL translation models. In the
following experiments, we used a fixed context size
for SOUL of m = 10, and used k = 300.
2.3.3 Corpora and Data Preprocessing
The parallel data is word-aligned using
MGIZA++4 with default settings. For the En-
glish monolingual training data, we used the same
setup as last year5 and thus the same target language
model as detailed in (Allauzen et al, 2011).
For English, we took advantage of our in-house
text processing tools for tokenization and detok-
enization steps (De?chelotte et al, 2008) and our sys-
tem was built in ?true-case?. As German is mor-
phologically more complex than English, the default
policy which consists in treating each word form
independently is plagued with data sparsity, which
is detrimental both at training and decoding time.
Thus, the German side was normalized using a spe-
cific pre-processing scheme (Allauzen et al, 2010;
Durgar El-Kahlout and Yvon, 2010), which notably
aims at reducing the lexical redundancy by (i) nor-
malizing the orthography, (ii) neutralizing most in-
flections and (iii) splitting complex compounds.
2.4 SYSTRAN Software, Inc. Single System
The data submitted by SYSTRAN were obtained by
a system composed of the standard SYSTRAN MT
engine in combination with a statistical post editing
(SPE) component.
4http://geek.kyloo.net/software
5The fifth edition of the English Gigaword (LDC2011T07)
was not used.
The SYSTRAN system is traditionally classi-
fied as a rule-based system. However, over the
decades, its development has always been driven by
pragmatic considerations, progressively integrating
many of the most efficient MT approaches and tech-
niques. Nowadays, the baseline engine can be con-
sidered as a linguistic-oriented system making use of
dependency analysis, general transfer rules as well
as of large manually encoded dictionaries (100k -
800k entries per language pair).
The SYSTRAN phrase-based SPE component
views the output of the rule-based system as the
source language, and the (human) reference trans-
lation as the target language, see (L. Dugast and
Koehn, 2007). It performs corrections and adaptions
learned from the 5-gram language model trained on
the parallel target-to-target corpus. Moreover, the
following measures - limiting unwanted statistical
effects - were applied:
? Named entities, time and numeric expressions
are replaced by special tokens on both sides.
This usually improves word alignment, since
the vocabulary size is significantly reduced. In
addition, entity translation is handled more re-
liably by the rule-based engine.
? The intersection of both vocabularies (i.e. vo-
cabularies of the rule-based output and the ref-
erence translation) is used to produce an addi-
tional parallel corpus to help to improve word
alignment.
? Singleton phrase pairs are deleted from the
phrase table to avoid overfitting.
? Phrase pairs not containing the same number
of entities on the source and the target side are
also discarded.
The SPE language model was trained on 2M bilin-
gual phrases from the news/Europarl corpora, pro-
vided as training data for WMT 2012. An addi-
tional language model built from 15M phrases of
the English LDC Gigaword corpus using Kneser-
Ney (Kneser and Ney, 1995) smoothing was added.
Weights for these separate models were tuned by
the Mert algorithm provided in the Moses toolkit
(P. Koehn et al, 2007), using the provided news de-
velopment set.
325
3 RWTH Aachen System Combination
System combination is used to produce consensus
translations from multiple hypotheses produced with
different translation engines that are better in terms
of translation quality than any of the individual hy-
potheses. The basic concept of RWTH?s approach
to machine translation system combination has been
described by Matusov et al (2006; 2008). This ap-
proach includes an enhanced alignment and reorder-
ing framework. A lattice is built from the input hy-
potheses. The translation with the best score within
the lattice according to a couple of statistical models
is selected as consensus translation.
4 Experiments
This year, we tried different sets of single systems
for system combination. As RWTH has two dif-
ferent translation systems, we put the output of
both systems into system combination. Although
both systems have the same preprocessing and lan-
guage model, their hypotheses differ because of
their different decoding approach. Compared to
the other systems, the system by SYSTRAN has a
completely different approach (see section 2.4). It
is mainly based on a rule-based system. For the
German?English pair, SYSTRAN achieves a lower
BLEU score in each test set compared to the other
groups. However, since the SYSTRAN system is
very different to the others, we still obtain an im-
provement when we add it also to system combina-
tion.
We did experiments with different optimization
criteria for the system combination optimization.
All results are listed in Table 1 (unoptimized), Table
2 (optimized on BLEU) and Table 3 (optimized on
TER-BLEU). Further, we investigated, whether we
will loose performance, if a single system is dropped
from the system combination. The results show that
for each optimization criteria we need all systems to
achieve the best results.
For the BLEU optimized system combination, we
obtain an improvement compared to the best sin-
gle systems for all dev sets. For newstest2008, we
get an improvement of 1.5 points in BLEU and 1.5
points in TER compared to the best single system of
Karlsruhe Institute of Technology. For newstest2009
we get an improvement of 1.9 points in BLEU and
1.5 points in TER compared to the best single sys-
tem. The system combination of all systems outper-
forms the best single system with 1.9 points in BLEU
and 1.9 points in TER for newstest2010. For new-
stest2011 the improvement is 1.3 points in BLEU
and 2.9 points in TER.
For the TER-BLEU optimized system combina-
tion, we achieved more improvement in TER com-
pared to the BLEU optimized system combination.
For newstest2008, we get an improvement of 0.8
points in BLEU and 3.0 points in TER compared to
the best single system of Karlsruhe Institute of Tech-
nology. The system combinations performs better
on newstest2009 with 1.3 points in BLEU and 2.7
points in TER. For newstest2010, we get an im-
provement of 1.7 points in BLEU and 3.4 points in
TER and for newstest2011 we get an improvement
of 0.7 points in BLEU and 2.5 points in TER.
5 Conclusion
The four statistical machine translation systems of
Karlsruhe Institute of Technology, RWTH Aachen
and LIMSI and the very structural approach of SYS-
TRAN produce hypotheses with a huge variability
compared to the others. Finally, the RWTH Aachen
system combination combined all single system hy-
potheses to one hypothesis with a higher BLEU and
a lower TER score compared to each single sys-
tem. For each optimization criteria the system com-
binations using all single systems outperforms the
system combinations using one less single system.
Although the single system of SYSTRAN has the
worst error scores and the RWTH single systems are
similar, we achieved the best result in using all single
systems. For the WMT 12 evaluation, we submitted
the system combination of all systems optimized on
BLEU.
Acknowledgments
This work was achieved as part of the Quaero Pro-
gramme, funded by OSEO, French State agency for
innovation.
References
Alexandre Allauzen, Josep M. Crego, I?lknur Durgar El-
Kahlout, and Francois Yvon. 2010. LIMSI?s statis-
tical translation systems for WMT?10. In Proc. of the
326
Table 1: All systems for the WMT 2012 German?English translation task (truecase). BLEU and TER results are in
percentage. sc denotes system combination. All system combinations are unoptimized.
system newstest2008 newstest2009 newstest2010 newstest2011
BLEU TER BLEU TER BLEU TER BLEU TER TER-BLEU
KIT 22.2 61.8 21.3 61.0 24.1 59.0 22.4 60.2 37.9
RWTH.PBT 21.4 62.0 21.3 61.1 23.9 59.1 21.4 61.2 39.7
Limsi 22.2 63.0 22.0 61.8 23.9 59.9 21.8 62.0 40.2
RWTH.HPBT 21.5 62.6 21.5 61.6 23.6 60.2 21.5 61.8 40.4
SYSTRAN 18.3 64.6 17.9 63.4 21.1 60.5 18.3 63.1 44.8
sc-withAllSystems 23.4 59.7 22.9 59.0 26.2 56.5 23.3 58.8 35.5
sc-without-RWTH.PBT 23.2 59.8 22.8 59.0 25.9 56.6 23.1 58.7 35.6
sc-without-RWTH.HPBT 23.2 59.6 22.7 58.9 26.1 56.2 23.1 58.7 35.6
sc-without-Limsi 22.7 60.1 22.4 59.2 25.5 56.7 22.8 58.8 36.0
sc-without-SYSTRAN 23.0 60.3 22.5 59.5 25.7 57.2 23.1 59.2 36.1
sc-without-KIT 23.0 59.9 22.5 59.1 25.9 56.6 22.9 59.1 36.3
Table 2: All systems for the WMT 2012 German?English translation task (truecase). BLEU and TER results are in
percentage. sc denotes system combination. All system combinations are optimized on BLEU .
system newstest2008 newstest2009 newstest2010 newstest2011
BLEU TER BLEU TER BLEU TER BLEU TER TER-BLEU
sc-withAllSystems 23.7 60.3 23.2 59.5 26.0 57.1 23.7 59.2 35.6
sc-without-RWTH.PBT 23.4 61.1 23.1 59.8 25.5 57.6 23.5 59.5 36.1
sc-without-SYSTRAN 23.3 61.1 22.6 60.5 25.3 58.1 23.5 60.0 36.5
sc-without-Limsi 23.1 60.7 22.6 59.7 25.4 57.5 23.3 59.4 36.2
sc-without-KIT 23.4 60.7 23.0 59.7 25.6 57.7 23.3 59.8 36.5
sc-without-RWTH.HPBT 23.3 59.4 22.8 58.6 26.1 56.0 23.1 58.4 35.2
Table 3: All systems for the WMT 2012 German?English translation task (truecase). BLEU and TER results are in
percentage. sc denotes system combination. All system combinations are optimized on TER-BLEU .
system newstest2008 newstest2009 newstest2010 newstest2011
BLEU TER BLEU TER BLEU TER BLEU TER TER-BLEU
sc-withAllSystems 23.0 58.8 22.4 58.3 25.8 55.6 23.1 57.7 34.6
sc-without-RWTH.PBT 23.0 59.3 22.5 58.5 25.6 56.0 23.1 58.0 34.9
sc-without-RWTH.HPBT 23.1 59.0 22.6 58.3 25.8 55.6 23.0 58.0 35.0
sc-without-SYSTRAN 22.9 59.7 22.4 59.1 25.6 56.7 23.2 58.5 35.3
sc-without-Limsi 22.7 59.4 22.2 58.7 25.3 56.1 22.7 58.1 35.5
sc-without-KIT 22.9 59.3 22.4 58.5 25.7 55.8 22.7 58.1 35.4
327
Joint Workshop on Statistical Machine Translation and
MetricsMATR, pages 54?59, Uppsala, Sweden.
Alexandre Allauzen, Gilles Adda, He?le`ne Bonneau-
Maynard, Josep M. Crego, Hai-Son Le, Aure?lien Max,
Adrien Lardilleux, Thomas Lavergne, Artem Sokolov,
Guillaume Wisniewski, and Franc?ois Yvon. 2011.
LIMSI @ WMT11. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation, pages 309?
315, Edinburgh, Scotland, July. Association for Com-
putational Linguistics.
F. Casacuberta and E. Vidal. 2004. Machine translation
with inferred stochastic finite-state transducers. Com-
putational Linguistics, 30(3):205?225.
D. Chiang. 2007. Hierarchical Phrase-Based Transla-
tion. Computational Linguistics, 33(2):201?228.
J.M. Crego and J.B. Marin?o. 2007. Improving statistical
MT by coupling reordering and decoding. Machine
Translation, 20(3):199?215.
Josep M. Crego, Franois Yvon, and Jos B. Mario.
2011. N-code: an open-source Bilingual N-gram SMT
Toolkit. Prague Bulletin of Mathematical Linguistics,
96:49?58.
D. De?chelotte, O. Galibert G. Adda, A. Allauzen, J. Gau-
vain, H. Meynard, and F. Yvon. 2008. LIMSI?s statis-
tical translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
Ilknur Durgar El-Kahlout and Franois Yvon. 2010. The
pay-offs of preprocessing for German-English Statis-
tical Machine Translation. In Marcello Federico, Ian
Lane, Michael Paul, and Franois Yvon, editors, Pro-
ceedings of the seventh International Workshop on
Spoken Language Translation (IWSLT), pages 251?
258.
L. Huang and D. Chiang. 2007. Forest Rescoring: Faster
Decoding with Integrated Language Models. In Proc.
Annual Meeting of the Association for Computational
Linguistics, pages 144?151, Prague, Czech Republic,
June.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Proceedings of the In-
ternational Conference on Acoustics, Speech, and Sig-
nal Processing, ICASSP?95, pages 181?184, Detroit,
MI.
P. Koehn and K. Knight. 2003a. Empirical Methods for
Compound Splitting. In EACL, Budapest, Hungary.
P. Koehn and K. Knight. 2003b. Empirical Methods
for Compound Splitting. In Proceedings of European
Chapter of the ACL (EACL 2009), pages 187?194.
J. Senellart L. Dugast and P. Koehn. 2007. Statistical
post-editing on systran?s rule-based translation system.
In Proceedings of the Second Workshop on Statisti-
cal Machine Translation, StatMT ?07, pages 220?223,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-Luc
Gauvain, and Franc?ois Yvon. 2011. Structured output
layer neural network language model. In Proceedings
of ICASSP?11, pages 5524?5527.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous space translation models with neu-
ral networks. In NAACL ?12: Proceedings of the
2012 Conference of the North American Chapter of the
Association for Computational Linguistics on Human
Language Technology.
Jose? B. Marin?o, R. Banchs, J.M. Crego, A. de Gispert,
P. Lambert, J.A.R. Fonollosa, and M.R. Costa-jussa`.
2006. N-gram-based machine translation. Computa-
tional Linguistics, 32(4).
E. Matusov, N. Ueffing, and H. Ney. 2006. Computing
Consensus Translation from Multiple Machine Trans-
lation Systems Using Enhanced Hypotheses Align-
ment. In Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 33?40.
E. Matusov, G. Leusch, R.E. Banchs, N. Bertoldi,
D. Dechelotte, M. Federico, M. Kolss, Y.-S. Lee,
J.B. Mari no, M. Paulik, S. Roukos, H. Schwenk, and
H. Ney. 2008. System Combination for Machine
Translation of Spoken and Written Language. IEEE
Transactions on Audio, Speech and Language Pro-
cessing, 16(7):1222?1237.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2009. Ex-
tending Statistical Machine Translation with Discrim-
inative and Trigger-based Lexicon Models. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing: Volume 1 - Vol-
ume 1, EMNLP ?09, Singapore.
Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa
Herrmann, and Alex Waibel. 2011. The KIT English-
French Translation Systems for IWSLT 2011. In Pro-
ceedings of the Eighth International Workshop on Spo-
ken Language Translation (IWSLT).
R.C. Moore and W. Lewis. 2010. Intelligent Selection
of Language Model Training Data. In ACL (Short Pa-
pers), pages 220?224, Uppsala, Sweden, July.
J. Niehues and M. Kolss. 2009. A POS-Based Model for
Long-Range Reorderings in SMT. In Fourth Work-
shop on Statistical Machine Translation (WMT 2009),
Athens, Greece.
J. Niehues and S. Vogel. 2008. Discriminative Word
Alignment via Alignment Matrix Modeling. In Proc.
of Third ACL Workshop on Statistical Machine Trans-
lation, Columbus, USA.
Jan Niehues and Alex Waibel. 2011. Using Wikipedia
to Translate Domain-specific Terms in SMT. In Pro-
328
ceedings of the Eighth International Workshop on Spo-
ken Language Translation (IWSLT), San Francisco,
CA.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and Alex
Waibel. 2011. Wider Context by Using Bilingual Lan-
guage Models in Machine Translation. In Sixth Work-
shop on Statistical Machine Translation (WMT 2011),
Edinburgh, UK.
F.J. Och and H. Ney. 2003. A Systematic Comparison of
Various Statistical Alignment Models. Computational
Linguistics, 29(1):19?51.
F.J. Och. 2003. Minimum Error Rate Training for Statis-
tical Machine Translation. In Proc. Annual Meeting of
the Association for Computational Linguistics, pages
160?167, Sapporo, Japan, July.
A. Birch P. Koehn, H. Hoang, C. Callison-Burch, M. Fed-
erico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,
R. Zens, C. Dyer, O. Bojar, A. Constantin, and
E. Herbst. 2007. Moses: open source toolkit for
statistical machine translation. In Proceedings of the
45th Annual Meeting of the ACL on Interactive Poster
and Demonstration Sessions, ACL ?07, pages 177?
180, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
M. Popovic?, D. Stein, and H. Ney. 2006. Statistical
Machine Translation of German Compound Words.
In FinTAL - 5th International Conference on Natural
Language Processing, Springer Verlag, LNCS, pages
616?624.
Anna N. Rafferty and Christopher D. Manning. 2008.
Parsing three German treebanks: lexicalized and un-
lexicalized baselines. In Proceedings of the Workshop
on Parsing German.
K. Rottmann and S. Vogel. 2007. Word Reordering in
Statistical Machine Translation with a POS-Based Dis-
tortion Model. In TMI, Sko?vde, Sweden.
H. Schmid. 1994. Probabilistic Part-of-Speech Tagging
Using Decision Trees. In International Conference
on NewMethods in Language Processing, Manchester,
UK.
Helmut Schmid. 1995. Improvements in part-of-speech
tagging with an application to German. In Evelyne
Tzoukermann and SusanEditors Armstrong, editors,
Proceedings of the ACL SIGDATWorkshop, pages 47?
50. Kluwer Academic Publishers.
A. Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proc. Int. Conf. on Spoken Language
Processing, volume 2, pages 901?904, Denver, Col-
orado, USA, September.
C. Tillmann. 2004. A unigram orientation model for sta-
tistical machine translation. In Proceedings of HLT-
NAACL 2004, pages 101?104. Association for Com-
putational Linguistics.
A. Venugopal, A. Zollman, and A. Waibel. 2005. Train-
ing and Evaluation Error Minimization Rules for Sta-
tistical Machine Translation. In Workshop on Data-
drive Machine Translation and Beyond (WPT-05), Ann
Arbor, MI.
D. Vilar, S. Stein, M. Huck, and H. Ney. 2010. Jane:
Open Source Hierarchical Translation, Extended with
Reordering and Lexicon Models. In ACL 2010 Joint
Fifth Workshop on Statistical Machine Translation and
Metrics MATR, pages 262?270, Uppsala, Sweden,
July.
S. Vogel. 2003. SMT Decoder Dissected: Word Re-
ordering. In Int. Conf. on Natural Language Process-
ing and Knowledge Engineering, Beijing, China.
R. Zens and H. Ney. 2008. Improvements in Dynamic
Programming Beam Search for Phrase-based Statisti-
cal Machine Translation. In Proc. of the Int. Workshop
on Spoken Language Translation (IWSLT), Honolulu,
Hawaii, October.
329
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 185?192,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Joint WMT 2013 Submission of the QUAERO Project
?Stephan Peitz, ?Saab Mansour, ?Matthias Huck, ?Markus Freitag, ?Hermann Ney,
?Eunah Cho, ?Teresa Herrmann, ?Mohammed Mediani, ?Jan Niehues, ?Alex Waibel,
?Alexandre Allauzen, ?Quoc Khanh Do,
?Bianka Buschbeck, ?Tonio Wandmacher
?RWTH Aachen University, Aachen, Germany
?Karlsruhe Institute of Technology, Karlsruhe, Germany
?LIMSI-CNRS, Orsay, France
?SYSTRAN Software, Inc.
?surname@cs.rwth-aachen.de
?firstname.surname@kit.edu
?firstname.lastname@limsi.fr ?surname@systran.fr
Abstract
This paper describes the joint submis-
sion of the QUAERO project for the
German?English translation task of the
ACL 2013 Eighth Workshop on Statisti-
cal Machine Translation (WMT 2013).
The submission was a system combina-
tion of the output of four different transla-
tion systems provided by RWTH Aachen
University, Karlsruhe Institute of Technol-
ogy (KIT), LIMSI-CNRS and SYSTRAN
Software, Inc. The translations were
joined using the RWTH?s system com-
bination approach. Experimental results
show improvements of up to 1.2 points in
BLEU and 1.2 points in TER compared to
the best single translation.
1 Introduction
QUAERO is a European research and develop-
ment program with the goal of developing multi-
media and multilingual indexing and management
tools for professional and general public applica-
tions (http://www.quaero.org). Research in ma-
chine translation is mainly assigned to the four
groups participating in this joint submission. The
aim of this submission was to show the quality of
a joint translation by combining the knowledge of
the four project partners. Each group develop and
maintain their own different machine translation
system. These single systems differ not only in
their general approach, but also in the preprocess-
ing of training and test data. To take advantage
of these differences of each translation system, we
combined all hypotheses of the different systems,
using the RWTH system combination approach.
This paper is structured as follows. First, the
different engines of all four groups are introduced.
In Section 3, the RWTH Aachen system combina-
tion approach is presented. Experiments with dif-
ferent system selections for system combination
are described in Section 4. This paper is concluded
in Section 5.
2 Translation Systems
For WMT 2013, each QUAERO partner trained
their systems on the parallel Europarl (EPPS),
News Commentary (NC) corpora and the web-
crawled corpus. All single systems were tuned on
the newstest2009 and newstest2010 development
set. The newstest2011 development set was used
to tune the system combination parameters. Fi-
nally, on newstest2012 the results of the different
system combination settings are compared. In this
Section, all four different translation engines are
presented.
2.1 RWTH Aachen Single System
For the WMT 2013 evaluation, RWTH utilized a
phrase-based decoder based on (Wuebker et al,
2012) which is part of RWTH?s open-source SMT
toolkit Jane 2.1 1. GIZA++ (Och and Ney, 2003)
was employed to train a word alignment, language
models have been created with the SRILM toolkit
(Stolcke, 2002).
After phrase pair extraction from the word-
aligned parallel corpus, the translation probabil-
ities are estimated by relative frequencies. The
standard feature set alo includes an n-gram lan-
guage model, phrase-level IBM-1 and word-,
phrase- and distortion-penalties, which are com-
bined in log-linear fashion. Furthermore, we used
an additional reordering model as described in
(Galley and Manning, 2008). By this model six
1http://www-i6.informatik.rwth-aachen.
de/jane/
185
additional feature are added to the log-linear com-
bination. The model weights are optimized with
standard Mert (Och, 2003a) on 200-best lists. The
optimization criterion is BLEU.
2.1.1 Preprocessing
In order to reduce the source vocabulary size trans-
lation, the German text was preprocessed by split-
ting German compound words with the frequency-
based method described in (Koehn and Knight,
2003). To further reduce translation complexity
for the phrase-based approach, we performed the
long-range part-of-speech based reordering rules
proposed by (Popovic? et al, 2006).
2.1.2 Translation Model
We applied filtering and weighting for domain-
adaptation similarly to (Mansour et al, 2011) and
(Mansour and Ney, 2012). For filtering the bilin-
gual data, a combination of LM and IBM Model
1 scores was used. In addition, we performed
weighted phrase extraction by using a combined
LM and IBM Model 1 weight.
2.1.3 Language Model
During decoding a 4-gram language model is ap-
plied. The language model is trained on the par-
allel data as well as the provided News crawl,
the 109 French-English, UN and LDC Gigaword
Fourth Edition corpora.
2.2 Karlsruhe Institute of Technology Single
System
2.2.1 Preprocessing
The training data was preprocessed prior to the
training. Symbols such as quotes, dashes and
apostrophes are normalized. Then the first words
of each sentence are smart-cased. For the Ger-
man part of the training corpus, the hunspell2 lex-
icon was used, in order to learn a mapping from
old German spelling to new German writing rules.
Compound-splitting was also performed as de-
scribed in Koehn and Knight (2003). We also re-
moved very long sentences, empty lines, and sen-
tences which show big mismatch on the length.
2.2.2 Filtering
The web-crawled corpus was filtered using an
SVM classifier as described in (Mediani et al,
2011). The lexica used in this filtering task were
obtained from Giza alignments trained on the
2http://hunspell.sourceforge.net/
cleaner corpora, EPPS and NC. Assuming that this
corpus is very noisy, we biased our classifier more
towards precision than recall. This was realized
by giving higher number of false examples (80%
of the training data).
This filtering technique ruled out more than
38% of the corpus (the unfiltered corpus contains
around 2.4M pairs, 0.9M of which were rejected
in the filtering task).
2.2.3 System Overview
The in-house phrase-based decoder (Vogel, 2003)
is used to perform decoding. Optimization with
regard to the BLEU score is done using Minimum
Error Rate Training (MERT) as described in Venu-
gopal et al (2005).
2.2.4 Reordering Model
We applied part-of-speech (POS) based reordering
using probabilistic continuous (Rottmann and Vo-
gel, 2007) and discontinuous (Niehues and Kolss,
2009) rules. This was learned using POS tags gen-
erated by the TreeTagger (Schmid, 1994) for short
and long range reorderings respectively.
In addition to this POS-based reordering, we
also used tree-based reordering rules. Syntactic
parse trees of the whole training corpus and the
word alignment between source and target lan-
guage are used to learn rules on how to reorder the
constituents in a German source sentence to make
it match the English target sentence word order
better (Herrmann et al, 2013). The training corpus
was parsed by the Stanford parser (Rafferty and
Manning, 2008). The reordering rules are applied
to the source sentences and the reordered sentence
variants as well as the original sequence are en-
coded in a word lattice which is used as input to
the decoder.
Moreover, our reordering model was extended
so that it could include the features of lexicalized
reordering model. The reordering probabilities for
each phrase pair are stored as well as the origi-
nal position of each word in the lattice. During
the decoding, the reordering origin of the words
is checked along with its probability added as an
additional score.
2.2.5 Translation Models
The translation model uses the parallel data of
EPPS, NC, and the filtered web-crawled data. As
word alignment, we used the Discriminative Word
Alignment (DWA) as shown in (Niehues and Vo-
186
gel, 2008). The phrase pairs were extracted using
different source word order suggested by the POS-
based reordering models presented previously as
described in (Niehues et al, 2009).
In order to extend the context of source lan-
guage words, we applied a bilingual language
model (Niehues et al, 2011). A Discriminative
Word Lexicon (DWL) introduced in (Mauser et
al., 2009) was extended so that it could take the
source context also into the account. For this,
we used a bag-of-ngrams instead of representing
the source sentence as a bag-of-words. Filtering
based on counts was then applied to the features
for higher order n-grams. In addition to this, the
training examples were created differently so that
we only used the words that occur in the n-best list
but not in the reference as negative example.
2.2.6 Language Models
We build separate language models and combined
them prior to decoding. As word-token based
language models, one language model is built on
EPPS, NC, and giga corpus, while another one is
built using crawled data. We combined the LMs
linearly by minimizing the perplexity on the de-
velopment data. As a bilingual language model we
used the EPPS, NC, and the web-crawled data and
combined them. Furthermore, we use a 5-gram
cluster-based language model with 1,000 word
clusters, which was trained on the EPPS and NC
corpus. The word clusters were created using the
MKCLS algorithm.
2.3 LIMSI-CNRS Single System
2.3.1 System overview
LIMSI?s system is built with n-code (Crego et al,
2011), an open source statistical machine transla-
tion system based on bilingual n-gram3. In this
approach, the translation model relies on a spe-
cific decomposition of the joint probability of a
sentence pair using the n-gram assumption: a sen-
tence pair is decomposed into a sequence of bilin-
gual units called tuples, defining a joint segmen-
tation of the source and target. In the approach of
(Marin?o et al, 2006), this segmentation is a by-
product of source reordering which ultimately de-
rives from initial word and phrase alignments.
2.3.2 An overview of n-code
The baseline translation model is implemented as
a stochastic finite-state transducer trained using
3http://ncode.limsi.fr/
a n-gram model of (source,target) pairs (Casacu-
berta and Vidal, 2004). Training this model re-
quires to reorder source sentences so as to match
the target word order. This is performed by
a stochastic finite-state reordering model, which
uses part-of-speech information4 to generalize re-
ordering patterns beyond lexical regularities.
In addition to the translation model, eleven fea-
ture functions are combined: a target-language
model; four lexicon models; two lexicalized re-
ordering models (Tillmann, 2004) aiming at pre-
dicting the orientation of the next translation unit;
a ?weak? distance-based distortion model; and
finally a word-bonus model and a tuple-bonus
model which compensate for the system prefer-
ence for short translations. The four lexicon mod-
els are similar to the ones use in a standard phrase
based system: two scores correspond to the rel-
ative frequencies of the tuples and two lexical
weights estimated from the automatically gener-
ated word alignments. The weights associated to
feature functions are optimally combined using a
discriminative training framework (Och, 2003b).
The overall search is based on a beam-search
strategy on top of a dynamic programming algo-
rithm. Reordering hypotheses are computed in a
preprocessing step, making use of reordering rules
built from the word reorderings introduced in the
tuple extraction process. The resulting reordering
hypotheses are passed to the decoder in the form
of word lattices (Crego and Mario, 2006).
2.3.3 Continuous space translation models
One critical issue with standard n-gram translation
models is that the elementary units are bilingual
pairs, which means that the underlying vocabu-
lary can be quite large, even for small translation
tasks. Unfortunately, the parallel data available to
train these models are typically order of magni-
tudes smaller than the corresponding monolingual
corpora used to train target language models. It is
very likely then, that such models should face se-
vere estimation problems. In such setting, using
neural network language model techniques seem
all the more appropriate. For this study, we fol-
low the recommendations of Le et al (2012), who
propose to factor the joint probability of a sen-
tence pair by decomposing tuples in two (source
and target) parts, and further each part in words.
This yields a word factored translation model that
4Part-of-speech labels for English and German are com-
puted using the TreeTagger (Schmid, 1995).
187
can be estimated in a continuous space using the
SOUL architecture (Le et al, 2011).
The design and integration of a SOUL model for
large SMT tasks is far from easy, given the com-
putational cost of computing n-gram probabilities.
The solution used here was to resort to a two pass
approach: the first pass uses a conventional back-
off n-gram model to produce a k-best list; in the
second pass, the k-best list is reordered using the
probabilities of m-gram SOUL translation models.
In the following experiments, we used a fixed con-
text size for SOUL of m= 10, and used k = 300.
2.3.4 Corpora and data pre-processing
All the parallel data allowed in the constrained
task are pooled together to create a single par-
allel corpus. This corpus is word-aligned using
MGIZA++5 with default settings. For the English
monolingual training data, we used the same setup
as last year6 and thus the same target language
model as detailed in (Allauzen et al, 2011).
For English, we also took advantage of our in-
house text processing tools for the tokenization
and detokenization steps (Dchelotte et al, 2008)
and our system is built in ?true-case?. As Ger-
man is morphologically more complex than En-
glish, the default policy which consists in treat-
ing each word form independently is plagued with
data sparsity, which is detrimental both at training
and decoding time. Thus, the German side was
normalized using a specific pre-processing scheme
(described in (Allauzen et al, 2010; Durgar El-
Kahlout and Yvon, 2010)), which notably aims at
reducing the lexical redundancy by (i) normalizing
the orthography, (ii) neutralizing most inflections
and (iii) splitting complex compounds.
2.4 SYSTRAN Software, Inc. Single System
In the past few years, SYSTRAN has been focus-
ing on the introduction of statistical approaches
to its rule-based backbone, leading to Hybrid Ma-
chine Translation.
The technique of Statistical Post-Editing
(Dugast et al, 2007) is used to automatically edit
the output of the rule-based system. A Statistical
Post-Editing (SPE) module is generated from a
bilingual corpus. It is basically a translation mod-
ule by itself, however it is trained on rule-based
5http://geek.kyloo.net/software
6The fifth edition of the English Gigaword
(LDC2011T07) was not used.
translations and reference data. It applies correc-
tions and adaptations learned from a phrase-based
5-gram language model. Using this two-step
process will implicitly keep long distance re-
lations and other constraints determined by the
rule-based system while significantly improving
phrasal fluency. It has the advantage that quality
improvements can be achieved with very little
but targeted bilingual data, thus significantly
reducing training time and increasing translation
performance.
The basic setup of the SPE component is identi-
cal to the one described in (Dugast et al, 2007).
A statistical translation model is trained on the
rule-based translation of the source and the target
side of the parallel corpus. Language models are
trained on each target half of the parallel corpora
and also on additional in-domain corpora. More-
over, the following measures - limiting unwanted
statistical effects - were applied:
? Named entities are replaced by special tokens
on both sides. This usually improves word
alignment, since the vocabulary size is sig-
nificantly reduced. In addition, entity trans-
lation is handled more reliably by the rule-
based engine.
? The intersection of both vocabularies (i.e. vo-
cabularies of the rule-based output and the
reference translation) is used to produce an
additional parallel corpus (whose target is
identical to the source). This was added to the
parallel text in order to improve word align-
ment.
? Singleton phrase pairs are deleted from the
phrase table to avoid overfitting.
? Phrase pairs not containing the same number
of entities on the source and the target side
are also discarded.
? Phrase pairs appearing less than 2 times were
pruned.
The SPE language model was trained on 2M
phrases from the news/europarl and Common-
Crawl corpora, provided as training data for WMT
2013. Weights for these separate models were
tuned by the Mert algorithm provided in the Moses
toolkit (Koehn et al, 2007), using the provided
news development set.
188
0
1
5:
th
at
/1
7:
th
is/
3
2
3:
is/
3
8:
w
as
/1
3
0:
*E
PS
*/
3
4:
it/
1
4
0:
*E
PS
*/
3
2:
in
/1
5
0:
*E
PS
*/
3
6:
th
e/
1
6
0:
*E
PS
*/
1
1:
fu
tu
re
/3
Figure 1: Confusion network of four different hypotheses.
3 RWTH Aachen System Combination
System combination is used to produce consen-
sus translations from multiple hypotheses gener-
ated with different translation engines. First, a
word to word alignment for the given single sys-
tem hypotheses is produced. In a second step a
confusion network is constructed. Then, the hy-
pothesis with the highest probability is extracted
from this confusion network. For the alignment
procedure, each of the given single systems gen-
erates one confusion network with its own as pri-
mary system. To this primary system all other hy-
potheses are aligned using the METEOR (Lavie
and Agarwal, 2007) alignment and thus the pri-
mary system defines the word order. Once the
alignment is given, the corresponding confusion
network is constructed. An example is given in
Figure 1. The final network for one source sen-
tence is the union of all confusion networks gen-
erated from the different primary systems. That
allows the system combination to select the word
order from different system outputs.
Before performing system combination, each
translation output was normalized by tokenization
and lowercasing. The output of the combination
was then truecased based on the original truecased
output.
The model weights of the system combination
are optimized with standard Mert (Och, 2003a)
on 100-best lists. We add one voting feature for
each single system to the log-linear framework of
the system combination. The voting feature fires
for each word the single system agrees on. More-
over, a word penalty, a language model trained on
the input hypotheses, a binary feature which pe-
nalizes word deletions in the confusion network
and a primary feature which marks the system
which provides the word order are combined in
this log-linear model. The optimization criterion
is 4BLEU-TER.
4 Experimental Results
In this year?s experiments, we tried to improve the
result of the system combination further by com-
bining single systems tuned on different develop-
Table 1: Comparison of single systems tuned on
newstest2009 and newstest2010. The results are
reported on newstest2012.
single systems tuned on newstest2012
newstest BLEU TER
KIT 2009 24.6 58.4
2010 24.6 58.6
LIMSI 2009 22.5 61.5
2010 22.6 59.8
SYSTRAN 2009 20.9 63.3
2010 21.2 62.2
RWTH 2009 23.7 60.8
2010 24.4 58.8
ment sets. The idea is to achieve a more stable
performance in terms of translation quality, if the
single systems are not optimized on the same data
set. In Table 1, the results of each provided single
system tuned on newstest2009 and newstest2010
are shown. For RWTH, LIMSI and SYSTRAN,
it seems that the performance of the single system
depends on the chosen tuning set. However, the
translation quality of the single systems provided
by KIT is stable.
As initial approach and for the final submis-
sion, we grouped single systems with dissimilar
approaches. Thus, KIT (phrase-based SMT) and
SYSTRAN (rule-based MT) tuned their system on
newstest2010, while RWTH (phrase-based SMT)
and LIMSI (n-gram) optimized on newstest2009.
To compare the impact of this approach, all pos-
sible combinations were checked (Table 2). How-
ever, it seems that the translation quality can not be
improved by this approach. For the test set (new-
stest2012), BLEU is steady around 25.6 points.
Even if the single system with lowest BLEU are
combined (KIT 2010, LIMSI 2009, SYSTRAN
2010, RWTH 2009), the translation quality in
terms of BLEU is comparable with the combina-
tion of the best single systems (KIT 2009, LIMSI
2010, SYSTRAN 2010, RWTH 2010). However,
we could gain 1.0 point in TER.
Due to the fact, that for the final submission the
initial grouping was available only, we kept this
189
Table 2: Comparison of different system combination settings. For each possible combination of systems
tuned on different tuning sets, a system combination was set up, re-tuned on newstest2011 and evaluated
on newstest2012. The setting used for further experiments is set in boldface.
single systems system combinations
KIT LIMSI SYSTRAN RWTH newstest2011 newstest2012
tuned on newstest BLEU TER BLEU TER
2009 2009 2009 2009 24.6 58.0 25.6 56.8
2010 2010 2010 2010 24.2 58.1 25.6 57.7
2010 2009 2009 2009 24.5 57.9 25.7 57.4
2009 2010 2009 2009 24.4 58.3 25.7 57.0
2009 2009 2010 2009 24.5 57.9 25.6 57.0
2009 2009 2009 2010 24.5 58.0 25.6 56.8
2009 2010 2010 2010 24.1 57.5 25.4 56.4
2010 2009 2010 2010 24.3 57.6 25.6 56.9
2010 2010 2009 2010 24.2 58.0 25.6 57.3
2010 2010 2010 2009 24.3 57.9 25.5 57.6
2010 2010 2009 2009 24.4 58.1 25.6 57.5
2009 2009 2010 2010 24.4 57.8 25.5 56.6
2009 2010 2010 2009 24.4 58.2 25.5 57.0
2009 2010 2009 2010 24.2 57.8 25.5 56.8
2010 2009 2009 2010 24.4 57.9 25.6 57.4
2010 2009 2010 2009 24.4 57.7 25.6 57.4
Table 3: Results of the final submission (bold-
face) compared with best single system on new-
stest2012.
newstest2011 newstest2012
BLEU TER BLEU TER
best single 23.2 60.9 24.6 58.4
system comb. 24.4 57.7 25.6 57.4
+ IBM-1 24.6 58.1 25.6 57.6
+ bigLM 24.6 57.9 25.8 57.2
combination. To improve this baseline further, two
additional models were added. We applied lexi-
cal smoothing (IBM-1) and an additional language
model (bigLM) trained on the English side of the
parallel data and the News shuffle corpus. The re-
sults are presented in Table 3.
The baseline was slightly improved by 0.2
points in BLEU and TER. Note, this system com-
bination was the final submission.
5 Conclusion
For the participation in the WMT 2013 shared
translation task, the partners of the QUAERO
project (Karlsruhe Institute of Technology, RWTH
Aachen University, LIMSI-CNRS and SYSTRAN
Software, Inc.) provided a joint submission. By
joining the output of four different translation sys-
tems with RWTH?s system combination, we re-
ported an improvement of up to 1.2 points in
BLEU and TER.
Combining systems optimized on different tun-
ing sets does not seem to improve the translation
quality. However, by adding additional model, the
baseline was slightly improved.
All in all, we conclude that the variability in
terms of BLEU does not influence the final result.
It seems that using different approaches of MT in
a system combination is more important (Freitag
et al, 2012).
Acknowledgments
This work was achieved as part of the Quaero Pro-
gramme, funded by OSEO, French State agency
for innovation.
References
Alexandre Allauzen, Josep M. Crego, I?lknur Durgar El-
Kahlout, and Franc?ois Yvon. 2010. LIMSI?s statis-
tical translation systems for WMT?10. In Proc. of
190
the Joint Workshop on Statistical Machine Transla-
tion and MetricsMATR, pages 54?59, Uppsala, Swe-
den.
Alexandre Allauzen, Gilles Adda, He?le`ne Bonneau-
Maynard, Josep M. Crego, Hai-Son Le, Aure?lien
Max, Adrien Lardilleux, Thomas Lavergne, Artem
Sokolov, Guillaume Wisniewski, and Franc?ois
Yvon. 2011. LIMSI @ WMT11. In Proceedings of
the Sixth Workshop on Statistical Machine Transla-
tion, pages 309?315, Edinburgh, Scotland, July. As-
sociation for Computational Linguistics.
Francesco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(3):205?
225.
Josep M. Crego and Jose? B. Mario. 2006. Improving
statistical MT by coupling reordering and decoding.
Machine Translation, 20(3):199?215.
Josep M. Crego, Franois Yvon, and Jos B. Mario.
2011. N-code: an open-source Bilingual N-gram
SMT Toolkit. Prague Bulletin of Mathematical Lin-
guistics, 96:49?58.
Lo??c Dugast, Jean Senellart, and Philipp Koehn. 2007.
Statistical post-editing on systran?s rule-based trans-
lation system. In Proceedings of the Second Work-
shop on Statistical Machine Translation, StatMT
?07, pages 220?223, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Ilknur Durgar El-Kahlout and Franois Yvon. 2010.
The pay-offs of preprocessing for German-English
Statistical Machine Translation. In Marcello Fed-
erico, Ian Lane, Michael Paul, and Franois Yvon, ed-
itors, Proceedings of the seventh International Work-
shop on Spoken Language Translation (IWSLT),
pages 251?258.
Daniel Dchelotte, Gilles Adda, Alexandre Allauzen,
Olivier Galibert, Jean-Luc Gauvain, Hlne Maynard,
and Franois Yvon. 2008. LIMSI?s statistical
translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
Markus Freitag, Stephan Peitz, Matthias Huck, Her-
mann Ney, Teresa Herrmann, Jan Niehues, Alex
Waibel, Alexandre Allauzen, Gilles Adda, Bianka
Buschbeck, Josep Maria Crego, and Jean Senellart.
2012. Joint wmt 2012 submission of the quaero
project. In NAACL 2012 Seventh Workshop on Sta-
tistical Machine Translation, pages 322?329, Mon-
treal, Canada, June.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 847?855, Honolulu, Hawaii, October. As-
sociation for Computational Linguistics.
Teresa Herrmann, Jan Niehues, and Alex Waibel.
2013. Combining Word Reordering Methods on
different Linguistic Abstraction Levels for Statisti-
cal Machine Translation. In Proceedings of the Sev-
enth Workshop on Syntax, Semantics and Structure
in Statistical Translation, Altanta, Georgia, USA,
June. Association for Computational Linguistics.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In EACL, Bu-
dapest, Hungary.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantine, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
pages 177?180, Prague, Czech Republic, June.
Alon Lavie and Abhaya Agarwal. 2007. ME-
TEOR: An Automatic Metric for MT Evaluation
with High Levels of Correlation with Human Judg-
ments. pages 228?231, Prague, Czech Republic,
June.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-
Luc Gauvain, and Franc?ois Yvon. 2011. Structured
output layer neural network language model. In Pro-
ceedings of ICASSP?11, pages 5524?5527.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous space translation models with
neural networks. In NAACL ?12: Proceedings of
the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguistics
on Human Language Technology.
Saab Mansour and Hermann Ney. 2012. A sim-
ple and effective weighted phrase extraction for ma-
chine translation adaptation. In International Work-
shop on Spoken Language Translation, pages 193?
200, Hong Kong, December.
Sab Mansour, Joern Wuebker, and Hermann Ney.
2011. Combining Translation and Language Model
Scoring for Domain-Specific Data Filtering. In Pro-
ceedings of the International Workshop on Spoken
Language Translation (IWSLT), San Francisco, CA,
December.
Jose? B. Marin?o, Rafael E. Banchs, Josep M. Crego,
Adria` de Gispert, Patrick Lambert, Jose? A.R. Fonol-
losa, and Marta R. Costa-Jussa`. 2006. N-gram-
based machine translation. Computational Linguis-
tics, 32(4):527?549.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2009.
Extending Statistical Machine Translation with Dis-
criminative and Trigger-based Lexicon Models. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume
1 - Volume 1, EMNLP ?09, Singapore.
191
Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa
Herrmann, and Alex Waibel. 2011. The KIT
English-French Translation Systems for IWSLT
2011. In Proceedings of the Eighth Interna-
tional Workshop on Spoken Language Translation
(IWSLT).
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Jan Niehues and Stephan Vogel. 2008. Discriminative
Word Alignment via Alignment Matrix Modeling.
In Proc. of Third ACL Workshop on Statistical Ma-
chine Translation, Columbus, USA.
Jan Niehues, Teresa Herrmann, Muntsin Kolss, and
Alex Waibel. 2009. The Universita?t Karlsruhe
Translation System for the EACL-WMT 2009. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and
Alex Waibel. 2011. Wider Context by Using Bilin-
gual Language Models in Machine Translation. In
Sixth Workshop on Statistical Machine Translation
(WMT 2011), Edinburgh, UK.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51,
March.
Franz Josef Och. 2003a. Minimum Error Rate Train-
ing in Statistical Machine Translation. In Proc. of
the 41th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 160?167, Sap-
poro, Japan, July.
Franz Josef Och. 2003b. Minimum error rate training
in statistical machine translation. In ACL ?03: Proc.
of the 41st Annual Meeting on Association for Com-
putational Linguistics, pages 160?167.
M. Popovic?, D. Stein, and H. Ney. 2006. Statistical
Machine Translation of German Compound Words.
In FinTAL - 5th International Conference on Nat-
ural Language Processing, Springer Verlag, LNCS,
pages 616?624.
Anna N. Rafferty and Christopher D. Manning. 2008.
Parsing three German treebanks: lexicalized and un-
lexicalized baselines. In Proceedings of the Work-
shop on Parsing German.
Kay Rottmann and Stephan Vogel. 2007. Word Re-
ordering in Statistical Machine Translation with a
POS-Based Distortion Model. In TMI, Sko?vde,
Sweden.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In International
Conference on New Methods in Language Process-
ing, Manchester, UK.
Helmut Schmid. 1995. Improvements in part-of-
speech tagging with an application to German.
In Evelyne Tzoukermann and SusanEditors Arm-
strong, editors, Proceedings of the ACL SIGDAT-
Workshop, pages 47?50. Kluwer Academic Publish-
ers.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. Int. Conf. on Spo-
ken Language Processing, volume 2, pages 901?
904, Denver, Colorado, USA.
Christoph Tillmann. 2004. A unigram orientation
model for statistical machine translation. In Pro-
ceedings of HLT-NAACL 2004, pages 101?104. As-
sociation for Computational Linguistics.
Ashish Venugopal, Andreas Zollman, and Alex Waibel.
2005. Training and Evaluation Error Minimization
Rules for Statistical Machine Translation. In Work-
shop on Data-drive Machine Translation and Be-
yond (WPT-05), Ann Arbor, MI.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In Int. Conf. on Natural Language
Processing and Knowledge Engineering, Beijing,
China.
Joern Wuebker, Matthias Huck, Stephan Peitz, Malte
Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab
Mansour, and Hermann Ney. 2012. Jane 2: Open
source phrase-based and hierarchical statistical ma-
chine translation. In International Conference on
Computational Linguistics, pages 483?491, Mum-
bai, India, December.
192
