Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 369?376,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Exploring the Potential of Intractable Parsers
Mark Hopkins
Dept. of Computational Linguistics
Saarland University
Saarbr?ucken, Germany
mhopkins@coli.uni-sb.de
Jonas Kuhn
Dept. of Computational Linguistics
Saarland University
Saarbr?ucken, Germany
jonask@coli.uni-sb.de
Abstract
We revisit the idea of history-based pars-
ing, and present a history-based parsing
framework that strives to be simple, gen-
eral, and flexible. We also provide a de-
coder for this probability model that is
linear-space, optimal, and anytime. A
parser based on this framework, when
evaluated on Section 23 of the Penn Tree-
bank, compares favorably with other state-
of-the-art approaches, in terms of both ac-
curacy and speed.
1 Introduction
Much of the current research into probabilis-
tic parsing is founded on probabilistic context-
free grammars (PCFGs) (Collins, 1996; Charniak,
1997; Collins, 1999; Charniak, 2000; Charniak,
2001; Klein and Manning, 2003). For instance,
consider the parse tree in Figure 1. One way to de-
compose this parse tree is to view it as a sequence
of applications of CFG rules. For this particular
tree, we could view it as the application of rule
?NP ? NP PP,? followed by rule ?NP ? DT NN,?
followed by rule ?DT ? that,? and so forth. Hence
instead of analyzing P (tree), we deal with the
more modular:
P(NP ? NP PP, NP ? DT NN,
DT ? that, NN ? money, PP ? IN NP,
IN ? in, NP ? DT NN, DT ? the,
NN ? market)
Obviously this joint distribution is just as diffi-
cult to assess and compute with as P (tree). How-
ever there exist cubic-time dynamic programming
algorithms to find the most likely parse if we as-
sume that all CFG rule applications are marginally
NP
NP
DT
that
NN
money
PP
IN
in
NP
DT
the
NN
market
Figure 1: Example parse tree.
independent of one another. The problem, of
course, with this simplification is that although
it is computationally attractive, it is usually too
strong of an independence assumption. To miti-
gate this loss of context, without sacrificing algo-
rithmic tractability, typically researchers annotate
the nodes of the parse tree with contextual infor-
mation. A simple example is the annotation of
nodes with their parent labels (Johnson, 1998).
The choice of which annotations to use is
one of the main features that distinguish parsers
based on this approach. Generally, this approach
has proven quite effective in producing English
phrase-structure grammar parsers that perform
well on the Penn Treebank.
One drawback of this approach is its inflexibil-
ity. Because we are adding probabilistic context
by changing the data itself, we make our data in-
creasingly sparse as we add features. Thus we are
constrained from adding too many features, be-
cause at some point we will not have enough data
to sustain them. We must strike a delicate bal-
ance between how much context we want to in-
clude versus how much we dare to partition our
data set.
369
The major alternative to PCFG-based ap-
proaches are so-called history-based parsers
(Black et al, 1993). These parsers differ from
PCFG parsers in that they incorporate context by
using a more complex probability model, rather
than by modifying the data itself. The tradeoff to
using a more powerful probabilistic model is that
one can no longer employ dynamic programming
to find the most probable parse. Thus one trades
assurances of polynomial running time for greater
modeling flexibility.
There are two canonical parsers that fall into
this category: the decision-tree parser of (Mager-
man, 1995), and the maximum-entropy parser of
(Ratnaparkhi, 1997). Both showed decent results
on parsing the Penn Treebank, but in the decade
since these papers were published, history-based
parsers have been largely ignored by the research
community in favor of PCFG-based approaches.
There are several reasons why this may be. First
is naturally the matter of time efficiency. Mager-
man reports decent parsing times, but for the pur-
poses of efficiency, must restrict his results to sen-
tences of length 40 or less. Furthermore, his two-
phase stack decoder is a bit complicated and is ac-
knowledged to require too much memory to han-
dle certain sentences. Ratnaparkhi is vague about
the running time performance of his parser, stat-
ing that it is ?observed linear-time,? but in any
event, provides only a heuristic, not a complete al-
gorithm.
Next is the matter of flexibility. The main ad-
vantage of abandoning PCFGs is the opportunity
to have a more flexible and adaptable probabilis-
tic parsing model. Unfortunately, both Magerman
and Ratnaparkhi?s models are rather specific and
complicated. Ratnaparkhi?s, for instance, consists
of the interleaved sequence of four different types
of tree construction operations. Furthermore, both
are inextricably tied to the learning procedure that
they employ (decision trees for Magerman, maxi-
mum entropy for Ratnaparkhi).
In this work, our goal is to revisit history-based
parsers, and provide a general-purpose framework
that is (a) simple, (b) fast, (c) space-efficient and
(d) easily adaptable to new domains. As a method
of evaluation, we use this framework with a very
simple set of features to see how well it performs
(both in terms of accuracy and running time) on
the Penn Treebank. The overarching goal is to de-
velop a history-based hierarchical labeling frame-
work that is viable not only for parsing, but for
other application areas that current rely on dy-
namic programming, like phrase-based machine
translation.
2 Preliminaries
For the following discussion, it will be useful to
establish some terminology and notational con-
ventions. Typically we will represent variables
with capital letters (e.g. X , Y ) and sets of vari-
ables with bold-faced capital letters (e.g. X,
Y). The domain of a variable X will be denoted
dom(X), and typically we will use the lower-case
correspondent (in this case, x) to denote a value in
the domain of X . A partial assignment (or simply
assignment) of a set X of variables is a function
w that maps a subset W of the variables of X
to values in their respective domains. We define
dom(w) = W. When W = X, then we say that
w is a full assignment of X. The trivial assign-
ment of X makes no variable assignments.
Let w(X) denote the value that partial assign-
ment w assigns to variable X . For value x ?
dom(X), let w[X = x] denote the assignment
identical to w except that w[X = x](X) = x.
For a set Y of variables, let w|Y denote the re-striction of partial assignment w to the variables
in dom(w) ? Y.
3 The Generative Model
The goal of this section is to develop a probabilis-
tic process that generates labeled trees in a manner
considerably different from PCFGs. We will use
the tree in Figure 2 to motivate our model. In this
example, nodes of the tree are labeled with either
an A or a B. We can represent this tree using two
charts. One chart labels each span with a boolean
value, such that a span is labeled true iff it is a
constituent in the tree. The other chart labels each
span with a label from our labeling scheme (A or
B) or with the value null (to represent that the
span is unlabeled). We show these charts in Fig-
ure 3. Notice that we may want to have more than
one labeling scheme. For instance, in the parse
tree of Figure 1, there are three different types of
labels: word labels, preterminal labels, and nonter-
minal labels. Thus we would use four 5x5 charts
instead of two 3x3 charts to represent that tree.
We will pause here and generalize these con-
cepts. Define a labeling scheme as a set of symbols
including a special symbol null (this will desig-
370
A
B
A B
B
Figure 2: Example labeled tree.
1 2 3
1 true true true
2 - true false
3 - - true
1 2 3
1 A B A
2 - B null
3 - - B
Figure 3: Chart representation of the example tree:
the left chart tells us which spans are tree con-
stituents, and the right chart tells us the labels of
the spans (null means unlabeled).
nate that a given span is unlabeled). For instance,
we can define L1 = {null, A,B} to be a labeling
scheme for the example tree.
Let L = {L1, L2, ...Lm} be a set of labeling
schemes. Define a model variable of L as a sym-
bol of the form Sij or Lkij , for positive integers i,
j, k, such that i ? j and k ? m. Model vari-
ables of the form Sij indicate whether span (i, j)is a tree constituent, hence the domain of Sij is
{true, false}. Such variables correspond to en-
tries in the left chart of Figure 3. Model variables
of the form Lkij indicate which label from scheme
Lk is assigned to span (i, j), hence the domain of
model variable Lkij is Lk. Such variables corre-spond to entries in the right chart of Figure 3. Here
we have only one labeling scheme.
Let VL be the (countably infinite) set of modelvariables of L. Usually we are interested in trees
over a given sentence of finite length n. Let VnLdenote the finite subset of VL that includes pre-cisely the model variables of the form Sij or Lkij ,where j ? n.
Basically then, our model consists of two types
of decisions: (1) whether a span should be labeled,
and (2) if so, what label(s) the span should have.
Let us proceed with our example. To generate the
tree of Figure 2, the first decision we need to make
is how many leaves it will have (or equivalently,
how large our tables will be). We assume that we
have a probability distribution PN over the set ofpositive integers. For our example tree, we draw
the value 3, with probability PN (3).Now that we know our tree will have three
leaves, we can now decide which spans will be
constituents and what labels they will have. In
other words, we assign values to the variables in
V3L. First we need to choose the order in whichwe will make these assignments. For our exam-
ple, we will assign model variables in the follow-
ing order: S11, L111, S22, L122, S33, L133, S12, L112,
S23, L123, S13, L113. A detailed look at this assign-ment process should help clarify the details of the
model.
Assigning S11: The first model variable in ourorder is S11. In other words, we need to decidewhether the span (1, 1) should be a constituent.
We could let this decision be probabilistically de-
termined, but recall that we are trying to gener-
ate a well-formed tree, thus the leaves and the root
should always be considered constituents. To han-
dle situations when we would like to make deter-
ministic variable assignments, we supply an aux-
illiary function A that tells us (given a model vari-
able X and the history of decisions made so far)
whether X should be automatically determined,
and if so, what value it should be assigned. In our
running example, we ask A whether S11 should beautomatically determined, given the previous as-
signments made (so far only the value chosen for
n, which was 3). The so-called auto-assignment
function A responds (since S11 is a leaf span) that
S11 should be automatically assigned the value
true, making span (1, 1) a constituent.
Assigning L111: Next we want to assign a la-bel to the first leaf of our tree. There is no com-
pelling reason to deterministically assign this la-
bel. Therefore, the auto-assignment function A
declines to assign a value to L111, and we pro-ceed to assign its value probabilistically. For this
task, we would like a probability distribution over
the labels of labeling scheme L1 = {null, A,B},
conditioned on the decision history so far. The dif-
ficulty is that it is clearly impractical to learn con-
ditional distributions over every conceivable his-
tory of variable assignments. So first we distill
the important features from an assignment history.
For instance, one such feature (though possibly
not a good one) could be whether an odd or an
even number of nodes have so far been labeled
with an A. Our conditional probability distribu-
tion is conditioned on the values of these features,
instead of the entire assignment history. Consider
specifically model variable L111. We compute itsfeatures (an even number of nodes ? zero ? have
so far been labeled with an A), and then we use
these feature values to access the relevant prob-
371
ability distribution over {null, A,B}. Drawing
from this conditional distribution, we probabilis-
tically assign the value A to variable L111.
Assigning S22, L122, S33, L133: We proceed inthis way to assign values to S22, L122, S33, L133 (the
S-variables deterministically, and the L1-variables
probabilistically).
Assigning S12: Next comes model variable
S12. Here, there is no reason to deterministicallydictate whether span (1, 2) is a constituent or not.
Both should be considered options. Hence we
treat this situation the same as for the L1 variables.
First we extract the relevant features from the as-
signment history. We then use these features to
access the correct probability distribution over the
domain of S12 (namely {true, false}). Drawingfrom this conditional distribution, we probabilis-
tically assign the value true to S12, making span
(1, 2) a constituent in our tree.
Assigning L112: We proceed to probabilisti-cally assign the value B to L112, in the same man-ner as we did with the other L1 model variables.
Assigning S23: Now we must determinewhether span (2, 3) is a constituent. We could
again probabilistically assign a value to S23 as wedid for S12, but this could result in a hierarchi-cal structure in which both spans (1, 2) and (2, 3)
are constituents, which is not a tree. For trees,
we cannot allow two model variables Sij and Sklto both be assigned true if they properly over-
lap, i.e. their spans overlap and one is not a sub-
span of the other. Fortunately we have already es-
tablished auto-assignment function A, and so we
simply need to ensure that it automatically assigns
the value false to model variable Skl if a prop-erly overlapping model variable Sij has previouslybeen assigned the value true.
Assigning L123, S13, L113: In this manner, wecan complete our variable assignments: L123 is au-tomatically determined (since span (2, 3) is not a
constituent, it should not get a label), as is S13 (toensure a rooted tree), while the label of the root is
probabilistically assigned.
We can summarize this generative process as a
general modeling tool. Define a hierarchical la-
beling process (HLP) as a 5-tuple ?L, <,A,F ,P?
where:
? L = {L1, L2, ..., Lm} is a finite set of label-
ing schemes.
? < is a model order, defined as a total ordering
of the model variables VL such that for all
HLPGEN(HLP H = ?L, <,A,F ,P?):
1. Choose a positive integer n from distribution
PN . Let x be the trivial assignment of VL.
2. In the order defined by <, compute step 3 for
each model variable Y of VnL.
3. If A(Y,x, n) = ?true, y? for some y in the
domain of model variable Y , then let x =
x[Y = y]. Otherwise assign a value to Y
from its domain:
(a) If Y = Sij , then let x = x[Sij = sij ],where sij is a value drawn from distri-bution PS(s|FS(x, i, j, n)).
(b) If Y = Lkij , then let x = x[Lkij = lkij ],
where lkij is a value drawn from distribu-
tion Pk(lk|Fk(x, i, j, n)).
4. Return ?n,x?.
Figure 4: Pseudocode for the generative process.
i, j, k: Sij < Lkij (i.e. we decide whethera span is a constituent before attempting to
label it).
? A is an auto-assignment function. Specifi-
cally A takes three arguments: a model vari-
able Y of VL, a partial assignment x of VL,and integer n. The function A maps this 3-
tuple to false if the variable Y should not be
automatically assigned a value based on the
current history, or the pair ?true, y?, where y
is the value in the domain of Y that should be
automatically assigned to Y .
? F = {FS ,F1,F2, ...,Fm} is a set of fea-
ture functions. Specifically, F k (resp., FS)
takes four arguments: a partial assignment
x of VL, and integers i , j , n such that
1 ? i ? j ? n. It maps this 4-tuple to a
full assignment f k (resp., fS) of some finite
set Fk (resp., FS) of feature variables.
? P = {PN , PS , P1, P2, ..., Pm} is a set ofprobability distributions. PN is a marginalprobability distribution over the set of pos-
itive integers, whereas {PS , P1, P2, ..., Pm}are conditional probability distributions.
Specifically, Pk (respectively, PS) is a func-tion that takes as its argument a full assign-
ment fk (resp., fS) of feature set Fk (resp.,
372
A(variable Y , assignment x, int n):
1. If Y = Sij , and there exists a properlyoverlapping model variable Skl such that
x(Skl) = true, then return ?true, false?.
2. If Y = Sii or Y = S1n, then return
?true, true?.
3. If Y = Lkij , and x(Sij) = false, then return
?true, null?.
4. Else return false.
Figure 5: An example auto-assignment function.
FS). It maps this to a probability distribution
over dom(Lk) (resp., {true, false}).
An HLP probabilistically generates an assign-
ment of its model variables using the generative
process shown in Figure 4. Taking an HLP H =
?L, <,A,F ,P? as input, HLPGEN outputs an in-
teger n, and an H-labeling x of length n, defined
as a full assignment of VnL.Given the auto-assignment function in Figure 5,
every H-labeling generated by HLPGEN can be
viewed as a labeled tree using the interpretation:
span (i, j) is a constituent iff Sij = true; span
(i, j) has label lk ? dom(Lk) iff Lkij = lk.
4 Learning
The generative story from the previous section al-
lows us to express the probability of a labeled tree
as P (n,x), where x is an H-labeling of length n.
For model variable X , define V<L (X) as the sub-set of VL appearing before X in model order <.With the help of this terminology, we can decom-
pose P (n,x) into the following product:
P0(n) ?
?
Sij?Y
PS(x(Sij)|fSij)
?
?
Lkij?Y
Pk(x(Lkij)|fkij)
where fSij = FS(x|V<L (Sij), i, j, n) and
fkij = Fk(x|V<L (Lkij), i, j, n) and Y is the sub-set of VnL that was not automatically assigned byHLPGEN.
Usually in parsing, we are interested in comput-
ing the most likely tree given a specific sentence.
In our framework, this generalizes to computing:
argmaxxP (x|n,w), where w is a subassignmentof an H-labeling x of length n. In natural lan-
guage parsing, w could specify the constituency
and word labels of the leaf-level spans. This would
be equivalent to asking: given a sentence, what is
its most likely parse?
Let W = dom(w) and suppose that we choose
a model order < such that for every pair of model
variables W ? W, X ? VL\W, either W < Xor W is always auto-assigned. Then P (x|n,w)
can be expressed as:
?
Sij?Y\W
PS(x(Sij)|fSij)
?
?
Lkij?Y\W
Pk(x(Lkij)|fkij)
Hence the distributions we need to learn
are probability distributions PS(sij|fS) and
Pk(lkij |fk). This is fairly straightforward. Givena data bank consisting of labeled trees (such as
the Penn Treebank), we simply convert each tree
into its H-labeling and use the probabilistically
determined variable assignments to compile our
training instances. In this way, we compile k + 1
sets of training instances that we can use to induce
PS , and the Pk distributions. The choice of whichlearning technique to use is up to the personal
preference of the user. The only requirement
is that it must return a conditional probability
distribution, and not a hard classification. Tech-
niques that allow this include relative frequency,
maximum entropy models, and decision trees.
For our experiments, we used maximum entropy
learning. Specifics are deferred to Section 6.
5 Decoding
For the PCFG parsing model, we can find
argmaxtreeP (tree|sentence) using a cubic-timedynamic programming-based algorithm. By
adopting a more flexible probabilistic model, we
sacrifice polynomial-time guarantees. The central
question driving this paper is whether we can jetti-
son these guarantees and still obtain good perfor-
mance in practice. For the decoding of the prob-
abilistic model of the previous section, we choose
a depth-first branch-and-bound approach, specif-
ically because of two advantages. First, this ap-
proach takes linear space. Second, it is anytime,
373
HLPDECODE(HLP H, int n, assignment w):
1. Initialize stack S with the pair ?x?, 1?, where
x? is the trivial assignment of VL. Let
xbest = x?; let pbest = 0. Until stack S isempty, repeat steps 2 to 4.
2. Pop topmost pair ?x, p? from stack S.
3. If p > pbest and x is an H-labeling of length
n, then: let xbest = x; let pbest = p.
4. If p > pbest and x is not yet a H-labeling oflength n, then:
(a) Let Y be the earliest variable in VnL (ac-cording to model order <) unassigned
by x.
(b) If Y ? dom(w), then push pair ?x[Y =
w(Y )], p? onto stack S.
(c) Else if A(Y,x, n) = ?true, y? for some
value y ? dom(Y ), then push pair
?x[Y = y], p? onto stack S.
(d) Otherwise for every value y ? dom(Y ),
push pair ?x[Y = y], p ?q(y)? onto stack
S in ascending order of the value of
q(y), where:
q(y) =
{PS(y|FS(x, i, j, n)) if Y = Sij
Pk(y|Fk(x, i, j, n)) if Y = Lkij
5. Return xbest.
Figure 6: Pseudocode for the decoder.
i.e. it finds a (typically good) solution early and
improves this solution as the search progresses.
Thus if one does not wish the spend the time to
run the search to completion (and ensure optimal-
ity), one can use this algorithm easily as a heuristic
by halting prematurely and taking the best solution
found thus far.
The search space is simple to define. Given an
HLP H, the search algorithm simply makes as-
signments to the model variables (depth-first) in
the order defined by <.
This search space can clearly grow to be quite
large, however in practice the search speed is
improved drastically by using branch-and-bound
backtracking. Namely, at any choice point in the
search space, we first choose the least cost child
to expand (i.e. we make the most probable assign-
ment). In this way, we quickly obtain a greedy
solution (in linear time). After that point, we can
continue to keep track of the best solution we have
found so far, and if at any point we reach an inter-
nal node of our search tree with partial cost greater
than the total cost of our best solution, we can dis-
card this node and discontinue exploration of that
subtree. This technique can result in a significant
aggregrate savings of computation time, depend-
ing on the nature of the cost function.
Figure 6 shows the pseudocode for the depth-
first branch-and-bound decoder. For an HLP H =
?L, <,A,F ,P?, a positive integer n, and a partial
assignment w of VnL, the call HLPDECODE(H, n,
w) returns the H-labeling x of length n such that
P (x|n,w) is maximized.
6 Experiments
We employed a familiar experimental set-up. For
training, we used sections 2?21 of the WSJ section
of the Penn treebank. As a development set, we
used the first 20 files of section 22, and then saved
section 23 for testing the final model. One uncon-
ventional preprocessing step was taken. Namely,
for the entire treebank, we compressed all unary
chains into a single node, labeled with the label of
the node furthest from the root. We did so in or-
der to simplify our experiments, since the frame-
work outlined in this paper allows only one label
per labeling scheme per span. Thus by avoiding
unary chains, we avoid the need for many label-
ing schemes or more complicated compound la-
bels (labels like ?NP-NN?). Since our goal here
was not to create a parsing tool but rather to ex-
plore the viability of this approach, this seemed a
fair concession. It should be noted that it is indeed
possible to create a fully general parser using our
framework (for instance, by using the above idea
of compound labels for unary chains).
The main difficulty with this compromise is that
it renders the familiar metrics of labeled preci-
sion and labeled recall incomparable with previ-
ous work (i.e. the LP of a set of candidate parses
with respect to the unmodified test set differs from
the LP with respect to the preprocessed test set).
This would be a major problem, were it not for
the existence of other metrics which measure only
the quality of a parser?s recursive decomposition
of a sentence. Fortunately, such metrics do exist,
thus we used cross-bracketing statistics as the ba-
sic measure of quality for our parser. The cross-
bracketing score of a set of candidate parses with
374
word(i+k) = w word(j+k) = w
preterminal(i+k) = p preterminal(j+k) = p
label(i+k) = l label(j+k) = l
category(i+k) = c category(j+k) = c
signature(i,i+k) = s
Figure 7: Basic feature templates used to deter-
mine constituency and labeling of span (i, j). k is
an arbitrary integer.
respect to the unmodified test set is identical to the
cross-bracketing score with respect to the prepro-
cessed test set, hence our preprocessing causes no
comparability problems as viewed by this metric.
For our parsing model, we used an HLP H =
?L, <,A,F ,P? with the following parameters. L
consisted of three labeling schemes: the set Lwd
of word labels, the set Lpt of preterminal labels,
and the set Lnt of nonterminal labels. The or-
der < of the model variables was the unique or-
der such that for all suitable integers i, j, k, l: (1)
Sij < Lwdij < L
pt
ij < Lntij , (2) Lntij < Skl iffspan (i, j) is strictly shorter than span (k, l) or they
have the same length and integer i is less than inte-
ger k. For auto-assignment function A, we essen-
tially used the function in Figure 5, modified so
that it automatically assigned null to model vari-
ables Lwdij and Lptij for i 6= j (i.e. no preterminal orword tagging of internal nodes), and to model vari-
ables Lntii (i.e. no nonterminal tagging of leaves,rendered unnecessary by our preprocessing step).
Rather than incorporate part-of-speech tagging
into the search process, we opted to pretag the sen-
tences of our development and test sets with an
off-the-shelf tagger, namely the Brill tagger (Brill,
1994). Thus the object of our computation was
HLPDECODE(H, n, w), where n was the length
of the sentence, and partial assignment w speci-
fied the word and PT labels of the leaves. Given
this partial assignment, the job of HLPDECODE
was to find the most probable assignment of model
variables Sij and Lntij for 1 ? i < j ? n.
The two probability models, P S and P nt, were
trained in the manner described in Section 4.
Two decisions needed to be made: which fea-
tures to use and which learning technique to em-
ploy. As for the learning technique, we used
maximum entropy models, specifically the imple-
mentation called MegaM provided by Hal Daume
(Daume? III, 2004). For P S , we needed features
? 40 ? 100
CB 0CB CB 0CB
Magerman (1995) 1.26 56.6
Collins (1996) 1.14 59.9
Klein/Manning (2003) 1.10 60.3 1.31 57.2
this paper 1.09 58.2 1.25 55.2
Charniak (1997) 1.00 62.1
Collins (1999) 0.90 67.1
Figure 8: Cross-bracketing results for Section 23
of the Penn Treebank.
that would be relevant to deciding whether a given
span (i, j) should be considered a constituent. The
basic building blocks we used are depicted in Fig-
ure 7. A few words of explanation are in or-
der. By label(k), we mean the highest nonter-
minal label so far assigned that covers word k, or
if such a label does not yet exist, then the preter-
minal label of k (recall that our model order was
bottom-up). By category(k), we mean the cat-
egory of the preterminal label of word k (given
a coarser, hand-made categorization of pretermi-
nal labels that grouped all noun tags into one
category, all verb tags into another, etc.). By
signature(k,m), where k ? m, we mean the
sequence ?label(k), label(k + 1), ..., label(m)?,
from which all consecutive sequences of identi-
cal labels are compressed into a single label. For
instance, ?IN,NP,NP, V P, V P ? would become
?IN,NP, V P ?. Ad-hoc conjunctions of these ba-
sic binary features were used as features for our
probability model P S . In total, approximately
800,000 such conjunctions were used.
For P nt, we needed features that would be rele-
vant to deciding which nonterminal label to give
to a given constituent span. For this somewhat
simpler task, we used a subset of the basic fea-
tures used for P S , shown in bold in Figure 7. Ad-
hoc conjunctions of these boldface binary features
were used as features for our probability model
P nt. In total, approximately 100,000 such con-
junctions were used.
As mentioned earlier, we used cross-bracketing
statistics as our basis of comparision. These re-
sults as shown in Figure 8. CB denotes the av-
erage cross-bracketing, i.e. the overall percent-
age of candidate constituents that properly overlap
with a constituent in the gold parse. 0CB denotes
the percentage of sentences in the test set that ex-
hibit no cross-bracketing. With a simple feature
set, we manage to obtain performance compara-
ble to the unlexicalized PCFG parser of (Klein and
Manning, 2003) on the set of sentences of length
375
40 or less. On the subset of Section 23 consist-
ing of sentences of length 100 or less, our parser
slightly outperforms their results in terms of av-
erage cross-bracketing. Interestingly, our parser
has a lower percentage of sentences exhibiting no
cross bracketing. To reconcile this result with the
superior overall cross-bracketing score, it would
appear that when our parser does make bracketing
errors, the errors tend to be less severe.
The surprise was how quickly the parser per-
formed. Despite its exponential worst-case time
bounds, the search space turned out to be quite
conducive to depth-first branch-and-bound prun-
ing. Using an unoptimized Java implementation
on a 4x Opteron 848 with 16GB of RAM, the
parser required (on average) less than 0.26 sec-
onds per sentence to optimally parse the subset of
Section 23 comprised of sentences of 40 words or
less. It required an average of 0.48 seconds per
sentence to optimally parse the sentences of 100
words or less (an average of less than 3.5 seconds
per sentence for those sentences of length 41-100).
As noted earlier, the parser requires space linear in
the size of the sentence.
7 Discussion
This project began with a question: can we de-
velop a history-based parsing framework that is
simple, general, and effective? We sought to
provide a versatile probabilistic framework that
would be free from the constraints that dynamic
programming places on PCFG-based approaches.
The work presented in this paper gives favorable
evidence that more flexible (and worst-case in-
tractable) probabilistic approaches can indeed per-
form well in practice, both in terms of running
time and parsing quality.
We can extend this research in multiple direc-
tions. First, the set of features we selected were
chosen with simplicity in mind, to see how well a
simple and unadorned set of features would work,
given our probabilistic model. A next step would
be a more carefully considered feature set. For in-
stance, although lexical information was used, it
was employed in only a most basic sense. There
was no attempt to use head information, which has
been so successful in PCFG parsing methods.
Another parameter to experiment with is the
model order, i.e. the order in which the model vari-
ables are assigned. In this work, we explored only
one specific order (the left-to-right, leaves-to-head
assignment) but in principle there are many other
feasible orders. For instance, one could try a top-
down approach, or a bottom-up approach in which
internal nodes are assigned immediately after all
of their descendants? values have been determined.
Throughout this paper, we strove to present the
model in a very general manner. There is no rea-
son why this framework cannot be tried in other
application areas that rely on dynamic program-
ming techniques to perform hierarchical labeling,
such as phrase-based machine translation. Apply-
ing this framework to such application areas, as
well as developing a general-purpose parser based
on HLPs, are the subject of our continuing work.
References
Ezra Black, Fred Jelinek, John Lafferty, David M.Magerman, Robert Mercer, and Salim Roukos.
1993. Towards history-based grammars: usingricher models for probabilistic parsing. In Proc.
ACL.
Eric Brill. 1994. Some advances in rule-based part ofspeech tagging. In Proc. AAAI.
Eugene Charniak. 1997. Statistical parsing with acontext-free grammar and word statistics. In Proc.
AAAI.
Eugene Charniak. 2000. A maximum entropy-inspiredparser. In Proc. NAACL.
Eugene Charniak. 2001. Immediate-head parsing forlanguage models. In Proc. ACL.
Michael Collins. 1996. A new statistical parser based
on bigram lexical dependencies. In Proc. ACL.
Michael Collins. 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, Univer-sity of Pennsylvania.
Hal Daume? III. 2004. Notes on CG and LM-BFGS op-
timization of logistic regression. Paper available athttp://www.isi.edu/ hdaume/docs/daume04cg-bfgs.ps, implementation available at
http://www.isi.edu/ hdaume/megam/, August.
Mark Johnson. 1998. Pcfg models of linguistic
tree representations. Computational Linguistics,24:613?632.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proc. ACL.
David M. Magerman. 1995. Statistical decision-tree
models for parsing. In Proc. ACL.
Adwait Ratnaparkhi. 1997. A linear observed time sta-tistical parser based on maximum entropy models.
In Proc. EMNLP.
376
A Framework for Incorporating Alignment Information in Parsing
Mark Hopkins
Dept. of Computational Linguistics
Saarland University
Saarbru?cken, Germany
mhopkins@coli.uni-sb.de
Jonas Kuhn
Dept. of Computational Linguistics
Saarland University
Saarbru?cken, Germany
jonask@coli.uni-sb.de
Abstract
The standard PCFG approach to parsing is
quite successful on certain domains, but is
relatively inflexible in the type of feature
information we can include in its prob-
abilistic model. In this work, we dis-
cuss preliminary work in developing a new
probabilistic parsing model that allows us
to easily incorporate many different types
of features, including crosslingual infor-
mation. We show how this model can
be used to build a successful parser for a
small handmade gold-standard corpus of
188 sentences (in 3 languages) from the
Europarl corpus.
1 Introduction
Much of the current research into probabilis-
tic parsing is founded on probabilistic context-
free grammars (PCFGs) (Collins, 1999; Charniak,
2000; Charniak, 2001). For instance, consider
the parse tree in Figure 1. One way to decom-
pose this parse tree is to view it as a sequence
of applications of CFG rules. For this particular
tree, we could view it as the application of rule
?NP? NP PP,? followed by rule ?NP? DT NN,?
followed by rule ?DT? that,? and so forth. Hence
instead of analyzing P (tree), we deal with the
more modular:
P(NP ? NP PP, NP ? DT NN,
DT? that, NN?money, PP? IN NP,
IN ? in, NP ? DT NN, DT ? the,
NN? market)
Obviously this joint distribution is just as diffi-
cult to assess and compute with as P (tree). How-
ever there exist cubic time algorithms to find the
most likely parse if we assume that all CFG rule
applications are marginally independent of one an-
other. In other words, we need to assume that the
above expression is equivalent to the following:
P(NP ? NP PP) ? P(NP ? DT NN) ?
P(DT ? that) ? P(NN ? money) ?
P(PP ? IN NP) ? P(IN ? in) ?
P(NP ? DT NN) ? P(DT ? the) ?
P(NN? market)
It is straightforward to assess the probability of
the factors of this expression from a corpus us-
ing relative frequency. Then using these learned
probabilities, we can find the most likely parse of
a given sentence using the aforementioned cubic
algorithms.
The problem, of course, with this simplifica-
tion is that although it is computationally attrac-
tive, it is usually too strong of an independence
assumption. To mitigate this loss of context, with-
out sacrificing algorithmic tractability, typically
researchers annotate the nodes of the parse tree
with contextual information. For instance, it has
been found to be useful to annotate nodes with
their parent labels (Johnson, 1998), as shown in
Figure 2. In this case, we would be learning prob-
abilities like: P(PP-NP? IN-PP NP-PP).
The choice of which annotations to use is
one of the main features that distinguish parsers
based on this approach. Generally, this approach
has proven quite effective in producing English
phrase-structure grammar parsers that perform
well on the Penn Treebank.
One drawback of this approach is that it is
somewhat inflexible. Because we are adding prob-
abilistic context by changing the data itself, we
make our data increasingly sparse as we add fea-
tures. Thus we are constrained from adding too
9
NP
NP
DT
that
NN
money
PP
IN
in
NP
DT
the
NN
market
Figure 1: Example parse tree.
NP-TOP
NP-NP
DT-NP
that
NN-NP
money
PP-NP
IN-PP
in
NP-PP
DT-NP
the
NN-NP
market
Figure 2: Example parse tree with parent annota-
tions.
many features, because at some point we will not
have enough data to sustain them. Hence in this
approach, feature selection is not merely a matter
of including good features. Rather, we must strike
a delicate balance between how much context we
want to include versus how much we dare to par-
tition our data set.
This poses a problem when we have spent time
and energy to find a good set of features that work
well for a given parsing task on a given domain.
For a different parsing task or domain, our parser
may work poorly out-of-the-box, and it is no triv-
ial matter to evaluate how we might adapt our fea-
ture set for this new task. Furthermore, if we gain
access to a new source of feature information, then
it is unclear how to incorporate such information
into such a parser.
Namely, in this paper, we are interested in see-
ing how the cross-lingual information contained
by sentence alignments can help the performance
of a parser. We have a small gold-standard cor-
pus of shallow-parsed parallel sentences (in En-
glish, French, and German) from the Europarl cor-
pus. Because of the difficulty of testing new fea-
tures using PCFG-based parsers, we propose a
new probabilistic parsing framework that allows
us to flexibly add features. The closest relative
1 2 3 4 5
1 true true false false true
2 - true false false false
3 - - true false true
4 - - - true true
5 - - - - true
Figure 3: Span chart for example parse tree. Chart
entry (i, j) = true iff span (i, j) is a constituent
in the tree.
of our framework is the maximum-entropy parser
of Ratnaparkhi(Ratnaparkhi, 1997). Both frame-
works are bottom-up, but while Ratnaparkhi?s
views parse trees as the sequence of applications
of four different types of tree construction rules,
our framework strives to be somewhat simpler and
more general.
2 The Probability Model
The example parse tree in Figure 1 can also be de-
composed in the following manner. First, we can
represent the unlabeled tree with a boolean-valued
chart (which we will call the span chart) that as-
signs the value of true to a span if it is a con-
stituent in the tree, and false otherwise. The span
chart for Figure 1 is shown in Figure 3.
To represent the labels, we simply add similar
charts for each labeling scheme present in the tree.
For a parse tree, there are typically three types
of labels: words, preterminal tags, and nontermi-
nals. Thus we need three labeling charts. Labeling
charts for our example parse tree are depicted in
Figure 4. Note that for words and preterminals, it
is not really necessary to have a two-dimensional
chart, but we do so here to motivate the general
model.
The general model is as follows. Define a la-
beling scheme as a set of symbols including a
special symbol null (this will designate that a
given span is unlabeled). For instance, we might
define LNT = {null, NP, PP, IN, DT} to be
a labeling scheme for non-terminals. Let L =
{L1, L2, ...Lm} be a set of labeling schemes. De-
fine a model variable of L as a symbol of the form
Sij or Lkij , for positive integers i, j, k, such that
j ? i and k ? m. The domain of model vari-
able Sij is {true, false} (these variables indicate
whether a given span is a tree constituent). The do-
main of model variable Lkij is Lk (these variables
indicate which label from Lk is assigned to span
10
1 2 3 4 5
1 that null null null null
2 - money null null null
3 - - in null null
4 - - - the null
5 - - - - market
1 2 3 4 5
1 DT null null null null
2 - NN null null null
3 - - IN null null
4 - - - DT null
5 - - - - NN
1 2 3 4 5
1 null NP null null NP
2 - null null null null
3 - - null null PP
4 - - - null NP
5 - - - - null
Figure 4: Labeling charts for example parse tree:
the top chart is for word labels, the middle chart is
for preterminal tag labels, and the bottom chart is
for nonterminal labels. null denotes an unlabeled
span.
i, j). Define a model order of L as a total order-
ing ? of the model variables of L such that for all
i, j, k: ?(Sij) < ?(Lkij) (i.e. we decide whether a
span is a constituent before attempting to label it).
Let ?n denote the finite subset of ? that includes
precisely the model variables of the form Sij or
Lkij , where j ? n.
Given a set L of labeling schemes and a model
order ? of L, a preliminary generative story might
look like the following:
1. Choose a positive integer n.
2. In the order defined by ?n, assign a value
to every model variable of ?n from its do-
main, conditioned on any previous assign-
ments made.
Thus some model order ? for our example
might instruct us to first choose whether span (4,
5) is a constituent, for which we might say ?true,?
then instruct us to choose a label for that con-
stituent, for which we might say ?NP,? and so
forth.
There are a couple of problems with this genera-
tive story. One problem is that it allows us to make
structural decisions that do not result in a well-
formed tree. For instance, we should not be per-
mitted to assign the value true to both variable S
13
and S
24
. Generally, we cannot allow two model
variables Sij and Skl to both be assigned true if
they properly overlap, i.e. their spans overlap and
one is not a subspan of the other. We should also
ensure that the leaves and the root are considered
constituents. Another problem is that it allows us
to make labeling decisions that do not correspond
with our chosen structure. It should not be possi-
ble to label a span which is not a constituent.
With this in mind, we revise our generative
story.
1. Choose a positive integer n from distribution
P
0
.
2. In the order defined by ?n, process model
variable x of ?n:
(a) If x = Sij , then:
i. Automatically assign the value
false if there exists a properly
overlapping model variable Skl such
that Skl has already been assigned
the value true.
ii. Automatically assign the value true
if i = j or if i = 1 and j = n.
iii. Otherwise assign a value sij to Sij
from its domain, drawn from some
probability distribution PS condi-
tioned on all previous variable as-
signments.
(b) If x = Lkij , then:
i. Automatically assign the value null
to Lkij if Sij was assigned the value
false (note that this is well-defined
because of way we defined model
order).
ii. Otherwise assign a value lkij to L
k
ij
from its domain, drawn from some
probability distribution Pk condi-
tioned on all previous variable as-
signments.
Defining ?<n (x) = {y ? ?n|?(y) < ?(x)}
for x ? ?n, we can decompose P (tree) into the
following expression:
11
P
0
(n) ?
?
S
ij
??
n
PS(sij |n,?
<
n (Sij))
?
?
Lk
ij
??
n
Pk(l
k
ij |n,?
<
n (L
k
ij))
where PS and Pk obey the constraints given in
the generative story above (e.g. PS(Sii = true) =
1, etc.)
Obviously it is impractical to learn conditional
distributions over every conceivable history, so in-
stead we choose a small set F of feature variables,
and provide a set of functions Fn that map every
partial history of ?n to some feature vector f ? F
(later we will see examples of such feature func-
tions). Then we make the assumption that:
PS(sij |n,?
<
n (Sij) = PS(sij|f)
where f = Fn(?<n (Sij)) and that
Pk(l
k
ij |n,?
<
n (Sij) = Pk(l
k
ij |f)
where f = Fn(?<n (L
k
ij)).
In this way, our learning task is simplified to
learn functions P
0
(n), PS(sij |f), and Pk(lkij |f).
Given a corpus of labeled trees, it is straightfor-
ward to extract the training instances for these dis-
tributions and then use these instances to learn dis-
tributions using one?s preferred learning method
(e.g., maximum entropy models or decision trees).
For this paper, we are interested in parse trees
which have three labeling schemes. Let L =
{Lword, LPT , LNT }, where Lword is a labeling
scheme for words, LPT is a labeling scheme for
preterminals, and LNT is a labeling scheme for
nonterminals. We will define model order ? such
that:
1. ?(Sij) < ?(Lwordij ) < ?(L
PT
ij ) < ?(L
NT
ij ).
2. ?(LNTij ) < ?(Skl) iff j?i < l?k or (j?i =
l ? k and i < k).
In this work, we are not as much interested in
learning a marginal distribution over parse trees,
but rather a conditional distribution for parse trees,
given a tagged sentence (from which n is also
known). We will assume that Pword is condition-
ally independent of all the other model variables,
given n and the Lwordij variables. We will also as-
sume that Ppt is conditionally independent of the
other model variables, given n, the Lwordij vari-
ables, and the Lptij variables. These assumptions
allow us to express P (tree|n, Lwordij , L
pt
ij ) as the
following:
?
S
ij
??
n
PS(sij |fS) ?
?
Lnt
ij
??
n
Pnt(l
nt
ij |fnt)
where fS = Fn(?<n (Sij)) and fnt =
Fn(?
<
n (L
nt
ij )). Hence our learning task in this pa-
per will be to learn the probability distributions
PS(sij|fS) and Pnt(lntij |fnt), for some choice of
feature functions Fn.
3 Decoding
For the PCFG parsing model, we can find
argmaxtreeP (tree|sentence) using a cubic-time
dynamic programming-based algorithm. By
adopting a more flexible probabilistic model, we
sacrifice polynomial-time guarantees. Neverthe-
less, we can still devise search algorithms that
work efficiently in practice. For the decoding of
the probabilistic model of the previous section, we
choose a depth-first branch-and-bound approach,
specifically because of two advantages. First, this
approach is linear space. Second, it is anytime, i.e.
it finds a (typically good) solution early and im-
proves this solution as the search progresses. Thus
if one does not wish the spend the time to run the
search to completion (and ensure optimality), one
can use this algorithm easily as a heuristic.
The search space is simple to define. Given a
set L of labeling schemes and a model order ? of
L, the search algorithm simply makes assignments
to the model variables (depth-first) in the order de-
fined by ?.
This search space can clearly grow to be quite
large, however in practice the search speed is
improved drastically by using branch-and-bound
backtracking. Namely, at any choice point in the
search space, we first choose the least cost child to
expand. In this way, we quickly obtain a greedy
solution. After that point, we can continue to keep
track of the best solution we have found so far,
and if at any point we reach an internal node of
our search tree with partial cost greater than the
total cost of our best solution, we can discard this
node and discontinue exploration of that subtree.
This technique can result in a significant aggre-
grate savings of computation time, depending on
12
EN: [
1
[
2
On behalf of the European People ?s Party , ] [
3
I] call [
5
for a vote [
6
in favour of that motion ] ] ]
FR: [
1
[
2
Au nom du Parti populaire europe?en ,] [
3
je] demande [
5
l? adoption [
6
de cette re?solution] ] ]
DE: [
1
[
2
Im Namen der Europa?ischen Volkspartei ] rufe [
3
ich] [
4
Sie] auf , [
5
[
6
diesem Entschlie?ungsantrag] zuzustimmen
] ]
ES: [
1
[
2
En nombre del Grupo del Partido Popular Europeo ,] solicito [
5
la aprobacio?n [
6
de la resolucio?n] ] ]
Figure 5: Annotated sentence tuple
the nature of the cost function. For our limited
parsing domain, it appears to perform quite well,
taking fractions of a second to parse each sentence
(which are short, with a maximum of 20 words per
sentence).
4 Experiments
Our parsing domain is based on a ?lean? phrase
correspondence representation for multitexts from
parallel corpora (i.e., tuples of sentences that are
translations of each other). We defined an anno-
tation scheme that focuses on translational corre-
spondence of phrasal units that have a distinct,
language-independent semantic status. It is a hy-
pothesis of our longer-term project that such a se-
mantically motivated, relatively coarse phrase cor-
respondence relation is most suitable for weakly
supervised approaches to parsing of large amounts
of parallel corpus data. Based on this lean phrase
structure format, we intend to explore an alter-
native to the annotation projection approach to
cross-linguistic bootstrapping of parsers by (Hwa
et al, 2005). They depart from a standard treebank
parser for English, ?projecting? its analyses to an-
other language using word alignments over a par-
allel corpus. Our planned bootstrapping approach
will not start out with a given parser for English (or
any other language), but use a small set of manu-
ally annotated seed data following the lean phrase
correspondence scheme, and then bootstrap con-
sensus representations on large amounts of unan-
notated multitext data. At the present stage, we
only present experiments for training an initial
system on a set of seed data.
The annotation scheme underlying in the gold
standard annotation consists of (A) a bracketing
for each language and (B) a correspondence rela-
tion of the constituents across languages. Neither
the constituents nor the embedding or correspon-
dent relations were labelled.
The guiding principle for bracketing (A) is very
simple: all and only the units that clearly play
the role of a semantic argument or modifier in a
larger unit are bracketed. This means that function
words, light verbs, ?bleeched? PPs like in spite
of etc. are included with the content-bearing el-
ements. This leads to a relatively flat bracketing
structure. Referring or quantified expressions that
may include adjectives and possessive NPs or PPs
are also bracketed as single constituents (e.g., [ the
president of France ]), unless the semantic rela-
tions reflected by the internal embedding are part
of the predication of the sentence. A few more
specific annotation rules were specified for cases
like coordination and discontinuous constituents.
The correspondence relation (B) is guided by
semantic correspondence of the bracketed units;
the mapping need not preserve the tree structure.
Neither does a constituent need to have a corre-
spondent in all (or any) of the other languages
(since the content of this constituent may be im-
plicit in other languages, or subsumed by the con-
tent of another constituent). ?Semantic correspon-
dence? is not restricted to truth-conditional equiv-
alence, but is generalized to situations where two
units just serve the same rhetorical function in the
original text and the translation.
Figure 5 is an annotation example. Note that
index 4 (the audience addressed by the speaker)
is realized overtly only in German (Sie ?you?); in
Spanish, index 3 is realized only in the verbal in-
flection (which is not annotated). A more detailed
discussion of the annotation scheme is presented
in (Kuhn and Jellinghaus, to appear).
For the current parsing experiments, only the
bracketing within each of three languages (En-
glish, French, German) is used; the cross-
linguistic phrase correspondences are ignored (al-
though we intend to include them in future ex-
periments). We automatically tagged the train-
ing and test data in English, French, and German
with Schmid?s decision-tree part-of-speech tagger
(Schmid, 1994).
The training data were taken from the sentence-
aligned Europarl corpus and consisted of 188 sen-
tences for each of the three languages, with max-
13
Feature Notation Description
p(language) the preterminal tag of word x ? 1 (null if does not exist)
f(language) the preterminal tag of word x
l(language) the preterminal tag of word y
n(language) the preterminal tag of word y ? 1 (null if does not exist)
lng the length of the span (i.e. y ? x + 1)
Figure 6: Features for span (x, y). E = English, F = French, G = German
English Crosslingual Rec. Prec. F- No
features features score cross
p(E), f(E), l(E) none 40.3 63.6 49.4 (?3.9%) 57.1
p(F), f(F), l(F) 43.1 67.6 52.6 (?4.0%) 61.2
p(G), f(G), l(G) 45.9 66.8 54.4 (?4.0%) 69.4
p(F), f(F), l(F), 44.5 65.5 53.0 (?3.9%) 65.3
p(G), f(G), l(G)
p(E), f(E), l(E), n(E) none 57.2 68.6 62.4 (?4.0%) 65.3
p(F), f(F), l(F), n(F) 56.6 71.9 63.3 (?4.0%) 75.5
p(G), f(G), l(G), n(G) 57.9 67.7 62.5 (?3.9%) 67.3
p(F), f(F), l(F), n(F), 57.9 72.1 64.2 (?4.0%) 77.6
p(G), f(G), l(G), n(G)
p(E), f(E), l(E), n(E), lng none 64.8 71.2 67.9 (?4.0%) 79.6
p(F), f(F), l(F), n(F), lng 62.1 74.4 67.7 (?4.0%) 83.7
p(G), f(G), l(G), n(G), lng 61.4 78.8 69.0 (?4.1%) 83.7
p(F), f(F), l(F), n(F), 63.1 76.9 69.3 (?4.1%) 81.6
p(G), f(G), l(G), n(G), lng
BIKEL 57.9 60.2 59.1 (?3.8%) 57.1
Figure 7: Parsing results for various feature sets, and the Bikel baseline. The F-scores are annotated with
95% confidence intervals.
imal length of 21 words in English (French: 38;
German: 24) and an average length of 14.0 words
in English (French 16.8; German 13.6). The test
data were 50 sentences for each language, picked
arbitrarily with the same length restrictions. The
training and test data were manually aligned fol-
lowing the guidelines.1
For the word alignments used as learning fea-
tures, we used GIZA++, relying on the default pa-
rameters. We trained the alignments on the full
Europarl corpus for both directions of each lan-
guage pair.
As a baseline system we trained Bikel?s reim-
plementation (Bikel, 2004) of Collins? parser
(Collins, 1999) on the gold standard (En-
1A subset of 39 sentences was annotated by two people
independently, leading to an F-Score in bracketing agreement
between 84 and 90 for the three languages. Since finding an
annotation scheme that works well in the bootstrapping set-
up is an issue on our research agenda, we postpone a more
detailed analysis of the annotation process until it becomes
clear that a particular scheme is indeed useful.
glish) training data, applying a simple additional
smoothing procedure for the modifier events in or-
der to counteract some obvious data sparseness is-
sues.2
Since we were attempting to learn unlabeled
trees, in this experiment we only needed to learn
the probabilistic model of Section 3 with no la-
beling schemes. Hence we need only to learn the
probability distribution:
PS(sij|fS)
In other words, we need to learn the probabil-
ity that a given span is a tree constituent, given
some set of features of the words and preterminal
tags of the sentences, as well as the previous span
decisions we have made. The main decision that
2For the nonterminal labels, we defined the left-most lex-
ical daughter in each local subtree of depth 1 to project its
part-of-speech category to the phrase level and introduced
a special nonterminal label for the rare case of nonterminal
nodes dominating no preterminal node.
14
remains, then, is which feature set to use. The fea-
tures we employ are very simple. Namely, for span
(i, j) we consider the preterminal tags of words
i ? 1, i, j, and j + 1, as well as the French and
German preterminal tags of the words to which
these English words align. Finally, we also use
the length of the span as a feature. The features
considered are summarized in Figure 6.
To learn the conditional probability distributu-
tions, we choose to use maximum entropy mod-
els because of their popularity and the availabil-
ity of software packages. Specifically, we use
the MEGAM package (Daume? III, 2004) from
USC/ISI.
We did experiments for a number of different
feature sets, with and without alignment features.
The results (precision, recall, F-score, and the per-
centage of sentences with no cross-bracketing) are
summarized in Figure 7. Note that with a very
simple set of features (the previous, first, last, and
next preterminal tags of the sequence), our parser
performs on par with the Bikel baseline. Adding
the length of the sequence as a feature increases
the quality of the parser to a statistically signif-
icant difference over the baseline. The crosslin-
gual information provided (which is admittedly
naive) does not provide a statistically significant
improvement over the vanilla set of features. The
conclusion to be drawn is not that crosslingual in-
formation does not help (such a conclusion should
not be drawn from the meager set of crosslingual
features we have used here for demonstration pur-
poses). Rather, the take-away point is that such
information can be easily incorporated using this
framework.
5 Discussion
One of the primary concerns about this framework
is speed, since the decoding algorithm for our
probabilistic model is not polynomial-time like the
decoding algorithms for PCFG parsing. Neverthe-
less, in our experiments with shallow parsed 20-
word sentences, time was not a factor. Further-
more, in our ongoing research applying this prob-
abilistic framework to the task of Penn Treebank-
style parsing, this approach appears to also be vi-
able for the 40-word sentences of Sections 22 and
23 of theWSJ treebank. A strong mitigating factor
of the theoretical intractibility is the fact that we
have an anytime decoding algorithm, hence even
in cases when we cannot run the algorithm to com-
pletion (for a guaranteed optimal solution), the al-
gorithm always returns some solution, the quality
of which increases over time. Hence we can tell
the algorithm how much time it has to compute,
and it will return the best solution it can compute
in that time frame.
This work suggests that one can get a good qual-
ity parser for a new parsing domain with relatively
little effort (the features we chose are extremely
simple and certainly could be improved on). The
cross-lingual information that we used (namely,
the foreign preterminal tags of the words to which
our span was aligned by GIZA) did not give a sig-
nificant improvement to our parser. However the
goal of this work was not to make definitive state-
ments about the value of crosslingual features in
parsing, but rather to show a framework in which
such crosslingual information could be easily in-
corporated and exploited. We believe we have pro-
vided the beginnings of one in this work, and work
continues on finding more complex features that
will improve performance well beyond the base-
line.
Acknowledgement
The work reported in this paper was supported by
theDeutsche Forschungsgemeinschaft (DFG; Ger-
man Research Foundation) in the Emmy Noether
project PTOLEMAIOS on grammar learning from
parallel corpora.
References
Daniel M. Bikel. 2004. Intricacies of collins? parsing
model. Computational Linguistics, 30(4):479?511.
Eugene Charniak. 2000. A maximum entropy-inspired
parser. In NAACL.
Eugene Charniak. 2001. Immediate-head parsing for
language models. In ACL.
Michael Collins. 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Hal Daume? III. 2004. Notes on CG and LM-BFGS op-
timization of logistic regression. Paper available at
http://www.isi.edu/ hdaume/docs/daume04cg-
bfgs.ps, implementation available at
http://www.isi.edu/ hdaume/megam/, August.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering, 11(3):311?325.
15
Mark Johnson. 1998. PCFG models of linguistic tree
representation. Computational Linguistics, 24:613?
632.
Jonas Kuhn and Michael Jellinghaus. to appear. Mul-
tilingual parallel treebanking: a lean and flexible ap-
proach. In Proceedings of the Fifth International
Conference on Language Resources and Evaluation,
Genoa, Italy.
Adwait Ratnaparkhi. 1997. A linear observed time sta-
tistical parser based on maximum entropy models.
In EMNLP.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In International Con-
ference on New Methods in Language Processing.
16
What?s in a translation rule?
Michel Galley
Dept. of Computer Science
Columbia University
New York, NY 10027
galley@cs.columbia.edu
Mark Hopkins
Dept. of Computer Science
University of California
Los Angeles, CA 90024
mhopkins@cs.ucla.edu
Kevin Knight and Daniel Marcu
Information Sciences Institute
University of Southern California
Marina Del Rey, CA 90292
{knight,marcu}@isi.edu
Abstract
We propose a theory that gives formal seman-
tics to word-level alignments defined over par-
allel corpora. We use our theory to introduce a
linear algorithm that can be used to derive from
word-aligned, parallel corpora the minimal set
of syntactically motivated transformation rules
that explain human translation data.
1 Introduction
In a very interesting study of syntax in statistical machine
translation, Fox (2002) looks at how well proposed trans-
lation models fit actual translation data. One such model
embodies a restricted, linguistically-motivated notion of
word re-ordering. Given an English parse tree, children
at any node may be reordered prior to translation. Nodes
are processed independently. Previous to Fox (2002), it
had been observed that this model would prohibit certain
re-orderings in certain language pairs (such as subject-
VP(verb-object) into verb-subject-object), but Fox car-
ried out the first careful empirical study, showing that
many other common translation patterns fall outside the
scope of the child-reordering model. This is true even
for languages as similar as English and French. For
example, English adverbs tend to move outside the lo-
cal parent/children in environment. The English word
?not? translates to the discontiguous pair ?ne ... pas.?
English parsing errors also cause trouble, as a normally
well-behaved re-ordering environment can be disrupted
by wrong phrase attachment. For other language pairs,
the divergence is expected to be greater.
In the face of these problems, we may choose among
several alternatives. The first is to abandon syntax in
statistical machine translation, on the grounds that syn-
tactic models are a poor fit for the data. On this view,
adding syntax yields no improvement over robust phrase-
substitution models, and the only question is how much
does syntax hurt performance. Along this line, (Koehn
et al, 2003) present convincing evidence that restricting
phrasal translation to syntactic constituents yields poor
translation performance ? the ability to translate non-
constituent phrases (such as ?there are?, ?note that?, and
?according to?) turns out to be critical and pervasive.
Another direction is to abandon conventional English
syntax and move to more robust grammars that adapt to
the parallel training corpus. One approach here is that of
Wu (1997), in which word-movement is modeled by rota-
tions at unlabeled, binary-branching nodes. At each sen-
tence pair, the parse adapts to explain the translation pat-
tern. If the same unambiguous English sentence were to
appear twice in the corpus, with different Chinese trans-
lations, then it could have different learned parses.
A third direction is to maintain English syntax and
investigate alternate transformation models. After all,
many conventional translation systems are indeed based
on syntactic transformations far more expressive than
what has been proposed in syntax-based statistical MT.
We take this approach in our paper. Of course, the broad
statistical MT program is aimed at a wider goal than
the conventional rule-based program ? it seeks to under-
stand and explain human translation data, and automati-
cally learn from it. For this reason, we think it is impor-
tant to learn from the model/data explainability studies of
Fox (2002) and to extend her results. In addition to being
motivated by rule-based systems, we also see advantages
to English syntax within the statistical framework, such
as marrying syntax-based translation models with syntax-
based language models (Charniak et al, 2003) and other
potential benefits described by Eisner (2003).
Our basic idea is to create transformation rules that
condition on larger fragments of tree structure. It is
certainly possible to build such rules by hand, and we
have done this to formally explain a number of human-
translation examples. But our main interest is in collect-
ing a large set of such rules automatically through corpus
analysis. The search for these rules is driven exactly by
the problems raised by Fox (2002) ? cases of crossing
and divergence motivate the algorithms to come up with
better explanations of the data and better rules. Section
2 of this paper describes algorithms for the acquisition
of complex rules for a transformation model. Section 3
gives empirical results on the explanatory power of the
acquired rules versus previous models. Section 4 presents
examples of learned rules and shows the various types of
transformations (lexical and nonlexical, contiguous and
noncontiguous, simple and complex) that the algorithms
are forced (by the data) to invent. Section 5 concludes.
Due to space constraints, all proofs are omitted.
2 Rule Acquisition
Suppose that we have a French sentence, its translation
into English, and a parse tree over the English translation,
as shown in Figure 1. Generally one defines an alignment
as a relation between the words in the French sentence
and the words in the English sentence. Given such an
alignment however, what kinds of rules are we entitled
to learn from this instance? How do we know when it is
valid to extract a particular rule, especially in the pres-
ence of numerous crossings in the alignment? In this sec-
tion, we give principled answers to these questions, by
constructing a theory that gives formal semantics to word
alignments.
2.1 A Theory of Word Alignments
We are going to define a generative process through
which a string from a source alphabet is mapped to a
rooted tree whose nodes are labeled from a target alha-
bet. Henceforth we will refer to symbols from our source
alphabet as source symbols and symbols from our target
alphabet as target symbols. We define a symbol tree over
an alphabet ? as a rooted, directed tree, the nodes of
which are each labeled with a symbol of ?.
We want to capture the process by which a symbol tree
over the target language is derived from a string of source
symbols. Let us refer to the symbol tree that we want to
derive as the target tree. Any subtree of this tree will be
called a target subtree. Furthermore, we define a deriva-
tion string as an ordered sequence of elements, each of
which is either a source symbol or a target subtree.
Now we are ready to define the derivation process.
Given a derivation string S, a derivation step replaces
a substring S? of S with a target subtree T that has the
following properties:
1. Any target subtree in S ? is a subtree of T .
2. Any target subtree in S but not in S ? does not share
nodes with T .
S
NP VP
PRP RBAUX VB
he notdoes go
il vane pas
Figure 1: A French sentence aligned with an English
parse tree.
il ne va pas
ne va pas
he
PRP
NP
ne pas
he
PRP
NP
S
NP VP
PRP RBAUX VB
he notdoes go
VB
go
il ne va pas
ne va pasRB
not
ne heRB
not
S
NP VP
PRP RBAUX VB
he notdoes go
il ne va pas
S
NP VP
PRP RBAUX VB
he notdoes go
NP VP
PRP RBAUX VB
he notdoes go
Figure 2: Three alternative derivations from a source sen-
tence to a target tree.
Moreover, a derivation from a string S of source sym-
bols to the target tree T is a sequence of derivation steps
that produces T from S.
Moving away from the abstract for a moment, let us
revisit the example from Figure 1. Figure 2 shows three
derivations of the target tree from the source string ?il
ne va pas?, which are all consistent with our defini-
tions. However, it is apparent that one of these deriva-
tions seems much more ?wrong? than the other. Specif-
ically, in the second derivation, ?pas? is replaced by the
English word ?he,? which makes no sense. Given the vast
space of possible derivations (according to the definition
above), how do we distinguish between good ones and
bad ones? Here is where the notion of an alignment be-
comes useful.
Let S be a string of source symbols and let T be a target
tree. First observe the following facts about derivations
from S to T (these follow directly from the definitions):
1. Each element of S is replaced at exactly one step of
the derivation.
SNP VP
PRP RBAUX VB
he notdoes go
il vane pas
S
NP VP
PRP RBAUX VB
he notdoes go
il vane pas
S
NP VP
PRP RBAUX VB
he notdoes go
il vane pas
Figure 3: The alignments induced by the derivations in
Figure 2
2. Each node of T is created at exactly one step of the
derivation.
Thus for each element s of S, we can define
replaced(s, D) to be the step of the derivation D during
which s is replaced. For instance, in the leftmost deriva-
tion of Figure 2, ?va? is replaced by the second step of the
derivation, thus replaced(va, D) = 2. Similarly, for each
node t of T , we can define created(t, D) to be the step
of derivation D during which t is created. For instance,
in the same derivation, the nodes labeled by ?AUX? and
?VP? are created during the third step of the derivation,
thus created(AUX, D) = 3 and created(VP, D) = 3.
Given a string S of source symbols and a target tree
T , an alignment A with respect to S and T is a relation
between the leaves of T and the elements of S. Choose
some derivation D from S to T . The alignment A in-
duced by D is created as follows: an element s of S is
aligned with a leaf node t of T iff replaced(s, D) =
created(t, D). In other words, a source word is aligned
with a target word if the target word is created during the
same step in which the source word is replaced. Figure 3
shows the alignments induced by the derivations of Fig-
ure 2.
Now, say that we have a source string, a target tree,
and an alignment A. A key observation is that the set
of ?good? derivations according to A is precisely the set
of derivations that induce alignments A? such that A is
a subalignment of A?. By subalignment, we mean that
A ? A? (recall that alignments are simple mathematical
relations). In other words, A is a subalignment of A? if A
aligns two elements only if A? also aligns them.
We can see this intuitively by examining Figures 2 and
3. Notice that the two derivations that seem ?right? (the
first and the third) are superalignments of the alignment
given in Figure 1, while the derivation that is clearly
wrong is not. Hence we now have a formal definition
of the derivations that we are interested in. We say that
a derivation is admitted by an alignment A if it induces a
superalignment of A. The set of derivations from source
string S to target tree T that are admitted by alignment A
can be denoted ?A(S, T ). Given this, we are ready to ob-
tain a formal characterization of the set of rules that can
ne pas
he
PRP
NP
VB
go
NP VP
PRP RBAUX VB
he notdoes go
Derivationstep: Inducedrule:
input: ne VB?pas
output: VP
RBAUX x2
notdoes
S
NP VP
PRP RBAUX VB
he notdoes go
input: NP?VP
output: S
x1 x2
Figure 4: Two derivation steps and the rules that are in-
duced from them.
be inferred from the source string, target tree, and align-
ment.
2.2 From Derivations to Rules
In essence, a derivation step can be viewed as the applica-
tion of a rule. Thus, compiling the set of derivation steps
used in any derivation of ?A(S, T ) gives us, in a mean-
ingful sense, all relevant rules that can be extracted from
the triple (S, T, A). In this section, we show in concrete
terms how to convert a derivation step into a usable rule.
Consider the second-last derivation step of the first
derivation in Figure 2. In it, we begin with a source sym-
bol ?ne?, followed by a target subtree rooted at V B, fol-
lowed by another source symbol ?pas.? These three ele-
ments of the derivation string are replaced with a target
subtree rooted at V P that discards the source symbols
and contains the target subtree rooted at V B. In general,
this replacement process can be captured by the rule de-
picted in Figure 4. The input to the rule are the roots
of the elements of the derivation string that are replaced
(where we define the root of a symbol to be simply the
symbol itself), whereas the output of the rule is a symbol
tree, except that some of the leaves are labeled with vari-
ables instead of symbols from the target alhabet. These
variables correspond to elements of the input to the rule.
For instance, the leaf labeled x2 means that when this rule
is applied, x2 is replaced by the target subtree rooted at
V B (since V B is the second element of the input). Ob-
serve that the second rule induced in Figure 4 is simply
a CFG rule expressed in the opposite direction, thus this
rule format can (and should) be viewed as a strict gener-
alization of CFG rules.
SNP VP
PRP RBAUX VB
he notdoes go
il vane pas
{ il, ne, va,pas}
{ ne, va,pas}{ il }
{ il }
{ il }
{ il }
{ne,pas} {ne,pas}
{ne,pas} {ne,pas}
{ va }
{ ne } { va }
{ va }
{pas}
Figure 5: An alignment graph. The nodes are annotated
with their spans. Nodes in the frontier set are boldfaced
and italicized.
Every derivation step can be mapped to a rule in this
way. Hence given a source string S, a target tree T , and
an alignment A, we can define the set ?A(S, T ) as the set
of rules in any derivation D ? ?A(S, T ). We can regard
this as the set of rules that we are entitled to infer from
the triple (S, T, A).
2.3 Inferring Complex Rules
Now we have a precise problem statement: learn the set
?A(S, T ). It is not immediately clear how such a set can
be learned from the triple (S, T, A). Fortunately, we can
infer these rules directly from a structure called an align-
ment graph. In fact, we have already seen numerous ex-
amples of alignment graphs. Graphically, we have been
depicting the triple (S, T, A) as a rooted, directed, acyclic
graph (where direction is top-down in the diagrams). We
refer to such a graph as an alignment graph. Formally,
the alignment graph corresponding to S, T , and A is just
T , augmented with a node for each element of S, and
edges from leaf node t ? T to element s ? S iff A aligns
s with t. Although there is a difference between a node
of the alignment graph and its label, we will not make a
distinction, to ease the notational burden.
To make the presentation easier to follow, we assume
throughout this section that the alignment graph is con-
nected, i.e. there are no unaligned elements. All of the
results that follow have generalizations to deal with un-
aligned elements, but unaligned elements incur certain
procedural complications that would cloud the exposi-
tion.
It turns out that it is possible to systematically con-
vert certain fragments of the alignment graph into rules
of ?A(S, T ). We define a fragment of a directed, acyclic
graph G to be a nontrivial (i.e. not just a single node) sub-
graph G? of G such that if a node n is in G? then either n
is a sink node of G? (i.e. it has no children) or all of its
children are in G? (and it is connected to all of them). In
VP
RBAUX VB
notdoes
ne pas
S
NP VP
input: ne VB?pas
output: VP
RBAUX x2
notdoes
input: NP?VP
output: S
x1 x2
{ ne } {pas}
{ va }
{ ne, va,pas}
{ il } { ne, va,pas}
{ il, ne, va,pas}
Figure 6: Two frontier graph fragments and the rules in-
duced from them. Observe that the spans of the sink
nodes form a partition of the span of the root.
Figure 6, we show two examples of graph fragments of
the alignment graph of Figure 5.
The span of a node n of the alignment graph is the
subset of nodes from S that are reachable from n. Note
that this definition is similar to, but not quite the same
as, the definition of a span given by Fox (2002). We
say that a span is contiguous if it contains all elements
of a contiguous substring of S. The closure of span(n)
is the shortest contiguous span which is a superset of
span(n). For instance, the closure of {s2, s3, s5, s7}
would be {s2, s3, s4, s5, s6, s7} The alignment graph in
Figure 5 is annotated with the span of each node.
Take a look at the graph fragments in Figure 6. These
fragments are special: they are examples of frontier
graph fragments. We first define the frontier set of an
alignment graph to be the set of nodes n that satisfy the
following property: for every node n? of the alignment
graph that is connected to n but is neither an ancestor nor
a descendant of n, span(n?) ? closure(span(n)) = ?.
We then define a frontier graph fragment of an align-
ment graph to be a graph fragment such that the root and
all sinks are in the frontier set. Frontier graph fragments
have the property that the spans of the sinks of the frag-
ment are each contiguous and form a partition of the span
of the root, which is also contiguous. This allows the fol-
lowing transformation process:
1. Place the sinks in the order defined by the partition
(i.e. the sink whose span is the first part of the span
of the root goes first, the sink whose span is the sec-
ond part of the span of the root goes second, etc.).
This forms the input of the rule.
2. Replace sink nodes of the fragment with a variable
corresponding to their position in the input, then
take the tree part of the fragment (i.e. project the
fragment on T ). This forms the output of the rule.
Figure 6 shows the rules derived from the given graph
fragments. We have the following result.
Theorem 1 Rules constructed according to the above
procedure are in ?A(S, T ).
Rule extraction: Algorithm 1. Thus we now have a
simple method for extracting rules of ?A(S, T ) from the
alignment graph: search the space of graph fragments for
frontier graph fragments.
Unfortunately, the search space of all fragments of a
graph is exponential in the size of the graph, thus this
procedure can also take a long time to execute. To ar-
rive at a much faster procedure, we take advantage of the
following provable facts:
1. The frontier set of an alignment graph can be identi-
fied in time linear in the size of the graph.
2. For each node n of the frontier set, there is a unique
minimal frontier graph fragment rooted at n (ob-
serve that for any node n? not in the frontier set,
there is no frontier graph fragment rooted at n?, by
definition).
By minimal, we mean that the frontier graph fragment
is a subgraph of every other frontier graph fragment with
the same root. Clearly, for an alignment graph with k
nodes, there are at most k minimal frontier graph frag-
ments. In Figure 7, we show the seven minimal frontier
graph fragments of the alignment graph of Figure 5. Fur-
thermore, all other frontier graph fragments can be cre-
ated by composing 2 or more minimal graph fragments,
as shown in Figure 8. Thus, the entire set of frontier graph
fragments (and all rules derivable from these fragments)
can be computed systematically as follows: compute the
set of minimal frontier graph fragments, compute the set
of graph fragments resulting from composing 2 minimal
frontier graph fragments, compute the set of graph frag-
ments resulting from composing 3 minimal graph frag-
ments, etc. In this way, the rules derived from the min-
imal frontier graph fragments can be regarded as a ba-
sis for all other rules derivable from frontier graph frag-
ments. Furthermore, we conjecture that the set of rules
derivable from frontier graph fragments is in fact equiva-
lent to ?A(S, T ).
Thus we have boiled down the problem of extracting
complex rules to the following simple problem: find the
set of minimal frontier graph fragments of a given align-
ment graph.
The algorithm is a two-step process, as shown below.
Rule extraction: Algorithm 2
1. Compute the frontier set of the alignment graph.
2. For each node of the frontier set, compute the mini-
mal frontier graph fragment rooted at that node.
VP
RBAUX VB
notdoes
ne pas
S
NP VP
NP
PRP
PRP
he
VB
go
go
vahe
il
Figure 7: The seven minimal frontier graph fragments of
the alignment graph in Figure 5
VP
RBAUX VB
notdoes
ne pas
VB
go
+ =
VP
RBAUX VB
notdoes
ne pas
go
S
NP VP
+ + =
NP
PRP
PRP
he
S
NP VP
PRP
he
Figure 8: Example compositions of minimal frontier
graph fragments into larger frontier graph fragments.
Step 1 can be computed in a single traversal of the
alignment graph. This traversal annotates each node with
its span and its complement span. The complement span
is computed as the union of the complement span of its
parent and the span of all its siblings (siblings are nodes
that share the same parent). A node n is in the frontier
set iff complement span(n) ? closure(span(n)) = ?.
Notice that the complement span merely summarizes the
spans of all nodes that are neither ancestors nor descen-
dents of n. Since this step requires only a single graph
traversal, it runs in linear time.
Step 2 can also be computed straightforwardly. For
each node n of the frontier set, do the following: expand
n, then as long as there is some sink node n? of the result-
ing graph fragment that is not in the frontier set, expand
n?. Note that after computing the minimal graph frag-
ment rooted at each node of the frontier set, every node
of the alignment graph has been expanded at most once.
Thus this step also runs in linear time.
For clarity of exposition and lack of space, a couple of
issues have been glossed over. Briefly:
? As previously stated, we have ignored here the is-
sue of unaligned elements, but the procedures can
be easily generalized to accommodate these. The
results of the next two sections are all based on im-
plementations that handle unaligned elements.
? This theory can be generalized quite cleanly to in-
clude derivations for which substrings are replaced
by sets of trees, rather than one single tree. This
corresponds to allowing rules that do not require the
output to be a single, rooted tree. Such a general-
ization gives some nice power to effectively explain
certain linguistic phenomena. For instance, it allows
us to immediately translate ?va? as ?does go? in-
stead of delaying the creation of the auxiliary word
?does? until later in the derivation.
3 Experiments
3.1 Language Choice
We evaluated the coverage of our model of transforma-
tion rules with two language pairs: English-French and
English-Chinese. These two pairs clearly contrast by
the underlying difficulty to understand and model syntac-
tic transformations among pairs: while there is arguably
a fair level of cohesion between English and French,
English and Chinese are syntactically more distant lan-
guages. We also chose French to compare our study with
that of Fox (2002). The additional language pair provides
a good means of evaluating how our transformation rule
extraction method scales to more problematic language
pairs for which child-reordering models are shown not to
explain the data well.
3.2 Data
We performed experiments with two corpora, the FBIS
English-Chinese Parallel Text and the Hansard French-
English corpus.We parsed the English sentences with
a state-of-the-art statistical parser (Collins, 1999). For
the FBIS corpus (representing eight million English
words), we automatically generated word-alignments us-
ing GIZA++ (Och and Ney, 2003), which we trained on
a much larger data set (150 million words). Cases other
than one-to-one sentence mappings were eliminated. For
the Hansard corpus, we took the human annotation of
word alignment described in (Och and Ney, 2000). The
corpus contains two kinds of alignments: S (sure) for
unambiguous cases and P (possible) for unclear cases,
e.g. idiomatic expressions and missing function words
(S ? P ). In order to be able to make legitimate com-
parisons between the two language pairs, we also used
GIZA++ to obtain machine-generated word alignments
for Hansard: we trained it with the 500 sentences and
additional data representing 13.7 million English words
(taken from the Hansard and European parliament cor-
pora).
3.3 Results
From a theoretical point of view, we have shown that our
model can fully explain the transformation of any parse
tree of the source language into a string of the target lan-
guage. The purpose of this section is twofold: to pro-
vide quantitative results confirming the full coverage of
our model and to analyze some properties of the trans-
formation rules that support these derivations (linguistic
analyses of these rules are presented in the next section).
Figure 9 summarizes the coverage of our model with
respect to the Hansard and FBIS corpora. For the for-
mer, we present results for the three alignments: S align-
ments, P alignments, and the alignments computed by
GIZA++. Each plotted value represents a percentage of
parse trees in a corpus that can be transformed into a tar-
get sentence using transformation rules. The x-axis rep-
resents different restrictions on the size of these rules: if
we use a model that restrict rules to a single expansion
of a non-terminal into a sequence of symbols, we are in
the scope of the child-reordering model of (Yamada and
Knight, 2001; Fox, 2002). We see that its explanatory
power is quite poor, with only 19.4%, 14.3%, 16.5%, and
12.1% (for the respective corpora). Allowing more ex-
pansions logically expands the coverage of the model,
until the point where it is total: transformation rules no
larger than 17, 18, 23, and 43 (in number of rule expan-
sions) respectively provide enough coverage to explain
the data at 100% for each of the four cases.
It appears from the plot that the quality of alignments
plays an important role. If we compare the three kinds of
alignments available for the Hansard corpus, we see that
much more complex transformation rules are extracted
from noisy GIZA++ alignments. It also appears that the
language difference produces quite contrasting results.
Rules acquired for the English-Chinese pair have, on av-
erage, many more nodes. Note that the language differ-
ence in terms of syntax might be wider than what the plot
seems to indicate, since word alignments computed for
the Hansard corpus are likely to be more errorful than the
ones for FBIS because the training data used to induce the
latter is more than ten times larger than for the former.
In Figure 10, we show the explanatory power of our
model at the node level. At each node of the frontier
set, we determine whether it is possible to extract a rule
that doesn?t exceed a given limit k on its size. The plot-
ted values represent the percentage of frontier set inter-
nal nodes that satisfy this condition. These results appear
more promising for the child-reordering model, with cov-
erage ranging from 72.3% to 85.1% of the nodes, but we
should keep in mind that many of these nodes are low in
the tree (e.g. base NPs); extraction of 1-level transfor-
mation rules generally present no difficulties when child
nodes are pre-terminals, since any crossings can be re-
solved by lexicalizing the elements involved in it. How-
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1 2 3 4 5 6 7 8 9 10 15 20 25 30 35 40 4550
Pa
rse
 tr
ee
 co
ve
ra
ge
Maximum number of rule expansions
"Hansard-S"
"Hansard-P"
"Hansard-GIZA"
"FBIS"
Figure 9: Percentage of parse trees covered by the model
given different constraints on the maximum size of the
transformation rules.
0.7
0.75
0.8
0.85
0.9
0.95
1
1 2 3 4 5 6 7 8 9 10 15 20 25 30 35 40 4550
No
de
 co
ve
ra
ge
Maximum number of rule expansions
"Hansard-S"
"Hansard-P"
"Hansard-GIZA"
"FBIS"
Figure 10: Same as Figure 9, except that here coverage is
evaluated at the node level.
ever, higher level syntactic constituents are more prob-
lematic for child-reordering models, and the main rea-
sons they fail to provide explanation of the parses at the
sentence level.
Table 1 shows that the extraction of rules can be per-
formed quite efficiently. Our first algorithm, which has an
exponential running time, cannot scale to process large
corpora and extract a sufficient number of rules that a
syntax-based statistical MT system would require. The
second algorithm, which runs in linear time, is on the
other hand barely affected by the size of rules it extracts.
k=1 3 5 7 10 20 50
I 4.1 10.2 57.9 304.2 - - -
II 4.3 5.4 5.9 6.4 7.33 9.6 11.8
Table 1: Running time in seconds of the two algorithms
on 1000 sentences. k represent the maximum size of rules
to extract.
NPB
DT NN RB
that Government simply tells
ADVP
VBZ
NPB
DT NNS
the people what is themgood for
WP VBZ JJ IN PRP
NPB
ADJP
VP
SG-A
SBAR-A
VHPN
VP
S
le gouvernement dit tout simplement ? les gens ce qui est bon pour eux
input:
VBZ ADVP ?NPB SBAR -S
output: S
VPx2
x1 x3 x4
Figure 11: Adverb-verb reordering.
4 Discussions
In this section, we present some syntactic transformation
rules that our system learns. Fox (2002) identified three
major causes of crossings between English and French:
the ?ne ... pas? construct, modals and adverbs, which a
child-reordering model doesn?t account for. In section 2,
we have already explained how we learn syntactic rules
involving ?ne ... pas?. Here we describe the other two
problematic cases.
Figure 11 presents a frequent cause of crossings be-
tween English and French: adverbs in French often ap-
pear after the verb, which is less common in English.
Parsers generally create nested verb phrases when ad-
verbs are present, thus no child reordering can allow a
verb and an adverb to be permuted. Multi-level reodering
as the rule in the figure can prevent crossings. Fox?s solu-
tion to the problem of crossings is to flatten verb phrases.
This is a solution for this sentence pair, since this ac-
counts for adverb-verb reorderings, but flattening the tree
structure is not a general solution. Indeed, it can only ap-
ply to a very limited number of syntactic categories, for
which the advantage of having a deep syntactic structure
is lost.
Figure 12 (dotted lines are P alignments) shows an in-
teresting example where flattening the tree structure can-
not resolve all crossings in node-reordering models. In
these models, a crossing remains between MD and AUX
no matter how VPs are flattened. Our transformation rule
model creates a lexicalized rule as shown in the figure,
where the transformation of ?will be? into ?sera? is the
only way to resolve the crossing.
In the Chinese-English domain, the rules extracted by
our algorithm often have the attractive quality that they
are the kind of common-sense constructions that are used
in Chinese language textbooks to teach students. For in-
stance, there are several that illustrate the complex re-
orderings that occur around the Chinese marker word
?de.?
NPB
DT JJ NN
the full report will
MD AUX VB
be coming in before the fall
RB IN DT NN
NPB
PP
VP-A
ADVP
VP
S
le rapport complet sera d?pos? de ici le automne prochain
input: sera  VP-A
output:
VP
VP-Awill/MD
be/AUX
VP-A
x2
Figure 12: Crossing due to a modal.
5 Conclusion
The fundamental assumption underlying much recent
work in statistical machine translation (Yamada and
Knight, 2001; Eisner, 2003; Gildea, 2003) is that lo-
cal transformations (primarily child-node re-orderings)
of one-level parent-children substructures are an adequate
model for parallel corpora. Our empirical results suggest
that this may be too strong of an assumption. To explain
the data in two parallel corpora, one English-French, and
one English-Chinese, we are often forced to learn rules
involving much larger tree fragments. The theory, algo-
rithms, and transformation rules we learn automatically
from data have several interesting aspects.
1. Our rules provide a good, realistic indicator of the
complexities inherent in translation. We believe that
these rules can inspire subsequent developments of
generative statistical models that are better at ex-
plaining parallel data than current ones.
2. Our rules put at the fingertips of linguists a very
rich source of information. They encode translation
transformations that are both syntactically and lex-
ically motivated (some of our rules are purely syn-
tactic; others are lexically grounded). A simple sort
on the counts of our rules makes explicit the trans-
formations that occur most often. A comparison of
the number of rules extracted from parallel corpora
specific to multiple language pairs provide a quanti-
tative estimator of the syntactic ?closeness? between
various language pairs.
3. The theory we proposed in this paper is independent
of the method that one uses to compute the word-
level alignments in a parallel corpus.
4. The theory and rule-extraction algorithm are also
well-suited to deal with the errors introduced by
the word-level alignment and parsing programs one
uses. Our theory makes no a priori assumptions
about the transformations that one is permitted to
learn. If a parser, for example, makes a systematic
error, we expect to learn a rule that can neverthe-
less be systematically used to produce correct trans-
lations.
In this paper, we focused on providing a well-founded
mathematical theory and efficient, linear algorithms
for learning syntactically motivated transformation rules
from parallel corpora. One can easily imagine a range
of techniques for defining probability distributions over
the rules that we learn. We suspect that such probabilis-
tic rules could be also used in conjunction with statistical
decoders, to increase the accuracy of statistical machine
translation systems.
Acknowledgements
This work was supported by DARPA contract N66001-
00-1-9814 and MURI grant N00014-00-1-0617.
References
E. Charniak, K. Knight, and K. Yamada. 2003. Syntax-
based language models for machine translation. In
Proc. MT Summit IX.
M. Collins. 1999. Head-driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University of
Pennsylvania, Philadelphia.
J. Eisner. 2003. Learning non-isomorphic tree mappings
for machine translation. In Proc. of the 41st Meeting
of the Association for Computational Linguistics.
H. Fox. 2002. Phrasal cohesion and statistical machine
translation. In Proc. of Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP).
D. Gildea. 2003. Loosely tree-based alignment for ma-
chine translation. In Proc. of the 41th Annual Confer-
ence of the Association for Computational Linguistics.
P. Koehn, F. Och, and D. Marcu. 2003. Statistical phrase-
based translation. In Proceedings of HLT/NAACL.
F. Och and H. Ney. 2000. Improved statistical alignment
models. Proc. of the 38th Annual Meeting of the Asso-
ciation for Computational Linguistics.
F. Och and H Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377?404.
K. Yamada and K. Knight. 2001. A syntax-based statis-
tical translation model. In ACL, pages 523?530.
Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 41?48,
Rochester, New York, April 2007. c?2007 Association for Computational Linguistics
Machine Translation as Tree Labeling
Mark Hopkins
Department of Linguistics
University of Potsdam, Germany
hopkins@ling.uni-potsdam.de
Jonas Kuhn
Department of Linguistics
University of Potsdam, Germany
kuhn@ling.uni-potsdam.de
Abstract
We present the main ideas behind a new
syntax-based machine translation system,
based on reducing the machine translation
task to a tree-labeling task. This tree la-
beling is further reduced to a sequence of
decisions (of four varieties), which can be
discriminatively trained. The optimal tree
labeling (i.e. translation) is then found
through a simple depth-first branch-and-
bound search. An early system founded
on these ideas has been shown to be
competitive with Pharaoh when both are
trained on a small subsection of the Eu-
roparl corpus.
1 Motivation
Statistical machine translation has, for a while now,
been dominated by the phrase-based translation par-
adigm (Och and Ney, 2003). In this paradigm,
sentences are translated from a source language to
a target language through the repeated substitution
of contiguous word sequences (?phrases?) from the
source language for word sequences in the target
language. Training of the phrase translation model
builds on top of a standard statistical word align-
ment over the training corpus for identifying corre-
sponding word blocks, assuming no further linguis-
tic analysis of the source or target language. In de-
coding, these systems then typically rely on n-gram
language models and simple statistical reordering
models to shuffle the phrases into an order that is
coherent in the target language.
There are limits to what such an approach can ul-
timately achieve. Machine translation based on a
deeper analysis of the syntactic structure of a sen-
tence has long been identified as a desirable objec-
tive in principle (consider (Wu, 1997; Yamada and
Knight, 2001)). However, attempts to retrofit syn-
tactic information into the phrase-based paradigm
have not met with enormous success (Koehn et al,
2003; Och et al, 2003)1, and purely phrase-based
machine translation systems continue to outperform
these syntax/phrase-based hybrids.
In this work, we try to make a fresh start with
syntax-based machine translation, discarding the
phrase-based paradigm and designing a machine
translation system from the ground up, using syntax
as our central guiding star. Evaluation with BLEU
and a detailed manual error analysis of our nascent
system show that this new approach might well have
the potential to finally realize some of the promises
of syntax.
2 Problem Formulation
We want to build a system that can learn to translate
sentences from a source language to a destination
language. As our first step, we will assume that the
system will be learning from a corpus consisting of
triples ?f, e, a?, where: (i) f is a sentence from our
source language, which is parsed (the words of the
sentence and the nodes of the parse tree may or may
not be annotated with auxiliary information), (ii) e is
a gold-standard translation of sentence f (the words
of sentence e may or may not be annotated with aux-
iliary information), and (iii) a is an automatically-
generated word alignment (e.g. via GIZA++) be-
tween source sentence f and destination sentence e.
1(Chiang, 2005) also reports that with his hierarchical gen-
eralization of the phrase-based approach, the addition of parser
information doesn?t lead to any improvements.
41
Figure 1: Example translation object.
Let us refer to these triples as translation objects.
The learning task is: using the training data, pro-
duce a scoring function P that assigns a score to
every translation object ?f, e, a?, such that this scor-
ing function assigns a high score to good transla-
tions, and a low score to poor ones. The decoding
task is: given scoring function P and an arbitrary
sentence f from the source language, find transla-
tion object ?f, e, a? that maximizes P (?f, e, a?).
To facilitate matters, we will map translation ob-
jects to an alternate representation. In (Galley et al,
2003), the authors give a semantics to every trans-
lation object by associating each with an annotated
parse tree (hereafter called a GHKM tree) represent-
ing a specific theory about how the source sentence
was translated into the destination sentence.
In Figure 1, we show an example translation ob-
ject and in Figure 2, we show its associated GHKM
tree. The GHKM tree is simply the parse tree f of
the translation object, annotated with rules (hereafter
referred to as GHKM rules). We will not describe in
depth the mapping process from translation object to
GHKM tree. Suffice it to say that the alignment in-
duces a set of intuitive translation rules. Essentially,
a rule like: ?not 1? ne 1 pas? (see Figure 2) means:
if we see the word ?not? in English, followed by a
phrase already translated into French, then translate
the entire thing as the word ?ne? + the translated
phrase + the word ?pas.? A parse tree node gets la-
beled with one of these rules if, roughly speaking,
its span is still contiguous when projected (via the
alignment) into the target language.
Formally, what is a GHKM tree? Define a rule el-
ement as a string or an indexed variable (e.g. x1,
x4, x32). A GHKM rule of rank k (where k is
a non-negative integer) is a pair ?Rs, Rd?, where
source list Rs and destination list Rd are both lists
of rule elements, such that each variable of Xk ,
{x1, x2, ..., xk} appears exactly once in Rs and ex-
actly once in Rd. Moreover, in Rs, the variables ap-
pear in ascending order. In Figure 2, some of the
tree nodes are annotated with GHKM rules. For
clarity, we use a simplified notation. For instance,
rule ??x1, x2, x3?, ?x3, ?,?, x1, x2?? is represented as
?1 2 3 ? 3 , 1 2?. We have also labeled the nodes
with roman numerals. When we want to refer to a
particular node in later examples, we will refer to it,
e.g., as t(i) or t(vii).
A rule node is a tree node annotated with a
GHKM rule (for instance, nodes t(i) or t(v) of Fig-
ure 2, but not node t(iv)). A tree node t2 is reachable
from tree node t1 iff node t2 is a proper descendant
of node t1 and there is no rule node (not including
nodes t1, t2) on the path from node t1 to node t2.
Define the successor list of a tree node t as the list
of rule nodes and leaves reachable from t (ordered in
left-to-right depth-first search order). For Figure 2,
the successor list of node t(i) is ?t(ii), t(v), t(xiii)?,
and the successor list of node t(v) is ?t(vii), t(viii)?.
The rule node successor list of a tree node is its suc-
cessor list, with all non-rule nodes removed.
Define the signature of a parse tree node t as the
result of taking its successor list, replacing the jth
rule node with variable xj , and replacing every non-
rule node with its word label (observe that all non-
rule nodes in the successor list are parse tree leaves,
and therefore they have word labels). For Figure 2,
the signature of node t(i) is ?x1, x2, x3?, and the sig-
nature of node t(v) is ??am?, x1?.
Notice that the signature of every rule node in Fig-
ure 2 coincides with the source list of its GHKM
rule. This is no accident, but rather a requirement.
Define a GHKM tree node as a parse tree node
whose children are all GHKM tree nodes, and whose
GHKM rule?s source list is equivalent to its signa-
ture (if the node is a rule node).
Given these definitions, we can proceed to define
how a GHKM tree expresses a translation theory.
Suppose we have a list S = ?s1, ..., sk? of strings.
Define the substitution of string list S into rule ele-
42
Figure 2: GHKM tree equivalent of example translation object. The light gray nodes are rule nodes of the
GHKM tree.
ment r as:
r[S] =
 si if r is indexed var xi
r otherwise
Notice that this operation always produces a
string. Define the substitution of string list S into
rule element list R = ?r1, ..., rj? as:
R[S] = concat(r1[S], r2[S], ..., rj [S])
where concat(s1, ..., sk) is the spaced concatenation
of strings s1, ..., sk (e.g., concat( ?hi?, ?there? ) =
?hi there?). This operation also produces a string.
Finally, define the translation of GHKM tree node
t as:
?(t) , Rd[??(t1), ..., ?(tk)?]
where ?t1, ..., tk? is the rule node successor list of
GHKM tree node t.
For Figure 2, the rule node successor list of node
t(viii) is ?t(xi)?. So:
?(t(viii)) = ??ne?, x1, ?pas??[??(t(xi))?]
= ??ne?, x1, ?pas??[??vais??]
= ?ne vais pas?
A similar derivation gives us:
?(t(i)) = ?aujourd?hui , je ne vais pas?
In this way, every GHKM tree encodes a transla-
tion. Given this interpretation of a translation object,
the task of machine translation becomes something
concrete: label the nodes of a parsed source sentence
with a good set of GHKM rules.
3 Probabilistic Approach
To achieve this ?good? labeling of GHKM rules,
we will define a probabilistic generative model P
of GHKM trees, which will serve as our scoring
function. We would like to depart from the stan-
dard probabilistic approach of most phrase-based
translators, which employ very simple probability
models to enable polynomial-time decoding. In-
stead, we will use an alternative probabilistic ap-
proach (an assignment process), which sacrifices
polynomial-time guarantees in favor of a more flexi-
ble and powerful model. This sacrifice of guaranteed
polynomial-time decoding does not entail the sacri-
fice of good running time in practice.
3.1 Assignment Processes
An assignment process builds a sequence of vari-
able assignments (called an assignment history) by
repeatedly iterating the following steps. First, it re-
quests a variable name (say x22) from a so-named
variable generator. It takes this variable name
and the assignment history built so far and com-
presses this information into a set of features (say
{f2, f6, f80}) using a feature function. These fea-
tures are then mapped to a probability distribution by
a function (say p7) requested from a so-named distri-
bution generator. The iteration ends by assigning to
the chosen variable a value (say v4) drawn from this
distribution. In the above running example, the iter-
ation assigns v4 to x22, which was drawn according
to distribution p7({f2, f6, f80}). The process ends
when the variable generator produces the reserved
token STOP instead of a variable name. At this
43
Var Assignment Distribution Features
x23 true p4 {}
x7 ?the? p10 {f12, f102}
x8 blue p2 {f5, f55}
x51 red p2 {f5, f15, f50}
x19 7.29 p5 {f2}
x30 false p4 {f2, f5, f7}
x1 ?man? p10 {f1, f2, f12}
x102 blue p2 {f1, f55, f56}
Figure 3: A example assignment history generated
by an assignment process.
point, the assignment history built so far (like the
example in Figure 3) is returned.
Formally, define a variable signature as a pair
? = ?X, V ?, where X is a set of variable names
and V is a set of values. Define a variable assign-
ment of signature ?X, V ? as a pair ?x, v?, for vari-
able x ? X and value v ? V . Define an assignment
history of signature ? as an ordered list of variable
assignments of ?. The notation H(?) represents the
set of all assignment histories of signature ?.
We define a feature function of signature ? =
?X, V ? as a function f that maps every pair of set
X ?H(?) to a set of assignments (called features)
of an auxiliary variable signature ?f .
We define an assignment process of signature
? = ?X, V ? as a tuple ?f, P, gx, gp?, where: (i) f is
a feature function of ?, (ii) P = {p1, ..., pk} is a fi-
nite set of k functions (called the feature-conditional
distributions) that map each feature set in range(f)
to a probability distribution over V , (iii) gx is a func-
tion (called the variable generator) mapping each
assignment history in the set H(?) to either a vari-
able name in X or the reserved token STOP , and
(iv) gp is a function (called the distribution gener-
ator) mapping each assignment history in the set
H(?) to a positive integer between 1 and k.
An assignment process probabilistically generates
an assignment history of signature ? in the follow-
ing way:
1. h? empty list
2. Do until gx(h) = STOP :
(a) Let x = gx(h) and let j = gp(h).
(b) Draw value v probabilistically from distri-
bution pj(f(x, h)).
(c) Append assignment ?x, v? to history h.
3. Return history h.
3.2 Training
Given all components of an assignment process
of signature ? except for the set P of feature-
conditional distributions, the training task is to learn
P from a training corpus of assignment histories of
signature ?. This can be achieved straightforwardly
by taking the feature vectors generated by a partic-
ular distribution and using them to discriminatively
learn the distribution. For instance, say that our cor-
pus consists of the single history given in Figure ??.
To learn distribution p2, we simply take the three
variable assignments produced by p2 and feed these
feature vectors to a generic discriminative learner.
We prefer learners that produce distributions (rather
than hard classifiers) as output, but this is not re-
quired.
3.3 Decoding
Notice that an assignment process of signature ? in-
duces a probability distribution over the set H(?) of
all assignment histories of ?. The decoding ques-
tion is: given a partial assignment history h, what
is the most probable completion of the history, ac-
cording to this induced distribution? We will use
the natural naive search space for this question. The
nodes of this search space are the assignment his-
tories of H(?). The children of the search node
representing history h are those histories that can be
generated from h in one iteration of the assignment
process. The value of a search node is the proba-
bility of its assignment history (according to the as-
signment process). To decode, we begin at the node
representing history h, and search for the highest-
value descendant that represents a complete assign-
ment history (i.e. an assignment history terminated
by the STOP token).
This is, potentially, a very large and intractible
search space. However, if most assignment deci-
sions can be made with relative confidence, then the
great majority of search nodes have values which
are inferior to those of the best solutions. The
standard search technique of depth-first branch-and-
bound search takes advantage of search spaces with
this particular characteristic by first finding greedy
good-quality solutions and using their values to opti-
mally prune a significant portion of the search space.
44
Figure 4: Partial GHKM tree, after rule nodes have been identified (light gray). Notice that once we identify
the rule node, the rule left-hand sides are already determined.
Depth-first branch-and-bound search has the follow-
ing advantage: it finds a good (suboptimal) solution
in linear time and continually improves on this solu-
tion until it finds the optimal. Thus it can be run ei-
ther as an optimal decoder or as a heuristic decoder,
since we can interrupt its execution at any time to get
the best solution found so far. Additionally, it takes
only linear space to run.
4 Generative Model
We now return to where we left off at the end of Sec-
tion 2, and devise an assignment process that pro-
duces a GHKM tree from an unlabeled parse tree.
This will give us a quality measure that we can use
to produce a ?good? labeling of a given parse tree
with GHKM rules (i.e., the probability of such a la-
beling according to the assignment process).
The simplest assignment process would have a
variable for each node of the parse tree, and these
variables would all be assigned by the same feature-
conditional distribution over the space of all possible
GHKM rules. The problem with such a formulation
is that such a distribution would be inachievably dif-
ficult to learn. We want an assignment process in
which all variables can take only a very small num-
ber of possible values, because it will be much eas-
ier to learn distributions over such variables. This
means we need to break down the process of con-
structing a GHKM rule into simpler steps.
Our assignment process will begin by sequen-
tially assigning a set of boolean variables (which we
will call rule node indicator variables), one for each
node in the parse tree. For parse tree node t, we de-
note its corresponding rule node indicator variable
xrt . Variable xrt is assigned true iff the parse tree
node t will be a rule node in the GHKM tree.
In Figure 3.3, we show a partial GHKM tree af-
ter these assignments are made. The key thing to
observe is that, after this sequence of boolean deci-
sions, the LHS of every rule in the tree is already
determined! To complete the tree, all we need to do
is to fill in their right-hand sides.
Again, we could create variables to do this di-
rectly, i.e. have a variable for each rule whose do-
main is the space of possible right-hand sides for its
established left-hand sides. But this is still a wide-
open decision, so we will break it down further.
For each rule, we will begin by choosing the
template of its RHS, which is a RHS in which
all sequences of variables are replaced with an
empty slot into which variables can later be placed.
For instance, the template of ??ne?, x1, ?pas?? is
??ne?, X, ?pas?? and the template of ?x3, ?,?, x1, x2?
is ?X, ?,?, X?, where X represents the empty slots.
Once the template is chosen, it simply needs to be
filled with the variables from the LHS. To do so, we
process the LHS variables, one by one. By default,
they are placed to the right of the previously placed
variable (the first variable is placed in the first slot).
We repeatedly offer the option to push the variable
to the right until the option is declined or it is no
longer possible to push it further right. If the vari-
able was not pushed right at all, we repeatedly offer
the option to push the variable to the left until the
option is declined or it is no longer possible to push
it further left. Figure 4 shows this generative story
in action for the rule RHS ?x3, ?,?, x1, x2?.
These are all of the decisions we need to make
45
Decision to make Decision RHS so far
RHS template? X , X X , X
default placement of var 1 1 , X
push var 1 right? yes X , 1
default placement of var 2 X , 1 2
push var 2 left? no X , 1 2
default placement of var 3 X , 1 2 3
push var 3 left? yes X , 1 3 2
push var 3 left? yes X , 3 1 2
push var 3 left? yes 3 , 1 2
Figure 5: Trace of the generative story for the right-
hand side of a GHKM rule.
in order to label a parse tree with GHKM rules. No-
tice that, aside from the template decisions, all of the
decisions are binary (i.e. feasible to learn discrimi-
natively). Even the template decisions are not terri-
bly large-domain, if we maintain a separate feature-
conditional distribution for each LHS template. For
instance, if the LHS template is ??not?, X?, then
RHS template ??ne?, X, ?pas?? and a few other se-
lect candidates should bear most of the probability
mass.
5 Evaluation
In this section, we evaluate a preliminary English-
to-German translation system based on the ideas
outlined in this paper. We first present a quantia-
tive comparison with the phrase-based approach, us-
ing the BLEU metric; then we discuss two con-
crete translation examples as a preliminary qualita-
tive evaluation. Finally, we present a detailed man-
ual error analysis.
Our data was a subset of the Europarl corpus con-
sisting of sentences of lengths ranging from 8 to 17
words. Our training corpus contained 50000 sen-
tences and our test corpus contained 300 sentences.
We also had a small number of reserved sentences
for development. The English sentences were parsed
using the Bikel parser (Bikel, 2004), and the sen-
tences were aligned with GIZA++ (Och and Ney,
2000). We used the WEKA machine learning pack-
age (Witten and Frank, 2005) to train the distribu-
tions (specifically, we used model trees).
For comparison, we also trained and evaluated
Pharaoh (Koehn, 2005) on this limited corpus, us-
ing Pharaoh?s default parameters. Pharaoh achieved
a BLEU score of 11.17 on the test set, whereas our
system achieved a BLEU score of 11.52. What is
notable here is not the scores themselves (low due to
the size of the training corpus). However our system
managed to perform comparably with Pharaoh in a
very early stage of its development, with rudimen-
tary features and without the benefit of an n-gram
language model.
Let?s take a closer look at the sentences produced
by our system, to gain some insight as to its current
strengths and weaknesses.
Starting with the English sentence (note that all
data is lowercase):
i agree with the spirit of those amendments .
Our system produces:
ich
I
stimme
vote
die
the.FEM
geist
spirit.MASC
dieser
these
a?nderungsantra?ge
change-proposals
zu
to
.
.
The GHKM tree is depicted in Figure 5. The key
feature of this translation is how the English phrase
?agree with? is translated as the German ?stimme
... zu? construction. Such a feat is difficult to pro-
duce consistently with a purely phrase-based sys-
tem, as phrases of arbitrary length can be placed be-
tween the words ?stimme? and ?zu?, as we can see
happening in this particular example. By contrast,
Pharaoh opts for the following (somewhat less de-
sirable) translation:
ich
I
stimme
vote
mit
with
dem
the.MASC
geist
spirit.MASC
dieser
these
a?nderungsantra?ge
change-proposals
.
.
A weakness in our system is also evident here.
The German noun ?Geist? is masculine, thus our
system uses the wrong article (a problem that
Pharaoh, with its embedded n-gram language model,
does not encounter).
In general, it seems that our system is superior to
Pharaoh at figuring out the proper way to arrange the
words of the output sentence, and inferior to Pharaoh
at finding what the actual translation of those words
should be.
Consider the English sentence:
we shall submit a proposal along these lines before
the end of this year .
46
Figure 6: GHKM tree output for the first test sentence.
Here we have an example of a double verb: ?shall
submit.? In German, the second verb should go at
the end of the sentence, and this is achieved by our
system (translating ?shall? as ?werden?, and ?sub-
mit? as ?vorlegen?).
wir
we
werden
will
eine
a.FEM
vorschlag
proposal.MASC
in
in
dieser
these
haushaltslinien
budget-lines
vor
before
die
the.FEM
ende
end.NEUT
dieser
this.FEM
jahres
year.NEUT
vorlegen
submit
.
.
Pharaoh does not manage this (translating ?sub-
mit? as ?unterbreiten? and placing it mid-sentence).
werden
will
wir
we
unterbreiten
submit
eine
a
vorschlag
proposal
in
in
dieser
these
haushaltslinien
budget-lines
vor
before
ende
end
dieser
this.FEM
jahr
year.NEUT
.
.
It is worth noting that while our system gets the
word order of the output system right, it makes sev-
eral agreement mistakes and (like Pharaoh) doesn?t
get the translation of ?along these lines? right.
To have a more systematic basis for comparison,
we did a manual error analysis for 100 sentences
from the test set. A native speaker of German (in the
present pilot study one of the authors) determined
the editing steps required to transform the system
output into an acceptable translation ? both in terms
of fluency and adequacy of translation. In order to
avoid a bias for our system, we randomized the pre-
sentation of output from one of the two systems.
We defined the following basic types of edits, with
further subdistinctions depending on the word type:
ADD, DELETE, CHANGE and MOVE. A special type
TRANSLATE-untranslated was assumed for untrans-
lated source words in the output. For the CHANGE,
more fine-grained distinctions were made.2 A sin-
gle MOVE operation was assumed to displace an en-
tire phrase; the distance of the movement in terms
of the number of words was calculated. The table in
Figure 7 shows the edits required for correcting the
output of the two systems on 100 sentences.
We again observe that our system, which is at
an early stage of development and contrary to the
Pharaoh system does not include an n-gram lan-
guage model trained on a large corpus, already
yields promising results. The higher proportion
of CHANGE operations, in particular CHANGE-
inflection and CHANGE-function-word edits is pre-
sumably a direct consequence of providing a lan-
guage model or not. An interesting observation is
that our system currently tends to overtranslate, i.e.,
redundantly produce several translations for a word,
which leads to the need of DELETE operations. The
Pharaoh system had a tendency to undertranslate, of-
ten with crucial words missing.
2CHANGE-inflection: keeping the lemma and category the
same, e.g. taken ? takes; CHANGE-part-of-speech: choos-
ing a different derivational form, e.g., judged ? judgement;
CHANGE-function-word: e.g., in ? from; CHANGE-content-
word: e.g., opinion ? consensus.
47
TL-MT Pharaoh
ADD-function-word 40 49
ADD-content-word 17 35
ADD-punctuation 12 13
ADD (total) 69 97
DELETE-function-word 37 18
DELETE-content-word 22 10
DELETE-punctuation 13 15
DELETE-untranslated 2 1
DELETE (total) 74 44
CHANGE-content-word 24 19
CHANGE-function-word 44 26
CHANGE-inflection 101 80
CHANGE-part-of-speech 4 10
CHANGE (total) 173 135
TRANSLATE-untranslated 34 1
MOVE (distance)
1 16 17
2 12 16
3 13 11
4 3 6
? 5 7 5
MOVE (total) 51 55
TOTAL # EDITS 401 332
edits-per-word ratio 0.342 0.295
Figure 7: Edits required for an acceptable system
output, based on 100 test sentences.
6 Discussion
In describing this pilot project, we have attempted
to give a ?big picture? view of the essential ideas
behind our system. To avoid obscuring the presen-
tation, we have avoided many of the implementation
details, in particular our choice of features. There
are exactly four types of decisions that we need to
train: (1) whether a parse tree node should be a rule
node, (2) the RHS template of a rule, (3) whether a
rule variable should be pushed left, and (4) whether
a rule variable should be pushed right. For each of
these decisions, there are a number of possible fea-
tures that suggest themselves. For instance, recall
that in German, typically the second verb of a double
verb (such as ?shall submit? or ?can do?) gets placed
at the end of the sentence or clause. So when the
system is considering whether to push a rule?s noun
phrase to the left, past an existing verb, it would be
useful for it to consider (as a feature) whether that
verb is the first or second verb of its clause.
This system was designed to be very flexible with
the kind of information that it can exploit as fea-
tures. Essentially any aspect of the parse tree, or
of previous decisions that have been taken by the
assignment process, can be used. Furthermore, we
can mark-up the parse tree with any auxiliary infor-
mation that might be beneficial, like noun gender or
verb cases. The current implementation has hardly
begun to explore these possibilities, containing only
features pertaining to aspects of the parse tree.
Even in these early stages of development, the
system shows promise in using syntactic informa-
tion flexibly and effectively for machine translation.
We hope to develop the system into a competitive
alternative to phrase-based approaches.
References
Daniel M. Bikel. 2004. Intricacies of Collins? parsing model.
Computational Linguistics, 30(4):479?511.
David Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proceedings of ACL, pages
263?270.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2003. What?s in a translation rule? In Proc. NAACL.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Sta-
tistical phrase-based translation. In Proceedings of the Hu-
man Language Technology Conference 2003 (HLT-NAACL
2003), Edmonton, Canada.
Philipp Koehn. 2005. Pharaoh: a beam search decoder for
phrase-based statistical machine translation models. In Pro-
ceedings of the Sixth Conference of the Association for Ma-
chine Translation in the Americas, pages 115?124.
F. J. Och and H. Ney. 2000. Improved statistical alignment
models. In Proc. ACL, pages 440?447, Hongkong, China,
October.
Franz Josef Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51.
F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Yamada,
A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng, Viren
Jain, Z.Jin, and D. Radev. 2003. Syntax for statistical ma-
chine translation. Technical report, Center for Language and
Speech Processing, Johns Hopkins University, Baltimore.
Summer Workshop Final Report.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Practical
machine learning tools and techniques. Morgan Kaufmann.
Dekai Wu. 1997. Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora. Computational
Linguistics, 23(3):377?403.
Kenji Yamada and Kevin Knight. 2001. A syntax-based statis-
tical translation model. In Proceedings of the 39th Annual
Meeting of the Association for Computational Linguistics,
pages 523?530.
48
Proceedings of the 5th Workshop on Important Unresolved Matters, pages 33?40,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Deep Grammars in a Tree Labeling Approach to
Syntax-based Statistical Machine Translation
Mark Hopkins
Department of Linguistics
University of Potsdam, Germany
hopkins@ling.uni-potsdam.de
Jonas Kuhn
Department of Linguistics
University of Potsdam, Germany
kuhn@ling.uni-potsdam.de
Abstract
In this paper, we propose a new syntax-
based machine translation (MT) approach
based on reducing the MT task to a tree-
labeling task, which is further decom-
posed into a sequence of simple decisions
for which discriminative classifiers can be
trained. The approach is very flexible and
we believe that it is particularly well-suited
for exploiting the linguistic knowledge en-
coded in deep grammars whenever possi-
ble, while at the same time taking advantage
of data-based techniques that have proven a
powerful basis for MT, as recent advances in
statistical MT show.
A full system using the Lexical-Functional
Grammar (LFG) parsing system XLE and
the grammars from the Parallel Grammar
development project (ParGram; (Butt et
al., 2002)) has been implemented, and we
present preliminary results on English-to-
German translation with a tree-labeling sys-
tem trained on a small subsection of the Eu-
roparl corpus.
1 Motivation
Machine translation (MT) is probably the oldest ap-
plication of what we call deep linguistic processing
techniques today. But from its inception, there have
been alternative considerations of approaching the
task with data-based statistical techniques (cf. War-
ren Weaver?s well-known memo from 1949). Only
with fairly recent advances in computer technology
have researchers been able to build effective statis-
tical MT prototypes, but in the last few years, the
statistical approach has received enormous research
interest and made significant progress.
The most successful statistical MT paradigm has
been, for a while now, the so-call phrase-based MT
approach (Och and Ney, 2003). In this paradigm,
sentences are translated from a source language to
a target language through the repeated substitution
of contiguous word sequences (?phrases?) from the
source language for word sequences in the target
language. Training of the phrase translation model
builds on top of a standard statistical word alignment
over the training corpus of parallel text (Brown et al,
1993) for identifying corresponding word blocks,
assuming no further linguistic analysis of the source
or target language. In decoding, i.e. the application
of the acquired translation model to unseen source
sentences, these systems then typically rely on n-
gram language models and simple statistical reorder-
ing models to shuffle the phrases into an order that
is coherent in the target language.
An obvious advantage of statistical MT ap-
proaches is that they can adopt (often very id-
iomatic) translations of mid- to high-frequency con-
structions without requiring any language-pair spe-
cific engineering work. At the same time it is clear
that a linguistics-free approach is limited in what
it can ultimately achieve: only linguistically in-
formed systems can detect certain generalizations
from lower-frequency constructions in the data and
successfully apply them in a similar but different lin-
guistic context. Hence, the idea of ?hybrid? MT, ex-
ploiting both linguistic and statistical information is
fairly old. Here we will not consider classical, rule-
based systems with some added data-based resource
acquisition (although they may be among the best
candidates for high-quality special-purpose transla-
tion ? but adaption to new language pairs and sub-
languages is very costly for these systems). The
other form of hybridization ? a statistical MT model
that is based on a deeper analysis of the syntactic
33
structure of a sentence ? has also long been iden-
tified as a desirable objective in principle (consider
(Wu, 1997; Yamada and Knight, 2001)). However,
attempts to retrofit syntactic information into the
phrase-based paradigm have not met with enormous
success (Koehn et al, 2003; Och et al, 2003)1, and
purely phrase-based MT systems continue to outper-
form these syntax/phrase-based hybrids.
In this work, we try to make a fresh start with
syntax-based statistical MT, discarding the phrase-
based paradigm and designing a MT system from
the ground up, using syntax as our central guid-
ing star ? besides the word alignment over a par-
allel corpus. Our approach is compatible with and
can benefit substantially from rich linguistic rep-
resentations obtained from deep grammars like the
ParGram LFGs. Nevertheless, contrary to classi-
cal interlingual or deep transfer-based systems, the
generative stochastic model that drives our system
is grounded only in the cross-language word align-
ment and a surface-based phrase structure tree for
the source language and will thus degrade grace-
fully on input with parsing issues ? which we sus-
pect is an important feature for making the overall
system competitive with the highly general phrase-
based MT approach.
Preliminary evaluation of our nascent system in-
dicates that this new approach might well have the
potential to finally realize some of the promises of
syntax in statistical MT.
2 General Task
We want to build a system that can learn to translate
sentences from a source language to a destination
language. The general set-up is simple.
Firstly, we have a training corpus of paired sen-
tences f and e, where target sentence e is a gold
standard translation of source sentence f . These
sentence pairs are annotated with auxiliary informa-
tion, which can include word alignments and syntac-
tic information. We refer to these annotated sentence
pairs as complete translation objects.
Secondly, we have an evaluation corpus of source
sentences. These sentences are annotated with a sub-
set of the auxiliary information used to annotate the
1(Chiang, 2005) also reports that with his hierarchical gen-
eralization of the phrase-based approach, the addition of parser
information doesn?t lead to any improvements.
Figure 1: Example translation object.
training corpus. We refer to these partially annotated
source sentences as partial translation objects.
The task at hand: use the training corpus to learn
a procedure, through which we can successfully in-
duce a complete translation object from a partial
translation object. This is what we will define as
translation.
3 Specific Task Addressed by this Paper
Before going on to actually describe a translation
procedure (and how to induce it), we need to spec-
ify our prior assumptions about how the translation
objects will be annotated. For this paper, we want to
exploit the syntax information that we can gain from
an LFG-parser, hence we will assume the following
annotations:
(1) In the training and evaluation corpora, the
source sentences will be parsed with the XLE-
parser. The attribute-value information from LFG?s
f-structure is restructured so it is indexed by (c-
structure) tree nodes; thus a tree node can bear mul-
tiple labels for various pieces of morphological, syn-
tactic and semantic information.
(2) In the training corpus, the source and target
sentence of every translation object will be aligned
using GIZA++ (http://www.fjoch.com/).
In other words, our complete translation objects
will be aligned tree-string pairs (for instance, Fig-
ure 1), while our partial translation objects will be
trees (the tree portion of Figure 1). No other annota-
tions will be assumed for this paper.
34
Figure 2: GHKM tree equivalent of example translation object. The light gray nodes are rule nodes of the
GHKM tree.
4 Syntax MT as Tree Labeling
It is not immediately clear how one would learn a
process to map a parsed source sentence into an
aligned tree-string pair. To facilitate matters, we
will map complete translation objects to an alternate
representation. In (Galley et al, 2003), the authors
give a semantics to aligned tree-string pairs by asso-
ciating each with an annotated parse tree (hereafter
called a GHKM tree) representing a specific theory
about how the source sentence was translated into
the destination sentence.
In Figure 1, we show an example translation ob-
ject and in Figure 2, we show its associated GHKM
tree. The GHKM tree is simply the parse tree f of
the translation object, annotated with rules (hereafter
referred to as GHKM rules). We will not describe in
depth the mapping process from translation object to
GHKM tree. Suffice it to say that the alignment in-
duces a set of intuitive translation rules. Essentially,
a rule like: ?not 1 ? ne 1 pas? (see Figure 2) means:
if we see the word ?not? in English, followed by a
phrase already translated into French, then translate
the entire thing as the word ?ne? + the translated
phrase + the word ?pas.? A parse tree node gets la-
beled with one of these rules if, roughly speaking,
its span is still contiguous when projected (via the
alignment) into the target language.
The advantage of using the GHKM interpretation
of a complete translation object is that our transla-
tion task becomes simpler. Now, to induce a com-
plete translation object from a partial translation ob-
ject (parse tree), all we need to do is label the nodes
of the tree with appropriate rules. We have reduced
the vaguely defined task of translation to the con-
crete task of tree labeling.
5 The Generative Process
At the most basic level, we could design a naive gen-
erative process that takes a parse tree and then makes
a series of decisions, one for each node, about what
rule (if any) that node should be assigned. How-
ever it is a bit unreasonable to expect to learn such
a decision without breaking it down somewhat, as
there are an enormous number of rules that could po-
tentially be used to label any given parse tree node.
So let?s break this task down into simpler decisions.
Ideally, we would like to devise a generative process
consisting of decisions between a small number of
possibilities (ideally binary decisions).
We will begin by deciding, for each node, whether
or not it will be annotated with a rule. This is clearly
a binary decision. Once a generative process has
made this decision for each node, we get a conve-
nient byproduct. As seen in Figure 3, the LHS of
each rule is already determined. Hence after this se-
quence of binary decisions, half of our task is al-
ready completed.
The question remains: how do we determine the
RHS of these rules? Again, we could create a gen-
erative process that makes these decisions directly,
but choosing the RHS of a rule is still a rather wide-
open decision, so we will break it down further. For
each rule, we will begin by choosing the template of
its RHS, which is a RHS in which all sequences of
variables are replaced with an empty slot into which
variables can later be placed. For instance, the tem-
35
Figure 3: Partial GHKM tree, after rule nodes have been identified (light gray). Notice that once we identify
the rule node, the rule left-hand sides are already determined.
plate of ??ne?, x1, ?pas?? is ??ne?,X, ?pas?? and the
template of ?x3, ?,?, x1, x2? is ?X, ?,?,X?, where X
represents the empty slots.
Once the template is chosen, it simply needs to be
filled with the variables from the LHS. To do so, we
process the LHS variables, one by one. By default,
they are placed to the right of the previously placed
variable (the first variable is placed in the first slot).
We repeatedly offer the option to push the variable
to the right until the option is declined or it is no
longer possible to push it further right. If the vari-
able was not pushed right at all, we repeatedly offer
the option to push the variable to the left until the
option is declined or it is no longer possible to push
it further left. Figure 4 shows this generative story
in action for the rule RHS ?x3, ?,?, x1, x2?.
These are all of the decisions we need to make
in order to label a parse tree with GHKM rules. A
trace of this generative process for the GHKM tree
of Figure 2 is shown in Figure 5. Notice that, aside
from the template decisions, all of the decisions are
binary (i.e. feasible to learn discriminatively). Even
the template decisions are not terribly large-domain,
if we maintain a separate feature-conditional dis-
tribution for each LHS template. For instance, if
the LHS template is ??not?,X?, then RHS template
??ne?,X, ?pas?? and a few other select candidates
should bear most of the probability mass.
5.1 Training
Having established this generative story, training is
straightforward. As a first step, we can convert each
complete translation object of our training corpus
to the trace of its generative story (as in Figure 5).
Decision to make Decision RHS so far
RHS template? X , X X , X
default placement of var 1 1 , X
push var 1 right? yes X , 1
default placement of var 2 X , 1 2
push var 2 left? no X , 1 2
default placement of var 3 X , 1 2 3
push var 3 left? yes X , 1 3 2
push var 3 left? yes X , 3 1 2
push var 3 left? yes 3 , 1 2
Figure 4: Trace of the generative story for the right-
hand side of a GHKM rule.
These decisions can be annotated with whatever fea-
ture information we might deem helpful. Then we
simply divide up these feature vectors by decision
type (for instance, rule node decisions, template de-
cisions, etc.) and train a separate discriminative clas-
sifier for each decision type from the feature vectors.
This method is quite flexible, in that it allows us to
use any generic off-the-shelf classification software
to train our system. We prefer learners that produce
distributions (rather than hard classifiers) as output,
but this is not required.
5.2 Exploiting deep linguistic information
The use of discriminative classifiers makes our ap-
proach very flexible in terms of the information that
can be exploited in the labeling (or translation) pro-
cess. Any information that can be encoded as fea-
tures relative to GHKM tree nodes can be used. For
the experiments reported in this paper, we parsed
the source language side of a parallel corpus (a
small subsection of the English-German Europarl
corpus; (Koehn, 2002)) with the XLE system, using
36
the ParGram LFG grammar and applying probabilis-
tic disambiguation (Riezler et al, 2002) to obtain
a single analysis (i.e., a c-structure [phrase struc-
ture tree] and an f-structure [an associated attribute-
value matrix with morphosyntactic feature informa-
tion and a shallow semantic interpretation]) for each
sentence. A fall-back mechanism integrated in the
parser/grammar ensures that even for sentences that
do not receive a full parse, substrings are deeply
parsed and can often be treated successfully.
We convert the c-structure/f-structure represen-
tation that is based on XLE?s sophisticated word-
internal analysis into a plain phrase structure tree
representation based on the original tokens in the
source language string. The morphosyntactic fea-
ture information from f-structure is copied as addi-
tional labeling information to the relevant GHKM
tree nodes, and the f-structural dependency relation
among linguistic units is translated into a relation
among corresponding GHKM tree nodes. The rela-
tional information is then used to systematically ex-
tend the learning feature set for the tree-node based
classifiers.
In future experiments, we also plan to exploit lin-
guistic knowledge about the target language by fac-
torizing the generation of target language words into
separate generation of lemmas and the various mor-
phosyntactic features. In decoding, a morphological
generator will be used to generate a string of surface
words.
5.3 Decoding
Because we have purposely refused to make any
Markov assumptions in our model, decoding cannot
be accomplished in polynomial time. Our hypothe-
sis is that it is better to find a suboptimal solution of
a high-quality model than the optimal solution of a
poorer model. We decode through a simple search
through the space of assignments to our generative
process.
This is, potentially, a very large and intractible
search space. However, if most assignment deci-
sions can be made with relative confidence (i.e. the
classifiers we have trained make fairly certain deci-
sions), then the great majority of search nodes have
values which are inferior to those of the best so-
lutions. The standard search technique of depth-
first branch-and-bound search takes advantage of
search spaces with this particular characteristic by
first finding greedy good-quality solutions and using
their values to optimally prune a significant portion
of the search space. Depth-first branch-and-bound
search has the following advantage: it finds a good
(suboptimal) solution in linear time and continually
improves on this solution until it finds the optimal.
Thus it can be run either as an optimal decoder or as
a heuristic decoder, since we can interrupt its execu-
tion at any time to get the best solution found so far.
Additionally, it takes only linear space to run.
6 Preliminary results
In this section, we present some preliminary results
for an English-to-German translation system based
on the ideas outlined in this paper.
Our data was a subset of the Europarl corpus
consisting of sentences of lengths ranging from 8
to 17 words. Our training corpus contained 50000
sentences and our test corpus contained 300 sen-
tences. We also had a small number of reserved
sentences for development. The English sentences
were parsed with XLE, using the English ParGram
LFG grammar, and the sentences were word-aligned
with GIZA++. We used the WEKA machine learn-
ing package (Witten and Frank, 2005) to train the
distributions (specifically, we used model trees).
For comparison, we also trained and evaluated
the phrase-based MT system Pharaoh (Koehn, 2005)
on this limited corpus, using Pharaoh?s default pa-
rameters. In a different set of MT-as-Tree-Labeling
experiments, we used a standard treebank parser
trained on the PennTreebank Wall Street Journal
section. Even with this parser, which produces less
detailed information than XLE, the results are com-
petitive when assessed with quantitative measures:
Pharaoh achieved a BLEU score of 11.17 on the test
set, whereas our system achieved a BLEU score of
11.52. What is notable here is not the scores them-
selves (low due to the size of the training corpus).
However our system managed to perform compara-
bly with Pharaoh in a very early stage of its devel-
opment, with rudimentary features and without the
benefit of an n-gram language model.
For the XLE-based system we cannot include
quantitative results for the same experimental setup
at the present time. As a preliminary qualitative
37
Decision to make Decision Active features
rule node (i)? YES NT=?S?; HEAD = ?am?
rule node (ii)? YES NT=?NP?; HEAD = ?I?
rule node (iv)? NO NT=?VP?; HEAD = ?am?
rule node (v)? YES NT=?VP?; HEAD = ?am?
rule node (vi)? NO NT=?MD?; HEAD = ?am?
rule node (viii)? YES NT=?VP?; HEAD = ?going?
rule node (ix)? NO NT=?RB?; HEAD = ?not?
rule node (xi)? YES NT=?VB?; HEAD = ?going?
rule node (xiii)? YES NT=?ADJP?; HEAD = ?today?
RHS template? (i) X , X NT=?S?
push var 1 right? (i) YES VARNT=?NP?; PUSHPAST= ?,?
push var 2 left? (i) NO VARNT=?VP?; PUSHPAST= ?NP?
push var 3 left? (i) YES VARNT=?ADJP?; PUSHPAST= ?VP?
push var 3 left? (i) YES VARNT=?ADJP?; PUSHPAST= ?NP?
push var 3 left? (i) YES VARNT=?ADJP?; PUSHPAST= ?,?
RHS template? (ii) je NT=?NP?; WD=?I?
RHS template? (v) X NT=?VP?
RHS template? (viii) ne X pas NT=?VP?; WD=?not?
RHS template? (xi) vais NT=?VB?; WD=?going?
RHS template? (xiii) aujourd?hui NT=?ADJP?; WD=?today?
Figure 5: Trace of a top-down generative story for the GHKM tree in Figure 2.
evaluation, let?s take a closer look at the sentences
produced by our system, to gain some insight as to
its current strengths and weaknesses.
Starting with the English sentence (1) (note that
all data is lowercase), our system produces (2).
(1) i agree with the spirit of those amendments .
(2) ich
I
stimme
vote
die
the.FEM
geist
spirit.MASC
dieser
these
a?nderungsantra?ge
change-proposals
zu
to
.
.
The GHKM tree is depicted in Figure 6. The key
feature of this translation is how the English phrase
?agree with? is translated as the German ?stimme
... zu? construction. Such a feat is difficult to pro-
duce consistently with a purely phrase-based sys-
tem, as phrases of arbitrary length can be placed be-
tween the words ?stimme? and ?zu?, as we can see
happening in this particular example. By contrast,
Pharaoh opts for the following (somewhat less de-
sirable) translation:
(3) ich
I
stimme
vote
mit
with
dem
the.MASC
geist
spirit.MASC
dieser
these
a?nderungsantra?ge
change-proposals
.
.
A weakness in our system is also evident here.
The German noun ?Geist? is masculine, thus our
system uses the wrong article (a problem that
Pharaoh, with its embedded n-gram language model,
does not encounter).
In general, it seems that our system is superior to
Pharaoh at figuring out the proper way to arrange the
words of the output sentence, and inferior to Pharaoh
at finding what the actual translation of those words
should be.
Consider the English sentence (4). Here we have
an example of a modal verb with an embedded in-
finitival VP. In German, infinite verbs should go at
the end of the sentence, and this is achieved by our
system (translating ?shall? as ?werden?, and ?sub-
mit? as ?vorlegen?), as is seen in (5).
(4) ) we shall submit a proposal along these lines before the
end of this year .
(5) wir
we
werden
will
eine
a.FEM
vorschlag
proposal.MASC
in
in
dieser
these
haushaltslinien
budget-lines
vor
before
die
the.FEM
ende
end.NEUT
dieser
this.FEM
jahres
year.NEUT
vorlegen
submit
.
.
Pharaoh does not manage this (translating ?sub-
mit? as ?unterbreiten? and placing it mid-sentence).
(6) werden
will
wir
we
unterbreiten
submit
eine
a
vorschlag
proposal
in
in
dieser
these
haushaltslinien
budget-lines
vor
before
ende
end
dieser
this.FEM
jahr
year.NEUT
.
.
It is worth noting that while our system gets the
word order of the output system right, it makes sev-
38
Figure 6: GHKM tree output for a test sentence.
eral agreement mistakes and (like Pharaoh) doesn?t
get the translation of ?along these lines? right.
In Figure 7, we show sample translations by the
three systems under discussion for the first five sen-
tences in our evaluation set. For the LFG-based ap-
proach, we can at this point present only results for
a version trained on 10% of the sentence pairs. This
explains why more source words are left untrans-
lated. But note that despite the small training set,
the word ordering results are clearly superior for this
system: the syntax-driven rules place the untrans-
lated English words in the correct position in terms
of German syntax.
The translations with Pharaoh contain relatively
few agreement mistakes (note that it exploits a lan-
guage model of German trained on a much larger
corpus). The phrase-based approach does however
skip words and make positioning mistakes some of
which are so serious (like in the last sentence) that
they make the result hard to understand.
7 Discussion
In describing this pilot project, we have attempted
to give a ?big picture? view of the essential ideas
behind our system. To avoid obscuring the presen-
tation, we have avoided many of the implementation
details, in particular our choice of features. There
are exactly four types of decisions that we need to
train: (1) whether a parse tree node should be a rule
node, (2) the RHS template of a rule, (3) whether a
rule variable should be pushed left, and (4) whether
a rule variable should be pushed right. For each
of these decisions, there are a number of possible
features that suggest themselves. For instance, re-
call that in German, embedded infinitival verbs get
placed at the end of the sentence or clause. So
when the system is considering whether to push a
rule?s noun phrase to the left, past an existing verb,
it would be useful for it to consider (as a feature)
whether that verb is the first or second verb of its
clause and what the morphological form of the verb
is.
Even in these early stages of development, the
MT-as-Tree-Labeling system shows promise in us-
ing syntactic information flexibly and effectively for
MT. Our preliminary comparison indicates that us-
ing deep syntactic analysis leads to improved trans-
lation behavior. We hope to develop the system
into a competitive alternative to phrase-based ap-
proaches.
References
P.F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L. Mer-
cer. 1993. The mathematics of statistical machine trans-
lation: Parameter estimation. Computational Linguistics,
19(2):263?311.
Miriam Butt, Helge Dyvik, Tracy Holloway King, Hiroshi Ma-
suichi, and Christian Rohrer. 2002. The parallel gram-
39
source we believe that this is a fundamental element .
professional translation wir denken , dass dies ein grundlegender aspekt ist .
PHARAOH (50k) wir halten dies fu?r
:::
eine
::::::::::::
grundlegende element .
TL-WSJ (50k) wir glauben , dass
:::::
diesen ist ein grundlegendes element .
TL-LFG (5k) wir meinen , dass dies
:::
eine
::::::::::::
grundlegende element ist .
source it is true that lisbon is a programme for ten years .
professional translation nun ist lissabon ein programm fu?r zehn jahre .
PHARAOH (50k) es ist richtig , dass lissabon ist
:::
eine programm fu?r zehn
:::::
jahren .
TL-WSJ (50k) es ist richtig , dass lissabon ist
:::
eine programm fu?r zehn
:::::
jahren .
TL-LFG (5k) es ist true , dass lisbon
:::
eine programm fu?r zehn
:::::
jahren ist .
source i completely agree with each of these points .
professional translation ich bin mit jeder einzelnen dieser aussagen voll und ganz einverstanden .
PHARAOH (50k) ich ..... vo?llig einverstanden mit jedem dieser punkte .
TL-WSJ (50k) ich bin vo?llig mit
::::
jedes
:::::
diese fragen einer meinung .
TL-LFG (5k) ich agree completely mit
::::
jeder dieser punkte .
source however , i would like to add one point .
professional translation aber ich mo?chte gern einen punkt hinzufu?gen .
PHARAOH (50k) allerdings mo?chte ich noch eines sagen .
TL-WSJ (50k) ich mo?chte jedoch an noch einen punkt hinzufu?gen .
TL-LFG (5k) allerdings mo?chte ich einen punkt add .
source this is undoubtedly a point which warrants attention .
professional translation ohne jeden zweifel ist dies ein punkt , der aufmerksamkeit verdient .
PHARAOH (50k) das ist sicherlich
:::
eine punkt .... rechtfertigt das aufmerksamkeit .
TL-WSJ (50k) das ist ohne zweifel
::::
eine punkt ,
::
die warrants beachtung .
TL-LFG (5k) das ist undoubtedly .... sache , die attention warrants .
Figure 7: Sample translations by (1) the PHARAOH system, (2) our system with a treebank parser (TL-WSJ),
(3) our system with the XLE parser (TL-LFG). (1) and (2) were trained on 50,000 sentence pairs, (3) just
on (3) sentence pairs. Error coding:
::::::
wrong
:::::::::::::::
morphological
:::::
form, incorrectly positioned word, untranslated
source word, missed word: ...., extra word.
mar project. In Proceedings of COLING-2002 Workshop on
Grammar Engineering and Evaluation, pages 1?7.
David Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proceedings of ACL, pages
263?270.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2003. What?s in a translation rule? In Proc. NAACL.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Sta-
tistical phrase-based translation. In Proceedings of the Hu-
man Language Technology Conference 2003 (HLT-NAACL
2003), Edmonton, Canada.
Philipp Koehn. 2002. Europarl: A multilingual corpus for eval-
uation of machine translation. Ms., University of Southern
California.
Philipp Koehn. 2005. Pharaoh: a beam search decoder for
phrase-based statistical machine translation models. In Pro-
ceedings of the Sixth Conference of the Association for Ma-
chine Translation in the Americas, pages 115?124.
Franz Josef Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51.
F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Yamada,
A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng, Viren
Jain, Z.Jin, and D. Radev. 2003. Syntax for statistical ma-
chine translation. Technical report, Center for Language and
Speech Processing, Johns Hopkins University, Baltimore.
Summer Workshop Final Report.
Stefan Riezler, Dick Crouch, Ron Kaplan, Tracy King, John
Maxwell, and Mark Johnson. 2002. Parsing the Wall Street
Journal using a Lexical-Functional Grammar and discrim-
inative estimation techniques. In Proceedings of the 40th
Annual Meeting of the Association for Computational Lin-
guistics (ACL?02), Pennsylvania, Philadelphia.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Practical
machine learning tools and techniques. Morgan Kaufmann.
Dekai Wu. 1997. Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora. Computational
Linguistics, 23(3):377?403.
Kenji Yamada and Kevin Knight. 2001. A syntax-based statis-
tical translation model. In Proceedings of the 39th Annual
Meeting of the Association for Computational Linguistics,
pages 523?530.
40
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 62?71,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Cube Pruning as Heuristic Search
Mark Hopkins and Greg Langmead
Language Weaver, Inc.
4640 Admiralty Way, Suite 1210
Marina del Rey, CA 90292
{mhopkins,glangmead}@languageweaver.com
Abstract
Cube pruning is a fast inexact method for
generating the items of a beam decoder.
In this paper, we show that cube pruning
is essentially equivalent to A* search on a
specific search space with specific heuris-
tics. We use this insight to develop faster
and exact variants of cube pruning.
1 Introduction
In recent years, an intense research focus on ma-
chine translation (MT) has raised the quality of
MT systems to the degree that they are now viable
for a variety of real-world applications. Because
of this, the research community has turned its at-
tention to a major drawback of such systems: they
are still quite slow. Recent years have seen a flurry
of innovative techniques designed to tackle this
problem. These include cube pruning (Chiang,
2007), cube growing (Huang and Chiang, 2007),
early pruning (Moore and Quirk, 2007), clos-
ing spans (Roark and Hollingshead, 2008; Roark
and Hollingshead, 2009), coarse-to-fine methods
(Petrov et al, 2008), pervasive laziness (Pust and
Knight, 2009), and many more.
This massive interest in speed is bringing rapid
progress to the field, but it comes with a certain
amount of baggage. Each technique brings its own
terminology (from the cubes of (Chiang, 2007)
to the lazy lists of (Pust and Knight, 2009)) into
the mix. Often, it is not entirely clear why they
work. Many apply only to specialized MT situ-
ations. Without a deeper understanding of these
methods, it is difficult for the practitioner to com-
bine them and adapt them to new use cases.
In this paper, we attempt to bring some clarity
to the situation by taking a closer look at one of
these existing methods. Specifically, we cast the
popular technique of cube pruning (Chiang, 2007)
in the well-understood terms of heuristic search
(Pearl, 1984). We show that cube pruning is essen-
tially equivalent to A* search on a specific search
space with specific heuristics. This simple obser-
vation affords a deeper insight into how and why
cube pruning works. We show how this insight en-
ables us to easily develop faster and exact variants
of cube pruning for tree-to-string transducer-based
MT (Galley et al, 2004; Galley et al, 2006; DeN-
ero et al, 2009).
2 Motivating Example
We begin by describing the problem that cube
pruning addresses. Consider a synchronous
context-free grammar (SCFG) that includes the
following rules:
A ? ?A 0 B 1 , A 0 B 1 ? (1)
B ? ?A 0 B 1 , B 1 A 0 ? (2)
A ? ?B 0 A 1 , c B 0 b A 1 ? (3)
B ? ?B 0 A 1 , B 0 A 1 ? (4)
Figure 1 shows CKY decoding in progress. CKY
is a bottom-up algorithm that works by building
objects known as items, over increasingly larger
spans of an input sentence (in the context of SCFG
decoding, the items represent partial translations
of the input sentence). To limit running time, it is
common practice to keep only the n ?best? items
per span (this is known as beam decoding). At
this point in Figure 1, every span of size 2 or less
has already been filled, and now we want to fill
span [2, 5] with the n items of lowest cost. Cube
pruning addresses the problem of how to compute
the n-best items efficiently.
We can be more precise if we introduce some
terminology. An SCFG rule has the form X ?
??, ?,??, where X is a nonterminal (called the
postcondition), ?, ? are strings that may contain
terminals and nonterminals, and ? is a 1-1 corre-
spondence between equivalent nonterminals of ?
and ?.
62
Figure 1: CKY decoding in progress. We want to
fill span [2,5] with the lowest cost items.
Usually SCFG rules are represented like the ex-
ample rules (1)-(4). The subscripts indicate cor-
responding nonterminals (according to ?). Define
the preconditions of a rule as the ordered sequence
of its nonterminals. For clarity of presentation, we
will henceforth restrict our focus to binary rules,
i.e. rules of the form: Z ? ?X 0 Y 1 , ??. Observe
that all the rules of our example are binary rules.
An item is a triple that contains a span and two
strings. We refer to these strings as the postcon-
dition and the carry, respectively. The postcon-
dition tells us which rules may be applied to the
item. The carry gives us extra information re-
quired to correctly score the item (in SCFG decod-
ing, typically it consists of boundary words for an
n-gram language model). 1 To flatten the notation,
we will generally represent items as a 4-tuple, e.g.
[2, 4,X, a ? b].
In CKY, new items are created by applying rules
to existing items:
r : Z ? ?X 0 Y 1 , ?? [?, ?,X, ?1] [?, ?,Y, ?2]
[?, ?,Z, carry(r, ?
1
, ?
2
)]
(5)
In other words, we are allowed to apply a
rule r to a pair of items ?
1
, ?
2
if the item
spans are complementary and preconditions(r) =
?postcondition(?
1
), postcondition(?
2
)?. The new
item has the same postcondition as the applied
rule. We form the carry for the new item through
an application-dependent function carry that com-
bines the carries of its subitems (e.g. if the carry is
n-gram boundary words, then carry computes the
1Note that the carry is a generic concept that can store any
kind of non-local scoring information.
new boundary words). As a shorthand, we intro-
duce the notation ?
1
? r ? ?
2
to describe an item
created by applying formula (5) to rule r and items
?
1
, ?
2
.
When we create a new item, it is scored using
the following formula: 2
cost(?
1
? r ? ?
2
) , cost(r)
+ cost(?
1
)
+ cost(?
2
)
+ interaction(r, ?
1
, ?
2
)
(6)
We assume that each grammar rule r has an
associated cost, denoted cost(r). The interac-
tion cost, denoted interaction(r, ?
1
, ?
2
), uses the
carry information to compute cost components
that cannot be incorporated offline into the rule
costs (again, for our purposes, this is a language
model score).
Cube pruning addresses the problem of effi-
ciently computing the n items of lowest cost for
a given span.
3 Item Generation as Heuristic Search
Refer again to the example in Figure 1. We want to
fill span [2,5]. There are 26 distinct ways to apply
formula (5), which result in 10 unique items. One
approach to finding the lowest-cost n items: per-
form all 26 distinct inferences, compute the cost of
the 10 unique items created, then choose the low-
est n.
The 26 different ways to form the items can be
structured as a search tree. See Figure 2. First
we choose the subspans, then the rule precondi-
tions, then the rule, and finally the subitems. No-
tice that this search space is already quite large,
even for such a simple example. In a realistic situ-
ation, we are likely to have a search tree with thou-
sands (possibly millions) of nodes, and we may
only want to find the best 100 or so goal nodes.
To explore this entire search space seems waste-
ful. Can we do better?
Why not perform heuristic search directly on
this search space to find the lowest-cost n items?
In order to do this, we just need to add heuristics
to the internal nodes of the space.
Before doing so, it will help to elaborate on
some of the details of the search tree. Let
rules(X,Y) be the subset of rules with precondi-
tions ?X,Y?, sorted by increasing cost. Similarly,
2Without loss of generality, we assume an additive cost
function.
63
Figure 2: Item creation, structured as a search
space. rule(X,Y, k) denotes the kth lowest-cost
rule with preconditions ?X,Y?. item(?, ?,X, k)
denotes the kth lowest-cost item of span [?, ?]
with postcondition X.
let items(?, ?,X) be the subset of items with span
[?, ?] and postcondition X, also sorted by increas-
ing cost. Finally, let rule(X,Y, k) denote the kth
rule of rules(X,Y) and let item(?, ?,X, k) denote
the kth item of items(?, ?,X).
A path through the search tree consists of the
following sequence of decisions:
1. Set i, j, k to 1.
2. Choose the subspans: [?, ?], [?, ?].
3. Choose the first precondition X of the rule.
4. Choose the second precondition Y of the
rule.
5. While rule not yet accepted and i <
|rules(X,Y)|:
(a) Choose to accept/reject rule(X,Y, i). If
reject, then increment i.
6. While item not yet accepted for subspan
[?, ?] and j < |items(?, ?,X)|:
(a) Choose to accept/reject item(?, ?,X, j).
If reject, then increment j.
7. While item not yet accepted for subspan [?, ?]
and k < |items(?, ?,Y)|:
(a) Choose to accept/reject item(?, ?,Y, k).
If reject, then increment k.
Figure 3: The lookahead heuristic. We set the
heuristics for rule and item nodes by looking
ahead at the cost of the greedy solution from that
point in the search space.
Figure 2 shows two complete search paths for
our example, terminated by goal nodes (in black).
Notice that the internal nodes of the search space
can be classified by the type of decision they
govern. To distinguish between these nodes, we
will refer to them as subspan nodes, precondition
nodes, rule nodes, and item nodes.
We can now proceed to attach heuristics to the
nodes and run a heuristic search protocol, say A*,
on this search space. For subspan and precondition
nodes, we attach trivial uninformative heuristics,
i.e. h = ??. For goal nodes, the heuristic is the
actual cost of the item they represent. For rule and
item nodes, we will use a simple type of heuristic,
often referred to in the literature as a lookahead
heuristic. Since the rule nodes and item nodes are
ordered, respectively, by rule and item cost, it is
possible to ?look ahead? at a greedy solution from
any of those nodes. See Figure 3. This greedy so-
lution is reached by choosing to accept every de-
cision presented until we hit a goal node.
If these heuristics were admissible (i.e. lower
bounds on the cost of the best reachable goal
node), this would enable us to exactly generate the
n-best items without exhausting the search space
(assuming the heuristics are strong enough for A*
to do some pruning). Here, the lookahead heuris-
tics are clearly not admissible, however the hope
is that A* will generate n ?good? items, and that
the time savings will be worth sacrificing exact-
ness for.
64
4 Cube Pruning as Heuristic Search
In this section, we will compare cube pruning with
our A* search protocol, by tracing through their
respective behaviors on the simple example of Fig-
ure 1.
4.1 Phase 1: Initialization
To fill span [?, ?], cube pruning (CP) begins by
constructing a cube for each tuple of the form:
?[?, ?], [?, ?], X , Y?
where X and Y are nonterminals. A cube consists
of three axes: rules(X,Y) and items(?, ?,X) and
items(?, ?,Y). Figure 4(left) shows the nontrivial
cubes for our example scenario.
Contrast this with A*, which begins by adding
the root node of our search space to an empty heap
(ordered by heuristic cost). It proceeds to repeat-
edly pop the lowest-cost node from the heap, then
add its children to the heap (we refer to this op-
eration as visiting the node). Note that before A*
ever visits a rule node, it will have visited every
subspan and precondition node (because they all
have cost h = ??). Figure 4(right) shows the
state of A* at this point in the search. We assume
that we do not generate dead-end nodes (a simple
matter of checking that there exist applicable rules
and items for the chosen subspans and precondi-
tions). Observe the correspondence between the
cubes and the heap contents at this point in the A*
search.
4.2 Phase 2: Seeding the Heap
Cube pruning proceeds by computing the ?best?
item of each cube ?[?, ?], [?, ?], X , Y?, i.e.
item(?, ?,X, 1)? rule(X,Y, 1)? item(?, ?,Y, 1)
Because of the interaction cost, there is no guaran-
tee that this will really be the best item of the cube,
however it is likely to be a good item because the
costs of the individual components are low. These
items are added to a heap (to avoid confusion, we
will henceforth refer to the two heaps as the CP
heap and the A* heap), and prioritized by their
costs.
Consider again the example. CP seeds its heap
with the ?best? items of the 4 cubes. There is now
a direct correspondence between the CP heap and
the A* heap. Moreover, the costs associated with
the heap elements also correspond. See Figure 5.
4.3 Phase 3: Finding the First Item
Cube pruning now pops the lowest-cost item from
the CP heap. This means that CP has decided to
keep the item. After doing so, it forms the ?one-
off? items and pushes those onto the CP heap. See
Figure 5(left). The popped item is:
item (viii) ? rule (1) ? item (xii)
CP then pushes the following one-off successors
onto the CP heap:
item (viii) ? rule (2) ? item (xii)
item (ix) ? rule (1) ? item (xii)
item (viii) ? rule (1) ? item (xiii)
Contrast this with A*, which pops the lowest-
cost search node from the A* heap. Here we need
to assume that our A* protocol differs slightly
from standard A*. Specifically, it will practice
node-tying, meaning that when it visits a rule node
or an item node, then it also (atomically) visits all
nodes on the path to its lookahead goal node. See
Figure 5(right). Observe that all of these nodes
have the same heuristic cost, thus standard A* is
likely to visit these nodes in succession without
the need to enforce node-tying, but it would not
be guaranteed (because the heuristics are not ad-
missible). A* keeps the goal node it finds and adds
the successors to the heap, scored with their looka-
head heuristics. Again, note the direct correspon-
dence between what CP and A* keep, and what
they add to their respective heaps.
4.4 Phase 4: Finding Subsequent Items
Cube pruning and A* continue to repeat Phase
3 until k unique items have been kept. While
we could continue to trace through the example,
by now it should be clear: cube pruning and our
A* protocol with node-tying are doing the same
thing at each step. In fact, they are exactly the
same algorithm. We do not present a formal proof
here; this statement should be regarded as confi-
dent conjecture.
The node-tying turns out to be an unnecessary
artifact. In our early experiments, we discovered
that node-tying has no impact on speed or qual-
ity. Hence, for the remainder of the paper, we
view cube pruning in very simple terms: as noth-
ing more than standard A* search on the search
space of Section 3.
65
Figure 4: (left) Cube formation for our example. (right) The A* protocol, after all subspan and precon-
dition nodes have been visited. Notice the correspondence between the cubes and the A* heap contents.
Figure 5: (left) One step of cube pruning. (right) One step of the A* protocol. In this figure,
cost(r, ?
1
, ?
2
) , cost(?
1
? r ? ?
2
).
66
5 Augmented Cube Pruning
Viewed in this light, the idiosyncracies of cube
pruning begin to reveal themselves. On the one
hand, rule and item nodes are associated with
strong but inadmissible heuristics (the short expla-
nation for why cube pruning is an inexact algo-
rithm). On the other hand, subspan and precondi-
tion nodes are associated with weak trivial heuris-
tics. This should be regarded neither as a surprise
nor a criticism, considering cube pruning?s origins
in hierarchical phrase-based MT models (Chiang,
2007), which have only a small number of distinct
nonterminals.
But the situation is much different in tree-
to-string transducer-based MT (Galley et al,
2004; Galley et al, 2006; DeNero et al, 2009).
Transducer-based MT relies on SCFGs with large
nonterminal sets. Binarizing the grammars (Zhang
et al, 2006) further increases the size of these sets,
due to the introduction of virtual nonterminals.
A key benefit of the heuristic search viewpoint
is that it is well positioned to take advantage of
such insights into the structure of a particular de-
coding problem. In the case of transducer-based
MT, the large set of preconditions encourages us
to introduce a nontrivial heuristic for the precon-
dition nodes. The inclusion of these heuristics into
the CP search will enable A* to eliminate cer-
tain preconditions from consideration, giving us a
speedup. For this reason we call this strategy aug-
mented cube pruning.
5.1 Heuristics on preconditions
Recall that the total cost of a goal node is given by
Equation (6), which has four terms. We will form
the heuristic for a precondition node by creating
a separate heuristic for each of the four terms and
using the sum as the overall heuristic.
To describe these heuristics, we will make intu-
itive use of the wildcard operator ? to extend our
existing notation. For instance, items(?, ?, *) will
denote the union of items(?, ?,X) over all possi-
ble X, sorted by cost.
We associate the heuristic h(?,X,Y) with the
search node reached by choosing subspans [?, ?],
[?, ?], precondition X (for span [?, ?]), and precon-
dition Y (for span [?, ?]). The heuristic is the sum
of four terms, mirroring Equation (6):
h(?,X,Y) = cost(rule(X,Y, 1))
+ cost(item(?, ?,X, 1))
+ cost(item(?, ?,Y, 1))
+ ih(?,X,Y)
The first three terms are admissible because
each is simply the minimum possible cost of
some choice remaining to be made. To con-
struct the interaction heuristic ih(?,X,Y), con-
sider that in a translation model with an inte-
grated n-gram language model, the interaction
cost interaction(r, ?
1
, ?
2
) is computed by adding
the language model costs of any new complete n-
grams that are created by combining the carries
(boundary words) with each other and with the
lexical items on the rule?s target side, taking into
account any reordering that the rule may perform.
We construct a backoff-style estimate of these
new n-grams by looking at item(?, ?,X, 1) =
[?, ?,X, ?
1
], item(?, ?,Y, 1) = [?, ?,Y, ?
2
], and
rule(X,Y, 1). We set ih(?,X,Y) to be a linear
combination of the backoff n-grams of the carries
?
1
and ?
2
, as well as any n-grams introduced by
the rule. For instance, if
?
1
= a b ? c d
?
2
= e f ? g h
rule(X,Y, 1) = Z ? ?X 0 Y 1 , X 0 g h i Y 1 ?
then
ih(?,X,Y) = ?
1
? LM(a) + ?
2
? LM(a b)
+ ?
1
? LM(e) + ?
2
? LM(e f)
+ ?
1
? LM(g) + ?
2
? LM(g h)
+ ?
3
? LM(g h i)
The coefficients of the combination are free pa-
rameters that we can tune to trade off between
more pruning and more admissability. Setting the
coefficients to zero gives perfect admissibility but
is also weak.
The heuristic for the first precondition node is
computed similarly:
h(?,X, ?) = cost(rule(X, ?, 1))
+ cost(item(?, ?,X, 1))
+ cost(item(?, ?, ?, 1))
+ ih(?,X, ?)
67
Standard CP Augmented CP
nodes (k) BLEU time nodes (k) BLEU time
80 34.9 2.5 52 34.7 1.9
148 36.1 3.9 92 35.9 2.4
345 37.2 7.9 200 37.3 5.4
520 37.7 13.4 302 37.7 8.5
725 38.2 17.1 407 38.0 10.7
1092 38.3 27.1 619 38.2 16.3
1812 38.6 45.9 1064 38.5 27.7
Table 1: Results of standard and augmented cube
pruning. The number of (thousands of) search
nodes visited is given along with BLEU and av-
erage time to decode one sentence, in seconds.
0 500000 1x1
06
1.
5
x1
06 2
x1
06
Search nodes visited
35
36
37
38
BL
EU
S
tandard C
P
A
ugme
nt
e
d C
P
Figure 6: Nodes visited by standard and aug-
mented cube pruning.
We also apply analogous heuristics to the subspan
nodes.
5.2 Experimental setup
We evaluated all of the algorithms in this paper on
a syntax-based Arabic-English translation system
based on (Galley et al, 2006), with rules extracted
from 200 million words of parallel data from NIST
2008 and GALE data collections, and with a 4-
gram language model trained on 1 billion words
of monolingual English data from the LDC Giga-
word corpus. We evaluated the system?s perfor-
mance on the NIST 2008 test corpus, which con-
sists of 1357 Arabic sentences from a mixture of
newswire and web domains, with four English ref-
erence translations. We report BLEU scores (Pa-
pineni et al, 2002) on untokenized, recapitalized
output.
5.3 Results for Augmented Cube Pruning
The results for augmented cube pruning are com-
pared against cube pruning in Table 1. The data
0 10 20 30 40 50
A
verage time per sentence (s)
35
36
37
38
BL
EU
S
tandard C
P
A
ugme
nt
e
d C
P
Figure 7: Time spent by standard and augmented
cube pruning, average seconds per sentence.
Standard CP Augmented CP
subspan 12936 12792
precondition 851458 379954
rule 33734 33331
item 119703 118889
goal 74618 74159
TOTAL 1092449 619125
BLEU 38.33 38.22
Table 2: Breakdown of visited search nodes by
type (for a fixed beam size).
from that table are also plotted in Figure 6 and
Figure 7. Each line gives the number of nodes
visited by the heuristic search, the average time
to decode one sentence, and the BLEU of the out-
put. The number of items kept by each span (the
beam) is increased in each subsequent line of the
table to indicate how the two algorithms differ at
various beam sizes. This also gives a more com-
plete picture of the speed/BLEU tradeoff offered
by each algorithm. Because the two algorithms
make the same sorts of lookahead computations
with the same implementation, they can be most
directly compared by examining the number of
visited nodes. Augmenting cube pruning with ad-
missible heuristics on the precondition nodes leads
to a substantial decrease in visited nodes, by 35-
44%. The reduction in nodes converges to a con-
sistent 40% as the beam increases. The BLEU
with augmented cube pruning drops by an average
of 0.1 compared to standard cube pruning. This is
due to the additional inadmissibility of the interac-
tion heuristic.
To see in more detail how the heuristics affect
the search, we give in Table 2 the number of nodes
of each type visited by both variants for one beam
68
size. The precondition heuristic enables A* to
prune more than half the precondition nodes.
6 Exact Cube Pruning
Common wisdom is that the speed of cube prun-
ing more than compensates for its inexactness (re-
call that this inexactness is due to the fact that it
uses A* search with inadmissible heuristics). Es-
pecially when we move into transducer-based MT,
the search space becomes so large that brute-force
item generation is much too slow to be practi-
cal. Still, within the heuristic search framework
we may ask the question: is it possible to apply
strictly admissible heuristics to the cube pruning
search space, and in so doing, create a version of
cube pruning that is both fast and exact, one that
finds the n best items for each span and not just
n good items? One might not expect such a tech-
nique to outperform cube pruning in practice, but
for a given use case, it would give us a relatively
fast way of assessing the BLEU drop incurred by
the inexactness of cube pruning.
Recall again that the total cost of a goal node
is given by Equation (6), which has four terms. It
is easy enough to devise strong lower bounds for
the first three of these terms by extending the rea-
soning of Section 5. Table 3 shows these heuris-
tics. The major challenge is to devise an effective
lower bound on the fourth term of the cost func-
tion, the interaction heuristic, which in our case is
the incremental language model cost.
We take advantage of the following observa-
tions:
1. In a given span, many boundary word pat-
terns are repeated. In other words, for a par-
ticular span [?, ?] and carry ?, we often see
many items of the form [?, ?,X, ?], where
the only difference is the postcondition X.
2. Most rules do not introduce lexical items. In
other words, most of the grammar rules have
the form Z ? ?X
0
Y
1
, X
0
Y
1
? (concatena-
tion rules) or Z ? ?X
0
Y
1
, Y
1
X
0
? (inver-
sion rules).
The idea is simple. We split the search into three
searches: one for concatenation rules, one for in-
version rules, and one for lexical rules. Each
search finds the n?best items that can be created
using its respective set of rules. We then take these
3n items and keep the best n.
10 20 30 40 50 60 70
A
verage time per sentence (s)
35
36
37
38
BL
EU
S
tandard C
P
E
x
act C
P
Figure 8: Time spent by standard and exact cube
pruning, average seconds per sentence.
Doing this split enables us to precompute a
strong and admissible heuristic on the interaction
cost. Namely, for a given span [?, ?], we pre-
compute ih
adm
(?,X,Y), which is the best LM
cost of combining carries from items(?, ?,X)
and items(?, ?,Y). Notice that this statistic is
only straightforward to compute once we can as-
sume that the rules are concatenation rules or
inversion rules. For the lexical rules, we set
ih
adm
(?,X,Y) = 0, an admissible but weak
heuristic that we can fortunately get away with be-
cause of the small number of lexical rules.
6.1 Results for Exact Cube Pruning
Computing the ih
adm
(?,X,Y) heuristic is not
cheap. To be fair, we first compare exact CP to
standard CP in terms of overall running time, in-
cluding the computational cost of this overhead.
We plot this comparison in Figure 8. Surprisingly,
the time/quality tradeoff of exact CP is extremely
similar to standard CP, suggesting that exact cube
pruning is actually a practical alternative to stan-
dard CP, and not just of theoretical value. We
found that the BLEU loss of standard cube prun-
ing at moderate beam sizes was between 0.4 and
0.6.
Another surprise comes when we contrast the
number of visited search nodes of exact CP and
standard CP. See Figure 9. While we initially ex-
pected that exact CP must visit fewer nodes to
make up for the computational overhead of its ex-
pensive heuristics, this did not turn out to be the
case, suggesting that the computational cost of
standard CP?s lookahead heuristics is just as ex-
pensive as the precomputation of ih
adm
(?,X,Y).
69
heuristic components
subspan precondition1 precondition2 rule item1 item2
h(?) h(?,X) h(?,X,Y) h(?,X,Y, i) h(?,X,Y, i, j) h(?,X,Y, i, j, k)
r rule(?, ?, 1) rule(X, ?, 1) rule(X,Y, 1) rule(X,Y, i) rule(X,Y, i) rule(X,Y, i)
?
1
item(?, ?, ?, 1) item(?, ?,X, 1) item(?, ?,X, 1) item(?, ?,X, 1) item(?, ?,X, j) item(?, ?,X, j)
?
2
item(?, ?, ?, 1) item(?, ?, ?, 1) item(?, ?,Y, 1) item(?, ?,Y, 1) item(?, ?,Y, 1) item(?, ?,Y, k)
ih ih
adm
(?, ?, ?) ih
adm
(?,X, ?) ih
adm
(?,X,Y) ih
adm
(?,X,Y) ih
adm
(?,X,Y) ih
adm
(?,X,Y)
Table 3: Admissible heuristics for exact CP. We attach heuristic h(?,X,Y, i, j, k) to the search node
reached by choosing subspans [?, ?], [?, ?], preconditions X and Y, the ith rule of rules(X,Y), the jth
item of item(?, ?,X), and the kth item of item(?, ?,Y). To form the heuristic for a particular type of
search node (column), compute the following: cost(r) + cost(?
1
) + cost(?
2
) + ih
500000 1x1
06
1.
5
x1
06 2
x1
06
Search nodes visited
35
36
37
38
BL
EU
S
tandard C
P
E
x
act C
P
Figure 9: Nodes visited by standard and exact
cube pruning.
7 Implications
This paper?s core idea is the utility of framing
CKY item generation as a heuristic search prob-
lem. Once we recognize cube pruning as noth-
ing more than A* on a particular search space
with particular heuristics, this deeper understand-
ing makes it easy to create faster and exact vari-
ants for other use cases (in this paper, we focus
on tree-to-string transducer-based MT). Depend-
ing on one?s own particular use case, a variety of
possibilities may present themselves:
1. What if we try different heuristics? In this pa-
per, we do some preliminary inquiry into this
question, but it should be clear that our minor
changes are just the tip of the iceberg. One
can easily imagine clever and creative heuris-
tics that outperform the simple ones we have
proposed here.
2. What if we try a different search space? Why
are we using this particular search space?
Perhaps a different one, one that makes de-
cisions in a different order, would be more
effective.
3. What if we try a different search algorithm?
A* has nice guarantees (Dechter and Pearl,
1985), but it is space-consumptive and it is
not anytime. For a use case where we would
like a finer-grained speed/quality tradeoff, it
might be useful to consider an anytime search
algorithm, like depth-first branch-and-bound
(Zhang and Korf, 1995).
By working towards a deeper and unifying under-
standing of the smorgasbord of current MT speed-
up techniques, our hope is to facilitate the task of
implementing such methods, combining them ef-
fectively, and adapting them to new use cases.
Acknowledgments
We would like to thank Abdessamad Echihabi,
Kevin Knight, Daniel Marcu, Dragos Munteanu,
Ion Muslea, Radu Soricut, Wei Wang, and the
anonymous reviewers for helpful comments and
advice. Thanks also to David Chiang for the use
of his LaTeX macros. This work was supported in
part by CCS grant 2008-1245117-000.
References
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Rina Dechter and Judea Pearl. 1985. Generalized best-
first search strategies and the optimality of a*. Jour-
nal of the ACM, 32(3):505?536.
John DeNero, Mohit Bansal, Adam Pauls, and Dan
Klein. 2009. Efficient parsing for transducer gram-
mars. In Proceedings of the Human Language Tech-
nology Conference of the NAACL, Main Conference.
70
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proceedings of HLT/NAACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic models. In Proceedings of
ACL-COLING.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proceedings of ACL.
Robert C. Moore and Chris Quirk. 2007. Faster
beam-search decoding for phrasal statistical ma-
chine translation. In Proceedings of MT Summit XI.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318.
Judea Pearl. 1984. Heuristics. Addison-Wesley.
Slav Petrov, Aria Haghighi, and Dan Klein. 2008.
Coarse-to-fine syntactic machine translation using
language projections. In Proceedings of EMNLP.
Michael Pust and Kevin Knight. 2009. Faster mt de-
coding through pervasive laziness. In Proceedings
of NAACL.
Brian Roark and Kristy Hollingshead. 2008. Classi-
fying chart cells for quadratic complexity context-
free inference. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics
(Coling 2008), pages 745?752.
Brian Roark and Kristy Hollingshead. 2009. Lin-
ear complexity context-free parsing pipelines via
chart constraints. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 647?655,
Boulder, Colorado, June. Association for Computa-
tional Linguistics.
Weixiong Zhang and Richard E. Korf. 1995. Perfor-
mance of linear-space search algorithms. Artificial
Intelligence, 79(2):241?292.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for ma-
chine translation. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, Main
Conference, pages 256?263.
71
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 646?655,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
SCFG Decoding Without Binarization
Mark Hopkins and Greg Langmead
SDL Language Weaver, Inc.
6060 Center Drive, Suite 150
Los Angeles, CA 90045
{mhopkins,glangmead}@languageweaver.com
Abstract
Conventional wisdom dictates that syn-
chronous context-free grammars (SCFGs)
must be converted to Chomsky Normal Form
(CNF) to ensure cubic time decoding. For ar-
bitrary SCFGs, this is typically accomplished
via the synchronous binarization technique of
(Zhang et al, 2006). A drawback to this ap-
proach is that it inflates the constant factors as-
sociated with decoding, and thus the practical
running time. (DeNero et al, 2009) tackle this
problem by defining a superset of CNF called
Lexical Normal Form (LNF), which also sup-
ports cubic time decoding under certain im-
plicit assumptions. In this paper, we make
these assumptions explicit, and in doing so,
show that LNF can be further expanded to
a broader class of grammars (called ?scope-
3?) that also supports cubic-time decoding.
By simply pruning non-scope-3 rules from a
GHKM-extracted grammar, we obtain better
translation performance than synchronous bi-
narization.
1 Introduction
At the heart of bottom-up chart parsing (Younger,
1967) is the following combinatorial problem. We
have a context-free grammar (CFG) rule (for in-
stance, S ? NP VP PP) and an input sentence of
length n (for instance, ?on the fast jet ski of mr
smith?). During chart parsing, we need to apply the
rule to all relevant subspans of the input sentence.
See Figure 1. For this particular rule, there are
(n+1
4
)
application contexts, i.e. ways to choose the sub-
spans. Since the asymptotic running time of chart
parsing is at least linear in this quantity, it will take
on the fast jet ski of mr smithNP VP PPNP VP PP
NP VP PP
NP VP PPNP VP PP
choice point choice point choice point choice point
?
?
Figure 1: A demonstration of application contexts. There
are
(n+1
4
)
application contexts for the CFG rule ?S? NP
VP PP?, where n is the length of the input sentence.
at least O(
(n+1
4
)
) = O(n4) time if we include this
rule in our grammar.
Fortunately, we can take advantage of the fact that
any CFG has an equivalent representation in Chom-
sky Normal Form (CNF). In CNF, all rules have
the form X ? Y Z or X ? x, where x is a termi-
nal and X, Y, Z are nonterminals. If a rule has the
form X ? Y Z, then there are only
(n+1
3
)
applica-
tion contexts, thus the running time of chart parsing
is O(
(n+1
3
)
) = O(n3) when applied to CNF gram-
mars.
A disadvantage to CNF conversion is that it in-
creases both the overall number of rules and the
overall number of nonterminals. This inflation of
the ?grammar constant? does not affect the asymp-
totic runtime, but can have a significant impact on
the performance in practice. For this reason, (DeN-
646
the NPB of NNP
on the fast jet ski of mr smiththe NPB of NNP
on the fast jet ski of mr smiththe JJ NPB of NNPthe JJ NPB of NNPthe JJ NPB of NNPthe JJ NPB of NNP
choice point
choice point choice point
Figure 2: A demonstration of application contexts for
rules with lexical anchors. There are O(n) application
contexts for CFG rule ?S ? the NPB of NNP?, and
O(n2) application contexts for CFG rule ?S ? the JJ
NPB of NNP?, if we assume that the input sentence has
length n and contains no repeated words.
ero et al, 2009) provide a relaxation of CNF called
Lexical Normal Form (LNF). LNF is a superclass of
CNF that also allows rules whose right-hand sides
have no consecutive nonterminals. The intuition is
that the terminals provide anchors that limit the ap-
plicability of a given rule. For instance, consider the
rule NP? the NPB of NNP. See Figure 2. Because
the terminals constrain our choices, there are only
two different application contexts. The implicit as-
sumption is that input sentences will not repeat the
same word more than a small constant number of
times. If we make the explicit assumption that all
words of an input sentence are unique, then there
are O(n2) application contexts for a ?no consecu-
tive nonterminals? rule. Thus under this assumption,
the running time of chart parsing is stillO(n3) when
applied to LNF grammars.
But once we make this assumption explicit, it be-
comes clear that we can go even further than LNF
and still maintain the cubic bound on the runtime.
Consider the rule NP ? the JJ NPB of NNP. This
rule is not LNF, but there are still only O(n2) ap-
plication contexts, due to the anchoring effect of the
terminals. In general, for a rule of the form X? ?,
there are at most O(np) application contexts, where
p is the number of consecutive nonterminal pairs in
the string X ??? X (where X is an arbitrary nontermi-
nal). We refer to p as the scope of a rule. Thus chart
parsing runs in time O(nscope(G)), where scope(G)
is the maximum scope of any of the rules in CFG G.
Specifically, any scope-3 grammar can be decoded
in cubic time.
Like (DeNero et al, 2009), the target of our in-
terest is synchronous context-free grammar (SCFG)
decoding with rules extracted using the GHKM al-
gorithm (Galley et al, 2004). In practice, it turns out
that only a small percentage of the lexical rules in
our system have scope greater than 3. By simply re-
moving these rules from the grammar, we can main-
tain the cubic running time of chart parsing without
any kind of binarization. This has three advantages.
First, we do not inflate the grammar constant. Sec-
ond, unlike (DeNero et al, 2009), we maintain the
synchronous property of the grammar, and thus can
integrate language model scoring into chart parsing.
Finally, a system without binarized rules is consid-
erably simpler to build and maintain. We show that
this approach gives us better practical performance
than a mature system that binarizes using the tech-
nique of (Zhang et al, 2006).
2 Preliminaries
Assume we have a global vocabulary of symbols,
containing the reserved substitution symbol ?. De-
fine a sentence as a sequence of symbols. We will
typically use space-delimited quotations to represent
example sentences, e.g. ?the fast jet ski? rather than
?the, fast, jet, ski?. We will use the dot operator to
represent the concatenation of sentences, e.g. ?the
fast? ? ?jet ski? = ?the fast jet ski?.
Define the rank of a sentence as the count
of its ? symbols. We will use the no-
tation SUB(s, s1, ..., sk) to denote the substitu-
tion of k sentences s1, ..., sk into a k-rank sen-
tence s. For instance, if s = ?the ? ? of
??, then SUB(s, ?fast?, ?jet ski?, ?mr smith?) =
?the fast jet ski of mr smith?.
To refer to a subsentence, define a span as a pair
[a, b] of nonnegative integers such that a < b. For
a sentence s = ?s1, s2, ..., sn? and a span [a, b] such
that b ? n, define s[a,b] = ?sa+1, ..., sb?.
647
NP -> the JJ NN of NNP
PP -> on NP
JJ -> fast NN -> jet ski NNP -> mr smith
NP -> < the JJ1 NN2 of NNP3, le NN2 JJ1 de NNP3 >
PP -> < on NP1, sur NP1>
JJ -> < fast, vite > NN -> < jet ski, jet ski > NNP -> < mr smith, m smith >
Figure 3: An example CFG derivation (above) and an ex-
ample SCFG derivation (below). Both derive the sen-
tence SUB(?on ??, SUB( ?the ? ? of ??, ?fast?, ?jet
ski?, ?mr smith?) ) = ?on the fast jet ski of mr smith?.
The SCFG derivation simultaneously derives the auxil-
iary sentence ?sur le jet ski vite de m smith?.
3 Minimum Derivation Cost
Chart parsing solves a problem which we will re-
fer to as Minimum Derivation Cost. Because we
want our results to be applicable to both CFG decod-
ing and SCFG decoding with an integrated language
model, we will provide a somewhat more abstract
formulation of chart parsing than usual.
In Figure 3, we show an example of a CFG deriva-
tion. A derivation is a tree of CFG rules, constructed
so that the preconditions (the RHS nonterminals) of
any rule match the postconditions (the LHS nonter-
minal) of its child rules. The purpose of a derivation
is to derive a sentence, which is obtained through
recursive substitution. In the example, we substitute
?fast?, ?jet ski?, and ?mr smith? into the lexical pat-
tern ?the ? ? of ?? to obtain ?the fast jet ski of mr
smith?. Then we substitute this result into the lexi-
cal pattern ?on ?? to obtain ?on the fast jet ski of mr
smith?.
The cost of a derivation is simply the sum of the
base costs of its rules. Thus the cost of the CFG
derivation in Figure 3 is C1 + C2 + C3 + C4 + C5,
where C1 is the base cost of rule ?PP? on NP?, etc.
Notice that this cost can be distributed locally to the
nodes of the derivation (Figure 4).
An SCFG derivation is similar to a CFG deriva-
NP -> the JJ NN of NNP
PP -> on NP
JJ -> fast NN -> jet ski NNP -> mr smith
C 3
C 4
C 5
C 2
C 1
Figure 4: The cost of the CFG derivation in Figure 3 is
C1 + C2 + C3 + C4 + C5, where C1 is the base cost
of rule ?PP ? on NP?, etc. Notice that this cost can be
distributed locally to the nodes of the derivation.
tion, except that it simultaneously derives two sen-
tences. For instance, the SCFG derivation in Fig-
ure 3 derives the sentence pair ? ?on the fast jet ski
of mr smith?, ?sur le jet ski vite de m smith? ?. In
machine translation, often we want the cost of the
SCFG derivation to include a language model cost
for this second sentence. For example, the cost of the
SCFG derivation in Figure 3 might beC1+C2+C3+
C4+C5+LM(sur le)+LM(le jet)+LM(jet ski)+
LM(ski de) + LM(de m) + LM(m smith), where
LM is the negative log of a 2-gram language model.
This new cost function can also be distributed lo-
cally to the nodes of the derivation, as shown in Fig-
ure 5. However, in order to perform the local com-
putations, we need to pass information (in this case,
the LM boundary words) up the tree. We refer to
this extra information as carries. Formally, define a
carry as a sentence of rank 0.
In order to provide a chart parsing formulation
that applies to both CFG decoding and SCFG de-
coding with an integrated language model, we need
abstract definitions of rule and derivation that cap-
ture the above concepts of pattern, postcondition,
preconditions, cost, and carries.
3.1 Rules
Define a rule as a tuple ?k, s?, X, pi,?, c?, where k is
a nonnegative integer called the rank, s? is a rank-k
648
NP -> < the JJ1 NN2 of NNP3, le NN2 JJ1 de NNP3 >
PP -> < on NP1, sur NP1>
JJ -> < fast, vite > NN -> < jet ski, jet ski > NNP -> < mr smith, m smith >
m * smith
C 5 + LM(m smith)C 4 + LM(jet ski)C 3
vite * vite jet * ski
le * smith
C 2 + LM(le jet) + LM(ski vite) + LM( vite de) + LM(de m)C 1 + LM ( sur le)
Figure 5: The cost of the SCFG derivation in Figure 3
(with an integrated language model score) can also be dis-
tributed to the nodes of the derivation, but to perform the
local computations, information must be passed up the
tree. We refer to this extra information as a carry.
sentence called the pattern 1, X is a symbol called
the postcondition, pi is a k-length sentence called the
preconditions, ? is a function (called the carry func-
tion) that maps a k-length list of carries to a carry,
and c is a function (called the cost function) that
maps a k-length list of carries to a real number. Fig-
ure 6 shows a CFG and an SCFG rule, deconstructed
according to this definition. 2 Note that the CFG rule
has trivial cost and carry functions that map every-
thing to a constant. We refer to such rules as simple.
We will use post(r) to refer to the postcondition
of rule r, and pre(r, i) to refer to the ith precondition
of rule r.
Finally, define a grammar as a finite set of rules.
A grammar is simple if all its rules are simple.
3.2 Derivations
For a grammarR, define deriv(R) as the smallest set
that contains every tuple ?r, ?1, ..., ?k? satisfying the
following conditions:
1For simplicity, we also impose the condition that ??? is not
a valid pattern. This is tantamount to disallowing unary rules.
2One possible point of confusion is why the pattern of the
SCFG rule refers only to the primary sentence, and not the aux-
iliary sentence. To reconstruct the auxiliary sentence from an
SCFG derivation in practice, one would need to augment the
abstract definition of rule with an auxiliary pattern. However
this is not required for our theoretical results.
NP -> < the JJ1 NN2 of NNP3, le NN2 JJ1 de NNP3 >
postcondition preconditions rank
pattern the ??of ?carry function ?( ?u*v? , ?w*x? , ?y*z? )= ??????cost function c( ?u*v? , ?w*x? , ?y*z? ) = C + LM( w|le ) + LM( u|x ) + LM( de|v ) + LM( y |de )
NP -> the JJ NN of NNP
postcondition preconditions
pattern the ??of ?carry function ?( ?? , ?? , ?? ) = ??cost function c( ?? , ?? , ?? ) = C
rank = 3
Figure 6: Deconstruction of a CFG rule (left) and SCFG
rule (right) according to the definition of rule in Sec-
tion 3.1. The carry function of the SCFG rule computes
boundary words for a 2-gram language model. In the cost
functions, C is a real number and LM returns the negative
log of a language model query.
? r ? R is a k-rank rule
? ?i ? deriv(R) for all 1 ? i ? k
? pre(r, i) = post(ri) for all 1 ? i ? k, where ri
is the first element of tuple ?i.
An R?derivation is an element of deriv(R). Con-
sider a derivation ? = ?r, ?1, ..., ?k?, where rule
r = ?k, s?, X, pi,?, c?. Define the following prop-
erties:
post(?) = post(r)
sent(?) = SUB(s?, sent(?1), ..., sent(?k))
carry(?) = ?(carry(?1), ..., carry(?k))
cost(?) = c(carry(?1), ..., carry(?k)) +
k?
j=1
cost(?j)
In words, we say that derivation ? derives sen-
tence sent(?). If for some span ? of a particular sen-
tence s, it holds that sent(?) = s?, then we will say
that ? is a derivation over span ?.
3.3 Problem Statement
The Minimum Derivation Cost problem is the fol-
lowing. Given a set R of rules and an input sentence
649
on the fast jet ski of mr smith
the ? ? of ?
0 1 2 3 4 5 6 7 8
Figure 7: An application context for the pattern ?the ? ?
of ?? and the sentence ?on the fast jet ski of mr smith?.
s, find the minimum cost of any R?derivation that
derives s. In other words, compute:
MinDCost(R, s) , min
??deriv(R)|sent(?)=s
cost(?)
4 Application Contexts
Chart parsing solves Minimum Derivation Cost via
dynamic programming. It works by building deriva-
tions over increasingly larger spans of the input sen-
tence s. Consider just one of these spans ?. How do
we build a derivation over that span?
Recall that a derivation takes the form
?r, ?1, ..., ?k?. Given the rule r and its pattern
s?, we need to choose the subderivations ?i such
that SUB(s?, sent(?1), ..., sent(?k)) = s?. To do
so, we must match the pattern to the span, so that
we know which subspans we need to build the
subderivations over. Figure 7 shows a matching
of the pattern ?the ? ? of ?? to span [1, 8] of the
sentence ?on the fast jet ski of mr smith?. It tells
us that we can build a derivation over span [1, 8] by
choosing this rule and subderivations over subspans
[2, 3], [3, 5], and [6, 8].
We refer to these matchings as application con-
texts. Formally, given two sentences s? and s
of respective lengths m and n, define an ?s?, s??
context as an monotonically increasing sequence
?x0, x1, ..., xm? of integers between 0 and n such
that for all i:
s?[i?1,i] 6= ? implies that s
?
[i?1,i] = s[xi?1,xi]
The context shown in Figure 7 is ?1, 2, 3, 5, 6, 8?.
Use cxt(s?, s) to denote the set of all ?s?, s??
contexts.
An ?s?, s??context x = ?x0, x1, ..., xm? has the
following properties:
span(x; s?, s) = [x0, xm]
subspans(x; s?, s) = ?[x0, x1], ..., [xm?1, xm]?
Moreover, define varspans(x; s?, s) as the sub-
sequence of subspans(x; s?, s) including only
[xi?1, xi] such that s?[i?1,i] = ?. For the context
x shown in Figure 7:
span(x; s?, s) = [1, 8]
subspans(x; s?, s) = ?[1, 2], [2, 3], [3, 5], [5, 6], [6, 8]?
varspans(x; s?, s) = ?[2, 3], [3, 5], [6, 8]?
An application context x ? cxt(s?, s) tells us that
we can build a derivation over span(x) by choosing
a rule with pattern s? and subderivations over each
span in varspans(x; s?, s).
5 Chart Parsing Algorithm
We are now ready to describe the chart parsing al-
gorithm. Consider a span ? of our input sentence
s and assume that we have computed and stored all
derivations over any subspan of ?. A naive way to
compute the minimum cost derivation over span ? is
to consider every possible derivation:
1. Choose a rule r = ?k, s?, X, pi,?, c?.
2. Choose an application context x ? cxt(s?, s)
such that span(x; s?, s) = ?.
3. For each subspan ?i ? varspans(x; s?, s),
choose a subderivation ?i such that post(?i) =
pre(r, i).
The key observation here is the following. In or-
der to score such a derivation, we did not actually
need to know each subderivation in its entirety. We
merely needed to know the following information
about it: (a) the subspan that it derives, (b) its post-
condition, (c) its carry.
650
Chart parsing takes advantage of the above obser-
vation to avoid building all possible derivations. In-
stead it groups together derivations that share a com-
mon subspan, postcondition, and carry, and records
only the minimum cost for each equivalence class.
It records this cost in an associative map referred to
as the chart.
Specifically, assume that we have computed and
stored the minimum cost of every derivation class
???, X ?, ???, where X ? is a postcondition, ?? is a
carry, and ?? is a proper subspan of ?. Chart pars-
ing computes the minimum cost of every derivation
class ??,X, ?? by adapting the above naive method
as follows:
1. Choose a rule r = ?k, s?, X, pi,?, c?.
2. Choose an application context x ? cxt(s?, s)
such that span(x; s?, s) = ?.
3. For each subspan ?i ? varspans(x; s?, s),
choose a derivation class ??i, Xi, ?i? from the
chart such that Xi = pre(r, i).
4. Update3 the cost of derivation class
??, post(r),?(?1, ..., ?k)? with:
c(?1, ..., ?k) +
k?
i=1
chart[?i, Xi, ?i]
where chart[?i, Xi, ?i] refers to the stored
cost of derivation class ??i, Xi, ?i?.
By iteratively applying the above method to all sub-
spans of size 1, 2, etc., chart parsing provides an
efficient solution for the Minimum Derivation Cost
problem.
6 Runtime Analysis
At the heart of chart parsing is a single operation:
the updating of a value in the chart. The running
time is linear in the number of these chart updates.
4 The typical analysis counts the number of chart
updates per span. Here we provide an alternative
3Here, update means ?replace the cost associated with the
class if the new cost is lower.?
4This assumes that you can linearly enumerate the relevant
updates. One convenient way to do this is to frame the enumer-
ation problem as a search space, e.g. (Hopkins and Langmead,
2009)
analysis that counts the number of chart updates per
rule. This provides us with a finer bound with prac-
tical implications.
Let r be a rule with rank k and pattern s?. Con-
sider the chart updates involving rule r. There is
(potentially) an update for every choice of (a) span,
(b) application context, and (c) list of k derivation
classes. If we let C be the set of possible carries,
then this means there are at most |cxt(s?, s)| ? |C|k
updates involving rule r. 5 If we are doing beam de-
coding (i.e. after processing a span, the chart keeps
only the B items of lowest cost), then there are at
most |cxt(s?, s)| ?Bk updates.
We can simplify the above by providing an upper
bound for |cxt(s?, s)|. Define an ambiguity as the
sentence ?? ??, and define scope(s?) as the number
of ambiguities in the sentence ??? ?s?? ???. The
following bound holds:
Lemma 1. Assume that a zero-rank sentence s does
not contain the same symbol more than once. Then
|cxt(s?, s)| ? |s|scope(s
?).
Proof. Suppose s? and s have respective lengths m
and n. Consider ?x0, x1, ..., xm? ? cxt(s?, s). Let
I be the set of integers i between 1 and m such that
s?i 6= ? and let I
+ be the set of integers i between
0 and m ? 1 such that s?i+1 6= ?. If i ? I , then we
know the value of xi, namely it is the unique integer
j such that sj = s?i . Similarly, if i ? I
+, then the
value of xi must be the unique integer j such that
sj = s?i+1. Thus the only nondetermined elements
of context xi are those for which i 6? I ? I+. Hence
|cxt(s?, s)| ? |s|{0,1,...,m}?I?I
+
= |s|scope(s
?).
Hence, under the assumption that the input sen-
tence s does not contain the same symbol more than
once, then there are at most |s|scope(s
?) ? |C|k chart
updates involving a rule with pattern s?.
For a rule r with pattern s?, define scope(r) =
scope(s?). For a grammar R, define scope(R) =
maxr?R scope(r) and rank(R) = maxr?R rank(r).
Given a grammar R and an input sentence s,
the above lemma tells us that chart parsing makes
5For instance, in SCFG decoding with an integrated j-gram
language model, a carry consists of 2(j ? 1) boundary words.
Generally it is assumed that there are O(n) possible choices for
a boundary word, and hence O(n2(j?1)) possible carries.
651
O(|s|scope(R) ? |C|rank(R)) chart updates. If we re-
strict ourselves to beam search, than chart parsing
makes O(|s|scope(R)) chart updates. 6
6.1 On the Uniqueness Assumption
In practice, it will not be true that each input sen-
tence contains only unique symbols, but it is not too
far removed from the practical reality of many use
cases, for which relatively few symbols repeat them-
selves in a given sentence. The above lemma can
also be relaxed to assume only that there is a con-
stant upper bound on the multiplicity of a symbol
in the input sentence. This does not affect the O-
bound on the number of chart updates, as long as we
further assume a constant limit on the length of rule
patterns.
7 Scope Reduction
From this point of view, CNF binarization can be
viewed as a specific example of scope reduction.
Suppose we have a grammar R of scope p. See Fig-
ure 8. If we can find a grammar R? of scope p? < p
which is ?similar? to grammar R, then we can de-
code in O(np?) rather than O(np) time.
We can frame the problem by assuming the fol-
lowing parameters:
? a grammar R
? a desired scope p
? a loss function ? that returns a (non-negative
real-valued) score for any two grammars R and
R?; if ?(R, R?) = 0, then the grammars are con-
sidered to be equivalent
A scope reduction method with loss ? finds a gram-
mar R? such that scope(R?) ? p and ?(R, R?) = ?.
A scope reduction method is lossless when its loss
is 0.
In the following sections, we will use the loss
function:
?(R, R?) = |MinDCost(R, s)?MinDCost(R?, s)|
where s is a fixed input sentence. Observe that if
?(R, R?) = 0, then the solution to the Minimum
6Assuming rank(R) is bounded by a constant.
CNF LNF
Scope 3
All Grammars
Figure 8: The ?scope reduction? problem. Given a gram-
mar of large scope, find a similar grammar of reduced
scope.
Derivation Cost problem is the same for both R and
R?. 7
7.1 CNF Binarization
A rule r is CNF if its pattern is ???? or ?x?, where x
is any non-substitution symbol. A grammar is CNF
if all of its rules are CNF. Note that the maximum
scope of a CNF grammar is 3.
CNF binarization is a deterministic process that
maps a simple grammar to a CNF grammar. Since
binarization takes subcubic time, we can decode
with any grammar R in O(n3) time by converting
R to CNF grammar R?, and then decoding with R?.
This is a lossless scope reduction method.
What if grammar R is not simple? For SCFG
grammars, (Zhang et al, 2006) provide a scope
reduction method called synchronous binarization
with quantifiable loss. Synchronous binarization se-
lects a ?binarizable? subgrammar R? of grammar R,
and then converts R? into a CNF grammar R?. The
cost and carry functions of these new rules are con-
structed such that the conversion from R? to R? is
a lossless scope reduction. Thus the total loss of
the method is |MinDCost(R, s)?MinDCost(R?, s)|.
Fortunately, they find in practice thatR? usually con-
tains the great majority of the rules of R, thus they
7Note that if we want the actual derivation and not just its
cost, then we need to specify a more finely grained loss func-
tion. This is omitted for clarity and left as an exercise.
652
a ????aa ??b?a ????a ????aa ???a b ????a ba ??b ?a ??b c
a b ??c?a ??ba ?b ????a ?b???a ba ???ba ??b ?ca ????ba ??b c ??da ????b ?c ?da ??b ??c ??d
Figure 9: A selection of rule patterns that are scope ? 3
but not LNF or CNF.
assert that this loss is negligable.
A drawback of their technique is that the resulting
CNF grammar contains many more rules and post-
conditions than the original grammar. These con-
stant factors do not impact asymptotic performance,
but do impact practical performance.
7.2 Lexical Normal Form
Concerned about this inflation of the grammar con-
stant, (DeNero et al, 2009) consider a superset of
CNF called Lexical Normal Form (LNF). A rule is
LNF if its pattern does not contain an ambiguity as
a proper subsentence (recall that an ambiguity was
defined to be the sentence ?? ??). Like CNF, the
maximum scope of an LNF grammar is 3. In the
worst case, the pattern s? is ?? ??, in which case
there are three ambiguities in the sentence ??? ?s??
???.
(DeNero et al, 2009) provide a lossless scope
reduction method that maps a simple grammar to
an LNF grammar, thus enabling cubic-time decod-
ing. Their principal objective is to provide a scope
reduction method for SCFG that introduces fewer
postconditions than (Zhang et al, 2006). However
unlike (Zhang et al, 2006), their method only ad-
dresses simple grammars. Thus they cannot inte-
grate LM scoring into their decoding, requiring them
to rescore the decoder output with a variant of cube
growing (Huang and Chiang, 2007).
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 1 2 3 4 5 6 7 8
% of r
ules 
with sc
ope
 
<= P
P
AE Lexical CE Lexical AE Nonlexical CE Nonlexical
Figure 10: Breakdown of rules by scope (average per sen-
tence in our test sets). In practice, most of the lexical rules
applicable to a given sentence (95% for Arabic-English
and 85% for Chinese-English) are scope 3 or less.
7.3 Scope Pruning
To exercise the power of the ideas presented in this
paper, we experimented with a third (and very easy)
scope reduction method called scope pruning. If we
consider the entire space of scope-3 grammars, we
see that it contains a much richer set of rules than
those permitted by CNF or LNF. See Figure 9 for
examples. Scope pruning is a lossy scope reduc-
tion method that simply takes an arbitrary grammar
and prunes all rules with scope greater than 3. By
not modifying any rules, we preserve their cost and
carry functions (enabling integrated LM decoding),
without increasing the grammar constant. The prac-
tical question is: how many rules are we typically
pruning from the original grammar?
We experimented with two pretrained syntax-
based machine translation systems with rules ex-
tracted via the GHKM algorithm (Galley et al,
2004). The first was an Arabic-English system, with
rules extracted from 200 million words of parallel
data from the NIST 2008 data collection, and with
a 4-gram language model trained on 1 billion words
of monolingual English data from the LDC Giga-
word corpus. We evaluated this system?s perfor-
mance on the NIST 2008 test corpus, which con-
sists of 1357 Arabic sentences from a mixture of
newswire and web domains, with four English refer-
ence translations. The second system was a Chinese-
653
Arabic -English Chinese -English
33
34
35
36
37
38
39
40
0 2000 4000 6000 8000
BLEU
-4
Words per minute
27
28
29
30
31
32
33
34
35
36
37
0 2000 4000 6000 8000
BLEU
-4
Words per minute
Figure 11: Speed-quality tradeoff curves comparing the baseline scope reduction method of synchronous binarization
(dark gray diamonds) with scope-3 pruning (light gray squares).
English system, with rules extracted from 16 million
words of parallel data from the mainland-news do-
main of the LDC corpora, and with a 4-gram lan-
guage model trained on monolingual English data
from the AFP and Xinhua portions of the LDC Gi-
gaword corpus. We evaluated this system?s perfor-
mance on the NIST 2003 test corpus, which con-
sists of 919 Chinese sentences, with four English
reference translations. For both systems, we report
BLEU scores (Papineni et al, 2002) on untokenized,
recapitalized output.
In practice, how many rules have scope greater
than 3? To answer this question, it is useful to dis-
tinguish between lexical rules (i.e. rules whose pat-
terns contain at least one non-substitution symbol)
and non-lexical rules. Only a subset of lexical rules
are potentially applicable to a given input sentence.
Figure 10 shows the scope profile of these applicable
rules (averaged over all sentences in our test sets).
Most of the lexical rules applicable to a given sen-
tence (95% for Arabic-English, 85% for Chinese-
English) are scope 3 or less. 8 Note, however, that
scope pruning also prunes a large percentage of non-
lexical rules.
Figure 11 compares scope pruning with the base-
line technique of synchronous binarization. To gen-
erate these speed-quality tradeoff curves, we de-
coded the test sets with 380 different beam settings.
We then plotted the hull of these 380 points, by elim-
inating any points that were dominated by another
(i.e. had better speed and quality). We found that
this simple approach to scope reduction produced
a better speed-quality tradeoff than the much more
complex synchronous binarization. 9
8For contrast, the corresponding numbers for LNF are 64%
and 53%, respectively.
9We also tried a hybrid approach in which we scope-pruned
654
8 Conclusion
In this paper, we made the following contributions:
? We provided an abstract formulation of chart
parsing that generalizes CFG decoding and
SCFG decoding with an integrated LM.
? We framed scope reduction as a first-class ab-
stract problem, and showed that CNF binariza-
tion and LNF binarization are two specific solu-
tions to this problem, each with their respective
advantages and disadvantages.
? We proposed a third scope reduction technique
called scope pruning, and we showed that it can
outperform synchronous CNF binarization for
particular use cases.
Moreover, this work gives formal expression to the
extraction heuristics of hierarchical phrase-based
translation (Chiang, 2007), whose directive not to
extract SCFG rules with adjacent nonterminals can
be viewed as a preemptive pruning of rules with
scope greater than 2 (more specifically, the prun-
ing of non-LNF lexical rules). In general, this work
provides a framework in which different approaches
to tractability-focused grammar construction can be
compared and discussed.
References
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
John DeNero, Mohit Bansal, Adam Pauls, and Dan Klein.
2009. Efficient parsing for transducer grammars. In
Proceedings of the Human Language Technology Con-
ference of the NAACL, Main Conference.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of HLT/NAACL.
Mark Hopkins and Greg Langmead. 2009. Cube pruning
as heuristic search. In Proceedings of EMNLP.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of ACL.
the lexical rules and synchronously binarized the non-lexical
rules. This had a similar performance to scope-pruning all
rules. The opposite approach of scope-pruning the lexical rules
and synchronously binarizing the non-lexical rules had a similar
performance to synchronous binarization.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318.
Daniel Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information and
Control, 10(2):189?208.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In Proceedings of the Human Language
Technology Conference of the NAACL, Main Confer-
ence, pages 256?263.
655
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1352?1362,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Tuning as Ranking
Mark Hopkins and Jonathan May
SDL Language Weaver
Los Angeles, CA 90045
{mhopkins,jmay}@sdl.com
Abstract
We offer a simple, effective, and scalable
method for statistical machine translation pa-
rameter tuning based on the pairwise approach
to ranking (Herbrich et al, 1999). Unlike
the popular MERT algorithm (Och, 2003), our
pairwise ranking optimization (PRO) method
is not limited to a handful of parameters and
can easily handle systems with thousands of
features. Moreover, unlike recent approaches
built upon the MIRA algorithm of Crammer
and Singer (2003) (Watanabe et al, 2007; Chi-
ang et al, 2008b), PRO is easy to imple-
ment. It uses off-the-shelf linear binary classi-
fier software and can be built on top of an ex-
isting MERT framework in a matter of hours.
We establish PRO?s scalability and effective-
ness by comparing it to MERT and MIRA and
demonstrate parity on both phrase-based and
syntax-based systems in a variety of language
pairs, using large scale data scenarios.
1 Introduction
The MERT algorithm (Och, 2003) is currently the
most popular way to tune the parameters of a sta-
tistical machine translation (MT) system. MERT
is well-understood, easy to implement, and runs
quickly, but can behave erratically and does not scale
beyond a handful of features. This lack of scalability
is a significant weakness, as it inhibits systems from
using more than a couple dozen features to discrimi-
nate between candidate translations and stymies fea-
ture development innovation.
Several researchers have attempted to address
this weakness. Recently, Watanabe et al (2007)
and Chiang et al (2008b) have developed tuning
methods using the MIRA algorithm (Crammer and
Singer, 2003) as a nucleus. The MIRA technique of
Chiang et al has been shown to perform well on
large-scale tasks with hundreds or thousands of fea-
tures (2009). However, the technique is complex and
architecturally quite different from MERT. Tellingly,
in the entire proceedings of ACL 2010 (Hajic? et al,
2010), only one paper describing a statistical MT
system cited the use of MIRA for tuning (Chiang,
2010), while 15 used MERT.1
Here we propose a simpler approach to tuning that
scales similarly to high-dimensional feature spaces.
We cast tuning as a ranking problem (Chen et al,
2009), where the explicit goal is to learn to correctly
rank candidate translations. Specifically, we follow
the pairwise approach to ranking (Herbrich et al,
1999; Freund et al, 2003; Burges et al, 2005; Cao et
al., 2007), in which the ranking problem is reduced
to the binary classification task of deciding between
candidate translation pairs.
Of primary concern to us is the ease of adoption of
our proposed technique. Because of this, we adhere
as closely as possible to the established MERT ar-
chitecture and use freely available machine learning
software. The end result is a technique that scales
and performs just as well as MIRA-based tuning,
but which can be implemented in a couple of hours
by anyone with an existing MERT implementation.
Mindful that many would-be enhancements to the
1The remainder either did not specify their tuning method
(though a number of these used the Moses toolkit (Koehn et al,
2007), which uses MERT for tuning) or, in one case, set weights
by hand.
1352
state-of-the-art are false positives that only show im-
provement in a narrowly defined setting or with lim-
ited data, we validate our claims on both syntax and
phrase-based systems, using multiple language pairs
and large data sets.
We describe tuning in abstract and somewhat for-
mal terms in Section 2, describe the MERT algo-
rithm in the context of those terms and illustrate its
scalability issues via a synthetic experiment in Sec-
tion 3, introduce our pairwise ranking optimization
method in Section 4, present numerous large-scale
MT experiments to validate our claims in Section 5,
discuss some related work in Section 6, and con-
clude in Section 7.
2 Tuning
In Figure 1, we show an example candidate space,
defined as a tuple ??, I, J, f, e,x? where:
? ? is a positive integer referred to as the dimen-
sionality of the space
? I is a (possibly infinite) set of positive integers,
referred to as sentence indices
? J maps each sentence index to a (possibly infi-
nite) set of positive integers, referred to as can-
didate indices
? f maps each sentence index to a sentence from
the source language
? e maps each pair ?i, j? ? I ? J(i) to the jth
target-language candidate translation of source
sentence f(i)
? x maps each pair ?i, j? ? I ? J(i) to a
?-dimension feature vector representation of
e(i, j)
The example candidate space has two source sen-
tences, three candidate translations for each source
sentence, and feature vectors of dimension 2. It is
an example of a finite candidate space, defined as
a candidate space for which I is finite and J maps
each index of I to a finite set.
A policy of candidate space ??, I, J, f, e,x? is a
function that maps each member i ? I to a member
of J(i). A policy corresponds to a choice of one
candidate translation for each source sentence. For
the example in Figure 1, policy p1 = {1 7? 2, 2 7?
3} corresponds to the choice of ?he does not go? for
the first source sentence and ?I do not go? for the
second source sentence. Obviously some policies
are better than others. Policy p2 = {1 7? 3, 2 7? 1}
corresponds to the inferior translations ?she not go?
and ?I go not.?
We assume the MT system distinguishes between
policies using a scoring function for candidate trans-
lations of the form hw(i, j) = w ? x(i, j), where w
is a weight vector of the same dimension as feature
vector x(i, j). This scoring function extends to a
policy p by summing the cost of each of the policy?s
candidate translations: Hw(p) = ?i?I hw(i, p(i)).
As can be seen in Figure 1, using w = [?2, 1],
Hw(p1) = 9 and Hw(p2) = ?8.
The goal of tuning is to learn a weight vector w
such that Hw(p) assigns a high score to good poli-
cies, and a low score to bad policies.2 To do so,
we need information about which policies are good
and which are bad. This information is provided by
a ?gold? scoring function G that maps each policy
to a real-valued score. Typically this gold function
is BLEU (Papineni et al, 2002), though there are
several common alternatives (Lavie and Denkowski,
2009; Melamed et al, 2003; Snover et al, 2006;
Chiang et al, 2008a).
We want to find a weight vector w such that Hw
behaves ?similarly? to G on a candidate space s.
We assume a loss function ls(Hw, G) which returns
the real-valued loss of using scoring function Hw
when the gold scoring function is G and the candi-
date space is s. Thus, we may say the goal of tuning
is to find the weight vector w that minimizes loss.
3 MERT
In general, the candidate space may have infinitely
many source sentences, as well as infinitely many
candidate translations per source sentence. In prac-
tice, tuning optimizes over a finite subset of source
sentences3 and a finite subset of candidate transla-
tions as well. The classic tuning architecture used
in the dominant MERT approach (Och, 2003) forms
the translation subset and learns weight vector w via
2Without loss of generality, we assume that a higher score
indicates a better translation.
3See Section 5.2 for the tune sets used in this paper?s exper-
iments.
1353
Source Sentence Candidate Translations
i f(i) j e(i, j) x(i, j) hw(i, j) g(i, j)
1 ?il ne va pas? 1 ?he goes not? [2 4] 0 0.28
2 ?he does not go? [3 8] 2 0.42
3 ?she not go? [6 1] -11 0.12
2 ?je ne vais pas? 1 ?I go not? [-3 -3] 3 0.15
2 ?we do not go? [1 -5] -7 0.18
3 ?I do not go? [-5 -3] 7 0.34
Figure 1: Example candidate space of dimensionality 2. Note: I = {1, 2}, J(1) = J(2) = {1, 2, 3}. We also show a
local scoring function hw(i, j) (where w = [?2, 1]) and a local gold scoring function g(i, j).
Algorithm TUNE(s, G):
1: initialize pool: let s? = ??, I ?, J ?, f, e,x?,
where I ? ? I and J ? = ?
2: for the desired number of iterations do
3: candidate generation: choose index pairs
(i, j); for each, add j to J ?(i)
4: optimization: find vector w that minimizes
ls?(Hw, G)
5: return w
Figure 2: Schema for iterative tuning of base candidate
space s = ??, I, J, f, e,x? w.r.t. gold function G.
a feedback loop consisting of two phases. Figure 2
shows the pseudocode. During candidate genera-
tion, candidate translations are selected from a base
candidate space s and added to a finite candidate
space s? called the candidate pool. During optimiza-
tion, the weight vector w is optimized to minimize
loss ls?(Hw, G).
For its candidate generation phase, MERT gener-
ates the k-best candidate translations for each source
sentence according to hw, where w is the weight
vector from the previous optimization phase (or an
arbitrary weight vector for the first iteration).
For its optimization phase, MERT defines the loss
function as follows:
ls(Hw, G) = maxp G(p)?G(arg maxp
Hw(p))
In other words, it prefers weight vectors w such
that the gold function G scores Hw?s best policy as
highly as possible (if Hw?s best policy is the same
as G?s best policy, then there is zero loss). Typically
the optimization phase is implemented using Och?s
line optimization algorithm (2003).
MERT has proven itself effective at tuning candi-
date spaces with low dimensionality. However, it is
often claimed that MERT does not scale well with
dimensionality. To test this claim, we devised the
following synthetic data experiment:
1. We created a gold scoring function G that is
also a linear function of the same form as Hw,
i.e.,G(p) = Hw?(p) for some gold weight vec-
tor w?. Under this assumption, the role of the
optimization phase reduces to learning back the
gold weight vector w?.
2. We generated a ?-dimensionality candidate
pool with 500 source ?sentences? and 100 can-
didate ?translations? per sentence. We created
the corresponding feature vectors by drawing
? random real numbers uniformly from the in-
terval [0, 500].
3. We ran MERT?s line optimization on this syn-
thetic candidate pool and compared the learned
weight vector w to the gold weight vector w?
using cosine similarity.
We used line optimization in the standard way,
by generating 20 random starting weight vectors and
hill-climbing on each independently until no further
progress is made, then choosing the final weight vec-
tor that minimizes loss. We tried various dimen-
sionalities from 10 to 1000. We repeated each set-
ting three times, generating different random data
each time. The results in Figure 3 indicate that as
the dimensionality of the problem increases MERT
rapidly loses the ability to learn w?. Note that this
synthetic problem is considerably easier than a real
MT scenario, where the data is noisy and interdepen-
dent, and the gold scoring function is nonlinear. If
1354
MERT cannot scale in this simple scenario, it has lit-
tle hope of succeeding in a high-dimensionality de-
ployment scenario.
4 Optimization via Pairwise Ranking
We would like to modify MERT so that it scales well
to high-dimensionality candidate spaces. The most
prominent example of a tuning method that per-
forms well on high-dimensionality candidate spaces
is the MIRA-based approach used by Watanabe et
al. (2007) and Chiang et al (2008b; 2009). Unfortu-
nately, this approach requires a complex architecture
that diverges significantly from the MERT approach,
and consequently has not been widely adopted. Our
goal is to achieve the same performance with mini-
mal modification to MERT.
With MERT as a starting point, we have a choice:
modify candidate generation, optimization, or both.
Although alternative candidate generation methods
have been proposed (Macherey et al, 2008; Chiang
et al, 2008b; Chatterjee and Cancedda, 2010), we
will restrict ourselves to MERT-style candidate gen-
eration, in order to minimize divergence from the
established MERT tuning architecture. Instead, we
focus on the optimization phase.
4.1 Basic Approach
While intuitive, the MERT optimization module fo-
cuses attention on Hw?s best policy, and not on its
overall prowess at ranking policies. We will cre-
ate an optimization module that directly addresses
Hw?s ability to rank policies in the hope that this
more holistic approach will generalize better to un-
seen data.
Assume that the gold scoring function G decom-
poses in the following way:
G(p) =
?
i?I
g(i, p(i)) (1)
where g(i, j) is a local scoring function that scores
the single candidate translation e(i, j). We show an
example g in Figure 1. For an arbitrary pair of can-
didate translations e(i, j) and e(i, j?), the local gold
function g tells us which is the better translation.
Note that this induces a ranking on the candidate
translations for each source sentence.
We follow the pairwise approach to ranking (Her-
brich et al, 1999; Freund et al, 2003; Burges et al,
2005; Cao et al, 2007). In the pairwise approach,
the learning task is framed as the classification of
candidate pairs into two categories: correctly or-
dered and incorrectly ordered. Specifically, for can-
didate translation pair e(i, j) and e(i, j?), we want:
g(i, j) > g(i, j?) ? hw(i, j) > hw(i, j?). We can
re-express this condition:
g(i, j) > g(i, j?)? hw(i, j) > hw(i, j?)
? hw(i, j)? hw(i, j?) > 0
? w ? x(i, j)?w ? x(i, j?) > 0
? w ? (x(i, j)? x(i, j?)) > 0
Thus optimization reduces to a classic binary clas-
sification problem. We create a labeled training in-
stance for this problem by computing difference vec-
tor x(i, j) ? x(i, j?), and labeling it as a positive
or negative instance based on whether, respectively,
the first or second vector is superior according to
gold function g. To ensure balance, we consider
both possible difference vectors from a pair. For ex-
ample, given the candidate space of Figure 1, since
g(1, 1) > g(1, 3), we would add ([?4, 3],+) and
([4,?3],?) to our training set. We can then feed this
training data directly to any off-the-shelf classifica-
tion tool that returns a linear classifier, in order to ob-
tain a weight vector w that optimizes the above con-
dition. This weight vector can then be used directly
by the MT system in the subsequent candidate gen-
eration phase. The exact loss function ls?(Hw, G)
optimized depends on the choice of classifier.4
Typical approaches to pairwise ranking enumer-
ate all difference vectors as training data. For tuning
however, this means O(|I| ? J2max) vectors, where
Jmax is the cardinality of the largest J(i). Since
I and Jmax commonly range in the thousands, a
full enumeration would produce billions of feature
vectors. Out of tractability considerations, we sam-
ple from the space of difference vectors, using the
sampler template in Figure 4. For each source sen-
tence i, the sampler generates ? candidate transla-
tion pairs ?j, j??, and accepts each pair with proba-
bility ?i(|g(i, j) ? g(i, j?)|). Among the accepted
pairs, it keeps the ? with greatest g differential, and
adds their difference vectors to the training data.5
4See (Chen et al, 2009) for a brief survey.
5The intuition for biasing toward high score differential is
1355
 
0
 
0.2
 
0.4
 
0.6
 
0.8 1  0
 
200
 
400
 
600
 
800
 
1000
Cosine similarity 
of learned parameter weights
Dimen
sional
ity
Synth
etic pa
ramet
er lear
ning
of ME
RT an
d PRO PRO
Noisy
 PRO MERT
Noisy
 MER
T
Figure 3: Result of synthetic data learning experiment
for MERT and PRO, with and without added noise. As
the dimensionality increases MERT is unable to learn the
original weights but PRO still performs adequately.
4.2 Scalability
We repeated the scalability study from Section 3,
now using our pairwise ranking optimization (here-
after, PRO) approach. Throughout all experiments
with PRO we choose ? = 5000, ? = 50, and the
following step function ? for each ?i: 6
?(n) =
{
0 if n < 0.05
1 otherwise
We used MegaM (Daume? III, 2004) as a binary
classifier in our contrasting synthetic experiment and
ran it ?out of the box,? i.e., with all default settings
for binary classification.7 Figure 3 shows that PRO
is able to learn w? nearly perfectly at all dimension-
alities from 10 to 1000.
As noted previously, though, this is a rather sim-
ple task. To encourage a disconnect between g and
hw and make the synthetic scenario look more like
MT reality, we repeated the synthetic experiments
that our primary goal is to ensure good translations are preferred
to bad translations, and not to tease apart small differences.
6We obtained these parameters by trial-and-error experi-
mentation on a single MT system (Urdu-English SBMT), then
held them fixed throughout our experiments. We obtained sim-
ilar results using ? = ? = 100, and for each ?i, a logistic sig-
moid function centered at the mean g differential of candidate
translation pairs for the ith source sentence. This alternative ap-
proach has the advantage of being agnostic about which gold
scoring function is used.
7With the sampling settings previously described and
MegaM as our classifier we were able to optimize two to three
times faster than with MERT?s line optimization.
Algorithm SAMPLERs,g( ?, ?, i, ?i ):
1: V = ??
2: for ? samplings do
3: Choose ?j, j?? ? J(i)?J(i) uniformly at ran-
dom.
4: With probability ?i(|g(i, j)-g(i, j?)|), add
(x(i, j),x(i, j?), |g(i, j)-g(i, j?)|) to V .
5: Sort V decreasingly by |g(i, j)-g(i, j?)|.
6: return (x(i, j) ? x(i, j?), sign(g(i, j)-g(i, j?))
and (x(i, j?)-x(i, j), sign(g(i, j?)-g(i, j))) for
each of the first ? members of V .
Figure 4: Pseudocode for our sampler. Arguments: s =
??, I, J, f, e,x? is a finite candidate space; g is a scoring
function; ?, ?, i are nonnegative integers; ?i is a func-
tion from the nonnegative real numbers to the real interval
[0, 1].
but added noise to each feature vector, drawn from
a zero-mean Gaussian with a standard deviation of
500. The results of the noisy synthetic experiments,
also in Figure 3 (the lines labeled ?Noisy?), show
that the pairwise ranking approach is less successful
than before at learning w? at high dimensionality,
but still greatly outperforms MERT.
4.3 Discussion
The idea of learning from difference vectors also lies
at the heart of the MIRA-based approaches (Watan-
abe et al, 2007; Chiang et al, 2008b) and the ap-
proach of Roth et al (2010), which, similar to our
method, uses sampling to select vectors. Here, we
isolate these aspects of those approaches to create
a simpler tuning technique that closely mirrors the
ubiquitous MERT architecture. Among other sim-
plifications, we abstract away the choice of MIRA
as the classification method (our approach can use
any classification technique that learns a separating
hyperplane), and we eliminate the need for oracle
translations.
An important observation is that BLEU does not
satisfy the decomposability assumption of Equa-
tion (1). An advantage of MERT is that it can di-
rectly optimize for non-decomposable scoring func-
tions like BLEU. In our experiments, we use
the BLEU+1 approximation to BLEU (Liang et al,
2006) to determine class labels. We will neverthe-
less use BLEU to evaluate the trained systems.
1356
PBMT
Language Experiment BLEUfeats method tune test
Urdu-English
base
MERT 20.5 17.7
MIRA 20.5 17.9
PRO 20.4 18.2
ext MIRA 21.8 17.8PRO 21.6 18.1
Arabic-English
base
MERT 46.8 41.2
MIRA 47.0 41.1
PRO 46.9 41.1
ext MIRA 47.5 41.7PRO 48.5 41.9
Chinese-English
base
MERT 23.8 22.2
MIRA 24.1 22.5
PRO 23.8 22.5
ext MIRA 24.8 22.6PRO 24.9 22.7
SBMT
Language Experiment BLEUfeats method tune test
Urdu-English
base
MERT 23.4 21.4
MIRA 23.6 22.3
PRO 23.4 22.2
ext MIRA 25.2 22.8PRO 24.2 22.8
Arabic-English
base
MERT 44.7 39.0
MIRA 44.6 39.0
PRO 44.5 39.0
ext MIRA 45.8 39.8PRO 45.9 40.3
Chinese-English
base
MERT 25.5 22.7
MIRA 25.4 22.9
PRO 25.5 22.9
ext MIRA 26.0 23.3PRO 25.6 23.5
Table 1: Machine translation performance for the experiments listed in this paper. Scores are case-sensitive IBM
BLEU. For every choice of system, language pair, and feature set, PRO performs comparably with the other methods.
5 Experiments
We now turn to real machine translation condi-
tions to validate our thesis: We can cleanly replace
MERT?s line optimization with pairwise ranking op-
timization and immediately realize the benefits of
high-dimension tuning. We now detail the three
language pairs, two feature scenarios, and two MT
models used for our experiments. For each language
pair and each MT model we used MERT, MIRA, and
PRO to tune with a standard set of baseline features,
and used the latter two methods to tune with an ex-
tended set of features.8 At the end of every experi-
ment we used the final feature weights to decode a
held-out test set and evaluated it with case-sensitive
BLEU. The results are in Table 1.
5.1 Systems
We used two systems, each based on a different MT
model. Our syntax-based system (hereafter, SBMT)
follows the model of Galley et al (2004). Our
8MERT could not run to a satisfactory completion in any
extended feature scenario; as implied in the synthetic data ex-
periment of Section 3, the algorithm makes poor choices for
its weights and this leads to low-quality k-best lists and dismal
performance, near 0 BLEU in every iteration.
phrase-based system (hereafter, PBMT) follows the
model of Och and Ney (2004). In both systems
we learn alignments with GIZA++ (Och and Ney,
2000) using IBM Model 4; for Urdu-English and
Chinese-English we merged alignments with the re-
fined method, and for Arabic-English we merged
with the union method.
5.2 Data
Table 2 notes the sizes of the datasets used in our ex-
periments. All tune and test data have four English
reference sets for the purposes of scoring.
Data U-E A-E C-E
Train lines 515K 6.5M 7.9Mwords 2.2M 175M 173M
Tune lines 923 1994 1615words 16K 65K 42K
Test lines 938 1357 1357words 18K 47K 37K
Table 2: Data sizes for the experiments reported in this
paper (English words shown).
1357
Class
Urdu-English Arabic-English Chinese-English
PBMT SBMT PBMT SBMT PBMT SBMT
base ext base ext base ext base ext base ext base ext
baseline 15 15 19 19 15 15 19 19 15 15 19 19
target word ? 51 ? 50 ? 51 ? 50 ? 51 ? 299
discount ? 11 ? 11 ? 11 ? 10 ? 11 ? 10
node count ? ? ? 99 ? ? ? 138 ? ? ? 96
rule overlap ? ? ? 98 ? ? ? 136 ? ? ? 93
word pair ? 2110 ? ? ? 6193 ? ? ? 1688 ? ?
phrase length ? 63 ? ? ? 63 ? ? ? 63 ? ?
total 15 2250 19 277 15 6333 18 352 15 1828 19 517
Table 3: Summary of features used in experiments in this paper.
5.2.1 Urdu-English
The training data for Urdu-English is that made
available in the constrained track in the NIST 2009
MT evaluation. This includes many lexicon entries
and other single-word data, which accounts for the
large number of lines relative to word count. The
NIST 2008 evaluation set, which contains newswire
and web data, is split into two parts; we used roughly
half each for tune and test. We trained a 5-gram
English language model on the English side of the
training data.
5.2.2 Arabic-English
The training data for Arabic English is that made
available in the constrained track in the NIST 2008
MT evaluation. The tune set, which contains only
newswire data, is a mix from NIST MT evaluation
sets from 2003?2006 and from GALE development
data. The test set, which contains both web and
newswire data, is the evaluation set from the NIST
2008 MT evaluation. We trained a 4-gram English
language model on the English side of the training
data.
5.2.3 Chinese-English
For Chinese-English we used 173M words of
training data from GALE 2008. For SBMT we used
a 32M word subset for extracting rules and building
a language model, but used the entire training data
for alignments, and for all PBMT training. The tune
and test sets both contain web and newswire data.
The tune set is selected from NIST MT evaluation
sets from 2003?2006. The test set is the evaluation
set from the NIST 2008 MT evaluation. We trained a
3-gram English language model on the English side
of the training data.
5.3 Features
For each of our systems we identify two feature sets:
baseline, which correspond to the typical small fea-
ture set reported in current MT literature, and ex-
tended, a superset of baseline, which adds hundreds
or thousands of features. Specifically, we use 15
baseline features for PBMT, similar to the baseline
features described by Watanabe et al (2007). We
use 19 baseline features for SBMT, similar to the
baseline features described by Chiang et al (2008b).
We used the following feature classes in SBMT
and PBMT extended scenarios:
? Discount features for rule frequency bins (cf.
Chiang et al (2009), Section 4.1)
? Target word insertion features9
We used the following feature classes in SBMT ex-
tended scenarios only (cf. Chiang et al (2009), Sec-
tion 4.1):10
? Rule overlap features
? Node count features
9For Chinese-English and Urdu-English SBMT these fea-
tures only fired when the inserted target word was unaligned to
any source word.
10The parser used for Arabic-English had a different nonter-
minal set than that used for the other two SBMT systems, ac-
counting for the wide disparity in feature count for these feature
classes.
1358
 
20
 
21
 
22
 
23
 
24
 
25
 
26  0
 
5
 
10
 
15
 
20
 
25
 
30
4-ref BLEU
Iterati
on
Urdu-
Englis
h SBM
T base
line fe
ature 
tuning
TUNE TEST
MER
T MIRA PRO
 
20
 
21
 
22
 
23
 
24
 
25
 
26  0
 
5
 
10
 
15
 
20
 
25
 
30
4-ref BLEU
Iterati
on
Urdu-
Englis
h SBM
T exte
nded f
eature
 tunin
g
TUNE TEST
MIRA PRO
Figure 5: Comparison of MERT, PRO, and MIRA on tuning Urdu-English SBMT systems, and test results at every
iteration. PRO performs comparably to MERT and MIRA.
We used the following feature classes in PBMT
extended scenarios only:
? Unigram word pair features for the 80 most fre-
quent words in both languages plus tokens for
unaligned and all other words (cf. Watanabe et
al. (2007), Section 3.2.1)11
? Source, target, and joint phrase length fea-
tures from 1 to 7, e.g. ?tgt=4?, ?src=2?, and
?src/tgt=2,4?
The feature classes and number of features used
within those classes for each language pair are sum-
marized in Table 3.
5.4 Tuning settings
Each of the three approaches we compare in this
study has various details associated with it that may
prove useful to those wishing to reproduce our re-
sults. We list choices made for the various tuning
methods here, and note that all our decisions were
made in keeping with best practices for each algo-
rithm.
5.4.1 MERT
We used David Chiang?s CMERT implementation
of MERT that is available with the Moses system
(Koehn et al, 2007). We ran MERT for up to 30 it-
erations, using k = 1500, and stopping early when
11This constitutes 6,723 features in principle (822 ? 1 since
?unaligned-unaligned? is not considered) but in practice far
fewer co-occurrences were seen. Table 3 shows the number of
actual unigram word pair features observed in data.
the accumulated k-best list does not change in an it-
eration. In every tuning iteration we ran MERT once
with weights initialized to the last iteration?s chosen
weight set and 19 times with random weights, and
chose the the best of the 20 ending points according
to G on the development set. The G we optimize
is tokenized, lower-cased 4-gram BLEU (Papineni et
al., 2002).
5.4.2 MIRA
We for the most part follow the MIRA algorithm
for machine translation as described by Chiang et al
(2009)12 but instead of using the 10-best of each of
the best hw, hw +g, and hw-g, we use the 30-best
according to hw.13 We use the same sentence-level
BLEU calculated in the context of previous 1-best
translations as Chiang et al (2008b; 2009). We ran
MIRA for 30 iterations.
5.4.3 PRO
We used the MegaM classifier and sampled as de-
scribed in Section 4.2. As previously noted, we used
BLEU+1 (Liang et al, 2006) for g. MegaM was easy
to set up and ran fairly quickly, however any linear
binary classifier that operates on real-valued features
can be used, and in fact we obtained similar results
12and acknowledge the use of David Chiang?s code
13This is a more realistic scenario for would-be implementers
of MIRA, as obtaining the so-called ?hope? and ?fear? transla-
tions from the lattice or forest is significantly more complicated
than simply obtaining a k-best list. Other tests comparing these
methods have shown between 0.1 to 0.3 BLEU drop using 30-
best hw on Chinese-English (Wang, 2011).
1359
using the support vector machine module of WEKA
(Hall et al, 2009) as well as the Stanford classifier
(Manning and Klein, 2003). We ran for up to 30 iter-
ations and used the same k and stopping criterion as
was used for MERT, though variability of sampling
precluded list convergence.
While MERT and MIRA use each iteration?s final
weights as a starting point for hill-climbing the next
iteration, the pairwise ranking approach has no ex-
plicit tie to previous iterations. To incorporate such
stability into our process we interpolated the weights
w? learned by the classifier in iteration t with those
from iteration t ? 1 by a factor of ?, such that
wt = ? ?w? + (1??) ?wt?1. We found ? = 0.1
gave good performance across the board.
5.5 Discussion
We implore the reader to avoid the natural tendency
to compare results using baseline vs. extended fea-
tures or between PBMT and SBMT on the same lan-
guage pair. Such discussions are indeed interesting,
and could lead to improvements in feature engineer-
ing or sartorial choices due to the outcome of wagers
(Goodale, 2008), but they distract from our thesis.
As can be seen in Table 1, for each of the 12 choices
of system, language pair, and feature set, the PRO
method performed nearly the same as or better than
MIRA and MERT on test data.
In Figure 5 we show the tune and test BLEU us-
ing the weights learned at every iteration for each
Urdu-English SBMT experiment. Typical of the rest
of the experiments, we can clearly see that PRO ap-
pears to proceed more monotonically than the other
methods. We quantified PRO?s stability as compared
to MERT by repeating the Urdu-English baseline
PBMT experiment five times with each configura-
tion. The tune and test BLEU at each iteration is
depicted in Figure 6. The standard deviation of the
final test BLEU of MERT was 0.13 across the five
experiment instances, while PRO had a standard de-
viation of just 0.05.
6 Related Work
Several works (Shen et al, 2004; Cowan et al,
2006; Watanabe et al, 2006) have used discrimina-
tive techniques to re-rank k-best lists for MT. Till-
mann and Zhang (2005) used a customized form of
 
17
 
18
 
19
 
20
 
21  0
 
5
 
10
 
15
 
20
 
25
 
30
4-ref BLEU
Iterati
on
Urdu-
Englis
h PBM
T tuni
ng sta
bility
TUNE TEST
MERT PRO
Figure 6: Tune and test curves of five repetitions of the
same Urdu-English PBMT baseline feature experiment.
PRO is more stable than MERT.
multi-class stochastic gradient descent to learn fea-
ture weights for an MT model. Och and Ney (2002)
used maximum entropy to tune feature weights but
did not compare pairs of derivations. Ittycheriah and
Roukos (2005) used a maximum entropy classifier to
train an alignment model using hand-labeled data.
Xiong et al (2006) also used a maximum entropy
classifier, in this case to train the reordering com-
ponent of their MT model. Lattice- and hypergraph-
based variants of MERT (Macherey et al, 2008; Ku-
mar et al, 2009) are more stable than traditional
MERT, but also require significant engineering ef-
forts.
7 Conclusion
We have described a simple technique for tuning
an MT system that is on par with the leading tech-
niques, exhibits reliable behavior, scales gracefully
to high-dimension feature spaces, and is remark-
ably easy to implement. We have demonstrated, via
a litany of experiments, that our claims are valid
and that this technique is widely applicable. It is
our hope that the adoption of PRO tuning leads to
fewer headaches during tuning and motivates ad-
vanced MT feature engineering research.
Acknowledgments
Thanks to Markus Dreyer, Kevin Knight, Saiyam
Kohli, Greg Langmead, Daniel Marcu, Dragos
Munteanu, and Wei Wang for their assistance.
Thanks also to the anonymous reviewers, especially
the reviewer who implemented PRO during the re-
view period and replicated our results.
1360
References
Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier,
Matt Deeds, Nicole Hamilton, and Greg Hullender.
2005. Learning to rank using gradient descent. In Pro-
ceedings of the 22nd International Conference on Ma-
chine Learning, ICML ?05, pages 89?96, Bonn, Ger-
many. ACM.
Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and
Hang Li. 2007. Learning to rank: From pairwise
approach to listwise approach. In Proceedings of the
24th International Conference on Machine Learning,
pages 129?136, Corvalis, OR.
Samidh Chatterjee and Nicola Cancedda. 2010. Mini-
mum error rate training by sampling the translation lat-
tice. In Proceedings of the 2010 Conference on Empir-
ical Methods in Natural Language Processing, pages
606?615, Cambridge, MA, October. Association for
Computational Linguistics.
Wei Chen, Tie-Yan Liu, Yanyan Lan, Zhi-Ming Ma, and
Hang Li. 2009. Ranking measures and loss functions
in learning to rank. In Y. Bengio, D. Schuurmans,
J. Lafferty, C. K. I. Williams, and A. Culotta, editors,
Advances in Neural Information Processing Systems
22, pages 315?323.
David Chiang, Steve DeNeefe, Yee Seng Chan, and
Hwee Tou Ng. 2008a. Decomposability of transla-
tion metrics for improved evaluation and efficient al-
gorithms. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 610?619, Honolulu, HI, October. Association
for Computational Linguistics.
David Chiang, Yuval Marton, and Philip Resnik. 2008b.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 224?233, Honolulu, HI, Oc-
tober. Association for Computational Linguistics.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, pages 218?226, Boulder, CO, June. Associa-
tion for Computational Linguistics.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1443?1452, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
Brooke Cowan, Ivona Kuc?erova?, and Michael Collins.
2006. A discriminative model for tree-to-tree trans-
lation. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing,
pages 232?241, Sydney, Australia, July. Association
for Computational Linguistics.
Koby Crammer and Yoram Singer. 2003. Ultraconserva-
tive online algorithms for multiclass problems. Jour-
nal of Machine Learning Research, 3:951?991.
Hal Daume? III. 2004. Notes on CG and LM-BFGS
optimization of logistic regression. Paper available at
http://pub.hal3.name#daume04cg-bfgs,
implementation available at http://hal3.name/
megam/, August.
Yoav Freund, Raj Iyer, Robert E. Schapire, and Yoram
Singer. 2003. An efficient boosting algorithm for
combining preferences. Journal of Machine Learning
Research, 4:933?969.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In HLT-
NAACL 2004: Main Proceedings, pages 273?280,
Boston, MA, May. Association for Computational Lin-
guistics.
Gloria Goodale. 2008. Language Weaver: fast
in translation. The Christian Science Monitor,
October 1. http://www.csmonitor.com/
Innovation/Tech-Culture/2008/1001/
language-weaver-fast-in-translation.
Jan Hajic?, Sandra Carberry, Stephen Clark, and Joakim
Nivre, editors. 2010. Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics. Association for Computational Linguistics, Upp-
sala, Sweden, July.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An update.
SIGKDD Explorations, 11(1).
Ralf Herbrich, Thore Graepel, and Klaus Obermayer.
1999. Support vector learning for ordinal regression.
In Proceedings of the 1999 International Conference
on Artificial Neural Networks, pages 97?102.
Abraham Ittycheriah and Salim Roukos. 2005. A max-
imum entropy word aligner for Arabic-English ma-
chine translation. In Proceedings of Human Language
Technology Conference and Conference on Empiri-
cal Methods in Natural Language Processing, pages
89?96, Vancouver, Canada, October. Association for
Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages 177?
1361
180, Prague, Czech Republic, June. Association for
Computational Linguistics.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate train-
ing and minimum bayes-risk decoding for translation
hypergraphs and lattices. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 163?171, Sun-
tec, Singapore, August. Association for Computational
Linguistics.
Alon Lavie and Michael J. Denkowski. 2009. The
METEOR metric for automatic evaluation of machine
translation. Machine Translation, 23(2?3):105?115,
September.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative ap-
proach to machine translation. In Proceedings of the
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association
for Computational Linguistics, pages 761?768, Syd-
ney, Australia, July. Association for Computational
Linguistics.
Wolfgang Macherey, Franz Josef Och, Ignacio Thayer,
and Jakob Uszkoreit. 2008. Lattice-based minimum
error rate training for statistical machine translation.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages 725?
734, Honolulu, HI, October. Association for Compu-
tational Linguistics.
Christopher Manning and Dan Klein. 2003. Optimiza-
tion, maxent models, and conditional estimation with-
out magic. Tutorial at HLT-NAACL 2003 and ACL
2003.
I. Dan Melamed, Ryan Green, and Joseph P. Turian.
2003. Precision and recall of machine translation. In
Companion Volume of the Proceedings of HLT-NAACL
2003 - Short Papers, pages 61?63, Edmonton, Canada,
May?June. Association for Computational Linguis-
tics.
Franz Och and Hermann Ney. 2000. Improved statistical
alignment models. In Proceedings of the 38th Annual
Meeting of the Association for Computational Linguis-
tics, pages 440?447, Hong Kong, October.
Franz Josef Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for sta-
tistical machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 295?302, Philadelphia, PA, July.
Association for Computational Linguistics.
Franz Och and Hermann Ney. 2004. The alignment tem-
plate approach to statistical machine translation. Com-
putational Linguistics, 30(4):417?449.
Franz Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 160?167, Sapporo, Japan, July. As-
sociation for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
40th Annual Meeting of the Association for Computa-
tional Linguistics, pages 311?318, Philadelphia, PA,
July. Association for Computational Linguistics.
Benjamin Roth, Andrew McCallum, Marc Dymetman,
and Nicola Cancedda. 2010. Machine translation us-
ing overlapping alignments and samplerank. In Pro-
ceedings of Association for Machine Translation in the
Americas, Denver, CO.
Libin Shen, Anoop Sarkar, and Franz Josef Och. 2004.
Discriminative reranking for machine translation. In
Daniel Marcu Susan Dumais and Salim Roukos, ed-
itors, HLT-NAACL 2004: Main Proceedings, pages
177?184, Boston, MA, May 2 - May 7. Association
for Computational Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Translation
in the Americas, pages 223?231.
Christoph Tillmann and Tong Zhang. 2005. A localized
prediction model for statistical machine translation. In
Proceedings of the 43rd Annual Meeting of the ACL,
pages 557?564, Ann Arbor, MI, June. Association for
Computational Linguistics.
Wei Wang. 2011. Personal communication.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2006. NTT statistical machine translation for
IWSLT 2006. In Proceedings of IWSLT 2006, pages
95?102.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for sta-
tistical machine translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 764?
773, Prague, Czech Republic, June. Association for
Computational Linguistics.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for sta-
tistical machine translation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 521?528, Sydney,
Australia, July. Association for Computational Lin-
guistics.
1362
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 239?248,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Source-side Preordering for Translation using Logistic Regression and
Depth-first Branch-and-Bound Search
?
Laura Jehl
?
Adri
`
a de Gispert
?
Mark Hopkins
?
William Byrne
?
?
Dept. of Computational Linguistics, Heidelberg University. 69120 Heidelberg, Germany
jehl@cl.uni-heidelberg.de
?
SDL Research. East Road, Cambridge CB1 1BH, U.K.
{agispert,mhopkins,bbyrne}@sdl.com
Abstract
We present a simple preordering approach
for machine translation based on a feature-
rich logistic regression model to predict
whether two children of the same node
in the source-side parse tree should be
swapped or not. Given the pair-wise chil-
dren regression scores we conduct an effi-
cient depth-first branch-and-bound search
through the space of possible children per-
mutations, avoiding using a cascade of
classifiers or limiting the list of possi-
ble ordering outcomes. We report exper-
iments in translating English to Japanese
and Korean, demonstrating superior per-
formance as (a) the number of crossing
links drops by more than 10% absolute
with respect to other state-of-the-art pre-
ordering approaches, (b) BLEU scores im-
prove on 2.2 points over the baseline with
lexicalised reordering model, and (c) de-
coding can be carried out 80 times faster.
1 Introduction
Source-side preordering for translation is the task
of rearranging the order of a given source sen-
tence so that it best resembles the order of the tar-
get sentence. It is a divide-and-conquer strategy
aiming to decouple long-range word movement
from the core translation task. The main advan-
tage is that translation becomes computationally
cheaper as less word movement needs to be con-
sidered, which results in faster and better transla-
tions, if preordering is done well and efficiently.
Preordering also can facilitate better estimation
of alignment and translation models as the paral-
lel data becomes more monotonically-aligned, and
?
This work was done during an internship of the first au-
thor at SDL Research, Cambridge.
translation gains can be obtained for various sys-
tem architectures, e.g. phrase-based, hierarchical
phrase-based, etc.
For these reasons, preordering has a clear re-
search and commercial interest, as reflected by the
extensive previous work on the subject (see Sec-
tion 2). From these approaches, we are particu-
larly interested in those that (i) involve little or no
human intervention, (ii) require limited computa-
tional resources at runtime, and (iii) make use of
available linguistic analysis tools.
In this paper we propose a novel preordering
approach based on a logistic regression model
trained to predict whether to swap nodes in
the source-side dependency tree. For each pair
of sibling nodes in the tree, the model uses a
feature-rich representation that includes lexical
cues to make relative reordering predictions be-
tween them. Given these predictions, we conduct
a depth-first branch-and-bound search through
the space of possible permutations of all sibling
nodes, using the regression scores to guide the
search. This approach has multiple advantages.
First, the search for permutations is efficient and
does not require specific heuristics or hard limits
for nodes with many children. Second, the inclu-
sion of the regression prediction directly into the
search allows for finer-grained global decisions as
the predictions that the model is more confident
about are preferred. Finally, the use of a single
regression model to handle any number of child
nodes avoids incurring sparsity issues, while al-
lowing the integration of a vast number of features
into the preordering model.
We empirically contrast our proposed method
against another preordering approach based on
automatically-extracted rules when translating En-
glish into Japanese and Korean. We demonstrate
a significant reduction in number of crossing links
of more than 10% absolute, as well as translation
gains of over 2.2 BLEU points over the baseline.
239
We also show it outperforms a multi-class classifi-
cation approach and analyse why this is the case.
2 Related work
One useful way to organize previous preordering
techniques is by how they incorporate linguistic
knowledge.
On one end of the spectrum we find those ap-
proaches that rely on syntactic parsers and hu-
man knowledge, typically encoded via a set of
hand-crafted rules for parse tree rewriting or trans-
formation. Examples of these can be found
for French-English (Xia and McCord, 2004),
German-English (Collins et al., 2005), Chinese-
English (Wang et al., 2007), English-Arabic (Badr
et al., 2009), English-Hindi (Ramanathan et al.,
2009), English-Korean (Hong et al., 2009), and
English-Japanese (Lee et al., 2010; Isozaki et
al., 2010). A generic set of rules for transform-
ing SVO to SOV languages has also been de-
scribed (Xu et al., 2009). The main advantage of
these approaches is that a relatively small set of
good rules can yield significant improvements in
translation. The common criticism they receive is
that they are language-specific.
On the other end of the spectrum, there are pre-
ordering models that rely neither on human knowl-
edge nor on syntactic analysis, but only on word
alignments. One such approach is to form a cas-
cade of two translation systems, where the first
one translates the source to its preordered ver-
sion (Costa-juss`a and Fonollosa, 2006). Alterna-
tively, one can define models that assign a cost to
the relative position of each pair of words in the
sentence, and search for the sequence that opti-
mizes the global score as a linear ordering prob-
lem (Tromble and Eisner, 2009) or as a travel-
ing salesman problem (Visweswariah et al., 2011).
Yet another line of work attempts to automatically
induce a parse tree and a preordering model from
word alignments (DeNero and Uszkoreit, 2011;
Neubig et al., 2012). These approaches are at-
tractive due to their minimal reliance on linguistic
knowledge. However, their findings reveal that the
best performance is obtained when using human-
aligned data which is expensive to create.
Somewhere in the middle of the spectrum are
works that rely on automatic source-language syn-
tactic parses, but no direct human intervention.
Preordering rules can be automatically extracted
from word alignments and constituent trees (Li
et al., 2007; Habash, 2007; Visweswariah et
al., 2010), dependency trees (Genzel, 2010) or
predicate-argument structures (Wu et al., 2011),
or simply part-of-speech sequences (Crego and
Mari?no, 2006; Rottmann and Vogel, 2007). Rules
are assigned a cost based on Maximum En-
tropy (Li et al., 2007) or Maximum Likelihood es-
timation (Visweswariah et al., 2010), or directly
on their ability to make the training corpus more
monotonic (Genzel, 2010). The latter performs
very well in practice but comes at the cost of a
brute-force extraction heuristic that cannot incor-
porate lexical information. Recently, other ap-
proaches treat ordering the children of a node as
a learning to rank (Yang et al., 2012) or discrimi-
native multi-classification task (Lerner and Petrov,
2013). These are appealing for their use of finer-
grained lexical information, but they struggle to
adequately handle nodes with multiple children.
Our approach is closely related to this latter
work, as we are interested in feature-rich discrim-
inative approaches that automatically learn pre-
ordering rules from source-side dependency trees.
Similarly to Yang et al. (2012) we train a large
discriminative linear model, but rather than model
each child?s position in an ordered list of children,
we model a more natural pair-wise swap / no-swap
preference (like Tromble and Eisner (2009) did at
the word level). We then incorporate this model
into a global, efficient branch-and-bound search
through the space of permutations. In this way, we
avoid an error-prone cascade of classifiers or any
limit on the possible ordering outcomes (Lerner
and Petrov, 2013).
3 Preordering using logistic regression
and branch-and-bound search
Like Genzel (2010), our method starts with depen-
dency parses of source sentences (which we con-
vert to shallow constituent trees; see Figure 1 for
an example), and reorders the source text by per-
muting sibling nodes in the parse tree. For each
non-terminal node, we first apply a logistic regres-
sion model which predicts, for each pair of child
nodes, the probability that they should be swapped
or kept in their original order. We then apply
a depth-first branch-and-bound search to find the
global optimal reordering of children.
240
VB
he
NN
1
could
MD
2
stand
VB
3
NN
4
the
DT
smell
NN
nsubj
aux
HEAD
dobj
det HEAD
Figure 1: Shallow constituent tree generated from
the dependency tree. Non-terminal nodes inherit
the tag from the head.
3.1 Logistic regression
We build a regression model that assigns a prob-
ability of swapping any two sibling nodes, a and
b, in the source-side dependency tree. The proba-
bility of swapping them is denoted p(a, b) and the
probability of keeping them in their original order
is 1 ? p(a, b). We use LIBLINEAR (Fan et al.,
2008) for training an L1-regularised logistic re-
gression model based on positively and negatively
labelled samples.
3.1.1 Training data
We generate training examples for the logistic re-
gression from word-aligned parallel data which is
annotated with source-side dependency trees. For
each non-terminal node, we extract all possible
pairs of child nodes. For each pair, we obtain a
binary label y ? {?1, 1} by calculating whether
swapping the two nodes would reduce the number
of crossing alignment links. The crossing score of
having two nodes a and b in the given order is
cs(a, b) := |{(i, j) ? A
a
?A
b
: i > j}|
where A
a
and A
b
are the target-side positions to
which the words spanned by a and b are aligned.
The label is then given as
y(a, b) =
{
1 , cs(a, b) > cs(b, a)
?1 , cs(b, a) > cs(a, b)
Instances for which cs(a, b) = cs(b, a) are not
included in the training data. This usually happens
if either A
a
or A
b
is empty, and in this case the
alignments provide no indication of which order
is better. We also discard any samples from nodes
that have more than 16 children, as these are rare
cases that often result from parsing errors.

1
2 3 4
2 3
2
2
1
. . .
. . .
Figure 2: Branch-and-bound search: Partial search
space of permutations for a dependency tree node
with four children. The gray node marks a goal
node. For the root node of the tree in Figure 1, the
permutation corresponding to this path (1,4,3,2)
would produce ?he the smell stand could?.
3.1.2 Features
Using a machine learning setup allows us to in-
corporate fine-grained information in the form of
features. We use the following features to charac-
terise pairs of nodes:
l The dependency labels of each node
t The part-of-speech tags of each node.
hw The head words and classes of each node.
lm, rm The left-most and right-most words and classes
of a node.
dst The distances between each node and the head.
gap If there is a gap between nodes, the left-most
and right-most words and classes in the gap.
In order to keep the size of our feature space
manageable, we only consider features which oc-
cur at least 5 times
1
. For the lexical features, we
use the top 100 vocabulary items from our training
data, and 51 clusters generated by mkcls (Och,
1999). Similarly to previous work (Genzel, 2010;
Yang et al., 2012), we also explore feature con-
junctions. For the tag and label classes, we gen-
erate all possible combinations up to a given size.
For the lexical and distance features, we explicitly
specify conjunctions with the tag and label fea-
tures. Results for various feature configurations
are discussed in Section 4.3.1.
3.2 Search
For each non-terminal node in the source-side de-
pendency tree, we search for the best possible
1
Additional feature selection is achieved through L1-
regularisation.
241
permutation of its children. We define the score
of a permutation pi as the product of the proba-
bilities of its node pair orientations (swapped or
unswapped):
score(pi) =
?
1?i<j?k|pi[i]>pi[j]
p(i, j)
?
?
1?i<j?k|pi[i]<pi[j]
1? p(i, j)
Here, we represent a permutation pi of k nodes
as a k-length sequence containing each integer in
{1, ..., k} exactly once. Define a partial permu-
tation of k nodes as a k
?
< k length sequence
containing each integer in {1, ..., k} at most once.
We can construct a search space over partial per-
mutations in the natural way (see Figure 2). The
root node represents the empty sequence  and has
score 1. Then, given a search node representing
a k
?
-length partial permutation pi
?
, its successor
nodes are obtained by extending it by one element:
score(pi
?
? ?i?) = score(pi
?
)
?
?
j?V |i>j
p(i, j)
?
?
j?V |i<j
1? p(i, j)
where V = {1, ..., k}\(pi
?
? ?i?) is the set of source
child positions that have not yet been visited. Ob-
serve that the nodes at search depth k correspond
exactly to the set of complete permutations. To
search this space, we employ depth-first branch-
and-bound (Balas and Toth, 1983) as our search
algorithm. The idea of branch-and-bound is to
remember the best scoring goal node found thus
far, abandoning any partial paths that cannot lead
to a better scoring goal node. Algorithm 1 gives
pseudocode for the algorithm
2
. If the initial bound
(bound
0
) is set to 0, the search is guaranteed to
find the optimal solution. By raising the bound,
which acts as an under-estimate of the best scor-
ing permutation, search can be faster but possibly
fail to find any solution. All our experiments were
done with bound
0
= 0, i.e. exact search, but we
discuss search time in detail and pruning alterna-
tives in Section 4.3.2.
Since we use a logistic regression model and in-
corporate its predictions directly as swap probabil-
ities, our search prefers those permutations with
swaps which the model is more confident about.
2
See (Poole and Mackworth, 2010) for more details and a
worked example.
Algorithm 1 Depth-first branch-and-bound
Require: k: maximum sequence length, : empty sequence,
bound
0
: initial bound
procedure BNBSEARCH(, bound
0
, k)
best path? ?
bound? bound
0
SEARCH(??)
return best path
end procedure
procedure SEARCH(pi
?
)
if score(pi
?
) > bound then
if |pi
?
| = k then
best path? ?pi
?
?
bound? score(pi
?
)
return
else
for each i ? {1, ..., k}\pi
?
do
SEARCH(pi
?
? ?i?)
end for
end if
end if
end procedure
4 Experiments
4.1 Setup
We report translation results in English-to-
Japanese/Korean. Our corpora are comprised of
generic parallel data extracted from the web, with
some documents extracted manually and some au-
tomatically crawled. Both have about 6M sentence
pairs and roughly 100M words per language.
The dev and test sets are also generic. Source
sentences were extracted from the web and one
target reference was produced by a bilingual
speaker. These sentences were chosen to evenly
represent 10 domains, including world news,
chat/SMS, health, sport, science, business, and
others. The dev/test sets contain 602/903 sen-
tences and 14K/20K words each. We do English
part-of-speech tagging using SVMTool (Gim?enez
and M`arquez, 2004) and dependency parsing us-
ing MaltParser (Nivre et al., 2007).
For translation experiments, we use a phrase-
based decoder that incorporates a set of standard
features and a hierarchical reordering model (Gal-
ley and Manning, 2008) with weights tuned us-
ing MERT to optimize the character-based BLEU
score on the dev set. The Japanese and Korean lan-
guage models are 5-grams estimated on > 350M
words of generic web text.
For training the logistic regression model, we
automatically align the parallel training data and
intersect the source-to-target and target-to-source
alignments. We reserve a random 5K-sentence
242
approach EJ cs (%) EK cs (%)
rule-based (Genzel, 2010) 61.9 64.2
multi-class 65.2 -
df-bnb 51.4 51.8
Table 1: Percentage of the original crossing score
on the heldout set, obtained after applying each
preordering approach in English-Japanese (EJ,
left) and Korean (EK, right). Lower is better.
subset for intrinsic evaluation of preordering, and
use the remainder for model parameter estimation.
We evaluate our preordering approach with lo-
gistic regression and depth-first branch-and-bound
search (in short, ?df-bnb?) both in terms of reorder-
ing via crossing score reduction on the heldout set,
and in terms of translation quality as measured by
character-based BLEU on the test set.
4.2 Preordering baselines
We contrast our work against two data-driven pre-
ordering approaches. First, we implemented the
rule-based approach of Genzel (2010) and opti-
mised its multiple parameters for our task. We
report only the best results achieved, which corre-
spond to using ?100K training sentences for rule
extraction, applying a sliding window width of 3
children, and creating rule sequences of?60 rules.
This approach cannot incorporate lexical features
as that would make the brute-force rule extraction
algorithm unmanageable.
We also implemented a multi-class classifica-
tion setup where we directly predict complete per-
mutations of children nodes using multi-class clas-
sification (Lerner and Petrov, 2013). While this
is straightforward for small numbers of children,
it leads to a very large number of possible per-
mutations for larger sets of children nodes, mak-
ing classification too difficult. While Lerner and
Petrov (2013) use a cascade of classifiers and im-
pose a hard limit on the possible reordering out-
comes to solve this, we follow Genzel?s heuristic:
rather than looking at the complete set of children,
we apply a sliding window of size 3 starting from
the left, and make classification/reordering deci-
sions for each window separately. Since the win-
dows overlap, decisions made for the first window
affect the order of nodes in the second window,
etc. We address this by soliciting decisions from
the classifier on the fly as we preorder. One lim-
Figure 3: Crossing scores and classification accu-
racy improve with training data size.
itation of this approach is that it is able to move
children only within the window. We try to rem-
edy this by applying the method iteratively, each
time re-training the classifier on the preordered
data from the previous run.
4.3 Crossing score
We now report contrastive results in the intrin-
sic preordering task, as measured by the num-
ber of crossing links (Genzel, 2010; Yang et al.,
2012) on the 5K held-out set. Without preorder-
ing, there is an average of 22.2 crossing links in
English-Japanese and 20.2 in English-Korean. Ta-
ble 1 shows what percentage of these links re-
main after applying each preordering approach to
the data. We find that the ?df-bnb? method out-
performs the other approaches in both language
pairs, achieving more than 10 additional percent-
age points reduction over the rule-based approach.
Interestingly, the multi-class approach is not able
to match the rule-based approach despite using ad-
ditional lexical cues. We hypothesise that this is
due to the sliding window heuristic, which causes
a mismatch in train-test conditions: while samples
are not independent of each other at test time due
to window overlaps, they are considered to be so
when training the classifier.
4.3.1 Impact of training size and feature
configuration
We now report the effects of feature configura-
tion and training data size for the English-Japanese
case. We assess our ?df-bnb? approach in terms of
the classification accuracy of the trained logistic
243
features used acc (%) cs (%)
l,t,hw,lm,rm,dst,gap 82.43 51.3
l,t,hw,lm,rm,dst 82.44 51.4
l,t,hw,lm,rm 82.32 53.1
l,t,hw 82.02 55
l,t 81.07 58.4
Table 2: Ablation tests showing crossing scores
and classification accuracy as features are re-
moved. All models were trained on 8M samples.
regression model (using it to predict ?1 labels in
the held-out set) and by the percentage of crossing
alignment links reduced by preordering.
Figure 3 shows the performance of the logistic
regression model over different training set sizes,
extracted from the training corpus as described in
Section 3. We observe a constant increase in pre-
diction accuracy, mirrored by a steady decrease in
crossing score. However, gains are less for more
than 8M training examples. Note that a small vari-
ation in accuracy can produce a large variation in
crossing score if two nodes are swapped which
have a large number of crossing alignments.
Table 2 shows an ablation test for various fea-
ture configurations. We start with all features, in-
cluding head word and class (hw), left-most and
right-most word in each node?s span (lm, rm), each
node?s distance to the head (dst), and left-most
and right-most word of the gap between nodes
(gap). We then proceed by removing features to
end with only label and tag features (l,t), as in
Genzel (2010). For each configuration, we gener-
ated all tag- and label- combinations of size 2. We
then specified combinations between tag and label
and all other features. For the lexical features we
always used conjunctions of the word itself, and its
class. Class information is included for all words,
not just those in the top 100 vocabulary. Table 2
shows that lexical and distance feature groups con-
tribute to prediction accuracy and crossing score,
except for the gap features, which we omit from
further experiments.
4.3.2 Run time
We now demonstrate the efficiency of branch-and-
bound search for the problem of finding the opti-
mum permutation of n children at runtime. Even
though in the worst case the search could ex-
plore all n! permutations, making it prohibitive for
Figure 4: Average number of nodes explored in
branch-and-bound search by number of children.
nodes with many children, in practice this does
not happen. Many low-scoring paths are discarded
early by branch-and-bound search so that the opti-
mal solution can be found quickly. The top curve
in Figure 4 shows the average number of nodes
explored in searches run on our validation set (5K
sentences) as a function of the number of children.
All instances are far from the worst case
3
.
In our experiments, the time needed to conduct
exact search (bound
0
= 0) was not a problem ex-
cept for a few bad cases (nodes with more than 16
children), which we simply chose not to preorder;
in our data, 90% of the nodes have less than 6 chil-
dren, while only 0.9% have 10 children or more, so
this omission does not affect performance notice-
ably. We verified this on our held-out set, by car-
rying out exhaustive searches. We found that not
preordering nodes with 16 children did not worsen
the crossing score. In fact, setting a harsher limit
of 10 nodes would still produce a crossing score
of 51.9%, compared to the best score of 51.4%.
There are various ways to speed up the search,
if needed. First, one could impose a hard limit
on the number of explored nodes
4
. As shown
in Figure 4, a limit of 4K would still allow ex-
act search on average for permutations of up to
11 children, while stopping search early for more
children. We tested this for limits of 1K/4K nodes
and obtained crossing scores of 51.9/51.5%. Al-
ternatively, one could define a higher initial bound;
since the score of a path is a product of proba-
bilities, one would select a threshold probability
3
Note that 12!?479M nodes, whereas our search finds the
optimal permutation path after exploring <10K nodes.
4
As long as the limit exceeds the permutation length, a
solution will always be found as search is depth-first.
244
d approach ?LRM ? +LRM ?
baseline 25.39 - 26.62 -
rule-based 25.93 +0.54 27.65 +1.03
10
multi-class 25.60 +0.21 26.10 ?0.52
df-bnb 26.73 +1.34 28.09 +1.47
baseline 25.07 - 25.92 -
rule-based 26.35 +1.28 27.54 +1.62
4
multi-class 25.37 +0.30 26.31 +0.39
df-bnb 26.98 +1.91 28.13 +2.21
Table 3: English-Japanese BLEU scores with var-
ious preordering approaches (and improvement
over baseline) under two distortion limits d. Re-
sults reported both excluding and including lexi-
calised reordering model features (LRM).
p and calculate a bound depending on the size n
of the permutation as bound
0
= p
n?(n?1)
2
. Exam-
ples of this would be the lower curves of Figure 4.
The curve labels show the crossing score produced
with each threshold, and in parenthesis the per-
centage of searches that fail to find a solution with
a better score than bound
0
, in which case children
are left in their original order. As shown, this strat-
egy proves less effective than simply limiting the
number of explored nodes, because the more fre-
quent cases with less children remain unaffected.
4.4 Translation performance
Table 3 reports English-Japanese translation re-
sults for two different values of the distortion limit
d, i.e. the maximum number of source words that
the decoder is allowed to jump during search. We
draw the following conclusions. Firstly, all the
preordering approaches outperform the baseline
and the BLEU score gain they provide increases as
the distortion limit decreases. This is further anal-
ysed in Figure 5, where we report BLEU as a func-
tion of the distortion limit in decoding for both
English-Japanese and English-Korean. This re-
veals the power of preordering as a targeted strat-
egy to obtain high performance at fast decoding
times, since d can be drastically reduced with-
out performance degradation which leads to huge
decoding speed-ups; this is consistent with the
observations in (Xu et al., 2009; Genzel, 2010;
Visweswariah et al., 2011). We also find that with
preordering it is possible to apply harsher pruning
conditions in decoding while still maintaining the
Figure 5: BLEU scores as a function of distor-
tion limit in decoder (+LRM case). Top: English-
Japanese. Bottom: English-Korean.
exact same performance, achieving further speed-
ups. With preordering, our system is able to de-
code 80 times faster while producing translation
output of the same quality.
Secondly, we observe that the preordering
gains, which are correlated with the crossing score
reductions of Table 1, are largely orthogonal to
the gains obtained when incorporating a lexi-
calised reordering model (LRM). In fact, preorder-
ing gains are slightly larger with LRM, suggest-
ing that this reordering model can be better esti-
mated with preordered text. This echoes the notion
that reordering models are particularly sensitive
to alignment noise (DeNero and Uszkoreit, 2011;
Neubig et al., 2012; Visweswariah et al., 2013),
and that a ?more monotonic? training corpus leads
to better translation models.
Finally, ?df-bnb? outperforms all other preorder-
ing approaches, and achieves an extra 0.5?0.8
BLEU over the rule-based one even at zero distor-
tion limit. This is consistent with the substantial
crossing score reductions reported in Section 4.3.
We argue that these improvements are due to
the usage of lexical features to facilitate finer-
grained ordering decisions, and to our better
search through the children permutation space
which is not restricted by sliding windows, does
245
E
x
a
m
p
l
e
1
reference [
1
?????]
Barlow
[
2
???]
the smell
[
3
??]
endure
[
4
??????]
could
[
5
???]
hoped
[
6
?]
source [
1
Barlow] [
5
hoped] he [
4
could] [
3
stand] [
2
the smell] [
6
.]
preordered [
1
Barlow] he [
2
the smell] [
3
stand] [
4
could] [
5
hoped] [
6
.]
E
x
a
m
p
l
e
2
reference [
1
????]
my own
[
2
??]
experience
[
3
????]
in
, [
4
???????]
Rosa Parks
[
5
???]
called
[
6
???]
black
[
7
???]
woman
, [
8
???]
one day
[
9
????????]
somehow
[
10
???]
bus of
[
11
?????]
back seat in
[
12
??]
sit
??? [
13
????]
told being
[
14
???]
of
[
15
?????]
was fed up with
?
source [
3
In] [
1
my own] [
2
experience] , a [
6
black] [
7
woman] [
5
named] [
4
Rosa Parks] [
14
was just tired] [
8
one day]
[
14
of] [
13
being told] [
12
to sit] [
11
in the back] [
10
of the bus] .
rule-based [
1
my own] [
2
experience] [
3
In] [
14
was just tired] [
13
being told] [
10
the bus of] [
11
the back in] [
12
sit to] [
14
of]
[
8
one day] , [
6
a black] [
7
woman] [
4
Rosa Parks] [
5
named] .
df-bnb [
1
my own] [
2
experience] [
3
In] , [
5
named] [
6
a black] [
7
woman] [
4
Rosa Parks] [
10
the bus of] [
11
the back in]
[
12
sit to] [
13
told being] [
14
of] [
8
one day] [
14
was just tired] .
E
x
a
m
p
l
e
3
reference [
1
????]
we
?[
2
????]
quite
[
3
???]
Xi?an
[
4
??]
like
[
5
?]
to
[
6
?????]
come have
?
source [
1
we] [
6
have come] [
5
to] [
2
quite] [
4
like] [
1
xi?an] .
rule-based [
1
we] [
2
quite] [
4
like] [
3
xi?an] [
5
to] [
6
come have] .
df-bnb [
1
we] have [
2
quite] [
3
xi?an] [
4
like] [
5
to] [
6
come] .
baseline ????????????????
rule-based ??????????????????
df-bnb ????????????????
Table 4: Examples from our test data illustrating the differences between the preordering approaches.
not depend heavily on getting the right decision
in a multi-class scenario, and which incorporates
regression to carry out a score-driven search.
4.5 Analysis
Table 4 gives three English-Japanese examples
to illustrate the different preordering approaches.
The first, very short, example is preordered cor-
rectly by the rule-based and the df-bnb approach,
as the order of the brackets matches the order of
the Japanese reference.
For longer sentences we see more differences
between approaches, as illustrated by Example 2.
In this case, both approaches succeed at moving
prepositions to the back of the phrase (?my expe-
rience in?, ?the bus of?). However, while the df-
bnb approach correctly moves the predicate of the
second clause (?was just tired?) to the back, the
rule-based approach incorrectly moves the subject
(?a black woman named Rosa Parks?) to this posi-
tion - possibly because of the verb ?named? which
occurs in the phrase. This could be an indication
that the df-bnb is better suited for more compli-
cated constructions. With the exception of phrases
4 and 8, all other phrases are in the correct order
in the df-bnb reordering. None of the approaches
manage to reorder ?a black woman named Rosa
Parks? to the correct order.
Example 3 shows that the translations into
Japanese also reflect preordering quality. The
original source results in ?like? being translated
as the main verb (which is incorrectly interpreted
as ?to be like, to be equal to?). The rule-based
version correctly moves ?have come? to the end,
but fails to swap ?xi?an? and ?like?, resulting in
?come? being interpreted as a full verb, rather than
an auxiliary. Only the df-bnb version achieves al-
most perfect reordering, resulting in the correct
word choice of ?? (to get to, to become) for
?have come to?.
5
5 Conclusion
We have presented a novel preordering approach
that estimates a preference for swapping or not
swapping pairs of children nodes in the source-
side dependency tree by training a feature-rich
logistic regression model. Given the pair-wise
scores, we efficiently search through the space
of possible children permutations using depth-first
branch-and-bound search. The approach is able
to incorporate large numbers of features includ-
ing lexical cues, is efficient at runtime even with
a large number of children, and proves superior to
other state-of-the-art preordering approaches both
in terms of crossing score and translation perfor-
mance.
5
This translation is still not perfect, since it uses the wrong
level of politeness, an important distinction in Japanese.
246
References
Ibrahim Badr, Rabih Zbib, and James Glass. 2009.
Syntactic Phrase Reordering for English-to-Arabic
Statistical Machine Translation. In Proceedings of
EACL, pages 86?93, Athens, Greece.
Egon Balas and Paolo Toth. 1983. Branch and
Bound Methods for the Traveling Salesman Prob-
lem. Carnegie-Mellon Univ. Pittsburgh PA Manage-
ment Sciences Research Group.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause Restructuring for Statistical Machine
Translation. In Proceedings of ACL, pages 531?540,
Ann Arbor, Michigan.
Marta R. Costa-juss`a and Jos?e A. R. Fonollosa. 2006.
Statistical Machine Reordering. In Proceedings of
EMNLP, pages 70?76, Sydney, Australia.
Josep M. Crego and Jos?e B. Mari?no. 2006. Integra-
tion of POStag-based Source Reordering into SMT
Decoding by an Extended Search Graph. In Pro-
ceedings of AMTA, pages 29?36, Cambridge, Mas-
sachusetts.
John DeNero and Jakob Uszkoreit. 2011. Inducing
Sentence Structure from Parallel Corpora for Re-
ordering. In Proceedings of EMNLP, pages 193?
203, Edinburgh, Scotland, UK.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A Library for Large Linear Classification. Journal
of Machine Learning Research, 9:1871?1874.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reorder-
ing Model. In Proceedings of EMNLP, pages 847?
855, Honolulu, Hawaii.
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine trans-
lation. In Proceedings of COLING, pages 376?384,
Beijing, China.
Jes?us Gim?enez and Llu??s M`arquez. 2004. SVMTool:
A general POS tagger generator based on Support
Vector Machines. In Proceedings of LREC, Lisbon,
Portugal.
Nizar Habash. 2007. Syntactic Preprocessing for Sta-
tistical Machine Translation. In Proceedings of MT-
Summit, pages 215?222, Copenhagen, Denmark.
Gumwon Hong, Seung-Wook Lee, and Hae-Chang
Rim. 2009. Bridging Morpho-Syntactic Gap be-
tween Source and Target Sentences for English-
Korean Statistical Machine Translation. In Proceed-
ings of ACL-IJCNLP, pages 233?236, Suntec, Sin-
gapore.
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010. Head Finalization: A Simple Re-
ordering Rule for SOV Languages. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 244?251, Up-
psala, Sweden.
Young-Suk Lee, Bing Zhao, and Xiaoqian Luo.
2010. Constituent Reordering and Syntax Models
for English-to-Japanese Statistical Machine Trans-
lation. In Proceedings of COLING, pages 626?634,
Beijing, China.
Uri Lerner and Slav Petrov. 2013. Source-Side Clas-
sifier Preordering for Machine Translation. In Pro-
ceedings of EMNLP, Seattle, USA.
Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li,
Ming Zhou, and Yi Guan. 2007. A Probabilistic
Approach to Syntax-based Reordering for Statistical
Machine Translation. In Proceedings of ACL, pages
720?727, Prague, Czech Republic.
Graham Neubig, Taro Watanabe, and Shinsuke Mori.
2012. Inducing a Discriminative Parser to Optimize
Machine Translation Reordering. In Proceedings of
EMNLP-CoNLL, pages 843?853, Jeju Island, Korea.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G?ulsen Eryigit, Sandra K?ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95?135.
Franz Josef Och. 1999. An efficient method for de-
termining bilingual word classes. In Proceedings of
EACL, pages 71?76, Bergen, Norway.
David L. Poole and Alan K. Mackworth. 2010. Ar-
tificial Intelligence: Foundations of Computational
Agents. Cambridge University Press. Full text on-
line at http://artint.info.
Ananthakrishnan Ramanathan, Hansraj Choudhary,
Avishek Ghosh, and Pushpak Bhattacharyya. 2009.
Case markers and Morphology: Addressing the crux
of the fluency problem in English-Hindi SMT. In
Proceedings of ACL-IJCNLP, pages 800?808, Sun-
tec, Singapore.
Kay Rottmann and Stephan Vogel. 2007. Word Re-
ordering in Statistical Machine Translation with a
POS-Based Distortion Model. In Proceedings of
TMI, pages 171?180, Sk?ovde, Sweden.
Roy Tromble and Jason Eisner. 2009. Learning linear
ordering problems for better translation. In Proceed-
ings of EMNLP, pages 1007?1016, Singapore.
Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen,
Vijil Chenthamarakshan, and Nandakishore Kamb-
hatla. 2010. Syntax based reordering with auto-
matically derived rules for improved statistical ma-
chine translation. In Proceedings of COLING, pages
1119?1127, Beijing, China.
247
Karthik Visweswariah, Rajakrishnan Rajkumar, Ankur
Gandhe, Ananthakrishnan Ramanathan, and Jiri
Navratil. 2011. A word reordering model for
improved machine translation. In Proceedings of
EMNLP, pages 486?496, Edinburgh, United King-
dom.
Karthik Visweswariah, Mitesh M. Khapra, and Anan-
thakrishnan Ramanathan. 2013. Cut the noise: Mu-
tually reinforcing reordering and alignments for im-
proved machine translation. In Proceedings of ACL,
pages 1275?1284, Sofia, Bulgaria.
Chao Wang, Michael Collins, and Philipp Koehn.
2007. Chinese Syntactic Reordering for Statistical
Machine Translation. In Proceedings of EMNLP-
CoNLL, pages 737?745, Prague, Czech Republic.
Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Hajime
Tsukada, and Masaaki Nagata. 2011. Extracting
Pre-ordering Rules from Predicate-Argument Struc-
tures. In Proceedings of IJCNLP, pages 29?37, Chi-
ang Mai, Thailand.
Fei Xia and Michael McCord. 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. In Proceedings of COLING,
Geneva, Switzerland.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a Dependency Parser to Improve
SMT for Subject-Object-Verb Languages. In Pro-
ceedings of HTL-NAACL, pages 245?253, Boulder,
Colorado.
Nan Yang, Mu Li, Dongdong Zhang, and Nenghai Yu.
2012. A ranking-based approach to word reordering
for statistical machine translation. In Proceedings of
ACL, pages 912?920, Jeju Island, Korea.
248
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1416?1424,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Models of Translation Competitions
Mark Hopkins and Jonathan May
SDL Research
6060 Center Drive, Suite 150
Los Angeles, CA 90045
{mhopkins,jmay}@sdl.com
Abstract
What do we want to learn from a trans-
lation competition and how do we learn
it with confidence? We argue that a dis-
proportionate focus on ranking competi-
tion participants has led to lots of differ-
ent rankings, but little insight about which
rankings we should trust. In response, we
provide the first framework that allows an
empirical comparison of different analy-
ses of competition results. We then use
this framework to compare several analyt-
ical models on data from the Workshop on
Machine Translation (WMT).
1 The WMT Translation Competition
Every year, the Workshop on Machine Transla-
tion (WMT) conducts a competition between ma-
chine translation systems. The WMT organizers
invite research groups to submit translation sys-
tems in eight different tracks: Czech to/from En-
glish, French to/from English, German to/from
English, and Spanish to/from English.
For each track, the organizers also assemble a
panel of judges, typically machine translation spe-
cialists.1 The role of a judge is to repeatedly rank
five different translations of the same source text.
Ties are permitted. In Table 1, we show an ex-
ample2 where a judge (we?ll call him ?jdoe?) has
ranked five translations of the French sentence ?Il
ne va pas.?
Each such elicitation encodes ten pairwise com-
parisons, as shown in Table 2. For each compe-
tition track, WMT typically elicits between 5000
and 20000 comparisons. Once the elicitation pro-
cess is complete, WMT faces a large database
of comparisons and a question that must be an-
swered: whose system is the best?
1Although in recent competitions, some of the judging has
also been crowdsourced (Callison-Burch et al, 2010).
2The example does not use actual system output.
rank system translation
1 bbn ?He does not go.?
2 (tie) uedin ?He goes not.?
2 (tie) jhu ?He did not go.?
4 cmu ?He go not.?
5 kit ?He not go.?
Table 1: WMT elicits preferences by asking
judges to simultaneously rank five translations,
with ties permitted. In this (fictional) example, the
source sentence is the French ?Il ne va pas.?
source text sys1 sys2 judge preference
?Il ne va pas.? bbn cmu jdoe 1
?Il ne va pas.? bbn jhu jdoe 1
?Il ne va pas.? bbn kit jdoe 1
?Il ne va pas.? bbn uedin jdoe 1
?Il ne va pas.? cmu jhu jdoe 2
?Il ne va pas.? cmu kit jdoe 1
?Il ne va pas.? cmu uedin jdoe 2
?Il ne va pas.? jhu kit jdoe 1
?Il ne va pas.? jhu uedin jdoe 0
?Il ne va pas.? kit uedin jdoe 2
Table 2: Pairwise comparisons encoded by Ta-
ble 1. A preference of 0 means neither translation
was preferred. Otherwise the preference specifies
the preferred system.
2 A Ranking Problem
For several years, WMT used the following heuris-
tic for ranking the translation systems:
ORIGWMT(s) = win(s) + tie(s)win(s) + tie(s) + loss(s)
For system s, win(s) is the number of pairwise
comparisons in which s was preferred, loss(s) is
the number of comparisons in which s was dispre-
ferred, and tie(s) is the number of comparisons in
which s participated but neither system was pre-
ferred.
Recently, (Bojar et al, 2011) questioned the ad-
equacy of this heuristic through the following ar-
1416
gument. Consider a competition with systems A
and B. Suppose that the systems are different but
equally good, such that one third of the time A
is judged better than B, one third of the time B
is judged better than A, and one third of the time
they are judged to be equal. The expected values
of ORIGWMT(A) and ORIGWMT(B) are both
2/3, so the heuristic accurately judges the systems
to be equivalently good. Suppose however that
we had duplicated B and had submitted it to the
competition a second time as system C. Since B
and C produce identical translations, they should
always tie with one another. The expected value
of ORIGWMT(A) would not change, but the ex-
pected value of ORIGWMT(B) would increase to
5/6, buoyed by its ties with system C.
This vulnerability prompted (Bojar et al, 2011)
to offer the following revision:
BOJAR(s) = win(s)win(s) + loss(s)
The following year, it was BOJAR?s turn to be crit-
icized, this time by (Lopez, 2012):
Superficially, this appears to be an im-
provement....couldn?t a system still be
penalized simply by being compared
to [good systems] more frequently than
its competitors? On the other hand,
couldn?t a system be rewarded simply
by being compared against a bad system
more frequently than its competitors?
Lopez?s concern, while reasonable, is less obvi-
ously damning than (Bojar et al, 2011)?s criti-
cism of ORIGWMT. It depends on whether the
collected set of comparisons is small enough or
biased enough to make the variance in competi-
tion significant. While this hypothesis is plausi-
ble, Lopez makes no attempt to verify it. Instead,
he offers a ranking heuristic of his own, based on
a Minimum Feedback Arc solver.
The proliferation of ranking heuristics contin-
ued from there. The WMT 2012 organizers
(Callison-Burch et al, 2012) took Lopez?s ranking
scheme and provided a variant called Most Proba-
ble Ranking. Then, noting some potential pitfalls
with that, they created two more, called Monte
Carlo Playoffs and Expected Wins. While one
could raise philosophical objections about each of
these, where would it end? Ultimately, the WMT
2012 findings presented five different rankings for
the English-German competition track, with no
guidance about which ranking we should pay at-
tention to. How can we know whether one rank-
ing is better than other? Or is this even the right
question to ask?
3 A Problem with Rankings
Suppose four systems participate in a translation
competition. Three of these systems are extremely
close in quality. We?ll call these close1, close2,
and close3. Nevertheless, close1 is very slightly
better3 than close2, and close2 is very slightly bet-
ter than close3. The fourth system, called terrific,
is a really terrific system that far exceeds the other
three.
Now which is the better ranking?
terrific, close3, close1, close2 (1)
close1, terrific, close2, close3 (2)
Spearman?s rho4 would favor the second ranking,
since it is a less disruptive permutation of the gold
ranking. But intuition favors the first. While its
mistakes are minor, the second ranking makes the
hard-to-forgive mistake of placing close1 ahead of
the terrific system.
The problem is not with Spearman?s rho. The
problem is the disconnnect between the knowl-
edge that we want a ranking to reflect and the
knowledge that a ranking actually contains. With-
out this additional knowledge, we cannot deter-
mine whether one ranking is better than another,
even if we know the gold ranking. We need to
determine what information they lack, and define
more rigorously what we hope to learn from a
translation competition.
4 From Rankings to Relative Ability
Ostensibly the purpose of a translation competi-
tion is to determine the relative ability of a set
of translation systems. Let S be the space of all
translation systems. Hereafter, we will refer to S
as the space of students. We choose this term to
evoke the metaphor of a translation competition as
a standardized test, which shares the same goal: to
assess the relative abilities of a set of participants.
But what exactly do we mean by ?ability?? Be-
fore formally defining this term, first recognize
that it means little without context, namely:
3What does ?better? mean? We?ll return to this question.
4Or Pearson?s correlation coefficient.
1417
1. What kind of source text do we want the
systems to translate well? Say system A is
great at translating travel-related documents,
but terrible at translating newswire. Mean-
while, system B is pretty good at both. The
question ?which system is better?? requires
us to state how much we care about travel
versus newswire documents ? otherwise the
question is underspecified.
2. Who are we trying to impress? While it?s
tempting to think that translation quality is
a universal notion, the 50-60% interannota-
tor agreement in WMT evaluations (Callison-
Burch et al, 2012) suggests otherwise. It?s
also easy to imagine reasons why one group
of judges might have different priorities than
another. Think a Fortune 500 company ver-
sus web forum users. Lawyers versus lay-
men. Non-native versus native speakers.
Posteditors versus Google Translate users.
Different groups have different uses for trans-
lation, and therefore different definitions of
what ?better? means.
With this in mind, let?s define some additional el-
ements of a translation competition. Let X be the
space of all possible segments of source text, J be
the space of all possible judges, and ? = {0, 1, 2}
be the space of pairwise preferences.5 We as-
sume all spaces are countable. Unless stated oth-
erwise, variables s1 and s2 represent students from
S, variable x represents a segment from X , vari-
able j represents a judge from J , and variable pi
represents a preference from ?. Moreover, define
the negation p?i of preference pi such that p?i = 2 (if
pi = 1), p?i = 1 (if pi = 2), and p?i = 0 (if pi = 0).
Now assume a joint distribution
P (s1, s2, x, j, pi) specifying the probability
that we ask judge j to evaluate students s1 and
s2?s respective translations of source text x, and
that judge j?s preference is pi. We will further
assume that the choice of student pair, source
text, and judge are marginally independent of one
another. In other words:
P (s1, s2, x, j, pi)
=
P (pi|s1, s2, x, j) ? P (x|s1, s2, j)
?P (j|s1, s2) ? P (s1, s2)
= P (pi|s1, s2, x, j) ? P (x) ? P (j) ? P (s1, s2)
= PX (x) ? PJ (j) ? P (s1, s2) ? P (pi|s1, s2, x, j)
5As a reminder, 0 indicates no preference.
It will be useful to reserve notation PX and PJ
for the marginal distributions over source text and
judges. We can marginalize over the source seg-
ments and judges to obtain a useful quantity:
P (pi|s1, s2)
=
?
x?X
?
j?J
PX (x) ? PJ (j) ? P (pi|s1, s2, x, j)
We refer to this as the ?PX , PJ ?-relative ability of
students s1 and s2. By using different marginal
distributions PX , we can specify what kinds of
source text interest us (for instance, PX could
focus most of its probability mass on German
tweets). Similarly, by using different marginal
distributions PJ , we can specify what judges we
want to impress (for instance, PJ could focus all
of its mass on one important corporate customer
or evenly among all fluent bilingual speakers of a
language pair).
With this machinery, we can express the pur-
pose of a translation competition more clearly:
to estimate the ?PX , PJ ?-relative ability of a set
of students. In the case of WMT, PJ presum-
ably6 defines a space of competent source-to-
target bilingual speakers, while PX defines a space
of newswire documents.
We?ll refer to an estimate of P (pi|s1, s2) as
a preference model. In other words, a prefer-
ence model is a distribution Q(pi|s1, s2). Given
a set of pairwise comparisons (e.g., Table 2),
the challenge is to estimate a preference model
Q(pi|s1, s2) such that Q is ?close? to P . For mea-
suring distributional proximity, a natural choice is
KL-divergence (Kullback and Leibler, 1951), but
we cannot use it here because P is unknown.
Fortunately, if we have i.i.d. data drawn from P ,
then we can do the next best thing and compute the
perplexity of preference model Q on this heldout
test data. LetD be a sequence of triples ?s1, s2, pi?
where the preferences pi are i.i.d. samples from
P (pi|s1, s2). The perplexity of preference model
Q on test data D is:
perplexity(Q|D) = 2?
?
?s1,s2,pi??D
1
|D| log2Q(pi|s1,s2)
How do we obtain such a test set from competi-
tion data? Recall that a WMT competition pro-
duces pairwise comparisons like those in Table 2.
6One could argue that it specifies a space of machine
translation specialists, but likely these individuals are thought
to be a representative sample of a broader community.
1418
Let C be the set of comparisons ?s1, s2, x, j, pi?
obtained from a translation competition. Com-
petition data C is not necessarily7 sampled i.i.d.
from P (s1, s2, x, j, pi) because we may intention-
ally8 bias data collection towards certain students,
judges or source text. Also, because WMT elicits
its data in batches (see Table 1), every segment x
of source text appears in at least ten comparisons.
To create an appropriately-sized test set that
closely resembles i.i.d. data, we isolate the sub-
set C? of comparisons whose source text appears
in at most k comparisons, where k is the smallest
positive integer such that |C?| >= 2000. We then
create the test set D from C?:
D = {?s1, s2, pi?|?s1, s2, x, j, pi? ? C?}
We reserve the remaining comparisons for training
preference models. Table 3 shows the resulting
dataset sizes for each competition track.
Unlike with raw rankings, the claim that
one preference model is better than another has
testable implications. Given two competing mod-
els, we can train them on the same comparisons,
and compare their perplexities on the test set. This
gives us a quantitative9 answer to the question of
which is the better model. We can then publish
a system ranking based on the most trustworthy
preference model.
5 Baselines
Let?s begin then, and create some simple prefer-
ence models to serve as baselines.
5.1 Uniform
The simplest preference model is a uniform distri-
bution over preferences, for any choice of students
s1, s2:
Q(pi|s1, s2) =
1
3 ?pi ? ?
This will be our only model that does not require
training data, and its perplexity on any test set will
be 3 (i.e. equal to number of possible preferences).
5.2 Adjusted Uniform
Now suppose we have a set C of comparisons
available for training. Let Cpi ? C denote the
subset of comparisons with preference pi, and let
7In WMT, it certainly is not.
8To collect judge agreement statistics, for instance.
9As opposed to philosophical.
C(s1, s2) denote the subset comparing students s1
and s2.
Perhaps the simplest thing we can do with the
training data is to estimate the probability of ties
(i.e. preference 0). We can then distribute the
remaining probability mass uniformly among the
other two preferences:
Q(pi|s1, s2) =
?
????
????
|C0|
|C| if pi = 0
1? |C0||C|
2 otherwise
6 Simple Bayesian Models
6.1 Independent Pairs
Another simple model is the direct estimation of
each relative ability P (pi|s1, s2) independently. In
other words, for each pair of students s1 and s2, we
estimate a separate preference distribution. The
maximum likelihood estimate of each distribution
would be:
Q(pi|s1, s2) =
|Cpi(s1, s2)|+ |Cp?i(s2, s1)|
|C(s1, s2)|+ |C(s2, s1)|
However the maximum likelihood estimate would
test poorly, since any zero probability estimates
for test set preferences would result in infinite per-
plexity. To make this model practical, we assume a
symmetric Dirichlet prior with strength ? for each
preference distribution. This gives us the follow-
ing Bayesian estimate:
Q(pi|s1, s2) =
?+ |Cpi(s1, s2)|+ |Cp?i(s2, s1)|
3?+ |C(s1, s2)|+ |C(s2, s1)|
We call this the Independent Pairs preference
model.
6.2 Independent Students
The Independent Pairs model makes a strong inde-
pendence assumption. It assumes that even if we
know that student A is much better than student B,
and that student B is much better than student C,
we can infer nothing about how student A will fare
versus student C. Instead of directly estimating the
relative ability P (pi|s1, s2) of students s1 and s2,
we could instead try to estimate the universal abil-
ity P (pi|s1) = ?s2?S P (pi|s1, s2) ? P (s2|s1) ofeach individual student s1 and then try to recon-
struct the relative abilities from these estimates.
For the same reasons as before, we assume a
symmetric Dirichlet prior with strength ? for each
1419
preference distribution, which gives us the follow-
ing Bayesian estimate:
Q(pi|s1) =
?+
?
s2?S |Cpi(s1, s2)|+ |Cp?i(s2, s1)|
3?+
?
s2?S |C(s1, s2)|+ |C(s2, s1)|
The estimatesQ(pi|s1) do not yet constitute a pref-
erence model. A downside of this approach is that
there is no principled way to reconstruct a pref-
erence model from the universal ability estimates.
We experiment with three ad-hoc reconstructions.
The asymmetric reconstruction simply ignores any
information we have about student s2:
Q(pi|s1, s2) = Q(pi|s1)
The arithmetic and geometric reconstructions
compute an arithmetic/geometric average of the
two universal abilities:
Q(pi|s1, s2) =
Q(pi|s1) +Q(p?i|s2)
2
Q(pi|s1, s2) = [Q(pi|s1) ?Q(p?i|s2)]
1
2
We respectively call these the (Asymmet-
ric/Arithmetic/Geometric) Independent Students
preference models. Notice the similarities be-
tween the universal ability estimates Q(pi|s1) and
the BOJAR ranking heuristic. These three models
are our attempt to render the BOJAR heuristic as
preference models.
7 Item-Response Theoretic (IRT) Models
Let?s revisit (Lopez, 2012)?s objection to the BO-
JAR ranking heuristic: ?...couldn?t a system still be
penalized simply by being compared to [good sys-
tems] more frequently than its competitors?? The
official WMT 2012 findings (Callison-Burch et al,
2012) echoes this concern in justifying the exclu-
sion of reference translations from the 2012 com-
petition:
[W]orkers have a very clear preference
for reference translations, so includ-
ing them unduly penalized systems that,
through (un)luck of the draw, were pit-
ted against the references more often.
Presuming the students are paired uniformly at
random, this issue diminishes as more compar-
isons are elicited. But preference elicitation is ex-
pensive, so it makes sense to assess the relative
ability of the students with as few elicitations as
possible. Still, WMT 2012?s decision to eliminate
references entirely is a bit of a draconian mea-
sure, a treatment of the symptom rather than the
(perceived) disease. If our models cannot function
in the presence of training data variation, then we
should change the models, not the data. A model
that only works when the students are all about the
same level is not one we should rely on.
We experiment with a simple model that relaxes
some independence assumptions made by previ-
ous models, in order to allow training data vari-
ation (e.g. who a student has been paired with)
to influence the estimation of the student abili-
ties. Figure 1(left) shows plate notation (Koller
and Friedman, 2009) for the model?s indepen-
dence structure. First, each student?s ability dis-
tribution is drawn from a common prior distribu-
tion. Then a number of translation items are gen-
erated. Each item is authored by a student and has
a quality drawn from the student?s ability distri-
bution. Then a number of pairwise comparisons
are generated. Each comparison has two options,
each a translation item. The quality of each item
is observed by a judge (possibly noisily) and then
the judge states a preference by comparing the two
observations.
We investigate two parameterizations of this
model: Gaussian and categorical. Figure 1(right)
shows an example of the Gaussian parameteriza-
tion. The student ability distributions are Gaus-
sians with a known standard deviation ?a, drawn
from a zero-mean Gaussian prior with known stan-
dard deviation ?0. In the example, we show
the ability distributions for students 6 (an above-
average student, whose mean is 0.4) and 14 (a
poor student, whose mean is -0.6). We also show
an item authored by each student. Item 43 has
a somewhat low quality of -0.3 (drawn from stu-
dent 14?s ability distribution), while item 205 is
not student 6?s best work (he produces a mean
quality of 0.4), but still has a decent quality at 0.2.
Comparison 1 pits these items against one another.
A judge draws noise from a zero-mean Gaussian
with known standard deviation ?obs, then adds this
to the item?s actual quality to get an observed qual-
ity. For the first option (item 43), the judge draws a
noise of -0.12 to observe a quality of -0.42 (worse
than it actually is). For the second option (item
205), the judge draws a noise of 0.15 to observe a
quality of 0.35 (better than it actually is). Finally,
the judge compares the two observed qualities. If
the absolute difference is lower than his decision
1420
student.6.ability 
Gauss(0.4, ?a) 
item.43.author 
14 
item.43.quality 
-0.3 
comp.1.opt1 
43 
comp.1.opt1.obs 
-0.42 
comp.1.pref 
2 
comp.1.opt2 
205 
comp.1.opt2.obs 
0.35 
student.prior 
Gauss(0.0, ?0)  
decision.radius 
0.5 
obs.parameters 
Gauss(0.0, ?obs) 
item.205.author 
6 
item.205.quality 
0.2 
student.14.ability 
Gauss(-0.6, ?a) 
student.s.ability item.i.author 
item.i.quality 
comp.c.opt1 
comp.c.opt1.obs 
comp.c.pref 
comp.c.opt2 
comp.c.opt2.obs 
S 
I 
C 
student.prior 
decision.radius 
obs.parameters 
Figure 1: Plate notation (left) showing the independence structure of the IRT Models. Example instan-
tiated subnetwork (right) for the Gaussian parameterization. Shaded rectangles are hyperparameters.
Shaded ellipses are variables observable from a set of comparisons.
radius (which here is 0.5), then he states no prefer-
ence (i.e. a preference of 0). Otherwise he prefers
the item with the higher observed quality.
The categorical parameterization is similar to
the Gaussian parameterization, with the following
differences. Item quality is not continuous, but
rather a member of the discrete set {1, 2, ...,?}.
The student ability distributions are categorical
distributions over {1, 2, ...,?}, and the student
ability prior is a symmetric Dirichlet with strength
?a. Finally, the observed quality is the item qual-
ity ? plus an integer-valued noise ? ? {1 ?
?, ...,?? ?}. Noise ? is drawn from a discretized
zero-mean Gaussian with standard deviation ?obs.
Specifically, Pr(?) is proportional to the value of
the probability density function of the zero-mean
Gaussian N (0, ?obs).
We estimated the model parameters with Gibbs
sampling (Geman and Geman, 1984). We found
that Gibbs sampling converged quickly and con-
sistently10 for both parameterizations. Given the
parameter estimates, we obtain a preference model
Q(pi|s1, s2) through the inference query:
Pr(comp.c?.pref = pi | item.i?.author = s1,
item.i??.author = s2,
comp.c?.opt1 = i?,
comp.c?.opt2 = i??)
10We ran 200 iterations with a burn-in of 50.
where c?, i?, i?? are new comparison and item ids
that do not appear in the training data.
We call these models Item-Response Theo-
retic (IRT) models, to acknowledge their roots
in the psychometrics (Thurstone, 1927; Bradley
and Terry, 1952; Luce, 1959) and item-response
theory (Hambleton, 1991; van der Linden and
Hambleton, 1996; Baker, 2001) literature. Item-
response theory is the basis of modern testing
theory and drives adaptive standardized tests like
the Graduate Record Exam (GRE). In particular,
the Gaussian parameterization of our IRT models
strongly resembles11 the Thurstone (Thurstone,
1927) and Bradley-Terry-Luce (Bradley and Terry,
1952; Luce, 1959) models of paired compari-
son and the 1PL normal-ogive and Rasch (Rasch,
1960) models of student testing. From the test-
ing perspective, we can view each comparison as
two students simultaneously posing a test question
to the other: ?Give me a translation of the source
text which is better than mine.? The students can
answer the question correctly, incorrectly, or they
can provide a translation of analogous quality. An
extra dimension of our models is judge noise, not
a factor when modeling multiple-choice tests, for
which the right answer is not subject to opinion.
11These models are not traditionally expressed using
graphical models, although it is not unprecedented (Mislevy
and Almond, 1997; Mislevy et al, 1999).
1421
wmt10 wmt11 wmt12
lp train test train test train test
ce 3166 2209 1706 3216 5969 6806
fe 5918 2376 2556 4430 7982 5840
ge 7422 3002 3708 5371 8106 6032
se 8411 2896 1968 3684 3910 7376
ec 10490 3048 8859 9016 13770 9112
ef 5720 2242 3328 5758 7841 7508
eg 10852 2842 5964 7032 10210 7191
es 2962 2212 4768 6362 5664 8928
Table 3: Dataset sizes for each competition track
(number of comparisons).
Figure 2: WMT10 model perplexities. The per-
plexity of the uniform preference model is 3.0 for
all training sizes.
8 Experiments
We organized the competition data as described at
the end of Section 4. To compare the preference
models, we did the following:
? Randomly chose a subset of k compar-
isons from the training set, for k ?
{100, 200, 400, 800, 1600, 3200}.12
? Trained the preference model on these com-
parisons.
? Evaluated the perplexity of the trained model
on the test preferences, as described in Sec-
tion 4.
For each model and training size, we averaged
the perplexities from 5 trials of each competition
track. We then plotted average perplexity as a
function of training size. These graphs are shown
12If k was greater than the total number of training com-
parisons, then we took the entire set.
Figure 3: WMT11 model perplexities.
Figure 4: WMT12 model perplexities.
in Figure 2 (WMT10)13, and Figure 4 (WMT12).
For WMT10 and WMT11, the best models were
the IRT models, with the Gaussian parameteriza-
tion converging the most rapidly and reaching the
lowest perplexity. For WMT12, in which refer-
ence translations were excluded from the compe-
tition, four models were nearly indistinguishable:
the two IRT models and the two averaged Indepen-
dent Student models. This somewhat validates the
organizers? decision to exclude the references, par-
ticularly given WMT?s use of the BOJAR ranking
heuristic (the nucleus of the Independent Student
models) for its official rankings.
13Results for WMT10 exclude the German-English and
English-German tracks, since we used these to tune our
model hyperparameters. These were set as follows. The
Dirichlet strength for each baseline was 1. For IRT-Gaussian:
?0 = 1.0, ?obs = 1.0, ?a = 0.5, and the decision radius was
0.4. For IRT-Categorical: ? = 8, ?obs = 1.0, ?a = 0.5, and
the decision radius was 0.
1422
Figure 6: English-Czech WMT11 results (average of 5 trainings on 1600 comparisons). Error bars
(left) indicate one stddev of the estimated ability means. In the heatmap (right), cell (s1, s2) is darker if
preference model Q(pi|s1, s2) skews in favor of student s1, lighter if it skews in favor of student s2.
Figure 5: WMT10 model perplexities (crowd-
sourced versus expert training).
The IRT models proved the most robust at han-
dling judge noise. We repeated the WMT10 ex-
periment using the same test sets, but using the
unfiltered crowdsourced comparisons (rather than
?expert?14 comparisons) for training. Figure 5
shows the results. Whereas the crowdsourced
noise considerably degraded the Geometric Inde-
pendent Students model, the IRT models were re-
markably robust. IRT-Gaussian in particular came
close to replicating the performance of Geometric
Independent Students trained on the much cleaner
expert data. This is rather impressive, since the
crowdsourced judges agree only 46.6% of the
time, compared to a 65.8% agreement rate among
14I.e., machine translation specialists.
expert judges (Callison-Burch et al, 2010).
Another nice property of the IRT models is
that they explicitly model student ability, so they
yield a natural ranking. For training size 1600 of
the WMT11 English-Czech track, Figure 6 (left)
shows the mean student abilities learned by the
IRT-Gaussian model. The error bars show one
standard deviation of the ability means (recall that
we performed 5 trials, each with a random training
subset of size 1600). These results provide fur-
ther insight into a case analyzed by (Lopez, 2012),
which raised concern about the relative ordering
of online-B, cu-bojar, and cu-marecek. Accord-
ing to IRT-Gaussian?s analysis of the data, these
three students are so close in ability that any order-
ing is essentially arbitrary. Short of a full ranking,
the analysis does suggest four strata. Viewing one
of IRT-Gaussian?s induced preference models as
a heatmap15 (Figure 6, right), four bands are dis-
cernable. First, the reference sentences are clearly
the darkest (best). Next come students 2-7, fol-
lowed by the slightly lighter (weaker) students 8-
10, followed by the lightest (weakest) student 11.
9 Conclusion
WMT has faced a crisis of confidence lately, with
researchers raising (real and conjectured) issues
with its analytical methodology. In this paper,
we showed how WMT can restore confidence in
15In the heatmap, cell (s1, s2) is darker if preference model
Q(pi|s1, s2) skews in favor of student s1, lighter if it skews
in favor of student s2.
1423
its conclusions ? by shifting the focus from rank-
ings to relative ability. Estimates of relative ability
(the expected head-to-head performance of system
pairs over a probability space of judges and source
text) can be empirically compared, granting sub-
stance to previously nebulous questions like:
1. Is my analysis better than your analysis?
Rather than the current anecdotal approach
to comparing competition analyses (e.g. pre-
senting example rankings that seem some-
how wrong), we can empirically compare the
predictive power of the models on test data.
2. How much of an impact does judge noise
have on my conclusions? We showed
that judge noise can have a significant im-
pact on the quality of our conclusions, if we
use the wrong models. However, the IRT-
Gaussian appears to be quite noise-tolerant,
giving similar-quality conclusions on both
expert and crowdsourced comparisons.
3. How many comparisons should I elicit?
Many of our preference models (including
IRT-Gaussian and Geometric Independent
Students) are close to convergence at around
1000 comparisons. This suggests that we can
elicit far fewer comparisons and still derive
confident conclusions. This is the first time
a concrete answer to this question has been
provided.
References
F.B. Baker. 2001. The basics of item response theory.
ERIC.
Ondej Bojar, Milos? Ercegovc?evic?, Martin Popel, and
Omar Zaidan. 2011. A grain of salt for the wmt
manual evaluation. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
1?11, Edinburgh, Scotland, July. Association for
Computational Linguistics.
Ralph Allan Bradley and Milton E Terry. 1952. Rank
analysis of incomplete block designs: I. the method
of paired comparisons. Biometrika, 39(3/4):324?
345.
C. Callison-Burch, P. Koehn, C. Monz, K. Peterson,
M. Przybocki, and O.F. Zaidan. 2010. Findings of
the 2010 joint workshop on statistical machine trans-
lation and metrics for machine translation. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 17?
53. Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation.
S. Geman and D. Geman. 1984. Stochastic relaxation,
gibbs distributions, and the bayesian restoration of
images. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 6(6):721?741.
R.K. Hambleton. 1991. Fundamentals of item re-
sponse theory, volume 2. Sage Publications, Incor-
porated.
D. Koller and N. Friedman. 2009. Probabilistic graph-
ical models: principles and techniques. MIT press.
S. Kullback and R.A. Leibler. 1951. On information
and sufficiency. The Annals of Mathematical Statis-
tics, 22(1):79?86.
Adam Lopez. 2012. Putting human assessments of
machine translation systems in order. In Proceed-
ings of WMT.
R. Ducan Luce. 1959. Individual Choice Behavior a
Theoretical Analysis. John Wiley and sons.
R.J. Mislevy and R.G. Almond. 1997. Graphical mod-
els and computerized adaptive testing. UCLA CSE
Technical Report 434.
R.J. Mislevy, R.G. Almond, D. Yan, and L.S. Stein-
berg. 1999. Bayes nets in educational assessment:
Where the numbers come from. In Proceedings
of the fifteenth conference on uncertainty in artifi-
cial intelligence, pages 437?446. Morgan Kaufmann
Publishers Inc.
G. Rasch. 1960. Studies in mathematical psychology:
I. probabilistic models for some intelligence and at-
tainment tests.
Louis L Thurstone. 1927. A law of comparative judg-
ment. Psychological review, 34(4):273?286.
W.J. van der Linden and R.K. Hambleton. 1996.
Handbook of modern item response theory.
Springer.
1424
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 523?532,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Extraction Programs: A Unified Approach to Translation Rule Extraction
Mark Hopkins and Greg Langmead and Tai Vo
SDL Language Technologies Division
6060 Center Drive, Suite 150
Los Angeles, CA 90045
{mhopkins,glangmead,tvo}@sdl.com
Abstract
We provide a general algorithmic schema
for translation rule extraction and show that
several popular extraction methods (includ-
ing phrase pair extraction, hierarchical phrase
pair extraction, and GHKM extraction) can be
viewed as specific instances of this schema.
This work is primarily intended as a survey of
the dominant extraction paradigms, in which
we make explicit the close relationship be-
tween these approaches, and establish a lan-
guage for future hybridizations. This facili-
tates a generic and extensible implementation
of alignment-based extraction methods.
1 Introduction
The tradition of extracting translation rules from
aligned sentence pairs dates back more than a
decade. A prominent early example is phrase-based
extraction (Och et al, 1999).
Around the middle of the last decade, two ex-
traction paradigms were proposed for syntax-based
machine translation: the Hiero paradigm of (Chi-
ang, 2005) and the GHKM paradigm of (Galley et
al., 2004). From these papers followed two largely
independent lines of research, respectively dubbed
formally syntax-based machine translation (Chiang,
2007; Zollmann and Venugopal, 2006; Venugopal et
al., 2007; Lopez, 2007; Marton and Resnik, 2008;
Li et al, 2009; de Gispert et al, 2010) and linguis-
tically syntax-based machine translation (Galley et
al., 2006; Marcu et al, 2006; Liu et al, 2006; Huang
et al, 2006; Liu et al, 2007; Mi and Huang, 2008;
Zhang et al, 2008; Liu et al, 2009).
In this paper, we unify these strands of research
by showing how to express Hiero extraction, GHKM
extraction, and phrase-based extraction as instances
of a single master extraction method. Specifically,
we express each technique as a simple ?program?
given to a generic ?evaluator?. Table 1 summarizes
how to express several popular extraction methods
as ?extraction programs.?
Besides providing a unifying survey of popular
alignment-based extraction methods, this work has
the practical benefit of facilitating the implementa-
tion of these methods. By specifying the appropri-
ate input program, the generic evaluator (coded, say,
as a Python module) can be used to execute any of
the extraction techniques in Table 1. New extraction
techniques and hybridizations of existing techniques
can be supported with minimal additional program-
ming.
2 Building Blocks
The family of extraction algorithms under consider-
ation share a common setup: they extract translation
rules from a sentence pair and an alignment. In this
section, we define these concepts.
2.1 Patterns and Sentences
Assume we have a global vocabulary of atomic sym-
bols, containing the reserved substitution symbol?.
Define a pattern as a sequence of symbols. Define
the rank of a pattern as the count of its ? symbols.
Let ?k , ?
k
? ?? ?
?,?, ...,??.
We will typically use space-delimited quotations
to represent example patterns, e.g. ?ne? pas? rather
than ?ne,?, pas?. We will use the dot operator to
represent the concatenation of patterns, e.g. ?il ne? ?
?va pas? = ?il ne va pas?.
523
Extraction Program
Method Primary Secondary Labeling
Protocol Protocol Protocol
PBMT (Och et al, 1999) RANKPP0 TRIVSPA TRIVLP
Hiero (Chiang, 2005) RANKPP? TRIVSPA TRIVLP
GHKM (Galley et al, 2004) MAPPPt TRIVSPA PMAPLPt
SAMT (Zollmann and Venugopal, 2006) RANKPP? TRIVSPA PMAPLPt?
Forest GHKM (Mi and Huang, 2008) MAPPPT TRIVSPA PMAPLPT
Tree-to-Tree GHKM (Liu et al, 2009) MAPPPt MAPSP?,A IMAPLP{t},{?}
Forest-to-Forest GHKM (Liu et al, 2009) MAPPPT MAPSPT ,A IMAPLPT,T
Fuzzy Dual Syntax (Chiang, 2010) MAPPPt? MAPSP?? ,A IMAPLP{t?},{??}
Table 1: Various rule extraction methods, expressed as extraction programs. Boldfaced methods are proven in this
paper; the rest are left as conjecture. Parameters: t, ? are spanmaps (see Section 3); t?, ?? are fuzzy spanmaps (see
Section 7); T, T are sets of spanmaps (typically encoded as forests); A is an alignment (see Section 2).
We refer to a contiguous portion of a pattern with
a span, defined as either the null span ? , or a pair
[b, c] of positive integers such that b ? c. We will
treat span [b, c] as the implicit encoding of the set
{b, b+ 1, ..., c}, and employ set-theoretic operations
on spans, e.g. [3, 8] ? [6, 11] = [6, 8]. Note that the
null span encodes the empty set.
If a set I of positive integers is non-empty, then it
has a unique minimal enclosing span, defined by the
operator span(I) = [min(I),max(I)]. For instance,
span({1, 3, 4}) = [1, 4]. Define span({}) = ?.
Finally, define a sentence as a pattern of rank 0.
2.2 Alignments
An alignment is a triple ?m,n,A?, where m and n
are positive integers, andA is a set of ordered integer
pairs (i, j) such that 1 ? i ? m and 1 ? j ? n.
In Figure 1(a), we show a graphical depiction of
alignment ?4, 6, {(1, 1), (2, 3), (4, 3), (3, 5)}?. Ob-
serve that alignments have a primary side (top) and
a secondary side (bottom)1. For alignment A =
?m,n,A?, define |A|p = m and |A|s = n. A pri-
mary index (resp., secondary index) ofA is any pos-
itive integer less than or equal to |A|p (resp., |A|s).
A primary span (resp., secondary span) of A is any
span [b, c] such that 1 ? b ? c ? |A|p (resp., |A|s).
Define a
A
? ? to mean that (a, ?) ? A (in words,
we say that A aligns primary index a to secondary
1The terms primary and secondary allow us to be agnostic
about how the extracted rules are used in a translation system,
i.e. the primary side can refer to the source or target language.
[3,5]
[2,4][2,4] 1 2 3 4
2 3 4 51 6
1 2 3 4
2 3 4 51 6
1 2 3 4
2 3 4 51 6
(a)
(d)(c)
(b)
1 2 3 4
2 3 4 51 6
Figure 1: A demonstration of alignment terminology.
(a) An alignment is a relation between positive integer
sets. (b) The primary domain of the example alignment
is {1,2,3,4} and the secondary domain is {1,3,5}. (c)
The image of primary span [2,4] is {3,5}. (d) The mini-
mal projection of primary span [2,4] is [3,5]. Secondary
spans [2,5], [3,6], and [2,6] are also projections of pri-
mary span [2,4].
index ?), and define a 6
A
? ? to mean that (a, ?) 6? A.
Define an aligned sentence pair as a triple
?s, ?,A? where A is an alignment and s, ? are sen-
tences of length |A|p and |A|s, respectively.
Primary and Secondary Domain: The primary
domain of alignment A is the set of primary in-
dices that are aligned to some secondary index, i.e.
pdom(A) = {a|?? s.t. a A? ?}. Analogously,
define sdom(A) = {?|?a s.t. a A? ?}. For the
example alignment of Figure 1(b), pdom(A) =
524
{1, 2, 3, 4} and sdom(A) = {1, 3, 5}.
Image: The image of a set I of primary indices
(denoted pimageA(I)) is the set of secondary in-
dices to which the primary indices of I align. In
Figure 1(c), for instance, the image of primary span
[2, 4] is the set {3, 5}. Formally, for a set I of pri-
mary indices of alignment A, define:
pimageA(I) = {?|?a ? I s.t. (a, ?) ? A}
Projection: The minimal projection of a set I of
primary indices (denoted pmprojA(I)) is the min-
imal enclosing span of the image of I . In other
words, pmprojA(I) = span(pimageA(I)). In Fig-
ure 1(d), for instance, the minimal projection of pri-
mary span [2, 4] is the secondary span [3, 5].
Consider Figure 1(d). We will also allow a more
relaxed type of projection, in which we allow the
broadening of the minimal projection to include un-
aligned secondary indices. In the example, sec-
ondary spans [2, 5], [3, 6], and [2, 6] (in addition
to the minimal projection [3, 5]) are all considered
projections of primary span [2, 4]. Formally, de-
fine pprojA([b, c]) as the set of superspans [?, ?]
of pmprojA([b, c]) such that [?, ?] ? sdom(A) ?
pmprojA([b, c]).
2.3 Rules
We define an unlabeled rule as a tuple ?k, s?, ??, pi?
where k is a nonnegative integer, s? and ??
are patterns of rank k, and pi is a permuta-
tion of the sequence ?1, 2, ..., k?. Such rules
can be rewritten using a more standard Syn-
chronous Context-Free Grammar (SCFG) format,
e.g. ?3, ?le? ? de??, ?? ?s ? ??, ?3, 2, 1?? can
be written: ? ? ?le?1 ?2 de ?3,?3 ?s?2 ?1?.
A labeled rule is a pair ?r, l?, where r is an un-
labeled rule, and l is a ?label?. The unlabeled rule
defines the essential structure of a rule. The label
gives us auxiliary information we can use as decod-
ing constraints or rule features. This deliberate mod-
ularization lets us unify sequence-based and tree-
based extraction methods.
Labels can take many forms. Two examples (de-
picted in Figure 2) are:
1. An SCFG label is a (k+ 1)-length sequence of
symbols.
DT NN JJ
NPB
NP
< NP, NN, JJ, NNP > 
IN NNP
PP
* *
NNP POS JJ
NP
*
NN
NP ? < le NN1 JJ2 de NNP3 ,NNP3 ?s JJ2 NN1 >
DT NN JJ
NPB
NP
IN NNP
PP
le de
NNP POS JJ
NP
?s
NN
label labeled rule
Figure 2: An example SCFG label (top) and
STSG label (bottom) for unlabeled rule ? ?
?le ?1 ?2 de ?3,?3 ?s ?2 ?1?.
2. An STSG label (from Synchronous Tree Sub-
stitution Grammar (Eisner, 2003)) is a pair of
trees.
STSG labels subsume SCFG labels. Thus STSG
extraction techniques can be used as SCFG extrac-
tion techniques by ignoring the extra hierarchical
structure of the STSG label. Due to space con-
straints, we will restrict our focus to SCFG labels.
When considering techniques originally formulated
to extract STSG rules (GHKM, for instance), we
will consider their SCFG equivalents.
3 A General Rule Extraction Schema
In this section, we develop a general algorithmic
schema for extracting rules from aligned sentence
pairs. We will do so by generalizing the GHKM al-
gorithm (Galley et al, 2004). The process goes as
follows:
? Repeatedly:
? Choose a ?construction request,? which
consists of a ?primary subrequest? (see
Figure 3a) and a ?secondary subrequest?
(see Figure 3b).
? Construct the unlabeled rule correspond-
ing to this request (see Figure 3, bottom).
? Label the rule (see Figure 2).
525
[1,4] [4,4][1,1]
ne va pasil
he does not go
INDEX
SORT
1 4 2 3
? ? does not
[1,4 ]
[3,3][1,1]
[1,4] [4,4][1,1]
ne va pasil
he does not go
? does not ?
INDEX
SORT
1 3 2 4
? ? ne pas
? ne ? p a s
INDEX
SORT
1 3
1 2 1  2
primar y
pa ttern
secondary
pa ttern
permutat io n
(a ) (b )
Figure 3: Extraction of the unlabeled rule ? ? ??1 does not?2,?1 ne?2 pas?. (a) Choose primary subre-
quest [1, 4]  [1, 1][4, 4]. (b) Choose secondary subrequest [1, 4]  [1, 1][3, 3]. (bottom) Construct the rule
? ? ??1 does not ?2,?1 ne?2 pas?.
3.1 Choose a Construction Request
The first step in the extraction process is to choose a
?construction request,? which directs the algorithm
about which unlabeled rule(s) we wish to construct.
A ?construction request? consists of two ?subre-
quests.?
Subrequests: A subrequest is a
nonempty sequence of non-null spans
?[b0, c0], [b1, c1], ..., [bk, ck]? such that, for all
1 ? i < j ? k, [bi, ci] and [bj , cj ] are disjoint
proper2 subsets of [b0, c0]. If it also true that
ci < bj , for all 1 ? i < j ? k, then the subrequest
is called monotonic. We refer to k as the rank of the
subrequest.
We typically write subrequest
?[b0, c0], [b1, c1], ..., [bk, ck]? using the notation:
2If unary rules are desired, i.e. rules of the form ? ? ?,
then this condition can be relaxed.
[b0, c0] [b1, c1]...[bk, ck]
or as [b0, c0]  if k = 0.
For subrequest x = [b0, c0]  [b1, c1]...[bk, ck],
define:
covered(x) = ?ki=1[bi, ci]
uncovered(x) = [b0, c0]\covered(x)
Primary Subrequests: Given an alignment A,
define the set frontier(A) as the set of primary spans
[b, c] of alignment A such that pmprojA([b, c])) is
nonempty and disjoint from pimageA([1, b ? 1]) ?
pimageA([c+ 1, |A|p]).
3
3Our definition of the frontier property is an equivalent re-
expression of that given in (Galley et al, 2004). We reexpress
it in these terms in order to highlight the fact that the frontier
526
Algorithm CONSTRUCTRULEs,?,A(x, ?):
if construction request ?x, ?? matches alignment A then
{u1, ..., up} = uncovered([b0, c0] [b1, c1]...[bk, ck])
{?1, ..., ?q} = uncovered([?0, ?0] [?1, ?1]...[?k, ?k])
s? = INDEXSORT(?b1, b2, ..., bk, u1, u2, ..., up?, ?
k
? ?? ?
?,?, ...,?, su1 , su2 , ..., sup?)
?? = INDEXSORT(??1, ?2, ..., ?k, ?1, ?2, ..., ?q?, ?
k
? ?? ?
?,?, ...,?, ??1 , ??2 , ..., ??q?)
pi = INDEXSORT(??1, ?2, ..., ?k?, ?1, 2, ..., k?)
return {?k, s?, ??, pi?}
else
return {}
end if
Figure 4: Pseudocode for rule construction. Arguments: s = ?s1 s2 ... sm? and ? = ??1 ?2 ... ?n? are sentences,
A = ?m,n,A? is an alignment, x = [b0, c0] [b1, c1]...[bk, ck] and ? = [?0, ?0] [?1, ?1]...[?k, ?k] are subrequests.
Define preqs(A) as the set of monotonic subre-
quests whose spans are all in frontier(A). We refer
to members of preqs(A) as primary subrequests of
alignment A. Figure 3a shows a primary subrequest
of an example alignment.
Secondary Subrequests: Given a primary sub-
request x = [b0, c0]  [b1, c1]...[bk, ck] of align-
ment A, define sreqs(x,A) as the set of subrequests
[?0, ?0]  [?1, ?1]...[?k, ?k] such that [?i, ?i] ?
pprojA([bi, ci]), for all 0 ? i ? k. We refer to
members of sreqs(x,A) as secondary subrequests
of primary subrequest x and alignmentA. Figure 3b
shows a secondary subrequest of the primary subre-
quest selected in Figure 3a.
Construction Requests: A construction request
is a pair of subrequests of equivalent rank. Con-
struction request ?x, ?? matches alignment A if x ?
preqs(A) and ? ? sreqs(x,A).
3.2 Construct the Unlabeled Rule
The basis of rule construction is the INDEXSORT
operator, which takes as input a sequence of
integers I = ?i1, i2, ..., ik?, and an equivalent-
length sequence of arbitrary values ?v1, v2, ..., vk?,
and returns a sequence ?vj1 , vj2 , ..., vjk?, where
?j1, j2, ..., jk? is a permutation of sequence
I in ascending order. For instance, INDEX-
SORT(?4, 1, 50, 2?, ??a?, ?b?, ?c?, ?d??) =
property is a property of the alignment alone. It is independent
of the auxiliary information that GHKM uses, in particular the
tree.
Primary Protocol RANKPPk:
{[b0, c0] [b1, c1]...[bj , cj ]
s.t. 1 ? b0 ? c0 and 0 ? j ? k}
Primary Protocol MAPPPt:
{[b0, c0] [b1, c1]...[bk, ck]
s.t. ?0 ? i ? k [bi, ci] ? spans(t)}
Primary Protocol MAPPPT :
?
t?T
MAPPPt
Figure 5: Various primary protocols. Parameters: k is a
nonnegative integer; t is a spanmap; T is a set of span-
maps (typically encoded as a forest).
??b?, ?d?, ?a?, ?c??. Note that the output of
INDEXSORT(I, V ) is nondeterministic if sequence
I has repetitions. In Figure 4, we show the pseu-
docode for rule construction. We show an example
construction in Figure 3 (bottom).
3.3 Label the Rule
Rule construction produces unlabeled rules. To label
these rules, we use a labeling protocol, defined as a
function that takes a construction request as input,
and returns a set of labels.
Figure 7 defines a number of general-purpose la-
527
Secondary Protocol TRIVSPA(x):
return sreqs(x,A)
Secondary Protocol MAPSP?,A(x):
{[?0, ?0] [?1, ?1]...[?k, ?k] ? sreqs(x,A)
s.t. ?0 ? i ? k : [?i, ?i] ? spans(?)}
Figure 6: Various secondary protocols. Parameters: ?
is a spanmap; A is an alignment; x = [b0, c0]  
[b1, c1]...[bk, ck] is a subrequest.
beling protocols. Some of these are driven by trees.
We will represent a tree as a spanmap, defined as
a function that maps spans to symbol sequences.
For instance, if a parse tree has constituent NP over
span [4, 7], then the corresponding spanmap t has
t([4, 7]) = ?NP?. We map spans to sequences in or-
der to accommodate unary chains in the parse tree.
Nonconstituent spans are mapped to the empty se-
quence. For spanmap t, let spans(t) be the set of
spans [b, c] for which t([b, c]) is a nonempty se-
quence.
4 Extraction Programs
In the previous section, we developed a general
technique for extracting labeled rules from aligned
sentence pairs. Note that this was not an algorithm,
but rather an algorithmic schema, as it left two ques-
tions unanswered:
1. What construction requests do we make?
2. What labeling protocol do we use?
We answer these questions with an extraction pro-
gram, defined as a triple ?X ,?,L?, where:
? X is a set of subrequests, referred to as the pri-
mary protocol. It specifies the set of primary
subrequests that interest us. Figure 5 defines
some general-purpose primary protocols.
? ? maps every subrequest to a set of subre-
quests. We refer to ? as the secondary protocol.
It specifies the set of secondary subrequests that
interest us, given a particular primary subre-
quest. Figure 6 defines some general-purpose
secondary protocols.
Labeling Protocol TRIVLP(x, ?):
return ?k+1
Labeling Protocol PMAPLPt(x, ?):
{?l0, ..., lk? s.t. ?0 ? i ? k : li ? t([bi, ci])}
Labeling Protocol PMAPLPT (x, ?):
?
t?T
PMAPLPt(x, ?)
Labeling Protocol SMAPLP? (x, ?):
{??0, ..., ?k? s.t. ?0 ? i ? k : ?i ? ?([?i, ?i])}
Labeling Protocol SMAPLPT (x, ?):
?
??T
SMAPLP? (x, ?)
Labeling Protocol IMAPLPT,T (x, ?):
{?(l0, ?0), ..., (lk, ?k)?
s.t. ?l0, ..., lk? ? PMAPLPT (x, ?)
and ??0, ..., ?k? ? SMAPLPT (x, ?)}
Figure 7: Various labeling protocols. Parameters: t, ? are
spanmaps; T, T are sets of spanmaps; x = [b0, c0]  
[b1, c1]...[bk, ck] and ? = [?0, ?0]  [?1, ?1]...[?k, ?k]
are subrequests.
? L is a labeling protocol. Figure 7 defines some
general-purpose labeling protocols.
Figure 8 shows the pseudocode for an ?evaluator?
that takes an extraction program (and an aligned sen-
tence pair) as input and returns a set of labeled rules.
4.1 The GHKM Extraction Program
As previously stated, we developed our extraction
schema by generalizing the GHKM algorithm (Gal-
ley et al, 2004). To recover GHKM as an instance
of this schema, use the following program:
EXTRACTs,?,A(MAPPPt, TRIVSPA, PMAPLPt)
where t is a spanmap encoding a parse tree over the
primary sentence.
528
Algorithm EXTRACTs,?,A(X ,?,L):
R = {}
for all subrequests x ? X do
for all subrequests ? ? ?(x) do
U = CONSTRUCTRULEs,?,A(x, ?)
L = L(x, ?)
R = R ? (U ? L)
end for
end for
return R
Figure 8: Evaluator for extraction programs. Parameters:
?s, ?,A? is an aligned sentence pair; X is a primary pro-
tocol; ? is a secondary protocol; L is a labeling protocol.
5 The Phrase Pair Extraction Program
In this section, we express phrase pair extraction
(Och et al, 1999) as an extraction program.
For primary span [b, c] and secondary span [?, ?]
of alignment A, let [b, c]
A
? [?, ?] if the following
three conditions hold:
1. a
A
? ? for some a ? [b, c] and ? ? [?, ?]
2. a
A
6? ? for all a ? [b, c] and ? 6? [?, ?]
3. a
A
6? ? for all a 6? [b, c] and ? ? [?, ?]
Define the ruleset PBMT(s, ?,A) to be the set of la-
beled rules ?r,?1? such that:
? r = ?0, ?sb...sc?, ???...???, ??
? [b, c]
A
? [?, ?]
We want to express PBMT(s, ?,A) as an extrac-
tion program. First we establish a useful lemma and
corollary.
Lemma 1. [b, c] A? [?, ?] iff [b, c] ? frontier(A) and
[?, ?] ? pprojA([b, c]).
Proof. Let [b, c]c = [1, b? 1] ? [c+ 1, |A|p].
[b, c] ? frontier(A) and [?, ?] ? pprojA ([b, c])
(1)
??
{
pmprojA ([b, c]) ? pimageA ([b, c]
c) = {}
[?, ?] ? pprojA ([b, c])
(2)
??
{
[?, ?] ? pimageA ([b, c]
c) = {}
[?, ?] ? pprojA ([b, c])
(3)
??
{
[?, ?] ? pimageA ([b, c]
c) = {}
pimageA ([b, c]) ? [?, ?]
(4)
??
{
conditions 2 and 3 hold
[?, ?] 6= {}
(5)
?? conditions 1, 2 and 3 hold
Equivalence 1 holds by definition of frontier(A).
Equivalence 2 holds because [?, ?] differs from
pmprojA ([b, c]) only in unaligned indices. Equiv-
alence 3 holds because given the disjointness
from pimageA ([b, c]
c), [?, ?] differs from
pimageA ([b, c]) only in unaligned indices. Equiva-
lences 4 and 5 are a restatement of conditions 2 and
3 plus the observation that empty spans can satisfy
conditions 2 and 3.
Corollary 2. Consider monotonic subrequest x =
[b0, c0]  [b1, c1]...[bk, ck] and arbitary subrequest
? = [?0, ?0]  [?1, ?1]...[?k, ?k]. Construction
request ?x, ?? matches alignment A iff [bi, ci]
A
?
[?i, ?i] for all 0 ? i ? k.
We are now ready to express the rule set
PBMT(s, ?,A) as an extraction program.
Theorem 3. PBMT(s, ?,A) =
EXTRACTs,?,A(RANKPP0, TRIVSPA, TRIVLP)
Proof.
?r, l? ? EXTs,?,A(RANKPP0, TRIVSPA, TRIVLP)
(1)
??
?
?????
?????
x = [b, c]  and ? = [?, ?] 
?x, ?? matches alignment A
{r} = CONSTRUCTRULEs,?,A(x, ?)
l = ?1
(2)
??
?
?????
?????
x = [b, c]  and ? = [?, ?] 
?x, ?? matches alignment A
r = ?0, ?sb...sc?, ???...???, ??
l = ?1
(3)
??
?
???
???
[b, c]
A
? [?, ?]
r = ?0, ?sb...sc?, ???...???, ??
l = ?1
(4)
?? ?r, l? ? PBMT(s, ?,A)
529
Equivalence 1 holds by the definition of EXTRACT
and RANKPP0. Equivalence 2 holds by the pseu-
docode of CONSTRUCTRULE. Equivalence 3 holds
from Corollary 2. Equivalence 4 holds from the def-
inition of PBMT(s, ?,A).
6 The Hiero Extraction Program
In this section, we express the hierarchical phrase-
based extraction technique of (Chiang, 2007) as
an extraction program. Define HIERO0(s, ?,A) =
PBMT(s, ?,A). For positive integer k, define
HIEROk(s, ?,A) as the smallest superset of HI-
EROk?1(s, ?,A) satisfying the following condition:
? For any labeled rule ??k ? 1, s?, ??, pi?,?k? ?
HIEROk?1(s, ?,A) such that:
1. s? = s?1 ? ?sb...sc? ? s
?
2
2. ?? = ??1 ? ???...??? ? ?
?
2
3. pi = ?pi1, pi2, ..., pik?1?
4. s?2 has rank 0.
4
5. ??1 has rank j.
6. [b, c]
A
? [?, ?]
it holds that labeled rule ?r,?k+1? is a member
of HIEROk(s, ?,A), where r is:
?k, s?1 ? ??? ? s
?
2, ?
?
1 ? ??? ? ?
?
2,
?pi1, ..., pij , k, pij+1, ..., pik?1??
Theorem 4. HIEROk(s, ?,A) =
EXTRACTs,?,A(RANKPPk, TRIVSPA, TRIVLP)
Proof. By induction. Define ext(k) to mean
EXTRACTs,?,A(RANKPPk, TRIVSPA, TRIVLP).
From Theorem 3, HIERO0(s, ?,A) = ext(0).
Assume that HIEROk?1(s, ?,A) = ext(k ? 1) and
prove that HIEROk(s, ?,A)\HIEROk?1(s, ?,A) =
ext(k)\ext(k ? 1).
?r?, l?? ? ext(k)\ext(k ? 1)
(1)
??
?
???????
???????
x? = [b0, c0] [b1, c1]...[bk, ck]
?? = [?0, ?0] [?1, ?1]...[?k, ?k]
?x?, ??? matches alignment A
{r?} = CONSTRUCTRULEs,?,A(x
?, ??)
l? = ?k+1
4This condition is not in the original definition. It is a cos-
metic addition, to enforce the consecutive ordering of variable
indices on the rule LHS.
(2)
??
?
????????????????????????????
????????????????????????????
x = [b0, c0] [b1, c1]...[bk?1, ck?1]
? = [?0, ?0] [?1, ?1]...[?k?1, ?k?1]
{r} = CONSTRUCTRULEs,?,A(x, ?)
pi = ?pi1, ..., pik?1?
r =
?k ? 1,s?1 ? ?sbk ...sck? ? s
?
2,
??1 ? ???k ...??k? ? ?
?
2, pi?
s?2 has rank 0 and ?
?
1 has rank j
x? = [b0, c0] [b1, c1]...[bk, ck]
?? = [?0, ?0] [?1, ?1]...[?k, ?k]
?x?, ??? matches alignment A
pi? = ?pi1, ..., pij , k, pij+1, ..., pik?1?
r? = ?k, s?1 ? ??? ? s
?
2, ?
?
1 ? ??? ? ?
?
2, pi
??
l? = ?k+1
(3)
??
?
??????????????????
??????????????????
pi = ?pi1, ..., pik?1?
r =
?k ? 1,s?1 ? ?sbk ...sck? ? s
?
2,
??1 ? ???k ...??k? ? ?
?
2, pi?
s?2 has rank 0 and ?
?
1 has rank j
?r,?k? ? HIEROk?1(s, ?,A)
pi? = ?pi1, ..., pij , k, pij+1, ..., pik?1?
r? = ?k, s?1 ? ??? ? s
?
2, ?
?
1 ? ??? ? ?
?
2, pi
??
[bi, ci]
A
? [?i, ?i] for all 0 ? i ? k
l? = ?k+1
(4)
?? ?r?, l?? ? HIEROk(s, ?,A)\HIEROk?1(s, ?,A)
Equivalence 1 holds by the definition of
ext(k)\ext(k ? 1). Equivalence 2 holds by
the pseudocode of CONSTRUCTRULE. Equivalence
3 holds by the inductive hypothesis and Corol-
lary 2. Equivalence 4 holds by the definition of
HIEROk(s, ?,A)\HIEROk?1(s, ?,A).
7 Discussion
In this paper, we have created a framework that al-
lows us to express a desired rule extraction method
as a set of construction requests and a labeling pro-
tocol. This enables a modular, ?mix-and-match? ap-
proach to rule extraction. In Table 1, we summa-
rize the results of this paper, as well as our conjec-
tured extraction programs for several other methods.
For instance, Syntax-Augmented Machine Transla-
tion (SAMT) (Zollmann and Venugopal, 2006) is a
530
hybridization of Hiero and GHKM that uses the pri-
mary protocol of Hiero and the labeling protocol of
GHKM. To bridge the approaches, SAMT employs
a fuzzy version5 of the spanmap t that assigns a triv-
ial label to non-constituent primary spans:
t?([b, c]) =
{
t([b, c]) if [b, c] ? spans(t)
??? otherwise
Other approaches can be similarly expressed as
straightforward variants of the extraction programs
we have developed in this paper.
Although we have focused on idealized meth-
ods, this framework also allows a compact and pre-
cise characterization of practical restrictions of these
techniques. For instance, (Chiang, 2007) lists six
criteria that he uses in practice to restrict the gener-
ation of Hiero rules. His condition 4 (?Rules can
have at most two nonterminals.?) and condition 5
(?It is prohibited for nonterminals to be adjacent on
the French side.?) can be jointly captured by replac-
ing Hiero?s primary protocol with the following:
{[b0, c0] [b1, c1]...[bj , cj ] s.t. 1 ? b0 ? c0
0 ? j ? 2
b2 > c1 + 1}
His other conditions can be similarly captured with
appropriate changes to Hiero?s primary and sec-
ondary protocols.
This work is primarily intended as a survey of the
dominant translation rule extraction paradigms, in
which we make explicit the close relationship be-
tween these approaches, and establish a language for
future hybridizations. From a practical perspective,
we facilitate a generic and extensible implementa-
tion which supports a wide variety of existing meth-
ods, and which permits the precise expression of
practical extraction heuristics.
5This corresponds with the original formulation of Syntax
Augmented Machine Translation (Zollmann and Venugopal,
2006). More recent versions of SAMT adopt a more refined
?fuzzifier? that assigns hybrid labels to non-constituent primary
spans.
References
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL, pages 263?270.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of ACL, pages 1443?
1452.
A. de Gispert, G. Iglesias, G. Blackwood, E.R. Banga,
and W. Byrne. 2010. Hierarchical phrase-based
translation with weighted finite state transducers and
shallow-n grammars. Computational Linguistics,
36(3):505?533.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proceedings of ACL,
pages 205?208.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of HLT/NAACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic models. In Proceedings of ACL-
COLING.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of AMTA.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
Thornton, Jonathan Weese, and Omar Zaidan. 2009.
Joshua: An open source toolkit for parsing-based ma-
chine translation. In Proceedings of the Fourth ACL
Workshop on Statistical Machine Translation, pages
135?139.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of ACL/COLING, pages 609?
616.
Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin. 2007.
Forest-to-string statistical translation rules. In Pro-
ceedings of ACL.
Yang Liu, Yajuan Lu, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Pro-
ceedings of ACL/IJCNLP, pages 558?566.
Adam Lopez. 2007. Hierarchical phrase-based transla-
tion with suffix arrays. In Proceedings of EMNLP-
CoNLL.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. Spmt: Statistical machine trans-
lation with syntactified target language phrases. In
Proceedings of EMNLP, pages 44?52.
531
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based translation.
In Proceedings of ACL.
Haitao Mi and Liang Huang. 2008. Forest-based transla-
tion rule extraction. In Proceedings of EMNLP.
Franz J. Och, Christof Tillmann, and Hermann Ney.
1999. Improved alignment models for statistical ma-
chine translation. In Proceedings of the Joint Conf. of
Empirical Methods in Natural Language Processing
and Very Large Corpora, pages 20?28.
Ashish Venugopal, Andreas Zollmann, and Stephan Vo-
gel. 2007. An efficient two-pass approach to
synchronous-cfg driven statistical mt. In Proceedings
of HLT/NAACL.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008. A tree se-
quence alignment-based tree-to-tree translation model.
In Proceedings of ACL.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of NAACL Workshop on Statistical Ma-
chine Translation.
532
