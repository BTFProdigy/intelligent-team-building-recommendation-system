Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1517?1526,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Compositional-ly Derived Representations of
Morphologically Complex Words in Distributional Semantics
Angeliki Lazaridou and Marco Marelli and Roberto Zamparelli and Marco Baroni
Center for Mind/Brain Sciences (University of Trento, Italy)
first.last@unitn.it
Abstract
Speakers of a language can construct an
unlimited number of new words through
morphological derivation. This is a major
cause of data sparseness for corpus-based
approaches to lexical semantics, such as
distributional semantic models of word
meaning. We adapt compositional meth-
ods originally developed for phrases to the
task of deriving the distributional meaning
of morphologically complex words from
their parts. Semantic representations con-
structed in this way beat a strong baseline
and can be of higher quality than represen-
tations directly constructed from corpus
data. Our results constitute a novel evalua-
tion of the proposed composition methods,
in which the full additive model achieves
the best performance, and demonstrate the
usefulness of a compositional morphology
component in distributional semantics.
1 Introduction
Effective ways to represent word meaning are
needed in many branches of natural language pro-
cessing. In the last decades, corpus-based meth-
ods have achieved some degree of success in mod-
eling lexical semantics. Distributional semantic
models (DSMs) in particular represent the mean-
ing of a word by a vector, the dimensions of which
encode corpus-extracted co-occurrence statistics,
under the assumption that words that are semanti-
cally similar will occur in similar contexts (Turney
and Pantel, 2010). Reliable distributional vectors
can only be extracted for words that occur in many
contexts in the corpus. Not surprisingly, there is
a strong correlation between word frequency and
vector quality (Bullinaria and Levy, 2007), and
since most words occur only once even in very
large corpora (Baroni, 2009), DSMs suffer data
sparseness.
While word rarity has many sources, one of the
most common and systematic ones is the high pro-
ductivity of morphological derivation processes,
whereby an unlimited number of new words can
be constructed by adding affixes to existing stems
(Baayen, 2005; Bauer, 2001; Plag, 1999).1 For
example, in the multi-billion-word corpus we in-
troduce below, perfectly reasonable derived forms
such as lexicalizable or affixless never occur. Even
without considering the theoretically infinite num-
ber of possible derived nonce words, and restrict-
ing ourselves instead to words that are already
listed in dictionaries, complex forms cover a high
portion of the lexicon. For example, morphologi-
cally complex forms account for 55% of the lem-
mas in the CELEX English database (see Section
4.1 below). In most of these cases (80% according
to our corpus) the stem is more frequent than the
complex form (e.g., the stem build occurs 15 times
more often than the derived form rebuild, and the
latter is certainly not an unusual derived form).
DSMs ignore derivational morphology alto-
gether. Consequently, they cannot provide mean-
ing representations for new derived forms, nor can
they harness the systematic relation existing be-
tween stems and derivations (any English speaker
can infer that to rebuild is to build again, whether
they are familiar with the prefixed form or not)
in order to mitigate derived-form sparseness prob-
lems. A simple way to handle derivational mor-
1Morphological derivation constructs new words (in
the sense of lemmas) from existing lexical items (re-
source+ful?resourceful). In this work, we do not treat in-
flectional morphology, pertaining to affixes that encode gram-
matical features such as number or tense (dog+s). We use
morpheme for any component of a word (resource and -ful
are both morphemes). We use stem for the lexical item that
constitutes the base of derivation (resource) and affix (pre-
fix or suffix) for the element attached to the stem to derive
the new form (-ful). In English, stems are typically indepen-
dent words, affixes bound morphemes, i.e., they cannot stand
alone. Note that a stem can in turn be morphologically de-
rived, e.g., point+less in pointless+ly. Finally, we use mor-
phologically complex as synonymous with derived.
1517
phology would be to identify the stem of rare de-
rived words and use its distributional vector as a
proxy to derived-form meaning.2 The meaning of
rebuild is not that far from that of build, so the
latter might provide a reasonable surrogate. Still,
something is clearly lost (if the author of a text
felt the need to use the derived form, the stem was
not fully appropriate), and sometimes the jump in
meaning can be quite dramatic (resourceless and
resource mean very different things!).
In the past few years there has been much in-
terest in how DSMs can scale up to represent the
meaning of larger chunks of text such as phrases
or even sentences. Trying to represent the mean-
ing of arbitrarily long constructions by directly
collecting co-occurrence statistics is obviously in-
effective and thus methods have been developed
to derive the meaning of larger constructions as a
function of the meaning of their constituents (Ba-
roni and Zamparelli, 2010; Coecke et al, 2010;
Mitchell and Lapata, 2008; Mitchell and Lapata,
2010; Socher et al, 2012). Compositional distri-
butional semantic models (cDSMs) of word units
aim at handling, compositionally, the high produc-
tivity of phrases and consequent data sparseness.
It is natural to hypothesize that the same methods
can be applied to morphology to derive the mean-
ing of complex words from the meaning of their
parts: For example, instead of harvesting a rebuild
vector directly from the corpus, the latter could be
constructed from the distributional representations
of re- and build. Besides alleviating data sparse-
ness problems, a system of this sort, that automati-
cally induces the semantic contents of morpholog-
ical processes, would also be of tremendous theo-
retical interest, given that the semantics of deriva-
tion is a central and challenging topic in linguistic
morphology (Dowty, 1979; Lieber, 2004).
In this paper, we explore, for the first time (ex-
cept for the proof-of-concept study in Guevara
(2009)), the application of cDSMs to derivational
morphology. We adapt a number of composition
methods from the literature to the morphological
setting, and we show that some of these methods
can provide better distributional representations of
derived forms than either those directly harvested
from a large corpus, or those obtained by using
the stem as a proxy to derived-form meaning. Our
2Of course, spotting and segmenting complex words is a
big research topic unto itself (Beesley and Karttunen, 2000;
Black et al, 1991; Sproat, 1992), and one we completely
sidestep here.
results suggest that exploiting morphology could
improve the quality of DSMs in general, extend
the range of tasks that cDSMs can successfully
model and support the development of new ways
to test their performance.
2 Related work
Morphological induction systems use corpus-
based methods to decide if two words are mor-
phologically related and/or to segment words into
morphemes (Dreyer and Eisner, 2011; Goldsmith,
2001; Goldwater and McClosky, 2005; Goldwater,
2006; Naradowsky and Goldwater, 2009; Wicen-
towski, 2004). Morphological induction has re-
cently received considerable attention since mor-
phological analysis can mitigate data sparseness in
domains such as parsing and machine translation
(Goldberg and Tsarfaty, 2008; Lee, 2004). Among
the cues that have been exploited there is distri-
butional similarity among morphologically related
words (Schone and Jurafsky, 2000; Yarowsky and
Wicentowski, 2000). Our work, however, dif-
fers substantially from this track of research. We
do not aim at segmenting morphological complex
words or identifying paradigms. Our goal is to
automatically construct, given distributional rep-
resentations of stems and affixes, semantic repre-
sentations for the derived words containing those
stems and affixes. A morphological induction sys-
tem, given rebuild, will segment it into re- and
build (possibly using distributional similarity be-
tween the words as a cue). Our system, given
re- and build, predicts the (distributional seman-
tic) meaning of rebuild.
Another emerging line of research uses distribu-
tional semantics to model human intuitions about
the semantic transparency of morphologically de-
rived or compound expressions and how these im-
pact various lexical processing tasks (Kuperman,
2009; Wang et al, 2012). Although these works
exploit vectors representing complex forms, they
do not attempt to generate them compositionally.
The only similar study we are aware of is that
of Guevara (2009). Guevara found a systematic
geometric relation between corpus-based vectors
of derived forms sharing an affix and their stems,
and used this finding to motivate the composition
method we term lexfunc below. However, unlike
us, he did not test alternative models, and he only
presented a qualitative analysis of the trajectories
triggered by composition with various affixes.
1518
3 Composition methods
Distributional semantic models (DSMs), also
known as vector-space models, semantic spaces,
or by the names of famous incarnations such as
Latent Semantic Analysis or Topic Models, ap-
proximate the meaning of words with vectors that
record their patterns of co-occurrence with cor-
pus context features (often, other words). There
is an extensive literature on how to develop such
models and on their evaluation. Recent surveys
include Clark (2012), Erk (2012) and Turney and
Pantel (2010). We focus here on compositional
DSMs (cDSMs). Since the very inception of dis-
tributional semantics, there have been attempts to
compose meanings for sentences and larger pas-
sages (Landauer and Dumais, 1997), but inter-
est in compositional DSMs has skyrocketed in
the last few years, particularly since the influen-
tial work of Mitchell and Lapata (2008; 2009;
2010). For the current study, we have reimple-
mented and adapted to the morphological setting
all cDSMs we are aware of, excluding the tensor-
product-based models that Mitchell and Lapata
(2010) have shown to be empirically disappointing
and the models of Socher and colleagues (Socher
et al, 2011; Socher et al, 2012), that require com-
plex optimization procedures whose adaptation to
morphology we leave to future work.
Mitchell and Lapata proposed a set of simple
and effective models in which the composed vec-
tors are obtained through component-wise opera-
tions on the constituent vectors. Given input vec-
tors u and v, the multiplicative model (mult) re-
turns a composed vector c with: ci = uivi. In the
weighted additive model (wadd), the composed
vector is a weighted sum of the two input vectors:
c = ?u + ?v, where ? and ? are two scalars. In
the dilation model, the output vector is obtained
by first decomposing one of the input vectors, say
v, into a vector parallel to u and an orthogonal
vector. Following this, the parallel vector is dilated
by a factor ? before re-combining. This results in:
c = (?? 1)?u,v?u+ ?u,u?v.
Guevara (2010) and Zanzotto et al (2010) pro-
pose the full additive model (fulladd), where the
two vectors to be added are pre-multiplied by
weight matrices: c = Au+Bv
Since the Mitchell and Lapata and fulladd mod-
els were developed for phrase composition, the
two input vectors were taken to be, very straight-
forwardly, the vectors of the two words to be com-
posed into the phrase of interest. In morphological
derivation, at least one of the items to be composed
(the affix) is a bound morpheme. In our adapta-
tion of these composition models, we build bound
morpheme vectors by accumulating the contexts
in which a set of derived words containing the rel-
evant morphemes occur, e.g., the re- vector aggre-
gates co-occurrences of redo, remake, retry, etc.
Baroni and Zamparelli (2010) and Coecke et
al. (2010) take inspiration from formal semantics
to characterize composition in terms of function
application, where the distributional representa-
tion of one element in a composition (the func-
tor) is not a vector but a function. Given that
linear functions can be expressed by matrices and
their application by matrix-by-vector multiplica-
tion, in this lexical function (lexfunc) model, the
functor is represented by a matrix U to be multi-
plied with the argument vector v: c = Uv. In
the case of morphology, it is natural to treat bound
affixes as functions over stems, since affixes en-
code the systematic semantic patterns we intend
to capture. Unlike the other composition meth-
ods, lexfunc does not require the construction of
distributional vectors for affixes. A matrix repre-
sentation for every affix is instead induced directly
from examples of stems and the corresponding de-
rived forms, in line with the intuition that every af-
fix corresponds to a different pattern of change of
the stem meaning.
Finally, as already discussed in the Introduc-
tion, performing no composition at all but using
the stem vector as a surrogate of the derived form
is a reasonable strategy. We saw that morphologi-
cally derived words tend to appear less frequently
than their stems, and in many cases the meanings
are close. Consequently, we expect a stem-only
?composition? method to be a strong baseline in
the morphological setting.
4 Experimental setup
4.1 Morphological data
We obtained a list of stem/derived-form pairs from
the CELEX English Lexical Database, a widely
used 100K-lemma lexicon containing, among
other things, information about the derivational
structure of words (Baayen et al, 1995). For each
derivational affix present in CELEX, we extracted
from the database the full list of stem/derived
pairs matching its most common part-of-speech
signature (e.g., for -er we only considered pairs
1519
Affix Stem/Der. Training HQ/Tot. Avg.
POS Items Test Items SDR
-able verb/adj 177 30/50 5.96
-al noun/adj 245 41/50 5.88
-er verb/noun 824 33/50 5.51
-ful noun/adj 53 42/50 6.11
-ic noun/adj 280 43/50 5.99
-ion verb/noun 637 38/50 6.22
-ist noun/noun 244 38/50 6.16
-ity adj/noun 372 33/50 6.19
-ize noun/verb 105 40/50 5.96
-less noun/adj 122 35/50 3.72
-ly adj/adv 1847 20/50 6.33
-ment verb/noun 165 38/50 6.06
-ness adj/noun 602 33/50 6.29
-ous noun/adj 157 35/50 5.94
-y noun/adj 404 27/50 5.25
in- adj/adj 101 34/50 3.39
re- verb/verb 86 27/50 5.28
un- adj/adj 128 36/50 3.23
tot */* 6549 623/900 5.52
Table 1: Derivational morphology dataset
having a verbal stem and nominal derived form).
Since CELEX was populated by semi-automated
morphological analysis, it includes forms that are
probably not synchronically related to their stems,
such as crypt+ic or re+form. However, we did not
manually intervene on the pairs, since we are in-
terested in training and testing our methods in re-
alistic, noisy conditions. In particular, the need to
pre-process corpora to determine which forms are
?opaque?, and should thus be bypassed by our sys-
tems, would greatly reduce their usefulness. Pairs
in which either word occurred less than 20 times
in our source corpus (described in Section 4.2 be-
low) were filtered out and, in our final dataset, we
only considered the 18 affixes (3 prefixes and 15
suffixes) with at least 100 pairs meeting this con-
dition. We randomly chose 50 stem/derived pairs
(900 in total) as test data. The remaining data were
used as training items to estimate the parameters
of the composition methods. Table 1 summarizes
various characteristics of the dataset3 (the last two
columns of the table are explained in the next para-
graphs).
Annotation of quality of test vectors The qual-
ity of the corpus-based vectors representing de-
rived test items was determined by collecting hu-
man semantic similarity judgments in a crowd-
sourcing survey. In particular, we use the similar-
ity of a vector to its nearest neighbors (NNs) as a
proxy measure of quality. The underlying assump-
3Available from http://clic.cimec.unitn.it/
composes
tion is that a vector, in order to be a good represen-
tation of the meaning of the corresponding word,
should lie in a region of semantic space populated
by intuitively similar meanings, e.g., we are more
likely to have captured the meaning of car if the
NN of its vector is the automobile vector rather
than potato. Therefore, to measure the quality of
a given vector, we can look at the average simi-
larity score provided by humans when comparing
this very vector with its own NNs.
All 900 derived vectors from the test set were
matched with their three closest NNs in our se-
mantic space (see Section 4.2), thus producing a
set of 2, 700 word pairs. These pairs were admin-
istered to CrowdFlower users,4 who were asked
to judge the relatedness of the two meanings on a
7-point scale (higher for more related). In order
to ensure that participants were committed to the
task and exclude non-proficient English speakers,
we used 60 control pairs as gold standard, consist-
ing of either perfect synonyms or completely un-
related words. We obtained 30 judgments for each
derived form (10 judgments for each of 3 neighbor
comparisons), with mean participant agreement of
58%. These ratings were averaged item-wise, re-
sulting in a Gaussian distribution with a mean of
3.79 and a standard deviation of 1.31. Finally,
each test item was marked as high-quality (HQ)
if its derived form received an average score of at
least 3, as low-quality (LQ) otherwise. Table 1 re-
ports the proportion of HQ test items for each af-
fix, and Table 2 reports some examples of HQ and
LQ items with the corresponding NNs. It is worth
observing that the NNs of the LQ items, while not
as relevant as the HQ ones, are hardly random.
Annotation of similarity between stem and de-
rived forms Derived forms differ in terms of
how far their meaning is with respect to that of
their stem. Certain morphological processes have
systematically more impact than others on mean-
ing: For example, the adjectival prefix in- negates
the meaning of the stem, whereas -ly has the sole
function to convert an adjective into an adverb.
But the very same affix can affect different stems
in different ways. For example, remelt means lit-
tle more than to melt again, but rethink has subtler
implications of changing one?s way to look at a
problem, and while one of the senses of cycling is
present in recycle, it takes some effort to see their
relation.
4http://www.crowdflower.com
1520
Affix Type Derived form Neighbors
-ist HQ transcendentalist mythologist, futurist, theosophistLQ florist Harrod, wholesaler, stockist
-ity HQ publicity publicise, press, publicizeLQ sparsity dissimilarity, contiguity, perceptibility
-ment HQ advertisement advert, promotional, advertisingLQ inducement litigant, contractually, voluntarily
in- HQ inaccurate misleading, incorrect, erroneousLQ inoperable metastasis, colorectal, biopsy
re- HQ recapture retake, besiege, captureLQ rename defunct, officially, merge
Table 2: Examples of HQ and LQ derived vectors with their NNs
We conducted a separate crowdsourcing study
where participants were asked to rate the 900
test stem/derived pairs for the strength of their
semantic relationship on a 7-point scale. We
followed a procedure similar to the one de-
scribed for quality measurement; 7 judgments
were collected for each pair. Participants? agree-
ment was at 60%. The last column of Ta-
ble 1 reports the average stem/derived related-
ness (SDR) for the various affixes. Note that
the affixes with systematically lower SDR are
those carrying a negative meaning (in-, un-, -less),
whereas those with highest SDR do little more
than changing the POS of the stem (-ion, -ly, -
ness). Among specific pairs with very low related-
ness we encounter hand/handy, bear/bearable and
active/activist, whereas compulsory/compulsorily,
shameless/shamelessness and chaos/chaotic have
high SDR. Since the distribution of the average
ratings was negatively skewed (mean rating: 5.52,
standard deviation: 1.26),5 we took 5 as the rating
threshold to classify items as having high (HR) or
low (LR) relatedness to their stems.
4.2 Distributional semantic space6
We use as our source corpus the concatenation of
ukWaC, the English Wikipedia (2009 dump) and
the BNC,7 for a total of about 2.8 billion tokens.
We collect co-occurrence statistics for the top 20K
content words (adjectives, adverbs, nouns, verbs)
5The negative skew is not surprising, as derived forms
must have some relation to their stems!
6Most steps of the semantic space construction
and composition pipelines were implemented using
the DISSECT toolkit: https://github.com/
composes-toolkit/dissect.
7http://wacky.sslmit.unibo.it, http:
//en.wikipedia.org, http://www.natcorp.
ox.ac.uk
in lemma format, plus any item from the mor-
phological dataset described above that was below
this rank. The top 20K content words also con-
stitute our context elements. We use a standard
bag-of-words approach, counting collocates in a
narrow 2-word before-and-after window. We ap-
ply (non-negative) Pointwise Mutual Information
as weighting scheme and dimensionality reduc-
tion by Non-negative Matrix Factorization, setting
the number of reduced-space dimensions to 350.
These settings are chosen without tuning, and are
based on previous experiments where they pro-
duced high-quality semantic spaces (Boleda et al,
2013; Bullinaria and Levy, 2007).
4.3 Implementation of composition methods
All composition methods except mult and stem
have weights to be estimated (e.g., the ? parame-
ter of dilation or the affix matrices of lexfunc). We
adopt the estimation strategy proposed by Gue-
vara (2010) and Baroni and Zamparelli (2010),
namely we pick parameter values that optimize
the mapping between stem and derived vectors di-
rectly extracted from the corpus. To learn, say, a
lexfunc matrix representing the prefix re-, we ex-
tract vectors of V/reV pairs that occur with suffi-
cient frequency (visit/revisit, think/rethink. . . ). We
then use least-squares methods to find weights for
the re- matrix that minimize the distance between
each reV vector generated by the model given the
input V and the corresponding corpus-observed
derived vector (e.g., we try to make the model-
predicted re+visit vector as similar as possible
to the corpus-extracted one). This is a general
estimation approach that does not require task-
specific hand-labeled data, and for which simple
analytical solutions of the least-squares error prob-
1521
lem exist for all our composition methods. We use
only the training items from Section 4.1 for esti-
mation. Note that, unlike the test items, these have
not been annotated for quality, so we are adopting
an unsupervised (no manual labeling) but noisy es-
timation method.8
For the lexfunc model, we use the training items
separately to obtain weight matrices represent-
ing each affix, whereas for the other models all
training data are used together to globally de-
rive single sets of affix and stem weights. For
the wadd model, the learning process results in
0.16?affix+0.33? stem, i.e., the affix contributes
only half of its mass to the composition of the
derived form. For dilation, we stretch the stem
(i.e., v of the dilation equation is the stem vector),
since it should provide richer contents than the af-
fix to the derived meaning. We found that, on av-
erage across the training pairs, dilation weighted
the stem 20 times more heavily than the affix
(0.05?affix+1?stem). We then expect that the di-
lation model will have similar performance to the
baseline stem model, as confirmed below.9
For all methods, vectors were normalized be-
fore composing both in training and in generation.
5 Experiment 1: approximating
high-quality corpus-extracted vectors
The first experiment investigates to what extent
composition models can approximate high-quality
(HQ) corpus-extracted vectors representing de-
rived forms. Note that since the test items were
excluded from training, we are simulating a sce-
nario in which composition models must generate
representations for nonce derived forms.
Cosine similarity between model-generated and
corpus-extracted vectors were computed for all
models, including the stem baseline (i.e., co-
sine between stem and derived form). The first
row of Table 3 reports mean similarities. The
stem method sets the level of performance rel-
atively high, confirming its soundness. Indeed,
the parameter-free mult model performs below the
baseline.10 As expected, dilation performs simi-
8More accurately, we relied on semi-manual CELEX in-
formation to identify derived forms. A further step towards a
fully knowledge-free system would be to pre-process the cor-
pus with an unsupervised morphological induction system to
extract stem/derived pairs.
9The other models have thousands of weights to be es-
timated, so we cannot summarize the outcome of parameter
estimation here.
10This result does not necessarily contradict those of
stem mult dil. wadd fulladd lexfunc
All 0.47 0.39 0.48 0.50 0.56 0.54
HR 0.52 0.43 0.53 0.55 0.61 0.58
LR 0.32 0.28 0.33 0.38 0.41 0.42
Table 3: Mean similarity of composed vectors to
high-quality corpus-extracted derived-form vec-
tors, for all as well as high- (HR) and low-
relatedness (LR) test items
larly to the baseline, while wadd outperforms it,
although the effect does not reach significance
(p=.06).11 Both fulladd and lexfunc perform sig-
nificantly better than stem (p < .001). Lexfunc
provides a flexible way to account for affixation,
since it models it directly as a function mapping
from and onto word vectors, without requiring a
vector representation of bound affixes. The rea-
son at the base of its good performance is thus
quite straightforward. On the other hand, it is
surprising that a simple representation of bound
affixes (i.e., as vectors aggregating the contexts
of words containing them) can work so well, at
least when used in conjunction with the granular
dimension-by-dimension weights assigned by the
fulladd method. We hypothesize that these aggre-
gated contexts, by providing information about the
set of stems an affix combines with, capture the
shared semantic features that the affix operates on.
When the meaning of the derived form is far
from that of its stem, the stem baseline should no
longer constitute a suitable surrogate of derived-
form meaning. The LR cases (see Section 4.1
above) are thus crucial to understand how well
composition methods capture not only stem mean-
ing, but also affix-triggered semantics. The HR
and LR rows of Table 3 present the results for
the respective test subsets. As expected, the stem
approach undergoes a strong drop when perfor-
mance is measured on LR items. At the other ex-
treme, fulladd and lexfunc, while also finding the
LR cases more difficult, still clearly outperform
the baseline (p<.001), confirming that they cap-
ture the meaning of derived forms beyond what
their stems contribute to it. The effect of wadd,
again, approaches significance when compared to
the baseline (p= .05). Very encouragingly, both
Mitchell and Lapata and others who found mult to be highly
competitive. Due to differences in co-occurrence weighting
schemes (we use a logarithmically scaled measure, they do
not), their multiplicative model is closer to our additive one.
11Significance assessed by means of Tukey Honestly Sig-
nificant Difference tests (Abdi and Williams, 2010)
1522
stem mult wadd dil. fulladd lexfunc
-less 0.22 0.23 0.30 0.24 0.38 0.44
in- 0.39 0.34 0.45 0.40 0.47 0.45
un- 0.33 0.33 0.41 0.34 0.44 0.46
Table 4: Mean similarity of composed vectors to
high-quality corpus-extracted derived-form vec-
tors with negative affixes
fulladd and lexfunc significantly outperform stem
also in the HR subset (p<.001). That is, the mod-
els provide better approximations of derived forms
even when the stem itself should already be a good
surrogate. The difference between the two models
is not significant.
We noted in Section 4.1 that forms containing
the ?negative? affixes -less, un- and in- received
on average low SDR scores, since negation im-
pacts meaning more drastically than other opera-
tions. Table 4 reports the performance of the mod-
els on these affixes. Indeed, the stem baseline per-
forms quite poorly, whereas fulladd, lexfunc and,
to a lesser extent, wadd are quite effective in this
condition as well, all performing greatly above the
baseline. These results are intriguing in light of
the fact that modeling negation is a challenging
task for DSMs (Mohammad et al, 2013) as well as
cDSMs (Preller and Sadrzadeh, 2011). To the ex-
tent that our best methods have captured the negat-
ing function of a prefix such as in-, they might be
applied to tasks such as recognizing lexical op-
posites, or even simple forms of syntactic nega-
tion (modeling inoperable is just a short step away
from modeling not operable compositionally).
6 Experiment 2: Comparing the quality
of corpus-extracted and
compositionally generated words
The first experiment simulated the scenario in
which derived forms are not in our corpus, so
that directly extracting their representation from
it is not an option. The second experiment tests
if compositionally-derived representations can be
better than those extracted directly from the corpus
when the latter is a possible strategy (i.e., the de-
rived forms are attested in the source corpus). To
this purpose, we focused on those 277 test items
that were judged as low-quality (LQ, see Section
4.1), which are presumably more challenging to
generate, and where the compositional route could
be most useful.
We evaluated the derived forms generated by
corpus stem wadd fulladd lexfunc
All 2.28 3.26 4.12 3.99 3.09
HR 2.29 3.56 4.48 4.31 3.31
LR 2.22 2.48 3.14 3.12 2.52
Table 5: Average quality ratings of derived vectors
Target Model Neighbors
florist
wadd flora, fauna, ecosystem
fulladd flora, fauna, egologist
lexfunc ornithologist, naturalist, botanist
sparsity
wadd sparse, sparsely, dense
fulladd sparse, sparseness, angularity
lexfunc fragility, angularity, smallness
inducement
wadd induce, inhibit, inhibition
fulladd induce, inhibition, mediate
lexfunc impairment, cerebral, ocular
inoperable
wadd operable, palliation, biopsy
fulladd operable, inoperative, ventilator
lexfunc inoperative, unavoidably, flaw
rename
wadd name, later, namesake
fulladd name, namesake, later
lexfunc temporarily, reinstate, thereafter
Table 6: Examples of model-predicted neighbors
for words with LQ corpus-extracted vectors
the models that performed best in the first exper-
iment (fulladd, lexfunc and wadd), as well as the
stem baseline, by means of another crowdsourcing
study. We followed the same procedure used to
assess the quality of corpus-extracted vectors, that
is, we asked judges to rate the relatedness of the
target forms to their NNs (we obtained on average
29 responses per form).
The first line of Table 5 reports the average
quality (on a 7-point scale) of the representations
of the derived forms as produced by the models
and baseline, as well as of the corpus-harvested
ones (corpus column). All compositional models
produce representations that are of significantly
higher quality (p < .001) than the corpus-based
ones. The effect is also evident in qualitative
terms. Table 6 presents the NNs predicted by the
three compositional methods for the same LQ test
items whose corpus-based NNs are presented in
Table 2. These results indicate that morpheme
composition is an effective solution when the qual-
ity of corpus-extracted derived forms is low (and
the previous experiment showed that, when their
quality is high, composition can at least approxi-
mate corpus-based vectors).
With respect to Experiment 1, we obtain a dif-
ferent ranking of the models, with lexfunc being
outperformed by both wadd and fulladd (p<.001),
that are statistically indistinguishable. The wadd
1523
composition is dominated by the stem, and by
looking at the examples in Table 6 we notice that
both this model and fulladd tend to feature the
stem as NN (100% of the cases for wadd, 73%
for fulladd in the complete test set). The question
thus arises as to whether the good performance of
these composition techniques is simply due to the
fact that they produce derived forms that are near
their stems, with no added semantic value from the
affix (a ?stemploitation? strategy).
However, the stemploitation hypothesis is dis-
pelled by the observation that both models signifi-
cantly outperform the stem baseline (p<.001), de-
spite the fact that the latter, again, has good per-
formance, significantly outperforming the corpus-
derived vectors (p < .001). Thus, we confirm
that compositional models provide higher qual-
ity vectors that are capturing the meaning of de-
rived forms beyond the information provided by
the stem.
Indeed, if we focus on the third row of Ta-
ble 5, reporting performance on low stem-derived
relatedness (LR) items (annotated as described in
Section 4.1), fulladd and wadd still significantly
outperform the corpus representations (p<.001),
whereas the quality of the stem representations of
LR items is not significantly different form that of
the corpus-derived ones. Interestingly, lexfunc dis-
plays the smallest drop in performance when re-
stricting evaluation to LR items; however, since it
does not significantly outperform the LQ corpus
representations, this is arguably due to a floor ef-
fect.
7 Conclusion and future work
We investigated to what extent cDSMs can gener-
ate effective meaning representations of complex
words through morpheme composition. Several
state-of-the-art composition models were adapted
and evaluated on this novel task. Our results sug-
gest that morpheme composition can indeed pro-
vide high-quality vectors for complex forms, im-
proving both on vectors directly extracted from the
corpus and on a stem-backoff strategy. This re-
sult is of practical importance for distributional se-
mantics, as it paves the way to address one of the
main causes of data sparseness, and it confirms the
usefulness of the compositional approach in a new
domain. Overall, fulladd emerged as the best per-
forming model, with both lexfunc and the simple
wadd approach constituting strong rivals. The ef-
fectiveness of the best models extended also to the
challenging cases where the meaning of derived
forms is far from that of the stem, including nega-
tive affixes.
The fulladd method requires a vector represen-
tation for bound morphemes. A first direction for
future work will thus be to investigate which as-
pects of the meaning of bound morphemes are
captured by our current simple-minded approach
to populating their vectors, and to explore alterna-
tive ways to construct them, seeing if they further
improve fulladd performance.
A natural extension of our research is to ad-
dress morpheme composition and morphological
induction jointly, trying to model the intuition that
good candidate morphemes should have coherent
semantic representations. Relatedly, in the cur-
rent setting we generate complex forms from their
parts. We want to investigate the inverse route,
namely ?de-composing? complex words to de-
rive representations of their stems, especially for
cases where the complex words are more frequent
(e.g. comfort/comfortable).
We would also like to apply composition to in-
flectional morphology (that currently lies outside
the scope of distributional semantics), to capture
the nuances of meaning that, for example, distin-
guish singular and plural nouns (consider, e.g., the
difference between the mass singular tea and the
plural teas, which coerces the noun into a count
interpretation (Katz and Zamparelli, 2012)).
Finally, in our current setup we focus on a single
composition step, e.g., we derive the meaning of
inoperable by composing the morphemes in- and
operable. But operable is in turn composed of op-
erate and -able. In the future, we will explore re-
cursive morpheme composition, especially since
we would like to apply these methods to more
complex morphological systems (e.g., agglutina-
tive languages) where multiple morphemes are the
norm.
8 Acknowledgments
We thank Georgiana Dinu and Nghia The Pham
for helping out with DISSECT-ion and the review-
ers for helpful feedback. This research was sup-
ported by the ERC 2011 Starting Independent Re-
search Grant n. 283554 (COMPOSES).
1524
References
Herve? Abdi and Lynne Williams. 2010. Newman-
Keuls and Tukey test. In Neil Salkind, Bruce Frey,
and Dondald Dougherty, editors, Encyclopedia of
Research Design, pages 897?904. Sage, Thousand
Oaks, CA.
Harald Baayen, Richard Piepenbrock, and Leon Gu-
likers. 1995. The CELEX lexical database (re-
lease 2). CD-ROM, Linguistic Data Consortium,
Philadelphia, PA.
Harald Baayen. 2005. Morphological productivity. In
Rajmund Piotrowski Reinhard Ko?hler, Gabriel Alt-
mann, editor, Quantitative Linguistics: An Inter-
national Handbook, pages 243?256. Mouton de
Gruyter, Berlin, Germany.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183?1193, Boston,
MA.
Marco Baroni. 2009. Distributions in text. In Anke
Lu?deling and Merja Kyto?, editors, Corpus Linguis-
tics: An International Handbook, volume 2, pages
803?821. Mouton de Gruyter, Berlin, Germany.
Laurie Bauer. 2001. Morphological Productivity.
Cambridge University Press, Cambridge, UK.
Kenneth Beesley and Lauri Karttunen. 2000. Finite-
State Morphology: Xerox Tools and Techniques.
Cambridge University Press, Cambridge, UK.
Alan Black, Stephen Pulman, Graeme Ritchie, and
Graham Russell. 1991. Computational Morphol-
ogy. MIT Press, Cambrdige, MA.
Gemma Boleda, Marco Baroni, Louise McNally, and
Nghia Pham. 2013. Intensionality was only alleged:
On adjective-noun composition in distributional se-
mantics. In Proceedings of IWCS, pages 35?46,
Potsdam, Germany.
John Bullinaria and Joseph Levy. 2007. Extracting
semantic representations from word co-occurrence
statistics: A computational study. Behavior Re-
search Methods, 39:510?526.
Stephen Clark. 2012. Vector space models of lexical
meaning. In Shalom Lappin and Chris Fox, editors,
Handbook of Contemporary Semantics, 2nd edition.
Blackwell, Malden, MA. In press.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical foundations for a com-
positional distributional model of meaning. Linguis-
tic Analysis, 36:345?384.
David Dowty. 1979. Word Meaning and Montague
Grammar. Springer, New York.
Markus Dreyer and Jason Eisner. 2011. Discover-
ing morphological paradigms from plain text using
a Dirichlet process mixture model. In Proceedings
of EMNLP, pages 616?627, Edinburgh, UK.
Katrin Erk. 2012. Vector space models of word mean-
ing and phrase meaning: A survey. Language and
Linguistics Compass, 6(10):635?653.
Yoav Goldberg and Reut Tsarfaty. 2008. A single gen-
erative model for joint morphological segmentation
and syntactic parsing. In Proceedings of ACL, pages
371?379, Columbus, OH.
John Goldsmith. 2001. Unsupervised learning of the
morphology of a natural language. Computational
Linguistics, 2(27):153?198.
Sharon Goldwater and David McClosky. 2005. Im-
proving statistical MT through morphological anal-
ysis. In Proceedings of EMNLP, pages 676?683,
Vancouver, Canada.
Sharon Goldwater. 2006. Nonparametric Bayesian
Models of Lexical Acquisition. Ph.D. thesis, Brown
University.
Emiliano Guevara. 2009. Compositionality in distribu-
tional semantics: Derivational affixes. In Proceed-
ings of the Words in Action Workshop, Pisa, Italy.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of GEMS, pages 33?37,
Uppsala, Sweden.
Graham Katz and Roberto Zamparelli. 2012. Quanti-
fying count/mass elasticity. In Proceedings of WC-
CFL, pages 371?379, Tucson, AR.
Victor Kuperman. 2009. Semantic transparency revis-
ited. Presentation at the 6th International Morpho-
logical Processing Conference.
Thomas Landauer and Susan Dumais. 1997. A solu-
tion to Plato?s problem: The latent semantic analysis
theory of acquisition, induction, and representation
of knowledge. Psychological Review, 104(2):211?
240.
Young-Suk Lee. 2004. Morphological analysis for sta-
tistical machine translation. In Proceedings of HLT-
NAACL, pages 57?60, Boston, MA.
Rochelle Lieber. 2004. Morphology and Lexical Se-
mantics. Cambridge University Press, Cambridge,
UK.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL, pages 236?244, Columbus, OH.
Jeff Mitchell and Mirella Lapata. 2009. Language
models based on semantic composition. In Proceed-
ings of EMNLP, pages 430?439, Singapore.
1525
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Saif Mohammad, Bonnie Dorr, Graeme Hirst, and Pe-
ter Turney. 2013. Computing lexical contrast. Com-
putational Linguistics. In press.
Jason Naradowsky and Sharon Goldwater. 2009. Im-
proving morphology induction by learning spelling
rules. In Proceedings of IJCAI, pages 11?17,
Pasadena, CA.
Ingo Plag. 1999. Morphological Productivity: Struc-
tural Constraints in English Derivation. Mouton de
Gruyter, Berlin, Germany.
Anne Preller and Mehrnoosh Sadrzadeh. 2011. Bell
states and negative sentences in the distributed
model of meaning. Electr. Notes Theor. Comput.
Sci., 270(2):141?153.
Patrick Schone and Daniel Jurafsky. 2000.
Knowledge-free induction of morphology us-
ing latent semantic analysis. In Proceedings of the
ConLL workshop on learning language in logic,
pages 67?72, Lisbon, Portugal.
Richard Socher, Eric Huang, Jeffrey Pennin, Andrew
Ng, and Christopher Manning. 2011. Dynamic
pooling and unfolding recursive autoencoders for
paraphrase detection. In Proceedings of NIPS, pages
801?809, Granada, Spain.
Richard Socher, Brody Huval, Christopher Manning,
and Andrew Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of EMNLP, pages 1201?1211, Jeju Island, Ko-
rea.
Richard Sproat. 1992. Morphology and Computation.
MIT Press, Cambrdige, MA.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
Hsueh-Cheng Wang, Yi-Min Tien, Li-Chuan Hsu, and
Marc Pomplun. 2012. Estimating semantic trans-
parency of constituents of English compounds and
two-character Chinese words using Latent Semantic
Analysis. In Proceedings of CogSci, pages 2499?
2504, Sapporo, Japan.
Richard Wicentowski. 2004. Multilingual noise-
robust supervised morphological analysis using the
wordframe model. In Proceedings of SIGPHON,
pages 70?77, Barcelona, Spain.
David Yarowsky and Richard Wicentowski. 2000.
Minimally supervised morphological analysis by
multimodal alignment. In Proceedings of ACL,
pages 207?216, Hong Kong.
Fabio Zanzotto, Ioannis Korkontzelos, Francesca
Falucchi, and Suresh Manandhar. 2010. Estimat-
ing linear models for compositional distributional
semantics. In Proceedings of COLING, pages 1263?
1271, Beijing, China.
1526
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 53?57,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A relatedness benchmark to test the role of determiners
in compositional distributional semantics
Raffaella Bernardi and Georgiana Dinu and Marco Marelli and Marco Baroni
Center for Mind/Brain Sciences (University of Trento, Italy)
first.last@unitn.it
Abstract
Distributional models of semantics cap-
ture word meaning very effectively, and
they have been recently extended to ac-
count for compositionally-obtained rep-
resentations of phrases made of content
words. We explore whether compositional
distributional semantic models can also
handle a construction in which grammat-
ical terms play a crucial role, namely de-
terminer phrases (DPs). We introduce a
new publicly available dataset to test dis-
tributional representations of DPs, and we
evaluate state-of-the-art models on this set.
1 Introduction
Distributional semantics models (DSMs) approx-
imate meaning with vectors that record the dis-
tributional occurrence patterns of words in cor-
pora. DSMs have been effectively applied to in-
creasingly more sophisticated semantic tasks in
linguistics, artificial intelligence and cognitive sci-
ence, and they have been recently extended to
capture the meaning of phrases and sentences via
compositional mechanisms. However, scaling up
to larger constituents poses the issue of how to
handle grammatical words, such as determiners,
prepositions, or auxiliaries, that lack rich concep-
tual content, and operate instead as the logical
?glue? holding sentences together.
In typical DSMs, grammatical words are treated
as ?stop words? to be discarded, or at best used
as context features in the representation of content
words. Similarly, current compositional DSMs
(cDSMs) focus almost entirely on phrases made
of two or more content words (e.g., adjective-noun
or verb-noun combinations) and completely ig-
nore grammatical words, to the point that even
the test set of transitive sentences proposed by
Grefenstette and Sadrzadeh (2011) contains only
Tarzan-style statements with determiner-less sub-
jects and objects: ?table show result?, ?priest say
mass?, etc. As these examples suggest, however,
as soon as we set our sight on modeling phrases
and sentences, grammatical words are hard to
avoid. Stripping off grammatical words has more
serious consequences than making you sound like
the Lord of the Jungle. Even if we accept the
view of, e.g., Garrette et al (2013), that the log-
ical framework of language should be left to other
devices than distributional semantics, and the lat-
ter should be limited to similarity scoring, still ig-
noring grammatical elements is going to dramat-
ically distort the very similarity scores (c)DSMs
should provide. If we want to use a cDSM for
the classic similarity-based paraphrasing task, the
model shouldn?t conclude that ?The table shows
many results? is identical to ?the table shows no
results? since the two sentences contain the same
content words, or that ?to kill many rats? and ?to
kill few rats? are equally good paraphrases of ?to
exterminate rats?.
We focus here on how cDSMs handle determin-
ers and the phrases they form with nouns (deter-
miner phrases, or DPs).1 While determiners are
only a subset of grammatical words, they are a
large and important subset, constituting the natu-
ral stepping stone towards sentential distributional
semantics: Compositional methods have already
been successfully applied to simple noun-verb and
noun-verb-noun structures (Mitchell and Lapata,
2008; Grefenstette and Sadrzadeh, 2011), and de-
terminers are just what is missing to turn these
skeletal constructions into full-fledged sentences.
Moreover, determiner-noun phrases are, in super-
ficial syntactic terms, similar to the adjective-noun
phrases that have already been extensively studied
from a cDSM perspective by Baroni and Zampar-
1Some linguists refer to what we call DPs as noun phrases
or NPs. We say DPs simply to emphasize our focus on deter-
miners.
53
elli (2010), Guevara (2010) and Mitchell and Lap-
ata (2010). Thus, we can straightforwardly extend
the methods already proposed for adjective-noun
phrases to DPs.
We introduce a new task, a similarity-based
challenge, where we consider nouns that are
strongly conceptually related to certain DPs and
test whether cDSMs can pick the most appropri-
ate related DP (e.g., monarchy is more related to
one ruler than many rulers).2 We make our new
dataset publicly available, and we hope that it will
stimulate further work on the distributional seman-
tics of grammatical elements.3
2 Composition models
Interest in compositional DSMs has skyrocketed
in the last few years, particularly since the influ-
ential work of Mitchell and Lapata (2008; 2009;
2010), who proposed three simple but effective
composition models. In these models, the com-
posed vectors are obtained through component-
wise operations on the constituent vectors. Given
input vectors u and v, the multiplicative model
(mult) returns a composed vector p with: pi =
uivi. In the weighted additive model (wadd), the
composed vector is a weighted sum of the two in-
put vectors: p = ?u+?v, where ? and ? are two
scalars. Finally, in the dilation model, the output
vector is obtained by first decomposing one of the
input vectors, say v, into a vector parallel to u and
an orthogonal vector. Following this, the parallel
vector is dilated by a factor ? before re-combining.
This results in: p = (?? 1)?u,v?u+ ?u,u?v.
A more general form of the additive model
(fulladd) has been proposed by Guevara (2010)
(see also Zanzotto et al (2010)). In this approach,
the two vectors to be added are pre-multiplied by
weight matrices estimated from corpus-extracted
examples: p = Au+Bv.
Baroni and Zamparelli (2010) and Coecke et
al. (2010) take inspiration from formal semantics
to characterize composition in terms of function
application. The former model adjective-noun
phrases by treating the adjective as a function from
nouns onto modified nouns. Given that linear
functions can be expressed by matrices and their
application by matrix-by-vector multiplication, a
2Baroni et al (2012), like us, study determiner phrases
with distributional methods, but they do not model them com-
positionally.
3Dataset and code available from clic.cimec.
unitn.it/composes.
functor (such as the adjective) is represented by a
matrix U to be multiplied with the argument vec-
tor v (e.g., the noun vector): p = Uv. Adjective
matrices are estimated from corpus-extracted ex-
amples of noun vectors and corresponding output
adjective-noun phrase vectors, similarly to Gue-
vara?s approach.4
3 The noun-DP relatedness benchmark
Paraphrasing a single word with a phrase is a
natural task for models of compositionality (Tur-
ney, 2012; Zanzotto et al, 2010) and determin-
ers sometimes play a crucial role in defining the
meaning of a noun. For example a trilogy is com-
posed of three works, an assemblage includes sev-
eral things and an orchestra is made of many
musicians. These examples are particularly in-
teresting, since they point to a ?conceptual? use
of determiners, as components of the stable and
generic meaning of a content word (as opposed to
situation-dependent deictic and anaphoric usages):
for these determiners the boundary between con-
tent and grammatical word is somewhat blurred,
and they thus provide a good entry point for testing
DSM representations of DPs on a classic similarity
task. In other words, we can set up an experiment
in which having an effective representation of the
determiner is crucial in order to obtain the correct
result.
Using regular expressions over WordNet
glosses (Fellbaum, 1998) and complementing
them with definitions from various online dic-
tionaries, we constructed a list of more than 200
nouns that are strongly conceptually related to a
specific DP. We created a multiple-choice test set
by matching each noun with its associated DP
(target DP), two ?foil? DPs sharing the same noun
as the target but combined with other determiners
(same-N foils), one DP made of the target deter-
miner combined with a random noun (same-D
foil), the target determiner (D foil), and the target
noun (N foil). A few examples are shown in Table
1. After the materials were checked by all authors,
two native speakers took the multiple-choice test.
We removed the cases (32) where these subjects
provided an unexpected answer. The final set,
4Other approaches to composition in DSMs have been re-
cently proposed by Socher et al (2012) and Turney (2012).
We leave their empirical evaluation on DPs to further work,
in the first case because it is not trivial to adapt their complex
architecture to our setting; in the other because it is not clear
how Turney would extend his approach to represent DPs.
54
noun target DP same-N foil 1 same-N foil 2 same-D foil D foil N foil
duel two opponents various opponents three opponents two engineers two opponents
homeless no home too few homes one home no incision no home
polygamy several wives most wives fewer wives several negotiators several wives
opulence too many goods some goods no goods too many abductions too many goods
Table 1: Examples from the noun-DP relatedness benchmark
characterized by full subject agreement, contains
173 nouns, each matched with 6 possible answers.
The target DPs contain 23 distinct determiners.
4 Setup
Our semantic space provides distributional repre-
sentations of determiners, nouns and DPs. We
considered a set of 50 determiners that include all
those in our benchmark and range from quanti-
fying determiners (every, some. . . ) and low nu-
merals (one to four), to multi-word units analyzed
as single determiners in the literature, such as a
few, all that, too much. We picked the 20K most
frequent nouns in our source corpus considering
singular and plural forms as separate words, since
number clearly plays an important role in DP se-
mantics. Finally, for each of the target determiners
we added to the space the 2K most frequent DPs
containing that determiner and a target noun.
Co-occurrence statistics were collected from the
concatenation of ukWaC, a mid-2009 dump of the
English Wikipedia and the British National Cor-
pus,5 with a total of 2.8 billion tokens. We use
a bag-of-words approach, counting co-occurrence
with all context words in the same sentence with
a target item. We tuned a number of parameters
on the independent MEN word-relatedness bench-
mark (Bruni et al, 2012). This led us to pick the
top 20K most frequent content word lemmas as
context items, Pointwise Mutual Information as
weighting scheme, and dimensionality reduction
by Non-negative Matrix Factorization.
Except for the parameter-free mult method, pa-
rameters of the composition methods are esti-
mated by minimizing the average Euclidean dis-
tance between the model-generated and corpus-
extracted vectors of the 20K DPs we consider.6
For the lexfunc model, we assume that the deter-
miner is the functor and the noun is the argument,
5wacky.sslmit.unibo.it; www.natcorp.ox.
ac.uk
6All vectors are normalized to unit length before compo-
sition. Note that the objective function used in estimation
minimizes the distance between model-generated and corpus-
extracted vectors. We do not use labeled evaluation data to
optimize the model parameters.
method accuracy method accuracy
lexfunc 39.3 noun 17.3
fulladd 34.7 random 16.7
observed 34.1 mult 12.7
dilation 31.8 determiner 4.6
wadd 23.1
Table 2: Percentage accuracy of composition
methods on the relatedness benchmark
and estimate separate matrices representing each
determiner using the 2K DPs in the semantic space
that contain that determiner. For dilation, we treat
direction of stretching as a parameter, finding that
it is better to stretch the noun.
Similarly to the classic TOEFL synonym detec-
tion challenge (Landauer and Dumais, 1997), our
models tackle the relatedness task by measuring
cosines between each target noun and the candi-
date answers and returning the item with the high-
est cosine.
5 Results
Table 2 reports the accuracy results (mean ranks
of correct answers confirm the same trend). All
models except mult and determiner outperform the
trivial random guessing baseline, although they
are all well below the 100% accuracy of the hu-
mans who took our test. For the mult method we
observe a very strong bias for choosing a single
word as answer (>60% of the times), which in
the test set is always incorrect. This leads to its
accuracy being below the chance level. We sus-
pect that the highly ?intersective? nature of this
model (we obtain very sparse composed DP vec-
tors, only ?4% dense) leads to it not being a re-
liable method for comparing sequences of words
of different length: Shorter sequences will be con-
sidered more similar due to their higher density.
The determiner-only baseline (using the vector of
the component determiner as surrogate for the DP)
fails because D vectors tend to be far from N vec-
tors, thus the N foil is often preferred to the correct
response (that is represented, for this baseline, by
its D). In the noun-only baseline (use the vector
of the component noun as surrogate for the DP),
55
the correct response is identical to the same-N and
N foils, thus forcing a random choice between
these. Not surprisingly, this approach performs
quite badly. The observed DP vectors extracted di-
rectly from the corpus compete with the top com-
positional methods, but do not surpass them.7
The lexfunc method is the best compositional
model, indicating that its added flexibility in mod-
eling composition pays off empirically. The ful-
ladd model is not as good, but also performs well.
The wadd and especially dilation models perform
relatively well, but they are penalized by the fact
that they assign more weight to the noun vectors,
making the right answer dangerously similar to the
same-N and N foils.
Taking a closer look at the performance of the
best model (lexfunc), we observe that it is not
equally distributed across determiners. Focusing
on those determiners appearing in at least 4 cor-
rect answers, they range from those where lexfunc
performance was very significantly above chance
(p<0.001 of equal or higher chance performance):
too few, all, four, too much, less, several; to
those on which performance was still significant
but less impressively so (0.001<p< 0.05): sev-
eral, no, various, most, two, too many, many, one;
to those where performance was not significantly
better than chance at the 0.05 level: much, more,
three, another. Given that, on the one hand, per-
formance is not constant across determiners, and
on the other no obvious groupings can account
for their performance difference (compare the ex-
cellent lexfunc performance on four to the lousy
one on three!), future research should explore the
contextual properties of specific determiners that
make them more or less amenable to be captured
by compositional DSMs.
6 Conclusion
DSMs, even when applied to phrases, are typically
seen as models of content word meaning. How-
ever, to scale up compositionally beyond the sim-
plest constructions, cDSMs must deal with gram-
matical terms such as determiners. This paper
started exploring this issue by introducing a new
and publicly available set testing DP semantics in
a similarity-based task and using it to systemati-
cally evaluate, for the first time, cDSMs on a con-
7The observed method is in fact at advantage in our ex-
periment because a considerable number of DP foils are not
found in the corpus and are assigned similarity 0 with the tar-
get.
struction involving grammatical words. The most
important take-home message is that distributional
representations are rich enough to encode infor-
mation about determiners, achieving performance
well above chance on the new benchmark.
Theoretical considerations would lead one to
expect a ?functional? approach to determiner rep-
resentations along the lines of Baroni and Zampar-
elli (2010) and Coecke et al (2010) to outperform
those approaches that combine vectors separately
representing determiners and nouns. This predic-
tion was largely borne out in the results, although
the additive models, and particularly fulladd, were
competitive rivals.
We attempted to capture the distributional se-
mantics of DPs using a fairly standard, ?vanilla?
semantic space characterized by latent dimensions
that summarize patterns of co-occurrence with
content word contexts. By inspecting the con-
text words that are most associated with the var-
ious latent dimensions we obtained through Non-
negative Matrix Factorization, we notice how they
are capturing broad, ?topical? aspects of meaning
(the first dimension is represented by scripture, be-
liever, resurrection, the fourth by fever, infection,
infected, and so on). Considering the sort of se-
mantic space we used (which we took to be a rea-
sonable starting point because of its effectiveness
in a standard lexical task), it is actually surpris-
ing that we obtained the significant results we ob-
tained. Thus, a top priority in future work is to ex-
plore different contextual features, such as adverbs
and grammatical terms, that might carry informa-
tion that is more directly relevant to the semantics
of determiners.
Another important line of research pertains to
improving composition methods: Although the
best model, at 40% accuracy, is well above chance,
we are still far from the 100% performance of hu-
mans. We will try, in particular, to include non-
linear transformations in the spirit of Socher et al
(2012), and look for better ways to automatically
select training data.
Last but not least, in the near future we
would like to test if cDSMs, besides dealing with
similarity-based aspects of determiner meaning,
can also help in capturing those formal properties
of determiners, such as monotonicity or definite-
ness, that theoretical semanticists have been tradi-
tionally interested in.
56
7 Acknowledgments
This research was supported by the ERC 2011
Starting Independent Research Grant n. 283554
(COMPOSES).
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183?1193, Boston,
MA.
Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do,
and Chung-Chieh Shan. 2012. Entailment above
the word level in distributional semantics. In Pro-
ceedings of EACL, pages 23?32, Avignon, France.
Elia Bruni, Gemma Boleda, Marco Baroni, and
Nam Khanh Tran. 2012. Distributional semantics
in Technicolor. In Proceedings of ACL, pages 136?
145, Jeju Island, Korea.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical foundations for a com-
positional distributional model of meaning. Linguis-
tic Analysis, 36:345?384.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
MA.
Dan Garrette, Katrin Erk, and Ray Mooney. 2013. A
formal approach to linking logical form and vector-
space lexical semantics. In H. Bunt, J. Bos, and
S. Pulman, editors, Computing Meaning, Vol. 4. In
press.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of EMNLP, pages 1394?1404, Edinburgh, UK.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of GEMS, pages 33?37,
Uppsala, Sweden.
Thomas Landauer and Susan Dumais. 1997. A solu-
tion to Plato?s problem: The latent semantic analysis
theory of acquisition, induction, and representation
of knowledge. Psychological Review, 104(2):211?
240.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL, pages 236?244, Columbus, OH.
Jeff Mitchell and Mirella Lapata. 2009. Language
models based on semantic composition. In Proceed-
ings of EMNLP, pages 430?439, Singapore.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Richard Socher, Brody Huval, Christopher Manning,
and Andrew Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of EMNLP, pages 1201?1211, Jeju Island, Ko-
rea.
Peter Turney. 2012. Domain and function: A dual-
space model of semantic relations and compositions.
Journal of Artificial Intelligence Research, 44:533?
585.
Fabio Zanzotto, Ioannis Korkontzelos, Francesca
Falucchi, and Suresh Manandhar. 2010. Estimat-
ing linear models for compositional distributional
semantics. In Proceedings of COLING, pages 1263?
1271, Beijing, China.
57
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 1?8,
Dublin, Ireland, August 23-24, 2014.
SemEval-2014 Task 1: Evaluation of Compositional Distributional
Semantic Models on Full Sentences through Semantic Relatedness and
Textual Entailment
Marco Marelli
(1)
Luisa Bentivogli
(2)
Marco Baroni
(1)
Raffaella Bernardi
(1)
Stefano Menini
(1,2)
Roberto Zamparelli
(1)
(1)
University of Trento, Italy
(2)
FBK - Fondazione Bruno Kessler, Trento, Italy
{name.surname}@unitn.it, {bentivo,menini}@fbk.eu
Abstract
This paper presents the task on the evalu-
ation of Compositional Distributional Se-
mantics Models on full sentences orga-
nized for the first time within SemEval-
2014. Participation was open to systems
based on any approach. Systems were pre-
sented with pairs of sentences and were
evaluated on their ability to predict hu-
man judgments on (i) semantic relatedness
and (ii) entailment. The task attracted 21
teams, most of which participated in both
subtasks. We received 17 submissions in
the relatedness subtask (for a total of 66
runs) and 18 in the entailment subtask (65
runs).
1 Introduction
Distributional Semantic Models (DSMs) approx-
imate the meaning of words with vectors sum-
marizing their patterns of co-occurrence in cor-
pora. Recently, several compositional extensions
of DSMs (CDSMs) have been proposed, with the
purpose of representing the meaning of phrases
and sentences by composing the distributional rep-
resentations of the words they contain (Baroni and
Zamparelli, 2010; Grefenstette and Sadrzadeh,
2011; Mitchell and Lapata, 2010; Socher et al.,
2012). Despite the ever increasing interest in the
field, the development of adequate benchmarks for
CDSMs, especially at the sentence level, is still
lagging. Existing data sets, such as those intro-
duced by Mitchell and Lapata (2008) and Grefen-
stette and Sadrzadeh (2011), are limited to a few
hundred instances of very short sentences with a
fixed structure. In the last ten years, several large
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
data sets have been developed for various com-
putational semantics tasks, such as Semantic Text
Similarity (STS)(Agirre et al., 2012) or Recogniz-
ing Textual Entailment (RTE) (Dagan et al., 2006).
Working with such data sets, however, requires
dealing with issues, such as identifying multiword
expressions, recognizing named entities or access-
ing encyclopedic knowledge, which have little to
do with compositionality per se. CDSMs should
instead be evaluated on data that are challenging
for reasons due to semantic compositionality (e.g.
context-cued synonymy resolution and other lexi-
cal variation phenomena, active/passive and other
syntactic alternations, impact of negation at vari-
ous levels, operator scope, and other effects linked
to the functional lexicon). These issues do not oc-
cur frequently in, e.g., the STS and RTE data sets.
With these considerations in mind, we devel-
oped SICK (Sentences Involving Compositional
Knowledge), a data set aimed at filling the void,
including a large number of sentence pairs that
are rich in the lexical, syntactic and semantic phe-
nomena that CDSMs are expected to account for,
but do not require dealing with other aspects of
existing sentential data sets that are not within
the scope of compositional distributional seman-
tics. Moreover, we distinguished between generic
semantic knowledge about general concept cate-
gories (such as knowledge that a couple is formed
by a bride and a groom) and encyclopedic knowl-
edge about specific instances of concepts (e.g.,
knowing the fact that the current president of the
US is Barack Obama). The SICK data set contains
many examples of the former, but none of the lat-
ter.
2 The Task
The Task involved two subtasks. (i) Relatedness:
predicting the degree of semantic similarity be-
tween two sentences, and (ii) Entailment: detect-
ing the entailment relation holding between them
1
(see below for the exact definition). Sentence re-
latedness scores provide a direct way to evalu-
ate CDSMs, insofar as their outputs are able to
quantify the degree of semantic similarity between
sentences. On the other hand, starting from the
assumption that understanding a sentence means
knowing when it is true, being able to verify
whether an entailment is valid is a crucial chal-
lenge for semantic systems.
In the semantic relatedness subtask, given two
sentences, systems were required to produce a re-
latedness score (on a continuous scale) indicating
the extent to which the sentences were expressing
a related meaning. Table 1 shows examples of sen-
tence pairs with different degrees of semantic re-
latedness; gold relatedness scores are expressed on
a 5-point rating scale.
In the entailment subtask, given two sentences
A and B, systems had to determine whether the
meaning of B was entailed by A. In particular, sys-
tems were required to assign to each pair either
the ENTAILMENT label (when A entails B, viz.,
B cannot be false when A is true), the CONTRA-
DICTION label (when A contradicted B, viz. B is
false whenever A is true), or the NEUTRAL label
(when the truth of B could not be determined on
the basis of A). Table 2 shows examples of sen-
tence pairs holding different entailment relations.
Participants were invited to submit up to five
system runs for one or both subtasks. Developers
of CDSMs were especially encouraged to partic-
ipate, but developers of other systems that could
tackle sentence relatedness or entailment tasks
were also welcome. Besides being of intrinsic in-
terest, the latter systems? performance will serve
to situate CDSM performance within the broader
landscape of computational semantics.
3 The SICK Data Set
The SICK data set, consisting of about 10,000 En-
glish sentence pairs annotated for relatedness in
meaning and entailment, was used to evaluate the
systems participating in the task. The data set
creation methodology is outlined in the following
subsections, while all the details about data gen-
eration and annotation, quality control, and inter-
annotator agreement can be found in Marelli et al.
(2014).
3.1 Data Set Creation
SICK was built starting from two existing data
sets: the 8K ImageFlickr data set
1
and the
SemEval-2012 STS MSR-Video Descriptions data
set.
2
The 8K ImageFlickr dataset is a dataset of
images, where each image is associated with five
descriptions. To derive SICK sentence pairs we
randomly chose 750 images and we sampled two
descriptions from each of them. The SemEval-
2012 STS MSR-Video Descriptions data set is a
collection of sentence pairs sampled from the short
video snippets which compose the Microsoft Re-
search Video Description Corpus. A subset of 750
sentence pairs were randomly chosen from this
data set to be used in SICK.
In order to generate SICK data from the 1,500
sentence pairs taken from the source data sets, a 3-
step process was applied to each sentence compos-
ing the pair, namely (i) normalization, (ii) expan-
sion and (iii) pairing. Table 3 presents an example
of the output of each step in the process.
The normalization step was carried out on the
original sentences (S0) to exclude or simplify in-
stances that contained lexical, syntactic or seman-
tic phenomena (e.g., named entities, dates, num-
bers, multiword expressions) that CDSMs are cur-
rently not expected to account for.
The expansion step was applied to each of the
normalized sentences (S1) in order to create up to
three new sentences with specific characteristics
suitable to CDSM evaluation. In this step syntac-
tic and lexical transformations with predictable ef-
fects were applied to each normalized sentence, in
order to obtain (i) a sentence with a similar mean-
ing (S2), (ii) a sentence with a logically contradic-
tory or at least highly contrasting meaning (S3),
and (iii) a sentence that contains most of the same
lexical items, but has a different meaning (S4) (this
last step was carried out only where it could yield
a meaningful sentence; as a result, not all normal-
ized sentences have an (S4) expansion).
Finally, in the pairing step each normalized
sentence in the pair was combined with all the
sentences resulting from the expansion phase and
with the other normalized sentence in the pair.
Considering the example in Table 3, S1a and S1b
were paired. Then, S1a and S1b were each com-
bined with S2a, S2b,S3a, S3b, S4a, and S4b, lead-
1
http://nlp.cs.illinois.edu/HockenmaierGroup/data.html
2
http://www.cs.york.ac.uk/semeval-
2012/task6/index.php?id=data
2
Relatedness score Example
1.6
A: ?A man is jumping into an empty pool?
B: ?There is no biker jumping in the air?
2.9
A: ?Two children are lying in the snow and are making snow angels?
B: ?Two angels are making snow on the lying children?
3.6
A: ?The young boys are playing outdoors and the man is smiling nearby?
B: ?There is no boy playing outdoors and there is no man smiling?
4.9
A: ?A person in a black jacket is doing tricks on a motorbike?
B: ?A man in a black jacket is doing tricks on a motorbike?
Table 1: Examples of sentence pairs with their gold relatedness scores (on a 5-point rating scale).
Entailment label Example
ENTAILMENT
A: ?Two teams are competing in a football match?
B: ?Two groups of people are playing football?
CONTRADICTION
A: ?The brown horse is near a red barrel at the rodeo?
B: ?The brown horse is far from a red barrel at the rodeo?
NEUTRAL
A: ?A man in a black jacket is doing tricks on a motorbike?
B: ?A person is riding the bicycle on one wheel?
Table 2: Examples of sentence pairs with their gold entailment labels.
ing to a total of 13 different sentence pairs.
Furthermore, a number of pairs composed of
completely unrelated sentences were added to the
data set by randomly taking two sentences from
two different pairs.
The result is a set of about 10,000 new sen-
tence pairs, in which each sentence is contrasted
with either a (near) paraphrase, a contradictory or
strongly contrasting statement, another sentence
with very high lexical overlap but different mean-
ing, or a completely unrelated sentence. The ra-
tionale behind this approach was that of building
a data set which encouraged the use of a com-
positional semantics step in understanding when
two sentences have close meanings or entail each
other, hindering methods based on individual lex-
ical items, on the syntactic complexity of the two
sentences or on pure world knowledge.
3.2 Relatedness and Entailment Annotation
Each pair in the SICK dataset was annotated to
mark (i) the degree to which the two sentence
meanings are related (on a 5-point scale), and (ii)
whether one entails or contradicts the other (con-
sidering both directions). The ratings were col-
lected through a large crowdsourcing study, where
each pair was evaluated by 10 different subjects,
and the order of presentation of the sentences was
counterbalanced (i.e., 5 judgments were collected
for each presentation order). Swapping the order
of the sentences within each pair served a two-
fold purpose: (i) evaluating the entailment rela-
tion in both directions and (ii) controlling pos-
sible bias due to priming effects in the related-
ness task. Once all the annotations were collected,
the relatedness gold score was computed for each
pair as the average of the ten ratings assigned by
participants, whereas a majority vote scheme was
adopted for the entailment gold labels.
3.3 Data Set Statistics
For the purpose of the task, the data set was ran-
domly split into training and test set (50% and
50%), ensuring that each relatedness range and en-
tailment category was equally represented in both
sets. Table 4 shows the distribution of sentence
pairs considering the combination of relatedness
ranges and entailment labels. The ?total? column
3
Original pair
S0a: A sea turtle is hunting for fish S0b: The turtle followed the fish
Normalized pair
S1a: A sea turtle is hunting for fish S1b: The turtle is following the fish
Expanded pairs
S2a: A sea turtle is hunting for food S2b: The turtle is following the red fish
S3a: A sea turtle is not hunting for fish S3b: The turtle isn?t following the fish
S4a: A fish is hunting for a turtle in the sea S4b: The fish is following the turtle
Table 3: Data set creation process.
indicates the total number of pairs in each range
of relatedness, while the ?total? row contains the
total number of pairs in each entailment class.
SICK Training Set
relatedness CONTRADICT ENTAIL NEUTRAL TOTAL
1-2 range 0 (0%) 0 (0%) 471 (10%) 471
2-3 range 59 (1%) 2 (0%) 638 (13%) 699
3-4 range 498 (10%) 71 (1%) 1344 (27%) 1913
4-5 range 155 (3%) 1344 (27%) 352 (7%) 1851
TOTAL 712 1417 2805 4934
SICK Test Set
relatedness CONTRADICT ENTAIL NEUTRAL TOTAL
1-2 range 0 (0%) 1 (0%) 451 (9%) 452
2-3 range 59 (1%) 0 (0%) 615(13%) 674
3-4 range 496 (10%) 65 (1%) 1398 (28%) 1959
4-5 range 157 (3%) 1338 (27%) 326 (7%) 1821
TOTAL 712 1404 2790 4906
Table 4: Distribution of sentence pairs across the
Training and Test Sets.
4 Evaluation Metrics and Baselines
Both subtasks were evaluated using standard met-
rics. In particular, the results on entailment were
evaluated using accuracy, whereas the outputs on
relatedness were evaluated using Pearson correla-
tion, Spearman correlation, and Mean Squared Er-
ror (MSE). Pearson correlation was chosen as the
official measure to rank the participating systems.
Table 5 presents the performance of 4 base-
lines. The Majority baseline always assigns
the most common label in the training data
(NEUTRAL), whereas the Probability baseline
assigns labels randomly according to their rela-
tive frequency in the training set. The Overlap
baseline measures word overlap, again with
parameters (number of stop words and EN-
TAILMENT/NEUTRAL/CONTRADICTION
thresholds) estimated on the training part of the
data.
Baseline Relatedness Entailment
Chance 0 33.3%
Majority NA 56.7%
Probability NA 41.8%
Overlap 0.63 56.2%
Table 5: Performance of baselines. Figure of merit
is Pearson correlation for relatedness and accuracy
for entailment. NA = Not Applicable
5 Submitted Runs and Results
Overall, 21 teams participated in the task. Partici-
pants were allowed to submit up to 5 runs for each
subtask and had to choose the primary run to be in-
cluded in the comparative evaluation. We received
17 submissions to the relatedness subtask (for a
total of 66 runs) and 18 for the entailment subtask
(65 runs).
We asked participants to pre-specify a pri-
mary run to encourage commitment to a
theoretically-motivated approach, rather than
post-hoc performance-based assessment. Inter-
estingly, some participants used the non-primary
runs to explore the performance one could reach
by exploiting weaknesses in the data that are not
likely to hold in future tasks of the same kind
(for instance, run 3 submitted by The Meaning
Factory exploited sentence ID ordering informa-
tion, but it was not presented as a primary run).
Participants could also use non-primary runs to
test smart baselines. In the relatedness subtask
six non-primary runs slightly outperformed the
official winning primary entry,
3
while in the
entailment task all ECNU?s runs but run 4 were
better than ECNU?s primary run. Interestingly,
the differences between the ECNU?s runs were
3
They were: The Meaning Factory?s run3 (Pearson
0.84170) ECNU?s runs2 (0.83893) run5 (0.83500) and Stan-
fordNLP?s run4 (0.83462) and run2 (0.83103).
4
due to the learning methods used.
We present the results achieved by primary runs
against the Entailment and Relatedness subtasks in
Table 6 and Table 7, respectively.
4
We witnessed
a very close finish in both subtasks, with 4 more
systems within 3 percentage points of the winner
in both cases. 4 of these 5 top systems were the
same across the two subtasks. Most systems per-
formed well above the best baselines from Table
5.
The overall performance pattern suggests that,
owing perhaps to the more controlled nature of
the sentences, as well as to the purely linguistic
nature of the challenges it presents, SICK entail-
ment is ?easier? than RTE. Considering the first
five RTE challenges (Bentivogli et al., 2009), the
median values ranged from 56.20% to 61.75%,
whereas the average values ranged from 56.45%
to 61.97%. The entailment scores obtained on
the SICK data set are considerably higher, being
77.06% for the median system and 75.36% for
the average system. On the other hand, the re-
latedness task is more challenging than the one
run on MSRvid (one of our data sources) at STS
2012, where the top Pearson correlation was 0.88
(Agirre et al., 2012).
6 Approaches
A summary of the approaches used by the sys-
tems to address the task is presented in Table 8.
In the table, systems in bold are those for which
the authors submitted a paper (Ferrone and Zan-
zotto, 2014; Bjerva et al., 2014; Beltagy et al.,
2014; Lai and Hockenmaier, 2014; Alves et al.,
2014; Le?on et al., 2014; Bestgen, 2014; Zhao et
al., 2014; Vo et al., 2014; Bic?ici and Way, 2014;
Lien and Kouylekov, 2014; Jimenez et al., 2014;
Proisl and Evert, 2014; Gupta et al., 2014). For the
others, we used the brief description sent with the
system?s results, double-checking the information
with the authors. In the table, ?E? and ?R? refer
to the entailment and relatedness task respectively,
and ?B? to both.
Almost all systems combine several kinds of
features. To highlight the role played by com-
position, we draw a distinction between compo-
sitional and non-compositional features, and di-
vide the former into ?fully compositional? (sys-
4
ITTK?s primary run could not be evaluated due to tech-
nical problems with the submission. The best ITTK?s non-
primary run scored 78,2% accuracy in the entailment task and
0.76 r in the relatedness task.
ID Compose ACCURACY
Illinois-LH run1 P/S 84.6
ECNU run1 S 83.6
UNAL-NLP run1 83.1
SemantiKLUE run1 82.3
The Meaning Factory run1 S 81.6
CECL ALL run1 80.0
BUAP run1 P 79.7
UoW run1 78.5
Uedinburgh run1 S 77.1
UIO-Lien run1 77.0
FBK-TR run3 P 75.4
StanfordNLP run5 S 74.5
UTexas run1 P/S 73.2
Yamraj run1 70.7
asjai run5 S 69.8
haLF run2 S 69.4
RTM-DCU run1 67.2
UANLPCourse run2 S 48.7
Table 6: Primary run results for the entailment
subtask. The table also shows whether a sys-
tem exploits composition information at either the
phrase (P) or sentence (S) level.
tems that compositionally computed the meaning
of the full sentences, though not necessarily by as-
signing meanings to intermediate syntactic con-
stituents) and ?partially compositional? (systems
that stop the composition at the level of phrases).
As the table shows, thirteen systems used compo-
sition in at least one of the tasks; ten used compo-
sition for full sentences and six for phrases, only.
The best systems are among these thirteen sys-
tems.
Let us focus on such compositional methods.
Concerning the relatedness task, the fine-grained
analyses reported for several systems (Illinois-
LH, The Meaning Factory and ECNU) shows that
purely compositional systems currently reach per-
formance above 0.7 r. In particular, ECNU?s
compositional feature gives 0.75 r, The Meaning
Factory?s logic-based composition model 0.73 r,
and Illinois-LH compositional features combined
with Word Overlap 0.75 r. While competitive,
these scores are lower than the one of the best
5
ID Compose r ? MSE
ECNU run1 S 0.828 0.769 0.325
StanfordNLP run5 S 0.827 0.756 0.323
The Meaning Factory run1 S 0.827 0.772 0.322
UNAL-NLP run1 0.804 0.746 0.359
Illinois-LH run1 P/S 0.799 0.754 0.369
CECL ALL run1 0.780 0.732 0.398
SemantiKLUE run1 0.780 0.736 0.403
RTM-DCU run1 0.764 0.688 0.429
UTexas run1 P/S 0.714 0.674 0.499
UoW run1 0.711 0.679 0.511
FBK-TR run3 P 0.709 0.644 0.591
BUAP run1 P 0.697 0.645 0.528
UANLPCourse run2 S 0.693 0.603 0.542
UQeResearch run1 0.642 0.626 0.822
ASAP run1 P 0.628 0.597 0.662
Yamraj run1 0.535 0.536 2.665
asjai run5 S 0.479 0.461 1.104
Table 7: Primary run results for the relatedness
subtask (r for Pearson and ? for Spearman corre-
lation). The table also shows whether a system ex-
ploits composition information at either the phrase
(P) or sentence (S) level.
purely non-compositional system (UNAL-NLP)
which reaches the 4th position (0.80 r UNAL-NLP
vs. 0.82 r obtained by the best system). UNAL-
NLP however exploits an ad-hoc ?negation? fea-
ture discussed below.
In the entailment task, the best non-
compositional model (again UNAL-NLP)
reaches the 3rd position, within close reach of the
best system (83% UNAL-NLP vs. 84.5% obtained
by the best system). Again, purely compositional
models have lower performance. haLF CDSM
reaches 69.42% accuracy, Illinois-LH Word
Overlap combined with a compositional feature
reaches 71.8%. The fine-grained analysis reported
by Illinois-LH (Lai and Hockenmaier, 2014)
shows that a full compositional system (based
on point-wise multiplication) fails to capture
contradiction. It is better than partial phrase-based
compositional models in recognizing entailment
pairs, but worse than them on recognizing neutral
pairs.
Given our more general interest in the distri-
butional approaches, in Table 8 we also classify
the different DSMs used as ?Vector Space Mod-
els?, ?Topic Models? and ?Neural Language Mod-
els?. Due to the impact shown by learning methods
(see ECNU?s results), we also report the different
learning approaches used.
Several participating systems deliberately ex-
ploit ad-hoc features that, while not helping a true
understanding of sentence meaning, exploit some
systematic characteristics of SICK that should be
controlled for in future releases of the data set.
In particular, the Textual Entailment subtask has
been shown to rely too much on negative words
and antonyms. The Illinois-LH team reports that,
just by checking the presence of negative words
(the Negation Feature in the table), one can detect
86.4% of the contradiction pairs, and by combin-
ing Word Overlap and antonyms one can detect
83.6% of neutral pairs and 82.6% of entailment
pairs. This approach, however, is obviously very
brittle (it would not have been successful, for in-
stance, if negation had been optionally combined
with word-rearranging in the creation of S4 sen-
tences, see Section 3.1 above).
Finally, Table 8 reports about the use of external
resources in the task. One of the reasons we cre-
ated SICK was to have a compositional semantics
benchmark that would not require too many ex-
ternal tools and resources (e.g., named-entity rec-
ognizers, gazetteers, ontologies). By looking at
what the participants chose to use, we think we
succeeded, as only standard NLP pre-processing
tools (tokenizers, PoS taggers and parsers) and rel-
atively few knowledge resources (mostly, Word-
Net and paraphrase corpora) were used.
7 Conclusion
We presented the results of the first task on the
evaluation of compositional distributional seman-
tic models and other semantic systems on full sen-
tences, organized within SemEval-2014. Two sub-
tasks were offered: (i) predicting the degree of re-
latedness between two sentences, and (ii) detect-
ing the entailment relation holding between them.
The task has raised noticeable attention in the
community: 17 and 18 submissions for the relat-
edness and entailment subtasks, respectively, for a
total of 21 participating teams. Participation was
not limited to compositional models but the major-
ity of systems (13/21) used composition in at least
one of the subtasks. Moreover, the top-ranking
systems in both tasks use compositional features.
However, it must be noted that all systems also ex-
6
Participant ID Non composition features Comp features Learning Methods External Resources 
Ve
cto
r S
em
an
tic
s M
od
el 
To
pic
 M
od
el 
Ne
ura
l L
an
gu
ag
e M
od
el 
De
no
tat
ion
al 
Mo
de
l 
Wo
rd 
Ov
erl
ap
 
Wo
rd 
Sim
ila
rit
y 
Sy
nta
cti
c F
ea
tur
es
 
Se
nte
nc
e d
iffe
ren
ce
 
Ne
ga
tio
n F
ea
tur
es
 
Se
nte
nc
e C
om
po
sit
ion
 
Ph
ras
e c
om
po
sit
ion
  
SV
M 
an
d K
ern
el 
me
tho
ds
 
K-
Ne
are
st 
Ne
igh
bo
urs
 
Cla
ssi
fie
r C
om
bin
ati
on
 
Ra
nd
om
 Fo
res
t 
Fo
L/P
rob
ab
ilis
tic
 Fo
L 
Cu
rri
cu
lum
 ba
se
d l
ea
rni
ng
 
Ot
he
r 
Wo
rdN
et 
Pa
rap
hra
se
s D
B 
Ot
he
r C
orp
ora
 
Im
ag
eF
lick
er 
 ST
S M
SR
-V
ide
o 
De
scr
ipt
ion
 
ASAP R R R R R R R R R 
ASJAI B B B B B B B B E B R B 
BUAP B B B B E B E B 
UEdinburgh B B B B B E R B 
CECL B B B B B B 
ECNU B B B B B B B B B B B B B 
FBK-TR R R R E B E E B R E R R E 
haLF E E E E 
IITK B B B B B B B B B 
Illinois-LH B B B B B B B B B B B B 
RTM-DCU B B B B B 
SemantiKLUE B B B B B B B B 
StandfordNLP B B R R R B E 
The Meaning Factory R R R R R R B E R E B B R 
UANLPCourse B B B B B 
UIO-Lien E E 
UNAL-NLP B B B B R B B 
UoW B B B B B B 
UQeRsearch R R R R R R R 
UTexas B B B B B B B 
Yamarj B B B B 
Table 8: Summary of the main characteristics of the participating systems on R(elatedness), E(ntailment)
or B(oth)
ploit non-compositional features and most of them
use external resources, especially WordNet. Al-
most all the participating systems outperformed
the proposed baselines in both tasks. Further anal-
yses carried out by some participants in the task
show that purely compositional approaches reach
accuracy above 70% in entailment and 0.70 r for
relatedness. These scores are comparable with the
average results obtained in the task.
Acknowledgments
We thank the creators of the ImageFlickr, MSR-
Video, and SemEval-2012 STS data sets for grant-
ing us permission to use their data for the task. The
University of Trento authors were supported by
ERC 2011 Starting Independent Research Grant
n. 283554 (COMPOSES).
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pi-
lot on semantic textual similarity. In Proceedings of
the Sixth International Workshop on Semantic Eval-
uation (SemEval 2012), volume 2.
Ana O. Alves, Adirana Ferrugento, Mariana Lorenc?o,
and Filipe Rodrigues. 2014. ASAP: Automatica se-
mantic alignment for phrases. In Proceedings of Se-
mEval 2014: International Workshop on Semantic
Evaluation.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183?1193, Boston,
MA.
Islam Beltagy, Stephen Roller, Gemma Boleda, Katrin
Erk, and Raymon J. Mooney. 2014. UTexas: Nat-
ural language semantics using distributional seman-
tics and probablisitc logic. In Proceedings of Se-
mEval 2014: International Workshop on Semantic
Evaluation.
7
Luisa Bentivogli, Ido Dagan, Hoa T. Dang, Danilo Gi-
ampiccolo, and Bernardo Magnini. 2009. The fifth
PASCAL recognizing textual entailment challenge.
In The Text Analysis Conference (TAC 2009).
Yves Bestgen. 2014. CECL: a new baseline and a non-
compositional approach for the Sick benchmark. In
Proceedings of SemEval 2014: International Work-
shop on Semantic Evaluation.
Ergun Bic?ici and Andy Way. 2014. RTM-DCU: Ref-
erential translation machines for semantic similar-
ity. In Proceedings of SemEval 2014: International
Workshop on Semantic Evaluation.
Johannes Bjerva, Johan Bos, Rob van der Goot, and
Malvina Nissim. 2014. The Meaning Factory: For-
mal Semantics for Recognizing Textual Entailment
and Determining Semantic Similarity. In Proceed-
ings of SemEval 2014: International Workshop on
Semantic Evaluation.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL recognising textual entailment
challenge. In Machine learning challenges. Evalu-
ating predictive uncertainty, visual object classifica-
tion, and recognising textual entailment, pages 177?
190. Springer.
Lorenzo Ferrone and Fabio Massimo Zanzotto. 2014.
haLF:comparing a pure CDSM approach and a stan-
dard ML system for RTE. In Proceedings of Se-
mEval 2014: International Workshop on Semantic
Evaluation.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of EMNLP, pages 1394?1404, Edinburgh, UK.
Rohit Gupta, Ismail El Maarouf Hannah Bechara, and
Costantin Oras?an. 2014. UoW: NLP techniques de-
veloped at the University of Wolverhampton for Se-
mantic Similarity and Textual Entailment. In Pro-
ceedings of SemEval 2014: International Workshop
on Semantic Evaluation.
Sergio Jimenez, George Duenas, Julia Baquero, and
Alexander Gelbukh. 2014. UNAL-NLP: Combin-
ing soft cardinality features for semantic textual sim-
ilarity, relatedness and entailment. In Proceedings
of SemEval 2014: International Workshop on Se-
mantic Evaluation.
Alice Lai and Julia Hockenmaier. 2014. Illinois-lh: A
denotational and distributional approach to seman-
tics. In Proceedings of SemEval 2014: International
Workshop on Semantic Evaluation.
Sa?ul Le?on, Darnes Vilarino, David Pinto, Mireya To-
var, and Beatrice Beltr?an. 2014. BUAP:evaluating
compositional distributional semantic models on full
sentences through semantic relatedness and textual
entailment. In Proceedings of SemEval 2014: Inter-
national Workshop on Semantic Evaluation.
Elisabeth Lien and Milen Kouylekov. 2014. UIO-
Lien: Entailment recognition using minimal recur-
sion semantics. In Proceedings of SemEval 2014:
International Workshop on Semantic Evaluation.
Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, and Roberto Zam-
parelli. 2014. A SICK cure for the evaluation of
compositional distributional semantic models. In
Proceedings of LREC, Reykjavik.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL, pages 236?244, Columbus, OH.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Thomas Proisl and Stefan Evert. 2014. SemantiK-
LUE: Robust semantic similarity at multiple levels
using maximum weight matching. In Proceedings of
SemEval 2014: International Workshop on Semantic
Evaluation.
Richard Socher, Brody Huval, Christopher Manning,
and Andrew Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of EMNLP, pages 1201?1211, Jeju Island, Ko-
rea.
An N. P. Vo, Octavian Popescu, and Tommaso Caselli.
2014. FBK-TR: SVM for Semantic Relatedness and
Corpus Patterns for RTE. In Proceedings of Se-
mEval 2014: International Workshop on Semantic
Evaluation.
Jiang Zhao, Tian Tian Zhu, and Man Lan. 2014.
ECNU: One Stone Two Birds: Ensemble of Het-
erogenous Measures for Semantic Relatedness and
Textual Entailment. In Proceedings of SemEval
2014: International Workshop on Semantic Evalu-
ation.
8
