Re-Engineering Letter-to-Sound Rules
Martin Jansche
The Ohio State University
Columbus, OH 43210, U.S.A.
jansche.1@osu.edu
Abstract
Using finite-state automata for the text analysis
component in a text-to-speech system is problem-
atic in several respects: the rewrite rules from which
the automata are compiled are difficult to write and
maintain, and the resulting automata can become
very large and therefore inefficient. Converting the
knowledge represented explicitly in rewrite rules
into a more efficient format is difficult. We take an
indirect route, learning an efficient decision tree rep-
resentation from data and tapping information con-
tained in existing rewrite rules, which increases per-
formance compared to learning exclusively from a
pronunciation lexicon.
1 Introduction
Text-to-speech (TTS) systems, like any other piece
of sophisticated software, suffer from the shortcom-
ings of the traditional software development pro-
cess. Highly skilled developers are a costly re-
source, the complexity and sheer size of the code
involved are difficult to manage. A paradigmatic
example of this is the letter-to-sound component
within the text analysis module of a mature large-
scale text-to-speech system. In the system described
in (Sproat, 1998) text analysis is performed using
finite-state transducers compiled from rewrite rules
(Kaplan and Kay, 1994; Mohri and Sproat, 1996)
and other high-level descriptions. While the exclu-
sive use of finite-state technology has advantages, it
is not without its shortcomings, both technical and
stemming from the use of hand-crafted rule sets and
how they are represented:
1. Extensive rule sets need to be constructed by
human experts, which is labor-intensive and
expensive (Sproat et al, 1998).
2. Realistic rule sets are difficult to maintain be-
cause of complex interactions between serially
composed rules.
3. Although rewrite rules can, in principle, be
compiled into a huge monolithic transducer
that is then very time-efficient, in practice this
is not feasible because of the enormous sizes of
the resulting machines (cf. the numbers given
in (Mohri and Sproat, 1996) and (Sproat et al,
1998, 74)).
4. For reasons of space efficiency, certain com-
putations are deferred until run-time (Mohri et
al., 1996; Mohri et al, 2000), with a significant
impact on time efficiency.
While there is a clear need for human expert
knowledge (Sproat et al, 1998, 75ff.), those experts
should not have to deal with the performance as-
pects of the knowledge representation. Ideally we
would like to use a knowledge representation that
is both time and space efficient and can be con-
structed automatically from individually meaning-
ful features supplied by human experts. For practi-
cal reasons we have to be content with methods that
address the efficiency issues and can make use of
explicitly represented knowledge from legacy sys-
tems, so that moving to a new way of building TTS
systems does not entail starting over from scratch.
As a case study of how this transition might be
achieved we took the letter-to-phoneme rules for
French in the TTS system described in (Sproat,
1998) and proceeded to
1. Construct a lexicon using the existing system.
2. Produce an alignment for that lexicon.
3. Convert the aligned lexicon into training in-
stances for an automatically induced classifier.
4. Train and evaluate decision trees.
By running the existing system on a small news-
paper corpus (ca. 1M words of newspaper text from
Le Monde) and eliminating abbreviations we ob-
tained a lexicon of about 18k words. This means
that the performance of the automatically trained
system built from this lexicon is relative to the ex-
isting system.
The key steps, aligning the lexicon and building
a training set, are described in detail in Sections 2
and 3 below.
Our choice of decision trees was motivated by
their following desirable properties:
1. Space and time efficiency, provided the feature
functions can be represented and computed ef-
ficiently, which they can be in our case.
2. Generality.
3. Symbolic representation that can easily be in-
spected and converted.
The first property addresses the efficiency re-
quirements stated above: if every feature function
can be computed in time O(f), where the function
f does not involve the height of the decision tree h,
then the classification function represented by the
decision tree can be computed in time O(?n. h ?
f(n)) = O(f) if feature values can be mapped to
child nodes in constant time, e. g. through hashing;
and similarly for space.
The other properties justify the use of decision
trees as a knowledge representation format. In par-
ticular, decision trees can be converted into im-
plicational rules that an expert could inspect and
can in principle be compiled back into finite-state
machines (Sproat and Riley, 1996), although that
would re-introduce the original efficiency problems.
On the other hand, finite-state transducers have the
advantage of being invertible, which can be ex-
ploited e. g. for testing hand-crafted rule sets.
We use a standard decision tree learner (Quin-
lan, 1993), since we believe that it would be pre-
mature to investigate the implications of different
choices of machine learning algorithms while the
fundamental question of what any such algorithm
should use as training data is still open. This topic
is explored further in Section 5. Related work is
discussed in Section 6.
2 Aligning the Lexicon
Learning a mapping between sets of strings is dif-
ficult unless the task is suitably restricted or addi-
tional supervision is provided. Aligning the lexicon
allows us to transform the learning task into a clas-
sification task to which standard machine learning
techniques can be applied.
Given a lexical entry we ideally would want to
align each letter with zero or more phonemes in
a way that minimizes the descriptions of the func-
tion performing the mapping and of the exceptions.
Since we do not know how to do this efficiently,
we chose to be content with an alignment produced
by the first phase of the algorithm described in
(Luk and Damper, 1996): we treat the strings to be
aligned as bags of symbols, count all possible com-
binations, and use this to estimate the parameters for
a zeroth-order Markov model.
(a) t e x . t e
t E k s t .
(b) t e x t e . . . . .
. . . . . t E k s t
Figure 1: Two possible alignments
Figure 1 shows two examples of an alignment,
where the dot represents the empty string (for rea-
sons of visual clarity), also referred to as ?. Align-
ment (b), while not as intuitively plausible as align-
ment (a), is possible as an extreme case. In gen-
eral, when counting the combinations of ` letters
with p phonemes, we want to include p empty let-
ters and ` empty phonemes. For example, given the
letters ?texte? and corresponding phonemes /tEkst/,
we count CL(t, ?) = 10, CL(t, t) = 4, CL(t, k) = 2,
etc. By normalizing the counts we arrive at an em-
pirical joint probability distribution P?L for the lexi-
con.
The existing rewrite rules were another source of
information. A rewrite rule is of the form
?? ? / ? ?
where ? is usually a string of letters and ? a string
of phonemes. The contextual restrictions expressed
by ? and ? will be ignored. Typically ? and ?
are very short, rarely consisting of more than four
symbols. We created a second lexicon consisting
of around 200 pairs ??, ?? mentioned in the rewrite
rules, and applied the same procedure as before to
obtain counts CR and from those a joint probability
distribution P?R.
The two empirical distributions were combined
and smoothed by linear interpolation with a uniform
distribution PU :
P (x, y) = ?1P?R(x, y) + ?2P?L(x, y) + ?3PU (x, y)
where each ?i ? 0 and ?1 + ?2 + ?3 = 1. The
effects of using different coefficient vectors ~? will
be discussed in Section 4.
Since we had available a library for manipulating
weighted automata (Mohri et al, 2000), the align-
ments were computed by using negative log proba-
bilities as weights for a transducer with a single state
(hence equivalent to a zeroth-order Markov model),
composing on the left with the letter string and on
the right with the phoneme string, and finding the
best path (Searls and Murphy, 1995; Mohri et al,
2000). This amounts to inserting ?-symbols into
both the string of letters and the string of phonemes
in a way that minimizes the overall weight of the
transduction, i. e. maximizes the probability of the
alignment with respect to the model.
3 Building Training Instances
Now we bring in additional restrictions that allow
us to express the task of finding a function that maps
letter sequences to phoneme sequences as the sim-
pler task of inducing a mapping from a single letter
to a single phoneme. This is a standard classifica-
tion task, and once we have a set of feature func-
tions and training instances we can choose from a
multitude of learning algorithms and target repre-
sentations. However, investigating the implications
of different choices is not our goal.
The first simplifying assumption is to pretend that
translating an entire text amounts to translating each
word in isolation (but see the discussion of liaison
in Section 5 below). Secondly we make use of the
fact that the pronunciation of a letter is in most cases
fully determined by its local context, much more so
in French (Laporte, 1997) than in English.
Each letter is to be mapped to a phoneme, or the
empty string ?, in the case of ?silent? letters (dele-
tions). An additional mechanism is needed for those
cases where a letter corresponds to more than one
phoneme (insertions), e. g. the letter ?x? correspond-
ing to the phonemes /ks/ in Figure 2a. The problem
is the non-uniform appearance of an explicit empty
string symbol that allows for insertions. We avoided
having to build a separate classifier to predict these
insertion points (see (Riley, 1991) in the context of
pronunciation modeling) by simply pretending that
an explicit empty string is present before each letter
and after the last letter. This is illustrated in Fig-
ure 2b. Visual inspection of several aligned lexica
revealed that at most one empty string symbol is
needed between any two letters.
From these aligned and padded strings we derived
training instances by considering local windows of
a fixed size. A context of size one requires a win-
(a) t e x . t e
t E k s t .
(b) . t . e . x . t . e .
. t . E . k s t . . .
Figure 2: Padding aligned strings
dow of size three, which is centered on the letter
aligned with the target phoneme. Figure 3 shows
the first few training instances derived from the ex-
ample in Figure 2b above. The beginning and end
of the string are marked with a special symbol. Note
that the empty string symbol only appears in the
center of the window, never in the contextual part,
where it would not convey any information.
$ . t 7? .
$ t e 7? t
t . e 7? .
t e x 7? E
e . x 7? .
e x t 7? k
x . t 7? s
x t e 7? t
Figure 3: A few training instances (context size: 1)
4 Evaluation
We delineated a 90%/10% split of the lexicon and
performed the alignment using a probability distri-
bution with coefficients ?1 = 0, ?2 = 0.9, and
?3 = 0.1, i. e., no information from the rewrite
rules was used and the empirical probabilities de-
rived from the lexicon were smoothed slightly. The
value for ?3 was determined empirically after sev-
eral trial runs on a held-out portion. We then gener-
ated training instances as described in the previous
section, and set aside the 10% we had earmarked
earlier for testing purposes. We ran C4.5 on the re-
maining portion of the data, using the held out 10%
for testing. Table 1 summarizes the following as-
pects of the performance of the induced decision
tree classifiers on the test data relative to the size of
context used for classification: classification accu-
racy per symbol; micro-averaged precision (P) and
recall (R) per symbol; size of the tree in number of
nodes; and size of the saved tree data in kilobytes.
All trees were pruned and the subsetting option of
C4.5 was used to further reduce the size of the trees.
Further increasing the context size did not result
in better performance. We did see a performance in-
context acc. P R size of tree
letters % % % nodes kB
0 84.0 51.9 86.6 44 7
1 96.6 90.0 91.3 917 149
2 98.6 97.0 97.1 2664 435
3 98.7 97.5 97.4 3585 586
Table 1: Performance relative to context size, align-
ment based on lexicon
crease, however, when we repeated the above proce-
dure with different coefficients ~?. This time we set
?1 = 0.9, ?2 = 0.09, and ?3 = 0.01. These partic-
ular values were again determined empirically. The
important thing to note is that the information from
the rewrite rules is now dominant, as compared to
before when it was completely absent. The effect
this had on performance is summarized in Table 2
for three letters of context. As before, classification
accuracy is given on a per-symbol basis; average ac-
curacy per word is around 85%. Notice that the size
of the tree decreases as a result of a better alignment.
alignment acc. P R size of tree
% % % nodes kB
lexicon 98.7 97.5 97.4 3585 586
lex. + rules 98.9 97.8 97.9 3394 555
Table 2: Performance relative to alignment quality
(context size: 3)
These figures are all relative to our existing sys-
tem. What is most important to us are the vast im-
provements in efficiency: the decision trees take up
less than 10% of the space of the original letter-to-
phoneme component, which weighs in at 6.7 MB
total with composition deferred until runtime, since
off-line composition would have resulted in an im-
practically large machine. The size of the origi-
nal component could be reduced through the use
of compression techniques (Kiraz, 1999), which
would lead to an additional run-time overhead.
Classification speed of the decision trees is on
the order of several thousand letters per second (de-
pending on platform details), which is many times
faster than the existing system. The exact details of
a speed comparison depend heavily on platform is-
sues and what one considers to be the average case,
but a conservative estimate places the speedup at a
factor of 20 or more.
5 Directions for Further Research
The tremendous gains in efficiency will enable us
to investigate the use of additional processing mod-
ules that are not included in the existing system be-
cause they would have pushed performance below
an acceptable bound. For example no sophisticated
part-of-speech (POS) disambiguation is done at the
moment, but would be needed to distinguish, e. g.,
between different pronunciations of French words
ending in -ent, which could be verbs, nouns, ad-
verbs, etc. The need for POS disambiguation is
even clearer for languages with ?deep? orthogra-
phies, such as English. In conjunction with shallow
parsing, POS disambiguation would give us enough
information to deal with most cases of liaison, an
inter-word phenomenon that required special atten-
tion in the existing system and that we have so far
ignored in the new approach because of the exclu-
sive focus on regularities at the level of isolated
words.
We have been using the existing automaton-based
system as our baseline, which is unfair because
that system makes mistakes which could very well
obscure some regularities the inductive approach
might otherwise have discovered. Future compar-
isons should use an independent gold standard, such
as a large dictionary, to evaluate and compare both
approaches. The advantage of using the existing
system instead of a dictionary is that we could gen-
erate large amounts of training data from corpora.
But even with plenty of training data available,
the paradigms of verbal inflections, for example,
are quite extensive in French, inflected verb forms
are typically not listed in a dictionary, and we can-
not guarantee that sufficiently many forms appear
in a corpus to guarantee full coverage. In this case
it would make sense to use a hybrid approach that
reuses the explicit representations of verbal inflec-
tions from the existing system.
More importantly, having more training data
available for use with our new approach would
only help to a small extent. Though more and/or
cleaner data would possibly result in better align-
ments, we do not expect to find vast improvements
unless the restriction imposed by the zeroth-order
Markov assumption used for alignment is dropped,
which could easily be done. However, it is not clear
that using a bigram or trigram model for alignment
would optimize the alignment in such a way that the
decision tree classifier learned from the aligned data
is as small and accurate as possible.
This points to a fundamental shortcoming of the
usual two-step procedure, which we followed here:
the goodness of an alignment performed in the first
step should be determined by the impact it has on
producing an optimal classifier that is induced in
the second step. However, there is no provision for
feedback from the second step to the first step. For
this a different setup would be needed that would
discover an optimal alignment and classifier at the
same time. This, to us, is one of the key research
questions yet to be addressed in learning letter-to-
sound rules, since the quality of an alignment and
hence the training data for a classifier learner is es-
sential for ensuring satisfactory performance of the
induced classifier. The question of which classifier
(learner) to use is secondary and not necessarily spe-
cific to the task of learning letter?sound correspon-
dences.
6 Relation to Existing Research
The problem of letter-to-sound conversion is very
similar to the problem of modeling pronuncia-
tion variation, or phonetic/phonological model-
ing (Miller, 1998). For pronunciation modeling
where alternative pronunciations are generated from
known forms one can use standard similarity met-
rics for strings (Hamming distance, Levenshtein
distance, etc.), which are not meaningful for map-
pings between sequences over dissimilar alphabets,
such as letter-to-phoneme mappings.
General techniques for letter-to-phoneme con-
version need to go beyond dictionary lookups and
should be able to handle all possible written word
forms. Since the general problem of learning reg-
ular mappings between regular languages is in-
tractable because of the vast hypothesis space, all
existing research on automatic methods has im-
posed restrictions on the class of target functions. In
almost all cases, this paper included, one only con-
siders functions that are local in the sense that only
a fixed amount of context is relevant for mapping a
letter to a phoneme.
One exception to this is (Gildea and Jurafsky,
1995), where the target function space are the subse-
quential transducers, for which a limit-identification
algorithm exists (Oncina et al, 1993). However,
without additional guidance, that algorithm cannot
be directly applied to the phonetic modeling task
due to data sparseness and/or lack of sufficient bias
(Gildea and Jurafsky, 1995). We would argue that
the lack of locality restrictions is at the root of the
convergence problems for that approach.
Our approach effectively restricts the hypothe-
sis space even further to include only the k-local
(or strictly k-testable) sequential transducers, where
a classification decision is made deterministically
and based on a fixed amount of context. We con-
sider this to be a good target since we would like
the letter-to-sound mapping to be a function (every
piece of text has exactly one contextually appropri-
ate phonetic realization) and to be deterministically
computable without involving any kind of search.
Locality gives us enough bias for efficiently learn-
ing classifiers with good performance. Since we
are dealing with a restricted subclass of finite-state
transducers, our approach is, at a theoretical level,
fully consistent with the claim in (Sproat, 2000) that
letter?phoneme correspondences can be expressed
as regular relations. However, it must be stressed
that just because something is finite-state does not
mean it should be implemented directly as a finite-
state automaton.
Other machine learning approaches employ es-
sentially the same locality restrictions. Different
learning algorithms can be used, including Artificial
neural networks (Sejnowski and Rosenberg, 1987;
Miller, 1998), decision tree learners (Black et al,
1998), memory-based learners and hybrid symbolic
approaches (Van den Bosch and Daelemans, 1993;
Daelemans and van den Bosch, 1997), or Markov
models. Out of these the approach in (Black et al,
1998) is most similar to ours, but it presupposes
that phoneme strings are never longer than the cor-
responding letter strings, which is mostly true, but
has systematic exceptions, e. g. ?exact? in English
or French. English has many more exceptions that
do not involve the letter ?x?, such as ?cubism? (/kju-
bIz@m/ according to cmudict.0.6) or ?mutual-
sim?.
The problem of finding a good alignment has not
received its due attention in the literature. Work
on multiple alignments in computational biology
cannot be adapted directly because the letter-to-
sound mapping is between dissimilar alphabets.
The alignment problem in statistical machine trans-
lation (Brown et al, 1990) is too general: long-
distance displacement of large chunks of material
may occur frequently when translating whole sen-
tences, but are unlikely to play any role for the
letter-to-sound mapping, though local reorderings
do occur (Sproat, 2000). Ad hoc figures of merit for
alignments (Daelemans and van den Bosch, 1997)
or hand-corrected alignments (Black et al, 1998)
might give good results in practice, but do not get
us any closer to a principled solution. The present
work is another step towards obtaining better align-
ments by exploiting easily available knowledge in a
systematic fashion.
7 Conclusion
We presented a method for building efficient letter-
to-sound rules from information extractable from,
or with the help of, existing hand-crafted rewrite
rules. Using decision trees as the new target rep-
resentation, significant improvements in time and
space efficiency could be achieved at the cost of
a reduction in accuracy. Our approach relies on
finding an alignment between strings of letters and
phonemes. We identified a way to improve align-
ments and argued that finding a good alignment is
crucial for success and should receive more atten-
tion.
Acknowledgments
The work reported on here was carried out within
Lucent Technologies? Summer Research Program.
I would like to thank the people at Bell Labs for
their help and support, especially Gerald Penn, who
was my mentor for the summer, and Evelyne Tzouk-
ermann. Thanks also to Chris Brew, Gerald Penn,
Richard Sproat, and three anonymous reviewers for
valuable feedback on this paper. The usual dis-
claimers apply.
References
Alan W. Black, Kevin Lenzo, and Vincent Pagel.
1998. Issues in building general letter to sound
rules. In Proc. of the 3rd ESCA Workshop on
Speech Synthesis, pages 77?80.
Antal van den Bosch and Walter Daelemans.
1993. Data-oriented methods for grapheme-to-
phoneme conversion. In Proc. of the 6th Euro-
pean Conference of the Association for Compu-
tational Linguistics, pages 45?53.
Peter F. Brown, John Cocke, Stephen A.
Della Pietra, Vincent J. Della Pietra, Fredrick
Jelinek, John D. Lafferty, Robert L. Mercer, and
Paul S. Rossin. 1990. A statistical approach to
machine translation. Computational Linguistics,
16(2):79?85.
Walter M. P. Daelemans and Antal P. J. van den
Bosch. 1997. Language-independent data-
oriented grapheme-to-phoneme conversion. In
Jan P. H. van Santen, Richard W. Sproat,
Joseph P. Olive, and Julia Hirschberg, edi-
tors, Progress in Speech Synthesis, pages 77?89.
Springer, New York.
Dan Gildea and Dan Jurafsky. 1995. Automatic
induction of finite state transducers for simple
phonological rules. In Proc. of the 33rd Annual
Meeting of the Association for Computational
Linguistics, pages 9?15.
Ronald M. Kaplan and Martin Kay. 1994. Regular
models of phonological rule systems. Computa-
tional Linguistics, 20(3):331?378.
George Anton Kiraz. 1999. Compressed storage
of sparse finite-state transducers. In Proc. of the
1999 Workshop on Implementing Automata, Pots-
dam, Germany.
?ric Laporte. 1997. Rational transductions for pho-
netic conversion and phonology. In Emmanuel
Roche and Yves Schabes, editors, Finite-State
Language Processing, chapter 14, pages 407?
430. MIT Press, Cambridge.
Robert Luk and Robert Damper. 1996. Stochas-
tic phonographic transduction for English. Com-
puter Speech and Language, 10:133?153.
Corey Andrew Miller. 1998. Pronunciation Model-
ing in Speech Synthesis. Ph.D. thesis, University
of Pennsylvania.
Mehryar Mohri and Richard Sproat. 1996. An ef-
ficient compiler for weighted rewrite rules. In
Proc. of the 34th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 231?
238.
Mehryar Mohri, Fernando Pereira, and Michael Ri-
ley. 1996. Weighted automata in text and speech
processing. In Extended Finite State Models of
Language: Proc. of the ECAI ?96 Workshop,
pages 46?50.
Mehryar Mohri, Fernando Pereira, and Michael Ri-
ley. 2000. The design principles of a weighted
finite-state transducer library. Theoretical Com-
puter Science, 231(1):17?32.
Jos? Oncina, Pedro Garc?a, and Enrique Vi-
dal. 1993. Learning subsequential transducers
for pattern recognition and interpretation tasks.
IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 15(5):448?458.
J. R. Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann, San Mateo, CA.
Michael D. Riley. 1991. A statistical model for
generating pronunciation networks. In Proc.
of the International Conference on Acoustics,
Speech, and Signal Processing, pages 737?740.
David B. Searls and Kevin P. Murphy. 1995.
Automata-theoretic models of mutation and
alignment. In International Conference on Intel-
ligent Systems in Molecular Biology, pages 341?
349.
T. J. Sejnowski and C. R. Rosenberg. 1987. Paral-
lel networks that learn to pronounce English text.
Complex Systems, 1:145?168.
Richard Sproat and Michael Riley. 1996. Compila-
tion of weighted finite-state transducers from de-
cision trees. In Proc. of the 34th Annual Meeting
of the Association for Computational Linguistics,
pages 215?222.
Richard Sproat, Bernd M?bius, Kazuaki Maeda,
and Evelyne Tzoukermann. 1998. Multilingual
text analysis. In (Sproat, 1998), chapter 3, pages
31?87.
Richard Sproat, editor. 1998. Multilingual Text-
to-Speech Synthesis: The Bell Labs Approach.
Kluwer, Dordrecht.
Richard Sproat. 2000. A Computational Theory
of Writing Systems. Cambridge University Press,
Cambridge.
Parametric Models of Linguistic Count Data
Martin Jansche
Department of Linguistics
The Ohio State University
Columbus, OH 43210, USA
jansche@acm.org
Abstract
It is well known that occurrence counts
of words in documents are often mod-
eled poorly by standard distributions like
the binomial or Poisson. Observed counts
vary more than simple models predict,
prompting the use of overdispersed mod-
els like Gamma-Poisson or Beta-binomial
mixtures as robust alternatives. Another
deficiency of standard models is due to the
fact that most words never occur in a given
document, resulting in large amounts of
zero counts. We propose using zero-
inflated models for dealing with this, and
evaluate competing models on a Naive
Bayes text classification task. Simple
zero-inflated models can account for prac-
tically relevant variation, and can be easier
to work with than overdispersed models.
1 Introduction
Linguistic count data often violate the simplistic as-
sumptions of standard probability models like the
binomial or Poisson distribution. In particular, the
inadequacy of the Poisson distribution for model-
ing word (token) frequency is well known, and ro-
bust alternatives have been proposed (Mosteller and
Wallace, 1984; Church and Gale, 1995). In the case
of the Poisson, a commonly used robust alternative
is the negative binomial distribution (Pawitan, 2001,
?4.5), which has the ability to capture extra-Poisson
variation in the data, in other words, it is overdis-
persed compared with the Poisson. When a small
set of parameters controls all properties of the dis-
tribution it is important to have enough parameters
to model the relevant aspects of one?s data. Sim-
ple models like the Poisson or binomial do not have
enough parameters for many realistic applications,
and we suspect that the same might be true of log-
linear models. When applying robust models like
the negative binomial to linguistic count data like
word occurrences in documents, it is natural to ask
to what extent the extra-Poisson variation has been
captured by the model. Answering that question is
our main goal, and we begin by reviewing some of
the classic results of Mosteller and Wallace (1984).
2 Word Frequency in Fixed-Length Texts
In preparation of their authorship study of The Fed-
eralist, Mosteller and Wallace (1984, ?2.3) investi-
gated the variation of word frequency across con-
tiguous passages of similar length, drawn from pa-
pers of known authorship. The occurrence frequen-
cies of any in papers by Hamilton (op. cit., Ta-
ble 2.3?3) are repeated here in Figure 1: out of a
total of 247 passages there are 125 in which the
word any does not occur; it occurs once in 88 pas-
sages, twice in 26 passages, etc. Figure 1 also shows
the counts predicted by a Poisson distribution with
mean 0.67. Visual inspection (?chi by eye?) indi-
cates an acceptable fit between the model and the
data, which is confirmed by a ?2 goodness-of-fit
test. This demonstrates that certain words seem to
be adequately modeled by a Poisson distribution,
whose probability mass function is shown in (1):
Poisson(? )(x) = ?
x
x!
1
exp? (1)
126
50
75
100
125
0 1 2 3 4 5
88
7fre
qu
en
cy
(n
um
be
ro
fp
as
sa
ge
s)
occurrences of "any" [Hamilton]
observed
Poisson(0.67)
Figure 1: Occurrence counts of any in Hamilton pas-
sages: raw counts and counts predicted under a Pois-
son model.
For other words the Poisson distribution gives a
much worse fit. Take the occurrences of were in pa-
pers by Madison, as shown in Figure 2 (ibid.). We
calculate the ?2 statistic for the counts expected un-
der a Poisson model for three bins (0, 1, and 2?5, to
ensure that the expected counts are greater than 5)
and obtain 6.17 at one degree of freedom (number
of bins minus number of parameters minus one),
which is enough to reject the null hypothesis that
the data arose from a Poisson(0.45) distribution. On
the other hand, the ?2 statistic for a negative bino-
mial distribution NegBin(0.45,1.17) is only 0.013
for four bins (0, 1, 2, and 3?5), i. e., again 1 degree
of freedom, as two parameters were estimated from
the data. Now we are very far from rejecting the null
hypothesis. This provides some quantitative back-
ing for Mosteller and Wallace?s statement that ?even
the most motherly eye can scarcely make twins of
the [Poisson vs. empirical] distributions? for certain
words (op. cit., 31).
The probability mass function of the negative bi-
nomial distribution, using Mosteller and Wallace?s
parameterization, is shown in (2):
NegBin(? ,?)(x) = ?
x
x!
?(? + x)
(? +?)?+x
??
?(?) (2)
If one recalls that the Gamma function is well be-
haved and that
exp? = lim
???
(
1+
?
?
)?
= lim
???
(? +?)?
?? ,
it is easy to see that NegBin(? ,?) converges to
Poisson(? ) for ? constant and ? ? ?. On the other
5
25
50
75
100
125
150
179
0 1 2 3 4 5
1
18
58
167
fre
qu
en
cy
(n
um
be
ro
fp
as
sa
ge
s)
occurrences of "were" [Madison]
observed
Poisson(0.45)
NegBin(0.45, 1.17)
Figure 2: Occurrence counts of were in Madison
passages: raw counts and counts predicted under
Poisson and negative binomial models.
hand, small values of ? drag the mode of the nega-
tive binomial distribution towards zero and increase
its variance, compared with the Poisson.
As more and more probability mass is concen-
trated at 0, the negative binomial distribution starts
to depart from the empirical distribution. One can
already see this tendency in Mosteller and Wallace?s
data, although they themselves never comment on
it. The problem with a huge chunk of the proba-
bility mass at 0 is that one is forced to say that the
outcome 1 is still fairly likely and that the probabil-
ity should drop rapidly from 2 onwards as the term
1/x! starts to exert its influence. This is often at odds
with actual data.
Take the word his in papers by Hamilton and
Madison (ibid., pooled from individual sections of
Table 2.3?3). It is intuitively clear that his may
not occur at all in texts that deal with certain as-
pects of the US Constitution, since many aspects of
constitutional law are not concerned with any sin-
gle (male) person. For example, Federalist No. 23
(The Necessity of a Government as Energetic as the
One Proposed to the Preservation of the Union, ap-
prox. 1800 words, by Hamilton) does not contain a
single occurrence of his, whereas Federalist No. 72
(approx. 2000 words, a continuation of No. 71 The
Duration in Office of the Executive, also by Hamil-
ton) contains 35 occurrences. The difference is that
No. 23 is about the role of a federal government in
the abstract, and Nos. 71/72 are about term limits for
offices filled by (male) individuals. We might there-
fore expect the occurrences of his to vary more, de-
405
200
100
48
26
12
0 1 2 3
71
39
18
fre
qu
en
cy
(n
um
be
ro
fp
as
sa
ge
s)
occurrences of "his" [Hamilton, Madison]
observed
NegBin(0.54, 0.15)
NegBin(0.76, 0.11)
0.34 NegBin(1.56, 0.89)
Figure 3: Occurrence counts of his in Hamilton and
Madison passages (NB: y-axis is logarithmic).
pending on topic, than any or were.
The overall distribution of his is summarized in
Figure 3; full details can be found in Table 1. Ob-
serve the huge number of passages with zero oc-
currences of his, which is ten times the number of
passages with exactly one occurrence. Also notice
how the negative binomial distribution fitted using
the Method of Maximum Likelihood (MLE model,
first line in Figure 3, third column in Table 1) over-
shoots at 1, but underestimates the number of pas-
sages with 2 and 3 occurrences.
The problem cannot be solved by trying to fit the
two parameters of the negative binomial based on
the observed counts of two points. The second line
in Figure 3 is from a distribution fitted to match the
observed counts at 0 and 1. Although it fits those two
points perfectly, the overall fit is worse than that of
the MLE model, since it underestimates the observed
counts at 2 and 3 more heavily.
The solution we propose is illustrated by the third
line in Figure 3. It accounts for only about a third
of the data, but covers all passages with one or more
occurrences of his. Visual inspection suggests that it
provides a much better fit than the other two models,
if we ignore the outcome 0; a quantitative compari-
son will follow below. This last model has relaxed
the relationship between the probability of the out-
come 0 and the probabilities of the other outcomes.
In particular, we obtain appropriate counts for the
outcome 1 by pretending that the outcome 0 oc-
curs only about 71 times, compared with an actual
405 observed occurrences. Recall that the model
accounts for only 34% of the data; the remaining
NegBin ZINB
obsrvd expctd expctd
0 405 403.853 405.000
1 39 48.333 40.207
2 26 21.686 24.206
3 18 12.108 14.868
4 5 7.424 9.223
5?6 9 8.001 9.361
7?14 7 6.996 5.977
?2 statistic 6.447 2.952
df 4 3
?2 cumul. prob 0.832 0.601
? logL(??) 441.585 439.596
Table 1: Occurrence counts of his in Hamilton and
Madison passages.
counts for the outcome 0 are supplied entirely by
a second component whose probability mass is con-
centrated at zero. The expected counts under the full
model are found in the rightmost column of Table 1.
The general recipe for models with large counts
for the zero outcome is to construe them as two-
component mixtures, where one component is a de-
generate distribution whose entire probability mass
is assigned to the outcome 0, and the other compo-
nent is a standard distribution, call it F (?). Such a
nonstandard mixture model is sometimes known as
a ?modified? distribution (Johnson and Kotz, 1969,
?8.4) or, more perspicuously, as a zero-inflated dis-
tribution. The probability mass function of a zero-
inflated F distribution is given by equation (3),
where 0 ? z ? 1 (z < 0 may be allowable subject
to additional constraints) and x? 0 is the Kronecker
delta ?x,0.
ZIF (z,?)(x) = z (x? 0)+(1? z)F (?)(x) (3)
It corresponds to the following generative process:
toss a z-biased coin; if it comes up heads, generate 0;
if it comes up tails, generate according to F (?). If
we apply this to word frequency in documents, what
this is saying is, informally: whether a given word
appears at all in a document is one thing; how often
it appears, if it does, is another thing.
This is reminiscent of Church?s statement that
?[t]he first mention of a word obviously depends
on frequency, but surprisingly, the second does
not.? (Church, 2000) However, Church was con-
cerned with language modeling, and in particular
cache-based models that overcome some of the limi-
tations introduced by a Markov assumption. In such
a setting it is natural to make a distinction between
the first occurrence of a word and subsequent occur-
rences, which according to Church are influenced
by adaptation (Church and Gale, 1995), referring
to an increase in a word?s chance of re-occurrence
after it has been spotted for the first time. For
empirically demonstrating the effects of adaptation,
Church (2000) worked with nonparametric methods.
By contrast, our focus is on parametric methods, and
unlike in language modeling, we are also interested
in words that fail to occur in a document, so it is nat-
ural for us to distinguish between zero and nonzero
occurrences.
In Table 1, ZINB refers to the zero-inflated neg-
ative binomial distribution, which takes a parame-
ter z in addition to the two parameters of its nega-
tive binomial component. Since the negative bino-
mial itself can already accommodate large fractions
of the probability mass at 0, we must ask whether the
ZINB model fits the data better than a simple nega-
tive binomial. The bottom row of Table 1 shows the
negative log likelihood of the maximum likelihood
estimate ?? for each model. Log odds of 2 in favor of
ZINB are indeed sufficient (on Akaike?s likelihood-
based information criterion; see e. g. Pawitan 2001,
?13.5) to justify the introduction of the additional
parameter. Also note that the cumulative ?2 proba-
bility of the ?2 statistic at the appropriate degrees of
freedom is lower for the zero-inflated distribution.
It is clear that a large amount of the observed
variation of word occurrences is due to zero infla-
tion, because virtually all words are rare and many
words are simply not ?on topic? for a given docu-
ment. Even a seemingly innocent word like his turns
out to be ?loaded? (and we are not referring to gen-
der issues), since it is not on topic for certain dis-
cussions of constitutional law. One can imagine that
this effect is even more pronounced for taboo words,
proper names, or technical jargon (cf. Church 2000).
Our next question is whether the observed variation
is best accounted for in terms of zero-inflation or
overdispersion. We phrase the discussion in terms of
a practical task for which it matters whether a word
is on topic for a document.
3 Word Frequency Conditional on
Document Length
Word occurrence counts play an important role in
document classification under an independent fea-
ture model (commonly known as ?Naive Bayes?).
This is not entirely uncontroversial, as many ap-
proaches to document classification use binary in-
dicators for the presence and absence of each word,
instead of full-fledged occurrence counts (see Lewis
1998 for an overview). In fact, McCallum and
Nigam (1998) claim that for small vocabulary sizes
one is generally better off using Bernoulli indicator
variables; however, for a sufficiently large vocab-
ulary, classification accuracy is higher if one takes
word frequency into account.
Comparing different probability models in terms
of their effects on classification under a Naive Bayes
assumption is likely to yield very conservative re-
sults, since the Naive Bayes classifier can perform
accurate classifications under many kinds of adverse
conditions and even when highly inaccurate prob-
ability estimates are used (Domingos and Pazzani,
1996; Garg and Roth, 2001). On the other hand, an
evaluation in terms of document classification has
the advantages, compared with language modeling,
of computational simplicity and the ability to benefit
from information about non-occurrences of words.
Making a direct comparison of overdispersed and
zero-inflated models with those used by McCal-
lum and Nigam (1998) is difficult, since McCal-
lum and Nigam use multivariate models ? for which
the ?naive? independence assumption is different
(Lewis, 1998) ? that are not as easily extended to
the cases we are concerned about. For example,
the natural overdispersed variant of the multinomial
model is the Dirichlet-multinomial mixture, which
adds just a single parameter that globally controls
the overall variation of the entire vocabulary. How-
ever, Church, Gale and other have demonstrated re-
peatedly (Church and Gale, 1995; Church, 2000)
that adaptation or ?burstiness? are clearly properties
of individual words (word types). Using joint inde-
pendent models (one model per word) brings us back
into the realm of standard independence assump-
tions, makes it easy to add parameters that control
overdispersion and/or zero-inflation for each word
individually, and simplifies parameter estimation.
020
40
60
80
100
10 100 1000 10000 100000cl
as
si
fic
at
io
n
ac
cu
ra
cy
(p
er
ce
nt
)
vocabulary size (number of word types)
Newsgroups
Binomial
Bernoulli
Figure 4: A comparison of event models for differ-
ent vocabulary sizes on the Newsgroup data set.
So instead of a single multinomial distribution
we use independent binomials, and instead of a
multivariate Bernoulli model we use independent
Bernoulli models for each word. The overall joint
model is clearly wrong since it wastes probability
mass on events that are known a priori to be impos-
sible, like observing documents for which the sum of
the occurrences of each word is greater than the doc-
ument length. On the other hand, it allows us to take
the true document length into account while using
only a subset of the vocabulary, whereas on McCal-
lum and Nigam?s approach one has to either com-
pletely eliminate all out-of-vocabulary words and
adjust the document length accordingly, or else map
out-of-vocabulary words to an unknown-word token
whose observed counts could then easily dominate.
In practice, using joint independent models does
not cause problems. We replicated McCallum and
Nigam?s Newsgroup experiment1 and did not find
any major discrepancies. The reader is encour-
aged to compare our Figure 4 with McCallum and
Nigam?s Figure 3. Not only are the accuracy fig-
ures comparable, we also obtained the same criti-
cal vocabulary size of 200 words below which the
Bernoulli model results in higher classification ac-
curacy.
The Newsgroup data set (Lang, 1995) is a strati-
1Many of the data sets used by McCallum and Nigam (1998)
are available at http://www.cs.cmu.edu/~TextLearning/
datasets.html.
fied sample of approximately 20,000 messages to-
tal, drawn from 20 Usenet newsgroups. The fact
that 20 newsgroups are represented in equal pro-
portions makes this data set well suited for compar-
ing different classifiers, as class priors are uniform
and baseline accuracy is low at 5%. Like McCal-
lum and Nigam (1998) we used (Rain)bow (McCal-
lum, 1996) for tokenization and to obtain the word/
document count matrix. Even though we followed
McCallum and Nigam?s tokenization recipe (skip-
ping message headers, forming words from contigu-
ous alphabetic characters, not using a stemmer), our
total vocabulary size of 62,264 does not match Mc-
Callum and Nigam?s figure of 62,258, but does come
reasonably close. Also following McCallum and
Nigam (1998) we performed a 4:1 random split into
training and test data. The reported results were ob-
tained by training classification models on the train-
ing data and evaluating on the unseen test data.
We compared four models of token frequency.
Each model is conditional on the document length n
(but assumes that the parameters of the distribution
do not depend on document length), and is derived
from the binomial distribution
Binom(p)(x | n) =
(
n
x
)
px (1? p)n?x, (4)
which we view as a one-parameter conditional
model, our firstmodel: x represents the token counts
(0? x? n); and n is the length of the document mea-
sured as the total number of token counts, including
out-of-vocabulary items.
The second model is the Bernoulli model, which
is derived from the binomial distribution by replac-
ing all non-zero counts with 1:
Bernoulli(p)(x | n)
= Binom(p)
(?
x
x+1
?
|
?
n
n+1
?)
(5)
Our third model is an overdispersed binomial
model, a ?natural? continuous mixture of binomi-
als with the integrated binomial likelihood ? i. e. the
Beta density (6), whose normalizing term involves
the Beta function ? as the mixing distribution.
Beta(?,? )(p) = p
??1(1? p)??1
B(?,? ) (6)
The resulting mixture model (7) is known as the
Po?lya?Eggenberger distribution (Johnson and Kotz,
1969) or as the beta-binomial distribution. It has
been used for a comparatively small range of NLP
applications (Lowe, 1999) and certainly deserves
more widespread attention.
BetaBin(?,? )(x | n)
=
? 1
0
Binom(p)(x | n) Beta(?,? )(p) dp
=
(
n
x
)
B(x+?,n? x+? )
B(?,? ) (7)
As was the case with the negative binomial (which
is to the Poisson as the beta-binomial is to the bino-
mial), it is convenient to reparameterize the distribu-
tion. We choose a slightly different parameterization
than Lowe (1999); we follow Ennis and Bi (1998)
and use the identities
p = ?/(? +? ),
? = 1/(? +? +1).
To avoid confusion, we will refer to the distribution
parameterized in terms of p and ? as BB:
BB(p,?) = BetaBin
(
p
1? ?
? , (1? p)
1? ?
?
)
(8)
After reparameterization the expectation and vari-
ance are
E[x;BB(p,?)(x | n)] = n p,
Var[x;BB(p,?)(x | n)] = n p (1? p) (1+(n?1) ?).
Comparing this with the expectation and variance of
the standard binomial model, it is obvious that the
beta-binomial has greater variance when ? > 0, and
for ? = 0 the beta-binomial distribution coincides
with a binomial distribution.
Using the method of moments for estimation is
particularly straightforward under this parameteri-
zation (Ennis and Bi, 1998). Suppose one sample
consists of observing x successes in n trials (x occur-
rences of the target word in a document of length n),
where the number of trials may vary across samples.
Now we want to estimate parameters based on a se-
quence of s samples ?x1,n1?, . . . ,?xs,ns?. We equate
sample moments with distribution moments
?
i
ni p? =?
i
xi,
?
i
ni p? (1? p?) (1+(ni?1) ??) =?
i
(xi?ni p?)
2,
and solve for the unknown parameters:
p? =
?i xi
?i ni
, (9)
?? = ?i(xi?ni p?)
2/(p? (1? p?))??i ni
?i n
2
i ??i ni
. (10)
In our experience, the resulting estimates are suf-
ficiently close to the maximum likelihood esti-
mates, while method-of-moment estimation is much
faster than maximum likelihood estimation, which
requires gradient-based numerical optimization2 in
this case. Since we estimate parameters for up to
400,000 models (for 20,000 words and 20 classes),
we prefer the faster procedure. Note that the
maximum likelihood estimates may be suboptimal
(Lowe, 1999), but full-fledged Bayesian methods
(Lee and Lio, 1997) would require even more com-
putational resources.
The fourth and final model is a zero-inflated bino-
mial distribution, which is derived straightforwardly
via equation (3):
ZIBinom(z, p)(x | n)
= z (x? 0)+(1? z)Binom(p)(x | n)
=
?
?
?
z+(1? z)(1? p)n if x = 0
(1? z)
(
n
x
)
px (1? p)n?x if x > 0
(11)
Since the one parameter p of a single binomial
model can be estimated directly using equation (9),
maximum likelihood estimation for the zero-inflated
binomial model is straightforward via the EM al-
gorithm for finite mixture models. Figure 5 shows
pseudo-code for a single EM update.
Accuracy results of Naive Bayes document classi-
fication using each of the four word frequency mod-
els are shown in Table 2. One can observe that the
differences between the binomial models are small,
2Not that there is anything wrong with that. In fact, we cal-
culated the MLE estimates for the negative binomial models us-
ing a multidimensional quasi-Newton algorithm.
1: Z? 0; X ? 0; N? 0
2: {E step}
3: for i? 1 to s do
4: if xi = 0 then
5: z?i? z/(z+(1? p)ni)
6: Z? Z+ z?i
7: X ? X +(1? z?i) xi
8: N? X +(1? z?i)ni
9: else {xi 6= 0, z?i = 0}
10: X ? X + xi
11: N? N +ni
12: end if
13: end for
14: {M step}
15: z? Z/s
16: p? X/N
Figure 5: Maximum likelihood estimation of ZI-
Binom parameters z and p: Pseudo-code for a single
EM iteration that updates the two parameters.
but even small effects can be significant on a test set
of about 4,000 messages. More importantly, note
that the beta-binomial and zero-inflated binomial
models outperform both the simple binomial and the
Bernoulli, except on unrealistically small vocabu-
laries (intuitively, 20 words are hardly adequate for
discriminating between 20 newsgroups, and those
words would have to be selected much more care-
fully). In light of this we can revise McCallum and
Nigam?s McCallum and Nigam (1998) recommen-
dation to use the Bernoulli distribution for small vo-
cabularies. Instead we recommend that neither the
Bernoulli nor the binomial distributions should be
used, since in all reasonable cases they are outper-
formed by the more robust variants of the binomial
distribution. (The case of a 20,000 word vocabulary
is quickly declared unreasonable, since most of the
words occur precisely once in the training data, and
so any parameter estimate is bound to be unreliable.)
We want to knowwhether the differences between
the three binomial models could be dismissed as a
chance occurrence. The McNemar test (Dietterich,
1998) provides appropriate answers, which are sum-
marized in Table 3. As we can see, the classifi-
cation results under the zero-inflated binomial and
beta-binomial models are never significantly differ-
Bernoulli Binom ZIBinom BetaBin
20 30.94 28.19 29.48 29.93
50 45.28 44.04 44.85 45.15
100 53.36 52.57 53.84 54.16
200 59.72 60.15 60.47 61.16
500 66.58 68.30 67.95 68.58
1,000 69.31 72.24 72.46 73.20
2,000 71.45 75.92 76.35 77.03
5,000 73.80 80.64 80.51 80.19
10,000 74.18 82.61 82.58 82.58
20,000 74.05 83.70 83.06 83.06
Table 2: Accuracy of the four models on the News-
group data set for different vocabulary sizes.
Binom Binom ZIBinom
ZIBinom BetaBin BetaBin
20 7 7
50 7 7
100 7 7
200 7
500
1,000 7
2,000 7
5,000
10,000
20,000 7
Table 3: Pairwise McNemar test results. A 7 in-
dicates a significant difference of the classification
results when comparing a pair of of models.
ent, in most cases not even approaching significance
at the 5% level. A classifier based on the beta-
binomial model is significantly different from one
based on the binomial model; the difference for a
vocabulary of 20,000 words is marginally significant
(the ?2 value of 3.8658 barely exceeds the critical
value of 3.8416 required for significance at the 5%
level). Classification based on the zero-inflated bi-
nomial distribution differs most from using a stan-
dard binomial model. We conclude that the zero-
inflated binomial distribution captures the relevant
extra-binomial variation just as well as the overdis-
persed beta-binomial distribution, since their classi-
fication results are never significantly different.
The differences between the four models can be
seen more visually clearly on the WebKB data set
70
75
80
85
90
20k10k5k2k1k5002001005020cla
ssi
fic
ati
on
ac
cu
rac
y(
pe
rce
nt)
vocabulary size (number of word types)
WebKB 4
Bernoulli
Binomial
ZIBinom
BetaBin
Figure 6: Accuracy of the four models on the Web-
KB data set as a function of vocabulary size.
(McCallum and Nigam, 1998, Figure 4). Evaluation
results for Naive Bayes text classification using the
four models are displayed in Figure 6. The zero-
inflated binomial model provides the overall high-
est classification accuracy, and clearly dominates the
beta-binomial model. Either one should be preferred
over the simple binomial model. The early peak
and rapid decline of the Bernoulli model had already
been observed by McCallum and Nigam (1998).
We recommend that the zero-inflated binomial
distribution should always be tried first, unless there
is substantial empirical or prior evidence against
it: the zero-inflated binomial model is computation-
ally attractive (maximum likelihood estimation us-
ing EM is straightforward and numerically stable,
most gradient-based methods are not), and its z pa-
rameter is independently meaningful, as it can be in-
terpreted as the degree to which a given word is ?on
topic? for a given class of documents.
4 Conclusion
We have presented theoretical and empirical evi-
dence for zero-inflation among linguistic count data.
Zero-inflated models can account for increased vari-
ation at least as well as overdispersed models on
standard document classification tasks. Given the
computational advantages of simple zero-inflated
models, they can and should be used in place of stan-
dard models. For document classification, an event
model based on a zero-inflated binomial distribu-
tion outperforms conventional Bernoulli and bino-
mial models.
Acknowledgements
Thanks to Chris Brew and three anonymous review-
ers for valuable feedback. Cue the usual disclaimers.
References
Kenneth W. Church. 2000. Empirical estimates of adaptation:
The chance of two Noriegas is closer to p/2 than p2. In
18th International Conference on Computational Linguis-
tics, pages 180?186. ACL Anthology C00-1027.
Kenneth W. Church and William A. Gale. 1995. Poisson mix-
tures. Natural Language Engineering, 1:163?190.
Thomas G. Dietterich. 1998. Approximate statistical tests
for comparing supervised classification learning algorithms.
Neural Computation, 10:1895?1924.
Pedro Domingos and Michael J. Pazzani. 1996. Beyond in-
dependence: Conditions for the optimality of the simple
Bayesian classifier. In 13th International Conference on Ma-
chine Learning, pages 105?112.
Daniel M. Ennis and Jian Bi. 1998. The beta-binomial model:
Accounting for inter-trial variation in replicated difference
and preference tests. Journal of Sensory Studies, 13:389?
412.
Ashutosh Garg and Dan Roth. 2001. Understanding probabilis-
tic classifiers. In 12th European Conference on Machine
Learning, pages 179?191.
Norman L. Johnson and Samuel Kotz. 1969. Discrete Distribu-
tions, volume 1. Wiley, New York, NY, first edition.
Ken Lang. 1995. Newsweeder: Learning to filter netnews. In
12th International Conference on Machine Learning, pages
331?339.
Jack C. Lee and Y. L. Lio. 1997. A note on Bayesian estima-
tion and prediction for the beta-binomial model. Journal of
Statistical Computation and Simulation, 63:73?91.
David D. Lewis. 1998. Naive (Bayes) at forty: The indepen-
dence assumption in information retrieval. In 10th European
Conference on Machine Learning, pages 4?15.
Stephen A. Lowe. 1999. The beta-binomial mixture model for
word frequencies in documents with applications to informa-
tion retrieval. In 6th European Conference on Speech Com-
munication and Technology, pages 2443?2446.
Andrew McCallum and Kamal Nigam. 1998. A comparison
of event models for naive Bayes text classification. In AAAI
Workshop on Learning for Text Categorization, pages 41?48.
Andrew Kachites McCallum. 1996. Bow: A toolkit for sta-
tistical language modeling, text retrieval, classification and
clustering. http://www.cs.cmu.edu/~mccallum/bow/.
Frederick Mosteller and David L. Wallace. 1984. Applied
Bayesian and Classical Inference: The Case of The Fed-
eralist Papers. Springer, New York, NY, second edition.
Yudi Pawitan. 2001. In All Likelihood: Statistical Modelling
and Inference Using Likelihood. Oxford University Press,
New York, NY.
Information Extraction from Voicemail Transcripts
Martin Jansche
Department of Linguistics
The Ohio State University
Columbus, OH 43210, USA
jansche.1@osu.edu
Steven P. Abney
AT&T Labs ? Research
180 Park Avenue
Florham Park, NJ 07932, USA
abney@research.att.com
Abstract
Voicemail is not like email. Even such ba-
sic information as the name of the caller/
sender or a phone number for returning
calls is not represented explicitly and must
be obtained from message transcripts or
other sources. We discuss techniques for
doing this and the challenges these tasks
present.
1 Introduction
When you?re away from the phone and someone
takes a message for you, at the very least you?d ex-
pect to be told who called and whether they left a
number for you to call back. If the same call is
picked up by a voicemail system, even such basic in-
formation like the name of the caller and their phone
number may not be directly available, forcing one to
listen to the entire message1 in the worst case. By
contrast, information about the sender of an email
message has always been explicitly represented in
the message headers, starting with early standard-
ization attempts (Bhushan et al, 1973) and including
the two decade old current standard (Crocker, 1982).
Applications that aim to present voicemail messages
through an email-like interface ? take as an example
the idea of a ?uniform inbox? presentation of email,
voicemail, and other kinds of messages2 ? must deal
with the problem of how to obtain information anal-
ogous to what would be contained in email headers.
1The average message length in the corpus described below
is 36 seconds.
2Similar issues arise with FAX messages, for example.
Here we will discuss one way of addressing this
problem, treating it exclusively as the task of extract-
ing relevant information from voicemail transcripts.
In practice, e.g. in the context of a sophisticated
voicemail front-end (Hirschberg et al, 2001) that is
tightly integrated with an organization-wide voice-
mail system and private branch exchange (PBX), ad-
ditional sources of information may be available: the
voicemail system or the PBX might provide infor-
mation about the originating station of a call, and
speaker identification can be used to match a caller?s
voice against models of known callers (Rosenberg
et al, 2001). Restricting our attention to voicemail
transcripts means that our focus and goals are sim-
ilar to those of Huang et al (2001), but the features
and techniques we use are very different.
While the present task may seem broadly similar
to named entity extraction from broadcast news (Go-
toh and Renals, 2000), it is quite distinct from the
latter: first, we are only interested in a small subset
of the named entities; and second, the structure of
the voicemail transcripts in our corpus is very dif-
ferent from broadcast news and certain aspects of
this structure can be exploited for extracting caller
names.
Huang et al (2001) discuss three approaches:
hand-crafted rules; grammatical inference of subse-
quential transducers; and log-linear classifiers with
bigram and trigram features used as taggers (Ratna-
parkhi, 1996). While the latter are reported to yield
the best overall performance, the hand-crafted rules
resulted in higher recall. Our phone number extrac-
tor is based on a two-phase procedure that employs a
small hand-crafted component to propose candidate
phrases, followed by a classifier that retains the de-
sirable candidates. This allows for more or less inde-
                                            Association for Computational Linguistics.
                    Language Processing (EMNLP), Philadelphia, July 2002, pp. 320-327.
                         Proceedings of the Conference on Empirical Methods in Natural
pendent optimization of recall and precision, some-
what similar to the PNrule classifier learner (Agar-
wal and Joshi, 2001; Joshi et al, 2001). We shall see
that hand-crafted rules achieve very good recall, just
as Huang et al (2001) had observed, and the prun-
ing phase successfully eliminates most undesirable
candidates without affecting recall too much. Over-
all performance of our method is better than if we
employ a log-linear model with trigram features.
The success of the method proposed here is also
due to the use of a rich set of features for candi-
date classification. For example, the majority of
phone numbers in voicemail messages has either
four, seven, or ten digits, whereas nine digits would
indicate a social security number. In our two-phase
approach it is straightforward for the second-phase
classifier to take the length of a candidate phone
number into account. On the other hand, standard
named entity taggers that use trigram features do not
exploit this information, and doing so would entail
significant changes to the underlying models and pa-
rameter estimation procedures.
The rest of this paper is organized as follows. A
brief overview of the data we used in ?2 is followed
by a discussion of methods for extracting two kinds
of caller information in ?3. Methods for extracting
telephone numbers are discussed in ?4, and ?5 sum-
marizes and concludes.
2 Voicemail Corpus
Development and evaluation was done using a pro-
prietary corpus of almost 10,000 voicemail mes-
sages that had been manually transcribed and
marked up for content. Some more details about
this corpus can be found in (Bacchiani, 2001). The
relevant content labeling is perhaps best illustrated
with an (anonymized) excerpt form a typical mes-
sage transcript:
?greeting? hi Jane ?/greeting? ?caller? this
is Pat Caller ?/caller? I just wanted to I
know you?ve probably seen this or maybe
you already know about it . . . so if you
could give me a call at ?telno? one two
three four five ?/telno? when you get the
message I?d like to chat about it hope
things are well with you ?closing? talk to
you soon ?/closing?
This transcript is representative of a large class of
messages that start out with a short greeting fol-
lowed by a phrase that identifies the caller either
by name as above or by other means (?hi, it?s me?).
A phone number may be mentioned as part of the
caller?s self-identification, or is often mentioned
near the end of the message. It may seem natu-
ral and obvious that voicemail messages should be
structured in this way, and this prototypical struc-
ture can therefore be exploited for purposes of lo-
cating caller information or deciding whether a digit
string constitutes a phone number. The next sections
discuss this in more detail.
The corpus was partitioned into two subsets, with
8120 messages used for development and 1869 for
evaluation. Approximately 5% of all messages are
empty. Empty messages were not discarded from
the evaluation set since they constitute realistic sam-
ples that the information extraction component has
to cope with. The development set contains 7686
non-empty messages.
3 Caller Information
Of the non-empty messages in the development set,
7065 (92%) transcripts contain a marked-up caller
phrase. Of those, 6731 messages mention a name in
the caller phrase. Extracting caller information can
be broken down into two slightly different tasks: we
might want to reproduce the existing caller annota-
tion as closely as possible, producing caller phrases
like ?this is Pat Caller? or ?it?s me?; or we might only
be interested in caller names such as ?Pat Caller? in
our above example. We make use of the fact that
for the overwhelming majority of cases, the caller?s
self-identification occurs somewhere near the begin-
ning of the message.
3.1 Caller Phrases
Most caller phrases tend to start one or two words
into the message. This is because they are typi-
cally preceded by a one-word (?hi?) or two-word
(?hi Jane?) greeting. Figure 1 shows the empiri-
cal distribution of the beginning of the caller phrase
across the 7065 applicable transcripts in the devel-
opment data. As can be seen, more than 97% of
all caller phrases start somewhere between one and
seven words from the beginning of the message,
though in one extreme case the start of the caller
phrase occurred 135 words into the message.
0
10
20
30
40
50
60
70
80
90
100
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
observed max.: 135
entropy: 1.48 bits
density
cumulative
Figure 1: Empirical probability of a caller phrase
starting x words into the message
These observations strongly suggest that when ex-
tracting caller phrases, positional cues should be
taken into account. This is good news, especially
since intrinsic features of the caller phrase may not
be as reliable: a caller phrase is likely to contain
names that are problematic for an automatic speech
recognizer. While this is less of a problem when
evaluating on manual transcriptions, the experience
reported in (Huang et al, 2001) suggests that the
relatively high error rate of speech recognizers may
negatively affect performance of caller name ex-
traction on automatically generated transcripts. We
therefore avoid using anything but a small number
of greetings and commonly occurring words like
?hi?, ?this?, ?is? etc. and a small number of common
first names for extracting caller phrases and use po-
sitional information in addition to word-based fea-
tures.
We locate caller phrases by first identifying their
start position in the message and then predicting
the length of the phrase. The empirical distribu-
tion of caller phrase lengths in the development data
is shown in Figure 2. Most caller phrases are be-
tween two and four words long (?it?s Pat?, ?this is
Pat Caller?) and there are moderately good lexical
indicators that signal the end of a caller phrase (?I?,
?could?, ?please?, etc.). Again, we avoid the use of
names as features and rely on a small set of fea-
tures based on common words, in addition to phrase
length, for predicting the length of the caller phrase.
0
10
20
30
40
50
60
70
80
90
100
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
observed max.: 47
entropy: 3.11 bits
density
cumulative
Figure 2: Empirical probability of a caller phrase
being x words long
We have thus identified two classes of features
that allow us to predict the start of the caller phrase
relative to the beginning of the message, as well as
the end of the caller phrase relative to its start. Since
we are dealing with discrete word indices in both
cases, we treat this as a classification task, rather
than a regression task. A large number of classifier
learners can be used to automatically infer classifiers
for the two subtasks at hand. We chose a decision
tree learner for convenience and note that this choice
does not affect the overall results nearly as much as
modifying our feature inventory.
Since a direct comparison to the log-linear named
entity tagger described in (Huang et al, 2001) (we
refer to this approach as HZP log-linear below) is
not possible due to the use of different corpora and
annotation standards, we applied a similar named
entity tagger based on a log-linear model with tri-
gram features to our data (we refer to this approach
as Col log-linear as the tagger was provided by
Michael Collins). Table 1 summarizes precision (P),
recall (R), and F-measure (F) for three approaches
evaluated on manual transcriptions: row HZP log-
linear repeats the results of the best model from
(Huang et al, 2001); row Col log-linear contains
the results we obtained using a similar named entity
tagger on our own data; and row JA classifiers shows
the performance of the classifier method proposed in
this section.
Like Huang et al (2001), we count a proposed
caller phrase as correct if and only if it matches
the annotation of the evaluation data perfectly. The
numbers could be made to look better by using con-
tainment as the evaluation criterion, i.e., we would
count a proposed phrase as correct if it contained an
actual phrase plus perhaps some additional material.
While this may be more useful in practice (see be-
low), it is not the objective that was maximized dur-
ing training, and so we prefer the stricter criterion
for evaluation on previously annotated transcripts.
P R F
HZP log-linear .89 .80 .84
Col log-linear .83 .78 .81
JA classifiers .73 .68 .71
Table 1: Performance of caller phrase extraction
(manual transcriptions)
While the results for the approach proposed
here appear clearly worse than those reported by
Huang et al (2001), we hasten to point out that this
is most likely not due to any difference in the cor-
pora that were used. This is corroborated by the fact
that we were able to obtain performance much closer
to that of the best, finely tuned log-linear model from
(Huang et al, 2001) by using a generic named entity
tagger that was not adapted in any way to the par-
ticular task at hand. The log-linear taggers employ
n-gram features based on family names and other
particular aspects of the development data that do
not necessarily generalize to other settings, where
the family names of the callers may be different or
may not be transcribed properly. In fact, it seems
rather likely that the log-linear models and the fea-
tures they employ over-fit the training data.
This becomes clearer when one evaluates on un-
seen transcripts produced by an automatic speech
recognizer (ASR),3 as summarized in Table 2. Rows
HZP strict and HZP containment repeat the figures
for the best model from (Huang et al, 2001) when
evaluated on automatic transcriptions. The differ-
ence is that HZP strict uses the strict evaluation cri-
terion described above, whereas HZP containment
uses the weaker criterion of containment, i.e., an
extracted phrase counts as correct if it contains ex-
actly one whole actual phrase. Row JA containment
summarizes the performance of our approach when
3An automatic transcription is the single best word hypoth-
esis of the ASR for a given voicemail message.
evaluated on 101 unseen automatically transcribed
messages. Since we did not have any labeled au-
tomatic transcriptions available to compare with the
predicted caller phrase labels using the strict crite-
rion, we only report results based on the weaker
criterion of containment. In fact, we count caller
phrases as correct as long as they contain the full
name of the caller, since this is the common denom-
inator in the otherwise somewhat heterogeneous la-
beling of our training corpus; more on this issue in
the next section.
P R F
HZP strict .24 .16 .19
HZP containment .73 .41 .52
JA containment .74 .66 .70
Table 2: Performance of caller phrase extraction (au-
tomatic transcriptions)
The difference between the approach in (Huang et
al., 2001) and ours may be partly due to the perfor-
mance of the ASR components: Huang et al (2001)
report a word error rate of ?about 35%?, whereas
we used a recognizer (Bacchiani, 2001) with a word
error rate of only 23%. Still, the reduced perfor-
mance of the HZP model on ASR transcripts com-
pared with manual transcripts is points toward over-
fitting, or reliance on features that do not generalize
to ASR transcripts. Our main approach, on the other
hand, uses classifiers that are extremely knowledge-
poor in comparison with the many features of the
log-linear models for the various named entity tag-
gers, employing no more than a few dozen categori-
cal features.
3.2 Caller Names
Extracting an entire caller phrase like ?this is Pat
Caller? may not be all that relevant in practice: the
prefix ?this is? does not provide much useful infor-
mation, so simply extracting the name of the caller
should suffice. This is more or less a problem with
the annotation standard used for marking up voice-
mail transcripts. We decided to test the effects of
changing that standard post hoc. This was relatively
easy to do, since proper names are capitalized in
the message transcripts. We heuristically identify
caller names as the leftmost longest contiguous sub-
sequence of capitalized words inside a marked-up
caller phrase. This leaves us with 6731 messages
with caller names in our development data.4
As we did for caller phrases, we briefly examine
the distributions of the start position of caller names
(see Figure 3) as well as their lengths (see Figure 4).
Comparing the entropies of the empirical distribu-
tions with the corresponding ones for caller phrases
suggests that we might be dealing with a simpler
extraction task here. The entropy of the empirical
name length distribution is not much more than one
bit, since predicting the length of a caller name is
mostly a question of deciding whether a first name
or full name was mentioned.
0
10
20
30
40
50
60
70
80
90
100
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
observed max.: 138
entropy: 2.20 bits
density
cumulative
Figure 3: Empirical probability of a caller name
starting x words into the message
0
10
20
30
40
50
60
70
80
90
100
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
observed max.: 8
entropy: 1.17 bits
density
cumulative
Figure 4: Empirical probability of a caller name be-
ing x words long
4The vast majority of messages that do not mention a name
as part of their caller phrase employ the caller phrase ?it?s me?,
which would be easy to detect and treat separately.
The performance comparison in Table 3 shows
that we are in fact dealing with a simpler task. No-
tice however that our method has not changed at all.
We still use one classifier to predict the beginning
of the caller name and a second classifier to predict
its length, with the same small set of lexical features
that do not include any names other than a handful
of common first names.
P R F
phrase .730 .684 .706
name .860 .871 .865
Table 3: Caller phrase vs. name extraction (manual
transcriptions)
4 Phone Numbers
The development data contain 5303 marked-up
phone numbers, for an average of almost 0.7 phone
numbers per non-empty message. These phone
numbers fall into the following categories based on
their realization:
? 4472 (84%) consist exclusively of spoken num-
bers
? 679 (13%) consist of spoken numbers and the
words ?area?, ?code?, and ?extension?
? 152 (3%) have additional material, due to cor-
rections, hesitations, fragments, and question-
able markup
Note that phone numbers in the North American
Numbering Plan are either ten or seven digits long,
depending on whether the Numbering Plan Area
code is included or not. Two other frequent lengths
for phone numbers in the development data are four
(for internal lines) and, to a lesser extent, eleven
(when the long distance dialing prefix is included,
as in ?one eight hundred . . . ?).
This allows us to formulate the following baseline
approach: find all maximal substrings consisting of
spoken digits (?zero? through ?nine?) and keep those
of length four, seven, and ten. Simple as it may
seem, this approach (which we call digits below)
performs surprisingly well. Its precision is more
than 78%, partly because in our corpus there do not
occur many seven or ten digit numbers that are not
phone numbers.
Named entity taggers based on conditional mod-
els with trigram features are not particularly suited
for this task. The reason is that trigrams do not pro-
vide enough history to allow the tagger to judge the
length of a proposed phone number: it inserts begin-
ning and end tags without being able to tell how far
apart they are. Data sparseness is another problem,
since we are dealing with 1000 distinct trigrams over
digits alone, so a different event model that replaces
all spoken digits with the same representative token
might be better suited, also because it avoids over-
fitting issues like accidentally learning area codes
and other number patterns that are frequent in the
development data.
However, there is a more serious problem. Even
if the distance between the start and end tags that a
named entity tagger predicts could be taken into ac-
count, this would not help with all spoken renditions
of phone numbers. For example, ?327-1025? could
be read aloud using only six words (?three two seven
ten twenty five?), and might be incorrectly rejected
because it appears to be of a length that is not very
common for phone numbers.
We therefore approach the phone number extrac-
tion task differently, using a two-phase procedure.
In the first phase we use a hand-crafted grammar to
propose candidate phone numbers. This avoids all
of the problems mentioned so far, yet the complex-
ity of the task remains manageable because of the
rather simple structure of most phone numbers in
our development data noted above. The advantage
is that it allows us to simultaneously convert spo-
ken digits and numbers to a numeric representation,
whose length can then be used as an important fea-
ture for deciding whether to keep or throw away a
candidate. Note that such a conversion process is
desirable in any case, since a text-based application
would presumably want to present digit strings like
?327-1025? to a user, rather than ?three two seven
ten twenty five?. This conversion step is not entirely
trivial, though: for example, one might transcribe
the spoken words ?three hundred fourteen ninety
nine? as either ?300-1499? or ?314.99? depending on
whether they are preceded by ?call me back at? vs. ?I
can sell it to you for?, for example. But since we are
only interested in finding phone numbers, the extrac-
tion component can treat all candidates it proposes
as if they were phone numbers.
Adjustments of the hand-crafted grammar were
only made in order to increase recall on the devel-
opment data. The grammar should locate as many
actual phone numbers in the development corpus as
possible, but was free to also propose spurious can-
didates that did not correspond to marked-up phone
numbers. While it has recently been argued that
such separate optimization of recall and precision is
generally desirable for certain learning tasks (Agar-
wal and Joshi, 2001; Joshi et al, 2001), the main
advantage in connection with hand-crafted compo-
nents is simplified development. Since we noted
above that 97% of all phone numbers in our devel-
opment data are expressed fairly straightforwardly
in terms of digits, numbers, and a few other words
particular to the phone number domain, we might
expect to achieve recall figures close to 97% without
doing anything special to deal with the remaining
3% of difficult cases. It was very easy to achieve this
recall figure on the development data, while the ratio
of proposed phone numbers to actual phone numbers
was about 3.2 at worst.5
A second phase is now charged with the task of
weeding through the set of candidates proposed dur-
ing the first phase, retaining those that correspond to
actual phone numbers. This is a simple binary clas-
sification task, and again many different techniques
can be applied. As a baseline we use a classifier
that accepts any candidate of length four or more
(now measured in terms of numeric digits, rather
than spoken words), and rejects candidates of length
three and less. Without this simple step (which we
refer to as prune below), the precision of our hand-
crafted extraction grammar is only around 30%, but
by pruning away candidate phone numbers shorter
than four digits precision almost doubles while re-
call is unaffected.
We again used a decision tree learner to automat-
ically infer a classifier for the second phase. The
features we made available to the learner were the
length of the phone number in numeric digits, its
5It would of course be trivial to achieve 100% recall by ex-
tracting all possible substrings of a transcript. The fact that our
grammar extracts only about three times as many phrases as
needed is evidence that it falls within the reasonable subset of
possible extraction procedures.
distance from the end of the message, and a small
number of lexical cues in the surrounding context of
a candidate number (?call?, ?number?, etc.). This ap-
proach (which we call classify below) increases the
precision of the combined two steps to acceptable
levels without hurting recall too much.
A comparison of performance results is presented
in Table 4. Rows HZP rules and HZP log-linear re-
fer to the rule-based baseline and the best log-linear
model of (Huang et al, 2001) and the figures are
simply taken from that paper; row Col log-linear
refers to the same named entity tagger we used in the
previous section and is included for comparison with
the HZP models; row JA digits refers to the simple
baseline where we extract strings of spoken digits of
plausible lengths. Our main results appear in the re-
maining rows. The performance of our hand-crafted
extraction grammar (in row JA extract) was about
what we had seen on the development data before,
with recall being as high as one could reasonably ex-
pect. As mentioned above, using a simple pruning
step in the second phase (see JA extract + prune)
results in a doubling of precision and leaves recall
essentially unaffected (a single fragmentary phone
number was wrongly excluded). Finally, if we use
a decision tree classifier in the second phase, we
can achieve extremely high precision with a minimal
impact on recall. Our two-phase procedure outper-
forms all other methods we considered.
P R F
HZP rules .81 .83 .82
HZP log-linear .90 .83 .86
Col log-linear .88 .93 .91
JA digits .78 .70 .74
JA extract .30 .96 .45
JA extract + prune .59 .96 .73
JA extract + classify .94 .94 .94
Table 4: Performance of phone number extraction
(manual transcriptions)
We evaluated the performance of our best models
on the same 101 unseen ASR transcripts used above
in the evaluation of the caller phrase extraction. The
results are summarized in Table 5, which also re-
peats the best results from (Huang et al, 2001), us-
ing the same terminology as earlier: rows HZP strict
and HZP containment refer to the best model from
(Huang et al, 2001) ? corresponding to row HZP
log-linear in Table 4 ? when evaluated using the
strict criterion and containment, respectively; and
row JA containment refers to our own best model
? corresponding to row JA extract + classify in Ta-
ble 4.
P R F
HZP strict .56 .52 .54
HZP containment .85 .79 .82
JA containment .95 .94 .95
Table 5: Performance of phone number extraction
(automatic transcriptions)
It is not very plausible that the differences be-
tween the approaches in Table 5 would be due to
a difference in the performance of the ASR compo-
nents that generated the message transcripts. From
inspecting our own data it is clear that ASR mistakes
inside phone numbers are virtually absent, and we
would expect the same to hold even of an automatic
recognizer with an overall much higher word error
rate. Also, for most phone numbers the labeling is
uncontroversial, so we expect the corpora used by
Huang et al (2001) and ourselves to be extremely
similar in terms of mark-up of phone numbers. So
the observed performance difference is most likely
due to the difference in extraction methods.
5 Conclusion and Outlook
The novel contributions of this paper can be summa-
rized as follows:
? We demonstrated empirically that positional
cues can be an important source of information
for locating caller names and phrases.
? We showed that good performance on the task
of extracting caller information can be achieved
using a very small inventory of lexical and po-
sitional features.
? We argued that for extracting telephone num-
bers it is extremely useful to take the length
of their numeric representation into account.
Our grammar-based extractor translates spoken
numbers into such a numeric representation.
? Our two-phase approach allows us to efficiently
develop a simple extraction grammar for which
the only requirement is high recall. This places
less of a burden on the grammar developers
than having to write an accurate set of rules like
the baseline of (Huang et al, 2001).
? The combined performance of our simple ex-
traction grammar and the second-phase clas-
sifier exceeded the performance of all other
methods, including the current state of the art
(Huang et al, 2001).
Our results point towards approaches that use a
small inventory of features that have been tailored
to specific tasks. Generic methods like the named
entity tagger used by Huang et al (2001) may not
be the best tools for particular tasks; in fact, we do
not expect the bigram and trigram features used by
such taggers to be sufficient for accurately extract-
ing phone numbers. We also believe that using all
available lexical information for extracting caller in-
formation can easily lead to over-fitting, which can
partly be avoid by not relying on names being tran-
scribed correctly by an ASR component.
In practice, determining the identity of a caller
might have to take many diverse sources of infor-
mation into account. The self-identification of a
caller and the phone numbers mentioned in the same
message are not uncorrelated, since there is usually
only a small number of ways to reach any particular
caller. In an application we might therefore try to use
a combination of speaker identification (Rosenberg
et al, 2001), caller name extraction, and recognized
phone numbers to establish the identity of the caller.
An investigation of how to combine these sources of
information is left for future research.
Acknowledgements
We would like to thank Michiel Bacchiani, Michael
Collins, Julia Hirschberg, and the SCANMail group
at AT&T Labs. Special thanks to Michiel Bacchiani
for help with ASR transcripts and to Michael Collins
for letting us use his named entity tagger.
References
Ramesh C. Agarwal and Mahesh V. Joshi. 2001. PNrule:
A new classification framework in data mining (A case
study in network intrusion detection). In First SIAM
International Conference on Data Mining, Chicago,
IL.
Michiel Bacchiani. 2001. Automatic transcription of
voicemail at AT&T. In International Conference on
Acoustics, Speech, and Signal Processing, Salt Lake
City, UT.
Abhay Bhushan, Ken Pogran, Ray Tomlinson, and Jim
White. 1973. Standardizing network mail headers.
Internet RFC 561.
David H. Crocker. 1982. Standard for the format of
ARPA internet text messages. Internet RFC 822,
STD 11.
Yoshihiko Gotoh and Steve Renals. 2000. Informa-
tion extraction from broadcast news. Philosophical
Transactions of the Royal Society of London, Series
A, 358:1295?1310.
Julia Hirschberg, Michiel Bacchiani, Don Hindle, Phil
Isenhour, Aaron Rosenberg, Litza Stark, Larry Stead,
Steve Whittaker, and Gary Zamchick. 2001. SCAN-
Mail: Browsing and searching speech data by content.
In 7th European Conference on Speech Communica-
tion and Technology, Aalborg, Denmark.
Jing Huang, Geoffrey Zweig, and Mukund Padmanab-
han. 2001. Information extraction from voicemail. In
39th Annual Meeting of the Association for Computa-
tional Linguistics, Toulouse, France.
Mahesh V. Joshi, Ramesh C. Agarwal, and Vipin Ku-
mar. 2001. Mining needles in a haystack: Classify-
ing rare classes via two-phase rule induction. In ACM
SIGMOD International Conference on Management of
Data, Santa Barbara, CA.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Empirical Methods in
Natural Language Processing, Philadelphia, PA.
Aaron Rosenberg, Julia Hirschberg, Michiel Bacchiani,
S. Parthasarathy, Philip Isenhour, and Larry Stead.
2001. Caller identification for the SCANMail voice-
mail browser. In 7th European Conference on Speech
Communication and Technology, Aalborg, Denmark.
Named Entity Extraction with Conditional Markov Models and Classifiers
Martin Jansche
Department of Linguistics
The Ohio State University
Columbus, OH 43210, USA
jansche.1@osu.edu
1 Introduction
Our approach to multilingual named entity (NE)
recognition in the context of the CoNLL Shared
Task consists of the following ingredients:
Feature engineering A human expert (though not
necessarily a language expert) determines rel-
evant features to be used to determine whether
or not a word is part of a named entity.
Extraction In a first phase a conditional Markov
model extracts candidate NE phrases, but does
not classify them yet into LOC, ORG, etc.
Classification In the second phase a classifier looks
at candidate phrases proposed by the extractor
in their sentential context and labels them as
LOC, ORG, etc.
2 Feature engineering
Languages differ widely in the conventions they use
to signal named entities. Spanish, French, and En-
glish use the case distinction of the modern Roman
alphabet to indicate proper names, and upper case
is a fairly good indicator of a proper name. The
situation is quite different in German, where upper
case is a poor cue. In traditional Chinese scholarly
works, certain proper names are indicated by under-
lining, and without that form of annotation locat-
ing a proper name would seem quite challenging.
In light of the diversity found across languages and
orthographic conventions, it is unclear whether any
effective multilingual named entity extraction sys-
tem will ever be built that does not rely on human
expertise for customizing it to a particular language
and domain.
Since we started by building a Spanish system
without knowing what other language it would have
to be applied to, the features we ended up using are
all rather simple and generic in nature. No language
experts were consulted. In the extraction compo-
nent, we look at the orthographic string of a word
with accents removed (since there are some incon-
sistencies regarding the presence or absence of ac-
cents), and we determine whether it starts with an
upper case character. For the classification compo-
nent we look at entire candidate phrases, and de-
termine the length of the phrase, its position in a
sentence, the immediately surrounding words, and
what words occur within the phrase. For a word
inside the phrase we determine whether it starts
with and upper or lower case character (or neither),
whether it contains any upper case or lower case
characters (or neither), and we also use the entire
orthographic string with accents removed.
Fortunately, these features carry over fairly well
to Dutch, the second language of the Shared Task,
and may also have been sufficient for French or En-
glish, but would probably fall short for German.
Needless to say, radically different orthographic
systems may require entirely different approaches,
so the multilingual scope of our proposal is fairly
limited.
3 Extraction
The extraction component discards the specific la-
bels LOC, ORG, etc. (from now on we refer to these
as sort labels) and only predicts whether a token
is at the beginning of (B), inside (I), or outside
(O) a phrase (we will call these bare labels phrase
tags). While this move is not unproblematic, we de-
termined empirically that overall performance was
higher using only bare phrase tags without sort la-
bels, compared with a single-phase approach that
tries to predict phrase tags and sort labels together
using a single (conditional or joint) Markov model.
The underlying rationale was to enable the extrac-
tor to concentrate on any morpho-syntactic regular-
ities across different sorts of phrases without having
to determine the sort label yet, which may require
more context: for example, Spanish named entities
can contain de, and this is the case across all sorts;
or certain names like Holanda are ambiguous be-
tween LOC and ORG depending on whether they re-
fer to countries on the one hand, or their govern-
ments or national soccer teams on the other. In light
of this it makes sense to delay the assignment of
sort labels and concentrate on extracting candidate
phrases first.
Our extraction approach uses conditional Markov
models, and we shall illustrate it using a first order
model. Generalizations to higher order models are
straightforwardly possible. The problem we are try-
ing to solve is this: we want to find a sequence of
phrase tags t given a sequence of words w. We find
the optimal t
 
as
t
 
arg max
t
P

t  w 

argmax
t
G

w  t 
W

w 

where the conditional model P is expressed in terms
of a joint generative model G of tags and words, and
a language model W .
Since t and w have the same length n, we regard
the training data as a sequence of pairs, rather than
a pair of sequences (the two representations are iso-
morphic via a zip operation familiar to Python or
Haskell programmers), and decompose the genera-
tive model G using a first order Markov assumption:
G

w  t 

S

w1  t1 
n
?
i  2
G1

wi  ti  wi  1  ti  1 
	
Doing the same for W and using a designated start
event

w0  t0  instead of the start distribution S we
obtain:
P

t  w 

n
?
i  1
G1

wi  ti  wi  1  ti  1 
W1

wi  wi  1 
	
We further decompose the conditional distribution
G1 as follows:
G1

wi  ti  wi  1  ti  1 

T

ti wi  wi  1  ti  1  U

wi wi  1  ti  1 
	
In addition to the first order assumption above,
the only other assumption we make is that
U

wi wi  1  ti  1 

U

wi  wi  1  , which allows us to
conclude that U

W1, and so our conditional se-
quence model simplifies to
P

t  w 

n
?
i  1
T

ti wi  wi  1  ti  1 
	
This is starting to look familiar: T is a conditional
distribution over a finite set of phrase tags, so in
principle any probabilistic classifier that uses (fea-
tures derived from) the variables that T is condi-
tioned on could be substituted in its place. Ap-
proaches like this have apparently been used infor-
mally in practice for some time, perhaps with a clas-
sifier instead of T that does not necessarily return a
proper probability distribution over tags. Probabil-
ity models that predict the next tag conditional on
the current tag and an observed word have been crit-
icized for a weakness known as the Label Bias Prob-
lem (Lafferty et al, 2001); on the other hand, the
practical effectiveness of approaches like the one
proposed here for a very similar task was demon-
strated by Punyakanok and Roth (2001).
Finding the optimal tag sequence for a given se-
quence of words can be done in the usual fashion
using Viterbi decoding. Training is fully supervised,
since we have labeled training data, but could in
principle be extended to the (partly) unsupervised
case. We only implemented supervised training,
which is mostly trivial. When using a simple con-
ditional next-tag model it is especially important to
have good estimates of T

ti  wi  wi  1  ti  1  . We use a
strategy of backing off to less and less informative
contexts. In the worst case, T

ti  ti  1  can be esti-
mated very reliably from the training data (in fact,
good estimates for much longer tag histories can be
found). When conditioning on words, the situation
is rather different. For example, we see relatively
few events of the form

wi  wi  1  ti  1  in the training
data (out of the space of all possible events of that
form), and so we may back off to  wi  ui  1  ti  1  ,
where ui  1 is binary valued and indicates whether
the preceding word started with an upper case let-
ter. We have not determined an optimal back-off
strategy, and for now we use an intuitively plausi-
ble strategy that tries to use as much conditioning
information as possible and backs off to strictly less
informative histories. In all cases it is important to
always condition on the preceding tag ti  1, or else
we would be left with no information about likely
tag sequences.
We used first and second order models of this
form and manually searched for good parameter
settings on a held-out portion of the training data.
It turns out that the second order model performs
about the same as the first order model, but is
at a disadvantage because of data sparseness.
Therefore we only consider first order models
in the rest of this paper. The performance of
the first order model on the development data
sets is summarized in Table 1. Note that these
figures can be obtained for any system by first
piping its output through sed using the command
s/-\(LOC\|MISC\|ORG\|PER\)/-FOO/g.
As will become clearer below, within each lan-
guage it so happens that the extraction component
performs better than the classification component,
i.e. for now the performance bottleneck is the
classification component.
Spanish dev. precision recall F?  1
overall 87.60% 86.86% 87.23
Dutch devel. precision recall F?  1
overall 85.84% 84.55% 85.19
Table 1: Extraction results obtained for the devel-
opment data sets for the two languages used in this
shared task.
4 Classification
The candidate phrases proposed by the extraction
component are subsequently annotated with sort la-
bels. The main advantage of dividing up the task
this way is that we can take a lot more context into
account for classifying phrases. For example, fea-
tures that may be relevant now include: the length
of the phrase, the first/last k words in the phrase,
the position of the phrase in the sentence, whether
the words fu?tbol or liga were mentioned in the same
sentence, etc. Such features would be awkward
to incorporate into a single-phase approach using
a Markov model to predict phrase tags at the same
time as sort labels.
We chose a fairly standard independent feature
(a.k.a. ?naive Bayes?) model, mostly as a matter of
convenience. Obviously any other classifier frame-
work could have been used instead. For both lan-
guages we use as features the length of the phrase,
its distance from the start of the sentence, the iden-
tity of the words inside the phrase viewing it as
a set of words (i.e., discarding positional informa-
tion), the identity and other properties (including
whether a word starts with an upper/lower case let-
ter) of the first k and last k words in the phrase, and
the identity and other properties of the word(s) pre-
ceding and following the phrase. The optimal pa-
rameter settings differ for Spanish and Dutch. For
example, in Spanish the identities of the first k

6
words is very important for classification perfor-
mance, whereas long preceding or trailing contexts
do not help much, if at all. For Dutch, the identi-
ties of words inside the phrase is less helpful (k  3
is optimal), and more preceding and trailing con-
text has to be used. In addition, knowing whether
a sentence (or, ideally, a news article) is about soc-
cer was helpful for Spanish. A feature that tests for
the presence of fu?tbol and a few semantically related
words is the only aspect of the classification com-
ponent that is particular to one language. Other lan-
guage specific information, e.g. names of Spanish
provinces, did not turn out to be useful.
Table 2 shows performance figures for the clas-
sification component on the raw development data.
Equivalently one can think of these results as if we
had applied our classifiers to the output of a perfect
extraction component that does not make any mis-
takes. We can already see for Spanish that perfor-
mance is lowest for the sort MISC, which does not
seem very homogeneous, and may perhaps best be
chosen by default if no other class applies. Trying
to predict MISC directly seems to be a misguided ef-
fort. This will become even clearer below when we
look at the overall performance of our approach.
Spanish dev. precision recall F?  1
LOC 71.37% 87.82% 78.74
MISC 70.80% 79.55% 74.92
ORG 87.95% 78.18% 82.78
PER 91.05% 84.12% 87.45
overall 82.17% 82.17% 82.17
Dutch devel. precision recall F?  1
LOC 75.57% 70.17% 72.77
MISC 79.68% 80.43% 80.05
ORG 82.30% 66.42% 73.51
PER 70.21% 85.88% 77.26
overall 76.39% 76.39% 76.39
Table 2: Classification results obtained for the de-
velopment data sets for the two languages used in
this shared task.
5 Putting it all together
A theoretical problem with our task decomposition
is how to train the classifiers used in the second
phase. What they will eventually see as input is
the output of the extraction component, which may
contain mistakes, e.g., cases where the beginning or
end of a phrase was mispredicted. Since we want
to build and refine the classification component in-
dependently of the extraction component, we have
to train the classifiers on the phrases in the labeled
training data. It is not clear a priori that this kind
of independent development comes without a per-
formance penalty, since we may have forgotten to
show the second-phase classifiers examples of trun-
cated or badly mangled phrases that were produced
because of imperfections of the extraction compo-
nent which makes up the first phase of our approach.
Based on the independence assumption behind the
task decomposition we would expect the overall
performance on the Spanish development data set
to be
0 	 8723  0 	 8217  0 	 7168 	
As we can see from the actual results in Table 3, this
is not very far from the observed performance. We
conclude that independent development of the two
components did not impact overall performance.
6 Conclusion
We presented a simple, knowledge-poor named en-
tity recognizer using standard components. Our
decomposition into extraction and classification
phases was motivated by the common syntactic reg-
ularities and the ambiguous status of some named
entities. We have shown that the conditional next-
tag model used for extraction is not unprincipled
(a criticism brought forward by McCallum et al
(2000) against next-tag classifiers that do not out-
put probabilities), but arises naturally from a condi-
tional sequence model and plausible independence
assumptions. This extraction model achieves fairly
high accuracy (and just as observed by Punyakanok
and Roth (2001) it outperforms a joint genera-
tive Markov model). A separate classification step
makes it easy to use sentence-level features and
large amounts of contexts. Such features would be
difficult to integrated into standard models, the ma-
jor exception being conditional random fields (Laf-
ferty et al, 2001), compared to which the approach
proposed here is much simpler.
References
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistics models for segmenting and labeling se-
quence data. In ICML-01, pages 282?289.
Spanish dev. precision recall F?  1
LOC 65.76% 76.45% 70.70
MISC 45.56% 54.16% 49.49
ORG 77.40% 70.12% 73.58
PER 85.01% 76.60% 80.59
overall 72.35% 71.74% 72.04
Spanish test precision recall F?  1
LOC 77.18% 73.34% 75.21
MISC 44.47% 50.88% 47.46
ORG 76.75% 77.57% 77.16
PER 80.20% 77.69% 78.92
overall 74.03% 73.76% 73.89
Dutch devel. precision recall F?  1
LOC 69.87% 67.23% 68.52
MISC 64.69% 62.87% 63.77
ORG 69.65% 56.11% 62.15
PER 63.93% 75.85% 69.38
overall 66.42% 65.43% 65.92
Dutch test precision recall F?  1
LOC 79.04% 76.78% 77.89
MISC 69.60% 59.98% 64.43
ORG 64.27% 63.17% 63.71
PER 69.21% 78.80% 73.70
overall 70.11% 69.26% 69.68
Table 3: Results obtained for the development and
the test data sets for the two languages used in this
shared task.
Andrew McCallum, Dayne Freitag, and Fernando
Pereira. 2000. Maximum entropy Markov mod-
els for information extraction and segmentation.
In ICML 2000, pages 591?598.
Vasin Punyakanok and Dan Roth. 2001. The use
of classifiers in sequential inference. In NIPS-13,
pages 995?1001.
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 692?699, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Maximum Expected F-Measure Training of Logistic Regression Models
Martin Jansche
Center for Computational Learning Systems
Columbia University
New York, NY 10027, USA
jansche@acm.org
Abstract
We consider the problem of training logis-
tic regression models for binary classifi-
cation in information extraction and infor-
mation retrieval tasks. Fitting probabilis-
tic models for use with such tasks should
take into account the demands of the task-
specific utility function, in this case the
well-known F-measure, which combines
recall and precision into a global measure
of utility. We develop a training proce-
dure based on empirical risk minimiza-
tion / utility maximization and evaluate it
on a simple extraction task.
1 Introduction
Log-linear models have been used in many areas of
Natural Language Processing (NLP) and Information
Retrieval (IR). Scenarios in which log-linear models
have been applied often involve simple binary clas-
sification decisions or probability assignments, as
in the following three examples: Ratnaparkhi et al
(1994) consider a restricted form of the prepositional
phrase attachment problem where attachment deci-
sions are binary; Ittycheriah et al (2003) reduce en-
tity mention tracking to the problem of modeling
the probability of two mentions being linked; and
Greiff and Ponte (2000) develop models of proba-
bilistic information retrieval that involve binary de-
cisions of relevance. What is common to all three
approaches is the application of log-linear models to
binary classification tasks.1 As Ratnaparkhi (1998,
1These kinds of log-linear models are also known among the
NLP community as ?maximum entropy models? (Berger et al,
p. 27f.) points out, log-linear models of binary re-
sponse variables are equivalent to, and in fact mere
notational variants of, logistic regression models.
In this paper we focus on binary classification
tasks, and in particular on the loss or utility associ-
ated with classification decisions. The three prob-
lems mentioned before ? prepositional phrase at-
tachment, entity mention linkage, and relevance of
a document to a query ? differ in one crucial aspect:
The first is evaluated in terms of accuracy or, equiva-
lently, symmetric zero?one loss; but the second and
third are treated as information extraction/retrieval
problems and evaluated in terms of recall and preci-
sion. Recall and precision are combined into a single
overall utility function, the well-known F-measure.
It may be desirable to estimate the parameters of a
logistic regression model by maximizing F-measure
during training. This is analogous, and in a cer-
tain sense equivalent, to empirical risk minimiza-
tion, which has been used successfully in related
areas, such as speech recognition (Rahim and Lee,
1997), language modeling (Paciorek and Rosenfeld,
2000), and machine translation (Och, 2003).
The novel contribution of this paper is a training
procedure for (approximately) maximizing the ex-
pected F-measure of a probabilistic classifier based
on a logistic regression model. We formulate a
vector-valued utility function which has a well-
defined expected value; F-measure is then a rational
function of this expectation and can be maximized
numerically under certain conventional regularizing
assumptions.
1996; Ratnaparkhi, 1998). This is an unfortunate choice of
terminology, because the term ?maximum entropy? does not
uniquely determine a family of models unless the constraints
subject to which entropy is being maximized are specified.
692
We begin with a review of logistic regression
(Section 2) and then discuss the use of F-measure
for evaluation (Section 3). We reformulate F-
measure as a function of an expected utility (Sec-
tion 4) which is maximized during training (Sec-
tion 5). We discuss the differences between our pa-
rameter estimation technique and maximum likeli-
hood training on a toy example (Section 6) as well
as on a real extraction task (Section 7). We conclude
with a discussion of further applications and gener-
alizations (Section 8).
2 Review of Logistic Regression
Bernoulli regression models are conditional proba-
bility models of a binary response variable Y given
a vector ~X of k explanatory variables (X1, . . . ,Xk).
We will use the convention2 that Y takes on a value
y ? {?1,+1}.
Logistic regression models (Cox, 1958) are per-
haps best viewed as instances of generalized linear
models (Nelder and Wedderburn, 1972; McCullagh
and Nelder, 1989) where the the response variable
follows a Bernoulli distribution and the link func-
tion is the logit function. Let us summarize this first,
before expanding the relevant definitions:
Y ? Bernoulli(p)
logit(p) = ?0 + x1 ?1 + x2 ?2 + ? ? ?+ xk ?k
What this means is that there is an unobserved quan-
tity p, the success probability of the Bernoulli distri-
bution, which we interpret as the probability that Y
will take on the value +1:
Pr(Y = +1 |~X = (x1,x2, . . . ,xk),~?) = p
The logit (log odds) function is defined as follows:
logit(p) = ln
(
p
1? p
)
The logit function is used to transform a probabil-
ity, constrained to fall within the interval (0,1), into
a real number ranging over (??,+?). The inverse
function of the logit is the cumulative distribution
2The natural choice may seem to be for Y to range over the
set {0,1}, but the convention adopted here is more common for
classification problems and has certain advantages which will
become clear soon.
function of the standard logistic distribution (also
known as the sigmoid or logistic function), which
we call g:
g(z) =
1
1+ exp(?z)
This allows us to write
p = g(?0 + x1 ?1 + x2 ?2 + ? ? ?+ xk ?k).
We also adopt the usual convention that ~x =
(1,x1,x2, . . . ,xk), which is a k + 1-dimensional vec-
tor whose first component is always 1 and whose
remaining k components are the values of the k ex-
planatory variables. So the Bernoulli probability can
be expressed as
p = g
(
k
?
j=0
x j ? j
)
= g
(
~x ?~?
)
.
The conditional probability model then takes the
following abbreviated form, which will be used
throughout the rest of this paper:
Pr(+1 |~x,~?) = 1
1+ exp(?~x ?~?)
(1)
A classifier can be constructed from this probabil-
ity model using the MAP decision rule. This means
predicting the label +1 if Pr(+1 |~x,~?) exceeds 1/2,
which amounts to the following:
ymap(~x) = argmax
y
Pr(y |~x,~?) = sgn
(
~x ?~?
)
This illustrates the well-known result that a MAP
classifier derived from a logistic regression model
is equivalent to a (single-layer) perceptron (Rosen-
blatt, 1958) or linear threshold unit.
3 F-Measure
Suppose the parameter vector ? of a logistic regres-
sion model is known. The performance of the re-
sulting classifier can then be evaluated in terms of
the recall (or sensitivity) and precision of the classi-
fier on an evaluation dataset. Recall (R) and preci-
sion (P) are defined in terms of the number of true
positives (A), misses (B), and false alarms (C) of the
classifier (cf. Table 1):
R =
A
A+B
P =
A
A+C
693
predicted
+1 ?1 to
ta
l
tr
ue
+1 A B npos
?1 C D nneg
total mpos mneg n
Table 1: Schema for a 2?2 contingency table
The F? measure ? familiar from Information Re-
trieval ? combines recall and precision into a single
utility criterion by taking their ?-weighted harmonic
mean:
F?(R,P) =
(
? 1
R
+(1??) 1
P
)?1
The F? measure can be expressed in terms of the
triple (A,B,C) as follows:
F?(A,B,C) =
A
A+? B+(1??)C (2)
In order to define A, B, and C formally, we use the
notation JpiK to denote a variant of the Kronecker
delta defined like this, where pi is a Boolean expres-
sion:
JpiK =
{
1 if pi
0 if ?pi
Given an evaluation dataset (~x1,y1), . . . ,(~xn,yn) the
counts of hits (true positives), misses, and false
alarms are, respectively:
A =
n
?
i=1
q
ymap(~xi) = +1
y
Jyi = +1K
B =
n
?
i=1
q
ymap(~xi) =?1
y
Jyi = +1K
C =
n
?
i=1
q
ymap(~xi) = +1
y
Jyi =?1K
Note that F-measure is seemingly a global measure
of utility that applies to an evaluation dataset as a
whole: while the F-measure of a classifier evaluated
on a single supervised instance is well defined, the
overall F-measure on a larger dataset is not a func-
tion of the F-measure evaluated on each instance
in the dataset. This is in contrast to ordinary loss/
utility, whose grand total (or average) on a dataset
can be computed by direct summation.
4 Relation to Expected Utility
We reformulate F-measure as a scalar-valued ratio-
nal function composed with a vector-valued utility
function. This allows us to define notions of ex-
pected and average utility, setting up the discussion
of parameter estimation in terms of empirical risk
minimization (or rather, utility maximization).
Define the following vector-valued utility func-
tion u, where u(y? | y) is the utility of choosing the
label y? if the true label is y:
u(+1 |+1) = (1,0,0)
u(?1 |+1) = (0,1,0)
u(+1 |?1) = (0,0,1)
u(?1 |?1) = (0,0,0)
This function indicates whether a classification deci-
sion is a hit, miss, or false alarm. Correct rejections
are not counted.
Expected values are, of course, well-defined for
vector-valued functions. For example, the expected
utility is
E[u] = ?
(~x,y)
u(ymap(~x) | y) Pr(~x,y).
In empirical risk minimization we approximate the
expected utility of a classifier by its average utility
US on a given dataset S = (~x1,y1), . . . ,(~xn,yn):
E[u]?US =
1
n
n
?
i=1
u(ymap(~xi) | yi)
=
1
n
n
?
i=1
u(+1 | yi)
q
ymap(~xi) = +1
y
+u(?1 | yi)
q
ymap(~xi) =?1
y
Now it is easy to see that US is the following vector:
US =
1
n
?
?
?
?
?
?
?
?
?
n
?
i=1
q
ymap(~xi) = +1
y
Jyi = +1K
n
?
i=1
q
ymap(~xi) =?1
y
Jyi = +1K
n
?
i=1
q
ymap(~xi) = +1
y
Jyi =?1K
?
?
?
?
?
?
?
?
?
(3)
So US = n?1 (A,B,C) where A, B, and C are as de-
fined before. This means that we can interpret the
694
F-measure of a classifier as a simple rational func-
tion of its empirical average utility (the scaling fac-
tor 1/n in (3) can in fact be omitted). This allows
us to approach the parameter estimation task as an
empirical risk minimization or utility maximization
problem.
5 Discriminative Parameter Estimation
In the preceding two sections we assumed that the
parameter vector ~? was known. Now we turn to
the problem of estimating ~? by maximizing the F-
measure formulated in terms of expected utility. We
make the dependence on ~? explicit in the formula-
tion of the optimization task:
~? ? = argmax
~?
F?(A(~?),B(~?),C(~?)),
where (A(~?),B(~?),C(~?)) =US(~?) as defined in (3).
We encounter the usual problem: the basic quan-
tities involved are integers (counts of hits, misses,
and false alarms), and the optimization objective is
a piecewise-constant functions of the parameter vec-
tor ~? , due to the fact that ~? occurs exclusively inside
Kronecker deltas. For example:
q
ymap(~x) = +1
y
=
r
Pr(+1 |~x,~?) > 0.5
z
In general, we can set
r
Pr(+1 |~x,~?) > 0.5
z
? Pr(+1 |~x,~?), (4)
and in the case of logistic regression this arises as a
special case of approximating the limit
r
Pr(+1 |~x,~?) > 0.5
z
= lim
???
g(? ~x ?~?)
with a fixed value of ? = 1. The choice of ? does
not matter much. The important point is that we are
now dealing with approximate quantities which de-
pend continuously on ~? . In particular A(~?)? ?A(~?),
where
?A(~?) =
n
?
i=1
yi=+1
g(? ~xi ?~?). (5)
Since the marginal total of positive instances npos
(cf. Table 1) does not depend on~? , we use the identi-
ties ?B(~?) = npos? ?A(~?) and m?pos(~?) = ?A(~?)+ ?C(~?)
to rewrite the optimization objective as ?F? :
?F?(~?) =
?A(~?)
? npos +(1??) m?pos(~?)
, (6)
where ?A(~?) is given by (5) and m?pos(~?) is
m?pos(~?) =
n
?
i=1
g(? ~xi ?~?).
Maximization of ?F as defined in (6) can be car-
ried out numerically using multidimensional opti-
mization techniques like conjugate gradient search
(Fletcher and Reeves, 1964) or quasi-Newton meth-
ods such as the BFGS algorithm (Broyden, 1967;
Fletcher, 1970; Goldfarb, 1970; Shanno, 1970). This
requires the evaluation of partial derivatives. The jth
partial derivative of ?F is as follows:
? ?F(~?)
?? j
= h ?
?A(~?)
?? j
?h2 ?A(~?)(1??)
? m?pos(~?)
?? j
h = 1
? npos +(1??) m?pos(~?)
? ?A(~?)
?? j
=
n
?
i=1
yi=+1
g?(? ~xi ?~?)? xi j
? m?pos(~?)
?? j
=
n
?
i=1
g?(? ~xi ?~?)? xi j
g?(z) = g(z)(1?g(z))
One can compute the value of ?F(~?) and its gradient
? ?F(~?) simultaneously at a given point ~? in O(nk)
time and O(k) space. Pseudo-code for such an al-
gorithm appears in Figure 1. In practice, the inner
loops on lines 8?9 and 14?18 can be made more ef-
ficient by using a sparse representation of the row
vectors x[i]. A concrete implementation of this al-
gorithm can then be used as a callback to a multi-
dimensional optimization routine. We use the BFGS
minimizer provided by the GNU Scientific Library
(Galassi et al, 2003). Important caveat: the func-
tion ?F is generally not concave. We deal with this
problem by taking the maximum across several runs
of the optimization algorithm starting from random
initial values. The next section illustrates this point
further.
695
x y
0 +1
1 ?1
2 +1
3 +1
Table 2: Toy dataset
6 Comparison with Maximum Likelihood
A comparison with the method of maximum like-
lihood illustrates two important properties of dis-
criminative parameter estimation. Consider the toy
dataset in Table 2 consisting of four supervised in-
stances with a single explanatory variable. Thus the
logistic regression model has two parameters and
takes the following form:
Pr toy(+1 | x,?0,?1) =
1
1+ exp(??0? x?1)
The log-likelihood function L is simply
L(?0,?1) = logPr toy(+1 |0,?0,?1)
+ logPr toy(?1 |1,?0,?1)
+ logPr toy(+1 |2,?0,?1)
+ logPr toy(+1 |3,?0,?1).
A surface plot of L is shown in Figure 2. Ob-
serve that L is concave; its global maximum occurs
near (?0,?1) ? (0.35,0.57), and its value is always
strictly negative because the toy dataset is not lin-
early separable. The classifier resulting from maxi-
mum likelihood training predicts the label +1 for all
training instances and thus achieves a recall of 3/3
and precision 3/4 on its training data. The F?=0.5
measure is 6/7.
Contrast the shape of the log-likelihood function
L with the function ?F? . Surface plots of ?F?=0.5 and
?F?=0.25 appear in Figure 3. The figures clearly illus-
trate the first important (but undesirable) property of
?F , namely the lack of concavity. They also illustrate
a desirable property, namely the ability to take into
account certain properties of the loss function dur-
ing training. The ?F?=0.5 surface in the left panel of
Figure 3 achieves its maximum in the right corner
for (?0,?1)? (+?,+?). If we choose (?0,?1) =
(20,15) the classifier labels every instance of the
training data with +1.
fdf(?):
1: m? 0
2: A? 0
3: for j? 0 to k do
4: dm[ j]? 0
5: dA[ j]? 0
6: for i? 1 to n do
7: p? 0
8: for j? 0 to k do
9: p? p+ x[i][ j]?? [ j]
10: p? 1/(1+ exp(?d))
11: m? m+ p
12: if y[i] = +1 then
13: A? A+ p
14: for j? 0 to k do
15: t? p? (1? p)? x[i][ j]
16: dm[ j]? dm[ j]+ t
17: if y[i] = +1 then
18: dA[ j]? dA[ j]+ t
19: h? 1/(??npos +(1??)?m)
20: F ? h?A
21: t? F? (1??)
22: for j? 0 to k do
23: dF[ j]? h? (dA[ j]? t?dm[ j])
24: return (F,dF)
Figure 1: Algorithm for computing ?F and ? ?F
L(?0, ?1)
-25
-20
-15
-10
-5
 0
 5
 10
 15
?0
-20 -15
-10 -5
 0  5
 10  15
 20
?1
-180
-160
-140
-120
-100
-80
-60
-40
-20
 0
Figure 2: Surface plot of L on the toy dataset
Observe the difference between the ?F?=0.5 surface
and the ?F?=0.25 surface in the right hand panel of
Figure 3: ?F?=0.25 achieves its maximum in the back
corner for (?0,?1)? (??,+?). If we set (?0,?1) =
(?20,15) the resulting classifier labels the first two
696
F0.5(?0, ?1)
-25
-20
-15
-10
-5
 0
 5
 10
 15
?0
-20 -15
-10 -5
 0  5
 10  15
 20
?1
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
F0.25(?0, ?1)
-25
-20
-15
-10
-5
 0
 5
 10
 15
?0
-20 -15
-10 -5
 0  5
 10  15
 20
?1
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
Figure 3: Surface plot of ?F?=0.5 (left) and ?F?=0.25 (right) on the toy dataset
instances (x = 0 and x = 1) as ?1 and the last two
instances (x = 2 and x = 3) as +1.
The classifier trained according to the ?F?=0.5 cri-
terion achieves an F?=0.5 measure of 6/7 ? 0.86,
compared with 4/5 = 0.80 for the classifier trained
according to the ?F?=0.25 criterion. Conversely, that
classifier achieves an F?=0.25 measure of 8/9? 0.89
compared with 4/5 = 0.80 for the classifier trained
according to the ?F?=0.5 criterion. This demonstrates
that the training procedure can effectively take infor-
mation from the utility function into account, pro-
ducing a classifier that performs well under a given
evaluation criterion. This is the result of optimizing
a task-specific utility function during training, not
simply a matter of adjusting the decision threshold
of a trained classifier.
7 Evaluation on an Extraction Problem
We evaluated our discriminative training procedure
on a real extraction problem arising in broadcast
news summarization. The overall task is to summa-
rize the stories in an audio news broadcast (or in the
audio portion of an A/V broadcast). We assume that
story boundaries have been identified and that each
story has been broken up into sentence-like units. A
simple way of summarizing a story is then to classify
each sentence as either belonging into a summary or
not, so that a relevant subset of sentences can be ex-
tracted to form the basis of a summary. What makes
the classification task hard, and therefore interesting,
is the fact that reliable features are hard to come by.
Existing approaches such as Maskey and Hirschberg
2005 do well only when combining diverse features
such as lexical cues, acoustic properties, structural/
positional features, etc.
The task has another property which renders it
problematic, and which prompted us to develop
the discriminative training procedure described in
this paper. Summarization, by definition, aims for
brevity. This means that in any dataset the number
of positive instances will be much smaller than the
number of negative instances. Given enough data,
balance could be restored by discarding negative in-
stances. This, however, was not an option in our
case: a moderate amount of manually labeled data
had been produced and about one third would have
had to be discarded to achieve a balance in the dis-
tribution of class labels. This would have eliminated
precious supervised training data, which we were
not prepared to do.
The training and test data were prepared by
Maskey and Hirschberg (2005), who performed the
feature engineering, imputation of missing values,
and the training?test split. We used the data un-
changed in order to allow for a comparison between
approaches. The dataset is made up of 30 fea-
tures, divided into one binary response variable, and
one binary explanatory variable plus 28 integer- and
real-valued explanatory variables. The training por-
tion consists of 3 535 instances, the test portion of
408 instances.
We fitted logistic regression models in three dif-
ferent ways: by maximum likelihood ML, by ?F?=0.5
maximization, and by ?F?=0.75 maximization. Each
697
Method R P F?=0.5 F?=0.75
ML 24/99 24/33 0.3636 0.2909
ML? 85/99 85/229 0.5183 0.6464
?F?=0.5 85/99 85/211 0.5484 0.6693
?F?=0.75 95/99 95/330 0.4429 0.6061
Table 3: Evaluation results
classifier was evaluated on the test dataset and its re-
call (R), precision (P), F?=0.5 measure, and F?=0.75
measure recorded. The results appear in Table 3.
The row labeled ML? is special: the classifier used
here is the logistic regression model fitted by maxi-
mum likelihood; what is different is that the thresh-
old for positive predictions was adjusted post hoc to
match the number of true positives of the first dis-
criminatively trained classifier. This has the same
effect as manually adjusting the threshold parameter
?0 based on partial knowledge of the test data (via
the performance of another classifier) and is thus
not permissible. It is interesting to note, however,
that the ML trained classifier performs worse than
the ?F?=0.5 trained classifier even when one param-
eter is adjusted by an oracle with knowledge of the
test data and the performance of the other classifier.
Fitting a model based on ?F?=0.75, which gives in-
creased weight to recall compared with ?F?=0.5, led
to higher recall as expected. However, we also ex-
pected that the F?=0.75 score of the ?F?=0.75 trained
classifier would be higher than the F?=0.75 score of
the ?F?=0.5 trained classifier. This is not the case, and
could be due to the optimization getting stuck in a
local maximum, or it may have been an unreason-
able expectation to begin with.
8 Conclusions
We have presented a novel estimation procedure
for probabilistic classifiers which we call, by a
slight abuse of terminology, maximum expected F-
measure training. We made use of the fact that ex-
pected utility computations can be carried out in a
vector space, and that an ordering of vectors can be
imposed for purposes of maximization which can
employ auxiliary functions like the F-measure (2).
This technique is quite general and well suited for
working with other quantities that can be expressed
in terms of hits, misses, false alarms, correct rejec-
tions, etc. In particular, it could be used to find a
point estimate which provides a certain tradeoff be-
tween specificity and sensitivity, or operating point.
A more general method would try to optimize sev-
eral such operating points simultaneously, an issue
which we will leave for future research.
The classifiers discussed in this paper are logistic
regression models. However, this choice is not cru-
cial. The approximation (4) is reasonable for binary
decisions in general, and one can use it in conjunc-
tion with any well-behaved conditional Bernoulli
model or related classifier. For Support Vector Ma-
chines, approximate F-measure maximization was
introduced by Musicant et al (2003).
Maximizing F-measure during training seems es-
pecially well suited for dealing with skewed classes.
This can happen by accident, because of the nature
of the problem as in our summarization example
above, or by design: for example, one can expect
skewed binary classes as the result of the one-vs-all
reduction of multi-class classification to binary clas-
sification; and in multi-stage classification one may
want to alternate between classifiers with high recall
and classifiers with high precision.
Finally, the ability to incorporate non-standard
tradeoffs between precision and recall at training
time is useful in many information extraction and
retrieval applications. Human end-users often create
asymmetries between precision and recall, for good
reasons: they may prefer to err on the side of caution
(e.g., it is less of a problem to let an unwanted spam
email reach a user than it is to hold back a legitimate
message), or they may be better at some tasks than
others (e.g., search engine users are good at filtering
out irrelevant documents returned by a query, but are
not equipped to crawl the web in order to look for
relevant information that was not retrieved). In the
absence of methods that work well for a wide range
of operating points, we need training procedures that
can be made sensitive to rare cases depending on the
particular demands of the application.
Acknowledgements
I would like to thank Julia Hirschberg, Sameer
Maskey, and the three anonymous reviewers for
helpful comments. I am especially grateful to
698
Sameer Maskey for allowing me to use his speech
summarization dataset for the evaluation in Sec-
tion 7. The usual disclaimers apply.
References
Adam L. Berger, Vincent J. Della Pietra, and
Stephen A. Della Pietra. 1996. A maximum
entropy approach to natural language processing.
Computational Linguistics, 22(1):39?71.
C. G. Broyden. 1967. Quasi-Newton methods and
their application to function minimisation. Math-
ematics of Computation, 21(99):368?381.
D. R. Cox. 1958. The regression analysis of binary
sequences. Journal of the Royal Statistical Soci-
ety, Series B (Methodological), 20(2):215?242.
R. Fletcher. 1970. A new approach to variable
metric algorithms. The Computer Journal,
13(3):317?322. doi:10.1093/comjnl/13.3.317.
R. Fletcher and C. M. Reeves. 1964. Func-
tion minimization by conjugate gradients.
The Computer Journal, 7(2):149?154.
doi:10.1093/comjnl/7.2.149.
Mark Galassi, Jim Davies, James Theiler, Brian
Gough, Gerard Jungman, Michael Booth, and
Fabrice Rossi. 2003. GNU Scientific Library
Reference Manual. Network Theory, Bristol,
UK, second edition.
Donald Goldfarb. 1970. A family of variable-metric
methods derived by variational means. Mathe-
matics of Computation, 24(109):23?26.
Warren R. Greiff and Jay M. Ponte. 2000.
The maximum entropy approach and prob-
abilistic IR models. ACM Transactions
on Information Systems, 18(3):246?287.
doi:10.1145/352595.352597.
Abraham Ittycheriah, Lucian Lita, Nanda Kamb-
hatla, Nicolas Nicolov, Salim Roukos, and
Margo Stys. 2003. Identifying and tracking
entity mentions in a maximum entropy frame-
work. In HLT/NAACL 2003. ACL Anthology
N03-2014.
Sameer Maskey and Julia Hirschberg. 2005. Com-
paring lexical, acoustic/prosodic, structural and
discourse features for speech summarization. In
Interspeech 2005 (Eurospeech).
P. McCullagh and J. A. Nelder. 1989. Generalized
Linear Models. Chapman & Hall/CRC, Boca
Raton, FL, second edition.
David R. Musicant, Vipin Kumar, and Aysel Ozgur.
2003. Optimizing F-measure with Support Vec-
tor Machines. In FLAIRS 16, pages 356?360.
J. A. Nelder and R. W. M. Wedderburn. 1972. Gen-
eralized linear models. Journal of the Royal Sta-
tistical Society, Series A (General), 135(3):370?
384.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In ACL 41.
ACL Anthology P03-1021.
Chris Paciorek and Roni Rosenfeld. 2000. Mini-
mum classification error training in exponential
language models. In NIST/DARPA Speech Tran-
scription Workshop.
Mazin Rahim and Chin-Hui Lee. 1997. String-
based minimum verification error (SB-MVE)
training for speech recognition. Computer,
Speech and Language, 11(2):147?160.
Adwait Ratnaparkhi. 1998. Maximum Entropy
Models for Natural Language Ambiguity Reso-
lution. Ph.D. thesis, University of Pennsylvania,
Computer and Information Science.
Adwait Ratnaparkhi, Jeff Reynar, and Salim
Roukos. 1994. A maximum entropy model
for prepositional phrase attachment. In ARPA
Human Language Technology Workshop, pages
250?255. ACL Anthology H94-1048.
Frank Rosenblatt. 1958. The perceptron: A prob-
abilistic model for information storage and or-
ganization in the brain. Psychological Review,
65(6):386?408.
D. F. Shanno. 1970. Conditioning of quasi-Newton
methods for function minimization. Mathematics
of Computation, 24(111):647?656.
699
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 74?82,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
Treebank Transfer
Martin Jansche
Center for Computational Learning Systems
Columbia University
New York, NY 10027, USA
jansche@acm.org
Abstract
We introduce a method for transferring
annotation from a syntactically annotated
corpus in a source language to a target lan-
guage. Our approach assumes only that
an (unannotated) text corpus exists for the
target language, and does not require that
the parameters of the mapping between
the two languages are known. We outline
a general probabilistic approach based on
Data Augmentation, discuss the algorith-
mic challenges, and present a novel algo-
rithm for sampling from a posterior distri-
bution over trees.
1 Introduction
Annotated corpora are valuable resources for Natu-
ral Language Processing (NLP) which often require
significant effort to create. Syntactically annotated
corpora ? treebanks, for short ? currently exist for a
small number of languages; but for the vast majority
of the world?s languages, treebanks are unavailable
and unlikely to be created any time soon.
The situation is especially difficult for dialectal
variants of many languages. A prominent exam-
ple is Arabic: syntactically annotated corpora ex-
ist for the common written variety (Modern Stan-
dard Arabic or MSA), but the spoken regional di-
alects have a lower status in written communication
and lack annotated resources. This lack of dialect
treebanks hampers the development of syntax-based
NLP tools, such as parsers, for Arabic dialects.
On the bright side, there exist very large anno-
tated (Maamouri et al, 2003, 2004a,b) corpora for
Modern Standard Arabic. Furthermore, unannotated
text corpora for the various Arabic dialects can also
be assembled from various sources on the Internet.
Finally, the syntactic differences between the Ara-
bic dialects and Modern Standard Arabic are rela-
tively minor (compared with the lexical, phonologi-
cal, and morphological differences). The overall re-
search question is then how to combine and exploit
these resources and properties to facilitate, and per-
haps even automate, the creation of syntactically an-
notated corpora for the Arabic dialects.
We describe a general approach to this problem,
which we call treebank transfer: the goal is to
project an existing treebank, which exists in a source
language, to a target language which lacks annotated
resources. The approach we describe is not tied in
any way to Arabic, though for the sake of concrete-
ness one may equate the source language with Mod-
ern Standard Arabic and the target language with a
dialect such as Egyptian Colloquial Arabic.
We link the two kinds of resources that are avail-
able ? a treebank for the source language and an
unannotated text corpus for the target language ?
in a generative probability model. Specifically, we
construct a joint distribution over source-language
trees, target-language trees, as well as parameters,
and draw inferences by iterative simulation. This al-
lows us to impute target-language trees, which can
then be used to train target-language parsers and
other NLP components.
Our approach does not require aligned data,
unlike related proposals for transferring annota-
tions from one language to another. For exam-
ple, Yarowksy and Ngai (2001) consider the transfer
of word-level annotation (part-of-speech labels and
bracketed NPs). Their approach is based on aligned
74
corpora and only transfers annotation, as opposed to
generating the raw data plus annotation as in our ap-
proach.
We describe the underlying probability model of
our approach in Section 2 and discuss issues per-
taining to simulation and inference in Section 3.
Sampling from the posterior distribution of target-
language trees is one of the key problems in iterative
simulation for this model. We present a novel sam-
pling algorithm in Section 4. Finally in Section 5 we
summarize our approach in its full generality.
2 The Probability Model
Our approach assumes that two kinds of resources
are available: a source-language treebank, and a
target-language text corpus. This is a realistic
assumption, which is applicable to many source-
language/target-language pairs. Furthermore, some
knowledge of the mapping between source-language
syntax and target-language syntax needs to be incor-
porated into the model. Parallel corpora are not re-
quired, but may help when constructing this map-
ping.
We view the source-language treebank as a se-
quence of trees S1, . . . ,Sn, and assume that these
trees are generated by a common process from a
corresponding sequence of latent target-language
trees T1, . . . ,Tn. The parameter vector of the pro-
cess which maps target-language trees to source-
language trees will be denoted by ?. The mapping
itself is expressed as a conditional probability distri-
bution p(Si | Ti,?) over source-language trees. The
parameter vector ? is assumed to be generated ac-
cording to a prior distribution p(? | ? ) with hyper-
parameter ? , assumed to be fixed and known.
We further assume that each target-language tree
Ti is generated from a common language model ?
for the target language, p(Ti | ?). For expository rea-
sons we assume that ? is a bigram language model
over the terminal yield (also known as the fringe) of
Ti. Generalizations to higher-order n-gram models
are completely straightforward; more general mod-
els that can be expressed as stochastic finite au-
tomata are also possible, as discussed in Section 5.
Let t1, . . . , tk be the terminal yield of tree T . Then
p(T | ?) = ?(t1 | #)
(
k
?
j=2
?(t j | t j?1)
)
?($ | tk),
where # marks the beginning of the string and $
marks the end of the string.
There are two options for incorporating the lan-
guage model ? into the overall probability model.
In the first case ? which we call the full model ?
? is generated by an informative prior distribution
p(? | ? ) with hyper-parameter ? . In the second case
? the reduced model ? the language model ? is fixed.
The structure of the full model is specified graph-
ically in Figure 1. In a directed acyclic graphical
model such as this one, we equate vertices with ran-
dom variables. Directed edges are said to go from a
parent to a child node. Each vertex depends directly
on all of its parents. Any particular vertex is condi-
tionally independent from all other vertices given its
parents, children, and the parents of its children.
The portion of the full model we are interested in
is the following factored distribution, as specified by
Figure 1:
p(S1, . . . ,Sn,T1, . . . ,Tn,?,? | ? ,? )
= p(? | ? ) p(? | ? )
n
?
i=1
p(Ti | ?) p(Si | Ti,?) (1)
In the reduced model, we drop the leftmost term/
vertex, corresponding to the prior for ? with hyper-
parameter ? , and condition on ? instead:
p(S1, . . . ,Sn,T1, . . . ,Tn,? | ?,? )
= p(? | ? )
n
?
i=1
p(Ti | ?) p(Si | Ti,?) (2)
The difference between the full model (1) and the
reduced model (2) is that the reduced model assumes
that the language model ? is fixed and will not be
informed by the latent target-language trees Ti. This
is an entirely reasonable assumption in a situation
where the target-language text corpus is much larger
than the source-language treebank. This will typ-
ically be the case, since it is usually very easy to
collect large corpora of unannotated text which ex-
ceed the largest existing annotated corpora by sev-
eral orders of magnitude. When a sufficiently large
target-language text corpus is available, ? is simply
a smoothed bigram model which is estimated once
from the target-language corpus.
If the target-language corpus is relatively small,
then the bigram model ? can be refined on the ba-
sis of the imputed target-language trees. A bigram
75
? ? ?
S1T1
?
T2 S2
Tn Sn
... ...
Figure 1: The graphical structure of the full probability model. Bold circles indicate observed variables,
dotted circles indicate parameters.
model is simply a discrete collection of multinomial
distributions. A simple prior for ? takes the form
of a product of Dirichlet distributions, so that the
hyper-parameter ? is a vector of bigram counts. In
the full model (1), we assume ? is fixed and set it to
the observed bigram counts (plus a constant) in the
target-language text corpus. This gives us an infor-
mative prior for ?. If the bigram counts are suffi-
ciently large, ? will be fully determined by this in-
formative prior distribution, and the reduced model
(2) can be used instead.
By contrast, usually very little is known a pri-
ori about the syntactic transfer model ?. Instead ?
needs to be estimated from data. We assume that ?
too is a discrete collection of multinomial distribu-
tions, governed by Dirichlet priors. However, unlike
in the case of ?, the priors for ? are noninforma-
tive. This is not a problem, since a lot of informa-
tion about the target language is provided by the lan-
guage model ?.
As one can see in Figure 1 and equation (1),
the overall probability model constrains the latent
target-language trees Ti in two ways: From the left,
the language model ? serves as a prior distribution
over target-language trees. On the one hand, ? is
an informative prior, based on large bigram counts
obtained from the target-language text corpus; on
the other hand, it only informs us about the fringe
of the target-language trees and has very little di-
rectly to say about their syntactic structure. From the
right, the observed source-language trees constrain
the latent target-language trees in a complementary
fashion. Each target-language tree Ti gives rise to a
corresponding source-language tree Si according to
the syntactic transfer mapping ?. This mapping is
initially known only qualitatively, and comes with a
noninformative prior distribution.
Our goal is now to simultaneously estimate the
transfer parameter ? and impute the latent trees Ti.
This is simplified by the following observation: if
T1, . . . ,Tn are known, then finding ? is easy; vice
versa, if ? is known, then finding Ti is easy. Si-
multaneous inference for ? and T1, . . . ,Tn is possible
via Data Augmentation (Tanner and Wong, 1987),
or, more generally, Gibbs sampling (Geman and Ge-
man, 1984).
3 Simulation of the Joint Posterior
Distribution
We now discuss the simulation of the joint poste-
rior distribution over the latent trees T1, . . . ,Tn, the
transfer model parameter ?, and the language model
parameter ?. This joint posterior is derived from the
overall full probability model (1). Using the reduced
model (2) instead of the full model amounts to sim-
ply omitting ? from the joint posterior. We will deal
primarily with the more general full model in this
section, since the simplification which results in the
reduced model will be straightforward.
The posterior distribution we focus on is
p(T1, . . . ,Tn,?,? | S1, . . . ,Sn,? ,? ), which provides
us with information about all the variables of inter-
est, including the latent target-language trees Ti, the
syntactic transfer model ?, and the target-language
76
language model ?. It is possible to simulate this
joint posterior distribution using simple sampling-
based approaches (Gelfand and Smith, 1990), which
are instances of the general Markov-chain Monte
Carlo method (see, for example, Liu, 2001).
Posterior simulation proceeds iteratively, as fol-
lows. In each iteration we draw the three kinds of
random variables ? latent trees, language model pa-
rameters, and transfer model parameters ? from their
conditional distributions while holding the values of
all other variables fixed. Specifically:
? Initialize ? and ? by drawing each from its
prior distribution.
? Iterate the following three steps:
1. Draw each Ti from its posterior distribu-
tion given Si, ?, and ?.
2. Draw ? from its posterior distribution
given T1, . . . ,Tn and ? .
3. Draw ? from its posterior distribution
given S1, . . . ,Sn, T1, . . . ,Tn, and ? .
This simulation converges in the sense that the draws
of T1, . . . ,Tn, ?, and ? converge in distribution to
the joint posterior distribution over those variables.
Further details can be found, for example, in Liu,
2001, as well as the references cited above.
We assume that the bigram model ? is a family of
multinomial distributions, and we write ?(t j | t j?1)
for the probability of the word t j following t j?1.
Using creative notation, ?( ? | t j?1) can be seen as
a multinomial distribution. Its conjugate prior is
a Dirichlet distribution whose parameter vector ?w
are the counts of words types occurring immediately
after the word type w of t j?1. Under the conven-
tional assumptions of exchangeability and indepen-
dence, the prior distribution for ? is just a product of
Dirichlet priors. Since we employ a conjugate prior,
the posterior distribution of ?
p(? | S1, . . . ,Sn,T1, . . . ,Tn,?,? ,? )
= p(? | T1, . . . ,Tn,? ) (3)
has the same form as the prior ? it is likewise a prod-
uct of Dirichlet distributions. In fact, for each word
type w the posterior Dirichlet density has parameter
?w+cw, where ?w is the parameter of the prior distri-
bution and cw is a vector of counts for all word forms
appearing immediately after w along the fringe of
the imputed trees.
We make similar assumptions about the syntactic
transfer model ? and its posterior distribution, which
is
p(? | S1, . . . ,Sn,T1, . . . ,Tn,?,? ,? )
= p(? | S1, . . . ,Sn,T1, . . . ,Tn,? ). (4)
In particular, we assume that syntactic transfer in-
volves only multinomial distributions, so that the
prior and posterior for ? are products of Dirichlet
distributions. This means that sampling ? and ?
from their posterior distributions is straightforward.
The difficult part is the first step in each scan of
the Gibbs sampler, which involves sampling each
target-language latent tree from the corresponding
posterior distribution. For a particular tree Tj, the
posterior takes the following form:
p(Tj | S1, . . . ,Sn,T1, . . . ,Tj?1,Tj+1, . . . ,Tn,?,?,? ,? )
= p(Tj | S j,?,?) =
p(Tj,S j | ?,?)
?Tj p(Tj,S j | ?,?)
? p(Tj | ?) p(S j | Tj,?) (5)
The next section discusses sampling from this poste-
rior distribution in the context of a concrete example
and presents an algorithmic solution.
4 Sampling from the Latent Tree Posterior
We are faced with the problem of sampling Tj from
its posterior distribution, which is proportional to the
product of its language model prior p(Tj | ?) and
transfer model likelihood p(S j | Tj,?). Rejection
sampling using the prior as the proposal distribution
will not work, for two reasons: first, the prior is only
defined on the yield of a tree and there are poten-
tially very many tree structures with the same fringe;
second, even if the first problem could be overcome,
it is unlikely that a random draw from an n-gram
prior would result in a target-language tree that cor-
responds to a particular source-language tree, as the
prior has no knowledge of the source-language tree.
Fortunately, efficient direct sampling from the la-
tent tree posterior is possible, under one very rea-
sonable assumption: the set of all target-language
trees which map to a given source-language tree S j
77
CS
1
v
2
O
3
a1
1
n1
2
a2
1
n2
2
Figure 2: Syntax tree illustrating SVO constituent
order within a sentence, and prenominal adjectives
within noun phrases.
should be finite and representable as a packed for-
est. More specifically, we assume that there is a
compact (polynomial space) representation of po-
tentially exponentially many trees. Moreover, each
tree in the packed forest has an associated weight,
corresponding to its likelihood under the syntactic
transfer model.
If we rescale the weights of the packed forest so
that it becomes a normalized probabilistic context-
free grammar (PCFG), we can sample from this new
distribution (corresponding to the normalized likeli-
hood) efficiently. For example, it is then possible to
use the PCFG as a proposal distribution for rejection
sampling.
However, we can go even further and sample
from the latent tree posterior directly. The key
idea is to intersect the packed forest with the n-
gram language model and then to normalize the re-
sulting augmented forest. The intersection opera-
tion is a special case of the intersection construction
for context-free grammars and finite automata (Bar-
Hillel et al, 1961, pp. 171?172). We illustrate it here
for a bigram language model.
Consider the tree in Figure 2 and assume it is
a source-language tree, whose root is a clause (C)
which consists of a subject (S), verb (v) and object
(O). The subject and object are noun phrases consist-
ing of an adjective (a) and a noun (n). For simplicity,
we treat the part-of-speech labels (a, n, v) as termi-
nal symbols and add numbers to distinguish multiple
occurrences. The syntactic transfer model is stated
as a conditional probability distribution over source-
language trees conditional on target language trees.
Syntactic transfer amounts to independently chang-
ing the order of the subject, verb, and object, and
changing the order of adjectives and nouns, for ex-
ample as follows:
p(SvO | SvO) = ?1
p(SOv | SvO) = (1??1)?2
p(vSO | SvO) = (1??1)(1??2)
p(SvO | SOv) = ?3
p(SOv | SOv) = (1??3)?4
p(vSO | SOv) = (1??3)(1??4)
p(SvO | vSO) = ?5
p(SOv | vSO) = (1??5)?6
p(vSO | vSO) = (1??5)(1??6)
p(an | an) = ?7
p(na | an) = 1??7
p(an | na) = ?8
p(na | na) = 1??8
Under this transfer model, the likelihood of a target-
language tree [A v [S a1 n1][O n2 a2]] corresponding to
the source-language tree shown in Figure 2 is ?5 ?
?7 ??8. It is easy to construct a packed forest of all
target-language trees with non-zero likelihood that
give rise to the source-language tree in Figure 2.
Such a forest is shown in Figure 3. Forest nodes are
shown as ellipses, choice points as rectangles con-
nected by dashed lines. A forest node is to be un-
derstood as an (unordered) disjunction of the choice
points directly underneath it, and a choice point as
an (ordered, as indicated by numbers) conjunction
of the forest nodes directly underneath it. In other
words, a packed forest can be viewed as an acyclic
and-or graph, where choice points represent and-
nodes (whose children are ordered). As a simpli-
fying convention, for nodes that dominate a single
choice node, that choice node is not shown. The for-
est in Figure 3 represents SvO, SOv, and vSO permu-
tations at the sentence level and an, na permutations
below the two noun phrases. The twelve overall per-
mutations are represented compactly in terms of two
choices for the subject, two choices for the object,
and three choices for the root clause.
78
CC_1C_2 C_3
S
1 VO
2
S_1 S_2
a1
1
n1
2 2 1
v
1
O
2
O_1 O_2
a2
1
n2
2 2 1
1 OV
2
21
1SO
2
1 2
Figure 3: Plain forest of target-language trees that can correspond to the source-language tree in Figure 2.
We intersect/compose the packed forest with the
bigram language model ? by augmenting each node
in the forest with a left context word and a right pe-
ripheral word: a node N is transformed into a triple
(a,N,b) that dominates those trees which N domi-
nates in the original forest and which can occur after
a word a and end with a word b. The algorithm is
roughly1 as shown in Figure 5 for binary branching
forests; it requires memoization (not shown) to be
efficient. The generalization to forests with arbitrary
branching factors is straightforward, but the presen-
tation of that algorithm less so. At the root level, we
call forest_composition with a left context of #
(indicating the start of the string) and add dummy
nodes of the form (a,$,$) (indicating the end of the
string). Further details can be found in the prototype
implementation. Each node in the original forest is
augmented with two words; if there are n leaf nodes
in the original forest, the total number of nodes in
the augmented forest will be at most n2 times larger
than in the original forest. This means that the com-
pact encoding property of the packed forest (expo-
nentially many trees can be represented in polyno-
mial space) is preserved by the composition algo-
rithm. An example of composing a packed forest
1A detailed implementation is available from http://www.
cs.columbia.edu/?jansche/transfer/.
with a bigram language model appears in Figure 4,
which shows the forest that results from composing
the forest in Figure 3 with a bigram language model.
The result of the composition is an augmented
forest from which sampling is almost trivial. The
first thing we have to do is to recursively propagate
weights from the leaves upwards to the root of the
forest and associate them with nodes. In the non-
recursive case of leaf nodes, their weights are pro-
vided by the bigram score of the augmented forest:
observe that leaves in the augmented forest have la-
bels of the form (a,b,b), where a and b are terminal
symbols, and a represents the immediately preced-
ing left context. The score of such a leaf is sim-
ply ?(b | a). There are two recursive cases: For
choice nodes (and-nodes), their weight is the prod-
uct of the weights of the node?s children times a lo-
cal likelihood score. For example, the node (v,O,n)
in Figure 4 dominates a single choice node (not
shown, per the earlier conventions), whose weight
is ?(a | v) ?(n | a) ?7. For other forest nodes (or-
nodes), their weight is the sum of the weights of the
node?s children (choice nodes).
Given this very natural weight-propagation algo-
rithm (and-nodes correspond to multiplication, or-
nodes to summation), it is clear that the weight of the
root node is the sum total of the weights of all trees
in the forest, where the weight of a tree is the prod-
79
(#,ro
ot,$)
(#,ro
ot,$)
_
1
(#,ro
ot,$)
_
2
(#,ro
ot,$)
_
3
(#,C,
n)
1 (n,
$,$)2
(#,C,
n)_1
(#,C,
n)_2
(#,C,
n)_3
(#,S,
n)1
(n,V
O,n
)
2
(#,a1
,a)
1
(a,n1
,n)
2
(n,v,
v)
1
(v,O
,n)
2 (v,a
2,a)
1
(a,n2
,n)
2
(#,S,
a)
1
(a,VO
,n)2
(#,n1
,n)1
(n,a1
,a)2
2
(a,v,v
)
1
(#,v,
v)
1
(v,SO
,n)
2
(v,SO
,n)_1
(v,SO
,n)_2
(v,S,
n)
1
(n,O
,n)2
2
(v,a1
,a)1
2 (n,a
2,a)1
(v,S,
a)
1
(a,O,
n)
2
2
(v,n1
,n)1
2
(a,a2
,a)1
(#,C,
a)
1
(a,$,$
)2
(#,C,
a)_1
(#,C,
a)_2
(#,C,
a)_3
1
(a,VO
,a)
2 1 (v,O
,a)
2
(v,n2
,n)1
(n,a2
,a)
2
1
(n,V
O,a)
2
1
2
1
(v,SO
,a)
2
(v,SO
,a)_1
(v,SO
,a)_2
1
(a,O,
a)
2
2 (a,
n2,n
)1
1
(n,O
,a)
2
2
(n,n2
,n)1
(#,C,
v)1
(v,$,
$)
2
(#,C,
v)_1
(#,C,
v)_2
1
(n,O
V,v
)2
(n,O
V,v
)_1
(n,O
V,v
)_2
2
1
2
1
1
(a,OV
,v)
2
(a,OV
,v)_1
(a,OV
,v)_2
2
1
2
1
Figure 4: Augmented forest obtained by intersecting the forest in Figure 3 with a bigram language model.
80
forest_composition(N, a):
if N is a terminal:
return { (a,N,N) }
else:
nodes = {}
for each (L,R) in N.choices:
left_nodes <- forest_composition(L, a)
for each (a,L,b) in left_nodes:
right_nodes <- forest_composition(R, b)
for each (b,R,c) in right_nodes:
new_n = (a,N,c)
nodes <- nodes + { new_n }
new_n.choices <- new_n.choices + [((a,L,b), (b,R,c))]
return nodes
Figure 5: Algorithm for computing the intersection of a binary forest with a bigram language model.
uct of the local likelihood scores times the language
model score of the tree?s terminal yield. We can
then associate outgoing normalized weights with the
children (choice points) of each or-node, where the
probability of going to a particular choice node from
a given or-node is equal to the weight of the choice
node divided by the weight of the or-node.
This means we have managed to calculate the
normalizing constant of the latent tree posterior (5)
without enumerating the individual trees in the for-
est. Normalization ensures that we can sample from
the augmented and normalized forest efficiently, by
proceeding recursively in a top-down fashion, pick-
ing a child of an or-node at random with probability
proportional to the outgoing weight of that choice.
It is easy to see (by a telescoping product argument)
that by multiplying together the probabilities of each
such choice we obtain the posterior probability of a
latent tree. We thus have a method for sampling la-
tent trees efficiently from their posterior distribution.
The sampling procedure described here is very
similar to the lattice-based generation procedure
with n-gram rescoring developed by Langkilde
(2000), and is in fact based on the same intersection
construction (Langkilde seems to be unaware that
the CFG-intersection construction from (Bar-Hillel
et al, 1961) is involved). However, Langkilde is in-
terested in optimization (finding the best tree in the
forest), which allows her to prune away less prob-
able trees from the composed forest in a procedure
that combines composition, rescoring, and pruning.
Alternatively, for a somewhat different but related
formulation of the probability model, the sampling
method developed by Mark et al (1992) can be used.
However, its efficiency is not well understood.
5 Conclusions
The approach described in this paper was illustrated
using very simple examples. The simplicity of the
exposition should not obscure the full generality of
our approach: it is applicable in the following situa-
tions:
? A prior over latent trees is defined in terms of
stochastic finite automata.
We have described the special case of bigram
models, and pointed out how our approach
will generalize to higher-order n-gram models.
However, priors are not generally constrained
to be n-gram models; in fact, any stochastic
finite automaton can be employed as a prior,
since the intersection of context-free grammars
and finite automata is well-defined. However,
the intersection construction that appears to be
necessary for sampling from the posterior dis-
tribution over latent trees may be rather cum-
bersome when higher-order n-gram models or
more complex finite automata are used as pri-
ors.
81
? The inverse image of an observed tree under the
mapping from latent trees to observed trees can
be expressed in terms of a finite context-free
language, or equivalently, a packed forest.
The purpose of Gibbs sampling is to simulate the
posterior distribution of the unobserved variables in
the model. As the sampling procedure converges,
knowledge contained in the informative but struc-
turally weak prior ? is effectively passed to the syn-
tactic transfer model ?. Once the sampling proce-
dure has converged to a stationary distribution, we
can run it for as many additional iterations as we
want and sample the imputed target-language trees.
Those trees can then be collected in a treebank, thus
creating novel syntactically annotated data in the tar-
get language, which can be used for further process-
ing in syntax-based NLP tasks.
Acknowledgements
I would like to thank Steven Abney, the participants
of the 2005 Johns Hopkins workshop on Arabic di-
alect parsing, and the anonymous reviewers for help-
ful discussions. The usual disclaimers apply.
References
Y. Bar-Hillel, M. Perles, and E. Shamir. 1961. On
formal properties of simple phrase structure
grammars. Zeitschrift fu?r Phonetik, Sprach-
wissenschaft und Kommunikationsforschung,
14(2):143?172.
Alan E. Gelfand and Adrian F. M. Smith. 1990.
Sampling-based approaches to calculating
marginal densities. Journal of the American
Statistical Association, 85(410):398?409.
Stuart Geman and Donald Geman. 1984. Stochastic
relaxation, Gibbs distributions, and the Bayesian
restoration of images. IEEE Transactions on
Pattern Matching and Machine Intelligence,
6(6):721?741.
Irene Langkilde. 2000. Forest-based statistical
sentence generation. In Proceedings of the First
Meeting of the North American Chapter of the
Association for Computational Linguistics, pages
170?177. ACL Anthology A00-2023.
Jun S. Liu. 2001. Monte Carlo Strategies in Scien-
tific Computing. Springer.
Mohamed Maamouri, Ann Bies, Tim Buckwalter,
and Hubert Jin. 2004a. Arabic Treebank: Part 2
v 2.0. Electronic resource, available from LDC.
Mohamed Maamouri, Ann Bies, Tim Buckwalter,
and Hubert Jin. 2004b. Arabic Treebank: Part 3
v 1.0. Electronic resource, available from LDC.
Mohamed Maamouri, Ann Bies, Hubert Jin, and
Tim Buckwalter. 2003. Arabic Treebank: Part 1
v 2.0. Electronic resource, available from LDC.
Kevin Mark, Michael Miller, Ulf Grenander, and
Steve Abney. 1992. Parameter estimation for
constrained context-free language models. In
Speech and Natural Language: Proceedings of a
Workshop Held at Harriman, New York, Febru-
ary 23?26, 1992, pages 146?149. ACL Anthol-
ogy H92-1028.
Martin A. Tanner and Wing Hung Wong. 1987. The
calculation of posterior distributions by data aug-
mentation. Journal of the American Statistical
Association, 82(398):528?540.
David Yarowksy and Grace Ngai. 2001. Induc-
ing multilingual POS taggers and NP bracketers
via robust projection across aligned corpora. In
Proceedings of the Second Meeting of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 200?207.
82
Proceedings of NAACL HLT 2009: Tutorials, pages 9?10,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
OpenFst: An Open-Source, Weighted Finite-State Transducer Library
and its Applications to Speech and Language
Michael Riley, Cyril Allauzen, and Martin Jansche, Google Inc.
Finite-state methods are well established in language and speech processing. OpenFst (available from
www.openfst.org) is a free and open-source software library for building and using finite automata, in
particular, weighted finite-state transducers (FSTs). This tutorial is an introduction to weighted finite-
state transducers and their uses in speech and language processing. While there are other weighted
finite-state transducer libraries, OpenFst (a) offers, we believe, the most comprehensive, general and
efficient set of operations; (b) makes available full source code; (c) exposes high- and low-level C++
APIs that make it easy to embed and extend; and (d) is a platform for active research and use among
many colleagues.
1 Tutorial Outline
1. Introduction to OpenFst
The first part of the tutorial introduces operations on weighted automata such as determiniza-
tion and intersection/composition as well as the corresponding OpenFst binaries and library-level
APIs that implement those operations. We describe how to read FSTs from simple textual de-
scriptions and combine them into larger and more complex machines, and optimize them using
simple command-line and library calls.
? Introduction
? Motivating examples
? Finite-state methods in NLP and Speech
? A quick tour of OpenFst
? Finite-state machines and operations
? OpenFst binaries
? High-level C++ API
? Human-readable file formats
? Comparison of OpenFst and competing libraries
? Comparison with the AT&T FSM LibraryTM
? Brief comparison with SFST and related libraries
? Advanced usage of OpenFst
? Low-level C++ API
? Constructing and modifying Fst objects programmatically
? Implementing new concrete Fst classes
? Adding new weight semirings
? Customizing matchers and filters for composition
9
2. Applications
The second part of the tutorial focuses on several application areas of interest to the NAACL HLT
audience, including speech recognition, speech synthesis, and general text processing. In each
application area we discuss a straightforward example in detail, then delve into an advanced ex-
ample that highlights important features of OpenFst, common pitfalls, efficiency considerations,
new research directions, etc. These examples are drawn from our extensive experience in applying
OpenFst to problems in speech and language processing.
? Automatic speech recognition
? Context dependency transducers
? Language models and grammars
? Natural Language Processing
? Unicode processing
? Text analysis, text normalization
? Pronunciation models
? Other areas
? Computational biology
? Pattern matching
2 Target Audience
This tutorial is intended for students, researchers, and practitioners interested in applying finite-state
methods. It is suitable for participants of a variety of backgrounds, including those without any back-
ground in finite-state techniques as well as advanced users of existing software packages. For those
unfamiliar with finite-state transducers, the first portion of the tutorial provides an introduction to the
theory and application areas. Users of other finite-state libraries will particularly benefit from the con-
trastive description of the unique features of OpenFst, its C++ API, and the examples drawn from
real-world applications.
3 Presenters
Michael Riley began his career at Bell Labs and AT&T Labs where he, together with Mehryar Mohri and
Fernando Pereira, introduced and developed the theory and use of weighted finite-state transducers (WF-
STs) in speech and language. This work was recognized in best paper awards from the journals Speech
Communication and Computer Speech and Language. He has been a research scientist at Google, Inc.
since 2003. He is a principal author of the OpenFst library and the AT&T FSM LibraryTM. He has given
several tutorials on WFSTs before: at ACL 1994, Coling 1997 and Interspeech 2002.
Cyril Allauzen is another key author of the OpenFst library. His main research interests are in finite-state
methods and their applications to text, speech and natural language processing and machine learning.
Before joining Google, he worked as a researcher at AT&T Labs ? Research and at NYU?s Courant
Institute of Mathematical Sciences.
Martin Jansche has applied the OpenFst library to several speech and language problems at Google. His
FST-related interests are in text processing for speech tasks and in learning and applying pronunciation
and transliteration models.
10
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 32?35,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Named Entity Transcription with Pair n-Gram Models
Martin Jansche
Google Inc.
mjansche@google.com
Richard Sproat
Google Inc. and OHSU
rws@google.com
Abstract
We submitted results for each of the eight
shared tasks. Except for Japanese name
kanji restoration, which uses a noisy channel
model, our Standard Run submissions were
produced by generative long-range pair n-
gram models, which we mostly augmented
with publicly available data (either from
LDC datasets or mined from Wikipedia) for
the Non-Standard Runs.
1 Introduction
This paper describes the work that we did at Google,
Inc. for the NEWS 2009 Machine Transliteration
Shared Task (Li et al, 2009b; Li et al, 2009a). Except
for the Japanese kanji task (which we describe be-
low), all models were pair n-gram language models.
Briefly, we took the training data, and ran an iterative
alignment algorithm using a single-state weighted
finite-state transducer (WFST).We then trained a lan-
guage model on the input-output pairs of the align-
ment, which was then converted into a WFST encod-
ing a joint model. For the Non-Standard runs, we use
additional data fromWikipedia or from the LDC, ex-
cept where noted below. In the few instances where
we used data not available from Wikipedia or LDC,
wewill be happy to share themwith other participants
of this competition.
2 Korean
For Korean, we created a mapping between each
Hangul glyph and its phonetic transcription inWorld-
Bet (Hieronymus, 1993) based on the tables from
Unitran (Yoon et al, 2007). Vowel-initial syllables
were augmented with a ?0? at the beginning of the
syllable, to avoid spurious resyllabifications: Abbott
should be ???, never ???. We also filtered the
set of possible Hangul syllable combinations, since
certain syllables are never used in transliterations, e.g.
any with two consonants in the coda. The mapping
between Hangul syllables and phonetic transcription
was handled with a simple FST.
The main transliteration model for the Standard
Run was a 10-gram pair language model trained on
an alignment of English letters to Korean phonemes.
All transliteration pairs observed in the training/
development data were cached, and made available
if those names should recur in the test data. We
also submitted a Non-Standard Run with English/
Korean pairs mined from Wikipedia. These were de-
rived from the titles of corresponding interlinked En-
glish and Korean articles. Obviously not all such
pairs are transliterations, so we filtered the raw list
by predicting, for each English word, and using the
trained transliteration model, what the ten most likely
transliterations were in Korean; and then accepting
any pair in Wikipedia where the string in Korean also
occurred in the set of predicted transliterations. This
resulted in 11,169 transliteration pairs. In addition a
dictionary of 9,047 English and Korean translitera-
tion pairs that we had obtained from another source
was added. These pairs were added to the cache, and
were also used to retrain the transliteration model,
along with the provided data.
3 Indian Languages
For the Indian languages Hindi, Tamil and Kannada,
the same basic approach as for Korean was used. We
created a reversible map between Devanagari, Tamil
or Kannada symbols and their phonemic values, us-
ing a modified version of Unitran. However, since
Brahmi-derived scripts distinguish between diacritic
and full vowel forms, in order to map back from
phonemic transcription into the script form, it is nec-
essary to know whether a vowel comes after a conso-
nant or not, in order to select the correct form. These
and other constraints were implementedwith a simple
hand-constructed WFST for each script.
The main transliteration model for the Standard
Run was a 6-gram pair language model trained on
an alignment of English letters to Hindi, Kannada
32
or Tamil phonemes in the training and development
sets. At test time, this WFST was composed with the
phoneme to letter WFST just described to produce a
WFST that maps directly between English letters and
Indian script forms. As with Korean, all observed
transliteration pairs from the training/development
data were cached, and made available if those names
should recur in the test data. For each Indian lan-
guage we also submitted a Non-Standard Run which
included English/Devanagari, English/Tamil and En-
glish/Kannada pairs mined from Wikipedia, and fil-
tered as described above for Korean. This resulted
in 11,674 pairs for English/Hindi, 10,957 pairs for
English/Tamil and 2,436 pairs for English/Kannada.
These pairs were then added to the cache, and were
also used to retrain the transliteration model, along
with the provided data.
4 Russian
For Russian, we computed a direct letter/letter cor-
respondences between the Latin representation of
English and the Cyrillic representation of Russian
words. This seemed to be a reasonable choice since
Russian orthography is fairly phonemic, at least at an
abstract level, and it was doubtful that any gain would
be had from trying to model the pronunciation better.
We note that many of the examples were, in fact, not
English to begin with, but a variety of languages, in-
cluding Polish and others, that happen to be written
in the Latin script.
We used a 6-gram pair language model for the
Standard Run. For the Non-Standard Runs we in-
cluded: (for NSR1) a list of 3,687 English/Russian
pairs mined from the Web; and (for NSR2), those,
plus a set of 1,826 mined fromWikipedia and filtered
as described above. In each case, the found pairs were
put in the cache, and were used to retrain the language
model.
5 Chinese
For Chinese, we built a direct stochastic model be-
tween strings of Latin characters representing the En-
glish names and strings of hanzi representing their
Chinese transcription. It is well known (Zhang et al,
2004) that the direct approach produces significantly
better transcription quality than indirect approaches
based on intermediate pinyin or phoneme represen-
tations. This observation is consistent with our own
experience during system development.
In our version of the direct approach, we first
aligned the English letter strings with their corre-
sponding Chinese hanzi strings using the same mem-
oryless monotonic alignment model as before. We
then built standard n-gram models over the align-
ments, which were then turned, for use at runtime,
into weighted FSTs computing a mapping from En-
glish to Chinese.
The transcription model we chose for the Stan-
dard Run is a 6-gram language model over align-
ments, built with Kneser-Ney smoothing and a mini-
mal amount of Seymore-Rosenfeld shrinking.
We submitted two Non-Standard Runs with addi-
tional names taken from the LDC Chinese/English
Name Entity Lists v 1.0 (LDC2005T34). The only list
from this collection we used was Propernames Peo-
ple EC, which contains 572,213 ?English? names (in
fact, names from many languages, all represented in
the Latin alphabet) with one or more Chinese tran-
scriptions for each name. Data of similar quality can
be easily extracted from theWeb as well. For the sake
of reproducible results, we deliberately chose to work
with a standard corpus. The LDC name lists have
all of the problems that are usually associated with
data extracted from the Web, including improbable
entries, genuine mistakes, character substitutions, a
variety of unspecified source languages, etc.
We removed names with symbols other than let-
ters ?a? through ?z? from the list and divided it into
a held-out portion, consisting of names that occur in
the development or test data of the Shared Task, and
a training portion, consisting of everything else, for a
total of 622,187 unique English/Chinese name pairs.
We then used the model from the Standard Run to
predict multiple pronunciations for each of the names
in the training portion of the LDC list and retained
up to 5 pronunciations for each English name where
the prediction from the Standard model agreed with
a pronunciation found in the LDC list.
For our first Non-Standard Run, we trained a 7-
gram language model based on the Shared Task train-
ing data (31,961 name pairs) plus an additional 95,576
name pairs from the intersection of the LDC list and
the Standard model predictions. Since the selection
of additional training data was, by design, very con-
servative, we got a small improvement over the Stan-
dard Run.
The reason for this cautious approach was that the
additional LDC data did not match the provided train-
ing and development data very well, partly due to
noise, partly due to different transcription conven-
tions. For example, the Pinyin syllable bo? is predom-
inantly written as? in the LDC data, but? does not
33
occur at all in the Shared Task training data:
Character Occurrences
Train LDC
? 0 13,110
? 1,547 3,709
We normalized the LDC data (towards the tran-
scription conventions implicit in the Shared Task
data) by replacing hanzi for frequent Pinyin syllables
with the predominant homophonous hanzi from the
Shared Task data. This resembles a related approach
to pronunciation extraction from the web (Ghoshal et
al., 2009), where extraction validation and pronunci-
ation normalization steps were found to be tremen-
dously helpful, even necessary, when using web-
derived pronunciations. One of the conclusions there
was that extracted pronunciations should be used di-
rectly when available.
This is what we did in our second Non-Standard
Run. We used the filtered and normalized LDC data
as a static dictionary in which to look up the transcrip-
tion of names in the test data. This is how the shared
task problem would be solved in practice and it re-
sulted in a huge gain in quality. Notice, however, that
doing so is non-trivial, because of the data quality and
data mismatch problems described above.
6 Japanese Katakana
The ?English? to Japanese katakana task suffered
from the usual problem that the Latin alphabet side
covered many languages besides English. It thus be-
came an exercise in guessing which one of many valid
ways of pronouncing the Latin letter string would be
chosen as the basis for the Japanese transcription. We
toyed with the idea of building mixture models before
deciding that this issue is more appropriate for a pro-
nunciation modeling shared task. In the end, we built
the same kinds of straightforward pair n-grammodels
as in the tasks described earlier.
For Japanese katakana we performed a similar
kind of preprocessing as for the Indian languages:
since it is possible (under minimal assumptions)
to construct an isomorphism between katakana and
Japanese phonemes, we chose to use phonemes as
the main level of representation in our model. This
is because Latin letters encode phonemes as opposed
to syllables or morae (to a first approximation) and
one pays a penalty (a loss of about 4% in accuracy on
the development data) for constructingmodels that go
from Latin letters directly to katakana.
For the Standard Run, we built a 5-grammodel that
maps from Latin letter strings to Japanese phoneme
strings. The model used the same kind of Kneser-
Ney smoothing and Seymore-Rosenfeld shrinking as
before. In addition, we restrict the model to only pro-
duce well-formed Japanese phoneme strings, by com-
posing it with an unweighted Japanese phonotactic
model that enforces the basic syllable structure.
7 Japanese Name Kanji
It is important to note that the Japanese name kanji
task is conceptually completely different from all of
the other tasks. We argue that this conceptual dif-
ference must translate into a different modeling and
system building approach.
The conceptual difference is this: In all other tasks,
we?re given well-formed ?English? names. For the
sake of argument, let?s say that they are indeed just
English names. These names have an English pro-
nunciation which is then mapped to a correspond-
ing Hindi or Korean pronunciation, and the resulting
Hindi or Korean ?words? (which do not look like or-
dinary Hindi or Korean words at all, except for su-
perficially following the phonology of the target lan-
guage) can be written down in Devanagari or Hangul.
Information is lost when distinct English sounds get
mapped to the same phonemes in the target language
andwhen semantic information (such as the gender of
the bearer of a name) is simply not transmitted across
the phonetic channel that produces the approximation
in the target language (transcription into Chinese is an
exception in this regard). We call this forward tran-
scription because we?re projecting the original repre-
sentation of a name onto an impoverished approxima-
tion.
In name kanji restoration, we?re moving in the op-
posite direction. The most natural, information-rich
form of a Japanese name is its kanji representation
(ja-Hani). When this gets transcribed into ro?maji (ja-
Latn), only the sound of the name is preserved. In
this task, we?re asked to recover the richer kanji form
from the impoverished ro?maji form. This is the op-
posite of the forward transcription tasks and just begs
to be described by a noisy channel model, which is
exactly what we did.
The noisy channel model is a factored generative
model that can be thought of as operating by drawing
an item (kanji string) from a source model over the
universe of Japanese names, and then, conditional on
the kanji, generating the observation (ro?maji string)
in a noisy, nondeterministic fashion, by drawing it at
random from a channel model (in this case, basically
a model of kanji readings).
To simplify things, we make the natural assump-
34
tion that there is a latent segmentation of the ro?maji
string into segments of one or more syllables and
that each individual kanji in a name generates exactly
one segment. For illustration, consider the example
abukawa ??, which has three possible segmenta-
tions: a+bukawa, abu+kawa, and abuka+wa. Note
that boundaries can fall into the middle of ambisyl-
labic long consonants, as in matto??.
Complicating this simple picture are several kinds
of noise in the training data: First, Chinese pinyin
mixed in with Japanese ro?maji, which we removed
mostly automatically from the training and develop-
ment data and for which we deliberately chose not to
produce guesses in the submitted runs on the test data.
Second, the seemingly arbitrary coalescence of cer-
tain vowel sequences. For example, o?numa ?? and
onuma?? appear as onuma, and kouda??? and
ko?da?? appear as koda in the training data. Severe
space limitations prevent us from going into further
details here: we will however discuss the issues dur-
ing our presentation at the workshop.
For the Standard Run, we built a trigram character
language model on the kanji names (16,182 from the
training data plus 3,539 from the development data,
discarding pinyin names). We assume a zero-order
channel model, where each kanji generates its portion
of the ro?maji observation independent of its kanji or
ro?maji context. We applied an EM algorithm to the
parallel ro?maji/kanji data (19,684 items) in order to
segment the ro?maji under the stated assumptions and
train the channel model. We pruned the model by re-
placing the last EM step with a Viterbi step, result-
ing in faster runtime with no loss in quality. NSR 1
uses more than 100k additional names (kanji only,
no additional parallel data) extracted from biograph-
ical articles in Wikipedia, as well as a list, found on
the Web, of the 10,000 most common Japanese sur-
names. A total of 117,782 names were used to train a
trigram source model. Everything else is identical to
the Standard Run. NSR 2 is like NSR 1 but adds dic-
tionary lookup. If we find the ro?maji name in a dictio-
nary of 27,358 names extracted from Wikipedia and
if a corresponding kanji name from the dictionary is
among the top 10 hypotheses produced by the model,
that hypothesis is promoted to the top (again, this per-
forms better than using the extracted names blindly).
NSR 3 is like NSR 1 but the channel model is trained
on a total of 108,172 ro?maji/kanji pairs consisting of
the training and development data plus data extracted
from biographies in Wikipedia. Finally NSR 4 is like
NSR 3 but adds the same kind of dictionary lookup as
in NSR 2. Note that the biggest gains are due first to
the richer source model in NSR 1 and second to the
richer channel model in NSR 3. The improvements
due to dictionary lookups in NSR 2 and 4 are small
by comparison.
8 Results
Results for the runs are summarized below. ?Rank?
is rank in SR/NSR as appropriate:
Run ACC F Rank
en/ta SR 0.436 0.894 2
NSR1 0.437 0.894 5
ja-Latn/ SR 0.606 0.749 2
ja-Hani NSR1 0.681 0.790 4
NSR2 0.703 0.805 3
NSR3 0.698 0.805 2
NSR4 0.717 0.818 1
en/ru SR 0.597 0.925 3
NSR1 0.609 0.928 2
NSR2 0.955 0.989 1
en/zh SR 0.646 0.867 6
NSR1 0.658 0.865 10
NSR2 0.909 0.960 1
en/hi SR 0.415 0.858 9
NSR1 0.424 0.862 8
en/ko SR 0.476 0.742 1
NSR1 0.794 0.894 1
en/kn SR 0.370 0.867 2
NSR1 0.374 0.868 4
en/ja-Kana SR 0.503 0.843 3
NSR1 0.564 0.862 n/a
Acknowledgments
The authors acknowledge the use of the English-Chinese
(EnCh) (Li et al, 2004), English-Japanese Katakana (EnJa),
English-Korean Hangul (EnKo), Japanese Name (in English)-
Japanese Kanji (JnJk) (http://www.cjk.org), and English-
Hindi (EnHi), English-Tamil (EnTa), English-Kannada (EnKa),
English-Russian (EnRu) (Kumaran and Kellner, 2007) corpora.
References
Arnab Ghoshal, Martin Jansche, Sanjeev Khudanpur, Michael
Riley, and Morgan E. Ulinksi. 2009. Web-derived pronunci-
ations. In ICASSP.
James L. Hieronymus. 1993. ASCII phonetic symbols for the
world?s languages: Worldbet. AT&T Bell Laboratories, tech-
nical memorandum.
A. Kumaran and Tobias Kellner. 2007. A generic framework for
machine transliteration. In SIGIR--30.
Haizhou Li, Min Zhang, and Jian Su. 2004. A joint source chan-
nel model for machine transliteration. In ACL-42.
Haizhou Li, A. Kumaran, Vladimir Pervouchine, andMin Zhang.
2009a. Report on NEWS 2009 machine transliteration shared
task. In ACL-IJCNLP 2009 Named Entities Workshop, Singa-
pore.
Haizhou Li, A. Kumaran, Min Zhang, andVladimir Pervouchine.
2009b. Whitepaper of NEWS 2009 machine transliteration
shared task. In ACL-IJCNLP 2009 Named Entities Workshop,
Singapore.
Su-Youn Yoon, Kyoung-Young Kim, and Richard Sproat. 2007.
Multilingual transliteration using feature based phonetic
method. In ACL.
Min Zhang, Haizhou Li, and Jian Su. 2004. Direct orthographi-
cal mapping for machine transliteration. In COLING.
35
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 736?743,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A Maximum Expected Utility Framework for Binary Sequence Labeling
Martin Jansche?
jansche@acm.org
Abstract
We consider the problem of predictive infer-
ence for probabilistic binary sequence label-
ing models under F-score as utility. For a
simple class of models, we show that the
number of hypotheses whose expected F-
score needs to be evaluated is linear in the
sequence length and present a framework for
efficiently evaluating the expectation of many
common loss/utility functions, including the
F-score. This framework includes both exact
and faster inexact calculation methods.
1 Introduction
1.1 Motivation and Scope
The weighted F-score (van Rijsbergen, 1974) plays
an important role in the evaluation of binary classi-
fiers, as it neatly summarizes a classifier?s ability to
identify the positive class. A variety of methods ex-
ists for training classifiers that optimize the F-score,
or some similar trade-off between false positives and
false negatives, precision and recall, sensitivity and
specificity, type I error and type II error rate, etc.
Among the most general methods are those of Mozer
et al (2001), whose constrained optimization tech-
nique is similar to those in (Gao et al, 2006; Jansche,
2005). More specialized methods also exist, for ex-
ample for support vector machines (Musicant et al,
2003) and for conditional random fields (Gross et al,
2007; Suzuki et al, 2006).
All of these methods are about classifier training.
In this paper we focus primarily on the related, but
orthogonal, issue of predictive inference with a fully
trained probabilistic classifier. Using the weighted
F-score as our utility function, predictive inference
amounts to choosing an optimal hypothesis which
maximizes the expected utility. We refer to this as
?Current affiliation: Google Inc. Former affiliation: Center
of Computational Learning Systems, Columbia University.
the prediction or decoding task. In general, decoding
can be a hard computational problem (Casacuberta
and de la Higuera, 2000; Knight, 1999). In this paper
we show that the maximum expected F-score decod-
ing problem can be solved in polynomial time under
certain assumptions about the underlying probabil-
ity model. One key ingredient in our solution is a
very general framework for evaluating the expected
F-score, and indeed many other utility functions, of
a fixed hypothesis.1 This framework can also be ap-
plied to discriminative classifier training.
1.2 Background and Notation
We formulate our approach in terms of sequence la-
beling, although it has applications beyond that. This
is motivated by the fact that our framework for evalu-
ating expected utility is indeed applicable to general
sequence labeling tasks, while our decoding method
is more restricted. Another reason is that the F-score
is only meaningful for comparing two (multi)sets or
two binary sequences, but the notation for multisets
is slightly more awkward.
All tasks considered here involve strings of binary
labels. We write the length of a given string y ?
{0,1}n as |y|= n. It is convenient to view such strings
as real vectors ? whose components happen to be 0
or 1 ? with the dot product defined as usual. Then
y ? y is the number of ones that occur in the string y.
For two strings x,y of the same length |x| = |y| the
number of ones that occur at corresponding indices
is x ? y.
Given a hypothesis z and a gold standard label
sequence y, we define the following quantities:
1. T = y ? y, the genuine positives;
2. P = z ? z, the predicted positives;
3. A = z ? y, the true positives (predicted positives
that are genuinely positive);
1A proof-of-concept implementation is available at http:
//purl.org/net/jansche/meu_framework/.
736
4. Recl = A/T , recall (a.k.a. sensitivity or power);
5. Prec = A/P, precision.
The ? -weighted F-score is then defined as the
weighted harmonic mean of recall and precision. This
simplifies to
F? =
(? +1)A
P+? T (? > 0) (1)
where we assume for convenience that 0/0
def
= 1 to
avoid explicitly dealing with the special case of the
denominator being zero. We will write the weighted
F-score from now on as F(z,y) to emphasize that it
is a function of z and y.
1.3 Expected F-Score
In Section 3 we will develop a method for evaluating
the expectation of the F-score, which can also be
used as a smooth approximation of the raw F-score
during classifier training: in that task (which we will
not discuss further in this paper), z are the supervised
labels, y is the classifier output, and the challenge is
that F(z,y) does not depend smoothly on the param-
eters of the classifier. Gradient-based optimization
techniques are not applicable unless some of the quan-
tities defined above are replaced by approximations
that depend smoothly on the classifier?s parameters.
For example, the constrained optimization method
of (Mozer et al, 2001) relies on approximations of
sensitivity (which they call CA) and specificity2 (their
CR); related techniques (Gao et al, 2006; Jansche,
2005) rely on approximations of true positives, false
positives, and false negatives, and, indirectly, recall
and precision. Unlike these methods we compute the
expected F-score exactly, without relying on ad hoc
approximations of the true positives, etc.
Being able to efficiently compute the expected
F-score is a prerequisite for maximizing it during de-
coding. More precisely, we compute the expectation
of the function
y 7? F(z,y), (2)
which is a unary function obtained by holding the
first argument of the binary function F fixed. It will
henceforth be abbreviated as F(z, ?), and we will de-
note its expected value by
E [F(z, ?)] = ?
y?{0,1}|z|
F(z,y) Pr(y). (3)
2Defined as [(~1? z) ? (~1? y)]
/
[(~1? y) ? (~1? y)].
This expectation is taken with respect to a probability
model over binary label sequences, written as Pr(y)
for simplicity. This probability model may be condi-
tional, that is, in general it will depend on covariates
x and parameters ? . We have suppressed both in our
notation, since x is fixed during training and decod-
ing, and we assume that the model is fully identified
during decoding. This is for clarity only and does not
limit the class of models, though we will introduce
additional, limiting assumptions shortly. We are now
ready to tackle the inference task formally.
2 Maximum Expected F-Score Inference
2.1 Problem Statement
Optimal predictive inference under F-score utility
requires us to find an hypothesis z? of length n which
maximizes the expected F-score relative to a given
probabilistic sequence labeling model:
z? = argmax
z?{0,1}n
E [F(z, ?)] = argmax
z?{0,1}n
?
y
F(z,y) Pr(y).
(4)
We require the probability model to factor into inde-
pendent Bernoulli components (Markov order zero):
Pr(y = (y1, . . . ,yn)) =
n
?
i=1
pyii (1? pi)
1?yi . (5)
In practical applications we might choose the overall
probability distribution to be the product of indepen-
dent logistic regression models, for example. Ordi-
nary classification arises as a special case when the
yi are i.i.d., that is, a single probabilistic classifier is
used to find Pr(yi = 1 | xi). For our present purposes
it is sufficient to assume that the inference algorithm
takes as its input the vector (p1, . . . , pn), where pi is
the probability that yi = 1.
The discrete maximization problem (4) cannot be
solved naively, since the number of hypotheses that
would need to be evaluated in a brute-force search for
an optimal hypothesis z? is exponential in the sequence
length n. We show below that in fact only a few
hypotheses (n+1 instead of 2n) need to be examined
in order to find an optimal one.
The inference algorithm is the intuitive one, analo-
gous to the following simple observation: Start with
the hypothesis z = 00 . . .0 and evaluate its raw F-
score F(z,y) relative to a fixed but unknown binary
737
string y. Then z will have perfect precision (no posi-
tive labels means no chance to make mistakes), and
zero recall (unless y = z). Switch on any bit of z that
is currently off. Then precision will decrease or re-
main equal, while recall will increase or remain equal.
Repeat until z = 11 . . .1 is reached, in which case re-
call will be perfect and precision at its minimum. The
inference algorithm for expected F-score follows the
same strategy, and in particular it switches on the
bits of z in order of non-increasing probability: start
with 00 . . .0, then switch on the bit i1 = argmaxi pi,
etc. until 11 . . .1 is reached. We now show that this
intuitive strategy is indeed admissible.
2.2 Outer and Inner Maximization
In general, maximization can be carried out piece-
wise, since
argmax
x?X
f (x) = argmax
x?{argmaxy?Y f (y)|Y?pi(X)}
f (x),
where pi(X) is any family (Y1,Y2, . . .) of nonempty
subsets of X whose union
?
iYi is equal to X . (Recur-
sive application would lead to a divide-and-conquer
algorithm.) Duplication of effort is avoided if pi(X)
is a partition of X .
Here we partition the set {0,1}n into equivalence
classes based on the number of ones in a string
(viewed as a real vector). Define Sm to be the set
Sm = {s ? {0,1}
n | s ? s = m}
consisting of all binary strings of fixed length n that
contain exactly m ones. Then the maximization prob-
lem (4) can be transformed into an inner maximiza-
tion
s?(m) = argmax
s?Sm
E [F(s, ?)] , (6)
followed by an outer maximization
z? = argmax
z?{s?(0),...,s?(n)}
E [F(z, ?)] . (7)
2.3 Closed-Form Inner Maximization
The key insight is that the inner maximization prob-
lem (6) can be solved analytically. Given a vector
p = (p1, . . . , pn) of probabilities, define z(m) to be the
binary label sequence with exactly m ones and n?m
zeroes where for all indices i,k we have
[
z(m)i = 1? z
(m)
k = 0
]
? pi ? pk.
Algorithm 1 Maximizing the Expected F-Score.
1: Input: probabilities p = (p1, . . . , pn)
2: I? indices of p sorted by non-increasing probability
3: z? 0 . . .0
4: a? 0
5: v? expectF(z, p)
6: for j? 1 to n do
7: i? I[ j]
8: z[i]? 1 // switch on the ith bit
9: u? expectF(z, p)
10: if u > v then
11: a? j
12: v? u
13: for j? a+1 to n do
14: z[I[ j]]? 0
15: return (z,v)
In other words, the most probable m bits (according
to p) in z(m) are set and the least probable n?m bits
are off. We rely on the following result, whose proof
is deferred to Appendix A:
Theorem 1. (?s ? Sm) E [F(z(m), ?)]? E [F(s, ?)].
Because z(m) is maximal in Sm, we may equate
z(m) = argmaxs?Sm E [F(s, ?)] = s?
(m) (modulo ties,
which can always arise with argmax).
2.4 Pedestrian Outer Maximization
With the inner maximization (6) thus solved, the outer
maximization (7) can be carried out naively, since
only n+ 1 hypotheses need to be evaluated. This
is precisely what Algorithm 1 does, which keeps
track of the maximum value in v. On termination
z = argmaxs E [F(s, ?)]. Correctness follows directly
from our results in this section.
Algorithm 1 runs in time O(n logn+ n f (n)). A
total of O(n logn) time is required for accessing the
vector p in sorted order (line 2). This dominates the
O(n) time required to explicitly generate the optimal
hypothesis (lines 13?14). The algorithm invokes a
subroutine expectF(z, p) a total of n+1 times. This
subroutine, which is the topic of the next section,
evaluates, in time f (n), the expected F-score (with
respect to p) of a given hypothesis z of length n.
3 Computing the Expected F-Score
3.1 Problem Statement
We now turn to the problem of computing the ex-
pected value (3) of the F-score for a given hypothesis
z relative to a fully identified probability model. The
method presented here does not strictly require the
738
zeroth-order Markov assumption (5) instated earlier
(a higher-order Markov assumption will suffice), but
it shall remain in effect for simplicity.
As with the maximization problem (4), the sum
in (3) is over exponentially many terms and cannot be
computed naively. But observe that the F-score (1)
is a (rational) function of integer counts which are
bounded, so it can take on only a finite, and indeed
small, number of distinct values. We shall see shortly
that the function (2) whose expectation we wish to
compute has a domain whose cardinality is exponen-
tial in n, but the cardinality of its range is polynomial
in n. The latter is sufficient to ensure that its ex-
pectation can be computed in polynomial time. The
method we are about to develop is in fact very general
and applies to many other loss and utility functions
besides the F-score.
3.2 Expected F-Score as an Integral
A few notions from real analysis are helpful because
they highlight the importance of thinking about func-
tions in terms of their range, level sets, and the equiv-
alence classes they induce on their domain (the kernel
of the function). A function g : ?? R is said to be
simple if it can be expressed as a linear combination
of indicator functions (characteristic functions):
g(x) = ?
k?K
ak ?Bk(x),
where K is a finite index set, ak ? R, and Bk ? ?.
(?S : S?{0,1} is the characteristic function of set S.)
Let ? be a countable set and P be a probability
measure on ?. Then the expectation of g is given by
the Lebesgue integral of g. In the case of a simple
function g as defined above, the integral, and hence
the expectation, is defined as
E [g] =
?
?
g dP = ?
k?K
akP(Bk). (8)
This gives us a general recipe for evaluating E[g]
when ? is much larger than the range of g. Instead of
computing the sum ?y?? g(y)P({y}) we can com-
pute the sum in (8) above. This directly yields an
efficient algorithm whenever K is sufficiently small
andP(Bk) can be evaluated efficiently.
The expected F-score is thus the Lebesgue integral
of the function (2). Looking at the definition of the
0,0
Y:n, n:n 1,1Y:Y
0,1
n:Y
Y:n, n:n
2,2
Y:Y
1,2
n:Y
Y:n, n:n Y:Y
0,2
n:Y
Y:n, n:n 3,3Y:Y
2,3
n:Y
Y:n, n:n Y:Y
1,3
n:Y
Y:n, n:n Y:Y
0,3
n:Y
Y:n, n:n
Y:n, n:n
Y:n, n:n
Y:n, n:n
Figure 1: Finite State Classifier h?.
F-score in (1) we see that the only expressions which
depend on y are A = z ? y and T = y ? y (P = z ? z is
fixed because z is). But 0 ? z ? y ? y ? y ? n = |z|.
Therefore F(z, ?) takes on at most (n+1)(n+2)/2,
i.e. quadratically many, distinct values. It is a simple
function with
K = {(A,T ) ? N0?N0 | A? T ? |z|, A? z ? z}
a(A,T ) =
(? +1)A
z ? z+? T where 0/0
def
= 1
B(A,T ) = {y | z ? y = A, y ? y = T}.
3.3 Computing Membership in Bk
Observe that the family of sets
(
B(A,T )
)
(A,T )?K
is a
partition (namely the kernel of F(z, ?)) of the set ?=
{0,1}n of all label sequences of length n. In turn it
gives rise to a function h : ?? K where h(y) = k
iff y ? Bk. The function h can be computed by a
deterministic finite automaton, viewed as a sequence
classifier: rather than assigning binary accept/reject
labels, it assigns arbitrary labels from a finite set, in
this case the index set K. For simplicity we show
the initial portion of a slightly more general two-tape
automaton h? in Figure 1. It reads the two sequences
z and y on its two input tapes and counts the number
of matching positive labels (represented as Y) as well
as the number of positive labels on the second tape.
Its behavior is therefore h?(z,y) = (z ? y, y ? y). The
function h is obtained as a special case when z (the
first tape) is fixed.
Note that this only applies to the special case when
739
Algorithm 2 Simple Function Instance for F-Score.
def start():
return (0,0)
def transition(k,z, i,yi):
(A,T )? k
if yi = 1 then
T ? T +1
if z[i] = 1 then
A? A+1
return (A,T )
def a(k,z):
(A,T )? k
F ?
(? +1)A
z ? z+? T // where 0/0
def
= 1
return F
Algorithm 3 Value of a Simple Function.
1: Input: instance g of the simple function interface, strings z
and y of length n
2: k? g.start()
3: for i? 1 to n do
4: k? g.transition(k,z, i,y[i])
5: return g.a(k,z)
the family B = (Bk)k?K is a partition of ?. It is al-
ways possible to express any simple function in this
way, but in general there may be an exponential in-
crease in the size of K when the family B is required
to be a partition. However for the special cases we
consider here this problem does not arise.
3.4 The Simple Function Trick
In general, what we will call the simple function trick
amounts to representing the simple function g whose
expectation we want to compute by:
1. a finite index set K (perhaps implicit),
2. a deterministic finite state classifier h : ?? K,
3. and a vector of coefficients (ak)k?K .
In practice, this means instantiating an interface with
three methods: the start and transition function of the
transducer which computes h? (and from which h can
be derived), and an accessor method for the coeffi-
cients a. Algorithm 2 shows the F-score instance.
Any simple function g expressed as an instance of
this interface can then be evaluated very simply as
g(x) = ah(x). This is shown in Algorithm 3.
Evaluating E [g] is also straightforward: Compose
the DFA h with the probability model p and use an al-
gebraic path algorithm to compute the total probabil-
ity massP(Bk) for each final state k of the resulting
automaton. If p factors into independent components
as required by (5), the composition is greatly sim-
Algorithm 4 Expectation of a Simple Function.
1: Input: instance g of the simple function interface, string z
and probability vector p of length n
2: M?Map()
3: M[g.start()]? 1
4: for i? 1 to n do
5: N?Map()
6: for (k,P) ?M do
7: // transition on yi = 0
8: k0? g.transition(k,z, i,0)
9: if k0 /? N then
10: N[k0]? 0
11: N[k0]? N[k0]+P? (1? p[i])
12: // transition on yi = 1
13: k1? g.transition(k,z, i,1)
14: if k1 /? N then
15: N[k1]? 0
16: N[k1]? N[k1]+P? p[i]
17: M? N
18: E? 0
19: for (k,P) ?M do
20: E? E +g.a(k,z)?P
21: return E
plified. If p incorporates label history (higher-order
Markov assumption), nothing changes in principle,
though the following algorithm assumes for simplic-
ity that the stronger assumption is in effect.
Algorithm 4 expands the following composed au-
tomaton, represented implicitly: the finite-state trans-
ducer h? specified as part of the simple function object
g is composed on the left with the string z (yielding
h) and on the right with the probability model p. The
outer loop variable i is an index into z and hence a
state in the automaton that accepts z; the variable
k keeps track of the states of the automaton imple-
mented by g; and the probability model has a single
state by assumption, which does not need to be rep-
resented explicitly. Exploring the states in order of
increasing i puts them in topological order, which
means that the algebraic path problem can be solved
in time linear in the size of the composed automaton.
The maps M and N keep track of the algebraic dis-
tance from the start state to each intermediate state.
On termination of the first outer loop (lines 4?17),
the map M contains the final states together with
their distances. The algebraic distance of a final state
k is now equal to P(Bk), so the expected value E
can be computed in the second loop (lines 18?20) as
suggested by (8).
When the utility function interface g is instantiated
as in Algorithm 2 to represent the F-score, the run-
time of Algorithm 4 is cubic in n, with very small
740
constants.3 The first main loop iterates over n. The
inner loop iterates over the states expanded at itera-
tion i, of which there are O(i2) many when dealing
with the F-score. The second main loop iterates over
the final states, whose number is quadratic in n in
this case. The overall cubic runtime of the first loop
dominates the computation.
3.5 Other Utility Functions
With other functions g the runtime of Algorithm 4
will depend on the asymptotic size of the index set K.
If there are asymptotically as many intermediate
states at any point as there are final states, then the
general asymptotic runtime is O(n |K|).
Many loss/utility functions are subsumed by the
present framework. Zero?one loss is trivial: the au-
tomaton has two states (success, failure); it starts and
remains in the success state as long as the symbols
read on both tapes match; on the first mismatch it
transitions to, and remains in, the failure state.
Hamming (1950) distance is similar to zero?one
loss, but counts the number of mismatches (bounded
by n), whereas zero?one loss only counts up to a
threshold of one.
A more interesting case is given by the Pk-score
(Beeferman et al, 1999) and its generalizations,
which moves a sliding window of size k over a pair
of label sequences (z,y) and counts the number of
windows which contain a segment boundary on one
of the sequences but not the other. To compute its
expectation in our framework, all we have to do is
express the sliding window mechanism as an automa-
ton, which can be done very naturally (see the proof-
of-concept implementation for further details).
4 Faster Inexact Computations
Because the exact computation of the expected F-
score by Algorithm 4 requires cubic time, the overall
runtime of Algorithm 1 (the decoder) is quartic.4
3A tight upper bound on the total number of states of the com-
posed automaton in the worst case is
?
1
12 n
3 + 58 n
2 + 1712 n+1
?
.
4It is possible to speed up the decoding algorithm in absolute
terms, though not asymptotically, by exploiting the fact that it
explores very similar hypotheses in sequence. Algorithm 4 can
be modified to store and return all of its intermediate map data-
structures. This modified algorithm then requires cubic space
instead of quadratic space. This additional storage cost pays
off when the algorithm is called a second time, with its formal
parameter z bound to a string that differs from the one of the
Faster decoding can be achieved by modifying Al-
gorithm 4 to compute an approximation (in fact, a
lower bound) of the expected F-score.5 This is done
by introducing an additional parameter L which limits
the number of intermediate states that get expanded.
Instead of iterating over all states and their associ-
ated probabilities (inner loop starting at line 6), one
iterates over the top L states only. We require that
L? 1 for this to be meaningful. Before entering the
inner loop the entries of the map M are expanded
and, using the linear time selection algorithm, the
top L entries are selected. Because each state that
gets expanded in the inner loop has out-degree 2, the
new state map N will contain at most 2L states. This
means that we have an additional loop invariant: the
size of M is always less than or equal to 2L. There-
fore the selection algorithm runs in time O(L), and
so does the abridged inner loop, as well as the sec-
ond outer loop. The overall runtime of this modified
algorithm is therefore O(n L).
If L is a constant function, the inexact computation
of the expected F-score runs in linear time and the
overall decoding algorithm in quadratic time. In par-
ticular if L = 1 the approximate expected F-score is
equal to the F-score of the MAP hypothesis, and the
modified inference algorithm reduces to a variant of
Viterbi decoding. If L is a linear function of n, the
overall decoding algorithm runs in cubic time.
We experimentally compared the exact quartic-
time decoding algorithm with the approximate decod-
ing algorithm for L= 2n and for L= 1. We computed
the absolute difference between the expected F-score
of the optimal hypothesis (as found by the exact al-
gorithm) and the expected F-score of the winning
hypothesis found by the approximate decoding algo-
rithm. For different sequence lengths n ? {1, . . . ,50}
we performed 10 runs of the different decoding al-
gorithms on randomly generated probability vectors
p, where each pi was randomly drawn from a contin-
uous uniform distribution on (0,1), or, in a second
experiment, from a Beta(1/2,1/2) distribution (to
simulate an over-trained classifier).
For L = 1 there is a substantial difference of about
preceding run in just one position. This means that the map
data-structures only need to be recomputed from that position
forward. However, this does not lead to an asymptotically faster
algorithm in the worst case.
5For error bounds, see the proof-of-concept implementation.
741
0.6 between the expected F-scores of the winning
hypothesis computed by the exact algorithm and by
the approximate algorithm. Nevertheless the approx-
imate decoding algorithm found the optimal hypoth-
esis more than 99% of the time. This is presumably
due to the additional regularization inherent in the
discrete maximization of the decoder proper: even
though the computed expected F-scores may be far
from their exact values, this does not necessarily af-
fect the behavior of the decoder very much, since it
only needs to find the maximum among a small num-
ber of such scores. The error introduced by the ap-
proximation would have to be large enough to disturb
the order of the hypotheses examined by the decoder
in such a way that the true maximum is reordered.
This generally does not seem to happen.
For L = 2n the computed approximate expected F-
scores were indistinguishable from their exact values.
Consequently the approximate decoder found the true
maximum every time.
5 Conclusion and Related Work
We have presented efficient algorithms for maximum
expected F-score decoding. Our exact algorithm runs
in quartic time, but an approximate cubic-time variant
is indistinguishable in practice. A quadratic-time
approximation makes very few mistakes and remains
practically useful.
We have further described a general framework
for computing the expectations of certain loss/utility
functions. Our method relies on the fact that many
functions are sparse, in the sense of having a finite
range that is much smaller than their codomain. To
evaluate their expectations, we can use the simple
function trick and concentrate on their level sets:
it suffices to evaluate the probability of those sets/
events. The fact that the commonly used utility func-
tions like the F-score have only polynomially many
level sets is sufficient (but not necessary) to ensure
that our method is efficient. Because the coefficients
ak can be arbitrary (in fact, they can be generalized to
be elements of a vector space over the reals), we can
deal with functions that go beyond simple counts.
Like the methods developed by Allauzen et al
(2003) and Cortes et al (2003) our technique incor-
porates finite automata, but uses a direct threshold-
counting technique, rather than a nondeterministic
counting technique which relies on path multiplici-
ties. This makes it easy to formulate the simultaneous
counting of two distinct quantities, such as our A and
T , and to reason about the resulting automata.
The method described here is similar in spirit to
those of Gao et al (2006) and Jansche (2005), who
discuss maximum expected F-score training of deci-
sion trees and logistic regression models. However,
the present work is considerably more general in two
ways: (1) the expected utility computations presented
here are not tied in any way to particular classifiers,
but can be used with large classes of probabilistic
models; and (2) our framework extends beyond the
computation of F-scores, which fall out as a special
case, to other loss and utility functions, including the
Pk score. More importantly, expected F-score com-
putation as presented here can be exact, if desired,
whereas the cited works always use an approximation
to the quantities we have called A and T .
Acknowledgements
Most of this research was conducted while I was affilated with
the Center for Computational Learning Systems, Columbia Uni-
versity. I would like to thank my colleagues at Google, in partic-
ular Ryan McDonald, as well as two anonymous reviewers for
valuable feedback.
References
Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2003. Gen-
eralized algorithms for constructing language models. In
Proceedings of the 41st Annual Meeting of the Association
for Computational Linguistics.
Doug Beeferman, Adam Berger, and John Lafferty. 1999. Sta-
tistical models for text segmentation. Machine Learning,
34(1?3):177?210.
Francisco Casacuberta and Colin de la Higuera. 2000. Computa-
tional complexity of problems on probabilistic grammars and
transducers. In 5th International Colloquium on Grammatical
Inference.
Corinna Cortes, Patrick Haffner, and Mehryar Mohri. 2003. Ra-
tional kernels. In Advances in Neural Information Processing
Systems, volume 15.
Sheng Gao, Wen Wu, Chin-Hui Lee, and Tai-Seng Chua. 2006.
A maximal figure-of-merit (MFoM)-learning approach to ro-
bust classifier design for text categorization. ACM Transac-
tions on Information Systems, 24(2):190?218. Also in ICML
2004.
Samuel S. Gross, Olga Russakovsky, Chuong B. Do, and Ser-
afim Batzoglou. 2007. Training conditional random fields
for maximum labelwise accuracy. In Advances in Neural
Information Processing Systems, volume 19.
R. W. Hamming. 1950. Error detecting and error correcting
codes. The Bell System Technical Journal, 26(2):147?160.
Martin Jansche. 2005. Maximum expected F-measure training
of logistic regression models. In Proceedings of Human Lan-
guage Technology Conference and Conference on Empirical
Methods in Natural Language Processing.
742
Kevin Knight. 1999. Decoding complexity in word-replacement
translation models. Computational Linguistics, 25(4):607?
615.
Michael C. Mozer, Robert Dodier, Michael D. Colagrosso,
Ce?sar Guerra-Salcedo, and Richard Wolniewicz. 2001. Prod-
ding the ROC curve: Constrained optimization of classifier
performance. In Advances in Neural Information Processing
Systems, volume 14.
David R. Musicant, Vipin Kumar, and Aysel Ozgur. 2003.
Optimizing F-measure with support vector machines. In
Proceedings of the Sixteenth International Florida Artificial
Intelligence Research Society Conference.
Jun Suzuki, Erik McDermott, and Hideki Isozaki. 2006. Train-
ing conditional random fields with multivariate evaluation
measures. In Proceedings of the 21st International Confer-
ence on Computational Linguistics and 44th Annual Meeting
of the Association for Computational Linguistics.
C. J. van Rijsbergen. 1974. Foundation of evaluation. Journal
of Documentation, 30(4):365?373.
Appendix A Proof of Theorem 1
The proof of Theorem 1 employs the following lemma:
Theorem 2. For fixed n and p, let s, t ? Sm for some m with
1 ? m < n. Further assume that s and t differ only in two bits,
i and k, in such a way that si = 1, sk = 0; ti = 0, tk = 1; and
pi ? pk. Then E [F(s, ?)]? E [F(t, ?)].
Proof. Express the expected F-score E [F(s, ?)] as a sum and
split the summation into two parts:
?
y
F(s,y) Pr(y) = ?
y
yi=yk
F(s,y) Pr(y) +?
y
yi 6=yk
F(s,y) Pr(y).
If yi = yk then F(s,y) = F(t,y), for three reasons: the number
of ones in s and t is the same (namely m) by assumption; y is
constant; and the number of true positives is the same, that is
s ? y = t ? y. The latter holds because s and y agree everywhere
except on i and k; if yi = yk = 0, then there are no true positives
at i and k; and if yi = yk = 1 then si is a true positive but sk is
not, and conversely tk is but ti is not. Therefore
?
y
yi=yk
F(s,y) Pr(y) = ?
y
yi=yk
F(t,y) Pr(y). (9)
Focus on those summands where yi 6= yk. Specifically group
them into pairs (y,z) where y and z are identical except that
yi = 1 and yk = 0, but zi = 0 and zk = 1. In other words, the two
summations on the right-hand side of the following equality are
carried out in parallel:
?
y
yi 6=yk
F(s,y) Pr(y) = ?
y
yi=1
yk=0
F(s,y) Pr(y)+ ?
z
zi=0
zk=1
F(s,z) Pr(z).
Then, focusing on s first:
F(s,y) Pr(y)+F(s,z) Pr(z)
=
(? +1)(A+1)
m+?T Pr(y)+
(? +1)A
m+?T Pr(z)
= [(A+1)pi (1? pk)+A(1? pi)pk]
(? +1)
m+?T C
= [pi +(pi + pk?2pi pk)A? pi pk]
(? +1)
m+?T C
= [pi +C0]C1,
where A = s ? z is the number of true positives between s and z
(s and y have an additional true positive at i by construction);
T = y ?y= z ?z is the number of positive labels in y and z (identical
by assumption); and
C =
Pr(y)
pi (1? pk)
=
Pr(z)
(1? pi) pk
is the probability of y and z evaluated on all positions except for
i and k. This equality holds because of the zeroth-order Markov
assumption (5) imposed on Pr(y). C0 and C1 are constants that
allow us to focus on the essential aspects.
The situation for t is similar, except for the true positives:
F(t,y) Pr(y)+F(t,z) Pr(z)
=
(? +1)A
m+?T Pr(y)+
(? +1)(A+1)
m+?T Pr(z)
= [A pi (1? pk)+(A+1)(1? pi)pk]
(? +1)
m+?T C
= [pk +(pi + pk?2pi pk)A? pi pk]
(? +1)
m+?T C
= [pk +C0]C1
where all constants have the same values as above. But pi ? pk
by assumption, pk +C0 ? 0, and C1 ? 0, so we have
F(s,y) Pr(y)+F(s,z) Pr(z) = [pi +C0]C1
? F(t,y) Pr(y)+F(t,z) Pr(z) = [pk +C0]C1,
and therefore
?
y
yi 6=yk
F(s,y) Pr(y)? ?
y
yi 6=yk
F(t,y) Pr(y). (10)
The theorem follows from equality (9) and inequality (10).
Proof of Theorem 1: (?s ? Sm) E [F(z(m), ?)]? E [F(s, ?)].
Observe that z(m) ? Sm by definition (see Section 2.3). For
m = 0 and m = n the theorem holds trivially because Sm is a
singleton set. In the nontrivial cases, Theorem 2 is applied
repeatedly. The string z(m) can be transformed into any other
string s ? Sm by repeatedly clearing a more likely set bit and
setting a less likely unset bit.
In particular this can be done as follows: First, find the indices
where z(m) and s disagree. By construction there must be an even
number of such indices; indeed there are equinumerous sets
{
i
?
? z(m)i = 1? si = 0
}
?
{
j
?
? z(m)j = 0? s j = 1
}
.
This holds because the total number of ones is fixed and identical
in z(m) and s, and so is the total number of zeroes. Next, sort
those indices by non-increasing probability and represent them
as i1, . . . , ik and j1, . . . , jk. Let s0 = z(m). Then let s1 be identical
to s0 except that si1 = 0 and s j1 = 1. Form s2, . . . ,sk along the
same lines and observe that sk = s by construction. By definition
of z(m) it must be the case that pir ? p jr for all r ? {1, . . . ,k}.
Therefore Theorem 2 applies at every step along the way from
z(m) = s0 to sk = s, and so the expected utility is non-increasing
along that path.
743
Coling 2010: Poster Volume, pages 276?284,
Beijing, August 2010
A Comparison of Features for Automatic Readability Assessment
Lijun Feng
City University of New York
lijun7.feng@gmail.com
Martin Jansche
Google, Inc.
jansche@acm.org
Matt Huenerfauth
City University of New York
matt@cs.qc.cuny.edu
Noe?mie Elhadad
Columbia University
noemie@dbmi.columbia.edu
Abstract
Several sets of explanatory variables ? in-
cluding shallow, language modeling, POS,
syntactic, and discourse features ? are com-
pared and evaluated in terms of their im-
pact on predicting the grade level of read-
ing material for primary school students.
We find that features based on in-domain
language models have the highest predic-
tive power. Entity-density (a discourse fea-
ture) and POS-features, in particular nouns,
are individually very useful but highly cor-
related. Average sentence length (a shal-
low feature) is more useful ? and less ex-
pensive to compute ? than individual syn-
tactic features. A judicious combination
of features examined here results in a sig-
nificant improvement over the state of the
art.
1 Introduction
1.1 Motivation and Method
Readability Assessment quantifies the difficulty
with which a reader understands a text. Automatic
readability assessment enables the selection of ap-
propriate reading material for readers of varying
proficiency. Besides modeling and understanding
the linguistic components involved in readability, a
readability-prediction algorithm can be leveraged
for the task of automatic text simplification: as sim-
plification operators are applied to a text, the read-
ability is assessed to determine whether more sim-
plification is needed or a particular reading level
was reached.
Identifying text properties that are strongly cor-
related with text complexity is itself complex. In
this paper, we explore a broad range of text proper-
ties at various linguistic levels, ranging from dis-
course features to language modeling features, part-
of-speech-based grammatical features, parsed syn-
tactic features and well studied shallow features,
many of which are inspired by previous work.
We use grade levels, which indicate the number
of years of education required to completely under-
stand a text, as a proxy for reading difficulty. The
corpus in our study consists of texts labeled with
grade levels ranging from grade 2 to 5. We treat
readability assessment as a classification task and
evaluate trained classifiers in terms of their predic-
tion accuracy. To investigate the contributions of
various sets of features, we build prediction models
and examine how the choice of features influences
the model performance.
1.2 Related Work
Many traditional readability metrics are linear mod-
els with a few (often two or three) predictor vari-
ables based on superficial properties of words, sen-
tences, and documents. These shallow features
include the average number of syllables per word,
the number of words per sentence, or binned word
frequency. For example, the Flesch-Kincaid Grade
Level formula uses the average number of words
per sentence and the average number of syllables
per word to predict the grade level (Flesch, 1979).
The Gunning FOG index (Gunning, 1952) uses av-
erage sentence length and the percentage of words
with at least three syllables. These traditional met-
rics are easy to compute and use, but they are not
reliable, as demonstrated by several recent stud-
ies in the field (Si and Callan, 2001; Petersen and
Ostendorf, 2006; Feng et al, 2009).
276
With the advancement of natural language pro-
cessing tools, a wide range of more complex text
properties have been explored at various linguis-
tic levels. Si and Callan (2001) used unigram
language models to capture content information
from scientific web pages. Collins-Thompson and
Callan (2004) adopted a similar approach and used
a smoothed unigram model to predict the grade lev-
els of short passages and web documents. Heilman
et al (2007) continued using language modeling
to predict readability for first and second language
texts. Furthermore, they experimented with vari-
ous statistical models to test their effectiveness at
predicting reading difficulty (Heilman et al, 2008).
Schwarm/Petersen and Ostendorf (Schwarm and
Ostendorf, 2005; Petersen and Ostendorf, 2006)
used support vector machines to combine features
from traditional reading level measures, statistical
language models and automatic parsers to assess
reading levels. In addition to lexical and syntactic
features, several researchers started to explore dis-
course level features and examine their usefulness
in predicting text readability. Pitler and Nenkova
(2008) used the Penn Discourse Treebank (Prasad
et al, 2008) to examine discourse relations. We
previously used a lexical-chaining tool to extract
entities that are connected by certain semantic re-
lations (Feng et al, 2009).
In this study, we systematically evaluate all
above-mentioned types of features, as well as a
few extensions and variations. A detailed descrip-
tion of the features appears in Section 3. Section
4 discusses results of experiments with classifiers
trained on these features. We begin with a descrip-
tion of our data in the following section.
2 Corpus
We contacted the Weekly Reader1 corporation, an
on-line publisher producing magazines for elemen-
tary and high school students, and were granted
access in October 2008 to an archive of their ar-
ticles. Among the articles retrieved, only those
for elementary school students are labeled with
grade levels, which range from 2 to 5. We selected
only this portion of articles (1629 in total) for the
1http://www.weeklyreader.com
Table 1: Statistics for the Weekly Reader Corpus
Grade docs. words/document words/sentence
mean std. dev. mean std. dev.
2 174 128.27 106.03 9.54 2.32
3 289 171.96 106.05 11.39 2.42
4 428 278.03 187.58 13.67 2.65
5 542 335.56 230.25 15.28 3.21
study.2 These articles are intended to build chil-
dren?s general knowledge and help them practice
reading skills. While pre-processing the texts, we
found that many articles, especially those for lower
grade levels, consist of only puzzles and quizzes,
often in the form of simple multiple-choice ques-
tions. We discarded such texts and kept only 1433
full articles. Some distributional statistics of the
final corpus are listed in Table 1.
3 Features
3.1 Discourse Features
We implement four subsets of discourse fea-
tures: entity-density features, lexical-chain fea-
tures, coreference inference features and entity grid
features. The coreference inference features are
novel and have not been studied before. We pre-
viously studied entity-density features and lexical-
chain features for readers with intellectual disabili-
ties (Feng et al, 2009). Entity-grid features have
been studied by Barzilay and Lapata (2008) in a
stylistic classification task. Pitler and Nenkova
(2008) used the same features to evaluate how well
a text is written. We replicate this set of features
for grade level prediction task.
3.1.1 Entity-Density Features
Conceptual information is often introduced in a
text by entities, which consist of general nouns
and named entities, e.g. people?s names, locations,
organizations, etc. These are important in text
comprehension, because established entities form
basic components of concepts and propositions, on
which higher level discourse processing is based.
Our prior work illustrated the importance of en-
tities in text comprehension (Feng et al, 2009).
2A corpus of Weekly Reader articles was previously used
in work by Schwarm and Ostendorf (2005). However, the two
corpora are not identical in size nor content.
277
Table 2: New Entity-Density Features
1 percentage of named entities per document
2 percentage of named entities per sentences
3 percentage of overlapping nouns removed
4 average number of remaining nouns per sentence
5 percentage of named entities in total entities
6 percentage of remaining nouns in total entities
We hypothesized that the number of entities in-
troduced in a text relates to the working memory
burden on their targeted readers ? individuals with
intellectual disabilities. We defined entities as a
union of named entities and general nouns (nouns
and proper nouns) contained in a text, with over-
lapping general nouns removed. Based on this, we
implemented four kinds of entity-density features:
total number of entity mentions per document, total
number of unique entity mentions per document,
average number of entity mentions per sentence,
and average number of unique entity mentions per
sentence.
We believe entity-density features may also re-
late to the readability of a text for a general au-
dience. In this paper, we conduct a more re-
fined analysis of general nouns and named entities.
To collect entities for each document, we used
OpenNLP?s3 name-finding tool to extract named
entities; general nouns are extracted from the out-
put of Charniak?s Parser (see Section 3.3). Based
on the set of entities collected for each document,
we implement 12 new features. We list several of
these features in in Table 2.
3.1.2 Lexical Chain Features
During reading, a more challenging task with enti-
ties is not just to keep track of them, but to resolve
the semantic relations among them, so that infor-
mation can be processed, organized and stored in
a structured way for comprehension and later re-
trieval. In earlier work (Feng et al, 2009), we
used a lexical-chaining tool developed by Galley
and McKeown (2003) to annotate six semantic re-
lations among entities, e.g. synonym, hypernym,
hyponym, etc. Entities that are connected by these
semantic relations were linked through the text to
form lexical chains. Based on these chains, we
implemented six features, listed in Table 3, which
3http://opennlp.sourceforge.net/
Table 3: Lexical Chain Features
1 total number of lexical chains per document
2 avg. lexical chain length
3 avg. lexical chain span
4 num. of lex. chains with span ? half doc. length
5 num. of active chains per word
6 num. of active chains per entity
Table 4: Coreference Chain Features
1 total number of coreference chains per document
2 avg. num. of coreferences per chain
3 avg. chain span
4 num. of coref. chains with span ? half doc. length
5 avg. inference distance per chain
6 num. of active coreference chains per word
7 num. of active coreference chains per entity
we use in our current study. The length of a chain
is the number of entities contained in the chain,
the span of chain is the distance between the index
of the first and last entity in a chain. A chain is
defined to be active for a word or an entity if this
chain passes through its current location.
3.1.3 Coreference Inference Features
Relations among concepts and propositions are of-
ten not stated explicitly in a text. Automatically re-
solving implicit discourse relations is a hard prob-
lem. Therefore, we focus on one particular type,
referential relations, which are often established
through anaphoric devices, e.g. pronominal refer-
ences. The ability to resolve referential relations is
important for text comprehension.
We use OpenNLP to resolve coreferences. En-
tities and pronominal references that occur across
the text and refer to the same person or object
are extracted and formed into a coreference chain.
Based on the chains extracted, we implement seven
features as listed in Table 4. The chain length,
chain span and active chains are defined in a sim-
ilar way to the lexical chain features. Inference
distance is the difference between the index of the
referent and that of its pronominal reference. If the
same referent occurs more than once in a chain,
the index of the closest occurrence is used when
computing the inference distance.
3.1.4 Entity Grid Features
Coherent texts are easier to read. Several computa-
tional models have been developed to represent and
278
measure discourse coherence (Lapata and Barzilay,
2005; Soricut and Marcu, 2006; Elsner et al, 2007;
Barzilay and Lapata, 2008) for NLP tasks such as
text ordering and text generation. Although these
models are not intended directly for readability re-
search, Barzilay and Lapata (2008) have reported
that distributional properties of local entities gen-
erated by their grid models are useful in detecting
original texts from their simplified versions when
combined with well studied lexical and syntactic
features. This approach was subsequently pursued
by Pitler and Nenkova (2008) in their readability
study. Barzilay and Lapata?s entity grid model is
based on the assumption that the distribution of
entities in locally coherent texts exhibits certain
regularities. Each text is abstracted into a grid
that captures the distribution of entity patterns at
the level of sentence-to-sentence transitions. The
entity grid is a two-dimensional array, with one di-
mension corresponding to the salient entities in the
text, and the other corresponding to each sentence
of the text. Each grid cell contains the grammatical
role of the specified entity in the specified sentence:
whether it is a subject (S), object (O), neither of
the two (X), or absent from the sentence (-).
We use the Brown Coherence Toolkit (v0.2) (El-
sner et al, 2007), based on (Lapata and Barzilay,
2005), to generate an entity grid for each text in
our corpus. The distribution patterns of entities
are traced between each pair of adjacent sentences,
resulting in 16 entity transition patterns4. We then
compute the distribution probability of each entity
transition pattern within a text to form 16 entity-
grid-based features.
3.2 Language Modeling Features
Our language-modeling-based features are inspired
by Schwarm and Ostendorf?s (2005) work, a study
that is closely related to ours. They used data
from the same data ? the Weekly Reader ? for
their study. They trained three language mod-
els (unigram, bigram and trigram) on two paired
complex/simplified corpora (Britannica and Litera-
cyNet) using an approach in which words with high
information gain are kept and the remaining words
4These 16 transition patterns are: ?SS?, ?SO?, ?SX?, ?S-?,
?OS?, ?OO?, ?OX?, ?O-?, ?XS?, ?XO?, ?XX?, ?X-?, ?-S?,
?-O?, ?-X?, ?- -?.
are replaced with their parts of speech. These lan-
guage models were then used to score each text
in the Weekly Reader corpus by perplexity. They
reported that this approach was more successful
than training LMs on text sequences of word la-
bels alone, though without providing supporting
statistics.
It?s worth pointing out that their LMs were not
trained on the Weekly Reader data, but rather on
two unrelated paired corpora (Britannica and Lit-
eracyNet). This seems counter-intuitive, because
training LMs directly on the Weekly Reader data
would provide more class-specific information for
the classifiers. They justified this choice by stating
that splitting limited Weekly Reader data for train-
ing and testing purposes resulted in unsuccessful
performance.
We overcome this problem by using a hold-
one-out approach to train LMs directly on our
Weekly Reader corpus, which contains texts rang-
ing from Grade 2 to 5. We use grade levels to
divide the whole corpus into four smaller subsets.
In addition to implementing Schwarm and Osten-
dorf?s information-gain approach, we also built
LMs based on three other types of text sequences
for comparison purposes. These included: word-
token-only sequence (i.e., the original text), POS-
only sequence, and paired word-POS sequence.
For each grade level, we use the SRI Language
Modeling Toolkit5 (with Good-Turing discounting
and Katz backoff for smoothing) to train 5 lan-
guage models (1- to 5-gram) using each of the four
text sequences, resulting in 4?5?4= 80 perplex-
ity features for each text tested.
3.3 Parsed Syntactic Features
Schwarm and Ostendorf (2005) studied four parse
tree features (average parse tree height, average
number of SBARs, noun phrases, and verb phrases
per sentences). We implemented these and addi-
tional features, using the Charniak parser (Char-
niak, 2000). Our parsed syntactic features focus on
clauses (SBAR), noun phrases (NP), verb phrases
(VP) and prepositional phrases (PP). For each
phrase, we implement four features: total num-
ber of the phrases per document, average number
of phrases per sentence, and average phrase length
5http://www.speech.sri.com/projects/srilm/
279
measured by number of words and characters re-
spectively. In addition to average tree height, we
implement two non-terminal-node-based features:
average number of non-terminal nodes per parse
tree, and average number of non-terminal nodes
per word (terminal node).
3.4 POS-based Features
Part-of-speech-based grammatical features were
shown to be useful in readability prediction (Heil-
man et al, 2007; Leroy et al, 2008). To extend
prior work, we systematically studied a number of
common categories of words and investigated to
what extent they are related to a text?s complex-
ity. We focus primarily on five classes of words
(nouns, verbs, adjectives, adverbs, and preposi-
tions) and two broad categories (content words,
function words). Content words include nouns,
verbs, numerals, adjectives, and adverbs; the re-
maining types are function words. The part of
speech of each word is obtained from examining
the leaf node based on the output of Charniak?s
parser, where each leaf node consists of a word and
its part of speech. We group words based on their
POS labels. For each class of words, we imple-
ment five features. For example, for the adjective
class, we implemented the following five features:
percent of adjectives (tokens) per document, per-
cent of unique adjectives (types) per document,
ratio of unique adjectives per total unique words
in a document, average number of adjectives per
sentence and average number of unique adjectives
per sentence.
3.5 Shallow Features
Shallow features refer to those used by traditional
readability metrics, such as Flesch-Kincaid Grade
Level (Flesch, 1979), SMOG (McLaughlin, 1969),
Gunning FOG (Gunning, 1952), etc. Although
recent readability studies have strived to take ad-
vantage of NLP techniques, little has been revealed
about the predictive power of shallow features.
Shallow features, which are limited to superficial
text properties, are computationally much less ex-
pensive than syntactic or discourse features. To en-
able a comparison against more advanced features,
we implement 8 frequently used shallow features
as listed in Table 5.
Table 5: Shallow Features
1 average number of syllables per word
2 percentage of poly-syll. words per doc.
3 average number of poly-syll. words per sent.
4 average number of characters per word
5 Chall-Dale difficult words rate per doc.
6 average number of words per sentence
7 Flesch-Kincaid score
8 total number of words per document
3.6 Other Features
For comparison, we replicated 6 out-of-vocabulary
features described in Schwarm and Ostendorf
(2005). For each text in the Weekly Reader corpus,
these 6 features are computed using the most com-
mon 100, 200 and 500 word tokens and types based
on texts from Grade 2. We also replicated the 12
perplexity features implemented by Schwarm and
Ostendorf (2005) (see Section 3.2).
4 Experiments and Discussion
Previous studies on reading difficulty explored vari-
ous statistical models, e.g. regression vs. classifica-
tion, with varying assumptions about the measure-
ment of reading difficulty, e.g. whether labels are
ordered or unrelated, to test the predictive power
of models (Heilman et al, 2008; Petersen and Os-
tendorf, 2009; Aluisio et al, 2010). In our re-
search, we have used various models, including
linear regression; standard classification (Logis-
tic Regression and SVM), which assumes no rela-
tion between grade levels; and ordinal regression/
classification (provided by Weka, with Logistic
Regression and SMO as base function), which as-
sumes that the grade levels are ordered. Our exper-
iments show that, measured by mean squared error
and classification accuracy, linear regression mod-
els perform considerably poorer than classification
models. Measured by accuracy and F-measure,
ordinal classifiers perform comparable or worse
than standard classifiers. In this paper, we present
the best results, which are obtained by standard
classifiers. We use two machine learning packages
known for efficient high-quality multi-class classi-
fication: LIBSVM (Chang and Lin, 2001) and the
Weka machine learning toolkit (Hall et al, 2009),
from which we choose Logistic Regression as clas-
sifiers. We train and evaluate various prediction
280
Table 6: Comparison of discourse features
Feature Set LIBSVM Logistic Regress.
Entity-Density 59.63?0.632 57.59?0.375
Lexical Chain 45.86?0.815 42.58?0.241
Coref. Infer. 40.93?0.839 42.19?0.238
Entity Grid 45.92?1.155 42.14?0.457
all combined 60.50?0.990 58.79?0.703
models using the features described in Section 3.
We evaluate classification accuracy using repeated
10-fold cross-validation on the Weekly Reader cor-
pus. Classification accuracy is defined as the per-
centage of texts predicted with correct grade levels.
We repeat each experiment 10 times and report the
mean accuracy and its standard deviation.
4.1 Discourse Features
We first discuss the improvement made by extend-
ing our earlier entity-density features (Feng et al,
2009). We used LIBSVM to train and test mod-
els on the Weekly Reader corpus with our earlier
features and our new features respectively. With
earlier features only, the model achieves 53.66%
accuracy. With our new features added, the model
performance is 59.63%.
Table 6 presents the classification accuracy of
models trained with discourse features. We see
that, among four subsets of discourse features,
entity-density features perform significantly better
than the other three feature sets and generate the
highest classification accuracy (LIBSVM: 59.63%,
Logistic Regression: 57.59%). While Logistic Re-
gression results show that there is not much perfor-
mance difference among lexical chain, coreference
inference, and entity grid features, classification
accuracy of LIBSVM models indicates that lexical
chain features and entity grid features are better
in predicting text readability than coreference in-
ference features. Combining all discourse features
together does not significantly improve accuracy
compared with models trained only with entity-
density features.
4.2 Language Modeling Features
Table 7 compares the performance of models gen-
erated using our approach and our replication of
Schwarm and Ostendorf?s (2005) approach. In our
approach, features were obtained from language
Table 7: Comparison of lang. modeling features
Feature Set LIBSVM Logistic Regress.
IG 62.52?1.202 62.14?0.510
Text-only 60.17?1.206 60.31?0.559
POS-only 56.21?2.354 57.64?0.391
Word/POS pair 60.38?0.820 59.00?0.367
all combined 68.38?0.929 66.82?0.448
IG by Schwarm 52.21?0.832 51.89?0.405
Table 8: Comparison of parsed syntactic features
Feature Set # Feat. LIBSVM
Original features 4 50.68?0.812
Expanded features 21 57.79?1.023
models trained on the Weekly Reader corpus. Not
surprisingly, these are more effective than LMs
trained on the Britannica and LiteracyNet corpora,
in Schwarm and Ostendorf?s approach. Our results
support their claim that LMs trained with infor-
mation gain outperform LMs trained with POS la-
bels. However, we also notice that training LMs on
word labels alone or paired word/POS sequences
achieved similar classification accuracy to the IG
approach, while avoiding the complicated feature
selection of the IG approach.
4.3 Parsed Syntactic Features
Table 8 compares a classifier trained on the four
parse features of Schwarm and Ostendorf (2005) to
a classifier trained on our expanded set of parse fea-
tures. The LIBSVM classifier with the expanded
feature set scored 7 points higher than the one
trained on only the original four features, improv-
ing from 50.68% to 57.79%. Table 9 shows a
detailed comparison of particular parsed syntactic
features. The two non-terminal-node-based fea-
tures (average number of non-terminal nodes per
tree and average number of non-terminal nodes
per word) have higher discriminative power than
average tree height. Among SBARs, NPs, VPs and
PPs, our experiments show that VPs and NPs are
the best predictors.
4.4 POS-based Features
The classification accuracy generated by models
trained with various POS features is presented
in Table 10. We find that, among the five word
classes investigated, noun-based features gener-
281
Table 9: Detailed comp. of syntactic features
Feature Set LIBSVM Logistic Regress.
Non-term.-node ratios 53.02?0.571 51.80?0.171
Average tree height 44.26?0.914 43.45?0.269
SBARs 44.42?1.074 43.50?0.386
NPs 51.56?1.054 48.14?0.408
VPs 53.07?0.597 48.67?0.484
PPs 49.36?1.277 46.47?0.374
all combined 57.79?1.023 54.11?0.473
Table 10: Comparison of POS features
Feature Set LIBSVM Logistic Regress.
Nouns 58.15?0.862 57.01?0.256
Verbs 54.40?1.029 55.10?0.291
Adjectives 53.87?1.128 52.75?0.427
Adverbs 52.66?0.970 50.54?0.327
Prepositions 56.77?1.278 54.13?0.312
Content words 56.84?1.072 56.18?0.213
Function words 52.19?1.494 50.95?0.298
all combined 59.82?1.235 57.86?0.547
ate the highest classification accuracy, which is
consistent with what we have observed earlier
about entity-density features. Another notable ob-
servation is that prepositions demonstrate higher
discriminative power than adjectives and adverbs.
Models trained with preposition-based features per-
form close to those trained with noun-based fea-
tures. Among the two broader categories, content
words (which include nouns) demonstrate higher
predictive power than function words (which in-
clude prepositions).
4.5 Shallow Features
We present some notable findings on shallow fea-
tures in Table 11. Experimental results generated
by models trained with Logistic Regression show
that average sentence length has dominating predic-
tive power over all other shallow features. Features
based on syllable counting perform much worse.
The Flesch-Kincaid Grade Level score uses a fixed
linear combination of average words per sentence
and average syllables per word. Combining those
two features (without fixed coefficients) results in
the best overall accuracy, while using the Flesch-
Kincaid score as a single feature is significantly
worse.
Table 11: Comparison of shallow features
Feature Set Logistic Regress.
Avg. words per sent. 52.17?0.193
Avg. syll. per word 42.51?0.264
above two combined 53.04?0.514
Flesch-Kincaid score 50.83?0.144
Avg. poly-syll. words per sent. 45.70?0.306
all 8 features combined 52.34?0.242
4.6 Comparison with Previous Studies
A trivial baseline of predicting the most frequent
grade level (grade 5) predicts 542 out of 1433 texts
(or 37.8%) correctly. With this in mind, we first
compare our study with the widely-used Flesch-
Kincaid Grade Level formula, which is a linear
function of average words per sentence and average
syllables per word that aims to predict the grade
level of a text directly. Since this is a fixed formula
with known coefficients, we evaluated it directly
on our entire Weekly Reader corpus without cross-
validation. We obtain the predicted grade level
of a text by rounding the Flesch-Kincaid score
to the nearest integer. For only 20 out of 1433
texts the predicted and labeled grade levels agree,
resulting in a poor accuracy of 1.4%. By contrast,
using the Flesch-Kincaid score as a feature of a
simple logistic regression model achieves above
50% accuracy, as discussed in Section 4.5.
The most closely related previous study is the
work of Schwarm and Ostendorf (2005). How-
ever, because their experiment design (85/15 train-
ing/test data split) and machine learning tool
(SV Mlight) differ from ours, their results are not
directly comparable to ours. To make a compar-
ison, we replicated all the features used in their
study and then use LIBSVM and Weka?s Logistic
Regression to train two models with the replicated
features and evaluate them on our Weekly Reader
corpus using 10-fold cross-validation.
Using the same experiment design, we train clas-
sifiers with three combinations of our features as
listed in Table 12. ?All features? refers to a naive
combination of all features. ?AddOneBest? refers
to a subset of features selected by a group-wise
add-one-best greedy feature selection. ?WekaFS?
refers to a subset of features chosen by Weka?s
feature selection filter.
?WekaFS? consists of 28 features selected au-
282
Table 12: Comparison with previous work
baseline accuracy (majority class) 37.8
Flesch-Kincaid Grade Level 1.4
Feature Set # Feat. LIBSVM Logistic Reg.
Schwarm 25 63.18?1.664 60.50?0.477
All features 273 72.21?0.821 63.71?0.576
AddOneBest 122 74.01?0.847 69.22?0.411
WekaFS 28 70.06?0.777 65.46?0.336
tomatically by Weka?s feature selection filter us-
ing a best-first search method. The 28 features
include language modeling features, syntactic fea-
tures, POS features, shallow features and out-of-
vocabulary features. Aside from 4 shallow features
and 5 out-of-vocabulary features, the other 19 fea-
tures are novel features we have implemented for
this paper.
As Table 12 shows, a naive combination of all
features results in classification accuracy of 72%,
which is much higher than the current state of the
art (63%). This is not very surprising, since we are
considering a greater variety of features than any
previous individual study. Our WekaFS classifier
uses roughly the same number of features as the
best published result, yet it has a higher accuracy
(70.06%). Our best results were obtained by group-
wise add-one-best feature selection, resulting in
74% classification accuracy, a big improvement
over the state of the art.
5 Conclusions
We examined the usefulness of features at various
linguistic levels for predicting text readability in
terms of assigning texts to elementary school grade
levels. We implemented a set of discourse features,
enriched previous work by creating several new
features, and systematically tested and analyzed
the impact of these features.
We observed that POS features, in particular
nouns, have significant predictive power. The high
discriminative power of nouns in turn explains the
good performance of entity-density features, based
primarily on nouns. In general, our selected POS
features appear to be more correlated to text com-
plexity than syntactic features, shallow features
and most discourse features.
For parsed syntactic features, we found that verb
phrases appear to be more closely correlated with
text complexity than other types of phrases. While
SBARs are commonly perceived as good predic-
tors for syntactic complexity, they did not prove
very useful for predicting grade levels of texts in
this study. In future work, we plan to examine this
result in more detail.
Among the 8 shallow features, which are used
in various traditional readability formulas, we iden-
tified that average sentence length has dominating
predictive power over all other lexical or syllable-
based features.
Not surprisingly, among language modeling
features, combined features obtained from LMs
trained directly on the Weekly Reader corpus show
high discriminative power, compared with features
from LMs trained on unrelated corpora.
Discourse features do not seem to be very use-
ful in building an accurate readability metric. The
reason could lie in the fact that the texts in the cor-
pus we studied exhibit relatively low complexity,
since they are aimed at primary-school students. In
future work, we plan to investigate whether these
discourse features exhibit different discriminative
power for texts at higher grade levels.
A judicious combination of features examined
here results in a significant improvement over the
state of the art.
References
Sandra Aluisio, Lucia Specia, Caroline Gasperin,
and Carolina Scarton. 2010. Readability assess-
ment for text simplification. In NAACL-HLT
2010: The 5th Workshop on Innovative Use of
NLP for Building Educational Applications.
Regina Barzilay and Mirella Lapata. 2008. Model-
ing local coherence: An entity-based approach.
Computational Linguistics, 34(1):1?34.
Chih-Chung Chang and Chih-Jen Lin. 2001. LIB-
SVM: A Library for Support Vector Machines.
Software available at http://www.csie.ntu.
edu.tw/~cjlin/libsvm.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st Con-
ference of the North American Chapter of the
ACL, pages 132?139.
283
Kevyn Collins-Thompson and Jamie Callan. 2004.
A language modeling approach to predicting
reading difficulty. In Proceedings of the Hu-
man Language Technology Conference of the
North American Chapter of the Association for
Computational Linguistics (HLT-NAACL 2004).
Micha Elsner, Joseph Austerweil, and Eugene
Charniak. 2007. A unified local and global
model for discourse coherence. In Proceed-
ings of the Conference on Human Language
Technology and North American chapter of the
Association for Computational Linguistics (HLT-
NAACL 2007).
Lijun Feng, Noe?mie Elhadad, and Matt Huener-
fauth. 2009. Cognitively motivated features for
readability assessment. In The 12th Conference
of the European Chapter of the Association for
Computational Linguistics (EACL 2009).
Rudolf Flesch. 1979. How to write plain English.
Harper and Brothers, New York.
Michel Galley and Kathleen McKeown. 2003. Im-
proving word sense disambiguation in lexical
chaining. In Proceedings of the 18th Inter-
national Joint Conference on Artificial Intelli-
gence.
Robert Gunning. 1952. The Technique of Clear
Writing. McGraw-Hill.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bern-
hard Pfahringer, Peter Reutemann, and Ian H.
Witten. 2009. The WEKA data mining software:
An update. SIGKDD Explorations, 11(1):10?18.
Michael J. Heilman, Kevyn Collins-Thompson,
Jamie Callan, and Maxine Eskenazi. 2007. Com-
bining lexical and grammatical features to im-
prove readability measures for first and second
language texts. In Human Language Technolo-
gies 2007: The Conference of the North Amer-
ican Chapter of the Association for Computa-
tional Linguistics.
Michael J. Heilman, Kevyn Collins-Thompson,
and Maxine Eskenazi. 2008. An analysis of sta-
tistical models and features for reading difficulty
prediction. In ACL 2008: The 3rd Workshop on
Innovative Use of NLP for Building Educational
Applications.
Mirella Lapata and Regina Barzilay. 2005. Auto-
matic evaluation of text coherence: Models and
representations. In Proceedings of the Interna-
tional Joint Conference on Artificial Intelligence
(IJCAI?05), pages 1085?1090.
Gondy Leroy, Stephen Helmreich, James R. Cowie,
Trudi Miller, and Wei Zheng. 2008. Evaluating
online health information: Beyond readability
formulas. In AMIA 2008 Symposium Proceed-
ings.
G. Harry McLaughlin. 1969. Smog grading a
new readability formula. Journal of Reading,
12(8):639?646.
Sarah E. Petersen and Mari Ostendorf. 2006. A
machine learning approach to reading level as-
sessment. Technical report, University of Wash-
ington CSE Technical Report.
Sarah E. Petersen and Mari Ostendorf. 2009. A ma-
chine learning approach to reading level assess-
ment. Computer Speech and Language, 23:89?
106.
Emily Pitler and Ani Nenkova. 2008. Revisiting
readability: A unified framework for predict-
ing text quality. In Proceedings of the 2008
Conference on Empirical Methods in Natural
Language Processing.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki,
L. Robaldo, A. Joshi, and B. Webber. 2008. The
Penn discourse treebank. In The Sixth Interna-
tional Conference on Language Resources and
Evaluation (LREC?08).
Sarah E. Schwarm and Mari Ostendorf. 2005.
Reading level assessment using support vector
machines and statistical language models. In
Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics.
Luo Si and Jamie Callan. 2001. A statistical model
for scientific readability. In Proceedings of the
Tenth International Conference on Information
and Knowledge Management.
Radu Soricut and Daniel Marcu. 2006. Discourse
generation using utility-trained coherence mod-
els. In Proceedings of the 21st International
Conference on Computational Linguistics and
44th Annual Meeting of the Association for Com-
putational Linguistics.
284
