Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 126?131,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Exploiting CCG Structures with Tree Kernels for Speculation Detection
Liliana Mamani Sa?nchez, Baoli Li, Carl Vogel
Computational Linguistics Group
Trinity College Dublin
Dublin 2, Ireland
{mamanisl,baoli.li,vogel}@tcd.ie
Abstract
Our CoNLL-2010 speculative sentence
detector disambiguates putative keywords
based on the following considerations: a
speculative keyword may be composed of
one or more word tokens; a speculative
sentence may have one or more specula-
tive keywords; and if a sentence contains
at least one real speculative keyword, it is
deemed speculative. A tree kernel classi-
fier is used to assess whether a potential
speculative keyword conveys speculation.
We exploit information implicit in tree
structures. For prediction efficiency, only
a segment of the whole tree around a spec-
ulation keyword is considered, along with
morphological features inside the segment
and information about the containing doc-
ument. A maximum entropy classifier
is used for sentences not covered by the
tree kernel classifier. Experiments on the
Wikipedia data set show that our system
achieves 0.55 F-measure (in-domain).
1 Introduction
Speculation and its impact on argumentation has
been studied by linguists and logicians since at
least as far back as Aristotle (trans 1991, 1407a,
1407b), and under the category of linguistic
?hedges? since Lakoff (1973). Practical appli-
cation of this research has emerged due to the
efforts to create a biomedical database of sen-
tences tagged with speculation information: Bio-
Scope (Szarvas et al, 2008) and because of the
association of some kinds of Wikipedia data with
the speculation phenomenon (Ganter and Strube,
2009). It is clear that specific words can be con-
sidered as clues that can qualify a sentence as
speculative. However, the presence of a specu-
lative keyword not always conveys a speculation
assertion which makes the speculation detection a
tough problem. For instance, the sentences below
contain the speculative keyword ?may?, but only
the sentence (a) is speculative.
(a) These effects may be reversible.
(b) Members of an alliance may not attack each other.
The CoNLL-2010 Shared Task (Farkas et al,
2010), ?Learning to detect hedges and their scope
in natural language text? proposed two tasks re-
lated to speculation research. Task 1 aims to detect
sentences containing uncertainty and Task 2 aims
to resolve the intra-sentential scope of hedge cues.
We engaged in the first task in the biomedical and
Wikipedia domains as proposed by the organizers,
but eventually we got to submit only Wikipedia
domain results. However, in this paper we include
results in the biomedical domain as well.
The BioScope corpus is a linguistically hand an-
notated corpus of negation and speculation phe-
nomena for medical free texts, biomedical article
abstracts and full biomedical articles. The afore-
said phenomena have been annotated at sentence
level with keyword tags and linguistic scope tags.
Some previous research on speculation detection
and boundary determination over biomedical data
has been done by Medlock & Briscoe (2007) and
O?zgu?r & Radev (2009) from a computational view
using machine learning methods.
The Wikipedia speculation dataset was gener-
ated by exploiting a weasel word marking. As
weasel words convey vagueness and ambiguity by
providing an unsupported opinion, they are dis-
couraged by Wikipedia editors. Ganter & Strube
(2009) proposed a system to detect hedges based
on frequency measures and shallow information,
achieving a F-score of 0.691.
We formulate the speculation detection prob-
lem as a word disambiguation problem and de-
veloped a system as a pipelined set of natural
1They used different Wikipedia data.
126
language processing tools and procedures to pre-
process the datasets. A Combinatory Categorial
Grammar parsing (CCG) (Steedman, 2000) tool
and a Tree Kernel (TK) classifier constitute the
core of the system.
The Section 2 of this paper describes the over-
all architecture of our system. Section 3 depicts
the dataset pre-processing. Section 4 shows how
we built the speculation detection module, outlines
the procedure of examples generation and the use
of the Tree-kernel classifier. Section 5 presents
the experiments and results, we show that sentence
CCG derivation information helps to differentiate
between apparent and real speculative words for
speculation detection. Finally Section 6 gives our
conclusions.
2 Speculation detection system
Our system for speculation detection is a machine
learning (ML) based system (Figure 1). In the pre-
processing module a dataset of speculative/non-
speculative sentences goes through a process of
information extraction of three kinds: specula-
tive word or keyword extraction,2 sentence extrac-
tion and document feature extraction (i.e docu-
ment section). Later the extracted keywords are
used to tag potential speculative sentences in the
training/evaluation datasets and used as features
by the classifiers. The sentences are submitted to
the tokenization and parsing modules in order to
provide a richer set of features necessary for creat-
ing the training/evaluation datasets, including the
document features as well.
In the ML module two types of dataset are built:
one used by a TK classifier and other one by a bag-
of-features based maximum entropy classifier. As
the first one processes only those sentences that
contain speculative words, we use the second clas-
sifier, which is able to process samples of all the
sentences.
The models built by these classifiers are com-
bined in order to provide a better performance and
coverage for the speculation problem in the clas-
sification module which finally outputs sentences
labeled as speculative or non-speculative. Used
tools are the GeniaTagger (Tsuruoka et al, 2005)
for tokenization and lemmatization, and the C&C
Parser (Clark and Curran, 2004). The next sec-
tions explain in detail the main system compo-
nents.
2Extraction of keywords for the training stage.
3 Dataset pre-processing for rich feature
extraction
The pre-processing module extracts keywords,
sentences and document information.
All sentences are processed by the tok-
enizer/lemmatizer and at the same time specific in-
formation about the keywords is extracted.
Speculative keywords
Speculative sentences are evidenced by the pres-
ence of speculation keywords. We have the fol-
lowing observations:
? A hedge cue or speculative keyword 3 may be
composed of one or more word tokens.
? In terms of major linguistic categories, the
word tokens are heterogeneous: they may be
verbs, adjectives, nouns, determiners, etc. A
stop-word removing strategy was dismissed,
since no linguistic category can be elimi-
nated.
? A keyword may be covered by another longer
one. For instance, the keyword most can be
seen in keywords like most of all the heroes
or the most common.
Considering these characteristics for each sen-
tence, in the training stage, the keyword extraction
module retrieves the speculative/non-speculative
property of each sentence, the keyword occur-
rences, number of keywords in a sentence, the ini-
tial word token position and the number of word
tokens in the keyword. We build a keyword lex-
icon with all the extracted keywords and their
frequency in the training dataset, this speculative
keyword lexicon is used to tag keyword occur-
rences in non-speculative training sentences and
in all the evaluation dataset sentences.
The overlapping problem when tagging key-
words is solved by maximal matching strategy. It
is curious that speculation phrases come in de-
grees of specificity; the approach adopted here
favors ?specific? multi-word phrases over single-
word expressions.
Sentence processing
Often, speculation keywords convey certain in-
formation that can not be successfully expressed
by morphology or syntactic relations provided by
phrase structure grammar parsers. On the other
3Or just ?keyword? for sake of simplicity.
127
Figure 1: Block diagram for the speculation detection system.
hand, CCG derivations or dependencies provide
deeper information, in form of predicate-argument
relations. Previous works on semantic role label-
ing (Gildea and Hockenmaier, 2003; Boxwell et
al., 2009) have used features derived from CCG
parsings and obtained better results.
C&C parser provides CCG predicate-argument
dependencies and Briscoe and Carroll (2006) style
grammatical relations. We parsed the tokenized
sentences to obtain CCG derivations which are
binary trees as shown in the Figure 2. The
CCG derivation trees contain function category
and part-of-speech labels; this information is con-
tained in the tree structures to be used in building
a subtree dataset for the TK classifier.
4 Speculative sentence classifier
4.1 Tree Kernel classification
The subtree dataset is processed by a Tree Kernel
classifier (Moschitti, 2006) based on Support Vec-
tor Machines. TK uses a kernel function between
two trees, allowing a comparison between their
substructures, which can be subtrees (ST) or sub-
set trees (SST). We chose the comparison between
subset trees since it expands the kernel calculation
to those substructures with constituents that are
not in the leaves. Our intuition is that real specula-
tive sentences have deep semantic structures that
are particularly different from those ones in ap-
parent speculative sentences, and consequently the
comparison between the structures of well identi-
fied and potential speculative sentences may en-
hance the identification of real speculative key-
words.
4.2 Extracting tree structures
The depth of a CCG derivation tree is propor-
tional to the number of word tokens in the sen-
tence. Therefore, the processing of a whole deriva-
tion tree by the classifier is highly demanding and
many subtrees are not relevant for the classifica-
tion of speculative/non-speculative sentences, in
particular when the scope of the speculation is a
small proportion of a sentence.
In order to tackle this problem, a fragment of
the CCG derivation tree is extracted. This frag-
ment or subtree spans the keyword together with
neighbors terms in a fixed-size window of n word
tokens, (i.e. n word tokens to the left and n word
tokens to the right of the keyword) and has as root
the lower upper bound node of the first and last
tokens of this span. After applying the subtree ex-
traction, the subtree can contain more word tokens
in addition to those contained in the n-span, which
are replaced by a common symbol.
Potential speculative sentences are turned into
training examples. However, as described in Sec-
tion 3, a speculative sentence can contain one or
more speculative keywords. This can produce an
overlapping between their respective n-spans of
individual keywords during the subtree extraction,
producing subtrees with identical roots for both
keywords. For instance, in the following sen-
tence(c), the spans for the keywords suggests and
thought will overlap if n = 3.
(c) This suggests that diverse agents thought to ac-
tivate NF-kappa B ...
The overlapping interacts with the windows size
and potential extraction of dependency relations
128
It was reported to have burned for a day
PRP VBD VBN TO VB VBN IN DT NN
NP (S[dcl]\NP)/(S[pss]\NP) (S[pss]\NP)/(S[to]\NP) (S[to]\NP)/(S[b]\NP) (S[b]\NP)/(S[pt]\NP) S[pt]\NP ((S\NP)\(S\NP))/NP NP[nb]/N N
NP[nb]
(S[X]\NP)\(S[X]\NP)
S[pt]\NP
S[b]\NP
S[to]\NP
S[pss]\NP
S[dcl]\NP
S[dcl]
Figure 2: CCG derivations tree for It was reported to have burned for a day.
shared by terms belonging to the two different
spans. We deal with this issue by extracting one
training example if two spans have a common root
and two different examples otherwise.
4.3 Bag of features model
By default, our system classifies the sentences not
covered by the TK model using a baseline clas-
sifier that labels a sentence as speculative if this
has at least one keyword. Alternatively, a bag of
features classifier is used to complement the tree
kernel, aimed to provide a more precise method
that might detect even speculative sentences with
new keywords in the evaluation dataset. The set of
features used to build this model includes:
a) Word unigrams;
b) Lemma unigrams;
c) Word+POS unigrams;
d) Lemma+POS unigrams;
e) Word+Supertag unigrams;
f) Lemma+Supertag unigrams;
g) POS+Supertag unigrams;
h) Lemma bigrams;
i) POS bigrams;
j) Supertag bigrams;
k) Lemma+POS bigrams;
l) Lemma+Supertag bigrams;
m) POS+Supertag bigrams;
n) Lemma trigrams;
o) POS trigrams;
p) Supertag trigrams;
q) Lemma+POS trigrams;
r) Lemma+Supertag trigrams;
s) POS+Supertag trigrams;
t) Number of tokens;
u) Type of section in the document (Title, Text,
Section);
v) Name of section in the document;
w) Position of the sentence in a section starting
from beginning;
Dataset Dev. Train. Eval.
Biomedical 39 14541 5003
Wikipedia 124 11111 9634
Table 1: Datasets sizes.
x) Position of the sentence in a section starting
from end.
Position of the sentence information, composed by
the last four features, represents the information
about the sentence relative to a whole document.
The bag of features model is generated using a
Maximum Entropy algorithm (Zhang, 2004).
5 Experiments and results
5.1 Datasets
In the CoNLL-2010 Task 1, biomedical and
Wikipedia datasets were provided for develop-
ment, training and evaluation in the BioScope
XML format. Development and training datasets
are tagged with cue labels and a certainty feature.4
The number of sentences for each dataset 5 is de-
tailed in Table 1.
After manual revision of sentences not parsed
by C&C parser, we found that they contain equa-
tions, numbering elements (e.g. (i), (ii).. 1),
2) ), or long n-grams of named-entities, for in-
stance: ...mannose-capped lipoarabinomannan (
ManLAM ) of Mycobacterium tuberculosis ( M.
tuberculosis )... that out of a biomedical domain
appear to be ungrammatical. Similarly, in the
Wikipedia datasets, some sentences have many
named entities. This suggests the need of a spe-
cific pre-processor or a parser for this kind of sen-
tences like a named entity tagger.
In Table 2, we present the number of parsed sen-
tences, processed sentences by the TK model and
examples obtained in the tree structure extraction.
4certainty=?uncertain? and certainty=?certain?.
5The biomedical abstracts and biomedical articles training
datasets are processed as a single dataset.
129
Dataset Parsed Process. Samples
Biomedical train. 14442 10852 23511
Biomedical eval. 4903 3395 7826
Wikipedia train. 10972 7793 13461
Wikipedia eval. 9559 4666 8467
Table 2: Count of processed sentences.
5.2 Experimental results
The CoNLL-2010 organizers proposed in-domain
and cross-domain evaluations. In cross-domain
experiments, test datasets of one domain can be
used with classifiers trained on the other or on the
union of both domains. We report here our results
for the Wikipedia and biomedical datasets.
So far, we mentioned two settings for our clas-
sifier: a TK classifier complemented by a baseline
classifier (BL) and TK classifier complemented
by a bag of features classifier (TK+BF). Table
3 shows the scores of our submitted system (in-
domain Task 1) on the Wikipedia dataset, whereas
Table 4 gives the scores of the baseline system.
TP FP FN Precision Recall F
Our system 1033 480 1201 0.6828 0.4624 0.5514
Max. 1154 448 1080 0.7204 0.5166 0.6017
Min. 147 9 2087 0.9423 0.0658 0.123
Table 3: Comparative scores for our system with
CoNLL official maximum and minimum scores in
Task 1, Wikipedia dataset in-domain.
TP FP FN Precision Recall F
Biomedical 786 2690 4 0.2261 0.9949 0.3685
Wikipedia 1980 2747 254 0.4189 0.8863 0.5689
Table 4: Baseline results.
Additionally, we consider a bag of features clas-
sifier (BF) and a classifier that combines the base-
line applied to the sentences that have at least one
keyword plus the BF classifier for the remaining
sentences (BL+BF). In Tables 5 to 10, results for
the four classifiers (TK, TK+BF, BF, BL+BF) with
evaluations in-domain and cross-domain are pre-
sented6.The baseline scores confirm that relying on just
the keywords is not enough to identify speculative
sentences. In the biomedical domain, the classi-
fiers give high recall but too low precision result-
ing in low F-scores. Still, the TK, TK+BF and BF
(in-domain configurations) gives much better re-
sults than BL and BL+BF which indicates that the
information from CCG improves the performance
6It is worth to note that the keyword lexicons have been
not used in cross-domain way, so the TK and TK+BF models
have not been tested in regards to keywords.
TP FP FN Precision Recall F
BL 1980 2747 254 0.4189 0.8863 0.5689
TK 1033 480 1201 0.6828 0.4624 0.5514
TK+BF 1059 516 1175 0.6729 0.4740 0.5560
BF 772 264 1462 0.7452 0.3456 0.4722
BL+BF 2028 2810 206 0.4192 0.9078 0.5735
Table 5: Results for Wikipedia dataset in-domain.
TP FP FN Precision Recall F
BL 1980 2747 254 0.4189 0.8863 0.5689
TK 1776 2192 458 0.4476 0.7950 0.5727
TK+BF 1763 2194 471 0.4455 0.7892 0.5695
BF 403 323 1831 0.5551 0.1804 0.2723
BL+BF 1988 2772 246 0.4176 0.8899 0.5685
Table 6: Wikipedia data classified with biomedical
model scores (cross-domain).
TP FP FN Precision Recall F
BL 1980 2747 254 0.4189 0.8863 0.5689
TK 1081 624 1153 0.6340 0.4839 0.5489
TK+BF 1099 636 1135 0.6334 0.4919 0.5538
BF 770 271 1464 0.7397 0.3447 0.4702
BL+BF 2017 2786 217 0.4199 0.9029 0.5733
Table 7: Wikipedia data classified with biomedical
+ Wikipedia model scores (cross-domain).
TP FP FN Precision Recall F
BL 786 2690 4 0.2261 0.9949 0.3685
TK 759 777 31 0.4941 0.9606 0.6526
TK+BF 751 724 39 0.5092 0.9506 0.6631
BF 542 101 248 0.8429 0.6861 0.7565
BL+BF 786 2695 4 0.2258 0.9949 0.3681
Table 8: Biomedical data scores (in-domain).
TP FP FN Precision Recall F
BL 786 2690 4 0.2261 0.9949 0.3685
TK 786 2690 4 0.2261 0.9949 0.3685
TK+BF 771 2667 19 0.2243 0.9759 0.3647
BF 174 199 616 0.4665 0.2206 0.2992
BL+BF 787 2723 3 0.2242 0.9962 0.3660
Table 9: Biomedical data classified with
Wikipedia model scores (cross-domain).
TP FP FN Precision Recall F
BL 786 2690 4 0.2261 0.9949 0.3685
TK 697 357 93 0.6613 0.8823 0.7560
TK+BF 685 305 105 0.6919 0.8671 0.7697
BF 494 136 296 0.7841 0.6253 0.6958
BL+BF 786 2696 4 0.2257 0.9949 0.3679
Table 10: Biomedical data classified with biomed-
ical + Wikipedia model scores (cross-domain).
of the classifiers when compared to the baseline
classifier.
Even though in the Wikipedia domain the
TK+BF score is less than the baseline score, still
the performance of the classifiers do not fall much
in any of the in-domain and cross-domain exper-
iments. On the other hand, BF does not have a
good performance in 5 of 6 the experiments. To
make a more precise comparison between TK and
BF, the TK and BL+BF scores show that BL+BF
performs better than TK in only 2 of the 6 ex-
periments but the better performances achieved
by BL+BF are very small. This suggests that
130
the complex processing made by tree kernels is
more useful when disambiguating speculative key-
words than BF. Nonetheless, the bag-of-features
approach is also of importance for the task at hand
when combined with TK. We observe that the TK
classifer and BF classifier perform well making us
believe that the CCG derivations provide relevant
information for speculation detection. The use of
tree kernels needs further investigations in order to
evaluate the suitability of this approach.
6 Concluding remarks
Speculation detection is found to be a tough task
given the high ambiguity of speculative keywords.
We think these results can be improved by study-
ing the influences of context on speculation asser-
tions.
This paper presents a new approach for disam-
biguating apparent speculative keywords by us-
ing CCG information in the form of supertags and
CCG derivations. We introduce the use of the tree
kernel approach for CCG derivations trees. The
inclusion of other features like grammatical rela-
tions provided by the parser needs to be studied
before incorporating this information into the cur-
rent classifier and possibly to resolve the boundary
speculation detection problem.
Acknowledgments
This research is supported by the Trinity College
Research Scholarship Program and the Science
Foundation Ireland (Grant 07/CE/I1142) as part
of the Centre for Next Generation Localisation
(www.cngl.ie) at Trinity College of Dublin.
References
Aristotle. trans. 1991. The Art of Rhetoric. Penguin
Classics, London. Translated with an Introduction
and Notes by H.C. Lawson-Tancred.
Stephen Boxwell, Dennis Mehay, and Chris Brew.
2009. Brutus: A semantic role labeling system in-
corporating CCG, CFG, and dependency features.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, pages 37?45, Suntec, Singapore.
Ted Briscoe and John Carroll. 2006. Evaluating the
accuracy of an unlexicalized statistical parser on
the PARC DepBank. In Proceedings of the COL-
ING/ACL on Main conference poster sessions, pages
41?48, Morristown, NJ, USA.
Stephen Clark and James R. Curran. 2004. Parsing
the WSJ using CCG and log-linear models. In ACL
?04: Proceedings of the 42nd Annual Meeting on As-
sociation for Computational Linguistics, page 103,
Morristown, NJ, USA.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos
Csirik, and Gyo?rgy Szarvas. 2010. The CoNLL-
2010 Shared Task: Learning to Detect Hedges and
their Scope in Natural Language Text. In Proceed-
ings of the Fourteenth Conference on Computational
Natural Language Learning (CoNLL-2010): Shared
Task, pages 1?12, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Viola Ganter and Michael Strube. 2009. Finding
hedges by chasing weasels: Hedge detection using
Wikipedia tags and shallow linguistic features. In
Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers, pages 173?176, Suntec, Singapore.
Daniel Gildea and Julia Hockenmaier. 2003. Identi-
fying semantic roles using combinatory categorial
grammar. In Proceedings of 2003 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), Sapporo, Japan.
George Lakoff. 1973. Hedges: A study in meaning
criteria and the logic of fuzzy concepts. Journal of
Philosophical Logic, 2(4):458?508.
Ben Medlock and Ted Briscoe. 2007. Weakly super-
vised learning for hedge classification in scientific
literature. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 992?999, Prague, Czech Republic.
AlessandroMoschitti. 2006. Making tree kernels prac-
tical for natural language learning. In Proceedings
of the 11th Conference of the European Chapter of
the Association for Computational Linguistics.
Arzucan O?zgu?r and Dragomir R. Radev. 2009. Detect-
ing speculations and their scopes in scientific text.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1398?1407, Singapore.
Mark Steedman. 2000. The syntactic process. MIT
Press, Cambridge, MA, USA.
Gyo?rgy Szarvas, Veronika Vincze, Richa?rd Farkas, and
Ja?nos Csirik. 2008. The BioScope corpus: anno-
tation for negation, uncertainty and their scope in
biomedical texts. In Proceedings of the Workshop
on Current Trends in Biomedical Natural Language
Processing, pages 38?45, Columbus, Ohio.
Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim,
Tomoko Ohta, John McNaught, Sophia Ananiadou,
and Jun?ichi Tsujii. 2005. Developing a robust part-
of-speech tagger for biomedical text. In Advances in
Informatics, pages 382?392.
Le Zhang. 2004. Maximum entropy modeling toolkit
for Python and C++ (version 20041229). In Natural
Language Processing Lab, Northeastern.
131
Proceedings of the SIGDIAL 2013 Conference, pages 309?313,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
IMHO: An Exploratory Study of Hedging in Web Forums
Liliana Mamani Sanchez
Trinity College Dublin
mamanisl@scss.tcd.ie
Carl Vogel
Trinity College Dublin
vogel@tcd.ie
Abstract
We explore hedging in web forum con-
versations, which is interestingly different
to hedging in academic articles, the main
focus of recent automatic approaches to
hedge detection. One of our main results
is that forum posts using hedges are more
likely to get high ratings of their useful-
ness. We also make a case for focusing
annotation efforts on hedges that take the
form of first-person epistemic phrases.
1 Introduction
Computational linguistics research in hedging, use
of linguistic expressions whose contribution to
sentence meaning is a modulation of the accuracy
of the content they embed, and speculation detec-
tion has been done intensively in the domain of
scholarly texts. The interest created in this do-
main has expanded to some extent to other do-
mains such as news and reviews. Automatic pro-
cessing of speculation requires at some stage the
annotation of words or phrases conveying uncer-
tainty (Vincze et al, 2008). More complex en-
deavours imply the annotation of various elements
of context involved in the expression of hedging
(Rubin et al, 2005; Wiebe et al, 2005).
In web forums where users? contributions play
a vital role in the forum dynamics, such as mutual
support forums that are part of the ecosystem of
technology company supports for users, exploring
the features that make a contributor outstanding
is relevant.1 A user shows a distinctive behavior
by writing useful posts that help other users in the
problem that first motivated their participation in
1Throughout, we use ?web forum? to refer to such ecosys-
tems: we speculate that their informal nature makes our ob-
servations generalize to other sorts of web forum in which
solutions to problems are not the focal point; even general
discussion forums can be witnessed to trigger community
weighting of contributions.
the forum. This paper emerges from our interest
in finding features that predict which contributors
will be most appreciated.
Many lexical and grammatical devices aid
hedging (expressions such as epistemics verbs,
modals, adjectives, etc. name but a few) as do non-
lexical devices such as conditionals. We deem sin-
gular first person epistemic phrases as hedges that
can help to identify the subject of a hedging event.
We analyze the correlation between the use of
epistemic phrases (vs. other types of hedges) and
the probability of posts containing these hedges of
being considered useful by the forum community.
We also explore whether epistemic phrases consti-
tute a distinctive feature that support user classifi-
cations. In ?2, we described the function of hedges
according to a hedging classification framework
and in relation to the domain of web forums. Then
?3 describes the profiling work done and discusses
the main findings. We conclude in ?4.
2 Functions of hedging
The research by Hyland (1998) is one of the broad-
est studies about hedging functions in scientific
articles, and which makes use of categories that
have strong relationship, at face value, to the like-
lihood that the reader of hedged material will find
the material sufficiently useful or sufficiently well
expressed to prompt the reader to rate highly the
message containing the material, whether with an
explicit facility to record kudos or otherwise. Hy-
land proposed a poly-pragmatic classification of
hedges based on their indicating function: reader-
oriented, writer-oriented, attribute and reliability.
Briefly, attribute and reliability hedges both re-
late to the accuracy of the message conveyed. At-
tribute hedges relate to the conformity of the de-
scribed situation with encyclopedic expectations
(1), while reliability hedges relate to the level of
certainty of the speaker about the propositional
content (2). In a different dimension, reader ori-
309
ented hedges are composed with the concern that
the ?reader? accept the truth of the embedded
content (3), thereby presupposing the ?writer?s?
commitment to the content, while writer oriented
hedges disclaim commitment to the content (4).
(1) Protypical mammals are land-dwellers.
(2) Probably, respected ancient Greeks
thought whales to be fish.
(3) I think that if you reboot, the changes will
take effect.
(4) Based on what you?ve said, you seem
right.
Applying this classification scheme not to schol-
arly prose but to web forums, it seems likely that
readers in technical forums would prefer the accu-
racy of attribute hedges (1) over the relative uncer-
tainty of reliability hedges (2), and that the reader
oriented hedges (3) supply comfort in the implica-
tion of both the quality of the embedded claims
and the absence of arrogance. This research is
attempting to test these hypotheses by assessing
the relationship between the likelihood of posts re-
ceiving kudos and the quantity of hedges in these
categories that the posts contain.
Unfortunately, answering the question is com-
plex, because it is not in all cases obvious whether
a linguistic expression contains a hedge or what
function the hedges serve when they do exist.
Therefore, we attempt a partial answer to the ques-
tion by examining those hedge expressions which
can be processed with some reliability using au-
tomated means. Consider the taxonomy of lin-
guistic expressions in Fig. 1. The boxed regions
of this taxonomy are amenable to automatic pro-
cessing. Further, epistemic hedges with first-
person singular subjects relate strongly to reader
oriented hedges (3) in Hyland?s taxonomy. The
non-phrasal hedges are heterogeneous in function.
Figure 1: A taxonomy of linguistic expressions.
Linguistic Expressions
epistemic hedges other expressions
non?phrasal
lexical
phrasal
1st person singular other
conditionals ...
...
...
We do not claim this separation of hedging
markers can fully account for pragmatic and se-
mantic analysis of hedging in web forums, but we
are confident this classification supports reliable
annotation for quantificational assessment of cer-
tainty and hedging in this informal domain. We
base our profiling experiments (?3) on this func-
tional separation of hedging markers.
3 Profiling posts by hedging
3.1 Description of the forum dataset
The dataset we used created out of a forum that
is part of customer support services provided by
a software vendor company. Although we were
not able to confirm the forum demographics, we
can infer they are mostly American English speak-
ers as the forum was set up first for USA cus-
tomers. Some other features are best described by
Vogel and Mamani Sanchez (2013). Our dataset
is composed of 172,253 posts that yield a total
of 1,044,263 sentences. This dataset has been
intensively ?cleaned?, as originally it presented
a great variety of non-linguistic items such as
HTML codes for URLS, emoticons, IP addresses,
etc. These elements were replaced by wild-cards
and also user names have been anonymised, al-
though some non-language content may remain.
A forum user can give a post ?kudos? if he/she
finds it useful or relevant to the topic being ad-
dressed in a forum conversation.2 We counted
the number of kudos given to each post. There
are four user categories in the forum: {employee,
guru, notranked, ranked}.3 A poster?s rank de-
pends, among other factors, on the number of
posts they make and their aggregate kudos.
3.2 Epistemic phrases versus other hedges
We created two lexicons, one composed by first
person singular epistemic phrases and one by non-
phrasal hedges. Initially, a set of epistemic phrases
where taken from Ka?rkka?inen (2010): {I think, I
don?t know, I know, etc.} and from Wierzbicka
(2006). The non-phrasal hedge lexicon was cre-
ated from words conveying at least some degree of
uncertainty: {appear, seem, sometimes, suggest,
unclear, think, etc.}, taken from Rubin (2006).
Additional hedges were included after the pilot
2A user may accord kudos for any reason at all, in fact.
3In the forum we studied, there are actually many ranks,
with guru as the pinnacle for a non-employee; we grouped
the non-guru ranked posters together.
310
annotation. The lexicons are composed by 76
and 109 items, respectively. There are many
other hedge instances that are not included in
these lexicons but our experiment restricts to these
items. Epistemic phrases include acronyms such
as ?IMHO?, ?IMO? and ?AFAIK? that we deem
meet functions described in ?2.
A pilot manual annotation of hedges was con-
ducted on in order to verify the viability of auto-
matic annotation. Our automatic annotation pro-
cedure performs a sentence by sentence matching
and tagging of both kinds of hedging. The pro-
cedure uses a maximal matching strategy to tag
hedges, e.g. if ?I would suggest? is found, this
is tagged and not ?suggest?. This automatic tag-
ging procedure does not account for distinctions
between epistemic and deontic readings of hedges,
nor between speculative or non-speculative uses of
non-phrasal hedges. 107,134 posts contain at least
one hedge: 34,301 posts contain at least one epis-
temic phrase; 101,086, at least one non-phrasal
hedge; 28,253, at least one of each.
3.3 Methods of analysis
In ?3.1 we showed there are two ways to charac-
terize a post: 1) By its writer category and 2) by
the number of times it gets accorded kudos. We
devise a third characterisation by exploring epis-
temic phrases and non-phrasal hedge usage in in-
dividual posts as a whole, tracking use of both
types of hedge in each post. We devised three
discretization functions (DF) for assigning a la-
bel to each post depending on the type of hedges
contained within. The DFs take two parameters,
each one representing either the relative or bina-
rized frequency non-phrasal hedges and epistemic
phrases (nphr or epphr). DF1 relies on the oc-
currence of either type of hedge; a post is of a
mixed nature if it has at least one of each hedge
type. DF2 is based on a majority decision depend-
ing on the hedge type that governs the post and
only assigns the label hedgmixed when both types
of hedges appear in the same magnitude. DF3
expands DF1 and DF2 by evaluating whether ei-
ther majority or only one type of hedge is found,
e.g. we wanted to explore the fact that even when
non-phrasal hedges domain one post, an epistemic
phrase is contained as well, in contrast to when
only non-phrasal hedges occur in a post.
DF
1 epphr==0 epphr>0
nphr ==0 nohedges epphrasal
nphr >0 nonphrasal hedgmixed
DF
2 nphr=0 & epphr=0 nohedges
nphr > epphr nonphrasal
nphr < epphr epphrasal
nphr =epphr hedgmixed
DF
3
epphr=0 epphr>0
nphr=0 nohedges epphronly
nphr>0 nonphronly
nphr > epphr nonphrmostly
nphr < epphr epphrmostly
nphr =epphr hedgmixed
We computed four measures for each post based
on these functions, m1 is calculated by using DF1
having raw frequencies of hedges as parameters,
m2 and m3 result from applying DF3 and DF2
respectively to frequencies of hedge type averaged
by the corresponding lexicon size, and m4 is cal-
culated from DF3 over hedge frequencies aver-
aged by post word count. Other measures are also
possible, but these seemed most intuitive.
We were interested in the extent that hedge-
based post categories correlate with a post?s kudos
and with a post?s user category as tests of hypoth-
esis outlined in ?2. We want to know which cor-
relations hold regardless of the choice of intuitive
measure and which are measure dependent.
3.4 Results and discussion
1.40
1.45
1.50
1.55
1.60
Mea
n of
 kud
os
epphrasal
hedgmixed
nohedges
nonphrasal
epphrmostly
epphronly
hedgmixed
nohedges
nonphrmostly
nonphronly
epphrasal
hedgmixed
nohedges
nonphrasal
epphrmostlyepphronly
hedgmixed
nohedges
nonphrmostly
nonphronly
m1 m2 m3 m4
Figure 2: Design plot with the mean of kudos of
each kind of post per each measure.
In Fig. 2, we show how the different hedge-
based classifications of posts (m1, m2, m3, m4)
relate to the average kudo counts for posts. Each
measure is shown in an individual scale.4 The hor-
izontal line represents the average of kudos for
all posts so we can observe which categories are
above/below the mean. Comparison and contrast
4For this comparison, we dropped extreme outliers in the
number of kudos and hedges, and we calculated these mea-
sures only in posts that had at least one kudo attribution.
311
of the relationship between categorisation of posts
with each mi and mean kudos is interesting. For
example, when epistemic phrases dominate a post
(epphrmostly), there is the greatest mean of ku-
dos visible with the measure m2. The second
highest positive effect is of non-phrasal hedges
dominating a post (nonphrmostly) in m2 and m4.
The next strongest effect occurs when both of
hedges types appear in a post (hedgmixed in m1
and m3) and when they have about the same av-
erage density (m4), followed by when non-phrasal
hedges appear exclusively in a post. While there is
no consensus across the different scales that epis-
temic phrase-dominated posts are the most likely
to obtain kudos, still their occurrence has a posi-
tive effect in the average of kudos obtained. There
is low probability of kudos when only epistemic
phrases appear and the lowest probability when no
hedge occurs.5 Thus, we argue that the four mea-
sures are jointly and individually useful.
employee guru notranked ranked
0.0
0.1
0.2
0.3
0.4
0.5
epp
hras
al
hed
gmi
xed
noh
edg
es
non
phra
sal
epp
hras
al
hed
gmi
xed
noh
edg
es
non
phra
sal
epp
hras
al
hed
gmi
xed
noh
edg
es
non
phra
sal
epp
hras
al
hed
gmi
xed
noh
edg
es
non
phra
sal
m1
Per
cen
tage
 of p
osts
 per
 hed
ge t
ype
Figure 3: Percentages of m1-hedge types in each
user category.
The relationship between hedge use and user
category is depicted (for m1) in Fig. 3. While
for all four user roles, epistemic phrases are exclu-
sively present in the lowest percentage of posts,
their contribution is shown in posts with mixed
hedge types. Posts with only non-phrasal hedges
are the most frequent across all user categories.
We had predicted no significance in this respect
5The contribution of epistemic phrases to the likelihood
of kudos could be due to other factors such as the use of first
person in general. We profiled the use of pronouns ?I? and
?my? and we found a negative correlation between frequency
of these pronouns and the number of kudos per post. There
is a small but not significant correlation restricting to those
posts with non-zero kudos.
since non-phrasal hedges could map into any of
Hyland?s functions, however our intuition was
wrong as there is a significant difference (p<0.05)
in the proportions of posts per hedge type category
when making comparisons across user categories
one to one. Only when comparing proportions of
hedge type posts by gurus and notranked users
is there no significant difference in hedgmixed,
nonphrasal and nohedges posts.6 Employees and
ranked users have the highest rates of use of mixed
hedges. Ranked and guru posts have the high-
est ratios of exclusively epistemic phrase hedges,
meeting expectations. Employees have the low-
est ratio of user of epistemic phrases on their own,
this presumably since they frequently write posts
on behalf of the company so they are least likely
to make subjective comments: their posts have the
lowest percentage of use of ?I? and ?my?.
These two approaches to assessing associations
between different classifications of forum posts re-
veal that posts using hedges are the most likely to
be accorded kudos and that guru and ranked users
are the most frequent users of epistemic phrases
in general. This lends support to the view that
first person singular epistemic phrases, the epit-
ome of reader-oriented hedges, are predictive of
coarse grained rank in the forum.
4 Conclusions and future work
We have found that the hedges used contribute to
the probability of a post getting high ratings. Posts
with no hedges are the ones awarded least kudos.
We have still to test the correlation between epis-
temic phrases and other types of hedges when they
both are found in a single post. We think that au-
tomatic methods should focus in first person epis-
temic phrases as they show writer?s stance at the
same time as softening their commitment or antic-
ipating reader?s response. Following the annota-
tion described here, manual annotation work is un-
der way, where epistemic phrases and non-phrasal
hedges constitute two distinct categories. Our on-
going work seeks other ways to measure the con-
tribution of these categories to reader expression
of appreciation of posts and whether hedge us-
age creates natural user categorizations. We also
study other types of web forum dialogue to explore
whether hedging follows similar trends.
6A two-sample test of proportions was used to test the
significance of differences between amounts of hedge type
posts for each category.
312
Acknowledgements
This research is supported by the Trinity College
Research Scholarship Program and the Science
Foundation Ireland (Grant 07/CE/I1142) as part
of the Centre for Next Generation Localisation
(www.cngl.ie) at Trinity College Dublin. This pa-
per has benefited from input from the anonymous
reviewers and from Ron Artstein.
References
Ken Hyland. 1998. Hedging in Scientific Research Ar-
ticles. Pragmatics & beyond. John Benjamins Pub-
lishing Company.
Elise Ka?rkka?inen. 2010. Position and scope of epis-
temic phrases in planned and unplanned american
english. In New approaches to hedging, pages 207?
241. Elsevier, Amsterdam.
Victoria Rubin, Elizabeth Liddy, and N. Kando. 2005.
Certainty identification in texts: Categorization
model and manual tagging results. In James G.
Shanahan, Yan Qu, and Janyce Wiebe, editors, Com-
puting Attitude and Affect in Text. Springer.
Victoria L. Rubin. 2006. Identifying Certainty in Texts.
Ph.D. thesis, Syracuse University, Syracuse, NY.
Veronika Vincze, Gyorgy Szarvas, Richard Farkas, Gy-
orgy Mora, and Janos Csirik. 2008. The BioScope
corpus: biomedical texts annotated for uncertainty,
negation and their scopes. BMC Bioinformatics,
9(Suppl 11).
Carl Vogel and Liliana Mamani Sanchez. 2013. Epis-
temic signals and emoticons affect kudos. Cognitive
Infocommunications (CogInfoCom), 2012 IEEE 3rd
International Conference on, pages 517?522.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language ANN. Language Resources and
Evaluation, 39(2/3):164?210.
A. Wierzbicka. 2006. English: meaning and culture.
Oxford University Press, USA.
313
