Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 642?647,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Probabilistic Document Modeling for Syntax Removal in Text
Summarization
William M. Darling
School of Computer Science
University of Guelph
50 Stone Rd E, Guelph, ON
N1G 2W1 Canada
wdarling@uoguelph.ca
Fei Song
School of Computer Science
University of Guelph
50 Stone Rd E, Guelph, ON
N1G 2W1 Canada
fsong@uoguelph.ca
Abstract
Statistical approaches to automatic text sum-
marization based on term frequency continue
to perform on par with more complex sum-
marization methods. To compute useful fre-
quency statistics, however, the semantically
important words must be separated from the
low-content function words. The standard ap-
proach of using an a priori stopword list tends
to result in both undercoverage, where syn-
tactical words are seen as semantically rele-
vant, and overcoverage, where words related
to content are ignored. We present a genera-
tive probabilistic modeling approach to build-
ing content distributions for use with statisti-
cal multi-document summarization where the
syntax words are learned directly from the
data with a Hidden Markov Model and are
thereby deemphasized in the term frequency
statistics. This approach is compared to both a
stopword-list and POS-tagging approach and
our method demonstrates improved coverage
on the DUC 2006 and TAC 2010 datasets us-
ing the ROUGE metric.
1 Introduction
While the dominant problem in Information Re-
trieval in the first part of the century was finding
relevant information within a datastream that is ex-
ponentially growing, the problem has arguably tran-
sitioned from finding what we are looking for to sift-
ing through it. We can now be quite confident that
search engines like Google will return several pages
relevant to our queries, but rarely does one have time
to go through the enormous amount of data that is
supplied. Therefore, automatic text summarization,
which aims at providing a shorter representation of
the salient parts of a large amount of information,
has been steadily growing in both importance and
popularity over the last several years. The summa-
rization tracks at the Document Understanding Con-
ference (DUC), and its successor the Text Analysis
Conference (TAC)1, have helped fuel this interest by
hosting yearly competitions to promote the advance-
ment of automatic text summarization methods.
The tasks at the DUC and TAC involve taking
a set of documents as input and outputting a short
summary (either 100 or 250 words, depending on
the year) containing what the system deems to be the
most important information contained in the original
documents. While a system matching human perfor-
mance will likely require deep language understand-
ing, most existing systems use an extractive, rather
than abstractive, approach whereby the most salient
sentences are extracted from the original documents
and strung together to form an output summary.2
In this paper, we present a summarization model
based on (Griffiths et al, 2005) that integrates top-
ics and syntax. We show that a simple model that
separates syntax and content words and uses the
content distribution as a representative model of
the important words in a document set can achieve
high performance in multi-document summariza-
tion, competitive with state-of-the-art summariza-
tion systems.
1http://www.nist.gov/tac
2NLP techniques such as sentence compression are often
used, but this is far from abstractive summarization.
642
2 Related Work
2.1 SumBasic
Nenkova et al (2006) describe SumBasic, a simple,
yet high-performing summarization system based on
term frequency. While the methodology underly-
ing SumBasic departs very little from the pioneer-
ing summarization work performed at IBM in the
1950?s (Luhn, 1958), methods based on simple word
statistics continue to outperform more complicated
approaches to automatic summarization.3 Nenkova
et al (2006) empirically showed that a word that ap-
pears more frequently in the original text will be
more likely to appear in a human generated sum-
mary.
The SumBasic algorithm uses the empirical uni-
gram probability distribution of the non-stop-words
in the input such that for each word w, p(w) = nwN
where nw is the number of occurrences of word w
andN is the total number of words in the input. Sen-
tences are then scored based on a composition func-
tion CF (?) that composes the score for the sentence
based on its contained words. The most commonly
used composition function adds the probabilities of
the words in a sentence together, and then divides by
the number of words in that sentence. However, to
reduce redundancy, once a sentence has been chosen
for summary inclusion, the probability distribution
is recalculated such that any word that appears in
the chosen sentence has its probability diminished.
Sentences are continually marked for inclusion un-
til the summary word-limit is reached. Despite its
simplicity, SumBasic continues to be one of the top
summarization performers in both manual and auto-
matic evaluations (Nenkova et al, 2006).
2.2 Modeling Content and Syntax
Griffiths et al (2005) describe a composite gener-
ative model that combines syntax and semantics.
The semantic portion of the model is similar to La-
tent Dirichlet Allocation and models long-range the-
matic word dependencies with a set of topics, while
short-range (sentence-wide) word dependencies are
modeled with syntax classes using a Hidden Markov
Model. The model has an HMM at its base where
3A system based on SumBasic was one of the top performers
at the Text Analysis Conference 2010 summarization track.
one of its syntax classes is replaced with an LDA-
like topic model. When the model is in the semantic
class state, it chooses a topic from the given docu-
ment?s topic distribution, samples a word from that
topic?s word distribution, and generates it. Other-
wise, the model samples a word from the current
syntax class in the HMM and outputs that word.
3 Our Summarization Model
Nenkova et al (2006) show that using term fre-
quency is a powerful approach to modeling human
summarization. Nevertheless, for SumBasic to per-
form well, stop-words must be removed from the
composition scoring function. Because these words
add nothing to the content of a summary, if they
were not removed for the scoring calculation, the
sentence scores would no longer provide a good fit
with sentences that a human summarizer would find
salient. However, by simply removing pre-selected
words from a list, we will inevitably miss words
that in different contexts would be considered non-
content words. In contrast, if too many words are
removed, the opposite problem appears and we may
remove important information that would be useful
in determining sentence scores. These problems are
referred to as undercoverage and overcoverage, re-
spectively.
To alleviate this problem, we would like to put
less probability mass for our document set proba-
bility distribution on non-content words and more
on words with strong semantic meaning. One ap-
proach that could achieve this would be to build sep-
arate stopword lists for specific domains, and there
are approaches to automatically build such lists (Lo
et al, 2005). However, a list-based approach can-
not take context into account and therefore, among
other things, will encounter problems with poly-
semy and synonymy. Another approach would be to
use a part-of-speech (POS) tagger on each sentence
and ignore all non-noun words because high-content
words are almost exclusively nouns. One could also
include verbs, adverbs, adjectives, or any combina-
tion thereof, and therefore solve some of the context-
based problems associated with using a stopword
list. Nevertheless, this approach introduces deeper
context-related problems of its own (a noun, for ex-
ample, is not always a content word). A separate ap-
643
DM
N
M
c w
?
?
C
?
?
?
?
z
Figure 1: Graphical model depiction of our content and
syntax summarization method. There are D document
sets, M documents in each set, NM words in document
M , and C syntax classes.
proach would be to model the syntax and semantic
words used in a document collection in an HMM, as
in Griffiths et al (2005), and use the semantic class
as the content-word distribution for summarization.
Our approach to summarization builds on Sum-
Basic, and combines it with a similar approach
to separating content and syntax distributions as
that described in (Griffiths et al, 2005). Like
(Haghighi and Vanderwende, 2009), (Daume? and
Marcu, 2006), and (Barzilay and Lee, 2004), we
model words as being generated from latent distribu-
tions. However, instead of background, content, and
document-specific distributions, we model all words
in a document set as being there for one of only two
purposes: a semantic (content) purpose, or a syntac-
tic (functional) purpose. We model the syntax class
distributions using an HMM and model the content
words using a simple language model. The princi-
pal difference between our generative model and the
one described in (Griffiths et al, 2005) is that we
simplify the model by assuming that each document
is generated solely from one topic distribution that is
shared throughout each document set. This results in
a smoothed language model for each document set?s
content distribution where the counts from content
words (as determined through inference) are used to
determine their probability, and the syntax words are
essentially discarded.
Therefore, our model describes the process of
generating a document as traversing an HMM and
in
at
of
on
with
by
el
nino
weather
pacific
ocean
normal
temperatures
said
told
asked
say
says
...
...
Figure 2: Portion of Content and Syntax HMM. The
left and right states show the top words for those syntax
classes while the middle state shows the top words for the
given document set?s content distribution.
emitting either a content word from a single topic?s
(document set?s) content word distribution, or a syn-
tax word from one of C corpus-wide syntax classes
where C is a parameter input to the algorithm. More
specifically, a document is generated as follows:
1. Choose a topic z corresponding to the given
document set (z = {z1, ..., zk} where k is the
number of document sets to summarize.)
2. For each word wi in document d
(a) Draw ci from pi(ci?1)
(b) If ci = 1, then draw wi from ?(z), other-
wise draw wi from ?(ci)
Each class ci and topic z correspond to multinomial
distributions over words, and transitions between
classes follow the transition distribution pi(ci?1).
When ci = 1, a content word is emitted from
the topic word distribution ?(z) for the given doc-
ument set z. Otherwise, a syntax word is emitted
from the corpus-wide syntax word distribution ?(ci).
The word distributions and transition vectors are all
drawn from Dirichlet priors. A graphical model de-
piction of this distribution is shown in Figure 1. A
portion of an example HMM (from the DUC 2006
dataset) is shown in Figure 2 with the most proba-
ble words in the content class in the middle and two
syntax classes on either side of it.
3.1 Inference
Because the posterior probability of the content
(document set) word distributions and syntax class
word distributions cannot be solved analytically, as
with many topic modeling approaches, we appeal
644
to an approximation. Following Griffiths et al
(2005), we use Markov Chain Monte Carlo (see,
e.g. (Gilks et al, 1999)), or more specifically, ?col-
lapsed? Gibbs sampling where the multinomial pa-
rameters are integrated out.4 We ran our sampler for
between 500 and 5,000 iterations (though the dis-
tributions would typically converge by 1,000 itera-
tions), and chose between 5 and 10 (with negligible
changes in results) for the cardinality of the classes
set C. We leave optimizing the number of syntax
classes, or determining them directly from the data,
for future work.
3.2 Summarization
Here we describe how we use the estimated topic
and syntax distributions to perform extractive multi-
document summarization. We follow the SumBasic
algorithm, but replace the empirical unigram distri-
bution of the document set with the learned topic
distributions for the given documents. This models
the effect of not only ignoring stop-words, but also
reduces the amount of probability mass in the distri-
bution placed on functional words that serve no se-
mantic purpose and that would likely be less useful
in a summary. Because this is a fully probabilistic
model, we do not entirely ?ignore? stop-words; in-
stead, the model forces the probability mass of these
words to the syntax classes.
For a given document set to be summarized, each
sentence is assigned a score corresponding to the
average probability of the words contained within
it: Score(S) = 1|S|
?
w?S p(w). In SumBasic,
p(wi) =
ni
N . In our model, SyntaxSum, p(wi) =
p(wi|?(z)), where ?(z) is a multinomial distribution
over the corpus? fixed vocabulary that puts high
probabilities on content words that are used often
in the given document set and low probabilities
on words that are more important in other syntax
classes. The middle node in Figure 2 is a true repre-
sentation of the top words in the ?(z) distribution for
document set 43 in the DUC 2006 dataset.
4 Experiments and Results
Here we describe our experiments and give quanti-
tative results using the ROUGE automatic text sum-
4See http://lingpipe.files.wordpress.com/
2010/07/lda1.pdf for more information.
Method
ROUGE ROUGE (-s)
R-1 R-2 R-SU4 R-1 R-2 R-SU4
SB- 37.0 5.5 11.0 23.3 3.8 6.2
SumBasic 38.1 6.7 11.9 29.4 5.3 8.1
N 36.8 7.0 12.2 25.5 4.8 7.3
N,V 36.9 6.5 12.0 24.4 4.4 6.9
N,J 37.4 6.8 12.3 26.5 5.0 7.7
N,V,J 37.4 6.8 12.2 25.5 4.9 7.4
SBH 38.9 7.3 12.6 30.7 5.9 8.7
Table 1: ROUGE Results on the DUC 2006 dataset. Re-
sults statistically significantly higher than SumBasic (as
determined by a pairwise t-test with 99% confidence) are
displayed in bold.
marization metric for unigram (R-1), bigram (R-2),
and skip-4 bigram (R-SU4) recall both with and
without (-s) stopwords removed (Lin, 2004). We
tested our models on the popular DUC 2006 dataset
which aids in model comparison and also on the
more recent TAC 2010 dataset. The DUC 2006
dataset consists of 50 sets of 25 news articles each,
whereas the TAC 2010 dataset consists of 46 sets of
10 news articles each.5 For DUC 2006, summaries
are a maximum of 250 words; for TAC 2010, they
can be at most 100. Our approach is compared to
using an a priori stopword list, and using a POS-
tagger to build distributions of words coming from
only a subset of the parts-of-speech.
4.1 SumBasic
To cogently demonstrate the effect of ignoring non-
semantic words in term frequency-based summa-
rization, we implemented two initial versions of
SumBasic. The first, SB-, does not ignore stop-
words while the second, SumBasic, ignores all stop-
words from a list included in the Python NLTK li-
brary.6 For SumBasic without stop-word removal
(SB-), we obtain 3.8 R-2 and 6.2 R-SU4 (with the -s
flag).7 With stop-words removed from the sentence
scoring calculation (SumBasic), our results increase
to 5.3 R-2 and 8.1 R-SU4, a significantly large in-
crease. For complete ROUGE results of all of our
tested models on DUC 2006, see Table 1.
5We limit our testing to the initial TAC 2010 data as opposed
to the update portion.
6Available at http://www.nltk.org.
7Note that we present our ROUGE scores scaled by 100 to
aid in readability.
645
4.2 POS Tagger
Because the content distributions learned from our
model seem to favor almost exclusively nouns (see
Figure 2), another approach to building a seman-
tically strong word distribution for determining
salient sentences in summarization might be to ig-
nore all words except nouns. This would avoid
most stopwords (many of which are modeled as their
own part-of-speech) and would serve as a simpler
approach to finding important content. Neverthe-
less, adjectives and verbs also often carry impor-
tant semantic information. Therefore, we ran a POS
tagger over the input sentences and tried selecting
sentences based on word distributions that included
only nouns; nouns and verbs; nouns and adjectives;
and nouns, verbs, and adjectives. In each case,
this approach performs either worse than or no bet-
ter than SumBasic using a priori stopword removal.
The nouns and adjectives distribution did the best,
whereas the nouns and verbs were the worst.
4.3 Content and Syntax Model
Finally, we test our model. Using the content dis-
tributions found by separating the ?content? words
from the ?syntax? words in our modified topics and
syntax model, we replaced the unigram probabil-
ity distribution p(w) of each document set with the
learned content distribution for that document set?s
topic, ?(z), where z is the topic for the given docu-
ment set. Following this method, which we call SBH
for ?SumBasic with HMM?, our ROUGE scores in-
crease considerably and we obtain 5.9 R-2 and 8.7
R-SU4 without stop-word removal. This is the high-
est performing model we tested. Due to space con-
straints, we omit full TAC 2010 results but R-2 and
R-SU4 results without stopwords improved from
SumBasic?s 7.3 and 8.6 to 8.0 and 9.1, respectively,
both of which were statistically significant increases.
5 Conclusions and Future Work
This paper has described using a domain-
independent document modeling approach of
avoiding low-content syntax words in an NLP task
where high-content semantic words should be the
principal focus. Specifically, we have shown that
we can increase summarization performance by
modeling the document set probability distribution
using a hybrid LDA-HMM content and syntax
model. We model a document set?s creation by
separating content and syntax words through
observing short-range and long-range word depen-
dencies, and then use that information to build a
word distribution more representative of content
than either a simple stopword-removed unigram
probability distribution, or one made up of words
from a particular subset of the parts-of-speech.
This is a very flexible approach to finding content
words and works well for increasing performance of
simple statistics-based text summarization. It could
also, however, prove to be useful in any other NLP
task where stopwords should be removed. Some
future work includes applying this model to areas
such as topic tracking and text segmentation, and
coherently adjusting it to fit an n-gram modeling
approach.
Acknowledgments
William Darling is supported by an NSERC Doc-
toral Postgraduate Scholarship. The authors would
like to acknowledge the financial support provided
from Ontario Centres of Excellence (OCE) through
the OCE/Precarn Alliance Program. We also thank
the anonymous reviewers for their helpful com-
ments.
References
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In HLT-NAACL
2004: Proceedings of the Main Conference, pages
113?120. Best paper award.
Hal Daume?, III and Daniel Marcu. 2006. Bayesian
query-focused summarization. In ACL-44: Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and the 44th annual meeting of
the Association for Computational Linguistics, pages
305?312, Morristown, NJ, USA. Association for Com-
putational Linguistics.
W. R. Gilks, S. Richardson, and D. J. Spiegelhalter. 1999.
Markov ChainMonte Carlo In Practice. Chapman and
Hall/CRC.
Thomas L. Griffiths, Mark Steyvers, David M. Blei, and
Joshua B. Tenenbaum. 2005. Integrating topics and
syntax. In In Advances in Neural Information Pro-
cessing Systems 17, pages 537?544. MIT Press.
646
Aria Haghighi and Lucy Vanderwende. 2009. Exploring
content models for multi-document summarization. In
NAACL ?09: Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 362?370, Morristown, NJ,
USA. Association for Computational Linguistics.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Stan Szpakowicz Marie-
Francine Moens, editor, Text Summarization Branches
Out: Proceedings of the ACL-04 Workshop, pages 74?
81, Barcelona, Spain, July. Association for Computa-
tional Linguistics.
Rachel Tsz-Wai Lo, Ben He, and Iadh Ounis. 2005. Au-
tomatically building a stopword list for an information
retrieval system. JDIM, pages 3?8.
H. P. Luhn. 1958. The automatic creation of literature
abstracts. IBM J. Res. Dev., 2(2):159?165.
Ani Nenkova, Lucy Vanderwende, and Kathleen McKe-
own. 2006. A compositional context sensitive multi-
document summarizer: exploring the factors that in-
fluence summarization. In SIGIR ?06: Proceedings of
the 29th annual international ACM SIGIR conference
on Research and development in information retrieval,
pages 573?580, New York, NY, USA. ACM.
647
Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 96?103,
24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational Linguistics
Feature Selection for Sentiment Analysis
Based on Content and Syntax Models
Adnan Duric and Fei Song
School of Computer Science, University of Guelph, 50 Stone Road East,
Guelph, Ontario, N1G 2W1, Canada
{aduric,fsong}@uoguelph.ca
Abstract
Recent solutions for sentiment analysis have
relied on feature selection methods ranging
from lexicon-based approaches where the set
of features are generated by humans, to ap-
proaches that use general statistical measures
where features are selected solely on empiri-
cal evidence. The advantage of statistical ap-
proaches is that they are fully automatic, how-
ever, they often fail to separate features that
carry sentiment from those that do not. In this
paper we propose a set of new feature selec-
tion schemes that use a Content and Syntax
model to automatically learn a set of features
in a review document by separating the enti-
ties that are being reviewed from the subjec-
tive expressions that describe those entities in
terms of polarities. By focusing only on the
subjective expressions and ignoring the enti-
ties, we can choose more salient features for
document-level sentiment analysis. The re-
sults obtained from using these features in a
maximum entropy classifier are competitive
with the state-of-the-art machine learning ap-
proaches.
1 Introduction
As user generated data become more commonplace,
we seek to find better approaches to extract and clas-
sify relevant content automatically. This gives users
a richer, more informative, and more appropriate set
of information in an efficient and organized manner.
One way for organizing such data is text classifica-
tion, which involves mapping documents into topi-
cal categories based on the occurrences of particular
features. Sentiment Analysis (SA) can be framed as
a text classification task where the categories are po-
larities such as positive and negative. However, the
similarities end here. Whereas general text classi-
fication is concerned with features that distinguish
different topics, sentiment analysis deals with fea-
tures about subjectivity, affect, emotion, and points-
of-view that describe or modify the related entities.
Since user-generated review documents contain both
kinds of features, SA solutions ultimately face the
challenge of separating the factual content from the
subjective content describing it.
For example, taking a segment from a randomly
chosen document in Pang et al?s movie review cor-
pus1, we see how entities and modifiers are related
to each other:
... Of course, it helps that Kaye has an
actor as talented as Norton to play this
part. It?s astonishing how frightening
Norton looks with a shaved head and a
swastika on his chest. ... Visually, the film
is very powerful. Kaye indulges in a lot of
interesting artistic choices, and most of
them work nicely.
Indeed, most of the information about an entity
that relates it to a particular polarity comes from the
modifying words. In the example above, these words
are adjectives such as talented, frightening, interest-
ing, and powerful. They can also be verbs such as
work and adverbs such as nicely. The entities are
1http://www.cs.cornell.edu/people/pabo/movie-review-
data/
96
represented by various nouns and pronouns such as:
Kaye, Norton, actor and them.
Therefore, the task of classifying a review doc-
ument can be explored by taking into account a
mixture of entities and their modifiers. An impor-
tant characteristic of review documents is that the
reviewers tend to discuss the whole set of entities
throughout the entire document, whereas the modi-
fiers for those entities tend to be more localized at
the sentence or phrase level. In other words, each
entity can be polymorphous within the document,
with a long-range semantic relationship between its
forms while the modifiers in each case are bound
to the entity in a short-range, syntactic relationship.
Generalizing a single entity to all the entities that are
found in a document, and taking all their respective
modifiers into account, we can start to infer the po-
larity of the entire document based on the set of all
the modifiers. This reduces to finding all the syn-
tactic words in the document and disregarding the
entities.
Taking another look at the example modifiers, we
might assume that all of the relevant indicators for
SA come from specific parts of speech categories
such as adjectives and adverbs, while other parts
of speech classes such as nouns are more relevant
for general text classification, and can be discarded.
However, as demonstrated by Pang et al (2002),
Pang and Lee (2004), Hu and Liu (2004), and Riloff
et al (2003), there are some nouns and verbs that
are useful sentiment indicators as well. Therefore,
a clear distinction cannot be made along parts of
speech categories.
To address this issue, we propose a feature selec-
tion scheme in which we can obtain important senti-
ment indicators that:
1. Do not rely on specific parts of speech classes
while maintaining the focus on syntax words.
2. Separate semantic words that do not indicate
sentiment while keeping nouns that do.
3. Reflect the domain for the set of documents.
By using feature selection schemes that focus on
the outlined sentiment indicators as a basis for our
machine learning approach, we should achieve com-
petitive accuracy results when classifying document
polarities.
The rest of this paper is organized as follows. Sec-
tion 2 discusses some important work and results
for SA and outlines the modelling and classification
techniques used by our approach. Section 3 provides
details about our feature selection methods. Our ex-
periments and analyses are given in section 4, and
conclusions and future directions are presented in
section 5.
2 Related Work
2.1 Feature Selection in Sentiment Analysis
The majority of the approaches for SA involve a
two-step process:
1. Identify the parts of the document that will
likely contribute to positive or negative senti-
ments.
2. Combine these parts of the document in ways
that increase the odds of the document falling
into one of these two polar categories.
The simplest approach for (1) by Pang et al
(2002) is to use the most frequently-occurring words
in the corpus as polarity indicators. This approach
is commonly used with general text classification,
and the results achieved indicate that simple docu-
ment frequency cutoffs can be an effective feature
selection scheme. However, this scheme picks up
on many entity words that do not contain any sub-
jectivity.
The most common approach, used by researchers
such as Das and Chen (2007), starts with a manu-
ally created lexicon specific to their particular do-
main whereas others (Hurst and Nigam, 2004; Yi et
al., 2003) attempt to craft a general-purpose opin-
ion lexicon that can be used across domains. More
recent lexicon-based approaches (Ding et al, 2008;
Hu and Liu, 2004; Kim and Hovy, 2004; Riloff et
al., 2003) begin with a small set of ?seed? words
and bootstrap this set through synonym detection
or various on-line resources to obtain a larger lex-
icon. However, lexicon-based approaches have sev-
eral key difficulties. First, they take time to com-
pile. Whitelaw et al (2005) report that their feature
selection process took 20 person-hours, since it in-
volves work done by human annotators. In separate
qualitative experiments done by Pang et al (2002),
97
Wilson et al (2005) and Kim and Hovy (2004), the
agreement between human judges when given a list
of sentiment-bearing words is as low as 58% and no
higher than 76%. In addition, some words may not
be frequent enough for a classification algorithm.
2.2 Topic Modelling and HMM-LDA
Topic models such as Latent Dirichlet Allocation
(LDA) are generative models that allow documents
to be explained by a set of unobserved (latent) top-
ics. Hidden Markov Model LDA (HMM-LDA)
(Griffiths et al, 2005) is a topic model that simul-
taneously models topics and syntactic structure in a
collection of documents. The idea behind the model
is that a typical word can play different roles. It can
either be part of the content and serve in a seman-
tic (topical) purpose or it can be used as part of the
grammatical (syntactic) structure. It can also be used
in both contexts. HMM-LDA models this behavior
by inducing syntactic classes for each word based
on how they appear together in a sentence using a
Hidden Markov Model. Each word gets assigned to
a syntactic class, but one class is reserved for the se-
mantic words. Words in this class behave as they
would in a regular LDA topic model, participating
in different topics and having certain probabilities of
appearing in a document. More formally, the model
is defined in terms of three sets of variables and a
generative process. Let w = {w1, ..., wn} be a se-
quence of words where each word wi is one of V
words; z = {z1, ..., zn}, a sequence of topic as-
signments where each zi is one of K topics; and
c = {c1, ..., cn}, a sequence of class assignments
where each ci is one of C classes. One class, ci = 1
is designated as the ?semantic class?, and the rest,
the ?syntactic? classes.
Since we are dealing with a Hidden Markov
Model, we require a variable representing the tran-
sition probabilities between the classes, given by a
C ? C transition matrix ? that models transitions
between classes ci?1 and ci. The generative process
is described as follows:
1. Sample ?(d) from a Dirichlet prior Dir(?)
2. For each word wi in document d:
(a) Draw zi ? ?(d)
(b) Draw ci ? ?(ci?1)
(c) If ci = 1, then draw wi ? ?(zi), else draw
wi ? ?(ci)
where ?(zi) ? Dir(?) and ?(ci) ? Dir(?), both
from Dirichlet distributions.
2.3 Text Classification Based on Maximum
Entropy Modelling
Maximum Entropy Modelling (Manning and
Schu?tze, 1999) is a framework whereby the features
represent constraints on the overall model and the
idea is to incorporate the knowledge that we have
while preserving as much uncertainty as possible
about the knowledge we do not have. The features
fi are binary functions where there is a vector x
representing input elements (unigram features in our
case) and c, the class label for one of the possible
categories. More specifically, a feature function is
defined as follows:
fi,c?(x, c) =
{
1 if x contains wi and c = c?
0 otherwise
(2.1)
where word wi and category c? correspond to a spe-
cific feature.
Employing the feature functions described above,
a Maximum Entropy model takes the following
form:
P (x, c) = 1Z
K
?
i=1
?fi(x,c)i (2.2)
where K is the number of features, ?i is the weight
for feature fi, and Z is a normalizing constant. By
taking the logarithm on both sides, we get the log-
linear model:
logP (x, c) = ? logZ +
K
?
i=1
fi(x, c) log?i (2.3)
To classify a document, we compute P (c|x) so
that the c with the highest probability will be the cat-
egory for the given document.
98
3 Feature Selection (FS) Based on
HMM-LDA
3.1 Characteristics of Salient Features
To motivate our approach, we first describe criteria
that are useful in selecting salient features for SA:
1. Features should be expressive enough to add
useful information to the classification process.
As discussed in section 1, the most expressive
features in terms of polarity are the modifying
words that describe an entity in a certain way.
These are usually, but not restricted to, adjec-
tives, adverbs, subjective verbs and nouns.
2. All features together should form a broad and
comprehensive viewpoint of the entire corpus.
In a corpus of many documents, some features
can represent a subset of the corpus very accu-
rately, while other features may represent an-
other subset of the corpus. The problem arises
when representing the whole corpus with a spe-
cific feature set (Sebastiani, 2002).
3. Features should be as domain-dependent as
possible. Examples from Hurst and Nigam
(2004) and Das and Chen (2007) as well as
many other approaches indicate that SA is a
domain-dependant task, and the final features
should reflect the domain of the corpus that
they are representing.
4. Features must be frequent enough. Rare fea-
tures do not occur in many documents and
make it difficult to train a machine learning al-
gorithm. Experiments by Pang et al (2002) in-
dicate that having more features does not help
learning, and the best accuracy was achieved
by selecting features based on document fre-
quency.
5. Features should be discriminative enough. A
learning system needs to be able to pick up on
their presence in certain documents for one out-
come and absence in other documents for an-
other outcome in classification.
3.2 FS Based on Syntactic Classes
Our proposed FS scheme is to utilize HMM-LDA
to obtain words that, for the most part, follow the
criteria we set out in subsection 3.1. We train an
HMM-LDA model to give us the syntactic classes
that we further combine to form our final features.
Let word wi ? V where V is the vocabulary. Also
let cj ? C be a class. We define Pcj (wi) as the prob-
ability of word wi in class cj , and one class, cj = 1
indicates the semantic class. Since each class (syn-
tactic and semantic) has a probability distribution
over all words, we need to select words that offer
a good representation of the class. The representa-
tive words in each class have a much higher proba-
bility than the other words. Therefore, we can select
the representative words by the cumulative probabil-
ity. Specifically, we select the top percentage of the
words in a class whereby the sum of their probabil-
ities will be within some pre-defined range. This is
necessary since there are many words in each class
with low probabilities in which we are not interested
(Steyvers and Griffiths, 2006). The cumulative dis-
tribution function is defined as:
Fj(wi) =
?
Pcj (w)?Pcj (wi)
Pcj (w) (3.1)
Then, we can define the set of words in class cj as:
Wcj = {wi|Fj(wi) ? ?} (3.2)
where ? is a pre-defined threshold such that 0 ? ? ?
1. Next, we define the set of words in all the syntac-
tic classes Wsyn as:
Wsyn = {wi|wi ? Wcj and cj 6= 1} (3.3)
and the set of words in the semantic class Wsem as:
Wsem = {wi|wi ? Wcj and cj = 1} (3.4)
Since modifying words for sentiment typically
fall into syntactic classes, we could use words in
Wsyn as features for SA. However, as observed by
Pang et al (2002), the best classification perfor-
mance is achieved by a subset of features (typically
around 2500). As a general step, we can apply a
document frequency (DF) cutoff to select the most
frequent features. Let df(wi) denote the document
frequency of word wi, indicating the number of doc-
uments in which wi occurs in the corpus. Then the
99
resulting features selected based on df can be de-
fined as:
cut(Wsyn, ?) = {wi|wi ? Wsyn and df(wi) ? ?}
(3.5)
where ? is the minimum document frequency re-
quired for feature selection.
3.3 FS Based on Set Difference between
Syntactic and Semantic Classes
The main characteristic of using HMM-LDA classes
for feature selection is that the set of words in the
syntactic classes and the set of words in the semantic
class are not disjoint. In fact, there is quite a large
overlap. In this and the next subsections, we dis-
cuss ways to remedy and even exploit this situation
to get a higher level of accuracy. In the Pang et al
movie review data, there is about 35% overlap be-
tween words in the syntactic and semantic classes
for ? = 0.9. Our first systematic approach attempts
to gain better accuracy by lowering the ratio of se-
mantic words in the final feature set.
More formally, given the set of syntactic words
Wsyn, we can reduce the overlap with Wsem by do-
ing a set difference operation:
Wsyn ?Wsem (3.6)
This will give us all the words that are more
favoured in the syntactic classes. However, as we
shall see shortly, and also as we earlier speculated,
by subtracting all the words in the semantic class, we
are actually getting rid of some useful features. This
is because (a) it is possible for the semantic class
to contain words that are syntactic, and as a result
are useful, and (b) there exist some semantic words
that are good indicators of polarity. Therefore, we
seek to ?lessen? the influence of the semantic class
by cutting only a certain portion of it out, but not all
of them.
For the above scheme, we outline Algorithm 1
that enables us to select features from Wsyn by ap-
plying a percentage cutoff for Wsem and then doing
a set difference operation. We define top(Wsem, ?)
to be the ?% of the words with top probabilities in
Wsem.
Note that when ? = 1.0, we get the same result as
Wsyn ? Wsem. In our experiments, we try a range
of ? values for SA.
Algorithm 1 Syntactic-Semantic Set Difference
Require: Wsyn and Wsem as input
1: W ?sem = top(Wsem, ?)
2: Wdiff = Wsyn ?W ?sem
3: W ?syn = cut(Wdiff , ?)
3.4 FS Based on Max Scores of Syntactic
Features
The running theme through the HMM-LDA feature
selection schemes is that if a word is highly ranked
(has a high probability of occurring) in a syntactic
class, we should use that word in our feature set.
Moreover, if a word is highly ranked in the seman-
tic class, we usually do not want to use that word
in our feature set because the word usually indicates
a frequent noun. Therefore, the desirable words are
those that occur with high probability in the syntac-
tic classes, but do not occur with high probability in
the semantic class, or do not occur there at all.
To this end, we have formulated a scheme that
adds such words to our feature set. For each word,
we obtain its highest probability in the set of syn-
tactic classes. Comparing this probability with the
probability of the same word in the semantic class,
we disregard the word if the probability in the se-
mantic class is greater.
We define the max scores for word wi for both the
syntactic and semantic classes and describe how we
select features based on the max scores in Algorithm
2.
Algorithm 2 Max Scores of Syntactic Features
Require: cj ? C where 1 ? j ? |C|
1: for all wi ? V do
2: Ssyn(wi) = maxcj 6=1Pcj (wi)
3: Ssem(wi) = Pc1(wi)
4: Wmax = {wi|Ssyn(wi) > Ssem(wi)}
5: end for
6: W ?syn = cut(Wmax, ?)
4 Experiments
This section describes the steps taken to gener-
ate some experimental results for each scheme de-
scribed in the previous section. Before we can an-
alyze these sets of results, we take a look at some
100
baselines.
4.1 Evaluation
We use the corpus of 2000 movie reviews (Pang and
Lee, 2004) that consists of 1000 positive and 1000
negative documents selected from on-line forums.
In our experiments, we randomize the documents
and split the data into 1800 for training / testing pur-
poses and 200 as the validation set. For the 1800
documents, we run a 3-fold cross validation proce-
dure where we train on 1200 documents and test on
600. We compare the resultant feature sets after each
FS scheme using the OpenNLP2 Maximum Entropy
classifier.
Throughout these experiments, we are interested
in the classification accuracy. This is evaluated
simply by comparing the resultant class from the
classifier and the actual class annotated by Pang
and Lee (2004). The number of matches is di-
vided by the number of documents in the test
set. Thus, given an annotated test set dtestA =
{(d1, o1), (d2, o2), . . . (dS , oS)} and the classified
set, dtestB = {(d1, q1), (d2, q2), . . . (dS , qS)}, we
calculate the accuracy as follows:
?S
i=1 I(oi = qi)
S (4.1)
where I(?) is the indicator function.
4.2 Baseline Results
After replicating the results from Pang et al (2002),
we varied the number of iterations per fold by using
a held-out validation set ?eval?. The higher accu-
racy achieved suggests that the model was not fully
trained after 10 iterations.
In order to compare with our HMM-LDA based
schemes, we ran experiments to explore a basic
POS-based feature selection scheme. In this ap-
proach, we first tagged the words in each document
with POS tags and selected the most frequently-
occurring unigrams that were not tagged as ?NN?,
?NNP?, ?NNS? or ?NNPS? (the ?noun? categories).
This corresponds to POS (-NN*) in Table 1. Next,
we tagged all the words and only selected the words
that were tagged as ?JJ*?, ?RB*?, and ?VB*? cate-
gories (the ?syntactic? categories). The idea is to
2http://incubator.apache.org/opennlp/
include as part of the feature set al the words that
are not ?semantically oriented?. This corresponds to
POS (JJ* + RB* + VB*) in Table 1.
Iterations DF
cutoff
POS
(-NN*)
POS
(JJ*+RB*
+VB*)
10 0.821 0.827 0.811
25 0.836 0.831 0.824
eval 0.845 0.848 0.826
Table 1: Baseline results with a different number of iter-
ations. Each column represents a different feature selec-
tion method.
4.3 HMM-LDA Training
Our feature selection methods involve training an
HMM-LDA model on the Pang et al corpus of
movie reviews, taking the class assignments, and
combining the resultant unigrams to create features
for the MaxEnt classifier. Since HMM-LDA is an
unsupervised topic model, we can train it on the en-
tire corpus. We trained the model using the Topic
Modelling Toolbox3 MATLAB package on the 2000
movie reviews. Since the HMM-LDA model re-
quires sentences to be outlined, we used the usual
end-of-sentence markers (?.?, ?!?, ???, ?:?). The train-
ing parameters are T = 50 topics, S = 20 classes, AL-
PHA = 1.0, BETA = 0.01, and GAMMA = 0.1. We
found that 1000 iterations is sufficient as we tracked
the log-likelihood of every 10 iterations.
After training, we have both the topic assignments
z and the class assignments c for each word in each
of the samples.
4.4 Selecting Features Based on Syntactic
Classes
In this experiment we fix ? = 0.9 to get the top
words in each class having a cumulative probabil-
ity under 0.9. These are the representative words
in each class which we merge into Wsyn. Finally,
we select 2500 words by the df cutoff method. This
list of words is then used as features for the Max-
Ent classifier. We run the classifier for 10, 25 and
?eval? number of iterations in order to compare with
the baseline results.
3http://psiexp.ss.uci.edu/research/programs data/toolbox.htm
101
Iterations FS Based on
Syntactic Features
10 0.823
25 0.839
eval 0.863
Table 2: Results for FS Based on Syntactic Classes at 10,
25 and ?eval? iterations.
At ? = 0.9, there are 6,189 words in Wsyn before
we select the top 2500 using the df cutoff. From
Table 2, we see that the accuracy has increased from
0.845 to 0.863 at the ?eval? number iterations.
In all of our experiments, we use df cutoff to
get a manageable number of features for the clas-
sifier. This is partly based on Pang et al (2002)
and partly based on calculating the Pearson correla-
tion for each class between the document frequency
and word probability at ? = 0.9. Since every class
has a positive correlation in the range of [0.313938,
0.888160] where the average is 0.576, we can say
that there is a correlation between the two values.
4.5 Selecting Features Based on Set Difference
The result for Set Difference is derived by varying
the percentage of top semantic words that should be
excluded in the final feature set. For example, some
words in Wsyn?Wsem that have a higher probability
in Wsem are: ?hollywod?, ?war?, and ?fiction? while
some words that have a higher probability in Wsyn
include: ?good?, ?love? and ?funny?. The ? value is
defined by the percentage of the words in Wsem that
we exclude from Wsyn. The results for 0.0 ? ? ?
1.0 for increments of ??|Wsem|, are summarized in
Table 3.
? FS Based on
Set Difference
? FS Based on
Set Difference
0.0 0.861 0.5 0.852
0.1 0.862 0.6 0.846
0.2 0.865 0.7 0.849
0.3 0.858 0.8 0.847
0.4 0.857 0.9 0.840
1.0 0.831
Table 3: Results for FS Based on Syntactic-Semantic
set difference method. Each row represents the accuracy
achieved at a particular ? value.
From the results, we can see that as we remove
more and more words from Wsem, the accuracy level
decreases. This suggests that Wsem?Wsyn contains
some important features and if we subtract Wsem en-
tirely, we essentially eliminate them. At each cutoff
level, we are eliminating 10% until we have elimi-
nated the whole set. Clearly, a more fine-grained ap-
proach is needed, and that leads us to the Max-Score
results.
4.6 Selecting Features Based on Max Scores
For the method based on Max Scores, we may select
features that are in both Wsem and Wsyn sets as long
as their max scores in Wsyn are higher than those in
Wsem.
Iterations FS Based on
Max Scores
eval 0.875
Table 4: Result for FS Based on Max Scores.
Comparing the accuracy in Table 4 with those
in the previous subsections, we can say that using
the fine-grained Max-Score algorithm improves the
classification accuracy. This means that iteratively
removing words that have a relatively higher prob-
ability in Wsem compared to Wsyn does not elim-
inate important words occurring in both sets, but
lessens the influence of some high probability words
in Wsem.
4.7 Discussion of the Results
For our experiments, the best accuracy is achieved
by utilizing the Max-Score algorithm (outlined in
subsection 3.4) after a further selection of 2500 with
the df cutoff. As discussed in subsection 3.4, the
Max-Score algorithm enables us to select words that
have a higher score in Wsyn than in Wsem. This ap-
proach has the dual advantage of keeping the words
that are present in both Wsyn and Wsem but have
higher scores in Wsyn and ignoring the words that
are also present in both sets but have higher scores
in Wsem. Ultimately, this decreases the influence of
the frequent and overlapped words that have a high
probability in Wsem.
Finally, to quantify the significance level of our
best approach against the baseline methods in sub-
102
section 4.2, we calculated the p-values for the one-
tailed t-tests comparing our best approach based on
Max Scores with the DF and POS (-NN*) baselines,
respectively. The resulting p-values of 0.011 and
0.014 suggest that our best approach is significantly
better than the baseline approaches.
5 Conclusions and Future Directions
In this paper, we have described a method for fea-
ture selection based on long-range and short-range
dependencies given by the HMM-LDA topic model.
By modelling review documents based on the com-
binations of syntactic and semantic classes, we have
devised a method of separating the topical con-
tent that describes the entities under review from
the opinion context (given by sentiment modifiers)
about that entity in each case. By grouping all the
sentiment modifiers for each entity in a document,
we are selecting the features that are intuitively in
line with the outlined characteristics of salient fea-
tures for SA (see subsection 3.1). This is backed up
by our experiments where we achieve competitive
results for document polarity classification.
One avenue for future development of this frame-
work could include identifying and extracting as-
pects from a review document. So far, we have not
identified aspects from the entities, choosing instead
to classify a document as a whole. However, this
framework can be readily applied to extract relevant
(most probable) aspects using the LDA topic model
and then restrict the syntactic modifiers to the range
of sentences where an aspect occurs. This would
give us an unsupervised aspect extraction scheme
that we can combine with a classifier to predict po-
larities for each aspect.
References
Sanjiv R. Das and Mike Y. Chen. 2007. Yahoo! for
Amazon: Sentiment extraction from small talk on the
Web. Management Science, 53(9):1375?1388.
Xiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A
holistic lexicon-based approach to opinion mining. In
Proceedings of the Conference on Web Search and Web
Data Mining (WSDM).
Thomas L. Griffiths, Mark Steyvers, David M. Blei, and
Joshua B. Tenenbaum. 2005. Integrating topics and
syntax. In In Advances in Neural Information Pro-
cessing Systems 17, pages 537?544. MIT Press.
Minqing Hu and Bing Liu. 2004. Mining opinion fea-
tures in customer reviews. In Proceedings of AAAI,
pages 755?760.
Matthew Hurst and Kamal Nigam. 2004. Retrieving top-
ical sentiments from online document collections. In
Document Recognition and Retrieval XI, pages 27?34.
Soo-Min Kim and Eduard Hovy. 2004. Determining
the sentiment of opinions. In Proceedings of the In-
ternational Conference on Computational Linguistics
(COLING).
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of statistical natural language process-
ing. MIT Press, Cambridge, MA, USA.
Bo Pang and Lillian Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of the As-
sociation for Computational Linguistics (ACL), pages
271?278.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using ma-
chine learning techniques. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 79?86.
Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003.
Learning subjective nouns using extraction pattern
bootstrapping. In Proceedings of the Conference on
Natural Language Learning (CoNLL), pages 25?32.
Fabrizio Sebastiani. 2002. Machine learning in auto-
mated text categorization. ACM Computing Surveys,
34(1):1?47.
Mark Steyvers and Tom Griffiths. 2006. Probabilistic
topic models. In T. Landauer, D. Mcnamara, S. Den-
nis, and W. Kintsch, editors, Latent Semantic Analysis:
A Road to Meaning. Laurence Erlbaum.
Casey Whitelaw, Navendu Garg, and Shlomo Argamon.
2005. Using appraisal groups for sentiment analy-
sis. In Proceedings of the ACM SIGIR Conference
on Information and Knowledge Management (CIKM),
pages 625?631. ACM.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the Human Lan-
guage Technology Conference and the Conference on
Empirical Methods in Natural Language Processing
(HLT/EMNLP), pages 347?354.
Jeonghee Yi, Tetsuya Nasukawa, Razvan Bunescu, and
Wayne Niblack. 2003. Sentiment analyzer: Ex-
tracting sentiments about a given topic using natural
language processing techniques. In Proceedings of
the IEEE International Conference on Data Mining
(ICDM).
103
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 1?9,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Unsupervised Part-of-Speech Tagging in Noisy and Esoteric Domains
with a Syntactic-Semantic Bayesian HMM
William M. Darling
School of Computer Science
University of Guelph
wdarling@uoguelph.ca
Michael J. Paul
Dept. of Computer Science
Johns Hopkins University
mpaul@cs.jhu.edu
Fei Song
School of Computer Science
University of Guelph
fsong@uoguelph.ca
Abstract
Unsupervised part-of-speech (POS) tag-
ging has recently been shown to greatly
benefit from Bayesian approaches where
HMM parameters are integrated out, lead-
ing to significant increases in tagging ac-
curacy. These improvements in unsuper-
vised methods are important especially in
specialized social media domains such as
Twitter where little training data is avail-
able. Here, we take the Bayesian approach
one step further by integrating semantic in-
formation from an LDA-like topic model
with an HMM. Specifically, we present
Part-of-Speech LDA (POSLDA), a syntac-
tically and semantically consistent genera-
tive probabilistic model. This model dis-
covers POS specific topics from an unla-
belled corpus. We show that this model
consistently achieves improvements in un-
supervised POS tagging and language mod-
eling over the Bayesian HMM approach
with varying amounts of side information
in the noisy and esoteric domain of Twitter.
1 Introduction
The explosion of social media in recent years has
led to the need for NLP tools like part-of-speech
(POS) taggers that are robust enough to handle
data that is becoming increasingly ?noisy.? Unfor-
tunately, many NLP systems fail at out-of-domain
data and struggle with the informal style of social
text. With spelling errors, abbreviations, uncom-
mon acronyms, and excessive use of slang, sys-
tems that are designed for traditional corpora such
as news articles may perform poorly when given
difficult input such as a Twitter feed (Ritter et al,
2010).
Recognizing the limitations of existing sys-
tems, Gimpel et al (2011) develop a POS tagger
specifically for Twitter, by creating a training cor-
pus as well as devising a tag set that includes parts
of speech that are uniquely found in online lan-
guage, such as emoticons (smilies). This is an im-
portant step forward, but a POS tagger tailored to
Twitter cannot tackle the social Web as a whole.
Other online communities have their own styles,
slang, memes, and other idiosyncrasies, so a sys-
tem trained for one community may not apply to
others.
For example, the 140-character limit of Twit-
ter encourages abbreviations and word-dropping
that may not be found in less restrictive venues.
The first-person subject is often assumed in ?sta-
tus messages? that one finds in Twitter and Face-
book, so the pronominal subject can be dropped,
even in English (Weir, 2012), leading to messages
like ?Went out? instead of ?I went out.? Not
only does Twitter follow these unusual grammat-
ical patterns, but many messages contain ?hash-
tags? which could be considered their own syn-
tactic class not found in other data sources. For
these reasons, POS parameters learned from Twit-
ter data will not necessarily fit other social data.
In general, concerns about the limitations of
domain-dependent models have motivated the use
of sophisticated unsupervised methods. Inter-
est in unsupervised POS induction has been re-
vived in recent years after Bayesian HMMs are
shown to increase accuracy by up to 14 percent-
age points over basic maximum-likelihood esti-
mation (Goldwater and Griffiths, 2007). Despite
falling well short of the accuracy obtained with
supervised taggers, unsupervised approaches are
preferred in situations where there is no access to
1
large quantities of training data in a specific do-
main, which is increasingly common with Web
data. We therefore hope to continue improving
accuracy with unsupervised approaches by intro-
ducing semantics as an additional source of infor-
mation for this task.
The ambiguities of language are amplified
through social media, where new words or
spellings of words are routinely invented. For ex-
ample, ?ow? on Twitter can be a shorthand for
?how,? in addition to its more traditional use as
an expression of pain (ouch). While POS assign-
ment is inherently a problem of syntactic disam-
biguation, we hypothesize that the underlying se-
mantic content can aid the disambiguation task.
If we know that the overall content of a message
is about police, then the word ?cop? is likely to
be a noun, whereas if the context is about shop-
ping, this could be slang for acquiring or stealing
(verb). The HMM approach will often be able to
tag these occurrences appropriately given the con-
text, but in many cases the syntactic context may
be limited or misleading due to the noisy nature
of the data. Thus, we believe that semantic con-
text will offer additional evidence toward making
an accurate prediction.
Following this intuition, this paper presents a
semantically and syntactically coherent Bayesian
model that uncovers POS-specific sub-topics
within general semantic topics, as in latent Dirich-
let alocation (LDA) (Blei et al, 2003), which we
call part-of-speech LDA, or POSLDA. The re-
sulting posterior distributions will reflect special-
ized topics such as ?verbs about dining? or ?nouns
about politics?. To the best of our knowledge, we
also present the first experiments with unsuper-
vised tagging for a social media corpus. In this
work, we focus on Twitter because the labeled
corpus by Gimpel et al (2011) allows us to quan-
titatively evaluate our approach. We demonstrate
the model?s utility as a predictive language model
by its low perplexity on held-out test data as com-
pared to several related topic models, and most
importantly, we show that this model achieves
statistically significant and consistent improve-
ments in unsupervised POS tagging accuracy over
a Bayesian HMM. These results support our hy-
pothesis that semantic information can directly
improve the quality of POS induction, and our ex-
periments present an in-depth exploration of this
task on informal social text.
The next section discusses related work, which
is followed by a description of our model,
POSLDA. We then present POS tagging results
on the Twitter POS dataset (Gimpel et al, 2011).
Section 5 describes further experiments on the
POSLDA model and section 6 includes a discus-
sion on the results and why POSLDA can do bet-
ter on POS tagging than a vanilla Bayesian HMM.
Finally, section 7 concludes with a discussion on
future work.
2 Related Work
Modern unsupervised POS tagging originates
with Merialdo (1993) who trained a trigram
HMM using maximum likelihood estimation
(MLE). Goldwater and Griffiths (2007) improved
upon this approach by treating the HMM in a
Bayesian sense; the rows of the transition matrix
are random variables with proper Bayesian priors
and the state emission probabilities are also ran-
dom variables with their own priors. The posterior
distribution of tags is learned using Gibbs sam-
pling and this model improves in accuracy over
the MLE approach by up to 14 percentage points.
In the ?Topics and Syntax? model (or
HMMLDA), the generative process of a corpus
is cast as a composite model where syntax is
modeled with an HMM and semantics are mod-
eled with LDA (Griffiths et al, 2005). Here, one
state of an HMM is replaced with a topic model
such that the words with long-range dependen-
cies (?content? words) will be drawn from a set
of topics. The remaining states are reserved for
?syntax? words that exhibit only short-range de-
pendencies. Griffiths et al (2005) briefly touch
on POS tagging with their model, but its supe-
riority to a plain Bayesian HMM is not shown
and the authors note that this is partially because
all semantic-like words get assigned to the sin-
gle semantic class in their model. This misses the
distinction between at least nouns and verbs, but
many other semantic-dependent words as well. If
more variation could be provided in the seman-
tic portion of the model, the POS tagging results
would likely improve.
3 Part-of-Speech LDA (POSLDA)
In their canonical form, topic models do not cap-
ture local dependencies between words (i.e. syn-
tactic relations), but they do capture long-range
2
K*S
CON
 + S
FUN
S
M
??
w
1
z
1
c
1
?
?
? ?
w
2
z
2
c
2
w
3
z
3
c
3
...
...
...
Figure 1: Graphical model depiction of POSLDA.
context such as the overall topical content or gist
of a document. Conversely, under an HMM,
words are assumed completely independent of
their broader context by the Markov assumption.
We seek to bridge these restrictions with our uni-
fied model, Part-of-Speech LDA (POSLDA).
Under this model, each word token is now asso-
ciated with two latent variables: a semantic topic
z and a syntactic class c. We posit that the top-
ics are generated through the LDA process, while
the classes are generated through an HMM. The
observed word tokens are then dependent on both
the topic and the class: rather than a single multi-
nomial for a particular topic z or a particular class
c, there are distributions for each topic-class pair
(z, c) from which we assume words are sampled.
We denote the set of classes C = CCON ? CFUN,
which includes the set of content or ?semantic?
classes CCON for word types such as nouns and
verbs that depend on the current topic, and func-
tional or ?syntactic-only? classes CFUN. If a word
is generated from a functional class, it does not
depend on the topic. This allows our model to
accommodate functional words like determiners
which appear independently of the topical content
of a document.
We use the same notation as LDA, where ? is a
document-topic distribution and ? is a topic-word
distribution. Additionally, we denote the HMM
transition rows as pi, which we assume is drawn
from a Dirichlet with hyperparameter ?. Denote
S = |C| and K = |Z|, the numbers of classes
and topics, respectively. There are SFUN word
distributions ?(FUN) for function word classes and
K ? SCON word distributions ?(CON) for content
word classes. A graphical model depiction of
POSLDA is shown in Figure 1.
Thus, the generative process of a corpus can be
described as:
1. Draw pi ? Dirichlet(?)
2. Draw ? ? Dirichlet(?)
3. For each document d ? D:
(a) Draw ?d ? Dirichlet(?)
(b) For each word token wi ? d:
i. Draw ci ? pici?1
ii. If ci /? CCON:
A. Draw wi ? ?
(FUN)
ci
iii. Else:
A. Draw zi ? ?d
B. Draw wi ? ?
(CON)
ci,zi
In topic models, it is generally true that com-
mon function words may overwhelm the word
distributions, leading to suboptimal results that
are difficult to interpret. This is usually accom-
modated by data pre-processing (e.g. stop word
removal), by backing off to ?background? word
models (Chemudugunta et al, 2006), or by per-
forming term re-weighting (Wilson and Chew,
2010). In the case of POSLDA, these common
words are naturally captured by the functional
classes.
3.1 Relations to Other Models
The idea of having multinomials for the cross
products of topics and classes is related to multi-
faceted topic models where word tokens are as-
sociated with multiple latent variables (Paul and
Girju, 2010; Ahmed and Xing, 2010). Under such
models, words can be explained by a latent topic
as well as a second underlying variable such as
the perspective or dialect of the author, and words
may depend on both factors. In our case, the sec-
ond variable is the part-of-speech ? or functional
purpose ? of the token.
We note that POSLDA is a generalization of
many existing models. POSLDA becomes a
Bayesian HMM when the number of topics K =
1; the original LDA model when the number of
3
classes S = 1; and the HMMLDA model of Grif-
fiths et al (2005) when the number of content
word classes SCON = 1. The beauty of these gen-
eralizations is that one can easily experiment with
any of these models by simply altering the model
parameters under a single POSLDA implementa-
tion.
3.2 Inference
As with many complex probabilistic models, ex-
act posterior inference is intractable for POSLDA.
Nevertheless, a number of approximate inference
techniques are at our disposal. In this work, we
use collapsed Gibbs sampling to sample the latent
class assignments and topic assignments (c and
z), and from these we can compute estimates of
the multinomial parameters for the topics (?), the
document-topic portions (?), and the HMM tran-
sition matrix (pi). Under a trigram version of the
model ? which we employ for all our experiments
in this work ? the sampling equation for word to-
ken i is as follows:
p(ci, zi|c?i, z?i,w) ?
?
??
??
?ci ?
n(d)zi +?zi
n(d). +?.
n
(ci,zi)
w +?
n(ci,zi). +W?
ci ? SCON
?ci ?
n
(ci)
w +?
n(ci). +W?
ci ? SFUN
where
?ci =
n(ci?2,ci?1,ci)+?ci
n(ci?2,ci?1)+?.
?
n(ci?1,ci,ci+1)+?ci
n(ci?1,ci)+?.
?
n(ci,ci+1,ci+2)+?ci
n(ci,ci+1)+?.
Although we sample the pair (ci, zi) jointly as a
block, which requires computing a sampling dis-
tribution over SFUN +K ?SCON, it is also valid to
sample ci and zi separately, which requires only
S + K computations. In this case, the sampling
procedure would be somewhat different. Despite
the lower number of computations per iteration,
however, the sampler is likely to converge faster
with our blocked approach because the two vari-
ables are tightly coupled. The intuition is that a
non-block-based sampler could have difficulty es-
caping local optima because we are interested in
the most probable pair; a highly probable class
c sampled on its own, for example, could pre-
vent the sampler from choosing a more likely pair
(c?, z).
4 POS Tagging Experiments
To demonstrate the veracity of our approach, we
performed a number of POS tagging experiments
using the POSLDA model. Our data is the re-
cent Twitter POS dataset released at ACL 2011 by
Gimpel et al (2011) consisting of approximately
26,000 words across 1,827 tweets. This dataset
provides a unique opportunity to test our unsuper-
vised approach in a domain where it would likely
be of most use ? one that is novel and therefore
lacking large amounts of training data. We feel
that this sort of specialized domain will become
the norm ? particularly in social media analysis
? as user generated content continues to grow in
size and accessibility. The Twitter dataset uses a
domain-dependent tag set of 25 tags that are de-
scribed in (Gimpel et al, 2011).
For our experiments, we follow the established
form of Merialdo (1993) and Goldwater and Grif-
fiths (2007) for unsupervised POS tagging by
making use of a tag dictionary to constrain the
possible tag choices for each word and there-
fore render the problem closer to disambiguation.
Like Goldwater and Griffiths (2007), we employ
a number of dictionaries with varying degrees of
knowledge.
We use the full corpus of tweets1 and construct
a tag dictionary which contains the tag informa-
tion for a word only when it appears more than d
times in the corpus. We ran experiments for d =
1, 2, 3, 5, 10, and ? where the problem becomes
POS clustering. We report both tagging accu-
racy and the variation of information (VI), which
computes the information lost in moving from
one clustering C to another C ?: V I(C,C ?) =
H(C) +H(C ?)? 2I(C,C ?) (Meila?, 2007). This
can be interpreted as a measure of similarity be-
tween the clusterings, where a smaller value indi-
cates higher similarity.
We run our Gibbs sampler for 20,000 iterations
and obtain a maximum a posteriori (MAP) esti-
mate for each word?s tag by employing simulated
annealing. Each posterior probability p(c, z|?) in
the sampling distribution is raised to the power of
1
? where ? is a temperature that approaches 0 as
the sampler converges. This approach is akin to
1The Twitter POS dataset consists of three subsets of
tweets: development, training, and testing. Because we are
performing fully unsupervised tagging, however, we com-
bine these three subsets into one.
4
Accuracy 1 2 3 5 10 ?
random 62.8 49.6 45.2 40.2 35.0
BHMM 78.4 65.4 59.0 51.8 44.0
POSLDA 80.9 67.5 62.0 55.9 47.6
VI
random 2.34 3.31 3.56 3.81 4.05 5.86
BHMM 1.41 2.47 2.84 3.22 3.61 5.07
POSLDA 1.30 2.34 2.66 2.98 3.35 4.96
Corpus stats
% ambig. 54.2 67.9 72.2 76.4 80.4 100
tags / token 2.62 5.91 7.19 8.59 10.3 25
Table 1: POS tagging results on Twitter dataset.
bringing a system from an arbitrary state to one
with the lowest energy, thus viewing the Gibbs
sampling procedure as a random search whose
goal is to identify the MAP tag sequence ? a tech-
nique that is also employed by Goldwater and
Griffiths (2007). Finally, we run each experiment
5 times from random initializations and report the
average accuracy and variation of information.
4.1 Results for Twitter Dataset
In our experiments, we use 8 content classes
that correspond to the following parts-of-speech:
noun, proper noun, proper noun + possessive,
proper noun + verbal, verb, adjective, adverb, and
other abbreviations / foreign words. We chose
these classes because intuitively they are the types
of words whose generative probability will de-
pend on the given latent topic. As the Twitter POS
data consists of 25 distinct tags, this leaves 17 re-
maining classes for function words. In this sec-
tion, we report results for K = 10 topics. We
will discuss the effect of varyingK in section 4.2.
We set symmetric priors with ? = 1.0/K = 0.1,
? = 0.5, and ? = 0.01.
As is demonstrated in Table 1, our POSLDA
model shows marked improvements over a ran-
dom tag assignment and, more importantly, the
Bayesian HMM approach described by Goldwa-
ter and Griffiths (2007). It does so for every set-
ting of d on both accuracy and variation of infor-
mation. For d = 1 our method outperforms the
BHMM by 2.5 percentage points. With higher
values of d, however, POSLDA increases its im-
provement over the BHMM to up to 4.1 percent-
age points. The increase in tagging accuracy as
d increases suggests that our method may be par-
ticularly suitable for domains with little training
K Accuracy ?
1 (HMM) 78.6 0.23
5 80.0 0.06
10 80.9 0.17
15 80.1 0.10
20 80.2 0.21
25 80.1 0.25
30 80.2 0.15
35 80.1 0.12
40 79.9 0.20
45 80.1 0.12
Table 2: POS tagging results as K varies on Twitter
dataset.
data.2 For d = ?, where we are performing
POS clustering, our model improves the variation
of information by 0.11. Each of these improve-
ments over the Bayesian HMM is statistically sig-
nificant with p  0.01. Despite the clear im-
provements in POS tagging accuracy and cluster-
ing that we demonstrate in this section, we trained
our POSLDA model with a ?blind? topic setting
of K = 10. In the following section, we will
investigate how this parameter affects the achiev-
able results with our technique.
4.2 Topic Variance
In the previous section we set the number of topics
a priori to K = 10. However, it is well known in
topic modeling research that different datasets ex-
hibit different numbers of ?inherent? topics (Blei
et al, 2003). Therefore, a POSLDA model fit with
the ?correct? number of topics will likely achieve
higher accuracy in POS tagging. A standard ap-
proach to tuning the number of topics to fit a topic
model is to try a number of different topics and
choose the one that results in the lowest perplexity
on a held-out test set (Claeskens and Hjort, 2008).
Here, we can choose the optimal K more directly
by trying a number of different values and choos-
ing the one that maximizes the POS tagging accu-
racy.
For this experiment, we again make use of the
Twitter POS dataset (Gimpel et al, 2011). We use
the same setup as that described above with sim-
ulated annealing, 20,000 iterations, and a tag dic-
2The differences in tagging accuracy in terms of per-
centage points between POSLDA and the BHMM for
d = {1, 2, 3, 5, 10} are ?a = {2.5, 2.1, 3.0, 4.1, 3.6},
respectively. For clustering, the increases in VI are
even more clear as d increases. They are ?V I =
{0.11, 0.13, 0.18, 0.24, 0.26}.
5
tionary with d = 1. As before, we set ? = 1.0/K,
? = 0.5, and ? = 0.01. We perform experiments
with K = {1, 5, 10, . . . , 40, 45}, where K = 1
corresponds to the Bayesian HMM. The results
averaged over 3 runs are tabulated in Table 2 with
the associated standard deviations (?), and shown
graphically in Figure 2.
topics
acc
ura
cy
78.5
79.0
79.5
80.0
80.5
81.0
l
l
l
l
l
l
l
l
l
l
10 20 30 40
Figure 2: Number of topics K vs. POS tagging ac-
curacy on the Twitter dataset. The average accuracies,
along with their standard errors, are shown in black,
while a smoothed curve of the same data is shown in
blue.
As we expect, the tagging accuracy depends on
the number of topics specified by the model. In
fact, the accuracy improves by nearly a full per-
centage point from both the previous and next
topic settings when we hit a critical point at
K = 10. When K = 1 the model reduces to
the Bayesian HMM and our accuracy suffers. It
steadily increases until we hit the critical point
and then drops off again but plateaus at a level
that is approximately 1.5 percentage points higher
than the BHMM. This shows that determining an
appropriate setting for the number of topics is es-
sential for the best possible tagging accuracy us-
ing POSLDA. Nevertheless, even with a ?blind?
setting within a large range of topic values (here
from K = 5 to at least K = 45), we see marked
improvements over the baseline system that does
not include any semantic topic information.
5 Model Evaluation
In this section we present further experiments
on the raw output of POSLDA to demonstrate
its capabilities beyond simply POS tagging. We
show the model?s ability both qualitatively and
quantitatively to capture the semantic (or ?con-
tent?) and syntactic (or ?functional?) axes of in-
formation prevalent in a corpus made up of social
media data. We begin qualitatively with topic in-
terpretability when the model is learned given a
collection of unannotated Twitter messages, and
then present quantitative results on the ability of
POSLDA as a predictive language model in the
Twitter domain.
5.1 Topic Interpretability
Judging the interpretability of a set of topics is
highly subjective, and there are understandably
various differing approaches of evaluating topic
cohesiveness. For example, Chang et al (2009)
look at ?word intrusion? where a user determines
an intruding word from a set of words that does
not thematically fit with the other words, and
?topic intrusion? where a user determines whether
the learned document-topic portion ?d appropri-
ately describes the semantic theme of the doc-
ument. In this section, we are most interested
in subjectively demonstrating the low incidence
of ?word intrusion? both in terms of semantics
(theme) and syntax (part-of-speech). We do not
conduct formal experiments to demonstrate this,
but we subjectively show that our model learns
semantic and syntactic word distributions that are
likely robust towards problems of word intrusion
and that are therefore ?interpretable? for humans
examining the learned posterior word distribu-
tions.
Table 3 shows three topics ? manually la-
belled as ?party?, ?status update?, and ?politics?
? learned from the relatively small Twitter POS
dataset. We set the number of topics K = 20,
the number of classes S = 25, and the num-
ber of content word classes SCON = 8, following
our earlier POS tagging experiments. We show
the top five words from three POS-specific top-
ics labelled manually as noun, verb, and adjec-
tive. Given the relatively small size of the dataset,
the short length of the documents, and the eso-
teric language and grammar use, the interpretabil-
ity of the topics is reasonable. All three topics
assign high probability to words that one would
6
PARTY STATUS UPDATE POLITICS
noun verb adj noun verb adj noun verb adj
party gets awesome day is nice anything say late
man is old pm looking nasty truth has real
shit knew original school so last face wait high
men were fake today have hard city cant republican
person wasnt drunk body got tired candidate going important
Table 3: Example topics learned from the Twitter POS dataset with POSLDA.
CONJ DET PREP RP
and the to to
but a of it
or my in up
n your for away
in this on in
yet that with on
plus is at around
nd some NUMBER out
an an if over
to his from off
Table 4: Example topic-independent function class
distributions (CFUN) learned from the Twitter POS
dataset with POSLDA.
expect to have high importance with one or two
outliers. More importantly, however, the POS-
specific topics also generally reflect their syntac-
tic roles. Each of the verbs is assuredly (even
without the proper context) a verb (with the sin-
gle outlier being the word ?so?), and the same
thing for the nouns. The adjectives seem to fit
as well; though many of the words could be con-
sidered nouns depending on the context, it is clear
how given the topic each of the words could very
well act as an adjective. A final point worth
mentioning is that, unlike LDA, we do not per-
form stopword removal. Instead, the POSLDA
model has pushed stopwords to their own func-
tion classes (rather than content) freeing us from
having to perform pre- or post-processing steps
to ensure interpretable topics. The top words in
four of these topic-independent function classes,
learned from the Twitter POS dataset, are shown
in Table 4.3 These function word distributions are
even more cohesive than the content word distri-
butions, showing that the standard stopwords have
been accounted for as we expect in their respec-
tive function classes.
3Note that we make use of the tag dictionary when learn-
ing these word distributions.
5.2 Predictive Language Modeling
While we have demonstrated that our model can
achieve improved accuracy in POS tagging for
Twitter data, it can also be useful for other kinds
of language analysis in the social media do-
main. In the following experiments, we test the
POSLDA model quantitatively by determining its
ability as a predictive language model. Follow-
ing a standard practice in topic modeling research
(Blei et al, 2003; Griffiths et al, 2005), we fit a
model to a training set and then compute the per-
plexity of a held-out test set. For this experiment,
we use the Twitter POS training dataset described
earlier (16,348 words across 999 tweets). We then
perform testing on the Twitter POS testing dataset
(8,027 words across 500 tweets). We compare
the perplexity ? a monotonically decreasing func-
tion of the log likelihood ? to LDA, a Bayesian
HMM, and HMMLDA. Finally, we use Minka?s
fixed-point method (Wallach, 2008) to optimize
the hyperparameters ? and ?.
topics
perp
lexit
y
640
660
680
700
720
l l l l l l
5 10 15 20 25 30
model
l BHMM
HMMLDA
LDA
POSLDA
Figure 3: Perplexity of POSLDA and other probabilis-
tic models.
7
Figure 3 shows the perplexity on the held-out
Twitter test set for models trained with K =
{5, 10, 15, 20, 25, 30}. The Bayesian HMM is not
affected by the number of topics and is able to
beat the HMMLDA model at K = 5. It also
achieves lower perplexity than the LDA model at
K = 5, 25, and 30. Our POSLDA model, how-
ever, achieves the lowest perplexity of all tested
models at all topic settings that we tested. This
demonstrates that POSLDA is a good candidate
for both language modeling and for further la-
tent probabilistic model-based analysis of Twitter
data.
6 Discussion
In the previous section we demonstrated both
qualitatively and quantitatively that our model
captures two sources of information from unstruc-
tured texts: thematic (or semantics) and func-
tional (or syntactic). An important question to
consider is why ? as we demonstrated in sec-
tion 4 ? learning this sort of information im-
proves our ability to perform unsupervised POS
tagging. One reason is discussed in the introduc-
tion: semantic information can help disambiguate
the POS for a word that typically serves a differ-
ent function depending on the topic that it is nor-
mally associated with. This phenomenon likely
plays an important role in the accuracy improve-
ments that we observe. However, another feature
of the model is the distinction between ?content?
POS classes and ?function? POS classes. The for-
mer will depend on the current topic while the
latter are universal across thematic space. This
will also represent an improvement over the bare
HMM because words that depend on the cur-
rent topic ? typically nouns, verbs, adjectives,
and adverbs ? will be forced to these classes due
to their long-range thematic dependencies while
words with only short-range dependencies will be
pushed to the function POS classes. This latter
type of words ? conjunctions, determiners, etc.
? naturally do not depend on themes so as they
are pushed to the function-only POS classes, and
so one step of disambiguation has already been
performed. This is the same behaviour as in the
HMMLDA model by Griffiths et al (2005), but
here we are able to perform proper POS tagging
because there is more than just a single content
word class and we are therefore able to discern
between the topic-dependent parts-of-speech.
7 Conclusions and Future Work
In this paper, we have shown that incorporating
semantic topic information into a Bayesian HMM
can result in impressive increases in accuracy for
unsupervised POS tagging. Specifically, we pre-
sented POSLDA ? a topic model consistent across
the axes of both semantic and syntactic meanings.
Using this model to perform unsupervised POS
tagging results in consistent and statistically sig-
nificant increases in POS tagging accuracy and
decreases in variation of information when per-
forming POS clustering. These improvements are
demonstrated on a novel release of data from the
microblogging social network site Twitter. This
type of dataset is of particular interest because un-
supervised POS tagging will likely be most im-
portant in specialized idiosyncratic domains with
atypical features and small amounts of labelled
training data. Crucially, we showed that even
with the inconsistent and at times strange use of
grammar, slang, and acronyms, the syntactic por-
tion of the model demonstrably improves not only
the predictive ability of the model in terms of
perplexity, but also the accuracy in unsupervised
POS tagging. This is important because in gen-
eral tweets are far from being representative of
?proper? grammar. Nevertheless, there clearly ex-
ists some adherence to syntactic structure as the
use of the HMM within our model improves word
prediction and POS tagging.
This work represents the first ? to our knowl-
edge ? application of latent thematic information
to the unsupervised POS tagging task.4 How-
ever, due to the encouraging results, there are a
number of future research directions that present
themselves from this work. One immediate task is
to extend POSLDA to a nonparametric Bayesian
model. Section 4.2 shows how varying the num-
ber of topics K in the model can affect the tag-
ging accuracy by up to a full percentage point. A
nonparametric version of the model would free us
from having to perform the initial model selection
step to get the best accuracy. Another avenue for
future work is to infuse more structure into the
model such as word morphology.
4There has been some work done to include semantic in-
formation collected separately in a supervised POS tagging
approach (Toutanova and Johnson, 2008).
8
Acknowledgments
William Darling is supported by an NSERC Doc-
toral Postgraduate Scholarship, and Michael Paul
is supported by an NSF Graduate Research Fel-
lowship. The authors would like to thank the
anonymous reviewers for their helpful comments
and suggestions.
References
Amr Ahmed and Eric P. Xing. 2010. Staying
informed: supervised and semi-supervised multi-
view topical analysis of ideological perspective. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?10, pages 1140?1150, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022.
Jonathan Chang, Jordan Boyd-Graber, Chong Wang,
Sean Gerrish, and David M. Blei. 2009. Reading
tea leaves: How humans interpret topic models. In
Neural Information Processing Systems.
Chaitanya Chemudugunta, Padhraic Smyth, and Mark
Steyvers. 2006. Modeling general and specific as-
pects of documents with a probabilistic topic model.
In NIPS, pages 241?248.
G. Claeskens and N.L. Hjort. 2008. Model Selection
and Model Averaging. Cambridge University Press.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flani-
gan, and Noah A. Smith. 2011. Part-of-speech tag-
ging for twitter: Annotation, features, and experi-
ments. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics.
Sharon Goldwater and Tom Griffiths. 2007. A fully
Bayesian approach to unsupervised part-of-speech
tagging. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 744?751. Association for Computational Lin-
guistics.
Thomas L. Griffiths, Mark Steyvers, David M. Blei,
and Joshua B. Tenenbaum. 2005. Integrating topics
and syntax. In In Advances in Neural Information
Processing Systems 17, pages 537?544. MIT Press.
Donna Harman. 1992. Overview of the first text re-
trieval conference (trec-1). In TREC, pages 1?20.
M. Meila?. 2007. Comparing clusteringsan informa-
tion based distance. Journal of Multivariate Analy-
sis, 98(5):873?895, May.
Bernard Merialdo. 1993. Tagging english text with
a probabilistic model. Computational Linguistics,
20:155?171.
Michael J. Paul and Roxana Girju. 2010. A
two-dimensional topic-aspect model for discover-
ing multi-faceted topics. In AAAI.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Un-
supervised modeling of twitter conversations. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, HLT
?10, pages 172?180, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: training log-linear models on unlabeled
data. In Proceedings of the 43rd Annual Meeting
on Association for Computational Linguistics, ACL
?05, pages 354?362, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Kristina Toutanova and Mark Johnson. 2008. A
bayesian lda-based model for semi-supervised part-
of-speech tagging. In J.C. Platt, D. Koller,
Y. Singer, and S. Roweis, editors, Advances in
Neural Information Processing Systems 20, pages
1521?1528. MIT Press, Cambridge, MA.
Hanna Wallach, David Mimno, and Andrew McCal-
lum. 2009. Rethinking lda: Why priors matter. In
NIPS.
Hanna M. Wallach. 2008. Structured Topic Models for
Language. Ph.D. thesis, University of Cambridge.
Andrew Weir. 2012. Left-edge deletion in english and
subject omission in diaries. English Language and
Linguistics.
Andrew T. Wilson and Peter A. Chew. 2010. Term
weighting schemes for latent dirichlet alocation.
In Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics, HLT
?10, pages 465?473, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
9
