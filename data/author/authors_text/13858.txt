Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 585?590,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Dependency Parser Adaptation with Subtrees
from Auto-Parsed Target Domain Data
Xuezhe Ma
Department of Linguistics
University of Washington
Seattle, WA 98195, USA
xzma@uw.edu
Fei Xia
Department of Linguistics
University of Washington
Seattle, WA 98195, USA
fxia@uw.edu
Abstract
In this paper, we propose a simple and
effective approach to domain adaptation
for dependency parsing. This is a fea-
ture augmentation approach in which the
new features are constructed based on sub-
tree information extracted from the auto-
parsed target domain data. To demon-
strate the effectiveness of the proposed ap-
proach, we evaluate it on three pairs of
source-target data, compared with sever-
al common baseline systems and previous
approaches. Our approach achieves signif-
icant improvement on all the three pairs of
data sets.
1 Introduction
In recent years, several dependency parsing algo-
rithms (Nivre and Scholz, 2004; McDonald et al,
2005a; McDonald et al, 2005b; McDonald and
Pereira, 2006; Carreras, 2007; Koo and Collins,
2010; Ma and Zhao, 2012) have been proposed
and achieved high parsing accuracies on several
treebanks of different languages. However, the
performance of such parsers declines when train-
ing and test data come from different domain-
s. Furthermore, the manually annotated treebanks
that these parsers rely on are highly expensive to
create. Therefore, developing dependency pars-
ing algorithms that can be easily ported from one
domain to another?say, from a resource-rich do-
main to a resource-poor domain?is of great im-
portance.
Several approaches have been proposed for the
task of parser adaptation. McClosky et at. (2006)
successfully applied self-training to domain adap-
tation for constituency parsing using the rerank-
ing parser of Charniak and Johnson (2005). Re-
ichart and Rappoport (2007) explored self-training
when the amount of the annotated data is small
and achieved significant improvement. Zhang and
Wang (2009) enhanced the performance of depen-
dency parser adaptation by utilizing a large-scale
hand-crafted HPSG grammar. Plank and van No-
ord (2011) proposed a data selection method based
on effective measures of domain similarity for de-
pendency parsing.
There are roughly two varieties of domain adap-
tation problem?fully supervised case in which
there are a small amount of labeled data in the
target domain, and semi-supervised case in which
there are no labeled data in the target domain. In
this paper, we present a parsing adaptation ap-
proach focused on the fully supervised case. It is a
feature augmentation approach in which the new
features are constructed based on subtree infor-
mation extracted from the auto-parsed target do-
main data. For evaluation, we run experiments
on three pairs of source-target domains?WSJ-
Brown, Brown-WSJ, and WSJ-Genia. Our ap-
proach achieves significant improvement on al-
l these data sets.
2 Our Approach for Parsing Adaptation
Our approach is inspired by Chen et al (2009)?s
work on semi-supervised parsing with addition-
al subtree-based features extracted from unlabeled
data and by the feature augmentation method pro-
posed by Daume III (2007). In this section, we
first summarize Chen et al?s work and explain
how we extend that for domain adaptation. We
will then highlight the similarity and difference
between our work and Daume?s method.
2.1 Semi-supervised parsing with
subtree-based features
One of the most well-known semi-supervised
parsing methods is self-training, where a parser
trained from the labeled data set is used to parse
unlabeled data, and some of those auto-parsed data
are added to the labeled data set to retrain the pars-
585
ing models. Chen et al (2009)?s approach differs
from self-training in that partial information (i.e.,
subtrees), instead of the entire trees, from the auto-
parsed data is used to re-train the parsing models.
A subtree is a small part of a dependency
tree. For example, a first-order subtree is a single
edge consisting of a head and a dependent, and a
second-order sibling subtree is one that consists of
a head and two dependents. In Chen et al (2009),
they first extract all the subtrees in auto-parsed da-
ta and store them in a list Lst. Then they count
the frequency of these subtrees and divide them
into three groups according to their levels of fre-
quency. Finally, they construct new features for
the subtrees based on which groups they belongs
to and retrain a new parser with feature-augmented
training data.1
2.2 Parser adaptation with subtree-based
Features
Chen et al (2009)?s work is for semi-supervised
learning, where the labeled training data and the
test data come from the same domain; the subtree-
based features collected from auto-parsed data are
added to all the labeled training data to retrain the
parsing model. In the supervised setting for do-
main adaptation, there is a large amount of labeled
data in the source domain and a small amount of
labeled data in the target domain. One intuitive
way of applying Chen?s method to this setting is to
simply take the union of the labeled training data
from both domains and add subtree-based features
to all the data in the union when re-training the
parsing model. However, it turns out that adding
subtree-based features to only the labeled training
data in the target domain works better. The steps
of our approach are as follows:
1. Train a baseline parser with the small amount
of labeled data in the target domain and use
the parser to parse the large amount of unla-
beled sentences in the target domain.
2. Extract subtrees from the auto-parsed data
and add subtree-based features to the labeled
training data in the target domain.
3. Retrain the parser with the union of the la-
beled training data in the two domains, where
the instances from the target domain are aug-
mented with the subtree-based features.
1If a subtree does not appear in Lst, it falls to the fourth
group for ?unseen subtrees?.
To state our feature augmentation approach
more formally, we use X to denote the input s-
pace, and Ds and Dt to denote the labeled da-
ta in the source and target domains, respective-
ly. Let X ? be the augmented input space, and ?s
and ?t be the mappings from X to X ? for the in-
stances in the source and target domains respec-
tively. The mappings are defined by Eq 1, where
0 =< 0, 0, . . . , 0 >? X is the zero vector.
?s(xorg) = < xorg,0 >
?t(xorg) = < xorg,xnew > (1)
Here, xorg is the original feature vector in X ,
and xnew is the vector of the subtree-based fea-
tures extracted from auto-parsed data of the target
domain. The subtree extraction method used in
our approach is the same as in (Chen et al, 2009)
except that we use different thresholds when di-
viding subtrees into three frequency groups: the
threshold for the high-frequency level is TOP 1%
of the subtrees, the one for the middle-frequency
level is TOP 10%, and the rest of subtrees belong
to the low-frequency level. These thresholds are
chosen empirically on some development data set.
The idea of distinguishing the source and tar-
get data is similar to the method in (Daume III,
2007), which did feature augmentation by defin-
ing the following mappings:2
?s(xorg) = < xorg,0 >
?t(xorg) = < xorg,xorg > (2)
Daume III showed that differentiating features
from the source and target domains improved per-
formance for multiple NLP tasks. The difference
between that study and our approach is that our
new features are based on subtree information in-
stead of copies of original features. Since the new
features are based on the subtree information ex-
tracted from the auto-parsed target data, they rep-
resent certain properties of the target domain and
that explains why adding them to the target data
works better than adding them to both the source
and target data.
3 Experiments
For evaluation, we tested our approach on three
pairs of source-target data and compared it with
2The mapping in Eq 2 looks different from the one pro-
posed in (Daume III, 2007), but it can be proved that the two
are equivalent.
586
several common baseline systems and previous
approaches. In this section, we first describe the
data sets and parsing models used in each of the
three experiments in section 3.1. Then we pro-
vide a brief introduction to the systems we have
reimplemented for comparison in section 3.2. The
experimental results are reported in section 3.3.
3.1 Data and Tools
In the first two experiments, we used the Wal-
l Street Journal (WSJ) and Brown (B) portion-
s of the English Penn TreeBank (Marcus et al,
1993). In the first experiment denoted by ?WSJ-
to-B?, WSJ corpus is used as the source domain
and Brown corpus as the target domain. In the
second experiment, we use the reverse order of
the two corpora and denote it by ?B-to-WSJ?. The
phrase structures in the treebank are converted into
dependencies using Penn2Malt tool3 with the stan-
dard head rules (Yamada and Matsumoto, 2003).
For the WSJ corpus, we used the standard data
split: sections 2-21 for training and section 23 for
test. In the experiment of B-to-WSJ, we random-
ly selected about 2000 sentences from the training
portion of WSJ as the labeled data in the target do-
main. The rest of training data in WSJ is regarded
as the unlabeled data of the target domain.
For Brown corpus, we followed Reichart and
Rappoport (2007) for data split. The training and
test sections consist of sentences from all of the
genres that form the corpus. The training portion
consists of 90% (9 of each 10 consecutive sen-
tences) of the data, and the test portion is the re-
maining 10%. For the experiment of WSJ-to-B,
we randomly selected about 2000 sentences from
training portion of Brown and use them as labeled
data and the rest as unlabeled data in the target do-
main.
In the third experiment denoted by ??WSJ-to-
G?, we used WSJ corpus as the source domain and
Genia corpus (G)4 as the target domain. Following
Plank and van Noord (2011), we used the train-
ing data in CoNLL 2008 shared task (Surdeanu
et al, 2008) which are also from WSJ sections
2-21 but converted into dependency structure by
the LTH converter (Johansson and Nugues, 2007).
The Genia corpus is converted to CoNLL format
with LTH converter, too. We randomly selected
3http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html
4Genia distribution in Penn Treebank format is avail-
able at http://bllip.cs.brown.edu/download/genia1.0-division-
rel1.tar.gz
Source Target
training training unlabeled test
WSJ-to-B 39,832 2,182 19,632 2,429
B-to-WSJ 21,814 2,097 37,735 2,416
WSJ-to-G 39,279 1,024 13,302 1,360
Table 1: The number of sentences for each data set
used in our experiments
about 1000 sentences from the training portion of
Genia data and use them as the labeled data of the
target domain, and the rest of training data of Ge-
nia as the unlabeled data of the target domain. Ta-
ble 1 shows the number of sentences of each data
set used in the experiments.
The dependency parsing models we used in this
study are the graph-based first-order and second-
order sibling parsing models (McDonald et al,
2005a; McDonald and Pereira, 2006). To be more
specific, we use the implementation of MaxPars-
er5 with 10-best MIRA (Crammer et al, 2006; M-
cDonald, 2006) learning algorithm and each pars-
er is trained for 10 iterations. The feature sets of
first-order and second-order sibling parsing mod-
els used in our experiments are the same as the
ones in (Ma and Zhao, 2012). The input to Max-
Parser are sentences with Part-of-Speech tags; we
use gold-standard POS tags in the experiments.
Parsing accuracy is measured with unlabeled at-
tachment score (UAS) and the percentage of com-
plete matches (CM) for the first and second experi-
ments. For the third experiment, we also report la-
beled attachment score (LAS) in order to compare
with the results in (Plank and van Noord, 2011).
3.2 Comparison Systems
For comparison, we re-implemented the follow-
ing well-known baselines and previous approach-
es, and tested them on the three data sets:
SrcOnly: Train a parser with the labeled data
from the source domain only.
TgtOnly: Train a parser with the labeled data
from the target domain only.
Src&Tgt: Train a parser with the labeled data
from the source and target domains.
Self-Training: Following Reichart and Rap-
poport (2007), we train a parser with the
union of the source and target labeled data,
parse the unlabeled data in the target domain,
5http://sourceforge.net/projects/maxparser/
587
add the entire auto-parsed trees to the man-
ually labeled data in a single step without
checking their parsing quality, and retrain the
parser.
Co-Training: In the co-training system, we first
train two parsers with the labeled data from
the source and target domains, respectively.
Then we use the parsers to parse unlabeled
data in the target domain and select sentences
for which the two parsers produce identical
trees. Finally, we add the analyses for those
sentences to the union of the source and tar-
get labeled data to retrain a new parser. This
approach is similar to the one used in (Sagae
and Tsujii, 2007), which achieved the highest
scores in the domain adaptation track of the
CoNLL 2007 shared task (Nivre et al, 2007).
Feature-Augmentation: This is the approach
proposed in (Daume III, 2007).
Chen et al (2009): The algorithm has been ex-
plained in Section 2.1. We use the union of
the labeled data from the source and target
domains as the labeled training data. The
unlabeled data needed to construct subtree-
based features come from the target domain.
Plank and van Noord (2011): This system per-
forms data selection on a data pool consisting
of large amount of labeled data to get a train-
ing set that is similar to the test domain. The
results of the system come from their paper,
not from the reimplementation of their sys-
tem.
Per-corpus: The parser is trained with the large
training set from the target domain. For ex-
ample, for the experiment of WSJ-to-B, all
the labeled training data from the Brown cor-
pus is used for training, including the subset
of data which are treated as unlabeled in our
approach and other comparison systems. The
results serve as an upper bound of domain
adaptation when there is a large amount of
labeled data in the target domain.
3.3 Results
Table 2 illustrates the results of our approach with
the first-order parsing model in the first and sec-
ond experiments, together with the results of the
comparison systems described in section 3.2. The
WSJ-to-B B-to-WSJ
UAS CM UAS CM
SrcOnlys 88.8 43.8 86.3 26.5
TgtOnlyt 86.6 38.8 88.2 29.3
Src&Tgts,t 89.1 44.3 89.4 31.2
Self-Trainings,t 89.2 45.1 89.8 32.1
Co-Trainings,t 89.2 45.1 89.8 32.7
Feature-Augs,t 89.1 45.1 89.8 32.8
Chen (2009)s,t 89.3 45.0 89.7 31.8
this papers,t 89.5 45.5 90.2 33.4
Per-corpusT 89.9 47.0 92.7 42.1
Table 2: Results with the first-order parsing model
in the first and second experiments. The super-
script indicates the source of labeled data used in
training.
WSJ-to-B B-to-WSJ
UAS CM UAS CM
SrcOnlys 89.8 47.3 88.0 30.4
TgtOnlyt 87.7 42.2 89.7 34.2
Src&Tgts,t 90.2 48.2 90.9 36.6
Self-Trainings,t 90.3 48.8 91.0 37.1
Co-Trainings,t 90.3 48.5 90.9 38.0
Feature-Augs,t 90.0 48.4 91.0 37.4
Chen (2009)s,t 90.3 49.1 91.0 37.6
this papers,t 90.6 49.6 91.5 38.8
Per-corpusT 91.1 51.1 93.6 47.9
Table 3: Results with the second-order sibling
parsing model in the first and second experiments.
results with the second-order sibling parsing mod-
el is shown in Table 3. The superscript s, t and T
indicates from which domain the labeled data are
used in training: tag s refers to the labeled data in
the source domain, tag t refers to the small amount
of labeled data in the target domain, and tag T in-
dicates that all the labeled training data from the
target domain, including the ones that are treated
as unlabeled in our approach, are used for training.
Table 4 shows the results in the third experimen-
t with the first-order parsing model. We also in-
clude the result from (Plank and van Noord, 2011),
which use the same parsing model as ours. Note
that this result is not comparable with other num-
bers in the table as it uses a larger set of labeled
data, as indicated by the ? superscript.
All three tables show that our system out-
performs the comparison systems in all three
588
WSJ-to-G
UAS LAS
SrcOnlys 83.8 82.0
TgtOnlyt 87.0 85.7
Src&Tgts,t 87.2 85.9
Self-Trainings,t 87.3 86.0
Co-Trainings,t 87.3 86.0
Feature-Augs,t 87.9 86.5
Chen (2009)s,t 87.5 86.2
this papers,t 88.4 87.1
Plank (2011)? - 86.8
Per-corpusT 90.5 89.7
Table 4: Results with first-order parsing model in
the third experiment. ?Plank (2011)? refers to the
approach in Plank and van Noord (2011).
experiments.6 The improvement of our ap-
proach over the feature augmentation approach
in Daume III (2007) indicates that adding subtree-
based features provides better results than making
several copies of the original features. Our system
outperforms the system in (Chen et al, 2009), im-
plying that adding subtree-based features to only
the target labeled data is better than adding them
to the labeled data in both the source and target
domains.
Considering the three steps of our approach in
Section 2.2, the training data used to train the pars-
er in Step 1 can be from the target domain only or
from the source and target domains. Similarly, in
Step 3 the subtree-based features can be added to
the labeled data from the target domain only or
from the source and target domains. Therefore,
there are four combinations. Our approach is the
one that uses the labeled data from the target do-
main only in both steps, and Chen?s system uses
labeled data from the source and target domains in
both steps. Table 5 compares the performance of
the final parser in the WSJ-to-Genia experimen-
t when the parser is created with one of the four
combinations. The column label and the row label
indicate the choice in Step 1 and 3, respectively.
The table shows the choice in Step 1 does not have
a significant impact on the performance of the fi-
nal models; in contrast, the choice in Step 3 does
matter? adding subtree-based features to the la-
beled data in the target domain only is much better
than adding features to the data in both domains.
6The results of Per-corpus are better than ours but it uses
a much larger labeled training set in the target domain.
TgtOnly Src&Tgt
TgtOnly 88.4/87.1 88.4/87.1
Src&Tgt 87.6/86.3 87.5/86.2
Table 5: The performance (UAS/LAS) of the fi-
nal parser in the WSJ-to-Genia experiment when
different training data are used to create the final
parser. The column label and row label indicate
the choice of the labeled data used in Step 1 and 3
of the process described in Section 2.2.
4 Conclusion
In this paper, we propose a feature augmentation
approach for dependency parser adaptation which
constructs new features based on subtree informa-
tion extracted from auto-parsed data from the tar-
get domain. We distinguish the source and target
domains by adding the new features only to the
data from the target domain. The experimental re-
sults on three source-target domain pairs show that
our approach outperforms all the comparison sys-
tems.
For the future work, we will explore the po-
tential benefits of adding other types of features
extracted from unlabeled data in the target do-
main. We will also experiment with various ways
of combining our current approach with other do-
main adaptation methods (such as self-training
and co-training) to further improve system perfor-
mance.
References
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In Proceed-
ings of the CoNLL Shared Task Session of EMNLP-
CONLL, pages 957?961.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine-grained n-best parsing and discriminative r-
eranking. In Proceedings of the 43rd Meeting of
the Association for Computional Linguistics (ACL
2005), pages 132?139.
Wenliang Chen, Jun?ichi Kazama, Kiyotaka Uchimo-
to, and Kentaro Torisawa. 2009. Improving de-
pendency parsing with subtrees from auto-parsed
data. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 570?579, Singapore, August.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Jornal of Machine
Learning Research, 7:551?585.
589
Hal Daume III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of the 45th Annual Meeting
of the Association of Computational Linguistics (A-
CL 2007), pages 256?263, Prague, Czech Republic,
June.
Richard Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for en-
glish. In Proceedings of NODALIDA, Tartu, Estonia.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of 48th
Meeting of the Association for Computional Linguis-
tics (ACL 2010), pages 1?11, Uppsala, Sweden, July.
Xuezhe Ma and Hai Zhao. 2012. Fourth-order depen-
dency parsing. In Proceedings of COLING 2012:
Posters, pages 785?796, Mumbai, India, December.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Reranking and self-training for pars-
er adaptation. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Com-
putational Linguistics (COLING-ACL 2006), pages
337?344, Sydney, Australia, July.
Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proceedings of European Association for
Computational Linguistics (EACL-2006), pages 81?
88, Trento, Italy, April.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005a. Online large-margin training of
dependency parsers. In Proceedings of the 43rd
Annual Meeting on Association for Computational
Linguistics (ACL-2005), pages 91?98, Ann Arbor,
Michigan, USA, June 25-30.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005b. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceed-
ings of Human Language Technology Conference
and Conference on Empirical Methods in Natural
Language (HLT/EMNLP 05), pages 523?530, Van-
couver, Canada, October.
Ryan McDonald. 2006. Discriminative learning span-
ning tree algorithm for dependency parsing. Ph.D.
thesis, University of Pennsylvania.
Joakim Nivre and Mario Scholz. 2004. Determinis-
tic dependency parsing of english text. In Proceed-
ings of the 20th international conference on Com-
putational Linguistics (COLING?04), pages 64?70,
Geneva, Switzerland, August 23-27.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
915?932, Prague, Czech, June.
Barbara Plank and Gertjan van Noord. 2011. Effec-
tive measures of domain similarity for parsing. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (ACL-HLT 2011), pages 1566?
1576, Portland, Oregon, USA, June.
Roi Reichart and Ari Rappoport. 2007. Self-training
for enhancement and domain adaptation of statistical
parsers trained on small datasets. In Proceedings of
the 45th Annual Meeting of the Association of Com-
putational Linguistics (ACL-2007), pages 616?623,
Prague, Czech Republic, June.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependen-
cy parsing and domain adaptation with LR models
and parser ensembles. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
1044?1050, Prague, Czech Republic, June.
Mihai Surdeanu, Richard Johansson, Adam Meyers, L-
luis Marquez, and Joakim Nivre. 2008. The conll-
2008 shared task on joint parsing of syntactic and
semantic dependencies. In Proceedings of the 12th
Conference on Computational Natural Language
Learning (CoNLL-2008), pages 159?177, Manch-
ester, UK, Augest.
Hiroyasu Yamada and Yuji Matsumoto. 2003. S-
tatistical dependency analysis with support vector
machines. In Proceedings of the 8th Internation-
al Workshop on Parsing Technologies (IWPT-2003),
pages 195?206, Nancy, France, April.
Yi Zhang and Rui Wang. 2009. Cross-domain depen-
dency parsing using a deep linguistic grammar. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
(ACL-IJCNLP 2009), pages 378?386, Suntec, Sin-
gapore, August.
590
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1337?1348,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Unsupervised Dependency Parsing with Transferring
Distribution via Parallel Guidance and Entropy Regularization
Xuezhe Ma
Department of Linguistics
University of Washington
Seattle, WA 98195, USA
xzma@uw.edu
Fei Xia
Department of Linguistics
University of Washington
Seattle, WA 98195, USA
fxia@uw.edu
Abstract
We present a novel approach for induc-
ing unsupervised dependency parsers for
languages that have no labeled training
data, but have translated text in a resource-
rich language. We train probabilistic pars-
ing models for resource-poor languages by
transferring cross-lingual knowledge from
resource-rich language with entropy reg-
ularization. Our method can be used as
a purely monolingual dependency parser,
requiring no human translations for the
test data, thus making it applicable to a
wide range of resource-poor languages.
We perform experiments on three Data
sets ? Version 1.0 and version 2.0 of
Google Universal Dependency Treebanks
and Treebanks from CoNLL shared-tasks,
across ten languages. We obtain state-
of-the art performance of all the three
data sets when compared with previously
studied unsupervised and projected pars-
ing systems.
1 Introduction
In recent years, dependency parsing has gained
universal interest due to its usefulness in a wide
range of applications such as synonym gener-
ation (Shinyama et al, 2002), relation extrac-
tion (Nguyen et al, 2009) and machine trans-
lation (Katz-Brown et al, 2011; Xie et al,
2011). Several supervised dependency parsing
algorithms (Nivre and Scholz, 2004; McDonald
et al, 2005a; McDonald et al, 2005b; McDon-
ald and Pereira, 2006; Carreras, 2007; Koo and
Collins, 2010; Ma and Zhao, 2012; Zhang et al,
2013) have been proposed and achieved high pars-
ing accuracies on several treebanks, due in large
part to the availability of dependency treebanks in
a number of languages (McDonald et al, 2013).
However, the manually annotated treebanks that
these parsers rely on are highly expensive to cre-
ate, in particular when we want to build treebanks
for resource-poor languages. This led to a vast
amount of research on unsupervised grammar in-
duction (Carroll and Charniak, 1992; Klein and
Manning, 2004; Smith and Eisner, 2005; Cohen
and Smith, 2009; Spitkovsky et al, 2010; Blun-
som and Cohn, 2010; Marec?ek and Straka, 2013;
Spitkovsky et al, 2013), which appears to be a
natural solution to this problem, as unsupervised
methods require only unannotated text for training
parsers. Unfortunately, the unsupervised gram-
mar induction systems? parsing accuracies often
significantly fall behind those of supervised sys-
tems (McDonald et al, 2011). Furthermore, from
a practical standpoint, it is rarely the case that we
are completely devoid of resources for most lan-
guages.
In this paper, we consider a practically moti-
vated scenario, in which we want to build statisti-
cal parsers for resource-poor target languages, us-
ing existing resources from a resource-rich source
language (like English).1 We assume that there are
absolutely no labeled training data for the target
language, but we have access to parallel data with
a resource-rich language and a sufficient amount
of labeled training data to build an accurate parser
for the resource-rich language. This scenario ap-
pears similar to the setting in bilingual text pars-
ing. However, most bilingual text parsing ap-
proaches require bilingual treebanks ? treebanks
that have manually annotated tree structures on
both sides of source and target languages (Smith
and Smith, 2004; Burkett and Klein, 2008), or
have tree structures on the source side and trans-
lated sentences in the target languages (Huang et
1For the sake of simplicity, we refer to the resource-poor
language as the ?target language?, and resource-rich language
as the ?source language?. In addition, in this study we use En-
glish as the source resource-rich language, but our methodol-
ogy can be applied to any resource-rich languages.
1337
al., 2009; Chen et al, 2010). Obviously, bilingual
treebanks are much more difficult to acquire than
the resources required in our scenario, since the la-
beled training data and the parallel text in our case
are completely separated. What is more impor-
tant is that most studies on bilingual text parsing
assumed that the parser is applied only on bilin-
gual text. But our goal is to develop a parser that
can be used in completely monolingual setting for
each target language of interest.
This scenario is applicable to a large set of lan-
guages and many research studies (Hwa et al,
2005) have been made on it. Ganchev et al (2009)
presented a parser projection approach via paral-
lel text using the posterior regularization frame-
work (Graca et al, 2007). McDonald et al (2011)
proposed two parser transfer approaches between
two different languages ? one is directly trans-
ferred parser from delexicalized parsers, and the
other parser is transferred using constraint driven
learning algorithm where constraints are drawn
from parallel corpora. In that work, they demon-
strate that even the directly transferred delexi-
calized parser produces significantly higher ac-
curacies than unsupervised parsers. Cohen et
al. (2011) proposed an approach for unsupervised
dependency parsing with non-parallel multilingual
guidance from one or more helper languages, in
which parallel data is not used.
In this work, we propose a learning frame-
work for transferring dependency grammars from
a resource-rich language to resource-poor lan-
guages via parallel text. We train probabilistic
parsing models for resource-poor languages by
maximizing a combination of likelihood on par-
allel data and confidence on unlabeled data. Our
work is based on the learning framework used in
Smith and Eisner (2007), which is originally de-
signed for parser bootstrapping. We extend this
learning framework so that it can be used to trans-
fer cross-lingual knowledge between different lan-
guages.
Throughout this paper, English is used as the
source language and we evaluate our approach on
ten target languages ? Danish (da), Dutch (nl),
French (fr), German (de), Greek (el), Italian (it),
Korean (ko), Portuguese (pt), Spanish (es) and
Swedish (sv). Our approach achieves significant
improvement over previous state-of-the-art unsu-
pervised and projected parsing systems across all
the ten languages, and considerably bridges the
Economic news had little effect on financial marketsRoot
Figure 1: An example dependency tree.
gap to fully supervised dependency parsing per-
formance.
2 Our Approach
Dependency trees represent syntactic relationships
through labeled directed edges between heads and
their dependents. For example, Figure 1 shows a
dependency tree for the sentence, Economic news
had little effect on financial markets, with the sen-
tence?s root-symbol as its root. The focus of this
work is on building dependency parsers for target
languages, assuming that an accurate English de-
pendency parser and some parallel text between
the two languages are available. Central to our ap-
proach is a maximizing likelihood learning frame-
work, in which we use an English parser and par-
allel text to estimate the ?transferring distribution?
of the target language parsing model (See Section
2.2 for more details). Another advantage of the
learning framework is that it combines both the
likelihood on parallel data and confidence on unla-
beled data, so that both parallel text and unlabeled
data can be utilized in our approach.
2.1 Edge-Factored Parsing Model
In this paper, we will use the following notation:
x represents a generic input sentence, and y rep-
resents a generic dependency tree. T(x) is used
to denote the set of possible dependency trees
for sentence x. The probabilistic model for de-
pendency parsing defines a family of conditional
probability p
?
(y|x) over all y given sentence x,
with a log-linear form:
p
?
(y|x) =
1
Z(x)
exp
{
?
j
?
j
F
j
(y,x)
}
(1)
where F
j
are feature functions, ? = (?
1
, ?
2
, . . .)
are parameters of the model, and Z(x) is a nor-
malization factor, which is commonly referred to
as the partition function:
Z(x) =
?
y?T(x)
exp
{
?
j
?
j
F
j
(y,x)
}
(2)
1338
A common strategy to make this parsing model ef-
ficiently computable is to factor dependency trees
into sets of edges:
F
j
(y,x) =
?
e?y
f
j
(e,x). (3)
That is, dependency tree y is treated as a set
of edges e and each feature function F
j
(y,x) is
equal to the sum of all the features f
j
(e,x).
We denote the weight function of each edge e as
follows:
w(e,x) = exp
{
?
j
?
j
f
j
(e,x)
}
(4)
and the conditional probability p
?
(y|x) has the
following form:
p
?
(y|x) =
1
Z(x)
?
e?y
w(e,x) (5)
2.2 Model Training
One of the most common model training meth-
ods for supervised dependency parser is Maxi-
mum conditional likelihood estimation. For a su-
pervised dependency parser with a set of train-
ing data {(x
i
,y
i
)}, the logarithm of the likelihood
(a.k.a. the log-likelihood) is given by:
L(?) =
?
i
log p
?
(y
i
|x
i
) (6)
Maximum likelihood training chooses parameters
such that the log-likelihood L(?) is maximized.
However, in our scenario we have no labeled
training data for target languages but we have
some parallel and unlabeled data plus an En-
glish dependency parser. For the purpose of
transferring cross-lingual information from the
English parser via parallel text, we explore the
model training method proposed by Smith and
Eisner (2007), which presented a generalization of
K function (Abney, 2004), and related it to an-
other semi-supervised learning technique, entropy
regularization (Jiao et al, 2006; Mann and Mc-
Callum, 2007). The objective K function to be
minimized is actually the expected negative log-
likelihood:
K = ?
?
i
?
y
i
p?(y
i
|x
i
) log p
?
(y
i
|x
i
)
=
?
i
D(p?
i
||p
?,i
) +H(p?
i
) (7)
where p?
i
(?)
def
= p?(?|x
i
) and p
?,i
(?)
def
= p
?
(?|x
i
).
p?(y|x) is the ?transferring distribution? that re-
flects our uncertainty about the true labels, and we
are trying to learn a parametric model p
?
(y|x) by
minimizing the K function.
In our scenario, we have a set of aligned par-
allel data P = {xs
i
,x
t
i
, a
i
} where a
i
is the word
alignment for the pair of source-target sentences
(x
s
i
,x
t
i
), and a set of unlabeled sentences of the
target language U = {xt
i
}. We also have a trained
English parsing model p
?
E
(y|x). Then the K in
equation (7) can be divided into two cases, accord-
ing to whether x
i
belongs to parallel data set P or
unlabeled data set U . For the unlabeled examples
{x
i
? U}, some previous studies (e.g., (Abney,
2004)) simply use a uniform distribution over la-
bels (e.g., parses), to reflect that the label is un-
known. We follow the method in Smith and Eis-
ner (2007) and take the transferring distribution
p?
i
to be the actual current belief p
?,i
. The total
contribution of the unsupervised examples to K
then simplifies to K
U
=
?
x
i
?U
H(p
?,i
), which may
be regarded as the entropy item used to constrain
the model?s uncertainty H to be low, as presented
in the work on entropy regularization (Jiao et al,
2006; Mann and McCallum, 2007).
But how can we define the transferring distri-
bution for the parallel examples {xt
i
? P}? We
define the transferring distribution by defining the
transferring weight utilizing the English parsing
model p
?
E
(y|x) via parallel data with word align-
ments:
w?(e
t
,x
t
i
) =
{
w
E
(e
s
,x
s
i
), if et algn?? es
w
E
(e
t
delex
,x
s
i
), otherwise
(8)
where w
E
(?, ?) is the weight function of the En-
glish parsing model p
?
E
(y|x), and et
delex
is the
delexicalized form2 of the edge et. From the
definition of the transferring weight, we can see
that, if an edge et of the target language sentence
x
t
i
is aligned to an edge es of the English sen-
tence xs
i
, we transfer the weight of edge et to
the corresponding weight of edge es in the En-
glish parsing model p
?
E
(y|x). If the edge et
is not aligned to any edges of the English sen-
tence xs
i
, we reduce the edge et to the delexical-
ized form and calculate the transferring weight in
the English parsing model. There are two advan-
2The delexicalized form of an edge is an edge for which
only delexicalized features are considered.
1339
tages for this definition of the transferring weight.
First, by transferring the weight function to the
corresponding weight in the well-developed En-
glish parsing model, we can project syntactic in-
formation across language boundaries. Second,
McDonald et al (2011) demonstrates that parsers
with only delexicalized features produce consid-
erably high parsing performance. By reducing
unaligned edges to their delexicalized forms, we
can still use those delexicalized features, such as
part-of-speech tags, for those unaligned edges, and
can address problem that automatically generated
word alignments include errors.
From the definition of transferring weight in
equation (8), the transferring distribution can be
defined in the following way:
p?(y|x) =
1
?
Z(x)
?
e?y
w?(e,x) (9)
where
?
Z(x) =
?
y
?
e?y
w?(e,x) (10)
Due to the normalizing factor ?Z(x), the transfer-
ring distribution is a valid one.
We introduce a multiplier ? as a trade-off be-
tween the two contributions (parallel and unsuper-
vised) of the objective function K , and the final
objective function K ? has the following form:
K
?
= ?
?
x
i
?P
?
y
i
p?(y
i
|x
i
) log p
?
(y
i
|x
i
)
+ ?
?
x
i
?U
H(p
?,i
)
= K
P
+ ?K
U
(11)
K
P
and K
U
are the contributions of the parallel
and unsupervised data, respectively. One may re-
gard ? as a Lagrange multiplier that is used to
constrain the parser?s uncertainty H to be low, as
presented in several studies on entropy regulariza-
tion (Brand, 1998; Grandvalet and Bengio, 2004;
Jiao et al, 2006).
2.3 Algorithms and Complexity for Model
Training
To train our parsing model, we need to find out the
parameters ? that minimize the objective function
K
? in equation (11). This optimization problem
is typically solved using quasi-Newton numeri-
cal methods such as L-BFGS (Nash and Nocedal,
1991), which requires efficient calculation of the
objective function and the gradient of the objec-
tive function.
The first item (K
P
) of the K ? function in equa-
tion (11) can be rewritten in the following form:
K
P
= ?
?
x
i
?P
[
?
y
i
p?(y
i
|x
i
)
?
e?y
i
logw(e,x
i
)
? logZ(x
i
)
] (12)
and according to equation (1) and (3) the gradient
of K
P
can be written as:
?K
P
??
j
=
?
x
i
?P
?p?(y
i
|x
i
) log p
?
(y
i
|x
i
)
??
j
=
?
x
i
?P
[
?
y
i
p?(y
i
|x
i
)
?
e?y
i
f
j
(e,x
i
)
?
?
y
i
p
?
(y
i
|x
i
)
?
e?y
i
f
j
(e,x
i
)
]
(13)
According to equation (9), p?(y|x) can also be
factored into the multiplication of the weight of
each edge, so both K
P
and its gradient can be
calculated by running the O(n3) inside-outside al-
gorithm (Baker, 1979; Paskin, 2001) for projec-
tive parsing. For non-projective parsing, the anal-
ogy to the inside algorithm is the O(n3) matrix-
tree algorithm based on Kirchhoff?s Matrix-Tree
Theorem, which is dominated asymptotically by a
matrix determinant (Koo et al, 2007; Smith and
Smith, 2007). The gradient of a determinant may
be computed by matrix inversion, so evaluating the
gradient again has the same O(n3) complexity as
evaluating the function.
The second item (K
U
) of the K ? function in
equation (11) is the Shannon entropy of the pos-
terior distribution over parsing trees, and can be
written into the following form:
K
U
= ?
?
x
i
?U
[
?
y
i
p
?
(y
i
|x
i
)
?
e?y
i
logw(e,x
i
)
? logZ(x
i
)
] (14)
and the gradient of K
U
is in the following:
?K
U
??
j
=
?
x
i
?U
?p
?
(y
i
|x
i
) log p
?
(y
i
|x
i
)
??
j
= ?
?
y
i
p
?
(y
i
|x
i
) log p
?
(y
i
|x
i
)F
j
(y
i
,x
i
)
+
(
?
y
i
p
?
(y
i
|x
i
) log p
?
(y
i
|x
i
)
)
?
(
?
y
i
p
?
(y
i
|x
i
)F
j
(y
i
,x
i
)
)
(15)
1340
#sents/#tokens
training dev test
Version 1.0
de 2,200/30,460 800/12,215 1,000/16,339
es 3,345/94,232 370/10,191 300/8,295
fr 3,312/74,979 366/8,071 300/6,950
ko 5,308/62,378 588/6,545 298/2,917
sv 4,447/66,631 493/9,312 1,219/20,376
Version 2.0
de 14,118/26,4906 800/12,215 1,000/16,339
es 14,138/37,5180 1,569/40,950 300/8,295
fr 14,511/35,1233 1,611/38,328 300/6,950
it 6,389/14,9145 400/9,541 400/9,187
ko 5437/60,621 603/6,438 299/2,631
pt 9,600/23,9012 1,200/29,873 1,198/29,438
sv 4,447/66,631 493/9,312 1,219/20,376
Table 1: Data statistics of two versions of Google
Universal Treebanks for the target languages.
Similar with the calculation of K
P
, K
U
can also
be computed by running the inside-outside algo-
rithm (Baker, 1979; Paskin, 2001) for projective
parsing. For the gradient of K
U
, both the two
multipliers of the second item in equation (15) can
be computed using the same inside-outside algo-
rithm. For the first item in equation (15), an O(n3)
dynamic programming algorithm that is closely
related to the forward-backward algorithm (Mann
and McCallum, 2007) for the entropy regularized
CRF (Jiao et al, 2006) can be used for projective
parsing. For non-projective parsing, however, the
runtime rises to O(n4). In this paper, we focus on
projective parsing.
2.4 Summary of Our Approach
To summarize the description in the previous sec-
tions, our approach is performed in the following
steps:
1. Train an English parsing model p
?
E
(y|x),
which is used to estimate the transferring dis-
tribution p?(y|x).
2. Prepare parallel text by running word align-
ment method to obtain word alignments,3 and
prepare the unlabeled data.
3. Train a parsing model for the target lan-
guage by minimizing the objective K ? func-
tion which is the combination of expected
negative log-likelihood on parallel and unla-
beled data.
3The word alignment methods do not require additional
resources besides parallel text.
# sents
500 1000 2000 5000 10000 20000
da 12,568 25,225 49,889 126,623 254,565 509,480
de 13,548 26,663 53,170 133,596 265,589 527,407
el 14,198 28,302 56,744 143,753 286,126 572,777
es 15,147 29,214 57,526 144,621 290,517 579,164
fr 15,046 29,982 60,569 153,874 306,332 609,541
it 15,151 29,786 57,696 145,717 288,337 573,557
ko 3,814 7,679 15,337 38,535 77,388 155,051
nl 13,234 26,777 54,570 137,277 274,692 551,463
pt 14,346 28,109 55,998 143,221 285,590 571,109
sv 12,242 24,897 50,047 123,069 246,619 490,086
Table 2: The number of tokens in parallel data
used in our experiments. For all these corpora, the
other language is English.
3 Data and Tools
In this section, we illustrate the data sets used in
our experiments and the tools for data preparation.
3.1 Choosing Target Languages
Our experiments rely on two kinds of data sets:
(i) Monolingual Treebanks with consistent anno-
tation schema ? English treebank is used to train
the English parsing model, and the Treebanks for
target languages are used to evaluate the parsing
performance of our approach. (ii) Large amounts
of parallel text with English on one side. We se-
lect target languages based on the availability of
these resources. The monolingual treebanks in our
experiments are from the Google Universal De-
pendency Treebanks (McDonald et al, 2013), for
the reason that the treebanks of different languages
in Google Universal Dependency Treebanks have
consistent syntactic representations.
The parallel data come from the Europarl cor-
pus version 7 (Koehn, 2005) and Kaist Corpus4.
Taking the intersection of languages in the two
kinds of resources yields the following seven lan-
guages: French, German, Italian, Korean, Por-
tuguese, Spanish and Swedish.
The treebanks from CoNLL shared-tasks on
dependency parsing (Buchholz and Marsi, 2006;
Nivre et al, 2007) appear to be another reasonable
choice. However, previous studies (McDonald et
al., 2011; McDonald et al, 2013) have demon-
strated that a homogeneous representation is criti-
cal for multilingual language technologies that re-
quire consistent cross-lingual analysis for down-
stream components, and the heterogenous repre-
sentations used in CoNLL shared-tasks treebanks
weaken any conclusion that can be drawn.
4http://semanticweb.kaist.ac.kr/home/
index.php/Corpus10
1341
DTP DTP? PTP? -U +U OR
de 58.50 58.46 69.21 73.72 74.01 78.64
es 68.07 68.72 72.57 75.32 75.60 82.56
fr 70.14 71.13 74.60 76.65 76.93 83.69
ko 42.37 43.57 53.72 59.72 59.94 89.85
sv 70.56 70.59 75.87 78.91 79.27 85.59
Ave 61.93 62.49 69.19 72.86 73.15 84.67
Table 3: UAS for two versions of our approach, to-
gether with baseline and oracle systems on Google
Universal Treebanks version 1.0. ?Ave? is the
macro-average across the five languages.
For comparison with previous studies, never-
theless, we also run experiments on CoNLL tree-
banks (see Section 4.4 for more details). We eval-
uate our approach on three target languages from
CoNLL shared task treebanks, which do not ap-
pear in Google Universal Treebanks. The three
languages are Danish, Dutch and Greek. So totally
we have ten target languages. The parallel data for
these three languages are also from the Europarl
corpus version 7.
3.2 Word Alignments
In our approach, word alignments for the paral-
lel text are required. We perform word alignments
with the open source GIZA++ toolkit5. The paral-
lel corpus was preprocessed in standard ways, se-
lecting sentences with the length in the range from
3 to 100. Then we run GIZA++ with the default
setting to generate word alignments in both direc-
tions. We then make the intersection of the word
alignments of two directions to generate one-to-
one alignments.
3.3 Part-of-Speech Tagging
Several features in our parsing model involve part-
of-speech (POS) tags of the input sentences. The
set of POS tags needs to be consistent across lan-
guages and treebanks. For this reason we use
the universal POS tag set of Petrov et al (2011).
This set consists of the following 12 coarse-
grained tags: NOUN (nouns), VERB (verbs), ADJ
(adjectives), ADV (adverbs), PRON (pronouns),
DET (determiners), ADP (prepositions or postpo-
sitions), NUM (numerals), CONJ (conjunctions),
PRT (particles), PUNC (punctuation marks) and
X (a catch-all for other categories such as abbrevi-
ations or foreign words).
POS tags are not available for parallel data in
the Europarl and Kaist corpus, so we need to pro-
5https://code.google.com/p/giza-pp/
DTP? PTP? -U +U OR
de 58.56 69.77 73.92 74.30 81.65
es 68.72 73.22 75.21 75.53 83.92
fr 71.13 74.75 76.14 76.53 83.51
it 70.74 76.08 77.55 77.74 85.47
ko 38.55 43.34 59.71 59.89 90.42
pt 69.82 74.59 76.30 76.65 85.67
sv 70.59 75.87 78.91 79.27 85.59
Ave 64.02 69.66 73.96 74.27 85.18
Table 4: UAS for two versions of our approach, to-
gether with baseline and oracle systems on Google
Universal Treebanks version 2.0. ?Ave? is the
macro-average across the seven languages.
vide the POS tags for these data. In our experi-
ments, we train a Stanford POS Tagger (Toutanova
et al, 2003) for each language. The labeled train-
ing data for each POS tagger are extracted from
the training portion of each Treebanks. The aver-
age tagging accuracy is around 95%.
Undoubtedly, we are primarily interested in ap-
plying our approach to build statistical parsers
for resource-poor target languages without any
knowledge. For the purpose of evaluation of our
approach and comparison with previous work, we
need to exploit the gold POS tags to train the POS
taggers. As part-of-speech tags are also a form
of syntactic analysis, this assumption weakens the
applicability of our approach. Fortunately, some
recently proposed POS taggers, such as the POS
tagger of Das and Petrov (2011), rely only on la-
beled training data for English and the same kind
of parallel text in our approach. In practice we can
use this kind of POS taggers to predict POS tags,
whose tagging accuracy is around 85%.
4 Experiments
In this section, we will describe the details of our
experiments and compare our results with previ-
ous methods.
4.1 Data Sets
As presented in Section 3.1, we evaluate our pars-
ing approach on both version 1.0 and version
2.0 of Google Univereal Treebanks for seven lan-
guages6. We use the standard splits of the treebank
for each language as specified in the release of the
data7. Table 1 presents the statistics of the two ver-
sions of Google Universal Treebanks. We strip all
6Japanese and Indonesia are excluded as no practicable
parallel data are available.
7https://code.google.com/p/uni-dep-tb/
1342
Google Universal Treebanks V1.0
de es fr ko sv
# sents PTP? -U +U PTP? -U +U PTP? -U +U PTP? -U +U PTP? -U +U
500 63.23 70.79 70.93 70.09 72.32 72.64 72.24 74.64 74.90 47.71 56.87 57.22 71.70 75.88 76.13
1000 65.61 71.71 71.86 70.90 73.44 73.67 72.95 75.07 75.35 47.83 57.65 58.15 72.38 76.55 77.03
2000 66.52 72.33 72.48 72.01 73.57 73.81 73.69 75.88 76.22 48.37 58.19 58.44 73.65 77.86 78.12
5000 67.79 73.06 73.31 72.34 74.30 74.79 74.31 76.02 76.29 53.02 58.57 59.04 74.88 78.48 78.70
10000 68.44 73.59 73.92 72.48 74.86 75.26 74.43 76.14 76.34 53.61 59.17 59.55 75.34 78.78 79.08
20000 69.21 73.72 74.01 72.57 75.32 75.60 74.60 76.55 76.93 53.72 59.72 59.94 75.87 78.91 79.27
Google Universal Treebanks V2.0
de es fr ko it
# sents PTP? -U +U PTP? -U +U PTP? -U +U PTP? -U +U PTP? -U +U
500 60.10 71.07 71.39 69.52 72.97 73.28 71.10 74.57 74.70 40.09 56.60 57.10 72.80 75.67 75.94
1000 61.76 72.15 72.39 70.78 73.48 73.79 72.14 75.13 75.43 40.44 57.55 57.93 73.55 76.43 76.67
2000 65.35 72.73 73.04 71.75 74.10 74.35 73.21 75.78 76.06 40.87 58.11 58.43 74.44 76.99 77.39
5000 67.86 73.32 73.62 72.43 74.55 74.83 74.14 75.83 76.02 40.90 58.48 58.96 75.07 77.10 77.34
10000 68.70 73.71 74.02 72.85 74.80 74.95 74.53 75.97 76.17 41.29 59.13 59.44 75.65 77.50 77.71
20000 69.77 73.92 74.30 73.22 75.21 75.53 74.75 76.14 76.53 43.34 59.71 59.89 76.08 77.55 77.74
pt
# sents PTP? -U +U
500 71.34 74.41 74.68
1000 71.91 74.48 75.08
2000 72.93 75.10 75.32
5000 73.78 75.88 75.98
10000 74.40 75.99 76.15
20000 74.59 76.30 76.65
Table 5: Parsing results of our approach with different amount of parallel data on Google Universal
Treebanks version 1.0 and 2.0. We omit the results of Swedish for treebanks version 2.0 since the data
for Swedish from version 2.0 are exactly the same with those from version 1.0.
the dependency annotations off the training por-
tion of each treebank, and use that as the unla-
beled data for that target language. We train our
parsing model with different numbers of parallel
sentences to analyze the influence of the amount of
parallel data on the parsing performance of our ap-
proach. The parallel data sets contain 500, 1000,
2000, 5000, 10000 and 20000 parallel sentences,
respectively. We randomly extract parallel sen-
tences from each corpora, and smaller data sets are
subsets of larger ones. Table 2 shows the number
of tokens in the parallel data used in the experi-
ments.
4.2 System performance and comparison
on Google Universal Treebanks
For the comparison of parsing performance, we
run experiments on the following systems:
DTP: The direct transfer parser (DTP) proposed
by McDonald et al (2011), who train a delex-
icalized parser on English labeled training
data with no lexical features, then apply this
parser to parse target languages directly. It
is based on the transition-based dependency
parsing paradigm (Nivre, 2008). We di-
rectly cite the results reported in McDon-
ald et al (2013). In addition to their orig-
inal results, we also report results by re-
implementing the direct transfer parser based
on the first-order projective dependency pars-
ing model (McDonald et al, 2005a) (DTP?).
PTP The projected transfer parser (PTP) de-
scribed in McDonald et al (2011). The
results of the projected transfer parser re-
implemented by us is marked as ?PTP??.
-U: Our approach training on only parallel data
without unlabeled data for the target lan-
guage. The parallel data set for each language
contains 20,000 sentences.
+U: Our approach training on both parallel and
unlabeled data. The parallel data sets are the
ones contains 20,000 sentences.
OR: the supervised first-order projective depen-
dency parsing model (McDonald et al,
2005a), trained on the original treebanks with
maximum likelihood estimation (equation 6).
One may regard this system as an oracle of
transfer parsing.
Parsing accuracy is measured with unlabeled at-
tachment score (UAS): the percentage of words
with the correct head.
Table 3 and Table 4 shows the parsing results of
our approach, together with the results of the base-
line systems and the oracle, on version 1.0 and ver-
sion 2.0 of Google Universal Treebanks, respec-
tively. Our approaches significantly outperform all
the baseline systems across all the seven target lan-
guages. For the results on Google Universal Tree-
banks version 1.0, the improvement on average
over the projected transfer paper (PTP?) is 3.96%
1343
and up to 6.22% for Korean and 4.80% for Ger-
man. For the other three languages, the improve-
ments are remarkable, too ? 2.33% for French,
3.03% for Spanish and 3.40% for Swedish. By
adding entropy regularization from unlabeled data,
our full model achieves average improvement of
0.29% over the ?-U? setting. Moreover, our ap-
proach considerably bridges the gap to fully super-
vised dependency parsers, whose average UAS is
84.67%. For the results on treebanks version 2.0,
we can get similar observation and draw the same
conclusion.
4.3 Effect of the Amount of Parallel Text
Table 5 illustrates the UAS of our approach trained
on different amounts of parallel data, together
with the results of the projected transfer parser
re-implemented by us (PTP?). We run two ver-
sions of our approach for each of the parallel data
sets, one with unlabeled data (+U) and the other
without them (-U). From table 5 we can get three
observations. First, even the parsers trained with
only 500 parallel sentences achieve considerably
high parsing accuracies (average 70.10% for ver-
sion 1.0 and 71.59% for version 2.0). This demon-
strates that our approach does not rely on a large
amount of parallel data. Second, when gradually
increasing the amount of parallel data, the parsing
performance continues improving. Third, entropy
regularization with unlabeled data makes mod-
est improvement on parsing performance over the
parsers without unlabeled data. This proves the ef-
fectiveness of the entropy regularization from un-
labeled data.
4.4 Experiments on CoNLL Treebanks
To make a thorough empirical comparison with
previous studies, we also evaluate our system
without unlabeled data (-U) on treebanks from
CoNLL shared task on dependency parsing (Buch-
holz and Marsi, 2006; Nivre et al, 2007). To fa-
cilitate comparison, we use the same eight Indo-
European languages as target languages: Danish,
Dutch, German, Greek, Italian, Portuguese, Span-
ish and Swedish, and same experimental setup as
McDonald et al (2011). We report both the results
of the direct transfer and projected transfer parsers
directly cited from McDonald et al (2011) (DTP
and PTP) and re-implemented by us (DTP?and
PTP?).
Table 6 gives the results comparing the model
without unlabeled data (-U) presented in this work
DMV DTP DTP? PTP PTP? -U OR
da 33.4 45.9 46.8 48.2 50.0 50.1 87.1
de 18.0 47.2 46.0 50.9 52.4 57.3 87.0
el 39.9 63.9 62.9 66.8 65.3 67.4 82.3
es 28.5 53.3 54.4 55.8 59.9 60.3 83.6
it 43.1 57.7 59.9 60.8 63.4 64.0 83.9
nl 38.5 60.8 60.7 67.8 66.5 68.2 78.2
pt 20.1 69.2 71.1 71.3 74.8 75.1 87.2
sv 44.0 58.3 60.3 61.3 62.8 66.7 88.0
Ave 33.2 57.0 57.8 60.4 61.9 63.6 84.7
Table 6: Parsing results on treebanks from CoNLL
shared tasks for eight target languages. The results
of unsupervised DMV model are from Table 1 of
McDonald et al (2011).
to those five baseline systems and the oracle (OR).
The results of unsupervised DMV model (Klein
and Manning, 2004) are from Table 1 of McDon-
ald et al (2011). Our approach outperforms all
these baseline systems and achieves state-of-the-
art performance on all the eight languages.
In order to compare with more previous meth-
ods, we also report parsing performance on sen-
tences of length 10 or less after punctuation
has been removed. Table 7 shows the results
of our system and the results of baseline sys-
tems. ?USR?? is the weakly supervised system of
Naseem et al (2010). ?PGI? is the phylogenetic
grammar induction model of Berg-Kirkpatrick and
Klein (2010). Both the results of the two systems
are cited from Table 4 of McDonald et al (2011).
We also include the results of the unsupervised
dependency parsing model with non-parallel mul-
tilingual guidance (NMG) proposed by Cohen et
al. (2011)8, and ?PR? which is the posterior reg-
ularization approach presented in Gillenwater et
al. (2010). All the results are shown in Table 7.
From Table 7, we can see that among the eight
target languages, our approach achieves best pars-
ing performance on six languages ? Danish, Ger-
man, Greek, Italian, Portuguese and Swedish. It
should be noted that the ?NMG? system utilizes
more than one helper languages. So it is not di-
rectly comparable to our work.
4.5 Extensions
In this section, we briefly outline a few extensions
to our approach that we want to explore in future
work.
8For each language, we use the best result of the four sys-
tems in Table 3 of Cohen et al (2011)
1344
DTP DTP? PTP PTP? USR? PGI PR NMG -U
da 53.2 55.3 57.4 59.8 55.1 41.6 44.0 59.9 60.1
de 65.9 57.9 67.0 63.5 60.0 ? ? ? 67.5
el 73.9 70.8 73.9 72.3 60.3 ? ? 73.0 74.3
es 58.0 62.3 62.3 66.1 68.3 58.4 62.4 76.7 64.6
it 65.5 66.9 69.9 71.5 47.9 ? ? ? 73.6
nl 67.6 66.0 72.2 72.1 44.0 45.1 37.9 50.7 70.5
pt 77.9 79.2 80.6 82.9 70.9 63.0 47.8 79.8 83.3
sv 70.4 70.2 71.3 70.4 52.6 58.3 42.2 74.0 75.1
Ave 66.6 66.1 69.4 69.8 57.4 ? ? ? 71.1
Table 7: UAS on sentences of length 10 or less without punctuation from CoNLL shared task treebanks.
?USR?? is the weakly supervised system of Naseem et al (2010). ?PGI? is the phylogenetic grammar
induction model of Berg-Kirkpatrick and Klein (2010). Both the ?USR?? and ?PGI? systems are im-
plemented and reported by McDonald et al (2011). ?NMG? is the unsupervised dependency parsing
model with non-parallel multilingual guidance (Cohen et al, 2011). ?PR? is the posterior regularization
approach presented in Gillenwater et al (2010). Some systems? results for certain target languages are
not available as marked by ?.
4.5.1 Non-Projective Parsing
As mentioned in section 2.3, the runtime to com-
pute K
U
and its gradient is O(n4). One reasonable
speedup, as presented in Smith and Eisner (2007),
is to replace Shannon entropy with Re?nyi entropy.
The Re?nyi entropy is parameterized by ?:
R
?
(p) =
1
1 ? ?
log
(
?
y
p(y)
?
)
(16)
With Re?nyi entropy, the computation of K
U
and
its gradient is O(n3), even for non-projective case.
4.5.2 Higher-Order Models for Projective
Parsing
Our learning framework can be extended to
higher-order dependency parsing models. For ex-
ample, if we want to make our model capable of
utilizing more contextual information, we can ex-
tend our transferring weight to higher-order parts:
w?(p
t
,x
t
i
) =
{
w
E
(p
s
,x
s
i
), if pt align?? ps
w
E
(p
t
delex
,x
s
i
), otherwise
(17)
where p is a small part of tree y that has limited
interactions. For projective parsing, several al-
gorithms (McDonald and Pereira, 2006; Carreras,
2007; Koo and Collins, 2010; Ma and Zhao, 2012)
have been proposed to solve the model training
problems (calculation of objective function and
gradient) for different factorizations.
4.5.3 IGT Data
One possible direction to improve our approach
is to replace parallel text with Interlinear Glossed
Text (IGT) (Lewis and Xia, 2010), which is a
semi-structured data type encoding more syntactic
information than parallel data. By using IGT Data,
not only can we obtain more accurate word align-
ments, but also extract useful cross-lingual infor-
mation for the resource-poor language.
5 Conclusion
In this paper, we propose an unsupervised pro-
jective dependency parsing approach for resource-
poor languages, using existing resources from a
resource-rich source language. By presenting a
model training framework, our approach can uti-
lize parallel text to estimate transferring distribu-
tion with the help of a well-developed resource-
rich language dependency parser, and use unla-
beled data as entropy regularization. The exper-
imental results on three data sets across ten target
languages show that our approach achieves signif-
icant improvement over previous studies.
Acknowledgements
This material is based upon work supported by
the National Science Foundation under Grant No.
BCS-0748919. Any opinions, findings, and con-
clusions or recommendations expressed in this
material are those of the authors and do not nec-
essarily reflect the views of the National Science
Foundation.
1345
References
Steven Abney. 2004. Understanding the Yarowsky al-
gorithm. Computational Linguistics, 30:2004.
James K. Baker. 1979. Trainable grammars for speech
recognition. In Proceedings of 97th meeting of the
Acoustical Society of America, pages 547?550.
Taylor Berg-Kirkpatrick and Dan Klein. 2010. Phylo-
genetic grammar induction. In Proceedings of ACL-
2010, pages 1288?1297, Uppsala, Sweden, July.
Phil Blunsom and Trevor Cohn. 2010. Unsupervised
induction of tree substitution grammars for depen-
dency parsing. In Proceedings of EMNLP-2010,
pages 1204?1213, Cambridge, MA, October.
Matthew Brand. 1998. Structure learning in con-
ditional probability models via an entropic prior
and parameter extinction. Neural Computation,
11(5):1155?1182.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceeding of CoNLL-2006, pages 149?164, New
York, NY.
David Burkett and Dan Klein. 2008. Two languages
are better than one (for syntactic parsing). In Pro-
ceedings of EMNLP-2008, pages 877?886, Hon-
olulu, Hawaii, October.
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In Proceed-
ings of the CoNLL Shared Task Session of EMNLP-
CONLL, pages 957?961.
Glenn Carroll and Eugene Charniak. 1992. Two
experiments on learning probabilistic dependency
grammars from corpora. In Proceedings of Work-
ing Notes of the Workshop Statistically-Based NLP
Techniques.
Wenliang Chen, Jun?ichi Kazama, and Kentaro Tori-
sawa. 2010. Bitext dependency parsing with bilin-
gual subtree constraints. In Proceedings of ACL-
2010, pages 21?29, Uppsala, Sweden, July.
Shay Cohen and Noah A. Smith. 2009. Shared lo-
gistic normal distributions for soft parameter tying
in unsupervised grammar induction. In Proceedings
of NAACL/HLT-2009, pages 74?82, Boulder, Col-
orado, June.
Shay B. Cohen, Dipanjan Das, and Noah A. Smith.
2011. Unsupervised structure prediction with non-
parallel multilingual guidance. In Proceedings of
EMNLP-2011, pages 50?61, Edinburgh, Scotland,
UK., July.
Dipanjan Das and Slav Petrov. 2011. Unsuper-
vised part-of-speech tagging with bilingual graph-
based projections. In Proceedings of ACL/HLT-
2011, pages 600?609, Portland, Oregon, USA, June.
Kuzman Ganchev, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction
via bitext projection constraints. In Proceedings of
ACL/AFNLP-2009, pages 369?377, Suntec, Singa-
pore, August.
Jennifer Gillenwater, Kuzman Ganchev, Joa?o Grac?a,
Fernando Pereira, and Ben Taskar. 2010. Sparsity in
dependency grammar induction. In Proceedings of
the ACL 2010 Conference Short Papers, pages 194?
199, Uppsala, Sweden, July.
Joao V. Graca, Lf Inesc-id, Kuzman Ganchev, and Ben
Taskar. 2007. Expectation maximization and pos-
terior constraints. In Advances in NIPS, pages 569?
576.
Yves Grandvalet and Yoshua Bengio. 2004. Semi-
supervised learning by entropy minimization. In Ad-
vances in Neural Information Processing Systems.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of EMNLP-2009, pages
1222?1231, Singapore, August.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering, 11:11?311.
Feng Jiao, Shaojun Wang, Chi-Hoon Lee, Russell
Greiner, and Dale Schuurmans. 2006. Semi-
supervised conditional random fields for improved
sequence segmentation and labeling. In Proceed-
ings of COLING/ACL-2006, pages 209?216, Syd-
ney, Australia, July.
Jason Katz-Brown, Slav Petrov, Ryan McDon-
ald, Franz Och, David Talbot, Hiroshi Ichikawa,
Masakazu Seno, and Hideto Kazawa. 2011. Train-
ing a parser for machine translation reordering. In
Proceedings of EMNLP-2011, pages 183?192, Ed-
inburgh, Scotland, UK., July.
Dan Klein and Christopher Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of ACL-
2004, pages 478?485, Barcelona, Spain, July.
Philipp Koehn. 2005. Europarl: A Parallel Corpus
for Statistical Machine Translation. In Conference
Proceedings: the tenth Machine Translation Sum-
mit, pages 79?86, Phuket, Thailand. AAMT, AAMT.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of ACL-
2010, pages 1?11, Uppsala, Sweden, July.
Terry Koo, Amir Globerson, Xavier Carreras, and
Michael Collins. 2007. Structured predicition mod-
els via the matrix-tree theorem. In Proceedings
of EMNLP-CONLL 2007, pages 141?150, Prague,
Czech, June.
1346
William D. Lewis and Fei Xia. 2010. Developing
odin: A multilingual repository of annotated lan-
guage data for hundreds of the world?s languages.
LLC, 25(3):303?319.
Xuezhe Ma and Hai Zhao. 2012. Fourth-order depen-
dency parsing. In Proceedings of COLING 2012:
Posters, pages 785?796, Mumbai, India, December.
Gideon S. Mann and Andrew McCallum. 2007. Ef-
ficient computation of entropy gradient for semi-
supervised conditional random fields. In Proceed-
ings of NAACL/HLT-2007, pages 109?112, Strouds-
burg, PA, USA.
David Marec?ek and Milan Straka. 2013. Stop-
probability estimates computed on a large corpus
improve unsupervised dependency parsing. In Pro-
ceedings of ACL-2013, pages 281?290, Sofia, Bul-
garia, August.
Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proceedings of EACL-2006, pages 81?88,
Trento, Italy, April.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005a. Online large-margin training of
dependency parsers. In Proceedings of ACL-2005,
pages 91?98, Ann Arbor, Michigan, USA, June 25-
30.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005b. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of HLT/EMNLP-2005, pages 523?530, Vancouver,
Canada, October.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of EMNLP-2011, pages 62?
72, Edinburgh, Scotland, UK., July.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuz-
man Ganchev, Keith Hall, Slav Petrov, Hao
Zhang, Oscar Ta?ckstro?m, Claudia Bedini, Nu?ria
Bertomeu Castello?, and Jungmee Lee. 2013. Uni-
versal dependency annotation for multilingual pars-
ing. In Proceedings of ACL-2013, pages 92?97,
Sofia, Bulgaria, August.
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using universal linguistic knowl-
edge to guide grammar induction. In Proceedings of
EMNLP-2010, pages 1234?1244, Cambridge, MA,
October.
Stephen G. Nash and Jorge Nocedal. 1991. A numer-
ical study of the limited memory bfgs method and
truncated-newton method for large scale optimiza-
tion. SIAM Journal on Optimization, 1(2):358?372.
Truc-Vien T. Nguyen, Alessandro Moschitti, and
Giuseppe Riccardi. 2009. Convolution kernels on
constituent, dependency and sequential structures
for relation extraction. In Proceedings of EMNLP-
2009, pages 1378?1387, Singapore, August.
Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of English text. In Proceedings
of COLING-2004, pages 64?70, Geneva, Switzer-
land, August 23-27.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The conll 2007 shared task on depen-
dency parsing. In Proceeding of EMNLP-CoNLL
2007, pages 915?932, Prague, Czech.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Comput. Linguist.,
34(4):513?553, December.
Mark A. Paskin. 2001. Cubic-time parsing and
learning algorithms for grammatical bigram models.
Technical Report, UCB/CSD-01-1148.
Slav Petrov, Dipanjan Das, and Ryan T. McDonald.
2011. A universal part-of-speech tagset. CoRR,
abs/1104.2086.
Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.
2002. Automatic paraphrase acquisition from news
articles. In Proceeding of HLT-2002, pages 313?
318.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of ACL-2005, pages 354?362,
Ann Arbor, Michigan, June.
David A. Smith and Jason Eisner. 2007. Bootstrapping
feature-rich dependency parsers with entropic pri-
ors. In Proceedings of EMNLP/CoNLL-2007, pages
667?677, Prague, Czech Republic, June.
David A. Smith and Noah A. Smith. 2004. Bilin-
gual parsing with factored estimation: Using En-
glish to parse Korean. In Proceedings of EMNLP-
2004, pages 49?56.
David A. Smith and Noah A. Smith. 2007. Probabilis-
tic models of nonporjective dependency trees. In
Proceedings of EMNLP-CONLL 2007, pages 132?
140, Prague, Czech, June.
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Ju-
rafsky. 2010. From baby steps to leapfrog: How
?less is more? in unsupervised dependency parsing.
In Proceedings of NAACL/HLT-2010, pages 751?
759, Los Angeles, California, June.
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Ju-
rafsky. 2013. Breaking out of local optima with
count transforms and model recombination: A study
in grammar induction. In Proceedings of EMNLP-
2013, pages 1983?1995, Seattle, Washington, USA,
October.
1347
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of NAACL/HLT-2003, pages 252?
259.
Jun Xie, Haitao Mi, and Qun Liu. 2011. A novel
dependency-to-string model for statistical machine
translation. In Proceedings of EMNLP-2011, pages
216?226, Edinburgh, Scotland, UK., July.
Hao Zhang, Liang Huang, Kai Zhao, and Ryan Mc-
Donald. 2013. Online learning for inexact hy-
pergraph search. In Proceedings of EMNLP-2013,
pages 908?913, Seattle, Washington, USA, October.
1348
Dependency Parser for Chinese Constituent Parsing ?
Xuezhe Ma, Xiaotian Zhang, Hai Zhao, Bao-Liang Lu
1Center for Brain-Like Computing and Machine Intelligence
Department of Computer Science and Engineering, Shanghai Jiao Tong University
2MOE-Microsoft Key Laboratory for Intelligent Computing and Intelligent Systems
Shanghai Jiao Tong University, 800 Dong Chuan Rd., Shanghai 200240, China
{xuezhe.ma,xtian.zh}@gmail.com, {zhaohai,blu}@cs.sjtu.edu.cn
Abstract
This paper presents our work for participation
in the 2010 CIPS-ParsEval shared task on Chi-
nese syntactic constituent tree parsing. We
use dependency parsers for this constituent
parsing task based on a formal dependency-
constituent transformation method which con-
verts dependency to constituent structures us-
ing a machine learning approach. A condi-
tional random fields (CRF) tagger is adopted
for head information recognition. Our ex-
periments shows that acceptable parsing and
head tagging results are obtained on our ap-
proaches.
1 Introduction
Constituent parsing is a challenging but useful task
aiming at analyzing the constituent structure of a sen-
tence. Recently, it is widely adopted by the popular ap-
plications of natural language processing techniques,
such as machine translation (Ding and Palmer, 2005),
synonym generation (Shinyama et al, 2002), relation
extraction (Culotta and Sorensen, 2004) and lexical re-
source augmentation (Snow et al, 2004). A great deal
of researches have been conducted on this topic with
promising progress (Magerman, 1995; Collins, 1999;
Charniak, 2000; Charniak and Johnson, 2005; Sagae
and Lavie, 2006; Petrov and Klein, 2007; Finkel et al,
2008; Huang, 2008).
Recently, several effective dependency parsing al-
gorithms has been developed and shows excellent per-
formance in the responding parsing tasks (McDonald,
2006; Nivre and Scholz, 2004). Since graph struc-
tures of dependency and constituent parsing over a
sentence are strongly related, they should be benefited
from each other. It is true that constituent parsing may
be smoothly altered to fit dependency parsing. How-
ever, due to the inconvenience from dependency to
constituent structure, it is not so easy to adopt the latter
?This work is partially supported by the National Natu-
ral Science Foundation of China (Grant No. 60903119, Grant
No. 60773090 and Grant No. 90820018), the National Ba-
sic Research Program of China (Grant No. 2009CB320901),
and the National High-Tech Research Program of China
(Grant No.2008AA02Z315).
for the former. This means that most of these popular
and effective dependency parsing models can not be di-
rectly extended to constituents parsing. This paper pro-
poses an formal method for such a conversion which
adoptively solves the problem of ambiguity. Based on
the proposed method, a dependency parsing algorithm
can be used to solve tasks of constituent parsing.
A part of Tsinghua Chinese Treebank (TCT) (Zhou,
2004; Zhou, 2007; Chen et al, 2008) is used as
the training and test data for the 2010 CIPS-ParsEval
shared task. Being different from the annotation
scheme of the Penn Chinese Treebank (CTB), the TCT
has another annotation scheme, which combines both
the constituent tree structure and the head informa-
tion of each constituent. Specifically, there can be al-
ways multiple heads in a constituent. For the 2010
CIPS-ParsEval shared task, only segmented sentences
are given in test data without part-of-speech (POS)
tags, a POS tagger is required for this task. There-
fore, we divide our system into three major cascade
stages, namely POS tagging, constituent parsing and
head information recognition, which are connected as
a pipeline of processing. For the POS tagging, we
adopt the SVMTool tagger (Gimenez and Marquez,
2004); for the constituent parsing, we use the Maxi-
mum Spanning Tree (MST) (McDonald, 2006) parser
combined with a dependencies-to-constituents conver-
sion; and for the head information recognition, we ap-
ply a sequence labeling method to label head informa-
tion.
Section 2 presents the POS tagger in our approach.
The details of our parsing method is presented in sec-
tion 3. The head information recognition is described
in section 4. The data and experimental results are
shown in section 5. The last section is the conclusion
and future work.
2 POS Tagging
The SVMTool tagger (Gimenez and Marquez, 2004) is
used as our POS tagging tool for the first stage. It is a
POS tagger based on SVM classifier, written in Perl. It
can be trained on standardized collection of hand POS-
tagged sentences. It uses SVM-Light1 toolkit as the
1http://www.cs.cornell.edu/People/tj/
svm_light/.
implementation of SVM classifier and achieves 97.2%
accuracy on the Penn English Treebank. We test the
accuracy of the SVMTool tagger on the development
set of the TCT (see section 5.1) and achieve accuracy
of 94.98%.
3 Parsing Constituents Using
Dependency Parsing Algorithms
3.1 Convert Dependencies to Constituents
The conversion from constituent to dependency struc-
tures is straightforward with some specific rules based
on linguistic theory. However, there is not an effective
method which can accurately accomplish the opposite
transformation, from the dependency structures back
into constituent ones due to the existence of ambiguity
introduced by the former transformation.
Aimed at the above difficulty, our solution is to in-
troduce a formal dependency structure and a machine
learning method so that the ambiguity from depen-
dency structures to constituent structures can be dealt
with automatically.
3.1.1 Binarization
We first transform constituent trees into the form
that all productions for all subtrees are either unary or
binary, before converting them to dependency struc-
tures. Due to the binarization, the target constituent
trees of the conversion from dependency back to con-
stituent structures are binary branching.
This binarization is done by the left-factoring ap-
proach described in (Charniak et al, 1998; Petrov and
Klein, 2008), which converts each production with n
children, where n > 2, into n? 1 binary productions.
Additional non-terminal nodes introduced in this con-
version must be clearly marked. Transforming the bi-
nary branching trees into arbitrary branching trees is
accomplished by using the reverse process.
3.1.2 Using Binary Classifier
We train a classifier to decide which dependency
edges should be transformed first at each step of con-
version automatically. After the binarization described
in the previous section, only one dependency edge
should be transformed at each step. Therefore the
classifier only need to decide which dependency edge
should be transformed at each step during the conver-
sion.
As a result of the projective property of constituent
structures, this problem only happens in the cases that
modifiers are at both sides of their heads. And for these
cases that one head has multiple modifiers, only the
leftmost or the rightmost dependency edge could be
transformed first. Therefore, a binary classifier is al-
ways enough for the disambiguation at each step.
1. Word form of the parent
2. Part-of-speech (POS) tag of the parent
3. Word form of the leftmost child
4. POS tag of the leftmost child
5. Dependency label of the leftmost child
6. Word form of the rightmost child
7. POS tag of the rightmost child
8. Dependency label of the rightmost child
9. Distance between the leftmost child and
the parent
10. Distance between the rightmost child
and the parent
Table 1: Features used for conversion classifier.
Support Vector Machine (SVM) is adopted as the
learning algorithm for the binary classifier and the fea-
tures are in Table 1.
3.1.3 Convert Constituent Labels
The rest problem is that we should restore the label
for each constituent when dependency structure trees
are again converted to constituent structures. The prob-
lem is solved by storing constituent labels as labels of
dependency types. The label for each constituent is just
used as the label dependency type for each dependency
edge.
The conversion method is tested on the develop-
ment, too. Constituent trees are firstly converted into
dependency structures using the head rules described
in (Li and Zhou, 2009). Then, we transform those
trees back to constituent structure using our conversion
method and use the PARSEVAL (Black et al, 1991)
measures to evaluate the performance of the conver-
sion method. Our conversion method obtains 99.76%
precision and 99.76% recall, which is a great perfor-
mance.
3.2 Dependency Parser for Constituent Parsing
Based on the proposed conversion method, depen-
dency parsing algorithms can be used for constituent
parsing. This can be done by firstly transforming train-
ing data from constituents into dependencies and ex-
tract training instances to train a binary classifier for
dependency-constituent conversion, then training a de-
pendency parser using the transformed training data.
On the test step, parse the test data using the depen-
dency parser and convert output dependencies to con-
stituents using the binary classifier trained in advance.
In addition, since our conversion method needs depen-
dency types, labeled dependency parsing algorithms
are always required.
1. Constituent label of the constituent
2. Constituent label of each child of
the constituent.
3. Wether it is a terminal for each
child of the constituent
4. The leftmost word in the sentence
of each child of the constituent.
5. The leftmost word in the sentence
of each child of the constituent.
Table 2: CRF features for head information recogni-
tion.
1. Word form and POS tag of the parent.
2. Word form and POS tag of each child.
3. POS tag of the leftmost child of each child.
4. POS tag of the rightmost child of each child.
5. Dependency label between the parent and
its parent
Table 3: CRF features for dependency type labeling.
4 Head Information Recognition
Since head information of each constituent is always
determined by the syntactic label of its own and the
categories of the constituents in subtrees, the order and
relations between the productions of each constituent
strongly affects the head information labeling. It is
natural to apply a sequential labeling strategy to tackle
this problem. The linear chain CRF model is adopted
for the head information labeling, and the implemen-
tation of CRF model we used is the 0.53 version of
the CRF++ toolkit2. We assume that head information
is independent between different constituents, which
could decrease the length of sequence to be labeled for
the CRF model.
We use a binary tag set to determine whether a con-
stituent is a head, e.g. H for a head, O for a non-head,
which is the same as (Song and Kit, 2009). The fea-
tures in Table 2 are used for CRF model.
To test our CRF tagger, we remove all head informa-
tion from the development set, and use the CRF tagger
to retrieve the head. The result strongly proves its ef-
fectiveness by showing an accuracy of 99.52%.
5 Experiments
All experiments reported here were performed on a
Core 2 Quad 2.83Ghz CPU with 8GB of RAM.
2The CRF++ toolkit is publicly available from
http://crfpp.sourceforge.net/.
5.1 Data
There are 37,219 short sentences in official released
training data for the first sub-task and 17,744 long sen-
tences for the second sub-task (for the second sub-task,
one line in the training data set may contain more than
one sentence). We split one eighth of the data as our
development set. On the other hand, there are both
1,000 sentences in released test data for the first and
second sub-tasks.
5.2 Constituent Parsing
As mentioned in section 3, constituent parsing is
done by using a dependency parser combined with
our conversion method. We choose the second or-
der maximum spanning tree parser with k-best online
large-margin learning algorithm (Crammer and Singer,
2003; Crammer et al, 2003). The MST parser we use
is in the form of an open source program implemented
in C++3.
The features used for MST parser is the same as
(McDonald, 2006). Both the single-stage and two-
stage dependency type labeling approaches are applied
in our experiments. For the two-stage dependency type
labeling, The linear chain CRF model is adopted in-
stead of the first-order Markov model used in (McDon-
ald, 2006). The features in Table 3 are used for CRF
model. It takes about 7 hours for training the MST
parser, and about 24 hours for training the CRF model.
As mentioned in section 3.1.2, SVM is adopted as
the learning algorithm for the binary classifier. There
are about 40,000 training instances in the first sub-task
and about 80,000 in the second sub-task. Develop-
ment sets are used for tuning parameter C of SVM
and the training time of the SVM classifier for the first
and second sub-task is about 8 and 24 hours, respec-
tively. However, the conversion from dependencies to
constituents is extremely fast. Converting more than
2,000 trees takes less than 1 second.
To transform the constituent trees in training set into
dependency structures, we use the head rules of (Li and
Zhou, 2009).
5.3 Results
The evaluation metrics used in 2010 CIPS-ParsEval
shared task is shown in following:
1. syntactic parsing
Precision = number of correct constituents in proposed parsenumber of constituents in proposed parse
Recall = number of correct constituents in proposed parsenumber of constituents in standard parse
F1 = 2*Precision*RecallPrecision+Recall
3The Max-MSTParser is publicly available from
http://max-mstparser.sourceforge.net/.
without head with head
Precision Recall F1 Precision Recall F1
single-stage 77.78 78.13 77.96 75.78 76.13 75.95
two-stage 78.61 78.76 78.69 76.61 76.75 76.68
Table 4: Official scores of syntactic parsing. single-stage and two-stage are for single-stage and two-stage depen-
dency type labeling approached, respectively.
Micro-R Macro-R
single-stage 62.74 62.47
two-stage 63.14 62.48
Table 5: Official scores of event recognition
The correctness of syntactic constituents is judged
based on the following two criteria:
(a) the boundary, the POS tags of all the words
in the constituent and the constituent type la-
bel should match that of the constituent in
the gold standard data.
(b) the boundary, the POS tags of all the words
in the constituent, the constituent type la-
bel and head child index of the constituent
should match that of the constituent in the
gold standard data. (if the constituent con-
tains more than one head child index, at least
one of them should be correct.)
2. event pattern recognition
Micro-R = number of all correct events in proposed parsenumber of all events in standard parse
Macro-R = sum of recall of different target verbsnumber of target verbs
Here the event pattern of a sentence is defined to
be the sequence of event blocks controlled by the
target verb in a sentence. The criteria for judging
the correctness of event pattern recognition is:
? the event pattern should be completely con-
sistent with gold standard data (information
of each event block should completely match
and the order of event blocks should also
consistent).
There are both two submissions for the first and sec-
ond sub-tasks. One is using the single-stage depen-
dency type labeling and the other is two-stage. Since
there are some mistakes in our models for the second
sub-task, the results of our submissions are unexpect-
edly poor and are not shown in this paper. All the re-
sults in this paper is reported by the official organizer
of the 2010 CIPS-ParsEval shared task.
The accuracy of POS tagging on the official test data
is 92.77%. The results of syntactic parsing for the first
sub-task is shown in Table 4. And results of event
recognition is shown in Table 5.
From the Table 4 and 5, we can see that our system
achieves acceptable parsing and head tagging results,
and the results of event recognition is also reasonably
high.
5.4 Comparison with Previous Works
We comparison our approach with previous works of
2009 CIPS-ParsEval shared task. The data set and
evaluation measures of 2009 CIPS-ParsEval shared
task, which are quite different from that of 2010 CIPS-
ParsEval shared task, are used in this experiment for
the comparison purpose. Table 6 shows the compari-
son.
We compare our method with several main parsers
on the official data set of 2009 CIPS-ParsEval shared
task. All these results are evaluated with official
evaluation tool by the 2009 CIPS-ParsEval shared
task. Bikel?s parser4 (Bikel, 2004) in Table 6 is
a implementation of Collins? head-driven statistical
model (Collins, 2003). The Stanford parser5 is based
on the factored model described in (Klein and Man-
ning, 2002). The Charniak?s parser6 is based on the
parsing model described in (Charniak, 2000). Berke-
ley parser7 is based on unlexicalized parsing model de-
scribed in (Petrov and Klein, 2007). According to Ta-
ble 6, the performance of our method is better than all
the four parsers described above. Chen et al (2009)
and Jiang et al (2009) both make use of combination
of multiple parsers and achieve considerably high per-
formance.
4http://www.cis.upenn.edu/?dbikel/
software.html
5http://nlp.stanford.edu/software/
lex-parser.shtml/
6ftp://ftp.cs.brown.edu/pub/nlparser/
7http://nlp.cs.berkeley.edu/Main.html
F1
Bikel?s parser 81.8
Stanford parser 83.3
Charniak?s parser 83.9
Berkeley parser 85.2
this paper 85.6
Jiang et al(2009). 87.2
Chen et al(2009). 88.8
Table 6: Comparison with previous works
6 Conclusion
This paper describes our approaches for the parsing
task in CIPS-ParsEval 2010 shared task. A pipeline
system is used to solve the POS tagging, constituent
parsing and head information recognition. SVMTool
tagger is used for the POS tagging. For constituent
parsing, we proposes a conversion based method,
which can use dependency parsers for constituent pars-
ing. MST parser is chosen as our dependency parser.
A CRF tagger is used for head information recognition.
The official scores indicate that our system obtains ac-
ceptable results on constituent parsing and high perfor-
mance on head information tagging.
One of future work should apply parser combination
and reranking approaches to leverage this in producing
more accurate parsers.
References
Bikel, Daniel M. 2004. Intricacies of collins parsing
model. Computational Linguistics, 30(4):480?511.
Black, Ezra W., Steven P. Abney, Daniel P. Flickinger,
Cluadia Gdaniec, Ralph Grishman, Philio Harrison,
Donald Hindle, Robert J.P. Inqria, Frederick Jelinek,
Judith L. Klavans, Mark Y. Liberman, Mitchell P.
Marcus, Salim Roukos, and B Santorini. 1991. A
procedure for quantitatively comparing the syntac-
tic coverage of english grammars. In Proceedings
of the February 1991 DARPA Speech and Natural
Language Workshop.
Charniak, Eugene and Mark Johnson. 2005. Coarse-
to-fine-grained n-best parsing and discriminative
reranking. In Proceedings of the 43rd ACLL, pages
132?139.
Charniak, Eugene, Sharon Goldwater, and Mark John-
son. 1998. Edge-based best-first chart parsing. In
Proceedings of the Sixth Workshop on Very Large
Corpora.
Charniak, Eugene. 2000. A maximum-entropy-
inspired parser. In Proceedings of NAACL, pages
132?139, seattle, WA.
Chen, Yi, Qiang Zhou, and Hang Yu. 2008. Anal-
ysis of the hierarchical Chinese funcitional chunk
bank. Journal of Chinese Information Processing,
22(3):24?31.
Chen, Xiao, Changning Huang, Mu Li, and Chunyu
Kit. 2009. Better parser combination. In CIPS-
ParsEval-2009 shared task.
Collins, Michael. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Collins, Michael. 2003. Head-driven statistical mod-
els for natural language parsing. Computational
Linguistics, 29(4):589?637.
Crammer, Koby and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learining.
Crammer, Koby, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2003. Online
passive aggressive algorithms. In Proceedings of
NIPS.
Culotta, Aron and Jeffrey Sorensen. 2004. Depen-
dency tree kernels for relation extraction. In Pro-
ceedings of ACL.
Ding, Yuan and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency
insertion grammars. In Proceedings of ACL.
Finkel, Jenny Rose, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, condi-
tional random field parsing. pages 959?967, The
Ohio State University, Columbus, Ohio, USA.
Gimenez and Marquez. 2004. Svmtool: A general
POS tagger generator based on support vector ma-
chines. In Proceedings of the 4th International Con-
ference of Language Resources and Evaluation, Lis-
bon, Portugal.
Huang, Liang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL/HLT.
Jiang, Wenbin, Hao Xiong, and Qun Liu. 2009. Muti-
path shift-reduce parsing with online training. In
CIPS-ParsEval-2009 shared task.
Klein, Dan and Christopher Manning. 2002. Fast ex-
act inference with a factored model for natural lan-
guage parsing. In In Advances in NIPS 2002, pages
3?10.
Li, Junhui and Guodong Zhou. 2009. Soochow uni-
versity report for the 1st china workshop on syntac-
tic parsing. In CIPS-ParsEval-2009 shared task.
Magerman, David M. 1995. Statistical decision-tree
models for parsing. In Proceedings of ACL, pages
276?283, MIT, Cambridge, Massachusetts, USA.
McDonald, Ryan. 2006. Discriminative Learning
Spanning Tree Algorithm for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
Nivre, Joakim and Mario Scholz. 2004. Determinis-
tic dependency parsing of english text. In Proceed-
ings of the 20th international conference on Compu-
tational Linguistics (COLING-2004), pages 64?70,
Geneva, Switzerland, August 23rd-27th.
Petrov, Slav and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Proceedings of
HLT/NAACL, pages 404?411, Rochester, New York.
Petrov, Slav and Dan Klein. 2008. Discriminative log-
linear grammars with latent variables. In Proceed-
ings of NIPS 20.
Sagae, Kenji and Alon Lavie. 2006. A best-first prob-
abilistic shift-reduce parser. In Proceedings of COL-
ING/ACL, pages 689?691, Sydney, Australia.
Shinyama, Yusuke, Satoshi Sekine, and Kiyoshi Sudo.
2002. Automatic paraphrase acquisition from news
articles. In HLT-2002.
Snow, Rion, Daniel Jurafsky, and Andrew Y. Ng.
2004. Learning syntactic patterns for automatic hy-
pernym discovery. In Proceedings of NIPS.
Song, Yan and Chunyu Kit. 2009. PCFG parsing with
crf tagging for head recognition. In CIPS-ParsEval-
2009 shared task.
Zhou, Qiang. 2004. Annotation scheme for Chinese
treebank. Journal of Chinese Information Process-
ing, 18(4):1?8.
Zhou, Qiang. 2007. Base chuck scheme for the Chi-
nese language. Journal of Chinese Information Pro-
cessing, 21(3):21?27.
