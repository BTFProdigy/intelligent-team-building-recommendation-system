Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 968?977,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Supervised Models for Coreference Resolution
Altaf Rahman and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{altaf,vince}@hlt.utdallas.edu
Abstract
Traditional learning-based coreference re-
solvers operate by training a mention-
pair classifier for determining whether two
mentions are coreferent or not. Two in-
dependent lines of recent research have
attempted to improve these mention-pair
classifiers, one by learning a mention-
ranking model to rank preceding men-
tions for a given anaphor, and the other
by training an entity-mention classifier
to determine whether a preceding clus-
ter is coreferent with a given mention.
We propose a cluster-ranking approach to
coreference resolution that combines the
strengths of mention rankers and entity-
mention models. We additionally show
how our cluster-ranking framework natu-
rally allows discourse-new entity detection
to be learned jointly with coreference res-
olution. Experimental results on the ACE
data sets demonstrate its superior perfor-
mance to competing approaches.
1 Introduction
Noun phrase (NP) coreference resolution is the
task of identifying which NPs (or mentions) re-
fer to the same real-world entity or concept. Tra-
ditional learning-based coreference resolvers op-
erate by training a model for classifying whether
two mentions are co-referring or not (e.g., Soon
et al (2001), Ng and Cardie (2002b), Kehler et al
(2004), Ponzetto and Strube (2006)). Despite their
initial successes, these mention-pair models have
at least two major weaknesses. First, since each
candidate antecedent for a mention to be resolved
(henceforth an active mention) is considered inde-
pendently of the others, these models only deter-
mine how good a candidate antecedent is relative
to the active mention, but not how good a candi-
date antecedent is relative to other candidates. In
other words, they fail to answer the critical ques-
tion of which candidate antecedent is most prob-
able. Second, they have limitations in their ex-
pressiveness: the information extracted from the
two mentions alone may not be sufficient for mak-
ing an informed coreference decision, especially if
the candidate antecedent is a pronoun (which is se-
mantically empty) or a mention that lacks descrip-
tive information such as gender (e.g., Clinton).
To address the first weakness, researchers have
attempted to train a mention-ranking model for
determining which candidate antecedent is most
probable given an active mention (e.g., Denis and
Baldridge (2008)). Ranking is arguably a more
natural reformulation of coreference resolution
than classification, as a ranker allows all candidate
antecedents to be considered simultaneously and
therefore directly captures the competition among
them. Another desirable consequence is that there
exists a natural resolution strategy for a ranking
approach: a mention is resolved to the candidate
antecedent that has the highest rank. This con-
trasts with classification-based approaches, where
many clustering algorithms have been employed
to co-ordinate the pairwise coreference decisions
(because it is unclear which one is the best).
To address the second weakness, researchers
have investigated the acquisition of entity-mention
coreference models (e.g., Luo et al (2004), Yang
et al (2004)). Unlike mention-pair models, these
entity-mention models are trained to determine
whether an active mention belongs to a preced-
ing, possibly partially-formed, coreference cluster.
Hence, they can employ cluster-level features (i.e.,
features that are defined over any subset of men-
tions in a preceding cluster), which makes them
more expressive than mention-pair models.
Motivated in part by these recently developed
models, we propose in this paper a cluster-
ranking approach to coreference resolution that
combines the strengths of mention-ranking mod-
968
els and entity-mention models. Specifically, we
recast coreference as the problem of determining
which of a set of preceding coreference clusters
is the best to link to an active mention using a
learned cluster ranker. In addition, we show how
discourse-new detection (i.e., the task of determin-
ing whether a mention introduces a new entity in
a discourse) can be learned jointly with corefer-
ence resolution in our cluster-ranking framework.
It is worth noting that researchers typically adopt
a pipeline coreference architecture, performing
discourse-new detection prior to coreference res-
olution and using the resulting information to pre-
vent a coreference system from resolving men-
tions that are determined to be discourse-new (see
Poesio et al (2004) for an overview). As a re-
sult, errors in discourse-new detection could be
propagated to the resolver, possibly leading to a
deterioration of coreference performance (see Ng
and Cardie (2002a)). Jointly learning discourse-
new detection and coreference resolution can po-
tentially address this error-propagation problem.
In sum, we believe our work makes three main
contributions to coreference resolution:
Proposing a simple, yet effective coreference
model. Our work advances the state-of-the-art
in coreference resolution by bringing learning-
based coreference systems to the next level of
performance. When evaluated on the ACE 2005
coreference data sets, cluster rankers outperform
three competing models ? mention-pair, entity-
mention, and mention-ranking models ? by a
large margin. Also, our joint-learning approach
to discourse-new detection and coreference reso-
lution consistently yields cluster rankers that out-
perform those adopting the pipeline architecture.
Equally importantly, cluster rankers are conceptu-
ally simple and easy to implement and do not rely
on sophisticated training and inference procedures
to make coreference decisions in dependent rela-
tion to each other, unlike relational coreference
models (see McCallum and Wellner (2004)).
Bridging the gap between machine-learning
approaches and linguistically-motivated ap-
proaches to coreference resolution. While ma-
chine learning approaches to coreference resolu-
tion have received a lot of attention since the mid-
90s, popular learning-based coreference frame-
works such as the mention-pair model are ar-
guably rather unsatisfactory from a linguistic point
of view. In particular, they have not leveraged
advances in discourse-based anaphora resolution
research in the 70s and 80s. Our work bridges
this gap by realizing in a new machine learn-
ing framework ideas rooted in Lappin and Leass?s
(1994) heuristic-based pronoun resolver, which in
turn was motivated by classic salience-based ap-
proaches to anaphora resolution.
Revealing the importance of adopting the right
model. While entity-mention models have pre-
viously been shown to be worse or at best
marginally better than their mention-pair counter-
parts (Luo et al, 2004; Yang et al, 2008), our
cluster-ranking models, which are a natural exten-
sion of entity-mention models, significantly out-
performed all competing approaches. This sug-
gests that the use of an appropriate learning frame-
work can bring us a long way towards high-
performance coreference resolution.
The rest of the paper is structured as follows.
Section 2 discusses related work. Section 3 de-
scribes our baseline coreference models: mention-
pair, entity-mention, and mention-ranking. We
discuss our cluster-ranking approach in Section 4,
evaluate it in Section 5, and conclude in Section 6.
2 Related Work
Heuristic-based cluster ranking. As men-
tioned previously, the work most related to ours is
Lappin and Leass (1994), whose goal is to perform
pronoun resolution by assigning an anaphoric pro-
noun to the highest-scored preceding cluster. Nev-
ertheless, Lappin and Leass?s work differs from
ours in several respects. First, they only tackle
pronoun resolution rather than the full coreference
task. Second, their algorithm is heuristic-based; in
particular, the score assigned to a preceding clus-
ter is computed by summing over the weights as-
sociated with the factors applicable to the cluster,
where the weights are determined heuristically,
rather than learned, unlike ours.
Like many heuristic-based pronoun resolvers
(e.g., Mitkov (1998)), they first apply a set of con-
straints to filter grammatically incompatible can-
didate antecedents and then rank the remaining
ones using salience factors. As a result, their
cluster-ranking model employs only factors that
capture the salience of a cluster, and can therefore
be viewed as a simple model of attentional state
(see Grosz and Sidner (1986)) realized by coref-
erence clusters. By contrast, our resolution strat-
egy is learned without applying hand-coded con-
969
straints in a separate filtering step. In particular,
we attempt to determine the compatibility between
a cluster and an active mention, using factors that
determine not only salience (e.g., the distance be-
tween the cluster and the mention) but also lexical
and grammatical compatibility, for instance.
Entity-mention coreference models. Luo et al
(2004) represent one of the earliest attempts to
investigate learning-based entity-mention models.
They use the ANY predicate to generate cluster-
level features as follows: given a binary-valued
feature X defined over a pair of mentions, they
introduce an ANY-X cluster-level feature, which
has the value TRUE if X is true between the active
mention and any mention in the preceding clus-
ter under consideration. Contrary to common wis-
dom, this entity-mention model underperforms its
mention-pair counterpart in spite of the general-
ization from mention-pair to cluster-level features.
In Yang et al?s (2004) entity-mention model, a
training instance is composed of an active men-
tion m
k
, a preceding cluster C, and a mention
m
j
in C that is closest in distance to m
k
in the
associated text. The feature set used to repre-
sent the instance is primarily composed of fea-
tures that describe the relationship between m
j
and m
k
, as well as a few cluster-level features.
In other words, the model still relies heavily on
features used in a mention-pair model. In par-
ticular, the inclusion of m
j
in the feature vector
representation to some extent reflects the authors?
lack of confidence that a strong entity-mention
model can be trained without mention-pair-based
features. Our ranking model, on the other hand, is
trained without such features. More recently, Yang
et al (2008) have proposed another entity-mention
model trained by inductive logic programming.
Like their previous work, the scarcity of cluster-
level predicates (only two are used) under-exploits
the expressiveness of entity-mention models.
Mention ranking. The notion of ranking can-
didate antecedents can be traced back to center-
ing algorithms, many of which use grammatical
roles to rank forward-looking centers (see Grosz
et al (1995), Walker et al (1998), and Mitkov
(2002)). However, mention ranking has been
employed in learning-based coreference resolvers
only recently. As mentioned before, Denis and
Baldridge (2008) train a mention-ranking model.
Their work can be viewed as an extension of Yang
et al?s (2003) twin-candidate coreference model,
which ranks only two candidate antecedents at a
time. Unlike ours, however, their model ranks
mentions rather than clusters, and relies on an
independently-trained discourse-new detector.
Discourse-new detection. Discourse-new de-
tection is often tackled independently of coref-
erence resolution. Pleonastic its have been de-
tected using heuristics (e.g., Kennedy and Bogu-
raev (1996)) and learning-based techniques such
as rule learning (e.g., Mu?ller (2006)), kernels (e.g.,
Versley et al (2008)), and distributional methods
(e.g., Bergsma et al (2008)). Non-anaphoric defi-
nite descriptions have been detected using heuris-
tics (e.g., Vieira and Poesio (2000)) and unsu-
pervised methods (e.g., Bean and Riloff (1999)).
General discourse-new detectors that are applica-
ble to different types of NPs have been built using
heuristics (e.g., Byron and Gegg-Harrison (2004))
and modeled generatively (e.g., Elsner and Char-
niak (2007)) and discriminatively (e.g., Uryupina
(2003)). There have also been attempts to perform
joint inference for discourse-new detection and
coreference resolution using integer linear pro-
gramming (ILP), where a discourse-new classifier
and a coreference classifier are trained indepen-
dently of each other, and then ILP is applied as a
post-processing step to jointly infer discourse-new
and coreference decisions so that they are consis-
tent with each other (e.g., Denis and Baldridge
(2007)). Joint inference is different from our joint-
learning approach, which allows the two tasks to
be learned jointly and not independently.
3 Baseline Coreference Models
In this section, we describe three coreference mod-
els that will serve as our baselines: the mention-
pair model, the entity-mention model, and the
mention-ranking model. For illustrative purposes,
we will use the text segment shown in Figure 1.
Each mention m in the segment is annotated as
[m]
cid
mid
, where mid is the mention id and cid is
the id of the cluster to which m belongs. As we
can see, the mentions are partitioned into four sets,
with Barack Obama, his, and he in one cluster, and
each of the remaining mentions in its own cluster.
3.1 Mention-Pair Model
As noted before, a mention-pair model is a clas-
sifier that decides whether or not an active men-
tion m
k
is coreferent with a candidate antecedent
m
j
. Each instance i(m
j
,m
k
) represents m
j
and
970
[Barack Obama]1
1
nominated [Hillary Rodham Clinton]2
2
as
[[his]1
3
secretary of state]3
4
on [Monday]4
5
. [He]1
6
...
Figure 1: An illustrative example
m
k
and consists of the 39 features shown in Ta-
ble 1. These features have largely been employed
by state-of-the-art learning-based coreference sys-
tems (e.g., Soon et al (2001), Ng and Cardie
(2002b), Bengtson and Roth (2008)), and are com-
puted automatically. As can be seen, the features
are divided into four blocks. The first two blocks
consist of features that describe the properties of
m
j
and m
k
, respectively, and the last two blocks
of features describe the relationship between m
j
and m
k
. The classification associated with a train-
ing instance is either positive or negative, depend-
ing on whether m
j
and m
k
are coreferent.
If one training instance were created from each
pair of mentions, the negative instances would
significantly outnumber the positives, yielding
a skewed class distribution that will typically
have an adverse effect on model training. As
a result, only a subset of mention pairs will
be generated for training. Following Soon et
al. (2001), we create (1) a positive instance for
each discourse-old mention m
k
and its closest
antecedent m
j
; and (2) a negative instance for
m
k
paired with each of the intervening mentions,
m
j+1
,m
j+2
, . . . ,m
k?1
. In our running example
shown in Figure 1, three training instances will
be generated for He: i(Monday, He), i(secretary
of state, He), and i(his, He). The first two of
these instances will be labeled as negative, and
the last one will be labeled as positive. To train a
mention-pair classifier, we use the SVM learning
algorithm from the SVMlight package (Joachims,
2002), converting all multi-valued features into an
equivalent set of binary-valued features.
After training, the resulting SVM classifier is
used to identify an antecedent for a mention in a
test text. Specifically, an active mention m
k
se-
lects as its antecedent the closest preceding men-
tion that is classified as coreferent with m
k
. If m
k
is not classified as coreferent with any preceding
mention, it will be considered discourse-new (i.e.,
no antecedent will be selected for m
k
).
3.2 Entity-Mention Model
Unlike a mention-pair model, an entity-mention
model is a classifier that decides whether or not
an active mention m
k
is coreferent with a par-
tial cluster c
j
that precedes m
k
. Each training
instance, i(c
j
,m
k
), represents c
j
and m
k
. The
features for an instance can be divided into two
types: (1) features that describe m
k
(i.e, those
shown in the second block of Table 1), and (2)
cluster-level features, which describe the relation-
ship between c
j
and m
k
. Motivated by previ-
ous work (Luo et al, 2004; Culotta et al, 2007;
Yang et al, 2008), we create cluster-level fea-
tures from mention-pair features using four pred-
icates: NONE, MOST-FALSE, MOST-TRUE, and
ALL. Specifically, for each feature X shown in
the last two blocks in Table 1, we first convert X
into an equivalent set of binary-valued features if
it is multi-valued. Then, for each resulting binary-
valued feature X
b
, we create four binary-valued
cluster-level features: (1) NONE-X
b
is true when
X
b
is false between m
k
and each mention in c
j
; (2)
MOST-FALSE-X
b
is true when X
b
is true between
m
k
and less than half (but at least one) of the men-
tions in c
j
; (3) MOST-TRUE-X
b
is true when X
b
is
true between m
k
and at least half (but not all) of
the mentions in c
j
; and (4) ALL-X
b
is true when X
b
is true between m
k
and each mention in c
j
. Hence,
for each X
b
, exactly one of these four cluster-level
features evaluates to true.
Following Yang et al (2008), we create (1) a
positive instance for each discourse-old mention
m
k
and the preceding cluster c
j
to which it be-
longs; and (2) a negative instance for m
k
paired
with each partial cluster whose last mention ap-
pears between m
k
and its closest antecedent (i.e.,
the last mention of c
j
). Consider again our run-
ning example. Three training instances will be
generated for He: i({Monday}, He), i({secretary
of state}, He), and i({Barack Obama, his}, He).
The first two of these instances will be labeled as
negative, and the last one will be labeled as pos-
itive. As in the mention-pair model, we train an
entity-mention classifier using the SVM learner.
After training, the resulting classifier is used to
identify a preceding cluster for a mention in a test
text. Specifically, the mentions are processed in
a left-to-right manner. For each active mention
m
k
, a test instance is created between m
k
and
each of the preceding clusters formed so far. All
the test instances are then presented to the classi-
fier. Finally, m
k
will be linked to the closest pre-
ceding cluster that is classified as coreferent with
m
k
. If m
k
is not classified as coreferent with any
971
Features describing m
j
, a candidate antecedent
1 PRONOUN 1 Y if m
j
is a pronoun; else N
2 SUBJECT 1 Y if m
j
is a subject; else N
3 NESTED 1 Y if m
j
is a nested NP; else N
Features describing m
k
, the mention to be resolved
4 NUMBER 2 SINGULAR or PLURAL, determined using a lexicon
5 GENDER 2 MALE, FEMALE, NEUTER, or UNKNOWN, determined using a list of common first names
6 PRONOUN 2 Y if m
k
is a pronoun; else N
7 NESTED 2 Y if m
k
is a nested NP; else N
8 SEMCLASS 2 the semantic class of m
k
; can be one of PERSON, LOCATION, ORGANIZATION, DATE, TIME,
MONEY, PERCENT, OBJECT, OTHERS, determined using WordNet and an NE recognizer
9 ANIMACY 2 Y if m
k
is determined as HUMAN or ANIMAL by WordNet and an NE recognizer; else N
10 PRO TYPE 2 the nominative case of m
k
if it is a pronoun; else NA. E.g., the feature value for him is HE
Features describing the relationship between m
j
, a candidate antecedent and m
k
, the mention to be resolved
11 HEAD MATCH C if the mentions have the same head noun; else I
12 STR MATCH C if the mentions are the same string; else I
13 SUBSTR MATCH C if one mention is a substring of the other; else I
14 PRO STR MATCH C if both mentions are pronominal and are the same string; else I
15 PN STR MATCH C if both mentions are proper names and are the same string; else I
16 NONPRO STR MATCH C if the two mentions are both non-pronominal and are the same string; else I
17 MODIFIER MATCH C if the mentions have the same modifiers; NA if one of both of them don?t have a modifier;
else I
18 PRO TYPE MATCH C if both mentions are pronominal and are either the same pronoun or different only w.r.t.
case; NA if at least one of them is not pronominal; else I
19 NUMBER C if the mentions agree in number; I if they disagree; NA if the number for one or both
mentions cannot be determined
20 GENDER C if the mentions agree in gender; I if they disagree; NA if the gender for one or both mentions
cannot be determined
21 AGREEMENT C if the mentions agree in both gender and number; I if they disagree in both number and
gender; else NA
22 ANIMACY C if the mentions match in animacy; I if they don?t; NA if the animacy for one or both mentions
cannot be determined
23 BOTH PRONOUNS C if both mentions are pronouns; I if neither are pronouns; else NA
24 BOTH PROPER NOUNS C if both mentions are proper nouns; I if neither are proper nouns; else NA
25 MAXIMALNP C if the two mentions does not have the same maximial NP projection; else I
26 SPAN C if neither mention spans the other; else I
27 INDEFINITE C if m
k
is an indefinite NP and is not in an appositive relationship; else I
28 APPOSITIVE C if the mentions are in an appositive relationship; else I
29 COPULAR C if the mentions are in a copular construction; else I
30 SEMCLASS C if the mentions have the same semantic class; I if they don?t; NA if the semantic class
information for one or both mentions cannot be determined
31 ALIAS C if one mention is an abbreviation or an acronym of the other; else I
32 DISTANCE binned values for sentence distance between the mentions
Additional features describing the relationship between m
j
, a candidate antecedent and m
k
, the mention to be resolved
33 NUMBER? the concatenation of the NUMBER 2 feature values of m
j
and m
k
. E.g., if m
j
is Clinton and
m
k
is they, the feature value is SINGULAR-PLURAL, since m
j
is singular and m
k
is plural
34 GENDER? the concatenation of the GENDER 2 feature values of m
j
and m
k
35 PRONOUN? the concatenation of the PRONOUN 2 feature values of m
j
and m
k
36 NESTED? the concatenation of the NESTED 2 feature values of m
j
and m
k
37 SEMCLASS? the concatenation of the SEMCLASS 2 feature values of m
j
and m
k
38 ANIMACY? the concatenation of the ANIMACY 2 feature values of m
j
and m
k
39 PRO TYPE? the concatenation of the PRO TYPE 2 feature values of m
j
and m
k
Table 1: The feature set for coreference resolution. Non-relational features describe a mention and in
most cases take on a value of YES or NO. Relational features describe the relationship between the two
mentions and indicate whether they are COMPATIBLE, INCOMPATIBLE or NOT APPLICABLE.
preceding cluster, it will be considered discourse-
new. Note that all partial clusters preceding m
k
are formed incrementally based on the predictions
of the classifier for the first k ? 1 mentions.
3.3 Mention-Ranking Model
As noted before, a ranking model imposes a
ranking on all the candidate antecedents of an
active mention m
k
. To train a ranker, we
use the SVM ranker-learning algorithm from the
SVMlight package. Like the mention-pair model,
each training instance i(m
j
,m
k
) represents m
k
and a preceding mention m
j
. In fact, the fea-
tures that represent the instance as well as the
method for creating training instances are identi-
cal to those employed by the mention-pair model.
972
The only difference lies in the assignment of
class values to training instances. Assuming that
S
k
is the set of training instances created for
anaphoric mention m
k
, the class value for an in-
stance i(m
j
,m
k
) in S
k
is the rank of m
j
among
competing candidate antecedents, which is 2 if
m
j
is the closest antecedent of m
k
, and 1 other-
wise.1 To exemplify, consider our running exam-
ple. As in the mention-pair model, three training
instances will be generated for He: i(Monday, He),
i(secretary of state, He), i(his, He). The third in-
stance will have a class value of 2, and the remain-
ing two will have a class value of 1.
After training, the mention-ranking model is ap-
plied to rank the candidate antecedents for an ac-
tive mention in a test text as follows. Given an ac-
tive mention m
k
, we follow Denis and Baldridge
(2008) and use an independently-trained classifier
to determine whether m
k
is discourse-new. If so,
m
k
will not be resolved. Otherwise, we create test
instances for m
k
by pairing it with each of its pre-
ceding mentions. The test instances are then pre-
sented to the ranker, and the preceding mention
that is assigned the largest value by the ranker is
selected as the antecedent of m
k
.
The discourse-new classifier used in the resolu-
tion step is trained with 26 of the 37 features2 de-
scribed in Ng and Cardie (2002a) that are deemed
useful for distinguishing between anaphoric and
non-anaphoric mentions. These features can be
broadly divided into two types: (1) features that
encode the form of the mention (e.g., NP type,
number, definiteness), and (2) features that com-
pare the mention to one of its preceding mentions.
4 Coreference as Cluster Ranking
In this section, we describe our cluster-ranking ap-
proach to NP coreference. As noted before, our
approach aims to combine the strengths of entity-
mention models and mention-ranking models.
4.1 Training and Applying a Cluster Ranker
For ease of exposition, we will describe in this
subsection how to train and apply a cluster ranker
when it is used in a pipeline architecture, where
discourse-new detection is performed prior to
coreference resolution. In the next subsection, we
will show how the two tasks can be learned jointly.
1A larger class value implies a better rank in SVMlight.
2The 11 features that we did not employ are CONJ,
POSSESSIVE, MODIFIER, POSTMODIFIED, SPECIAL NOUNS,
POST, SUBCLASS, TITLE, and the positional features.
Recall that a cluster ranker ranks a set of pre-
ceding clusters for an active mention m
k
. Since
a cluster ranker is a hybrid of a mention-ranking
model and an entity-mention model, the way it is
trained and applied is also a hybrid of the two.
In particular, the instance representation employed
by a cluster ranker is identical to that used by
an entity-mention model, where each training in-
stance i(c
j
, m
k
) represents a preceding cluster c
j
and a discourse-old mention m
k
and consists of
cluster-level features formed from predicates. Un-
like in an entity-mention model, however, in a
cluster ranker, (1) a training instance is created be-
tween each discourse-old mention m
k
and each of
its preceding clusters; and (2) since we are train-
ing a model for ranking clusters, the assignment of
class values to training instances is similar to that
of a mention ranker. Specifically, the class value of
a training instance i(c
j
, m
k
) created for m
k
is the
rank of c
j
among the competing clusters, which is
2 if m
k
belongs to c
j
, and 1 otherwise.
Applying the learned cluster ranker to a test text
is similar to applying a mention ranker. Specifi-
cally, the mentions are processed in a left-to-right
manner. For each active mention m
k
, we first
apply an independently-trained classifier to deter-
mine if m
k
is discourse-new. If so, m
k
will not be
resolved. Otherwise, we create test instances for
m
k
by pairing it with each of its preceding clus-
ters. The test instances are then presented to the
ranker, and m
k
is linked to the cluster that is as-
signed the highest value by the ranker. Note that
these partial clusters preceding m
k
are formed in-
crementally based on the predictions of the ranker
for the first k?1 mentions; no gold-standard coref-
erence information is used in their formation.
4.2 Joint Discourse-New Detection and
Coreference Resolution
The cluster ranker described above can be used
to determine which preceding cluster a discourse-
old mention should be linked to, but it cannot be
used to determine whether a mention is discourse-
new or not. The reason is simple: all the training
instances are generated from discourse-old men-
tions. Hence, to jointly learn discourse-new de-
tection and coreference resolution, we must train
the ranker using instances generated from both
discourse-old and discourse-new mentions.
Specifically, when training the ranker, we pro-
vide each active mention with the option to start
973
a new cluster by creating an additional instance
that (1) contains features that solely describe the
active mention (i.e., the features shown in the sec-
ond block of Table 1), and (2) has the highest rank
value among competing clusters (i.e., 2) if it is
discourse-new and the lowest rank value (i.e., 1)
otherwise. The main advantage of jointly learning
the two tasks is that it allows the ranking model
to evaluate all possible options for an active men-
tion (i.e., whether to resolve it, and if so, which
preceding cluster is the best) simultaneously.
After training, the resulting cluster ranker pro-
cesses the mentions in a test text in a left-to-right
manner. For each active mention m
k
, we create
test instances for it by pairing it with each of its
preceding clusters. To allow for the possibility that
m
k
is discourse-new, we create an additional test
instance that contains features that solely describe
the active mention (similar to what we did in the
training step above). All these test instances are
then presented to the ranker. If the additional test
instance is assigned the highest rank value by the
ranker, then m
k
is classified as discourse-new and
will not be resolved. Otherwise, m
k
is linked to
the cluster that has the highest rank. As before,
all partial clusters preceding m
k
are formed incre-
mentally based on the predictions of the ranker for
the first k ? 1 mentions.
5 Evaluation
5.1 Experimental Setup
Corpus. We use the ACE 2005 coreference cor-
pus as released by the LDC, which consists of the
599 training documents used in the official ACE
evaluation.3 To ensure diversity, the corpus was
created by selecting documents from six different
sources: Broadcast News (bn), Broadcast Con-
versations (bc), Newswire (nw), Webblog (wb),
Usenet (un), and conversational telephone speech
(cts). The number of documents belonging to each
source is shown in Table 2. For evaluation, we par-
tition the 599 documents into a training set and a
test set following a 80/20 ratio, ensuring that the
two sets have the same proportion of documents
from the six sources.
Mention extractor. We evaluate each corefer-
ence model using both true mentions (i.e., gold
standard mentions4) and system mentions (i.e., au-
3Since we did not participate in ACE 2005, we do not
have access to the official test set.
4Note that only mention boundaries are used.
Dataset bn bc nw wl un cts
# of documents 60 226 106 119 49 39
Table 2: Statistics for the ACE 2005 corpus
tomatically identified mentions). To extract sys-
tem mentions from a test text, we trained a men-
tion extractor on the training texts. Following Flo-
rian et al (2004), we recast mention extraction as
a sequence labeling task, where we assign to each
token in a test text a label that indicates whether it
begins a mention, is inside a mention, or is outside
a mention. Hence, to learn the extractor, we create
one training instance for each token in a training
text and derive its class value (one of b, i, and o)
from the annotated data. Each instance represents
w
i
, the token under consideration, and consists of
29 linguistic features, many of which are modeled
after the systems of Bikel et al (1999) and Florian
et al (2004), as described below.
Lexical (7): Tokens in a window of 7:
{w
i?3
, . . . , w
i+3
}.
Capitalization (4): Determine whether w
i
IsAllCap, IsInitCap, IsCapPeriod, and
IsAllLower (see Bikel et al (1999)).
Morphological (8): w
i
?s prefixes and suffixes of
length one, two, three, and four.
Grammatical (1): The part-of-speech (POS)
tag of w
i
obtained using the Stanford log-linear
POS tagger (Toutanova et al, 2003).
Semantic (1): The named entity (NE) tag of w
i
obtained using the Stanford CRF-based NE recog-
nizer (Finkel et al, 2005).
Gazetteers (8): Eight dictionaries containing
pronouns (77 entries), common words and words
that are not names (399.6k), person names (83.6k),
person titles and honorifics (761), vehicle words
(226), location names (1.8k), company names
(77.6k), and nouns extracted from WordNet that
are hyponyms of PERSON (6.3k).
We employ CRF++5, a C++ implementation of
conditional random fields, for training the mention
detector, which achieves an F-score of 86.7 (86.1
recall, 87.2 precision) on the test set. These ex-
tracted mentions are to be used as system mentions
in our coreference experiments.
Scoring programs. To score the output of a
coreference model, we employ three scoring pro-
grams: MUC (Vilain et al, 1995), B3 (Bagga and
Baldwin, 1998), and ?
3
-CEAF (Luo, 2005).
5Available from http://crfpp.sourceforge.net
974
There is a complication, however. When scor-
ing a response (i.e., system-generated) partition
against a key (i.e., gold-standard) partition, a scor-
ing program needs to construct a mapping between
the mentions in the response and those in the key.
If the response is generated using true mentions,
then every mention in the response is mapped to
some mention in the key and vice versa; in other
words, there are no twinless (i.e., unmapped) men-
tions (Stoyanov et al, 2009). However, this is
not the case when system mentions are used. The
aforementioned complication does not arise from
the construction of the mapping, but from the fact
that Bagga and Baldwin (1998) and Luo (2005) do
not specify how to apply B3 and CEAF to score
partitions generated from system mentions.
We propose a simple solution to this problem:
we remove all and only those twinless system
mentions that are singletons before applying B3
and CEAF. The reason is simple: since the coref-
erence resolver has successfully identified these
mentions as singletons, it should not be penal-
ized, and removing them allows us to avoid such
penalty. Note that we only remove twinless (as op-
posed to all) system mentions that are singletons:
this allows us to reward a resolver for success-
ful identification of singleton mentions that have
twins, thus overcoming a major weakness of and
common criticism against the MUC scorer. Also,
we retain twinless system mentions that are non-
singletons, as the resolver should be penalized for
identifying spurious coreference relations. On the
other hand, we do not remove twinless mentions
in the key partition, as we want to ensure that the
resolver makes the correct (non-)coreference de-
cisions for them. We believe that our proposal ad-
dresses Stoyanov et al?s (2009) problem of hav-
ing very low precision when applying the CEAF
scorer to score partitions of system mentions.
5.2 Results and Discussions
The mention-pair baseline. We train our first
baseline, the mention-pair coreference classifier,
using the SVM learning algorithm as implemented
in the SVMlight package (Joachims, 2002).6 Re-
sults of this baseline using true mentions and sys-
tem mentions, shown in row 1 of Tables 3 and 4,
are reported in terms of recall (R), precision (P),
and F-score (F) provided by the three scoring pro-
6For this and subsequent uses of the SVM learner in our
experiments, we set al parameters to their default values.
grams. As we can see, this baseline achieves F-
scores of 54.3?70.0 and 53.4?62.5 for true men-
tions and system mentions, respectively.
The entity-mention baseline. Next, we train
our second baseline, the entity-mention corefer-
ence classifier, using the SVM learner. Results of
this baseline are shown in row 2 of Tables 3 and
4. For true mentions, this baseline achieves an F-
score of 54.8?70.7. In comparison to the mention-
pair baseline, F-score rises insignificantly accord-
ing to all three scorers.7 Similar trends can be ob-
served for system mentions, where the F-scores
between the two models are statistically indistin-
guishable across the board. While the insignifi-
cant performance difference is somewhat surpris-
ing given the improved expressiveness of entity-
mention models over mention-pair models, similar
trends have been reported by Luo et al (2004).
The mention-ranking baseline. Our third base-
line is the mention-ranking coreference model,
trained using the ranker-learning algorithm in
SVMlight. To identify discourse-new mentions,
we employ two methods. In the first method, we
adopt a pipeline architecture, where we train an
SVM classifier for discourse-new detection inde-
pendently of the mention ranker on the training set
using the 26 features described in Section 3.3. We
then apply the resulting classifier to each test text
to filter discourse-new mentions prior to corefer-
ence resolution. Results of the mention ranker are
shown in row 3 of Tables 3 and 4. As we can
see, the ranker achieves F-scores of 57.8?71.2 and
54.1?65.4 for true mentions and system mentions,
respectively, yielding a significant improvement
over the entity-mention baseline in all but one case
(MUC/true mentions).
In the second method, we perform discourse-
new detection jointly with coreference resolution
using the method described in Section 4.2. While
we discussed this joint learning method in the con-
text of cluster ranking, it should be easy to see
that the method is equally applicable to a men-
tion ranker. Results of the mention ranker using
this joint architecture are shown in row 4 of Ta-
bles 3 and 4. As we can see, the ranker achieves
F-scores of 61.6?73.4 and 55.6?67.1 for true men-
tions and system mentions, respectively. For both
types of mentions, the improvements over the cor-
responding results for the entity-mention baseline
7We use Approximate Randomization (Noreen, 1989) for
testing statistical significance, with p set to 0.05.
975
MUC CEAF B3
Coreference Model R P F R P F R P F
1 Mention-pair model 71.7 69.2 70.4 54.3 54.3 54.3 53.3 63.6 58.0
2 Entity-mention model 71.7 69.7 70.7 54.8 54.8 54.8 53.2 65.1 58.5
3 Mention-ranking model (Pipeline) 68.7 73.9 71.2 57.8 57.8 57.8 55.8 63.9 59.6
4 Mention-ranking model (Joint) 69.4 77.8 73.4 61.6 61.6 61.6 57.0 70.1 62.9
5 Cluster-ranking model (Pipeline) 71.7 78.2 74.8 61.8 61.8 61.8 58.2 69.1 63.2
6 Cluster-ranking model (Joint) 69.9 83.3 76.0 63.3 63.3 63.3 56.0 74.6 64.0
Table 3: MUC, CEAF, and B3 coreference results using true mentions.
MUC CEAF B3
Coreference Model R P F R P F R P F
1 Mention-pair model 70.0 56.4 62.5 56.1 51.0 53.4 50.8 57.9 54.1
2 Entity-mention model 68.5 57.2 62.3 56.3 50.2 53.1 51.2 57.8 54.3
3 Mention-ranking model (Pipeline) 62.2 68.9 65.4 51.6 56.7 54.1 52.3 61.8 56.6
4 Mention-ranking model (Joint) 62.1 73.0 67.1 53.0 58.5 55.6 50.4 65.5 56.9
5 Cluster-ranking model (Pipeline) 65.3 72.3 68.7 54.1 59.3 56.6 55.3 63.7 59.2
6 Cluster-ranking model (Joint) 64.1 75.4 69.3 56.7 62.6 59.5 54.4 70.5 61.4
Table 4: MUC, CEAF, and B3 coreference results using system mentions.
are significant, and suggest that mention ranking is
a precision-enhancing device. Moreover, in com-
parison to the pipeline architecture in row 3, we
see that F-score rises significantly by 2.2?3.8% for
true mentions, and improves by a smaller margin
of 0.3?1.7% for system mentions. These results
demonstrate the benefits of joint modeling.
Our cluster-ranking model. Finally, we evalu-
ate our cluster-ranking model. As in the mention-
ranking baseline, we employ both the pipeline ar-
chitecture and the joint architecture for discourse-
new detection. Results are shown in rows 5 and
6 of Tables 3 and 4, respectively, for the two ar-
chitectures. When true mentions are used, the
pipeline architecture yields an F-score of 61.8?
74.8, which represents a significant improvement
over the mention ranker adopting the pipeline ar-
chitecture. With the joint architecture, the clus-
ter ranker achieves an F-score of 63.3?76.0. This
also represents a significant improvement over the
mention ranker adopting the joint architecture, the
best of the baselines, and suggests that cluster
ranking is a better precision-enhancing model than
mention ranking. Moreover, comparing the re-
sults in these two rows reveals the superiority of
the joint architecture over the pipeline architec-
ture, particularly in terms of its ability to enhance
system precision. Similar performance trends can
be observed when system mentions are used.
6 Conclusions
We have presented a cluster-ranking approach that
recasts the mention resolution process as the prob-
lem of finding the best preceding cluster to link an
active mention to. Crucially, our approach com-
bines the strengths of entity-mention models and
mention-ranking models. Experimental results on
the ACE 2005 corpus show that (1) jointly learn-
ing coreference resolution and discourse-new de-
tection allows the cluster ranker to achieve bet-
ter performance than adopting a pipeline corefer-
ence architecture; and (2) our cluster ranker signif-
icantly outperforms the mention ranker, the best of
the three baseline coreference models, under both
the pipeline architecture and the joint architecture.
Overall, we believe that our cluster-ranking ap-
proach advances the state-of-the-art in coreference
resolution both theoretically and empirically.
Acknowledgments
We thank the three anonymous reviewers for their
invaluable comments on the paper. This work was
supported in part by NSF Grant IIS-0812261.
References
A. Bagga and B. Baldwin. 1998. Entity-based cross-
document coreferencing using the vector space
model. In Proc. of COLING-ACL, pages 79?85.
D. Bean and E. Riloff. 1999. Corpus-based identifica-
tion of non-anaphoric noun phrases. In Proc. of the
ACL, pages 373?380.
E. Bengtson and D. Roth. 2008. Understanding the
values of features for coreference resolution. In
Proc. of EMNLP, pages 294?303.
S. Bergsma, D. Lin, and R. Goebel. 2008. Distribu-
tional identification of non-referential pronouns. In
Proc. of ACL-08:HLT, pages 10?18.
976
D. Bikel, R. Schwartz, and R. Weischedel. 1999. An
algorithm that learns what?s in a name. Machine
Learning, 34(1?3):211?231.
D. Byron and W. Gegg-Harrison. 2004. Eliminating
non-referring noun phrases from coreference resolu-
tion. In Proc. of DAARC, pages 21?26.
A. Culotta, M. Wick, and A. McCallum. 2007. First-
order probabilistic models for coreference resolu-
tion. In Proc. of NAACL-HLT, pages 81?88.
P. Denis and J. Baldridge. 2007. Global, joint determi-
nation of anaphoricity and coreference resolution us-
ing integer programming. In Proc. of NAACL-HLT,
pages 236?243.
P. Denis and J. Baldridge. 2008. Specialized models
and ranking for coreference resolution. In Proc. of
EMNLP, pages 660?669.
M. Elsner and E. Charniak. 2007. A generative
discourse-new model for text coherence. Technical
Report CS-07-04, Brown University.
J. R. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information
extraction systems by Gibbs sampling. In Proc. of
the ACL, pages 363?370.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing,
N. Kambhatla, X. Luo, N. Nicolov, and I. Zitouni.
2004. A statistical model for multilingual entity de-
tection and tracking. In Proc. of HLT/NAACL.
B. J. Grosz, A. K. Joshi, and S. Weinstein. 1995.
Centering: A framework for modeling the local co-
herence of discourse. Computational Linguistics,
21(2):203?226.
B. J. Grosz and C. L. Sidner. 1986. Attention, inten-
tions, and the structure of discourse. Computational
Linguistics, 12(3):175?204.
T. Joachims. 2002. Optimizing search engines using
clickthrough data. In Proc. of KDD, pages 133?142.
A. Kehler, D. Appelt, L. Taylor, and A. Simma. 2004.
The (non)utility of predicate-argument frequencies
for pronoun interpretation. In Proc. of HLT/NAACL.
C. Kennedy and B. Boguraev. 1996. Anaphor for ev-
eryone: Pronominal anaphora resolution without a
parser. In Proc. of COLING, pages 113?118.
S. Lappin and H. Leass. 1994. An algorithm for
pronominal anaphora resolution. Computational
Linguistics, 20(4):535?562.
X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and
S. Roukos. 2004. A mention-synchronous coref-
erence resolution algorithm based on the Bell tree.
In Proc. of the ACL, pages 135?142.
X. Luo. 2005. On coreference resolution performance
metrics. In Proc. of HLT/EMNLP, pages 25?32.
A. McCallum and B. Wellner. 2004. Conditional mod-
els of identity uncertainty with application to noun
coreference. In Advances in NIPS.
R. Mitkov. 2002. Anaphora Resolution. Longman.
R. Mitkov. 1998. Robust pronoun resolution with lim-
ited knowledge. In Proc. of COLING/ACL, pages
869?875.
C. Mu?ller. 2006. Automatic detection of nonrefer-
ential it in spoken multi-party dialog. In Proc. of
EACL, pages 49?56.
V. Ng and C. Cardie. 2002a. Identifying anaphoric and
non-anaphoric noun phrases to improve coreference
resolution. In Proc. of COLING, pages 730?736.
V. Ng and C. Cardie. 2002b. Improving machine learn-
ing approaches to coreference resolution. In Proc. of
the ACL, pages 104?111.
E. W. Noreen. 1989. Computer Intensive Methods for
Testing Hypothesis: An Introduction. John Wiley &
Sons.
M. Poesio, O. Uryupina, R. Vieira, M. Alexandrov-
Kabadjov, and R. Goulart. 2004. Discourse-new
detectors for definite description resolution: A sur-
vey and a preliminary proposal. In Proc. of the ACL
Workshop on Reference Resolution.
S. P. Ponzetto and M. Strube. 2006. Exploiting seman-
tic role labeling, WordNet and Wikipedia for coref-
erence resolution. In Proc. of HLT/NAACL, pages
192?199.
W. M. Soon, H. T. Ng, and D. Lim. 2001. A
machine learning approach to coreference resolu-
tion of noun phrases. Computational Linguistics,
27(4):521?544.
V. Stoyanov, N. Gilbert, C. Cardie, and E. Riloff. 2009.
Conundrums in noun phrase coreference resolution:
Making sense of the state-of-the-art. In Proc. of the
ACL.
K. Toutanova, D. Klein, C. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with
a cyclic dependency network. In Proc. of HLT-
NAACL, pages 252?259.
O. Uryupina. 2003. High-precision identification of
discourse new and unique noun phrases. In Proc. of
the ACL Student Research Workshop.
Y. Versley, A. Moschitti, M. Poesio, and X. Yang.
2008. Coreference systems based on kernel meth-
ods. In Proc. of COLING, pages 961?968.
R. Vieira and M. Poesio. 2000. Processing definite de-
scriptions in corpora. In Corpus-based and Compu-
tational Approaches to Discourse Anaphora, pages
189?212. UCL Press.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference
scoring scheme. In Proc. of MUC-6, pages 45?52.
M. Walker, A. Joshi, and E. Prince, editors. 1998.
Centering Theory in Discourse. Oxford University
Press.
X. Yang, G. Zhou, J. Su, and C. L. Tan. 2003.
Coreference resolution using competitive learning
approach. In Proc. of the ACL, pages 176?183.
X. Yang, J. Su, G. Zhou, and C. L. Tan. 2004. An NP-
cluster based approach to coreference resolution. In
Proc. of COLING, pages 226?232.
X. Yang, J. Su, J. Lang, C. L. Tan, and S. Li. 2008.
An entity-mention model for coreference resolution
with inductive logic programming. In Proc. of the
ACL, pages 843?851.
977
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 354?362,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Learning-Based Named Entity Recognition for Morphologically-Rich,
Resource-Scarce Languages
Kazi Saidul Hasan and Md. Altaf ur Rahman and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{saidul,altaf,vince}@hlt.utdallas.edu
Abstract
Named entity recognition for morpholog-
ically rich, case-insensitive languages, in-
cluding the majority of semitic languages,
Iranian languages, and Indian languages,
is inherently more difficult than its English
counterpart. Worse still, progress on ma-
chine learning approaches to named entity
recognition for many of these languages
is currently hampered by the scarcity of
annotated data and the lack of an accu-
rate part-of-speech tagger. While it is
possible to rely on manually-constructed
gazetteers to combat data scarcity, this
gazetteer-centric approach has the poten-
tial weakness of creating irreproducible
results, since these name lists are not
publicly available in general. Motivated
in part by this concern, we present a
learning-based named entity recognizer
that does not rely on manually-constructed
gazetteers, using Bengali as our represen-
tative resource-scarce, morphologically-
rich language. Our recognizer achieves
a relative improvement of 7.5% in F-
measure over a baseline recognizer. Im-
provements arise from (1) using in-
duced affixes, (2) extracting information
from online lexical databases, and (3)
jointly modeling part-of-speech tagging
and named entity recognition.
1 Introduction
While research in natural language processing has
gained a lot of momentum in the past several
decades, much of this research effort has been fo-
cusing on only a handful of politically-important
languages such as English, Chinese, and Arabic.
On the other hand, being the fifth most spoken lan-
guage1 with more than 200 million native speakers
residing mostly in Bangladesh and the Indian state
of West Bengal, Bengali has far less electronic
resources than the aforementioned languages. In
fact, a major obstacle to the automatic processing
of Bengali is the scarcity of annotated corpora.
One potential solution to the problem of data
scarcity is to hand-annotate a small amount of
data with the desired linguistic information and
then develop bootstrapping algorithms for com-
bining this small amount of labeled data with
a large amount of unlabeled data. In fact, co-
training (Blum and Mitchell, 1998) has been suc-
cessfully applied to English named entity recog-
nition (NER) (Collins & Singer [henceforth C&S]
(1999)). In C&S?s approach, consecutive words
tagged as proper nouns are first identified as poten-
tial NEs, and each such NE is then labeled by com-
bining the outputs of two co-trained classifiers.
Unfortunately, there are practical difficulties in ap-
plying this technique to Bengali NER. First, one
of C&S?s co-trained classifiers uses features based
on capitalization, but Bengali is case-insensitive.
Second, C&S identify potential NEs based on
proper nouns, but unlike English, (1) proper noun
identification for Bengali is non-trivial, due to the
lack of capitalization; and (2) there does not ex-
ist an accurate Bengali part-of-speech (POS) tag-
ger for providing such information, owing to the
scarcity of annotated data for training the tagger.
In other words, Bengali NER is complicated not
only by the scarcity of annotated data, but also by
the lack of an accurate POS tagger. One could
imagine building a Bengali POS tagger using un-
1See http://en.wikipedia.org/wiki/Bengali language.
354
supervised induction techniques that have been
successfully developed for English (e.g., Schu?tze
(1995), Clark (2003)), including the recently-
proposed prototype-driven approach (Haghighi
and Klein, 2006) and Bayesian approach (Gold-
water and Griffiths, 2007). The majority of these
approaches operate by clustering distributionally
similar words, but they are unlikely to work well
for Bengali for two reasons. First, Bengali is a
relatively free word order language, and hence
the distributional information collected for Ben-
gali words may not be as reliable as that for En-
glish words. Second, many closed-class words
that typically appear in the distributional repre-
sentation of an English word (e.g., prepositions
and particles such as ?in? and ?to?) are realized
as inflections in Bengali, and the absence of these
informative words implies that the context vector
may no longer capture sufficient information for
accurately clustering the Bengali words.
In view of the above problems, many learning-
based Bengali NE recognizers have relied heavily
on manually-constructed name lists for identify-
ing persons, organizations, and locations. There
are at least two weaknesses associated with this
gazetteer-centric approach. First, these name lists
are typically not publicly available, making it dif-
ficult to reproduce the results of these NE recog-
nizers. Second, it is not clear how comprehen-
sive these lists are. Relying on comprehensive lists
that comprise a large portion of the names in the
test set essentially reduces the NER problem to a
dictionary-lookup problem, which is arguably not
very interesting from a research perspective.
In addition, many existing learning-based Ben-
gali NE recognizers have several common weak-
nesses. First, they use as features pseudo-affixes,
which are created by extracting the first n and the
last n characters of a word (where 1 ? n ? 4)
(e.g., Dandapat et al (2007)). While affixes en-
code essential grammatical information in Ben-
gali due to its morphological richness, this extrac-
tion method is arguably too ad-hoc and does not
cover many useful affixes. Second, they typically
adopt a pipelined NER architecture, performing
POS tagging prior to NER and encoding the result-
ing not-so-accurate POS information as a feature.
In other words, errors in POS tagging are propa-
gated to the NE recognizer via the POS feature,
thus limiting its performance.
Motivated in part by these weaknesses, we in-
vestigate how to improve a learning-based NE rec-
ognizer that does not rely on manually-constructed
gazetteers. Specifically, we investigate two learn-
ing architectures for our NER system. The first
one is the aforementioned pipelined architecture
in which the NE recognizer uses as features the
output of a POS tagger that is trained indepen-
dently of the recognizer. Unlike existing Bengali
POS and NE taggers, however, we examine two
new knowledge sources for training these taggers:
(1) affixes induced from an unannotated corpus
and (2) semantic class information extracted from
Wikipedia. In the second architecture, we jointly
learn the POS tagging and the NER tasks, allow-
ing features for one task to be accessible to the
other task during learning. The goal is to exam-
ine whether any benefits can be obtained via joint
modeling, which could address the error propaga-
tion problem with the pipelined architecture.
While we focus on Bengali NER in this pa-
per, none of the proposed techniques are language-
specific. In fact, we believe that these techniques
are of relevance and interest to the EACL com-
munity because they can be equally applicable to
the numerous resource-scarce European and Mid-
dle Eastern languages that share similar linguis-
tic and extra-linguistic properties as Bengali. For
instance, the majority of semitic languages and
Iranian languages are, like Bengali, morpholog-
ically productive; and many East European lan-
guages such as Czech and Polish resemble Bengali
in terms of not only their morphological richness,
but also their relatively free word order.
The rest of the paper is organized as follows.
In Section 2, we briefly describe the related work.
Sections 3 and 4 show how we induce affixes from
an unannotated corpus and extract semantic class
information from Wikipedia. In Sections 5 and
6, we train and evaluate a POS tagger and an NE
recognizer independently, augmenting the feature
set typically used for these two tasks with our new
knowledge sources. Finally, we describe and eval-
uate our joint model in Section 7.
2 Related Work
Cucerzan and Yarowsky (1999) exploit morpho-
logical and contextual patterns to propose a
language-independent solution to NER. They use
affixes based on the paradigm that named enti-
ties corresponding to a particular class have sim-
ilar morphological structure. Their bootstrapping
355
approach is tested on Romanian, English, Greek,
Turkish, and Hindi. The recall for Hindi is the
lowest (27.84%) among the five languages, sug-
gesting that the lack of case information can sig-
nificantly complicate the NER task.
To investigate the role of gazetteers in NER,
Mikheev et al (1999) combine grammar rules with
maximum entropy models and vary the gazetteer
size. Experimental results show that (1) the F-
scores for NE classes like person and organiza-
tion are still high without gazetteers, ranging from
85% to 92%; and (2) a small list of country names
can improve the low F-score for locations substan-
tially. It is worth noting that their recognizer re-
quires that the input data contain POS tags and
simple semantic tags, whereas ours automatically
acquires such linguistic information. In addition,
their approach uses part of the dataset to extend the
gazetteer. Therefore, the resulting gazetteer list is
specific to a particular domain; on the other hand,
our approach does not generate a domain-specific
list, since it makes use of Wikipedia articles.
Kozareva (2006) generates gazetteer lists for
person and location names from unlabeled data
using common patterns and a graph exploration
algorithm. The location pattern is essentially
a preposition followed by capitalized context
words. However, this approach is inadequate for a
morphologically-rich language like Bengali, since
prepositions are often realized as inflections.
3 Affix Induction
Since Bengali is morphologically productive, a lot
of grammatical information about Bengali words
is expressed via affixes. Hence, these affixes could
serve as useful features for training POS and NE
taggers. In this section, we show how to induce
affixes from an unannotated corpus.
We rely on a simple idea proposed by Keshava
and Pitler (2006) for inducing affixes. Assume that
(1) V is a vocabulary (i.e., a set of distinct words)
extracted from a large, unannotated corpus, (2) ?
and ? are two character sequences, and (3) ?? is
the concatenation of ? and ?. If ?? and ? are
found in V , we extract ? as a suffix. Similarly, if
?? and ? are found in V , we extract ? as a prefix.
In principle, we can use all of the induced af-
fixes as features for training a POS tagger and an
NE recognizer. However, we choose to use only
those features that survive our feature selection
process (to be described below), for the follow-
ing reasons. First, the number of induced affixes
is large, and using only a subset of them as fea-
tures could make the training process more effi-
cient. Second, the above affix induction method is
arguably overly simplistic and hence many of the
induced affixes could be spurious.
Our feature selection process is fairly simple:
we (1) score each affix by multiplying its fre-
quency (i.e., the number of distinct words in V to
which each affix attaches) and its length2, and (2)
select only those whose score is above a certain
threshold. In our experiments, we set this thresh-
old to 50, and generate our vocabulary of 140K
words from five years of articles taken from the
Bengali newspaper Prothom Alo. This enables us
to induce 979 prefixes and 975 suffixes.
4 Semantic Class Induction from
Wikipedia
Wikipedia has recently been used as a knowl-
edge source for various language processing tasks,
including taxonomy construction (Ponzetto and
Strube, 2007a), coreference resolution (Ponzetto
and Strube, 2007b), and English NER (e.g.,
Bunescu and Pas?ca (2006), Cucerzan (2007),
Kazama and Torisawa (2007), Watanabe et al
(2007)). Unlike previous work on using Wikipedia
for NER, our goal here is to (1) generate a list
of phrases and tokens that are potentially named
entities from the 16914 articles in the Bengali
Wikipedia3 and (2) heuristically annotate each of
them with one of four classes, namely, PER (per-
son), ORG (organization), LOC (location), or OTH-
ERS (i.e., anything other than PER, ORG and LOC).
4.1 Generating an Annotated List of Phrases
We employ the steps below to generate our anno-
tated list.
Generating and annotating the titles Recall
that each Wikipedia article has been optionally as-
signed to one or more categories by its creator
and/or editors. We use these categories to help an-
notate the title of an article. Specifically, if an ar-
ticle has a category whose name starts with ?Born
on? or ?Death on,? we label the corresponding ti-
tle with PER. Similarly, if it has a category whose
name starts with ?Cities of? or ?Countries of,? we
2The dependence on frequency and length is motivated by
the observation that less frequent and shorter affixes are more
likely to be erroneous (see Goldsmith (2001)).
3See http://bn.wikipedia.org. In our experiments, we used
the Bengali Wikipedia dump obtained on October 22, 2007.
356
NE Class Keywords
PER ?born,? ?died,? ?one,? ?famous?
LOC ?city,? ?area,? ?population,? ?located,? ?part of?
ORG ?establish,? ?situate,? ?publish?
Table 1: Keywords for each named entity class
label the title as LOC. If an article does not be-
long to one of the four categories above, we label
its title with the help of a small set of seed key-
words shown in Table 1. Specifically, for each of
the three NE classes shown on the left of Table
1, we compute a weighted sum of its keywords:
a keyword that appears in the first paragraph has
a weight of 3, a keyword that appears elsewhere
in the article has a weight of 1, and a keyword
that does not appear in the article has a weight of
0. The rationale behind using different weights is
simple: the first paragraph is typically a brief ex-
position of the title, so it should in principle con-
tain words that correlate more closely with the ti-
tle than words appearing in the rest of the article.
We then label the title with the class that has the
largest weighted sum. Note, however, that we ig-
nore any article that contains fewer than two key-
words, since we do not have reliable evidence for
labeling its title as one of the NE classes. We put
all these annotated titles into a title list.
Getting more location names To get more loca-
tion names, we search for the character sequences
?birth place:? and ?death place:? in each article,
extracting the phrase following any of these se-
quences and label it as LOC. We put all such la-
beled locations into the title list.
Generating and annotating the tokens in the ti-
tles Next, we extract the word tokens from each
title in the title list and label each token with an
NE class. The reason for doing this is to improve
generalization: if ?Dhaka University? is labeled as
ORG in the title list, then it is desirable to also label
the token ?University? as ORG, because this could
help identify an unseen phrase that contains the
term ?University? as an organization. Our token
labeling method is fairly simple. First, we gener-
ate the tokens from each title in the title list, as-
signing to each token the same NE label as that
of the title from which it is generated. For in-
stance, from the title ?Anna Frank,? ?Anna? will
be labeled as PER; and from ?Anna University,? ?
Anna? will be labeled as LOC. To resolve such
ambiguities (i.e., assigning different labels to the
same token), we keep a count of how many times
?Anna? is labeled with each NE class, and set its
final label to be the most frequent NE class. We
put all these annotated tokens into a token list. If
the title list and the token list have an element in
common, we remove the element from the token
list, since we have a higher confidence in the la-
bels of the titles.
Merging the lists Finally, we append the token
list to the title list. The resulting title list contains
4885 PERs, 15176 LOCs, and 188 ORGs.
4.2 Applying the Annotated List to a Text
We can now use the title list to annotate a text.
Specifically, we process each word w in the text in
a left-to-right manner, using the following steps:
1. Check whether w has been labeled. If so, we
skip this word and process the next one.
2. Check whether w appears in the Samsad
Bengali-English Dictionary4. If so, we as-
sume that w is more likely to be used as a
non-named entity, thus leaving the word un-
labeled and processing the next word instead.
3. Find the longest unlabeled word sequence5
that begins with w and appears in the title
list. If no such sequence exists, we leave w
unlabeled and process the next word. Oth-
erwise, we label it with the NE tag given
by the title list. To exemplify, consider a
text that starts with the sentence ?Smith Col-
lege is in Massachusetts.? When processing
?Smith,? ?Smith College? is the longest se-
quence that starts with ?Smith? and appears
in the title list (as an ORG). As a result, we
label all occurrences of ?Smith College? in
the text as an ORG. (Note that without using
the longest match heuristic, ?Smith? would
likely be mislabeled as PER.) In addition, we
take the last word of the ORG (which in this
case is ?College?) and annotate each of its oc-
currence in the rest of the text as ORG.6
These automatic annotations will then be used
to derive a set of WIKI features for training our
POS tagger and NE recognizer. Hence, unlike
existing Bengali NE recognizers, our ?gazetteers?
are induced rather than manually created.
4See http://dsal.uchicago.edu/dictionaries/biswasbengali/.
5This is a sequence in which each word is unlabeled.
6However, if we have a PER match (e.g., ?Anna Frank?)
or a LOC match (e.g., ?Las Vegas?), we take each word in the
matched phrase and label each of its occurrence in the rest of
the text with the same NE tag.
357
Current word wi
Previous word wi?1
2nd previous word wi?2
Next word wi+1
2nd next word wi+2
Current pseudo-affixes pfi (prefix), sfi (suffix)
Current induced affixes pii (prefix), sii (suffix)
Previous induced affixes pii?1 (prefix), sii?1 (suffix)
Induced affix bigrams pii?1pii (prefix), sii?1sii (suffix)
Current Wiki tag wikii
Previous Wiki tag wikii?1
Wiki bigram wikii?1wikii
Word bigrams wi?2wi?1, wi?1wi, wiwi+1,
wi+1wi+2
Word trigrams wi?2wi?1wi
Current number qi
Table 2: Feature templates for the POS tagging
experiments
5 Part-of-Speech Tagging
In this section, we will show how we train and
evaluate our POS tagger. As mentioned before, we
hypothesize that introducing our two knowledge
sources into the feature set for the tagger could
improve its performance: using the induced affixes
could improve the extraction of grammatical infor-
mation from the words, and using the Wikipedia-
induced list, which in principle should comprise
mostly of names, could help improve the identifi-
cation of proper nouns.
Corpus Our corpus is composed of 77942 words
and is annotated with one of 26 POS tags in the
tagset defined by IIIT Hyderabad7. Using this cor-
pus, we perform 5-fold cross-validation (CV) ex-
periments in our evaluation. It is worth noting that
this dataset has a high unknown word rate of 15%
(averaged over the five folds), which is due to the
small size of the dataset. While this rate is compa-
rable to another Bengali POS dataset described in
Dandapat et al (2007), it is much higher than the
2.6% unknown word rate in the test set for Ratna-
parkhi?s (1996) English POS tagging experiments.
Creating training instances Following previ-
ous work on POS tagging, we create one train-
ing instance for each word in the training set. The
class value of an instance is the POS tag of the cor-
responding word. Each instance is represented by
a set of linguistic features, as described next.
7A detailed description of these POS tags can be found in
http://shiva.iiit.ac.in/SPSAL2007/iiit tagset guidelines.pdf,
and are omitted here due to space limitations. This tagset
and the Penn Treebank tagset differ in that (1) nouns do not
have a number feature; (2) verbs do not have a tense feature;
and (3) adjectives and adverbs are not subcategorized.
Features Our feature set consists of (1) base-
line features motivated by those used in Danda-
pat et al?s (2007) Bengali POS tagger and Singh
et al?s (2006) Hindi POS tagger, as well as (2)
features derived from our induced affixes and the
Wikipedia-induced list. More specifically, the
baseline feature set has (1) word unigrams, bi-
grams and trigrams; (2) pseudo-affix features that
are created by taking the first three characters and
the last three characters of the current word; and
(3) a binary feature that determines whether the
current word is a number. As far as our new fea-
tures are concerned, we create one induced prefix
feature and one induced suffix feature from both
the current word and the previous word, as well
as two bigrams involving induced prefixes and in-
duced suffixes. We also create three WIKI features,
including the Wikipedia-induced NE tag of the
current word and that of the previous word, as well
as the combination of these two tags. Note that
the Wikipedia-induced tag of a word can be ob-
tained by annotating the test sentence under con-
sideration using the list generated from the Ben-
gali Wikipedia (see Section 4). To make the de-
scription of these features more concrete, we show
the feature templates in Table 2.
Learning algorithm We used CRF++8, a C++
implementation of conditional random fields (Laf-
ferty et al, 2001), as our learning algorithm for
training a POS tagging model.
Evaluating the model To evaluate the resulting
POS tagger, we generate test instances in the same
way as the training instances. 5-fold CV results of
the POS tagger are shown in Table 3. Each row
consists of three numbers: the overall accuracy,
as well as the accuracies on the seen and the un-
seen words. Row 1 shows the accuracy when the
baseline feature set is used; row 2 shows the ac-
curacy when the baseline feature set is augmented
with our two induced affix features; and the last
row shows the results when both the induced af-
fix and the WIKI features are incorporated into
the baseline feature set. Perhaps not surprisingly,
(1) adding more features improves performance,
and (2) accuracies on the seen words are substan-
tially better than those on the unseen words. In
fact, adding the induced affixes to the baseline fea-
ture set yields a 7.8% reduction in relative error
in overall accuracy. We also applied a two-tailed
paired t-test (p < 0.01), first to the overall accura-
8Available from http://crfpp.sourceforge.net
358
Experiment Overall Seen Unseen
Baseline 89.83 92.96 72.08
Baseline+Induced Affixes 90.57 93.39 74.64
Baseline+Induced Affixes+Wiki 90.80 93.50 75.58
Table 3: 5-fold cross-validation accuracies for
POS tagging
Predicted Tag Correct Tag % of Error
NN NNP 22.7
NN JJ 9.6
JJ NN 7.4
NNP NN 5.0
NN VM 4.9
Table 4: Most frequent errors for POS tagging
cies in rows 1 and 2, and then to the overall accu-
racies in rows 2 and 3. Both pairs of numbers are
statistically significantly different from each other,
meaning that incorporating the two induced affix
features and then the WIKI features both yields sig-
nificant improvements.
Error analysis To better understand the results,
we examined the errors made by the tagger. The
most frequent errors are shown in Table 4. From
the table, we see that the largest source of errors
arises from mislabeling proper nouns as common
nouns. This should be expected, as proper noun
identification is difficult due to the lack of capital-
ization information. Unfortunately, failure to iden-
tify proper nouns could severely limit the recall of
an NE recognizer. Also, adjectives and common
nouns are difficult to distinguish, since these two
syntactic categories are morphologically and dis-
tributionally similar to each other. Finally, many
errors appear to involve mislabeling a word as a
common noun. The reason is that there is a larger
percentage of common nouns (almost 30%) in the
training set than other POS tags, thus causing the
model to prefer tagging a word as a common noun.
6 Named Entity Recognition
In this section, we show how to train and evaluate
our NE recognizer. The recognizer adopts a tradi-
tional architecture, assuming that POS tagging is
performed prior to NER. In other words, the NE
recognizer will use the POS acquired in Section 5
as one of its features. As in Section 5, we will fo-
cus on examining how our knowledge sources (the
induced affixes and the WIKI features) impact the
performance of our recognizer.
Corpus The corpus we used for NER evaluation
is the same as the one described in the previous
POS of current word ti
POS of previous word ti?1
POS of 2nd previous word ti?2
POS of next word ti+1
POS of 2nd next word ti+2
POS bigrams ti?2ti?1, ti?1ti, titi+1, ti+1ti+2
First word fwi
Table 5: Additional feature templates for the NER
experiments
section. Specifically, in addition to POS infor-
mation, each sentence in the corpus is annotated
with NE information. We focus on recognizing the
three major NE types in this paper, namely persons
(PER), organizations (ORG), and locations (LOC).
There are 1721 PERs, 104 ORGs, and 686 LOCs in
the corpus. As far as evaluation is concerned, we
conduct 5-fold CV experiments, dividing the cor-
pus into the same five folds as in POS tagging.
Creating training instances We view NE
recognition as a sequence labeling problem. In
other words, we combine NE identification and
classification into one step, labeling each word in
a test text with its NE tag. Any word that does not
belong to one of our three NE tags will be labeled
as OTHERS. We adopt the IOB convention, pre-
ceding an NE tag with a B if the word is the first
word of an NE and an I otherwise. Now, to train
the NE recognizer, we create one training instance
from each word in a training text. The class value
of an instance is the NE tag of the corresponding
word, or OTHERS if the word is not part of an NE.
Each instance is represented by a set of linguistic
features, as described next.
Features Our feature set consists of (1) base-
line features motivated by those used in Ekbal
et al?s (2008) Bengali NE recognizer, as well as
(2) features derived from our induced affixes and
the Wikipedia-induced list. More specifically, the
baseline feature set has (1) word unigrams; (2)
pseudo-affix features that are created by taking the
first three characters and the last three characters
of the current word; (3) a binary feature that deter-
mines whether the current word is the first word of
a sentence; and (4) a set of POS-related features,
including the POS of the current word and its sur-
rounding words, as well as POS bigrams formed
from the current and surrounding words. Our in-
duced affixes and WIKI features are incorporated
into the baseline NE feature set in the same man-
ner as in POS tagging. In essence, the feature tem-
359
Experiment R P F
Baseline 60.97 74.46 67.05
Person 66.18 74.06 69.90
Organization 29.81 44.93 35.84
Location 52.62 80.40 63.61
Baseline+Induced Affixes 60.45 73.30 66.26
Person 65.70 72.61 69.02
Organization 31.73 46.48 37.71
Location 51.46 80.05 62.64
Baseline+Induced Affixes+Wiki 63.24 75.19 68.70
Person 66.47 75.16 70.55
Organization 30.77 43.84 36.16
Location 60.06 79.69 68.50
Table 6: 5-fold cross-validation results for NER
plates employed by the NE recognizer are the top
12 templates in Table 2 and those in Table 5.
Learning algorithm We again use CRF++ as
our sequence learner for acquiring the recognizer.
Evaluating the model To evaluate the resulting
NE tagger, we generate test instances in the same
way as the training instances. To score the output
of the recognizer, we use the CoNLL-2000 scor-
ing program9, which reports performance in terms
of recall (R), precision (P), and F-measure (F). All
NE results shown in Table 6 are averages of the
5-fold CV experiments. The first block of the Ta-
ble 6 shows the overall results when the baseline
feature set is used; in addition, we also show re-
sults for each of the three NE tags. As we can see,
the baseline achieves an F-measure of 67.05. The
second block shows the results when the baseline
feature set is augmented with our two induced af-
fix features. Somewhat unexpectedly, F-measure
drops by 0.8% in comparison to the baseline. Ad-
ditional experiments are needed to determine the
reason. Finally, when the WIKI features are in-
corporated into the augmented feature set, the sys-
tem achieves an F-measure of 68.70 (see the third
block), representing a statistically significant in-
crease of 1.6% in F-measure over the baseline.
As we can see, improvements stem primarily from
dramatic gains in recall for locations.
Discussions Several points deserve mentioning.
First, the model performs poorly on the ORGs, ow-
ing to the small number of organization names
in the corpus. Worse still, the recall drops after
adding the WIKI features. We examined the list
of induced ORG names and found that it is fairly
noisy. This can be attributed in part to the diffi-
culty in forming a set of seed words that can ex-
tract ORGs with high precision (e.g., the ORG seed
?situate? extracted many LOCs). Second, using the
9http://www.cnts.ua.ac.be/conll2000/chunking/conlleval.txt
WIKI features does not help recalling the PERs. A
closer examination of the corpus reveals the rea-
son: many sentences describe fictitious characters,
whereas Wikipedia would be most useful for arti-
cles that describe famous people. Overall, while
the WIKI features provide our recognizer with a
small, but significant, improvement, the useful-
ness of the Bengali Wikipedia is currently lim-
ited by its small size. Nevertheless, we believe the
Bengali Wikipedia will become a useful resource
for language processing as its size increases.
7 A Joint Model for POS Tagging and
NER
The NE recognizer described thus far has adopted
a pipelined architecture, and hence its perfor-
mance could be limited by the errors of the POS
tagger. In fact, as discussed before, the major
source of errors made by our POS tagger concerns
the confusion between proper nouns and common
nouns, and this type of error, when propagated
to the NE recognizer, could severely limit its re-
call. Also, there is strong empirical support for
this argument: the NE recognizers, when given ac-
cess to the correct POS tags, have F-scores rang-
ing from 76-79%, which are 10% higher on aver-
age than those with POS tags that were automat-
ically computed. Consequently, we hypothesize
that modeling POS tagging and NER jointly would
yield better performance than learning the two
tasks separately. In fact, many approaches have
been developed to jointly model POS tagging and
noun phrase chunking, including transformation-
based learning (Ngai and Florian, 2001), factorial
HMMs (Duh, 2005), and dynamic CRFs (Sutton
et al, 2007). Some of these approaches are fairly
sophisticated and also require intensive computa-
tions during inference. For instance, when jointly
modeling POS tagging and chunking, Sutton et al
(2007) reduce the number of POS tags from 45
to 5 when training a factorial dynamic CRF on a
small dataset (with only 209 sentences) in order to
reduce training and inference time.
In contrast, we propose a relatively simple
model for jointly learning Bengali POS tagging
and NER, by exploiting the limited dependencies
between the two tasks. Specifically, we make the
observation that most of the Bengali words that are
part of an NE are also proper nouns. In fact, based
on statistics collected from our evaluation corpus
(see Sections 5 and 6), this observation is correct
360
Experiment R P F
Baseline 54.76 81.70 65.57
Baseline+Induced Affixes 56.79 88.96 69.32
Baseline+Induced Affixes+Wiki 61.73 86.35 71.99
Table 7: 5-fold cross-validation joint modeling re-
sults for NER
97.3% of the time. Note, however, that this ob-
servation does not hold for English, since many
prepositions and determiners are part of an NE.
On the other hand, this observation largely holds
for Bengali because prepositions and determiners
are typically realized as noun suffixes.
This limited dependency between the POS tags
and the NE tags allows us to develop a simple
model for jointly learning the two tasks. More
specifically, we will use CRF++ to learn the joint
model. Training and test instances are generated
as described in the previous two subsections (i.e.,
one instance per word). The feature set will con-
sist of the union of the features that were used to
train the POS tagger and the NE tagger indepen-
dently, minus the POS-related features that were
used in the NE tagger. The class value of an in-
stance is computed as follows. If a word is not a
proper noun, its class is simply its POS tag. Oth-
erwise, its class is its NE tag, which can be PER,
ORG, LOC, or OTHERS. In other words, our joint
model exploits the observation that we made ear-
lier in the section by assuming that only proper
nouns can be part of a named entity. This allows
us to train a joint model without substantially in-
creasing the number of classes.
We again evaluate our joint model using 5-fold
CV experiments. The NE results of the model are
shown in Table 7. The rows here can be interpreted
in the same manner as those in Table 6. Compar-
ing these three experiments with their counterparts
in Table 6, we can see that, except for the base-
line, jointly modeling offers a significant improve-
ment of 3.3% in overall F-measure.10 In particu-
lar, the joint model benefits significantly from our
10The POS tagging results are not shown due to space lim-
itations. Overall, the POS accuracies drop insignificantly as
a result of joint modeling, for the following reason. Recall
from Section 5 that the major source of POS tagging errors
arises from the mislabeling of many proper nouns as com-
mon nouns, due primarily to the large number of common
nouns in the corpus. The joint model aggravates this prob-
lem by subcategorizing the proper nouns into different NE
classes, causing the tagger to have an even stronger bias to-
wards labeling a proper noun as a common noun than before.
Nevertheless, as seen from the results in Tables 6 and 7, such
a bias has yielded an increase in NER precision.
two knowledge sources, achieving an F-measure
of 71.99% when both of them are incorporated.
Finally, to better understand the value of the in-
duced affix features in the joint model as well as
the pipelined model described in Section 6, we
conducted an ablation experiment, in which we in-
corporated only the WIKI features into the base-
line feature set. With pipelined modeling, the F-
measure for NER is 68.87%, which is similar to
the case where both induced affixes and the WIKI
features are used. With joint modeling, however,
the F-measure for NER is 70.87%, which is 1%
lower than the best joint modeling score. These
results provide suggestive evidence that the in-
duced affix features play a significant role in the
improved performance of the joint model.
8 Conclusions
We have explored two types of linguistic fea-
tures, namely the induced affix features and the
Wikipedia-related features, to improve a Bengali
POS tagger and NE recognizer. Our experimen-
tal results have demonstrated that (1) both types of
features significantly improve a baseline POS tag-
ger and (2) the Wikipedia-related features signif-
icantly improve a baseline NE recognizer. More-
over, by exploiting the limited dependencies be-
tween Bengali POS tags and NE tags, we pro-
posed a new model for jointly learning the two
tasks, which not only avoids the error-propagation
problem present in the pipelined system architec-
ture, but also yields statistically significant im-
provements over the NE recognizer that is trained
independently of the POS tagger. When applied in
combination, our three extensions contributed to a
relative improvement of 7.5% in F-measure over
the baseline NE recognizer. Most importantly, we
believe that these extensions are of relevance and
interest to the EACL community because many
European and Middle Eastern languages resemble
Bengali in terms of not only their morphological
richness but also their scarcity of annotated cor-
pora. We plan to empirically verify our belief in
future work.
Acknowledgments
We thank the three anonymous reviewers for their
invaluable comments on the paper. We also thank
CRBLP, BRAC University, Bangladesh, for pro-
viding us with Bengali resources. This work was
supported in part by NSF Grant IIS-0812261.
361
References
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Pro-
ceedings of COLT, pages 92?100.
Razvan Bunescu and Marius Pas?ca. 2006. Using en-
cyclopedic knowledge for named entity disambigua-
tion. In Proceedings of EACL, pages 9?16.
Alexander Clark. 2003. Combining distributional and
morphological information for part-of-speech induc-
tion. In Proceedings of EACL, pages 59?66.
Michael Collins and Yoram Singer. 1999. Unsuper-
vised models for named entity classification. In Pro-
ceedings of EMNLP/VLC, pages 100?110.
Silviu Cucerzan and David Yarowsky. 1999. Lan-
guage independent named entity recognition com-
bining morphological and contextual evidence. In
Proceedings of EMNLP/VLC, pages 90?99.
Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on Wikipedia data. In Proceed-
ings of EMNLP-CoNLL, pages 708?716.
Sandipan Dandapat, Sudeshna Sarkar, and Anupam
Basu. 2007. Automatic part-of-speech tagging for
Bengali: An approach for morphologically rich lan-
guages in a poor resource scenario. In Proceedings
of the ACL Companion Volume, pages 221?224.
Kevin Duh. 2005. Jointly labeling multiple sequences:
A factorial HMM approach. In Proceedings of the
ACL Student Research Workshop, pages 19?24.
Asif Ekbal, Rejwanul Haque, and Sivaji Bandyopad-
hyay. 2008. Named entity recognition in Bengali:
A conditional random field approach. In Proceed-
ings of IJCNLP, pages 589?594.
John Goldsmith. 2001. Unsupervised learning of the
morphology of a natural language. Computational
Linguistics, 27(2):153?198.
Sharon Goldwater and Thomas L. Griffiths. 2007.
A fully Bayesian approach to unsupervised part-of-
speech tagging. In Proceedings of the ACL, pages
744?751.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
HLT-NAACL, pages 320?327.
Jun?ichi Kazama and Kentaro Torisawa. 2007. Ex-
ploiting Wikipedia as external knowledge for named
entity recognition. In Proceedings of EMNLP-
CoNLL, pages 698?707.
Samarth Keshava and Emily Pitler. 2006. A simpler,
intuitive approach to morpheme induction. In PAS-
CAL Challenge Workshop on Unsupervised Segmen-
tation of Words into Morphemes.
Zornitsa Kozareva. 2006. Bootstrapping named entity
recognition with automatically generated gazetteer
lists. In Proceedings of the EACL Student Research
Workshop, pages 15?22.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of ICML, pages 282?
289.
Andrei Mikheev, Marc Moens, and Claire Grover.
1999. Named entity recognition without gazetteers.
In Proceedings of EACL, pages 1?8.
Grace Ngai and Radu Florian. 2001. Transformation
based learning in the fast lane. In Proceedings of
NAACL, pages 40?47.
Simone Paolo Ponzetto and Michael Strube. 2007a.
Deriving a large scale taxonomy from wikipedia. In
Proceedings of AAAI, pages 1440?1445.
Simone Paolo Ponzetto and Michael Strube. 2007b.
Knowledge derived from Wikipedia for computing
semantic relatedness. Journal of Artificial Intelli-
gence Research, 30:181?212.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Proceedings
of EMNLP, pages 133?142.
Hinrich Schu?tze. 1995. Distributional part-of-speech
tagging. In Proceedings of EACL, pages 141?148.
Smriti Singh, Kuhoo Gupta, Manish Shrivastava, and
Pushpak Bhattacharyya. 2006. Morphological rich-
ness offsets resource demand ? Experiences in con-
structing a POS tagger for Hindi. In Proceedings
of the COLING/ACL 2006 Main Conference Poster
Sessions, pages 779?786.
Charles Sutton, Andrew McCallum, and Khashayar
Rohanimanesh. 2007. Dynamic conditional random
fields: Factorized probabilistic models for labeling
and segmenting sequence data. Journal of Machine
Learning Research, 8:693?723.
Yotaro Watanabe, Masayuki Asahara, and Yuji Mat-
sumoto. 2007. A graph-based approach to named
entity categorization in Wikipedia using conditional
random fields. In Proceedings of EMNLP-CoNLL,
pages 649?657.
362
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 931?939,
Beijing, August 2010
Inducing Fine-Grained Semantic Classes via
Hierarchical and Collective Classification
Altaf Rahman and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
{altaf,vince}@hlt.utdallas.edu
Abstract
Research in named entity recognition and
mention detection has typically involved a
fairly small number of semantic classes,
which may not be adequate if seman-
tic class information is intended to sup-
port natural language applications. Moti-
vated by this observation, we examine the
under-studied problem of semantic sub-
type induction, where the goal is to au-
tomatically determine which of a set of
92 fine-grained semantic classes a noun
phrase belongs to. We seek to improve the
standard supervised approach to this prob-
lem using two techniques: hierarchical
classification and collective classification.
Experimental results demonstrate the ef-
fectiveness of these techniques, whether
or not they are applied in isolation or in
combination with the standard approach.
1 Introduction
Semantic class determination refers to the task
of classifying a noun phrase (NP), be it a name
or a nominal, as one of a set of pre-defined se-
mantic classes. A semantic class classifier is a
basic text-processing component in many high-
level natural language processing (NLP) applica-
tions, including information-extraction (IE) sys-
tems and question-answering (QA) systems. In
recent years, supervised semantic class determi-
nation has been tackled primarily in the context of
(1) coreference resolution (e.g., Ng (2007), Huang
et al (2009)), where semantic classes are induced
and subsequently used to disallow coreference be-
tween semantically incompatible NPs, and (2) the
mention detection task in the ACE evaluations
(e.g., Florian et al (2004; 2006)), where the goal
is to identify the boundary of a mention (i.e., a
noun phrase that belongs to one of the pre-defined
ACE semantic classes), its mention type (e.g., pro-
noun, name), and its semantic class. The output
of a mention detector is then used by downstream
IE components, which typically include a coref-
erence resolution system and a relation extraction
system. Owing in part to its potentially large in-
fluence on downstream IE components, accurate
semantic class determination is crucial.
Over the years, NLP researchers have focused
on a relatively small number of semantic classes in
both NE recognition and mention detection: seven
classes in the MUC-6 and MUC-7 NE recognition
task, four classes in the CoNLL 2002 and 2003
NE recognition shared task, and seven classes in
the ACE 2005 mention detection task. Given that
one of the uses of semantic class information is
to support NLP applications, it is questionable
whether this purpose can be adequately served by
such a small number of semantic classes. For ex-
ample, given the question ?Which city was the
first Olympic Games held in??, it would be help-
ful for a QA system to know which NEs are cities.
However, virtually all of the existing NE recog-
nizers and mention detectors can only determine
whether an NE is a location or not.
Our goal in this paper is to tackle the under-
studied problem of determining fine-grained se-
mantic classes (henceforth semantic subtypes).
More specifically, we aim to classify an NP as
one of the 92 fine-grained, domain-independent
semantic classes that are determined to be use-
ful for supporting the development of QA and
931
IE systems in the ACE and AQUAINT programs.
These 92 semantic subtypes have been used to
manually annotate the NPs in the BBN Entity Type
Corpus (Weischedel and Brunstein, 2005). Given
the availability of this semantic subtype-annotated
corpus, we adopt a supervised machine learn-
ing approach to semantic subtype determination.
Specifically, given (the boundary of) an NP, we
train a classification model to determine which of
the 92 semantic subtypes it belongs to.
More importantly, we seek to improve the stan-
dard approach to semantic subtype induction de-
scribed above by proposing two techniques. The
first technique, collective classification, aims to
address a common weakness in the standard su-
pervised learning paradigm, where a classifier
classifies each instance independently of the oth-
ers and is unable to exploit any relational informa-
tion between a pair (or a subset) of the instances
that may be helpful for classification. The sec-
ond technique, hierarchical classification, exploits
the observation that these 92 semantic subtypes
can be grouped into a smaller number of coarse-
grained semantic types (henceforth semantic su-
pertypes). With this two-level hierarchy, learning
can proceed in a sequential fashion: given an NP,
we first determine its semantic supertype and then
classify it as one of the semantic subtypes that
fall under the predicted supertype in the hierar-
chy. Empirical results show that these two tech-
niques, when applied in isolation to the standard
learning approach to subtype induction, can sig-
nificantly improve its accuracy, and the best result
is achieved when they are applied in combination.
The rest of the paper is organized as follows.
Section 2 provides an overview of the 92 seman-
tic subtypes and the evaluation corpus. In Sec-
tion 3, we present our baseline semantic subtype
classification system. Sections 4 and 5 introduce
collective classification and hierarchical classifi-
cation respectively, and describe how these two
techniques can be used to improve the baseline
semantic subtype classifier. We show evaluation
results in Section 6 and conclude in Section 7.
2 Semantic Subtypes
As noted before, each name and nominal in the
BBN Entity Type Corpus is annotated with one of
the 92 semantic subtypes. In our experiments, we
use all the 200 Penn Treebank Wall Street Journal
articles in the corpus, yielding 17,292 NPs that are
annotated with their semantic subtypes.
Table 1 presents an overview of these subtypes.
Since they have been manually grouped into 29
supertypes, we also show the supertypes in the ta-
ble. More specifically, the first column shows the
supertypes, the second column contains a brief de-
scription of a supertype, and the last column lists
the subtypes that correspond to the supertype in
the first column. In cases where a supertype con-
tains only one subtype (e.g., PERSON), the super-
type is not further partitioned into different sub-
types; for classification purposes, we simply treat
the subtype as identical to its supertype (and hence
the two always have the same name). A detailed
description of these supertypes and subtypes can
be found in Weischedel and Brunstein (2005). Fi-
nally, we show the class distribution: the paren-
thesized number after each subtype is the percent-
age of the 17,292 NPs annotated with the subtype.
3 Baseline Classification Model
We adopt a supervised machine learning approach
to train our baseline classifier for determining the
semantic subtype of an NP. This section describes
the details of the training process.
Training corpus. As mentioned before, we use
the Wall Street Journal articles in the BBN Entity
Type Corpus for training the classifier.
Training instance creation. We create one
training instance for each annotated NP, NPi,
which is either a name or a nominal, in each train-
ing text. The classification of an instance is its an-
notated semantic subtype value, which is one of
the 92 semantic subtypes. Each instance is repre-
sented by a set of 33 features1, as described below.
1. Mention String (3): Three features are de-
rived from the string of NPi. Specifically, we em-
ploy the NP string as a feature. If NPi contains
more than one token, we create one feature for
each of its constituent tokens. Finally, to distin-
guish the different senses of a nominal, we create
1As we will see, since we employ an exponential model,
an instance may be represented by fewer than 33 features.
932
Supertype Brief Description Subtypes
PERSON Proper names of people. Person (9.2).
PERSON DESC Any head word of a common noun Person Desc (16.8).
referring to a person or group of people.
NORP This type is named after its subtypes: Nationality (2.9), Religion (0.1), Political (0.6),
nationality, religion, political, etc. Other (0.1).
FACILITY Names of man-made structures, including Building (0.1), Bridge (0.02), Airport (0.01),
infrastructure, buildings, monuments, Attraction (0.01), Highway Street (0.05),
camps, farms, mines, ports, etc. Other (0.1).
FACILITY DESC Head noun of a noun phrase describing Building (0.5), Bridge (0.05), Airport (0.01),
buildings, bridges, airports, etc. Highway Street (0.2), Attraction (0.02), Other (0.5).
ORGANIZATION Names of companies, government Government (3.6), Corporation (8.3), Political (0.5),
agencies, educational institutions and Educational (0.3), Hotel (0.04), City (0.01),
other institutions. Hospital (0.01), Religious (0.1), Other (0.7).
ORG DESC Heads of descriptors of companies, Government (2.1), Corporation (4.3), Political (0.2),
educational institutions and other Educational (0.1), Religious (0.1), Hotel (0.1),
governments, government agencies, etc. City (0.01), Hospital (0.02), Other (0.7).
GPE Names of countries, cities, states, Country (4.2), City (3.2), State Province (1.4),
provinces, municipalities, boroughs. Other (0.1).
GPE DESC Heads of descriptors of countries, cities, Country (0.8), City (0.3), State Province (0.3),
states, provinces, municipalities. Other (0.1).
LOCATION Names of locations other than GPEs. River (0.03), Lake Sea Ocean (0.05), Region (0.2),
E.g., mountain ranges, coasts, borders, Continent (0.1), Other (0.2).
planets, geo-coordinates, bodies of water.
PRODUCT Name of any product. It does not Food (0.01), Weapon (0.02), Vehicle (0.2),
include the manufacturer). Other (0.2).
PRODUCT DESC Descriptions of weapons and vehicles Food (0.01), Weapon (0.2), Vehicle (0.97),
only. Cars, buses, machine guns, missiles, Other (0.02).
bombs, bullets, etc.
DATE Classify a reference to a date or period. Date (7.99), Duration (1.9), Age (0.5), Other (0.4).
TIME Any time ending with A.M. or P.M. Time (0.5).
PERCENT Percent symbol or the actual word percent. Percent (2.07).
MONEY Any monetary value. Money (2.9).
QUANTITY Used to classify measurements. E.g., 4 1D (0.11), 2D (0.08), 3D (0.1), Energy (0.01),
miles, 4 grams, 4 degrees, 4 pounds, etc. Speed (0.01), Weight (0.1), Other (0.04).
ORDINAL All ordinal numbers. E.g., First, fourth. Ordinal (0.6).
CARDINAL Numerals that provide a count or quantity. Cardinal (5.1).
EVENT Named hurricanes, battles, wars, sports War (0.03), Hurricane (0.1), Other (0.24).
events, and other named events.
PLANT Any plant, flower, tree, etc. Plant (0.2).
ANIMAL Any animal class or proper name of an Animal (0.7).
animal, real or fictional.
SUBSTANCE Any chemicals, elements, drugs, and Food (1.1), Drug (0.46), Chemical (0.23), Other (0.9).
foods. E.g., boron, penicillin, plutonium.
DISEASE Any disease or medical condition. Disease (0.6).
LAW Any document that has been made into Law (0.5).
a law. E.g., Bill of Rights, Equal Rights.
LANGUAGE Any named language. Language (0.2).
CONTACT INFO Address, phone. Address (0.01), Phone (0.04).
GAME Any named game. Game (0.1).
WORK OF ART Titles of books, songs and other creations. Book (0.16), Play (0.04), Song (0.03), Painting (0.01),
Other (0.4).
Table 1: The 92 semantic subtypes and their corresponding supertypes.
a feature whose value is the concatenation of the
head of NPi and its WordNet sense number.2
2We employ the sense number that is manually annotated
for each NP in the WSJ corpus as part of the OntoNotes
project (Hovy et al, 2006).
2. Verb String (3): If NPi is governed by a verb,
the following three features are derived from the
governing verb. First, we employ the string of the
governing verb as a feature. Second, we create
a feature whose value is the semantic role of the
933
governing verb.3 Finally, to distinguish the differ-
ent senses of the governing verb, we create a fea-
ture whose value is the concatenation of the verb
and its WordNet sense number.
3. Semantic (5): We employ five semantic fea-
tures. First, if NPi is an NE, we create a feature
whose value is the NE label of NPi, as determined
by the Stanford CRF-based NE recognizer (Finkel
et al, 2005). However, if NPi is a nominal, we cre-
ate a feature that encodes the WordNet semantic
class of which it is a hyponym, using the manu-
ally determined sense of NPi.4 Moreover, to im-
prove generalization, we employ a feature whose
value is the WordNet synset number of the head
noun of a nominal. If NPi has a governing verb,
we also create a feature whose value is the Word-
Net synset number of the verb. Finally, if NPi is a
nominal, we create a feature based on its WordNet
equivalent concept. Specifically, for each entity
type defined in ACE 20055, we create a list con-
taining all the word-sense pairs in WordNet (i.e.,
synsets) whose glosses are compatible with that
entity type.6 Then, given NPi and its sense, we use
these lists to determine if it belongs to any ACE
2005 entity type. If so, we create a feature whose
value is the corresponding entity type.
4. Morphological (8). If NPi is a nominal, we
create eight features: prefixes and suffixes of
length one, two, three, and four.
5. Capitalization (4): We create four cap-
italization features to determine whether NPi
IsAllCap, IsInitCap, IsCapPeriod, and
IsAllLower (see Bikel et al (1999)).
6. Gazetteers (8): We compute eight gazetteer-
based features, each of which checks whether NPi
is in a particular gazetteer. The eight dictionaries
contain pronouns (77 entries), common words and
words that are not names (399.6k), person names
(83.6k), person titles and honorifics (761), vehi-
3We also employ the semantic role that is manually anno-
tated for each NP in the WSJ corpus in OntoNotes.
4The semantic classes we considered are person, location,
organization, date, time, money, percent, and object.
5The ACE 2005 entity types include person, organization,
GPE, facility, location, weapon, and vehicle.
6Details of how these lists are constructed can be found
in Nicolae and Nicolae (2006).
cle words (226), location names (1.8k), company
names (77.6k), and nouns extracted from Word-
Net that are hyponyms of PERSON (6.3k).
7. Grammatical (2): We create a feature that
encodes the part-of-speech (POS) sequence of NPi
obtained via the Stanford POS tagger (Toutanova
et al, 2003). In addition, we have a feature that
determines whether NPi is a nominal or not.
We employ maximum entropy (MaxEnt) mod-
eling7 for training the baseline semantic subtype
classifier. MaxEnt is chosen because it provides
a probabilistic classification for each instance,
which we will need to perform collective classi-
fication, as described in the next section.
4 Collective Classification
One weakness of the baseline classification model
is that it classifies each instance independently. In
particular, the model cannot take into account re-
lationships between them that may be helpful for
improving classification accuracy. For example,
if two NPs are the same string in a given doc-
ument, then it is more likely than not that they
have the same semantic subtype according to the
?one sense per discourse? hypothesis (Gale et al,
1992). Incorporating this kind of relational infor-
mation into the feature set employed by the base-
line system is not an easy task, since each feature
characterizes only a single NP.
To make use of the relational information, one
possibility is to design a new learning procedure.
Here, we adopt a different approach: we perform
collective classification, or joint probabilistic in-
ference, on the output of the baseline model. The
idea is to treat the output for each NP, which is
a probability distribution over the semantic sub-
types, as its prior label/class distribution, and con-
vert it into a posterior label/class distribution by
exploiting the available relational information as
an additional piece of evidence. For this purpose,
we will make use of factor graphs. In this section,
we first give a brief overview of factor graphs8,
and show how they can be used to perform joint
7We use the MaxEnt implementation available at
http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html
8See Bunescu and Mooney (2004) and Loeliger (2004)
for a detailed introduction to factor graphs.
934
inference for semantic subtype determination.
4.1 Factor Graphs
Factor graphs model optimization problems of
an objective function g, which is a real-valued
function of n random variables X1, ..., Xn. We
assume that g can be decomposed into a product
of m factors. In other words, g (X1, ..., Xn) =
f1 (s1 (X1, ..., Xn)) ...fm (sm (X1, ..., Xn)),
where each factor fk is a real-valued function
of some subset of X1, ... , Xn, denoted as
sk (X1, ..., Xn). Each fk can be thought of as a
feature function that computes the compatibility
of an assignment of values to the variables in
sk (X1, ..., Xn) with respect to a user-defined
feature. Hence, a larger function value is more
desirable, as it corresponds to a more compatible
assignment of values to the variables involved.
A factor graph consists of two types of nodes:
variable nodes and factor nodes. Each random
variable Xi is represented by a variable node, and
each factor fk is represented by a factor node.
Each factor node fk is connected only to the nodes
corresponding to sk. This results in a bipartite
graph, where edges exist only between a variable
node and a factor node.
Given this graph, there are several methods for
finding an optimal assignment of the random vari-
ables X1, ..., Xn such that the objective function
g is maximized. Exact inference using the sum-
product algorithm (Kschischang et al, 2001) is
possible if there are no cycles in the graph; other-
wise a belief propagation algorithm, such as loopy
belief propagation (Murphy et al, 1999), can be
applied. Although there are no cycles in our factor
graphs, we choose to use loopy belief propagation
as our inferencer, since it performs approximate
inference and is therefore computationally more
efficient than an exact inferencer.
4.2 Application to Subtype Inference
To apply joint inference to semantic subtype in-
duction, we create one factor graph for each test
document, where each variable node is random
variable Xi over the set of semantic subtype la-
bels L and represents an NP, NPi, in the docu-
ment. To retain the prior probabilities over the
semantic subtype labels lq ? L obtained from the
baseline classification model, each variable node
is given a factor f (Xi) = P (Xi = lq). If no
additional factors that model the relation between
two nodes/instances are introduced, maximizing
the objective function for this graph (by maximiz-
ing the product of factors) will find an assignment
identical to the one obtained by taking the most
probable semantic subtype label assigned to each
instance by the baseline classifier.
Next, we exploit the relationship between two
random variables. Specifically, we want to en-
courage the inference algorithm to assign the
same label to two variables if there exists a rela-
tion between the corresponding NPs that can pro-
vide strong evidence that they should receive the
same label. To do so, we create a pairwise fac-
tor node that connects two variable nodes if the
aforementioned relation between the underlying
NPs is satisfied. However, to implement this idea,
we need to address two questions.
First, which relation between two NPs can pro-
vide strong evidence that they have the same se-
mantic subtype? We exploit the coreference re-
lation. Intuitively, the coreference relation is a
reasonable choice, as coreferent entities are likely
to have the same semantic subtype. Here, we
naively posit two NPs as coreferent if at least one
of the following conditions is satisfied: (1) they
are the same string after determiners are removed;
(2) they are aliases (i.e., one is an acronym or
abbreviation of the other); and (3) they are both
proper names and have at least one word in com-
mon (e.g., ?Delta? and ?Delta Airlines?).9
Second, how can we define a pairwise factor,
fpair, so that it encourages the inference algo-
rithm to assign the same label to two nodes? One
possibility is to employ the following definition:
fpair(Xi, Xj)
= P (Xi = lp, Xj = lq),where lp, lq ? L
=
{
1 if lp = lq
0 otherwise
In essence, fpair prohibits the assignment of dif-
ferent labels to the two nodes it connects. In our
9The third condition can potentially introduce many false
positives, positing ?Bill Clinton? and ?Hillary Clinton? as
coreferent, for instance. However, this kind of false positives
does not pose any problem for us, since the two NPs involved
belong to the same semantic subtype (i.e., PERSON).
935
experiments, however, we ?improve? fpair by in-
corporating semantic supertype information into
its definition, as shown below:
fpair(Xi, Xj)
= P (Xi = lp, Xj = lq),where lp, lq ? L
=
{
Psup(sup(lp)|NPi)Psup(sup(lq)|NPj) if lp = lq
0 otherwise
In this definition, sup(lq) is the supertype of lq
according to the semantic type hierarchy shown
in Section 2, and Psup(sup(lq)|NPj) is the proba-
bility that NPj belongs to sup(lq) according to the
semantic supertype classification model Psup (see
Section 5 for details on how this model can be
trained). In essence, we estimate the joint proba-
bility by (1) assuming that the two events are inde-
pendent, and then (2) computing each event using
supertype information. Intuitively, this definition
allows fpair to favor those label assignments that
are more compatible with the predictions of Psup.
After graph construction, we apply an infer-
encer to compute a marginal probability distribu-
tion over the labels for each node/instance in the
graph by maximizing the objective function g, and
output the most probable label for each instance
according to its marginal distribution.
5 Hierarchical Classification
The pairwise factor fpair defined above exploits
supertype information in a soft manner, meaning
that the most probable label assigned to an NP by
an inferencer is not necessarily consistent with its
predicted supertype (e.g., an NP may receive Ho-
tel as its subtype even if its supertype is PERSON).
In this section, we discuss how to use supertype
information for semantic subtype classification in
a hard manner so that the predicted subtype is
consistent with its supertype.
To exploit supertype information, we first train
a model, Psup, for determining the semantic su-
pertype of an NP using MaxEnt. This model is
trained in essentially the same way as the base-
line model described in Section 3. In particular,
it is trained on the same set of instances using the
same feature set as the baseline model. The only
difference is that the class value of each training
instance is the semantic supertype of the associ-
ated NP rather than its semantic subtype.
Next, we train 29 supertype-specific classifi-
cation models for determining the semantic sub-
type of an NP. For instance, the ORGANIZATION-
specific classification model will be used to clas-
sify an NP as belonging to one of its subtypes
(e.g., Government, Corporation, Political agen-
cies). A supertype-specific classification model is
trained much like the baseline model. Each in-
stance is represented using the same set of fea-
tures as in the baseline, and its class label is its
semantic subtype. The only difference is that the
model is only trained only on the subset of the
instances for which it is intended. For instance,
the ORGANIZATION-specific classification model
is trained only on instances whose class is a sub-
type of ORGANIZATION.
After training, we can apply the supertype clas-
sification model and the supertype-specific sub-
type classification model to determine the se-
mantic subtype of an NP in a hierarchical fash-
ion. Specifically, we first employ the supertype
model to determine its semantic supertype. Then,
depending on this predicted semantic supertype,
we use the corresponding subtype classification
model to determine its subtype.
6 Evaluation
For evaluation, we partition the 200 Wall Street
Journal Articles in the BBN Entity Type corpus
into a training set and a test set following a 80/20
ratio. As mentioned before, each text in the Entity
Type corpus has its NPs annotated with their se-
mantic subtypes. Test instances are created from
these texts in the same way as the training in-
stances described in Section 3. To investigate
whether we can benefit from hierarchical and col-
lective classifications, we apply these two tech-
niques to the Baseline classification model in iso-
lation and in combination, resulting in the four
sets of results in Tables 2 and 3.
The Baseline results are shown in the second
column of Table 2. Due to space limitations, it is
not possible to show the result for each semantic
subtype. Rather, we present semantic supertype
results, which are obtained by micro-averaging
the corresponding semantic subtype results and
are expressed in terms of recall (R), precision (P),
and F-measure (F). Note that only those semantic
936
Baseline only Baseline+Hierarchical
Semantic Supertype R P F R P F
1 PERSON 91.9 89.7 90.8 88.8 91.1 89.9
2 PERSON DESC 91.3 87.8 89.5 92.1 89.8 91.0
3 SUBSTANCE 60.0 66.7 63.2 70.0 58.3 63.6
4 NORP 87.8 90.3 89.0 91.9 90.7 91.3
5 FACILITY DESC 72.7 88.9 80.0 68.2 93.8 79.0
6 ORGANIZATION 76.6 73.8 75.2 78.5 73.2 75.8
7 ORG DESC 75.0 70.7 72.8 75.8 75.2 75.5
8 GPE 75.6 73.9 74.7 77.0 75.4 76.2
9 GPE DESC 60.0 75.0 66.7 70.0 70.0 70.0
10 PRODUCT DESC 53.3 88.9 66.7 53.3 88.9 66.7
11 DATE 85.0 85.0 85.0 84.5 85.4 85.0
12 PERCENT 100.0 100.0 100.0 100.0 100.0 100.0
13 MONEY 83.9 86.7 85.3 88.7 96.5 92.4
14 QUANTITY 22.2 100.0 36.4 66.7 66.7 66.7
15 ORDINAL 100.0 100.0 100.0 100.0 100.0 100.0
16 CARDINAL 96.0 77.4 85.7 94.0 81.0 87.0
Accuracy 81.56 82.60
Table 2: Results for Baseline only and Baseline with hierarchical classification.
Baseline+Collective Baseline+Both
Semantic Supertype R P F R P F
1 PERSON 93.8 98.1 95.9 91.9 100.0 95.8
2 PERSON DESC 93.9 88.5 91.1 92.6 89.5 91.0
3 SUBSTANCE 60.0 85.7 70.6 70.0 63.6 66.7
4 NORP 89.2 93.0 91.0 90.5 94.4 92.4
5 FACILITY DESC 63.6 87.5 73.7 68.2 93.8 79.0
6 ORGANIZATION 85.8 76.2 80.7 87.4 76.3 81.3
7 ORG DESC 75.8 74.1 74.9 75.8 74.6 75.2
8 GPE 74.1 75.8 74.9 81.5 81.5 81.5
9 GPE DESC 60.0 60.0 60.0 70.0 77.8 73.7
10 PRODUCT DESC 53.3 88.9 66.7 53.3 88.9 66.7
11 DATE 85.0 85.4 85.2 85.0 86.3 85.6
12 PERCENT 100.0 100.0 100.0 100.0 100.0 100.0
13 MONEY 83.9 86.7 85.3 90.3 96.6 93.3
14 QUANTITY 22.2 100.0 36.4 66.7 66.7 66.7
15 ORDINAL 100.0 100.0 100.0 100.0 100.0 100.0
16 CARDINAL 96.0 78.7 86.5 94.0 83.9 88.7
Accuracy 83.70 85.08
Table 3: Results for Baseline with collective classification and Baseline with both techniques.
supertypes with non-zero scores are shown. As we
can see, only 16 of the 29 supertypes have non-
zero scores.10 Among the ?traditional? seman-
tic types, the Baseline yields good performance
for PERSON, but only mediocre performance for
ORGANIZATION and GPE. While additional ex-
periments are needed to determine the reason, we
speculate that this can be attributed to the fact that
PERSON and PERSON DESC have only one seman-
tic subtype (which is the supertype itself), whereas
10The 13 supertypes that have zero scores are all under-
represented classes, each of which accounts for less than one
percent of the instances in the dataset.
ORGANIZATION and GPE have nine and four sub-
types, respectively. The classification accuracy is
shown in the last row of the table. As we can see,
the Baseline achieves an accuracy of 81.56.
Results obtained when hierarchical classifica-
tion is applied to the Baseline are shown in the
third column of Table 2. In comparison to the
Baseline, accuracy rises from 81.56 to 82.60. This
represents an error reduction of 5.6%, and the dif-
ference between these two accuracies is statisti-
cally significant at the p = 0.04 level.11
11All significance test results in this paper are obtained us-
ing Approximate Randomization (Noreen, 1989).
937
Results obtained when collective classification
alone is applied to the Baseline are shown in
the second column of Table 3. In this case, the
prior probability distribution over the semantic
subtypes that is needed to create the factor asso-
ciated with each node in the factor graph is sim-
ply the probabilistic classification of the test in-
stance that the node corresponds to. In compar-
ison to the Baseline, accuracy rises from 81.56
to 83.70. This represents an error reduction of
11.6%, and the difference is significant at the
p = 0.01 level. Also, applying collective clas-
sification to the Baseline yields slightly better re-
sults than applying hierarchical classification to
the Baseline, and the difference in their results is
significant at the p = 0.002 level.
Finally, results obtained when both hierarchi-
cal and collective classification are applied to the
Baseline are shown in the third column of Table
3. In this case, the prior distribution needed to
create the factor associated with each node in the
factor graph is provided by the supertype-specific
classification model that is used to classify the test
instance in hierarchical classification. In compar-
ison to the Baseline, accuracy rises from 81.56
to 85.08. This represents an error reduction of
19.1%, and the difference is highly significant
(p < 0.001). Also, applying both techniques to
the Baseline yields slightly better results than ap-
plying only collective classification to the Base-
line, and the difference in their results is signifi-
cant at the p = 0.003 level.
6.1 Feature Analysis
Next, we analyze the effects of the seven feature
types described in Section 3 on classification ac-
curacy. To measure feature performance, we take
the best-performing system (i.e., Baseline com-
bined with both techniques), begin with all seven
feature types, and iteratively remove them one by
one so that we get the best accuracy. The re-
sults are shown in Table 4. Across the top line,
we list the numbers representing the seven feature
classes. The feature class that corresponds to each
number can be found in Section 3, where they are
introduced. For instance, ?2? refers to the fea-
tures computed based on the governing verb. The
first row of results shows the system performance
1 3 7 4 2 5 6
81.4 75.8 83.3 83.7 84.1 85.2 85.6
80.4 74.9 84.3 85.3 85.3 86.1
80.4 78.3 83.9 86.5 86.7
81.8 76.2 85.2 87.6
75.4 83.4 84.6
66.2 80.9
Table 4: Results of feature analysis.
after removing just one feature class. In this
case, removing the sixth feature class (Gazetteers)
improves accuracy to 85.6, while removing the
mention string features reduces accuracy to 81.4.
The second row repeats this, after removing the
gazetteer features.
Somewhat surprisingly, using only mention
string, semantic, and grammatical features yields
the best accuracy (87.6). This indicates that
gazetteers, morphological features, capitalization,
and features computed based on the governing
verb are not useful. Removing the grammati-
cal features yields a 3% drop in accuracy. After
that, accuracy drops by 4% when semantic fea-
tures are removed, whereas a 18% drop in accu-
racy is observed when the mention string features
are removed. Hence, our analysis suggests that
the mention string features are the most useful fea-
tures for semantic subtype prediction.
7 Conclusions
We examined the under-studied problem of se-
mantic subtype induction, which involves clas-
sifying an NP as one of 92 semantic classes,
and showed that two techniques ? hierarchi-
cal classification and collective classification ?
can significantly improve a baseline classification
model trained using an off-the-shelf learning al-
gorithm on the BBN Entity Type Corpus. In par-
ticular, collective classification addresses a ma-
jor weakness of the standard feature-based learn-
ing paradigm, where a classification model classi-
fies each instance independently, failing to capture
the relationships among subsets of instances that
might improve classification accuracy. However,
collective classification has not been extensively
applied in the NLP community, and we hope that
our work can increase the awareness of this pow-
erful technique among NLP researchers.
938
Acknowledgments
We thank the three anonymous reviewers for their
invaluable comments on an earlier draft of the pa-
per. This work was supported in part by NSF
Grant IIS-0812261.
References
Bikel, Daniel M., Richard Schwartz, and Ralph M.
Weischedel. 1999. An algorithm that learns what?s
in a name. Machine Learning: Special Issue on
Natural Language Learning, 34(1?3):211?231.
Bunescu, Razvan and Raymond J. Mooney. 2004.
Collective information extraction with relational
markov networks. In Proceedings of the 42nd An-
nual Meeting of the Association for Computational
Linguistics, pages 483?445.
Finkel, Jenny Rose, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 363?370.
Florian, Radu, Hany Hassan, Abraham Ittycheriah,
Hongyan Jing, Nanda Kambhatla, Xiaoqiang Luo,
Nicolas Nicolov, and Salim Roukos. 2004. A sta-
tistical model for multilingual entity detection and
tracking. In HLT-NAACL 2004: Main Proceedings,
pages 1?8.
Florian, Radu, Hongyan Jing, Nanda Kambhatla, and
Imed Zitouni. 2006. Factorizing complex mod-
els: A case study in mention detection. In Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and the 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 473?480.
Gale, William, Ken Church, and David Yarowsky.
1992. One sense per discourse. In Proceedings
of the 4th DARPA Speech and Natural Language
Workshop, pages 233?237.
Hovy, Eduard, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
Ontonotes: The 90% solution. In Proceedings of
the Human Language Technology Conference of the
NAACL, Companion Volume: Short Papers, pages
57?60.
Huang, Zhiheng, Guangping Zeng, Weiqun Xu, and
Asli Celikyilmaz. 2009. Accurate semantic class
classifier for coreference resolution. In Proceed-
ings of the 2009 Conference on Empirical Methods
in Natural Language Processing, pages 1232?1240.
Kschischang, Frank, Brendan J. Frey, and Hans-
Andrea Loeliger. 2001. Factor graphs and the sum-
product algorithm. IEEE Transactions on Informa-
tion Theory, 47:498?519.
Loeliger, Hans-Andrea. 2004. An introduction to
factor graphs. IEEE Signal Processing Magazine,
21(1):28?41.
Murphy, Kevin P., Yair Weiss, and Michael I. Jordan.
1999. Loopy belief propagation for approximate in-
ference: An empirical study. In Proceedings of the
Fifteenth Conference on Uncertainty in Artificial In-
telligence, pages 467?475.
Ng, Vincent. 2007. Semantic class induction and
coreference resolution. In Proceedings of the 45th
Annual Meeting of the Association of Computa-
tional Linguistics, pages 536?543.
Nicolae, Cristina and Gabriel Nicolae. 2006. Best-
Cut: A graph algorithm for coreference resolution.
In Proceedings of the 2006 Conference on Empiri-
cal Methods in Natural Language Processing, pages
275?283.
Noreen, Eric W. 1989. Computer Intensive Methods
for Testing Hypothesis: An Introduction. John Wi-
ley & Sons.
Toutanova, Kristina, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-
of-speech tagging with a cyclic dependency net-
work. In HLT-NAACL 2003: Proceedings of the
Main Conference, pages 173?180.
Weischedel, Ralph and Ada Brunstein. 2005. BBN
pronoun coreference and entity type corpus. In Lin-
guistic Data Consortium, Philadelphia.
939
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1069?1080,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Learning the Information Status of Noun Phrases in Spoken Dialogues
Altaf Rahman and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{altaf,vince}@hlt.utdallas.edu
Abstract
An entity in a dialogue may be old, new,
or mediated/inferrable with respect to the
hearer?s beliefs. Knowing the information
status of the entities participating in a dia-
logue can therefore facilitate its interpreta-
tion. We address the under-investigated prob-
lem of automatically determining the informa-
tion status of discourse entities. Specifically,
we extend Nissim?s (2006) machine learning
approach to information-status determination
with lexical and structured features, and ex-
ploit learned knowledge of the information
status of each discourse entity for coreference
resolution. Experimental results on a set of
Switchboard dialogues reveal that (1) incor-
porating our proposed features into Nissim?s
feature set enables our system to achieve state-
of-the-art performance on information-status
classification, and (2) the resulting informa-
tion can be used to improve the performance
of learning-based coreference resolvers.
1 Introduction
Information status is not a term unfamiliar to re-
searchers working on discourse processing prob-
lems. It describes the extent to which a discourse en-
tity, which is typically a noun phrase (NP), is avail-
able to the hearer given the speaker?s assumptions
about the hearer?s beliefs. According to Nissim et
al. (2004), a discourse entity can be new, old, or me-
diated. Informally, a discourse entity is (1) old to
the hearer if it is known to the hearer and has pre-
viously been referred to in the dialogue, (2) new if
it is unknown to her and has not been previously re-
ferred to; and (3) mediated if it is newly mentioned
in the dialogue but she can infer its identity from
a previously-mentioned entity. Information status
is a subject that has received a lot of attention in
theoretical linguistics (Halliday, 1976; Prince, 1981;
Hajic?ova?, 1984; Vallduv??, 1992; Steedman, 2000).
Knowing the information status of discourse enti-
ties can potentially benefit many NLP applications.
One such task is anaphora resolution. While there is
general belief that definite descriptions are mostly
anaphoric, Vieira and Poesio (2000) empirically
show that only 30% of these NPs are anaphoric.
Without being able to determine whether an NP is
anaphoric, an anaphora resolver will attempt to re-
solve every NP, potentially damaging its precision.
Since new entities are by definition new to the hearer
and therefore cannot refer to a previously-introduced
NP, knowledge of information status could be used
to improve anaphora resolution.
Despite the potential usefulness of information
status in NLP tasks, there has been little work on
learning the information status of discourse entities.
To investigate the plausibility of learning informa-
tion status, Nissim et al (2004) annotate a set of
Switchboard dialogues with such information1 , and
subsequently present a rule-based approach and a
learning-based approach to acquiring such knowl-
edge from the manual annotations (Nissim, 2006).
Our goals in this paper are two-fold. First, we
describe a learning approach to the under-studied
problem of determining the information status of
discourse entities that extends Nissim?s (2006) fea-
ture set with two novel types of features: lexical
features and structured features based on syntactic
parse trees. Second, we employ the automatically
1These and other linguistic annotations on the Switchboard
dialogues were later released by the LDC as part of the NXT
corpus, which is described in detail in Calhoun et al (2010).
1069
acquired knowledge of information status for coref-
erence resolution. Experimental results on Nissim et
al.?s (2004) corpus of Switchboard dialogues show
that (1) adding our linguistic features to Nissim?s
feature set enables our system to outperform her sys-
tem by 8.1% in F-measure, and (2) learned knowl-
edge of information status can be used to improve
coreference resolvers by 1.1?2.6% in F-measure.
The rest of this paper is organized as follows. We
first illustrate with examples the concepts of new,
old, and mediated entities. Then, we describe the
dataset and the feature set that Nissim (2006) used
in her approach. After that, we introduce our lexi-
cal and structured features. Finally, we evaluate the
determination of information status as a standalone
task and in the context of coreference resolution.
2 Old, New, and Mediated Entities
Since the concepts of old, new, and mediated entities
are not widely known to researchers working outside
the area of discourse processing, in this section we
will explain them in more detail.
The terms old and new information have meant
a variety of things over the years (Allerton, 1978;
Prince, 1981; Horn, 1986). Since we use Nissim
et al?s (2004) corpus for training and evaluation,
the definitions of these concepts we present here are
those that Nissim et al used to annotate their cor-
pus. According to Nissim et al, their definitions are
built upon Prince?s (1981), and the categorization
into old, new, and mediated entities resemble those
of Strube (1998) and Eckert and Strube (2001).
Old. As mentioned before, an entity is old if it is
both known to the hearer and has been mentioned in
the conversation. More precisely, an entity is old if
(1) it is coreferential with an entity introduced ear-
lier, (2) it is a generic pronoun, or (3) it is a personal
pronoun referring to the dialogue participants. To
exemplify, consider the following sentences.
(1) I was angry that he destroyed my tent.
(2) You cannot leave until the test is over.
In Example 1, my is an old entity because it is
coreferent with I. In Example 2, You is an old entity
because it is a generic pronoun.
Mediated. An entity is mediated if it has not been
previously introduced in the conversation, but can be
inferred from already-mentioned entities or is gener-
ally known to the hearer. More specifically, an entity
is mediated if (1) it is a generally known entity (e.g.,
the Earth, China, and most proper names), (2) it is
a bound pronoun, or (3) it is an instance of bridging
(i.e., an entity that is inferrable from a related entity
mentioned earlier in the dialogue). As an example,
consider the following sentences.
(3a) He passed by the door of Mary?s house and
saw that the door was painted purple.
(3b) He passed by Mary?s house and saw that
the door was painted purple.
In Example 3a, by the time the hearer processes
the second occurrence of the door, she has already
had a mental entity corresponding to the door (af-
ter processing the first occurrence). As a result, the
second occurrence of the door is an old entity. In
Example 3b, on the other hand, the hearer is not as-
sumed to have any mental representation of the door
in question, but she can infer that the door she saw
was part of Mary?s house. Hence, this occurrence of
the door is a mediated entity. In general, an entity
that is related to an earlier entity via a part-whole
relation or a set-subset relation is mediated.
New. An entity is new if it has not been introduced
in the dialogue and the hearer cannot infer it from
previously mentioned entities.
In case more than one class is appropriate for
a given entity, Nissim et al employ additional tie-
breaking rules. Suppose, for instance, that we have
two occurrences of China in a dialogue. The second
occurrence can be labeled as old (because it is coref-
erential with an earlier entity) or mediated (because
it is a generally known entity). According to Nissim
et al?s rules, the entity will be labeled as old.
3 Dataset
We employ Nissim et al?s (2004) dataset, which
comprises 147 Switchboard dialogues. A total of
68,992 NPs are annotated with information status:
51.2% of them are labeled as old, 34.5% as mediated
(henceforth med), and 14.3% as new. Nissim (2006)
randomly split the instances created from these NPs
into a training set (for classifier training), a develop-
ment set (for feature development), and an evalua-
tion set (for testing). Hence, the NPs from the same
1070
Training Test
old 31358 (51.7%) 3931 (47.4%)
med 20778 (34.2%) 3036 (36.6%)
new 8567 (14.1%) 1322 (16.0%)
total 60703 (100%) 8289 (100%)
Table 1: Information status distribution of NPs.
document may be split across different sets.
Unlike Nissim (2006), we partition the 147 dia-
logues (rather than the instances) into a training set
(117 dialogues) and a test set (30 dialogues). In
other words, we do not randomize the instances, as
we believe that it represents an unrealistic evalua-
tion setting, for the following reasons. First, in prac-
tice, the test dialogues may not be available until test
time. Second, we may want to examine how a sys-
tem performs on a given dialogue. Finally, random-
izing the instances does not allow us to apply learned
knowledge of information status to coreference res-
olution, which needs to be performed for each dia-
logue. The information status distribution of the NPs
in the training and test sets are shown in Table 1.
4 Baseline System
In this section, we describe our baseline system,
which adopts a machine learning approach to deter-
mining the information status of a discourse entity.
Building SVM classifiers for information-status
determination. We employ the support vector
machine (SVM) learner as implemented in the
SVMlight package (Joachims, 1999) to train three
binary classifiers, one for predicting each of the
three possible classes (i.e., new, old, and med), us-
ing a linear kernel in combination with the one-
versus-all training scheme.2 Each training instance
represents a single NP and consists of the seven
morpho-syntactic features that Nissim (2006) used
in her learning-based approach (see Table 2 for an
overview). Following Nissim, we extract the NPs
directly from the gold-standard annotations, but the
features are computed entirely automatically.
2SVM was chosen because it provides the option to employ
kernels. The reason why we train three binary classifiers rather
than just one multi-class classifier (using SVMmulticlass) is that
SVMmulticlass does not permit the use of a non-linear kernel,
which we will need to incorporate structured features later on.
Feature Values
full prev mention numeric
mention time {first,second,more}
partial prev mention {yes,no,NA}
determiner {bare,def,dem,indef,poss,NA}
NP type {pronoun,common,proper,other}
NP length numeric
grammatical role {subject,subjpass,pp,other}
Table 2: Nissim?s feature set.
The seven features are all intuitively useful for
determining information status. For instance, if an
NP, NPk, and a discourse entity that appears before
it have the same string (full prev mention), then NPk
is likely to be an old entity. Mention time is the cat-
egorical version of full prev mention and therefore
serves to detect old entities. Partial prev mention
is useful for detecting mediated entities, especially
those that have a set-subset relation with a preceding
entity. For instance, your dogs would be considered
a partial previous mention of my dogs or my three
dogs. The value ?NA? stands for ?not applicable?,
and is used for pronouns. Determiners and NP type
are likely to be helpful for all three categories. For
instance, indefinite NPs and pronouns are likely to
be new and old, respectively. The ?NP length? fea-
ture is motivated by the observation that old entities
tend to contain less lexical materials than new enti-
ties. For instance, subsequent references to Barack
Obama may simply be Obama.
Applying the classifiers. To determine the infor-
mation status of an NP in a test dialogue, we create
an instance for it as during training and present it
independently to the three binary SVM classifiers,
each of which returns a real value representing the
signed distance of the instance from the hyperplane.
We assign the instance to the class that is associated
with the most positive classification value.
5 Our Features
We propose to extend Nissim?s (2006) feature set
with two types of features.
5.1 Lexical Features
As discussed, an entity should be labeled as med if it
has not been introduced in the dialogue but is gener-
1071
ally known to a human. Whether an entity is ?gener-
ally known? may be easily determined by a human
but not by a machine, since world knowledge is in-
volved in the decision process. In particular, Nis-
sim?s feature set does not contain any features that
encode the notion of a ?generally known? entity.
Hence, it would be desirable to augment Nissim?s
feature set with features that indicate whether an en-
tity is generally known or not. One way to do this is
to (1) create a list of generally known entities, and
then (2) create a binary feature that has the value
True if and only if the entity under consideration ap-
pears in this list. The question, then, is: how can
we obtain the list of generally known entities? We
may manually assemble this list, but this could be
a labor-intensive task. As a result, we propose to
acquire this kind of world knowledge automatically
from annotated data.
Specifically, we augment Nissim?s feature set
with the set of unigrams that appear in the training
data. Given a training/test instance (i.e., discourse
entity), we compute the values of its unigram fea-
tures (henceforth lexical features) as follows. For
each unigram, we check if it appears in the string
representing the discourse entity. If so, its feature
value is 1; otherwise, its value is 0. For instance, if
the entity is the red hat, then all of its lexical features
except the, red, and hat will have a value of 0.
It should perhaps not be too difficult to see why
these lexical features are useful for the information-
status classifier: these features enable the SVM
learner to determine the extent to which a unigram
correlates with each class. For instance, from the an-
notated data, the learner will learn that any instance
of China cannot be labeled as new, and the deci-
sion of whether it should be an old entity or a med
entity depends on whether it is coreferential with a
previously-mentioned entity. Hence, the use of lex-
ical features allows the learner to implicitly acquire
some world knowledge.
We believe that lexicalization is an important step
towards building high-performance text-processing
systems. In fact, lexicalized models have demon-
strated their effectiveness in other areas of language
processing, such as syntactic and semantic parsing.
While lexicalized models may be less portable to
new genres and domains than their unlexicalized
counterparts, we believe that this issue can be han-
dled via domain adaptation techniques and should
not be a reason against lexicalization.
5.2 Structured Features
In Nissim?s (2006) feature set, there are a couple of
features that capture NP-internal information, such
as determiner, NP length, and NP type. However,
there is only one feature that captures the syntactic
context of an NP, grammatical role, which is com-
puted based on the parse tree in which the NP re-
sides. This is arguably a very shallow representation
of its syntactic context. We hypothesize that we can
train more accurate information-status classifiers if
we have access to a richer representation of syntac-
tic context. This motivates us to employ syntactic
parse trees directly as features.
Before describing how this can be done, recall
that in a traditional learning setting, the feature set
employed by an off-the-shelf learning algorithm typ-
ically consists of flat features (i.e., features whose
values are discrete- or real-valued, as the ones de-
scribed in the previous section). Advanced machine
learning algorithms such as SVMs, on the other
hand, have enabled the use of structured features
(i.e., features whose values are structures such as
parse trees), owing to their ability to employ ker-
nels to efficiently compute the similarity between
two potentially complex structures.
Perhaps the main advantage of employing struc-
tured features is simplicity. To understand this ad-
vantage, consider learning in a setting where we can
only employ flat features. If we want to use informa-
tion from a parse tree as features in this setting, we
will need to design heuristics to extract the desired
parse-based features from parse trees. For certain
tasks, designing a good set of heuristics can be time-
consuming and sometimes difficult. On the other
hand, SVMs enable a parse tree to be employed di-
rectly as a structured feature, obviating the need to
design such heuristics.
Given two parse trees (as features), we com-
pute their similarity using a convolution tree ker-
nel (Collins and Duffy, 2001), which efficiently enu-
merates the number of common substructures in the
two trees via dynamic programming. Note, however,
that while we want to use a parse tree directly as a
feature, we do not want to use the entire parse tree as
a feature. Specifically, while using the entire parse
1072
tree enables a richer representation of the syntactic
context than using a partial parse tree, the increased
complexity of the tree also makes it more difficult
for the SVM learner to make generalizations.
To strike a better balance between having a rich
representation of the context and improving the
learner?s ability to generalize, we extract a substruc-
ture from a parse tree and use it as the value of the
structured feature of an instance. Specifically, given
an instance corresponding to discourse entity e, we
extract the substructure from the parse tree contain-
ing e as follows. Let n(e) be the root of the sub-
tree that spans all and only the words in e, and let
Parent(n(e)) be its immediate parent node. We (1)
take the subtree rooted at Parent(n(e)), (2) replace
each leaf node in this subtree with a node labeled
X, (3) replace the subtree rooted at n(e) with a leaf
node labeled Y, and (4) use the subtree rooted at
Parent(n(e)) as the structured feature for the in-
stance corresponding to e. Intuitively, the first three
steps aim to provide generalizations by simplifying
the tree. For instance, step (1) allows us to focus on
using a small window as the context. Steps (2) and
(3) help generalization by ignoring the words within
e and its context. Note that using two labels, X and
Y, enables the kernel to distinguish the discourse en-
tity under consideration from its context within this
substructure. In addition, we simply use a single
node (Y) to represent the discourse entity, since any
NP-internal information has presumably been cap-
tured by the flat features. We compute these struc-
tured features using hand-annotated parse trees.
While structured features have been employed for
a multitude of tasks in syntax, semantics, and in-
formation extraction such as syntactic parsing (e.g.,
Collins (2002)), semantic parsing (e.g., Moschitti
(2004)), named entity recognition (e.g., Cumby and
Roth (2003), and relation extraction (e.g., Zelenko
et al (2003)), the same is not true for discourse
processing tasks. We hope that our use of struc-
tured features for information-status classification
can promote their use in discourse processing.
5.3 Combining Kernels
Recall that the flat features are computed using a
linear kernel, while the structured features are com-
puted using a tree kernel. If we want our learner to
make use of more than one of these types of features,
we need to employ a composite kernel to combine
them. Specifically, we define and employ the fol-
lowing composite kernel:
Kc(F1, F2) = K1(F1, F2) + K2(F1, F2),
where F1 and F2 are the full set of features that rep-
resent the two entities under consideration, and K1
and K2 are the kernels we are combining. To ensure
that both kernels contribute equally to the compos-
ite kernel, we normalize the values they return to the
range [0,1].
6 Evaluation
Next, we evaluate the effectiveness of our features
in improving information-status classification.
6.1 Results and Discussion
Results of four information-status classification sys-
tems are shown in Table 3. Under Original Nis-
sim, we have the results copied verbatim from Nis-
sim?s (2006) paper. Baseline is the aforementioned
baseline system, which is trained using Nissim?s fea-
ture set. Baseline+Lexical is the system trained us-
ing Nissim?s feature set augmented with lexical fea-
tures. Finally, Baseline+Both is the system trained
using Nissim?s feature set augmented with both lex-
ical and structured features. For each system, we
show the recall (R), precision (P), and F-measure (F)
of each of the three classes: old, new, and med. Be-
fore we describe the results, two points deserve men-
tion. First, as noted earlier, Nissim partitioned the
dialogues into training and test folds in a different
way than us. In particular, Original Nissim and the
remaining three systems were not evaluated on the
same set of test instances. Hence, the Original Nis-
sim results are not directly comparable with those of
the other systems. We show them here just to pro-
vide another point of reference. Second, the results
of the remaining three systems were obtained by ag-
gregating the results of three binary SVM classifiers,
as described earlier.
Comparing Baseline and Baseline+Lexical, we
see that augmenting Nissim?s feature set with lexical
features improves the F-measure scores on all three
classes. In particular, the F-measure and recall for
med rise considerably by 3.0 and 7.8, respectively.
This provides indirect empirical support for our ear-
lier hypothesis that the med class can benefit from
1073
Original Nissim Baseline Baseline+Lexical Baseline+Both
R P F R P F R P F R P F
old 91.5 94.1 92.8 91.2 85.8 88.5 88.7 91.7 90.2 93.0 95.2 94.1
med 87.6 68.1 76.6 84.7 62.7 72.1 92.5 63.2 75.1 89.1 70.9 79.0
new 22.3 56.3 32.0 30.2 66.4 41.5 32.1 68.3 43.7 34.4 71.5 46.5
Accuracy 79.5 74.1 76.3 82.2
Table 3: Per-class performance of four information-status classifiers.
the shallow world knowledge that these lexical fea-
tures help to ?extract? from annotated data.
Comparing Baseline+Lexical and Baseline+Both,
we see that the addition of structured features en-
ables a further boost to performance: F-measure in-
creases by 2.8?3.9 for the three classes. These re-
sults substantiate our hypothesis that employing a
richer representation of syntactic context is benefi-
cial to information-status classification.
Comparing Baseline and Baseline+Both, we see
that F-measure improves considerably by 5?6.9 for
the three classes. Overall, these results provide sug-
gestive evidence that both types of features are ef-
fective at improving an information-status classifier
that employs Nissim?s features.
For further comparison, we show the classifica-
tion accuracies of the four systems in the last row
of Table 3. As we can see, adding lexical features
to the baseline features improves accuracy by 2.2%,
and adding structured features further improves ac-
curacy by 5.9%. Our two types of features, when
used in combination with Nissim?s features, improve
the baseline substantially by an accuracy of 8.1%.
Note that while our results and Original Nissim?s
are not directly comparable, the two systems are
consistent in terms of the relative performance for
the three classes: best for old and worst for new. The
poor performance for new is largely a consequence
of its low recall, which can in turn be attributed to its
lower representation in the dataset. Since many new
instances are misclassified, a natural question is: are
these instances misclassified as old or med? Simi-
lar questions can be raised for old and med, despite
their substantially higher recall values than new.
To answer these questions, we need to better
understand the kind of errors made by our ap-
proach. Consequently, we show in Table 4 the con-
fusion matrix generated from the test set for our
C? old med new
G ?
old 3656 257 18
med 167 2706 163
new 17 850 455
Table 4: Confusion matrix for the Baseline+Both
classifier. C=Classifier tag; G=Gold tag
best-performing information-status classifier, Base-
line+Both. The rows and the columns correspond
to the gold tags and the classifier tags, respectively.
As we can see, these numbers seem to suggest the
?in-between? nature of mediated entities: when an
old or new entity is misclassified, it is typically mis-
classified as med (rows 1 and 3); however, when a
med entity is misclassified, it is equally likely to be
misclassified as old and new (row 2).
These results are perhaps not surprisingly, since
intuitively med entities bear some resemblance to
both old and new entities. For instance, the simi-
larity between med and old stems from the fact that
different instances of the same entity (e.g., China)
can receive one of these two labels, with the deci-
sion dependent on whether the entity was previously
mentioned in the dialogue. On the other hand, med
and new are similar in that it may sometimes be dif-
ficult even for a human to determine whether certain
entities should be labeled as med or new, since the
decision depends on whether she believes these en-
tities are generally known or not.
6.2 Relation to Anaphoricity Determination
Anaphoricity determination refers to the task of de-
termining whether an NP is anaphoric or not, where
an NP is considered anaphoric if it is part of a (non-
singleton) coreference chain but is not the head of
the chain (Ng and Cardie, 2002). In other words, an
1074
Anaphoricity Baseline+Ana Baseline+Lexical+Ana Baseline+Both+Ana
R P F R P F R P F R P F
old 91.4 86.6 88.9 91.3 87.3 89.3 90.8 91.7 91.3 92.8 94.9 93.9
med 84.3 63.1 72.2 84.9 64.1 73.1 92.3 64.7 76.1 88.7 71.1 78.9
new 30.8 66.4 42.1 31.1 66.9 42.5 32.9 68.7 44.5 34.1 71.7 46.2
Accuracy 74.7 75.1 77.6 82.0
Table 5: Impact of knowledge of anaphoricity on the information-status classifiers.
NP is anaphoric if and only if it has an antecedent.
Given this definition, anaphoricity determination
bears resemblance to information-status classifica-
tion. For instance, an old entity is anaphoric, since it
has been introduced earlier in the conversation and
therefore have an antecedent. Similarly, a new or
med entity is non-anaphoric, since the entity has not
been previously introduced in the conversation and
therefore cannot have an antecedent.
There has been a lot of recent work on anaphoric-
ity determination (e.g., Bean and Riloff (1999),
Uryupina (2003), Ng (2004), Denis and Baldridge
(2007), Versley et al (2008), Ng (2009), Zhou and
Kong (2009)). Given the similarity between this task
and information-status classification, a natural ques-
tion is: will the anaphoricity features previously de-
veloped by coreference researchers be helpful for
information-status classification? To answer this
question, we (1) assemble a feature set composed
of the 26 anaphoricity features previously used by
Rahman and Ng (2009),3 and then (2) repeat the ex-
periments in Table 3, except that we augment the
feature set used in each of these experiments with
the anaphoricity features we assembled in step (1).
Results with the anaphoricity features are shown
in Table 5. Under Anaphoricity, we have the results
obtained using only the 29 anaphoricity features. As
we can see, these results are comparable to those
obtained using the Baseline features. Comparing
each of Baseline+Ana and Baseline+Lexical+Ana
with the corresponding experiments in Table 3, we
see that the addition of anaphoricity features yields
a mild performance improvement, which is consis-
tent over all three classes. However, comparing the
last column of the two tables, we can see that in the
3These 26 features are derived from those employed by Ng
and Cardie?s (2002) anaphoricity determination system. See
Footnote 2 of Rahman and Ng (2009) for details.
presence of the structured features, the anaphoricity
features do not contribute positively to overall per-
formance. Hence, in the coreference experiments in
the next section, we will not employ anaphoricity
features for information-status classification.
7 Application to Coreference Resolution
Since the significance of information-status classi-
fication stems in part from the potential benefits it
brings to higher-level NLP applications, we deter-
mine whether our information-status classification
systems can offer benefits to learning-based coref-
erence resolution. Since the 147 information-status
annotated dialogues are also coreference annotated,
we use them in our coreference evaluation. To our
knowledge, our work represents the first attempt to
report coreference results on this dataset.
7.1 Coreference Models
While the so-called mention-pair coreference model
has dominated coreference research for more than
a decade since its appearance in the mid-1990s, a
number of new coreference models have been pro-
posed in recent years. To investigate whether these
newer, presumably more sophisticated, coreference
models can better exploit the automatically acquired
information-status information, we will evaluate the
usefulness of information-status information when
used in combination with two different coreference
models, the aforementioned mention-pair model and
the recently-developed cluster-ranking model.
7.1.1 Mention-Pair Model
The mention-pair (MP) model, proposed by Aone
and Bennett (1995) and McCarthy and Lehnert
(1995), is a classifier that determines whether two
NPs are co-referring or not. Each instance i(NPj ,
NPk) corresponds to two NPs, NPj and NPk , and is
represented by 39 features. Table 1 of Rahman and
1075
Ng (2009) contains a detailed description of these
features. Linguistically, they can be divided into
four categories: string-matching, grammatical, se-
mantic, and positional. They can also be categorized
based on whether they are relational or not. Specifi-
cally, relational features capture the relationship be-
tween NPj and NPk , whereas non-relational features
capture the linguistic property of one of these NPs.
We follow Soon et al?s (2001) method for cre-
ating training instances. Specifically, we create (1)
a positive instance for each anaphoric NP NPk and
its closest antecedent NPj ; and (2) a negative in-
stance for NPk paired with each of the intervening
NPs, NPj+1, NPj+2, . . ., NPk?1. The classification
associated with a training instance is either positive
or negative, depending on whether the two NPs are
coreferent. To train the MP model, we use the SVM
learner from SVMlight (Joachims, 1999).4
After training, the classifier is used to identify an
antecedent for an NP in a test text. Specifically,
each NP, NPk, is compared in turn to each preced-
ing NP, NPj , from right to left, and selects NPj as its
antecedent if the pair is classified as coreferent. The
process terminates as soon as an antecedent is found
for NPk or the beginning of the text is reached.
Despite its popularity, the MP model has two
major weaknesses. First, since each candidate an-
tecedent for an NP to be resolved (henceforth an ac-
tive NP) is considered independently of the others,
this model only determines how good a candidate
antecedent is relative to the active NP, but not how
good a candidate antecedent is relative to other can-
didates. So, it fails to answer the critical question of
which candidate antecedent is most probable. Sec-
ond, it has limitations in its expressiveness: the in-
formation extracted from the two NPs alone may not
be sufficient for making a coreference decision.
7.1.2 Cluster-Ranking Model
The cluster-ranking (CR) model, proposed by
Rahman and Ng (2009), addresses the two weak-
nesses of the MP model by combining the strengths
of the entity-mention model (e.g., Luo et al (2004),
Yang et al (2008)) and the mention-ranking model
(e.g., Denis and Baldridge (2008)). Specifically,
the CR model ranks the preceding clusters for an
4For this and subsequent uses of the SVM learner in our
experiments, we set al parameters to their default values.
active NP so that the highest-ranked cluster is the
one to which the active NP should be linked. Em-
ploying a ranker addresses the first weakness, as
a ranker allows all candidates to be compared si-
multaneously. Considering preceding clusters rather
than antecedents as candidates addresses the second
weakness, as cluster-level features (i.e., features that
are defined over any subset of NPs in a preceding
cluster) can be employed.
Since the CR model ranks preceding clusters, a
training instance i(cj , NPk) represents a preceding
cluster cj and an anaphoric NP NPk. Each instance
consists of features that are computed based solely
on NPk as well as cluster-level features, which de-
scribe the relationship between cj and NPk . Mo-
tivated in part by Culotta et al (2007), we create
cluster-level features from the relational features in
our feature set using four predicates: NONE, MOST-
FALSE, MOST-TRUE, and ALL. Specifically, for each
relational feature X, we first convert X into an equiv-
alent set of binary-valued features if it is multi-
valued. Then, for each resulting binary-valued fea-
ture Xb, we create four binary-valued cluster-level
features: (1) NONE-Xb is true when Xb is false be-
tween NPk and each NP in cj ; (2) MOST-FALSE-Xb
is true when Xb is true between NPk and less than half
(but at least one) of the NPs in cj ; (3) MOST-TRUE-
Xb is true when Xb is true between NPk and at least
half (but not all) of the NPs in cj ; and (4) ALL-Xb is
true when Xb is true between NPk and each NP in cj .
We train a cluster ranker to jointly learn
anaphoricity determination and coreference reso-
lution using SVMlight?s ranker-learning algorithm.
Specifically, for each NP, NPk, we create a train-
ing instance between NPk and each preceding clus-
ter cj using the features described above. Since we
are learning a joint model, we need to provide the
ranker with the option to start a new cluster by creat-
ing an additional training instance that contains fea-
tures that solely describes NPk. The rank value of
a training instance i(cj , NPk) created for NPk is the
rank of cj among the competing clusters. If NPk is
anaphoric, its rank is HIGH if NPk belongs to cj , and
LOW otherwise. If NPk is non-anaphoric, its rank is
LOW unless it is the additional training instance de-
scribed above, which has rank HIGH.
After training, the cluster ranker processes the
NPs in a test text in a left-to-right manner. For each
1076
active NP, NPk , we create test instances for it by pair-
ing it with each of its preceding clusters. To allow
for the possibility that NPk is non-anaphoric, we cre-
ate an additional test instance that contains features
that solely describe the active NP (similar to what
we did in the training step above). All these test in-
stances are then presented to the ranker. If the addi-
tional test instance is assigned the highest rank value
by the ranker, then NPk is classified as non-anaphoric
and will not be resolved. Otherwise, NPk is linked to
the cluster that has the highest rank.
7.2 Coreference Experiments
7.2.1 Experimental Setup
The training/test split we use in the coreference
experiments is the same as that in the information-
status experiments. Specifically, we use the train-
ing set to train both the information-status classifier
and our coreference models, apply the information-
status classifier to each discourse entity in the test
set, and have the coreference models resolve all
and only those NPs that are labeled as old by the
information-status classifier. Our decision to allow
the coreference models to resolve only the old enti-
ties is motivated by the fact that med and new entities
have not been previously introduced in the conversa-
tion and therefore do not have antecedents. The NPs
used by the coreference models are the same as those
accessible to the information-status classifier.
We employ two scoring programs, B3 (Bagga and
Baldwin, 1998) and ?3-CEAF (Luo, 2005), to score
the output of a coreference model. Given a gold-
standard (i.e., key) partition, KP , and a system-
generated (i.e., response) partition, RP , B3 com-
putes the recall and precision of each NP and av-
erages these values at the end. Specifically, for each
NP, NPj , B3 first computes the number of NPs that
appear in both KPj and RPj , the clusters containing
NPj in KP and RP , respectively, and then divides
this number by |KPj| and |RPj| to obtain the re-
call and precision of NPj , respectively. On the other
hand, CEAF finds the best one-to-one alignment
between the key clusters and the response clusters
using the Kuhn-Munkres algorithm (Kuhn, 1955),
where the weight of an edge connecting two clusters
is equal to the number of NPs that appear in both
clusters. Precision and recall are equal to the sum of
the weights of the edges in the alignment divided by
the total number of NPs in the response and the key,
respectively.
7.2.2 Results and Discussion
As our baseline, we employ our coreference mod-
els to generate NP partitions on the test documents
without using any knowledge of information status.
Results, reported in terms of recall (R), precision
(P), and F-measure (F) using B3 and ?3-CEAF, are
shown in row 1 of Table 6.5 As we can see, the
baseline achieves B3 F-measures of 69.2 (MP) and
74.5 (CR) and CEAF F-measures of 61.6 (MP) and
68.5 (CR). These results suggest that the CR model
is stronger than the MP model, corroborating previ-
ous empirical findings (Rahman and Ng, 2009).
Next, we examine the impact of learned knowl-
edge of information status on the performance of a
coreference model. Since knowledge of information
status enables a coreference model to focus on re-
solving only the old entities, we hypothesize that the
resulting model will have a higher precision than one
that does not employ such knowledge. An equally
important question is: will the F-measure of the re-
sulting model improve? Since we are employing
knowledge of information status in a pipeline coref-
erence architecture where information-status classi-
fication is performed prior to coreference resolution,
errors made by the (upstream) information-status
classifier may propagate to the (downstream) coref-
erence system. Given this observation, we hypoth-
esize that the answer to the aforementioned ques-
tion depends in part on the accuracy of information-
status classification. In particular, the higher the
accuracy of information-status classification is, the
more likely the F-measure of the downstream coref-
erence model will improve. To test this hypothe-
sis, we conduct experiments where we employ the
knowledge provided by the three information-status
classifiers which, as discussed earlier, perform at
varying levels of accuracy ? the first one using only
Nissim?s features, the second one using both lexical
and Nissim?s features, and the last one using Nis-
sim?s features in combination with lexical and parse-
based features ? for our coreference models.
5Since gold-standard NPs are used in our experiments,
CEAF precision is always equal to CEAF recall. For brevity,
we only report F-measure scores for CEAF in the table.
1077
Mention-Pair Model Cluster-Ranking Model
B3 CEAF B3 CEAF
System R P F F R P F F
No knowledge of information status 78.6 61.8 69.2 61.6 78.2 71.1 74.5 68.5
Nissim features only 73.4 67.3 70.2 62.1 73.6 77.4 75.4 69.7
Nissim+Lexical features 71.0 69.5 70.2 61.9 73.7 77.3 75.4 69.9
Nissim+Lexical+Parse features 74.1 66.8 70.3 62.3 77.3 74.0 75.6 71.1
Perfect information status 76.7 68.1 72.1 66.4 77.1 79.5 78.3 74.2
Table 6: B3 and CEAF coreference results.
Results of the coreference models employing
knowledge provided by the three information-status
classifiers are shown in rows 2?4 of Table 6. As ex-
pected, B3 precision increases in comparison to the
baseline, regardless of the coreference model and the
scoring program. In addition, employing knowledge
of information status always improves coreference
performance: F-measure scores increase by 1.0?
1.1% (B3) and 0.3?0.7% (CEAF) for the MP model,
and by 0.9?1.1% (B3) and 1.2?2.6% (CEAF) for
the CR model. These results suggest that the three
information-status classifiers have achieved the level
of accuracy needed for the coreference models to
improve. On the other hand, it is somewhat surpris-
ing that the three information-status classifiers have
yielded coreference systems that perform at essen-
tially the same level of performance.
To understand why better information-status clas-
sification results do not necessarily yield better
coreference performance, we take a closer look at
the results of the coreference resolver employing
Nissim?s features (henceforth NISSIM) and the re-
solver employing our Nissim+Lexical+Parse fea-
tures (henceforth FULL-FEATURE). Among the old
entities that were correctly classified using our fea-
tures and incorrectly classified by Nissim?s features,
we found that the precision of the FULL-FEATURE
system suffered (since in many cases the corefer-
ence models identified wrong antecedents for these
old entities) whereas the NISSIM system remained
unaffected (since the entities were misclassified and
would not be resolved by the models). In addition,
although many med and new entities were correctly
classified using our features and incorrectly classi-
fied (as old) using Nissim?s features, we found that
in many cases no antecedents were identified for
these misclassified entities and hence the precision
of the NISSIM system was not adversely affected.
Finally, we investigate whether our coreference
system could be improved if it had access to per-
fect knowledge of information status (taken directly
from the gold-standard annotations). This experi-
ment will allow us to determine whether the useful-
ness of knowledge of information status for coref-
erence resolution is limited by the accuracy in com-
puting such knowledge. Results are shown in the
last row of Table 6. As we can see, using per-
fect information-status knowledge yields a corefer-
ence system that improves those that employs auto-
matically acquired information-status knowledge by
1.8?4.1% (MP) and 2.7?3.1% (CR) in F-measure.
This indicates that the accuracy in computing such
knowledge does play a role in determining its use-
fulness for coreference resolution.
8 Conclusions
We examined the problem of automatically deter-
mining the information status of discourse entities in
spoken dialogues. In particular, we augmented Nis-
sim?s feature set with two types of features: lexical
features, which capture in a shallow manner world
knowledge implicitly encoded in the annotated data;
and syntactic parse trees, which provide a richer rep-
resentation of the syntactic context in which a dis-
course entity appears than grammatical roles. Re-
sults on 147 Switchboard dialogues demonstrated
the effectiveness of these features: we obtained a
significant improvement of 8.1% in accuracy over
a information-status classifier trained on Nissim?s
feature set. In addition, we evaluated information-
status classification in the context of coreference
resolution, and showed that automatically acquired
knowledge of information status can be profitably
used to improve coreference systems.
1078
Acknowledgments
We thank the three reviewers for their invaluable
comments on an earlier draft of the paper. This work
was supported in part by NSF Grant IIS-0812261.
References
David J. Allerton. 1978. The notion of ?givenness? and
its relations to presupposition and to theme. Lingua,
44:133?168.
Chinatsu Aone and Scott William Bennett. 1995. Eval-
uating automated and manual acquisition of anaphora
resolution strategies. In Proceedings of the 33rd An-
nual Meeting of the Association for Computational
Linguistics, pages 122?129.
Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In Proceedings of the
Linguistic Coreference Workshop at the First Interna-
tional Conference on Language Resources and Evalu-
ation, pages 563?566.
David Bean and Ellen Riloff. 1999. Corpus-based iden-
tification of non-anaphoric noun phrases. In Proceed-
ings of the 37th Annual Meeting of the Association for
Computational Linguistics, pages 373?380.
Sasha Calhoun, Jean Carletta, Jason Brenier, Neil Mayo,
Dan Jurafsky, Mark Steedman, and David Beaver.
2010. The NXT-format Switchboard corpus: A rich
resource for investigating the syntax, semantics, prag-
matics and prosody of dialogue. Language Resources
and Evaluation, 44(4):387?419.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Advances in Neural
Information Processing Systems 14, pages 625?632.
Michael Collins. 2002. Ranking algorithms for named-
entity extraction: Boosting and the voted perceptron.
In Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics, pages 489?
496.
Aron Culotta, Michael Wick, and Andrew McCallum.
2007. First-order probabilistic models for coreference
resolution. In Human Language Technologies 2007:
The Conference of the North American Chapter of the
Association for Computational Linguistics; Proceed-
ings of the Main Conference, pages 81?88.
Chad Cumby and Dan Roth. 2003. On kernel methods
for relational learning. In Proceedings of the 20th In-
ternational Conference on Machine Learning, pages
107?114.
Pascal Denis and Jason Baldridge. 2007. Global, joint
determination of anaphoricity and coreference reso-
lution using integer programming. In Human Lan-
guage Technologies 2007: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics; Proceedings of the Main Con-
ference, pages 236?243.
Pascal Denis and Jason Baldridge. 2008. Specialized
models and ranking for coreference resolution. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 660?669.
Miriam Eckert and Michael Strube. 2001. Dialogue acts,
synchronising units and anaphora resolution. Journal
of Semantics, 17(1):51?89.
Eva Hajic?ova?. 1984. Topic and focus. In Contributions
to Functional Syntax, Semantics, and Language Com-
prehension (LLSEE 16), pages 189?202. John Ben-
jamins, Amsterdam.
Michael A. K. Halliday. 1976. Notes on transitivity and
theme in English. Journal of Linguistics, 3(2):199?
244.
Laurence R. Horn. 1986. Presupposition, theme, and
variations. In A. Farley et al, editor, Papers from the
Parasession on Pragmatics and Grammatical Theory
at the 22nd Regional Meeting, pages 168?192. CLS.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Bernhard Scholkopf and Alexan-
der Smola, editors, Advances in Kernel Methods - Sup-
port Vector Learning, pages 44?56. MIT Press.
Harold W. Kuhn. 1955. The Hungarian method for the
assignment problem. Naval Research Logistics Quar-
terly, 2(83).
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based
on the Bell tree. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Linguis-
tics, pages 135?142.
Xiaoqiang Luo. 2005. On coreference resolution perfor-
mance metrics. In Proceedings of Human Language
Technology Conference and Conference on Empirical
Methods in Natural Language Processing, pages 25?
32.
Joseph McCarthy and Wendy Lehnert. 1995. Using de-
cision trees for coreference resolution. In Proceedings
of the Fourteenth International Conference on Artifi-
cial Intelligence, pages 1050?1055.
Alessandro Moschitti. 2004. A study on convolution ker-
nels for shallow statistic parsing. In Proceedings of the
42nd Annual Meeting of the Association for Computa-
tional Linguistics, pages 335?342.
Vincent Ng and Claire Cardie. 2002. Identifying
anaphoric and non-anaphoric noun phrases to improve
coreference resolution. In Proceedings of the 19th In-
ternational Conference on Computational Linguistics,
pages 730?736.
1079
Vincent Ng. 2004. Learning noun phrase anaphoricity
to improve conference resolution: Issues in represen-
tation and optimization. In Proceedings of the 42nd
Annual Meeting of the Association for Computational
Linguistics, pages 151?158.
Vincent Ng. 2009. Graph-cut-based anaphoricity deter-
mination for coreference resolution. In Proceedings
of Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 575?
583.
Malvina Nissim, Shipra Dingare, Jean Carletta, and Mark
Steedman. 2004. An annotation scheme for infor-
mation status in dialogue. In Proceedings of the 4th
International Conference on Language Resources and
Evaluation.
Malvina Nissim. 2006. Learning information status of
discourse entities. In Proceedings of the 2006 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 94?102.
Ellen Prince. 1981. Toward a taxonomy of given-new
information. In P. Cole, editor, Radical Pragmatics,
pages 223?255. New York, N.Y.: Academic Press.
Altaf Rahman and Vincent Ng. 2009. Supervised mod-
els for coreference resolution. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 968?977.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational Lin-
guistics, 27(4):521?544.
Mark Steedman. 2000. The Syntactic Process. The MIT
Press, Cambridge, MA.
Michael Strube. 1998. Never look back: An alternative
to centering. In Proceedings of the 36th Annual Meet-
ing of the Association for Computational Linguistics
and the 17th International Conference on Computa-
tional Linguistics, pages 1251?1257.
Olga Uryupina. 2003. High-precision identification of
discourse new and unique noun phrases. In Proceed-
ings of the ACL Student Research Workshop, pages 80?
86.
Enric Vallduv??. 1992. The Informational Component.
Garland, New York.
Yannick Versley, Alessandro Moschitti, Massimo Poe-
sio, and Xiaofeng Yang. 2008a. Coreference systems
based on kernels methods. In Proceedings of the 22nd
International Conference on Computational Linguis-
tics, pages 961?968.
Renata Vieira and Massimo Poesio. 2000. An
empirically-based system for processing definite de-
scriptions. Computational Linguistics, 26(4):539?
593.
Xiaofeng Yang, Jian Su, Jun Lang, Chew Lim Tan, and
Sheng Li. 2008. An entity-mention model for coref-
erence resolution with inductive logic programming.
In Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 843?851.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation
extraction. Journal of Machine Learning Research,
3:1083?1106.
GuoDong Zhou and Fang Kong. 2009. Global learn-
ing of noun phrase anaphoricity in coreference reso-
lution via label propagation. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 978?986.
1080
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 777?789, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Resolving Complex Cases of Definite Pronouns:
The Winograd Schema Challenge
Altaf Rahman and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{altaf,vince}@hlt.utdallas.edu
Abstract
We examine the task of resolving complex
cases of definite pronouns, specifically those
for which traditional linguistic constraints
on coreference (e.g., Binding Constraints,
gender and number agreement) as well as
commonly-used resolution heuristics (e.g.,
string-matching facilities, syntactic salience)
are not useful. Being able to solve this task has
broader implications in artificial intelligence:
a restricted version of it, sometimes referred
to as the Winograd Schema Challenge, has
been suggested as a conceptually and practi-
cally appealing alternative to the Turing Test.
We employ a knowledge-rich approach to this
task, which yields a pronoun resolver that out-
performs state-of-the-art resolvers by nearly
18 points in accuracy on our dataset.
1 Introduction
Despite the significant amount of work on pronoun
resolution in the natural language processing com-
munity in the past forty years, the problem is still
far from being solved. Its difficulty stems in part
from its reliance on sophisticated knowledge sources
and inference mechanisms. The sentence pair below,
which we will subsequently refer to as the shout ex-
ample, illustrates how difficult the problem can be:
(1a) Ed shouted at Tim because he crashed the car.
(1b) Ed shouted at Tim because he was angry.
The pronoun he refers to Tim in 1a and Ed in 1b.
Humans can resolve the pronoun easily, but state-
of-the-art coreference resolvers cannot. The reason
is that humans have the kind of world knowledge
needed to resolve the pronouns that machines do not.
Our world knowledge tells us that if someone is an-
gry, he may shout at other people. Since Ed shouted,
he should be the one who was angry. Our world
knowledge also tells us that we may shout at some-
one who made a mistake and that crashing a car is
a mistake. Combining these two pieces of evidence,
we can easily infer that Tim crashed the car.
Our goal in this paper is to examine the resolu-
tion of complex cases of definite pronouns that ap-
pear in sentences exemplified by the shout example.
Specifically, each sentence (1) has two clauses sepa-
rated by a discourse connective (i.e., the connective
appears between the two clauses, just like because
in the shout example), where the first clause con-
tains two or more candidate antecedents (e.g., Ed
and Tim), and the second clause contains the tar-
get pronoun (e.g., he); and (2) the target pronoun
agrees in gender, number, and semantic class with
each candidate antecedent, but does not have any
overlap in content words with any of them. For con-
venience, we will refer to the target pronoun that ap-
pears in this kind of sentences as a difficult pronoun.
Note that many traditional linguistic constraints
on coreference are no longer useful for resolving dif-
ficult pronouns. For instance, syntactic constraints
such as the Binding Constraints will not be useful,
since the pronoun and the candidate antecedents ap-
pear in different clauses separated by a discourse
connective; and constraints concerning agreement in
gender, number, and semantic class will not be use-
ful, since the pronoun and the candidate antecedents
are compatible with respect to all these attributes.
Traditionally important clues provided by various
777
I(a) The city councilmen refused the demonstrators a permit because they feared violence.
I(b) The city councilmen refused the demonstrators a permit because they advocated violence.
II(a) James asked Robert for a favor, but he refused.
II(b) James asked Robert for a favor, but he was refused.
III(a) Keith fired Blaine but he did not regret.
III(b) Keith fired Blaine although he is diligent.
IV(a) Emma did not pass the ball to Janie, although she was open.
IV(b) Emma did not pass the ball to Janie, although she should have.
V(a) Medvedev will cede the presidency to Putin because he is more popular.
V(b) Medvedev will cede the presidency to Putin because he is less popular.
Table 1: Sample twin sentences. The target pronoun in each sentence is italicized, and its antecedent is boldfaced.
string-matching facilities will not be useful either,
since the pronoun and its candidate antecedents do
not have any words in common.
As in the shout example, we ensure that each sen-
tence has a twin. Twin sentences were used ex-
tensively by researchers in the 1970s to illustrate
the difficulty of pronoun resolution (Hirst, 1981).
We consider two sentences as twins if (1) they
are identical up to and possibly including the dis-
course connective; and (2) the difficult pronouns in
them are lexically identical but have different an-
tecedents. The presence of twins implies that syn-
tactic salience, a commonly-used heuristic in pro-
noun resolution that prefers the selection of syntac-
tically salient candidate antecedents, may no longer
be useful, since the candidate in the subject position
is not more likely to be the correct antecedent than
the other candidates.
To enable the reader to get a sense of how hard it is
to resolve difficult pronouns, Table 1 shows sample
twin sentences from our dataset. Note that state-of-
the-art pronoun resolvers (e.g., JavaRAP (Qiu et al
2004), GuiTaR (Poesio and Kabadjov, 2004), as well
as those designed by Mitkov (2002) and Charniak
and Elsner (2009)) and coreference resolvers (e.g.,
BART (Versley et al2008), CherryPicker (Rahman
and Ng, 2009), Reconcile (Stoyanov et al2010),
the Stanford resolver (Raghunathan et al2010; Lee
et al2011)) cannot accurately resolve the difficult
pronouns in these structurally simple sentences, as
they do not have the mechanism to capture the fine
distinctions between twin sentences. In other words,
when given these sentences, the best that the existing
resolvers can do to resolve the pronouns is guess-
ing. This could be surprising to a non-coreference
researcher, but it is indeed the state of the art.
A natural question is: why do existing resolvers
not attempt to handle difficult pronouns? One rea-
son could be that these difficult pronouns do not
appear frequently in standard evaluation corpora
such as MUC, ACE, and OntoNotes (Bagga, 1998;
Haghighi and Klein, 2009). In fact, the Stanford
coreference resolver (Lee et al2011), which won
the CoNLL-2011 shared task on coreference resolu-
tion, adopts the once-popular rule-based approach,
resolving pronouns simply with rules that encode
the aforementioned traditional linguistic constraints
on coreference, such as the Binding constraints and
gender and number agreement.
The infrequency of occurrences of difficult pro-
nouns in these standard evaluation corpora by no
means undermines their significance, however. In
fact, being able to automatically resolve difficult
pronouns has broader implications in artificial intel-
ligence. Recently, Levesque (2011) has argued that
the problem of resolving the difficult pronouns in
a carefully chosen set of twin sentences, which he
refers to as the Winograd Schema Challenge1, could
serve as a conceptually and practically appealing
alternative to the well-known Turing Test (Turing,
1Levesque (2011) defines a Winograd Schema as a small
reading comprehension test involving the question of which of
the two candidate antecedents for the definite pronoun in a given
sentence is its correct antecedent. Levesque names this chal-
lenge after Winograd because of his pioneering attempt to use a
well-known pair of twin sentences ? specifically the first pair
in Table 1 ? to illustrate the difficulty of natural language un-
derstanding (Winograd, 1972). Strictly speaking, we are ad-
dressing a relaxed version of the Challenge: while Levesque
focuses solely on definite pronouns whose resolution requires
background knowledge not expressed in the words of a sen-
tence, we do not impose such a condition on a sentence.
778
1950). The reason should perhaps be clear given the
above discussion: this is an easy task for a subject
who can ?understand? natural language but a chal-
lenging task for one who can only make intelligent
guesses. Levesque believes that ?with a very high
probability?, anything that can resolve correctly a
series of difficult pronouns ?is thinking in the full-
bodied sense we usually reserve for people?. Hence,
being able to make progress on this task enables us
to move one step closer to building an intelligent ma-
chine that can truly understand natural language.
To sum up, an important contribution of our work
is that it opens up a new line of research involving
a problem whose solution requires a deeper under-
standing of a text. With recent advances in knowl-
edge extraction from text, we believe that time is ripe
to tackle this problem. It is worth noting that some
researchers have focused on other kinds of anaphors
that are hard to resolve, including bridging anaphors
(e.g., Poesio et al2004)) and anaphors referring
to abstract entities, such as those realized by verb
phrases in dialogs (e.g., Byron (2002), Strube and
Mu?ller (2003), Mu?ller (2007)). Nevertheless, to our
knowledge, there has been little work that specifi-
cally targets difficult pronouns.
Given the complexity of our task, we investigate
a variety of sophisticated knowledge sources for re-
solving difficult pronouns, and combine them via a
machine learning approach. Note that there has been
a recent surge of interest in extracting world knowl-
edge from online encyclopedias such as Wikipedia
(e.g., Ponzetto and Strube (2006, 2007), Poesio et
al. (2007)), YAGO (e.g., Bryl et al2010), Rahman
and Ng (2011), Uryupina et al2011)), and Free-
base (e.g., Lee et al2011)). However, the resulting
extractions are primarily IS-A relations (e.g., Barack
Obama IS-A U. S. president), which would not be
useful for resolving definite pronouns.
2 Dataset Creation
We asked 30 undergraduate students who are not af-
filiated with this research to compose sentence pairs
(i.e., twin sentences) that conform to the constraints
specified in the introduction. Each student was also
asked to annotate the candidate antecedents, the tar-
get pronoun, and the correct antecedent for each
sentence she composed. Note that a sentence may
contain multiple pronouns, but exactly one of them
? the one explicitly annotated by its author ? is
the target pronoun. Each sentence pair was cross-
checked by one other student to ensure that it (1)
conforms to the desired constraints and (2) does not
contain pronouns with ambiguous antecedents (in
other words, a human should not be confused as
to which candidate antecedent is the correct one).
At the end of the process, 941 sentence pairs were
considered acceptable, and they formed our dataset.
These sentences cover a variety of topics, ranging
from real events (e.g., Iran?s plan to attack the Saudi
ambassador to the U.S.), to events and characters in
movies (e.g., Batman and Robin), to purely imagi-
nary situations (e.g., the shout example). We parti-
tion these sentence pairs into a training set and a test
set following a 70/30 ratio.
While not requested by us, the students annotated
exactly two candidate antecedents for each sentence.
For ease of exposition, we will assume below that
there are two candidate antecedents per sentence.
3 Machine Learning Framework
Since our goal is to determine which of the two can-
didate antecedents is the correct antecedent for the
target pronoun in each sentence, our system assumes
as input the sentence, the target pronoun, and the two
candidate antecedents.
We employ machine learning to combine the
features derived from different knowledge sources.
Specifically, we employ a ranking-based approach.
Ranking-based approaches have been shown to out-
perform their classification-based counterparts (De-
nis and Baldridge, 2007, 2008; Iida et al2003;
Yang et al2003). Given a pronoun and two can-
didate antecedents, we aim to train a ranking model
that ranks the two candidates such that the correct
antecedent is assigned a higher rank.
More formally, given training sentence Sk con-
taining target pronoun Ak, correct antecedent Ck
and incorrect antecedent Ik, we create two feature
vectors, xCAk and xIAk , where xCAk is generated
from Ak and Ck, and xIAk is generated from Ak
and Ik. The training set consists of ordered pairs
of feature vectors (xCAk , xIAk ), and the goal of the
training procedure is to acquire a ranker that mini-
mizes the number of violations of pairwise rankings
779
provided in the training set. We train this ranker us-
ing Joachims? (2002) SVMlight package. It is worth
noting that we do not exploit the fact that each sen-
tence has a twin in training or testing.
After training, the ranker can be applied to the test
instances, which are created in the same way as the
training instances. For each test instance, the target
pronoun is resolved to the higher-ranked candidate
antecedent.
4 Linguistic Features
We derive linguistic features for resolving difficult
pronouns from eight components, as described be-
low. To enable the reader to keep track of these fea-
tures more easily, we summarize them in Table 2.
4.1 Narrative Chains
Consider the following sentence:
(2) Ed punished Tim because he tried to escape.
Humans resolve he to Tim by exploiting the world
knowledge that someone who tried to escape is bad
and therefore should be punished. Such kind of
knowledge can be extracted from narrative chains.
Narrative chains are partially ordered sets of
events centered around a common protagonist, aim-
ing to encode the kind of knowledge provided by
scripts (Schank and Abelson, 1977). While scripts
are hand-written, narrative chains can be learned
from unannotated text. Below is a chain learned by
Chambers and Jurafsky (2008):
borrow-s invest-s spend-s pay-s raise-s lend-s
As we can see, a narrative chain is composed of a
sequence of events (verbs) together with the roles of
the protagonist. Here, ?s? denotes the subject role,
even though a chain can contain a mix of ?s? and ?o?
(the object role). From this chain, we know that the
person who borrows something (probably money)
may invest, spend, pay, or lend it.
We employ narrative chains to heuristically pre-
dict the antecedent for the target pronoun, and en-
code the prediction as a feature. The heuristic de-
cision procedure operates as follows. Given a sen-
tence, we first determine the event the target pro-
noun participates in and its role in the event. As
an example, we determine that in sentence (2) he
participates in the try event and the escape event
Component # Features Features
Narrative Chains 1 NC
Google 4 G1, G2, G3, G4
FrameNet 4 FN1, FN2, FN3, FN4
Heuristic Polarity 3 HPOL1, HPOL2, HPOL3
Learned Polarity 3 LPOL1, LPOL2, LPOL3
Connective-Based 1 CBR
Relation
Semantic Compat. 3 SC1, SC2, SC3
Lexical Features 68,331 antecedent- independent
and dependent features
Table 2: Summary of the features described in Section 4.
as a subject.2 Second, we determine the event(s)
that the candidate antecedents participate in. In (2),
both candidate antecedents participate in the pun-
ish event. Third, we pair each event participated
by each candidate antecedent with each event par-
ticipated by the pronoun. In our example, we would
create two pairs, (punish, try-s) and (punish, escape-
s). Note that try and escape are associated with the
role of the pronoun that we extracted in the first step.
Fourth, for each such pair, we extract all the narra-
tive chains containing both elements in the pair from
Chambers and Jurafsky?s output.3 This step results
in one chain being extracted, which contains punish-
o and escape-s. In other words, the protagonist in
this chain is the subject of an escape event and the
object of a punish event. Fifth, from the extracted
chain, we obtain the role played by the pronoun (i.e.,
the protagonist) in the event in which the candidate
antecedents participate. In our example, the pronoun
plays an object role in the punish event. Finally, we
extract the candidate antecedent that plays the ex-
tracted role, which in our example is the second an-
tecedent, Tim.4
We create a binary feature, NC, which encodes
this heuristic decision, and compute its value as fol-
lows. Assume in the rest of the paper that i1 and
i2 are the feature vectors corresponding to the first
candidate antecedent and the second candidate an-
2Throughout the paper, the subject/object of an event refers
to its deep rather than surface subject/object. We determine
the grammatical role of an NP using the Stanford dependency
parser (de Marneffe et al2006) and a set of simple heuristics.
3We employ narrative chains of length 12, which are
available from http://cs.stanford.edu/people/
nc/schemas/schemas-size12.
4For an alternative way of using narrative chains for coref-
erence resolution, see Irwin et al2011).
780
tecedent, respectively.5 For our running example,
since Tim is predicted to be the antecedent of he,
the value of NC in i2 is 1, and its value in i1 is 0.
For notational convenience, we write NC(i1)=0 and
NC(i2)=1, and will follow this convention when de-
scribing the features in the rest of the paper.
Finally, we note that NC(i1) and NC(i2) will
both be set to zero if (1) the pronoun and the an-
tecedents do not participate in events, or (2) no nar-
rative chains can be extracted in step 4 above, or (3)
step 4 enables us to extract more than one chain and
these chains indicate that the candidate antecedent
can have both a subject role and an object role.
4.2 Google
Consider the following sentences:
(3a) Lions eat zebras because they are predators.
(3b) The knife sliced through the flesh because
it was sharp.
Humans resolve they to Lions in (3a) by exploiting
the world knowledge that predators attack and eat
other animals. Similarly, humans resolve it to the
knife in (3b) by exploiting the world knowledge that
the word sharp can be used to describe a knife but
not flesh. To acquire this kind of world knowledge,
we learn patterns of word usage from the Web by
issuing search queries. To facilitate our discussion,
let us first introduce some notation. Let a sentence
S be denoted by a triple (Z1, Conn, Z2), where Z1
and Z2 are the clauses preceding and following the
discourse connective Conn, respectively; A ? Z2
be the pronoun governed by the verb V ; W be the
sequence of words following V in S; and C1, C2 ?
Z1 be the candidate antecedents.
Given a sentence, we generate four queries: (Q1)
C1V ; (Q2) C2V ; (Q3) C1V W ; and (Q4) C2V W . If
v is a verb-to-be followed by an adjective J , we gen-
erate two more queries: (Q5) JC1 and (Q6) JC2.
To exemplify, six queries are generated for (3b):
(Q1) ?knife was?; (Q2) ?flesh was?; (Q3) ?knife was
sharp?; (Q4) ?flesh was sharp?; (Q5) ?sharp knife?;
and (Q6) ?sharp flesh?. On the other hand, only four
queries are generated for (3a): (Q1) ?lions are?; (Q2)
5The nth candidate antecedent in a sentence is the nth an-
notated NP encountered when processing the sentence in a left-
to-right manner. In sentence (2), Ed is the first candidate an-
tecedent and Tim is the second.
?zebras are?; (Q3) ?lions are predators?; and (Q4)
?zebras are predators?.
Using the counts returned by Google for these
queries, we create four features, G1, G2, G3, and
G4, whose values are determined by Rules 1, 2, 3,
and 4, respectively, as described below.
Rule 1: if count(Q1) > count(Q2) by at
least x% then G1(i1)=1 and G1(i2)=0;
else if count(Q2) > count(Q1) by at least
x% then G1(i2)=1 and G1(i1)=0; else
G1(i1)=G1(i2)=0.
Rule 2: if count(Q3) > count(Q4) by at
least x% then G2(i1)=1 and G2(i2)=0;
else if count(Q4) > count(Q3) by at least
x% then G2(i2)=1 and G2(i1)=0; else
G2(i1)=G2(i2)=0.
Rule 3: if count(Q5) > count(Q6) by at
least x% then G3(i1)=1 and G3(i2)=0;
else if count(Q6) > count(Q5) by at least
x% then G3(i2)=1 and G3(i1)=0; else
G3(i1)=G3(i2)=0.
Rule 4: if one of G1(i1) and G1(i2) is 1,
then G4(i1)=G1(i1) and G4(i2)=G1(i2);
else if one of G2(i1) and G2(i2) is 1,
then G4(i1)=G2(i1) and G4(i2)=G2(i2);
else if one of G3(i1) and G3(i2) is 1,
then G4(i1)=G3(i1) and G4(i2)=G3(i2);
else G4(i1)=G4(i2)=0.
The role of the threshold x should be obvious: it
ensures that a heuristic decision is made only if the
difference between the counts for the two queries are
sufficiently large, because otherwise there is no rea-
son for us to prefer one candidate antecedent to the
other. In all of our experiments, we set x to 20.
Note that other researchers have also used lexico-
syntactic patterns to generate search queries for
bridging anaphora resolution (e.g., Poesio et al
(2004)), other-anaphora resolution (e.g., Modjeska
et al2003)), and learning selectional preferences
for pronoun resolution (e.g., Yang et al2005)).
However, in each of these three cases, the target re-
lations (e.g., the part-whole relation in the case of
bridging anaphora resolution, and the subject-verb
and verb-object relations in the case of selectional
preferences) are specific enough that they can be ef-
fectively captured by specific patterns. For example,
781
to determine whether the wheel is part of the car in
bridging anaphora resolution, Poesio et almploy
queries of the form ?X of Y?, where X and Y would
be replaced with the wheel and the car, respectively.
On the other hand, we are not targeting a particular
type of relation. Rather, we intend to capture world
knowledge like lions rather than zebras are preda-
tors. Such knowledge may not be expressed as a
relation and hence may not be easily captured using
specific patterns. For this reason, we need to employ
patterns as general as those such as Q3 and Q4.
4.3 FrameNet
If we generate search queries as described in the pre-
vious subsection for the shout example, it is unlikely
that Google will return meaningful counts to us. The
reason is that both candidate antecedents in the sen-
tence are proper names belonging to the same type
(which in this case is PERSON).
However, in some cases, we may be able to gener-
ate more meaningful queries from such kind of sen-
tences. Consider the following sentence:
(4) John killed Jim, so he was arrested.
To generate meaningful queries, we make one ob-
servation: John and Jim played different roles in a
kill event. Hence, we can replace these proper names
with their roles. We propose to obtain these roles
from FrameNet (Baker et al1998). More gener-
ally, for each proper name e in a given sentence, we
(1) determine the event in which e is involved (using
the Stanford dependency parser); (2) search for the
FrameNet frame corresponding to the event as well
as e?s role in the event; and (3) replace the name
with its FrameNet role. In our example, since both
names are involved in the kill event, we retrieve the
FrameNet frame for kill. Given that John and Jim are
the subject and object of kill, we can extract their se-
mantic roles directly from the frame, which are killer
and victim, respectively.6 Consequently, we replace
the two names with their extracted semantic roles,
and generate the search queries from the resulting
sentence in the same way as before.
Note that if no frames can be found for the verb in
the first clause, no search queries will be generated.
After obtaining the query counts, we generate four
binary features, FN1, FN2, FN3, FN4, whose values
6We heuristically map grammatical roles to semantic roles.
are computed based on the same four heuristic rules
that were discussed in the previous subsection.
4.4 Heuristic Polarity
Some sentences involve comparing the two candi-
date antecedents. Consider the following sentences:
(5a) John was defeated by Jim in the election
even though he is more popular.
(5b) John was defeated by Jim in the election
because he is more popular.
The pronoun he refers to John in (5a) and Jim in
(5b). To see how we can design an algorithm for re-
solving these pronouns, it would be useful to under-
stand how humans resolve them. The phrase more
popular has a positive sentiment. In (5a), the use
of even though yields a clause of concession, which
flips the polarity of more popular (from positive to
negative), whereas in (5b), the use of because yields
a clause of cause, which does not change the po-
larity of more popular (i.e., more popular remains
positive). Since more popular is used to describe he,
he is ?better? in (5b) but ?worse? in (5a). Now, the
word defeat has a positive sentiment, and since Jim
is the deep subject of defeat, Jim is ?better? and John
is ?worse?. Finally, in (5b), he and Jim are ?better?,
so he is resolved to Jim; on the other hand, in (5a),
he and John are ?worse?, so he is resolved to John.
We automate this (human) method for resolv-
ing pronouns as follows. We begin by determin-
ing whether we can assign a rank value (i.e., ?bet-
ter? or ?worse?) to the pronoun and the two can-
didate antecedents. For instance, to determine the
rank value of the pronoun A, we first determine the
polarity value pA of its anchor word wA, which is
either the verb v for which A serves as the deep sub-
ject, or the adjective modifying A if v does not ex-
ist,7 using Wilson et al (2005b) subjectivity lex-
icon.8 If pA is not NEUTRAL, we check whether
it can be flipped by the context of wA. We con-
sider three kinds of polarity-reversing context: nega-
tion, comparative adverb, and discourse connective.
Specifically, we determine whether wA is negated
using the Stanford dependency parser, which explic-
7In the sentiment analysis and opinion mining literature,
(wA, pA) is known as an opinion-target pair.
8The lexicon contains 8221 words, each of which is hand
labeled with a polarity of POSITIVE, NEGATIVE, or NEUTRAL.
782
itly annotates instances of negation; we determine
the existence of a comparative adverb (e.g., ?more?,
?less?) using the POS tag ?RBR?; and we determine
whether A exists in a clause headed by a polarity-
reversing connective, such as although. After flip-
ping pA by context, we can infer A?s rank value from
it. Specifically, A?s rank value is ?better? if pA is
positive; ?worse? if pA is negative; and ?cannot be
determined? if pA is neutral. The polarity values of
the two candidate antecedents can be determined in
a similar fashion. Note that sometimes we may need
to infer rank values. For example, given the sentence
?Jane is prettier than Jill?, prettier has a positive po-
larity, so its modifying NP, Jane, has a ?better? rank,
and we can infer that Jill?s rank is ?worse?.
We create three features, HPOL1, HPOL2, and
HPOL3, based on our heuristic polarity determina-
tion component. Specifically, if the rank value of
the pronoun or the rank value of one or both of the
candidate antecedents cannot be determined, the val-
ues of all three binary features will be set to zero
for both i1 and i2. Otherwise, we compute the val-
ues of the three features as follows. To compute
HPOL1, which is a binary feature, we (1) employ
a heuristic resolution procedure, which resolves the
pronoun to the candidate antecedent with the same
rank value, and then (2) encode the outcome of this
heuristic procedure as the value of HPOL1. For ex-
ample, since the first candidate antecedent, John, is
predicted to be the antecedent in (5a), HPOL1(i1)=1
and HPOL1(i2)=0. The value of HPOL2 is the
concatenation of the polarity values determined
for the pronoun and the candidate antecedent.
Referring again to (5a), HPOL2(i1)=positive-
positive and HPOL2(i2)=positive-negative. To
compute HPOL3 for a given instance, we sim-
ply take its HPOL2 value and append the
connective to it. Using (5a) as an exam-
ple, HPOL3(i1)=positive-positive-even-though and
HPOL3(i1)=positive-negative-even-though.
4.5 Machine-Learned Polarity
In the previous subsection, we compute the polarity
of a word by updating its prior polarity heuristically
with contextual information. We hypothesized that
polarity could be computed more accurately by em-
ploying a sentiment analyzer that can capture richer
contextual information. For this reason, we employ
OpinionFinder (Wilson et al2005a), which has a
pre-trained classifier for annotating the phrases in a
sentence with their contextual polarity values.
Given a sentence and the polarity values of the
phrases annotated by OpinionFinder, we determine
the rank values of the pronoun and the two candi-
date antecedents by mapping them to the polarized
phrases using the dependency relations provided by
the Stanford dependency parser. We create three bi-
nary features, LPOL1, LPOL2, and LPOL3, whose
values are computed in the same way as HPOL1,
HPOL2, and HPOL3, respectively, except that the
computation here is based on the machine-learned
polarity values rather than the heuristically deter-
mined polarity values.
4.6 Connective-Based Relations
Consider the following sentences:
(6a) Google bought Motorola because they
want its customer base.
(6b) Google bought Motorola because they
are rich.
Humans resolve they to Google in (6a) by exploit-
ing the world knowledge that there is a causal rela-
tion (signaled by the discourse connective because)
between the want event and the buy event. A simi-
lar mechanism is used to resolve they to Google in
(6b): from world knowledge we know that there is a
causal relation between rich and buy.
We automate this (human) method for resolving
pronouns as follows. First, we gather connective-
based relations of this kind from a large, unanno-
tated corpus. In our experiments, we use as our
unannotated corpus the documents in three text cor-
pora (namely, BLLIP, Reuters, and English Giga-
word), but retain only those sentences that con-
tain a single discourse connective and do not be-
gin with the connective. From these sentences,
we collect triples and their frequencies of occur-
rences in the corpus. Each triple is of the form
(V ,Conn,X), where Conn is a discourse connec-
tive, V is a stemmed verb in the clause preceding
Conn, and X is a stemmed verb or an adjective in
the clause following Conn. Each triple essentially
denotes a relation between V and X expressed by
Conn. Conceivably, the strength of the relation in a
triple increases with its frequency count.
783
We use the frequency counts of these triples to
heuristically predict the correct antecedent for a tar-
get pronoun. Given a sentence where Conn is the
discourse connective, X is the stemmed verb gov-
erning the target pronoun A or the adjective modify-
ing A (if X is a to be verb), and V is the stemmed
verb governing the candidate antecedents, we re-
trieve the frequency count of the triple (V ,Conn,X).
If the count is at least 100, we employ a procedure
for heuristically selecting the antecedent for the tar-
get anaphor. Specifically, if X is a verb, then it re-
solves the target pronoun to the candidate antecedent
that has the same grammatical role as the pronoun.
However, if X is an adjective and the sentence does
not involve comparison, then it resolves the target
pronoun to the candidate antecedent serving as the
subject of V .
We create a binary feature, CBR, that encodes
this heuristic decision. In our running example, the
triple (buy, because, want) occurs 860 times in our
corpus, so the pronoun they is resolved to the can-
didate antecedent that occurs as the subject of buy.
Hence, CBR(i1)=1 and CBR(i2)=0. However, had
the triple occurred less than 100 times, both of these
features would have been set to zero.
4.7 Semantic Compatibility
Some of the queries generated by the Google com-
ponent, such as Q1 and Q2, aim to capture the
semantic compatibility between a candidate an-
tecedent, C , and the verb governing the target pro-
noun, V . However, using web search queries to esti-
mate semantic compatibility has potential problems,
including (1) a precision problem: the fact that C
and V appear next to each other in a query does
not necessarily imply that a subject-verb relation ex-
ists between them; and (2) a recall problem: these
queries fail to capture subject-verb relations where
C and V are not immediately adjacent to each other.
To address these potential problems, we com-
pute knowledge of selectional preferences from a
large, unannotated corpus. As before, we cre-
ate our unannotated corpus using the documents in
BLLIP, Reuters, and English Gigaword. Specifi-
cally, we first parse each sentence in the corpus us-
ing the Stanford dependency parser. Then, for each
stemmed verb v and each stemmed noun n in the
corpus, we collect the following statistics: (1) the
number of times n is the subject of v; (2) the num-
ber of times n is the direct object of v; (3) the mutual
information (MI) of v and n (with n as the subject
of v); and (4) the MI of v and n (with n as the direct
object of v).9
To understand how we use these statistics to gen-
erate features for resolving pronouns, consider the
following sentence:
(7) The man stole the neighbor?s bike because
he needed one.
Assuming that the target pronoun and its govern-
ing verb V has grammatical relation GR, we create
three features, SC1, SC2, and SC3, based on our se-
mantic compatibility component. SC1 encodes the
MI value of the head noun of a candidate antecedent
and V (and GR). SC2 is a binary feature whose
value indicates which of the candidate antecedents
has a larger MI value with V (and GR). SC3 is the
same as SC2, except that MI is replaced with corpus
frequency. In other words, SC2 and SC3 employ
different measures to heuristically predict the cor-
rect antecedent for the target pronoun. If the target
pronoun is governed by a to be verb, the values of
these three features will all be set to zero.
Given our running example, we first retrieve
the following corpus-based statistics: MI(need:subj,
man)=0.6322; MI(need:subj, neighbor)=0.3975;
count(need:subj, man)=474; and count(need:subj,
neighbor)=68. Using these statistics, we can then
compute the aforementioned features for our exam-
ple. Specifically, SC1(i1)=0.6322, SC1(i2)=0.3975,
SC2(i1)=1, SC2(i2)=0, SC3(i1)=1, and SC3(i2)=0.
4.8 Lexical Features
We exploit the coreference-annotated training docu-
ments by creating lexical features from them. These
lexical features can be divided into two categories,
depending on whether they are computed based on
the candidate antecedents.
Let us begin with the antecedent-independent fea-
tures. Assuming that W is an arbitrary word in a
sentence S that is not part of a candidate antecedent
and Conn is the connective in S, we create three
types of binary-valued antecedent-independent fea-
tures, namely (1) unigrams, where we create one
9We use the same formula as described in Section 4.2 of
Bergsma and Lin (2006) to compute MI values.
784
feature for each W ; (2) word pairs, where we cre-
ate features by pairing each W appearing before
Conn with each W appearing after Conn, exclud-
ing adjective-noun and noun-adjective pairs10; and
(3) word triples, where we augment each word pair
in (2) with Conn. The value of each feature f indi-
cates the presence or absence of f in S.
Next, we compute the antecedent-dependent fea-
tures. Let (1) HC1 and HC2 be the head words of
candidate antecedents C1 and C2, respectively; (2)
VC1 , VC2 , and VA be the verbs governing C1, C2,
and the target pronoun A, respectively; and (3) JC1 ,
JC2 , and JA be the adjectives modifying C1, C2, and
A, respectively.11 We create from each candidate an-
tecedent four features, each of which is a word pair.
From C1, we create (HC1 , VC1), (HC1 , JC1 ), (HC1 ,
VA), and (HC1 , JA), all of which will appear in the
feature vector corresponding to C1. A similar set of
four features are created from C2. These antecedent-
dependent features are all binary-valued.
It is worth mentioning that while we also consid-
ered word triples in the connective-based relations
component and word pairs in the semantic compat-
ibility component, in those components we deter-
mine their usefulness in an unsupervised manner,
whereas by employing them as lexical features we
determine their usefulness in a supervised manner.
5 Evaluation
5.1 Experimental Setup
Dataset. We report results on the test set, which
comprises 30% of our hand-annotated sentence pairs
(see Section 2 for details).
Evaluation metrics. Results are expressed in
terms of accuracy, which is the percentage of cor-
rectly resolved target pronouns. We also report the
percentages of these pronouns that are (1) not re-
solved and (2) incorrectly resolved.
5.2 Results and Discussion
The Random baseline. Our first baseline is a re-
solver that randomly guesses the antecedent for the
10Pairing an adjective A in one clause with a noun N in an-
other clause may mislead the learner into thinking that N is
modified by A, and hence we do not create such pairs.
11If C1, C2, and A are not modified by adjectives, no
adjective-based features will be created.
target pronoun in each sentence. Since there are
two candidate antecedents per sentence, the Random
baseline should achieve an accuracy of 50%.
The Stanford resolver. Our second baseline is the
Stanford resolver (Lee et al2011), which achieves
the best performance in the CoNLL 2011 shared task
(Pradhan et al2011). As a rule-based resolver, it
does not exploit any coreference-annotated data.
Recall from Section 3 that our system assumes as
input not only a sentence containing a target pronoun
but also the two candidate antecedents. To ensure a
fair comparison, the same input is provided to this
and other baselines. Hence, if the Stanford resolver
decides to resolve the target pronoun, it will resolve
it to one of the two candidate antecedents. However,
if it does not have enough confidence about resolv-
ing it, it will leave it unresolved. Its performance on
the test set is shown in the ?Unadjusted Scores? col-
umn in row 1 of Table 3. As we can see, it correctly
resolves 40.1% of the pronouns, incorrectly resolves
29.8% of them, and does not make any decision on
the remaining 30.1%.
Given that the Random baseline correctly resolves
50% of pronouns and the Stanford resolver correctly
resolves only 40.1% of the pronouns, it is tempting
to conclude that Stanford does not perform as well
as Random. However, recall that Stanford leaves
30.1% of the pronouns unresolved. Hence, to ensure
a fairer comparison, we produce ?adjusted? scores
for the Stanford resolver, where we ?force? it to re-
solve all of the unresolved target pronouns by as-
suming that probabilistically half of them will be re-
solved correctly. This adjusted score is shown in the
?Adjusted Scores? column in row 1 of Table 3. As
we can see, Stanford achieves an accuracy of 55.1%,
which is 5.1 points higher than that of Random.
The Baseline Ranker. To understand whether the
somewhat unsatisfactory Stanford results can be at-
tributed to its inability to exploit the training data,
we employ as our third baseline a mention ranker
that is trained in the same way as our system (see
Section 3), except that it employs 39 commonly-
used linguistic features for learning-based corefer-
ence resolution (see Table 1 of Rahman and Ng
(2009) for a description of these features). Hence,
the performance difference between this Baseline
Ranker and our system can be attributed entirely
785
Unadjusted Scores Adjusted Scores
Coreference System Correct Wrong No Decision Correct Wrong No Decision
1 Stanford 40.07% 29.79% 30.14% 55.14% 44.86% 0.00%
2 Baseline Ranker 47.70% 47.16% 5.14% 50.27% 49.73% 0.00%
3 Stanford+Baseline Ranker 53.49% 43.12% 3.39% 55.19% 44.77% 0.00%
4 Our system 73.05% 26.95% 0.00% 73.05% 26.95% 0.00%
Table 3: Results of the Stanford resolver, the Baseline Ranker, the Combined resolver, and our system.
to the difference between the two linguistic feature
sets. Results of the Baseline Ranker are shown in
row 2 of Table 3. Before score adjustment, it cor-
rectly resolves 47.7% of the target pronouns, incor-
rectly resolves 47.2% of them, and leaves the re-
maining 5.1% unresolved. (Note that we output ?no
decision? if the ranker assigns the same rank value
to both candidate antecedents.) After score adjust-
ment, its accuracy is 50.3%, which is 0.3 points
higher than that of Random but statistically indis-
tinguishable from it.12 On the other hand, its accu-
racy is 4.9 points lower than that of Stanford, and
the difference between their performance is signifi-
cant. While it seems somewhat surprising that a su-
pervised resolver does not perform as well as a rule-
based resolver, neither of them employs knowledge
sources that are particularly useful for our dataset. In
other words, despite given access to annotated data,
the Baseline Ranker may not be able to make effec-
tive use of it due to the lack of useful features.
The Combined resolver. We create a fourth base-
line by combining the Stanford resolver and the
Baseline Ranker. The motivation is that the former
can provide better precision and the latter can pro-
vide better recall by handling ?no decision? cases
not covered by the former. Note that the Baseline
Ranker will be applied to resolve only those pro-
nouns that are left unresolved by Stanford. Results
in row 3 of Table 3 show that the adjusted accuracy
of this Combined resolver is 55.2%, which is sta-
tistically indistinguishable from Stanford?s adjusted
accuracy. Hence, these results show that the addi-
tion of the Baseline Ranker does not help improve
Stanford?s resolution accuracy.
Our system. Results of our system, which is
trained using the features described in Section 4 in
combination with a ranking model, are shown in
row 4 of Table 3. As we can see, our system achieves
12All statistical significance test results in this paper are ob-
tained using the paired t-test, with p < 0.05.
Feature Type Correct Wrong No Decision
All features 73.05% 26.95% 0.00%
?Narrative Chains 68.97% 31.03% 0.00%
?Google 65.96% 34.04% 0.00%
?FrameNet 72.16% 27.84% 0.00%
?Heuristic Polarity 71.45% 28.55% 0.00%
?Learned Polarity 72.70% 27.30% 0.00%
?Connective-Based Rel. 71.28% 28.72% 0.00%
?Semantic Compat. 71.81% 28.19% 0.00%
?Lexical Features 60.11% 25.35% 14.54%
Table 4: Results of feature ablation experiments.
an accuracy of 73.1%, significantly outperforming
the Combined resolver by 17.9 points in accuracy.
These results suggest that our features are more use-
ful for resolving difficult pronouns than those com-
monly used for coreference resolution.
5.3 Feature Analysis
In an attempt to gain additional insight into the per-
formance contribution of each of the eight types of
features used in our system, we conduct feature ab-
lation experiments. The unadjusted scores of these
experiments are shown in Table 4, where each row
shows the performance of the model trained on all
types of features except for the one shown in that
row. For easy reference, the performance of the
model trained on all types of features is shown in
row 1 of the table.
A few points deserve mention. First, perfor-
mance drops significantly whichever feature type is
removed. This suggests that all eight feature types
are contributing positively to overall accuracy. Sec-
ond, the Google-based features and the Lexical Fea-
tures are the most useful, and those generated via
FrameNet and Learned Polarity are the least use-
ful in the presence of other feature types. While it
is somewhat surprising that Learned Polarity is not
more useful than Heuristic Polarity, we speculate
the reason can be attributed to the fact that the cor-
pus on which OpinionFinder was trained was quite
different from ours. Finally, even without using the
786
Feature Type Correct Wrong No Decision
Narrative Chains 30.67% 24.47% 44.86%
Google 33.16% 7.09% 59.75%
FrameNet 7.27% 4.08% 88.65%
Learned Polarity 4.79% 2.66% 92.55%
Heuristic Polarity 7.27% 1.77% 90.96%
Connective-Based Rel. 14.01% 8.69% 77.30%
Semantic Compat. 23.58% 13.12% 63.30%
Lexical Features 56.91% 43.09% 0.00%
Table 5: Results of single-feature coreference models.
Lexical Features, our system still outperforms all the
baseline resolvers: as can been implied from the last
row of Table 4, in the absence of the Lexical Fea-
tures, our resolver achieves an adjusted accuracy of
67.4%, which is only 5.7 points less than that ob-
tained when the full feature set is employed. Hence,
while the Lexical Features are useful, their impor-
tance should not be over-emphasized.
To get a better idea of the utility of each feature
type, we conduct another experiment in which we
train eight models, each of which employs exactly
one type of features. Their unadjusted scores are
shown in Table 5. As we can see, Learned Polarity
has the smallest contribution, whereas the Lexical
Features have the largest contribution.
5.4 Error Analysis
While our resolver significantly outperforms state-
of-the-art resolvers, there is a lot of room for im-
provement. To help direct future research on the res-
olution of difficult pronouns, we analyze the major
sources of errors made by our resolver.
Our analysis reveals that many of the errors cor-
respond to cases that cannot be handled by any of
the eight components of our resolver. To understand
these cases, consider first the strengths and weak-
nesses of Narrative Chains and Google, the two
components that contribute the most to overall per-
formance after Lexical Features.
Google is especially good at capturing facts, such
as lions are predators and zebras are not predators,
helping us correctly resolve sentences such as (5a)
and (5b), as well as those in sentence pair (I) in Ta-
ble 1. However, it may not be good at handling pro-
nouns whose resolution requires an understanding of
the connection between the facts or events described
in the two clauses of a sentence. The reason is that
establishing such a connection requires that we con-
struct a search query composed of information ex-
tracted from both clauses, and the resulting, possi-
bly long, query is likely to receive no hit count due
to data sparseness. Investigating how to construct
such queries while avoiding data sparseness would
be an interesting line of future work.
Narrative chains, on the other hand, are useful
for capturing the relationship between the events de-
scribed in the two clauses. However, they are com-
puted over verbs, and therefore cannot capture such
a relationship when one or both of the events in-
volved are not described by verbs. For example,
narrative chains fail to capture the causal relation
between the event expressed by angry and shout in
sentence (1b). It is also worth mentioning that some
pronouns that could have been resolved using nar-
rative chains are not owing to the coverage and ac-
curacy of Chambers and Jurafsky?s (2008) chains,
but we believe that these recall and precision prob-
lems could be addressed by (1) inducing chains from
a larger corpus and (2) using semantic roles rather
than grammatical roles in the induction process.
Some resolution errors arise from errors in polar-
ity analysis. This can be attributed to the simplicity
of our Heuristic Polarity component: determining
the polarity of a word based on its prior polarity is
too na??ve. Fine-grained polarity analysis would be
a promising solution to this problem (see Pang and
Lee (2008) and Liu (2012) for related work).
6 Conclusions
We investigated the resolution of complex cases of
definite pronouns, a problem that was under exten-
sive discussion by coreference researchers in the
1970s but has received revived interest owing in part
to its relevance to the Turing Test. Our experimental
results indicate that it is a challenge for state-of-the-
art resolvers, and while we proposed new knowledge
sources for addressing this challenge, our resolver
still has a lot of room for improvement. In partic-
ular, our error analysis indicates that further gains
could be achieved via more accurate sentiment anal-
ysis and induction of world knowledge from corpora
or the Web. In addition, we plan to integrate our
resolver into a general-purpose coreference system
and evaluate the resulting resolver on standard eval-
uation corpora such as MUC, ACE, and OntoNotes.
787
Acknowledgments
We thank the three anonymous reviewers for their
detailed and insightful comments on an earlier draft
of the paper. This work was supported in part by
NSF Grants IIS-0812261 and IIS-1147644.
References
Amit Bagga. 1998. Coreference, Cross-Document
Coreference, and Information Extraction Methodolo-
gies. Ph.D. thesis, Duke University.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceed-
ings of the 36th Annual Meeting of the Association for
Computational Linguistics and the 17th International
Conference on Computational Linguistics, pages 86?
90.
Shane Bergsma and Dekang Lin. 2006. Bootstrapping
path-based pronoun resolution. In Proceedings of the
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association
for Computational Linguistics, pages 33?40.
Volha Bryl, Claudio Guiliano, Luciano Serafini, and
Kateryna Tymoshenko. 2010. Using background
knowledge to support coreference resolution. In Pro-
ceedings of the 19th European Conference on Artificial
Intelligence, pages 759?764.
Donna K. Byron. 2002. Resolving pronominal reference
to abstract entities. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics, pages 80?87.
Nathanael Chambers and Dan Jurafsky. 2008. Unsu-
pervised learning of narrative event chains. In Pro-
ceedings of the 46th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 787?797.
Eugene Charniak and Micha Elsner. 2009. EM works for
pronoun anaphora resolution. In Proceedings of the
12th Conference of the European Chapter of the Asso-
ciation for Computational Linguistics, pages 148?156.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of the 5th International Conference on Lan-
guage Resources and Evaluation, pages 449?454.
Pascal Denis and Jason Baldridge. 2007. A ranking ap-
proach to pronoun resolution. In Proceedings of the
Twentieth International Conference on Artificial Intel-
ligence, pages 1588?1593.
Pascal Denis and Jason Baldridge. 2008. Specialized
models and ranking for coreference resolution. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 660?669.
Aria Haghighi and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1152?1161.
Graeme Hirst. 1981. Anaphora in Natural Language
Understanding. Springer Verlag.
Ryu Iida, Kentaro Inui, Hiroya Takamura, and Yuji Mat-
sumoto. 2003. Incorporating contextual cues in train-
able models for coreference resolution. In Proceed-
ings of the EACL Workshop on The Computational
Treatment of Anaphora.
Joseph Irwin, Mamoru Komachi, and Yuji Matsumoto.
2011. Narrative schema as world knowledge for
coreference resolution. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 86?92.
Thorsten Joachims. 2002. Optimizing search engines us-
ing clickthrough data. In Proceedings of the Eighth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 133?142.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford?s multi-pass sieve coreference resolution sys-
tem at the CoNLL-2011 shared task. In Proceedings
of the Fifteenth Conference on Computational Natural
Language Learning: Shared Task, pages 28?34.
Hector J. Levesque. 2011. The Winograd Schema Chal-
lenge. In AAAI Spring Symposium: Logical Formal-
izations of Commonsense Reasoning.
Bing Liu. 2012. Sentiment Analysis and Opinion Min-
ing. Morgan & Claypool Publishers.
Ruslan Mitkov, Richard Evans, and Constantin Orasan.
2002. A new, fully automatic version of Mitkov?s
knowledge-poor pronoun resolution method. In Al.
Gelbukh, editor, Computational Linguistics and Intel-
ligent Text Processing, pages 169?187. Springer.
Natalia N. Modjeska, Katja Markert, and Malvina Nis-
sim. 2003. Using the web in machine learning for
other-anaphora resolution. In Proceedings of the 2003
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 176?183.
Christoph Mu?ller. 2007. Resolving it, this, and that in
unrestricted multi-party dialog. In Proceedings of the
45th Annual Meeting of the Association of Computa-
tional Linguistics, pages 816?823.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval 2(1?2):1?135.
Massimo Poesio and Mijail A. Kabadjov. 2004.
A general-purpose, off-the-shelf anaphora resolution
module: Implementation and preliminary evaluation.
In Proceedings of the 4th International Conference on
Language Resources and Evaluation, pages 663?666.
788
Massimo Poesio, Rahul Mehta, Axel Maroudas, and
Janet Hitzeman. 2004. Learning to resolve bridging
references. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics,
pages 143?150.
Massimo Poesio, David Day, Ron Artstein, Jason Dun-
can, Vladimir Eidelman, Claudio Giuliano, Rob Hall,
Janet Hitzeman, Alan Jern, Mijail Kabadjov, Stanley
Yong Wai Keong, Gideon Mann, Alessandro Mos-
chitti, Simone Ponzetto, Jason Smith, Josef Stein-
berger, Michael Strube, Jian Su, Yannick Versley,
Xiaofeng Yang, and Michael Wick. 2007. EL-
ERFED: Final report of the research group on Exploit-
ing Lexical and Encyclopedic Resources For Entity
Disambiguation. Technical report, Summer Workshop
on Language Engineering, Center for Language and
Speech Processing, Johns Hopkins University, Balti-
more, MD.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In Proceedings
of the Human Language Technology Conference and
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 192?
199.
Simone Paolo Ponzetto and Michael Strube. 2007.
Knowledge derived from Wikipedia for computing se-
mantic relatedness. Journal of Artificial Intelligence
Research, 30:181?212.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. CoNLL-2011 Shared Task: Modeling unre-
stricted coreference in OntoNotes. In Proceedings of
the Fifteenth Conference on Computational Natural
Language Learning: Shared Task, pages 1?27.
Long Qiu, Min-Yen Kan, and Tat-Seng Chua. 2004. A
public reference implementation of the RAP anaphora
resolution algorithm. In Proceedings of the 4th In-
ternational Conference on Language Resources and
Evaluation, pages 291?294.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nate Chambers, Mihai Surdeanu, Dan Juraf-
sky, and Christopher Manning. 2010. A multi-pass
sieve for coreference resolution. In Proceedings of
the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 492?501.
Altaf Rahman and Vincent Ng. 2009. Supervised mod-
els for coreference resolution. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 968?977.
Altaf Rahman and Vincent Ng. 2011. Coreference res-
olution with world knowledge. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 814?824.
Roger C. Schank and Robert P. Abelson. 1977. Scripts,
Plans, Goals, and Understanding. Lawrence Erl-
baum.
Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen
Riloff, David Buttler, and David Hysom. 2010. REC-
ONCILE: A coreference resolution research platform.
In Proceedings of the ACL 2010 Conference Short Pa-
pers, pages 156?161.
Michael Strube and Christoph Mu?ller. 2003. A machine
learning approach to pronoun resolution in spoken di-
alogue. In Proceedings of the 41st Annual Meeting of
the Association for Computational Linguistics, pages
168?175.
Alan M. Turing. 1950. Computing machinery and intel-
ligence. Mind, 59:433?460.
Olga Uryupina, Massimo Poesio, Claudio Giuliano, and
Kateryna Tymoshenko. 2011. Disambiguation and
filtering methods in using Web knowledge for coref-
erence resolution. In Proceedings of the 24th Interna-
tional Florida Artificial Intelligence Research Society
Conference, pages 317?322.
Yannick Versley, Simone Paolo Ponzetto, Massimo Poe-
sio, Vladimir Eidelman, Alan Jern, Jason Smith,
Xiaofeng Yang, and Alessandro Moschitti. 2008.
BART: A modular toolkit for coreference resolution.
In Proceedings of the ACL-08: HLT Demo Session,
pages 9?12.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi,
Claire Cardie, Ellen Riloff, and Siddharth Patwardhan.
2005a. Opinionfinder: A system for subjectivity anal-
ysis. In Proceedings of HLT/EMNLP 2005 Interactive
Demonstrations, pages 34?35.
Theresa Wilson, Janyce M. Wiebe, and Paul Hoffmann.
2005b. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Joint
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 347?354.
Terry Winograd. 1972. Understanding Natural Lan-
guage. Academic Press, Inc., New York.
Xiaofeng Yang, Guodong Zhou, Jian Su, and Chew Lim
Tan. 2003. Coreference resolution using competitive
learning approach. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguis-
tics, pages 176?183.
Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2005. Im-
proving pronoun resolution using statistics-based se-
mantic compatibility information. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics, pages 165?172.
789
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 798?807,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Learning the Fine-Grained Information Status of Discourse Entities
Altaf Rahman and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{altaf,vince}@hlt.utdallas.edu
Abstract
While information status (IS) plays a cru-
cial role in discourse processing, there have
only been a handful of attempts to automat-
ically determine the IS of discourse entities.
We examine a related but more challenging
task, fine-grained IS determination, which
involves classifying a discourse entity as
one of 16 IS subtypes. We investigate the
use of rich knowledge sources for this task
in combination with a rule-based approach
and a learning-based approach. In experi-
ments with a set of Switchboard dialogues,
the learning-based approach achieves an ac-
curacy of 78.7%, outperforming the rule-
based approach by 21.3%.
1 Introduction
A linguistic notion central to discourse processing
is information status (IS). It describes the extent
to which a discourse entity, which is typically re-
ferred to by noun phrases (NPs) in a dialogue, is
available to the hearer. Different definitions of IS
have been proposed over the years. In this paper,
we adopt Nissim et als (2004) proposal, since it
is primarily built upon Prince?s (1992) and Eck-
ert and Strube?s (2001) well-known definitions,
and is empirically shown by Nissim et alto yield
an annotation scheme for IS in dialogue that has
good reproducibility.1
Specifically, Nissim et al(2004) adopt a three-
way classification scheme for IS, defining a dis-
course entity as (1) old to the hearer if it is known
to the hearer and has previously been referred to in
the dialogue; (2) new if it is unknown to her and
1It is worth noting that several IS annotation schemes
have been proposed more recently. See Go?tze et al(2007)
and Riester et al(2010) for details.
has not been previously referred to; and (3) me-
diated (henceforth med) if it is newly mentioned
in the dialogue but she can infer its identity from
a previously-mentioned entity. To capture finer-
grained distinctions for IS, Nissim et alallow an
old or med entity to have a subtype, which subcat-
egorizes an old or med entity. For instance, a med
entity has the subtype set if the NP that refers to
it is in a set-subset relation with its antecedent.
IS plays a crucial role in discourse processing:
it provides an indication of how a discourse model
should be updated as a dialogue is processed in-
crementally. Its importance can be reflected in
part in the amount of attention it has received in
theoretical linguistics over the years (e.g., Halli-
day (1976), Prince (1981), Hajic?ova? (1984), Vall-
duv?? (1992), Steedman (2000)), and in part in the
benefits it can potentially bring to NLP applica-
tions. One task that could benefit from knowledge
of IS is identity coreference: since new entities by
definition have not been previously referred to, an
NP marked as new does not need to be resolved,
thereby improving the precision of a coreference
resolver. Knowledge of fine-grained or subcat-
egorized IS is valuable for other NLP tasks. For
instance, an NP marked as set signifies that it is in
a set-subset relation with its antecedent, thereby
providing important clues for bridging anaphora
resolution (e.g., Gasperin and Briscoe (2008)).
Despite the potential usefulness of IS in NLP
tasks, there has been little work on learning
the IS of discourse entities. To investigate the
plausibility of learning IS, Nissim et al(2004)
annotate a set of Switchboard dialogues with
such information2 , and subsequently present a
2These and other linguistic annotations on the Switch-
board dialogues were later released by the LDC as part of the
NXT corpus, which is described in Calhoun et al(2010).
798
rule-based approach and a learning-based ap-
proach to acquiring such knowledge (Nissim,
2006). More recently, we have improved Nissim?s
learning-based approach by augmenting her fea-
ture set, which comprises seven string-matching
and grammatical features, with lexical and syn-
tactic features (Rahman and Ng, 2011; hence-
forth R&N). Despite the improvements, the per-
formance on new entities remains poor: an F-
score of 46.5% was achieved.
Our goal in this paper is to investigate fine-
grained IS determination, the task of classifying
a discourse entity as one of the 16 IS subtypes
defined by Nissim et al(2004).3 Owing in part
to the increase in the number of categories, fine-
grained IS determination is arguably a more chal-
lenging task than the 3-class IS determination task
that Nissim and R&N investigated. To our knowl-
edge, this is the first empirical investigation of au-
tomated fine-grained IS determination.
We propose a knowledge-rich approach to fine-
grained IS determination. Our proposal is moti-
vated in part by Nissim?s and R&N?s poor per-
formance on new entities, which we hypothesize
can be attributed to their sole reliance on shallow
knowledge sources. In light of this hypothesis,
our approach employs semantic and world knowl-
edge extracted from manually and automatically
constructed knowledge bases, as well as corefer-
ence information. The relevance of coreference to
IS determination can be seen from the definition
of IS: a new entity is not coreferential with any
previously-mentioned entity, whereas an old en-
tity may. While our use of coreference informa-
tion for IS determination and our earlier claim that
IS annotation would be useful for coreference res-
olution may seem to have created a chicken-and-
egg problem, they do not: since coreference reso-
lution and IS determination can benefit from each
other, it may be possible to formulate an approach
where the two tasks can mutually bootstrap.
We investigate rule-based and learning-based
approaches to fine-grained IS determination. In
the rule-based approach, we manually compose
rules to combine the aforementioned knowledge
sources. While we could employ the same knowl-
edge sources in the learning-based approach, we
chose to encode, among other knowledge sources,
3One of these 16 classes is the new type, for which no
subtype is defined. For ease of exposition, we will refer to
the new type as one of the 16 subtypes to be predicted.
the hand-written rules and their predictions di-
rectly as features for the learner. In an evalua-
tion on 147 Switchboard dialogues, our learning-
based approach to fine-grained IS determina-
tion achieves an accuracy of 78.7%, substan-
tially outperforming the rule-based approach by
21.3%. Equally importantly, when employing
these linguistically rich features to learn Nissim?s
3-class IS determination task, the resulting classi-
fier achieves an accuracy of 91.7%, surpassing the
classifier trained on R&N?s state-of-the-art fea-
ture set by 8.8% in absolute accuracy. Improve-
ments on the new class are particularly substan-
tial: its F-score rises from 46.7% to 87.2%.
2 IS Types and Subtypes: An Overview
In Nissim et als (2004) IS classification scheme,
an NP can be assigned one of three main types
(old, med, new) and one of 16 subtypes. Below
we will illustrate their definitions with examples,
most of which are taken from Nissim (2003) or
Nissim et als (2004) dataset (see Section 3).
Old. An NP is marked is old if (i) it is corefer-
ential with an entity introduced earlier, (ii) it is a
generic pronoun, or (iii) it is a personal pronoun
referring to the dialogue participants. Six sub-
types are defined for old entities: identity, event,
general, generic, ident generic, and relative. In
Example 1, my is marked as old with subtype
identity, since it is coreferent with I.
(1) I was angry that he destroyed my tent.
However, if the markable has a verb phrase (VP)
rather than an NP as its antecedent, it will be
marked as old/event, as can be seen in Example
2, where the antecedent of That is the VP put my
phone number on the form.
(2) They ask me to put my phone number
on the form. That I think is not needed.
Other NPs marked as old include (i) relative
pronouns, which have the subtype relative; (ii)
personal pronouns referring to the dialogue par-
ticipants, which have the subtype general, and
(iii) generic pronouns, which have the subtype
generic. The pronoun you in Example 3 is an in-
stance of a generic pronoun.
(3) I think to correct the judicial system,
you have to get the lawyer out of it.
Note, however, that in a coreference chain of
generic pronouns, every element of the chain is
799
assigned the subtype ident generic instead.
Mediated. An NP is marked as med if the en-
tity it refers to has not been previously introduced
in the dialogue, but can be inferred from already-
mentioned entities or is generally known to the
hearer. Nine subtypes are available for med en-
tities: general, bound, part, situation, event, set,
poss, func value, and aggregation.
General is assigned to med entities that are
generally known, such as the Earth, China, and
most proper names. Bound is reserved for bound
pronouns, an instance of which is shown in Ex-
ample 4, where its is bound to the variable of the
universally quantified NP, Every cat.
(4) Every cat ate its dinner.
Poss is assigned to NPs involved in intra-phrasal
possessive relations, including prenominal geni-
tives (i.e., X?s Y) and postnominal genitives (i.e.,
Y of X). Specifically, Y will be marked as poss if
X is old or med; otherwise, Y will be new. For ex-
ample, in cases like a friend?s boat where a friend
is new, boat is marked as new.
Four subtypes, namely part, situation, event,
and set, are used to identify instances of bridg-
ing (i.e., entities that are inferrable from a related
entity mentioned earlier in the dialogue). As an
example, consider the following sentences:
(5a) He passed by the door of Jan?s house
and saw that the door was painted red.
(5b) He passed by Jan?s house and saw that
the door was painted red.
In Example 5a, by the time the hearer processes
the second occurrence of the door, she has already
had a mental entity corresponding to the door (af-
ter processing the first occurrence). As a result,
the second occurrence of the door refers to an
old entity. In Example 5b, on the other hand, the
hearer is not assumed to have any mental repre-
sentation of the door in question, but she can in-
fer that the door she saw was part of Jan?s house.
Hence, this occurrence of the door should be
marked as med with subtype part, as it is involved
in a part-whole relation with its antecedent.
If an NP is involved in a set-subset relation with
its antecedent, it inherits the med subtype set.
This applies to the NP the house payment in Ex-
ample 6, whose antecedent is our monthly budget.
(6) What we try to do to stick to our
monthly budget is we pretty much have
the house payment.
If an NP is part of a situation set up by a
previously-mentioned entity, it is assigned the
subtype situation, as exemplified by the NP a few
horses in the sentence below, which is involved in
the situation set up by John?s ranch.
(7) Mary went to John?s ranch and saw that
there were only a few horses.
Similar to old entities, an NP marked as med may
be related to a previously mentioned VP. In this
case, the NP will receive the subtype event, as ex-
emplified by the NP the bus in the sentence below,
which is triggered by the VP traveling in Miami.
(8) We were traveling in Miami, and the
bus was very full.
If an NP refers to a value of a previously men-
tioned function, such as the NP 30 degrees in Ex-
ample 9, which is related to the temperature, then
it is assigned the subtype func value.
(9) The temperature rose to 30 degrees.
Finally, the subtype aggregation is assigned to co-
ordinated NPs if at least one of the NPs involved
is not new. However, if all NPs in the coordinated
phrase are new, the phrase should be marked as
new. For instance, the NP My son and I in Exam-
ple 10 should be marked as med/aggregation.
(10) I have a son ... My son and I like to
play chess after dinner.
New. An entity is new if it has not been intro-
duced in the dialogue and the hearer cannot infer
it from previously mentioned entities. No subtype
is defined for new entities.
There are cases where more than one IS value
is appropriate for a given NP. For instance, given
two occurrences of China in a dialogue, the sec-
ond occurrence can be labeled as old/identity (be-
cause it is coreferential with an earlier NP) or
med/general (because it is a generally known
entity). To break ties, Nissim (2003) define a
precedence relation on the IS subtypes, which
yields a total ordering on the subtypes. Since
all the old subtypes are ordered before their med
counterparts in this relation, the second occur-
rence of China in our example will be labeled as
old/identity. Owing to space limitations, we refer
the reader to Nissim (2003) for details.
3 Dataset
We employ Nissim et als (2004) dataset, which
comprises 147 Switchboard dialogues. We parti-
800
tion them into a training set (117 dialogues) and a
test set (30 dialogues). A total of 58,835 NPs are
annotated with IS types and subtypes.4 The distri-
butions of NPs over the IS subtypes in the training
set and the test set are shown in Table 1.
Train (%) Test (%)
old/identity 10236 (20.1) 1258 (15.8)
old/event 1943 (3.8) 290 (3.6)
old/general 8216 (16.2) 1129 (14.2)
old/generic 2432 (4.8) 427 (5.4)
old/ident generic 1730 (3.4) 404 (5.1)
old/relative 1241 (2.4) 193 (2.4)
med/general 2640 (5.2) 325 (4.1)
med/bound 529 (1.0) 74 (0.9)
med/part 885 (1.7) 120 (1.5)
med/situation 1109 (2.2) 244 (3.1)
med/event 351 (0.7) 67 (0.8)
med/set 10282 (20.2) 1771 (22.3)
med/poss 1318 (2.6) 220 (2.8)
med/func value 224 (0.4) 31 (0.4)
med/aggregation 580 (1.1) 117 (1.5)
new 7158 (14.1) 1293 (16.2)
total 50874 (100) 7961 (100)
Table 1: Distributions of NPs over IS subtypes. The
corresponding percentages are parenthesized.
4 Rule-Based Approach
In this section, we describe our rule-based ap-
proach to fine-grained IS determination, where we
manually design rules for assigning IS subtypes to
NPs based on the subtype definitions in Section 2,
Nissim?s (2003) IS annotation guidelines, and our
inspection of the IS annotations in the training
set. The motivations behind having a rule-based
approach are two-fold. First, it can serve as a
baseline for fine-grained IS determination. Sec-
ond, it can provide insight into how the available
knowledge sources can be combined into predic-
tion rules, which can potentially serve as ?sophis-
ticated? features for a learning-based approach.
As shown in Table 2, our ruleset is composed of
18 rules, which should be applied to an NP in the
order in which they are listed. Rules 1?7 handle
the assignment of old subtypes to NPs. For in-
stance, Rule 1 identifies instances of old/general,
which comprises the personal pronouns referring
4Not all NPs have an IS type/subtype. For instance, a
pleonastic ?it? does not refer to any real-world entity and
therefore does not have any IS, and so are nouns such as
?course? in ?of course?, ?accident? in ?by accident?, etc.
to the dialogue participants. Note that this and
several other rules rely on coreference informa-
tion, which we obtain from two sources: (1)
chains generated automatically using the Stan-
ford Deterministic Coreference Resolution Sys-
tem (Lee et al 2011)5, and (2) manually iden-
tified coreference chains taken directly from the
annotated Switchboard dialogues. Reporting re-
sults using these two ways of obtaining chains fa-
cilitates the comparison of the IS determination
results that we can realistically obtain using ex-
isting coreference technologies against those that
we could obtain if we further improved exist-
ing coreference resolvers. Note that both sources
provide identity coreference chains. Specifically,
the gold chains were annotated for NPs belong-
ing to old/identity and old/ident generic. Hence,
these chains can be used to distinguish between
old/general NPs and old/ident generic NPs, be-
cause the former are not part of a chain whereas
the latter are. However, they cannot be used
to distinguish between old/general entities and
old/generic entities, since neither of them belongs
to any chains. As a result, when gold chains are
used, Rule 1 will classify all occurrences of ?you?
that are not part of a chain as old/general, regard-
less of whether the pronoun is generic. While the
gold chains alone can distinguish old/general and
old/ident generic NPs, the Stanford chains can-
not distinguish any of the old subtypes in the ab-
sence of other knowledge sources, since it gener-
ates chains for all old NPs regardless of their sub-
types. This implies that Rule 1 and several other
rules are only a very crude approximation of the
definition of the corresponding IS subtypes.
The rules for the remaining old subtypes can be
interpreted similarly. A few points deserve men-
tion. First, many rules depend on the string of
the NP under consideration (e.g., ?they? in Rule 2
and ?whatever? in Rule 4). The decision of which
strings are chosen is based primarily on our in-
spection of the training data. Hence, these rules
are partly data-driven. Second, these rules should
be applied in the order in which they are shown.
For instance, though not explicitly stated, Rule 3
is only applicable to the non-anaphoric ?you? and
?they? pronouns, since Rule 2 has already covered
their anaphoric counterparts. Finally, Rule 7 uses
non-anaphoricity as a test of old/event NPs. The
5The Stanford resolver is available from http://nlp.
stanford.edu/software/corenlp.shtml.
801
1. if the NP is ?I? or ?you? and it is not part of a coreference chain, then
subtype := old/general
2. if the NP is ?you? or ?they? and it is anaphoric, then
subtype := old/ident generic
3. if the NP is ?you? or ?they?, then
subtype := old/generic
4. if the NP is ?whatever? or an indefinite pronoun prefixed by ?some? or ?any? (e.g., ?somebody?), then
subtype := old/generic
5. if the NP is an anaphoric pronoun other than ?that?, or its string is identical to that of a preceding NP, then
subtype := old/ident
6. if the NP is ?that? and it is coreferential with the immediately preceding word, then
subtype := old/relative
7. if the NP is ?it?, ?this? or ?that?, and it is not anaphoric, then
subtype := old/event
8. if the NP is pronominal and is not anaphoric, then
subtype := med/bound
9. if the NP contains ?and? or ?or?, then
subtype := med/aggregation
10. if the NP is a multi-word phrase that (1) begins with ?so much?, ?something?, ?somebody?, ?someone?,
?anything?, ?one?, or ?different?, or (2) has ?another?, ?anyone?, ?other?, ?such?, ?that?, ?of? or ?type?
as neither its first nor last word, or (3) its head noun is also the head noun of a preceding NP, then
subtype := med/set
11. if the NP contains a word that is a hyponym of the word ?value? in WordNet, then
subtype := med/func value
12. if the NP is involved in a part-whole relation with a preceding NP based on information extracted from
ReVerb?s output, then
subtype := med/part
13. if the NP is of the form ?X?s Y? or ?poss-pro Y?, where X and Y are NPs and poss-pro is a possessive
pronoun, then
subtype := med/poss
14. if the NP fills an argument of a FrameNet frame set up by a preceding NP or verb, then
subtype := med/situation
15. if the head of the NP and one of the preceding verbs in the same sentence share the same WordNet
hypernym which is not in synsets that appear one of the top five levels of the noun/verb hierarchy, then
subtype := med/event
16. if the NP is a named entity (NE) or starts with ?the?, then
subtype := med/general
17. if the NP appears in the training set, then
subtype := its most frequent IS subtype in the training set
18. subtype := new
Table 2: Hand-crafted rules for assigning IS subtypes to NPs.
reason is that these NPs have VP antecedents, but
both the gold chains and the Stanford chains are
computed over NPs only.
Rules 8?16 concern med subtypes. Apart from
Rule 8 (med/bound), Rule 9 (med/aggregation),
and Rule 11 (med/func value), which are arguably
crude approximations of the definitions of the
corresponding subtypes, the med rules are more
complicated than their old counterparts, in part
because of their reliance on the extraction of so-
phisticated knowledge. Below we describe the ex-
traction process and the motivation behind them.
Rule 10 concerns med/set. The words and
phrases listed in the rule, which are derived manu-
ally from the training data, provide suggestive ev-
idence that the NP under consideration is a subset
or a specific portion of an entity or concept men-
tioned earlier in the dialogue. Examples include
?another bedroom?, ?different color?, ?somebody
else?, ?any place?, ?one of them?, and ?most other
cities?. Condition 3 of the rule, which checks
whether the head noun of the NP has been men-
tioned previously, is a good test for identity coref-
erence, but since all the old entities have suppos-
802
edly been identified by the preceding rules, it be-
comes a reasonable test for set-subset relations.
For convenience, we identify part-whole rela-
tions in Rule 12 based on the output produced by
ReVerb (Fader et al 2011), an open information
extraction system.6 The output contains, among
other things, relation instances, each of which is
represented as a triple, <A,rel,B>, where rel is
a relation, and A and B are its arguments. To pre-
process the output, we first identify all the triples
that are instances of the part-whole relation us-
ing regular expressions. Next, we create clusters
of relation arguments, such that each pair of ar-
guments in a cluster has a part-whole relation.
This is easy: since part-whole is a transitive rela-
tion (i.e., <A,part,B> and <B,part,C> implies
<A,part,C>), we cluster the arguments by taking
the transitive closure of these relation instances.
Then, given an NP NPi in the test set, we assign
med/part to it if there is a preceding NP NPj such
that the two NPs are in the same argument cluster.
In Rule 14, we use FrameNet (Baker et al
1998) to determine whether med/situation should
be assigned to an NP, NPi. Specifically, we check
whether it fills an argument of a frame set up by
a preceding NP, NPj , or verb. To exemplify, let
us assume that NPj is ?capital punishment?. We
search for ?punishment? in FrameNet to access
the appropriate frame, which in this case is ?re-
wards and punishments?. This frame contains a
list of arguments together with examples. If NPi is
one of these arguments, we assign med/situation
to NPi, since it is involved in a situation (described
by a frame) that is set up by a preceding NP/verb.
In Rule 15, we use WordNet (Fellbaum, 1998)
to determine whether med/event should be as-
signed to an NP, NPi, by checking whether NPi is
related to an event, which is typically described
by a verb. Specifically, we use WordNet to check
whether there exists a verb, v, preceding NPi such
that v and NPi have the same hypernym. If so, we
assign NPi the subtype med/event. Note that we
ensure that the hypernym they share does not ap-
pear in the top five levels of the WordNet noun
and verb hierarchies, since we want them to be
related via a concept that is not overly general.
Rule 16 identifies instances of med/general.
The majority of its members are generally-known
6We use ReVerb ClueWeb09 Extractions 1.1, which
is available from http://reverb.cs.washington.
edu/reverb_clueweb_tuples-1.1.txt.gz.
entities, whose identification is difficult as it re-
quires world knowledge. Consequently, we apply
this rule only after all other med rules are applied.
As we can see, the rule assigns med/general to
NPs that are named entities (NEs) and definite de-
scriptions (specifically those NPs that start with
?the?). The reason is simple. Most NEs are gener-
ally known. Definite descriptions are typically not
new, so it seems reasonable to assign med/general
to them given that the remaining (i.e., unlabeled)
NPs are presumably either new and med/general.
Before Rule 18, which assigns an NP to the new
class by default, we have a ?memorization? rule
that checks whether the NP under consideration
appears in the training set (Rule 17). If so, we
assign to it its most frequent subtype based on its
occurrences in the training set. In essence, this
heuristic rule can help classify some of the NPs
that are somehow ?missed? by the first 16 rules.
The ordering of these rules has a direct impact
on performance of the ruleset, so a natural ques-
tion is: what criteria did we use to order the rules?
We order them in such a way that they respect the
total ordering on the subtypes imposed by Nis-
sim?s (2003) preference relation (see Section 3),
except that we give med/general a lower priority
than Nissim due to the difficulty involved in iden-
tifying generally known entities, as noted above.
5 Learning-Based Approach
In this section, we describe our learning-based ap-
proach to fine-grained IS determination. Since
we aim to automatically label an NP with its IS
subtype, we create one training/test instance from
each hand-annotated NP in the training/test set.
Each instance is represented using five types of
features, as described below.
Unigrams (119704). We create one binary fea-
ture for each unigram appearing in the training
set. Its value indicates the presence or absence
of the unigram in the NP under consideration.
Markables (209751). We create one binary fea-
ture for each markable (i.e., an NP having an IS
subtype) appearing in the training set. Its value is
1 if and only if the markable has the same string
as the NP under consideration.
Markable predictions (17). We create 17 bi-
nary features, 16 of which correspond to the 16
IS subtypes and the remaining one corresponds to
a ?dummy subtype?. Specifically, if the NP un-
803
der consideration appears in the training set, we
use Rule 17 in our hand-crafted ruleset to deter-
mine the IS subtype it is most frequently associ-
ated with in the training set, and then set the value
of the feature corresponding to this IS subtype to
1. If the NP does not appear in the training set, we
set the value of the dummy subtype feature to 1.
Rule conditions (17). As mentioned before, we
can create features based on the hand-crafted rules
in Section 4. To describe these features, let us in-
troduce some notation. Let Rule i be denoted by
Ai ?? Bi, where Ai is the condition that must
be satisfied before the rule can be applied and Bi
is the IS subtype predicted by the rule. We could
create one binary feature from each Ai, and set its
value to 1 if Ai is satisfied by the NP under con-
sideration. These features, however, fail to cap-
ture a crucial aspect of the ruleset: the ordering of
the rules. For instance, Rule i should be applied
only if the conditions of the first i?1 rules are not
satisfied by the NP, but such ordering is not en-
coded in these features. To address this problem,
we capture rule ordering information by defining
binary feature fi as ?A1??A2? . . .?Ai?1?Ai,
where 1 ? i ? 16. In addition, we define a fea-
ture, f18, for the default rule (Rule 18) in a simi-
lar fashion, but since it does not have any condi-
tion, we simply define f18 as ?A1 ? . . . ? ?A16.
The value of a feature in this feature group is 1
if and only if the NP under consideration satis-
fies the condition defined by the feature. Note that
we did not create any features from Rule 17 here,
since we have already generated ?markables? and
?markable prediction? features for it.
Rule predictions (17). None of the features fi?s
defined above makes use of the predictions of our
hand-crafted rules (i.e., the Bi?s). To make use
of these predictions, we define 17 binary features,
one for each Bi, where i = 1, . . . , 16, 18. Specif-
ically, the value of the feature corresponding to
Bi is 1 if and only if fi is 1, where fi is a ?rule
condition? feature as defined above.
Since IS subtype determination is a 16-class
classification problem, we train a multi-class
SVM classifier on the training instances using
SVMmulticlass (Tsochantaridis et al 2004), and
use it to make predictions on the test instances.7
7For all the experiments involving SVMmulticlass, we
set C, the regularization parameter, to 500,000, since pre-
liminary experiments indicate that preferring generalization
6 Evaluation
Next, we evaluate the rule-based approach and
the learning-based approach to determining the IS
subtype of each hand-annotated NP in the test set.
Classification results. Table 3 shows the results
of the two approaches. Specifically, row 1 shows
their accuracy, which is defined as the percent-
age of correctly classified instances. For each
approach, we present results that are generated
based on gold coreference chains as well as auto-
matic chains computed by the Stanford resolver.
As we can see, the rule-based approach
achieves accuracies of 66.0% (gold coreference)
and 57.4% (Stanford coreference), whereas the
learning-based approach achieves accuracies of
86.4% (gold) and 78.7% (Stanford). In other
words, the gold coreference results are better than
the Stanford coreference results, and the learning-
based results are better than the rule-based results.
While perhaps neither of these results are surpris-
ing, we are pleasantly surprised by the extent to
which the learned classifier outperforms the hand-
crafted rules: accuracies increase by 20.4% and
21.3% when gold coreference and Stanford coref-
erence are used, respectively. In other words, ma-
chine learning has ?transformed? a ruleset that
achieves mediocre performance into a system that
achieves relatively high performance.
These results also suggest that coreference
plays a crucial role in IS subtype determination:
accuracies could increase by up to 7.7?8.6% if
we solely improved coreference resolution perfor-
mance. This is perhaps not surprising: IS and
coreference can mutually benefit from each other.
To gain additional insight into the task, we also
show in rows 2?17 of Table 3 the performance
on each of the 16 subtypes, expressed in terms of
recall (R), precision (P), and F-score (F). A few
points deserve mention. First, in comparison to
the rule-based approach, the learning-based ap-
proach achieves considerably better performance
on almost all classes. One that is of particular in-
terest is the new class. As we can see in row 17,
its F-score rises by about 30 points. These gains
are accompanied by a simultaneous rise in recall
and precision. In particular, recall increases by
about 40 points. Now, recall from the introduc-
to overfitting (by setting C to a small value) tends to yield
poorer classification performance. The remaining learning
parameters are set to their default values.
804
Rule-Based Approach Learning-Based Approach
Gold Coreference Stanford Coreference Gold Coreference Stanford Coreference
1 Accuracy 66.0 57.4 86.4 78.7
IS Subtype R P F R P F R P F R P F
2 old/ident 77.5 78.2 77.8 66.1 52.7 58.7 82.8 85.2 84.0 75.8 64.2 69.5
3 old/event 98.6 50.4 66.7 71.3 43.2 53.8 98.3 87.9 92.8 2.4 31.8 4.5
4 old/general 81.9 82.7 82.3 72.3 83.6 77.6 97.7 93.7 95.6 87.8 92.7 90.2
5 old/generic 55.9 55.2 55.5 39.2 39.8 39.5 76.1 87.3 81.3 39.9 85.9 54.5
6 old/ident generic 48.7 77.7 59.9 27.2 51.8 35.7 57.1 87.5 69.1 47.2 44.8 46.0
7 old/relative 55.0 69.2 61.3 55.1 63.4 59.0 98.0 63.0 76.7 99.0 37.5 54.4
8 med/general 29.9 19.8 23.8 29.5 19.6 23.6 91.2 87.7 89.4 84.0 72.2 77.7
9 med/bound 56.4 20.5 30.1 56.4 20.5 30.1 25.7 65.5 36.9 2.7 40.0 5.1
10 med/part 19.5 100.0 32.7 19.5 100.0 32.7 73.2 96.8 83.3 73.2 96.8 83.3
11 med/situation 28.7 100.0 44.6 28.7 100.0 44.6 68.4 95.4 79.7 68.0 97.7 80.2
12 med/event 10.5 100.0 18.9 10.5 100.0 18.9 46.3 100.0 63.3 46.3 100.0 63.3
13 med/set 82.9 61.8 70.8 78.0 59.4 67.4 90.4 87.8 89.1 88.4 86.0 87.2
14 med/poss 52.9 86.0 65.6 52.9 86.0 65.6 93.2 92.4 92.8 90.5 97.6 93.9
15 med/func value 81.3 74.3 77.6 81.3 74.3 77.6 88.1 85.9 87.0 88.1 85.9 87.0
16 med/aggregation 57.4 44.0 49.9 57.4 43.6 49.6 85.2 72.9 78.6 83.8 93.9 88.6
17 new 50.4 65.7 57.0 50.3 65.1 56.7 90.3 84.6 87.4 90.4 83.6 86.9
Table 3: IS subtype accuracies and F-scores. In each row, the strongest result, as well as those that are statistically
indistinguishable from it according to the paired t-test (p < 0.05), are boldfaced.
tion that previous attempts on 3-class IS determi-
nation by Nissim and R&N have achieved poor
performance on the new class. We hypothesize
that the use of shallow features in their approaches
were responsible for the poor performance they
observed, and that using our knowledge-rich fea-
ture set could improve its performance. We will
test this hypothesis at the end of this section.
Other subtypes that are worth discussing
are med/aggregation, med/func value, and
med/poss. Recall that the rules we designed for
these classes were only crude approximations, or,
perhaps more precisely, simplified versions of the
definitions of the corresponding subtypes. For
instance, to determine whether an NP belongs to
med/aggregation, we simply look for occurrences
of ?and? and ?or? (Rule 9), whereas its definition
requires that not all of the NPs in the coordinated
phrase are new. Despite the over-simplicity
of these rules, machine learning has enabled
the available features to be combined in such a
way that high performance is achieved for these
classes (see rows 14?16).
Also worth examining are those classes for
which the hand-crafted rules rely on sophisti-
cated knowledge sources. They include med/part,
which relies on ReVerb; med/situation, which re-
lies on FrameNet; and med/event, which relies on
WordNet. As we can see from the rule-based re-
sults (rows 10?12), these knowledge sources have
yielded rules that achieved perfect precision but
low recall: 19.5% for part, 28.7% for situation,
and 10.5 for event. Nevertheless, the learning
algorithm has again discovered a profitable way
to combine the available features, enabling the F-
scores of these classes to increase by 35.1?50.6%.
While most classes are improved by machine
learning, the same is not true for old/event and
med/bound, whose F-scores are 4.5% (row 3) and
5.1% (row 9), respectively, when Stanford coref-
erence is employed. This is perhaps not surpris-
ing. Recall that the multi-class SVM classifier
was trained to maximize classification accuracy.
Hence, if it encounters a class that is both difficult
to learn and is under-represented, it may as well
aim to achieve good performance on the easier-
to-learn, well-represented classes at the expense
of these hard-to-learn, under-represented classes.
Feature analysis. In an attempt to gain addi-
tional insight into the performance contribution
of each of the five types of features used in the
learning-based approach, we conduct feature ab-
lation experiments. Results are shown in Table 4,
where each row shows the accuracy of the classi-
fier trained on all types of features except for the
one shown in that row. For easy reference, the
accuracy of the classifier trained on all types of
features is shown in row 1 of the table. According
to the paired t-test (p < 0.05), performance drops
significantly whichever feature type is removed.
This suggests that all five feature types are con-
tributing positively to overall accuracy. Also, the
markables features are the least important in the
presence of other feature groups, whereas mark-
805
Feature Type Gold Coref Stanford Coref
All features 86.4 78.7
?rule predictions 77.5 70.0
?markable predictions 72.4 64.7
?rule conditions 81.1 71.0
?unigrams 74.4 58.6
?markables 83.2 75.5
Table 4: Accuracies of feature ablation experiments.
Feature Type Gold Coref Stanford Coref
rule predictions 49.1 45.2
markable predictions 39.7 39.7
rule conditions 58.1 28.9
unigrams 56.8 56.8
markables 10.4 10.4
Table 5: Accuracies of classifiers for each feature type.
able predictions and unigrams are the two most
important feature groups.
To get a better idea of the utility of each feature
type, we conduct another experiment in which we
train five classifiers, each of which employs ex-
actly one type of features. The accuracies of these
classifiers are shown in Table 5. As we can see,
the markables features have the smallest contribu-
tion, whereas unigrams have the largest contribu-
tion. Somewhat interesting are the results of the
classifiers trained on the rule conditions: the rules
are far more effective when gold coreference is
used. This can be attributed to the fact that the
design of the rules was based in part on the defini-
tions of the subtypes, which assume the availabil-
ity of perfect coreference information.
Knowledge source analysis. To gain some in-
sight into the extent to which a knowledge source
or a rule contributes to the overall performance of
the rule-based approach, we conduct ablation ex-
periments: in each experiment, we measure the
performance of the ruleset after removing a par-
ticular rule or knowledge source from it. Specifi-
cally, rows 2?4 of Table 6 show the accuracies of
the ruleset after removing the memorization rule
(Rule 17), the rule that uses ReVerb?s output (Rule
12), and the cue words used in Rules 4 and 10,
respectively. For easy reference, the accuracy of
the original ruleset is shown in row 1 of the ta-
ble. According to the paired t-test (p < 0.05),
performance drops significantly in all three abla-
tion experiments. This suggests that the memo-
rization rule, ReVerb, and the cue words all con-
tribute positively to the accuracy of the ruleset.
Feature Type Gold Coref Stanford Coref
All rules 66.0 57.4
?memorization 62.6 52.0
?ReVerb 64.2 56.6
?cue words 63.8 54.0
Table 6: Accuracies of the simplified ruleset.
R&N?s Features Our Features
IS Type R P F R P F
old 93.5 95.8 94.6 93.8 96.4 95.1
med 89.3 71.2 79.2 93.3 86.0 89.5
new 34.6 71.7 46.7 82.4 72.7 87.2
Accuracy 82.9 91.7
Table 7: Accuracies on IS types.
IS type results. We hypothesized earlier that
the poor performance reported by Nissim and
R&N on identifying new entities in their 3-class
IS classification experiments (i.e., classifying an
NP as old, med, or new) could be attributed to
their sole reliance on lexico-syntactic features. To
test this hypothesis, we (1) train a 3-class classi-
fier using the five types of features we employed
in our learning-based approach, computing the
features based on the Stanford coreference chains;
and (2) compare its results against those obtained
via the lexico-syntactic approach in R&N on our
test set. Results of these experiments, which are
shown in Table 7, substantiate our hypothesis:
when we replace R&N?s features with ours, accu-
racy rises from 82.9% to 91.7%. These gains can
be attributed to large improvements in identifying
new and med entities, for which F-scores increase
by about 40 points and 10 points, respectively.
7 Conclusions
We have examined the fine-grained IS determi-
nation task. Experiments on a set of Switch-
board dialogues show that our learning-based ap-
proach, which uses features that include hand-
crafted rules and their predictions, outperforms its
rule-based counterpart by more than 20%, achiev-
ing an overall accuracy of 78.7% when relying on
automatically computed coreference information.
In addition, we have achieved state-of-the-art re-
sults on the 3-class IS determination task, in part
due to our reliance on richer knowledge sources
in comparison to prior work. To our knowledge,
there has been little work on automatic IS subtype
determination. We hope that our work can stimu-
late further research on this task.
806
Acknowledgments
We thank the three anonymous reviewers for their
detailed and insightful comments on an earlier
draft of the paper. This work was supported
in part by NSF Grants IIS-0812261 and IIS-
1147644.
References
Collin F. Baker, Charles J. Fillmore, and John B.
Lowe. 1998. The Berkeley FrameNet project.
In Proceedings of the 36th Annual Meeting of the
Association for Computational Linguistics and the
17th International Conference on Computational
Linguistics, Volume 1, pages 86?90.
Sasha Calhoun, Jean Carletta, Jason Brenier, Neil
Mayo, Dan Jurafsky, Mark Steedman, and David
Beaver. 2010. The NXT-format Switchboard cor-
pus: A rich resource for investigating the syntax, se-
mantics, pragmatics and prosody of dialogue. Lan-
guage Resources and Evaluation, 44(4):387?419.
Miriam Eckert and Michael Strube. 2001. Dialogue
acts, synchronising units and anaphora resolution.
Journal of Semantics, 17(1):51?89.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1535?1545.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, MA.
Caroline Gasperin and Ted Briscoe. 2008. Statisti-
cal anaphora resolution in biomedical texts. In Pro-
ceedings of the 22nd International Conference on
Computational Linguistics, pages 257?264.
Michael Go?tze, Thomas Weskott, Cornelia En-
driss, Ines Fiedler, Stefan Hinterwimmer, Svetlana
Petrova, Anne Schwarz, Stavros Skopeteas, and
Ruben Stoel. 2007. Information structure. In
Working Papers of the SFB632, Interdisciplinary
Studies on Information Structure (ISIS). Potsdam:
Universita?tsverlag Potsdam.
Eva Hajic?ova?. 1984. Topic and focus. In Contri-
butions to Functional Syntax, Semantics, and Lan-
guage Comprehension (LLSEE 16), pages 189?202.
John Benjamins, Amsterdam.
Michael A. K. Halliday. 1976. Notes on transitiv-
ity and theme in English. Journal of Linguistics,
3(2):199?244.
Heeyoung Lee, Yves Peirsman, Angel Chang,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2011. Stanford?s multi-pass sieve corefer-
ence resolution system at the CoNLL-2011 shared
task. In Proceedings of the Fifteenth Confer-
ence on Computational Natural Language Learn-
ing: Shared Task, pages 28?34.
Malvina Nissim, Shipra Dingare, Jean Carletta, and
Mark Steedman. 2004. An annotation scheme for
information status in dialogue. In Proceedings of
the 4th International Conference on Language Re-
sources and Evaluation, pages 1023?1026.
Malvina Nissim. 2003. Annotation scheme
for information status in dialogue. Available
from http://www.stanford.edu/class/
cs224u/guidelines-infostatus.pdf.
Malvina Nissim. 2006. Learning information status of
discourse entities. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, pages 94?102.
Ellen F. Prince. 1981. Toward a taxonomy of given-
new information. In P. Cole, editor, Radical Prag-
matics, pages 223?255. New York, N.Y.: Academic
Press.
Ellen F. Prince. 1992. The ZPG letter: Subjects,
definiteness, and information-status. In Discourse
Description: Diverse Analysis of a Fund Raising
Text, pages 295?325. John Benjamins, Philadel-
phia/Amsterdam.
Altaf Rahman and Vincent Ng. 2011. Learning the
information status of noun phrases in spoken dia-
logues. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1069?1080.
Arndt Riester, David Lorenz, and Nina Seemann.
2010. A recursive annotation scheme for referential
information status. In Proceedings of the Seventh
International Conference on Language Resources
and Evaluation, pages 717?722.
Mark Steedman. 2000. The Syntactic Process. The
MIT Press, Cambridge, MA.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vec-
tor machine learning for interdependent and struc-
tured output spaces. In Proceedings of the 21st
International Conference on Machine Learning,
pages 104?112.
Enric Vallduv??. 1992. The Informational Component.
Garland, New York.
807
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 720?730,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Translation-Based Projection for Multilingual Coreference Resolution
Altaf Rahman and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{altaf,vince}@hlt.utdallas.edu
Abstract
To build a coreference resolver for a new
language, the typical approach is to first
coreference-annotate documents from this tar-
get language and then train a resolver on these
annotated documents using supervised learn-
ing techniques. However, the high cost asso-
ciated with manually coreference-annotating
documents needed by a supervised approach
makes it difficult to deploy coreference tech-
nologies across a large number of natural lan-
guages. To alleviate this corpus annotation
bottleneck, we examine a translation-based
projection approach to multilingual corefer-
ence resolution. Experimental results on two
target languages demonstrate the promise of
our approach.
1 Introduction
Noun phrase (NP) coreference resolution is the
task of determining which NPs (or mentions) refer
to each real-world entity in a document. Recent
years have witnessed a surge of interest in multilin-
gual coreference resolution. For instance, the ACE
2004/2005 evaluations and SemEval-2010 Shared
Task 1 have both involved coreference resolution in
multiple languages. As evidenced by the partici-
pants in these evaluations, the most common ap-
proach to building a resolver for a new language
is supervised, which involves training a resolver
on coreference-annotated documents from the tar-
get language. Although supervised approaches work
reasonably well, they present a challenge to deploy-
ing coreference technologies across a large number
of natural languages. Specifically, for each new lan-
guage of interest, one has to hire native speakers of
the language to go through the labor-intensive, time-
consuming process of hand-annotating a potentially
large number of documents with coreference anno-
tation before a supervised resolver can be trained.
One may argue that a potential solution to this
corpus annotation bottleneck is to employ an unsu-
pervised or heuristic approach to coreference resolu-
tion, especially in light of the fact that they have re-
cently started to rival their supervised counterparts.
However, by adopting these approaches, we are sim-
ply replacing the corpus annotation bottleneck by
another, possibly equally serious, bottleneck, the
knowledge acquisition bottleneck. Specifically, in
these approaches, one has to employ knowledge of
the target language to design coreference rules (e.g.,
Mitkov (1999), Poon and Domingos (2008), Raghu-
nathan et al (2010)) or sophisticated generative
models (e.g., Haghighi and Klein (2007,2010), Ng
(2008)) to combine the available knowledge sources.
One could argue that designing coreference
rules and generative models may not be as time-
consuming as annotating a large coreference corpus.
This may be true for a well-studied language like
English, where we can easily compose a rule that
disallows coreference between two mentions if they
disagree in number and gender, for instance. How-
ever, computing these features may not be as simple
as we hope for a language like Chinese: the lack of
morphology complicates the determination of num-
ber information, and the fact that most Chinese first
names are used by both genders makes gender deter-
mination difficult. The difficulty in accurately com-
puting features translates to difficulties in compos-
ing coreference rules: for example, the aforemen-
tioned rule involving gender and number agreement,
as well as rules that implement traditional linguistic
720
constraints on coreference, may no longer be accu-
rate and desirable to have if the features involved
cannot be accurately computed. Consequently, we
believe that research in multilingual coreference res-
olution will continue to be dominated by supervised
approaches.
Given the high cost of annotating data with coref-
erence chains, it is crucial to explore methods for
obtaining annotated data in a cost-effective manner.
Motivated in part by this observation, we examine
one such method that has recently shown promise
for a variety of NLP tasks, translation-based projec-
tion, which is composed of three steps. To coref-
erence annotate a text in the target language, we
(1) machine-translate it to a resource-rich language
(henceforth the source language); (2) automatically
produce the desired linguistic annotations (which in
our case are coreference annotations) on the trans-
lated text using the linguistic tool developed for the
source language (which in our case is a coreference
resolver) ; and (3) project the annotations from the
source language to the target language.
Unlike supervised approaches, this projection ap-
proach does not require any coreference-annotated
data from the target language. Equally importantly,
unlike its unsupervised counterparts, this approach
does not require that we have any linguistic knowl-
edge of the target language. In fact, we have no
knowledge of the target languages we employ in our
evaluation. One of our goals is to examine the fea-
sibility of building a coreference resolver for a lan-
guage for which we have no coreference-annotated
data and no linguistic knowledge of the language.
Recall that we view projection as an approach
for alleviating the corpus annotation bottleneck, not
as a solution to the multilingual coreference resolu-
tion problem. In fact, though rarely emphasized in
previous work on applying projection, we note that
projection alone cannot be used to solve multilin-
gual NLP problems, including coreference resolu-
tion. The reason is that every language has its own
idiosyncrasies with respect to linguistic properties,
and projection simply cannot produce annotations
capturing those properties that are specific to the tar-
get language. Our goal in this paper is to explore the
extent to which projection, which does not require
that we have any knowledge of the target language,
can push the limits of multilingual coreference res-
olution. If our results indicate that projection is a
promising approach, then the automatic coreference
annotations it produces can be used to augment the
manual annotations that capture the properties spe-
cific to the target language, thus alleviating the cor-
pus annotation bottleneck.
2 Related Work on Projection
The idea of projecting annotations from a resource-
rich language to a resource-scarce language was
originally proposed by Yarowsky and Ngai (2001)
and subsequently developed by others (e.g., Resnik
(2004), Hwa et al (2005)). These projection al-
gorithms assume as input a parallel corpus for the
source language and the target language. Given the
recent availability of machine translation (MT) ser-
vices on the Web, researchers have focused more
on translated-based projection rather than acquiring
a parallel corpus themselves. MT-based projection
has been applied to various NLP tasks, such as part-
of-speech tagging (e.g., Das and Petrov (2011)),
mention detection (e.g., Zitouni and Florian (2008)),
and sentiment analysis (e.g., Mihalcea et al (2007)).
There have been two initial attempts to apply pro-
jection to create coreference-annotated data for a
resource-poor language, both of which involve pro-
jecting hand-annotated coreference data from En-
glish to Romanian via a parallel corpus. Specifically,
Harabagiu and Maiorano (2000) create an English-
Romanian corpus by manually translating the MUC-
6 corpus into Romanian and manually project the
English annotations to Romanian. On the other
hand, Postolache et al (2006) apply a word align-
ment algorithm to project the hand-annotated En-
glish coreference chains and then manually fix the
projection errors on the Romanian side. Hence,
their goal is different from ours in at least two re-
spects. First, while they employ significant knowl-
edge of the target language to create a clean corefer-
ence corpus, we examine the quality of coreference-
annotated data created via an entirely automatic pro-
cess, determining quality by the performance of the
resolver trained on the data. Second, unlike ours,
neither of these attempts is at the level of defining
a technology for projection annotations that can po-
tentially be deployed across a large number of lan-
guages without coreference-annotated data.
721
3 Translation-Based Projection
Recall that our MT-based projection approach to
coreference resolution is composed of three steps.
Given a text in the target language, we (1) machine-
translate the text to the source language; (2) au-
tomatically produce coreference annotations on the
translated text using a coreference resolver devel-
oped for the source language; and (3) project the
annotations from the source language to the target
language. In this section, we employ our approach
in three settings, which differ in terms of the ex-
tent to which linguistic taggers (e.g., chunkers and
named entity (NE) recognizers) for the target lan-
guage are available. The goal is to examine whether
these linguistic taggers can be profitably exploited to
improve the performance of the projection approach.
Below we assume that English and French are our
source and target languages, respectively.
3.1 Setting 1: No French Taggers Available
In this setting, we assume that we do not have access
to any French tagger that we can exploit to improve
projection. Hence, all we can do is to employ the
three steps involved in the projection approach as
described at the beginning of this section to create
coreference-annotated data for French. Specifically,
we translate a French text to an English text using
GoogleTranslate1 , and create coreference chains for
the translated English text using Reconcile2 (Stoy-
anov et al, 2010). To project mentions from En-
glish to French, we first align the English and French
words in each pair of parallel sentences, and then
project the English mentions onto the French text us-
ing the alignment. However, since the alignment is
noisy, the French words to which the words in the
English mention are aligned may not form a con-
tiguous text span. To fix this problem, we follow
Yarowsky and Ngai (2001) and use the smallest text
span that covers all the aligned French words to cre-
ate the French mention.3 We process the English
mentions in the text in a left-to-right manner, as
processing the mentions sequentially enables us to
ensure that an English mention is not mapped to a
1See http://translate.google.com.
2See http://www.cs.utah.edu/nlp/reconcile.
We use the resolver pre-trained on the Wolverhampton corpus.
3Other methods for projecting mentions can be found in Pos-
tolache et al (2006), for example.
French text span that has already been mapped to by
a previously-processed English mention.4
To align English and French words, we trained
a word alignment model using GIZA++5 (Och and
Ney, 2000) on a parallel corpus comprising the
English-French section of Europarl6 (Koehn, 2005)
as well as all the French texts (and their translated
English counterparts) for which we want to auto-
matically create coreference chains. Following com-
mon practice, we stemmed the parallel corpus us-
ing the Porter stemmer (Porter, 1980) in order to
reduce data sparseness. However, even with stem-
ming, we found that many English words were not
aligned to any French words by the resulting align-
ment model. This would prevent many English men-
tions from being projected to the French side, poten-
tially harming the recall of the French coreference
annotations. To improve alignment coverage, we re-
trained the alignment model by supplying GIZA++
with an English-French bilingual dictionary that we
assembled using three online dictionary databases:
OmegaWiki, Wiktionary, and Universal Dictionary.
Furthermore, if a word w appears in both the English
side and the French side in a pair of parallel sen-
tences, we assume that it has the same orthographic
form in both languages and hence we augment the
bilingual dictionary with the entry (w, w).
Note that the use of a supervised resolver like
Reconcile does not render our approach supervised,
since we can replace it with any resolver, be it super-
vised, heuristic, or unsupervised. In other words, we
treat the resolver built for the source language as a
black box that can produce coreference annotations.
3.2 Setting 2: Mention Extractor Available
Next, we consider a comparatively less resource-
scarce setting where a French mention extractor is
available for identifying mentions in a French text7,
and describe how we can modify the projection ap-
proach to exploit this French mention extractor.
Given a French text we want to coreference-
4While we chose to process the mentions in a left-to-right
manner, any order of processing the mentions would work.
5See http://code.google.com/p/giza-pp/.
6See http://www.statmt.org/europarl/.
7Mention extraction is a term used in Automatic Content
Evaluation to refer to the task of determining the NPs that a
coreference system should consider in the resolution process.
722
annotate, we first translate it to English using
GoogleTranslate and align the French and English
words using a French-to-English word alignment
algorithm. Next, we identify the mentions in the
French text using the given mention extractor, and
project them onto the English text using the NP pro-
jection algorithm described in Setting 1. Finally, we
run Reconcile on the resulting English mentions to
generate coreference chains for the translated text,
and project these chains back to the French text.
As explained before, the performance of this
method is sensitive to the accuracy of the NP projec-
tion algorithm in recovering the English mentions,
which in turn depends on the accuracy of the word
alignment algorithm. To make this method more ro-
bust to noisy word alignment, we make a modifica-
tion to it. Rather than running Reconcile on the men-
tions produced by the NP projection algorithm, we
use Reconcile to identify the mentions directly from
the translated text. After that, we create a mapping
between the English mentions produced by the NP
projection algorithm and those produced by Recon-
cile using a small set of heuristics.
Specifically, let MP be the set of mentions identi-
fied by the NP projection algorithm and MR be the
set of mentions identified by Reconcile. For each
mention mP in MP , we map it to a mention in MR
that shares the same right boundary. If this fails, we
map it to a mention that covers its entire text span. If
this fails again, we map it to a mention that has a par-
tial overlap with it. If this still fails, we assume that
mP is not found by Reconcile and simply add mP to
MR. As before, we process the mentions in MP in
a left-to-right manner in order to ensure that no two
mentions in MP are mapped to the same Reconcile
mention. Finally, we discard all mentions in MR that
are not mapped by any mention in MP , and present
MR to Reconcile for coreference resolution. Since
we now have a 1-to-1 mapping between the Recon-
cile mentions and the French mentions, projecting
the coreference results back to French is trivial.
It may not be immediately clear why the exploita-
tion of the mention extractor in this setting may yield
better coreference annotations than those produced
in Setting 1. To see the reason, recall that one source
of errors inherent in a projection approach is word
alignment errors. In Setting 1, when we tried to
project English mentions to the French text, word
alignment errors would adversely affect the ability
of the NP projection algorithm to correctly define
the boundaries of the French mentions. Since coref-
erence performance depends crucially on the abil-
ity to correctly identify mentions (Stoyanov et al,
2009), the presence of word alignment errors im-
plies that the resulting French coreference annota-
tions could score poorly even if the English coref-
erence annotations produced by Reconcile were of
high quality. In the current setting, on the other
hand, we reduce the sensitivity of coreference per-
formance to word alignment errors via the use of the
French mention extractor to produce more accurate
French mention boundaries.
3.3 Setting 3: Additional Taggers Available
Finally, we consider a setting that is the least
resource-scarce of the three. We assume that in ad-
dition to a French mention extractor, we have access
to other French linguistic taggers (e.g., syntactic and
semantic parsers) that will allow us to generate the
linguistic features needed to train a French resolver
on the projected coreference annotations.
Specifically, assume that Test is a set of French
texts we want to coreference-annotate, and Training
is a set of French texts that is disjoint from Test but is
drawn from the same domain as Test.8 To annotate
the Test texts, we perform the following steps. First,
we employ the French mention extractor in combi-
nation with the method described in Setting 2 to au-
tomatically coreference-annotate the Training texts.
Next, motivated by Kobdani et al (2011), we train
a French coreference resolver on the automatically
coreference-annotated training texts, using the fea-
tures provided by the available linguistic taggers. Fi-
nally, we apply the resolver to generate coreference
chains for each Test text.
Two questions arise. First, is this method neces-
sarily better than the one described in Setting 2? We
hypothesize that the answer is affirmative: not only
can this method exploit the knowledge about the tar-
get language provided by the additional linguistic
taggers, but the resulting coreference resolver may
allow us to generalize from the (noisily labeled) data
and make this method more robust to the noise in-
8We assume that it is easy to assemble the Training set, since
unlabeled texts are typically easy to collect in practice.
723
herent in the projected coreference annotations than
the previously-described methods. Second, is this
method necessarily better than projection via a par-
allel corpus? Like the first question, this is also an
empirical question. Nevertheless, one reason why
this method is intuitively better is that it ensures that
the training and test documents are drawn from the
same domain. On the other hand, when project-
ing annotations via a parallel corpus, we may en-
counter a domain mismatch problem if the parallel
corpus and the test documents come from different
domains, and the coreference resolver may not work
well if it is trained and tested on different domains.
4 Coreference Resolution System
To train the coreference resolver employed in Set-
ting 3 in the previous section, we need to derive
linguistic features from the documents in the target
language. In our experiments, we employ the coref-
erence data sets produced as part of the SemEval-
2010 shared task on Coreference Resolution in Mul-
tiple Languages. The shared task organizers have
made publicly available six data sets that corre-
spond to six European languages. Each data set
comprises not only training and test documents that
are coreference-annotated, but also a number of
word-based linguistic features from which we derive
mention-based linguistic features for training a re-
solver. In this section, we will describe how this re-
solver is trained and then applied to generate coref-
erence chains for unseen documents.
Training the coreference classifier. As our coref-
erence model, we train a mention-pair model, which
is a classifier that determines whether two mentions
are co-referring or not (e.g., Soon et al (2001), Ng
and Cardie (2002)).9 Each instance i(mj ,mk) cor-
responds to mj (a candidate antecedent) and mk (the
mention to be resolved), and is represented by a set
of 23 features shown in Table 1. As we can see, each
feature is either relational, capturing the relation be-
tween mj and mk, or non-relational, capturing the
linguistic property of mk. The possible values of
a relational feature (except LEXICAL) are C (com-
patible), I (incompatible), and NA (the comparison
9Note that any supervised coreference model can be used,
such as an entity-mention model (e.g., Luo et al (2004), Yang
et al (2008)) or a ranking model (e.g., Denis and Baldridge
(2008), Rahman and Ng (2009)).
cannot be made due to missing data). For a non-
relational feature, we refer the reader to the data sets
for the list of possible values.10
We follow Soon et al?s (2001) method for creat-
ing training instances. Specifically, we create (1) a
positive instance for each anaphoric mention mk and
its closest antecedent mj; and (2) a negative instance
for mk paired with each of the intervening mentions,
mj+1,mj+2, . . . ,mk?1. The classification associ-
ated with a training instance is either positive or neg-
ative, depending on whether the two mentions are
coreferent in the associated text. To train the classi-
fier, we use SVMlight (Joachims, 1999).
Applying the classifier to a test text. After train-
ing, the classifier is used to identify an antecedent
for a mention in a test text. Specifically, each men-
tion, mk, is compared to each preceding mention,
mj , from right to left, and mj is selected as the an-
tecedent of mk if the pair is classified as coreferent.
The process terminates as soon as an antecedent is
found for mk or the beginning of the text is reached.
5 Evaluation
We evaluate our MT-based projection approach for
each of the three settings described in Section 3.
5.1 Experimental Setup
Data sets. We use the Spanish and Italian data sets
from the SemEval-2010 shared task on Coreference
Resolution in Multiple Languages.11 Each data set
is composed of a training set and a test set. Statistics
of these data sets are shown in Table 2.
Spanish Italian
Training Set Statistics
number of mentions 78779 24853
number of non-singleton clusters 48681 18376
number of singleton clusters 37336 15984
Test Set Statistics
number of mentions 14133 13394
number of non-singleton clusters 8789 9520
number singleton clusters 6737 8288
Table 2: Statistics of the data sets.
10The data sets can be downloaded from http://stel.
ub.edu/semeval2010-coref/datasets.
11Note, however, that our approach is equally applicable to
other languages evaluated in the shared task.
724
Features describing mk, the mention to be resolved
1 NUM WORDS the number of words in mk
2 COARSE POS the coarse POS of mk (see ?PoS? in Recasens et al (2010))
3 FINE POS the fine-grained POS of mk (see ?PoS type? in Recasens et al (2010))
4 NE the named entity tag of mk if mk is a named entity; else NA
5 SR the semantic role of mk
6 GRAMROLE the grammatical role of mk
7 NUMBER the number of mk
8 GENDER the gender of mk
9 PERSON the person of mk (e.g., first, second, third) if it is pronominal; else NA
Features describing the relationship between mj , a candidate antecedent and mk, the mention to be resolved
10 CS STR MATCH determines whether the mentions are the same string
11 CI STR MATCH same as feature 10, except that case differences are ignored
12 CS SUBSTR MATCH determines whether one mention is a substring of the other
13 CI SUBSTR MATCH same as feature 12, except that case differences are ignored
14 NUMBER MATCH determines whether the mentions agree in number
15 GENDER MATCH determines whether the mentions agree in gender
16 COARSE POS MATCH determines whether the mentions have the same coarse POS tag
17 FINE POS MATCH determines whether the mentions have the same fine-grained POS tag
18 ROLE MATCH determines whether the mentions have the same grammatical role
19 NE MATCH determines whether both are NEs and have the same NE type
20 SR MATCH determines whether the mentions have the same semantic role
21 ALIAS determines whether one mention is an abbreviation or an acronym of the other
22 PERSON MATCH determines whether both mentions are pronominal and have the same person
23 LEXICAL the concatenation of the heads of the two mentions
Table 1: Feature set for coreference resolution.
Scoring programs. To score the output of a coref-
erence resolver, we employ four scoring programs,
MUC (Vilain et al, 1995), B3 (Bagga and Baldwin,
1998), ?3-CEAF (Luo, 2005), and BLANC (Re-
casens and Hovy, 2011), which were downloaded
from the shared task website (see Footnote 10).
Gold-standard versus regular settings. The for-
mat of each data set follows that of a typical CoNLL
shared task data set. In other words, each row cor-
responds to a word in a document; moreover, all but
the last column contain the linguistic features com-
puted for the words, and the last column stores the
coreference information. Some of the features were
computed via automatic means, but some were ex-
tracted from human annotations. Given this distinc-
tion, the shared task organizers defined two evalua-
tion settings: in the regular setting, only the columns
that were computed automatically can be used to de-
rive coreference features for classifier training, and
results should be reported on system mentions; on
the other hand, in the gold-standard setting, only
the columns that were extracted from human annota-
tions can be used to derive coreference features, and
results should be reported on true mentions. We will
present results corresponding to both settings. Note
that these two settings should not be confused with
the three settings described in Section 3.
Mention extraction. Recall that Settings 2 and 3
both assume the availability of a mention extrac-
tor for extracting mentions in the target language.
In our experiments, we extract mentions using two
methods. First, we assume the availability of an
oracle mention extractor that will enable us to ex-
tract true mentions (i.e., gold-standard mentions) di-
rectly from the test texts. Second, we employ simple
heuristics to automatically extract system mentions.
Since coreference performance is sensitive to the
accuracy of mention extraction (Stoyanov et al,
2009), we experiment with several heuristic meth-
ods for extracting system mentions for both Span-
ish and Italian. According to our cross-validation
experiments on the training data, the best heuris-
tic for extracting Spanish mentions is different from
that for extracting Italian mentions. Specifically, for
725
Spanish, the best heuristic method operates as fol-
lows. First, it extracts all the syntactic heads (i.e.,
the word tokens whose gold dependency labels are
SUBJ, PRED, or GMOD). Second, for each syntac-
tic head, it identifies the smallest text span contain-
ing the head and all of its dependents, and creates a
mention from this text span. For Italian, on the other
hand, the best heuristic simply involves creating one
mention for each gold NE. The reason why this sim-
ple heuristic works well is that most of the Italian
mentions are NEs, owing to the fact that abstract
NPs and pronouns are also annotated as NEs in the
Italian data set. When evaluated on the test set, the
heuristic-based mention extractor achieves F-scores
of 80.2 (78.4 recall, 82.1 precision) for Spanish and
92.3 (85.9 recall, 99.6 precision) for Italian.
5.2 Results and Discussion
5.2.1 Supervised Results
Our supervised systems. While our MT-based
projection approach is unsupervised (i.e., it does not
rely on any coreference annotations from the target
language), it would be informative to see the perfor-
mance of the supervised resolvers, since their perfor-
mance can be viewed as a crude upper bound on the
performance of our unsupervised systems. Specif-
ically, we train a mention-pair model on the train-
ing set using the 23 features shown in Table 1 and
SVMlight as the underlying learning algorithm12,
and apply the resulting model in combination with
Soon et al?s clustering algorithm (see Section 4) to
generate coreference chains for the test texts.
Results on the test sets, reported in terms of re-
call (R), precision (P), and F-score (F) computed by
the four coreference scorers, are shown in the first
two rows of Table 3 (Spanish) and Table 4 (Italian).
For convenience, we summarize a system?s perfor-
mance using a single number, which is shown in the
last column (Average) and is obtained by taking a
simple average of the F-scores of the four scorers.
More specifically, row 1, which is marked with a
?G?, and row 2, which is marked with a ?R?, show
the results obtained under the gold-standard setting
and the regular setting, respectively.
As we can see, under the gold-standard setting,
12All SVM learning parameters in this and other experiments
in this paper are set to their default values.
the supervised resolver achieves an average F-score
of 66.1 (Spanish) and 65.9 (Italian). Not surpris-
ingly, under the regular setting, its average F-score
drops statistically significantly13 to 54.6 (Spanish)
and 63.4 (Italian).14
Best systems in the shared task. To determine
whether the upper bounds established by our su-
pervised systems are reasonable, we show the re-
sults of the best-performing resolvers participating
in the shared task for both languages under the gold-
standard and regular settings in rows 3 and 4 of Ta-
bles 3 and 4. Since none of the participating systems
achieved the best score over all four scorers, we re-
port the performance of the system that has the high-
est average F-score. According to the shared task
website, TANL-1 (Attardi et al, 2010) achieved the
best average F-score in the regular setting for Span-
ish, whereas SUCRE (Kobdani and Schu?tze, 2010)
outperformed others in the remaining settings.
Comparing these best shared task results with our
supervised results in rows 1 and 2, we see that our
average F-score for Spanish/Gold is worse than its
shared task counterpart by 0.7 points, but otherwise
our system outperforms in other settings w.r.t. av-
erage F-score, specifically by 5.0 points for Span-
ish/Regular (due to a better MUC F-score), by 3.4?
4.7 points for Italian (due to better CEAF, B3, and
BLANC scores). Overall, these results suggest that
the scores achieved by our systems are at least as
competitive as the best shared task scores.
5.2.2 Unsupervised Results
Next, we evaluate our projection algorithm.
Setting 1. Results of our approach, when applied
in Setting 1, are shown in row 5 of Tables 3 and 4.
Given that it has to operate under the severe condi-
tion where no linguistic taggers are available for the
target language, it is perhaps not surprising to see
that its performance is significantly worse than that
of its supervised counterparts.
Setting 2. Recall that this setting is less resource-
scarce than Setting 1 in that a mention extractor for
13All significance test results in this paper are obtained using
one-way ANOVA, with p set to 0.05.
14Separately, we determined whether the performance drop
in the regular setting is due to the use of automatically computed
features or the use of system mentions, and found that the latter
was almost entirely responsible for the drop.
726
CEAF MUC B3 BLANC Average
Approach R P F R P F R P F R P F F
1 Supervised (G) 68.8 68.8 68.8 58.2 52.6 55.3 76.5 75.1 75.8 62.9 66.1 64.3 66.1
2 Supervised (R) 57.4 60.1 58.8 41.0 46.3 43.5 57.6 64.8 61.0 53.9 65.0 55.2 54.6
3 Shared task best (G) 69.8 69.8 69.8 52.7 58.3 55.3 75.8 79.0 77.4 67.3 62.5 64.5 66.8
4 Shared task best (R) 58.6 60.0 59.3 14.0 48.4 21.7 56.6 79.0 66.0 51.4 74.7 51.4 49.6
5 Setting 1 35.9 52.9 42.8 10.8 48.7 17.7 30.5 63.9 41.3 51.2 72.6 48.7 37.6
6 Setting 2 (True) 65.6 65.6 65.6 16.8 64.7 26.7 64.3 96.9 77.3 52.8 78.8 54.6 56.1
7 Setting 2 (System) 53.2 55.7 54.4 13.4 58.5 21.8 49.8 79.7 61.3 50.7 75.5 49.5 46.8
8 Setting 3 (G) 65.9 65.9 65.9 48.1 45.2 46.6 72.3 72.6 72.5 60.1 61.4 60.7 61.4
9 Setting 3 (R) 55.3 55.3 55.3 34.1 41.6 37.5 55.1 63.6 59.0 53.8 62.1 54.9 51.7
Table 3: Results for Spanish
CEAF MUC B3 BLANC Average
Approach R P F R P F R P F R P F F
1 Supervised (G) 74.5 74.5 74.5 31.8 67.4 43.2 74.4 93.6 82.9 58.4 79.6 62.9 65.9
2 Supervised (R) 73.7 74.3 74.0 31.9 68.0 43.4 60.8 92.5 73.3 58.4 79.6 62.9 63.4
3 Shared task best (G) 66.0 66.0 66.0 48.1 42.3 45.0 76.7 76.9 76.8 54.8 63.5 56.9 61.2
4 Shared task best (R) 57.1 66.2 61.3 50.1 50.7 50.4 63.6 79.2 70.6 55.2 68.3 57.7 60.0
5 Setting 1 17.0 26.0 20.6 8.1 28.5 12.6 14.1 30.5 19.3 50.1 62.9 32.9 21.4
6 Setting 2 (True) 73.3 73.3 73.3 14.2 60.6 23.0 72.9 96.8 83.2 51.9 77.9 53.2 58.2
7 Setting 2 (System) 60.4 70.1 64.9 17.2 68.2 27.5 59.3 97.1 73.6 52.0 82.9 53.4 54.9
8 Setting 3 (G) 64.3 64.3 64.3 28.3 63.3 39.1 65.3 87.4 74.8 55.1 74.7 57.5 58.9
9 Setting 3 (R) 61.1 62.9 61.9 29.5 63.2 40.2 60.3 84.1 70.2 55.3 72.9 58.3 57.7
Table 4: Results for Italian
the target language is available. Results of our al-
gorithm, when operating under Setting 2 using true
mentions and system mentions, are shown in rows
6 and 7 of Tables 3 and 4, respectively. In com-
parison to the results for Setting 1, we see that the
F-scores obtained under Setting 2 increase signifi-
cantly, regardless of (1) the scoring programs and
(2) whether true mentions or system mentions are
used. These results provide evidence for our earlier
hypothesis that our projection algorithm can prof-
itably exploit the linguistic knowledge about the tar-
get language that is available to it. In particular, the
mention extractor helps make our approach less sen-
sitive to word alignment and NP projection errors.
In comparison to our supervised results in rows 1
and 2, our algorithm still lags behind by about 8?10
points in average F-score. However, this should not
be surprising, since our algorithm is unsupervised.
Looking closer at the results, we can see that the
performance lag by our approach can be attributed
to its lower recall: in general, the lag in MUC recall
appears to be more acute than that in B3 and CEAF
recall. Since MUC only scores non-singleton clus-
ters wheres B3 and CEAF score both singleton and
non-singleton clusters, these results suggest that our
approach is better at identifying singleton clusters
than recovering coreference links.
Setting 3. Finally, we evaluate our approach in a
setting where it has access to all the information
available to our supervised resolvers, except for the
gold-standard coreference annotations on the train-
ing sets. Specifically, our approach uses projected
coreference annotations to train a resolver on the
training texts, whereas the supervised resolvers do
so using gold-standard annotations.
Comparing Settings 2 and 3 with respect to true
mentions (rows 6 and 8 of Tables 3 and 4), we see
mixed results. According to MUC and BLANC, the
resolvers in Setting 3 are significantly better than
those in Setting 2 for both languages. According to
B3, the resolvers in Setting 2 are significantly better
than those in Setting 3 for both languages. Accord-
ing to CEAF, the Spanish resolvers in Setting 3 are
significantly better than their counterparts in Setting
2, but the opposite is true for the Italian resolvers.
To understand these somewhat contradictory per-
formance trends, let us first note that the dramatic in-
crease in the MUC F-score can be attributed to large
727
gains in MUC recall. This suggests that the clas-
sifiers being trained in Setting 3 have enabled the
discovery of additional coreference links. In other
words, there are benefits to be obtained just by learn-
ing over noisy coreference annotations, a result that
we believe is quite interesting. However, not all of
these newly discovered coreference links are correct.
The fact that some scoring programs (e.g., B3) are
more sensitive to spurious coreference links than the
others (e.g., MUC) explains these mixed results.
Nevertheless, according to average F-score, the
resolvers in Setting 3 perform significantly better
than those in Setting 2 for both languages: F-score
increases by 5.3 points for Spanish and 0.7 points for
Italian. Similar trends can be observed when com-
paring the two settings w.r.t. system mentions (rows
7 and 9 of Tables 3 and 4): F-score increases by 4.9
points for Spanish and 2.8 points for Italian.
While our Setting 3 results still underperform the
supervised results in rows 1 and 2, we can see that
they achieve 93?94% of the average F-scores of the
supervised Spanish resolvers and 89?91% of the av-
erage F-scores of the supervised Italian resolvers.
Importantly, recall that our approach achieves this
level of performance without relying on any gold-
standard coreference annotations in Spanish and
Italian, and we believe that these results demonstrate
the promise of our MT-based projection approach.
Since these results suggest that our approach can-
not be successfully applied without MT services, a
parallel corpus for learning a word alignment model,
and a mention extractor for the target language, a
natural question is: to what extent do these require-
ments limit the applicability of our approach? While
it is the case that our approach cannot be applied to
a truly resource-scarce language, it can be applied to
the numerous Indian and East European languages
for which the aforementioned requirements are sat-
isfied but coreference-annotated data is not readily
available.
6 Conclusions and Future Work
We explored the under-investigated yet challenging
task of performing coreference resolution for a lan-
guage for which we have no coreference-annotated
data and no linguistic knowledge of the language.
Our translation-based projection approach has the
flexibility to exploit any available knowledge about
the target language. In experiments with Spanish
and Italian, we obtained promising results: our ap-
proach achieved around 90% of the performance of
a supervised resolver when only a mention extrac-
tor for the target language was available. We believe
that this approach has the potential to allow coref-
erence technologies to be deployed across a larger
number of languages than is currently possible, and
that this is just the beginning of a new line of work.
To gain additional insights into our approach,
we plan to pursue several directions. First, we
will isolate the impact of each factor that ad-
versely affects its performance, including errors
in projection, translation, and coreference resolu-
tion in the resource-rich language. Second, we
will perform an empirical comparison of two ap-
proaches to projecting coreference annotations, our
translation-based approach and Camargo de Souza
and Orasan?s (2011) approach, where annotations
are projected via a parallel corpus. Third, rather than
translate from the target to the source language, we
will examine whether it is better to translate all the
coreference-annotated data available in the source
language to the target language, and train a coref-
erence model for the target language on the trans-
lated data. Fourth, since the success of our pro-
jection approach depends heavily on the accuracies
of machine translation as well as coreference res-
olution in the source language, we will determine
whether their accuracies can be improved via an en-
semble approach, where we employ multiple MT
engines and multiple coreference resolvers. Finally,
we plan to employ our approach to alleviate the
corpus-annotation bottleneck, specifically by using
the annotated data it produces to augment the man-
ual coreference annotations that capture the specific
properties of the target language.
Acknowledgments
We thank the three anonymous reviewers for their
detailed and insightful comments on an earlier draft
of the paper. This work was supported in part by
NSF Grants IIS-0812261 and IIS-1147644. Any
opinions, findings, or conclusions expressed in this
paper are those of the authors and do not necessarily
reflect the views or official policies of NSF.
728
References
Giuseppe Attardi, Maria Simi, and Stefano Dei Rossi.
2010. TANL-1: Coreference resolution by parse anal-
ysis and similarity clustering. In Proceedings of the
5th International Workshop on Semantic Evaluation,
pages 108?111.
Amit Bagga and Breck Baldwin. 1998. Entity-based
cross-document coreferencing using the vector space
model. In Proceedings of the 36th Annual Meeting of
the Association for Computational Linguistics and the
17th International Conference on Computational Lin-
guistics, pages 79?85.
Jennifer Camargo de Souza and Constantine Orasan.
2011. Can projected chains in parallel corpora help
coreference resolution? In Anaphora Processing and
Applications, pages 59?69. Springer.
Dipanjan Das and Slav Petrov. 2011. Unsupervised part-
of-speech tagging with bilingual graph-based projec-
tions. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 600?609.
Pascal Denis and Jason Baldridge. 2008. Specialized
models and ranking for coreference resolution. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 660?669.
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric bayesian
model. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics, pages
848?855.
Aria Haghighi and Dan Klein. 2010. Coreference res-
olution in a modular, entity-centered model. In Pro-
ceedings of Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
385?393.
Sanda Harabagiu and Steven Maiorano. 2000. Multi-
lingual coreference resolution. In Proceedings of the
Sixth Applied Natural Language Processing Confer-
ence, pages 142?149.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering, 11(3):311?325.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Bernhard Scho?lkopf, Christo-
pher Burges, and Alexander Smola, editors, Advances
in Kernel Methods ? Support Vector Learning, pages
44?56. MIT Press, Cambridge, MA.
Hamidreza Kobdani and Hinrich Schu?tze. 2010. SU-
CRE: A modular system for coreference resolution. In
Proceedings of the 5th International Workshop on Se-
mantic Evaluation, pages 92?95.
Hamidreza Kobdani, Hinrich Schu?tze, Michael
Schiehlen, and Hans Kamp. 2011. Bootstrap-
ping coreference resolution using word associations.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, pages 783?792.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of MT
Summit X.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based
on the Bell tree. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Linguis-
tics, pages 135?142.
Xiaoqiang Luo. 2005. On coreference resolution perfor-
mance metrics. In Proceedings of Human Language
Technology Conference and Conference on Empirical
Methods in Natural Language Processing, pages 25?
32.
Rada Mihalcea, Carmen Banea, and Janyce Wiebe. 2007.
Learning multilingual subjective language via cross-
lingual projections. In Proceedings of the 45th Annual
Meeting of the Association for Computational Linguis-
tics, pages 976?983..
Ruslan Mitkov. 1999. Multilingual anaphora resolution.
Machine Translation, 14(3?4):281?299.
Vincent Ng and Claire Cardie. 2002. Improving machine
learning approaches to coreference resolution. In Pro-
ceedings of the 40th Annual Meeting of the Association
for Computational Linguistics, pages 104?111.
Vincent Ng. 2008. Unsupervised models for coreference
resolution. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 640?649.
Franz Josef Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics.
Hoifung Poon and Pedro Domingos. 2008. Joint un-
supervised coreference resolution with Markov Logic.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages 650?
659.
Martin F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
Oana Postolache, Dan Cristea, and Constantin Orasan.
2006. Transferring coreference chains through word
alignment. In Proceedings of the Fifth International
Conference on Language Resources and Evaluation,
pages 889?892.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nate Chambers, Mihai Surdeanu, Dan Juraf-
sky, and Christopher Manning. 2010. A multi-pass
729
sieve for coreference resolution. In Proceedings of
the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 492?501.
Altaf Rahman and Vincent Ng. 2009. Supervised mod-
els for coreference resolution. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 968?977.
Marta Recasens and Eduard Hovy. 2011. BLANC: Im-
plementing the Rand Index for coreference evaluation.
Natural Language Engineering, 17(4):485?510.
Marta Recasens, Llu??s Ma`rquez, Emili Sapena,
M. Anto`nia Mart??, Mariona Taule?, Ve?ronique
Hoste, Massimo Poesio, and Yannick Versley. 2010.
Semeval-2010 task 1: Coreference resolution in multi-
ple languages. In Proceedings of the 5th International
Workshop on Semantic Evaluation, pages 1?8.
Philip Resnik. 2004. Exploiting hidden meanings: Us-
ing bilingual text for monolingual annotation. In Pro-
ceedings of the 5th International Conference on Com-
putational Linguistics and Intelligent Text Processing,
pages 283?299.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational Lin-
guistics, 27(4):521?544.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase coref-
erence resolution: Making sense of the state-of-the-
art. In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 656?664.
Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen
Riloff, David Buttler, and David Hysom. 2010. Coref-
erence resolution with reconcile. In Proceedings of the
ACL 2010 Conference Short Papers, pages 156?161.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceed-
ings of the Sixth Message Understanding Conference,
pages 45?52.
Xiaofeng Yang, Jian Su, Jun Lang, Chew Lim Tan, and
Sheng Li. 2008. An entity-mention model for coref-
erence resolution with inductive logic programming.
In Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 843?851.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual POS taggers and NP bracketers via robust
projection across aligned corpora. In Proceedings of
the Second Meeting of the North American Chapter of
the Association for Computational Linguistics, pages
200?207.
Imed Zitouni and Radu Florian. 2008. Mention detec-
tion crossing the language barrier. In Proceedings of
the 2008 Conference on Empirical Methods in Natural
Language Processing, pages 600?609.
730
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 814?824,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Coreference Resolution with World Knowledge
Altaf Rahman and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{altaf,vince}@hlt.utdallas.edu
Abstract
While world knowledge has been shown to
improve learning-based coreference resolvers,
the improvements were typically obtained by
incorporating world knowledge into a fairly
weak baseline resolver. Hence, it is not clear
whether these benefits can carry over to a
stronger baseline. Moreover, since there has
been no attempt to apply different sources of
world knowledge in combination to corefer-
ence resolution, it is not clear whether they of-
fer complementary benefits to a resolver. We
systematically compare commonly-used and
under-investigated sources of world knowl-
edge for coreference resolution by applying
them to two learning-based coreference mod-
els and evaluating them on documents anno-
tated with two different annotation schemes.
1 Introduction
Noun phrase (NP) coreference resolution is the task
of determining which NPs in a text or dialogue refer
to the same real-world entity. The difficulty of the
task stems in part from its reliance on world knowl-
edge (Charniak, 1972). To exemplify, consider the
following text fragment.
Martha Stewart is hoping people don?t run out on her.
The celebrity indicted on charges stemming from . . .
Having the (world) knowledge that Martha Stewart
is a celebrity would be helpful for establishing the
coreference relation between the two NPs. One may
argue that employing heuristics such as subject pref-
erence or syntactic parallelism (which prefers re-
solving an NP to a candidate antecedent that has the
same grammatical role) in this example would also
allow us to correctly resolve the celebrity (Mitkov,
2002), thereby obviating the need for world knowl-
edge. However, since these heuristics are not per-
fect, complementing them with world knowledge
would be an important step towards bringing coref-
erence systems to the next level of performance.
Despite the usefulness of world knowledge for
coreference resolution, early learning-based coref-
erence resolvers have relied mostly on morpho-
syntactic features (e.g., Soon et al (2001), Ng and
Cardie (2002), Yang et al (2003)). With recent ad-
vances in lexical semantics research and the devel-
opment of large-scale knowledge bases, researchers
have begun to employ world knowledge for corefer-
ence resolution. World knowledge is extracted pri-
marily from three data sources, web-based encyclo-
pedia (e.g., Ponzetto and Strube (2006), Uryupina
et al (2011)), unannotated data (e.g., Daume? III
and Marcu (2005), Ng (2007)), and coreference-
annotated data (e.g., Bengtson and Roth (2008)).
While each of these three sources of world knowl-
edge has been shown to improve coreference resolu-
tion, the improvements were typically obtained by
incorporating world knowledge (as features) into a
baseline resolver composed of a rather weak coref-
erence model (i.e., the mention-pair model) and a
small set of features (i.e., the 12 features adopted
by Soon et al?s (2001) knowledge-lean approach).
As a result, some questions naturally arise. First,
can world knowledge still offer benefits when used
in combination with a richer set of features? Sec-
ond, since automatically extracted world knowledge
is typically noisy (Ponzetto and Poesio, 2009), are
recently-developed coreference models more noise-
tolerant than the mention-pair model, and if so, can
they profit more from the noisily extracted world
knowledge? Finally, while different world knowl-
814
edge sources have been shown to be useful when ap-
plied in isolation to a coreference system, do they of-
fer complementary benefits and therefore can further
improve a resolver when applied in combination?
We seek answers to these questions by conduct-
ing a systematic evaluation of different world knowl-
edge sources for learning-based coreference reso-
lution. Specifically, we (1) derive world knowl-
edge from encyclopedic sources that are under-
investigated for coreference resolution, including
FrameNet (Baker et al, 1998) and YAGO (Suchanek
et al, 2007), in addition to coreference-annotated
data and unannotated data; (2) incorporate such
knowledge as features into a richer baseline feature
set that we previously employed (Rahman and Ng,
2009); and (3) evaluate their utility using two coref-
erence models, the traditional mention-pair model
(Soon et al, 2001) and the recently developed
cluster-ranking model (Rahman and Ng, 2009).
Our evaluation corpus contains 410 documents,
which are coreference-annotated using the ACE an-
notation scheme as well as the OntoNotes annota-
tion scheme (Hovy et al, 2006). By evaluating on
two sets of coreference annotations for the same set
of documents, we can determine whether the use-
fulness of world knowledge sources for coreference
resolution is dependent on the underlying annotation
scheme used to annotate the documents.
2 Preliminaries
In this section, we describe the corpus, the NP ex-
traction methods, the coreference models, and the
evaluation measures we will use in our evaluation.
2.1 Data Set
We evaluate on documents that are coreference-
annotated using both the ACE annotation scheme
and the OntoNotes annotation scheme, so that we
can examine whether the usefulness of our world
knowledge sources is dependent on the underlying
coreference annotation scheme. Specifically, our
data set is composed of the 410 English newswire
articles that appear in both OntoNotes-2 and ACE
2004/2005. We partition the documents into a train-
ing set and a test set following a 80/20 ratio.
ACE and OntoNotes employ different guide-
lines to annotate coreference chains. A major
difference between the two annotation schemes is
that ACE only concerns establishing coreference
chains among NPs that belong to the ACE entity
types, whereas OntoNotes does not have this re-
striction. Hence, the OntoNotes annotation scheme
should produce more coreference chains (i.e., non-
singleton coreference clusters) than the ACE anno-
tation scheme for a given set of documents. For our
data set, the OntoNotes scheme yielded 4500 chains,
whereas the ACE scheme yielded only 3637 chains.
Another difference between the two annotation
schemes is that singleton clusters are annotated in
ACE but not OntoNotes. As discussed below, the
presence of singleton clusters may have an impact
on NP extraction and coreference evaluation.
2.2 NP Extraction
Following common practice, we employ different
methods to extract NPs from the documents anno-
tated with the two annotation schemes.
To extract NPs from the ACE-annotated docu-
ments, we train a mention extractor on the train-
ing texts (see Section 5.1 of Rahman and Ng (2009)
for details), which recalls 83.6% of the NPs in the
test set. On the other hand, to extract NPs from the
OntoNotes-annotated documents, the same method
should not be applied. To see the reason, recall that
only the NPs in non-singleton clusters are annotated
in these documents. Training a mention extractor
on these NPs implies that we are learning to ex-
tract non-singleton NPs, which are typically much
smaller in number than the entire set of NPs. In
other words, doing so could substantially simplify
the coreference task. Consequently, we follow the
approach adopted by traditional learning-based re-
solvers and employ an NP chunker to extract NPs.
Specifically, we use the markable identification sys-
tem in the Reconcile resolver (Stoyanov et al, 2010)
to extract NPs from the training and test texts. This
identifier recalls 77.4% of the NPs in the test set.
2.3 Coreference Models
We evaluate the utility of world knowledge using the
mention-pair model and the cluster-ranking model.
2.3.1 Mention-Pair Model
The mention-pair (MP) model is a classifier that
determines whether two NPs are coreferent or not.
815
Each instance i(NPj , NPk) corresponds to NPj and
NPk, and is represented by a Baseline feature set con-
sisting of 39 features. Linguistically, these features
can be divided into four categories: string-matching,
grammatical, semantic, and positional. These fea-
tures can also be categorized based on whether they
are relational or not. Relational features capture
the relationship between NPj and NPk, whereas non-
relational features capture the linguistic property of
one of these two NPs. Since space limitations pre-
clude a description of these features, we refer the
reader to Rahman and Ng (2009) for details.
We follow Soon et al?s (2001) method for cre-
ating training instances: we create (1) a positive
instance for each anaphoric NP, NPk, and its clos-
est antecedent, NPj ; and (2) a negative instance for
NPk paired with each of the intervening NPs, NPj+1,
NPj+2, . . ., NPk?1. The classification of a training
instance is either positive or negative, depending on
whether the two NPs are coreferent in the associated
text. To train the MP model, we use the SVM learn-
ing algorithm from SVMlight (Joachims, 2002).1
After training, the classifier is used to identify an
antecedent for an NP in a test text. Specifically, each
NP, NPk , is compared in turn to each preceding NP,
NPj , from right to left, and NPj is selected as its an-
tecedent if the pair is classified as coreferent. The
process terminates as soon as an antecedent is found
for NPk or the beginning of the text is reached.
Despite its popularity, the MP model has two
major weaknesses. First, since each candidate an-
tecedent for an NP to be resolved (henceforth an ac-
tive NP) is considered independently of the others,
this model only determines how good a candidate
antecedent is relative to the active NP, but not how
good a candidate antecedent is relative to other can-
didates. So, it fails to answer the critical question of
which candidate antecedent is most probable. Sec-
ond, it has limitations in its expressiveness: the in-
formation extracted from the two NPs alone may not
be sufficient for making a coreference decision.
2.3.2 Cluster-Ranking Model
The cluster-ranking (CR) model addresses the two
weaknesses of the MP model by combining the
strengths of the entity-mention model (e.g., Luo et
1For this and subsequent uses of the SVM learner in our
experiments, we set al parameters to their default values.
al. (2004), Yang et al (2008)) and the mention-
ranking model (e.g., Denis and Baldridge (2008)).
Specifically, the CR model ranks the preceding clus-
ters for an active NP so that the highest-ranked clus-
ter is the one to which the active NP should be
linked. Employing a ranker addresses the first weak-
ness, as a ranker allows all candidates to be com-
pared simultaneously. Considering preceding clus-
ters rather than antecedents as candidates addresses
the second weakness, as cluster-level features (i.e.,
features that are defined over any subset of NPs in a
preceding cluster) can be employed. Details of the
CR model can be found in Rahman and Ng (2009).
Since the CR model ranks preceding clusters, a
training instance i(cj , NPk) represents a preceding
cluster, cj , and an anaphoric NP, NPk. Each instance
consists of features that are computed based solely
on NPk as well as cluster-level features, which de-
scribe the relationship between cj and NPk . Mo-
tivated in part by Culotta et al (2007), we create
cluster-level features from the relational features in
our feature set using four predicates: NONE, MOST-
FALSE, MOST-TRUE, and ALL. Specifically, for each
relational feature X, we first convert X into an equiv-
alent set of binary-valued features if it is multi-
valued. Then, for each resulting binary-valued fea-
ture Xb, we create four binary-valued cluster-level
features: (1) NONE-Xb is true when Xb is false be-
tween NPk and each NP in cj ; (2) MOST-FALSE-Xb
is true when Xb is true between NPk and less than half
(but at least one) of the NPs in cj ; (3) MOST-TRUE-
Xb is true when Xb is true between NPk and at least
half (but not all) of the NPs in cj ; and (4) ALL-Xb is
true when Xb is true between NPk and each NP in cj .
We train a cluster ranker to jointly learn
anaphoricity determination and coreference reso-
lution using SVMlight?s ranker-learning algorithm.
Specifically, for each NP, NPk, we create a training
instance between NPk and each preceding cluster cj
using the features described above. Since we are
learning a joint model, we need to provide the ranker
with the option to start a new cluster by creating an
additional training instance that contains the non-
relational features describing NPk . The rank value
of a training instance i(cj , NPk) created for NPk is the
rank of cj among the competing clusters. If NPk is
anaphoric, its rank is HIGH if NPk belongs to cj , and
LOW otherwise. If NPk is non-anaphoric, its rank is
816
LOW unless it is the additional training instance de-
scribed above, which has rank HIGH.
After training, the cluster ranker processes the
NPs in a test text in a left-to-right manner. For each
active NP, NPk , we create test instances for it by pair-
ing it with each of its preceding clusters. To allow
for the possibility that NPk is non-anaphoric, we cre-
ate an additional test instance as during training. All
these test instances are then presented to the ranker.
If the additional test instance is assigned the highest
rank value, then we create a new cluster containing
NPk. Otherwise, NPk is linked to the cluster that has
the highest rank. Note that the partial clusters pre-
ceding NPk are formed incrementally based on the
predictions of the ranker for the first k ? 1 NPs.
2.4 Evaluation Measures
We employ two commonly-used scoring programs,
B3 (Bagga and Baldwin, 1998) and CEAF (Luo,
2005), both of which report results in terms of recall
(R), precision (P), and F-measure (F) by comparing
the gold-standard (i.e., key) partition, KP , against
the system-generated (i.e., response) partition, RP .
Briefly, B3 computes the R and P values of each
NP and averages these values at the end. Specifi-
cally, for each NP, NPj , B3 first computes the number
of common NPs in KPj and RPj , the clusters con-
taining NPj in KP and RP , respectively, and then
divides this number by |KPj | and |RPj | to obtain
the R and P values of NPj , respectively. On the other
hand, CEAF finds the best one-to-one alignment be-
tween the key clusters and the response clusters.
A complication arises when B3 is used to score
a response partition containing automatically ex-
tracted NPs. Recall that B3 constructs a mapping
between the NPs in the response and those in the
key. Hence, if the response is generated using gold-
standard NPs, then every NP in the response is
mapped to some NP in the key and vice versa. In
other words, there are no twinless (i.e., unmapped)
NPs (Stoyanov et al, 2009). This is not the case
when automatically extracted NPs are used, but the
original description of B3 does not specify how
twinless NPs should be scored (Bagga and Baldwin,
1998). To address this problem, we set the recall
and precision of a twinless NP to zero, regardless of
whether the NP appears in the key or the response.
Note that CEAF can compare partitions with twin-
less NPs without any modification, since it operates
by finding the best alignment between the clusters in
the two partitions.
Additionally, in order not to over-penalize a re-
sponse partition, we remove all the twinless NPs in
the response that are singletons. The rationale is
simple: since the resolver has successfully identified
these NPs as singletons, it should not be penalized,
and removing them avoids such penalty.
Since B3 and CEAF align NPs/clusters, the lack
of singleton clusters in the OntoNotes annotations
implies that the resulting scores reflect solely how
well a resolver identifies coreference links and do
not take into account how well it identifies singleton
clusters.
3 Extracting World Knowledge
In this section, we describe how we extract world
knowledge for coreference resolution from three
different sources: large-scale knowledge bases,
coreference-annotated data and unannotated data.
3.1 World Knowledge from Knowledge Bases
We extract world knowledge from two large-scale
knowledge bases, YAGO and FrameNet.
3.1.1 Extracting Knowledge from YAGO
We choose to employ YAGO rather than the more
popularly-used Wikipedia due to its potentially
richer knowledge, which comprises 5 million facts
extracted from Wikipedia and WordNet. Each fact
is represented as a triple (NPj , rel, NPk), where rel
is one of the 90 YAGO relation types defined on
two NPs, NPj and NPk . Motivated in part by previ-
ous work (Bryl et al, 2010; Uryupina et al, 2011),
we employ the two relation types that we believe
are most useful for coreference resolution, TYPE
and MEANS. TYPE is essentially an IS-A relation.
For instance, the triple (AlbertEinstein, TYPE,
physicist) denotes the fact that Albert Einstein
is a physicist. MEANS provides different ways of
expressing an entity, and therefore allows us to deal
with synonymy and ambiguity. For instance, the two
triples (Einstein, MEANS, AlbertEinstein)
and (Einstein, MEANS, AlfredEinstein)
denote the facts that Einstein may refer to the physi-
cist Albert Einstein and the musicologist Alfred Ein-
stein, respectively. Hence, the presence of one or
817
both of these relations between two NPs provides
strong evidence that the two NPs are coreferent.
YAGO?s unification of the information in
Wikipedia and WordNet enables it to extract
facts that cannot be extracted with Wikipedia
or WordNet alne, such as (MarthaStewart,
TYPE, celebrity). To better appreciate YAGO?s
strengths, let us see how this fact was extracted.
YAGO first heuristically maps each of the Wiki
categories in the Wiki page for Martha Stewart
to its semantically closest WordNet synset. For
instance, the Wiki category AMERICAN TELE-
VISION PERSONALITIES is mapped to the synset
corresponding to sense #2 of the word personality.
Then, given that personality is a direct hyponym of
celebrity in WordNet, YAGO extracts the desired
fact. This enables YAGO to extract facts that cannot
be extracted with Wikipedia or WordNet alne.
We incorporate the world knowledge from YAGO
into our coreference models as a binary-valued fea-
ture. If the MP model is used, the YAGO feature
for an instance will have the value 1 if and only if
the two NPs involved are in a TYPE or MEANS re-
lation. On the other hand, if the CR model is used,
the YAGO feature for an instance involving NPk and
preceding cluster c will have the value 1 if and only
if NPk has a TYPE or MEANS relation with any of
the NPs in c. Since knowledge extraction from web-
based encyclopedia is typically noisy (Ponzetto and
Poesio, 2009), we use YAGO to determine whether
two NPs have a relation only if one NP is a named
entity (NE) of type person, organization, or location
according to the Stanford NE recognizer (Finkel et
al., 2005) and the other NP is a common noun.
3.1.2 Extracting Knowledge from FrameNet
FrameNet is a lexico-semantic resource focused on
semantic frames (Baker et al, 1998). As a schematic
representation of a situation, a frame contains the
lexical predicates that can invoke it as well as the
frame elements (i.e., semantic roles). For example,
the JUDGMENT COMMUNICATION frame describes
situations in which a COMMUNICATOR communi-
cates a judgment of an EVALUEE to an ADDRESSEE.
This frame has COMMUNICATOR and EVALUEE as
its core frame elements and ADDRESSEE as its non-
core frame elements, and can be invoked by more
than 40 predicates, such as acclaim, accuse, com-
mend, decry, denounce, praise, and slam.
To better understand why FrameNet contains po-
tentially useful knowledge for coreference resolu-
tion, consider the following text segment:
Peter Anthony decries program trading as ?limiting the
game to a few,? but he is not sure whether he wants to
denounce it because ...
To establish the coreference relation between it and
program trading, it may be helpful to know that de-
cry and denounce appear in the same frame and the
two NPs have the same semantic role.
This example suggests that features encoding both
the semantic roles of the two NPs under considera-
tion and whether the associated predicates are ?re-
lated? to each other in FrameNet (i.e., whether they
appear in the same frame) could be useful for iden-
tifying coreference relations. Two points regarding
our implementation of these features deserve men-
tion. First, since we do not employ verb sense dis-
ambiguation, we consider two predicates related as
long as there is at least one semantic frame in which
they both appear. Second, since FrameNet-style se-
mantic role labelers are not publicly available, we
use ASSERT (Pradhan et al, 2004), a semantic role
labeler that provides PropBank-style semantic roles
such as ARG0 (the PROTOAGENT, which is typi-
cally the subject of a transitive verb) and ARG1 (the
PROTOPATIENT, which is typically its direct object).
Now, assuming that NPj and NPk are the argu-
ments of two stemmed predicates, predj and predk ,
we create 15 features using the knowledge extracted
from FrameNet and ASSERT as follows. First, we
encode the knowledge extracted from FrameNet as
one of three possible values: (1) predj and predk
are in the same frame; (2) they are both predicates
in FrameNet but never appear in the same frame;
and (3) one or both predicates do not appear in
FrameNet. Second, we encode the semantic roles of
NPj and NPk as one of five possible values: ARG0-
ARG0, ARG1-ARG1, ARG0-ARG1, ARG1-ARG0,
and OTHERS (the default case).2 Finally, we create
15 binary-valued features by pairing the 3 possible
values extracted from FrameNet and the 5 possible
values provided by ASSERT. Since these features
2We focus primarily on ARG0 and ARG1 because they are
the most important core arguments of a predicate and may pro-
vide more useful information than other semantic roles.
818
are computed over two NPs, we can employ them di-
rectly for the MP model. Note that by construction,
exactly one of these features will have a non-zero
value. For the CR model, we extend their definitions
so that they can be computed between an NP, NPk,
and a preceding cluster, c. Specifically, the value of
a feature is 1 if and only if its value between NPk and
one of the NPs in c is 1 under its original definition.
The above discussion assumes that the two NPs
under consideration serve as predicate arguments. If
this assumption fails, we will not create any features
based on FrameNet for these two NPs.
To our knowledge, FrameNet has not been ex-
ploited for coreference resolution. However, the
use of related verbs is similar in spirit to Bean and
Riloff?s (2004) use of patterns for inducing contex-
tual role knowledge, and the use of semantic roles is
also discussed in Ponzetto and Strube (2006).
3.2 World Knowledge from Annotated Data
Since world knowledge is needed for coreference
resolution, a human annotator must have employed
world knowledge when coreference-annotating a
document. We aim to design features that can ?re-
cover? such world knowledge from annotated data.
3.2.1 Features Based on Noun Pairs
A natural question is: what kind of world knowl-
edge can we extract from annotated data? We may
gather the knowledge that Barack Obama is a U.S.
president if we see these two NPs appearing in the
same coreference chain. Equally importantly, we
may gather the commonsense knowledge needed for
determining non-coreference. For instance, we may
discover that a lion and a tiger are unlikely to refer
to the same real-world entity after realizing that they
never appear in the same chain in a large number of
annotated documents. Note that any features com-
puted based on WordNet distance or distributional
similarity are likely to incorrectly suggest that lion
and tiger are coreferent, since the two nouns are sim-
ilar distributionally and according to WordNet.
Given these observations, one may collect the
noun pairs from the (coreference-annotated) train-
ing data and use them as features to train a resolver.
However, for these features to be effective, we need
to address data sparseness, as many noun pairs in
the training data may not appear in the test data.
To improve generalization, we instead create dif-
ferent kinds of noun-pair-based features given an
annotated text. To begin with, we preprocess each
document. A training text is preprocessed by ran-
domly replacing 10% of its common nouns with the
label UNSEEN. If an NP, NPk , is replaced with UN-
SEEN, all NPs that have the same string as NPk will
also be replaced with UNSEEN. A test text is prepro-
cessed differently: we simply replace all NPs whose
strings are not seen in the training data with UN-
SEEN. Hence, artificially creating UNSEEN labels
from a training text will allow a learner to learn how
to handle unseen words in a test text.
Next, we create noun-pair-based features for the
MP model, which will be used to augment the Base-
line feature set. Here, each instance corresponds to
two NPs, NPj and NPk , and is represented by three
groups of binary-valued features.
Unseen features are applicable when both NPj
and NPk are UNSEEN. Either an UNSEEN-SAME fea-
ture or an UNSEEN-DIFF feature is created, depend-
ing on whether the two NPs are the same string be-
fore being replaced with the UNSEEN token.
Lexical features are applicable when neither NPj
nor NPk is UNSEEN. A lexical feature is an ordered
pair consisting of the heads of the NPs. For a pro-
noun or a common noun, the head is the last word of
the NP; for a proper name, the head is the entire NP.
Semi-lexical features aim to improve generaliza-
tion, and are applicable when neither NPj nor NPk is
UNSEEN. If exactly one of NPj and NPk is tagged
as a NE by the Stanford NE recognizer, we create
a semi-lexical feature that is identical to the lexical
feature described above, except that the NE is re-
placed with its NE label. On the other hand, if both
NPs are NEs, we check whether they are the same
string. If so, we create a *NE*-SAME feature, where
*NE* is replaced with the corresponding NE label.
Otherwise, we check whether they have the same NE
tag and a word-subset match (i.e., whether the word
tokens in one NP appears in the other?s list of word
tokens). If so, we create a *NE*-SUBSAME feature,
where *NE* is replaced with their NE label. Other-
wise, we create a feature that is the concatenation of
the NE labels of the two NPs.
The noun-pair-based features for the CR model
can be generated using essentially the same method.
Specifically, since each instance now corresponds to
819
an NP, NPk, and a preceding cluster, c, we can gener-
ate a noun-pair-based feature by applying the above
method to NPk and each of the NPs in c, and its value
is the number of times it is applicable to NPk and c.
3.2.2 Features Based on Verb Pairs
As discussed above, features encoding the seman-
tic roles of two NPs and the relatedness of the asso-
ciated verbs could be useful for coreference resolu-
tion. Rather than encoding verb relatedness, we may
replace verb relatedness with the verbs themselves
in these features, and have the learner learn directly
from coreference-annotated data whether two NPs
serving as the objects of decry and denounce are
likely to be coreferent or not, for instance.
Specifically, assuming that NPj and NPk are the
arguments of two stemmed predicates, predj and
predk , in the training data, we create five features
as follows. First, we encode the semantic roles of
NPj and NPk as one of five possible values: ARG0-
ARG0, ARG1-ARG1, ARG0-ARG1, ARG1-ARG0,
and OTHERS (the default case). Second, we create
five binary-valued features by pairing each of these
five values with the two stemmed predicates. Since
these features are computed over two NPs, we can
employ them directly for the MP model. Note that
by construction, exactly one of these features will
have a non-zero value. For the CR model, we extend
their definitions so that they can be computed be-
tween an NP, NPk , and a preceding cluster, c. Specif-
ically, the value of a feature is 1 if and only if its
value between NPk and one of the NPs in c is 1 un-
der its original definition.
The above discussion assumes that the two NPs
under consideration serve as predicate arguments. If
this assumption fails, we will not create any features
based on verb pairs for these two NPs.
3.3 World Knowledge from Unannotated Data
Previous work has shown that syntactic apposi-
tions, which can be extracted using heuristics from
unannotated documents or parse trees, are a useful
source of world knowledge for coreference resolu-
tion (e.g., Daume? III and Marcu (2005), Ng (2007),
Haghighi and Klein (2009)). Each extraction is an
NP pair such as <Barack Obama, the president>
and <Eastern Airlines, the carrier>, where the first
NP in the pair is a proper name and the second NP is
a common NP. Low-frequency extractions are typi-
cally assumed to be noisy and discarded.
We combine the extractions produced by Fleis-
chman et al (2003) and Ng (2007) to form a
database consisting of 1.057 million NP pairs, and
create a binary-valued feature for our coreference
models using this database. If the MP model is used,
this feature will have the value 1 if and only if the
two NPs appear as a pair in the database. On the
other hand, if the CR model is used, the feature for
an instance involving NPk and preceding cluster c
will have the value 1 if and only if NPk and at least
one of the NPs in c appears as a pair in the database.
4 Evaluation
4.1 Experimental Setup
As described in Section 2, we use as our evalua-
tion corpus the 411 documents that are coreference-
annotated using the ACE and OntoNotes annota-
tion schemes. Specifically, we divide these docu-
ments into five (disjoint) folds of roughly the same
size, training the MP model and the CR model us-
ing SVMlight on four folds and evaluate their per-
formance on the remaining fold. The linguistic fea-
tures, as well as the NPs used to create the training
and test instances, are computed automatically. We
employ B3 and CEAF as described in Section 2.3 to
score the output of a coreference system.
4.2 Results and Discussion
4.2.1 Baseline Models
Since our goal is to evaluate the effectiveness of
the features encoding world knowledge for learning-
based coreference resolution, we employ as our
baselines the MR model and the CR model trained
on the Baseline feature set, which does not con-
tain any features encoding world knowledge. For
the MP model, the Baseline feature set consists of
the 39 features described in Section 2.3.1; for the
CR model, the Baseline feature set consists of the
cluster-level features derived from the 39 features
used in the Baseline MP model (see Section 2.3.2).
Results of the MP model and the CR model em-
ploying the Baseline feature set are shown in rows 1
and 8 of Table 1, respectively. Each row contains the
B3 and CEAF results of the corresponding corefer-
ence model when it is evaluated using the ACE and
820
ACE OntoNotes
B3 CEAF B3 CEAF
Feature Set R P F R P F R P F R P F
Results for the Mention-Pair Model
1 Base 56.5 69.7 62.4 54.9 66.3 60.0 50.4 56.7 53.3 48.9 54.5 51.5
2 Base+YAGO Types (YT) 57.3 70.3 63.1 58.7 67.5 62.8 51.7 57.9 54.6 50.3 55.6 52.8
3 Base+YAGO Means (YM) 56.7 70.0 62.7 55.3 66.5 60.4 50.6 57.0 53.6 49.3 54.9 51.9
4 Base+Noun Pairs (WP) 57.5 70.6 63.4 55.8 67.4 61.1 51.6 57.6 54.4 49.7 55.4 52.4
5 Base+FrameNet (FN) 56.4 70.9 62.8 54.9 67.5 60.5 50.5 57.5 53.8 48.8 55.1 51.8
6 Base+Verb Pairs (VP) 56.9 71.3 63.3 55.2 67.6 60.8 50.7 57.9 54.0 49.0 55.4 52.0
7 Base+Appositives (AP) 56.9 70.0 62.7 55.6 66.9 60.7 50.3 57.1 53.5 49.1 55.1 51.9
Results for the Cluster-Ranking Model
8 Base 61.7 71.2 66.1 59.6 68.8 63.8 53.4 59.2 56.2 51.1 57.3 54.0
9 Base+YAGO Types (YT) 63.5 72.4 67.6 61.7 70.0 65.5 54.8 60.6 57.6 52.4 58.9 55.4
10 Base+YAGO Means (YM) 62.0 71.4 66.4 59.9 69.1 64.1 53.9 59.5 56.6 51.4 57.5 54.3
11 Base+Noun Pairs (WP) 64.1 73.4 68.4 61.3 70.1 65.4 55.9 62.1 58.8 53.5 59.1 56.2
12 Base+FrameNet (FN) 61.8 71.9 66.5 59.8 69.3 64.2 53.5 60.0 56.6 51.1 57.9 54.3
13 Base+Verb Pairs (VP) 62.1 72.2 66.8 60.1 69.3 64.4 54.4 60.1 57.1 51.9 58.2 54.9
14 Base+Appositives (AP) 63.1 71.7 67.1 60.5 69.4 64.6 54.1 60.1 56.9 51.9 57.8 54.7
Table 1: Results obtained by applying different types of features in isolation to the Baseline system.
ACE OntoNotes
B3 CEAF B3 CEAF
Feature Set R P F R P F R P F R P F
Results for the Mention-Pair Model
1 Base 56.5 69.7 62.4 54.9 66.3 60.0 50.4 56.7 53.3 48.9 54.5 51.5
2 Base+YT 57.3 70.3 63.1 58.7 67.5 62.8 51.7 57.9 54.6 50.3 55.6 52.8
3 Base+YT+YM 57.8 70.9 63.6 59.1 67.9 63.2 52.1 58.3 55.0 50.8 56.0 53.3
4 Base+YT+YM+WP 59.5 71.9 65.1 57.5 69.4 62.9 53.1 59.2 56.0 51.5 57.1 54.1
5 Base+YT+YM+WP+FN 59.6 72.1 65.3 57.2 69.7 62.8 53.1 59.5 56.2 51.3 57.4 54.2
6 Base+YT+YM+WP+FN+VP 59.9 72.5 65.6 57.8 70.0 63.3 53.4 59.8 56.4 51.8 57.7 54.6
7 Base+YT+YM+WP+FN+VP+AP 59.7 72.4 65.4 57.6 69.8 63.1 53.2 59.8 56.3 51.5 57.6 54.4
Results for the Cluster-Ranking Model
8 Base 61.7 71.2 66.1 59.6 68.8 63.8 53.4 59.2 56.2 51.1 57.3 54.0
9 Base+YT 63.5 72.4 67.6 61.7 70.0 65.5 54.8 60.6 57.6 52.4 58.9 55.4
10 Base+YT+YM 63.9 72.6 68.0 62.1 70.4 66.0 55.2 61.0 57.9 52.8 59.1 55.8
11 Base+YT+YM+WP 66.1 75.4 70.4 62.9 72.4 67.3 57.7 64.4 60.8 55.1 61.6 58.2
12 Base+YT+YM+WP+FN 66.3 75.1 70.4 63.1 72.3 67.4 57.3 64.1 60.5 54.7 61.2 57.8
13 Base+YT+YM+WP+FN+VP 66.6 75.9 70.9 63.5 72.9 67.9 57.7 64.4 60.8 55.1 61.6 58.2
14 Base+YT+YM+WP+FN+VP+AP 66.4 75.7 70.7 63.3 72.9 67.8 57.6 64.3 60.8 55.0 61.5 58.1
Table 2: Results obtained by adding different types of features incrementally to the Baseline system.
OntoNotes annotations as the gold standard. As we
can see, the MP model achieves F-measure scores of
62.4 (B3) and 60.0 (CEAF) on ACE and 53.3 (B3)
and 51.5 (CEAF) on OntoNotes, and the CR model
achieves F-measure scores of 66.1 (B3) and 63.8
(CEAF) on ACE and 56.2 (B3) and 54.0 (CEAF)
on OntoNotes. Also, the results show that the CR
model is stronger than the MP model, corroborating
previous empirical findings (Rahman and Ng, 2009).
4.2.2 Incorporating World Knowledge
Next, we examine the usefulness of world knowl-
edge for coreference resolution. The remaining rows
in Table 1 show the results obtained when different
types of features encoding world knowledge are ap-
plied to the Baseline system in isolation. The best
result for each combination of data set, evaluation
measure, and coreference model is boldfaced.
Two points deserve mention. First, each type
of features improves the Baseline, regardless of the
coreference model, the evaluation measure, and the
annotation scheme used. This suggests that all these
feature types are indeed useful for coreference reso-
lution. It is worth noting that in all but a few cases
involving the FrameNet-based and appositive-based
features, the rise in F-measure is accompanied by a
821
1. The Bush White House is breeding non-duck ducks the same way the Nixon White House did: It hops on an
issue that is unopposable ? cleaner air, better treatment of the disabled, better child care. The President came
up with a good bill, but now may end up signing the awful bureaucratic creature hatched on Capitol Hill.
2. The tumor, he suggested, developed when the second, normal copy also was damaged. He believed colon
cancer might also arise from multiple ?hits? on cancer suppressor genes, as it often seems to develop in stages.
Table 3: Examples errors introduced by YAGO and FrameNet.
simultaneous rise in recall and precision. This is per-
haps not surprising: as the use of world knowledge
helps discover coreference links, recall increases;
and as more (relevant) knowledge is available to
make coreference decisions, precision increases.
Second, the feature types that yield the best im-
provement over the Baseline are YAGO TYPE and
Noun Pairs. When the MP model is used, the best
coreference system improves the Baseline by 1?
1.3% (B3) and 1.3?2.8% (CEAF) in F-measure. On
the other hand, when the CR model is used, the best
system improves the Baseline by 2.3?2.6% (B3) and
1.7?2.2% (CEAF) in F-measure.
Table 2 shows the results obtained when the dif-
ferent types of features are added to the Baseline one
after the other. Specifically, we add the feature types
in this order: YAGO TYPE, YAGO MEANS, Noun
Pairs, FrameNet, Verb Pairs, and Appositives. In
comparison to the results in Table 1, we can see that
better results are obtained when the different types
of features are applied to the Baseline in combina-
tion than in isolation, regardless of the coreference
model, the evaluation measure, and the annotation
scheme used. The best-performing system, which
employs all but the Appositive features, outperforms
the Baseline by 3.1?3.3% in F-measure when the
MR model is used and by 4.1?4.8% in F-measure
when the CR model is used. In both cases, the
gains in F-measure are accompanied by a simulta-
neous rise in recall and precision. Overall, these
results seem to suggest that the CR model is mak-
ing more effective use of the available knowledge
than the MR model, and that the different feature
types are providing complementary information for
the two coreference models.
4.3 Example Errors
While the different types of features we considered
improve the performance of the Baseline primarily
via the establishment of coreference links, some of
these links are spurious. Sentences 1 and 2 of Table
3 show the spurious coreference links introduced by
the CR model when YAGO and FrameNet are used,
respectively. In sentence 1, while The President and
Bush are coreferent, YAGO caused the CR model
to establish the spurious link between The President
and Nixon owing to the proximity of the two NPs
and the presence of this NP pair in the YAGO TYPE
relation. In sentence 2, FrameNet caused the CR
model to establish the spurious link between The tu-
mor and colon cancer because these two NPs are the
ARG0 arguments of develop and arise, which appear
in the same semantic frame in FrameNet.
5 Conclusions
We have examined the utility of three major
sources of world knowledge for coreference resolu-
tion, namely, large-scale knowledge bases (YAGO,
FrameNet), coreference-annotated data (Noun Pairs,
Verb Pairs), and unannotated data (Appositives), by
applying them to two learning-based coreference
models, the mention-pair model and the cluster-
ranking model, and evaluating them on documents
annotated with the ACE and OntoNotes annotation
schemes. When applying the different types of fea-
tures in isolation to a Baseline system that does not
employ world knowledge, we found that all of them
improved the Baseline regardless of the underlying
coreference model, the evaluation measure, and the
annotation scheme, with YAGO TYPE and Noun
Pairs yielding the largest performance gains. Nev-
ertheless, the best results were obtained when they
were applied in combination to the Baseline system.
We conclude from these results that the different fea-
ture types we considered are providing complemen-
tary world knowledge to the coreference resolvers,
and while each of them provides fairly small gains,
their cumulative benefits can be substantial.
822
Acknowledgments
We thank the three reviewers for their invaluable
comments on an earlier draft of the paper. This work
was supported in part by NSF Grant IIS-0812261.
References
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In Proceedings of the Lin-
guistic Coreference Workshop at The First Interna-
tional Conference on Language Resources and Eval-
uation, pages 563?566.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceed-
ings of the 36th Annual Meeting of the Association for
Computational Linguistics and the 17th International
Conference on Computational Linguistics, Volume 1,
pages 86?90.
David Bean and Ellen Riloff. 2004. Unsupervised learn-
ing of contextual role knowledge for coreference reso-
lution. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter of
the Association for Computational Linguistics, pages
297?304.
Eric Bengtson and Dan Roth. 2008. Understanding the
values of features for coreference resolution. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 294?303.
Volha Bryl, Claudio Giuliano, Luciano Serafini, and
Kateryna Tymoshenko. 2010. Using background
knowledge to support coreference resolution. In Pro-
ceedings of the 19th European Conference on Artificial
Intelligence, pages 759?764.
Eugene Charniak. 1972. Towards a Model of Children?s
Story Comphrension. AI-TR 266, Artificial Intelli-
gence Laboratory, Massachusetts Institute of Technol-
ogy.
Aron Culotta, Michael Wick, and Andrew McCallum.
2007. First-order probabilistic models for coreference
resolution. In Human Language Technologies 2007:
The Conference of the North American Chapter of the
Association for Computational Linguistics; Proceed-
ings of the Main Conference, pages 81?88.
Hal Daume? III and Daniel Marcu. 2005. A large-scale
exploration of effective global features for a joint en-
tity detection and tracking model. In Proceedings of
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 97?104.
Pascal Denis and Jason Baldridge. 2008. Specialized
models and ranking for coreference resolution. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 660?669.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics, pages
363?370.
Michael Fleischman, Eduard Hovy, and Abdessamad
Echihabi. 2003. Offline strategies for online ques-
tion answering: Answering questions before they are
asked. In Proceedings of the 41st Annual Meeting of
the Association for Computational Linguistics, pages
1?7.
Aria Haghighi and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1152?1161.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
The 90% solution. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, Com-
panion Volume: Short Papers, pages 57?60.
Thorsten Joachims. 2002. Optimizing search engines us-
ing clickthrough data. In Proceedings of the Eighth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 133?142.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based
on the Bell tree. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Linguis-
tics, pages 135?142.
Xiaoqiang Luo. 2005. On coreference resolution perfor-
mance metrics. In Proceedings of Human Language
Technology Conference and Conference on Empirical
Methods in Natural Language Processing, pages 25?
32.
Ruslan Mitkov. 2002. Anaphora Resolution. Longman.
Vincent Ng and Claire Cardie. 2002. Improving machine
learning approaches to coreference resolution. In Pro-
ceedings of the 40th Annual Meeting of the Association
for Computational Linguistics, pages 104?111.
Vincent Ng. 2007. Shallow semantics for coreference
resolution. In Proceedings of the Twentieth Inter-
national Joint Conference on Artificial Intelligence,
pages 1689?1694.
Simone Paolo Ponzetto and Massimo Poesio. 2009.
State-of-the-art NLP approaches to coreference reso-
lution: Theory and practical recipes. In Tutorial Ab-
stracts of ACL-IJCNLP 2009, page 6.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In Proceedings
823
of the Human Language Technology Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 192?199.
Sameer S. Pradhan, Wayne H. Ward, Kadri Hacioglu,
James H. Martin, and Dan Jurafsky. 2004. Shallow
semantic parsing using support vector machines. In
Proceedings of the Human Language Technology Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics, pages 233?240.
Altaf Rahman and Vincent Ng. 2009. Supervised mod-
els for coreference resolution. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 968?977.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational Lin-
guistics, 27(4):521?544.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase coref-
erence resolution: Making sense of the state-of-the-
art. In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 656?664.
Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen
Riloff, David Buttler, and David Hysom. 2010. Coref-
erence resolution with Reconcile. In Proceedings of
the ACL 2010 Conference Short Papers, pages 156?
161.
Fabian Suchanek, Gjergji Kasneci, and Gerhard Weikum.
2007. YAGO: A core of semantic knowledge unifying
wordnet and wikipedia. In Proceedings of the World
Wide Web Conference, pages 697?706.
Olga Uryupina, Massimo Poesio, Claudio Giuliano, and
Kateryna Tymoshenko. 2011. Disambiguation and
filtering methods in using web knowledge for coref-
erence resolution. In Proceedings of the 24th Interna-
tional Florida Artificial Intelligence Research Society
Conference.
Xiaofeng Yang, Guodong Zhou, Jian Su, and Chew Lim
Tan. 2003. Coreference resolution using competitive
learning approach. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguis-
tics, pages 176?183.
Xiaofeng Yang, Jian Su, Jun Lang, Chew Lim Tan, and
Sheng Li. 2008. An entity-mention model for coref-
erence resolution with inductive logic programming.
In Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 843?851.
824
