Mul t i l i ngua l i ty  in a Text  Generat ion  System 
For Three  Slavic Languages  
Geert-Jan Kruijff a, Elke Teich t', John Bateman ~, Ivana Kruijit;Korbayovfi", 
Hana Skoumalovg ~,Serge Sharoff 'l, Lena Sokolova d, Tony Hartley ~, 
Kamenka Staykova/, Ji~'~ Hana" 
?Charles University, Prague; ~University of the Saarland; ~University of Bremen; 
aRRIAI, Moscow; ?University of Brighton; /IIT, BAS, Sofia 
http://www.itri.brighton.ac.uk/projects/agile/ 
Abstract 
This paper describes a lnultilingual text generation 
system in the domain of CAD/CAM software in-- 
structions tbr Bulgarian, Czech and l:\[ussian. Start- 
ing from a language-independent semantic represen- 
tation, the system drafts natural, continuous text 
as typically found in software inammls. The core 
modules for strategic and tactical gene,'ation are im- 
plemented using the KPML platform for linguistic 
resource development and generation. Prominent 
characteristics of the approach implemented a.re a 
treatment of multilinguality that makes maximal use 
of the cominonalities between languages while also 
accounting for their differences and a common repre- 
sentational strategy for both text planning and sen- 
tence generation. 
1 In t roduct ion  
This paper describes the Agile system I tbr the 
multilingual generation of instructional texts as 
found in soft;ware user-manuals in Bulgarian, 
Czech and Russian. The current prototype fo- 
cuses on the automatic drafting of CAD/CAM 
software documentation; routine passages as 
found in the AutoCAD user-manual have been 
taken as target texts. The application sce- 
nario of the Agile system is as follows. First, 
a user constructs, with the help of a GUI, 
language-independent task models that spec- 
ify the contents of the documentation to be 
generated. The user additionally specifies the 
language (currently Bulgarian, Czech or Rus- 
sian) and the register of the text to be gen- 
erated. The Agile system then produces con- 
tinuous instructional texts realizing the speci- 
fied content and conforming to the style of soft- 
ware user-manuals. The texts produced are 
1EU Inco-Copernicus project PL961004: 'Automatic 
Generation of Instructional Texts in the Languages of 
Eastern Europe' 
intended to serve as drafts for final revision; 
this ~drafting' scenario is therefbre analogous to 
that first explored within the Drafter project. 
Within the Agile project, however, we have ex- 
plored a more thoroughly nmltilingual architec- 
ture, making substantial use of existing linguis- 
tic resources and components. 
The design of the Agile system overall re, sts 
on the following three assumI)tions. 
First, the input of the system should be spec- 
ified irrespective of any particular output lan- 
guage. This means that the user must be able to 
express the content that she wants the texts to 
convey, irrespective of what natural language(s) 
she masters and in what language(s) the out- 
put text shouM be realized. Such language- 
independent content specification can take the 
form of some knowledge representation pertain- 
ing to the application domain. 
Second, the texts generated as the outtmt of 
the system should be well-formulated with re- 
spect to the expectations of natiw. ? speakers of 
each particular language covered by the system. 
Since differences among languages may appear 
at any level, language-sensitive d cisions about 
the realization of the specified content must be 
possible throughout he generation process. 
And third, the notion of multilinguality em- 
ployed in the system should be recursive, in 
the sense that the modules responsible tbr the 
generation should themselves be multilingual. 
The text generation tasks which are common 
to the languages under consideration should be 
pertbrmed only once. Ideally, there should be 
one process of generation yielding output in 
multiple languages rather than a sequence of 
monolingual processes. This view of 'intrin- 
sic multilinguality' builds on the approach set 
out in Bateman et al (1999). Each module of 
the system is fnlly multilingual in that it simul- 
474 
taneously enables both integration of linguistic 
resources, defining commonalities bel;ween lan- 
guages, and resource integrity, in |;bat the in- 
dividuality of each of the language-speeitic re- 
sources of a multilingnM ensemble is always pre- 
served. 
We consider these assuml)l;ions an(l the view 
of multilinguality entailed by |;hem to be cru- 
cial for the design of efli;ctive multilingual text 
generation systems. The results so far a(:hicved 
by the Agile system SUl)port this and also ofl'er 
a ~soli(l experiential basis tbr the develot)mcnt of 
fllrther multilingnal generation systems. 
The overall operation of 1;t1(; Agile sysl;em is 
as tbllows. Al/tcr the us(u' has Sl)ecilied some 
inl;en(led text (;OlltenI; (described in Section 2) 
via the Agile GUI, the system i)ro(:eeds to gen- 
eral;e the texts required. To do this, a text 
t)lammr (Section 3) first assigns parts of the, 
task model to text elements and arranges l;h(;m 
in a hierarchical fashion a text t)lan. Then, a 
sentence plammr organizes I;he content of the 
text elements into sentence-sized elml~ks and 
ere~,tes the corresponding input fin' l;he tacti- 
ca,1 generator, expressed in standard sentence 
l)lamfing language (SPI,) lbrmulae. Finally, 1;11(; 
tactical g(meral;or generates t;he linguistic real- 
izations corresponding 1;o these Sl)l~s the text 
(Sect;ion 4). In the stage of the l)rojccI; rt}l)orte(l 
here, we, conceal;rated i)arl;icularly on \])roccdu- 
ral texts. These otlhr sl;el)-by-st;e t) des(:rit)t;ions 
of how to perlbrm domain tasks using the given 
software tools. A simplified version of one such 
procedural text is given (tbr English) in Fig- 
ure 1. This architectm:e mirrors the reference 
architecture for generation diseusse(t in I/,eiter 
8z Dale (1.997). The modules of the system are 
1)ipelined so that a continuous text is generated 
realizing the intended meaning of the inlmt se- 
mantic representation without backtracking or 
revision. 
Several important properties have ('haracter- 
ized the method of development leading to the 
Agile system. These are to a large extent re- 
sponsible for the eflhetiveness of the system. 
These include: 
Re-use  and  adaptat ion  o f  ava i lab le  re-  
sources .  We have re-used snt)stantial bodics 
of e, xisting linguistic resources at all levels rel- 
evant for the system; this t)laye(l a (:rueial role 
in achieving the Sol)histieatcd generation capa- 
7b d~nw a polylinc 
First start the PLINE command using one of these meth- 
ods: 
Windows From the Polylinc tlyout on the, l)raw tool~ 
lmr, choose Polylinc. 
DOS and UNIX  lqom the Draw menu, choose Poly- 
line. 
1.. Spccit~y the start point of the polyline. 
2. S1)ecil~y tim next point of the 1)olylinc. 
3. Press ll,cturn t;o end the polyline. 
Figure l: Example "To draw a polyline" 
bilities now displayed by the system in each of 
its languages of expertise prior to the project 
t\]m'l.'e were 11o substantial ~mtomatic generation 
systenls fi)r any of the languages covered. The 
core modules for strategic and ta(:ti('al gener- 
ation were all imt)lemcnted using the Kernel- 
Penman Multilingual system (KPML: ef Bate- 
man et al, \]999) a Common l,isp base(t gram- 
mar development environment, in addition, 
we adopted the Pemnan Upt)er Model as used 
within Pemnan/KPMl~ as the basis tbr our 
linguistic semantics; a more rcstri(:ted domain 
model (DM) rclewmt o the CAD/CAM-domain 
was &',lined as a st)e('ialization of l;he UM con- 
(:epts. The I)M was iuspired by the domain 
me(tel of the Drafter l)rojet:t, but l)res(ml;s a 
g(m(',ralizati()n ()f the latter in that it allows for 
eml)(;d(ling t:asks and illsLrut'|;ions t:o any arlfi- 
l;rm:y re(:ursive depth (i.e., more complex l;cxt; 
plans). Ah'eady existing lexical resom:ces and 
morphological modules availabh; to the 1)ro.j(',ct 
were re-used tbr Bulgarian, Czech and l~.ussian: 
the Czech and Bulgarian components were mo(t- 
ules written in C (e.g., IIaji(: L; Hla(lk~, 1997, 
tbr Czech) that were interfimed with KPMI, us- 
ing a standard set of API-methods (of. Bate- 
man & Sharoff, 1998). Finally, because no 
grammars uitable for generation in Bulgarian, 
Czech and l/.ussia,n existed, a grammar tbr En- 
glish (NIGEL: Mann & Matthiessen, 1985) was 
re-used to lmild them; tbr the theoretical basis 
of this technique see Teich (1995). 
Combinat ion  o f  two  methods  o f  resources  
deve lopment .  Two methods  were com- 
bined to enable us to develop basic general- 
language grammars and sublanguage grammars 
fin: CAD/CAM instructional texts at; |;11(; same 
time. One nmthod is the system-oriented one 
aimed at lmildiug a computational resource 
475 
with a view of the whole language system: this 
is a method strongly supported by the KPML 
development environment. The other method 
is instance-oriented, and is guided by a detailed 
register analysis. The latter method was partic- 
ularly important given the Agile goal of being 
able to generate texts belonging to rather di- 
verse text types- -  e.g., impersonal vs. personal; 
procedural, flmetional descriptions, overviews 
etc. 
Cross-linguistic resource-sharing. A cross- 
linguistic approach to linguistic specifications 
and implementation was taken by maximizing 
resource sharing, i.e. taking into account sim- 
ilarities and differences among the treated lan- 
guages o that development tasks have been dis- 
tributed across different languages and re-used 
wherever possible. 
2 Language- independent  Content 
Specif icat ions 
The content constructed by a user via the Ag- 
ile GUI is specified in terms of Assertion-bozes 
or A-boxes. These A-boxes are considered to 
be entirely neutral with respect o the language 
that will be used to express the A-box's con- 
tent. Thus individual A-boxes can be used for 
generating multiple languages. A-boxes spec- 
i(y content by instantiating concepts from ~,he 
DM or UM, and placing these concepts in rela- 
tion to one another by means of configurational 
concepts. The configurational concepts define 
adnfissible ways in which content can be struc- 
tured. Figure 2 gives the configurational con- 
cepts distinguished within Agile. 
Procedure A procedure has three slots: 
(i) GOAL (obligatory,filled by a USER-AcTION), 
(ii) METIIODS (optional, filled by a METHOD-LIsT), 
(iii) SIDE-EPFECT (optional, filled by a USER- 
EVENT). 
Method A method has three slots: 
(i) CONSTRAINT (optionM, filled by an OPERATING- 
SYSTEM), 
(ii) PaEeONDITION (optional, filled by a PROCE- 
DURE), 
(iii) SUUSTEPS (obligatory, filled by a PI~OCEDUI/E- 
LIST). 
Method-List A METIIOD-LIST is a list of h/IETIIOD'S. 
Procedure-List A PROCEI)URE-LIST is a list of 
PROCEDURE:S. 
Figure 2: Configurational concepts 
Configurational concepts are devoid of actual 
content. Tile content is provided by inst, antia- 
tions of concepts that represent various user ac- 
tions, interface events, and interface modalities 
and functions. Taken together, these instanti- 
ations provide the basic propositional content 
tbr instructional texts and are taken as input 
tbr the text planning process. 
3 Strategic Generat ion: From 
Content  Specif icat ions to Sentence 
Plans 
To realize an A-box as a text, we go through suc- 
cessive stages of text planning, sentence plan- 
ning, and lexico-grammatical generation (cf 
also Reiter & Dale, 1997). At each stage there 
is an increase in sensitivity to, or dependency 
on, the target language in which output will 
be generated. Although the text planner itself 
is language-independent, the text; plamfing re- 
sources may (lifter fl'om language to language 
as much as is required. This is exactly analo- 
gous to the situation we find within the individ- 
ual language grammars as represented within 
KPML: we therefore represent the text planning 
resources in the same fashion. For the text type 
and languages of concern here, however, w~ria- 
lion across languages at the text planning stage 
turned out to be minimal. 
The organization of an A-box is used to guide 
the text planning process. Itere, we draw a dis- 
tinction between text structure elements (TSEs) 
as the elements from which a (task-oriented) 
text, is built ut), and text templates', which con- 
dition the way TSEs are to be realized linguis- 
tically. We locus on the relation between con- 
cepts on the one hand, and TSEs on the other. 
We are specifically interested in the configura- 
tional concepts that are used to configure the 
content specified in an A-box because we want 
to maintain a close connection between how the 
content can be defined in an A-box and how 
that content is to be spelled out in text. 
3.1 Structuring and Styling 
A text structure element is a predefined com- 
ponent that needs to be filled by one or more 
specific parts of the user's content definition. 
Using the reader-oriented terminology common 
in technical authoring guides, we distinguish 
a small (recursively defined) set of text TSEs; 
these are listed in Figure 3. 
476 
Task-Docmnent  A TASK-\])OCUMFNT has tWO slots: 
(i) TASK-TFI'I,E (ol)ligatory), 
(ii) TASK-INSTI{U(ITIONS (obligatory), being a list 
of" at least one ~\[NSTRUCTION. 
Instruction An INSTRUCTION has three slots: 
(i) TASKS (obligatory), being a list of at least one 
TASK~ 
(ii) CONSTRAINT (optional), 
(iii) Pm,ZCONDITION (optional). 
Task  A TASK has two slots: 
(i) INSTRUCTIONS (ol)tional), 
(ii) SII)I';-EI,'I"I,:C'I' (ol)tional). 
Figure 3: Text Structure Elements (TSEs) 
The TSEs are placed in correspondence with 
the configurational concet)ts of the DM (cf. Fig- 
ure 2); this enat)les us to lmild a text stru('ture 
l;hat folh)ws the structuring of the content in an 
A-1)ox (cf. Figure 4). 
Orthogonal to the notion of text structure l- 
ement is the notion of text temt)late. Whereas 
TSEs capture what needs to be realized, the 
text template (:al)tures how that content is to 
1)e realized. Thus, a feint)late defines a style 
for expressing the content. Am we discuss be- 
low, we define text templates in terms of con- 
straints on the realization of si)e(:iti(" (in(tivid- 
ual) TSEs. D)r examt)le, whereas in Bulgarian 
and Czech headings (to which the '\]'ASK-TITLE 
element corresponds: of. Figure 4) are usually 
realized as nominal groups, in the Russian Au- 
toCAD ulallnal headings are realized as nonii- 
nile purpose clauses as they are ill English. 
3.2 Tex~ P lann ing  g~ Sentence  P lann ing  
The major component of the text pbmner is 
fi)rnmd by a systemic network fi)r text struc- 
turing; this network, called the text structur- 
ing region, defines an additional level of linguis- 
tic resources for the level of genre. This region 
constructs text structures in a way that is very 
similar to the way in which the systemic net- 
works of the grammars of the tactical genera- 
|or build up grammatical structures. In fact, 
by using KPML to implement his means for 
text structuring, the interaction between global 
level text generation (strategic generation) and 
lexico-grammatical expression (tactical genera- 
tion) is greatly facilitated. Moreover, this al)- 
t)roach has the advantage |;tint constraints on 
output realization can 1)e easily accmnulated 
and propagated: for example, the text plan- 
ner can iml)ose constraints on the output lexico- 
grammatical realization of particular text t)lan 
elements, such am the realization of text head- 
ings by a nominalization ill Czech and Bulgar- 
|an or by an infinite purpose clause in Rus- 
sian. This is one contribution to overcoming the 
notorious generation gap prol)leln caused when 
a text planning module lacks control over the 
line-grained istinctions that m'e available in a 
grmmnar. Ill our case, both text plamfing and 
sentence planning are integrated into one and 
the same system and are distinguished by strat- 
ification. 
TASK-TITLE ~-} GOAl, of topmost  PROCEDURE 
TASK-INSTRUCTIONS ~-} METIIODS of PROCEDUI/E 
Sll)E-EIq,'ECT ~ SIDhl-EFFI~CT of PROCEDUII.I\] 
TASK /-~ GOAL of PROCEI)IHtI,; 
(-JONSTRA1NT <-} CONSTRAINT of ~41,VI'IIO1) 
PRECONI)ITION ~ PIH?COND1TION of ~,4ETI1OI) 
1NSTIIUCTI(IN-TAsKS 1--} SUBSTH)S of a METIIOD 
INST1HJCTION +5 MI,TI'IIOD 
Figure 4: Mapping TSEs and configurational 
concepts defined in the DM 
Following on from the orthogomflity of text 
t/;mplates and text structure elements, the text 
structuring region consists of two parts. One 
1)arl; deals wil;h interpreting the A-box in terms 
of TSEs: traversing l;he network of this part of 
the region produces a text structure for the A- 
b/lx contbrufing to the definitions above. The 
second part of the region imposes constraints 
on the realization of the TSEs introduced by 
the first part. Divers(; constraints can be ira- 
posed depending on the user's choice of style, 
e.g., personal (featuring ppredominantly imper- 
atives) vs. impersonal (tbaturing indicatives). 
Tile result of text plmming is a text plan. 
This can be thought of as a hierarchical struc- 
ture (built by TSEs) with lilts of A-box content 
at; its leaves together with additional constraints 
imposed by the text planning process: e.g., that 
the Title segment of the document should not be 
realized as a full (:lause but; rather as a nominal 
phrase or a lmrt)osive det)endent clause. The 
text plan may also include constraints on pre- 
ferred layout of the docmnent elements: this 
ilflbrmation is passed on via HTML annotations. 
The sentence plmmer then takes this text plan 
as intmt, and creates SPL tbrmulae to express 
477 
the content identified by the text plan's leaves. 
The resulting SPLs can also group one or more 
leaves together (aggregation) det)ending on de- 
cisions taken by the text planner concerning dis- 
course relations. Furthennore, constraints on 
realization that were introduced by the text- 
planner are also included into the SPLs at this 
stage. 
Of particular interest multilingually is the 
way concepts may require different kinds of re- 
alizations ill different languages. For example, 
languages need not of course realize concepts 
as single words: in Czech the concept Mcn,t 
gets realized as "menu" but the interface modal- 
ity Dialogboz is realized as a multiword expres- 
sion "dialogovd okno" (whose compofients i.e., 
an adjective and a nominal head may undergo 
various grammatical operations independently). 
The Agile system sentence plammr handles uch 
cases by inserting SPL fbrms corresponding to 
the literal semantics of the complex expressions 
required; these are then expressed via the tac- 
tical generator in the usual way. The result- 
ing SPL formulas thus represent the language- 
specitic semantics of the sentences to be gener- 
ated. Otherwise, if a concept maps to a single 
word, the sentence planner leaves the fnrther 
specification of how the concept should be re- 
alized to the lexico-grammar nd its concept- 
to-word mapI)ings. More extensive diflb.rences 
between languages are handled by conditional- 
izing the text and sentence planner resources 
fltrther according to language. 
4 Tactical Generat ion:  From 
Sentence P lans  to Sentences 
The tactical generation component hat colt- 
structs sentences (and other grammatical units) 
fl'om the SPL tbrmulae specified in the text 
plan relies on linguistic resources tbr Bulgarian, 
Czech and Russian. The necessary grammars 
and lexicons have been constrncted employing 
the methods described in Section 1. As ,toted 
there, the crucial characteristic of this model 
of nmltilingual representation is that it allows 
tbr the representation f both, commonalities and 
differences between languages, as required to 
cover the observable ontrastive-linguistic phe- 
nomena. This can be applied even among typo- 
logically rather distant languages. 
We first illustrate this with respect o some 
of the contrastive-linguistic t)henomena that are 
covered by this model employing exami)les ti'om 
English, Bulgarian, Czech and Russian. We 
then show the organization of the lexicons and 
briefly describe lexical dloice. 
4.1 Semantic and grammatical 
cross-linguistic variation 
One. of the tenets of our model of cross-linguistic 
variation is that languages have a rather high 
degree of similarity semantically attd tend to 
differ syntactically. We can thus expect o have 
identical SPL expressions for Bulgarian, Czech 
and Russian in many cases, although these may 
be realized by diverging syntactic structures. 
However, we also allow for the case in which 
there is no commonality at; this level and even 
the SPL expressions diverge. 2 Example 1 illus- 
trates the latter case (high semantic divergence, 
plus grammatical divergence), and example 2 
the former (semantic ommonality, plus gram- 
matical divergence). 
Example 1: English and Russian spa- 
tial PPs .  The major lexico-grammatical d i f  
ference l)etween English and Russian preposi- 
tional phrases is that the relation expressed by 
the PP is realized by the choice of the prepo- 
sition in English, whereas in Russian, it; is in 
addition realized by case-government. In the 
are.a of spatial PPs, the choice of a particular 
preI)osition in English corresl)onds to a distinc- 
tion in the dimensionality of the object that re- 
alizes the range of the relation expressed by the 
PP. For both PPs expressing a location and PPs 
expressing movement, English distinguishes be- 
tween three-dimensional objects (in, into), one- 
or-two-dimensional objects (on, onto) and zero- 
dimensional objects (at, to). 
In Russian, in contrast, zero-or-three dimen- 
sional objects (preposition: v) are opposed 
to one-or-two-dimensional objects (preposition: 
ha). A fnrther difference between the expres- 
sion of static location vs. movement is expressed 
by case selection: na/v+locative case expresses 
static location, v/na+accusative case expresses 
inovement (entering or reaching an object) and 
the preposition k+dative case expresses move- 
inent towards an object (,lot quite reaching or 
2This distinguishes our approach fl'om interlingua- 
based systems, which typically require a common seman- 
tic (or conceptual) input. 
478 
entering it). In the {-onverse relation, motion 
away from an object, s is sele, eted tbr move- 
ment from within an oh.joel;, and ot fbr move- 
men| away from the vicinity of an ot).jeet. Her(;, 
both prel)ositions govern genitive case. The di- 
mensionality of the object is only relevant for 
the distinction between v/na and s/ot, 1)ut not 
for h. Since the concel)tualizations of spatial re- 
lations are ditf'erent across \]'3nglish and Russian, 
the input SPL expressions diverge, as shown in 
Figure 5); rather than using domain model con- 
cepts, these SPL ext)ressions restrict hemselves 
to Ut)pe, r Model concepts in order to highlight 
the cross-linguistic contrast. This examl)le illus- 
trates well how it is (}ften ne{:e, ssary t{} 'semanti- 
{:ize,' eve, nts differently in (tilt'ere|d; languages in 
order 1;o achieve the most natural results. Not;{; 
that Cze, ch is here very similar to l/nssian. 
a. SPL Russian 
(example 
:name DO-Textl-Ku 
:targetform "Pomestite fragment v bufer." 
:logicalform 
(s / dispositive-material-action 
:lex pomestitj 
:speech-act-id command 
:actee (a / object :lex fragment) 
:destination (d / THREE-D-0BJECT 
:lex bufer))) 
1}. SPL Rn: English 
(example  
:name D0-Textl-En 
:targetform "Put the selection on the clipboard." 
:iogicalform 
(s / dispositive-material-action 
:lex put 
:speech-act-id command 
:actee (a / object :lex selection) 
:destination (d / ONE-0R-TW0-D-0BJECT 
:lex clipboard))) 
Figure 5: SPI, ext}ressions 
Example 2: English, Bulgar ian and Czech 
headers in CAD/CAM texts. Grammatical 
ullits (1) (4) below show all ex~?tIllt, e of ,:r,,ss- 
linguistic commonality at the level of sen|anti{: 
int}ut and divergence at the le, vel of grammar. 
These units all time|ion as selfsutficient Task- 
titles tbr the deseril}tions of particular actions 
that can be t)erformed with the given s{}t'tware. 
(1) En: T{} draw a polyline 
(2) BU: qepTaene na IlOJII4MI4IIFIH 
Drawing- of polylineqNDEF 
NOMINAL  
(3) Cz: Kreslenl kf'ivky 
drawing-NOMINAL \]ine-GEN 
(d) \]/,ll: LIwo6I,I Hal)I4COBaTI, IIO,KHJIIIIIHIO 
in-order draw-INF l)olyline-AcC 
There are two major dit  re,,,ces (:,) (4) 
that need to 1)e accounte, d for: (i) they exhibit 
divergent grammatieal  ranks in that (1) and 
(4) are clauses (uontinite), while (2) and (3) are 
nomil,al groul,s (nominalizations); and ( i i )they 
show divergent syntact ic realizations: (2) 
and (3) ditl'er in that in Bulgarian, wlfich does 
not have (:as(',, the relation 1)etween the syntactic 
head Met)q_'aelte (ch, crtacnc) and the modifier lie- 
:mamma (polilinia) is (;xt)ressed by a t)re, position 
na (ha), whereas in Cze, ch, which has cast, this 
relation is expressed by genitive case, (k?ivky). 
\])espite these (litferen(:es, only the first diver- 
gen(:e has any (;onsequen(:{;s for the S\])L ext)res- 
sions rcquir(;d; I;hc l)asie semantic ommona\]- 
ity among (1)(4)  is 1)reserve, d. This is shown 
in Figm:e 6 t)y me, ans of the standard linguis- 
tic conditionalization 1)rovided 1)y KPML l'or all 
levels of linguistic des(:ription. The COll(tition- 
alization shows that both the English (1.) and 
the Russian (4) ar(' nontinite clauses while, the 
\]hdgarian (2) and the Czech (3) are nominM- 
izations. These S\])l, ext)ressions also show the 
use of (lom~dn ('onc(;1)ts as i)rodu('e(l by the text 
tfl~mner rathe, r than Ut)lmr model concepts as in 
the SPLs in Figure, 5. 
(example 
:name DO-Textl 
:logicalform 
(s / DM::draw 
:en :ru :PROPOSAL-Q & PROPOSAL 
:bu :cz :EXIST-SPEECHACT-Q & NOSPEECHACT 
:actee (d / DM::polyline))) 
Figure 6: Multilingual SPL e, xpression tbr the 
header examlfles 
The second differen('e is handled by the gener- 
ation grmmnars internally. Here, Bulgarian and 
Czech share the basic tractional-grammatical 
description of t)ostmotlifie, rs tbr nomilmlizati(ms 
(Figm:e 7). The ditl'erence in structure only 
479 
shows in syntagmatic realization and is separate 
from the functional description: For Bulgarian, 
the postmodifier marker Ha (ha: %f') is inserted, 
and tbr Czech, the nominal group realizing the 
Postmodifier is attr ibuted genitive ease. a
(gate 
:name MEDIUM-QUALIFIER 
:inputs processual-mediated 
:outputs 
((i.0 medium-qualifier 
(:bu :cz preselect Medium nominal-group) 
(:cz preselect Medium noun-gen-case) 
(:bu insert Mediumqualifiermarker) 
(:bu lexify Mediumqualifiermarker na))) 
:region QUALIFICATION) 
Figure 7: Shared system tbr Bulgarian and 
Czech 
4.2 Lexical choice and lexicons 
The lexical items tbr each language are selected 
from the lexicon via the domain model. A DM 
concept is annotated with one or more lexical 
items from each language. If there is more than 
one item per language, the choice is constrained 
by features imposed by the gralnmar. 
For example the concept DN::draw is anno- 
tated with two lexical items which are the im- 
perfective and perfective forms of the verb draw 
in Czech, Bulgarian and Russian. If the gram- 
mar selects imperfective aspect, tim first is cho- 
sen; if the grammar selects perfective aspect, 
the second is chosen. This mechanism is used 
also fbr the choice between a verb and its nom- 
inalization, among others. With the help of the 
lexicon, the inflectional properties collected tbr 
a particular lexical item during generation are 
translated into a format suitable tbr external 
morphological modules, which are then called. 
The result of the external module, the inflected 
tbrm, is passed back to the KPML system and 
inserted into the grammatical structure. 
5 Eva luat ion  and Conc lus ions  
A first round of evaluation has been carried 
out on the Agile prototype. This directly as- 
sessed the ability of users to control multilin- 
3This description is also valid for Russian, which has a 
nominal group structure similar to Czech. The 13ulgarian 
one is more like English. 
gual generation in tile three languages, as well 
as the design and robustness of the system eom- 
1}onents. Groups of users were given a brief 
training period and then asked to construct 
A-boxes expressing iven content. Texts were 
cross-generated: i.e., the languages were w~ried 
across the A-boxes independently of the native 
languages of the subjects who created them. Er- 
rors were then classified and recommendations 
for the next and final Agile prototype collected. 
The generated texts were then evaluated by ex- 
pert technical authors. They were generally 
judged to be of a broadly similar quality to 
the texts originating from manuals, and both 
kinds of texts received similar criticism. The 
main source of criticism and errors was the de- 
sign of the GUI which is now being improved 
for the final prototype. The overall design of 
the system has theretbre shown itself to offer an 
etfective approach tbr multilingual generation. 
We are now extending the system to cover a 
broader ange of text types as well as the further 
grammatical and semantic variation required by 
the evaluators as well as by the additional text 
types. 
Re ferences  
Bateman, J. A., Matthiessen, C. M. I. M., & Zeng, L. 
(1999). Multilingual natural anguage generation 
for multilingual software: a flmctional inguistic 
approach. Applied Artificial hdelligencc, 13(6), 
607-639. 
Bateman, J. A. & Sharoff, S. (1998). Mult, ilingual 
grammars and multilingual lexicons for nmltilin- 
gual text; generation. In Mnltilinguality in the Icz- 
icon II, ECAI'98 Workshop 13, (pp. 1-8). 
Hajie, J. 8; Hladk?, B. (1997). Probabilistic and 
rule-based tagger of an inflective language a 
comparison. In Proceedings of ANLP'97, (pp. 
111-118). 
Mmm, W. C. & Matthiessen, C. M. I. M. (1985). 
Demonstration of the Nigel text generation com- 
puter progrmn. In J. D. Benson 8: W. S. Greaves 
(Eds.), Systemic Perspectives on Discourse, Vol- 
ume 1 (pp. 50-83). Ablex. 
Reiter, E. & Dale, R. (1997). Building applied natu- 
ral language generation systems. Journal of Nat- 
ural Language Engineering, 3, 57-87. 
Tcich, E. (1995). Towards a methodology for the 
construction of multilingual resources tbr multi- 
lingual text generation. In Proceedings of the I J- 
CAI'95 workshop on multilingual generation, (pp. 
136-148). 
480 
Linear order as higher-level decision:
Information Structure in strategic and tactical generation
Geert-Jan M. Kruijff,
Ivana Kruijff-Korbayova?
Computational Linguistics
University of the Saarland
Saarbru?cken, Germany
 
gj,korbay  @coli.uni-sb.de 
John Bateman
Applied Linguistics
University of Bremen
Bremen, Germany
 
bateman@uni-bremen.de 
Elke Teich
Applied Linguistics
University of the Saarland
Saarbru?cken, Germany
 
E.Teich@mx.uni-saarland.de 
Abstract
We propose a multilingual approach to
characterizing word order at the clause
level as a means to realize informa-
tion structure. We illustrate the prob-
lem with three languages which differ
in the degree of word order freedom
they exhibit: Czech, a free word or-
der language in which word order vari-
ation is pragmatically determined; En-
glish, a fixed word order language in
which word order is primarily gram-
matically determined; and German, a
language which is between Czech and
English on the scale of word order free-
dom. Our work is theoretically rooted
in previous work on information struc-
turing and word order in the Prague
School framework as well as on the
systemic-functional notion of Theme.
The approach we present has been im-
plemented in KPML.
1 Introduction
The aim of this paper is to describe an architecture
that addresses how information structure can be
integrated into strategic and tactical generation.
We focus primarily here on the tactical aspect of
how word order (henceforth: WO) may function
as a means of realizing information structure. The
approach we take is multilingually applicable. It
is implemented in KPML (Bateman, 1997b; Bate-
man, 1997a) and has been tested for Czech, Bul-
garian and Russian as three Slavonic languages
with different WO properties, as well as for En-
glish. The algorithm itself is not KPML-specific:
it combines the idea of WO constraints posed by
the grammar, with a complementary mechanism
of default ordering based on information struc-
ture. The algorithm could thus be applied in other
systems wich allow multiple sources of ordering
constraints.
Information structure is a means that a speaker
employs to indicate that some parts of a sen-
tence meaning are context-dependent (?given?),
and that others are context-affecting (?new?). In-
formation structure is therefore an inherent aspect
of sentence meaning, and it contributes in an im-
portant way to the overall coherence of a text.
While it is commonly accepted that information
structuring is a major source of constraints for the
organization of a given content in a particular lin-
ear order in many languages, there is very little
work in Natural Language Generation that explic-
itly models this relation.
From a practical perspective, in the most com-
monly employed generation systems such as
KPML, FUF (Elhadad, 1993; Elhadad and Robin,
1997) or REALPRO (Lavoie and Rambow, 1997),
linear ordering comes as a by-product of other
grammatical choices. This is fine for tactical
generation components and it is sufficient for
languages with grammatically determined WO
(?fixed? WO languages), such as English or Chi-
nese. However, most languages have some WO
variability and this variation usually reflects infor-
mation structure. When languages in which linear
order is primarily pragmatically determined are
involved, such as the Slavonic languages we have
dealt with, a number of problems become imme-
diately apparent.
A comprehensive account of WO variation for
natural language generation that is reusable across
languages is thus required. Such an account needs
to represent linearization as an explicit decision-
making process that involves both the representa-
tion of the language-specific linear ordering pos-
sibilites and the representation of the language-
specific (and possibly cross-linguistically valid)
motivations for particular linearizations. Again,
while the former is catered for in most tactical
generation systems, only selected aspects of the
latter have been dealt with and only for selected
languages (e.g., (Hoffman, 1994; Hoffman, 1995;
Hakkani et al, 1996)).
For example, (Hoffman, 1994) proposes a
treatment of WO in Turkish using a categorial
grammar framework (CCG, (Steedman, 2000))
and relating this to Steedman?s (earlier) account
of information structure (Steedman, 1991). How-
ever, the most important issue, that of providing
an integrated account of how information struc-
ture guides the choice of (or, is realized by) linear
ordering, is left unsolved (Kruijff, 2001).
Given that in many languages, information
structure is the major driving force for WO vari-
ation, it is indeed the most straighforward idea to
couple an account of information structure with
the choice of linear ordering. However, for mul-
tilingual application, the particular challenge is to
develop a solution that can be applied, no mat-
ter at which point on the free-to-fixed WO cline a
language is located.
The approach to WO proposed in this paper is a
move in exactly this direction. We start in  2 with
presenting data from Czech, German and English
that motivate the perspective we take on informa-
tion structure, and its role in generating coher-
ent discourse. In  3 we introduce the linguistic
notions employed in the present account. In  4
we discuss how information structure fits into a
general system architecture, and we discuss the
implementation of the strategic generation com-
ponent on the basis of KPML. We continue with
an elaboration of the role of information structure
in tactical generation, presenting an algorithm for
generating contextually appropriate linearization,
given a sentence?s information structure, and il-
lustrate its implementation on Czech and English
examples (  5). We conclude the paper with a
summary (  6).
2 Linguistic motivation
There are a number of factors commonly ac-
knowledged to play an important role in express-
ing a given content in a specific linear form.
The inventory of these factors contains at least
the following: information structure, syntactic
structure, intonation, rhythm and style. Cross-
linguistically, these factors may be involved in
constraining linear ordering to varying degrees.
English, for instance, is an example of a lan-
guage in which WO is rather rigid, i.e., strongly
constrained by syntactic structure. In such lan-
guages, differences in information structure are
often reflected by varying the intonation pat-
tern or by the choice of particular types of
grammatical constructions, such as clefting and
pseudo-clefting, or definiteness/indefiniteness of
the nominal group. Czech, in contrast, which has
a rich case system and no definite or indefinite
article, belongs to the so-called ?free word order?
languages, where the same effects are achieved by
varying WO. Finally, German lies between En-
glish and Czech in the spectrum between fixed
and free WO. We illustrate the general point that
WO selections are related to information structure
by appropriateness judgements of some examples
of instructions in Czech, German and English.1
(1) Otevr?eme
open-1PL
pr???kazem
command-INS
Open
Open
soubor.
file-ACC
Sie
You
o?ffnen
open
eine
a
Datei
file
mit
with
dem
the
Befehl
command
Open.
Open.
Open a file with the Open command.
The ordering in (1) is neutral in that no partic-
ular contextual constraints hold with respect to
the newsworthiness of any of the elements ex-
pressed in this clause. This kind of ordering can
1The English examples use imperative mood, while the
Czech and the German examples use indicative mood as
the most common way of conveying instructions of the dis-
cussed type. Alternatively, both Czech and German can use
also imperatives or infinitives for instructions, but these are
considered less polite than the indicative versions. Last but
not least, instructions can also be formulated in indicative
mood with passive voice in both Czech and German.
be elicited by the question What should we do?.2
We follow Prague School accounts (Firbas, 1992;
Sgall et al, 1986) in calling this neutral ordering
the systemic ordering (cf. also  5). Alternatively,
(1) could be used in a context characterized by
the question What should we open by the Open
command?, when the Open command is not be-
ing contrasted with some other entity.
(2) Otevr?eme
open-1PL
soubor
file-ACC
pr???kazem
command-INS
Open.
Open
Sie
you
o?ffnen
open
die
the
Datei
file
mit
with
dem
the
Befehl
command
Open.
Open.
?Open the file with the Open command.?
(3) Soubor
file-ACC
otevr?eme
open-1PL
pr???kazem
command-INS
Open.
Open
Die
the
Datei
file
o?ffnen
open
Sie
you
mit
with
dem
the
Befehl
command
Open.
Open.
?Open the file with the Open command.?
The word order variants illustrated in (2) and (3)
are appropriate when some file is active in the
context (Chafe, 1976), for instance when the user
is working with a file. In (2), the action of open-
ing is also active; in (3) it can, but does not have
to be active, too. The contexts in which (2) and
(3) can be appropriately used can be character-
ized by the questions What should we do with the
file? or How should we open the file?. Unlike
(2), example (3) can be used if file is contrasted
with another entity. In German, this contrast is
required, whereas in Czech it is optional. In En-
glish, intonation could mark whether contrast is
required.
(4) Pr???kazem
command-INS
Open
Open
otevr?eme
open-1PL
soubor.
file-ACC
Mit
with
dem
the
Befehl
command
Open
Open
o?ffnen
open
Sie
you
eine
a
Datei.
file.
With the Open command, open a file.
(5) Pr???kazem
command-INS
Open
Open
soubor
fileACC
otevr?eme.
open-1PL
Mit
with
dem
the
Befehl
command
Open
Open
o?ffnen
open
Sie
you
die
a
Datei.
file.
With the Open command, open the file.
2We use questions for presentational purposes to indicate
which contexts would be appropriate for uttering sentences
with particular WO variants. Such question-answer pairs are
known as question tests (Sgall et al, 1986).
The contexts in which (4) can be used are char-
acterized by What should we do with the Open
command?. While (4) does not refer to a spe-
cific file, in (5) an activated file is presumed. (5)
is appropriate in contexts characterized by What
should we do to the file with the Open command?.
It is also possible to use (4) in a context charac-
terized by What should we do?, and (5) in a con-
text characterized by What should we do to the
file?, if it is presumed that we are talking about
using various commands (or various means or in-
struments) to do various things. In the latter type
of context, the Open command does not have to
be activated.
(6) Soubor
file-ACC
pr???kazem
command-INS
Open
Open
otevr?ete.
open-I2PL
Die
the
Datei
file
o?ffnen
open
Sie
you
mit
with
dem
the
Befehl
command
Open.
Open
Open the file with the Open command.
Example (6) is like (5) in that it is appropriate
when both a file and the Open command are acti-
vated. The contexts in which (6) can be appropri-
ately used can be characterized by What should
we do to the file with the Open command?. Un-
like (5), (6) can also be used when file is con-
trasted with another entity. In German, there is
no difference in word order between (6) and (3)
(they differ only in intonation). This is a result of
the strong ordering constraint in German to place
the finite verb as second (in independent, declara-
tive clauses). In Czech verb secondness also plays
a role, but it is much weaker.
Analogous judgements concerning contextual
appropriateness apply to WO variants in differ-
ent mood and/or voice (when available in the in-
dividual languages). The orders in which the verb
is first do not presume the activation of either a
file or a command. The orders in which ?file?
precedes the verb appear to presume an active
file, the orders in which ?command? precedes the
verb appear to presume the activation of a com-
mand. When both ?file? and ?command? precede
the verb, the activation of both a file and a com-
mand appears to be presumed.
These judgements show that differences in WO
(in languages with a more flexible WO then En-
glish, e.g., Czech and German) very often corre-
spond to differences in how the speaker presents
the information status of the entities and pro-
cesses that are referred to in a text, in particu-
lar, whether they are assumed to be already fa-
miliar or not, and whether they are assumed to
be activated in the context. Note that in English,
the same distinction is expressed by the use of a
definite vs. an indefinite nominal expression, i.e.
?a  the file?.
To summarize: Since sentences which differ
only in WO (and not in the syntactic realizations
of clause elements) are not freely interchangable
in a given context, we have to be able to gen-
erate contextually appropriate WOs. In order to
achieve this, we need to be able to capture not
only the structural restrictions specific to individ-
ual languages, but also the restrictions reflecting
the information status of the entities (and pro-
cesses) being referred to.
3 Underlying notions
In order to provide constraints for WO decisions
within our generation architecture, we require
mechanisms through which particular patterns of
information structuring can constrain the choice
among the WO variants available. These patterns
are provided by our text planning component. We
have found two complementary approaches to the
relationship between aspects of information struc-
turing and WO to be ripe for application in the
generation of extended texts; these approaches are
briefly introduced below.
In order to clarify the complementary nature of
the approaches that we have adopted, it is neces-
sary first to distinguish between two dimensions
of organization that are often confused or whose
difference is contested: in his Systemic Func-
tional Grammar (SFG), (Halliday, 1970; Hall-
iday, 1985) distinguishes between the thematic
structure of a clause and its information struc-
ture: Whereas the Theme is ?the starting point
for the message, it is the ground from which the
clause is taking off? (Halliday, 1985, 38), infor-
mation structure concerns the distinction between
the Given as ?what is presented as being already
known to the listener? (Halliday, 1985, 59), and
the New as ?what the listener is being invited to
attend to as new, or unexpected, or important?
(ibid).
3.1 Information structure and ordering
In Halliday?s original approach (Halliday, 1967),
the basic assumption for English and also for
other languages is that ordering, apart from being
grammatically constrained, is iconic with respect
to ?newsworthiness?. So on a scale from Given
to New information, the ?newer? elements would
come towards the end of the information unit, the
?newest? element bearing the nuclear stress. This
approach relies on the possibility of giving a com-
plete ordering of all clause elements with respect
to their newsworthiness.
The notion of ordering by newsworthiness in
Halliday?s approach is parallel to the notion of
communicative dynamism (CD) introduced in the
early works of Firbas (for a recent formulation
see (Firbas, 1992)) and used also within the Func-
tional Generative Description (FGD, (Sgall et al,
1986)). Also from the viewpoint of CD, the pro-
totypical ordering of clause elements from left
to right respects newsworthiness: In prototypical
cases, WO corresponds to CD. However, textu-
ally motivated thematization or grammatical con-
straints may force WO to diverge from CD.
The FGD approach differs from Halliday?s in
that, in addition to CD, it works with a de-
fault (canonical) ordering, called systemic order-
ing (SO). SO is the language specific canonical
ordering of clause elements (complements and
adjuncts), as well as of elements of lower syntac-
tic levels, with respect to one another.
For the current purposes we concentrate on the
SO for a subset of the clause elements that are dis-
cerned in FGD. We use the following SOs for the
Slavonic languages and for English and German:3
SO for Czech, Russian, Bulgarian:
Actor  TemporalLocative  Purpose  Space-
Locative  Means  Addressee  Patient 
Source  Destination
SO for English: Actor  Addressee  Pa-
tient  SpaceLocative  TemporalLocative 
Means  Source  Destination  Purpose-
dependent
SO for German: Actor  TemporalLocative
 SpaceLocative  Means  Addressee  Pa-
tient  Source  Destination  Purpose
3The labels we use for the various types of elements are
a mixture of FGD and SFG terminology.
The SO for the Slavonic languages is based on
the one for Czech (Sgall et al, 1986); the only
difference is that we have placed Patient before
Source (?from where?). We follow (Sgall et al,
1986) in considering the SOs for the main types
of complementations in Russian and Bulgarian to
be similar to the Czech one, though there can be
slight differences (cf. the observations reported in
(Adonova et al 1999)). The SO for English com-
bines the suggestions made by (Sgall et al, 1986)
and the ordering defaults of the NIGEL grammar
of English (cf. Section 5.2). The SO for German
is based on (Heidolph et al, 1981, p.704).
The informational status of elements is estab-
lished through deviation of CD from the SO. This
leads us to the distinction FGD makes between
contextually bound (CB) and contextually non-
bound (NB) items in a sentence (Sgall et al,
1986). A CB item is assumed to convey some
content that bears on the preceding discourse con-
text. It may refer to an entity already explic-
itly referred to in the discourse, or an ?implicitly
evoked? entity. At each level of syntactic struc-
ture, CB items are ranked lower than NB items in
the CD ordering. The motivation behind and the
meaning of the CB/NB distinction in FGD cor-
responds to those underlying the Given/New di-
chotomy in SFG.
Contextual boundness can be used to constrain
WO (at the clause level) as follows:
 The CB elements (if there are any) typically
precede the NB elements.
 The mutual ordering of multiple CB items in
a clause corresponds to communicative dy-
namism, and the mutual ordering of mul-
tiple NB items in a clause follows the SO
(with the exceptions required by grammati-
cally constrained ordering as described be-
low). The default for communicative dy-
namism is SO.
 The main verb of a clause is ordered at the
boundary between the CB elements and the
NB elements, unless the grammar specifies
otherwise (verb secondness).
It is the above abstract ordering principles that
underly the algorithm we present in  5.
3.2 Thematic structure
In all languages we looked at so far, there are also
orders we cannot explain solely on the basis of
the CB/NB distinction along with SO and gram-
matical constraints. On the one hand, it has been
claimed that the ordering of CB elements follows
CD rather than SO, and that CD is determined
by contextual factors (Sgall et al, 1986). On the
other hand, cases where an NB element appears at
the beginning of a clause are far from rare. While
we currently do not have more to add to the for-
mer issue, the latter can be readily addressed us-
ing the notion of Theme. For illustration, consider
(8) in Czech, German and English, appearing in a
context where it is preceded only by (7).
(7) First open the Multiline styles dialog box using one
of the following methods.
(8) Z
From Data
menu
menu
Data
choose 	

vybereme
Style.
Style.
Im
In
Menu?
menu
Data
Data
wa?hlen
choose
Sie
you
Style.
Style.
In the Data menu, choose Style.
The preceding context does not refer to the ?Data
menu? or make it active in any way. Working
only with the notion of information structure dis-
cerning CB (Given) and NB (New) elements, one
is thus unable to explain this ordering. On the
other hand, the notion of thematic structure as
a reflection of a global text organization strategy
makes such explanation possible. In Halliday?s
approach, Theme has a particular textual function,
that of signposting the intended development or
?scaffolding? that a writer employs for structuring
an extended text. In software instruction manuals,
for example, we encounter regular thematization
of (i) the location where actions are performed,
(ii) the particular action that the user is instructed
to perform, or (iii) the goal that the user wants to
achieve (cf. (Kruijff-Korbayova? et al, in prep) for
a more detailed discussion).
4 Information structure and strategic
planning
In this section we briefly describe how we in-
tegrate information structure into strategic gen-
eration, i.e. text- and sentence-planning. The
Figure 1: A text plan. In our system, a text plan organizes content into a linear fashion, showing
where (and how) content might be aggregated syntactically (e.g. conjunction) or discursively (e.g.
RST-relations). In the example above, the text plan specifies a text consisting of an overall goal (the
title) and five substeps to resolve that goal (the tasks). The first task is a simple one, the second task
is a complex formed around an RST-purpose relation, after which follows a conjunction of tasks. (The
CONJOINED-INSTRUCTION-TASKS nodes indicate that the left-daughter node (a task) and the task
dominated by the immediate non-terminal node above a CONJOINED-INSTRUCTION-TASKS node,
are to be related by a conjunction.) The content to be realized is identified by the leaves of the text
plan. Whenever a leaf is introduced in the text plan, the discourse model is updated with the content?s
(A-box) concepts. The sentence planner decends through the text plan depth-first. Thereby it gathers
the leaves? content into sentence-specifications, following any indications of aggregation. It makes use
of the discourse model to specify whether content should be realized as contextually bound (or not).
principle idea is that during text-planning, a dis-
course model is built that is then used in sentence-
planning to determine a sentence?s information
structure.
We have developed a system using KPML. In
KPML, generation resources are divided into in-
teracting modules called regions. For the purpose
of text-planning we have constructed a region that
defines an additional level of linguistic resources
for the level of genre. The region facilitates the
composition of text structures in a way that is very
similar to the way the lexico-grammar builds up
grammatical structures. This enables us to have a
close interaction between global level text gener-
ation and lexico-grammatical expression, with the
possibility to accommodate and propagate con-
straints on output realization. While constructing
a text plan, the text planner constructs a (rudimen-
tary) discourse model that keeps track of the dis-
course entities introduced.
Text planning results in a text plan and a dis-
course model that serve as input to the sentence
planner. The text plan is a hierarchical structure,
organizing the content into a more linear fashion
(see Figure 3.2). The sentence planner creates
the input to the tactical generation phase as for-
mulas of the Sentence planning Language (SPL,
(Kasper, 1989)). The SPL formulas express the
bits of content identified by the text plan?s leaves,
and can also group one or more leaves together
(aggregation) depending on decisions taken by
the text planner concerning discourse relations.
Most importantly, during this phase of planning
what content is to be realized by a sentence, the
underlying information structure of that content
is determined: Whenever the sentence planner
encounters a piece of content that the discourse
model notes as previously used, it marks the cor-
responding item in the SPL formula as contextu-
ally bound (note that we are hereby making a sim-
plifying assumption that in the current version of
the sentence planner we equate contextual bound-
ness with previous mention).
The text planner can also choose a particular
textual organization and determine the element
which should become the Theme. If no particu-
lar element is chosen as the Theme, the grammar
chooses some element as the default Theme. This
can be the Subject (as in English), the least com-
municatively dynamic element (as in Czech); the
choice of the default Theme in German is freer
than in English, but more restricted than in Czech
(cf. (Steiner and Ramm, 1995) for a discussion).
The Theme is then placed at the beginning of the
clause, although not necessarily at the very first
position, as this might be occupied, e.g., by a con-
nective. The placement of the Theme is also re-
solved by the grammar.
5 Realizing information structure
through linearization
It is in the setting described in  4 that the issue of
generating contextually appropriate sentences re-
ally arises. In this section we describe the word
ordering algorithm (  5.1) and its application to
Czech and English (  5.2).
5.1 Flexible word order algorithm
As discussed, constraints from various sources
need to be combined in order to determine gram-
matically well-formed and contextually appropri-
ate WO. Contextual boundness is used to con-
strain WO at the clause level as specified above.
We combine the following two phases in which
information structure (CB/NB) is taken into ac-
count during tactical generation:
 information structure can determine partic-
ular realization choices made in the gram-
mar; for example, when inserting and plac-
ing the particle of a phrasal verb, when in-
serting and ordering the Source and Destina-
tion for a motion process;
 information structure can determine the or-
dering of elements whose placement has not
been sufficiently constrained by the gram-
mar.
For a multilingual resource, this allows each
language to establish its own balance between the
two phases. To show our approach in a nutshell,
we present an abstract WO algorithm in Figure 2.
Given:
a set GC of ordering constraints
imposed by the grammar
a list L1 of constituents
that are to be ordered,
a list D giving ordering of CB
constituents (default is SO)
Create two lists LC and LN of de-
fault orders:
Create empty lists LC (for CB items)
and LN (for NB items)
Repeat for each element E in L1
if E is CB,
then add E into LC,
else add E into LN.
Order all elements in LC
according to D
Order all elements in LN
according to SO
if the Verb is yet unordered then
Order the Verb at
the beginning of LN
Order the elements of L1
if GC is not empty then
use the contraints in GC, and
if the contraints in GC are
insufficient,
apply first the default
orders in LC and then those in LN
Figure 2: Abstract ordering algorithm
The ordering constraints posed by the gram-
mar have the highest priority. Note that this in-
cludes the ordering of the textually determined
Theme. Then, elements which are not ordered by
the grammar are subject to the ordering according
to information structure, i.e. systemic ordering in
combination with the CB/NB distinction. The or-
dering of the NB elements (i) is restricted by the
syntactic structure or (ii) follows SO. The order-
ing of the CB elements can be (i) specified on the
basis of the context, (ii) restricted by the syntactic
structure, or (iii) follow SO.
The ordering algorithm as such is not language
specific, and could be usefully applied in the gen-
eration of any language. What differs across lan-
guages is first of all the extent to which the gram-
mar of a particular language constrains ordering,
i.e. which elements are subject to ordering re-
quirements posed by the syntactic structure, and
which elements can be ordered according to infor-
mation structure. Also, it is desirable (and our al-
gorithm allows it) to specify different systemic or-
derings for different languages. And, even within
a single language, our algorithm allows the spec-
ification of different systemic orderings in differ-
ent grammatical contexts (just by adding a real-
ization statement that (partially) defines the SO
during strategic generation).
The algorithm is applicable in platforms other
than KPML. In the first place, any grammar
can modify its decisions to take information
structure into account. In addition, those tacti-
cal generators allows multiple sources of order-
ing constraints, e.g., a combination of grammar-
determined choices and defaults, as long as such
that the default ordering based on information
structure can be applied.
5.2 Algorithm application
The algorithm described above has been imple-
mented and used for generation of Czech and En-
glish instructional texts. The Czech grammar re-
sources used in tactical generation have been built
up along with Bulgarian and Russian grammar
resources as described in (Kruijff et al, 2000),
reusing the NIGEL grammar for English. The
original NIGEL grammar itself already combines
the specification of ordering constraints in the
grammar with the application of defaults. If an or-
dering is underspecified by the grammar, the de-
faults are applied. The defaults are ?static?, i.e.
specified once and for all. The algorithm we have
described replaces these ?static? defaults with a
?dynamic? construction of ordering constraints.
Two separate sets of ?dynamic? defaults are com-
puted on the basis of the SO for the CB and the
NB elements in each sentence/clause.
We use the SOs for Czech and English
specified above (cf.  3.1). For each ele-
ment in the input SPL we specify whether it
is CB (:contextual-boundness yes) or
NB (:contextual-boundness no); in ad-
dition, we can specify the textual Theme in the
SPL (theme <id>). The SPL in Figure 3 illus-
trates this.
Note that the information structure distinction
between CB vs. NB elements on the one hand,
and the informational status of referents as iden-
tifiable vs. non-identifiable on the other hand, are
orthogonal. Whereas CB/NB has to do with the
(R / RST-purpose
:speechact assertion
:DOMAIN (ch/DM::choose
:actor (a1/DM::user
:identifiability-q identifiable
:contextual-boundness yes)
:actee (a2/object :name gui-open
:identifiability-q identifiable
:contextual-boundness no)
:instrumental (mea/DM::mouse
:identifiability-q identifiable
:contextual-boundness no)
:spatial-locating (loc/DM::menu
:identifiability-q identifiable
:contextual-boundness yes
:class-ascription (label/object
:name gui-file))
:RANGE (open/DM::open
:contextual-boundness no
:actee (f/DM::file
:contextual-boundness no)))
:theme open)
Generated output:
Pro
for
otevr?en??
opening-GEN
souboru
file-GEN
uz?ivatel
user-NOM
v
in
menu
menu-LOC
vybere
choose-3SG
mys???
mouse-INS
Open.
Open
To open a file, the user chooses Open in the menu with the
mouse.
Figure 3: Sample input SPL for English and
Czech and generated outputs
speaker?s presenting an element as either bearing
on the context or context-affecting, identifiability
reflects whether the speaker assumes the hearer
to pick out the intended referent. These two di-
mensions are independent, though correlated (cf.
the discussion of activation vs. identifiability in
(Lambrecht, 1994)). What is encountered most
often is the correlation of CB with identifiable
and NB with non-identifiable. The correlation of
NB with identifiable corresponds is found, e.g., in
cases of ?reintroducing? an element talked about
before, or in cases like There is a square and a
circle. Delete the circle. ?in the second sentence,
the same ordering would be used also in German
(Lo?schen Sie den Kreis) and in Czech (Vymaz?te
kruh.).
What is hard to find is the correlation of CB
with non-identifiable, but it is the way we would
analyze a dollar bill in example (9) (Gregory
Ward, p.c.)4
(9) (What do you do if you see money laying on the
ground?)
Dolarovou
Dollar
bankovku
note
bych
would 	
zvedla.
pick-up 	
Eine
a
Dollarnote
dollarnote
wu?rde
would
ich
I
aufheben.
pick-up
A dollar bill I would pick up.
The CB/NB assignments can be varied to ob-
tain different WO variants. The examples below
show some of the CB/NB assignment combina-
tions and the outputs generated using the Czech
and English grammars.
(10) user
Actor-NB
Uz?ivatel
choose
(Finite-Verb)
vybere
Open
Purpose-NB
pro
menu
SpaceLoc.-NB
otevr?en??
mouse
Means-NB
souboru
open file
Patient-NB
v
The user chooses Open in the menu with the mouse
to open a file.
(11) user
Actor-CB
Uz?ivatel
choose
v
Open
SpaceLoc.-CB
menu
menu
(Finite-Verb)
vybere
mouse
Purpose-NB
pro
open file
Means-NB
otevr?en??
Patient-NB
souboru mys???
The user chooses Open in the menu with the mouse
to open a file.
(12) user
Purpose-CB
Pro
choose
otevr?en??
Open
Actor-CB
souboru
menu
SpaceLoc.-CB
uz?ivatel
mouse
Means-CB
v
open file
(Finite-Verb)
menu
Patient-NB
mys??? vybere
To open a file the user chooses Open in the menu
with the mouse.
As mentioned above, we preserve the notion of
textual Theme. An SPL can contain a specifica-
tion of a Theme, and the corresponding element
is then ordered at the front of the sentence, as de-
termined by the grammar. The WO of the rest of
the sentence is determined as described.
4Regarding intonation: in English, there are two into-
nation phrases, the first containing dollar bill with a L+H*
pitch accent on dollar, and the second with a H* pitch accent
on pick up. In Czech and German it seems that a contrastive
pitch accent on dolarovou bankovku is optional, and the rest
can have neutral intonation with nuclear stress on the last
word.
6 Summary and conclusions
We have presented a flexible word ordering al-
gorithm for natural language generation. The
novel contribution consists in offering one way
of implementing information structure as the ma-
jor source of constraints on word order varia-
tion for languages with pragmatically-determined
word order. Apart from that, the special feature of
the word order algorithm proposed is that it can
also be applied to languages with grammatically-
determined word order. We have illustrated the
application of the algorithm for Czech and En-
glish, Czech being a language in which word or-
der is primarily pragmatically determined and En-
glish being a grammatically-determined word or-
der language. We have thus provided evidence
that the algorithm can flexibly be applied to ?free?
word order languages as well as ?fixed? word or-
der languages.
From a linguistic theoretical point of view, the
most important precondition for achieving this
has been to take seriously the linguistic observa-
tion that in many languages information structure
is the driving force for word order variation. For
the modeling of information structure for strate-
gic generation, we have drawn upon two well es-
tablished linguistic frameworks, in both of which
the discourse-linguistic and pragmatic constraints
on grammatical realization are a focal interest, the
Prague School and Systemic Functional Linguis-
tics. From a technical point of view, we have
based the implementation on the KPML system,
integrating the proposed word order algorithm
with existing multilingual grammatical resources
and re-using KPML?s mechanisms for word or-
der realization as well as its systemic-functionally
based notion of Theme. The algorithm is not
KPML-specific, though, and could be applied in
other frameworks as well, especially if they allow
the combination of linearization constraints com-
ing from different sources.
Acknowledgements
The work presented here folows up on our
earlier work carried out partially within AG-
ILE (Automatic Generation of Instructions
in Languages of Eastern Europe), a project
funded by the European Community within
the INCO-COPERNICUS programme (grant
No. PL96114). We would also like to thank
the anonymous reviewers of this workshop for
valuable comments.
References
John A. Bateman. 1997a. Enabling technology for multilin-
gual natural language generation: The kpml development
environment. Natural Language Engineering, 3:15 ? 55.
John A. Bateman. 1997b. KPML Development Environ-
ment: multilingual linguistic resource development and
sentence generation. Darmstadt, Germany, March. (Re-
lease 1.0).
Wallae Chafe. 1976. Givenness, contrastiveness, definite-
ness, subjects, topics and point of view. Subject and
Topic. Charles Li (ed.). New York: Academic Press. p.
25 ? 56.
Michael Elhadad and Jacques Robin. 1997. Surge: A com-
prehensive plug-in syntactic realisation component for
text generation. Technical report, Department of Com-
puter Science, Ben Gurion University, Beer Shava, Israel.
Michael Elhadad. 1993. Fuf: The universal unifier user
manual 5.2. Technical report, Department of Computer
Science, Ben Gurion University, Beer Shava, Israel.
Jan Firbas. 1992. Functional Sentence Perspective in Writ-
ten and Spoken Communication. Studies in English Lan-
guage. Cambridge University Press, Cambridge.
Dilek Zeynep Hakkani, Kemal Oflazer, and Ilyas Cicekli.
1996. Tactical generation in a free constituent order lan-
guage. In Proceedings of the International Workshop on
Natural Language Generation, Herstmonceux, Sussex,
UK.
Michael A. K. Halliday. 1967. Notes on transitivity and
theme in English ? parts 1 and 2. Journal of Linguistics,
3(1 and 2):37?81 and 199?244.
Michael A.K. Halliday. 1970. A Course in Spoken English:
Intonation. Oxford Uniersity Press, Oxford.
Michael A.K. Halliday. 1985. Introduction to Functional
Grammar. Edward Arnold, London, U.K.
K. Heidolph, W. Fla?mig, and W. Motsch. 1981. Grundzu?ge
einer deutschen Grammatik. Akademie-Verlag.
Beryl Hoffman. 1994. Generating context-appropriate
word orders in turkish. In Proceedings of the Internati-
nal Workshop on Natural Language Generation, Kenneb-
unkport, Maine.
Beryl Hoffman. 1995. Integrating ?free? word order syntax
and information structure. In Proceedings of the Euro-
pean Chapter of the Association for computational Lin-
guistics (EACL), Dublin, Ireland.
Robert T. Kasper. 1989. A flexible interface for linking
applications to PENMAN?s sentence generator. In Pro-
ceedings of the DARPA Workshop on Speech and Natural
Language.
Geert-Jan M. Kruijff. 2001. A Categorial-Modal Logi-
cal Architecture of Informativity: Dependency Grammar
Logic & Information Structure. Ph.D. thesis, Faculty
of Mathematics and Physics, Charles University, Prague,
Czech Republic, April.
Geert-Jan M. Kruijff, Elke Teich, John Bateman, Ivana
Kruijff-Korbayova?, Hana Skoumalova?, Serge Sharoff,
Lena Sokolova, Tony Hartley, Kamy Staykova and Jir???
Hana. 2000. Multilingual generation for three slavic lan-
guages. In Proceedings COLING 2000.
Ivana Kruijff-Korbayova?, John Bateman, and Geert-Jan M.
Kruijff. in prep. Generation of contextually appropriate
word order. In Kees van Deemter and Rodger Kibble,
editors, Information Sharing, Lecture Notes. CSLI.
Knud Lambrecht. 1994. Information Structure and Sen-
tence Form. Cambridge Studies in Linguistics. Cam-
bridge University Press.
Benoit Lavoie and Owen Rambow. 1997. A fast and
portable realizer for text generation. In Proceedings of
the Fifth Conference on Applied Natural Language Pro-
cessing (ANLP), Washington DC.
Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?. 1986.
The Meaning of the Sentence in Its Semantic and Prag-
matic Aspects. D. Reidel Publishing Company, Dor-
drecht, Boston, London.
Mark J. Steedman. 1991. Structure and intonation. Lan-
guage, 68:260 ? 296.
Mark Steedman. 2000. The Syntactic Process. The MIT
Press, Cambridge Massachusetts.
Erich Steiner and Wiebke Ramm. 1995. On Theme as a
grammatical notion for German. Functions of Language,
2(1):57?93.
  	
Proceedings of the Workshop on Frontiers in Linguistically Annotated Corpora 2006, pages 86?93,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Corpus annotation by generation
Elke Teich
TU Darmstadt
Darmstadt, Germany
teich@linglit.tu-darmstadt.de
John A. Bateman
Universita?t Bremen
Bremen, Germany
bateman@uni-bremen.de
Richard Eckart
TU Darmstadt
Darmstadt, Germany
eckart@linglit.tu-darmstadt.de
Abstract
As the interest in annotated corpora is
spreading, there is increasing concern with
using existing language technology for
corpus processing. In this paper we ex-
plore the idea of using natural language
generation systems for corpus annotation.
Resources for generation systems often fo-
cus on areas of linguistic variability that
are under-represented in analysis-directed
approaches. Therefore, making use of
generation resources promises some sig-
nificant extensions in the kinds of anno-
tation information that can be captured.
We focus here on exploring the use of
the KPML (Komet-Penman MultiLingual)
generation system for corpus annotation.
We describe the kinds of linguistic infor-
mation covered in KPML and show the
steps involved in creating a standard XML
corpus representation from KPML?s gener-
ation output.
1 Introduction
Many high-quality, theory-rich language process-
ing systems can potentially be applied to corpus
processing. However, the application of exist-
ing language technology, such as lexical and/or
grammatical resources as well as parsers, turns out
not to be as straightforward as one might think
it should be. Using existing computational lexi-
cons or thesauri, for instance, can be of limited
value because they do not contain the domain-
specific vocabulary that is needed for a partic-
ular corpus. Similarly, most existing grammat-
ical resources for parsing have restricted cover-
age in precisely those areas of variation that are
now most in need of corpus-supported investiga-
tion (e.g., predicate-argument structure, informa-
tion structure, rhetorical structure). Apart from
limited coverage, further issues that may impede
the ready application of parsers in corpus process-
ing include:
? Annotation relevance. Specialized, theory-
specific parsers (also called ?deep parsers?;
e.g., LFG or HPSG parsers) have been built
with theoretical concerns in mind rather than
appliability to unrestricted text. They may
thus produce information that is not annota-
tionally relevant (e.g., many logically equiv-
alent readings of a single clause).
? Usability. Deep parsers are highly complex
tools that require expert knowledge. The ef-
fort in acquiring this expert knowledge may
be too high relative to the corpus processing
task.
? Completeness. Simple parsers (commonly
called ?shallow parsers?), on the other
hand, produce only one type of anno-
tationally relevant information (e.g., PoS,
phrase/dependency structure). Other desir-
able kinds of information are thus lack-
ing (e.g., syntactic functions, semantic roles,
theme-rheme).
? Output representation. Typically, a parsing
output is represented in a theory-specific way
(e.g., in the case of LFG or HPSG parsers,
a feature structure). Such output does not
conform to the common practices in corpus
representation.1 Thus, it has to be mapped
onto one of the standardly used data mod-
els for corpora (e.g., annotation graphs (Bird
and Liberman, 2001) or multi-layer hier-
archies (Sperberg-McQueen and Huitfeldt,
2001; Teich et al, 2001)) and transformed
to a commonly employed format, typically
XML.
1This is in contrast to the output representation of shal-
low parsers which have often been developed with the goal
of corpus processing.
86
In spite of these difficulties, there is a general
consensus that the reward for exploring deep pro-
cessing techniques to build up small to medium-
scale corpus resources lies in going beyond the
kinds of linguistic information typically covered
by treebanks (cf. (Baldwin et al, 2004; Cahill et
al., 2002; Frank et al, 2003)).
In this paper, we would like to contribute to this
enterprise by adding a novel, yet complementary
perspective on theory-rich, high-quality corpus an-
notation. In a reappraisal of the potential contribu-
tion of natural language generation technology for
providing richly annotated corpora, we explore the
idea of annotation by generation. Although this
may at first glance seem counter-intuitive, in fact a
generator, similar to a parser, creates rather com-
plex linguistic descriptions (which are ultimately
realized as strings). In our current investigations,
we are exploring the use of these complex linguis-
tic descriptions for creating annotations. We be-
lieve that this may offer a worthwhile alternative
or extension of corpus annotation methods which
may alleviate some of the problems encountered
in parsing-based approaches.
The generation system we are using is the KPML
(Komet-Penman MultiLingual; (Bateman, 1997))
system. One potential advantage of KPML over
other generation systems and over many parsing
systems is its multi-stratal design. The kinds
of linguistic information included in KPML range
from formal-syntactic (PoS, phrase structure) to
functional-syntactic (syntactic functions), seman-
tic (semantic roles/frames) and discoursal (e.g.,
theme-rheme, given-new). Also, since KPML has
been applied to generate texts from a broad spec-
trum of domains, its lexicogrammatical resources
cover a wide variety of registers?another poten-
tial advantage in the analysis of unrestricted text.
As well as our general concern with investigat-
ing the possible benefits of applying generation
resources to the corpus annotation task, we are
also more specifically concerned with a series of
experiments involving the KPML system as such.
Here, for example, we are working towards the
construction of ?treebanks? based on the theory of
Systemic-Functional Linguistics (SFL; (Halliday,
2004)), so as to be able to empirically test some of
SFL?s hypotheses concerning patterns of instantia-
tion of the linguistic system in authentic texts. An-
notating the variety of linguistic categories given
in SFL manually is very labor-intensive and an au-
tomated approach is clearly called for. We are also
working towards a more detailed comparison of
the coverage of the lexicogrammatical resources
of KPML with those of parsing systems that are
similarly theoretically-dedicated (e.g., the HPSG-
based English Resource Grammar (ERG) (Copes-
take and Flickinger, 2002) contained in LinGO
(Oepen et al, 2002)). Thus, the idea presented
here is also motivated by the need to provide a ba-
sis for comparing grammar coverage across pars-
ing and generation systems more generally.
The remainder of the paper is organized as fol-
lows. First, we present the main features of the
KPML system (Section 2). Second, we describe the
steps involved in annotation by generation, from
the generation output (KPML internal generation
record) to an XML representation and its refine-
ment to an XML multi-layer representation (Sec-
tion 3). Section 4 concludes the paper with a criti-
cal assessment of the proposed approach and a dis-
cussion of the prospects for application in the con-
struction of corpora comparable in size and qual-
ity to existing treebanks (such as, for example, the
Penn Treebank for English (Marcus et al, 1993)
or the TIGER Treebank for German (Brants et al,
2002)). Since our description here has the status
of a progress report of work still in its beginning
stages, we cannot yet provide the results of de-
tailed evaluation. In the final section, therefore, we
emphasize the concrete steps that we are currently
taking in order to be able carry out the detailed
evaluations necessary.
2 Natural language generation with
KPML
The KPML system is a mature grammar devel-
opment environment for supporting large-scale
grammar engineering work for natural language
generation using multilingual systemic-functional
grammars (Bateman et al, 2005). Grammars
within this framework consist of large lattices of
grammatical features, each of which brings con-
straints on syntactic structure. The features are
also linked back to semantic configurations so that
they can be selected appropriately when given a
semantic specification as input. The result of gen-
erating with a systemic-functional grammar with
KPML is then a rich feature-based representation
distributed across a relatively simple structural
backbone. Each node of the syntactic represen-
tation corresponds to an element of structure and
87
typically receives on the order of 50-100 linguistic
features, called the feature selection. Since within
systemic-functional grammars, it is the features
of the feature selection that carry most of the de-
scriptive load, we can see each feature selection as
an exhaustive description of its associated syntac-
tic constituent. Generation within KPML normally
proceeds on the basis of a semantic input specifi-
cation which triggers particular feature selections
from the grammar via a mediating linguistic ontol-
ogy.
The features captured in a systemic-functional
generation resource are drawn from the four com-
ponents of functional meaning postulated within
systemic-functional grammar: the ideational, ex-
pressing content-related decisions, the logical, ex-
pressing logical dependencies, the interpersonal,
expressing interactional, evaluative and speech act
information, and the textual, expressing how each
element contributes to an unfolding text. It is in
this extremely rich combination of features that
we see significant value in exploring the re-use of
such grammars for annotation purposes and cor-
pus enrichment.
For annotation purposes, we employ some
of the alternative modes of generation that
are provided by the full grammar development
environment?it is precisely these that allow for
ready incorporation and application within the cor-
pus annotation task. One of the simplest ways in
which generation can be achieved during grammar
development, for example, is by directly select-
ing linguistic features from the grammar. This can
therefore mimic directly the task of annotation: if
we consider a target sentence (or other linguistic
unit) to be annotated, then selecting the necessary
features to generate that unit is equivalent to anno-
tating that unit in a corpus with respect to a very
extensive set of corpus annotation features.
Several additional benefits immediately acrue
from the use of a generator for this task. First,
the generator actually constructs the sentence (or
other unit) as determined by the feature selection.
This means that it is possible to obtain immedi-
ate feedback concerning the correctness and com-
pleteness of the annotation choices with respect to
the target. A non-matching structure can be gener-
ated if: (a) an inappropriate linguistic feature has
been selected, (b) the linguistic resources do not
cover the target to be annotated, or (c) a combina-
tion of these. In order to minimise the influence
of (b), we only work with large-scale grammatical
resources whose coverage is potentially sufficient
to cover most of the target corpus. Further cor-
pus instances that lie beyond the capabilities of the
generation grammar used are an obvious source of
requirements for extensions to that grammar.
Second, the architecture of the KPML system
also allows for other kinds of annotation support.
During grammar development it is often required
that guidance is given directly to the semantics-
grammar linking mappings: this is achieved by
providing particular ?answers? to pre-defined ?in-
quiries?. This allows for a significantly more
abstract and ?intention?-near interaction with the
grammatical resource that can be more readily
comprehensible to a user than the details of the
grammatical features. This option is therefore also
available for annotation.
Moreover, the semantic specifications used rely
on a specified linguistic ontology that defines par-
ticular semantic types. These types can also be
used directly in order to constrain whole collec-
tions of grammatical features. Providing this kind
of guidance during annotation can also, on the one
hand, simplify the process of annotation while, on
the other, produce a semantic level of annotation
for the corpus.
In the following sections, we see a selection of
these layers of information working in annotation
in more detail, showing that the kinds of informa-
tion produced during generation corresponds ex-
tremely closely to the kinds of rich annotations
currently being targetted for sophisticated corpus
presentation.
3 Creating corpus annotations from
KPML output
3.1 KPML output
The output produced by KPML when being used
for generation is a recursive structure with the cho-
sen lexical items at the leaves. Figure 1 shows the
output tree for the sample sentence ?However they
will step up their presence in the next year?.
The nodes of this structure may be freely an-
notated by the user or application system to con-
tain further information: e.g., for passing through
hyperlinks and URLs directly with the semantics
when generating hypertext. Most users simply see
the result of flattening this structure into a string:
the generated sentence or utterance.
This result retains only a fraction of the in-
88
Figure 1: Tree generated by KPML
formation that is employed by the generator dur-
ing generation. Therefore, since we are using
the grammar development environment rather than
simply the generator component, we also have the
possibility of working directly with the internal
structures that KPML employs for display and de-
bugging of resources during development. These
internal structures contain a complete record of
the information provided to the generation pro-
cess and the generator decisions (including which
grammatical features have been selected) that have
been made during the construction of each unit.
This internal record structure is again a recursive
structure corresponding directly to the syntactic
structure of the generated result and with each
node having the information slots:
constituent:
{identifier, \\ unique id for the unit
concept, \\ link to the semantic concept expressed
spelling, \\ the substring for this portion of structure
gloss, \\ a label for use in inter-lineal glosses
features, \\ the set of grammatical features for this unit
lexeme, \\ the lexeme chosen to cover this unit (if any)
annotation, \\ user-specified information
functions \\ the grammatical functions the unit expresses
}
An extract from such an internal record structure
encoded in XML is given in the Appendix (5.1).
To support annotation, we make use of the XML-
export capabilities of KPML (cf. (Bateman and
Hartley, 2000)) in order to provide these com-
pleted structures in a form suitable for passing on
to the next stage of corpus annotation within an
XML-based multi-layer framework.
3.2 XML multi-layer representation
Systemic-functional analysis is inherently multi-
dimensional in that SFL adopts more than one view
on a linguistic unit. Here, we focus on three anno-
tationally relevant dimensions: axis (features and
functions), unit (clause, group/phrase, word, mor-
pheme) and metafunction (ideational, logical, in-
terpersonal and textual). Each metafunction may
chunk up a given string (e.g., a clause unit) in
Figure 2: Generation output viewed as multi-layer
annotation
<sfglayer metafunction="IDEATIONAL">
However,
<segment functions="AGENT">they</segment>
will step up
<segment functions="DIRECTCOMPLEMENT GOAL MEDIUM">
their presence
</segment>
<segment functions="TIMELOCATIVE">
in the next year
</segment>
.
</sfglayer>
Figure 3: Metafunction+Function layers
different ways, thus potentially creating overlap-
ping hierarchies. This is depicted schematically
for the running example in Figure 2. For instance,
in this example, according to the textual meta-
function, ?however they? constitutes a segment
(Theme) and according to the interpersonal meta-
function, ?they will? constitutes another segment
(Mood).
In order to be able to use the KPML output for
annotation purposes, we adopt a multi-layer model
that allows the representation of these different de-
scriptional dimensions as separate layers superim-
posed on a given string (cf. (Teich et al, 2005)).
The transformation from the KPML output to the
concrete multi-layer model adopted is defined in
XSLT.
From the KPML internal record structure we
use the information slots of identifier, spelling,
features, and functions. Each entry in the func-
tion slot is associated with one metafunctional as-
pect. For each metafunctional aspect, an annota-
tion layer is created for each constituent unit (e.g.,
a clause) holding all associated functions together
with the substrings they describe (see Figure 3 for
the ideational functions contained in the clause in
the running example).
An additional layer holds the complete con-
stituent structure of the clause (cf. Figure 4 for the
corresponding extract from the running example),
89
<constituent unit="-TOP-"
selexp="LEXICAL-VERB-TERM-RESOLUTION...">
<token features="HOWEVER">However,</token>
<constituent unit="TOPICAL"
selexp="THEY-PRONOUN...">
<token features="THEY PLURAL-FORM">they</token>
</constituent>
<token features="OUTCLASSIFY-REDUCED...">will</token>
<token features="DO-VERB...">step up</token>
<constituent unit="DIRECTCOMPLEMENT"
selexp="NOMINAL-TERM-RESOLUTION OBLIQUE...">
<constituent unit="DEICTIC"
selexp="THEIR GENITIVE NONSUPERLATIVE...">
<token features="THEIR PLURAL-FORM">their</token>
</constituent>
<token features="...COMMON-NOUN...">presence</token>
</constituent>
<constituent unit="TIMELOCATIVE"
selexp="IN STRONG-INCLUSIVE UNORDERED...">
<token features="IN">in</token>
<constituent unit="MINIRANGE"
selexp="NOMINAL-TERM-RESOLUTION...">
<token features="THE">the</token>
<constituent unit="STATUS"
selexp="QUALITY-TERM-RESOLUTION...">
<token features="...ADJECTIVE">next</token>
</constituent>
<token features="...COMMON-NOUN...">year .</token>
</constituent>
</constituent>
</constituent>
Figure 4: Constituent+Feature layer
i.e., the phrasal constituents and their features:
<constituent unit="..." selexp="...">
</constituent>
and the tokens and their (lexical) features:
<token features="..."> ... </token>
Thus, the KPML generation output, which di-
rectly reflects the trace of the generation process,
is reorganized into a meaningful corpus represen-
tation. Information not relevant to annotation can
be ignored without loss of information concerning
the linguistic description. The resulting represen-
tation for the running example is shown in the Ap-
pendix (5.2).2
4 Discussion
Although it is clear that the kind of informational
structures produced during generation with more
developed KPML grammars align quite closely
with that targetted by sophisticated corpus anno-
tation, there are several issues that need to be ad-
dressed in order to turn this process into a prac-
tical annotation alternative. Those which we are
currently investigating centre around usability and
coverage.
2To improve readability, we provide the integrated repre-
sentation rather than the stand-off representation which aligns
the different layers by using character offsets.
Usability/effort. Users need to be trained in pro-
viding information to guide the generation pro-
cess. This guidance is either in the form of di-
rect selections of grammatical features, in which
case the user needs to know when the features ap-
ply, or in the form of semantic specifications, in
which case the user needs information concerning
the appropriate semantic classification according
to the constructs of the linguistic ontology. One of
the methods by which the problem of knowing the
import of grammatical features may be alleviated
is to link each feature with sets of already anno-
tated/generated corpus examples. Thus, if a user
is unsure concerning a feature, she can call for
examples to be displayed in which the particular
linguistic unit carrying the feature is highlighted.
Even more useful is a further option which shows
not only examples containing the feature, but con-
trasting examples showing where the feature has
applied and where it has not. This provides users
with online training during the use of the system
for annotation. The mechanisms for showing ex-
amples and contrasting sets of generated sentences
for each feature were originally provided as part
of a teaching aid built on top of KPML: this allows
students to explore a grammar by means of the ef-
fects that each set of contrasting features brings
for generated structures. For complex grammars
this appears to offer a viable alternative to precise
documentation?especially for less skilled users.
Coverage. When features have been selected, it
may still be the case that the correct target string
has not been generated due to limited coverage
of grammar and/or semantics. This is indicative
of the need to extend the grammatical resources
further. A further alternative that we are explor-
ing is to allow users to specify the correspondence
between the units generated and the actual target
string more flexibly. This is covered by two cases:
(i) that additional material is in the target string
that was not generated, and (ii) that the surface
order of constituents is not exactly that produced
by the generator. In both cases we can refine the
stand-off annotation so that the structural result
of generation can be linked to the actual string.
Thus manual correction consists of minor align-
ment statements between generated structure and
string.
Certain other information that may not be avail-
able to the generator, such as lexical entries, can be
constructed semi-automatically on-the-fly, again
90
using the information produced in the generation
process (i.e., by collecting the lexical classifica-
tion features and adding lexemes containing those
features). This method can be applied for all open
word classes.
Next steps. In our future work, we will be car-
rying out an extensive annotation experiment with
the prediction that annotation time is not higher
than for interactive annotation from a parsing per-
spective. TIGER, for example, reports 10 min-
utes per sentence as an average annotation time.
We expect an experienced KPML user to be sig-
nificantly faster because the process of generation
or feature selection explicitly leads the annotator
through precisely those features that are relevant
and possible given the connectivity of the feature
lattice defined by the grammar. Annotation then
proceeds first by selecting the features that apply
and then by aligning the generated structure with
the corpus instance: both potentially rather rapid
stages. Also, we would expect to achieve similar
coverage as reported by (Baldwin et al, 2004) for
ERG when applied to a random 20,000 string sam-
ple of the BNC due to the coverage of the existing
grammars.
The results of such investigations will be SFL-
treebanks, analogous to such treebanks produced
using dependency approaches, LFG, HPSG, etc.
These treebanks will then support the subsequent
learning of annotations for automatic processing.
Acknowledgment. This work was partially supported
by Hessischer Innovationsfond of TU Darmstadt and PACE
(Partners for the Advancement of Collaborative Engineering
Education: www.pacepartners.org).
References
T. Baldwin, E. M. Bender, D. Flickinger, A. Kim, and
S. Oepen. 2004. Road-testing the EnglishResource
Grammar over the British National Corpus. In
Proceedings of the 4th International Conference on
Language Resources and Evaluation (LREC) 2004,
Lisbon, Portugal.
J. A. Bateman and A. F. Hartley. 2000. Target
suites for evaluating the coverage of text generators.
In Proceedings of the 3rd International Conference
on Language Resources and Evaluation (LREC),
Athens, Greece.
J. A. Bateman, I. Kruijff-Korbayova?, and G.-J. Krui-
jff. 2005. Multilingual resource sharing across
both related and unrelated languages: An imple-
mented, open-source framework for practical natu-
ral language generation. Research on Language and
Computation, 3(2):191?219.
J. A. Bateman. 1997. Enabling technology for multi-
lingual natural language generation: the KPML de-
velopment environment. Journal of Natural Lan-
guage Engineering, 3(1):15?55.
S. Bird and M. Liberman. 2001. A formal framework
for linguistic annotation. Speech Communication,
33(1-2):23?60.
S. Brants, S. Dipper, S. Hansen, W. Lezius, and
G. Smith. 2002. The TIGER treebank. In Proceed-
ings of the Workshop on Treebanks and Linguistic
Theories, Sozopol.
A. Cahill, M. McCarthy, J. van Genabith, and A. Way.
2002. Automatic annotation of the Penn-Treebank
with LFG f-structure information. In Proceedings of
the 3rd International Conference on Language Re-
sources and Evaluation (LREC) 2002, Las Palmas,
Spain.
A. Copestake and D. Flickinger. 2002. An open-source
grammar development environment and broad cov-
erage English grammar using HPSG. In Proceed-
ings of the 2nd International Conference on Lan-
guage Resources and Evaluation (LREC), Athens,
Greece.
A. Frank, L. Sadler, J. van Genabith, and A. Way.
2003. From treebank resources to LFG f-structures.
Automatic f-structure annotation of treebank trees
and CFGs extracted from treebanks. In A. Abeille,
editor, Treebanks. Building and using syntactically
annotated corpora, pages 367?389. Kluwer Aca-
demic Publishers, Dordrecht, Boston, London.
MAK Halliday. 2004. Introduction to Functional
Grammar. Arnold, London.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: the Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
S. Oepen, E. Callahan, D. Flickinger, C. D. Manning,
and K. Toutanova. 2002. LinGO Redwoods. A rich
and dynamic treebank for HPSG. In Workshop on
Parser Evaluation, 3rd International Conference on
Language Resources and Evaluation (LREC), Las
Palmas, Spain.
C.M. Sperberg-McQueen and C. Huitfeldt. 2001.
GODDAG: A Data Structure for Overlapping Hi-
erarchies. In Proceedings of PODDP?00 and
DDEP?00, New York.
E. Teich, S. Hansen, and P. Fankhauser. 2001. Rep-
resenting and querying multi-layer corpora. In
Proceedings of the IRCS Workshop on Linguistic
Databases, University of Pennsylvania, Philadel-
phia.
E. Teich, P. Fankhauser, R. Eckart, S. Bartsch, and
M. Holtz. 2005. Representing SFL-annotated cor-
pus resources. In Proceedings of the 1st Computa-
tional Systemic Functional Workshop, Sydney, Aus-
tralia.
91
5 Appendix
5.1 Extract from generation record (clause level)
<example>
<name>REUTERS29</name>
<generatedForm>However, they will step up their presence in the next year.</generatedForm>
<targetForm>But they will step up their presence in the next year.</targetForm>
<structures><constituent id="G3324" semantics="STEP-3278">
<functions>
<function metafunction="UNKNOWN">SENTENCE</function></functions>
<features/>
<subconstituents><constituent id="G3308" semantics="RR62-3289">
<functions>
<function metafunction="TEXTUAL">TEXTUAL</function>
<function metafunction="TEXTUAL">CONJUNCTIVE</function></functions>
<features>
<f>HOWEVER</f></features>
<subconstituents><string>However,</string></subconstituents>
</constituent><constituent id="G3310" semantics="PERSON-3291">
<functions>
<function metafunction="TEXTUAL">TOPICAL</function>
<function metafunction="INTERPERSONAL">SUBJECT</function>
<function metafunction="UNIFYING">ACTOR</function>
<function metafunction="IDEATIONAL">AGENT</function></functions>
<features/>
<subconstituents><constituent id="G3309" semantics="PERSON-3291">
<functions>
<function metafunction="LOGICAL">THING</function></functions>
<features>
<f>THEY</f>
<f>PLURAL-FORM</f></features>
<subconstituents><string>they </string></subconstituents>
</constituent>
</subconstituents>
</constituent><constituent id="G3311" semantics="ST59-3280-3297-3302">
<functions>
<function metafunction="LOGICAL">TEMPO0</function>
<function metafunction="INTERPERSONAL">FINITE</function></functions>
<features>
<f>OUTCLASSIFY-REDUCED</f>
<f>OUTCLASSIFY-NEGATIVE-AUX</f>
<f>FUTURE-AUX</f>
<f>PLURAL-FORM</f>
<f>THIRDPERSON-FORM</f></features>
<subconstituents><string>will </string></subconstituents>
</constituent><constituent id="G3312" semantics="STEP-3278">
<functions>
<function metafunction="UNIFYING">AUXSTEM</function>
<function metafunction="LOGICAL">VOICE</function>
<function metafunction="LOGICAL">LEXVERB</function>
<function metafunction="LOGICAL">PROCESS</function></functions>
<features>
<f>DO-VERB</f>
<f>EFFECTIVE-VERB</f>
<f>DISPOSAL-VERB</f>
<f>STEM</f></features>
<subconstituents><string>step up </string></subconstituents>
</constituent><constituent id="G3316" semantics="PRESENCE-3292-3306">
<functions>
<function metafunction="IDEATIONAL">DIRECTCOMPLEMENT</function>
<function metafunction="IDEATIONAL">GOAL</function>
<function metafunction="IDEATIONAL">MEDIUM</function></functions>
</constituent></subconstituents></constituent></structures>
<selectionexpressions>
<selexp sem="STEP-3278"><unit>-TOP-</unit><f>LEXICAL-VERB-TERM-RESOLUTION</f>
<f>DO-NEEDING-VERBS</f><f>AUXSTEM-VOICE</f><f>REAL</f><f>NON-MOTION-CLAUSE</f>
<f>PLURAL-FINITE</f><f>PLURAL-SUBJECT</f><f>TOPICAL-INSERT</f> ...
</selexp>
<selexp>...</selexp>
...
</selectionexpressions>
</example>
5.2 Multi-layer representation of generation record
Metafunction+Function layers
<sfglayer metafunction="UNKNOWN">
<segment functions="SENTENCE">
However, they will step up their presence in the next year .
</segment>
</sfglayer>
<sfglayer metafunction="UNIFYING">
However,
<segment functions="ACTOR">they</segment>
will
<segment functions="AUXSTEM">step up</segment>
their presence in the next year .
</sfglayer>
<sfglayer metafunction="TEXTUAL">
<segment functions="TEXTUAL CONJUNCTIVE">However,</segment>
<segment functions="TOPICAL">they</segment>
will step up their presence in the next year .
</sfglayer>
92
<sfglayer metafunction="LOGICAL">
However,
<segment functions="THING">they</segment>
<segment functions="TEMPO0">will</segment>
<segment functions="VOICE LEXVERB PROCESS">step up</segment>
<segment functions="THING">their</segment>
<segment functions="THING">presence</segment>
in the
<segment functions="QUALITY">next</segment>
<segment functions="THING">year .</segment>
</sfglayer>
<sfglayer metafunction="INTERPERSONAL">
However,
<segment functions="SUBJECT">they</segment>
<segment functions="FINITE">will</segment>
step up
<segment functions="DEICTIC">their</segment>
presence in
<segment functions="DEICTIC">the</segment>
next year .
</sfglayer>
<sfglayer metafunction="IDEATIONAL">
However,
<segment functions="AGENT">they</segment>
will step up
<segment functions="DIRECTCOMPLEMENT GOAL MEDIUM">
their presence
</segment>
<segment functions="TIMELOCATIVE">
<segment functions="MINORPROCESS">in</segment>
<segment functions="MINIRANGE">
the
<segment functions="STATUS">next</segment>
year .
</segment>
</segment>
</sfglayer>
Constituent+Feature layer
<constituent id="G3324" unit="-TOP-"
selexp="LEXICAL-VERB-TERM-RESOLUTION DO-NEEDING-VERBS AUXSTEM-VOICE REAL NON-MOTION-CLAUSE TOPICAL-INSERT ...">
<token features="HOWEVER">However,</token>
<constituent id="G3310" unit="TOPICAL"
selexp="THEY-PRONOUN NONDEMONSTRATIVE-SPECIFIC-PRONOUN NOMINATIVE NONSUPERLATIVE NONREPRESENTATION NONPARTITIVE ...">
<constituent id="G3309" unit="TOPICAL">
<token features="THEY PLURAL-FORM">they</token>
</constituent>
</constituent>
<token
features="OUTCLASSIFY-REDUCED OUTCLASSIFY-NEGATIVE-AUX FUTURE-AUX PLURAL-FORM THIRDPERSON-FORM">
will
</token>
<constituent id="G3312" unit="-TOP-">
<token features="DO-VERB EFFECTIVE-VERB DISPOSAL-VERB STEM">
step up
</token>
</constituent>
<constituent id="G3316" unit="DIRECTCOMPLEMENT"
selexp="NOMINAL-TERM-RESOLUTION OBLIQUE NONSUPERLATIVE NONREPRESENTATION NONPARTITIVE NONQUANTIFIED NOMINAL-GROUP ...">
<constituent id="G3314" unit="DEICTIC"
selexp="THEIR GENITIVE NONSUPERLATIVE NONREPRESENTATION NONPARTITIVE NONQUANTIFIED NOMINAL-GROUP ...">
<constituent id="G3313" unit="DEICTIC">
<token features="THEIR PLURAL-FORM">their</token>
</constituent>
</constituent>
<constituent id="G3315" unit="DIRECTCOMPLEMENT">
<token
features="OUTCLASSIFY-PROPERNOUN NOUN COMMON-NOUN COUNTABLE SINGULAR-FORM NOUN">
presence
</token>
</constituent>
</constituent>
<constituent id="G3323" unit="TIMELOCATIVE"
selexp="IN STRONG-INCLUSIVE UNORDERED TEMPORAL-PROCESS LOCATION-PROCESS SPATIO-TEMPORAL-PROCESS PREPOSITIONAL-PHRASE ...">
<token features="IN">in</token>
<constituent id="G3322" unit="MINIRANGE"
selexp="NOMINAL-TERM-RESOLUTION OBLIQUE NONSUPERLATIVE NONREPRESENTATION NONPARTITIVE NONQUANTIFIED NOMINAL-GROUP ...">
<token features="THE">the</token>
<constituent id="G3320" unit="STATUS"
selexp="QUALITY-TERM-RESOLUTION SIMPLEX-QUALITY NOTINTENSIFIED NONSCALABLE CONGRUENT-ADJECTIVAL-GROUP ...">
<constituent id="G3319" unit="STATUS">
<token features="OUTCLASSIFY-DEGREE-ADJ ADJ-NEUTRAL-FORM ADJECTIVE">
next
</token>
</constituent>
</constituent>
<constituent id="G3321" unit="MINIRANGE">
<token features="OUTCLASSIFY-PROPERNOUN NOUN COMMON-NOUN COUNTABLE SINGULAR-FORM NOUN">
year .
</token>
</constituent>
</constituent>
</constituent>
</constituent>
93
Matching a tone-based and tune-based approach to English 
intonation for concept-to-speech generation 
Elke  Teich 
Universitgt des Saarlandes, Saarbr{icken & University of Sydney 
Cather ine  I. Watson  and Cdci le Pere i ra  
Macquarie University, Sydney 
Abst ract  
Tlle paper describes the results of a compari- 
son of two annotation systems for isstoslal;ion, 
the tone-based ToBI al)proach and the 1;une- 
based api)roach proposed by Systemic Func- 
ti(mal Grammar (SFO). The goal of this compar- 
ison is to detine a mapping between the two sys- 
tems tbr the purpose of concept-to-speech gen- 
eration of English. Since ToB: is widely used 
in Sl)eech synthesis and SFG is widely used in 
nal;ural language generation and oft~rs a lin- 
guistically motivated aecollnt of intonation, it; 
appears a promising step to comt)ine the two 
approaches for concept-to-speech. A corpus of 
English utterances has been analysed with both 
~\].~()13I and SFG categories; eomparison of the 
analysis results has lead to the identification of 
some basic equivalents between the two systems 
on which a mapping can be based. 
1 In t roduct ion  
The pallet describes the main results of a con> 
parison of /;he ToB: (Tone-and-Break-Indices) 
ai)proach (Pierrehumbert, 1.9801 Silverman el; 
al.., 19961 to annotating English speech data 
with information about intonation and one of 
the British School approaches (e.g., Brazil et al 
(1980)), Systenfie Fmmtional Grammar (SFO; 
(Halliday, 19671 Halliday, 1970)). The goal of 
this comparison is the definition of a mapping 
between the two systems. 
This attempt has a two-fbld motiw~tion. 
First, it is motivated by computational ppli- 
cation in concept-to-si)eech systems, in which 
text in spoken mode is automatically generated 
from an underlying abstract lneaning represen- 
tation, it is widely acknowledged that in order 
for spoken language technology to gain wider 
acceptance, it has to improve on the quality of 
output considerably. Itere, appropriate intona- 
tion is one of the major factors (ct'. Cole et 
al. (1995)). The concrete goal we are pursu- 
ing is to connect an oil-the-shelf speech syn- 
thesizer for English (FESTIVAL; (Black et al, 
1998)) with an automatic text generation sys- 
tem tbr English based on SFO (Matthiessen &
Bateman, 19911. Since in the SFO approach, in- 
tonation is accounted for as part of grammar 
rather than as an independent component, it is 
straightforward to extend the grammatical re- 
sources of a systemically based text generation 
system with an account of intonation (cf Teich 
et al (1.997) iml)lenmnting such all approach for 
German concet/t-to-speech generation). Con- 
necting such a system to a speech synthesizer 
requires mapping the OUtl)ut of the generator 
to the input requirements of the st)eech synth(> 
sizer. In the FESTIVAL systei11, the intonation of 
the text to be synthesized can be manipulated 
1)y ~mnotation with TOBI labels. Therefore, a
mapl)ing betweeIl the SFC and the ToBI anno- 
tation systems is required. 
Second, there is a theoretical lnotivation. 
With a mapping between tile ToBI and the slpo 
systems for intonation almotation, it will be 
possible to link the 1)honetic analysis of speech 
data to an interpretation f intonational mean- 
ing as it is proposed by SFO. Existing speech 
corpora that are acoustically analysed and an- 
notated with ToBI tail then be used to test 
some of the assumptions brought forward by 
SFO about the natm:e of intonation. Also, with 
a mapping between ~oBI and SFG annotations, 
an exchange of annotated corpora between ToBI 
and SFO users would be possible. 
We report on the analysis of a sl)eech cor- 
pus compiled fl'om Halliday (1970) with ToBI 
and SFO labels (See. 3). The intonation analy- 
sis is based on an acoustic analysis of the speech 
data in terms of fundamental frequency (F0). 
829 
The data are represented in EMU (Cassidy &; 
Harrington, 1996), a database system for stor- 
ing speech data that provides for a nmltiple- 
tier analysis of acoustic (e.g., F0 contour and 
speech wavetbrm) and phonological (segmental 
and suprasegmental) features. We present he 
major differences and commonalities between 
ToBI and SFO (See. 2). On the basis of the 
corpus analysis, we identify matches between 
the tunes assmned by Halliday and unique se- 
quences of To\]~I tones (See. 4). We conclude 
with a smmnary and a sketch of future work. 
2 In tonat ion  Annotat ion  
The majority of text-to-speech systems that al- 
low for the manipulation of an input string so 
as to control intonation employ the ToBI system 
(Silverman et al, 19961, which is based on the 
autosegmental-metrical approach originally set 
up by Pierrehumbert (19801 to describe Amer- 
ican English intonation. Versions of ToBI for 
other languages have been developed, e.g., Grice 
et al (19961 for German, and are also widely 
used in computational contexts. One major the- 
oretical difference between the ToBI approach 
and the British School approaches, uch as the 
one advocated by SFG, is that in the latter there 
is a built-in focus on the relation between into- 
mttion and nmaning. In spG, intonation con- 
tours are distinguished according to their di, f
fcrcntial meanings, i.e., they label pitch move- 
ments that are commonly interpreted by the 
speakers of (British) English as having quite 
different pragmatic purport (cf. Teich et al 
(1997)). This is what snakes the SFO approach 
attractive in the context of concept-to-speech 
generation, in which it is crucial to be able 
to represent criteria for selecting an intonation 
contour appropriate in a given context. TOBI, 
on the other hand, is a phonetic-phonological 
annotation scheme tbr intonation. Since it is 
widely used, there exist nmnerous tools sup- 
porting analysis with a high degree of analyt- 
ical rigor. It seems theretbre doubly significant 
to combine the two approaches in an attempt o 
achieve high-quality synthesized speech output. 
While clearly some fimdamental theoretical 
ditferences exist between the ToBI and SFG ap- 
proaches, more technically there is a basic com- 
mortality. Any annotation scheme tbr intonation 
nmst establish three principal constructs for the 
representation f intonation: the units of into- 
nation, a set of categories that describe the pitch 
movement occurring in that unit, and a set of 
labels that mark the nuclear stress oi1 which the 
pitch movement is realised. 
In the remainder of this section we briefly de- 
scribe how these constructs are realised in ToBI 
(Sec. 2.1) and in SFG (See. 2.2) and sketch the 
mQor differences between them. 
2.1 ToBI  
There are two tiers to the ToBI analysis, the 
tonal analysis and the analysis of the strength 
of the word boundaries, which is referred to as 
the "break index". The Tom tones are either 
high (H) or low (L). The break index gives the 
strength of a word's association with the tbl- 
lowing word, where 0 is the strongest perceived 
conjoining and 4 is the most disjoint (Beckman 
gc Ayers, 19971. In our analysis (See. 3), we 
only consider the tonal part of TOBI. 
The Tom intonational phonology model 
aligns a tune with the words of an utterance 
(cf. Harrington 8c Cassidy (1999)), wherc some 
of these words are accented. The words of an 
utterance are grouped into phrases. There are 
two types of phrases, intonational and inter- 
mediate ph, mses. Utterances always consist of 
one or more intonational phrases which iu tm:n 
consist of one or lnore intermediate phrases. 
The break between two intonational 1)hrases is 
greater than 1)etween two intermediate )hrases, 
the bl'eak index being 4 in the former case and 
3 or 2 in the latter. 
Words that have prominence in a phrase or 
utterance m:e accented (sentence level stress). 
Unlike lexical stress which is usually fixed, sen- 
tence level stress is variable. When a word 
carries sentence level stress, a pitch accent is 
associated with the syllable of primary stress. 
Pitch accents are denoted by *. The most com- 
mon pitch accent is an H*, which is usually 
realised as a pitch peak near tim vowel in the 
primary stressed syllable, it is also possible to 
have pitch accents which are a combination of a 
pitch movement towards and including a peak 
or trough. One sudl bitonal accent is L+H*, 
which moves from a low in pitch towards a high. 
Intermediate and intonational phrases carry 
edge tones. Intermediate phrases carry phrase 
tones, indicated by - .  The phrase tone L- is low 
pitch following the final pitch accent of a phrase. 
830 
H* 
Margaret's looking for you 
L% 
L- 
c)  
eq 
C) 
C) 
C) 
22600 22800 23000 23200 23400 
time (ms) 
(a) 
23600 23800 
is there any more news of the French 
o H* L* 
elections 
H% 
H- 
24500 25000 25500 26000 
time (ms) 
(b) 
in the far 
H* 
25000 
H% 
L- 
corner of that field the 
!1t* 
 FI.T tUK, 
25500 26000 26500 
footpath 
L% 
L- 
goes over a stile 
H* 
27000 27500 28000 
time (ms) 
(c) 
Figure 1: Examples of' the pitch contours of three utterances in the corpus, mid the associated ToI\]I 
labels 
831 
tone 1 
tone 2 
tone 3 
tone 4 
tone 5 
\ (fidl) 
conveys certainty 
/ (rico) 
conveys uncertainty 
- -  (level/low rise) 
"continuation tone" 
\ /  (fall-rise) 
seems certain (reservation) 
/ \  (rise-fall) 
seems uncertain (strongly assertive) 
Figure 2: SFC tones and their meanings 
The phrase tone H- represents high pitdt follow- 
ing the last pitch accent. Tile tone associated 
with an intonation phrase is a boundary tone 
and is indicated by %. The boundary tone H% 
represents a final rise and the L% boundary tone 
is typically interpreted as the absence of a final 
rise (cf. Ladd (1996)). 
Every intermediate phrase must have at least 
one pitch accent. By definition, the last ac- 
cented word in any intermediate phrase is al- 
ways the nuclear accented word, and it is usu- 
ally perceived as more prominent han any other 
accented word. The utterance (a) in Fig. 1 is 
produced by an H'L-L% combination and typ- 
ically interpreted as a neutral declarative. The 
second utterance (b) has a H 'L 'H -H% combi- 
nation (yes/no question). The final example 
(c) illustrates a complex ntterance, made up of 
more than one intonation phrase. 
2.2 SFG 
According to SFG the unit to which intonation 
is attr ibuted is the tone group. A tone group 
consists of.feet, and feet consist of syllables. A 
tone group carries a tune or tone, which can be 
falling (tone 1), rising (tone 2), level (tone 3), 
faning-risiug (tone 4), or rising-f~lling (tone 5). 
See Fig. 2 giving these five options with their 
approximate pragmatic meanings. The exam- 
ples in Fig. 3 show how tone is annotated in 
SFG: the nmnber gives the kind of tone, the 
double slashes snark the tone group boundaries 
and the single slashes mark feet. Also, there 
may be combinations of different; tones in one 
utterance, e.g., tone 4 followed by tone 1 (ex- 
ample (c) in Fig. 3). 
Each tone group contains an element which 
carries the nuclear stress, called Tonic. In the 
default case, the Tonic is placed on the last lex- 
(a) / /1 Margaret's / looking for you / /  
(b) / /2 A is there / any more / news of the / Iq-ench 
e/ leet ions/ /  
(c) / /4  A in the/ far corner of that/f ield the / /  1 foot- 
path goes / over a / sq;i\]e_// 
Figure 3: Examples of SFG labelling 
ical elenmnt in tile tone group (unmarked nu- 
clear stress). In marked cases, the Tonic can 
be placed on other elements in the tone group. 
For an example of the tbrmer see (b) in Fig. 3 
(Tonic denoted by underlining); an example of 
the latter is (a) in Fig. 3. 
The Tonic represents the nuclear stress and 
is part of the tonic segment of the tone group. 
If the Tonic does not fall on the frst  syllable of 
the tone group, there is an element preceding 
it, called the pretonic segment. It carries a so- 
called Pretonic stress (see (b) in Fig.3). 
2.3 P re l iminary  compar i son  
On a technical level, the major differences we 
can observe between the ToBI and SFG annota- 
tion schemata of intonation are the following. 
Uni ts .  While there is a rough cor- 
respondence between the intonation 
phrase/intermediate phrase in ToB~ and 
the tone group in SFG (cf. Harrington & 
Cassidy (1999)), in Tom the refit of the foot is 
not acknowledged. 
P i tch  movement .  While in ToBI, the prim- 
itives of description of pitch movement are dis- 
tinct highs (It) and lows (L), where a particular 
pitch movement is described by a sequence of 
highs and/or lows in the pitch, in SFC the prim- 
itive of description is the tune, i.e., a relative 
concept, such as a rising, falling or level tune. 
Nuc lear  stress.  While in ToBI, the mmlear 
stress is marked by the last starred tone in the 
sequence of tones and is thus only implicitly in- 
dicated in the annotation, SFG marks nuclear 
stress explicitly by marking up the Tonic) 
While there is a basic match in terms of ac- 
counting for the pitch movement and we cast 
thus expect to be able to recast ToBI tone se- 
quences as SFC tones, we may encounter some 
problems due to the non-acknowledgement of 
tile unit of foot in ToBI on the one hand, and 
due to ToBI marking up pitch accents other 
ICE Sec. 2.1, however: the nuclear stress in Tom is 
by definition the last starred tone. 
832 
than the nuclear stress, on the other hand. 
3 Method 
3.1 The Corpus  
The eorl)us was obtained from tlm recorded (lat~ 
which colnes with Italliday (1970). We inv(;sti- 
gated tones 1, 2, and/l, and tone sequen('es 1 & 
1, l&  2, 2 & l, 2 & 2, l & 4, mid4  & 1. A 
total of 290 utterances were analysed (= 1700 
words of text, approx. 350 tone groul)s). The 
utter~mces ranged fl:om inono- and polysyllabic 
words to sentences. The utterances varied in 
tone, number of feet, the position of the Tonic, 
and whether there were silent t)eats in the tone 
group. Also, some of the utteran(:es had a pre- 
tonic segmenl;, others did not. 
3.2 Labe l l ing  
The labelling of the data a(:(:or(ling to SFG (:ri- 
teria was obtained from Halliday (1970). The 
labelling of the dater using ToBI was done l)y 
a trained acoustic l)honeti(:ian. 2 The exisl;ing 
recording was digitised at 20 kltz as 16 bit san> 
ples, and stored on a Unix machine. The pitch 
tracks were calculated using ESPS WAVES+. 
The labelling of the data was done in F, MU 
(Cassidy & Harrington, 1996). All the intona- 
tional and inl;ermedit~te l)hrases were marked, 
as', were the pit(:h ac(',ents, 1)hrasal and 1)oun(l- 
ary tones. 
4 Resu l ts  
The first l)art of the study estaMished that there 
is a basic eorresl)onden(:e l) tween the SFG tones 
mid particular sequences of ToBI lal)els tbr the 
simplest possible utterances, i.e., those consist- 
ing of a tonic segment only. As can be seen from 
~l~,l)le 1, tone 1 usually corresponds to H'L-L%, 
tone 2 to L 'H-H% and tone 4 to II*L-H%. a 
These siml)le milts usually have one pitch a('- 
cent and (;oincide with one intonation t)hrase 
(:(resisting of one internmdiate 1)hrase. 
In a second step, we looked at the more com- 
plicated utterances, i.e., those with a pretonic 
segment, and those consisting of a sequence of 
tone groups. In these cases there is usually more 
2The phonetieimt was aware of the Sl.'(' analysis. How- 
ever, the ToBI analysis was done listening to the audio 
files and looking at the pitch plots. 
aThis confirms e.g., Ladd (1.996) stating that the 
British-style "nuclear-tones" are merely the specific ont- 
binations of accents and edge tones. 
than one l)itch accent per utterance. Further, if 
the utterance has a Pretonic, there is always a 
pitch accent in that segment. Also, what can 
lie seen here is that there is no more than one 
internlediate l)hrase per tone group, and more 
than (}lie tone group per intonation phrase. 
Table 2 gives |;lie ToBI seqllenee for the ut- 
t;eran(;es which include a pretonic segnle.nt. The 
results are essentially the same as for the sin> 
ph; utterances (~151)le 1). One small difference is
that tone 1 and tone 4 can have either an H* or 
a !H* nuclear accent. This however is expected, 
|)ecause it simply means that although the nu- 
clear accent is high, it is down-stepped from an 
earlier It* accent. 
rl'al)le, 3 gives the TOBI S(Xluences for utter- 
anees consisting of SF(; tone groul) sequences. 
The Toni analysis tbr the final tone in a se- 
quence are essentially the same as tbr the utter- 
anees given in Table 2. The first tone group in 
a se(lllen(;e is more often than not an interme- 
(liate t)hrase rather than a separate intonation 
l)hrase. Itowevei', keel)ing in mind the dominat- 
ing intonation 1)hrase, the ToBI sequences for 
the first elenmnt in a sequence are essentially 
the same as t'omld for utterances with a 1)re- 
toni(: clement (Table 2). The results shown in 
Tal)les 1, 2, and 3 taken together show that tbr 
tones 1, 2, 4 there is one corresponding 'l.'oBI se- 
quence each tlmt characterizes tile interval 1)e- 
tween the nuclear accented word and the edge 
of the 1)hrase regardless of the complexity of 
the ul;terance. 
We also tbund a very close correspondence 
between the ~ibnic in SFG and the nuclear ac- 
cented syllable in the Tom analysis: In virtu- 
ally all cases they were in exactly the same place 
in the analyses. When the utteran(:es are more 
(:on lplex, e.g., they have a 1)retonic segment, or 
consist of sequences, in l;he ToBI analysis 1)itch 
accents are also lint in other places, not just 
on the mmlear accented syllaMe. ToBI analysis, 
unlike SFC, allows for more than just the nu- 
clear accented syllable to be marked up. The 
extra pitch accents from the ToBI analysis are 
potential ly a problem for a ToBI-SFG mapping. 
However, closer examination of the placelnent of 
these other 1)itch accents revealed that they al- 
ways fall on the first syllable of a foot (also when 
that is not the one carrying the nuclear stress). 
This suggests that the SFG feet can give some 
833 
information about where these other pitch ac- 
cents are likely to tM1 or, that these other pitch 
accents may be an indication of toot boundaries. 
5 Conc lus ions  
In this paper we have presented the results 
of a comparison between the ToBI and the 
SFG systems for analysing intonation. The 
goal of this comparison has been to establish 
equivalents between them. The motivation be- 
hind this is to make the two systems collabo- 
rate in concept-to-speech generation: Tom is a 
phonetic-phonological approach to the deserip- 
tion of intonation, SFG offers a linguistic ap- 
proach to intonation, tbcusing on the meaning- 
ful intonation patterns. ToBI i8 widely used in 
speech synthesis, SFG is widely used in natu- 
ral language generation. It seems therefore a 
promising step to combine the two approaches 
tbr concept-to-speech generation. 
Through this study we have established some 
basic matches between SFG tones and ToBI se- 
quences of pitch accents and edge tones. Here, 
we have concentrated on the SFG tones 1, 2 and 
4. We have analysed tones 3 and 5 as well and 
identified their ToBI equiwdents using the same 
method (cf. Sections 3 and 4). In the next step 
we will integrate the SFG description of intona- 
tion for English in the existing SFG-based Pen- 
man generation system and then interface the 
FESTIVAL synthesizer with the generator using 
the correspendences tablished by our analy- 
ses. 
In another step of analysis we will look more 
closely at other kinds of realization of nuclear 
stresses, such as bitonal pitch accents, to es- 
tablish whether they reflect linguistic meanings. 
What also remains to be investigated is the as- 
signment of pitch accents other than the nuclear 
stress. Nuclear stress can be predicted on the 
basis of linguistic and pragmatic information, 
but it is not clear under which conditions other 
pitch accents hould be placed. Our observation 
above (See. 4) that pitch accents other than the 
nuclear stress are typically placed on the first 
syllable of a foot may be a possible motivation. 
We are aware that there is controversy among 
researchers about rhythm. However, if it turns 
out that rhythm is a useful concept in the pre- 
diction of non-nuclear pitch accents, then we 
will consider including it in our approach. 
6 Acknowledgements  
We thank J. Harrington, C. Matthiessen, M. Hall- 
iday and the anonymous reviewers for their useful 
comments. 
References  
M. E. Beckman & G. M. Ayers. 1997. Guidelines 
for ToBI labeling (Version 7.0). Ohio State Uni- 
versify. (ling.ohio-state.edu/Phoneties/E-ToBI). 
A. Black, P. Taylor, & R. Caley. 1998. The FESTI- 
VAL speech synthesis ystem; system documen- 
tation, (Version 1.3.1). University of Edinburgh. 
(www.cstr.ed.ac.uk/projects/festival/). 
D. Brazil, M. Coulthard, & C. Johns. 1980. Dis- 
course Intonation and Language Teaching. Long- 
lIlan, London. 
S. Cassidy & J. Harrington. 1996. Emu: An en- 
hanced hierarchical speech data management sys- 
tem. PTvceedings o\]" the 6th Australian Interna- 
tional Conference on Speech Science and Technol- 
ogy , pp. 361--366. 
R.A. Cole, J. Marimfi, H. Uszkoreit, A. Zae- 
hen, & V. Zue. 1995. Survey of the State 
of the Art in Human Language Technology. 
(c.sht.cse.ogi.edu/nI~Tsurvey/ItLTsurvey.html). 
M. Grice, M. Reyelt, R. Benzlniiller, J. Mayer, & 
A. Batliner. 1996. Consistency in transcription 
labelling of German intonation with GTom. Pw- 
ceedings of the 4th International Conference on 
Spoken Language Processing, pp. 1716-1719. 
M. A.K. Halliday. 1967. Intonation and Grammar 
in British English. Mouton, The Hague. 
M. A.K. Halliday. 1970. A Course in Spoken En- 
glish: Intonation. Oxford University Press, Ox- 
ford. 
J. Harrington & S. Cassidy. 1999. Tcch, niques in 
Speech Acoustics. Kluwer Academic Publishers, 
Dordrecht. 
D.R. Lad& 1996. Intonational Phonology. Cam- 
bridge University Press, Cmnbridge. 
C. M.I.M. Matthiessen & J. A. Bateman. 1991. Text 
Generation and Systemic Functional Linguistics: 
Experiences from English and Japanese. Pinter, 
London. 
J. B. Pierrehulnbert. 1980. The phonology and pho- 
netics of English intonation. Ph.D. thesis, MIT. 
K. Silverman, M. Beckman, J. Petrelli, M. Osten- 
dorf, C. Wightman, P. Price, J. Pierrehumbert, 
& J. Hirschberg. 1996. ToBI: A standard tbr 
labelling English prosody. Proceedings of ICSLP 
92, volmne 2, pp. 867-870. 
E. Teich, E. Hagen, B. Grote, &: J. Bateman. 1997. 
From communicative context o speech: Integrat- 
ing dialogue processing, speech production, and 
natural anguage generation. Speech Communici- 
ation, 21:73-99. 
834 
Hallid;wan descrit)tion Toll(~ ql.'olu descril)tion 
Tonic:l foot 1 H*I,-L% (20) 
and 1 or more 2 L*II-H% (20) 
syllables 4 H'L-H% (19) 
Tonic:l in- 1. H'L-L% (18) 
coml)lete foot 2 L'H-H% (9) 
,% 1. foot 4 H*I,-H% (10) 
Tonic:>1 foot (first might 1. H'L-L% (17), L+H*L-L% (1), 
be incomplete) II*L-H*L-L% (1), H*L-!II*L-L% (1) 
2 I1*It-It% (1), L'H-It% (10) 
4 tt*H-It% (1), IVL-It% (9) 
2~d)lc' 1: Simple tone groups 
Ibdlidayan descril)tion Tone ToBI description 
1 (4O) !H'L-L% (18), H'L-L% (22) 
Pretonic + tonic with 1. or > 1 feet 2 (20) L'H-H% (20) 
4 (19) !H'L-n% (12), H'L-H% (7) 
Table 2: tivOli(? groups with a Pr(~,tOlfi(: 
2bnes Tone & ToB\] descrit)tion 
1 & 1 Tone 1 
(20) !It*L- (4), H'L-L% (5), It*L-(11) 
2 & 1 Tone 2 
(10) L'It- (5), L*It-tt% (4), L+It*H- (1) 
1 ~Q, 2 Tone 1
(9) H'L- (9) 
2 & 2 Tone 2 
(9) It*It- (1), II*L-It% (1), L*It- (8) 
1 & 4 Tone 1 
(10) It*L- (1.0) 
4 ck5 1. ~lbne 4 
(10) !H'H- (1), !H*L-II% (3), 1I*I:It% (6) 
Ton(; 1 
I~*L-L% (20) 
Tone 1 
H'L-L% (6), !H'L-L% (2), L'L-L% (2) 
Tone 2 
L*II-H% (9) 
Tone 2 
L'H-H% (10) 
Tone 4 
H'L-It% (10) 
Tone 1 
H'L-L% (9),!It*L-L% (1) 
2}d)lc 3: Tone group sequences 
835 
Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 59?68,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Scientific registers and disciplinary diversification:
a comparable corpus approach
Elke Teich
Universita?t des Saarlandes
e.teich@mx.uni-saarland.de
Stefania Degaetano-Ortlieb
Universita?t des Saarlandes
s.degaetano@mx.uni-saarland.de
Hannah Kermes
Universita?t des Saarlandes
h.kermes@mx.uni-saarland.de
Ekaterina Lapshinova-Koltunski
Universita?t des Saarlandes
e.lapshinova@mx.uni-saarland.de
Abstract
We present a study on linguistic con-
trast and commonality in English scien-
tific discourse on the basis of a mono-
lingually comparable corpus. The focus
is on selected scientific disciplines at the
boundaries to computer science (compu-
tational linguistics, bioinformatics, digital
construction, microelectronics). The data
basis is the English Scientific Text Cor-
pus (SCITEX) which covers a time range
of roughly thirty years (1970/80s to early
2000s). In particular, we investigate the
disciplinary diversification/relatedness of
scientific research articles in terms of reg-
ister. Our results are relevant for research
on multilingually comparable corpora as
used in machine translation and related re-
search, since they shed new light on the
notion of ?comparablity?.
1 Introduction: Motivation and Goals
In the context of statistical machine translation,
comparable corpora are typically bilingual, the-
matically similar corpora being utilized to extract
translation equivalents to enrich translation mod-
els. These have proved to be useful, especially for
technically specialized texts or for low resource
languages where parallel corpora are rare (Chiao
and Zweigenbaum (2002); Babych et al (2007)).
The overarching goal of the paper is to provide
evidence that the notion of comparability com-
monly used in that context is rather coarse and
misses important aspects of linguistic variation.
We report on a set of experiments in which a
monolingually comparable corpus is studied. The
corpus contains specialized, technical texts from
nine scientific disciplines, related to each other by
?interdisciplines? (such as computer science - lin-
guistics - computational linguistics) (cf. Section 2
for details). Our study establishes the linguistic
differences and commonalities between the disci-
plines considered on the basis of the concept of
register, i.e., language variation according to situ-
ational context. Situational context is convention-
ally described in terms of field, tenor and mode of
discourse (Quirk et al, 1985). It has been shown
in numerous corpus-linguistic studies that particu-
lar situational settings have specific linguistic cor-
relates at the level of lexico-grammar in the sense
of clusters of lexico-grammatical features that oc-
cur non-randomly (see notably the work by Biber
and colleagues, e.g., Biber (1988, 1993); Biber
et al (1999); Biber (2006, 2012)). Collectively,
the linguistic features associated with field, tenor
and mode then give rise to registers. More specif-
ically, field of discourse relates to the topic of a
discourse and is realized lexico-grammatically in
functional verb classes (e.g., activity, communica-
tion, etc.) with corresponding arguments (e.g., Ac-
tor, Goal, Medium, etc.) and adjunct types (e.g.,
Time, Place, Manner, etc.). Tenor of discourse re-
lates to the roles and attitudes of the participants in
a discourse and is realized lexico-grammatically
in mood, modality as well as stance expressions.
Mode of discourse relates to the presentational
function of language and is realized in Theme-
Rheme and Given-New constellations. A register
is then characterized by particular distributions of
lexico-grammatical features according to a given
contextual configuration.
Apart from exhibiting differences in field, tenor
and mode, scientific texts are associated with par-
ticular discourse ?styles? such as technicality, ab-
stractness or informational density, which may
again be linguistically realized in different ways
and to different degrees across disciplines. Fur-
thermore, in a highly dynamic social domain, such
as the scientific one, both registers and discourse
styles are relatively versatile and subject to change
(cf. Ure (1971, 1982)). This may, for instance,
59
affect conventional phraseology. Finally, register
and stylistic features may be distributed unevenly
across document parts, thus giving rise to varia-
tion according to document structure. In order to
arrive at a comprehensive picture of the linguistic
construal of disciplinarity, we thus need to con-
sider the linguistic encodings according to register
and the linguistic realization of discursive styles as
well as take into account the inherently dynamic
nature of scientific discourse.
Relating this back to the notion of comparabil-
ity, the concept of register may thus provide the
basis for a fine-grained description of comparabil-
ity, as it acknowledges the multi-dimensional na-
ture of linguistic variation.
Our methodology is informed by three sources:
corpus linguistics, linguistic theory and data min-
ing. Standard corpus methods are employed for
the quantification of instances of linguistic fea-
tures that are considered to be relevant indicators
of variation across scientific disciplines and may
be expected to significantly contribute to differ-
ences in language use across disciplines. The the-
oretical basis is provided by Systemic Functional
Linguistics (SFL; Halliday (2004)). The reason
for choosing SFL to inform analysis is its model
of association of contextual variables with lexico-
grammatical domains (cf. above on the notion of
register).
In contrast to other corpus-based studies on reg-
ister, our goal is not to uncover dimensions of vari-
ation or to discover text classes (as e.g. in Biber et
al?s work). The texts in our corpus are taken from
38 journals from nine disciplines (for details see
Section 2) and the text classes are thus extrinsi-
cally defined. We can then think of analysis as a
task of text classification, where we test whether
the extrinsically defined classes have distinctive
linguistic correlates and if so, how well the classes
are distinguished linguistically and which features
contribute most to their distinction. To this end,
we employ data mining techniques, in particular
automatic text classification (see Section 3 for de-
tails). A similar approach to the one developed
here, also working on linguistic variation in the
scientific domain, has been proposed earlier by
Argamon et al (2008). There is related work
in translation studies by Baroni and Bernardini
(2006) and Volansky et al (2011), which uses au-
tomatic text classification to describe the specific
properties of translations (?translationese?). The
earliest work, to our knowledge, combining SFL
with text classification is Whitelaw and Patrick?s
work on spam detection (Whitelaw and Patrick,
2004).
2 Corpus
2.1 Corpus Design and Pre-processing
We have built a corpus composed of English sci-
entific research articles ? the English Scientific
Text Corpus (SCITEX; cf. Teich and Fankhauser
(2010) and Degaetano-Ortlieb et al (forthcom-
ing)) ? that covers nine scientific domains and
amounts to approx. 34 million tokens, drawn from
38 sources. SCITEX contains full journal arti-
cles from two time periods, the 1970s/early 1980s
(SASCITEX) and the early 2000s (DASCITEX). We
selected at least two different journals for each dis-
cipline in both time slices. As our focus is on se-
Figure 1: Scientific disciplines in the SCITEX cor-
pus
lected scientific domains at the boundaries to com-
puter science and some other discipline, SCITEX
has a three-way partition: (1) A-subcorpus: com-
puter science, (2) B-subcorpus: computational lin-
guistics, bioinformatics, digital construction and
microelectronics, and (3) C-subcorpus: linguis-
tics, biology, mechanical engineering and elec-
trical engineering, as shown in Figure 1. In the
present paper, we are mainly interested in the lin-
guistic evolution of the inter-/transdisciplinary do-
mains represented by the B-subcorpus, as these
are the ones that have emerged in the given time
frame (1970s/80s to present). We term these do-
mains contact disciplines, since they have come
about through contact between two existing dis-
60
ciplines (here: computer science and another es-
tablished discipline represented in the A and C
subcorpora, which we term seed disciplines). The
main question we are interested in is whether the
seed and contact disciplines have clearly distin-
guishable linguistic correlates in terms of register.
The text sources for SCITEX are full academic
articles in the form of PDF files. These files were
converted to plain text using an existing commer-
cial software including optical character recogni-
tion (OCR).
In further processing we follow the common
practices in corpus linguistics by (a) accounting
for relevant metadata (e.g., author, title, jour-
nal, year of publications) and document structure
(e.g., abstract, conclusion), and (b) using stan-
dard tools for preprocessing (e.g., tokenization,
tagging, lemmatization, etc.). For corpus query,
we employ the Corpus Query Processor (CQP)
(CWB; Evert, 2004) which works on the basis of
regular expressions. Utilities of CQP allow for the
extraction of distributional information according
to the annotated metadata and document structure.
3 Methods of Analysis
We carry out a diachronic analysis comparing the
two time slices (1970s/80s vs. 2000s) represented
in the SCITEX corpus, aiming to provide answers
to the following questions:
1. How well are the individual disciplines dis-
tinguished?
2. How distinct are the contact disciplines from
their seed disciplines?
Thus, analysis involves comparisons along the
temporal and the disciplinary dimensions.
The hypothesis we have about the outcomes of
our analysis is that disciplines will be better dis-
tinguished from one another over time, including
the contact disciplines, reflecting a process of di-
versification within scientific writing over time.
3.1 Feature Selection
In the first step of analysis we need to determine
which features to investigate. These should be fea-
tures that bring out relevant and significant con-
trasts along the dimensions considered (time, dis-
cipline). For the choice of features potentially
distinguishing individual (scientific) registers, we
draw on SFL?s model of register variation in which
the contextual parameters of field, tenor and mode
are associated with particular lexico-grammatical
domains. Since we want to cover all three con-
textual parameters, we choose at least one fea-
ture for each. For field, we analyze functional
verb classes as well as PoS-patterns that are poten-
tially terminology-forming (e.g. noun-noun struc-
tures); for tenor, we analyze modal verbs and for
mode we analyze theme type as well as conjunc-
tive cohesive relations. As another feature, we an-
alyze n-grams on the basis of PoS combinations
(rather than words), since we have seen in a previ-
ous study that they may be involved in processes
of conventionalization (Kermes and Teich, 2012).
Additionally, on an abstract level, scientific
writing is a highly informational production that is
characterized by technicality, information density
and abstractness (cf. Halliday and Martin (1993)).
Among the linguistic features realizing these prop-
erties are a relatively low type-token ratio (techni-
cality), a relatively high lexical density and low
grammatical intricacy (information density) and
the frequent use of nominal categories (nouns, ad-
jectives) (abstractness).
Table 1 displays the features considered in the
analysis together with their associated contextual
variables and/or abstract discourse properties they
instantiate. Features are extracted from the cor-
pus with CQP. For example, simple queries com-
bine part-of-speech and concrete lemmas (e.g.,
[pos=?MD? & lemma=?must|should?]; for modal
verbs). More complex queries work with posi-
tional attributes, linguistic annotations and lists
(e.g., < s>[conj & lemma!=$modal-adverbs]... as
part of the extraction of textual Theme, which is
realized in English as the first constituent in the
clause).
3.2 Feature Evaluation
We employ statistical and machine learning meth-
ods to measure (a) how much individual features
contribute to a possible distinction and (b) how
well corpora are distinguished by these features.
We employ classification techniques by using fea-
ture ranking (Information Gain) to determine the
relative discriminatory force of features, and su-
pervised machine learning (decision trees and sup-
port vector machines) to distinguish between the
scientific registers in SCITEX. For these steps we
use the WEKA data mining platform (Witten and
Eibe, 2005).
61
contextual parameter/ feature category feature subcategory
abstract discourse property
FIELD
term patterns NN-of-NN, N-N, ADJ-N
verb classes
activity (e.g., make, show)
aspectual (e.g., start, end)
causative (e.g., let, allow)
communication (e.g., note, describe)
existence (e.g., exist, remain)
mental (e.g., see, know)
occurrence (e.g., change, grow)
TENOR modality
obligation/necessity (e.g., must)
permission/possibility/ability (e.g., can)
volition/prediction (e.g., will)
MODE
theme
experiential theme (e.g, The algorithm...)
interpersonal theme (e.g., Interestingly...)
textual theme (e.g., But...)
additive (e.g., and, furthermore)
conjunctive adversative (e.g., nonetheless, however)
cohesive relations causal (e.g., thus, for this reason)
temporal (e.g., then, at this point)
TECHNICALITY type-token ratio STTR
lexical vs. function words no. of lexical PoS categories
INFORMATION DENSITY
lexical density lexical items per clause/sentence
grammatical intricacy
clauses per sentence
wh-words per sentence
sentence length
ABSTRACTNESS PoS distribution no. of nominal vs. verbal categories
CONVENTIONALIZATION
n-grams on PoS basis 2-to-6-grams overall/per section
length of sections tokens per section
Table 1: Features used in analysis
4 Results and Interpretation
Our analysis addresses the question of how dis-
tinctive the subcorpora in SCITEX are comparing
the productions of the 1970/80s with those of the
early 2000s. Considering the diachronic perspec-
tive, we expect to encounter a clearer separation of
individual disciplines overall reflecting a process
of diversification within scientific writing.
The analysis has two parts: First, we calculate
Information Gain of the top twenty features, to see
which features are the most discriminatory ones
across disciplines. Second, we apply automatic
classification, to see how well the subcorpora are
distinguished on the basis of these features.
Table 2 shows the twenty most discriminatory
features for the 70/80s across all subcorpora. The
five highest ranking features are associated with
field (NN: IGain 0.39, LEX: IGain 0.36, commu-
nication verbs: IGain 0.31) and mode (WL: IGain
0.33, LEX/C: IGain 0.32). In the mid range, we
find some tenor features and in the lower range
some other field features as well as document
structure features.
When we compare these results with the ones
for the early 2000s (see Table 3), three main ob-
servations can be made. First, features become
much more pronounced, the IGain values rising
substantially for the top 20 features (1970s/80s
are in the range of 0.23 to 0.39, 2000s are in
the range of 0.31 to 3.1). This includes the nine
features that are identical across SASCITEX and
DASCITEX: existence and communication verbs
as well as adj-n term pattern for field, obliga-
tion modals for tenor, word and sentence length
as well as lexical words per clause for mode, bi-
grams for conventionalization, and length of main
part for document structure, all become more pro-
nounced in DASCITEX (higher IGains) and thus
contribute more to the distinction between disci-
plines. The second observation is that while in
SASCITEX only bi-grams ranges among the top 20
features, in DASCITEX we encounter an increase
in the contribution of gram-based features to the
DASCITEX-internal distinction.1 This may point
to the greater role of conventionalized language in
the distinction between disciplines over time. Ter-
minological studies based on n-grams might indi-
cate a thematic comparability of disciplines. Con-
sider one of the key concepts in computer science,
?algorithm?. The distribution (per million) across
the nine disciplines in DASCITEX varies greatly:
1Note again that in our analysis, n-grams are based on
parts-of-speech, not words.
62
feature IGain contextual parameter discourse property
NN 0.3931 field technicality, abstractness
LEX 0.3647 field technicality
communication 0.3119 field
mental 0.2526 field
existence 0.2372 field
ADV 0.2282 field abstractness
adj-n pattern 0.2253 field technicality
volition 0.3184 tenor
permission 0.2709 tenor
MD 0.2679 tenor
obligation 0.249 tenor
WL 0.3326 mode information density
LEX/C 0.3238 mode information density
SL 0.2974 mode information density
clauses/S 0.287 mode information density
additive 0.2574 mode
WH/S 0.2504 mode information density
bi-grams 0.2382 conventionalization
main 0.2301 document structure
introduction 0.2257 document structure
Table 2: Feature ranking for the 70/80s (SASCITEX): Top 20 features
feature IGain contextual parameter discourse property
existence 0.3987 field
activity 0.3677 field
communication 0.3636 field
STTR 0.3582 field technicality
adj-n pattern 0.3441 field technicality
obligation 0.3548 tenor
LEX/C 3.0803 mode information density
SL 0.5567 mode information density
WL 0.51 mode information density
experiential-theme 0.344 mode
causal 0.3302 mode
main 0.5324 document structure
abstract 0.4981 document structure
n-grams main 0.4925 conventionalization
bi-grams 0.3886 conventionalization
n-grams 0.3706 conventionalization
n-grams abstr 0.3609 conventionalization
n-grams 4 0.3287 conventionalization
n-grams 3 0.3209 conventionalization
n-grams intro 0.3115 conventionalization
Table 3: Feature ranking for the early 2000s (DASCITEX): Top 20 features
computer science (3427), microelectronics (1965),
bioinformatics (1913), digital construction (1735),
computational linguistics (1124), electrical engi-
neering (955), mechanical engineering (129), bi-
ology (59) and linguistics (51). When we look at
the top frequent token n-grams in which algorithm
participates, we find, for example, ?approximation
algorithm? which is mostly shared between com-
puter science, the contact discipines and electrical
engineering, ?learning algorithms? appears prac-
tically everywhere, and ?alignment algorithm? is
almost only mentioned in computational linguis-
tics and bioinformatics (with a few occurrences
in computer science and one in biology). The
stylistics across the disciplines is also notewor-
thy: pure stylistic tri-grams, such as the highly
frequent ?in order to?, ?the number of?, ?based on
the?, ?as shown in?, etc., are also good discrimi-
nators between different disciplines (cf. Kermes
and Teich (2012)). Finally, at the levels of con-
textual and discourse properties, it can be noted
that features associated with information density
become better discriminators between disciplines
in the 2000s having high IGain values, while tenor
features step back decreasing in number, tending
towards greater uniformity (only one tenor feature
(obligation modals) in the top 20 features in the
2000s compared to four in the 70s/80s).
To see how these data are reflected according to
disciplines, we perfom classification for both cor-
63
A B1 B2 B3 B4 C1 C2 C3 C4 total accuracy in %
A 108 2 11 25 1 0 4 6 45 202 53.47
B1 3 22 22 19 7 26 4 9 13 125 17.60
B2 10 21 142 55 30 8 60 60 71 457 31.07
B3 16 24 52 121 32 7 17 37 55 361 33.52
B4 1 4 32 27 91 4 36 45 32 272 33.46
C1 2 24 16 8 1 154 4 6 4 219 70.32
C2 3 6 70 16 22 2 358 30 28 535 66.92
C3 10 10 60 45 44 6 37 137 39 388 35.31
C4 52 25 60 49 39 2 25 24 248 524 47.33
A: Computer Science, B1: Computational Linguistics, B2: Bioinformatics, B3: Digital Construction, B4: Microelectronics,
C1: Linguistics, C2: Biology, C3: Mechanical Engineering, C4: Electrical Engineering
Table 4: Confusion matrix with decision tree for the 70/80s (SASCITEX)
A B1 B2 B3 B4 C1 C2 C3 C4 total accuracy in %
A 156 0 3 4 0 1 1 0 37 202 77.23
B1 1 26 23 11 7 27 3 12 15 125 20.80
B2 2 2 274 47 13 4 32 37 46 457 59.96
B3 8 1 72 156 21 3 16 24 60 361 43.21
B4 0 1 14 8 158 1 49 26 15 272 58.09
C1 2 11 12 0 0 183 0 5 6 219 83.56
C2 2 0 28 4 12 0 463 9 17 535 86.54
C3 3 4 53 18 22 2 40 213 33 388 54.90
C4 30 2 41 25 12 1 24 12 377 524 71.95
A: Computer Science, B1: Computational Linguistics, B2: Bioinformatics, B3: Digital Construction, B4: Microelectronics,
C1: Linguistics, C2: Biology, C3: Mechanical Engineering, C4: Electrical Engineering
Table 5: Confusion matrix with SVM for the 70/80s (SASCITEX)
A B1 B2 B3 B4 C1 C2 C3 C4 total accuracy in %
A 201 1 0 9 7 1 0 2 9 230 87.39
B1 4 97 4 19 1 8 1 0 3 137 70.80
B2 5 0 269 14 6 0 18 6 1 319 84.33
B3 5 3 8 168 8 0 6 30 14 242 69.42
B4 2 2 10 17 156 0 8 9 1 205 76.10
C1 1 11 6 3 0 90 0 0 0 111 81.08
C2 0 0 7 2 2 1 335 3 1 351 95.44
C3 4 1 7 23 6 0 15 229 18 303 75.58
C4 18 2 3 42 7 0 4 34 113 223 50.67
A: Computer Science, B1: Computational Linguistics, B2: Bioinformatics, B3: Digital Construction, B4: Microelectronics,
C1: Linguistics, C2: Biology, C3: Mechanical Engineering, C4: Electrical Engineering
Table 6: Confusion matrix with SVM for the early 2000s (DASCITEX)
pora (SASCITEX and DASCITEX), first, with deci-
sion trees, as they are based on Information Gain,
and second, with support vector machines (SVMs),
as they are used for text categorization tasks with
many relevant features achieving very good results
(cf. Joachims (1998)). Classification is performed
on all features with 10 fold cross-validation. Ta-
ble 4 shows the confusion matrix for all subcor-
pora for the 70/80s and classification accuracy for
each subcorpus achieved by decision tree. The
overall accuracy is 44.79% only, the correctly clas-
sified texts lying on the main diagonal of the ma-
trix.
The confusion matrix produced by SVM is
shown in Table 5, with an overall accuracy of
65.07%. Apart from computational linguistics
(B1), accuracy goes up by about 10% for digi-
tal contruction (B3) and linguistics (C1) and about
25-30% for the other subcorpora compared to de-
cision tree. Accuracy with SVM for the contact
disciplines (B1-B4) ranges from 20-60% and is
much lower than the accuracy achieved for the
seed disciplines (A and C1-C4) with around 54-
86%. Thus, the contact disciplines are not clearly
separated from the seed disciplines. Considering,
for instance the triple A-B1-C1, we can see that
more texts belonging to computational linguistics
(B1) are classified into linguistics (C1) than into
computational linguistics (27 texts in C1 vs. 26 in
B1), i.e., texts in B1 seem to be quite similar to
64
B1 vs A B2 vs A B3 vs A B4 vs A
WL 0.629 WL 0.501 WL 0.399 LEX 0.883
STTR 0.509 LEX 0.355 LEX 0.331 WL 0.763
LEX 0.372 causal 0.334 n-grams 6 0.265 STTR 0.574
ADJ 0.261 n-grams 6 0.306 STTR 0.258 causal 0.560
VV 0.230 STTR 0.303 clauses/S 0.202 NN 0.458
n-grams 6 0.205 n-grams 4 0.284 adj-n-n 0.168 additive 0.440
causal 0.187 temporal 0.283 causal 0.160 temporal 0.433
types 0.174 n-grams 5 0.282 NN 0.13 mental 0.416
adj-c-adj-n 0.145 ADJ 0.273 n-grams 4 0.118 commun. 0.379
introduction 0.129 causative 0.197 ADJ 0.114 n-grams 4 0.364
B1 vs C1 B2 vs C2 B3 vs C3 B4 vs C4
clauses/S 0.230 NN 0.269 LEX/S 0.260 LEX 0.469
ADV 0.204 MD 0.264 main 0.146 VV 0.311
LEX/C 0.196 WH 0.198 n-grams main 0.132 WL 0.309
NN 0.179 permission 0.178 introduction 0.127 main 0.153
WH/S 0.122 volition 0.166 causative 0.114 NN 0.148
LEX 0.120 WL 0.147 exper-theme 0.113 introduction 0.142
occurrence 0.119 SL 0.145 obligation 0.087 LEX/S 0.115
commun. 0.112 WH/S 0.137 n-grams intro 0.086 n-grams main 0.096
MD 0.110 LEX 0.104 aspectual 0.081 causal 0.093
n-grams abstr 0.108 LEX/C 0.098 LEX/C 0.077 n-grams intro 0.088
A: Computer Science, B1: Computational Linguistics, B2: Bioinformatics, B3: Digital Construction, B4: Microelectronics,
C1: Linguistics, C2: Biology, C3: Mechanical Engineering, C4: Electrical Engineering
Table 7: Feature ranking with IGain for the 70/80s (SASCITEX): Top 20 features contact vs seed disci-
plines
B1 vs A B2 vs A B3 vs A B4 vs A
WL 0.694 WL 0.701 WL 0.567 WL 0.791
STTR 0.631 main 0.680 causal 0.488 STTR 0.615
SL 0.441 STTR 0.678 STTR 0.385 VV 0.289
types 0.402 n-grams main 0.634 temporal 0.347 main 0.233
causal 0.237 causal 0.621 n-grams 4 0.345 causal 0.230
n-grams 6 0.217 n-grams 4 0.577 n-grams 0.319 LEX 0.21
n-n 0.192 n-grams 0.552 n-grams 5 0.318 mental 0.196
adj-n 0.171 abstract 0.537 n-grams main 0.282 temporal 0.190
adversative 0.128 bi-grams 0.521 LEX 0.280 n-of-n 0.189
adj-c-adj-n 0.125 introduction 0.487 bi-grams 0.262 aspectual 0.144
B1 vs C1 B2 vs C2 B3 vs C3 B4 vs C4
occurrence 0.264 SL 0.566 WL 0.156 VV 0.436
adj-adj-n 0.193 abstract 0.518 VV 0.139 WL 0.410
ADV 0.189 n-grams abstr 0.505 obligation 0.100 LEX/C 0.329
ADJ 0.137 main 0.412 LEX/C 0.100 ADV 0.243
NN 0.128 introduction 0.353 n-grams 5 0.097 n-grams 3 0.181
types 0.123 n-grams main 0.344 MD 0.088 LEX/S 0.162
LEX/C 0.123 n-grams intro 0.321 ADJ 0.075 activity 0.154
main 0.118 WH 0.204 aspectual 0.064 n-grams 0.147
commun. 0.107 MD 0.202 SL 0.061 STTR 0.135
abstract 0.107 WH/S 0.192 LEX/S 0.059 abstract 0.127
A: Computer Science, B1: Computational Linguistics, B2: Bioinformatics, B3: Digital Construction, B4: Microelectronics,
C1: Linguistics, C2: Biology, C3: Mechanical Engineering, C4: Electrical Engineering
Table 8: Feature ranking with IGain for the early 2000s (DASCITEX): Top 20 features contact vs seed
disciplines
texts in C1 in terms of the features investigated.
In order to check the separation of disciplines
over time, we need to compare classification re-
sults across SASCITEX and DASCITEX. We again
apply SVM, which returns an overall accuracy of
78.17%.2 Comparing the values for the individual
2Decision tree performed poorly again in comparison
subcorpora across SASCITEX and DASCITEX, we
can observe that accuracies are now much higher
for all subcorpora. Considering the contact disci-
plines, they have clearly gained distinctiveness in
the 2000s in comparison to the 1970/80s, as texts
in B1-B4 are classified correctly 69% to 84% of
achieving an accuracy of 57.24% only.
65
the time (instead of 20-60% in the 1970/80s).
In summary, the classification results match the
results obtained by feature ranking, which have
shown that the top 20 features increased discrim-
inatory force over time. This is reflected by a
higher classification accuracy overall and for the
subcorpora.3 The discriminatory force of features
in the 1970s/80s instead, was not strong enough to
clearly separate disciplines.
To see whether there are any particular features
involved in the differentiation of the contact dis-
ciplines in particular vis a` vis computer science
on the one hand and the other seed disciplines
on the other hand, we inspect the confusion ma-
trix as well as the IGains of each B vs. A and
each B vs. the respective C, both for SASCITEX
and DASCITEX. In the comparison to computer
science (A), we can see that the confusion ma-
trixes produced with SVM (cf. Table 5 and 6)
show few texts that are misclassified from the con-
tact disciplines (Bs) into computer science (A) for
both time slices. Thus, the features employed dis-
tinguish Bs from A quite well. Considering the
IGain values (see Table 7 and 8 for the top 10 fea-
tures), besides computational linguistics (B1; rel-
atively low classification accuracy of 20% in the
70/80s), the contact disciplines have the following
features in common: word length (WL), STTR,
causal verbs in the top 10 as well as four-grams,
lexical words (LEX) and temporal conjunctions in
the top 20 features. Except lexical words (LEX),
all features have a higher IGain in the 2000s. In
the comparison to the other seed discipines (Cs),
the confusion matrixes show more misclassifica-
tions of Bs into Cs. Considering the IGain val-
ues there are no tendencies uniformly applying to
the contact disciplines (Bs). They rather show
individual tendencies for each pair (B1 vs. C1,
B2 vs. C2, B3 vs. C3, B4 vs. C4). Features
that contribute to a better classification diachroni-
cally lie in the following parameters: (a) field (oc-
currence, term-patterns, ADV) for computational
linguistics (B1), (b) document structure (abstract,
main, intro), information density (SL) and conven-
tionalization (n-grams abstract) for bioinformatics
(B2), (c) information density (WL) and technical-
ity (VV) for digital construction (B3) and micro-
electronics (B4).
3There are only two exceptions: C1 (linguistics) goes
slightly down (around 2.5%), C4 (electrical engineering)
goes down by over 20% to 50.67% accuracy, i.e., it is not
really distinguishable any more.
5 Summary and Conclusions
We have looked at disciplinary linguistic diversifi-
cation in English scientific writing in terms of reg-
ister, discourse styles and document structure. The
results of our analysis provide evidence of major
motifs of development in scientific writing over
time, showing dynamicity over a time span of only
thirty years. Diversification over time is clearly
borne out for the contact disciplines but is also true
for most of the other disciplines.
Considering the contact disciplines we have
seen that (1) they can be distinguished quite well
from computer science with the same features be-
ing involved in better classification results, (2)
they show individual feature constellations in their
distinction from their seed disciplines. Moreover,
n-grams have gained discriminatory force over
time and are ranked relatively high among our fea-
tures in the 2000s subcorpus. As they are also rel-
evant in terms of terminology, they give an insight
in the relatedness of disciplines.
In terms of methods, we have combined state-
of-the-art corpus processing with techniques of
data analysis as developed in data mining. As such
techniques become more accessible to linguistic,
literary and cultural analysis, the repertoire of
methods for such analysis will be greatly enhanced
in that sounder empirical evidence can be sought
in text-based socio-cultural and historical studies
at large (cf. Jockers (2013)). The crucial factor
in employing such methods is the motivation of
the features to be used in analysis. Here, we have
deliberately not relied on word-based features but
instead mainly employed lexico-grammatical pat-
terns. While bags-of-words are strong discrim-
inators between texts/text classes, they can only
tell us something about lexical variation (e.g., as
an indicator of text topic). However, when reg-
ister or style rather than topicality are in the fo-
cus (such as e.g. the linguistic construal of techni-
cal, dense or abstract discourse or the expression
of field, tenor or mode relations), it will not be suf-
ficient to study lexical word distributions (cf. Co-
hen et al (2010); Teich and Fankhauser (2010) for
some other studies). Instead, one needs to identify
lexico-grammatical patterns that are potential in-
dicators of the more abstract discoursive and con-
textual properties that are in focus.
The insight to be gained from our study for mul-
tilingually comparable corpora is that more elab-
orate definitions of ?comparability? might be re-
66
quired. Our approach offers such a definition of
comparability by being firmly based on an estab-
lished model of linguistic variation, which has also
been widely applied in multilingual contexts, such
as for example, automatic text generation (see
e.g., Matthiessen and Bateman (1991); Bateman
(1997); Kruijff et al (2000)). The parameters of
variation we employ (register: field, tenor, mode;
discourse styles; time) provide a fine-grained grid
of features involved in linguistic variation, which
can be applied to other languages as well. For ex-
ample, we can extract and analyze field features,
such as term patterns (as produced for German by
Weller et al (2011)), tenor features, such as modal
verbs, as well as the other features investigated
using the same tools applied here (part-of-speech
tagger, CQP, R-scripts and WEKA modules) with
only little adaptations (e.g., tag sets, query formu-
lation). Overall, we would expect that applying
the concept of register to the problem of compara-
bility will enable finer-tuned comparable corpora
and thus contribute to their fuller potential for mul-
tilingual language technology.
Acknowledgments
We wish to thank the anonymous reviewers for
their helpful comments. We are especially grate-
ful to Peter Fankhauser for critically assessing our
data and to Noam Ordan for valuable suggestions
regarding the structure of the paper.
References
Shlomo Argamon, Jeff Dodick, and Paul Chase.
Language use reflects scientific methodology:
A corpus-based study of peer-reviewed journal
articles. Scientometrics, 75(2):203?238, 2008.
Bogdan Babych, Anthony Hartley, and Serge
Sharoff. Translating from under-resourced lan-
guages: Comparing direct transfer against pivot
translation. In Proceedings of the MT Sum-
mit XI, pages 412?418, Copenhagen, Denmark,
2007.
Marco Baroni and Silvia Bernardini. A new ap-
proach to the study of translationese: Machine-
learning the difference between original and
translated text. Literary and Linguistic Com-
puting, 21(3):259?274, 2006.
John A. Bateman. Enabling technology for
multilingual natural language generation: The
KPML development environment. Journal
of Natural Language Engineering, 3(1):15?55,
1997.
Douglas Biber. Variation Across Speech and Writ-
ing. Cambridge University Press, Cambridge,
1988.
Douglas Biber. The multi-dimensional approach
to linguistic analyses of genre variation: An
overview of methodology and findings. Com-
puters and the Humanities, 26(5-6):331?345,
1993.
Douglas Biber. University Language: A Corpus-
based Study of Spoken And Written Regis-
ters, volume 23 of Studies in Corpus Lin-
guistics. John Benjamins Publishing, Amster-
dam/Philadelphia, 2006.
Douglas Biber. Register as a predictor of linguis-
tic variation. Corpus Linguistics and Linguistic
Theory, 8(1):9?37, 2012.
Douglas Biber, Stig Johansson, and Geoffrey
Leech. Longman Grammar of Spoken and Writ-
ten English. Longman, Harlow, 1999.
Yun-Chuang Chiao and Pierre Zweigenbaum.
Looking for candidate translational equivalents
in specialized, comparable corpora. In Pro-
ceedings of the 19th international Conference
on Computational Linguistics (COLING), Vol.
2, pages 1?5, Taipei, Taiwan, 2002.
Kevin Bretonnel Cohen, Helen Johnson, Karin
Verspoor, Christophe Roeder, and Lawrence
Hunter. The structural and content aspects of
abstracts versus bodies of full text journal arti-
cles are different. BMC bioinformatics, 11(1):
492, 2010.
CWB. The IMS Open Corpus Workbench, 2010.
http://www.cwb.sourceforge.net.
Stefania Degaetano-Ortlieb, Kermes Hannah,
Ekaterina Lapshinova-Koltunski, and Teich
Elke. SciTex a diachronic corpus for analyz-
ing the development of scientific registers. In
Paul Bennett, Martin Durrell, Silke Scheible,
and Richard J. Whitt, editors, New Methods in
Historical Corpus Linguistics, Corpus Linguis-
tics and Interdisciplinary Perspectives on Lan-
guage (CLIP), Vol. 3. Narr, Tu?bingen, forth-
coming.
Stefan Evert. The Statistics of Word Cooccur-
rences: Word Pairs and Collocations. PhD the-
sis, IMS, University of Stuttgart, 2004.
67
M.A.K. Halliday. An Introduction to Functional
Grammar. Arnold, London, 2004.
M.A.K. Halliday and J.R. Martin. Writing sci-
ence: Literacy and discursive power. Falmer
Press, London, 1993.
Thorsten Joachims. Text categorization with sup-
port vector machines: Learning with many rel-
evant features. Machine Learning: ECML-98,
pages 137?142, 1998.
Matthew L. Jockers. Macroanalysis: Digital
Methods and Literary History. University of
Illinois Press, 2013.
Hannah Kermes and Elke Teich. Formulaic ex-
pressions in scientific texts: Corpus design, ex-
traction and exploration. Lexicographica, 28
(1):99?120, 2012.
Geert-Jan Kruijff, Elke Teich, John Bateman,
Ivana Kruijff-Korbayova?, Hana Skoumalova?,
Serge Sharoff, Lena Sokolova, Tony Hartley,
Kamenka Staykova, and Jir??? Hana. Multilin-
guality in a text generation system for three
Slavic languages. In Proceedings of the 18th in-
ternational Conference on Computational Lin-
guistics (COLING), Vol. 1, pages 474?480,
Saarbru?cken, Germany, 2000.
Christian M.I.M. Matthiessen and John A. Bate-
man. Text generation and systemic-functional
linguistics: Experiences from English and
Japanese. Communication in Artificial Intelli-
gence Series. Pinter, 1991.
Randolph Quirk, Sidney Greenbaum, Geoffrey
Leech, and Jan Svartvik. A Comprehensive
Grammar of the English Language. Longman,
London, 1985.
Elke Teich and Peter Fankhauser. Exploring a
corpus of scientific texts using data mining.
In S. Gries, S. Wulff, and M. Davies, editors,
Corpus-linguistic applications: Current stud-
ies, new directions, pages 233?247. Rodopi,
Amsterdam and New York, 2010.
Jean Ure. Lexical density and register differentia-
tion. In G. E. Perren and J. L. M. Trim, editors,
Applications of Linguistics. Selected papers of
the Second International Congress of Applied
Linguistics, Cambridge 1969, pages 443?452.
Cambridge University Press, 1971.
Jean Ure. Introduction: Approaches to the study
of register range. International Journal of the
Sociology of Language, 35:5?23, 1982.
Vered Volansky, Noam Ordan, and Shuly Wintner.
More human or more translated? Original texts
vs. human and machine translations. In Pro-
ceedings of the 11th Bar-Ilan Symposium on the
Foundations of AI with Israeli Seminar on Com-
putational Linguistics (ISCOL), Ramat Gan, Is-
rael, 2011.
Marion Weller, Helena Blancafort, Anita Gojun,
and Ulrich Heid. Terminology extraction and
term variation patterns: a study of French and
German data. In Proceedings of the GSCL:
German Society for Computational Linguistics
and Language Technology, Hamburg, Germany,
2011.
Casey Whitelaw and Jon Patrick. Selecting sys-
temic features for text classification. In Ash
Asudeh, Ce?cile Paris, and Stephen Wan, edi-
tors, Proceedings of the Australasian Language
Technology Workshop, pages 93?100, Sydney,
Australia, 2004.
Ian H. Witten and Frank Eibe. Data Mining: Prac-
tical Machine Learning Tools and Techniques.
Elsevier, Morgan Kaufmann Publishers, Ams-
terdam, Boston, second edition, 2005.
68
