Adaptive Dialogue Systems - Interaction with Interact
Kristiina Jokinen and Antti Kerminen and Mauri Kaipainen
Media Lab, University of Art and Design Helsinki
Ha?meentie 135 C, FIN-00560 Helsinki, Finland
{kjokinen|akermine|mkaipain}@uiah.fi
Tommi Jauhiainen and Graham Wilcock
Department of General Linguistics, University of Helsinki
FIN-00014 University of Helsinki, Finland
{tsjauhia|gwilcock}@ling.helsinki.fi
Markku Turunen and Jaakko Hakulinen
TAUCHI Unit, University of Tampere
FIN-33014 University of Tampere, Finland
{mturunen|jh}@cs.uta.fi
Jukka Kuusisto and Krista Lagus
Neural Networks Research Centre, Helsinki University of Technology
P.O.9800 FIN-02015 HUT, Finland
{krista|jkuusist}@james.hut.fi
Abstract
Technological development has made
computer interaction more common
and also commercially feasible, and
the number of interactive systems has
grown rapidly. At the same time, the
systems should be able to adapt to var-
ious situations and various users, so as
to provide the most efficient and help-
ful mode of interaction. The aim of
the Interact project is to explore nat-
ural human-computer interaction and
to develop dialogue models which will
allow users to interact with the com-
puter in a natural and robust way. The
paper describes the innovative goals of
the project and presents ways that the
Interact system supports adaptivity on
different system design and interaction
management levels.
1 Introduction
The need for flexible interaction is apparent not
only in everyday computer use, but also in vari-
ous situations and services where interactive sys-
tems can diminish routine work on the part of
the service provider, and also cater for the users
with fast and tailored access to digital infor-
mation (call centers, help systems, interactive
banking and booking facilities, routing systems,
information retrieval, etc.).
The innovative goal of the Finnish Interact
project is to enable natural language interac-
tion in a wider range of situations than has been
possible so far, and in situations where its use
has not been functional or robust enough. This
means that the systems should support rich in-
teraction and also be able to learn and adapt
their functionality to the changing situation. It
also implies that the needs of special groups will
be taken into account when designing more nat-
ural interactive systems. Within the current sys-
tem, such scenarios can e.g. include an intelli-
gent bus-stop which allows spoken and text in-
teraction concerning city transportation, with a
sign language help facility.
The project addresses especially the problem
of adaptivity: the users are situated in mo-
bile environments in which their needs, activities
and abilities vary. To allow the users to express
their wishes in a way characteristic to them and
       Philadelphia, July 2002, pp. 64-73.  Association for Computational Linguistics.
                  Proceedings of the Third SIGdial Workshop on Discourse and Dialogue,
the situation, interaction with the system should
take place in a robust and efficient manner, en-
abling rich and flexible communication. Natu-
ral language is thus the preferred mode of in-
teraction, compared to graphical interfaces for
example. Adaptivity also appears in the tech-
niques and methods used in the modelling of
the interaction and the system?s processing ca-
pabilities. An important aspect in this respect
is to combine machine learning techniques with
rule-based natural language processing, to in-
vestigate limitations and advantages of the two
approaches for language technology.
In this paper we focus on adaptivity which
manifests itself in various system properties:
? agent-based architecture
? natural language capability
? self-organising topic recognition
? conversational ability.
The paper is organized as follows. We first
introduce the dialogue system architecture. We
then explain how the modules function and
address the specific design decisions that con-
tribute to the system?s adaptivity. We conclude
by discussing the system?s capabilities and pro-
viding pointers for future work.
2 Agent-based architecture
To allow system development with reusable
modules, flexible application building and easy
combination of different techniques, the frame-
work must itself be designed specifically to sup-
port adaptivity. We argue in favour of a sys-
tem architecture using highly specialized agents,
and use the Jaspis adaptive speech application
framework (Turunen and Hakulinen, 2000; Tu-
runen and Hakulinen, 2001a). Compared to e.g.
Galaxy (Seneff et al, 1998), the system supports
more flexible component communication. The
system is depicted in Figure 1.
2.1 Information Storage
The Jaspis architecture contains several features
which support adaptive applications. First of
all, the information about the system state is
kept in a shared knowledge base called Informa-
tion Storage. This blackboard-type information
storage can be accessed by each system compo-
nent via the Information Manager, which allows
them to utilize all the information that the sys-
tem contains, such as dialogue history and user
profiles, directly. Since the important informa-
tion is kept in a shared place, system compo-
nents can be stateless, and the system can switch
between them dynamically. Information Stor-
age thus facilitates the system?s adaptation to
different internal situations, and it also enables
the most suitable component to be chosen to
handle each situation.
2.2 Flexible Component Management
The system is organized into modules which
contain three kinds of components: managers,
agents and evaluators. Each module contains
one manager which co-ordinates component in-
teraction inside the module. The present archi-
tecture implements e.g. the Input/Output Man-
ager, the Dialogue Manager and the Presenta-
tion Manager, and they have different priorities
which allow them to react to the interaction flow
differently. The basic principle is that whenever
a manager stops processing, all managers can
react to the situation, and based on their prior-
ities, one of them is selected. There is also the
Interaction Manager which coordinates applica-
tions on the most general level.
The number and type of modules that can be
connected to the system is not limited. The In-
teraction Manager handles all the connections
between modules and the system can be dis-
tributed for multiple computers. In Interact
we have built a demonstration application on
bus-timetable information which runs on several
platforms using different operating systems and
programming languages. This makes the system
highly modular and allows experiments with dif-
ferent approaches from multiple disciplines.
2.3 Interaction Agents and Evaluators
Inside the modules, there are several agents
which handle various interaction situations such
as speech output presentations and dialogue de-
cisions. These interaction agents can be very
Figure 1: The system architecture.
specialized, e.g. they deal only with speech
recognition errors or outputs related to greet-
ings. They can also be used to model differ-
ent interaction strategies for the same task, e.g.
different dialogue agents can implement alterna-
tive dialogue strategies and control techniques.
Using specialized agents it is possible to con-
struct modular, reusable and extendable inter-
action components that are easy to implement
and maintain. For example, different error han-
dling methods can be included to the system by
constructing new agents which handle errors us-
ing alternative approaches. Similarly, we can
support multilingual outputs by constructing
presentation agents that incorporate language
specific features for each language, while imple-
menting general interaction techniques, such as
error correction methods, to take care of error
situations in speech applications in general (Tu-
runen and Hakulinen, 2001b).
The agents have different capabilities and the
appropriate agent to handle a particular situa-
tion at hand is selected dynamically based on
the context. The choice is done using evalua-
tors which determine applicability of the agents
to various interaction situations. Each evaluator
gives a score for every agent, using a scale be-
tween [0,1]. Zero means that an agent is not
suitable for the situation, one means that an
agent is perfectly suitable for the situation, val-
ues between zero and one indicate the level of
suitability. Scaling functions can be used to em-
phasize certain evaluators over the others The
scores are then multiplied, and the final score, a
suitability factor, is given for every agent. Since
scores are multiplied, an agent which receives
zero from one evaluator is useless for that situ-
ation. It is possible to use different approaches
in the evaluation of the agents, and for instance,
the dialogue evaluators are based on reinforce-
ment learning.
Simple examples of evaluators are for instance
presentation evaluators that select presentation
agents to generate suitable implicit or explicit
confirmations based on the dialogue history and
the system?s knowledge of the user. Another ex-
ample concerns dialogue strategies: the evalua-
tors may give better scores for system-initiative
agents if the dialogue is not proceeding well with
the user-initiative dialogue style, or the evalua-
tors may prefer presentation agents which give
more detailed and helpful information, if the
users seem to have problems in communicating
with the application.
Different evaluators evaluate different aspects
of interaction, and this makes the evaluation
process highly adaptive itself: there is no single
evaluator which makes the final decision. In-
stead, the choice of the appropriate interaction
agent is a combination of different evaluations.
Evaluators have access to all information in the
Information Storage, for example dialogue his-
tory and other contextual information, and it is
also possible to use different approaches in the
evaluation of the agents (such as rule-based and
statistical approaches). Evaluators are the key
concept when considering the whole system and
its adaptation to various interaction situations.
2.4 Distributed Input and Output
The input/output subsystem is also distributed
which makes it possible to use several input and
output devices for the same purposes. For ex-
ample, we can use several speech recognition
engines, each of which with different capabili-
ties, to adapt the system to the user?s way of
talking. The system architecture contains vir-
tual devices which abstract the actual devices,
such as speech recognizers and speech synthesiz-
ers. From the application developers viewpoint
this makes it easy to experiment with different
modalities, since special agents are used to add
and interpret modality specific features. It is
also used for multilingual inputs and outputs,
although the Interact project focuses on Finnish
speech applications.
3 Natural Language Capabilities
The use of Finnish as an interaction language
brings special problems for the system?s nat-
ural language understanding component. The
extreme multiplicity of word forms prevents the
use of all-including dictionaries. For instance,
a Finnish noun can theoretically have around
2200, and a verb around 12000 different forms
(Karlsson, 1983). In spoken language these
numbers are further increased as all the differ-
ent ways to pronounce any given word come into
consideration (Jauhiainen, 2001). Our dialogue
system is designed to understand both written
and spoken input.
3.1 Written and spoken input
The different word forms are analyzed using
Fintwol, the two-level morphological analyzer
for Finnish (Koskenniemi, 1983). The forms are
currently input to the syntactic parser CPARSE
(Carlson, 2001). However, the flexible sys-
tem architecture also allows us to experiment
with different morphosyntactic analyzers, such
as TextMorfo (Kielikone Oy 1999) and Conexor
FDG (Conexor Oy 1997-2000), and we plan
to run them in parallel as separate competing
agents to test and compare their applicability
as well as the Jaspis architecture in the given
task.
We use the Lingsoft Speech Recognizer for the
spoken language input. The current state of the
Finnish speech recognizer forces us to limit the
user?s spoken input to rather restricted vocab-
ulary and utterance structure, compared to the
unlimited written input. The system uses full
word lists which include all the morphological
forms that are to be recognized, and a strict
context-free grammar which dictates all the pos-
sible utterance structures. We are currently ex-
ploring possibilities for a HMM-based language
model, with the conditional probabilities deter-
mined by a trigram backoff model.
3.2 Language analysis
The task of the parsing component is to map
the speaker utterances into task-relevant do-
main concepts which are to be processed by
the dialogue manager. The number of domain
concepts concerning the demonstration system?s
application domain, bus-timetables, is rather
small and contains e.g. bus, departure-time
and arrival-location. However, semantically
equivalent utterances can of course vary in the
lexical elements they contain, and in written and
especially in spoken Finnish the word order in
almost any given sentence can also be changed
without major changes on the semantic level un-
derstood by the system (the difference lies in the
information structure of the utterance). For in-
stance, the request How does one get to Malmi?
can be realised as given in Table 1.
There are two ways to approach the problem:
on one hand we can concentrate on finding the
keywords and their relevant word forms, on the
other hand we can use more specialized syntac-
tic analyzers. At the moment we use CPARSE
as the syntactic analyzer for text-based input.
The grammar has been adjusted for the demon-
Kuinka pa?a?see bussilla Malmille?
Miten pa?a?see Malmille bussilla?
Kuinka Malmille pa?a?see bussilla?
Malmille miten pa?a?see bussilla?
Milla? bussilla pa?a?se Malmille?
Malmille olisin bussia kysellyt.
Pa?a?seeko? bussilla Malmille?
Table 1: Some alternative utterances for Kuinka
pa?a?see Malmille bussilla? ?How does-one-get to-
Malmi by bus?
stration system so that it especially looks for
phrases relevant to the task at hand. For in-
stance, if we can correctly identify the inflected
word form Malmille from the input string, we
can be quite certain of the user wishing to know
something about getting to Malmi.
The current speech input does not go through
any special morpho-syntactic analysis because
of the strict context-free grammar used by the
speech recognizer. The dictionary used by the
recognizer is tagged with the needed morpholog-
ical information and the context-free rules are
tagged with the needed syntactic information.
3.3 Language generation
The language generation function is located in
the system?s Presentation Manager module. Un-
like language analysis, for which different ex-
isting Finnish morphosyntactic analyzers can
be used, there are no readily available general-
purpose Finnish language generators. We are
therefore developing specific generation compo-
nents for this project. The flexible system ar-
chitecture allows us to experiment with different
generators.
Unfortunately the existing Finnish syntactic
analyzers have been designed from the outset as
?parsing grammars?, which are difficult or im-
possible to use for generation. However, the two-
level morphology model (Koskenniemi, 1983) is
in principle bi-directional, and we are work-
ing towards its use in morphological generation.
Fortunately there is also an existing Finnish
speech synthesis project (Vainio, 2001), which
we can use together with the language genera-
tors.
Some of our language generation components
use the XML-based generation framework de-
scribed by Wilcock (2001), which has the ad-
vantage of integrating well with the XML-based
system architecture. The generator starts from
an agenda which is created by the dialogue man-
ager, and is available in the system?s Informa-
tion Storage in XML format. The agenda con-
tains a list of semantic concepts which the dia-
logue manager has tagged as Topic or NewInfo.
From the agenda the generator creates a re-
sponse plan, which passes through the genera-
tion pipeline stages for lexicalization, aggrega-
tion, referring expressions, syntactic and mor-
phological realization. At all stages the response
specification is XML-based, including the final
speech markup language which is passed to the
speech synthesizer.
The system architecture allows multiple gen-
erators to be used. In addition to the XML-
based pipeline components we have some pre-
generated outputs, such as greetings at the start
and end of the dialogue or meta-acts such as
wait-requests and thanking. We are also ex-
ploiting the agent-based architecture to increase
the system?s adaptivity in response generation,
using the level of communicative confidence as
described by Jokinen and Wilcock (2001).
4 Recognition of Discussion Topic
One of the important aspects of the system?s
adaptivity is that it can recognize the correct
topic that the user wants to talk about. By
?topic? we refer to the general subject matter
that a dialogue is about, such as ?bus timetables?
and ?bus tickets?, realized by particular words in
the utterances. In this sense, individual doc-
uments or short conversations may be seen to
have one or a small number of topics, one at a
time.
4.1 Topically ordered semantic space
Collections of short documents, such as news-
paper articles, scientific abstracts and the like,
can be automatically organized onto document
maps utilizing the Self-Organizing Map algo-
rithm (Kohonen, 1995). The document map
methodology has been developed in the WEB-
SOM project (Kohonen et al, 2000), where the
largest map organized consisted of nearly 7 mil-
lion patent abstracts.
We have applied the method to dialogue topic
recognition by carring out experiments on 57
Finnish dialogues, recorded from the customer
service phone line of Helsinki City Transport
and transcribed manually into text. The dia-
logues are first split into topically coherent seg-
ments (utterances or longer segments), and then
organized on a document map. On the ordered
map, each dialogue segment is found in a spe-
cific map location, and topically similar dialogue
segments are found near it. The document map
thus forms a kind of topically ordered semantic
space. A new dialogue segment, either an utter-
ance or a longer history, can likewise be auto-
matically positioned on the map. The coordi-
nates of the best-matching map unit may then
be considered as a latent topical representation
for the dialogue segment.
Furthermore, the map units can be labeled us-
ing named topic classes such as ?timetables? and
?tickets?. One can then estimate the probability
of a named topic class for a new dialogue seg-
ment by construing a probability model defined
on top of the map. A detailed description of the
experiments as well as results can be found in
(Lagus and Kuusisto, 2002).
4.2 Topic recognition module
The topical semantic representation, i.e. the
map coordinates, can be used as input for the
dialogue manager, as one of the values of the
current dialogue state. The system architecture
thus integrates a special topic recognition mod-
ule that outputs the utterance topic in the In-
formation Storage. For a given text segment,
say, the recognition result from the speech rec-
ognizer, the module returns the coordinates of
the best-matching dialogue map unit as well as
the most probable prior topic category (if prior
categorization was used in labeling the map).
5 Dialogue Management
The main task of the dialogue manager com-
ponent is to decide on the appropriate way to
react to the user input. The reasoning includes
recognition of communicative intentions behind
the user?s utterances as well as planning of the
system?s next action, whether this is information
retrieval from a database or a question to clarify
an insufficiently specified request. Natural inter-
action with the user also means that the system
should not produce relevant responses only in
terms of correct database facts but also in terms
of rational and cooperative reactions. The sys-
tem could learn suitable interaction strategies
from its interaction with the user, showing adap-
tation to various user habits and situations.
5.1 Constructive Dialogue Model
A uniform basis for dialogue management can
be found in the communicative principles re-
lated to human rational and coordinated inter-
action (Allwood et al, 2000; Jokinen, 1996).
The speakers are engaged in a particular activ-
ity, they have a certain role in that activity, and
their actions are constrained by communicative
obligations. They act by exchanging new in-
formation and constructing a shared context in
which to resolve the underlying task satisfacto-
rily.
The model consists of a set of dialogue states,
defined with the help of dialogue acts, obser-
vations of the context, and reinforcement val-
ues. Each action results in a new dialogue
state. The dialogue act, Dact, describes the act
that the speaker performs by a particular utter-
ance, while the topic Top and new information
NewInfo denote the semantic content of the ut-
terance and are related to the task domain. To-
gether these three create a useful first approx-
imation of the utterance meaning by abstract-
ing over possible linguistic realisations. Unfilled
task goals TGoals keep track of the activity re-
lated information still necessary to fulfil the un-
derlying task (a kind of plan), and the speaker
information is needed to link the state to pos-
sible speaker characteristics. The expectations,
Expect are related to communicative obligations,
and used to constrain possible interpretations of
the next act. Consequently, the system?s inter-
nal states can be reduced to a combination of
these categories, all of which form an indepen-
dent source of information for the system to de-
cide on the next move.
5.2 Dialogue agents and evaluators
A dialogue state and all agents that contribute
to a dialogue state are shown in Figure 2. The
Dialogue Model is used to classify the current
utterance into one of the dialogue act categories
(Jokinen et al, 2001), and to predict the next
dialogue acts (Expect). The Topic Model rec-
ognizes the domain, or discussion topic, of the
user input as described above.
Figure 2: Dialogue states for user?s utter-
ance and system action, together with dialogue
agents involved in producing various informa-
tion.
All domains out of the system?s capabili-
ties are handled with the help of a special
OutOfDomain-agent which informs the user of
the relevant tasks and possible topics directly.
This allows the system to deal with error sit-
uations, such as irrelevant user utterances, ef-
ficiently and flexibly without invoking the Dia-
logue Manager to evaluate appropriate dialogue
strategies. The information about error situ-
ations and the selected system action is still
available for dialogue and task goal management
through the shared Information Storage.
The utterance Topic and New Information
(Topic, NewInfo) of the relevant user utter-
ances are given by the parsing unit, and sup-
plemented with discourse knowledge by ellipsis
and anaphora resolution agents (which are In-
put Agents). Task related goals are produced by
Task Agents, located in a separate Task Man-
ager module. They also access the backend
database, the public transportation timetables
of Helsinki.
The Dialogue Manager (DM) consists of
agents corresponding to possible system actions
(Figure 3). There are also some agents for inter-
nal system interaction, illustrated in the figure
with a stack of agents labeled with Agent1. One
agent is selected at a time, and the architecture
permits us to experiment with various compet-
ing agents for the same subtask: the evaluators
are responsible for choosing the one that best
fits in the particular situation.
Figure 3: The Dialogue Manager component.
Two types of evaluators are responsible for
choosing the agent in DM, and thus implement-
ing the dialogue strategy. The QEstimate eval-
uator chooses the agent that has proven to be
most rewarding so far, according to a Q-learning
(Watkins and Dayan, 1992) algorithm with on-
line -greedy policy (Sutton and Barto, 1998).
That agent is used in the normal case and the
decision is based on the dialogue state presented
in Figure 2. The underlying structure of the
QEstimate evaluator is illustrated in Figure 4.
The evaluator is based on a table of real val-
ues, indexed by dialogue states, and updated af-
ter each dialogue. The agent with the highest
Figure 4: The QEstimate evaluator.
value for the current dialogue state gets selected.
Adaptivity of the dialogue management comes
from the reinforcement learning algorithm of
this evaluator.
On the other hand, if one of the error evalu-
ators (labeled with Error1..N) detects that an
error has occurred, the QEstimate evaluator is
overridden and a predetermined agent is selected
to handle the error situation (Figure 5). In these
cases, only the the correct agent is given a non-
zero value, forcing the dialogue manager to se-
lect that agent. Examples of such errors include
situations when the user utterance is not recog-
nized by the speech recognizer, its topic is irrele-
vant to the current domain, or its interpretation
is inconsistent with the dialogue context.
Figure 5: An error evaluator.
Because all possible system actions are
reusable agents, we can easily implement a dif-
ferent dialogue management strategy by adding
evaluators, or replacing the current QEstimate
evaluator. We are developing another strategy
based on recurrent self-organizing maps, that
learns to map dialogue states to correct actions
by fuzzy clustering, minimizing the amount of
human labor in designing the dialogue strategy.
6 Demo System and Future Work
The project deals with both speech and text
communication, interfacing with the user by
telephone and web-applications. An initial in-
tegrated system with basic functionality and in-
teraction capabilities concerning bus timetable
inquiries has been built. Sample interactions are
given in Figures 6 and 7. Various other appli-
cation types such as intelligent question-answer
systems, automatic call-centers, and other ser-
vices that would benefit from flexible natural
interaction can also be envisaged.
The current system is the first prototype and
intended to provide useful basis for further ex-
perimentation and system development. The
planned activities include further investigations
of the limitations and advantages of various ap-
proaches for language technology, as well as us-
ability studies concerning real users and users
with special needs.
(1) UT: mmilla?s bussilla ma? pa?a?sen malamille
?with which bus do I get to Malami?
UR: milla? bussilla pa?a?sen malmille
?with which bus do I get to Malmi?
SU: mista? olittekaan la?hdo?ssa? malmille
?where do you want to leave for Malmi?
UT: no hakaniemesta?
?well from Hakaniemi?
UR: hakaniemesta?
?from Hakaniemi?
SU: bussi 70 la?htee sielta? malmille
?bus 70 leaves from there to Malmi?
UT: selva? kiitos hei
?ok thank you bye?
UR: bussi kiitos hei
?bus thank you bye?
SU: kiitos hei
?thank you bye?
Figure 6: Sample dialogue with low recognition
score for the destination Malmille ?to Malmi?
(UT = Utterance as actually made by the user,
UR = Utterance as recognized by the system,
SU = System utterance).
(2) UT: koskas mennee seuraava pussi arapi-
asta malmille
?when will the next bus go from Arapia to
Malmi?
UR: koska menee seuraava bussi arabiasta
malmille
?when will the next bus go from Arabia to
Malmi?
SU: kello 14 30
?at 14:30?
UT: minka? numeroinen se on
?which number is it?
UR: mina? nelonen selva?
?I four okay?
SU: olen pahoillani osaan vastata vain
bussiaikatauluja koskeviin kysymyksiin
?sorry I can reply only to questions con-
cerning bus timetables?
UT: aha no joo jaa kiitos hei
?I see ok thank you bye?
UR: onko aijaa kiitos hei
?is I see thank you bye?
SU: kiitos hei
?thank you bye?
Figure 7: Sample dialogue with misrecognition
of user utterance minka? numeroinen se on
(UT = Utterance as actually made by the user,
UR = Utterance as recognized by the system,
SU = System utterance).
7 Acknowledgements
The research is carried out in the context of the
USIX Interact project, a collaboration project
between four Finnish universities, and funded
by the National Technology Agency, the leading
IT companies ICL Invia oyj, Sonera oyj, Ling-
soft oy, and Gurusoft oy, as well as the Finnish
Association for the Deaf and the Arla Institute.
References
J. Allwood, D. Traum, and K. Jokinen. 2000. Coop-
eration, dialogue and ethics. International Jour-
nal of Human-Computer Studies, 53:871?914.
L. Carlson. 2001. CPARSE manual. http://www
.ling.helsinki.fi/ lcarlson/cparse09en.html.
T. Jauhiainen. 2001. Using existing written lan-
guage analyzers in understanding natural spoken
Finnish. In Proceedings of Nodalida ?01, Uppsala.
K. Jokinen and G. Wilcock. 2001. Confidence-based
adaptivity in response generation for a spoken di-
alogue system. In Proceedings of the 2nd SIGdial
Workshop on Discourse and Dialogue, pages 80?
89, Aarhus.
K. Jokinen, T. Hurtig, K. Hynna?, K. Kanto,
M. Kaipainen, and A. Kerminen. 2001. Self-
organizing dialogue management. In Proceedings
of the 2nd Workshop on Natural Language Pro-
cessing and Neural Networks, pages 77?84, Tokyo.
K. Jokinen. 1996. Goal formulation based on com-
municative principles. In Proceedings of the 16th
COLING, pages 598?603.
F. Karlsson. 1983. Suomen kielen a?a?nne- ja muoto-
rakenne. WSOY, Juva.
T. Kohonen, S. Kaski, K. Lagus, J. Saloja?rvi,
V. Paatero, and A. Saarela. 2000. Organization
of a massive document collection. IEEE Transac-
tions on Neural Networks, Special Issue on Neural
Networks for Data Mining and Knowledge Discov-
ery, 11(3):574?585, May.
T. Kohonen. 1995. Self-Organizing Maps. Springer,
Berlin.
K. Koskenniemi. 1983. Two-level morphology: a
general computational model for word-form recog-
nition and production. University of Helsinki,
Helsinki.
K. Lagus and J. Kuusisto. 2002. Topic identifica-
tion in natural language dialogues using neural
networks. In Proceedings of the 3rd SIGdial Work-
shop on Discourse and Dialogue, Philadelphia.
S. Seneff, E. Hurley, R. Lau, C. Pao, P. Schmid, and
V. Zue. 1998. Galaxy-II: A reference architecture
for conversational system development. In Pro-
ceedings of ICSLP-98, Sydney.
R. Sutton and A. Barto. 1998. Reinforcement Learn-
ing: An Introduction. MIT Press, Cambridge,
Massachusetts.
M. Turunen and J. Hakulinen. 2000. Jaspis - a
framework for multilingual adaptive speech appli-
cations. In Proceedings of the 6th International
Conference on Spoken Language Processing, Bei-
jing.
M. Turunen and J. Hakulinen. 2001a. Agent-based
adaptive interaction and dialogue management ar-
chitecture for speech applications. In Text, Speech
and Dialogue. Proceedings of the Fourth Interna-
tional Conference (TSD-2001), pages 357?364.
M. Turunen and J. Hakulinen. 2001b. Agent-based
error handling in spoken dialogue systems. In Pro-
ceedings of Eurospeech 2001, pages 2189?2192.
M. Vainio. 2001. Artificial Neural Network Based
Prosody Models for Finnish Text-to-Speech Syn-
thesis. Ph.D. thesis, University of Helsinki.
C. Watkins and P. Dayan. 1992. Technical note:
Q-learning. Machine Learning, 8:279?292.
G. Wilcock. 2001. Pipelines, templates and transfor-
mations: XML for natural language generation. In
Proceedings of the 1st NLP and XML Workshop,
pages 1?8, Tokyo.
Proceedings of the EACL 2009 Demonstrations Session, pages 65?68,
Athens, Greece, 3 April 2009. c?2009 Association for Computational Linguistics
A Mobile Health and Fitness Companion Demonstrator?
Olov Sta?hl1 Bjo?rn Gamba?ck1,2 Markku Turunen3 Jaakko Hakulinen3
1ICE / Userware 2Dpt. Computer & Information Science 3Dpt. Computer Sciences
Swedish Inst. of Computer Science Norwegian Univ. of Science and Technology Univ. of Tampere
Kista, Sweden Trondheim, Norway Tampere, Finland
{olovs,gamback}@sics.se gamback@idi.ntnu.no {mturunen,jh}@cs.uta.fi
Abstract
Multimodal conversational spoken dia-
logues using physical and virtual agents
provide a potential interface to motivate
and support users in the domain of health
and fitness. The paper presents a multi-
modal conversational Companion system
focused on health and fitness, which has
both a stationary and a mobile component.
1 Introduction
Spoken dialogue systems have traditionally fo-
cused on task-oriented dialogues, such as mak-
ing flight bookings or providing public transport
timetables. In emerging areas, such as domain-
oriented dialogues (Dybkjaer et al, 2004), the in-
teraction with the system, typically modelled as a
conversation with a virtual anthropomorphic char-
acter, can be the main motivation for the interac-
tion. Recent research has coined the term ?Com-
panions? to describe embodied multimodal con-
versational agents having a long lasting interaction
history with their users (Wilks, 2007).
Such a conversational Companion within the
Health and Fitness (H&F) domain helps its users
to a healthier lifestyle. An H&F Companion has
quite different motivations for use than traditional
task-based spoken dialogue systems. Instead of
helping with a single, well-defined task, it truly
aims to be a Companion to the user, providing
social support in everyday activities. The system
should thus be a peer rather than act as an expert
system in health-related issues. It is important to
stress that it is the Companion concept which is
central, rather than the fitness area as such. Thus
it is not of vital importance that the system should
be a first-rate fitness coach, but it is essential that it
?The work was funded by the European Commis-
sion?s IST priority through the project COMPANIONS
(www.companions-project.org).
Figure 1: H&F Companion Architecture
should be able to take a persistent part in the user?s
life, that is, that it should be able to follow the user
in all the user?s activities. This means that the
Companion must have mobile capabilities. Not
necessarily self-mobile (as a robot), but allowing
the user to bring the system with her, like a hand-
bag or a pair of shoes ? or as a mobile phone.
The paper describes such a Health and Fitness
Companion. It has a stationary (?home?) compo-
nent accounting for the main part of the user in-
teraction and a mobile component which follows
the users in actual exercise activities. Section 2
outlines the overall system and its two basic com-
ponents, and Section 3 details the implementation.
Section 4 discusses some related work, while Sec-
tion 5 describes the demonstrator set-up and plans
for future work.
2 The Health and Fitness Companion
The overall system architecture of the Health and
Fitness Companion is shown in Figure 1. The
system components communicate with each other
over a regular mobile phone network. The home
system provides an exercise plan to the mobile part
and in return gets the results of the performed ex-
ercises from the mobile component.
65
Figure 2: Home Companion interface
2.1 The Home H&F Companion
The home part of the H&F Companion gathers in-
formation from the user and makes suggestions
targeted for a healthy lifestyle on a daily basis.
The Companion communicates with the user in
two main dialogue phases: a planning phase where
the system talks about the coming day with the
user and a reporting phase where the user?s actual
activities are assessed with reference to what was
agreed on earlier. The Companion can discuss the
following topics: travelling to work, getting lunch,
activities to be performed before dinner, getting
dinner, and activities to be performed after din-
ner. It knows activities such as playing football,
squash, or badminton; going to the gym or shop-
ping; and watching television or reading a book.
The Home Companion interface, illustrated
in Figure 2, uses a Nabaztag/tag WLAN rabbit
(www.nabaztag.com) as the physical agent in-
terface. The Nabaztag rabbit provides audio out-
put and push-to-talk speech input, moves its ears,
and operates four coloured lights to signal its sta-
tus. In the current implementation, the user needs
to push the button located on top of the Nabaz-
tag/tag device in order to speak to it. As an alter-
native, the system supports external micro-phones
and loudspeakers. The user is also able to pick a
topic by showing RFID tags (embedded in picture
cards or other objects).
The following dialogue example demonstrates
the interaction with the Companion.
C1: Do you have anything arranged for today?
U1: I?m playing Squash
C2: Is that before dinner?
U2: Yes
C3: Okay. Are you going to get the train to work?
U3: Yes
C4: Right. Could you take in a packed lunch to work?
U4: Yes, that would be fine
Figure 3: The Mobile Companion GUI
(Later on in the evening)
C6: Welcome back! How did your game of Squash go?
U5: The game was cancelled, so I went to the gym
C7: Okay. Did you take the train to work?
U6: Yes, I did
C8: Right. Did you take in a packed lunch?
U7: No. I was in a rush so I went to the cafeteria
2.2 The Mobile H&F Companion
The mobile part of the H&F Companion runs on a
mobile handset (e.g., a PDA), and is used during
physical exercise (e.g., while running or walking)
to track the distance, pace, duration, and calories
burned. The data gathered during an exercise is
stored in the device?s record store, and can be used
to compare the results to previous runs.
The user interface of the Mobile Companion
consists of a single screen showing an image of a
Nabaztag rabbit along with some text areas where
various exercise and device status information is
displayed (Figure 3). The rabbit image is intended
to give users a sense of communicating with the
same Companion, no matter if they are using the
home or mobile system. To further the feeling of
persistence, the home and mobile parts of the H&F
Companion also use the same TTS voice.
When the mobile Companion is started, it asks
the user whether it should connect to the home sys-
tem and download the current plan. Such a plan
consists of various tasks (e.g., shopping or exer-
cise tasks) that the user should try to achieve dur-
ing the day, and is generated by the home system
during a session with the user. If the user chooses
to download the plan the Companion summarizes
the content of the plan for the user, excluding all
tasks that do not involve some kind of exercise ac-
tivity. The Companion then suggests a suitable
task based on time of day and the user?s current
location. If the user chooses not to download the
plan, or rejects the suggested exercise(s), the Com-
panion instead asks the user to suggest an exercise.
66
Once an exercise has been agreed upon, the
Companion asks the user to start the exercise and
will then track the progress (distances travelled,
time, pace and calories burned) using a built-in
GPS receiver. While exercising, the user can ask
the Companion to play music or to give reports on
how the user is doing. After the exercise, the Com-
panion will summarize the result and up-load it to
the Home system so it can be referred to later on.
3 H&F Companion Implementation
This section details the actual implementation of
the Health and Fitness Companion, in terms of its
two components (the home and mobile parts).
3.1 Home Companion Implementation
The Home Companion is implemented on top
of Jaspis, a generic agent-based architecture de-
signed for adaptive spoken dialogue systems (Tu-
runen et al, 2005). The base architecture
is extended to support interaction with virtual
and physical Companions, in particular with the
Nabaztag/tag device.
For speech inputs and outputs, the Home Com-
panion uses LoquendoTMASR and TTS compo-
nents. ASR grammars are in ?Speech Recogni-
tion Grammar Specification? (W3C) format and
include semantic tags in ?Semantic Interpreta-
tion for Speech Recognition (SISR) Version 1.0?
(W3C) format. Domain specific grammars were
derived from a WoZ corpus. The grammars are
dynamically selected according to the current di-
alogue state. Grammars can be precompiled for
efficiency or compiled at run-time when dynamic
grammar generation takes place in certain situa-
tions. The current system vocabulary consists of
about 1400 words and a total of 900 CFG grammar
rules in 60 grammars. Statistical language models
for the system are presently being implemented.
Language understanding relies heavily on SISR
information: given the current dialogue state, the
input is parsed into a logical notation compati-
ble with the planning implemented in a Cognitive
Model. Additionally, a reduced set of DAMSL
(Core and Allen, 1997) tags is used to mark func-
tional dialogue acts using rule-based reasoning.
Language generation is implemented as a com-
bination of canned utterances and tree adjoining
grammar-based structures. The starting point for
generation is predicate-form descriptions provided
by the dialogue manager. Further details and
contextual information are retrieved from the di-
alogue history and the user model. Finally, SSML
(Speech Synthesis Markup Language) 1.0 tags are
used for controlling the Loquendo synthesizer.
Dialogue management is based on close-
cooperation of the Dialogue Manager and the Cog-
nitive Manager. The Cognitive Manager models
the domain, i.e., knows what to recommend to the
user, what to ask from the user, and what kind
of feedback to provide on domain level issues.
In contrast, the Dialogue Manager focuses on in-
teraction level phenomena, such as confirmations,
turn taking, and initiative management.
The physical agent interface is implemented
in jNabServer software to handle communication
with Nabaztag/tags, that is, Wi-Fi enabled robotic
rabbits. A Nabaztag/tag device can handle vari-
ous forms of interaction, from voice to touch (but-
ton press), and from RFID ?sniffing? to ear move-
ments. It can respond by moving its ears, or by
displaying or changing the colour of its four LED
lights. The rabbit can also play sounds such as
music, synthesized speech, and other audio.
3.2 Mobile Companion Implementation
The Mobile Companion runs on Windows Mobile-
based devices, such as the Fujitsu Siemens Pocket
LOOX T830. The system is made up of two pro-
grams, both running on the mobile device: a Java
midlet controls the main application logic (exer-
cise tracking, dialogue management, etc.) as well
as the graphical user interface; and a C++-based
speech server that performs TTS and ASR func-
tions on request by the Java midlet, such as load-
ing grammar files or voices.
The midlet is made up of Java manager classes
that provide basic services (event dispatching,
GPS input, audio play-back, TTS and ASR, etc.).
However, the main application logic and the GUI
are implemented using scripts in the Hecl script-
ing language (www.hecl.org). The script files
are read from the device?s file system and evalu-
ated in a script interpreter created by the midlet
when started. The scripts have access to a num-
ber of commands, allowing them to initiate TTS
and ASR operations, etc. Furthermore, events
produced by the Java code are dispatched to the
scripts, such as the user?s current GPS position,
GUI interactions (e.g., stylus interaction and but-
ton presses), and voice input. Scripts are also used
to control the dialogue with the user.
67
The speech server is based on the Loquendo
Embedded ASR (speaker-independent) and TTS
software.1 The Mobile Companion uses SRGS 1.0
grammars that are pre-compiled before being in-
stalled on the mobile device. The current system
vocabulary consists of about 100 words in 10 dy-
namically selected grammars.
4 Related Work
As pointed out in the introduction, it is not the aim
of the Health and Fitness Companion system to be
a full-fledged fitness coach. There are several ex-
amples of commercial systems that aim to do that,
e.g., miCoach (www.micoach.com) from Adi-
das and NIKE+ (www.nike.com/nikeplus).
MOPET (Buttussi and Chittaro, 2008) is a
PDA-based personal trainer system supporting
outdoor fitness activities. MOPET is similar to
a Companion in that it tries to build a relation-
ship with the user, but there is no real dialogue
between the user and the system and it does not
support speech input or output. Neither does
MPTrain/TripleBeat (Oliver and Flores-Mangas,
2006; de Oliveira and Oliver, 2008), a system that
runs on a mobile phone and aims to help users
to more easily achieve their exercise goals. This
is done by selecting music indicating the desired
pace and different ways to enhance user motiva-
tion, but without an agent user interface model.
InCA (Kadous and Sammut, 2004) is a spoken
language-based distributed personal assistant con-
versational character with a 3D avatar and facial
animation. Similar to the Mobile Companion, the
architecture is made up of a GUI client running on
a PDA and a speech server, but the InCA server
runs as a back-end system, while the Companion
utilizes a stand-alone speech server.
5 Demonstration and Future Work
The demonstration will consist of two sequential
interactions with the H&F Companion. First, the
user and the home system will agree on a plan,
consisting of various tasks that the user should try
to achieve during the day. Then the mobile system
will download the plan, and the user will have a
dialogue with the Companion, concerning the se-
lection of a suitable exercise activity, which the
user will pretend to carry out.
1As described in ?Loquendo embedded technologies:
Text to speech and automatic speech recognition.?
www.loquendo.com/en/brochure/Embedded.pdf
Plans for future work include extending the mo-
bile platform with various sensors, for example, a
pulse sensor that gives the Companion informa-
tion about the user?s pulse while exercising, which
can be used to provide feedback such as telling
the user to speed up or slow down. We are also in-
terested in using sensors to allow users to provide
gesture-like input, in addition to the voice and but-
ton/screen click input available today.
Another modification we are considering is to
unify the two dialogue management solutions cur-
rently used by the home and the mobile compo-
nents into one. This would cause the Companion
to ?behave? more consistently in its two shapes,
and make future extensions of the dialogue and the
Companion behaviour easier to manage.
References
Fabio Buttussi and Luca Chittaro. 2008. MOPET:
A context-aware and user-adaptive wearable sys-
tem for fitness training. Artificial Intelligence in
Medicine, 42(2):153?163.
Mark G. Core and James F. Allen. 1997. Coding di-
alogs with the DAMSL annotation scheme. In AAAI
Fall Symposium on Communicative Action in Hu-
mans and Machines, pages 28?35, Cambridge, Mas-
sachusetts.
Laila Dybkjaer, Niels Ole Bernsen, and Wolfgang
Minker. 2004. Evaluation and usability of multi-
modal spoken language dialogue systems. Speech
Communication, 43(1-2):33?54.
Mohammed Waleed Kadous and Claude Sammut.
2004. InCa: A mobile conversational agent. In Pro-
ceedings of the 8th Pacific Rim International Con-
ference on Artificial Intelligence, pages 644?653,
Auckland, New Zealand.
Rodrigo de Oliveira and Nuria Oliver. 2008. Triple-
Beat: Enhancing exercise performance with persua-
sion. In Proceedings of 10th International Con-
ference, on Mobile Human-Computer Interaction,
pages 255?264, Amsterdam, the Netherlands. ACM.
Nuria Oliver and Fernando Flores-Mangas. 2006.
MPTrain: A mobile, music and physiology-based
personal trainer. In Proceedings of 8th International
Conference, on Mobile Human-Computer Interac-
tion, pages 21?28, Espoo, Finland. ACM.
Markku Turunen, Jaakko Hakulinen, Kari-Jouko
Ra?iha?, Esa-Pekka Salonen, Anssi Kainulainen, and
Perttu Prusi. 2005. An architecture and applica-
tions for speech-based accessibility systems. IBM
Systems Journal, 44(3):485?504.
Yorick Wilks. 2007. Is there progress on talking sensi-
bly to machines? Science, 318(9):927?928.
68
Coling 2008: Proceedings of the workshop on Speech Processing for Safety Critical Translation and Pervasive Applications, pages 24?31
Manchester, August 2008
??2008.?Licensed?under?the Creative?Commons
Attribution?Noncommercial?Share?Alike?3.0?Unported li?
Cense?(http://creativecommons.org/licenses/by?nc?sa/3.0/).
Some?rights?reserved.
Interoperability?and?Knowledge?Representation?in?Distributed?Health
and?Fitness?Companion?Dialogue?System
Jaakko?Hakulinen
Department?of?Computer?Sciences
33014?University?of?Tampere,?Finland
jaakko.hakulinen@cs.uta.fi
Markku?Turunen
Department?of?Computer?Sciences
33014?University?of?Tampere,?Finland
markku.turunen@cs.uta.fi
Abstract
As? spoken? dialogue? systems? move? be?
yond?task?oriented?dialogues?and?become
distributed? in? the? pervasive? computing
environments,? their? growing? complexity
calls? for? more? modular? structures.? When
different? aspects? of? a? single? system? can
be? accessed? with? different? interfaces,
knowledge? representation? and? separation
of? low? level? interaction? modeling? from
high? level?reasoning?on?domain? level?be?
comes? important.? In? this? paper,? a? model
utilizing?a?dialogue?plan?to?communicate
information?from?domain?level?planner?to
dialogue?management?and?from?there?to?a
separate? mobile? interface? is? presented.
The? model? enables? each? part? of? the? sys?
tem? handle? the? same? information? from
their? own? perspectives? without? contain?
ing?overlapping?logic.
1 Introduction
Most? existing? spoken? dialogue? systems? pro?
vide? a? single? interface? to? solve? a? well?defined
task,? such? as? booking? tickets? or? providing? time?
table? information.?There?are? emerging?areas?that
differ? dramatically? from? task?oriented? systems.
In? domain?oriented? dialogues? (Dybkjaer? et? al,
2004)? the? interaction? with? the? system,? typically
modeled?as?a?conversation?with?a?virtual?human?
like?character,?can?be?the?main?motivation?for?the
interaction.?These?systems?are?often? multimodal,
and?may?take?place?in?pervasive?computing?envi?
ronments? where? various? mobile,? robotic,? and
other?untraditional? interface?are?used? to?commu?
nicate?with? the?system.?For?example,? in? the?EU?
funded? COMPANIONS?project? (Wilks,? 2007)
we? are? developing? a? conversational? Health? and
Fitness? Companion? that? develops? long?lasting
relationships? with? its? users? to? support? their
healthy? living? and? eating? habits? via? mobile? and
physical?agent?interfaces.?Such?systems?have?dif?
ferent? motivations? for? use? compared? to? tradi?
tional? task?based? spoken? dialogue? systems.? In?
stead?of?helping?with?a?single,?well?defined?task,
the?system?aims?at?building?a?long?term?relation?
ship? with? its? user? and? providing? support? on? a
daily?basis.
1.1 Mobile?and?Physical?Agent?Interfaces
New? kinds?of? interfaces? are?used? increasingly
often? in? conjunction? with? spoken? dialogue? tech?
nology.? Speech? suits? mobile? interfaces? well? be?
cause?it?can?overcome?the?limited? input?and?out?
put?modalities?of? the?small? devices?and?can?also
better? support? using? during? the? moments? when
their?hand?or?eyes?are?busy.?Physical?agent?inter?
faces,?on? the?other?hand,?have?been?used? in?sys?
tems,?which? try? to? make? dialogue?systems? more
part?of?people?s?life.?In?many?cases,?they?include
rich?multimodal?input?and?output?while?providing
a?physical?outlook?for?the?agent.?While?naturalis?
tic? human?like? physical? robots? are? under? devel?
opment,? especially? in? Japan,? there? is? room? for? a
variety? of? different? physical? interface? agents
ranging? from? completely? abstract? (e.g.,? simple
devices?with?lights?and?sound)?to?highly?sophisti?
cated? anthropomorphic? apparatus.? For? example,
Marti?and?Schmandt?(2005)?used?several?toy?ani?
mals,? such? as? bunnies? and? squirrels,? as? physical
embodied? agents? for? a? conversational? system.
Other?example?is?an?in?door?guidance?and?recep?
tionist? application? involving?a? physical? interface
agent? that? combines? pointing?gestures? with? con?
versational?speech?technology?(Kainulainen?et?al.,
2005).?Some?physical? agent? technology?has? also
24
been? commercialized.?For? example,? the? wireless
Nabaztag? /tag? rabbits
(http://www.nabaztag.com/)? have? been? success
and? an? active? user? community? has? emerged
around?it.
Both? mobile? use? and? physical? agent? interface
can? support? the? goal? of? making? a? spoken? dia?
logue? system? part? of? users?? everyday? life? and
building? a? meaningful? relationship? between? the
system?and?the?user.?It?has?been?found?that?mere
existence? of? a? physical? interface? changes? users?
attitude? toward?a? system?and? having?access? to? a
system?throughout?the?day?via?a?mobile?interface
is?likely?to?further?support?this?relationship.
In? this?work,?we?have?used? the?Nabaztag?as?a
multimodal?physical?interface?to?create?a?conver?
sational? Health? and? Fitness? Companion? and? a
mobile? version? interface? for? outdoor? usage? has
been?implemented?on?Windows?Mobile?platform.
1.2 Inter?component? Communication? and
Knowledge?Representation?Challenges
In? systems,? where? multiple? interfaces? can? be
used?to?access?parts?of?the?same?functionality?and
the?system?interacts?with?a?user?many?times?over
a? long? time?period,?modeling? the? interaction?and
domain? easily? becomes? complex.? For? example,
the?system?should?model?interaction?history?on?a
longer?timescale?than?a?single?session.?With?mul?
tiple? interfaces,? at? least? some? such? information
could?be?useful?if?they?can?be?shared?between?the
interfaces.?Furthermore,?the?system?must?include
a?model? capable?of? reasoning?about? the?domain,
and? learn?from?the?user?and?his?or?her?actions?to
provide? meaningful? interaction,? such? as? to? pro?
vide? reasonable? guidance? on? user?s? health? and
progress? as? the?user?s?condition?alters?over? time
in?our?case?with?the?Health?and?Fitness?Compan?
ion.? Such? reasoning? should? be? concentrated? on
one? component,? instead? of? duplicating? the? logic
to?keep?the?system?maintainable.?Still,? the? infor?
mation? must? be? communicated? over? different
interfaces?and?the?component?inside?them.?There?
fore,?modularization?of?the?system?and?appropri?
ate?knowledge?representation?become?vital.
On? dialogue? management? level,? a? common
way?to?take?some?complexity?away?from?the?dia?
logue? manager? and? limit? its? tasks? more? specifi?
cally? to? dialogue? management? is? to? separate?do?
main? specific? processing,? such? as? database? que?
ries,?into?a?back?end?component.?Many?research?
ers?have?worked?with?separating?generic?dialogue
management?processes?from?the?domain?specific
processes.? Example? solutions? include? shells
(J?nsson,? 1991)? and? object? oriented? program?
ming?methods?(Salonen,? et?al.,?2004,?O?Neill,?et
al.,?2003).?On?the?other?hand,?a?simple?back?end
interface,? e.g.,? SQL? queries,? can? be? included? as
configuration? parameters? (Pellon? et? al.,? 2000).
Since? dialogue? management? is? usually? based? on
state? transition? networks,? form? filling,? or? some
other? clearly? defined? model,? separating? domain
specific?processing?to?the?back?end?makes?it?pos?
sible? to? implemented? dialogue? management
purely?with?the?selected?model.
Health?and?Fitness?Companion,?as?discussed?in
the? following,? is? based? on? a? model? where? the
domain?specific? module? is?more?than?just?a?sim?
ple? interface? and? includes? active? processing? of
domain?information,?reasoning,?learning,?or?other
complex? processes.? We? call? such? a? component
the?cognitive?model.?While?the?task?of?a?dialogue
manager?is?to?maintain?and?update?dialogue?state,
the? cognitive? model? reasons? using? the? domain
level? knowledge.? In? our? case,? we? have? two? dia?
logue?managers,?one?for?the?home?system?with?a
physical? interface?agent?and?one?for?mobile?sys?
tem?(yet?another? is? in?development,?but?not?con?
sidered?here).?The?two?handle?somewhat?separate
tasks?but? each? provides? input? to? another? and? the
cognitive? model.? Separation?of? the? task?between
the? different? parts? is? not? trivial.? For? example,
managing? dialogue? level? phenomena,? such? as
error? handling? and? basic? input? processing,? are
tasks? clearly? in? the? areas? of? respective? dialogue
managers.?However,?cognitive?modeling?can?help
in? error? handling? by? spotting? input? that? seems
suspicious? based? on? domain? level? information
and? input? parsing? by? providing? information? on
potential?discussion?topics.?The?solution?we?have
devised?is?to?have?the?cognitive?model?produce?a
dialogue? plan? for? the? dialogue? management? in
home? system.? The? dialogue? management? in? the
home? system? provides? parsed? user? inputs? to? the
cognitive? model? and? to? the? mobile? system.? The
mobile?system?provides?similar?input?back?to?the
home?system,?which?communicates?it?back?to?the
cognitive?model.
In? the? following? we? describe? the? Health? and
Fitness?dialogue?system?in?general.?Then?we?dis?
cuss? the? mobile? interface,? the? dialogue? manager
of?the?home?system?and?the?cognitive?model,?be?
fore? going? into? details? on? how? the? components
have? been? separated.? The? solution,? which? pro?
vides? great? flexibility? for? each,? is? discussed? be?
fore?conclusions.
25
2 Health?and?Fitness?Companion
The?Health?and?Fitness?Companion?(H&F)?is?a
conversational? interface? for? supporting? healthy
lifestyle.?The?companion?plans?each?day?together
with? its? user? at? home,? and? suggests? healthy? ac?
tivities,?such?as?walking?to?work,?when?possible.
During? the? day,? a? mobile? interface? to? the? Com?
panion? can? be? used? to? record? various? physical
activities,? such? as? those? walks? to? work.? After?
wards,?the?user?is?able?to?report?back?to?the?com?
panion?on?the?day,?and?get?more?advice?and?sup?
port.? At? this? point? information? recorded? by? the
mobile? system? is? automatically?used?by? the? sys?
tem.
?Figure?1:?Health?and?Fitness?Companion?Sce?
nario.
As? seen? in? Figure? 1,? H&F? home? system? uses? a
Nabaztag/tag? WLAN? rabbit? as? a? physical? inter?
face.? Nabaztag? provides? audio? output? and? push?
to?talk?input,?and?is?able?to?move?its?ears?and?op?
erate? four? colored? lights? to? signal,? for? example,
emotions.?The?mobile?interface,?as?seen?in?figure
2,? runs? on? a? Window? Mobile? platform? and? uses
push?to?talk? speech? input,? speech? output? and? a
graphical?interface?with?key?and?stylus?input.?The
graphics?include?Nabaztag?graphics?and?the?same
voice?as?in?the?home?system?is?used?for?output?to
help?users?associate? the? two? interfaces.?The? mo?
bile? Companion? follows? the? user? for? physical
activities,? such? as? jogging,? and? collects? data? on
the? exercises? and? feeds? this? back? into? the? main
system.? While? it? includes? a? multimodal? speech
interface,?the?main?input?modality?for?the?mobile
Companion? can? be? considered? to? be? GPS? posi?
tioning.?It?is?used?to?collect?information?on?user?s
exercise? and? provide? feedback? during? the? exer?
cise.?It?is?also?used?as?the?detection?for?the?com?
pletion? of? the? exercises,? which? information? is
then?forwarded?to?the?home?system?and?the?cog?
nitive?model.
From?technical?viewpoint,?H&F?is?a?multimo?
dal? spoken? dialogue? system? containing? compo?
nents? for? speech? recognition? (ASR),?natural? lan?
guage? understanding? (NLU),? dialogue? manage?
ment? (DM),? natural? language?generation? (NLG),
and? speech? synthesis? (TTS).? Furthermore,? it? in?
cludes? a? separate? cognitive? model? (CM),? which
works? in? close? co?operation? with? DM? of? the
home? system,? as?presented? in? the? following? sec?
tions.?The?dialogue?system?in?the?home?system?is
implemented? using? Java? and? Jaspis? framework
(Turunen? et? al.,? 2005)? with? jNabServer
(http://www.cs.uta.fi/hci/spi/jnabserver/)? for? Na?
baztag? connectivity.? The? cognitive? model? is? im?
plemented? in?Lisp?and? integrated? into? the? Jaspis
framework.?The?mobile?interface?is?implemented
in?Java?with?native?C++?code?for?speech?technol?
ogy? components.? It? uses PART
(http://part.sourceforge.net/)? for? persistent? stor?
age?and?HECL?for?scripting?in?dialogue?manager
(http://sourceforge.net/projects/hecl).
?Figure?2:?Mobile?Companion?Interface.
GPS?status?bar
Exercise
status?bar
Avatar?and?text
output?area
??Good?morning,?anything?interesting?organized
for?today?
??I?m?going?for?a?walk.
??Is?that?walk?before?dinner?
??No,?I?think?I?ll?walk?after?I?ve?eaten.
??OK,?so?you?are?going?for?a?walk?after?dinner,?is
that?correct?
??Yes.
??Great,?why?don?t?you?cycle?to?work?
??Okay,?I?can?do?that.
??How?about?picking?up?lunch?from?the?shops?
26
For? speech? recognition? and? synthesis,? H&F
uses? Loquendo? ASR?and? TTS.? Current? recogni?
tion? grammars? for? the? home? system,? derived
from? a? WOZ? data? and? extended? using? user? test
data,?have?a?vocabulary?of?1090?words?and?a?to?
tal?of?436?grammar?rules.?Recognition?grammars
are? dynamically? selected? for? each? user? input,
based?on?the?dialogue?state.?The?mobile?interface
use? mobile? versions? of? Loquendo? technology.
Due? to? the? technological? limitations,? more? chal?
lenging?acoustic? environment,? potential?physical
exhaustion?of?users,?and?more?restricted?domain,
the?recognition?grammars?in?the?mobile?interface
will? remain? significantly? smaller? than? those? of
the?home?system.
NLU? is? based? on? SISR? semantic? tags
(http://www.w3.org/TR/semantic?interpretation/)
embedded? in? the? recognition? grammars.? In? the
home?system,? where? mixed? initiative? interaction
is?possible,? the?tags?provide?parameters?compati?
ble?with?predicates?used?to?represent?information
on?the?dialogue?management?level.?Input?parsing
unifies? these? parameters? into? full? predicates
based? on? the? current? dialogue? state.? In? mobile
system,? more? strict? state? based? dialogue? model?
ing? can? results? in? unambiguous? output? straight
from?the?SISR?tags.
Natural? language? generation? is? a? mixture? of
canned?strings?and,? in? the?home?system,? tree?ad?
joining? grammar? based? generation.? In? addition,
control? messages? for? Nabaztag? ears? and? lights
can?be?generated.
As? discussed? previously,? distribution? and? co?
ordination? of? the? different? tasks? between? differ?
ent? components? can? become? rather? complex? in
systems? such? as? H&F? without? proper? modeling
of? interaction,? domain,? and? reasoning? compo?
nents.? Next,? we? present? a? model? which? allows
flexible? interaction?between? the? cognitive? model
and?the?dialogue?management.
3 Dialogue? Management? and? Cognitive
Modeling
There?is?great?consensus?that?components?of?a
dialogue? system? can? be? split? into? at? least? three
parts:?an?input?module,?which?receives?user?input
and?parses? it? into? a? logical? form,?dialogue? man?
agement,? which? maintains? and? updates? dialogue
state? based? on? user? input? and? generates? output
requests,?and?an?output?module,?which?generates
natural? language?output? to?user? based?on? the? re?
quests.? In? the? case? of? H&F,? we? have? also? sepa?
rated? a? cognitive? model? (CM)? from? dialogue
manager?(DM),?as?seen?in?Figure?3.?We?call?this
module? the? cognitive? model,? because? it? contains
what? can? be? considered? higher? level? cognitive
processes?of?the?system.?Next,?we?present?DM?of
the? home? system,? CM? component,? and? the? mo?
bile?interface,?focusing?on?their?interaction.
Figure?3:?Information?passed?between?the
components.
3.1 Cognitive?Model?Responsibilities
The? task? of? CM? is? to? model? the? domain,? i.e.,
know? what? to? recommend? to? the? user,? what? to
ask? from? the?user? and? what?kind? of? feedback? to
provide.?CM? in?H&F?uses?hierarchical? task?net?
works? (HTNs)? (Cavazza? et? al.,? 2008)? as? the
method?of?planning?healthy?daily?activity?for? the
user.?Part? of? a?network?can?be? seen? in?Figure?4.
In?the?current?H&F?implementation,?the?planning
domain?included?16?axioms?and?111?methods,?49
operators,?42?semantic?tags,?113?evaluation?rules
and? there? are?17?different? topics? to?be?discussed
with?the?user.
Travel
Active Passive
Cycling
Walking
N?Stop?
Bus
N?Stop?
Subway
N?Stop?
Train
Figure?4:?Hierachical?Task?Network.
CM? is? aware? of? the? meaning? of? the? concepts
inside? the? system? on? a? domain? specific? level.? It
generates?and?updates?a?dialogue?plan?according
to? the? information? received? from? the? user.? The
plan?is?forwarded?to?DM.?Interaction?level?issues
are?not?directly?visible?to?CM.
27
3.2 Dialogue?Management?in?the?Home?Sys?
tem
The? task? of? DM? is? to? maintain? and? update? a
dialogue? state.? In? the?H&F?system,? the? dialogue
state? includes? a? dialogue? history? tree? (currently
linear),?a?stack?of?active?dialogue?topics,?and?cur?
rent?user?input,?including?ASR?confidence?scores
and?N?best? lists.? In? addition,? two?pools?of? items
that? need? to? be? confirmed? are? stored;? one? for
items? to? be? confirmed? individually? and? another
for? those? that? can? be? confirmed? together? in? one
question.
DM? receives? user? inputs? as? predicates? parsed
by? the? NLU? component.? If? an? utterance? is? suc?
cessfully? parsed? and? matches? the? current? dia?
logue?plan? (see?Section?3.3),? DM?does? not? need
to?know?what?the?meaning?of?the?input?actually?is.
It? just? takes? care? of? confirmations? and? provides
the? information? to?CM.? When? generating? output
requests?based?on? the?plan,?DM?can?also?be?un?
aware?of? the?specific?meaning?of? the?plan? items.
Overall,?DM?does?not?need?to?have?the?deep?do?
main?understanding?CM?specializes?in.
DM,?however,? is?aware?of? the?relations?of? the
predicates? on? the? topics? level,? i.e.,? it? knows,
which? predicates? belong? to? each? topic.? This? in?
formation? is? used? primarily? for? parsing? input.
DM? also? has? understanding? of? the? semantics? of
the? predicates? which? relates? to? interaction.
Namely,? relations? such? as? question? ?? answer
pairs? (suggestion? ?? agreement,? confirmation? ?
acceptance/rejection,?etc.)?are?modeled.
On? implementation? level,? dialogue? manage?
ment? is? implemented? as? a? collection? of? separate
small?dialogue?agents,?following?the?principles?of
the? underlying? Jaspis? architecture.? These? agents
are?small?software?components,?each?taking?care
of? a? specific? task?and? in? each?dialogue? turn? one
or? more? agents? are? selected? by? DM.? In? the? cur?
rent? H&F? prototype,? there? are? over? 30? dialogue
agents.? There? is? a? separate? agent? for? each? topic
that?can?occur? in? the?plan.? In?practice,?one? topic
maps? to? a? single?plan? item.?These? agents? are? all
instances?of?a?single?class?with?specific?configu?
rations.?Each?agent?handles? all? situations? related
to?its?topic;?when?the?topic?is?the?first?item?of?an
active? plan,? they? produce? related? output? and
when? the? user? provides? input? matching? to? the
topic? they? forward? that? information? back? to? the
cognitive? model.? In? addition,? topic? specific
agents?handle?explicit?topic?switch?requests?from
the? user? (e.g.,??let?s? talk? about? lunch?)? and? also
take? turn? if? the? topic? is? found?on? top?of? the?dia?
logue? topic? stack.? A? topic? ends? up? in? the? stack
when?it?has?not?been?finished?and?a?new?topic?is
activated.? The? other? agents? found? in? the? system
include? one? that? generates? a? confirmation? if? the
ASR? confidence? score? is? too? low,? one? that? re?
peats?the? last?system?utterance?when?the?user?re?
quests? it? (?please? repeat? the? last? one?),? and? an
agent?to?handle?ASR?rejection?errors.
3.3 Mobile?System
Mobile?system?is?designed?mainly?to?support?us?
ers??on?their?physical?exercises?and?collected?data
on?them?fro?the?home?system.?The?mobile?system
receives?the?day?plan?that?the?user?has?made?with
the?home?system?and?it?is?used?as?basis?when?us?
ers?activates?the?system.?This?way,?the?user?does
not?need?to?re?enter?information?such?as?the?type
of? an? exercise.? This? is? possible,? however,? with
simple?spoken?dialogue?or?by?using?the?graphical
user?interface.?During?the?exercise,?GPS?informa?
tion?is?used?by?the?system?to?provide?feedback?on
pace? to? the? user? using? speech? output.? For? dia?
logue? management,? the? mobile? system? uses? a
state?based? model,? based? on? scripting.? Since? the
mobile?system?focuses?on?the?physical?exercises,
it? is? aware? of? the? meaning? of? the? predicates? it
receives?on? that? level.? It?knows?more?about?run?
ning?and?walking? than?any?other? component.?At
the?same?time,? it? ignores?most?of? the?day?plan? it
receives.? For? example,? eating? related? plan? items
are?not?relevant?to?the?mobile?system?in?any?way
and?are?ignored?(however,?in?the?future?we?could
include?the?possibility?to?report?on?meals?as?well).
3.4 Dialogue?Plan?and?Day?Plan
The? communication? between? the? dialogue
managers? and? CM? is? based? on? a? dialogue? plan
and?a?day?plan.?Various?kinds?of? dialogue?plans
(Larsson? et? al.,? 2000,? Jullien? and? Marty,? 1989)
have?been?used? inside? dialogue? managers? in? the
past.?A?plan?usually?models?what?the?system?sees
as?the?optimal?route?to?task?completion.
In?H&F,?CM?provides?a?plan?on?how?the?cur?
rent? task? (planning? a? day,? reporting? on? a? day)
could?proceed.?The?plan?consists?of?items,?which
are? basically? expressions? on? domain? specific
propositional? logic.? Example? 1? contains? two
items? from? the? start? of? a? plan? for? planning? the
day? with? the?user? in? the? morning.? The? first? plan
item? (QUERY?PLANNED?ACTIVITY)? can? be
realized? as? the? question? ?Anything? interesting
planned?for?today???by?the?system.
As? new? information? becomes? available? (from
the?user),?it?forms?a?plan?for?the?day?or?a?report?of
the? day.? DM? provides? this? information? to? CM,
28
piece? by? piece? as? it? becomes? available.? At? the
same? time,? the? information? is? uploaded? into? a
web? server,? where? the? mobile? interface? can? ac?
cess?it?anytime.
As?CM?receives?the?information,?it?updates?the
dialogue? plan? as? necessary.? Query? type? items,
whose? information? has?been? gathered,?disappear
from?the?plan?and?new?items?may?appear.
The?messages? sent? to?CM?can?add?new? infor?
mation? (predicates)? to? CM? state.? DM? can? also
remove? information? from? CM? if? previously? en?
tered?information?is?found?to?be?untrue.?Similarly,
information?uploaded? to? the? web? server? for? mo?
bile? use? can? be? modified.? The? information? in?
cludes? statements? on? user?s? condition? (tired),
user?s? commitments? to? the? system? (will? walk? to
work),? user?s? preferences? (does? not? like? cafete?
rias)? and? user?s? reports? on? past? activity? (took? a
taxi? to?work),?which?can?be?accomplishments?or
failures?of?earlier?commitments.
<plan>
?<plan?name>Generate?Task?
Model?Questions</plan?name>
?<plan?item>
???<action>QUERY?PLANNED?
ACTIVITY</action>
?</plan?item>
?<plan?item>
???<action>SUGGEST?TRAVEL?
METHOD</action>
???<param>CYCLING?
TRAVEL</param>
???<param>HOME</param>
???<param>WORK</param>
?</plan?item>
?
Example?1:?Start?of?a?plan.
DM? in? the? home? system? can? follow? the? dia?
logue? plan? produced? by? CM? step? by? step.? Each
step? usually? maps? to? a? single? question,? but? can
naturally?result? in?a?longer?dialogue? if? the?user?s
answer? is? ambiguous? or? error? management? is
necessary,?or?if?DM?decides?to?split?a?single?item
to?multiple?questions.?For?example,? the?two?dia?
logue? turn?pairs?seen? in? example?2?are? the?result
of? a? single? plan? item? (QUERY?PLANNED?
ACTIVITY).? Since? the? first? user? utterance? does
not?result?in?a?complete,?unambiguous?predicate,
DM? asks? a? clarification? question.? A? single? user
utterance? can? also? result? in? multiple? predicates
(e.g.,? will? not? take? bus,? has? preference? to? walk?
ing).
When? the? mobile? interface? is? activated,? it
downloads? the? current? day? plan? from? the? web
server? and? uses? it? as? a? basis? for? the? dialogue? it
has? with? the? user.?The? exercise? which? will? then
take? place? can? be? linked? to? an? item? in? the? day
plan,?or?it?can?be?something?new.?As?the?exercise
is? completed? (or? aborted),? information? in? this? is
uploaded? to? the?web?server.?From? there? the?DM
of? the?home?system?can?download? it.?This? infor?
mation? is? relevant? to? the? DM? when? the? user? is
reporting?on?a?day.?The?home?system?downloads
the? information? provided? by? the? mobile? system
and?reports?it?back?to?CM?when?the?dialogue?plan
includes? a? related? item.? DM? may? also? provide
some?feedback?to?the?user?based?on?the?informa?
tion.? It? is? noteworthy,? that?CM?does?not?need? to
differentiate?in?any?way,?whether?the?information
on? the? exercise?came? from?the?mobile?system?or
was?gathered?in?a?dialogue?with?the?home?system.
(??<plan?item>
?? <action>QUERY?PLANNED?
ACTIVITY</action>
?</plan?item>)
S:?Good?morning.?Anything?in?
teresting?organized?for?today?
U:?I?m?going?jogging.
(<pred>
?? <action>PLANNED?
ACTIVITY</action>
?? <param>ACTIVITY?
JOGGING</param>
?? <param>unknownTime</param>
</pred>?)
S:?Is?that?jogging?exercise
before?dinner?
U:?No,?it?s?after.
(??<pred>
?? <action>PLANNED?
ACTIVITY</action>
?? <param>ACTIVITY?
JOGGING</param>
?? <param>AFTER?DINNER</param>
?</pred>?)
Example?2:?A?dialogue?fragment?and?a?corre?
sponding?plan?item?and?predicates,?latter?of
which?is?forwarded?to?the?cognitive?model?and
the?mobile?interface.
Similarly,? clarifications? and?confirmations? are
not? directly? visible? to? CM.? DM? can? confirm
items? immediately? (for? example,? when? low?con?
fidence? is?reported?by?the?NLG?component)?or?it
can?delay?confirmations?to?generate?a?single?con?
29
firmation? for? multiple? items? at? an? appropriate
moment.
Most? importantly,? when? presenting? questions
and?suggestions?to?the?user,?DM?is?free?to?choose
any? item? in? the? plan,? or? even? do? something? not
included? in? the? plan? at? all.? When? information
from?the?mobile?system?is?available,?it?can?direct
where?we?start?the?dialogue?from.?DM?could?also
decide? to? do? some? small?talk? to? introduce?sensi?
tive?topics,?which?can?be?useful?in?managing?the
user?system? relationship? (Bickmore? and? Picard,
2005).?In?the?future,?we?see?DM?to?have?various
kinds?of?knowledge?on?the?dialogue?topics:?it?can
know?how?personal?these?topics?are?and?how?top?
ics? are? related? to? each? other.? It? may? also? have
some?topics?of?its?own.?The?communication? that
is?not?related?to?the?domain?does?not?reach?CM?at
any?point.
CM? can? include? additional? annotation? in? the
plan.?One?such?example?is?the? importance?of?the
information.? If? information? is?marked? important,
it? is? likely,?but?not? certain,? that?DM? will? explic?
itly? confirm? it.? It? is? also?possible? for?CM? to? ex?
plicitly? request? a? confirmation? by? generating? a
separate?plan?item.?For?example,?if?a?user?reports
on?having?run?much?more?than?they?are?likely?to
be?capable?of?in?their?condition,?CM?can?generate
a?confirmation?plan?item.?It? is?worth?noting,?that
DM?cannot?do?reasoning?on?such?level?and?there?
fore? CM? must? participate? in? error? handling? in
such?cases.
3.5 Benefits?of?the?Model
The? presented? model? for? interoperability? be?
tween? the? mobile? system,? the? DM? of? the? home
system?and?CM?has?provided?great?flexibility?for
each?component.?While?the?dialogue?plan?gener?
ated? by? CM? provides? a? base? for? dialogue? man?
agement,?which,? in?most?cases,? is? followed,?DM
can? deviate? from? it.? DM? can? handle? confirma?
tions?as?it?pleases,?add?small?talk,?and?process?the
plan?items?in?any?order.?The?model?also?supports
mixed?initiative? dialogues;? while? DM? may? fol?
low? the?plan,? the?user?may?discuss? any? topic.? In
our? current? implementation,? user? input? is?parsed
first? against? the? previous? system? output,? next? to
the? current? topic,? and? finally? to? the? entire? space
of? known? predicates.? If? needed,? we? can? also
make? parsing? more? detailed? by? parsing? against
dialogue?history? and? the?current?plan.?This?way,
the?information?produced?by?CM?is?used?in?input
parsing.? The? dialogue? plan? can? be? used? in? dy?
namic? construction? of? recognition? grammars? to
support?this?on?ASR?grammar?level.
Most? importantly,? all? this? is? possible? without
including? domain? specific? knowledge.? All? such
information? is? kept? exclusive? in? CM.? Similarly,
CM? does?not? need? to?know? the? interaction? level
properties? of? the? topics,? such? as? recognition
grammars? and? natural? language? generation? de?
tails.?These? are? internal? to? their? specific? compo?
nents.?The? mobile? system?uses? the? same? knowl?
edge? representation? as? CM,? but? CM? does? not
need?to?be?aware?of?its?existence?at?all.?Similarly,
the?mobile?system?can?use?any?part?of? the? infor?
mation? it? receives,? but? is? not? forced? to? do? any?
thing? specific.? DM? just? feed? all? the? information
to? it?and? lets? it? decide? what? to?do?with? it.?When
the? mobile? system? provides? information? back? to
the?home? system,?DM?handles? this? and?CM?can
ignore?completely? the?fact? that?different?parts?of
the? information? it? receives?were?generated?using
different? systems.? Similarly,? the? mobile? system
does?not?see?any?of?the?internals?of?the?home?sys?
tem.
On? an? implementation? level,? the? model? is? in?
dependent?of?the?mechanics?of?either?DM?or?CM.
DM? can? be? implemented? using? state? transition
networks?(a?network?per?plan?item),?forms?(form
per? item),?agent?based?model,? like? in? the?case?of
mobile? system,? or? any? other? suitable? method.
Similarly,? the? plan? does? not? tie? CM? to? any? spe?
cific?implementation.
4 Conclusions
When? dialogue? systems? move?beyond? limited
task? based? domains? and? implement? multimodal
interfaces? in? pervasive? computing? environment,
their? complexity? increases? rapidly.? Dialogue
management,? which? in? most? cases? is? handled
with?well?understood?methods?such?as? form?fill?
ing? or? state? transition? networks,? tends? to? grow
more?complex.?Therefore,?a?model?to?modularize
dialogue? management? and? domain? reasoning? is
needed.? At? the? same? time,? distributed? systems
required? various? kinds? of? information? to? be
communicated?with?components?and?systems.
While? traditional? spoken? dialogue? systems
have? been? task?based,? the? Health? and? Fitness
Companions?are?part?of?the?users??life?for?a?long
time,? months,? or? even? years.? This? requires? that
they? are? part? of? life? physically,? i.e.,? interactions
can? take? place? on? mobile? setting? and? in? home
environment? outside? of? traditional,? task?based
computing? devices.? With? the? physical? presence
of?the?interface?agent?and?spoken,?conversational
dialogue? we? aim? at? building? social,? emotional
relationships?between?the?users?and?the?Compan?
30
ion.?Such?relationships?should?help?us?in?motivat?
ing?the?users?towards?healthier?lifestyle.?The?mo?
bility? of? the? interface? integrates? the? system? into
the?physical?activities?they?aim?at?supporting?us?
ers?in.
We? have? presented? a? model,? which? separates
cognitive? modeling? from? dialogue? management
and?enables?flexible?interoperability?between? the
two?and?also?enables?sharing?the?gathered?knowl?
edge? to? the? mobile?part? of? the? system?and?back.
This? division,? while? similar? to? separation? of? a
back?end? from? dialogue? management,? draws? the
line? deeper? into? the? area? of? interaction? manage?
ment.? The? cognitive? model? processes? domain
level? information? and? generates? dialogue? plans.
The? dialogue? manager? focuses? only? on? interac?
tion?level?phenomena,?such?as?initiative?and?error
management,? and?other? meta?communication.? In
order?to?enable?flexible?interaction,?the?plan?pro?
vides? a? potential? structure? for? the? dialogue,? but
the?dialogue? manager? is? free? to?handle? things? in
different? order,? and? even? add? new? topics.? It? can
also?include?input?from?a?mobile?interface?of?the
system?without?making?this?explicit?to?the?cogni?
tive? model.? One? example? of? flexibility? is? error
management;?while? the?actual?error?correction? is
the? task? of? the? dialogue? manager,? domain? level
knowledge?can?reveal? errors.?Using?the?dialogue
plan,? the? cognitive? model? can? provide? such? in?
formation? to? the? dialogue? manager? without
knowledge? on? details?of? error? management.?The
model? also? enables? user? initiative? topic? shifts,
management? of? user?system? relationship? and
other? novel? issues? relevant? in? domain?oriented
dialogue?systems.
Overall,? the? model? presented? has? enabled? a
clear?division?and?interoperability?of?the?different
components?handling?separate?parts?of? the? inter?
action.? The? presented? model? has? been? imple?
mented? in? the? Health? and? Fitness? Companion
prototype,?and?it?has?enabled?the?cognitive?model,
the?dialogue?manager,?and?the?mobile?interface?to
be? developed? in? parallel? by? different? groups? us?
ing?various?programming?languages?an?integrated
system.
5 Acknowledgements
This? work? is? part? of? the? EU?funded? COM?
PANIONS?project? (IST?34434).? The? Cognitive
Model? has? been? developed? by? University? of
Teesside,? UK,? while? the? mobile? interface? has
been? implemented? in?Swedish? Institute? of? Com?
puter?Science.
References
Dybkjaer,?L.,?Bernsen,?N.?O.,?Minker,?W.,?Evaluation
and? usability? of? multimodal? spoken? language? dia?
logue? systems,? Speech? Communication,? 43,? 1?2,? ,
June?2004,?pp.?33?54.
Wilks,? Y.,? Is? There? Progress? on? Talking? Sensibly? to
Machines?,?Science,?9?Nov?2007.
Marti,?S.?and?Schmandt,?C.?Physical?embodiments?for
mobile? communication? agents.? Proceedings? of? the
18th? annual? ACM? symposium? on? User? interface
software?and?technology:?231???240,?2005.
Kainulainen,?A.,?Turunen,?M.,?Hakulinen,?J.,?Salonen,
E.?P.,?Prusi,?P.,?and?Helin,?L.?A?Speech?based?and
Auditory?Ubiquitous?Office?Environment.?Proceed?
ings? of? 10th? International? Conference? on? Speech
and?Computer?(SPECOM?2005):?231?234,?2005.
J?nsson,?A.?A?Natural?Language?Shell?and?Tools? for
Customizing?the?Dialogue?in?Natural?Language?In?
terfaces,? Research? Report,? LiTH?IDA?R?91?10,
1991.
Salonen,?E.?P.,? Hartikainen,?M.,?Turunen,? M.,? Haku?
linen? J.,? Funk,? J.? A.? Flexible? Dialogue? Manage?
ment? Using? Distributed? and? Dynamic? Dialogue
Control.??Proceedings?of?ICSLP?2004.?pp.?197?200.
O'Neill,? I.? Hanna,? P.? Liu,? X.,? McTear,? M.,? The
Queen's? Communicator:? An? Object?Oriented? Dia?
logue? Manager,? Eurospeech? 2003,? Geneva,? Swit?
zerland?(2003),?pp.?593?596.
Pellom,?B.?Ward,?W.?Pradhan,?S.,?The?CU?Communi?
cator:?An?Architecture? for?Dialogue?Systems,?Pro?
ceedings?of?ICSLP?2000,?Beijing?China,?November
2000.
Turunen,?M.,?Hakulinen,?J.,?R?ih?,?K.?J.,?Salonen,?E.?
P.,? Kainulainen,? A.,? and? Prusi,? P.? An? architecture
and?applications?for?speech?based?accessibility?sys?
tems.? IBM?Systems? Journal,? Vol.? 44,?No? 3,?2005,
pp.?485?504.
Cavazza,?M.,?Smith,?C.,?Charlton,?D.,?Zhang,?L.,?Tu?
runen,?M.?and?Hakulinen,?J.,?A??Companion??ECA
with?Planning?and?Activity?Modelling,?Proceedings
of?AAMAS08,?2008?(to?appear).
Larsson,? S.? Ljungl?f,? P.? Cooper,? R.? Engdahl,? E.,
Ericsson.? S.? GoDiS? ?? an? accommodating? dialogue
system.? ANLP? /? NAACL? '00? Workshop? on? Con?
versational?Systems,?May?2000.
Jullien? C.,? Marty,? J.?C.? Plan? revision? in? person?
machine? dialogue,? Proceedings? of? ACL?89,? Man?
chester,?England,?April?1989,?pp.153?160.
Bickmore,? T.? W.,? Picard,? R.? W.? Establishing? and
maintaining? long?term? human?computer? relation?
ships.? ACM? Trans.? Computer?Human.? Interaction
Vol.?12,?No.?2.?(June?2005),?pp.?293?327.
31
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 47?50,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
?How was your day?? An architecture for multimodal ECA systems 
 
Ra?l Santos de la 
C?mara 
Telef?nica I+D 
C/ Emilio Vargas 6 
28043 Madrid, Spain 
e.rsai@tid.es 
Markku Turunen 
Univ. of Tampere 
Kanslerinrinne 1 
FI-33014, Finland 
mturunen@ 
cs.uta.fi 
Jaakko Hakulinen 
Univ. of Tampere 
Kanslerinrinne 1 
FI-33014, Finland 
jh@cs.uta.fi 
Debora Field 
Computer Science 
Univ. of  Sheffield 
S1 4DP, UK 
d.field@shef. 
ac.uk 
 
Abstract 
Multimodal conversational dialogue sys-
tems consisting of numerous software 
components create challenges for the un-
derlying software architecture and devel-
opment practices. Typically, such sys-
tems are built on separate, often pre-
existing components developed by dif-
ferent organizations and integrated in a 
highly iterative way. The traditional dia-
logue system pipeline is not flexible 
enough to address the needs of highly in-
teractive systems, which include parallel 
processing of multimodal input and out-
put. We present an architectural solution 
for a multimodal conversational social 
dialogue system. 
1 Introduction 
Multimodal conversational dialogue applica-
tions with embodied conversational agents 
(ECas) are complex software systems consisting 
of multiple software components. They require 
much of architectural solutions and development 
approaches compared to traditional spoken dia-
logue systems. These systems are mostly assem-
bled from separate, often pre-existing compo-
nents developed by different organizations. For 
such systems, the simple pipeline architecture is 
not a viable choice. When multimodal systems 
are built, software architecture should be flexible 
enough to enable the system to support natural 
interaction with features such as continuous and 
timely multimodal feedback and interruptions by 
both participants. Such features require parallel 
processing components and flexible communica-
tion between the components. Furthermore, the 
architecture should provide an open sandbox, 
where the components can be efficiently com-
bined and experimented with during the iterative 
development process. 
The HWYD (?How was your day??) Compan-
ion system is a multimodal virtual companion 
capable of affective social dialogue and for 
which we have developed a custom novel archi-
tecture. The application features an ECA which 
exhibits facial expressions and bodily move-
ments and gestures. The system is rendered on a 
HD screen with the avatar being presented as 
roughly life-size. The user converses with the 
ECA using a wireless microphone. A demonstra-
tion video of the virtual companion in action is 
available online1. 
The application is capable of long social con-
versations about events that take place during a 
user?s working day. The system monitors the 
user?s emotional state on acoustic and linguistic 
levels, generates affective spoken responses, and 
attempts to positively influence the user?s emo-
tional state. The system allows for user initiative, 
it asks questions, makes comments and sugges-
tions, gives warnings, and offers advice. 
2 Communications framework 
The HWYD Companion system architecture em-
ploys Inamode, a loosely coupled multi-hub 
framework which facilitates a loose, non-
hierarchical connection between any number of 
components. Every component in the system is 
connected to a repeating hub which broadcasts 
all messages sent to it to all connected compo-
nents. The hub and the components connected to 
it form a single domain. Facilitators are used to 
forward messages between different domains 
according to filtering rules. During development, 
we have experimented with a number of Facilita-
tors to create efficient and simple domains to 
overcome problems associated with single-hub 
systems. For example, multiple hubs allow the 
                                                
1
 http://www.youtube.com/ 
watch?v=BmDMNguQUmM 
47
reduction of broadcast messages, which is for 
example used in the audio processing pipeline, 
where a dedicated hub allows very rapid message 
broadcast (nearly 100 messages per second are 
exchanged) without compromising the stability 
of the system by flooding the common pipeline. 
For communication between components, a 
lightweight communication protocol is used to 
support components implemented in various 
programming languages. A common XML mes-
sage ?envelope? specifies the basic format of 
message headers as seen in Figure 1. 
 
Figure 1: System message XML format 
. 
Mandatory elements in the envelope (top 
block) are necessary so other modules can iden-
tify the purpose of the message and its contents 
upon a shallow inspection. These include the 
sender component and a unique message id. Ad-
ditional envelope fields elements include: mes-
sage type, turn id, dialogue segment identifier, 
recipient identifier, and a list of message identi-
fiers corresponding to the previous messages in 
the current processing sequence.  
For system-wide and persistent knowledge 
management, a central XML-database allows the 
system to have inter-session and intra-session 
?memory? of past events and dialogues. This da-
tabase (KB) includes information such the user 
and dialogue models, processing status of mod-
ules, and other system-wide information. 
3 Data flow in the architecture 
To maximize the naturalness of the ECA?s inter-
action, the system implements parallel process-
ing paths. It also makes use of a special module, 
the Interruption Manager (IM), to control 
components in situations where regular process-
ing procedure must be deviated from. In addi-
tion, there are ?long? and ?short? processing  se-
quences from user input to system output. Both 
?loops? operate simultaneously. The Main Dia-
logue (?long?) Loop, which is the normal proc-
essing path, is indicated by the bold arrows in 
Fig. 2, and includes all system components ex-
cept the IM. The dotted arrows signal the devia-
tions to this main path that are introduced by the  
Natural Language 
Understanding (NLU)
Acoustic
Analyzer (AA)
Autom
atic Speech
Recognition (ASR)
Acoustic Em
otion Classifier (AEC)
Sentiment
Analyzer (SA)
Dialogue Act 
Tagger (DAT)
Dialogue 
Manager(DM)
Affective Strategy 
(ASM
)
Multimodal Fission 
Manager (MFM)
Text-to-Speech
(TTS)
Avatar
(ECA)
Acoustic Turn-
Taking (ATT)
Interruption 
M
anager(IM
)
Emotional 
Model (EM)
Natural Language 
Generation (NLG)
Knowledge 
Base & UM
 
Figure 2:HWYD Companion main modules 
 
interruption management and feedback loops.  
The system has an activity detector in the input 
subsystem that is active permanently and analy-
ses user input in real-time. If there is a detection 
of user input at the same time as the ECA is talk-
ing, this module triggers a signal that is captured 
by the IM. The IM, which tracks the activity of 
the rest of the modules in the system, has a set of 
heuristics that are examined each time this trig-
gering signal is detected. If any heuristic 
matches, the system decides there has been a 
proper user interruption and decides upon a se-
ries of actions to recover from the interruption. 
4 Module Processing Procedure 
The first stage in the processing is the acoustic 
processing. User speech is processed by the 
Acoustic Analyzer, the Automatic Speech Rec-
ognizer, and the Acoustic Emotion Classifier 
simultaneously for maximum responsiveness. 
The Acoustic Analyzer (AA) extracts low-
level features (pitch, intensity and the probability 
that the input was from voiced speech) from the 
acoustic signal at frequent time intervals (typi-
cally 10 milliseconds). Features are passed to the 
Acoustic Turn-Taking Detector in larger buffers 
(a few hundred milliseconds) together with time-
stamps. AA is implemented in TCL using Snack 
toolkit (http://www.speech.kth.se/snack/). 
The Acoustic Turn-Taking detector (ATT) 
is a Java module, which estimates when the user 
has finished a turn by comparing intensity pause 
lengths and pitch information of user speech to 
configurable empirical thresholds. ATT also de-
cides whether the user has interrupted the system 
48
(?barge-in?), while ignoring shorter backchannel-
ling phrases (Crook et al (2010)). Interruption 
messages are passed to the Interruption Manager. 
ATT receives a message from the ECA module 
when the system starts or stops speaking. 
Dragon Naturally Speaking Automatic 
Speech Recognition (ASR) system is used to 
provide real-time large vocabulary speech recog-
nition. Per-user acoustic adaptation is used to 
improve recognition rates. ASR provides N-best 
lists, confidence scores, and phrase hypotheses. 
The Acoustic Emotion Classifier (AEC) 
component (EmoVoice (Vogt et al (2008)) cate-
gorizes segments of user speech into five va-
lence+arousal categories, also applying a confi-
dence score. The Interruption Manager monitors 
the messages of the AEC to include emotion-
related information into feedback loop messages 
sent to the ECA subsystem. This allows rapid 
reactions to the user mood. 
The Sentiment Analyzer (SA) labels ASR 
output strings with sentiment information at 
word and sentence levels using valence catego-
ries positive, neutral and negative. The SA uses 
the AFFECTiS Sentiment Server, which is a gen-
eral purpose .NET SOAP XML service for 
analysis and scoring of author sentiment. 
The Emotional Model (EM), written in Lisp, 
fuses information from the AEC and SA. It 
stores a globally accessible emotional representa-
tion of the user for other system modules to 
make use of. Affective fusion is rule-based, pre-
fers the SA?s valence information, and outputs 
the same five valence+arousal categories as used 
in the AEC. The EM can also serve as a basis for 
temporal integration (mood representation) as 
part of the affective content of the User Model. It 
also combines the potentially different segmenta-
tions by the ASR and AEC. 
The User Model (UM) stores facts about the 
user as objects and associated attributes. The in-
formation contained in the User Model is used by 
other system modules, in particular by Dialogue 
Manager and Affective Strategy Module. 
The Dialogue Act Tagger and Segmenter 
(DAT), written in C under Linux, uses the ATT 
results to compile all ASR results corresponding 
to each user turn. DAT then segments the com-
bined results into semantic units and labels each 
with a dialogue act (DA) tag (from a subset of 
SWBD-DAMSL (Jurafsky et al (2001)). A Sto-
chastic Machine Learning model combining 
Hidden Markov Model (HMM) and N-grams is 
used in a manner analogous to Mart?nez-
Hinarejos et al (2006). The N-grams yield the 
probability of a possible DA tag given the previ-
ous ones. The Viterbi algorithm is used to find 
the most likely sequence of DA tags.  
The Natural Language Understanding 
(NLU) component, implemented in Prolog, pro-
duces a logical form representing the semantic 
meaning of a user turn. The NLU consists of a 
part-of-speech tagger, a Noun Phrase and Verb 
Group chunker, a named-entity classification 
component (rule-based), and a set of pattern-
matching rules which recognize major gram-
matical relationships (subject, direct object, etc.) 
The resulting shallow-parsed text is further proc-
essed using pattern-matching rules. These recog-
nize configurations of entity and relation relevant 
to the templates needed by the Dialogue Man-
ager, the EM, and the Affective Strategy Module. 
The Dialogue Manager (DM), written in Java 
and Prolog, combines the SA and NLU results, 
decides on the system's next utterance and identi-
fies salient objects for the Affective Strategy 
Module. The DM maintains an information state 
containing information about concepts under dis-
cussion, as well as the system's agenda of current 
conversational goals.  
One of the main features of the HWYD Com-
panion is its ability to positively influence the 
user?s mood through its Affective Strategy 
Module (ASM). This module appraises the 
user?s situation, considering the events reported 
in the user turn and its (bi-modal) affective ele-
ments. From this appraisal, the ASM generates a 
long multi-utterance turn. Each utterance imple-
ments communicative acts constitutive of the 
strategy. ASM generates influence operators 
which are passed to the Natural Language Gen-
eration module. ASM output is triggered when 
the system has learned enough about a particular 
event to warrant affective influence. As input, 
ASM takes information extraction templates de-
scribing events, together with the emotional data 
attached. ASM is a Hierarchical Task Network 
(HTN) Planner implemented in Lisp.  
The Natural Language Generator (NLG), 
written in Lisp, produces linguistic surface forms 
from influence operators produced by the ASM. 
These operators correspond to communicative 
actions taking the form of performatives. NLG 
uses specific rhetorical structures and constructs 
associated with humour, and uses emotional TTS 
expressions through specific lexical choice.  
49
5 Multimodal ECA Control 
Multimodal control of the ECA, which consists 
of a tightly-synchronized naturalistic avatar and 
affective Text-To-Speech (TTS) generation, is 
highly challenging from an architectural view-
point, since the coordinating component needs to 
be properly synchronized with the rest of the sys-
tem, including both the main dialogue loop and 
the feedback and interruption loops. 
The system Avatar is in charge of generating a 
three-dimensional, human-like character to serve 
as the system?s ?face?. The avatar is connected to 
the TTS, and the speech is synchronized with the 
lip movements. The prototype is currently using 
the HaptekTM 3D avatar engine running inside a 
web browser. The Haptek engine provides a talk-
ing head and torso along with a low level API to 
control its interaction with any SAPI-compliant 
TTS subsystem, and also allows some manipula-
tion of the character animation. An intermediate 
layer consisting of a Java applet and Javascript 
code embeds the rendered avatar in a web page 
and provides connectivity with the Multimodal 
Fission Manager. We intend to replace the cur-
rent avatar with a photorealistic avatar under de-
velopment within the project consortium. 
LoquendoTM TTS SAPI synthesizer is used to 
vocalize system turns. The TTS engine works in 
close connection with the ECA software using 
the SAPI interface. TTS includes custom para-
linguistic events for producing expressive 
speech. TTS is based on the concatenative tech-
nique with variable length acoustic units. 
The Multimodal Fission Manager (MFM) con-
trols the Avatar and the TTS engine, enabling the 
system to construct complex communicative acts 
that chain together series of utterances and ges-
tures. It offers FML-standard-based syntax to 
make the avatar perform a series of body and 
facial gestures. 
The system features a template-based input 
mode in which a module can call ECA to per-
form actions without having to build a full FML-
based XML message. This is intended to be used 
in the feedback loops, for example, to convey the 
impression that the ECA is paying attention.  
6 Conclusions 
We have presented an advanced multimodal dia-
logue system that challenges the usual pipeline-
based implementation. To do so, it leverages on 
an architecture that provides the means for a 
flexible component interconnection, that can ac-
comodate the needs of a system using more than 
one processing path for its data. We have shown 
how this has enabled us to implement complex 
behavior such as interrupt and short loop han-
dling. We are currently expanding coverage and 
will carry out an evaluation with real users this 
September. 
Acknowledgements 
This work was funded by Companions, a Euro-
pean Commission Sixth Framework Programme 
Information Society Technologies Integrated 
Project (IST-34434). 
References 
Vogt, T., Andr?, E. and Bee, N. 2008. EmoVoice ? A 
framework for online recognition of emotions from 
voice. In: Proc. Workshop on Perception and In-
teractive Technologies for Speech-Based Systems, 
Springer, Kloster Irsee, Germany. 
Cavazza, M., Smith, C., Charlton, D., Crook, N., 
Boye, J., Pulman, S., Moilanen, K., Pizzi, D., San-
tos de la Camara, R., Turunen, M. 2010 Persuasive 
Dialogue based on a Narrative Theory: an ECA 
Implementation, Proc. 5th Int. Conf. on Persuasive 
Technology (to appear). 
Hern?ndez, A., L?pez, B., Pardo, D., Santos, R., 
Hern?ndez, L., Rela?o Gil, J. and Rodr?guez, M.C. 
2008 Modular definition of multimodal ECA 
communication acts to improve dialogue robust-
ness and depth of intention. In: Heylen, D., Kopp, 
S., Marsella, S., Pelachaud, C., and Vilhj?lmsson, 
H. (Eds.), AAMAS 2008 Workshop on Functional 
Markup Language. 
Crook, N., Smith, C., Cavazza, M., Pulman, S., 
Moore, R., and Boye, J. 2010 Handling User Inter-
ruptions in an Embodied Conversational Agent.  In 
Proc. AAMAS 2010. 
Wagner J., Andr?, E., and Jung, F. 2009 Smart sensor 
integration: A framework for multimodal emotion 
recognition in real-time. In Affective Computing 
and Intelligent Interaction 2009. 
Cavazza, M., Pizzi, D., Charles, F., Vogt, T. Andr?, 
E. 2009 Emotional input for character?based in-
teractive storytelling AAMAS (1) 2009: 313-320. 
Jurafsky, D. Shriberg, E., Biasca, D. 2001 
Switchboard swbd?damsl shallow? discourse?
function annotation coders manual. Tech. Rep. 97
?01, University of Colorado Institute of Cognitive 
Science 
Mart?nez?Hinarejos, C.D., Granell, R., Bened?, J.M. 
2006. Segmented and unsegmented dialogue?act 
annotation with statistical dialogue models. Proc. 
COLING/ACL Sydney, Australia, pp. 563?570. 
50
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 277?280,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
?How was your day?? An affective companion ECA prototype 
 
Marc Cavazza 
School of Computing 
Teesside University 
Middlesbrough TS1 3BA 
m.o.cavazza@tees.ac.uk 
Ra?l Santos de la C?mara 
Telef?nica I+D 
C/ Emilio Vargas 6 
28043 Madrid 
e.rsai@tid.es 
Markku Turunen 
University of Tampere 
Kanslerinrinne 1 
FI-33014 
mturunen@cs.uta.fi 
 
 
Jos? Rela?o Gil 
Telef?nica I+D 
C/ Emilio Vargas 6 
28043 Madrid 
joserg@tid.es 
Jaakko Hakulinen 
University of Tampere 
Kanslerinrinne 1 
FI-33014 
jh@cs.uta.fi 
Nigel Crook 
Oxford University 
Computing Laboratory 
Oxford OX1 3QD 
nigc@comlab.ox.
ac.uk 
Debora Field 
Computer Science 
Sheffield University 
Sheffield S1 4DP 
d.field@shef. 
ac.uk 
 
Abstract 
This paper presents a dialogue system in 
the form of an ECA that acts as a socia-
ble and emotionally intelligent compan-
ion for the user. The system dialogue is 
not task-driven but is social conversation 
in which the user talks about his/her day 
at the office. During conversations the 
system monitors the emotional state of 
the user and uses that information to in-
form its dialogue turns. The system is 
able to respond to spoken interruptions 
by the user, for example, the user can in-
terrupt to correct the system. The system 
is already fully implemented and aspects 
of actual output will be used to illustrate. 
1 Introduction 
Historically, Embodied Conversational Agents 
(ECAs) have been used in research and industry 
make information and complex tasks more ac-
cessible to customers and users. With the rise of 
new technologies in affective dialogue systems, 
we are beginning to see a future in which ECA 
dialogues are not all task-driven, but some will 
be focused on the social aspects of conversation. 
We envisage the development of ECAs that en-
hance the social quality of life of the user, and 
that build deep relationships with their users over 
years of use. Our affective companion ECA is 
our first contribution to this emerging technol-
ogy.  
Our emotionally intelligent dialogue system 
requires a high level of understanding of the 
meaning of the user turns, and so is quite unlike 
a ?chatbot?. Affective aspects of the situation are 
also key in influencing the course of the conver-
sation. The system attempts to balance both se-
mantic and affective understanding in order to 
create an engaging and enjoyable companion for 
social conversation. 
2 Scenario description 
The system conversation is free-ranging and fol-
lows the user?s initiative, rather than being task-
driven. We have for the time being restricted the 
range of conversation topics to things that hap-
pen in the life of someone who works in an of-
fice (the ?How was your day?? (HWYD) sce-
nario). The user ?offloads? about his/her day at 
work by chatting to the ECA, and the ECA tries 
to empathise with and advise the user. The ECA 
asks questions to elicit information from the user, 
makes empathetic and sympathetic comments, 
and offers advice.  
The system is able to process long user turns, 
as well as to generate long system turns (around 
50 words). If the user perhaps dislikes what 
he/she hears, he/she can interrupt the system by 
barging in, and the ECA will respond in a hu-
man-like way. The conversation style is therefore 
considerably different from traditional task-based 
dialogues that consist of short dialogue turns and 
in which the system takes all the initiative.  
The dialogue in Table 1 (actual output from a 
user session) illustrates some of the significant 
features of the system, including long system 
277
turns, user interruptions and emotional analysis 
of user input.  
 
User1 Worst day ever. I arrived late be-
cause of the traffic. 
System1 You have my sympathy. What 
happened next? 
User2 I missed the meeting on project 
responsibilities; got more work 
dumped on me. Just what I don?t 
need. I?m already likely to miss 
the deadline. [ECA looks worried 
when user?s negative tone is de-
tected] 
System2 Sorry to hear that. Who was at the 
meeting? 
User3 Only my team. They gave me a 
set of PowerPoint slides from the 
meeting. I spent the rest of the day 
reading them but my computer 
kept crashing because of  updates. 
System3 Oh dear! It is understandable how 
you must be feeling. It?s right to 
be unhappy about such a bad situ-
ation. Try not to worry about 
missing the deadline because of 
the technical problems. [At this 
point user interrupts ECA by 
beginning turn User4] It?s sad that 
your motivation could be affected 
by them. [System stops speaking] 
User4 No, I?m not worried about the 
technical problems, I?m worried 
about the amount of extra work on 
this project and the deadline. 
System4 What is the project about? 
 
Table 1: Dialogue example 
 
A video demonstration of the system in action 
is available at: 
http://www.youtube.com/watch?v=
BmDMNguQUmM 
3 Architecture 
Figure 1 shows a screen shot taken at run-time of 
actual system output. The ECA is represented on 
a screen as a woman (waist up) who displays 
natural, human-like movements and performs a 
wide range of complex facial expressions, bodily 
movements, and hand and arm gestures. 
The screen also displays a transcript of the 
user and system turns. The user turns shown con-
stitute the output of the Automatic Speech Rec-
ogniser (ASR). The system?s analysis of the 
user?s emotional state is also shown. 
The right-most panel of the screen shows 
graphics which convey real-time information 
about how the dialogue is being processed. It 
presents a streamlined view of the software 
modules that comprise the system. Module activ-
ity is visually represented at run-time by flashing 
colours. This ?glass-box? approach enables de-
tailed observation and analysis of system 
procedure at run-time. 
The system comprises a number of distinct 
modules that are connected using Inamode, a 
hub-based message-passing framework using 
XML formatted messages over plain text sock-
ets. 
The system?s ASR is the NuanceTM dictation 
engine. This is run in parallel with our own a-
coustic analysis pipeline which extracts low level 
(pitch, tone) speech features and also high-level 
features such as emotional characteristics. 
Analysis of the emotions is currently carried out 
 
Figure 1: Screenshot of the prototype interface 
 
278
by EmoVoice (Vogt et al (2008)). The ASR 
output strings are analysed for sentiment by the 
AFFECTiS system (Moilanen and Pulman (2007, 
2009)) and classed as positive, neutral, or nega-
tive. This output is fused with the output from 
EmoVoice to generate a value that represents the 
user?s current emotional state, which is ex-
pressed as a valence+arousal pairing (with five 
possible values). 
The ASR output goes to our own Natural Lan-
guage Understanding (NLU) module which per-
forms syntactic and semantic analysis of user 
utterances and derives noun phrases and verb 
groups and associated arguments. Events rele-
vant to the scenario (e.g., promotions, redundan-
cies, meetings, arguments, etc.) are recognised 
by the NLU and are used to populate an ontology 
(a model of the conversation content).  The sys-
tem is currently able to recognize and respond to 
more than 30 event types.  
The events recognised in a user turn are 
labelled with the output of the Emotion Module 
for that turn; the result is a representation of both 
the semantic and affective information that the 
user might be trying to convey. 
Our own rule-based Dialogue Manager (DM) 
takes the affect-annotated semantic output of the 
NLU, and from that and its model of the conver-
sation content determines the next system turn. It 
will either ask a question about the events that 
occurred in the user?s day, express an opinion on 
the events already described, or make empathetic 
comments. Whenever the system has gained suf-
ficient understanding of a key event in the user?s 
day, it generates a complex long turn that encap-
sulates comfort, opinion, warnings and advice to 
the user. 
These long system turns are generated by our 
own plan-based Affective Strategy Module that 
makes an appraisal of the user?s situation and  
generates an appropriate emotional strategy 
(Cavazza et al (2010)). This strategy?expressed 
as an abstract, conceptual representation?is han-
ded to our own Natural Language Generator 
(NLG) that maps it into a series of linguistic sur-
face forms (usually 4 or 5 sentences). We use a 
style-controllable system using Tree-Furcating 
Grammars (an extension of the Tree-Adjoining 
Grammars formalism (Joshi et al (1997)). This 
ensures the generation of a large set of different 
surface forms from the same semantic input. 
The output of the NLG is passed to a module 
that adds this information to its system turn 
instructions for the ECA. The ECA has been de-
veloped around the HaptekTM toolkit and is con-
trolled using an FML-like language (after 
Hern?ndez et al (2008)). This 2-D embodiment 
produces gestures, facial expressions, and body 
movements that convey the emotional state of 
the ECA. Its movements and expressions enable 
it to visually display interest and enjoyment in 
talking to the user, and to display empathy with 
the user. The speech synthesis module is our own 
emotion-focused extension of the LoquendoTM 
TTS system. It includes paralinguistic elements 
such as exclamations and laughter, and emo-
tional prosody generation for negative and posi-
tive utterances. 
4 Special procedural features 
A significant processing design feature of the 
system is that there are two main processing 
loops from user input to system output; a ?long 
loop? which passes through all the components 
of the system; and a ?short loop? or ?feedback 
loop? which will now be discussed (the proce-
dure already described in Section 3 is the long 
loop procedure). 
4.1 Feedback loop 
The feedback loop (?short loop?) bypasses many 
linguistic components and generates immediate 
reactions to user activity. The main function of 
the short loop is maintain user engagement by 
preventing unnaturally long gaps of ECA inactiv-
ity. The feedback loop engages the acoustic 
analysis components, the TTS, and the ECA. It is 
responsible for the generation of real-time (< 500 
ms) reactions in the ECA in response to the emo-
tional state of the user. It attempts to align  both 
verbal behaviour (backchannelling) and non-
verbal behaviour (facial expressions, gestures, 
and general body language) to the emotions de-
tected during most recent user turn. In order to 
achieve a reasonable level of realism, these sys-
tem reactions to the perceived emotional state of 
the user need to be perceptibly instantaneous. 
Using this short feedback loop that bypasses 
many of the linguistic components ensures this. 
The feedback loop is also occasionally used to 
make sympathetic comments immediately after 
the user stops speaking. These act as acknowl-
edgements of the emotion expressed by the user. 
An example can be seen in the System2 turn of 
the example dialogue in Table 1: 
1.?Sorry to hear that. Who was at the meeting?? 
Here, the first utterance was spoken by the sys-
tem within a few tenths of a second after the end 
279
of the previous user turn (User2). The system 
tried to identify the user?s emotion in the previ-
ous turn and then to behave linguistically and 
visually in an empathetic way. The actual sympa-
thetic utterance was randomly chosen from a set 
of ?negative emotion utterances? (there are also 
?positive? and ?neutral? sets).  
The second half of the system turn in (1) was 
derived by the system?s ?long loop?. It is a ques-
tion which refers to a meeting that the user men-
tioned in the previous turn. This ?meeting? event 
has been heard by the ASR, understood by the 
NLU system, remembered by the DM, and is 
now referred to by an appropriate definite noun 
phrase in the output of the NLG.   
The feedback and main loops run in parallel. 
However, the feedback loop generates its speech 
output almost immediately, giving time for the 
main dialogue loop to complete its more detailed 
analysis of the user?s utterance.  
4.2 Handling user interruptions 
This system has a complex strategy for handling 
situations in which the user interrupts long 
system turns.  The system?s response to ?barge-
in? user interruptions is overseen by the Interrup-
tion Manager (IM), which is alerted by the 
acoustic input modules whenever a genuine user 
interruption (as opposed to, say, a backchannel) 
is detected during a long system utterance. When 
alerted, the IM instructs the ECA to stop speak-
ing when it reaches a natural stopping point in its 
current turn (usually the end of the current 
phrase). The user?s interruption utterance is 
processed by the long loop. Its progress is 
tracked and controlled by the IM, for example, it 
makes sure that the linguistic modules know that 
the current utterance is an interruption, whic 
means it requires special treatment. The DM has 
a range of strategies for system recoveries from 
user interruptions, including different ways of 
continuing, replanning, and aborting. An exam-
ple of a user interruption is shown in Table 1. 
The user interrupts the long system utterance in 
the System3 turn. The system?s response to the 
interruption is to stop the speech output from the 
ECA, abort the long system turn altogether, and 
instead to ask for more details about the project 
that the user has just mentioned during the inter-
ruption. (See (Crook et al (2010))  for a more 
detailed description of the IM.) 
 
 
Acknowledgements 
This work was funded by Companions, a Eu-
ropean Commission Sixth Framework Pro-
gramme Information Society Technologies Inte-
grated Project (IST-34434).  
We would also like to thank the following 
people for their valuable contributions to the 
work presented here: Stephen Pulman, Ramon 
Granell, and Simon Dobnick (Oxford Univer-
sity), Johan Boye (KTH Stockholm), Cameron 
Smith and Daniel Charlton (Teesside Univer-
sity), Roger Moore, WeiWei Cheng and Lei Ye 
(University of Sheffield), Morena Danieli and 
Enrico Zovato (Loquendo). 
References 
Cavazza, M., Smith, C., Charlton, D., Crook, N., 
Boye, J., Pulman, S., Moilanen, K., Pizzi, D., San-
tos de la Camara, R., Turunen, M. 2010 Persuasive 
Dialogue based on a Narrative Theory: an ECA 
Implementation, Proc. of the 5th Int. Conf. on Per-
suasive Technology (Persuasive 2010), to appear 
2010. 
Crook, N., Smith, C., Cavazza, M., Pulman, S., 
Moore, R., and Boye, J. 2010 Handling User Inter-
ruptions in an Embodied Conversational Agent In 
proc. of AAMAS 2010. 
Hern?ndez, A., L?pez, B., Pardo, D., Santos, R., 
Hern?ndez, L., Rela?o Gil, J. and Rodr?guez, M.C. 
(2008) Modular definition of multimodal ECA 
communication acts to improve dialogue robust-
ness and depth of intention. In: Heylen, D., Kopp, 
S., Marsella, S., Pelachaud, C., and Vilhj?lmsson, 
H. (Eds.), AAMAS 2008 Workshop on Functional 
Markup Language.  
Joshi, A.K. & Schabes, Y. (1997) Tree-adjoining 
Grammars. Handbook of formal languages, vol. 3: 
Beyond Words, Springer-Verlag New York, Inc., 
New York, NY, 1997. 
Moilanen, K. and Pulman. S. (2009). Multi-entity 
Sentiment Scoring. Proc. Recent Advances in 
Natural Language Processing (RANLP 2009). 
September 14-16, Borovets, Bulgaria. pp. 258--
263.  
Moilanen, K. and Pulman. S. (2007). Sentiment Com-
position. Proc. Recent Advances in Natural Lan-
guage Processing (RANLP 2007). September 27-
29, Borovets, Bulgaria. pp. 378--382. 
Vogt, T., Andr?, E. and Bee, N. 2008. EmoVoice ? A 
framework for online recognition of emotions 
from voice. Proc. Workshop on Perception and 
Interactive Technologies for Speech-Based Sys-
tems, Springer, Kloster Irsee, Germany, (June 
2008). 
280
