Generating Training Data for Medical Dictations 
 
Sergey Pakhomov 
University of Minnesota, MN 
pakhomov.sergey@mayo.edu 
Michael Schonwetter 
Linguistech Consortium, NJ 
MSchonwetter@qwest.net 
Joan Bachenko 
Linguistech Consortium,NJ 
bachenko@mnic.net 
 
   
 
Abstract 
 
In automatic speech recognition (ASR) enabled 
applications for medical dictations, corpora of 
literal transcriptions of speech are critical for 
training both speaker independent and speaker 
adapted acoustic models.  Obtaining these 
transcriptions is both costly and time consuming.  
Non-literal transcriptions, on the other hand, are 
easy to obtain because they are generated in the 
normal course of a medical transcription operation.  
This paper presents a method of automatically 
generating texts that can take the place of literal 
transcriptions for training acoustic and language 
models.  ATRS1 is an automatic transcription 
reconstruction system that can produce near-literal 
transcriptions with almost no human labor. We will 
show that (i) adapted acoustic models trained on 
ATRS data perform as well as or better than 
adapted acoustic models trained on literal 
transcriptions (as measured by recognition 
accuracy) and (ii) language models trained on 
ATRS data have lower perplexity than language 
models trained on non-literal data. 
 
Introduction 
 
Dictation applications of automatic speech 
recognition (ASR) require literal transcriptions of 
speech in order to train both speaker independent 
and speaker adapted acoustic models.  Literal 
transcriptions may also be used to train stochastic 
language models that need to perform well on 
spontaneous or disfluent speech.  With the 
exception of personal desktop systems, however, 
obtaining these transcriptions is costly and time 
consuming since they must be produced manually 
                                                        
1
 patent pending (Serial No.: 09/487398) 
by humans educated for the task.  The high cost 
makes literal transcription unworkable for ASR 
applications that require adapted acoustic models 
for thousands of talkers as well as accurate 
language models for idiosyncratic natural speech.  
 
Non-literal transcriptions, on the other hand, are 
easy to obtain because they are generated in the 
normal course of a medical transcription operation.  
It has been previously shown by Wightman and 
Harder (1999) that the non-literal transcriptions can 
be successfully used in acoustic adaptation. 
However, non-literal transcriptions are incomplete.  
They exclude many utterances that commonly 
occur in medical dictation?filled pauses, 
repetitions, repairs, ungrammatical phrases, 
pleasantries, asides to the transcriptionist, etc.  
Depending on the talker, such material may 
constitute a significant portion of the dictation. 
 
We present a method of automatically generating 
texts that can take the place of literal transcriptions 
for training acoustic and language models.  ATRS 
is an automatic transcription reconstruction system 
that can produce near-literal transcriptions with 
almost no human labor.  
 
The following sections will describe ATRS and 
present experimental results from language and 
acoustic modeling. We will show that (i) adapted 
acoustic models trained on ATRS data perform as 
well as or better than adapted acoustic models 
trained on literal transcriptions (as measured by 
recognition accuracy) and (ii) language models 
trained on ATRS data have lower perplexity than 
language models trained on non-literal data. Data 
used in the experiments comes from medical 
dictations.  All of the dictations are telephone 
speech. 
 
1  Dictation Applications of ASR 
 
The application for our work is medical dictation 
over the telephone.  Medical dictation differs from 
other telephony based ASR applications, e.g. airline 
reservation systems, because the talkers are repeat 
users and utterances are long.  Dictations usually 
consist of 1-30 minutes of speech.  The talkers call 
in 3-5 days per week and produce between 1 and 12 
dictations each day they call.  Hence a medical 
dictation operation has access to hours of speech 
for each talker.   
 
Spontaneous telephone speech presents additional 
challenges that are caused partly by a poor acoustic 
signal and partly by the disfluent nature of 
spontaneous speech. A number of researchers have 
noted the effects of disfluencies on speech 
recognition and have suggested various approaches 
to dealing with them at language modeling and 
post-processing stages. (Shriberg 1994, Shriberg 
1996, Stolcke and Shriberg 1996, Stolcke et al 
1998, Shriberg and Stolcke 1996, Siu and 
Ostendorf 1996, Heeman et al 1996) Medical over-
the-telephone dictations can be classified as 
spontaneous or quasi-spontaneous discourse 
(Pakhomov 1999, Pakhomov and Savova 1999). 
Most physicians do not read a script prepared in 
advance, instead, they engage in spontaneous 
monologues that display the full spectrum of 
disfluencies found in conversational dialogs in 
addition to other "disfluencies" characteristic of 
dictated speech. An example of the latter is when a 
physician gives instructions to the transcriptionist 
to modify something in the preceding discourse, 
sometimes as far as several paragraphs back. 
 
Most ASR dictation applications focus on desktop 
users; for example, Dragon, IBM, Philips and 
Lernout & Hauspie all sell desktop dictation 
recognizers that work on high quality microphone 
speech.  Typically, the desktop system builds an 
adapted acoustic model if the talker "enrolls", i.e. 
reads a prepared script that serves as a literal 
transcription.  Forced alignment of the script and 
the speech provides the input to acoustic model 
adaptation.   
 
Enrollment makes it relatively easy to obtain literal 
transcriptions for adaptation.  However, enrollment 
is not feasible for dictation over the telephone 
primarily because most physicians will refuse to 
take the time to enroll.  The alternative is to hire 
humans who will type literal transcriptions of 
dictation until enough have been accumulated to 
build an adapted model, an impractical solution for 
a large scale operation that processes speech from 
thousands of talkers.  ATRS is appealing because it 
can generate an approximation of literal 
transcription that can replace enrollment scripts and 
the need for manually generated literal 
transcriptions. 
 
2 Three Classes of Training Data 
 
In this paper, training texts for language and 
acoustic models fall into three categories: 
 
Non-Literal:
 Non-literal transcripts present the 
meaning of what was spoken in a written form 
appropriate for the domain.  In a commercial 
medical transcription operation, the non-literal 
transcript will present the dictation in a format 
appropriate for a medical record.  This typically 
involves (i.) ignoring filled pauses, pleasantries, 
and repeats; (ii.) acting on directions for repairs 
("delete the second paragraph and put this in 
instead..."); (iii.) adding non-dictated punctuation; 
(iv.) correcting grammatical errors; and (v.) re-
formatting certain phrases such as "Lung are 
Clear", to a standard form such as "Lungs - Clear". 
 
Literal:
 Literal transcriptions are exact 
transcriptions of what was spoken.  This includes 
any elements not found in the non-literal transcript, 
such as filled pauses (um's and ah's), pleasantries 
and body noises ("thank you very much, just a 
moment, cough"), repeats, fragments, repairs and 
directions for repairs, and asides ("make that 
bold").  Literal transcriptions require significant 
human effort, and therefore are expensive to 
produce.  Even though they are carefully prepared, 
some errors will be present in the result. 
 
In their study of how humans deal with transcribing 
spoken discourse, Lindsay and O'Connell (1995) 
have found that literal transcripts were "far from 
verbatim." (p.111) They find that the transcribers in 
their study tended to have the most difficulty 
transcribing hesitation phenomena, followed by 
sentence fragments, adverbs and conjunctions and, 
finally, nouns, verbs, adjectives and prepositions. 
Our informal observations made from the 
transcripts produced by highly trained medical 
transcriptionists suggest approximately 5% error 
margin and a gradation of errors similar to 
the one found by Lindsay and O'Connell. 
 
Semi-Literal: Semi-literal transcripts are derived 
using non-literal transcripts, the recognizer output, 
a set of grammars, a dictionary, and an interpreter 
to integrate the recognized material into the non-
literal transcription.  Semi-literal transcripts will 
more closely resemble the literal transcripts, as 
many of the elements missing from the non-literal 
transcripts will be restored. 
 
3 Model Adaptation 
 
It is well known that ASR systems perform best 
when acoustic models are adapted to a particular 
talker?s speech.  This is why commercial desktop 
systems use enrollment.  Although less widely 
applied, language model adaptation based on linear 
interpolation is an effective technique for tailoring 
stochastic grammars to particular domains of 
discourse and to particular speakers (Savova et al 
(2000), Weng et al (1997)).  
 
The training texts used in acoustic modeling come 
from recognizer-generated texts, literal 
transcriptions or non-literal transcriptions.  Within 
the family of transformation and combined 
approaches to acoustic modeling (Digalakis and 
Neumeyer (1996), Strom (1996), Wightman and 
Harder (1999), Hazen and Glass (1997)) three basic 
adaptation methods can be identified: unsupervised, 
supervised, or semi-supervised.  Each adaptation 
method depends on a different type of training text.  
What follows will briefly introduce the three 
methods. 
 
Unsupervised adaptation relies on the 
recognizer?s output as the text guiding the 
adaptation.  Efficacy of unsupervised adaptation 
fully depends on the recognition accuracy.  As 
Wightman and Harder (1999) pointed out, 
unsupervised adaptation works well in laboratory 
conditions when the speech signal has large 
bandwidth and is relatively ?clean? of background 
noise, throat clearings, and other disturbances.  In 
laboratory conditions, the errors introduced by 
unsupervised adaptation can be averaged out by 
using more data (Zavaliagkos and Colthurst, 1997); 
however, in a telephony operation with degraded 
input that is not feasible.  
 
Supervised adaptation is dependent on literal 
transcription availability and is widely used in 
enrollment in most desktop ASR systems.  A 
speaker?s speech sample is transcribed verbatim 
and then the speech signal is aligned with 
pronunciations frame by frame for each individual 
word.  A speaker independent model is augmented 
to include the observations resulting from the 
alignment. 
 
Semi-supervised adaptation rests on the idea that 
the speech signal can be partially aligned by using 
of the recognition output and the non-literal 
transcription.  A significant problem with semi-
supervised adaptation is that only the speech that 
the recognizer already recognizes successfully ends 
up being used for adaptation.  This reinforces what 
is already well represented in the model.  
Wightman and Harder (1999) report that semi-
supervised adaptation has a positive side effect of 
excluding those segments of speech that were mis-
recognized for reasons other than a poor acoustic 
model.  They note that background noise and 
speech disfluency are detrimental to the 
unsupervised adaptation.   
 
In addition to the two problems with semi-
supervised adaptation pointed out by Wightman 
and Harder, we find one more potential problem.  
As a result of matching the word labels produced 
by the recognizer and the non-literal transcription, 
some words may be skipped which may introduce 
unnatural phone transitions at word boundaries.  
 
Language model adaptation is not an appropriate 
domain for acoustic adaptation methods.  However, 
adapted language models can be loosely described 
as supervised or unsupervised, based on the types 
of training texts?literal or non-literal?that were 
used in building the model. 
 
In the following sections we will describe the 
system of generating data that is well suited for 
acoustic and language adaptation and present 
results of experimental evaluation of this system. 
 
3.2 Generating semi-literal data 
ATRS is based on reconstruction of non-literal 
transcriptions to train utterance specific language 
models.  First, a non-literal transcription is used to 
train an augmented probabilistic finite state model 
(APFSM) which is, in turn, used by the recognizer 
to re-recognize the exact same utterance that the 
non-literal transcription was generated from.  The 
APFSM is constructed by linear interpolation of a 
finite state model where all transitional 
probabilities are equal to 1 with two other 
stochastic models.   
 
One of the two models is a background model that 
accounts for expressions such as greetings, 
thanking, false starts and repairs.  A list of these 
out-of-transcription expressions is derived by 
comparing already existing literal transcriptions 
with their non-literal transcription counterparts.  
The other model represents the same non-literal 
transcription populated with filled pauses (FP) 
(?um?s and ah?s?) using a stochastic FP model 
derived from a relatively large corpus of literal 
transcriptions (Pakhomov, 1999, Pakhomov and 
Savova, 1999). 
 
 
 
Interpolation weights are established empirically by 
calculating the resulting model?s perplexity against 
held out data.  Out-of-vocabulary (OOV) items are 
handled provisionally by generating on-the-fly 
pronunciations based on the existing dictionary 
spelling-pronunciation alignments. The result of 
interpolating these two background models is that 
some of the transitional probabilities found in the 
finite state model are no longer 1. 
 
The language model so derived can now be used to 
produce a transcription that is likely to be more true 
to what has actually been said than the non-literal 
transcription that we started to work with. 
 
Further refinement of the new semi-literal 
transcription is carried out by using dynamic 
programming alignment on the recognizer?s 
hypothesis (HYP) and the non-literal transcription 
that is used as reference (REF).  The alignment 
results in each HYP label being designated as a 
MATCH, a DELETION, a SUBSTITUTION or an 
INSERTION.  Those labels present in the HYP 
stream that do not align with anything in the REF 
stream are designated as insertions and are assumed 
to represent the out-of-transcription elements of the 
dictation.  Those labels that do align but do not 
match are designated as substitutions.  Finally, the 
labels found in the REF stream that do not align 
with anything in the HYP stream are designated as 
deletions.   
 
 
The final semi-literal transcription is constructed 
differently depending on the intended purpose of  
 
 
Figure { SEQ Figure \* ARABIC } Percent improvement in true data representation 
of ATRS reconstruction vs. Non-Literal data 
the transcription.  If the transcription will be used 
for acoustic modeling, then the MATCHES, the  
REF portion of SUBSTITUTIONS and the HYP 
portion of only those INSERTIONS that represent 
punctuation and filled pauses make it into the final  
semi-literal transcription.  It is important to filter 
out everything else because acoustic modeling is 
very sensitive to misalignment errors.  Language 
modeling, on the other hand, is less sensitive to 
alignment errors; therefore, INSERTIONS and 
DELETIONS can be introduced into the semi-
literal transcription. 
 
One method of ascertaining the quality of semi-
literal reconstruction is to measure its alignment 
errors against literal data using a dynamic 
programming application.  By measuring the 
correctness spread between ATRS and literal data, 
as well as the correctness spread between non-
literal and literal data, the ATRS alignment 
correctness rate was observed to be 4.4% higher 
absolute over 774 dictation files tested. Chart 1 
summarizes the results. The X axis represents the 
number of dictations in each bin displayed along 
the Y axis representing the % improvement over 
the non-literal counterparts. The results showed 
nearly all ATRS files had better alignment 
correctness than their non-literal counterparts.  The 
majority of the reconstructed dictations resemble 
literal transcriptions between 1% and 8% better 
than their non-literal counterparts.  These results 
are statistically significant as evidenced by a t-test 
at 0.05 confidence level.  Much of the increase in 
alignment can be attributed to the introduction of 
filled pauses by ATRS.  However, ignoring filled 
pauses, we have observed informally that the 
correctness still improves in ATRS files versus 
non-literal. 
 
In the following sections we will address acoustic 
and language modeling and show that semi-literal 
training data is a good substitute for literal data.   
 
 
4 Experimental results 
 
The usefulness of semi-literal transcriptions was 
evaluated in two ways: acoustic adaptation and 
language modeling.  
 
4.1 Adapted acoustic model evaluation 
Three speaker adapted acoustic models were 
trained for each of the 5 talkers in this study using 
the three types of label files and evaluated on the 
talker?s testing data. 
 
4.1.1 Setup 
The data collected for each talker were split into 
testing and training. 
Training Data 
45-55 minutes of audio data was collected for each 
of the six talkers in this experiment: 
 
A female 
B female 
C male 
D male 
F female 
 
All talkers are native speakers of English, two 
males and three females. 
 
Non-literal transcriptions
 of this data were 
obtained in the course of normal transcription 
operation where trained medical transcriptionists 
record the dictations while filtering out disfluency, 
asides and ungrammatical utterances. 
 
Literal transcriptions
 were obtained by having 5 
medical transcriptionists specially trained not to 
filter out disfluency and asides transcribe all the 
dictations used in this study. 
 
Semi-literal transcriptions
 were obtained with the 
system described in section 5 of this paper. 
 
Testing Data  
Three dictations (0.5 ? 2 min) each were pulled out 
of the Literal transcriptions training set and set 
aside for each talker for testing. 
Recognition and evaluation software and 
formalism 
 
Software licensed from Entropic Laboratory was 
used for performing recognition, evaluating 
accuracy and acoustic adaptation. (Valtchev, et al 
(1998)). Adapted models were trained using MLLR 
technique (Legetter and Woodland, (1996)) 
available as part of the Entropic package.  
 
Recognition accuracy and correctness reported in 
this study were calculated according to the 
following formulas: 
 
(1) Acc = hits ? insertions / total words 
(2) Correctness = hits / total words 
 
 
4.1.2 Experiment 
The following Acoustic Models were trained via 
adaptation with a general SI model for each talker 
using all available data (except for the testing data). 
Each model?s name reflects the kind of label data 
that was used for training. 
 
LITERAL 
 
Each audio file was aligned with the corresponding 
literal transcription.  
 
NON-LITERAL 
 
Each audio file was recognized using SI acoustic 
and language models. The recognition output was 
aligned with the non-literal transcription using 
dynamic programming. Only those portions of 
audio that corresponded to direct matches in the 
alignment were used to produce alignments for 
acoustic modeling. This method was originally used 
for medical dictations by Wightman and Harder 
(1999). 
 
SEMI-LITERAL 
 
Each audio file has been processed to produce a 
semi-literal transcription that was then aligned with 
recognition output generated in the process of 
creating semi-literal transcriptions. The portions of 
the audio corresponding to matching segments were 
used for acoustic adaptation training. 
 
The SI model had been trained on all available at 
the time (12 hours)2 similar medical dictations to 
the ones used in this study. The data for the 
                                                        
2
 Although 50-100 hours of data for SI modeling is the 
industry standard, the population we are dealing with is 
highly homogeneous and reasonable results can be 
obtained with lesser amount of data. 
speakers in this study were not used in training the 
SI model.  
 
4.1.3 Results 
Table 1 shows the test results. As expected, both 
recognition accuracy and correctness increase with 
any of the three kinds of adaptation. Adaptation 
using Literal transcriptions yields an overall 
10.84% absolute gain in correctness and 11.49% in 
accuracy over the baseline. 
 
Adaptation using Non-literal transcriptions yields 
an overall 6.36 % absolute gain in correctness and 
5.23 % in accuracy over the baseline. Adaptation 
with Semi-literal transcriptions yields an overall 
11.39 % absolute gain in correctness and 11.05 % 
in accuracy over the baseline. No statistical 
significance tests were performed on this data. 
 
Table 1. Recognition results for three adaptation 
methods 
 
4.1.4 Discussion 
The results of this experiment provide additional 
support for using automatically generated semi-
literal transcriptions as a viable (and possibly 
superior) substitute for literal data. The fact that 
three SEMI-LITERAL adapted AM?s out of 5 
performed better than their LITERAL counterparts 
seems to indicate that there may be undesirable 
noise either in the literal transcriptions or in the 
corresponding audio. It may also be due to the 
relatively small amount of training data used for SI 
modeling thus providing a baseline that can be 
improved with little effort. However, the results 
still indicate that generating semi-literal 
transcriptions may help eliminate the undesirable 
noise and, at the same time, get the benefits of 
broader coverage that semi-literal transcripts can 
afford over NON-LITERAL transcriptions. 
 
 Baseline (SI) 
% 
Literal       
% 
Semi-literal 
% 
Non-literal       
% 
Talker Cor Acc Cor Acc Cor  Acc Cor Acc 
A 58.76 48.47 66.57 58.09 68 58.28 64.76 51.8 
B 41.28 32.2 58.36 49.46 64.59 56.22 55.87 44.66 
C 57.22 54.99 64.38 61.54 61.25 59.31 60.65 58.71 
D 56.86 51.47 68.69 63.3 65.91 59.13 64.69 58.26 
F 54.83 43.69 61.97 53.57 64.7 54.41 61.13 48.73 
         
AVG 52.49 44.81 63.33 56.3 63.81 55.86 58.85 50.04 
4.2 Language Model Evaluation 
For ASR applications where there are significant 
discrepancies between an utterance and its formal 
transcription, the inclusion of literal data in the 
language model can reduce language model 
perplexity and improve recognition accuracy.  In 
medical transcription, the non-literal texts typically 
depart from what has actually been said.  Hence if 
the talker says "lungs are clear" or "lungs sound 
pretty clear", the typed transcription is likely to 
have "Lungs - clear".  In addition, as we noted 
earlier, the non-literal transcription will omit 
disfluencies and asides and will correct 
grammatical errors. 
 
Literal and semi-literal texts can be added onto 
language model training data or interpolated into 
an existing language model. Below we will present 
results of a language modeling experiment that 
compares language models built from literal, semi-
literal and non-literal versions of the same training 
set.  The results substantiate our claim that 
automatically generated semi-literal transcription 
can lead to a significant improvement in language 
model quality. 
 
In order to test the proposed method?s suitability 
for language modeling, we constructed three 
trigram language models and used perplexity as the 
measure of the models? goodness. 
 
Setup 
The following models were trained on three 
versions of a 270,000-word corpus.  The size of the 
training corpus is dictated by availability of literal 
transcriptions.  The vocabulary was derived from a 
combination of all three corpora to keep the OOV 
rate constant. 
 
LLM ? language model built from a corpus of 
literal transcriptions  
NLM ? language model built from non-literal 
transcriptions 
SLM ? language model built from semi-literal 
transcriptions  
 
Approximately 5,000-word literal transcriptions 
corpus consisting of 24 dictations was set aside for 
testing 
 
Results 
The results of perplexity tests of the three models 
on the held-out data at 3-gram level are 
summarized in Table 2. The tests were carried out 
using the Entropic Transcriber Toolkit 
 
It is apparent that SLM yields considerably better 
perplexity than NLM, which indicates that although 
semi-literal transcriptions are not as good as actual  
literal transcriptions, they are more suitable for  
 
Table 2.  Perplexity tests on LLM, NLM, SLM 
 
language modeling than non-literal transcriptions.  
These results are obtained with 270,000 words of 
training data; however, the typical amount is 
dozens of million. We would expect the differences 
in perplexity to become smaller with larger 
amounts of training data. 
 
Conclusions and future work 
 
We have described ATRS, a system for 
reconstructing semi-literal transcriptions 
automatically.  ATRS texts can be used as a 
substitute for literal transcriptions when the cost 
and time required for generating literal 
transcriptions are infeasible, e.g. in a telephony 
based transcription operation that processes 
thousands of acoustic and language models.  Texts 
produced with ATRS were used in training speaker 
adapted acoustic models, speaker independent 
acoustic models and language models.  
Experimental results show that models built from 
ATRS training data yield performance results that 
are equivalent to those obtained with models 
trained on literal transcriptions. In the future, we 
will address the issue of the amount of training data 
for the SI model. Also, current ATRS system does 
not take advantage of various confidence scores 
available in leading recognition engines. We 
believe that using such confidence measures can 
improve the generation of semi-literal transcriptions 
considerably. We would also like to investigate the 
point at which the size of the various kinds of data 
 Perplexity OOV rate (%) 
LLM 185 2.61 
NLM 613 2.61 
SLM 313 2.61 
used for adaptation stops making improvements in 
recognition accuracy.  
 
Acknowledgements 
We would like to thank the anonymous reviewers 
of this paper for very helpful feedback. We thank 
Guergana Savova for excellent suggestions and 
enthusiastic support. We would also like to thank 
Jim Wu for valuable input.   
 
References 
 
Digalakis, V and Neumyer, L. (1996). Speaker 
Adaptation Using Combined Transformation 
and Baysean Mehtods. IEEE Trans. Speech and 
Audio Processing. 
Hazen, T and Glass, J (1997). A Comparison of Novel 
Techniques for Instantaneous Speaker 
Adaptation. In Proc. Eurospeech ?97. 
Heeman, P., Loken-Kim, K and Allen J. (1996). 
Combining the Detection and Correction of 
Speech Repairs. In Proc. ICSLP ?96. 
Huang, X.  and Lee, K (1993).  On Speaker ?
Independent, Speaker-Dependent, and Speaker-
Adaptive Speech Recognition.  In IEEE 
Transactions on Speech and Audio processing, 
Vol.  1, No.  2, pp.  150 ? 157.   
Legetter, C. and Woodland, P. (1996). Maximum 
Likelihood Linear Regression for Speaker 
Adaptation of Continuous Density HMM?s. In 
Computer Speech and Language , 9, (171-186). 
Pakhomov, S.  (1999).  Modeling Filled Pauses in 
Medical Transcriptions.  In Student Section of 
Proc.  ACL?99. 
Pakhomov, S and Savova, G.  (1999).  Filled Pause 
Modeling in Quasi-Spontaneous Speech.  In 
Proc.  Disfluency in Spontaneous Speech 
Workshop at ICPHIS ?99. 
Savova, G, Schonwetter, M. and Pakhomov, S. (2000).  
Improving language model perplexity and 
recognition accuracy for medical dictations via 
within-domaininterpolation with literal and 
semi-literal corpora " In Proc. ICSLP ?00. 
Shriberg, E. 1994 Preliminaries to a Theory of Speech 
Disfluencies. Ph. D. thesis, University of 
California at Berkely. 
Shriberg, E. and Stolcke, A. (1996). Word Predictability 
after Hesitations: A Corpus-based Study. In 
Proc. ICSLP ?96. 
Siu, M and Ostendorf, M. (1996). Modeling Disfluencies 
in Conversational Speech. In Proc. ICSLP ?96. 
Stolcke, A. and Shriberg, E. (1996). Statistical Language 
Modeling for Speech Disfluencies. In proc. 
ICASSP ?96. 
Stolcke A., Shriberg E., Bates R., Ostendorf M., Hakkani 
D., Plauche M., Tur G., and Lu  Y. (1998). 
Automatic Detection of Sentence Boundaries 
and Disfluencies based on Recognized Words. 
Proc. Intl. Conf. on Spoken Language 
Processing. 
Str?m, N (1996): "Speaker Adaptation by Modeling the 
Speaker Variation in a Continuous Speech 
Recognition System," In Proc. ICSLP '96, 
Philadelphia, pp. 989-992.  
Valtchev, V.  Kershaw, D.  and Odell, J.  (1998).  The 
Truetalk Transcriber Book.  Entropic 
Cambridge Research Laboratory, Cambridge, 
England. 
Wightman, C.  W.  and Harder T.  A.  (1999).  Semi-
Supervised Adaptation of Acoustic Models for 
Large-Volume Dictation? In Proc. Eurospeech 
?98. pp 1371-1374. 
Weng, F.,  Stolcke, A., Sankar, A.  (1997). Hub4 
Language Modeling Using Domain 
Interpolation and Data Clustering. Proc. 
DARPA Speech Recognition Workshop, pp. 
147-151, Chantilly, VA. 
 
 
  
  
 
 
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 41?48
Manchester, August 2008
Verification and Implementation of Language-Based Deception 
Indicators in Civil and Criminal Narratives 
 
 
Joan Bachenko 
Deception Discovery Technologies 
Oxford, NJ 07863 
jbachenko@comcast.net 
 
Eileen Fitzpatrick 
Montclair State University 
Montclair, NJ 07043 
fitzpatricke@mail.montclair.edu
Michael Schonwetter 
Deception Discovery Technologies 
Minneapolis, MN 55416 
mschonwetter@synchronvideo.com 
 
Abstract 
Our goal is to use natural language proc-
essing to identify deceptive and non-
deceptive passages in transcribed narra-
tives.  We begin by motivating an analy-
sis of language-based deception that 
relies on specific linguistic indicators to 
discover deceptive statements.  The indi-
cator tags are assigned to a document us-
ing a mix of automated and manual 
methods.  Once the tags are assigned, an 
interpreter automatically discriminates 
between deceptive and truthful state-
ments based on tag densities.  The texts 
used in our study come entirely from 
?real world? sources?criminal state-
ments, police interrogations and legal tes-
timony.  The corpus was hand-tagged for 
the truth value of all propositions that 
could be externally verified as true or 
false. Classification and Regression Tree 
techniques suggest that the approach is 
feasible, with the model able to identify 
74.9% of the T/F propositions correctly. 
Implementation of an automatic tagger 
with a large subset of tags performed 
well on test data, producing an average 
score of 68.6% recall and 85.3% preci-
                                                          
 ? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
sion when compared to the performance 
of human taggers on the same subset.   
1. Introduction 
 
The ability to detect deceptive statements in text 
and speech has broad applications in law en-
forcement and intelligence gathering. The scien-
tific study of deception in language dates at least 
from Undeutsch (1954, 1989), who hypothesized 
that it is ?not the veracity of the reporting person 
but the truthfulness of the statement that matters 
and there are certain relatively exact, definable, 
descriptive criteria that form a key tool for the 
determination of the truthfulness of statements?. 
Reviews by Shuy (1998), Vrij (2000), and De-
Paulo et al (2003) indicate that many types of 
deception can be identified because the liar?s 
verbal and non-verbal behavior varies considera-
bly from that of the truth teller?s.  Even so, the 
literature reports that human lie detectors rarely 
perform at a level above chance. Vrij (2000) 
gives a summary of 39 studies of human ability 
to detect lies. The majority of the studies report 
accuracy rates between 45-60%, with the mean 
accuracy rate at 56.6%.  
The goal of our research is to develop and 
implement a system for automatically identifying 
deceptive and truthful statements in narratives 
and transcribed interviews. We focus exclusively 
on verbal cues to deception for this initial 
experiment,  ignoring at present potential 
prosodic cues (but see Hirschberg et al).   
 
41
In this paper, we describe a language-based 
analysis of deception that we have constructed 
and tested using ?real world? sources?criminal 
narratives, police interrogations and legal 
testimony.  Our analysis comprises two 
components:  a set of deception indicators that 
are used for tagging a document and an 
interpreter that associates tag clusters with a 
deception likelihood.   We tested the analysis by 
identifying propositions in the corpus that could 
be verified as true or false and then comparing 
the predictions of our model against this corpus 
of ground truth. Our analysis acheived an 
accuracy rate of 74.9%. In the remainder of this 
paper, we will present the analysis and a detailed 
description of our test results.  Implementation of 
the analysis will also be discussed.  
2. Studying Deception 
The literature on deception comes primarily from 
experimental psychology where much of the 
concentration is on lies in social life and much of 
the experimentation is done in laboratory settings 
where subjects are prompted to lie1. These stud-
ies lack the element of deception under stress. 
Because of the difficulties of collecting and cor-
roborating testimony in legal settings, analysis of 
so-called ?high stakes? data is harder to come by. 
To our knowledge, only two studies (Smith, 
2001; Adams, 2002) correlate linguistic cues 
with deception using high stakes data.  For our 
data we have relied exclusively on police de-
partment transcripts and high profile cases where 
the ground truth facts of the case can be estab-
lished. 
Previous studies correlating linguistic fea-
tures with deceptive behavior (Smith, 2001; Ad-
ams, 2002; Newman et al 2003, and studies cited 
in DePaulo et al 2003) have classified narrators 
as truth-tellers or liars according to the presence, 
number and distribution of deception indicators 
in their narratives. Newman, et al (2003), for 
example, proposes an analysis based on word 
likelihoods for semantically defined items such 
as action verbs, negative emotion words and pro-
nouns. Narratives for their study were generated 
in the laboratory by student subjects.  The goals 
of the project were to determine how well their 
word likelihood analysis classified the presumed 
author of each narrative as a liar or truth-teller 
and to compare their system's performance to 
that of human subjects.  The analysis correctly 
                                                          
 
 statements.    
ive load.  
d Stiff, 1993). 
1 We define deception as a deliberate attempt to mislead. 
We use the terms lying and deceiving interchangeably. 
achieved an overall distinction between liars and 
truth tellers 61% of the time.   
Our research on deception detection differs 
from most previous work in two important ways. 
First, we analyze naturally occurring data, i.e. 
actual civil and criminal narratives instead of 
laboratory generated data.  This gives us access 
to productions that cannot be replicated in 
laboratory experiments for ethical reasons.  
Second, we focus on the classification of specific 
statements within a narrative rather than 
characterizing an entire narrative or speaker as 
truthful or deceptive.  We assume that narrators 
are neither always truthful nor always deceptive. 
Rather, every narrative consists of declarations, 
or assertions of fact, that retain a constant value 
of truth or falsehood. In this respect, we are close 
to Undeutsch?s hypothesis in that we are not 
testing the veracity of the narrator but the 
truthfulness of the narrator?s
The purpose of our analysis is to assist 
human evaluators (e.g. legal professionals, 
intelligence analysts, employment interviewers) 
in assessing a text?s contents.  Hence the 
questions that we must answer are whether it is 
possible to classify specific declarations as true 
or deceptive using only linguistic cues and, if so, 
then how successfully an automated system can 
perform the task.  Our research makes no claim 
as to the cause of a speaker?s behavior, e.g. 
whether deception cues emerge as a function of 
emotional stress or excessive cognit
3. Linguistic Markers of Deception  
The literature on verbal cues to deception 
indicates that fabricated narrative may differ 
from truthful narrative at all levels from global 
discourse to individual word choice. Features of 
narrative structure and length, text coherence, 
factual and sensory detail, filled pauses, syntactic 
structure choice, verbal immediacy, negative 
expressions, tentative constructions, referential 
expressions, and particular phrasings have all 
been shown to differentiate truthful from 
deceptive statements in text (Adams, 2002; 
DePaulo et al, 2003; Miller an
In the area of forensic psychology, Statement 
Validity Assessment is the most commonly used 
technique for measuring the veracity of verbal 
statements. SVA examines a transcribed inter-
view for 19 criteria such as quantity of detail, 
embedding of the narrative in context, descrip-
tions of interactions and reproduction of conver-
sations (Steller & K?hnken, 1989). Tests of SVA 
 
42
show that users are able to detect deception 
above the level of chance -- the level at which 
the lay person functions in identifying deception 
? with some criteria performing considerably 
better (Vrij, 2000). An SVA analysis is admissi-
ble as court evidence in Germany, the Nether-
lands, and Sweden. 
In the criminal justice arena, another tech-
nique, Statement Analysis, or Scientific Content 
Analysis (SCAN), (Sapir, 1987) examines open-
ended written accounts in which the writers 
choose where to begin and what to include in the 
statements. According to Sapir (1995) ?when 
people are given the choice to give their own 
explanation in their own words, they would 
choose to be truthful . . . . it is very difficult to lie 
with commitment.? 
SCAN ?claims to be able to detect instances of 
potential deception within the language behav-
iour of an individual; it does not claim to identify 
whether the suspect is lying? (Smith, 2001). As 
such, its goal is the one we have adopted: to 
highlight areas of a text that require clarification 
as part of an interview strategy. 
Despite SCAN?s claim that it does not aim to 
classify a suspect as truthful or deceptive, the 
validations of SCAN cues to deception to date 
(Smith, 2001; Adams, 2002) evaluate the tech-
nique against entire statements classified as T or 
F. Our approach differs in that we evaluate sepa-
rately portions of the statement as true or decep-
tive based on the density of cues in that portion.  
4. Deception Analysis for an NLP System 
Our analysis is produced by two passes over the 
input text.  In the first pass the text is tagged for 
deception indicators using a mix of automated 
and manual techniques.  In the second pass the 
text is sent to an automated interpreter that calcu-
lates tag density using moving average and word 
proximity measures.    The output of the inter-
preter is a segmentation of the text into truthful 
and deceptive areas. 
4.1 Deception Indicators 
We have selected 12 linguistic indicators of de-
ception cited in the psychological and criminal 
justice literature that can be formally represented 
and automated in an NLP system.  The indicators 
fall into three classes.  
(1) Lack of commitment to a statement or dec-
laration.  The speaker uses linguistic devices to 
avoid making a direct statement of fact.  Five of 
the indicators fit into this class: (i) linguistic 
hedges (described below) including non-factive 
verbs and nominals; (ii) qualified assertions, 
which leave open whether an act was performed, 
e.g. I needed to get my inhaler; (iii) unexplained 
lapses of time, e.g. later that day; (iv) overzeal-
ous expressions, e.g. I swear to God, and (v) ra-
tionalization of an action, e.g. I was unfamiliar 
with the road. 
(2)  Preference for negative expressions in 
word choice, syntactic structure and semantics.  
This class comprises three indicators: (i) negative 
forms, either complete words such as never or 
negative morphemes as in inconceivable; (ii) 
negative emotions, e.g. I was a nervous wreck; 
(iii) memory loss, e.g. I forget. 
(3)  Inconsistencies with respect to verb and 
noun forms. Four of the indicators make up this 
class: (i) verb tense changes (described below); 
(ii) thematic role changes, e.g. changing the the-
matic role of a NP from agent in one sentence to 
patient in another; (iii) noun phrase changes, 
where different NP forms are used for the same 
referent or to change the focus of a narrative; (iv) 
pronoun changes (described below) which are 
similar to noun phrase changes  
To clarify our exposition, three of the indica-
tors are described in more detail below. It is im-
portant to note with respect to these indicators of 
deception that deceptive passages vary consid-
erably in the types and mix of indicators used, 
and the particular words used within an indicator 
type vary depending on factors such as race, 
gender, and socioeconomic status. 
Verb Tense 
The literature assumes that past tense narrative is 
the norm for truthful accounts of past events 
(Dulaney, 1982; Sapir, 1987; Rudacille, 1994). 
However, as Porter and Yuille (1996) demon-
strate, it is deviations from the past tense that 
correlate with deception. Indeed, changes in 
tense are often more indicative of deception than 
the overall choice of tense. The most often cited 
example of tense change in a criminal statement 
is that of Susan Smith, who released the brake on 
her car letting her two small children inside 
plunge to their deaths. "I just feel hopeless," she 
said. "I can't do enough. My children wanted me. 
They needed me. And now I can't help them. I 
just feel like such a failure." While her state-
ments about herself were couched in the present 
tense, those about her children were already in 
the past.
 
43
Hedges 
The terms ?hedge? and ?hedging? were intro-
duced by Lakoff (1972) to describe words 
?whose meaning implicitly involves fuzziness?, 
e.g., maybe, I guess, and sort of. The use of 
hedges has been widely studied in logic and 
pragmatics, and for practical applications like 
translation and language teaching (for a review, 
see Schr?der & Zimmer, 1997). In the forensic 
psychology literature, it has been correlated with 
deception (Knapp et al, 1974; Porter & Yuille, 
1996; Vrij & Heaven, 1999). 
Hedge types in our data include non-factive 
verbs like think and believe, non-factive NPs like 
my understanding and my recollection, epistemic 
adjectives and adverbs like possible and ap-
proximately, indefinite NPs like something and 
stuff, and miscellaneous phrases like a glimpse 
and between 9 and 9:30. 
The particular types of hedging that appear in 
our data depend heavily on the socioeconomic 
status of the speaker and the type of crime. The 
285 hedges in Jeffrey Skilling?s 7562 word En-
ron testimony include 21 cases of my recollec-
tion, 9 of my understanding, and 7 of to my 
knowledge while the 42 hedges in the car thief?s 
2282 word testimony include 6 cases of shit (do-
ing a little painting, and roofing, and shit), 6 of 
just and 4 of probably.  Despite the differences in 
style, however, the deceptive behavior in both 
cases is similar. 
Changes in Referential Expressions 
Laboratory studies of deception have found that 
deceivers tend to use fewer self-referencing ex-
pressions (I, my, mine) than truth-tellers and 
fewer references to others (Knapp et al, 1974; 
Dulaney, 1982; Newman et al, 2003). In exam-
ining a specific real world narrative, however, it 
is impossible to tell what a narrator?s truthful 
baseline use of referential expressions is, so the 
laboratory findings are hard to carry over to ac-
tual criminal narratives.  
On the other hand, changes in the use of refer-
ential expressions, like changes in verb tense, 
have also been cited as indicative of deception 
(Sapir, 1987; Adams, 1996), and these changes 
can be captured formally. Such changes in refer-
ence often involve the distancing of an item; for 
example, in the narrative of Captain McDonald, 
he describes ?my wife? and ?my daughter? sleep-
ing, but he reports the crime to an emergency 
number as follows, with his wife and daughter 
referred to as some people: 
 
So I told him that I needed a doctor and an 
ambulance and that some people had been 
stabbed. 
 
Deceptive statements may also omit refer-
ences entirely. Scott Peterson?s initial police in-
terview is characterized by a high number of 
omitted first person references: 
 
BROCCHINI: You drive straight home? 
PETERSON: To the warehouse, dropped 
off the boat. 
4.2 Identifying a Text Passage as Deceptive or 
Non-deceptive 
The presence or absence of a cue is not in itself 
sufficient to determine whether the language is 
deceptive or truthful.  Linguistic hedges and 
other deception indicators often occur in normal 
language use.  We hypothesize, however, that the 
distribution and density of the indicators would 
correlate with deceptive behavior.2  Areas of a 
narrative that contain a clustering of deceptive 
material may consist of outright lies or they may 
be evasive or misleading, while areas lacking in 
indicator clusters are likely to be truthful. 
   We use a moving average (MA) program to 
find clusters of indicators in a text.  Initially, the 
MA assigns each word in the text a proximity 
score based on its distance, measured in word 
count, to the nearest deception indicator.  Each 
score is then recalculated by applying a MA 
window of N words.  The MA sums the scores 
for N/2 words to the left and right of the current 
word and divides the result by N to obtain the 
revised score.  Clusters of low word scores indi-
cate deceptive areas of the text, high scoring 
clusters indicate truthful areas.  Hence, when 
applied to a text, the MA allows us to segment an 
entire text automatically into non-overlapping 
regions that are identified as likely true, likely 
deceptive or somewhere in between. 
   Our approach assumes that the input text will 
contain sufficient language to display scoring 
patterns. This rules out, for example, polygraph 
tests where answers are confined to Yes or No as 
                                                          
2 Currently the density algorithm does not take into account 
the possibility that some indicators may be more important 
than others. We plan to use the results of this initial test to 
determine the relative contribution of each tag type to the 
accuracy of the identification of deception. 
 
44
well as short answer interviews that focus on 
simple factual statements such as names and ad-
dresses.  Based on the data  examined so far, we 
estimate the analysis requires a minimum 100 
words to produce useful results. 
5. Corpora and Annotation 
The corpus used for developing our approach to 
deception detection was assembled from criminal 
statements, police interrogations, depositions and 
legal testimony; the texts describe a mix of vio-
lent and property crimes, white collar crime and 
civil litigation.  Because of the difficulty in ob-
taining corpora and ground truth information, the 
total corpus size is small--slightly over 30,000 
words. 
For this experiment, we selected a corpus sub-
set of 25,687 words.  Table 1 summarizes the 
corpus subset: 
 
Source 
 
Word Count 
 
Criminal statements (3) 1,527
Police interrogations (2) 3,922
Tobacco lawsuit deposition 12,762
Enron congress. testimony 7,476
 
Total 
 
25,687
 
Table 1. Corpora Used in the Experiment 
 
Each document in the experimental corpus 
was tagged for two factors: (1) linguistic decep-
tion indicators marked words and phrases associ-
ated with deception, and (2) True/False tags 
marked propositions that were externally veri-
fied. 
5.1. Linguistic Annotation (Tagging) 
A team of linguists tagged the corpus for the 
twelve linguistic indicators of deception de-
scribed above. For each document in the corpus, 
two people assigned the deception tags inde-
pendently.  Differences in tagging were then ad-
judicated by the two taggers and a third linguist.  
Because the original tagging work was focused 
on research and discovery, inter-rater reliability 
statistics are not very revealing.  However, cur-
rent work on new corpora more closely resem-
bles other tagging tasks.  In this case we have 
found inter-rater reliability at 96%. 
Tagging decisions were guided by a tagging 
manual that we developed.  The manual provides 
extensive descriptions and examples of each tag 
type.  Taggers did not have access to ground 
truth facts that could have influenced their tag 
assignments.   
5.2. True/False Annotation  
We then examined separate copies of each narra-
tive for propositions that could be externally 
verified. The following is a single proposition 
that asserts, despite its length, one verifiable 
claim?the birthrate went down: 
 
The number of births peaked in about 1955 
and from there on each year there were fewer 
births. As a result of that each year after 1973 
fewer people turned 18 so the company could 
no longer rely on this tremendous number of 
baby boomers reaching smoking age.  
  
Only propositions that could be verified were 
used. Verification came from supporting material 
such as police reports and court documents and 
from statements internal to the narrative, e.g. a 
confession at the end of an interview could be 
used to support or refute specific claims within 
the interview. The initial verification tagging was 
done by technical and legal researchers on the 
project.  The T/F tags were later reviewed by at 
least one other technical researcher. 
The experimental corpus contains 275 verifi-
able propositions. Table 2 gives examples of 
verified propositions in the corpus. 
 
Example True False 
I didn't do work specifically on 
teenage smoking 
 ? 
All right, man, I did it, the 
damage 
?  
 
Black male wearing a coat.  ? 
 
Table 2. Examples of Verified Propositions 
6. Results 
The dataset contained 275 propositions, of which 
164, or 59.6%, were externally verified as False 
and the remainder verified as True.  We tested 
the ability of the model to predict T/F using 
Classification and Regression Tree (CART) 
analysis (Breiman, et al 1984)3 with 25-fold 
cross-validation and a misclassification cost that 
penalizes True misclassified as False. Table 3 
shows the results of the CART analysis: 
                                                          
3 We used the QUEST program described in Loh and Shih 
(1997) for the modeling. QUEST is available at 
http://www.stat.wisc.edu/~loh/quest.html.
 
45
 
Predicted Class 
 False True % Correct 
False 124 40 75.6
  
Actual 
Class 
True 29 82 73.8
 
Table 3. T/F Classification Based on Cue Den-
sity 
 
We can conclude that the model identifies de-
ceptive language at a rate significantly better 
than chance.  Moreover, by tuning the scores to 
favor high recall for false propositions, it be-
comes possible to adapt the model to applications 
where low precision on true propositions is not a 
drawback, e.g. pre-trial interviews where investi-
gators are looking for leads.  The results in Table 
4 show how we might gear the analysis to this 
class of applications. 
 
 
Predicted Class 
 False True % Correct 
False 151 13 92.6
 
Actual 
Class 
True 66 45 40.5
 
Table 4. Penalizing F Misclassified as T  
 
 Finally, it should be noted that input to 
the analysis consisted of individual files with 
some files marked for topic changes.  In prepar-
ing the data for this test, we found that, in many 
cases, the moving average allowed the low 
scores assigned to deceptive language to influ-
ence the scores of nearby truthful language.  This 
typically occurs when the narrative contains a 
change in topic.  For example, in the deposition 
excerpt below, there is a topic change from teen-
age smokers to the definition of psychographic 
studies.  The hedge so far as I know belongs with 
the first topic but not the second.  However, the 
moving average allows the low scores triggered 
by the hedge to improperly affect scores in the 
new topic:  
 
Q:   Do you know anybody who did have 
data that would allow a market penetra-
tion study of the type I've asked about to 
be performed. 
A:  {So far as I know%HEDGE} only the 
federal government. 
Q:   Are you familiar with the phrase 
psychographic study from your work at 
Philip Morris? 
A:  Yes. 
Q:   What is a psychographic study? 
 
To mitigate the effect of topic change, we in-
serted eleven topic change boundaries. The re-
sults suggest that language is "reset" when a new 
topic is introduced by the interviewer or inter-
viewee.   
 
7. A Deception Indicator Tagger 
The results described in the previous section pro-
vide support for the deception indicator (DI) ap-
proach we have developed.  For the 
implementation, we selected a subset of tags 
whose contextual conditions were well estab-
lished by the literature and our own investiga-
tion.  In these cases we were able to formalize 
the rules for automatic assignment of the tags.  
We excluded tags whose contextual conditions 
are still being researched, i.e., tag assignments 
that require human judgment. 
The tagger was constructed as a rule-based 
system that uses a combination of context-free 
and context sensitive substitutions.  An example 
of a context free substitution is ?Mark all occur-
rences of Oh, God as an overzealous statement?.  
A context sensitive substitution is the rule that 
interprets something as a hedge if it is not modi-
fied, i.e., followed by a relative clause or prepo-
sitional phrase.   
In some cases the tagger refers to structure 
and part of speech.  For example, may as a modal 
verb (may_MD) is a hedge.  Certain verb+ infini-
tive complement constructions, e.g. I attempted 
to open the door, make up a qualified assertion.  
Syntactic structure is assigned by the CASS 
chunk parser (Abney, 1990).  Part of speech tags 
are assigned by Brill?s tagger (Brill, 1992).   The 
DI tag rules apply to the output of the parser and 
POS tagger.  
The subset of tags implemented in the tagger 
comprises 86% of all tags that occur in the train-
ing corpus.  To see how well the DI tagger cov-
ered the subset, we first ran the tagger on the 
training corpus.  70% of the subset tags were cor-
rectly identified in that corpus, with 76% preci-
sion.  We then tested the tagger on a test corpus 
of three files.  Each file was also handtagged by 
linguistic researchers on this project.  The results 
of the test are given in Table 5.  Tag amounts 
refer to the number of tags belonging to the sub-
set that was implemented.   
 
 
46
File name Handtags Autotags Correct 
Tags 
confession 31 20 19 
peterson 186 160 108 
deposition 720 665 625 
Total 937 845 752 
 
Table 5. DI Tagger Results on Three Test Files 
 
Table 6 provides a summary of the tagger?s 
performance. 
 
File name Recall Precision 
confession .61 .95 
peterson .58 .675 
deposition .868 .939 
Average .686 .853 
 
Table 6. Summary of DI Tagger Results 
 
These results may reflect a bias in our training 
data towards legal testimony?depositions are 
strongly represented in the corpus, police and 
criminal data less so.  Our test corpus consists of 
a police interview (?peterson?), a criminal state-
ment (?confession?) and a deposition (?deposi-
tion?).  The tagger?s best performance is 
associated with the deposition. 
8. Conclusion 
This paper has presented new results in the study 
of language-based cues to deception and truth-
fulness; these results come entirely from ?real 
world? sources?criminal narratives, interroga-
tions, and legal testimony.  Our goal is to provide 
a method of evaluating declarations within a sin-
gle narrative or document rather than deeming an 
entire narrative (or narrator) as truthful or decep-
tive.   
We first compared the predictions of linguistic 
cues that we adapted from the literature on de-
ception against actual True/False values that 
were manually determined for 275 propositions 
in our corpus.  Predictions from the linguistic 
indicators were determined by scoring the den-
sity of indicators in text areas that contain the 
propositions and using classification and regres-
sion to determine cut-off values for truth prob-
abilities.   
We then evaluated the performance of an 
automated tagger that implements a large subset 
of the linguistic indicators verified in our first 
experiment.  The automated tagger performed 
well on test data, averaging 80.2% correct when 
compared with human performance on the same 
data. 
The results strongly suggest that linguistic 
cues provide a guide to deceptive areas of a text.  
The predictions based on linguistic cues were 
correct in distinguishing False propositions over 
75% of the time, and over 90% for applications 
where recall of False, but not True, is required.  
Results of the automatic tagger?s performance 
suggest that we will eventually achieve a fully 
automated system for processing depositions and 
other documents in which veracity is an impor-
tant issue.  
References  
Abney, S.  1990.  Rapid incremental parsing with 
repair.  In Proceedings of the 6th New OED 
Conference: Electronic Text Research, pp. 1-
9.  University of Waterloo, Waterloo, Ontario. 
Adams, S. 1996. Statement analysis: What do 
suspects words really reveal? The FBI Law 
Enforcement Bulletin. 65(10). 
www.fbi.gov/publications/leb/1996/oct964.txt 
Adams, S. 2002. Communication under stress: 
indicators of veracity and deception in written 
narratives. Ph.D. dissertation, Virginia Poly-
technic Institute and State University 
Brill, E.  1992.  A simple rule-based part-of-
speech tagger.  In Proceedings of the Third 
Conference on Applied Natural Language 
Processing, pp. 152-155.  Trento, Italy. 
DePaulo, B. M., J.J. Lindsay, B.E. Malone, L. 
Muhlenbruck, K. Charlton, and H. Cooper. 
2003. Cues to deception. Psychological Bulle-
tin, 129(1), 74-118. 
Dulaney, E.F. Jr. 1982. Changes in language be-
havior as a function of veracity. Human Com-
munication Research 9, 75-82. 
Hirschberg, J., S. Benus, J. Brenier, F. Enos, S. 
Friedman, S. Gilman, C. Girand, M. Graci-
arena, A. Kathol, L. Michaelis, B. Pellom, E. 
Shriberg and A. Stolcke. 2005. 
INTERSPEECH 2005. Sept. 408, Lisbon, Por-
tugal. 
Knapp, M.L., Hart, R.P., and Dennis, H.S. 1974. 
An exploration of deception as a communica-
tion construct. Human Communication Re-
search, 1, 15-29. 
Lakoff, G.  1972.  Hedges: A study in meaning 
criteria and the logic of fuzzy concepts.  In 
 
47
Papers from the 8th Regional Meeting, Chi-
cago Linguistic Society.   
Loh, W.-Y. and Shih, Y.-S. 1997. Split selection 
methods for classification trees. Statistica 
Sinica 7:815-840. 
Miller, G. R. and J. B. Stiff.  1993. Deceptive 
Communication.  Sage Publications. Thousand 
Oaks, CA. 
Newman, M. L., Pennebaker, J. W., Berry, D. S. 
and J. M. Richards.  2003.  Lying words: pre-
dicting deception from linguistic styles.  Per-
sonality and Social Psychology Bulletin. 29, 
665-675. 
Porter, S. & Yuille, J. (1996). The language of 
deceit: An investigation of the verbal clues in 
the interrogation context. Law & Human Be-
havior, 20(4) 443-458. 
Rudacille, W.C. 1994. Identifying Lies in Dis-
guise. Kendall Hunt. Dubuque, IO. 
Sapir, A. 1987. Scientific Content Analysis 
(SCAN). Laboratory of Scientific Interroga-
tion. Phoenix, AZ. 
Sapir, A. 1995. The View Guidebook: Verbal 
Inquiry ? the Effective Witness. Laboratory of 
Scientific Interrogation. Phoenix, AZ. 
Schr?der, H. and D. Zimmer. 1997. Hedging re-
search in pragmatics: A bibliographical re-
search guide to hedging. In R. Markkanen and 
H. Schroder (eds.) Hedging and Discourse: 
Approaches to the Analysis of a Pragmatic 
Phenomenon in Academic Text. Walter de 
Gruyter, Berlin. 
Shuy, R.  1998.  The Language of Confession, 
Interrogation and Deception. Sage Publica-
tions, Thousand Oaks, CA. 
Smith, N. 2001. Reading between the lines: An 
evaluation of the scientific content analysis 
technique (SCAN). Police Research Series.  
London,UK. 
www.homeoffice.gov.uk/rds/prgpdfs/prs135.pdf 
Steller, M. and G. Kohnken. 1989. Criteria-
Based Content Analysis. In D.C. Raskin (ed.) 
Psychological Methods in Criminal Investiga-
tion and Evidence. Springer-Verlag, New 
York, 217-245. 
Undeutsch, U. 1989. The development of state-
ment reality analysis. In J.C. Yuille (ed.) 
Credibility Assessment. Dordrecht: Kluwer, 
101-121. 
Undeutsch, U. (1954). Die Entwicklung der ge-
richtspsychologischen Gutachtertatigkeit. In 
A. Wellek (Ed.), Bericht uber den 19, Kon-
gress der Deutschen Gesellschaft fur Psy-
chologie (pp. 1132-154). Gottingen: Verlag 
fur Psychologie. 
Vrij, A. 2000. Detecting Lies and Deceit. John 
Wiley & Sons, Chichester, UK. 
Vrij, A. and Heaven, S. 1999. Vocal and verbal 
indicators of deception as a function of lie 
complexity. Psychology, Crime, and Law 5, 
203-215. 
 
48
